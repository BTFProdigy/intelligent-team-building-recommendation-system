Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 59?68,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Discriminative Learning of Selectional Preference from Unlabeled Text
Shane Bergsma
Department of Computing Science
University of Alberta
Edmonton, Alberta
Canada, T6G 2E8
bergsma@cs.ualberta.ca
Dekang Lin
Google, Inc.
1600 Amphitheatre Parkway
Mountain View
California, 94301
lindek@google.com
Randy Goebel
Department of Computing Science
University of Alberta
Edmonton, Alberta
Canada, T6G 2E8
goebel@cs.ualberta.ca
Abstract
We present a discriminative method for learn-
ing selectional preferences from unlabeled
text. Positive examples are taken from ob-
served predicate-argument pairs, while nega-
tives are constructed from unobserved combi-
nations. We train a Support Vector Machine
classifier to distinguish the positive from the
negative instances. We show how to parti-
tion the examples for efficient training with
57 thousand features and 6.5 million training
instances. The model outperforms other re-
cent approaches, achieving excellent correla-
tion with human plausibility judgments. Com-
pared to Mutual Information, it identifies 66%
more verb-object pairs in unseen text, and re-
solves 37% more pronouns correctly in a pro-
noun resolution experiment.
1 Introduction
Selectional preferences (SPs) tell us which argu-
ments are plausible for a particular predicate. For
example, Table 2 (Section 4.4) lists plausible and
implausible direct objects (arguments) for particu-
lar verbs (predicates). SPs can help resolve syntac-
tic, word sense, and reference ambiguity (Clark and
Weir, 2002), and so gathering them has received a
lot of attention in the NLP community.
One way to determine SPs is from co-occurrences
of predicates and arguments in text. Unfortunately,
no matter how much text we use, many acceptable
pairs will be missing. Bikel (2004) found that only
1.49% of the bilexical dependencies considered by
Collins? parser during decoding were observed dur-
ing training. In our parsed corpus (Section 4.1),
for example, we find eat with nachos, burritos, and
tacos, but not with the equally tasty quesadillas,
chimichangas, or tostadas. Rather than solely re-
lying on co-occurrence counts, we would like to use
them to generalize to unseen pairs.
In particular, we would like to exploit a number
of arbitrary and potentially overlapping properties
of predicates and arguments when we assign SPs.
We do this by representing these properties as fea-
tures in a linear classifier, and training the weights
using discriminative learning. Positive examples
are taken from observed predicate-argument pairs,
while pseudo-negatives are constructed from unob-
served combinations. We train a Support Vector Ma-
chine (SVM) classifier to distinguish the positives
from the negatives. We refer to our model?s scores
as Discriminative Selectional Preference (DSP). By
creating training vectors automatically, DSP enjoys
all the advantages of supervised learning, but with-
out the need for manual annotation of examples.
We evaluate DSP on the task of assigning verb-
object selectional preference. We encode a noun?s
textual distribution as feature information. The
learned feature weights are linguistically interesting,
yielding high-quality similar-word lists as latent in-
formation. Despite its representational power, DSP
scales to real-world data sizes: examples are parti-
tioned by predicate, and a separate SVM is trained
for each partition. This allows us to efficiently learn
with over 57 thousand features and 6.5 million ex-
amples. DSP outperforms recently proposed alterna-
tives in a range of experiments, and better correlates
with human plausibility judgments. It also shows
strong gains over a Mutual Information-based co-
59
occurrence model on two tasks: identifying objects
of verbs in an unseen corpus and finding pronominal
antecedents in coreference data.
2 Related Work
Most approaches to SPs generalize from observed
predicate-argument pairs to semantically similar
ones by modeling the semantic class of the argu-
ment, following Resnik (1996). For example, we
might have a class Mexican Food and learn that the
entire class is suitable for eating. Usually, the classes
are from WordNet (Miller et al, 1990), although
they can also be inferred from clustering (Rooth et
al., 1999). Brockmann and Lapata (2003) compare
a number of WordNet-based approaches, including
Resnik (1996), Li and Abe (1998), and Clark and
Weir (2002), and found that the more sophisticated
class-based approaches do not always outperform
simple frequency-based models.
Another line of research generalizes using simi-
lar words. Suppose we are calculating the proba-
bility of a particular noun, n, occurring as the ob-
ject argument of a given verbal predicate, v. Let
Pr(n|v) be the empirical maximum-likelihood esti-
mate from observed text. Dagan et al (1999) define
the similarity-weighted probability, PrSIM, to be:
PrSIM(n|v) =
?
v??SIMS(v)
Sim(v?, v)Pr(n|v?) (1)
where Sim(v?, v) returns a real-valued similarity be-
tween two verbs v? and v (normalized over all pair
similarities in the sum). In contrast, Erk (2007)
generalizes by substituting similar arguments, while
Wang et al (2005) use the cross-product of simi-
lar pairs. One key issue is how to define the set
of similar words, SIMS(w). Erk (2007) compared a
number of techniques for creating similar-word sets
and found that both the Jaccard coefficient and Lin
(1998a)?s information-theoretic metric work best.
Similarity-smoothed models are simple to compute,
potentially adaptable to new domains, and require
no manually-compiled resources such as WordNet.
Selectional Preferences have also been a recent
focus of researchers investigating the learning of
paraphrases and inference rules (Pantel et al, 2007;
Roberto et al, 2007). Inferences such as ?[X wins
Y] ? [X plays Y]? are only valid for certain argu-
ments X and Y. We follow Pantel et al (2007) in us-
ing automatically-extracted semantic classes to help
characterize plausible arguments.
Discriminative techniques are widely used in NLP
and have been applied to the related tasks of word
prediction and language modeling. Even-Zohar and
Roth (2000) use a classifier to predict the most likely
word to fill a position in a sentence (in their ex-
periments: a verb) from a set of candidates (sets
of verbs), by inspecting the context of the target
token (e.g., the presence or absence of a particu-
lar nearby word in the sentence). This approach
can therefore learn which specific arguments occur
with a particular predicate. In comparison, our fea-
tures are second-order: we learn what kinds of argu-
ments occur with a predicate by encoding features
of the arguments. Recent distributed and latent-
variable models also represent words with feature
vectors (Bengio et al, 2003; Blitzer et al, 2005).
Many of these approaches learn both the feature
weights and the feature representation. Vectors must
be kept low-dimensional for tractability, while learn-
ing and inference on larger scales is impractical. By
partitioning our examples by predicate, we can effi-
ciently use high-dimensional, sparse vectors.
Our technique of generating negative examples
is similar to the approach of Okanohara and Tsujii
(2007). They learn a classifier to disambiguate ac-
tual sentences from pseudo-negative examples sam-
pled from an N-gram language model. Smith and
Eisner (2005) also automatically generate negative
examples. They perturb their input sequence (e.g.
the sentence word order) to create a neighborhood of
implicit negative evidence. We create negatives by
substitution rather than perturbation, and use corpus-
wide statistics to choose our negative instances.
3 Methodology
3.1 Creating Examples
To learn a discriminative model of selectional pref-
erence, we create positive and negative training ex-
amples automatically from raw text. To create the
positives, we automatically parse a large corpus, and
then extract the predicate-argument pairs that have
a statistical association in this data. We measure
this association using pointwise Mutual Information
(MI) (Church and Hanks, 1990). The MI between a
60
verb predicate, v, and its object argument, n, is:
MI(v, n) = log Pr(v, n)
Pr(v)Pr(n) = log
Pr(n|v)
Pr(n) (2)
If MI>0, the probability v and n occur together is
greater than if they were independently distributed.
We create sets of positive and negative examples
separately for each predicate, v. First, we extract all
pairs where MI(v, n)>? as positives. For each pos-
itive, we create pseudo-negative examples, (v, n?),
by pairing v with a new argument, n?, that either has
MI below the threshold or did not occur with v in the
corpus. We require each negative n? to have a similar
frequency to its corresponding n. This prevents our
learning algorithm from focusing on any accidental
frequency-based bias. We mix in K negatives for
each positive, sampling without replacement to cre-
ate all the negatives for a particular predicate. For
each v, 1K+1 of its examples will be positive. The
threshold ? represents a trade-off between capturing
a large number of positive pairs and ensuring these
pairs have good association. Similarly, K is a trade-
off between the number of examples and the com-
putational efficiency. Ultimately, these parameters
should be optimized for task performance.
Of course, some negatives will actually be plau-
sible arguments that were unobserved due to sparse-
ness. Fortunately, modern discriminative methods
like soft-margin SVMs can learn in the face of label
error by allowing slack, subject to a tunable regular-
ization penalty (Cortes and Vapnik, 1995).
If MI is a sparse and imperfect model of SP, what
can DSP gain by training on MI?s scores? We can
regard DSP as learning a view of SP that is or-
thogonal to MI, in a co-training sense (Blum and
Mitchell, 1998). MI labels the data based solely
on co-occurrence; DSP uses these labels to iden-
tify other regularities ? ones that extend beyond co-
occurring words. For example, many instances of
n where MI(eat, n)>? also have MI(buy, n)>? and
MI(cook, n)>? . Also, compared to other nouns,
a disproportionate number of eat-nouns are lower-
case, single-token words, and they rarely contain
digits, hyphens, or begin with a human first name
like Bob. DSP encodes these interdependent prop-
erties as features in a linear classifier. This classi-
fier can score any noun as a plausible argument of
eat if indicative features are present; MI can only
assign high plausibility to observed (eat,n) pairs.
Similarity-smoothed models can make use of the
regularities across similar verbs, but not the finer-
grained string- and token-based features.
Our training examples are similar to the data cre-
ated for pseudodisambiguation, the usual evalua-
tion task for SP models (Erk, 2007; Keller and La-
pata, 2003; Rooth et al, 1999). This data con-
sists of triples (v, n, n?) where v, n is a predicate-
argument pair observed in the corpus and v, n? has
not been observed. The models score correctly
if they rank observed (and thus plausible) argu-
ments above corresponding unobserved (and thus
likely implausible) ones. We refer to this as Pair-
wise Disambiguation. Unlike this task, we classify
each predicate-argument pair independently as plau-
sible/implausible. We also use MI rather than fre-
quency to define the positive pairs, ensuring that the
positive pairs truly have a statistical association, and
are not simply the result of parser error or noise.1
3.2 Partitioning for Efficient Training
After creating our positive and negative training
pairs, we must select a feature representation for our
examples. Let ? be a mapping from a predicate-
argument pair (v, n) to a feature vector, ? :
(v, n) ? ??1...?k?. Predictions are made based
on a weighted combination of the features, y =
? ??(v, n), where ? is our learned weight vector.
We can make training significantly more efficient
by using a special form of attribute-value features.
Let every feature ?i be of the form ?i(v, n) = ?v =
v??f(n)?. That is, every feature is an intersection of
the occurrence of a particular predicate, v?, and some
feature of the argument f(n). For example, a fea-
ture for a verb-object pair might be, ?the verb is eat
and the object is lower-case.? In this representation,
features for one predicate will be completely inde-
pendent from those for every other predicate. Thus
rather than a single training procedure, we can actu-
ally partition the examples by predicate, and train a
1For a fixed verb, MI is proportional to Keller and Lapata
(2003)?s conditional probability scores for pseudodisambigua-
tion of (v, n, n?) triples: Pr(v|n) = Pr(v, n)/Pr(n), which was
shown to be a better measure of association than co-occurrence
frequency f(v, n). Normalizing by Pr(v) (yielding MI) allows
us to use a constant threshold across all verbs. MI was also
recently used for inference-rule SPs by Pantel et al (2007).
61
classifier for each predicate independently. The pre-
diction becomes yv = ?v ??v(n), where ?v are the
learned weights corresponding to predicate v and all
features ?v(n)=f(n) depend on the argument only.
Some predicate partitions may have insufficient
examples for training. Also, a predicate may oc-
cur in test data that was unseen during training. To
handle these instances, we decided to cluster low-
frequency predicates. In our experiments assigning
SP to verb-object pairs, we cluster all verbs that have
less than 250 positive examples, using clusters gen-
erated by the CBC algorithm (Pantel and Lin, 2002).
For example, the low-frequency verbs incarcerate,
parole, and court-martial are all mapped to the same
partition, while more-frequent verbs like arrest and
execute each have their own partition. About 5.5%
of examples are clustered, corresponding to 30% of
the 7367 total verbs. 40% of verbs (but only 0.6% of
examples) were not in any CBC cluster; these were
mapped to a single backoff partition.
The parameters for each partition, ?v, can be
trained with any supervised learning technique. We
use SVM (Section 4.1) because it is effective in simi-
lar high-dimensional, sparse-vector settings, and has
an efficient implementation (Joachims, 1999). In
SVM, the sign of yv gives the classification. We can
also use the scalar yv as our DSP score (i.e. the posi-
tive distance from the separating SVM hyperplane).
3.3 Features
This section details our argument features, f(n), for
assigning verb-object selectional preference. For a
verb predicate (or partition) v and object argument
n, the form of our classifier is yv = ?i ?vi fi(n).
3.3.1 Verb co-occurrence
We provide features for the empirical probability
of the noun occurring as the object argument of other
verbs, Pr(n|v?). If we were to only use these features
(indexing the feature weights by each verb v?), the
form of our classifier would be:
yv =
?
v?
?vv?Pr(n|v?) (3)
Note the similarity between Equation (3) and Equa-
tion (1). Now the feature weights, ?vv? , take the role
of the similarity function, Sim(v?, v). Unlike Equa-
tion (1), however, these weights are not set by an
external similarity algorithm, but are optimized to
discriminate the positive and negative training ex-
amples. We need not restrict ourselves to a short list
of similar verbs; we include Probj(n|v?) features for
every verb that occurs more than 10 times in our cor-
pus. ?vv? may be positive or negative, depending on
the relation between v? and v. We also include fea-
tures for the probability of the noun occurring as the
subject of other verbs, Prsubj(n|v?). For example,
nouns that can be the object of eat will also occur as
the subject of taste and contain. Other contexts, such
as adjectival and nominal predicates, could also aid
the prediction, but have not yet been investigated.
The advantage of tuning similarity to the appli-
cation of interest has been shown previously by
Weeds and Weir (2005). They optimize a few meta-
parameters separately for the tasks of thesaurus gen-
eration and pseudodisambiguation. Our approach,
on the other hand, discriminatively sets millions of
individual similarity values. Like Weeds and Weir
(2005), our similarity values are asymmetric.
3.3.2 String-based
We include several simple character-based fea-
tures of the noun string: the number of tokens, the
case, and whether it contains digits, hyphens, an
apostrophe, or other punctuation. We also include a
feature for the first and last token, and fire indicator
features if any token in the noun occurs on in-house
lists of given names, family names, cities, provinces,
countries, corporations, languages, etc. We also fire
a feature if a token is a corporate designation (like
inc. or ltd.) or a human one (like Mr. or Sheik).
3.3.3 Semantic classes
Motivated by previous SP models that make use
of semantic classes, we generated word clusters us-
ing CBC (Pantel and Lin, 2002) on a 10 GB corpus,
giving 3620 clusters. If a noun belongs in a cluster,
a corresponding feature fires. If a noun is in none of
the clusters, a no-class feature fires.
As an example, CBC cluster 1891 contains:
sidewalk, driveway, roadway, footpath,
bridge, highway, road, runway, street, alley,
path, Interstate, . . .
In our training data, we have examples like widen
highway, widen road and widen motorway. If we
62
see that we can widen a highway, we learn that we
can also widen a sidewalk, bridge, runway, etc.
We also made use of the person-name/instance
pairs automatically extracted by Fleischman et al
(2003).2 This data provides counts for pairs such
as ?Edwin Moses, hurdler? and ?William Farley, in-
dustrialist.? We have features for all concepts and
therefore learn their association with each verb.
4 Experiments and Results
4.1 Set up
We parsed the 3 GB AQUAINT corpus (Voorhees,
2002) using Minipar (Lin, 1998b), and collected
verb-object and verb-subject frequencies, building
an empirical MI model from this data. Verbs and
nouns were converted to their (possibly multi-token)
root, and string case was preserved. Passive sub-
jects (the car was bought) were converted to objects
(bought car). We set the MI-threshold, ? , to be 0,
and the negative-to-positive ratio, K, to be 2.
Numerous previous pseudodisambiguation evalu-
ations only include arguments that occur between 30
and 3000 times (Erk, 2007; Keller and Lapata, 2003;
Rooth et al, 1999). Presumably the lower bound is
to help ensure the negative argument is unobserved
because it is unsuitable, not because of data sparse-
ness. We wish to use our model on arguments of
any frequency, including those that never occurred
in the training corpus (and therefore have empty co-
occurrence features (Section 3.3.1)). We proceed as
follows: first, we exclude pairs whenever the noun
occurs less than 3 times in our corpus, removing
many misspellings and other noun noise. Next, we
omit verb co-occurrence features for nouns that oc-
cur less than 10 times, and instead fire a low-count
feature. When we move to a new corpus, previously-
unseen nouns are treated like these low-count train-
ing nouns.
This processing results in a set of 6.8 million
pairs, divided into 2318 partitions (192 of which
are verb clusters (Section 3.2)). For each parti-
tion, we take 95% of the examples for training,
2.5% for development and 2.5% for a final unseen
test set. We provide full results for two models:
DSPcooc which only uses the verb co-occurrence fea-
tures, and DSPall which uses all the features men-
2Available at http://www.mit.edu/?mbf/instances.txt.gz
tioned in Section 3.3. Feature values are normalized
within each feature type. We train our (linear kernel)
discriminative models using SVMlight (Joachims,
1999) on each partition, but set meta-parameters C
(regularization) and j (cost of positive vs. nega-
tive misclassifications: max at j=2) on the macro-
averaged score across all development partitions.
Note that we can not use the development set to op-
timize ? and K because the development examples
are obtained after setting these values.
4.2 Feature weights
It is interesting to inspect the feature weights re-
turned by our system. In particular, the weights
on the verb co-occurrence features (Section 3.3.1)
provide a high-quality, argument-specific similarity-
ranking of other verb contexts. The DSP parameters
for eat, for example, place high weight on features
like Pr(n|braise), Pr(n|ration), and Pr(n|garnish).
Lin (1998a)?s similar word list for eat misses these
but includes sleep (ranked 6) and sit (ranked 14), be-
cause these have similar subjects to eat. Discrimina-
tive, context-specific training seems to yield a bet-
ter set of similar predicates, e.g. the highest-ranked
contexts for DSPcooc on the verb join,3
lead 1.42, rejoin 1.39, form 1.34, belong to
1.31, found 1.31, quit 1.29, guide 1.19, induct
1.19, launch (subj) 1.18, work at 1.14
give a better SIMS(join) for Equation (1) than the
top similarities returned by (Lin, 1998a):
participate 0.164, lead 0.150, return to 0.148,
say 0.143, rejoin 0.142, sign 0.142, meet
0.142, include 0.141, leave 0.140, work 0.137
Other features are also weighted intuitively. Note
that case is a strong indicator for some arguments,
for example the weight on being lower-case is high
for become (0.972) and eat (0.505), but highly nega-
tive for accuse (-0.675) and embroil (-0.573) which
often take names of people and organizations.
4.3 Pseudodisambiguation
We first evaluate DSP on disambiguating posi-
tives from pseudo-negatives, comparing to recently-
3Which all correspond to nouns occurring in the object po-
sition of the verb (e.g. Probj(n|lead)), except ?launch (subj)?
which corresponds to Prsubj(n|launch).
63
System MacroAvg MicroAvg PairwiseP R F P R F Acc Cov
Dagan et al (1999) 0.36 0.90 0.51 0.68 0.92 0.78 0.58 0.98
Erk (2007) 0.49 0.66 0.56 0.70 0.82 0.76 0.72 0.83
Keller and Lapata (2003) 0.72 0.34 0.46 0.80 0.50 0.62 0.80 0.57
DSPcooc 0.53 0.72 0.61 0.73 0.94 0.82 0.77 1.00
DSPall 0.60 0.71 0.65 0.77 0.90 0.83 0.81 1.00
Table 1: Pseudodisambiguation results averaged across each example (MacroAvg), weighted by word frequency (Mi-
croAvg), plus coverage and accuracy of pairwise competition (Pairwise).
proposed systems that also require no manually-
compiled resources like WordNet. We convert Da-
gan et al (1999)?s similarity-smoothed probability
to MI by replacing the empirical Pr(n|v) in Equa-
tion (2) with the smoothed PrSIM from Equation (1).
We also test an MI model inspired by Erk (2007):
MISIM(n, v) = log
?
n??SIMS(n)
Sim(n?, n) Pr(v, n
?)
Pr(v)Pr(n?)
We gather similar words using Lin (1998a), mining
similar verbs from a comparable-sized parsed cor-
pus, and collecting similar nouns from a broader 10
GB corpus of English text.4
We also use Keller and Lapata (2003)?s approach
to obtaining web-counts. Rather than mining parse
trees, this technique retrieves counts for the pattern
?V Det N? in raw online text, where V is any in-
flection of the verb, Det is the, a, or the empty
string, and N is the singular or plural form of the
noun. We compute a web-based MI by collecting
Pr(n, v), Pr(n), and Pr(v) using all inflections, ex-
cept we only use the root form of the noun. Rather
than using a search engine, we obtain counts from
the Google Web 5-gram Corpus.5
All systems are thresholded at zero to make a clas-
sification. Unlike DSP, the comparison systems may
4For both the similar-noun and similar-verb smoothing, we
only smooth over similar pairs that occurred in the corpus.
While averaging over all similar pairs tends to underestimate
the probability, averaging over only the observed pairs tends to
overestimate it. We tested both and adopt the latter because it
resulted in better performance on our development set.
5Available from the LDC as LDC2006T13. This collection
was generated from approximately 1 trillion tokens of online
text. Unfortunately, tokens appearing less than 200 times have
been mapped to the ?UNK? symbol, and only N-grams appear-
ing more than 40 times are included. Unlike results from search
engines, however, experiments with this corpus are replicable.
not be able to provide a score for each example.
The similarity-smoothed examples will be undefined
if SIMS(w) is empty. Also, the Keller and Lapata
(2003) approach will be undefined if the pair is un-
observed on the web. As a reasonable default for
these cases, we assign them a negative decision.
We evaluate disambiguation using precision (P),
recall (R), and their harmonic mean, F-Score (F).
Table 1 gives the results of our comparison. In the
MacroAvg results, we weight each example equally.
For MicroAvg, we weight each example by the fre-
quency of the noun. To more directly compare with
previous work, we also reproduced Pairwise Disam-
biguation by randomly pairing each positive with
one of the negatives and then evaluating each system
by the percentage it ranks correctly (Acc). For the
comparison approaches, if one score is undefined,
we choose the other one. If both are undefined, we
abstain from a decision. Coverage (Cov) is the per-
cent of pairs where a decision was made.6
Our simple system with only verb co-occurrence
features, DSPcooc, outperforms all comparison ap-
proaches. Using the richer feature set in DSPall
results in a statistically significant gain in perfor-
mance, up to an F-Score of 0.65 and a pairwise
disambiguation accuracy of 0.81.7 DSPall has both
broader coverage and better accuracy than all com-
peting approaches. In the remainder of the experi-
ments, we use DSPall and refer to it simply as DSP.
Some errors are because of plausible but unseen
arguments being used as test-set pseudo-negatives.
For example, for the verb damage, DSP?s three most
high-scoring false positives are the nouns jetliner,
carpet, and gear. While none occur with damage in
6I.e. we use the ?half coverage? condition from Erk (2007).
7The differences between DSPall and all comparison sys-
tems are statistically significant (McNemar?s test, p<0.01).
64
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 10  100  1000  10000  100000  1e+06
F-
Sc
or
e
Noun Frequency
DSPall
Erk (2007)
Keller and Lapata (2003)
Figure 1: Disambiguation results by noun frequency.
our corpus, all intuitively satisfy the verb?s SPs.
MacroAvg performance is worse than MicroAvg
because all systems perform better on frequent
nouns. When we plot F-Score by noun frequency
(Figure 1), we see that DSP outperforms comparison
approaches across all frequencies, but achieves its
biggest gains on the low-frequency nouns. A richer
feature set alows DSP to make correct inferences on
examples that provide minimal co-occurrence data.
These are also the examples for which we would ex-
pect co-occurrence models like MI to fail.
As a further experiment, we re-trained DSP but
with only the string-based features removed. Overall
macro-averaged F-score dropped from 0.65 to 0.64
(a statistically significant reduction in performance).
The system scored nearly identically to DSP on the
high-frequency nouns, but performed roughly 15%
worse on the nouns that occurred less than ten times.
This shows that the string-based features are impor-
tant for selectional preference, and particularly help-
ful for low-frequency nouns.
4.4 Human Plausibility
Table 2 compares some of our systems on data used
by Resnik (1996) (also Appendix 2 in Holmes et al
(1989)). The plausibility of these pairs was initially
judged based on the experimenters? intuitions, and
later confirmed in a human experiment. We include
the scores of Resnik?s system, and note that its errors
were attributed to sense ambiguity and other limi-
tations of class-based approaches (Resnik, 1996).8
8For example, warn-engine scores highly because engines
are in the class entity, and physical entities (e.g. people) are
often objects of warn. Unlike DSP, Resnik?s approach cannot
learn that for warn, ?the property of being a person is more
Seen Criteria Unseen Verb-Object Freq.All = 1 = 2 = 3 > 3
MI > 0 0.44 0.33 0.57 0.70 0.82
Freq. > 0 0.57 0.45 0.76 0.89 0.96
DSP > 0 0.73 0.69 0.80 0.85 0.88
Table 3: Recall on identification of Verb-Object pairs
from an unseen corpus (divided by pair frequency).
The other comparison approaches also make a num-
ber of mistakes, which can often be traced to a mis-
guided choice of similar word to smooth with.
We also compare to our empirical MI model,
trained on our parsed corpus. Although Resnik
(1996) reported that 10 of the 16 plausible pairs did
not occur in his training corpus, all of them occurred
in ours and hence MI gives very reasonable scores
on the plausible objects. It has no statistics, however,
for many of the implausible ones. DSP can make
finer decisions than MI, recognizing that ?warning
an engine? is more absurd than ?judging a climate.?
4.5 Unseen Verb-Object Identification
We next compare MI and DSP on a much larger set
of plausible examples, and also test how well the
models generalize across data sets. We took the MI
and DSP systems trained on AQUAINT and asked
them to rate observed (and thus likely plausible)
verb-object pairs taken from an unseen corpus. We
extracted the pairs by parsing the San Jose Mercury
News (SJM) section of the TIPSTER corpus (Har-
man, 1992). Each unique verb-object pair is a single
instance in this evaluation.
Table 3 gives recall across all pairs (All) and
grouped by pair-frequency in the unseen corpus (1,
2, 3, >3). DSP accepts far more pairs than MI
(73% vs. 44%), even far more than a system that
accepts any previously observed verb-object combi-
nation as plausible (57%). Recall is higher on more
frequent verb-object pairs, but 70% of the pairs oc-
curred only once in the corpus. Even if we smooth
MI by smoothing Pr(n|v) in Equation 2 using modi-
fied KN-smoothing (Chen and Goodman, 1998), the
recall of MI>0 on SJM only increases from 44.1%
to 44.9%, still far below DSP. Frequency-based
models have fundamentally low coverage. As fur-
important than the property of being an entity? (Resnik, 1996).
65
Verb Plaus./Implaus. Resnik Dagan et al Erk MI DSP
see friend/method 5.79/-0.01 0.20/1.40* 0.46/-0.07 1.11/-0.57 0.98/0.02
read article/fashion 6.80/-0.20 3.00/0.11 3.80/1.90 4.00/? 2.12/-0.65
find label/fever 1.10/0.22 1.50/2.20* 0.59/0.01 0.42/0.07 1.61/0.81
hear story/issue 1.89/1.89* 0.66/1.50* 2.00/2.60* 2.99/-1.03 1.66/0.67
write letter/market 7.26/0.00 2.50/-0.43 3.60/-0.24 5.06/-4.12 3.08/-1.31
urge daughter/contrast 1.14/1.86* 0.14/1.60* 1.10/3.60* -0.95/? -0.34/-0.62
warn driver/engine 4.73/3.61 1.20/0.05 2.30/0.62 2.87/? 2.00/-0.99
judge contest/climate 1.30/0.28 1.50/1.90* 1.70/1.70* 3.90/? 1.00/0.51
teach language/distance 1.87/1.86 2.50/1.30 3.60/2.70 3.53/? 1.86/0.19
show sample/travel 1.44/0.41 1.60/0.14 0.40/-0.82 0.53/-0.49 1.00/-0.83
expect visit/mouth 0.59/5.93* 1.40/1.50* 1.40/0.37 1.05/-0.65 1.44/-0.15
answer request/tragedy 4.49/3.88 2.70/1.50 3.10/-0.64 2.93/? 1.00/0.01
recognize author/pocket 0.50/0.50* 0.03/0.37* 0.77/1.30* 0.48/? 1.00/0.00
repeat comment/journal 1.23/1.23* 2.30/1.40 2.90/? 2.59/? 1.00/-0.48
understand concept/session 1.52/1.51 2.70/0.25 2.00/-0.28 3.96/? 2.23/-0.46
remember reply/smoke 1.31/0.20 2.10/1.20 0.54/2.60* 1.13/-0.06 1.00/-0.42
Table 2: Selectional ratings for plausible/implausible direct objects (Holmes et al, 1989). Mistakes are marked with
an asterisk (*), undefined scores are marked with a dash (?). Only DSP is completely defined and completely correct.
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0  0.2  0.4  0.6  0.8  1
In
te
rp
ol
at
ed
 P
re
cis
io
n
Recall
DSP>T
MI>T
DSP>0
MI>0
Figure 2: Pronoun resolution precision-recall on MUC.
ther evidence, if we build a model of MI on the SJM
corpus and use it in our pseudodisambiguation ex-
periment (Section 4.3), MI>0 gets a MacroAvg pre-
cision of 86% but a MacroAvg recall of only 12%.9
4.6 Pronoun Resolution
Finally, we evaluate DSP on a common application
of selectional preferences: choosing the correct an-
tecedent for pronouns in text (Dagan and Itai, 1990;
Kehler et al, 2004). We study the cases where a
9Recall that even the Keller and Lapata (2003) system, built
on the world?s largest corpus, achieves only 34% recall (Table 1)
(with only 48% of positives and 27% of all pairs previously
observed, but see Footnote 5).
pronoun is the direct object of a verb predicate, v. A
pronoun?s antecedent must obey v?s selectional pref-
erences. If we have a better model of SP, we should
be able to better select pronoun antecedents.
We parsed the MUC-7 (1997) coreference corpus
and extracted all pronouns in a direct object rela-
tion. For each pronoun, p, modified by a verb, v, we
extracted all preceding nouns within the current or
previous sentence. Thirty-nine anaphoric pronouns
had an antecedent in this window and are used in
the evaluation. For each p, let N(p)+ by the set of
preceding nouns coreferent with p, and let N(p)?
be the remaining non-coreferent nouns. We take
all (v, n+) where n+ ? N(p)+ as positive, and all
other pairs (v, n?), n? ? N(p)? as negative.
We compare MI and DSP on this set, classifying
every (v, n) with MI>T (or DSP>T ) as positive.
By varying T , we get a precision-recall curve (Fig-
ure 2). Precision is low because, of course, there
are many nouns that satisfy the predicate?s SPs that
are not coreferent. DSP>0 has both a higher recall
and higher precision than accepting every pair pre-
viously seen in text (the right-most point on MI>T ).
The DSP>T system achieves higher precision than
MI>T for points where recall is greater than 60%
(where MI<0). Interestingly, the recall of MI>0 is
66
System Acc
Most-Recent Noun 17.9%
Maximum MI 28.2%
Maximum DSP 38.5%
Table 4: Pronoun resolution accuracy on nouns in current
or previous sentence in MUC.
higher here than it is for general verb-objects (Sec-
tion 4.5). On the subset of pairs with strong empir-
ical association (MI>0), MI generally outperforms
DSP at equivalent recall values.
We next compare MI and DSP as stand-alone pro-
noun resolution systems (Table 4). As a standard
baseline, for each pronoun, we choose the most
recent noun in text as the pronoun?s antecedent,
achieving 17.9% resolution accuracy. This baseline
is quite low because many of the most-recent nouns
are subjects of the pronoun?s verb phrase, and there-
fore resolution violates syntactic coreference con-
straints. If instead we choose the previous noun with
the highest MI as antecedent, we get an accuracy of
28.2%, while choosing the previous noun with the
highest DSP achieves 38.5%. DSP resolves 37%
more pronouns correctly than MI. We leave as fu-
ture work a full-scale pronoun resolution system that
incorporates both MI and DSP as backed-off, inter-
polated, or separate semantic features.
5 Conclusions and Future Work
We have presented a simple, effective model of se-
lectional preference based on discriminative train-
ing. Supervised techniques typically achieve higher
performance than unsupervised models, and we du-
plicate these gains with DSP. Here, however, these
gains come at no additional labeling cost, as train-
ing examples are generated automatically from un-
labeled text. DSP allows an arbitrary combination of
features, including verb co-occurrence features that
yield high-quality similar-word lists as latent output.
This work only scratches the surface of possible fea-
ture mining; information from WordNet relations,
Wikipedia categories, or parallel corpora could also
provide valuable clues to SP. Also, if any other sys-
tem were to exceed DSP?s performance, it could also
be included as one of DSP?s features.
It would be interesting to expand our co-
occurrence features, including co-occurrence counts
across more grammatical relations and using counts
from external, unparsed corpora like the world wide
web. We could also reverse the role of noun and verb
in our training, having verb-specific features and
discriminating separately for each argument noun.
The latent information would then be lists of similar
nouns.
Finally, note that while we focused on word-word
co-occurrences, sense-sense SPs can also be learned
with our algorithm. If our training corpus was sense-
labeled, we could run our algorithm over the senses
rather than the words. The resulting model would
then require sense-tagged input if it were to be used
within an application like parsing or coreference res-
olution. Also, like other models of SP, our technique
can also be used for sense disambiguations: the
weightings on our semantic class features indicate,
for a particular noun, which of its senses (classes) is
most compatible with each verb.
Acknowledgments
We gratefully acknowledge support from the Natu-
ral Sciences and Engineering Research Council of
Canada, the Alberta Ingenuity Fund, and the Alberta
Informatics Circle of Research Excellence.
References
Yoshua Bengio, Re?jean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Research,
3:1137?1155.
Daniel M. Bikel. 2004. Intricacies of Collins? parsing
model. Computational Linguistics, 30(4):479?511.
John Blitzer, Amir Globerson, and Fernando Pereira.
2005. Distributed latent variable models of lexical co-
occurrences. In AISTATS.
Avrim Blum and Tom Mitchell. 1998. Combining la-
beled and unlabeled data with co-training. In Proceed-
ings of COLT, pages 92?100.
Carsten Brockmann and Mirella Lapata. 2003. Evalu-
ating and combining approaches to selectional prefer-
ence acquisition. In EACL, pages 27?34.
Stanley F. Chen and Joshua Goodman. 1998. An empir-
ical study of smoothing techniques for language mod-
eling. TR-10-98, Harvard University.
Kenneth Ward Church and Patrick Hanks. 1990. Word
association norms, mutual information, and lexicogra-
phy. Computational Linguistics, 16(1):22?29.
67
Stephen Clark and David Weir. 2002. Class-based prob-
ability estimation using a semantic hierarchy. Compu-
tational Linguistics, 28(2):187?206.
Corinna Cortes and Vladimir Vapnik. 1995. Support-
vector networks. Machine Learning, 20(3):273?297.
Ido Dagan and Alan Itai. 1990. Automatic processing of
large corpora for the resolution of anaphora references.
In COLING, volume 3, pages 330?332.
Ido Dagan, Lillian Lee, and Fernando C. N. Pereira.
1999. Similarity-based models of word cooccurrence
probabilities. Machine Learning, 34(1-3):43?69.
Katrin Erk. 2007. A simple, similarity-based model for
selectional preference. In ACL, pages 216?223.
Yair Even-Zohar and Dan Roth. 2000. A classification
approach to word prediction. In NAACL, pages 124?
131.
Michael Fleischman, Eduard Hovy, and Abdessamad
Echihabi. 2003. Offline strategies for online question
answering: answering questions before they are asked.
In ACL, pages 1?7.
Donna Harman. 1992. The DARPA TIPSTER project.
ACM SIGIR Forum, 26(2):26?28.
Virginia M. Holmes, Laurie Stowe, and Linda Cupples.
1989. Lexical expectations in parsing complement-
verb sentences. Journal of Memory and Language,
28:668?689.
Thorsten Joachims. 1999. Making large-scale Support
Vector Machine learning practical. In B. Scho?lkopf
and C. Burges, editors, Advances in Kernel Methods:
Support Vector Machines, pages 169?184. MIT-Press.
Andrew Kehler, Douglas Appelt, Lara Taylor, and Alek-
sandr Simma. 2004. The (non)utility of predicate-
argument frequencies for pronoun interpretation. In
HLT/NAACL, pages 289?296.
Frank Keller and Mirella Lapata. 2003. Using the web to
obtain frequencies for unseen bigrams. Computational
Linguistics, 29(3):459?484.
Hang Li and Naoki Abe. 1998. Generalizing case frames
using a thesaurus and the MDL principle. Computa-
tional Linguistics, 24(2):217?244.
Dekang Lin. 1998a. Automatic retrieval and clustering
of similar words. In COLING-ACL, pages 768?773.
Dekang Lin. 1998b. Dependency-based evaluation of
MINIPAR. In LREC Workshop on the Evaluation of
Parsing Systems.
George A. Miller, Richard Beckwith, Christiane Fell-
baum, Derek Gross, and Katherine J. Miller. 1990.
Introduction to WordNet: an on-line lexical database.
International Journal of Lexicography, 3(4):235?244.
MUC-7. 1997. Coreference task definition (v3.0, 13 Jul
97). In Proceedings of the Seventh Message Under-
standing Conference (MUC-7).
Daisuke Okanohara and Jun?ichi Tsujii. 2007. A
discriminative language model with pseudo-negative
samples. In ACL, pages 73?80.
Patrick Pantel and Dekang Lin. 2002. Discovering word
senses from text. In KDD, pages 613?619.
Patrick Pantel, Rahul Bhagat, Bonaventura Coppola,
Timothy Chklovski, and Eduard Hovy. 2007. ISP:
Learning inferential selectional preferences. In
NAACL-HLT, pages 564?571.
Philip Resnik. 1996. Selectional constraints: An
information-theoretic model and its computational re-
alization. Cognition, 61:127?159.
Basili Roberto, Diego De Cao, Paolo Marocco, and
Marco Pennacchiotti. 2007. Learning selectional
preferences for entailment or paraphrasing rules. In
RANLP.
Mats Rooth, Stefan Riezler, Detlef Prescher, Glenn Car-
roll, and Franz Beil. 1999. Inducing a semantically
annotated lexicon via EM-based clustering. In ACL,
pages 104?111.
Noah A. Smith and Jason Eisner. 2005. Contrastive esti-
mation: training log-linear models on unlabeled data.
In ACL, pages 354?362.
Ellen Voorhees. 2002. Overview of the TREC 2002
question answering track. In Proceedings of the
Eleventh Text REtrieval Conference (TREC).
Qin Iris Wang, Dale Schuurmans, and Dekang Lin. 2005.
Strictly lexical dependency parsing. In International
Workshop on Parsing Technologies, pages 152?159.
Julie Weeds and David Weir. 2005. Co-occurrence re-
trieval: a flexible framework for lexical distributional
similarity. Computational Linguistics, 31(4):439?475.
68
Proceedings of ACL-08: HLT, pages 10?18,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Distributional Identification of Non-Referential Pronouns
Shane Bergsma
Department of Computing Science
University of Alberta
Edmonton, Alberta
Canada, T6G 2E8
bergsma@cs.ualberta.ca
Dekang Lin
Google, Inc.
1600 Amphitheatre Parkway
Mountain View
California, 94301
lindek@google.com
Randy Goebel
Department of Computing Science
University of Alberta
Edmonton, Alberta
Canada, T6G 2E8
goebel@cs.ualberta.ca
Abstract
We present an automatic approach to deter-
mining whether a pronoun in text refers to
a preceding noun phrase or is instead non-
referential. We extract the surrounding tex-
tual context of the pronoun and gather, from
a large corpus, the distribution of words that
occur within that context. We learn to reliably
classify these distributions as representing ei-
ther referential or non-referential pronoun in-
stances. Despite its simplicity, experimental
results on classifying the English pronoun it
show the system achieves the highest perfor-
mance yet attained on this important task.
1 Introduction
The goal of coreference resolution is to determine
which noun phrases in a document refer to the same
real-world entity. As part of this task, coreference
resolution systems must decide which pronouns re-
fer to preceding noun phrases (called antecedents)
and which do not. In particular, a long-standing
challenge has been to correctly classify instances of
the English pronoun it. Consider the sentences:
(1) You can make it in advance.
(2) You can make it in Hollywood.
In sentence (1), it is an anaphoric pronoun refer-
ring to some previous noun phrase, like ?the sauce?
or ?an appointment.? In sentence (2), it is part of the
idiomatic expression ?make it? meaning ?succeed.?
A coreference resolution system should find an an-
tecedent for the first it but not the second. Pronouns
that do not refer to preceding noun phrases are called
non-anaphoric or non-referential pronouns.
The word it is one of the most frequent words in
the English language, accounting for about 1% of
tokens in text and over a quarter of all third-person
pronouns.1 Usually between a quarter and a half of
it instances are non-referential (e.g. Section 4, Ta-
ble 3). As with other pronouns, the preceding dis-
course can affect it?s interpretation. For example,
sentence (2) can be interpreted as referential if the
preceding sentence is ?You want to make a movie??
We show, however, that we can reliably classify a
pronoun as being referential or non-referential based
solely on the local context surrounding the pronoun.
We do this by turning the context into patterns and
enumerating all the words that can take the place of
it in these patterns. For sentence (1), we can ex-
tract the context pattern ?make * in advance? and
for sentence (2) ?make * in Hollywood,? where ?*?
is a wildcard that can be filled by any token. Non-
referential distributions tend to have the word it fill-
ing the wildcard position. Referential distributions
occur with many other noun phrase fillers. For ex-
ample, in our n-gram collection (Section 3.4), ?make
it in advance? and ?make them in advance? occur
roughly the same number of times (442 vs. 449), in-
dicating a referential pattern. In contrast, ?make it in
Hollywood? occurs 3421 times while ?make them in
Hollywood? does not occur at all.
These simple counts strongly indicate whether an-
other noun can replace the pronoun. Thus we can
computationally distinguish between a) pronouns
that refer to nouns, and b) all other instances: includ-
ing those that have no antecedent, like sentence (2),
1e.g. http://ucrel.lancs.ac.uk/bncfreq/flists.html
10
and those that refer to sentences, clauses, or implied
topics of discourse. Beyond the practical value of
this distinction, Section 3 provides some theoretical
justification for our binary classification.
Section 3 also shows how to automatically extract
and collect counts for context patterns, and how to
combine the information using a machine learned
classifier. Section 4 describes our data for learning
and evaluation, It-Bank: a set of over three thousand
labelled instances of the pronoun it from a variety
of text sources. Section 4 also explains our com-
parison approaches and experimental methodology.
Section 5 presents our results, including an interest-
ing comparison of our system to human classifica-
tion given equivalent segments of context.
2 Related Work
The difficulty of non-referential pronouns has been
acknowledged since the beginning of computational
resolution of anaphora. Hobbs (1978) notes his algo-
rithm does not handle pronominal references to sen-
tences nor cases where it occurs in time or weather
expressions. Hirst (1981, page 17) emphasizes the
importance of detecting non-referential pronouns,
?lest precious hours be lost in bootless searches
for textual referents.? Mu?ller (2006) summarizes
the evolution of computational approaches to non-
referential it detection. In particular, note the pio-
neering work of Paice and Husk (1987), the inclu-
sion of non-referential it detection in a full anaphora
resolution system by Lappin and Leass (1994), and
the machine learning approach of Evans (2001).
There has recently been renewed interest in
non-referential pronouns, driven by three primary
sources. First of all, research in coreference resolu-
tion has shown the benefits of modules for general
noun anaphoricity determination (Ng and Cardie,
2002; Denis and Baldridge, 2007). Unfortunately,
these studies handle pronouns inadequately; judg-
ing from the decision trees and performance fig-
ures, Ng and Cardie (2002)?s system treats all pro-
nouns as anaphoric by default. Secondly, while
most pronoun resolution evaluations simply exclude
non-referential pronouns, recent unsupervised ap-
proaches (Cherry and Bergsma, 2005; Haghighi and
Klein, 2007) must deal with all pronouns in unre-
stricted text, and therefore need robust modules to
automatically handle non-referential instances. Fi-
nally, reference resolution has moved beyond writ-
ten text into in spoken dialog. Here, non-referential
pronouns are pervasive. Eckert and Strube (2000)
report that in the Switchboard corpus, only 45%
of demonstratives and third-person pronouns have a
noun phrase antecedent. Handling the common non-
referential instances is thus especially vital.
One issue with systems for non-referential detec-
tion is the amount of language-specific knowledge
that must be encoded. Consider a system that jointly
performs anaphora resolution and word alignment
in parallel corpora for machine translation. For this
task, we need to identify non-referential anaphora in
multiple languages. It is not always clear to what
extent the features and modules developed for En-
glish systems apply to other languages. For exam-
ple, the detector of Lappin and Leass (1994) labels a
pronoun as non-referential if it matches one of sev-
eral syntactic patterns, including: ?It is Cogv-ed that
Sentence,? where Cogv is a ?cognitive verb? such
as recommend, think, believe, know, anticipate, etc.
Porting this approach to a new language would re-
quire not only access to a syntactic parser and a list
of cognitive verbs in that language, but the devel-
opment of new patterns to catch non-referential pro-
noun uses that do not exist in English.
Moreover, writing a set of rules to capture this
phenomenon is likely to miss many less-common
uses. Alternatively, recent machine-learning ap-
proaches leverage a more general representation of
a pronoun instance. For example, Mu?ller (2006)
has a feature for ?distance to next complementizer
(that, if, whether)? and features for the tokens and
part-of-speech tags of the context words. Unfor-
tunately, there is still a lot of implicit and explicit
English-specific knowledge needed to develop these
features, including, for example, lists of ?seem?
verbs such as appear, look, mean, happen. Sim-
ilarly, the machine-learned system of Boyd et al
(2005) uses a set of ?idiom patterns? like ?on the
face of it? that trigger binary features if detected in
the pronoun context. Although machine learned sys-
tems can flexibly balance the various indicators and
contra-indicators of non-referentiality, a particular
feature is only useful if it is relevant to an example
in limited labelled training data.
Our approach avoids hand-crafting a set of spe-
11
cific indicator features; we simply use the distribu-
tion of the pronoun?s context. Our method is thus
related to previous work based on Harris (1985)?s
distributional hypothesis.2 It has been used to deter-
mine both word and syntactic path similarity (Hin-
dle, 1990; Lin, 1998a; Lin and Pantel, 2001). Our
work is part of a trend of extracting other important
information from statistical distributions. Dagan and
Itai (1990) use the distribution of a pronoun?s con-
text to determine which candidate antecedents can fit
the context. Bergsma and Lin (2006) determine the
likelihood of coreference along the syntactic path
connecting a pronoun to a possible antecedent, by
looking at the distribution of the path in text. These
approaches, like ours, are ways to inject sophisti-
cated ?world knowledge? into anaphora resolution.
3 Methodology
3.1 Definition
Our approach distinguishes contexts where pro-
nouns cannot be replaced by a preceding noun
phrase (non-noun-referential) from those where
nouns can occur (noun-referential). Although coref-
erence evaluations, such as the MUC (1997) tasks,
also make this distinction, it is not necessarily
used by all researchers. Evans (2001), for exam-
ple, distinguishes between ?clause anaphoric? and
?pleonastic? as in the following two instances:
(3) The paper reported that it had snowed. It was
obvious. (clause anaphoric)
(4) It was obvious that it had snowed. (pleonastic)
The word It in sentence (3) is considered referen-
tial, while the word It in sentence (4) is considered
non-referential.3 From our perspective, this inter-
pretation is somewhat arbitrary. One could also say
that the It in both cases refers to the clause ?that it
had snowed.? Indeed, annotation experiments using
very fine-grained categories show low annotation re-
liability (Mu?ller, 2006). On the other hand, there
is no debate over the importance nor the definition
of distinguishing pronouns that refer to nouns from
those that do not. We adopt this distinction for our
2Words occurring in similar contexts have similar meanings
3The it in ?it had snowed? is, of course, non-referential.
work, and show it has good inter-annotator reliabil-
ity (Section 4.1). We henceforth refer to non-noun-
referential simply as non-referential, and thus con-
sider the word It in both sentences (3) and (4) as
non-referential.
Non-referential pronouns are widespread in nat-
ural language. The es in the German ?Wie geht es
Ihnen? and the il in the French ?S?il vous pla??t? are
both non-referential. In pro-drop languages that may
omit subject pronouns, there remains the question
of whether an omitted pronoun is referential (Zhao
and Ng, 2007). Although we focus on the English
pronoun it, our approach should differentiate any
words that have both a structural and a referential
role in language, e.g. words like this, there and
that (Mu?ller, 2007). We believe a distributional ap-
proach could also help in related tasks like identify-
ing the generic use of you (Gupta et al, 2007).
3.2 Context Distribution
Our method extracts the context surrounding a pro-
noun and determines which other words can take the
place of the pronoun in the context. The extracted
segments of context are called context patterns. The
words that take the place of the pronoun are called
pattern fillers. We gather pattern fillers from a large
collection of n-gram frequencies. The maximum
size of a context pattern depends on the size of n-
grams available in the data. In our n-gram collection
(Section 3.4), the lengths of the n-grams range from
unigrams to 5-grams, so our maximum pattern size
is five. For a particular pronoun in text, there are five
possible 5-grams that span the pronoun. For exam-
ple, in the following instance of it:
... said here Thursday that it is unnecessary to continue ...
We can extract the following 5-gram patterns:
said here Thursday that *
here Thursday that * is
Thursday that * is unnecessary
that * is unnecessary to
* is unnecessary to continue
Similarly, we extract the four 4-gram patterns.
Shorter n-grams were not found to improve perfor-
mance on development data and hence are not ex-
tracted. We only use context within the current sen-
tence (including the beginning-of-sentence and end-
of-sentence tokens) so if a pronoun occurs near a
sentence boundary, some patterns may be missing.
12
Pattern Filler Type String
#1: 3rd-person pron. sing. it/its
#2: 3rd-person pron. plur. they/them/their
#3: any other pronoun he/him/his/,
I/me/my, etc.
#4: infrequent word token ?UNK?
#5: any other token *
Table 1: Pattern filler types
We take a few steps to improve generality. We
change the patterns to lower-case, convert sequences
of digits to the # symbol, and run the Porter stem-
mer4 (Porter, 1980). To generalize rare names, we
convert capitalized words longer than five charac-
ters to a special NE tag. We also added a few simple
rules to stem the irregular verbs be, have, do, and
said, and convert the common contractions ?nt, ?s,
?m, ?re, ?ve, ?d, and ?ll to their most likely stem.
We do the same processing to our n-gram corpus.
We then find all n-grams matching our patterns, al-
lowing any token to match the wildcard in place of
it. Also, other pronouns in the pattern are allowed
to match a corresponding pronoun in an n-gram, re-
gardless of differences in inflection and class.
We now discuss how to use the distribution of pat-
tern fillers. For identifying non-referential it in En-
glish, we are interested in how often it occurs as a
pattern filler versus other nouns. However, deter-
mining part-of-speech in a large n-gram corpus is
not simple, nor would it easily extend to other lan-
guages. Instead, we gather counts for five differ-
ent classes of words that fill the wildcard position,
easily determined by string match (Table 1). The
third-person plural they (#2) reliably occurs in pat-
terns where referential it also resides. The occur-
rence of any other pronoun (#3) guarantees that at
the very least the pattern filler is a noun. A match
with the infrequent word token ?UNK? (#4) (ex-
plained in Section 3.4) will likely be a noun because
nouns account for a large proportion of rare words in
a corpus. Gathering any other token (#5) also mostly
finds nouns; inserting another part-of-speech usually
4Adapted from the Bow-toolkit (McCallum, 1996). Our
method also works without the stemmer; we simply truncate
the words in the pattern at a given maximum length (see Sec-
tion 5.1). With simple truncation, all the pattern processing can
be easily applied to other languages.
Pattern Filler Counts#1 #2 #3 #5
sai here NE that * 84 0 291 3985
here NE that * be 0 0 0 93
NE that * be unnecessari 0 0 0 0
that * be unnecessari to 16726 56 0 228
* be unnecessari to continu 258 0 0 0
Table 2: 5-gram context patterns and pattern-filler counts
for the Section 3.2 example.
results in an unlikely, ungrammatical pattern.
Table 2 gives the stemmed context patterns for our
running example. It also gives the n-gram counts
of pattern fillers matching the first four filler types
(there were no matches of the ?UNK? type, #4).
3.3 Feature Vector Representation
There are many possible ways to use the above
counts. Intuitively, our method should identify as
non-referential those instances that have a high pro-
portion of fillers of type #1 (i.e., the word it), while
labelling as referential those with high counts for
other types of fillers. We would also like to lever-
age the possibility that some of the patterns may be
more predictive than others, depending on where the
wildcard lies in the pattern. For example, in Table 2,
the cases where the it-position is near the beginning
of the pattern best reflect the non-referential nature
of this instance. We can achieve these aims by or-
dering the counts in a feature vector, and using a la-
belled set of training examples to learn a classifier
that optimally weights the counts.
For classification, we define non-referential as
positive and referential as negative. Our feature rep-
resentation very much resembles Table 2. For each
of the five 5-gram patterns, ordered by the position
of the wildcard, we have features for the logarithm
of counts for filler types #1, #2, ... #5. Similarly,
for each of the four 4-gram patterns, we provide the
log-counts corresponding to types #1, #2, ... #5 as
well. Before taking the logarithm, we smooth the
counts by adding a fixed number to all observed val-
ues. We also provide, for each pattern, a feature that
indicates if the pattern is not available because the
it-position would cause the pattern to span beyond
the current sentence. There are twenty-five 5-gram,
twenty 4-gram, and nine indicator features in total.
13
Our classifier should learn positive weights on the
type #1 counts and negative weights on the other
types, with higher absolute weights on the more pre-
dictive filler types and pattern positions. Note that
leaving the pattern counts unnormalized automati-
cally allows patterns with higher counts to contribute
more to the prediction of their associated instances.
3.4 N-Gram Data
We now describe the collection of n-grams and their
counts used in our implementation. We use, to our
knowledge, the largest publicly available collection:
the Google Web 1T 5-gram Corpus Version 1.1.5
This collection was generated from approximately 1
trillion tokens of online text. In this data, tokens ap-
pearing less than 200 times have been mapped to the
?UNK? symbol. Also, only n-grams appearing more
than 40 times are included. For languages where
such an extensive n-gram resource is not available,
the n-gram counts could also be taken from the page-
counts returned by an Internet search engine.
4 Evaluation
4.1 Labelled It Data
We need labelled data for training and evaluation of
our system. This data indicates, for every occurrence
of the pronoun it, whether it refers to a preceding
noun phrase or not. Standard coreference resolution
data sets annotate all noun phrases that have an an-
tecedent noun phrase in the text. Therefore, we can
extract labelled instances of it from these sets. We
do this for the dry-run and formal sets from MUC-7
(1997), and merge them into a single data set.
Of course, full coreference-annotated data is a
precious resource, with the pronoun it making up
only a small portion of the marked-up noun phrases.
We thus created annotated data specifically for the
pronoun it. We annotated 1020 instances in a col-
lection of Science News articles (from 1995-2000),
downloaded from the Science News website. We
also annotated 709 instances in the WSJ portion of
the DARPA TIPSTER Project (Harman, 1992), and
279 instances in the English portion of the Europarl
Corpus (Koehn, 2005).
A single annotator (A1) labelled all three datasets, while two additional annotators not connected
5Available from the LDC as LDC2006T13
Data Set Number of It % Non-Referential
Europarl 279 50.9
Sci-News 1020 32.6
WSJ 709 25.1
MUC 129 31.8
Train 1069 33.2
Test 1067 31.7
Test-200 200 30.0
Table 3: Data sets used in experiments.
with the project (A2 and A3) were asked to sepa-rately re-annotate a portion of each, so that inter-
annotator agreement could be calculated. A1 and
A2 agreed on 96% of annotation decisions, while
A1-A3, and A2-A3, agreed on 91% and 93% of de-cisions, respectively. The Kappa statistic (Jurafsky
and Martin, 2000, page 315), with P(E) computed
from the confusion matrices, was a high 0.90 for A1-
A2, and 0.79 and 0.81 for the other pairs, around the0.80 considered to be good reliability. These are,
perhaps surprisingly, the only known it-annotation-
agreement statistics available for written text. They
contrast favourably with the low agreement seen on
categorizing it in spoken dialog (Mu?ller, 2006).
We make all the annotations available in It-Bank,
an online repository for annotated it-instances.6
It-Bank also allows other researchers to distribute
their it annotations. Often, the full text of articles
containing annotations cannot be shared because of
copyright. However, sharing just the sentences con-
taining the word it, randomly-ordered, is permissible
under fair-use guidelines. The original annotators
retain their copyright on the annotations.
We use our annotated data in two ways. First
of all, we perform cross-validation experiments on
each of the data sets individually, to help gauge the
difficulty of resolution on particular domains and
volumes of training data. Secondly, we randomly
distribute all instances into two main sets, a training
set and a test set. We also construct a smaller test
set, Test-200, containing only the first 200 instances
in the Test set. We use Test-200 for human experi-
ments and error analysis (Section 5.2). Table 3 sum-
marizes all the sets used in the experiments.
6www.cs.ualberta.ca/?bergsma/ItBank/. It-Bank also con-
tains an additional 1,077 examples used as development data.
14
4.2 Comparison Approaches
We represent feature vectors exactly as described
in Section 3.3. We smooth by adding 40 to all
counts, equal to the minimum count in the n-gram
data. For classification, we use a maximum entropy
model (Berger et al, 1996), from the logistic re-
gression package in Weka (Witten and Frank, 2005),
with all default parameter settings. Results with
our distributional approach are labelled as DISTRIB.
Note that our maximum entropy classifier actually
produces a probability of non-referentiality, which
is thresholded at 50% to make a classification.
As a baseline, we implemented the non-referential
it detector of Lappin and Leass (1994), labelled as
LL in the results. This is a syntactic detector, a
point missed by Evans (2001) in his criticism: the
patterns are robust to intervening words and modi-
fiers (e.g. ?it was never thought by the committee
that...?) provided the sentence is parsed correctly.7
We automatically parse sentences with Minipar, a
broad-coverage dependency parser (Lin, 1998b).
We also use a separate, extended version of
the LL detector, implemented for large-scale non-
referential detection by Cherry and Bergsma (2005).
This system, also for Minipar, additionally detects
instances of it labelled with Minipar?s pleonastic cat-
egory Subj. It uses Minipar?s named-entity recog-
nition to identify time expressions, such as ?it was
midnight,? and provides a number of other patterns
to match common non-referential it uses, such as
in expressions like ?darn it,? ?don?t overdo it,? etc.
This extended detector is labelled as MINIPL (for
Minipar pleonasticity) in our results.
Finally, we tested a system that combines the
above three approaches. We simply add the LL and
MINIPL decisions as binary features in the DISTRIB
system. This system is called COMBO in our results.
4.3 Evaluation Criteria
We follow Mu?ller (2006)?s evaluation criteria. Pre-
cision (P) is the proportion of instances that we la-
bel as non-referential that are indeed non-referential.
Recall (R) is the proportion of true non-referentials
that we detect, and is thus a measure of the coverage
7Our approach, on the other hand, would seem to be suscep-
tible to such intervening material, if it pushes indicative context
tokens out of the 5-token window.
System P R F Acc
LL 93.4 21.0 34.3 74.5
MINIPL 66.4 49.7 56.9 76.1
DISTRIB 81.4 71.0 75.8 85.7
COMBO 81.3 73.4 77.1 86.2
Table 4: Train/Test-split performance (%).
of the system. F-Score (F) is the geometric average
of precision and recall; it is the most common non-
referential detection metric. Accuracy (Acc) is the
percentage of instances labelled correctly.
5 Results
5.1 System Comparison
Table 4 gives precision, recall, F-score, and accu-
racy on the Train/Test split. Note that while the LL
system has high detection precision, it has very low
recall, sharply reducing F-score. The MINIPL ap-
proach sacrifices some precision for much higher
recall, but again has fairly low F-score. To our
knowledge, our COMBO system, with an F-Score
of 77.1%, achieves the highest performance of any
non-referential system yet implemented. Even more
importantly, DISTRIB, which requires only minimal
linguistic processing and no encoding of specific in-
dicator patterns, achieves 75.8% F-Score. The dif-
ference between COMBO and DISTRIB is not statis-
tically significant, while both are significantly bet-
ter than the rule-based approaches.8 This provides
strong motivation for a ?light-weight? approach to
non-referential it detection ? one that does not re-
quire parsing or hand-crafted rules and ? is easily
ported to new languages and text domains.
Since applying an English stemmer to the con-
text words (Section 3.2) reduces the portability of
the distributional technique, we investigated the use
of more portable pattern abstraction. Figure 1 com-
pares the use of the stemmer to simply truncating the
words in the patterns at a certain maximum length.
Using no truncation (Unaltered) drops the F-Score
by 4.3%, while truncating the patterns to a length of
four only drops the F-Score by 1.4%, a difference
which is not statistically significant. Simple trunca-
tion may be a good option for other languages where
stemmers are not readily available. The optimum
8All significance testing uses McNemar?s test, p<0.05
15
 68
 70
 72
 74
 76
 78
 80
 1  2  3  4  5  6  7  8  9  10
F-
Sc
or
e
Truncated word length
Stemmed patterns
Truncated patterns
Unaltered patterns
Figure 1: Effect of pattern-word truncation on non-
referential it detection (COMBO system, Train/Test split).
System Europl. Sci-News WSJ MUC
LL 44.0 39.3 21.5 13.3
MINIPL 70.3 61.8 22.0 50.7
DISTRIB 79.7 77.2 69.5 68.2
COMBO 76.2 78.7 68.1 65.9
COMBO4 83.6 76.5 67.1 74.7
Table 5: 10-fold cross validation F-Score (%).
truncation size will likely depend on the length of
the base forms of words in that language. For real-
world application of our approach, truncation also
reduces the table sizes (and thus storage and look-
up costs) of any pre-compiled it-pattern database.
Table 5 compares the 10-fold cross-validation F-
score of our systems on the four data sets. The
performance of COMBO on Europarl and MUC is
affected by the small number of instances in these
sets (Section 4, Table 3). We can reduce data frag-
mentation by removing features. For example, if we
only use the length-4 patterns in COMBO (labelled as
COMBO4), performance increases dramatically on
Europarl and MUC, while dipping slightly for the
larger Sci-News and WSJ sets. Furthermore, select-
ing just the three most useful filler type counts as
features (#1,#2,#5), boosts F-Score on Europarl to
86.5%, 10% above the full COMBO system.
5.2 Analysis and Discussion
In light of these strong results, it is worth consid-
ering where further gains in performance might yet
be found. One key question is to what extent a lim-
ited context restricts identification performance. We
first tested the importance of the pattern length by
System P R F Acc
DISTRIB 80.0 73.3 76.5 86.5
COMBO 80.7 76.7 78.6 87.5
Human-1 92.7 63.3 75.2 87.5
Human-2 84.0 70.0 76.4 87.0
Human-3 72.2 86.7 78.8 86.0
Table 6: Evaluation on Test-200 (%).
using only the length-4 counts in the DISTRIB sys-
tem (Train/Test split). Surprisingly, the drop in F-
Score was only one percent, to 74.8%. Using only
the length-5 counts drops F-Score to 71.4%. Neither
are statistically significant; however there seems to
be diminishing returns from longer context patterns.
Another way to view the limited context is to ask,
given the amount of context we have, are we mak-
ing optimum use of it? We answer this by seeing
how well humans can do with the same information.
As explained in Section 3.2, our system uses 5-gram
context patterns that together span from four-to-the-
left to four-to-the-right of the pronoun. We thus pro-
vide these same nine-token windows to our human
subjects, and ask them to decide whether the pro-
nouns refer to previous noun phrases or not, based
on these contexts. Subjects first performed a dry-
run experiment on separate development data. They
were shown their errors and sources of confusion
were clarified. They then made the judgments unas-
sisted on the final Test-200 data. Three humans per-
formed the experiment. Their results show a range
of preferences for precision versus recall, with both
F-Score and Accuracy on average below the perfor-
mance of COMBO (Table 6). Foremost, these results
show that our distributional approach is already get-
ting good leverage from the limited context informa-
tion, around that achieved by our best human.
It is instructive to inspect the twenty-five Test-200
instances that the COMBO system classified incor-
rectly, given human performance on this same set.
Seventeen of the twenty-five COMBO errors were
also made by one or more human subjects, suggest-
ing system errors are also mostly due to limited con-
text. For example, one of these errors was for the
context: ?it takes an astounding amount...? Here, the
non-referential nature of the instance is not apparent
without the infinitive clause that ends the sentence:
?... of time to compare very long DNA sequences
16
with each other.?
Six of the eight errors unique to the COMBO sys-
tem were cases where the system falsely said the
pronoun was non-referential. Four of these could
have referred to entire sentences or clauses rather
than nouns. These confusing cases, for both hu-
mans and our system, result from our definition
of a referential pronoun: pronouns with verbal or
clause antecedents are considered non-referential
(Section 3.1). If an antecedent verb or clause is
replaced by a nominalization (Smith researched...
to Smith?s research), a referring pronoun, in the
same context, becomes referential. When we inspect
the probabilities produced by the maximum entropy
classifier (Section 4.2), we see only a weak bias for
the non-referential class on these examples, reflect-
ing our classifier?s uncertainty. It would likely be
possible to improve accuracy on these cases by en-
coding the presence or absence of preceding nomi-
nalizations as a feature of our classifier.
Another false non-referential decision is for the
phrase ?... machine he had installed it on.? The it is
actually referential, but the extracted patterns (e.g.
?he had install * on?) are nevertheless usually filled
with it.9 Again, it might be possible to fix such ex-
amples by leveraging the preceding discourse. No-
tably, the first noun-phrase before the context is the
word ?software.? There is strong compatibility be-
tween the pronoun-parent ?install? and the candidate
antecedent ?software.? In a full coreference resolu-
tion system, when the anaphora resolution module
has a strong preference to link it to an antecedent
(which it should when the pronoun is indeed refer-
ential), we can override a weak non-referential prob-
ability. Non-referential it detection should not be
a pre-processing step, but rather part of a globally-
optimal configuration, as was done for general noun
phrase anaphoricity by Denis and Baldridge (2007).
The suitability of this kind of approach to correct-
ing some of our system?s errors is especially obvious
when we inspect the probabilities of the maximum
entropy model?s output decisions on the Test-200
set. Where the maximum entropy classifier makes
mistakes, it does so with less confidence than when
it classifies correct examples. The average predicted
9This example also suggests using filler counts for the word
?the? as a feature when it is the last word in the pattern.
probability of the incorrect classifications is 76.0%
while the average probability of the correct classi-
fications is 90.3%. Many incorrect decisions are
ready to switch sides; our next step will be to use
features of the preceding discourse and the candi-
date antecedents to help give them a push.
6 Conclusion
We have presented an approach to detecting non-
referential pronouns in text based on the distribu-
tion of the pronoun?s context. The approach is sim-
ple to implement, attains state-of-the-art results, and
should be easily ported to other languages. Our tech-
nique demonstrates how large volumes of data can
be used to gather world knowledge for natural lan-
guage processing. A consequence of this research
was the creation of It-Bank, a collection of thou-
sands of labelled examples of the pronoun it, which
will benefit other coreference resolution researchers.
Error analysis reveals that our system is getting
good leverage out of the pronoun context, achiev-
ing results comparable to human performance given
equivalent information. To boost performance fur-
ther, we will need to incorporate information from
preceding discourse. Future research will also test
the distributional classification of other ambiguous
pronouns, like this, you, there, and that. Another
avenue of study will look at the interaction between
coreference resolution and machine translation. For
example, if a single form in English (e.g. that)
is separated into different meanings in another lan-
guage (e.g., Spanish demonstrative ese, nominal ref-
erence e?se, abstract or statement reference eso, and
complementizer que), then aligned examples pro-
vide automatically-disambiguated English data. We
could extract context patterns and collect statistics
from these examples like in our current approach.
In general, jointly optimizing translation and coref-
erence is an exciting and largely unexplored re-
search area, now partly enabled by our portable non-
referential detection methodology.
Acknowledgments
We thank Kristin Musselman and Christopher Pinchak for as-
sistance preparing the data, and we thank Google Inc. for shar-
ing their 5-gram corpus. We gratefully acknowledge support
from the Natural Sciences and Engineering Research Council
of Canada, the Alberta Ingenuity Fund, and the Alberta Infor-
matics Circle of Research Excellence.
17
References
Adam L. Berger, Stephen A. Della Pietra, and Vincent
J. Della Pietra. 1996. A maximum entropy approach
to natural language processing. Computational Lin-
guistics, 22(1):39?71.
Shane Bergsma and Dekang Lin. 2006. Bootstrap-
ping path-based pronoun resolution. In COLING-
ACL, pages 33?40.
Adrianne Boyd, Whitney Gegg-Harrison, and Donna By-
ron. 2005. Identifying non-referential it: a machine
learning approach incorporating linguistically moti-
vated patterns. In ACL Workshop on Feature Engi-
neering for Machine Learning in NLP, pages 40?47.
Colin Cherry and Shane Bergsma. 2005. An expecta-
tion maximization approach to pronoun resolution. In
CoNLL, pages 88?95.
Ido Dagan and Alan Itai. 1990. Automatic processing of
large corpora for the resolution of anaphora references.
In COLING, volume 3, pages 330?332.
Pascal Denis and Jason Baldridge. 2007. Joint determi-
nation of anaphoricity and coreference using integer
programming. In NAACL-HLT, pages 236?243.
Miriam Eckert and Michael Strube. 2000. Dialogue acts,
synchronizing units, and anaphora resolution. Journal
of Semantics, 17(1):51?89.
Richard Evans. 2001. Applying machine learning to-
ward an automatic classification of it. Literary and
Linguistic Computing, 16(1):45?57.
Surabhi Gupta, Matthew Purver, and Dan Jurafsky. 2007.
Disambiguating between generic and referential ?you?
in dialog. In ACL Demo and Poster Sessions, pages
105?108.
Aria Haghighi and Dan Klein. 2007. Unsupervised
coreference resolution in a nonparametric Bayesian
model. In ACL, pages 848?855.
Donna Harman. 1992. The DARPA TIPSTER project.
ACM SIGIR Forum, 26(2):26?28.
Zellig Harris. 1985. Distributional structure. In J.J.
Katz, editor, The Philosophy of Linguistics, pages 26?
47. Oxford University Press, New York.
Donald Hindle. 1990. Noun classification from
predicate-argument structures. In ACL, pages 268?
275.
Graeme Hirst. 1981. Anaphora in Natural Language
Understanding: A Survey. Springer Verlag.
Jerry Hobbs. 1978. Resolving pronoun references. Lin-
gua, 44(311):339?352.
Daniel Jurafsky and James H. Martin. 2000. Speech and
language processing. Prentice Hall.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In MT Summit X, pages
79?86.
Shalom Lappin and Herbert J. Leass. 1994. An algo-
rithm for pronominal anaphora resolution. Computa-
tional Linguistics, 20(4):535?561.
Dekang Lin and Patrick Pantel. 2001. Discovery of infer-
ence rules for question answering. Natural Language
Engineering, 7(4):343?360.
Dekang Lin. 1998a. Automatic retrieval and clustering
of similar words. In COLING-ACL, pages 768?773.
Dekang Lin. 1998b. Dependency-based evaluation of
MINIPAR. In LREC Workshop on the Evaluation of
Parsing Systems.
Andrew Kachites McCallum. 1996. Bow:
A toolkit for statistical language modeling,
text retrieval, classification and clustering.
http://www.cs.cmu.edu/?mccallum/bow.
MUC-7. 1997. Coreference task definition (v3.0, 13 Jul
97). In Proceedings of the Seventh Message Under-
standing Conference (MUC-7).
Christoph Mu?ller. 2006. Automatic detection of non-
referential It in spoken multi-party dialog. In EACL,
pages 49?56.
Christoph Mu?ller. 2007. Resolving It, This, and That in
unrestricted multi-party dialog. In ACL, pages 816?
823.
Vincent Ng and Claire Cardie. 2002. Identifying
anaphoric and non-anaphoric noun phrases to improve
coreference resolution. In COLING, pages 730?736.
Chris D. Paice and Gareth D. Husk. 1987. Towards the
automatic recognition of anaphoric features in English
text: the impersonal pronoun ?it?. Computer Speech
and Language, 2:109?132.
Martin F. Porter. 1980. An algorithm for suffix stripping.
Program, 14(3):130?137.
Ian H. Witten and Eibe Frank. 2005. Data Mining: Prac-
tical machine learning tools and techniques. Morgan
Kaufmann, second edition.
Shanheng Zhao and Hwee Tou Ng. 2007. Identification
and resolution of Chinese zero pronouns: A machine
learning approach. In EMNLP, pages 541?550.
18
Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), pages 120?128,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Glen, Glenda or Glendale:
Unsupervised and Semi-supervised Learning of English Noun Gender
Shane Bergsma
Department of Computing Science
University of Alberta
Edmonton, Alberta
Canada, T6G 2E8
bergsma@cs.ualberta.ca
Dekang Lin
Google, Inc.
1600 Amphitheatre Parkway
Mountain View
California, 94301
lindek@google.com
Randy Goebel
Department of Computing Science
University of Alberta
Edmonton, Alberta
Canada, T6G 2E8
goebel@cs.ualberta.ca
Abstract
English pronouns like he and they reliably re-
flect the gender and number of the entities to
which they refer. Pronoun resolution systems
can use this fact to filter noun candidates that
do not agree with the pronoun gender. In-
deed, broad-coverage models of noun gender
have proved to be the most important source
of world knowledge in automatic pronoun res-
olution systems.
Previous approaches predict gender by count-
ing the co-occurrence of nouns with pronouns
of each gender class. While this provides use-
ful statistics for frequent nouns, many infre-
quent nouns cannot be classified using this
method. Rather than using co-occurrence in-
formation directly, we use it to automatically
annotate training examples for a large-scale
discriminative gender model. Our model col-
lectively classifies all occurrences of a noun
in a document using a wide variety of con-
textual, morphological, and categorical gender
features. By leveraging large volumes of un-
labeled data, our full semi-supervised system
reduces error by 50% over the existing state-
of-the-art in gender classification.
1 Introduction
Pronoun resolution is the process of determining
which preceding nouns are referred to by a partic-
ular pronoun in text. Consider the sentence:
(1) Glen told Glenda that she was wrong about
Glendale.
A pronoun resolution system should determine that
the pronoun she refers to the noun Glenda. Pro-
noun resolution is challenging because it requires a
lot of world knowledge (general knowledge of word
types). If she is replaced with the pronoun he in (1),
Glen becomes the antecedent. Pronoun resolution
systems need the knowledge of noun gender that ad-
vises that Glen is usually masculine (and thus re-
ferred to by he) while Glenda is feminine.
English third-person pronouns are grouped in four
gender/number categories: masculine (he, his, him,
himself ), feminine (she, her, herself ), neutral (it, its,
itself ), and plural (they, their, them, themselves). We
broadly refer to these gender and number classes
simply as gender. The objective of our work is to
correctly assign gender to English noun tokens, in
context; to determine which class of pronoun will
refer to a given noun.
One successful approach to this problem is to
build a statistical gender model from a noun?s asso-
ciation with pronouns in text. For example, Ge et al
(1998) learn Ford has a 94% chance of being neu-
tral, based on its frequent co-occurrence with neu-
tral pronouns in text. Such estimates are noisy but
useful. Both Ge et al (1998) and Bergsma and Lin
(2006) show that learned gender is the most impor-
tant feature in their pronoun resolution systems.
English differs from other languages like French
and German in that gender is not an inherent gram-
matical property of an English noun, but rather a
property of a real-world entity that is being referred
to. A common noun like lawyer can be (semanti-
cally) masculine in one document and feminine in
another. While previous statistical gender models
learn gender for noun types only, we use document
context to correctly determine the current gender
class of noun tokens, making dynamic decisions on
common nouns like lawyer and ambiguous names
like Ford. Furthermore, if a noun type has not yet
120
been observed (an unknown word), previous ap-
proaches cannot estimate the gender. Our system,
on the other hand, is able to correctly determine that
unknown words corroborators and propeller-heads
are plural, while Pope Formosus is masculine, using
learned contextual and morphological cues.
Our approach is based on the key observation that
while gender information from noun-pronoun co-
occurrence provides imperfect noun coverage, it can
nevertheless provide rich and accurate training data
for a large-scale discriminative classifier. The clas-
sifier leverages a wide variety of noun properties to
generalize from the automatically-labeled examples.
The steps in our approach are:
1. Training:
(a) Automatically extract a set of seed
(noun,gender) pairs from high-quality in-
stances in a statistical gender database.
(b) In a large corpus of text, find documents con-
taining these nouns.
(c) For all instances of each noun in each document,
create a single, composite feature vector repre-
senting all the contexts of the noun in the docu-
ment, as well as encoding other selected proper-
ties of the noun type.
(d) Label each feature vector with the seed noun?s
corresponding gender.
(e) Train a 4-way gender classifier (masculine, fem-
inine, neutral, plural) from the automatically-
labeled vectors.
2. Testing:
(a) Given a new document, create a composite fea-
ture vector for all occurrences of each noun.
(b) Use the learned classifier to assign gender to
each feature vector, and thus all occurrences of
all nouns in the document.
This algorithm achieves significantly better per-
formance than the existing state-of-the-art statisti-
cal gender classifier, while requiring no manually-
labeled examples to train. Furthermore, by training
on a small number of manually-labeled examples,
we can combine the predictions of this system with
the counts from the original gender database. This
semi-supervised extension achieves 95.5% accuracy
on final unseen test data, an impressive 50% reduc-
tion in error over previous work.
2 Path-based Statistical Noun Gender
Seed (noun,gender) examples can be extracted re-
liably and automatically from raw text, providing
the training data for our discriminative classifier.
We call these examples pseudo-seeds because they
are created fully automatically, unlike the small set
of manually-created seeds used to initialize other
bootstrapping approaches (cf. the bootstrapping ap-
proaches discussed in Section 6).
We adopt a statistical approach to acquire the
pseudo-seed (noun,gender) pairs. All previous sta-
tistical approaches rely on a similar observation: if
a noun like Glen is often referred to by masculine
pronouns, like he or his, then Glen is likely a mas-
culine noun. But for most nouns we have no an-
notated data recording their coreference with pro-
nouns, and thus no data from which we can ex-
tract the co-occurrence statistics. Thus previous ap-
proaches rely on either hand-crafted coreference-
indicating patterns (Bergsma, 2005), or iteratively
guess and improve gender models through expec-
tation maximization of pronoun resolution (Cherry
and Bergsma, 2005; Charniak and Elsner, 2009). In
statistical approaches, the more frequent the noun,
the more accurate the assignment of gender.
We use the approach of Bergsma and Lin (2006),
both because it achieves state-of-the-art gender
classification performance, and because a database
of the obtained noun genders is available online.1
Bergsma and Lin (2006) use an unsupervised
algorithm to identify syntactic paths along which a
noun and pronoun are highly likely to corefer. To
extract gender information, they processed a large
corpus of news text, and obtained co-occurrence
counts for nouns and pronouns connected with these
paths in the corpus. In their database, each noun is
listed with its corresponding masculine, feminine,
neutral, and plural pronoun co-occurrence counts,
e.g.:
glen 555 42 32 34
glenda 8 102 0 11
glendale 24 2 167 18
glendalians 0 0 0 1
glenn 3182 207 95 54
glenna 0 6 0 0
1Available at http://www.cs.ualberta.ca/?bergsma/Gender/
121
This sample of the gender data shows that the
noun glenda, for example, occurs 8 times with mas-
culine pronouns, 102 times with feminine pronouns,
0 times with neutral pronouns, and 11 times with
plural pronouns; 84% of the time glenda co-occurs
with a feminine pronoun. Note that all nouns in the
data have been converted to lower-case.2
There are gender counts for 3.1 million English
nouns in the online database. These counts form the
basis for the state-of-the-art gender classifier. We
can either take the most-frequent pronoun-gender
(MFPG) as the class (e.g. feminine for glenda), or
we can supply the logarithm of the counts as features
in a 4-way multi-class classifier. We implement the
latter approach as a comparison system and refer to
it as PATHGENDER in our experiments.
In our approach, rather than using these counts
directly, we process the database to automatically
extract a high-coverage but also high-quality set of
pseudo-seed (noun,gender) pairs. First, we filter
nouns that occur less than fifty times and whose
MFPG accounts for less than 85% of counts. Next,
we note that the most reliable nouns should occur
relatively often in a coreferent path. For exam-
ple, note that importance occurs twice as often on
the web as Clinton, but has twenty-four times less
counts in the gender database. This is because im-
portance is unlikely to be a pronoun?s antecedent.
We plan to investigate this idea further in future
work as a possible filter on antecedent candidates
for pronoun resolution. For the present work, sim-
ply note that a high ratio of database-count to web-
count provides a good indication of the reliability of
a noun?s gender counts, and thus we filter nouns that
have such ratios below a threshold.3 After this fil-
tering, we have about 45 thousand nouns to which
we automatically assign gender according to their
MFPG. These (noun,gender) pairs provide the seed
examples for the training process described in the
2Statistical approaches can adapt to the idiosyncrasies of the
particular text domain. In the news text from which this data
was generated, for example, both the word ships and specific
instances of ships (the USS Cole, the Titanic, etc.) are neutral.
In Wikipedia, on the other hand, feminine pronouns are often
used for ships. Such differences can be learned automatically.
3We roughly tuned all the thresholds to obtain the highest
number of seeds such that almost all of them looked correct
(e.g. Figure 1). Further work is needed to determine whether a
different precision/recall tradeoff can improve performance.
. . .
stefanie
steffi graf
steinem
stella mccartney
stellar jayne
stepdaughter
stephanie
stephanie herseth
stephanie white
stepmother
stewardess
. . .
Figure 1: Sample feminine seed nouns
following section. Figure 1 provides a portion of the
ordered feminine seed nouns that we extracted.
3 Discriminative Learning of Gender
Once we have extracted a number of pseudo-seed
(noun,gender) pairs, we use them to automatically-
label nouns (in context) in raw text. The auto-
labeled examples provide training data for discrimi-
native learning of noun gender.
Since the training pairs are acquired from a
sparse and imperfect model of gender, what can
we gain by training over them? We can regard the
Bergsma and Lin (2006) approach and our discrim-
inative system as two orthogonal views of gender,
in a co-training sense (Blum and Mitchell, 1998).
Some nouns can be accurately labeled by noun-
pronoun co-occurrence (a view based on pronoun
co-occurrence), and these examples can be used to
deduce other gender-indicating regularities (a view
based on other features, described below).
We presently explain how examples are extracted
using our pseudo-seed pairs, turned into auto-
labeled feature vectors, and then used to train a su-
pervised classifier.
3.1 Automatic example extraction
Our example-extraction module processes a large
collection of documents (roughly a million docu-
ments in our experiments). For each document, we
extract all the nouns, including context words within
?5 tokens of each noun. We then group the nouns by
122
Class=masculine String=?Lee?
Contexts =
?led some to suggest that ? , who was born in?
?? also downloaded secret files to?
?? says he was just making?
?by mishandling the investigation of ? .?
. . .
Figure 2: Sample noun training instance
their (lower-case) string. If a group?s noun-string is
in our set of seed (noun,gender) pairs, we assign the
corresponding gender to be the class of the group.
Otherwise, we discard the group. To prevent fre-
quent nouns from dominating our training data, we
only keep the first 200 groups corresponding to each
noun string. Figure 2 gives an example training noun
group with some (selected) context sentences. At
test time, all nouns in the test documents are con-
verted to this format for further processing.
We group nouns because there is a strong ten-
dency for nouns to have only one sense (and hence
gender) per discourse. We extract contexts because
nearby words provide good clues about which gen-
der is being used. The notion that nouns have only
one sense per discourse/collocation was also ex-
ploited by Yarowsky (1995) in his seminal work on
bootstrapping for word sense disambiguation.
3.2 Feature vectors
Once the training instances are extracted, they are
converted to labeled feature vectors for supervised
learning. The automatically-determined gender pro-
vides the class label (e.g., masculine for the group
in Figure 2). The features identify properties of the
noun and its context that potentially correlate with a
particular gender category. We divide the features
into two sets: those that depend on the contexts
within the document (Context features: features of
the tokens in the document), and those that depend
on the noun string only (Type features). In both
cases we induce the feature space from the train-
ing examples, keeping only those features that occur
more than 5 times.
3.2.1 Context features
The first set of features represent the contexts of
the word, using all the contexts in the noun group.
To illustrate the potential utility of the context infor-
mation, consider the context sentences for the mas-
culine noun in Figure 2. Even if these snippets were
all the information we were given, it would be easy
to guess the gender of the noun.
We use binary attribute-value features to flag, for
any of the contexts, the presence of all words at con-
text positions ?1,?2, etc. (sometimes called col-
location features (Golding and Roth, 1999)). For
example, feature 255920 flags that the word two-to-
the-right of the noun is he. We also provide fea-
tures for the presence of all words anywhere within
?5 tokens of the noun (sometimes called context
words). We also parse the sentence and provide a
feature for the noun?s parent (and relationship with
the parent) in the parse tree. For example, the in-
stance in Figure 2 has features downloaded(subject),
says(subject), etc. Since plural nouns should be gov-
erned by plural verbs, this feature is likely to be es-
pecially helpful for number classification.
3.2.2 Type features
The next group of features represent morpholog-
ical properties of the noun. Binary features flag the
presence of all prefixes and suffixes of one-to-four
characters. For multi-token nouns, we have features
for the first and last token in the noun. Thus we hope
to learn that Bob begins masculine nouns while inc.
ends neutral ones.
Finally, we have features that indicate if the noun
or parts of the noun occur on various lists. Indica-
tor features specify if any token occurs on in-house
lists of given names, family names, cities, provinces,
countries, corporations, languages, etc. A feature
also indicates if a token is a corporate designation
(like inc. or ltd.) or a human one (like Mr. or Sheik).
We also made use of the person-name/instance
pairs automatically extracted by Fleischman et al
(2003).4 This data provides counts for pairs such
as (Zhang Qiyue, spokeswoman) and (Thorvald
Stoltenberg, mediator). We have features for all con-
cepts (like spokeswoman and mediator) and there-
fore learn their association with each gender.
3.3 Supervised learning and classification
Once all the feature vectors have been extracted,
they are passed to a supervised machine learn-
4Available at http://www.mit.edu/?mbf/instances.txt.gz
123
ing algorithm. We train and classify using a
multi-class linear-kernel Support Vector Machine
(SVM) (Crammer and Singer, 2001). SVMs are
maximum-margin classifiers that achieve good per-
formance on a range of tasks. At test time, nouns in
test documents are processed exactly as the training
instances described above, converting them to fea-
ture vectors. The test vectors are classified by the
SVM, providing gender classes for all the nouns in
the test document. Since all training examples are
labeled automatically (auto-trained), we denote sys-
tems using this approach as -AUTO.
3.4 Semi-supervised extension
Although a good gender classifier can be learned
from the automatically-labeled examples alone, we
can also use a small quantity of gold-standard la-
beled examples to achieve better performance.
Combining information from our two sets of la-
beled data is akin to a domain adaptation prob-
lem. The gold-standard data can be regarded as
high-quality in-domain data, and the automatically-
labeled examples can be regarded as the weaker, but
larger, out-of-domain evidence.
There is a simple but effective method for com-
bining information from two domains using predic-
tions as features. We train a classifier on the full set
of automatically-labeled data (as described in Sec-
tion 3.3), and then use this classifier?s predictions as
features in a separate classifier, which is trained on
the gold-standard data. This is like the competitive
Feats domain-adaptation system in Daume? III and
Marcu (2006).
For our particular SVM classifier (Section 4.1),
predictions take the form of four numerical scores
corresponding to the four different genders. Our
gold-standard classifier has features for these four
predictions plus features for the original path-based
gender counts (Section 2).5 Since this approach uses
both automatically-labeled and gold-standard data in
a semi-supervised learning framework, we denote
systems using this approach as -SEMI.
5We actually use 12 features for the path-based counts: the
4 original, and then 4 each for counts for the first and last token
in the noun string. See PATHGENDER+ in Section 4.2.
4 Experiments
4.1 Set-up
We parsed the 3 GB AQUAINT corpus (Vorhees,
2002) using Minipar (Lin, 1998) to create our un-
labeled data. We process this data as described in
Section 3, making feature vectors from the first 4
million noun groups. We train from these exam-
ples using a linear-kernel SVM via the the efficient
SVMmulticlass instance of the SVMstruct software
package (Tsochantaridis et al, 2004).
To create our gold-standard gender data, we fol-
low Bergsma (2005) in extracting gender informa-
tion from the anaphora-annotated portion6 of the
American National Corpus (ANC) (Ide and Sud-
erman, 2004). In each document, we first group
all nouns with a common lower-case string (exactly
as done for our example extraction (Section 3.1)).
Next, for each group we determine if a third-person
pronoun refers to any noun in that group. If so, we
label all nouns in the group with the gender of the
referring pronoun. For example, if the pronoun he
refers to a noun Brown, then all instances of Brown
in the document are labeled as masculine. We ex-
tract the genders for 2794 nouns in the ANC train-
ing set (in 798 noun groups) and 2596 nouns in the
ANC test set (in 642 groups). We apply this method
to other annotated corpora (including MUC corpora)
to create a development set.
The gold standard ANC training set is used to
set the weights on the counts in the PATHGENDER
classifiers, and to train the semi-supervised ap-
proaches. We also use an SVM to learn these
weights. We use the development set to tune the
SVM?s regularization parameter, both for systems
trained on automatically-generated data, and for sys-
tems trained on gold-standard data. We also opti-
mize each automatically-trained system on the de-
velopment set when we include this system?s pre-
dictions as features in the semi-supervised exten-
sion. We evaluate and state performance for all ap-
proaches on the final unseen ANC test set.
4.2 Evaluation
The primary purpose of our experiments is to de-
termine if we can improve on the existing state-of-
the-art in gender classification (path-based gender
6Available at http://www.cs.ualberta.ca/?bergsma/CorefTags/
124
counts). We test systems both trained purely on
automatically-labeled data (Section 3.3), and those
that leverage some gold-standard annotations in a
semi-supervised setting (Section 3.4). Another pur-
pose of our experiments is to investigate the relative
value of our context-based features and type-based
features. We accomplish these objectives by imple-
menting and evaluating the following systems:
1. PATHGENDER:
A classifier with the four path-based gender
counts as features (Section 2).
2. PATHGENDER+:
A method of back-off to help classify unseen
nouns: For multi-token nouns (like Bob John-
son), we also include the four gender counts
aggregated over all nouns sharing the first to-
ken (Bob .*), and the four gender counts over
all nouns sharing the last token (.* Johnson).
3. CONTEXT-AUTO:
Auto-trained system using only context fea-
tures (Section 3.2.1).
4. TYPE-AUTO:
Auto-trained system using only type features
(Section 3.2.2).
5. FULL-AUTO:
Auto-trained system using all features.
6. CONTEXT-SEMI:
Semi-sup. combination of the PATHGENDER+
features and the CONTEXT-AUTO predictions.
7. TYPE-SEMI:
Semi-sup. combination of the PATHGENDER+
features and the TYPE-AUTO predictions.
8. FULL-SEMI:
Semi-sup. combination of the PATHGENDER+
features and the FULL-AUTO predictions.
We evaluate using accuracy: the percentage of
labeled nouns that are correctly assigned a gender
class. As a baseline, note that always choosing
neutral achieves 38.1% accuracy on our test data.
5 Results and Discussion
5.1 Main results
Table 1 provides our experimental results. The orig-
inal gender counts already do an excellent job clas-
sifying the nouns; PATHGENDER achieves 91.0%
accuracy by looking for exact noun matches. Our
1. PATHGENDER 91.0
2. PATHGENDER+ 92.1
3. CONTEXT-AUTO 79.1
4. TYPE-AUTO 89.1
5. FULL-AUTO 92.6
6. CONTEXT-SEMI 92.4
7. TYPE-SEMI 91.3
8. FULL-SEMI 95.5
Table 1: Noun gender classification accuracy (%)
simple method of using back-off counts for the first
and last token, PATHGENDER+, achieves 92.1%.
While PATHGENDER+ uses gold standard data to
determine optimum weights on the twelve counts,
FULL-AUTO achieves 92.6% accuracy using no
gold standard training data. This confirms that our
algorithm, using no manually-labeled training data,
can produce a competitive gender classifier.
Both PATHGENDER and PATHGENDER+ do
poorly on the noun types that have low counts in
the gender database, achieving only 63% and 66%
on nouns with less than ten counts. On these
same nouns, FULL-AUTO achieves 88% perfor-
mance, demonstrating the robustness of the learned
classifier on the most difficult examples for previ-
ous approaches (FULL-SEMI achieves 94% on these
nouns).
If we break down the contribution of the two fea-
ture types in FULL-AUTO, we find that we achieve
89.1% accuracy by only using type features, while
we achieve 79.1% with only context features. While
not as high as the type-based accuracy, it is impres-
sive that almost four out of five nouns can be classi-
fied correctly based purely on the document context,
using no information about the noun itself. This is
information that has not previously been systemati-
cally exploited in gender classification models.
We examine the relationship between training
data size and accuracy by plotting a (logarithmic-
scale) learning curve for FULL-AUTO (Figure 3).
Although using four million noun groups originally
seemed sufficient, performance appears to still be in-
creasing. Since more training data can be generated
automatically, it appears we have not yet reached the
full power of the FULL-AUTO system. Of course,
even with orders of magnitude more data, the system
125
 70
 75
 80
 85
 90
 95
 100
 1000  10000  100000  1e+06  1e+07
Ac
cu
ra
cy
 (%
)
Number of training examples
Figure 3: Noun gender classification learning curve for
FULL-AUTO
does not appear destined to reach the performance
obtained through other means described below.
We achieve even higher accuracy when the output
of the -AUTO systems are combined with the orig-
inal gender counts (the semi-supervised extension).
The relative value of the context and type-based fea-
tures is now reversed: using only context-based fea-
tures (CONTEXT-SEMI) achieves 92.4%, while us-
ing only type-based features (TYPE-SEMI) achieves
91.3%. This is because much of the type informa-
tion is already implicit in the PATHGENDER counts.
The TYPE-AUTO predictions contribute little infor-
mation, only fragmenting the data and leading to
over-training and lower accuracy. On the other hand,
the CONTEXT-AUTO predictions improve accuracy,
as these scores provide orthogonal and hence helpful
information for the semi-supervised classifier.
Combining FULL-AUTO with our enhanced path
gender counts, PATHGENDER+, results in the over-
all best performance, 95.5% for FULL-SEMI, signif-
icantly better than PATHGENDER+ alone.7 This is
a 50% error reduction over the PATHGENDER sys-
tem, strongly confirming the benefit of our semi-
supervised approach.
To illustrate the importance of the unlabeled data,
we created a system that uses all features, including
the PATHGENDER+ counts, and trained this system
using only the gold standard training data. This sys-
tem was unable to leverage the extra features to im-
prove performance; its accuracy was 92.0%, roughly
equal to PATHGENDER+ alone. While SVMs work
7We evaluate significance using McNemar?s test, p<0.01.
Since McNemar?s test assumes independent classifications, we
apply the test to the classification of noun groups, not instances.
well with high-dimensional data, they simply cannot
exploit features that do not occur in the training set.
5.2 Further improvements
We can improve performance further by doing some
simple coreference before assigning gender. Cur-
rently, we only group nouns with the same string,
and then decide gender collectively for the group.
There are a few cases, however, where an ambiguous
surname, such as Willey, can only be classified cor-
rectly if we link the surname to an earlier instance of
the full name, e.g. Katherine Willey. We thus added
the following simple post-processing rule: If a noun
is classified as masculine or feminine (like the am-
biguous Willey), and it was observed earlier as the
last part of a larger noun, then re-assign the gender
to masculine or feminine if one of these is the most
common path-gender count for the larger noun. We
back off to counts for the first name (e.g. Kathleen
.*) if the full name is unobserved.
This enhancement improved the PATHGENDER
and PATHGENDER+ systems to 93.3% and 94.3%,
respectively, while raising the accuracy of our
FULL-SEMI system to 96.7%. This demonstrates
that the surname-matching post-processor is a sim-
ple but worthwhile extension to a gender predictor.8
The remaining errors represent a number of chal-
lenging cases: United States, group, and public la-
beled as plural but classified as neutral ; spectator
classified as neutral , etc. Some of these may yield
to more sophisticated joint classification of corefer-
ence and gender, perhaps along the lines of work in
named-entity classification (Bunescu and Mooney,
2004) or anaphoricity (Denis and Baldridge, 2007).
While gender has been shown to be the key fea-
ture for statistical pronoun resolution (Ge et al,
1998; Bergsma and Lin, 2006), it remains to be
seen whether the exceptional accuracy obtained here
will translate into improvements in resolution per-
formance. However, given the clear utility of gender
in coreference, substantial error reductions in gender
8One might wonder, why not provide special features so that
the system can learn how to handle ambiguous nouns that oc-
curred as sub-phrases in earlier names? The nature of our train-
ing data precludes this approach. We only include unambiguous
examples as pseudo-seeds in the learning process. Without
providing ambiguous (but labeled) surnames in some way, the
learner will not take advantage of features to help classify them.
126
assignment will likely be a helpful contribution.
6 Related Work
Most coreference and pronoun resolution papers
mention that they use gender information, but few
explain how it is acquired. Kennedy and Boguraev
(1996) use gender information produced by their en-
hanced part-of-speech tagger. Gender mistakes ac-
count for 35% of their system?s errors. Gender is
less crucial in some genres, like computer manuals;
most nouns are either neutral or plural and gender
can be determined accurately based solely on mor-
phological information (Lappin and Leass, 1994).
A number of researchers (Evans and Ora?san,
2000; Soon et al, 2001; Harabagiu et al, 2001) use
WordNet classes to infer gender knowledge. Unfor-
tunately, manually-constructed databases like Word-
Net suffer from both low coverage and rare senses.
Pantel and Ravichandran (2004) note that the nouns
computer and company both have a WordNet sense
that is a hyponym of person, falsely indicating these
nouns would be compatible with pronouns like he
or she. In addition to using WordNet classes, Soon
et al (2001) assign gender if the noun has a gen-
dered designator (like Mr. or Mrs.) or if the first
token is present on a list of common human first
names. Note that we incorporate such contextual
and categorical information (among many other in-
formation sources) automatically in our discrimina-
tive classifier, while they manually specify a few
high-precision rules for particular gender cues.
Ge et al (1998) pioneered the statistical approach
to gender determination. Like others, they consider
gender and number separately, only learning statis-
tical gender for the masculine, feminine, and neu-
tral classes. While gender and number can be han-
dled together for pronoun resolution, it might be use-
ful to learn them separately for other applications.
Other statistical approaches to English noun gender
are discussed in Section 2.
In languages with ?grammatical? gender and plen-
tiful gold standard data, gender can be tagged along
with other word properties using standard super-
vised tagging techniques (Hajic? and Hladka?, 1997).
While our approach is the first to exploit a dual
or orthogonal representation of English noun gen-
der, a bootstrapping approach has been applied to
determining grammatical gender in other languages
by Cucerzan and Yarowsky (2003). In their work,
the two orthogonal views are: 1) the context of the
noun, and 2) the noun?s morphological properties.
Bootstrapping with these views is possible in other
languages where context is highly predictive of gen-
der class, since contextual words like adjectives and
determiners inflect to agree with the grammatical
noun gender. We initially attempted a similar system
for English noun gender but found context alone to
be insufficiently predictive.
Bootstrapping is also used in general information
extraction. Brin (1998) shows how to alternate be-
tween extracting instances of a class and inducing
new instance-extracting patterns. Collins and Singer
(1999) and Cucerzan and Yarowsky (1999) apply
bootstrapping to the related task of named-entity
recognition. Our approach was directly influenced
by the hypernym-extractor of Snow et al (2005) and
we provided an analogous summary in Section 1.
While their approach uses WordNet to label hyper-
nyms in raw text, our initial labels are generated au-
tomatically. Etzioni et al (2005) also require no la-
beled data or hand-labeled seeds for their named-
entity extractor, but by comparison their classifier
only uses a very small number of both features and
automatically-generated training examples.
7 Conclusion
We have shown how noun-pronoun co-occurrence
counts can be used to automatically annotate the
gender of millions of nouns in unlabeled text. Train-
ing from these examples produced a classifier that
clearly exceeds the state-of-the-art in gender classi-
fication. We incorporated thousands of useful but
previously unexplored indicators of noun gender as
features in our classifier. By combining the pre-
dictions of this classifier with the original gender
counts, we were able to produce a gender predic-
tor that achieves 95.5% classification accuracy on
2596 test nouns, a 50% reduction in error over the
current state-of-the-art. A further name-matching
post-processor reduced error even further, resulting
in 96.7% accuracy on the test data. Our final system
is the broadest and most accurate gender model yet
created, and should be of value to many pronoun and
coreference resolution systems.
127
References
Shane Bergsma and Dekang Lin. 2006. Bootstrap-
ping path-based pronoun resolution. In COLING-
ACL, pages 33?40.
Shane Bergsma. 2005. Automatic acquisition of gen-
der information for anaphora resolution. In Canadian
Conference on Artificial Intelligence, pages 342?353.
Avrim Blum and Tom Mitchell. 1998. Combining la-
beled and unlabeled data with co-training. In COLT,
pages 92?100.
Sergey Brin. 1998. Extracting patterns and relations
from the world wide web. In WebDB Workshop at
6th International Conference on Extending Database
Technology, pages 172?183.
Razvan Bunescu and Raymond J. Mooney. 2004. Col-
lective information extraction with relational Markov
networks. In ACL, pages 438?445.
Eugene Charniak and Micha Elsner. 2009. EM works for
pronoun anaphora resolution. In EACL.
Colin Cherry and Shane Bergsma. 2005. An expecta-
tion maximization approach to pronoun resolution. In
CoNLL, pages 88?95.
Michael Collins and Yoram Singer. 1999. Unsupervised
models for named entity classification. In EMNLP-
VLC, pages 100?110.
Koby Crammer and Yoram Singer. 2001. On the al-
gorithmic implementation of multiclass kernel-based
vector machines. Journal of Machine Learning Re-
search, 2:265?292.
Silviu Cucerzan and David Yarowsky. 1999. Language
independent named entity recognition combining mor-
phological and contextual evidence. In EMNLP-VLC,
pages 90?99.
Silviu Cucerzan and David Yarowsky. 2003. Mini-
mally supervised induction of grammatical gender. In
NAACL, pages 40?47.
Hal Daume? III and Daniel Marcu. 2006. Domain adap-
tation for statistical classifiers. Journal of Artificial
Intelligence Research, 26:101?126.
Pascal Denis and Jason Baldridge. 2007. Joint determi-
nation of anaphoricity and coreference using integer
programming. In NAACL-HLT, pages 236?243.
Oren Etzioni, Michael Cafarella, Doug Downey, Ana-
Maria Popescu, Tal Shaked, Stephen Soderland,
Daniel S. Weld, and Alexander Yates. 2005. Unsu-
pervised named-entity extraction from the web: an ex-
perimental study. Artif. Intell., 165(1):91?134.
Richard Evans and Constantin Ora?san. 2000. Improving
anaphora resolution by identifying animate entities in
texts. In DAARC, pages 154?162.
Michael Fleischman, Eduard Hovy, and Abdessamad
Echihabi. 2003. Offline strategies for online question
answering: answering questions before they are asked.
In ACL, pages 1?7.
Niyu Ge, John Hale, and Eugene Charniak. 1998. A sta-
tistical approach to anaphora resolution. In Proceed-
ings of the Sixth Workshop on Very Large Corpora,
pages 161?171.
Andrew R. Golding and Dan Roth. 1999. A Winnow-
based approach to context-sensitive spelling correc-
tion. Machine Learning, 34(1-3):107?130.
Jan Hajic? and Barbora Hladka?. 1997. Probabilistic and
rule-based tagger of an inflective language: a compar-
ison. In ANLP, pages 111?118.
Sanda Harabagiu, Razvan Bunescu, and Steven Maio-
rano. 2001. Text and knowledge mining for coref-
erence resolution. In NAACL, pages 55?62.
Nancy Ide and Keith Suderman. 2004. The American
National Corpus first release. In LREC, pages 1681?
84.
Christopher Kennedy and Branimir Boguraev. 1996.
Anaphora for everyone: Pronominal anaphora resolu-
tion without a parser. In COLING, pages 113?118.
Shalom Lappin and Herbert J. Leass. 1994. An algo-
rithm for pronominal anaphora resolution. Computa-
tional Linguistics, 20(4):535?561.
Dekang Lin. 1998. Dependency-based evaluation of
MINIPAR. In LREC Workshop on the Evaluation of
Parsing Systems.
Patrick Pantel and Deepak Ravichandran. 2004. Auto-
matically labeling semantic classes. In HLT-NAACL,
pages 321?328.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2005.
Learning syntactic patterns for automatic hypernym
discovery. In NIPS, pages 1297?1304.
Wee Meng Soon, Hwee Tou Ng, and Daniel Chung Yong
Lim. 2001. A machine learning approach to corefer-
ence resolution of noun phrases. Computational Lin-
guistics, 27(4):521?544.
Ioannis Tsochantaridis, Thomas Hofmann, Thorsten
Joachims, and Yasemin Altun. 2004. Support vec-
tor machine learning for interdependent and structured
output spaces. In ICML.
Ellen Vorhees. 2002. Overview of the TREC 2002 ques-
tion answering track. In Proceedings of the Eleventh
Text REtrieval Conference (TREC).
David Yarowsky. 1995. Unsupervised word sense disam-
biguation rivaling supervised methods. In ACL, pages
189?196.
128
Proceedings of NAACL-HLT 2013, pages 868?877,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Open Information Extraction with Tree Kernels
Ying Xu, Mi-Young Kim, Kevin Quinn, Randy Goebel and Denilson Barbosa
Department of Computing Science
University of Alberta
{yx2,miyoung2,kfjquinn,goebel,denilson}@cs.ualberta.ca
Abstract
Traditional relation extraction seeks to iden-
tify pre-specified semantic relations within
natural language text, while open Information
Extraction (Open IE) takes a more general ap-
proach, and looks for a variety of relations
without restriction to a fixed relation set. With
this generalization comes the question, what
is a relation? For example, should the more
general task be restricted to relations medi-
ated by verbs, nouns, or both? To help answer
this question, we propose two levels of sub-
tasks for Open IE. One task is to determine if
a sentence potentially contains a relation be-
tween two entities? The other task looks to
confirm explicit relation words for two enti-
ties. We propose multiple SVM models with
dependency tree kernels for both tasks. For
explicit relation extraction, our system can ex-
tract both noun and verb relations. Our results
on three datasets show that our system is su-
perior when compared to state-of-the-art sys-
tems like REVERB and OLLIE for both tasks.
For example, in some experiments our system
achieves 33% improvement on nominal rela-
tion extraction over OLLIE. In addition we
propose an unsupervised rule-based approach
which can serve as a strong baseline for Open
IE systems.
1 Introduction
Relation Extraction (RE) systems are designed to
discover various semantic relations (e.g. <Obama,
president, the United States>) from natural lan-
guage text. Traditional RE systems extract spe-
cific relations for prespecified name-entity types
(Bunescu and Mooney, 2005; Chan and Dan, 2011;
Zhou and Zhu, 2011). To train such systems, ev-
ery relation needs manually annotated training ex-
amples, which supports limited scope and is diffi-
cult to extend. For this reason, Banko et al (2007)
proposed Open Information Extraction (Open IE),
whose goal is to extract general relations for two en-
tities. The idea is to avoid the need for specific train-
ing examples, and to extract a diverse range of rela-
tions. This generalized form has received significant
attention, e.g., (Banko et al, 2007; Akbik, 2009; Wu
and Weld, 2010; Fader et al, 2011; Mausam et al,
2012).
Because Open IE is not guided by or not restricted
to a prespecified list of relations, the immediate chal-
lenge is determining about what counts as a relation?
Most recent Open IE systems have targeted verbal
relations (Banko et al, 2007; Mausam et al, 2012),
claiming that these are the majority. However, Chan
and Dan (2011) show that only 20% of relations in
the ACE programs Relation Detection and Charac-
terization (RDC) are verbal. Our manually extracted
relation triple set from the Penn Treebank shows that
there are more nominal relations than verbal ones,
3 to 2. This difference arises because of the ambi-
guity of what constitutes a relation in Open IE. It
is often difficult even for humans to agree on what
constitutes a relation, and which words in the sen-
tence establish a relation between a pair of entities.
For example, in the sentence ?Olivetti broke Cocom
rules? is there a relation between Olivetti and Co-
com? This ambiguity in the problem definition leads
to significant challenges and confusion when eval-
uating and comparing the performance of different
methods and systems. An example are the results
in Fader et al (2011) and Mausam et al (2012). In
Fader et al (2011), REVERB ?is reported? as su-
868
perior to WOEparse, a system proposed in Wu and
Weld (2010); while in Mausam et al (2012), it is
reported the opposite.
To better answer the question, what counts as a
relation? we propose two tasks for Open IE. The
first task seeks to determine whether there is a re-
lation between two entities (called ?Binary task?).
The other is to confirm whether the relation words
extracted for the two entities are appropriate (the
?Triple task?). The Binary task does not restrict re-
lation word forms, whether they are mediated by
nouns, verbs, prepositions, or even implicit rela-
tions. The Triple task requires an abstract repre-
sentation of relation word forms, which we develop
here. We assume that relation words are nouns or
verbs; in our data, these two types comprise 71% of
explicit relations.
We adapt an SVM dependency tree kernel model
(Moschitti, 2006) for both tasks. The input to our
tasks is a dependency parse, created by Stanford
Parser. Selecting relevant features from a parse tree
for semantic tasks is difficult. SVM tree kernels
avoid extracting explicit features from parse trees
by calculating the inner product of the two trees.
For the Binary task, our dependency path is the path
between two entities. For the Triple task, the path
is among entities and relation words (i.e. relation
triples). Tree kernels have been used in traditional
RE and have helped achieve state of the art perfor-
mance (Culotta and Sorensen, 2004; Bunescu and
Mooney, 2005; Wang, 2008; Nguyen et al, 2009;
Zhou and Zhu, 2011). But one challenge of using
tree kernels on Open IE is that the lexicon of re-
lations is much larger than those of traditional RE,
making it difficult to include the lexical information
as features. Here we proposed an unlexicalized tree
structure for Open IE. As far as we know, this is the
first time an SVM tree kernel has been applied in
Open IE. Experimental results on multiple datasets
show our system outperforms state-of-the-art sys-
tems REVERB and OLLIE. Typically an Open IE
system is tested on one dataset. However, because
the definition of relation is ambiguous, we believe
that is necessary to test with multiple datasets.
In addition to the supervised model, we also pro-
pose an unsupervised model which relies on several
heuristic rules. Results with this approach show that
this simple unsupervised model provides a robust
strong baseline for other approaches.
In summary, our main contributions are:
? Use SVM tree kernels for Open IE. Our sys-
tem is robust comparing with other Open IE
systems, achieving superior scores in two test
sets and comparative scores in another set.
? Extend beyond verbal relations, which are
prevalent in current systems. Analyze implicit
relation problem in Open IE, which is ignored
by other work.
? Propose an unsupervised model for Open IE,
which can be a strong baseline for other ap-
proaches.
The rest of this paper is organized as follows. Sec-
tion 2 provides the problem description and system
structure, before summarizing previous work in Sec-
tion 3. Section 4 defines our representation of rela-
tion word patterns crucial to our task two, and Sec-
tion 5 describes tree kernels for SVM. Section 6 de-
scribes the unsupervised model, and Section 7 ex-
plains our experiment design and results. Section 8
concludes with a summary, and anticipation of fu-
ture work.
2 Problem Definition and System
Structure
The common definition of the Open IE task is a
function from a sentence, s, to a set of triples,
{< E1, R,E2 >}, where E1 and E2 are entities
(noun phrases) and R is a textual fragment indicat-
ing a semantic relation between the two entities. Our
?Triple task? is within this definition. However it is
often difficult to determine which textual fragments
to extract. In addition, semantic relations can be im-
plicit, e.g., consider the located in relation in the sen-
tence fragment ?Washington, US.? To illustrate how
much information is lost when restricting the rela-
tion forms, we add another task (the ?Binary task?),
determining if there is a relation between the two en-
tities. It is a function from s, to a set of binary rela-
tions over entities, {< E1, E2 >}. This binary task
is designed to overcome the disadvantage of current
Open IE systems, which suffer because of restricting
the relation form, e.g., to only verbs, or only nouns.
The two tasks are independent to each other.
869
	

	

		

		
		Proceedings of the Multiword Expressions: From Theory to Applications (MWE 2010), pages 55?63,
Beijing, August 2010
Application of the Tightness Continuum Measure
to Chinese Information Retrieval
Ying Xu?, Randy Goebel?, Christoph Ringlstetter? and Grzegorz Kondrak?
?Department of Computing Science ?Center for Language and
University of Alberta Information Processing (CIS)
Ludwig Maximilians University
{yx2,goebel,kondrak}@cs.ualberta.ca kristof@cis.uni-muenchen.de
Abstract
Most word segmentation methods em-
ployed in Chinese Information Retrieval
systems are based on a static dictionary
or a model trained against a manually
segmented corpus. These general seg-
mentation approaches may not be opti-
mal because they disregard information
within semantic units. We propose a novel
method for improving word-based Chi-
nese IR, which performs segmentation ac-
cording to the tightness of phrases. In
order to evaluate the effectiveness of our
method, we employ a new test collection
of 203 queries, which include a broad dis-
tribution of phrases with different tight-
ness values. The results of our experi-
ments indicate that our method improves
IR performance as compared with a gen-
eral word segmentation approach. The ex-
periments also demonstrate the need for
the development of better evaluation cor-
pora.
1 Introduction
What distinguishes Chinese Information Retrieval
from information retrieval (IR) in other languages
is the challenge of segmenting the queries and the
documents, created by the lack of word delimiters.
In general, there are two categories of segmenters:
character-based methods and word-based meth-
ods. Despite the superior performance of bigram
segmenters (Nie et al, 2000; Huang et al, 2000;
Foo and Li, 2004), word-based approaches con-
tinue to be investigated because of their applica-
tion in sophisticated IR tasks such as cross lan-
guage IR, and within techniques such as query ex-
pansion (Nie et al, 2000; Peng et al, 2002a).
Most word-based segmenters in Chinese IR are
either rule-based models, which rely on a lexi-
con, or statistical-based models, which are trained
on manually segmented corpora (Zhang et al,
2003). However, the relationship between the ac-
curacy of Chinese word segmentation and the per-
formance of Chinese IR is non-monotonic. Peng
et al (2002b) reported that segmentation meth-
ods achieving segmentation accuracy higher than
90% according to a manual segmentation standard
yield no improvement in IR performance. They
further argued that IR often benefits from splitting
compound words that are annotated as single units
by manual segmentation.
The essence of the problem is that there is no
clear definition of word in Chinese. Experiments
have shown only about 75% agreement among na-
tive speakers regarding the correct word segmen-
tation (Sproat et al, 1996). While units such as
??	? (peanut) and ???|? (match maker)
should clearly be considered as a single term in
Chinese IR, compounds such as ?????? (ma-
chine learning) are more controversial.1
Xu et al (2009) proposed a ?continuum hy-
pothesis? that rejects a clean binary classifica-
tion of Chinese semantic units as either compo-
sitional or non-compositional. Instead, they intro-
duced the notion of a tightness measure, which
quantifies the degree of compositionality. On
this tightness continuum, at one extreme are non-
1This issue is also present to a certain degree in languages
that do use explicit delimiters, including English (Halpern,
2000; McCarthy et al, 2003; Guenthner and Blanco, 2004).
55
compositional semantic units, such as ???
|? (match maker), and at the other end are se-
quences of consecutive words with no depen-
dency relationship, such as ??0??? (Shang-
hai where). In the middle of the spectrum are
compositional compounds such as ??????
(machine learning) and phrases such as ?thB
?? (legitimate income).
In this paper, we propose a method to ap-
ply the concept of semantic tightness to Chinese
IR, which refines the output of a general Chi-
nese word segmenter using tightness information.
In the first phase, we re-combine multiple units
that are considered semantically tight into single
terms. In the second phase, we break single units
that are not sufficiently tight. The experiments in-
volving two different IR systems demonstrate that
the newmethod improves IR performance as com-
pared to the general segmenter.
Most Chinese IR systems are evaluated on the
data from the TREC 5 and TREC 6 competi-
tions (Huang et al, 2000; Huang et al, 2003;
Nie et al, 2000; Peng et al, 2002a; Peng et al,
2002b; Shi and Nie, 2009). That data contains
only 54 queries, which are linked to relevancy-
judged documents. During our experiments, we
found the TREC query data is ill-suited for ana-
lyzing the effects of compound segmentation on
Chinese IR. For this reason, we created an addi-
tional set of queries based on the TREC corpus,
which includes a wide variety of semantic com-
pounds.
This paper is organized as follows. After sum-
marizing related work on Chinese IR and word
segmentation studies, we introduce the measure
of semantic tightness. Section 4 describes the in-
tegration of the semantic tightness measure into
an IR system. Section 5 discusses the available
data for Chinese IR evaluation, as well as an ap-
proach to acquire new data. Section 6 presents the
results of our method on word segmentation and
IR. A short conclusion wraps up and gives direc-
tions for future work.
2 Related Work
The impact of different Chinese word segmen-
tation methods on IR has received extensive at-
tention in the literature (Nie et al, 2000; Peng
et al, 2002a; Peng et al, 2002b; Huang et al,
2000; Huang et al, 2003; Liu et al, 2008; Shi
and Nie, 2009). For example, Foo and Li (2004)
tested the effects of manual segmentation and var-
ious character-based segmentations. In contrast
with most related work that only reports the over-
all performance, they provide an in-depth analysis
of query results. They note that a small test col-
lection diminishes the significance of the results.
In a series of papers on Chinese IR, Peng
and Huang compared various segmentation meth-
ods in IR, and proposed a new segmentation
method (Peng et al, 2002a; Peng et al, 2002b;
Huang et al, 2000; Huang et al, 2003). Their
experiments suggest that the relationship between
segmentation accuracy and retrieval performance
is non-monotonic, ranging from 44%-95%. They
hypothesize that weak word segmenters are able
to improve the accuracy of Chinese IR by break-
ing compound words into smaller constituents.
Shi and Nie (2009) proposed a probability-
based IR score function that combines a unigram
score with a word score according to ?phrase in-
separability.? Candidates for words in the query
are selected by a standard segmentation program.
Their results show a small improvement in com-
parison with a static combination of unigram and
word methods.
Liu et al (2008) is the research most similar
to our proposed method. They point out that cur-
rent segmentation methods which treat segmenta-
tion as a classification problem are not suitable
for Chinese IR. They propose a ranking support
vector machine (SVM) model to predict the inter-
nal association strength (IAS) between characters,
which is similar to our concept of tightness. How-
ever, they do not analyze their segmentation ac-
curacy with respect to a standard corpus, such as
Chinese Treebank. Their method does not reliably
segment function words, mistakenly identifying
?{|? (?s people) as tight, for example. Unlike
their approach, our segmentation method tackles
the problem by combining the tightness measure
with a general segmentation method.
Chinese word segmentation is closely related
to multiword expression extraction. McCarthy et
al. (2003) investigate various statistical measures
of compositionality of candidate multiword verbs.
56
Silva et al (1999) propose a new compositional-
ity measure based on statistical information. The
main difference with Xu et al?s measure is that
the latter is focused on word sense disambigua-
tion. In terms of multiword expressions in IR,
Vechtomova (2001) propose several approaches,
such as query expansion, to incorporating English
multiword expressions in IR. Braschler and Rip-
plinger (2004) analyze the effect of stemming and
decompounding on German text retrieval. How-
ever, Chinese compound segmentation in IR is a
thorny issue and needs more investigation for the
reasons mentioned earlier.
3 Semantic Tightness Continuum
We adopt the method developed by (Xu et al,
2009) for Chinese semantic unit tightness mea-
sure, which was shown to outperform the point-
wise mutual information method. For the sake
of completeness we briefly describe the basic ap-
proach here. The input of the measure is the prob-
ability distribution of a unit?s segmentation pat-
terns, i.e., potential segmentation candidates. The
output is a tightness value; the greater the value,
the tighter the unit. In this paper, we focus on 4-
gram sequences because 4-character compounds
are the most prominent in Chinese. There are
eight possible segmentations of any 4-character
sequence: ?ABCD,? ?A|BCD,? ?A|B|CD,? etc.
For a sequence of n characters, there are 2n?1 po-
tential segmentations. Equation 1 below defines
the tightness measure.
ratio =
?
??
??
?P t(s)
max(?P t(s1|s2))+ 1N
if ?P t(s) > ?
undef otherwise
(1)
In Equation 1, ?P t(s) stands for frequencies of
segmentation patterns of a potential semantic unit
s; Pt(s1|s2) is a pattern which segments the unit
s into two parts: s1 and s2; ? is a threshold to
exclude rare patterns; and N is a smoothing factor
which is set as the number of documents. Note
that when the first part of the denominator is zero,
the ratio of the unit will be very high. Intuitively,
the lack of certain separating patterns in the data
is evidence for the tightness of the units.
4 Application to Chinese IR
We propose a novel approach to segmentation
for Chinese IR which is based on the tight-
ness measure. Our segmenter revises the out-
put of a general segmenter according to the tight-
ness of units. The intuition behind our method
is that segmentation based on tightness of units
will lead to better IR performance. For exam-
ple, keeping ??CF? (Pinatubo) as a unit
should lead to better results than segmenting it
into ??(skin)|(include)|C(picture)|F(large)?.
On the other hand, segmenting the compositional
phrase ?)?)? (Kuwait country) into ?)?
(Kuwait)|)(country)? can improve recall. We
revise an initial segmentation in two steps: first,
we combine components that should not have
been separated, such as ??CF? (Pinatubo);
second, we split units which are compositional,
such as ?)?)? (Kuwait country).
In order to combine components, we first
extract 4-gram non-compositional compounds
whose tightness values are greater than a thresh-
old ?1 in a reference corpus, and then revise a
general segmenter by combining two separated
words if their combination is in the list. This ap-
proach is similar to the popular longest match first
method (LMF), but with segmentation chunks in-
stead of characters, and with the compound list
serving as the lexicon. For example, consider
a sequence ?ABCDEFGHIGK,? which a general
segmenter annotates as ?ABC|D|E|F|G|HI|GK.?
If our compound list constructed according to the
tightness measure contains {?DEFG?}, the re-
vised segmentation will be ?ABC|DEFG|HI|GK.?
Units of length less than 4 are segmented by using
the LMF rule against a dictionary.
In order to split a compositional unit, we set the
additional thresholds ?2, ?3, and ?4, and employ
the segmentation rules in Equation 2. The intu-
ition comes from the pattern lattice of a unit (Fig-
ure 1). For the patterns on the same level, the most
frequent pattern suggests the most reasonable seg-
mentation. For the patterns on different levels, the
frequency of each level indicates the tightness of
the unit.
57
Figure 1. The Lattice of the 8 Patterns.
if
v1 = ?Pt(ABCD)max(?Pt(A|BCD),?Pt(AB|CD),?Pt(ABC|D))+ 1N
> ?2
then ?ABCD? is one unit;
else if
v2 =
max(?Pt(A|BCD),?Pt(AB|CD),?Pt(ABC|D))+ 1N
max(?Pt(A|B|CD),?Pt(A|BC|D),?Pt(AB|C|D))+ 1N
> ?3
then ?ABCD? is segmented into two parts;
else if
v3 =
max(?Pt(A|B|CD),?Pt(A|BC|D),?Pt(AB|C|D))+ 1N
?Pt(A|B|C|D)+ 1N
> ?4
then ?ABCD? is segmented into three parts;
else
?ABCD? is segmented into four parts;
(2)
We apply the rules in Equation 2 to the se-
quence of 4-grams, with simple voting for select-
ing the segmentation pattern. For example, within
the sequence ?ABCDEF,? three 4-gram patterns
are considered: ?ABCD,? ?BCDE,? and ?CDEF.?
If only one of the 4-grams contains a segmentation
delimiter, the insertion of the delimiter depends
only upon that 4-gram. If two 4-grams contain the
same delimiter, the insertion of the delimiter de-
pends upon the two 4-grams. If the two 4-grams
disagree on the segmentation, a confidence value
is calculated as in Equation 3,
confidence = vi ? ?i+1, (3)
where i ? [1, 2, 3]. If three 4-grams contain the
same delimiter, voting is employed to decide the
segmentation. Returning to our example, suppose
that the first 4-gram is segmented as ?A|B|C|D,?
the second as ?BC|DE,? and the third as ?C|DE|F.?
Then the segmentation delimiter between ?A? and
?B? is inserted, but the delimiter between ?B? and
?C? depends on the confidence values of the first
two segmentation patterns. Finally, the delimiter
between ?C? and ?D? depends on the result of vot-
ing among the three 4-gram segmentations.
The two steps of combining and splitting can
either be applied in succession or separately. In
the former case, ?1 must be greater or equal to ?2.
In the remainder of this paper, we refer to the first
step as ?Tight Combine,? and to the second step
applied after the first step as ?Tight Split.? Note
that the second method can be used to segment
sentences directly instead of revising the output of
a general segmenter. This method, which we refer
to as ?Online Tight,? has the same shortcoming
as the method of Liu et al (2008), namely it fre-
quently fails to segment function words. For ex-
ample, it erroneously identifies ?{|? (?s people)
as tight. Therefore, we do not attempt to embed it
into the IR systems discussed in Section 6.
5 Test Collection
We analyzed the currently available Chinese test
collection of TREC, and found it unsuitable for
evaluating different strategies of compound seg-
mentation. One problem with the TREC data is
that the Chinese queries (topic titles) have too
many keywords. According to the output of ICT-
CLAS, a general segmenter, the average length of
Chinese queries is 12.2 words; in contrast, the av-
erage length of English ad-hoc queries in TREC-
5 and 6 (English topics 251-350) is 4.7. Even if
we use English translation of the Chinese queries
instead, the average length is still more than 7
words. The problem with long queries is that
they introduce complicating effects that interact
in ways difficult to understand. An example is
the co-occurrence between different keywords in
the base corpus. Sometimes a completely correct
segmentation causes a decrease in IR performance
because the score function assigns a higher score
to less important terms in a topic. For example,
for query 47 (Trec-6 dataset), ?9F5??C
F????????S??? (Philippines,
Mount Pinatubo, volcanic ash, magma, eruption),
preserving the unit Pinatubo makes the average
precision drop from 0.76 to 0.62 as compared to
the segmentation ??||C|F?. The score of the
58
unit is lower than that the sum of its components,
which results in a relatively low ranking for some
relevant documents. Another problem with the
TREC Chinese test collection is the small number
of queries (54). The number of of queries contain-
ing non-compositional words is smaller still. Sim-
ilarly, the other available corpus, NTCIR, com-
prises only 50 queries. In order to be confident of
our results, we would like to have a more substan-
tial number of queries containing units of varying
tightness.
Because of the shortcomings of available data
sets, we created our own test collection. There are
three components that define an IR test collection:
a query set, a corpus from which relevant docu-
ments are retrieved, and relevance judgements for
each query. Our criteria for gathering these com-
ponents are as follows.
First, the set of queries should contain both
tight queries and loose queries. For example,
there should be tight queries such as ???|?
(match maker), loose queries such as ??00?
(Shanghai customs), and queries with tightness
values in between, such as ?????? (machine
learning). Furthermore, the queries should be re-
alistic, rather than constructed by introspection.
In order to meet these requirements we randomly
chose 4-gram noun phrases (tagged by ICTCLAS)
from the TREC corpus. 51 queries are from a real
data set, the Sogou query logs2. The remaining
152 queries, which are selected manually based
on the initial 51 queries, represent queries that IR
system users are likely to enter. For example,
queries of locations and organizations are more
likely than queries such as ?how are you.? Fi-
nally, the queries should not be too general (i.e.,
resulting in too many relevant documents found),
nor too specific (no relevant documents). There-
fore, we selected the 4-grams which had the cor-
responding document frequency in the TREC cor-
pus between 30 and 300.
The second set of criteria concerns the rele-
vance judgements of documents. As our retrieval
corpus, we adopted the TREC Mandarin corpus,
which contains 24,959 documents. Because of re-
source limitation, we used the Minimum Test Col-
2Sogou query logs 2007 can be downloaded at
http://www.sogou.com/labs/dl/q.html.
lection (MTC) method (Carterette et al, 2006).
The method pools documents in such a way that
the documents which are best for discriminating
between different IR systems are judged first. We
applied this method on a document set that con-
tains all of the top 100 results of 8 IR systems
(two score functions, tf*idf and BM25, 4 index-
ing methods, unigram, bigram, ICTCLAS seg-
mentation, and our Tight Combine segmentation).
The systems were implemented with the Lucene
framework (http://lucene.apache.org/).
The last criterion determines which document
is relevant to a query. Annotators? opinions vary
about whether a document is relevant to a topic.
Is having the query in a document enough to be
the criterion of relevance? For the query ?Bei-
jing airport,? should the document that contains
the sentence ?Chairman Mao arrived at the Bei-
jing airport yesterday,? be classified as relevant?
Since our goal is to analyze the relationship be-
tween Chinese word segmentation, and IR, we
use weak relevant judgements. It is more related
to score functions to distinguish weak relevance
from strong relevance, that is, whether the query
is the topic of the document. This means the above
document is judged as relevant for the query ?Bei-
jing airport.?
In summary, our own test collection has about
200 queries, and at least 100 judged documents
per query with the TREC corpus as our base cor-
pus3.
6 Experiments
We conducted a series of experiments in word-
based Chinese information retrieval, with the aim
of establishing which segmenter is best for CIR,
while pursuing the best segmentation performance
in terms of segmented corpus is not the main crux.
In this section, we first present the accuracy of dif-
ferent segmentation methods, and then discuss the
results of IR systems.
6.1 Chinese Word Segmentation
ICTCLAS is a Chinese segmentation tool built by
the Institute of Computing Technology, Chinese
Academy of Sciences. Its segmentation model is a
3The query set and relevance judgements are available at
http://www.cs.ualberta.ca/?yx2/research.html
59
class-based hidden Markov model (HMM) model
(Zhang et al, 2003). The segmenter is trained
from manually segmented corpus, which makes
it ignore both the tightness of units and unknown
words such as ??CF? (Pinatubo), which are
difficult to identify.
In this experiment, we segmented the Chinese
Treebank using ICTCLAS and our three methods
that employ the tightness measure. The evalua-
tion is based on the manual segmentation of the
corpus. We evaluated the methods on the entire
Treebank corpus, employing 10-cross validation
for result significance verification.
In order to measure the tightness of Chinese
semantic units, pattern distributions of every 4-
gram were extracted from the Chinese Gigaword
corpus. Tight Combine is the ICTCLAS refined
segmentation that employs the non-compositional
compound list from the Chinese Gigaword cor-
pus. The threshold for non-compositional com-
pound ?1 is set to 11. Tight Split is the refined
segmentation of Tight Combine using Equation 2.
Online Tight is the segmentation using Equation
2 directly. For Tight Split and Online Tight, we
employed a lexicon which contains 41,245 words,
and set the thresholds ?2, ?3, and ?4 to 11, 0.01,
and 0.01, respectively. The parameters ?1 and ?2
are set according to the observation that the per-
centage of non-compositional units is high when
the tightness is greater than 11 for all the 4-grams
in the Chinese Gigaword corpus. The other two
parameters were established after experimenting
with several parameter pairs, such as (1,1), (0.1,
0.1), and (0.1, 0.01). We chose the one with the
best segmentation accuracy according to the stan-
dard corpus.
Table 1 shows the mean accuracy result over the
10 folders. The accuracy is the ratio of the number
of correctly segmented intervals to the number of
all intervals. The result shows that our method
improves over the ICTCLAS segmentation result,
but the improvement is not statistically significant
(measured by t-test). The only significant result is
that Online tight is worse than other methods.
Surprisingly, there is a large gap between
Tight Split and Online Tight, although they em-
ploy the same parameters. It turns out the ma-
jor difference lies in the segmentation of function
ICTCLAS 88.8%
Tight Combine 89.0%
Tight Split 89.1%
Online Tight 80.5%
Table 1. Segmentation accuracy of different seg-
menters.
words. Since it is based on ICTCLAS, Tight Split
does a good job in segmenting function words
such as verbal particles which represent past tense
??? and the nominalizer ?{.? Online Tight
tends to combine these words with the consecu-
tive one. For example, considering ????? (cu-
mulated), the Treebank and Tight Split segment it
into ???|?? (cumulate + particle); while On-
line Tight leaves it unsegmented.
6.2 IR Experiment Setup
We conducted our information retrieval experi-
ments using the Lucene package (Hatcher and
Gospodnetic, 2004). The documents and queries
were segmented by our three approaches before
indexing and searching process. In order to ana-
lyze the performance of our segmentation meth-
ods with different retrieval systems, we employed
two score functions: the BM25 function (Peng et
al., 2002b) 4; and BM25Beta (Function 4), which
prefers documents with more query terms.
Score(Q,D) ={
T
(1+?)?N
?T
i=0 score(ti, D) if T < N?N
i=0 score(ti, D) if T = N
(4)
In the above equation, score(ti, D) is the score
of the term ti in the document D. Although
we used BM25 as our base score function for
score(ti, D), it can be replaced by other score
functions, such as tf*idf, or a probability language
model. ? is a parameter to control a penalty com-
ponent for those documents that do not contain
all the query terms; T is the number of distinc-
tive query terms in the document; and N is the
number of query terms. The function penalizes
documents that do not contain all the query terms,
4An implementation of BM25 into Lucene can be down-
loaded at http://arxiv.org/abs/0911.5046
60
BM25 BM25Beta
ICTCLAS 62.78% 70.79%
Tight Combine 65.92% 71.19%
Tight Split 63.40% 70.95%
Table 2. MAP of different IR systems with differ-
ent segmenters.
which is an indirect way of incorporating proxim-
ity distance 5.
6.3 IR Experiment Results
Table 2 shows the comparison of our two seg-
menters to ICTCLAS on the IR task. The per-
formance of IR systems was measured by mean
average precision (MAP) of the query set. The re-
sults show that Tight Combine is better than the
ICTCLAS segmentation, especially when using
BM25. The relationship between Tight Split and
ICTCLAS is not clear.
In order to give a more in-depth analysis of
the word segmentation methods with respect to
the targeted phenomenon of semantic units, we
classified the 200 queries into three categories ac-
cording to their tightness as measured by func-
tion 1. The three classes are queries with tight-
ness in ranges [+?, 10), [10, 1), and [1, 0),
which contain 54, 41, and 108 queries respec-
tively. Queries in the range [+?, 10) are tight
queries, such as ?v?