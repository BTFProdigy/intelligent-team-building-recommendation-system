Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pp. 1139?1143,
Prague, June 2007. c?2007 Association for Computational Linguistics
Online learning for Deterministic Dependency Parsing
Prashanth Reddy Mannem
Language Technologies Research Center
IIIT-Hyderabad, India
prashanth@research.iiit.ac.in
Abstract
Deterministic parsing has emerged as an ef-
fective alternative for complex parsing algo-
rithms which search the entire search space
to get the best probable parse tree. In this pa-
per, we present an online large margin based
training framework for deterministic pars-
ing using Nivre?s Shift-Reduce parsing al-
gorithm. Online training facilitates the use
of high dimensional features without cre-
ating memory bottlenecks unlike the popu-
lar SVMs. We participated in the CoNLL
Shared Task-2007 and evaluated our system
for ten languages. We got an average multi-
lingual labeled attachment score of 74.54 %
(with 65.50% being the average and 80.32%
the highest) and an average multilingual un-
labeled attachment score of 80.30% (with
71.13% being the average and 86.55% the
highest).
1 Introduction
CoNLL-X had a shared task on multilingual depen-
dency parsing (Buchholz et al, 2006) by providing
treebanks for 13 languages in the same dependency
format. A look at the performance sheet in the con-
test shows that two systems with quite different ap-
proaches (one using deterministic parsing with SVM
and the other using MIRA with nondeterministic and
dynamic programming based MST approach ) per-
formed with good results (McDonald et al, 2006;
Nivre et al, 2006).
More recently, deterministic parsing has gener-
ated a lot of interest because of their simplicity
(Nivre, 2003). One of the main advantages of de-
terministic parsing lies in the ability to use the sub-
tree information in the features to decide the next
step. Parsing algorithms which search the entire
space (Eisner, 1996; McDonald, 2006) are restricted
in the features they use to score a relation. They rely
only on the context information and not the history
information to score a relation. Using history infor-
mation makes the search intractable. Whereas, since
deterministic parsers are at worst O(n2) (Yamada
and Matsumoto, 2003) (Nivre (2003) is only O(2n)
in the worst case), they can use the crucial history
information to make parsing decisions. So, in our
work Nivre?s parsing algorithm has been used to ar-
rive at the dependency parse tree.
Popular learning algorithms for deterministic
parsing like Support Vector Machines (SVM) run
into memory issues for large data since they are
batch learning algorithms. Though more informa-
tion is available in deterministic parsing in terms of
subtree information, high dimensional features can?t
be used due to the large training times for SVMs.
This is where online methods come into the picture.
Unlike batch algorithms, online algorithms con-
sider only one training instance at a time when
optimizing parameters. This restriction to single-
instance optimization might be seen as a weakness,
since the algorithm uses less information about the
objective function and constraints than batch algo-
rithms. However, McDonald (2006) argues that this
potential weakness is balanced by the simplicity of
online learning, which allows for more streamlined
training methods. This work focuses purely on on-
line learning for deterministic parsing.
1139
In the remaining part of the paper, we introduce
Nivre?s parsing algorithm, propose a framework for
online learning for deterministic parsing and present
the results for all the languages with various feature
models.
2 Parsing Algorithm
We used Nivre?s top-down/bottom-up linear time
parsing algorithm proposed in Nivre (2003). A
parser configuration is represented by triples
(S, I,E) where S is the stack (represented as a list),
I is the list of (remaining) tokens and E is the set of
edges for the dependency graph D. S is a list of par-
tially processed tokens, whose subtrees are incom-
plete i.e tokens whose parent or children have not
yet been established. top is the top of the stack S,
next is the next token in the list I .
Nivre?s algorithm consists of four elementary ac-
tions Shift, Left, Right and Reduce to build
the dependency tree from the initial configuration
(nil,W, ?), where W is the input sentence. Shift
pushes next onto the stack S. Reduce pops the
stack. Right adds an arc from top to next and
pushes next onto the stack S. Left adds an arc
from next to top and pops the stack. The parser ter-
minates when it reaches a configuration (S,nil,E)
( for any list S and set of edges E).
The labels for each relation are determined after
a new arc is formed (by left and right actions).
The parser always constructs a dependency graph
that is acyclic and projective. For non-projective
parsing, we followed the pseudo projective pars-
ing approach proposed by Nivre and Nilson (2005).
In this approach, the training data is projectivized
by a minimal transformation, lifting non-projective
arcs one step at a time, and extending the arc label
of the lifted arcs using the encoding scheme called
HEAD+PATH. The non-projective arcs can be re-
covered by applying an inverse transformation to the
output of the parser, using a left-to-right, top-down,
breadth-first search, guided by the extended arc la-
bels. This method has been used for all the lan-
guages.
3 Online Learning
McDonald (2005) applied online learning by scoring
edges in a connected graph and finding the Maxi-
mum Spanning Tree (MST) of the graph. McDonald
et al (2005) used Edge Based Factorization , where
the score of a dependency tree is factored as the sum
of scores of all edges in the tree. Let, x = x1 ? ? ? xn
represents a generic input sentence , and y represents
a generic dependency tree for sentence x. (i, j) ? y
denotes the presence of a dependency relation in y
from word xi (parent) to word xj (child).
In Nivre?s parsing algorithm the dependency
graph can be viewed as a graph resulting from a
set of parsing decisions (in this case Shift, Reduce
, Left & Right) made, starting with the initial con-
figuration (nil,W, ?) . We define this sequence of
parsing decisions as d = d1 ? ? ? dm. So, d is the se-
quence of parsing decisions made by the parser to
obtain a dependency tree y, from an input sentence
x. Lets also define c = c1 ? ? ? cm to be the config-
uration sequence starting from initial configuration
(nil,W, ?) to the final configuration (S,nil,E).
We define the score of a parsing decision for a par-
ticular configuration to be the dot product between a
high dimensional feature vector (based on both the
decision and the configuration) and a weight vector.
So,
s(di, ci) = w . f(di, ci)
where ci is the configuration at the ith instance
and di is any one of the four actions {Shift, Reduce,
Left, Right} .
The Margin Infused Relaxed Algorithm (MIRA)
proposed by Crammer et al (2003) attempts to keep
the norm of the change to the parameter vector as
small as possible, subject to correctly classifying the
instance under consideration with a margin at least
as large as the loss of the incorrect classifications.
McDonald et al (2005) defines the loss of a depen-
dency tree inferred by finding the Maximum Span-
ning Tree(MST), as the number of words that have
incorrect parent (i.e the no. of edges that have gone
wrong). This satisfies the global constraint that the
correct set of edges will have the highest weight.
However, in Nivre?s algorithm, as there is no one to
one correspondence between parsing decisions and
the graph edges, the number of errors in the edges
can?t be used as a loss function as it won?t reflect the
exact loss in the parsing decisions. In this method
of calculating the loss function based on edges, we
first get the series of decisions through inference on
1140
the training data, then concat their feature vectors
and finally run the normal updates with the edge
based loss (since the resulting decisions will produce
a parse tree). This method gave very poor results.
So we do a factored MIRA for Nivre?s algorithm
by factoring the output by decisions to obtain the
following constraints:
min?w(i+1) ? w(i)?
s.ts(di, ci) ? s(d
?
i, ci) ? 1
?ci ? dt(c) and
(di, d
?
i) ? (Shift, Left,Right,Reduce)
where di represents the correct decision and d
?
i
represents all the other decisions for the same con-
figuration ci. This states that the weight of the cor-
rect decision for a particular configuration and the
weight of all other decisions must be separated by a
margin of 1. For every sentence in the training data,
starting with the initial configuration (nil,W, ?),
weights are adjusted to satisfy the above constraints
before proceeding to the next correct configuration.
This process is repeated till we reach the final con-
figuration (S,nil,E).
4 Features
The two central elements in any configuration are
the token on the top of the stack (t) and the next input
token (n),the tokens which may be connected by a
dependency relation in the current configuration. We
categorize our features into basic, context, history
and in ? between feature sets. The basic feature
set contains information about these two tokens t
and n. This includes unigram, bigram combinations
of the word forms (FORM), root word (LEMMA),
features (FEATS) and the part-of-speech tags (both
CPOS and POS) of these words. The coarse POS tag
(CPOS) is useful and helps solve data sparseness to
some extent.
The existence or non-existence of a relation be-
tween two words is heavily dependent on the words
surrounding t and n which is the contextual infor-
mation. The context feature set has the information
about the surrounding words t?1, t+1, n?1, n+1,
n + 2, n + 3. Unigram and trigram combinations
(with t and n) of the lexical items, POS tags, CPOS
tags of these words are part of this context feature
set. We also included the second topmost element in
the stack (st? 1) word too.
The third feature set, which is the history feature
set contains the info about the subtree at a partic-
ular parser state. One of the advantages of using
deterministic parsing algorithm over nondeterminis-
tic algorithm is that history can be used as features.
History features have information about the Parent
(par), Left Sibling (ls) and Right Sibling (rs) of t.
Unigram and trigram combinations (with t and n) of
POS, CPOS, DEPREL tags of these words are in-
cluded in the History Features.
The features in the in ? between feature set
take the form of POS and CPOS trigrams: the
POS/CPOS of t, that of the word in between and
that of n.
All the features in these feature sets are conjoined
with distance between t & n and the parsing deci-
sion. We experimented with a combination of these
feature sets in our training. We define feature mod-
els ?1, ?2 and ?3 for our experiments. ?1 is a com-
bination on basic and context feature sets. ?2 is a
mixture of basic, context and in ? between fea-
ture sets whereas ?3 contains basic, context and
history feature sets. The feature models ?1?3 are
the same for all the languages.
5 Results and Discussion
The system with online learning and Nivre?s pars-
ing algorithm was trained on the data released by
CoNLL Shared Task Organizers for all the ten lan-
guages (Hajic? et al, 2004; Aduriz et al, 2003; Mart??
et al, 2007; Chen et al, 2003; Bo?hmova? et al, 2003;
Marcus et al, 1993; Johansson and Nugues, 2007;
Prokopidis et al, 2005; Csendes et al, 2005; Mon-
temagni et al, 2003; Oflazer et al, 2003). We evalu-
ated our system using the standard evaluation script
provided by the organizers (Nivre et al, 2007).
The evaluation metrics are Unlabeled Attachment
Score(UAS) and Labeled Attachment Score(LAS).
The results of our system with various feature
models are listed in Table 11. The history infor-
mation in ?3 contributed to a marginal improve-
ment in accuracy of Hungarian, Italian and Turkish.
Whereas, Arabic, Catalan, Czech, English, Greek
1Results aren?t available for the models with a ?-? mark.
1141
Language ?1 ?2 ?3
LAS UAS LAS UAS LAS UAS
Arabic 71.55 81.56 72.05 81.93 71.66 81.30
Basque 66.35 73.71 65.64 72.86 64.56 71.69
Catalan 84.45 89.65 84.47 89.81 - -
Chinese 74.06 79.09 73.76 78.84 72.93 77.52
Czech 70.49 77.05 70.68 77.20 - -
English 81.19 82.41 81.55 82.81 - -
Greek 71.52 78.77 71.69 78.89 71.46 78.48
Hungarian 70.42 75.01 70.94 75.39 71.05 75.54
Italian 78.30 82.54 78.67 82.91 79.18 83.38
Turkish 76.42 82.74 76.48 82.85 77.29 83.58
Table 1: Results of Online learning with Nivre?s parsing algorithm for feature models ?1, ?2, ?3
got their highest accuracies with feature model ?2
containing basic, context and in?between feature
sets. The rest of the languages, Basque and Chinese
achieved highest accuracies with ?1. But, a careful
look at the results table shows that there isn?t any
significant difference in the accuracies of the sys-
tem across different feature models. This is true for
all the languages. The feature models ?2 and ?3
did not show any significant difference in accuracies
even though they contain more information. Feature
model ?1 with basic and context feature sets has
achieved good accuracies.
5.1 K-Best Deterministic Parsing
The deterministic parsing algorithm does not han-
dle ambiguity. By choosing a single parser action at
each opportunity, the input string is parsed determin-
istically and a single dependency tree is built during
the parsing process from beginning to end (no other
trees are even considered). A simple extension to
this idea is to eliminate determinism by allowing the
parser to choose several actions at each opportunity,
creating different action sequences that lead to dif-
ferent parse trees. Since a score is assigned to every
parser action, the score of a parse tree can be com-
puted simply as the average of the scores of all ac-
tions that resulted in that parse tree (the derivation
of the tree). We performed a beam search by carry-
ing out a K-best search through the set of possible
sequences of actions as proposed by Johansson and
Nugues (2006). However, this did not increase the
accuracy. Moreover, with larger values of K, there
was a decrease in the parsing accuracy. The best-
first search proposed by Sagae and Lavie (2006) was
also tried out but there was similar drop in accuracy.
6 Conclusion
The evaluation shows that the labeled pseudo projec-
tive deterministic parsing with online learning gives
competitive parsing accuracy for most of the lan-
guages involved in the shared task. The level of ac-
curacy varies considerably between the languages.
Analyzing the results and the effects of various fea-
tures with online learning will be an important re-
search goal in the future.
References
A. Abeille?, editor. 2003. Treebanks: Building and Using
Parsed Corpora. Kluwer.
I. Aduriz, M. J. Aranzabe, J. M. Arriola, A. Atutxa,
A. Diaz de Ilarraza, A. Garmendia, and M. Oronoz.
2003. Construction of a Basque dependency treebank.
In Proc. of the 2nd Workshop on Treebanks and Lin-
guistic Theories (TLT), pages 201?204.
A. Bo?hmova?, J. Hajic?, E. Hajic?ova?, and B. Hladka?. 2003.
The PDT: a 3-level annotation scenario. In Abeille?
(Abeille?, 2003), chapter 7, pages 103?127.
S. Buchholz, E. Marsi, A. Dubey, and Y. Krymolowski.
2006. Conll-x shared task on multilingual dependency
parsing. In Proceedings of the Conference on Compu-
tational Natural Language Learning (CoNLL).
K. Chen, C. Luo, M. Chang, F. Chen, C. Chen, C. Huang,
and Z. Gao. 2003. Sinica treebank: Design criteria,
representational issues and implementation. In Abeille?
(Abeille?, 2003), chapter 13, pages 231?248.
1142
K. Crammer, O. Dekel, S. Shalev-Shwartz, and Y. Singer.
2003. Online passive aggressive algorithms. In Pro-
ceedings of Neural Information Processing Systems
(NIPS).
D. Csendes, J. Csirik, T. Gyimo?thy, and A. Kocsor. 2005.
The Szeged Treebank. Springer.
J. Eisner. 1996. Three new probabilistic models for de-
pendency parsing: An exploration. In Proceedings of
the 16th International Conference on Computational
Linguistics (COLING), pages 340?345.
J. Hajic?, O. Smrz?, P. Zema?nek, J. ?Snaidauf, and E. Bes?ka.
2004. Prague Arabic dependency treebank: Develop-
ment in data and tools. In Proc. of the NEMLAR In-
tern. Conf. on Arabic Language Resources and Tools,
pages 110?117.
R. Johansson and P. Nugues. 2006. Investigating
multilingual dependency parsing. In Proceedings of
the Tenth Conference on Computational Natural Lan-
guage Learning (CoNLL-X), pages 206?210.
R. Johansson and P. Nugues. 2007. Extended
constituent-to-dependency conversion for English. In
Proc. of the 16th Nordic Conference on Computational
Linguistics (NODALIDA).
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993.
Building a large annotated corpus of English: the Penn
Treebank. Computational Linguistics, 19(2):313?330.
M. A. Mart??, M. Taule?, L. Ma`rquez, and M. Bertran.
2007. CESS-ECE: A multilingual and multilevel
annotated corpus. Available for download from:
http://www.lsi.upc.edu/?mbertran/cess-ece/.
R. McDonald, K. Crammer, and F. Pereira. 2005. Online
large-margin training of dependency parsers. In Pro-
ceedings of the Annual Meeting of the Association for
Computational Linguistics (ACL).
R. McDonald, K. Lerman, and F. Pereira. 2006. Multi-
lingual dependency analysis with a two-stage discrim-
inative parser. In Proceedings of the Conference on
Computational Natural Language Learning (CoNLL).
R. McDonald. 2006. Discriminative learning and span-
ning tree algorithms for dependency parsing. Ph.D.
thesis, University of Pennsylvania.
S. Montemagni, F. Barsotti, M. Battista, N. Calzolari,
O. Corazzari, A. Lenci, A. Zampolli, F. Fanciulli,
M. Massetani, R. Raffaelli, R. Basili, M. T. Pazienza,
D. Saracino, F. Zanzotto, N. Nana, F. Pianesi, and
R. Delmonte. 2003. Building the Italian Syntactic-
Semantic Treebank. In Abeille? (Abeille?, 2003), chap-
ter 11, pages 189?210.
J. Nivre and J. Nilsson. 2005. Pseudo-projective de-
pendency parsing. In Proceedings of the 43rd Annual
Meeting of the Association for Computational Linguis-
tics (ACL), pages 99?106.
J. Nivre, J. Hall, J. Nilson, G. Eryigit, and S. Marinov.
2006. Labeled pseudo-projective dependency pars-
ing with support vector machines. In Proceedings of
the Conference on Computational Natural Language
Learning (CoNLL).
J. Nivre, J. Hall, S. Ku?bler, R. McDonald, J. Nilsson,
S. Riedel, and D. Yuret. 2007. The CoNLL 2007
shared task on dependency parsing. In Proc. of the
Joint Conf. on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL).
J. Nivre. 2003. An efficient algorithm for projective
dependency parsing. In Proceedings of International
Workshop on Parsing Technologies, pages 149?160.
K. Oflazer, B. Say, D. Zeynep Hakkani-Tu?r, and G. Tu?r.
2003. Building a Turkish treebank. In Abeille?
(Abeille?, 2003), chapter 15, pages 261?277.
P. Prokopidis, E. Desypri, M. Koutsombogera, H. Papa-
georgiou, and S. Piperidis. 2005. Theoretical and
practical issues in the construction of a Greek depen-
dency treebank. In Proc. of the 4th Workshop on Tree-
banks and Linguistic Theories (TLT), pages 149?160.
K. Sagae and A. Lavie. 2006. A best-first probabilis-
tic shift-reduce parser. In Proceedings of the COL-
ING/ACL 2006 Main Conference Poster Sessions.
H. Yamada and Y. Matsumoto. 2003. Statistical depen-
dency analysis with support vector machines. In Pro-
ceedings of IWPT-2003.
1143
Proceedings of the ACL-IJCNLP 2009 Student Research Workshop, pages 10?17,
Suntec, Singapore, 4 August 2009.
c?2009 ACL and AFNLP
Insights into Non-projectivity in Hindi
Prashanth Mannem, Himani Chaudhry, Akshar Bharati
Language Technologies Research Center,
International Institute of Information Technology,
Gachibowli, Hyderabad, India - 500032
{prashanth,himani}@research.iiit.ac.in
Abstract
Large scale efforts are underway to cre-
ate dependency treebanks and parsers
for Hindi and other Indian languages.
Hindi, being a morphologically rich, flex-
ible word order language, brings chal-
lenges such as handling non-projectivity
in parsing. In this work, we look
at non-projectivity in Hyderabad De-
pendency Treebank (HyDT) for Hindi.
Non-projectivity has been analysed from
two perspectives: graph properties that
restrict non-projectivity and linguistic
phenomenon behind non-projectivity in
HyDT. Since Hindi has ample instances
of non-projectivity (14% of all structures
in HyDT are non-projective), it presents
a case for an in depth study of this phe-
nomenon for a better insight, from both of
these perspectives.
We have looked at graph constriants like
planarity, gap degree, edge degree and
well-nestedness on structures in HyDT.
We also analyse non-projectivity in Hindi
in terms of various linguistic parameters
such as the causes of non-projectivity,
its rigidity (possibility of reordering) and
whether the reordered construction is the
natural one.
1 Introduction
Non-projectivity occurs when dependents do not
either immediately follow or precede their heads
in a sentence (Tesnire, 1959). These dependents
may be spread out over a discontinuous region of
the sentence. It is well known that this poses prob-
lems for both theoretical grammar formalisms as
well as parsing systems. (Kuhlmann and M?ohl,
2007; McDonald and Nivre, 2007; Nivre et al,
2007)
Hindi is a verb final, flexible word order lan-
guage and therefore, has frequent occurrences
of non-projectivity in its dependency structures.
Bharati et al (2008a) showed that a major chunk
of errors in their parser is due to non-projectivity.
So, there is a need to analyse non-projectivity in
Hindi for a better insight into such constructions.
We would like to say here, that as far as we are
aware, there hasn?t been any attempt to study non-
projectivity in Hindi before this work. Our work
is a step forward in this direction.
Non-projectivity can be analysed from two as-
pects. a) In terms of graph properties which re-
strict non-projectivity and b) in terms of linguis-
tic phenomenon giving rise to non-projectivity.
While a) gives an idea of the kind of grammar for-
malisms and parsing algorithms required to handle
non-projective cases in a language, b) gives an in-
sight into the linguistic cues necessary to identify
non-projective sentences in a language.
Parsing systems can explore algorithms and
make approximations based on the coverage of
these graph properties on the treebank and lin-
guistic cues can be used as features to restrict the
generation of non-projective constructions (Shen
and Joshi, 2008). Similarly, the analyses based on
these aspects can also be used to come up with
broad coverage grammar formalisms for the lan-
guage.
Graph constraints such as projectivity, pla-
narity, gap degree, edge degree and well-
nestedness have been used in previous works to
look at non-projective constructions in treebanks
like PDT and DDT (Kuhlmann and Nivre, 2006;
Nivre, 2006). We employ these constraints in our
work too. Apart from these graph constraints, we
also look at non-projective constructions in terms
of various parameters like factors leading to non-
projectivity, its rigidity (see Section 4), its approx-
imate projective construction and whether its the
natural one.
10
In this paper, we analyse dependency structures
in Hyderabad Dependency Treebank (HyDT).
HyDT is a pilot treebank containing dependency
annotations for 1865 Hindi sentences. It uses
the annotation scheme proposed by Begum et al
(2008), based on the Paninian grammar formal-
ism.
This paper is organised as follows: In section
2, we give an overview of HyDT and the annota-
tion scheme used. Section 3 discusses the graph
properties that are used in our analysis and section
4 reports the experimental results on the coverage
of these properties on HyDT. The linguistic anal-
ysis of non-projective constructions is discussed
case by case in Section 5. The conclusions of this
work are presented in section 6. Section 7 gives
directions for future works on non-projectivity for
Hindi.
2 Hyderabad Dependency Treebank
(HyDT)
HyDT is a dependency annotated treebank for
Hindi. The annotation scheme used for HyDT is
based on the Paninian framework (Begum et al,
2008). The dependency relations in the treebank
are syntactico-semantic in nature where the main
verb is the central binding element of the sentence.
The arguments including the adjuncts are anno-
tated taking the meaning of the verb into consid-
eration. The participants in an action are labeled
with karaka relations (Bharati et al, 1995). Syn-
tactic cues like case-endings and markers such as
post-positions and verbal inflections, help in iden-
tifying appropriate karakas.
The dependency tagset in the annotation
scheme has 28 relations in it. These include
six basic karaka relations (adhikarana [location],
apaadaan [source], sampradaan [recipient], karana
[instrument], karma [theme] and karta [agent] ).
The rest of the labels are non-karaka labels like
vmod, adv, nmod, rbmod, jjmod etc...
1
The
tagset alo includes special labels like pof and
ccof, which are not dependency relations in the
strict sense. They are used to handle special
constructions like conjunct verbs (ex:- prashna
kiyaa (question did)), coordinating conjunc-
tions and ellipses.
In the annotation scheme used for HyDT, re-
lations are marked between chunks instead of
1
The entire dependency tagset can be found at
http://ltrc.deptagset.googlepages.com/k1.htm
words. A chunk (with boundaries marked) in
HyDT, by definition, represents a set of adjacent
words which are in dependency relation with each
other, and are connected to the rest of the words
by a single incoming dependency arc. The rela-
tions among the words in a chunk are not marked.
Thus, in a dependency tree in HyDT, each node is
a chunk and the edge represents the relations be-
tween the connected nodes labeled with the karaka
or other relations. All the modifier-modified rela-
tions between the heads of the chunks (inter-chunk
relations) are marked in this manner. The annota-
tion is done using Sanchay
2
mark up tool in Shakti
Standard Format (SSF) (Bharati et al, 2005). For
the work in this paper, to get the complete depen-
dency tree, we used an automatic rule based intra-
chunk relation identifier. The rules mark these
intra-chunk relations with an accuracy of 99.5%,
when evaluated on a test set.
The treebank has 1865 sentences with a total of
16620 chunks and 35787 words. Among these,
14% of the sentences have non-projective struc-
tures and 1.87% of the inter-chunk relations are
non-projective. This figure drops to 0.87% if we
consider the intra-chunk relations too (as all intra-
chunk relations are projective). In comparison,
treebanks of other flexible word order languages
like Czech and Danish have non-projectivity in
23% (out of 73088 sentences) and 15% (out
of 4393 sentences) respectively (Kuhlmann and
Nivre, 2006; Nivre et al, 2007).
3 Non projectivity and graph properties
In this section, we define dependency graph for-
mally and discuss standard propertiess uch as sin-
gle headedness, acyclicity and projectivity. We
then look at complex graph constraints like gap de-
gree, edge degree, planarity and well-nestedness
which can be used to restrict non-projectivity in
graphs.
In what follows, a dependency graph for an in-
put sequence of words x
1
? ? ?x
n
is an unlabeled
directed graph D = (X,Y ) where X is a set of
nodes and Y is a set of directed edges on these
nodes. x
i
? x
j
denotes an edge from x
i
to x
j
,
(x
i
, x
j
) ? Y . ?
?
is used to denote the reflexive
and transitive closure of the relation. x
i
?
?
x
j
means that the node x
i
dominates the node x
j
,
i.e., there is a (possibly empty) path from x
i
to
x
j
. x
i
? x
j
denotes an edge from x
i
to x
j
or vice
2
http://sourceforge.net/projects/nlp-sanchay
11
versa. For a given node x
i
, the set of nodes domi-
nated by x
i
is the projection of x
i
. We use pi(x
i
) to
refer to the projection of x
i
arranged in ascending
order.
Every dependency graph satisfies two con-
straints: acyclicity and single head. Acyclicity
refers to there being no cycles in the graph. Sin-
gle head refers to each node in the graphD having
exactly one incoming edge (except the one which
is at the root). While acyclicity and single head
constraints are satisfied by dependency graphs in
almost all dependency theories. Projectivity is a
stricter constraint used and helps in reducing pars-
ing complexities.
Projectivity: If node x
k
depends on node x
i
,
then all nodes between x
i
and x
k
are also subordi-
nate to x
i
(i.e dominated by x
i
) (Nivre, 2006).
x
i
? x
k
? x
i
?
?
x
j
?x
j
? X : (x
i
< x
j
< x
k
? x
i
> x
j
> x
k
)
Any graph which doesn?t satisfy this constraint
is non-projective. Unlike acyclicity and the sin-
gle head constraints, which impose restrictions
on the dependency relation as such, projectivity
constrains the interaction between the dependency
relations and the order of the nodes in the sen-
tence (Kuhlmann and Nivre, 2006)..
Graph properties like planarity, gap degree,
edge degree and well-nestedness have been pro-
posed in the literature to constrain grammar for-
malisms and parsing algorithms from looking at
unrestricted non-projectivity. We define these
properties formally here.
Planarity: A dependency graph is planar if
edges do not cross when drawn above the sentence
(Sleator and Temperley, 1993). It is similar to pro-
jectivity except that the arc from dummy node at
the beginning (or the end) to the root node is not
considered.
?(x
i
, x
j
, x
k
, x
l
) ? X,
?((x
i
? x
k
? x
j
? x
l
) ? (x
i
< x
j
< x
k
< x
l
))
Gap degree: The gap degree of a node is the
number of gaps in the projection of a node. A gap
is a pair of nodes (pi(x
i
)
k
, pi(x
i
)
k+1
) adjacent in
pi(x
i
) but not adjacent in sentence. The gap de-
gree of node Gd(x
i
) is the number of such gaps
in its projection. The gap degree of a sentence
is the maximum among gap degrees of nodes in
D(X,Y ) (Kuhlmann, 2007).
Edge degree: The number of connected com-
ponents in the span of an edge which are not
dominated by the outgoing node in the edge.
Span span(x
i
? x
j
) = (min(i, j),max(i, j)).
Ed(x
i
? x
j
) is the number of connected com-
ponenets in the span span(x
i
? x
j
) whose parent
is not in the projection of x
i
. The edge degree of
a sentence is the maximum among edge degrees
of edges in D(X,Y ). (Nivre, 2006) defines it as
degree of non-projectivity. Following (Kuhlmann
and Nivre, 2006), we call this edge degree to avoid
confusion.
Well-nested: A dependency graph is well-
nested if no two disjoint subgraphs interleave
(Bodirsky et al, 2005). Two subgraphs are dis-
joint if neither of their roots dominates the other.
Two subtrees S
i
,S
j
interleave if there are nodes
x
l
, x
m
? S
i
and x
n
, x
o
? S
j
such that l < m <
n < o (Kuhlmann and Nivre, 2006).
The gap degree and the edge degree provide
a quantitative measure for the non-projectivity of
dependency structures. Well-nestedness is a qual-
itative property: it constrains the relative positions
of disjoint subtrees.
4 Experiments on HyDT
Property Count Percentage
All structures 1865
Gap degree
Gd(0) 1603 85.9%
Gd(1) 259 13.89%
Gd(2) 0 0%
Gd(3) 3 0.0016%
Edge degree
Ed(0) 1603 85.9%
Ed(1) 254 13.6%
Ed(2) 6 0.0032%
Ed(3) 1 0.0005%
Ed(4) 1 0.0005%
Projective 1603 85.9%
Planar 1639 87.9%
Non-projective 36 1.93%
& planar
Well-nested 1865 100%
Table 1: Results on HyDT
In this section, we present an experimental eval-
uation of the graph constraints mentioned in the
previous section on the dependency structures in
12
_ROOT_ tab     raat  lagabhag   chauthaaii   Dhal__chukii__thii     jab     unheM    behoshii__sii  aaiii
then   night  about      one?fourth    over    be.PastPerf.  when   him   unconsciouness  PART. came
About one?fourth of the night was over when he started becoming unconscious
_ROOT_   hamaaraa   maargadarshak__aur__saathii     saty__hai  ,   jo   iishvar__hai
Truth, which is God, is our guide and companion 
our             guide  and  companion             truth  is     , which God   is
a)
b)
Figure 1: a) Relative co-relative construction, b) Extraposed relative clause construction
HyDT. Since HyDT is a small corpus and is still
under construction, these results might not be the
exact reflection of naturally occurring sentences in
real-world. Nevertheless, we hope these results
will give an idea of the kind of structures one can
expect in Hindi.
We report the percentage of structures that
satisfy various graph properties in table 1. In
HyDT, we see that 14% of all structures are non-
projective. The highest gap degree for structures
in HyDT is 3 and in case of edge degree, it is 4.
Only 3 structures (1.5% approx.) have gap de-
gree of more than 1 in a total of 262 non-projective
sentences. When it comes to edge degree, only 8
structures (3%) have edge degree more than 1.
The difference in the coverage of gap degree
1 & 2 (and the fact that gap degree 1 accounts
for 13.9% of the structures) shows that a parser
should handle non-projective constructions at least
till gap degree 1 for good coverage. The same can
be said about edge degree.
5 Cases of non-projectivity in HyDT
We have carried out a study of the instances of
non-projectivity that HyDT brought forth. In
this section, we classify these instances based on
factors leading to non-projectivity and present
our analysis of them. For each of these classes,
we look at the rigidity of these non-projective
constructions and their best projective approxi-
mation possible by reordering. Rigidity here is
the reorderability of the constructions retaining
the gross meaning. Gross meaning refers to the
meaning of the sentence not taking the discourse
and topic-focus into consideration, which is how
parsing is typically done.
e.g., the non-projective construction in figure 1b,
yadi rupayoM kii zaruurat thii to
mujh ko bataanaa chaahiye thaa
3
can be reordered to form a projective construction
mujh ko bataanaa chaahiye thaa
yadi rupayoM kii zaruurat thii
to. Therefore, this sentence is not rigid.
Study of rigidity is important from natural lan-
guage generation perspective. Sentence genera-
tion from projective structures is easier and more
efficient than from non-projective ones. Non-
projectivity in constructions that are non-rigid can
be effectively dealt with through projectivisation.
Further, we see if these approximations are
more natural compared to the non-projective ones
as this impacts sentence generation quality. A nat-
ural construction is the one most preferred by na-
tive speakers of that language. Also, it more or less
abides by the well established rules and patterns of
the language.
We observed that non-projectivity is caused in
Hindi, due to various linguistic phenomena mani-
fested in the language, such as relative co-relative
constructions, paired connectives, complex co-
ordinating structures, interventions in verbal argu-
ments by non-verbal modifiers, shared arguments
in non-finite clauses, movement of modifiers, el-
lipsis etc. Also, non-projectivity in Hindi can oc-
cur within a clause (intra-clausal) as well as be-
tween elements across clauses (inter-clausal).
We now discuss some of these linguistic phe-
nomena causing non-projectivity.
3
The glosses for the sentences in this section are listed in
the corresponding figures and are not repeated to save space.
13
Gorki       if       this   new   literature  of       creator           was   then        socialism         its         solid         base        was
If Gorki was the creator of this new literature, then socialism was its solid base
b)
_ROOT_      gorkii    yadi    is__naye__saahity__ke__srishtikartaa         the    to       samaajavaad     isakaa     Thos    aadhaar    thaa
a)
_ROOT_     yadi       rupayoM   kii   zaruurat   thii       to       mujh     ko   bataanaa__chahiye__thaa
if              rupees     of      need      was    then      me      Dat.     told             should    be(past)
If [you] needed rupees then [you] should have told me
Figure 2: a) Paired connectives construction, b) Construction with non-projectivity within a clause
5.1 Relative co-relative constructions
The pattern in co-relatives is that a demonstra-
tive pronoun, which also functions as deter-
miner in Hindi, such as vo (that), always oc-
curs in correlation with a relative pronoun, jo
(which). In fact, the language employs a se-
ries of such pronouns : e.g., jis-us ?which-
that?, jahaaM-vahaaM ?where-there?, jidhar-
udhar ?where-there?, jab-tab ?when-then?,
aise-jaise (Butt et al, 2007).
Non-projectivity is seen to occur in relative co-
relative constructions with pairs such as jab-tab,
if the clause beginning with the tab precedes the
jab clause as seen in figure 1a. If the clause with
the relative pronoun comes before the clause with
the demonstrative pronoun, non-projectivity can
be ruled out. So, this class of non-projective con-
structions is not rigid since projective structures
can be obtained by reordering without any loss of
meaning. The projective case is relatively more
natural than the non-projective one. This is reaf-
firmed in the corpus where the projective relative
co-relative structures are more frequent than the
non-projective sentences.
In the example in figure 1a, the sentence can be
reordered by moving the tab clause to the right
of the jab clause, to remove non-projectivity.
jab unheM behoshii sii aaii tab
raat lagabhag chauthaaii Dhal
chukii thii ? when he started becoming
unconscious, about one-fourth of the night was
over
5.2 Extraposed relative clause constructions
If the relative clause modifying a noun phrase
(NP) occurs after the verb group (VP), it leads to
non-projectivity.
In the sentence in figure 1b, non-projectivity
occurs because jo iishvar hai, the rel-
ative clause modifying the NP hamaaraa
maargadarshak aur saathii is extra-
posed after the VP saty hai.
This class of constructions is not rigid as the
extraposed relative clause can be moved next to
the noun phrase, making it projective. However,
the resulting projective construction is less natural
than the original non-projective one.
The reordered projective construction
for the example sentence is hamaaraa
maargadarshak aur saathii, jo
iishvar hai, saty hai ? Our guide and
companion which is God is truth
This class of non-projective constructions ac-
counts for approximately half of the total non-
projective sentences in the treebank.
5.3 Intra-clausal non-projectivity
In this case, the modifier of the NP is a non-relative
clause and is different from the class 5.2.
In the example in figure 2b, the NP
gorkii and the phrase modifying it is
naye saahity ke srishtikartaa are
separated by yadi, a modifier of to clause.
Intra-clausal non-projectivity here is within the
clause gorkii yadi is naye saahity
ke srishtikartaa the.
14
He had such [a] liking for sniff that he was not able to give it up
a)
_ROOT_     naas     kaa    unheM       aisaa     shauk_thaa       ki       usako    tyaag    na        paate__the 
 sniff      of       him            such    liking   was      that       it        give?up not   able?to  was
_ROOT_   usakaa    is__hiire__ke__liye    lagaava    svata:    siddh__hai
his     this  diamond  for            love   by?itself  evident  is
his love for this diamond is evident by itself
b)
Figure 3: a) ki complement clause, b) Genetive relation split by a verb modifier
To remove non-projectivity, reordering of such
sentences is possible by moving the non-modifier,
so that it no more separates them. Here, moving
yadi to the left of gorkii takes care of non-
projectivity thus making this class not rigid. The
reordered projective construction is more natural.
yadi gorkii is naye saahity ke
srishtikartaa the to samaajavaad
isakaa Thos aadhaar thaa
5.4 Paired connectives
Paired connectives (such as agar-to ?if -then?,
yadi-to ?if -then?) give rise to non-projectivity in
HyDT on account of the annotation scheme used.
As shown in figure 2a, the to clause is modified
by the yadi clause in such constructions. Most of
these sentences can be reordered while still retain-
ing the meaning of the sentence: the phrase that
comes after to, followed by yadi clause, and
then to. Here mentioning to is optional.
This sentence can be reordered and is not rigid.
However, the resulting projective construction
is not a natural one. mujh ko bataanaa
chaahiye thaa yadi rupayoM kii
zaruurat thii [to] ? (you) should have
told me if (you) needed rupees
Connectives like yadi can also give rise to
intra-clausal non-projectivity apart from inter-
clausal non-projectivity as discussed. This hap-
pens when the connective moves away from the
beginning of the sentence (see figure 2b).
5.5 ki complement clause
A phrase (including a VP in it) appears between
the ki (that) clause and the word it modifies
(such as yaha (this), asiaa (such), is tarah
(such), itana (this much) ), resulting in non-
projectivity in the ki complement constructions.
The verb in this verb group is generally copular.
Since Hindi is a verb final language, the comple-
mentiser clause (ki clause) occurs after the verb
of the main clause, while its referent lies before
the verb in the main clause. This leads to non-
projectivity in such constructions. The yaha-ki
constructions follow the pattern: yaha-its prop-
erty-VP-ki clause.
E.g. yaha-rahasya-hai-ki shukl
jii pratham shreNii ke kavi kyoM
the.
This class of constructions are rigid and non-
projectivity can?t be removed from such sen-
tences. In cases where the VP has a transitive
verb, the ki clause and its referent, both mod-
ify the verb, making the construction projective.
For ex. In usane yaha kahaa ki vaha
nahin aayegaa, yaha and the ki clause both
modify the verb kahaa.
In figure 3a, the phrase shauk thaa sepa-
rates aisaa and the ki clause, resulting in non-
projectivity.
5.6 A genetive relation split by a verb
modifier
This is also a case of intra-clausal non-projectivity.
In such constructions, the verb has its modifier em-
bedded within the genetive construction.
In the example in figure 3b, the components of
the genetive relation, usakaa and lagaav are
separated by the phrase is hiire ke liye.
15
that    writers?       identity   Acc  we    proudly    publisher           before         put.non?fin    talk        do      be.Past
The writers? identity that we proudly put before the publisher and talked [to him]
_ROOT_      us__lekhakiiy__asmitaa__ko      ham  sagarv   prakaashak__ke?saamane       rakhakar         baat__karate__the
b)
a)
_ROOT_    isake__baad     vah    jamaan__shaah   aur?phir    1795__meM    shaah__shujaa   ko   milaa
  this      after      it       Jaman     Shah  and?then   1795      in        Shah     Shuja    to     got
After this Jaman Shah [got it] and then, in 1795 Shah Shuja got it
Figure 4: a) A phrase splitting a co-ordinating structure, b) Shared argument splitting the non finite
clause
The sentence is not rigid and can be reordered to
a projective construction by moving the phrase is
hiire ke liye to the left of usakaa. It re-
tains the meaning of the original construction and
is also, a more natural one.
is hiire ke liye usakaa lagaav
svata: siddh hai ? his love for this
diamond is evident by itself
5.7 A phrase splitting a co-ordinating
structure
As seen in figure 4a, non-projectivity is caused
in the sentence because, embedding of the
phrase 1795 meM splits the co-ordinating
structure jamaan shaah aur-phir shaah
shujaa. These kinds of constructions can be re-
ordered. So, they are not rigid. The projective
constructions are more natural.
isake baad vah jamaan shaah ko
aur-phir shaah shujaa ko 1795 meM
milaa
Non-projective Class Count %
Relative co-relatives constructions 18 6.8 %
Extraposed realtive clause constructions 101 38.0 %
Intra-clausal non-projectivity 12 4.5 %
Paired connectives 33 12.4 %
ki complement clauses 52 19.5 %
Genetive relation split by a verb modifier 10 3.8 %
Phrase splitting a co-ordinating structure 4 1.5 %
Shared argument splits the non-finite clause 10 3.8 %
Others 26 9.8 %
Table 2: Non-projectivity class distribution in HyDT
5.8 Shared argument splits the non finite
clause
In the example in 4b, hama is annotated as the ar-
gument of the main verb baawa karate the.
It also is the shared argument of the non finite
verb rakhakara (but isn?t marked explicitly in
the treebank). It splits the non finite clause us
lekhakiiya asmitaa ko ham sagarv
prakaashak ke saamane rakhakara
Through reordering, this sentence can easily be
made into a projective construction, which is also
the more natural construction for it.
ham us lekhakiiy asmitaa ko
sagarv prakaashak ke-saamane
rakhakar baat karate the
5.9 Others
There are a few non-projective constructions in
HyDTwhich haven?t been classified and discussed
in the eight categories above. This is because they
are single occurences in HyDT and seem to be rare
phenomenon. There are also a few instances of in-
consistent NULL placement and errors in chunk
boundary marking or annotation.
6 Conclusion
Our study of HyDT shows that non-projectivity in
Hindi is more or less confined to the classes dis-
cussed in this paper. There might be more types of
non-projective structures in Hindi which may not
have occurred in the treebank.
Recent experiments on Hindi dependency pars-
ing have shown that non-projective structures form
a major chunk of parsing errors (Bharati et al,
16
2008a). In spite of using state-of-art parsers which
handle non-projectivity, experiments show that the
types of non-projectivity discussed in this paper
are not handled effectively.
The knowledge of such non-projective classes
could possibly be used to enhance the perfor-
mance of a parser. This work further corrobo-
rates Kuhlmann?s work on Czech (PDT) for Hindi
(Kuhlmann and Nivre, 2006). Specifically, as dis-
cussed in section 4, the non-projective structures
in HyDT satisfy the constraints (gap degree ? 2
and well-nestedness) to be called as mildly non-
projective.
7 Future Work
We propose to use the analysis in this paper to
come up with non-projective parsers for Hindi.
This can be done in more than one ways, such as:
The constraint based dependency parser for
Hindi proposed in (Bharati et al, 2008b) can be
extended to incorporate graph properties discussed
in section 3 as constraints.
Further, linguistic insights into non-projectivity
can be used in parsing to identify when to generate
the non-projective arcs. The parser can have spe-
cialised machinery to handle non-projectivity only
when linguistic cues belonging to these classes are
active. The advantage of this is that one need not
come up with formal complex parsing algorithms
which give unrestricted non-projective structures.
As the HyDT grows, we are bound to come
across more instances as well as more types of
non-projective constructions that could bring forth
interesting phenomenon. We propose to look into
these for further insights.
References
R. Begum, S. Husain, A. Dhwaj, D. Sharma, L. Bai, and
R. Sangal. 2008. Dependency annotation scheme for in-
dian languages. In In Proceedings of The Third Interna-
tional Joint Conference on Natural Language Processing
(IJCNLP), Hyderabad, India.
Akshar Bharati, Vineet Chaitanya, and Rajeev Sangal. 1995.
Natural Language Processing: A Paninian Perspective.
Prentice-Hall of India.
Akshar Bharati, Rajeev Sangal, and Dipti Sharma. 2005.
Shakti analyser: Ssf representation. Technical report, In-
ternational Institute of Information Technology, Hyder-
abad, India.
Akshar Bharati, Samar Husain, Bharat Ambati, Sambhav
Jain, Dipti Sharma, and Rajeev Sangal. 2008a. Two se-
mantic features make all the difference in parsing accu-
racy. In Proceedings of the 6th International Conference
on Natural Language Processing (ICON-08), Pune, India.
Akshar Bharati, Samar Husain, Dipti Sharma, and Rajeev
Sangal. 2008b. A two-stage constraint based dependency
parser for free word order languages. In Proceedings of
the COLIPS International Conference on Asian Language
Processing 2008 (IALP), Chiang Mai, Thailand.
Manuel Bodirsky, Marco Kuhlmann, andMathiasMhl. 2005.
Well-nested drawings as models of syntactic structure. In
In Tenth Conference on Formal Grammar and Ninth Meet-
ing on Mathematics of Language, pages 88?1. University
Press.
M. Butt, T. H. King, and S. Roth. 2007. Urdu correlatives:
Theoretical and implementational issues. In Online Pro-
ceedings of the LFG07 Conference, pages 87?106. CSLI
Publications.
Marco Kuhlmann and Mathias M?ohl. 2007. Mildly context-
sensitive dependency languages. In Proceedings of the
45th Annual Meeting of the Association of Computa-
tional Linguistics, pages 160?167, Prague, Czech Repub-
lic, June. Association for Computational Linguistics.
Marco Kuhlmann and Joakim Nivre. 2006. Mildly non-
projective dependency structures. In Proceedings of the
COLING/ACL 2006 Main Conference Poster Sessions,
pages 507?514, Sydney, Australia, July. Association for
Computational Linguistics.
Marco Kuhlmann. 2007. Dependency Structures and Lexi-
calized Grammars. Ph.D. thesis, Saarland University.
Ryan McDonald and Joakim Nivre. 2007. Characterizing
the errors of data-driven dependency parsing models. In
Proceedings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Compu-
tational Natural Language Learning (EMNLP-CoNLL),
pages 122?131, Prague, Czech Republic, June. Associa-
tion for Computational Linguistics.
Joakim Nivre, Johan Hall, Sandra K?ubler, Ryan McDon-
ald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret.
2007. The CoNLL 2007 shared task on dependency pars-
ing. In Proceedings of the CoNLL Shared Task Session of
EMNLP-CoNLL 2007, pages 915?932, Prague, Czech Re-
public, June. Association for Computational Linguistics.
Joakim Nivre. 2006. Constraints on non-projective depen-
dency parsing. In In Proceedings of European Association
of Computational Linguistics (EACL), pages 73?80.
Libin Shen and Aravind Joshi. 2008. LTAG dependency
parsing with bidirectional incremental construction. In
Proceedings of the 2008 Conference on Empirical Meth-
ods in Natural Language Processing, pages 495?504,
Honolulu, Hawaii, October. Association for Computa-
tional Linguistics.
Daniel Sleator and Davy Temperley. 1993. Parsing english
with a link grammar. In In Third International Workshop
on Parsing Technologies.
L. Tesnire. 1959. lments de Syntaxe Structurale. Libraire C.
Klincksieck, Paris.
17
Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL),
pages 165?168, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Inferring semantic roles using sub-categorization frames and
maximum entropy model
Akshar Bharati, Sriram Venkatapathy and Prashanth Reddy
Language Technologies Research Centre, IIIT - Hyderabad, India.
{sriram,prashanth}@research.iiit.ac.in
Abstract
In this paper, we propose an approach
for inferring semantic role using sub-
categorization frames and maximum
entropy model. Our approach aims to
use the sub-categorization information
of the verb to label the mandatory ar-
guments of the verb in various possi-
ble ways. The ambiguity between the
assignment of roles to mandatory argu-
ments is resolved using the maximum
entropy model. The unlabelled manda-
tory arguments and the optional argu-
ments are labelled directly using the
maximum entropy model such that their
labels are not one among the frame el-
ements of the sub-categorization frame
used. Maximum entropy model is pre-
ferred because of its novel approach
of smoothing. Using this approach,
we obtained an F-measure of 68.14%
on the development set of the data
provided for the CONLL-2005 shared
task. We show that this approach per-
forms well in comparison to an ap-
proach which uses only the maximum
entropy model.
1 Introduction
Semantic role labelling is the task of assigning
appropriate semantic roles to the arguments of
a verb. The semantic role information is impor-
tant for various applications in NLP such as Ma-
chine Translation, Question Answering, Informa-
tion Extraction etc. In general, semantic role in-
formation is useful for sentence understanding.
We submitted our system for closed challenge
at CONLL-2005 shared task. This task encour-
ages participants to use novel machine learning
techniques suited to the task of semantic role la-
belling. Previous approaches on semantic role
labelling can be classified into three categories
(1) Explicit Probabilistic methods (Gildea and
Jurafsky, 2002). (2) General machine learning
algorithms (Pradhan et al, 2003) (Lim et al,
2004) and (3) Generative model (Thompson et
al., 2003).
Our approach has two stages; first, identifica-
tion whether the argument is mandatory or op-
tional and second, the classification or labelling
of the arguments. In the first stage, the arguments
of a verb are put into three classes, (1) mandatory,
(2) optional or (3) null. Null stands for the fact
that the constituent of the verb in the sentence is
not an semantic argument of the verb. It is used to
rule out the false argument of the verb which were
obtained using the parser. The maximum entropy
based classifier is used to classify the arguments
into one of the above three labels.
After obtaining information about the nature of
the non-null arguments, we proceed in the second
stage to classify the mandatory and optional ar-
guments into their semantic roles. The propbank
sub-categorization frames are used to assign roles
to the mandatory arguments. For example, in the
sentence ?John saw a tree?, the sub-categorization
frame ?A0 v A1? would assign the roles A0 to
John and A1 to tree respectively. After using
all the sub-categorization frames of the verb irre-
165
spective of the verb sense, there could be ambigu-
ity in the assignment of semantic roles to manda-
tory arguments. The unlabelled mandatory argu-
ments and the optional arguments are assigned
the most probable semantic role which is not one
of the frame elements of the sub-categorization
frame using the maximum entropy model. Now,
among all the sequences of roles assigned to the
non-null arguments, the sequence which has the
maximum joint probability is chosen. We ob-
tained an accuracy of 68.14% using our approach.
We also show that our approach performs better
in comparision to an approach with uses a simple
maximum entropy model. In section 4, we will
talk about our approach in greater detail.
This paper is organised as follows, (2) Features,
(3) Maximum entropy model, (4) Description of
our system, (5) Results, (6) Comparison with our
other experiments, (7) Conclusion and (8) Future
work.
2 Features
The following are the features used to train the
maximum entropy classifier for both the argument
identification and argument classification. We
used only simple features for these experiments,
we are planning to use richer features in the near
future.
1. Verb/Predicate.
2. Voice of the verb.
3. Constituent head and Part of Speech tag.
4. Label of the constituent.
5. Relative position of the constituent with re-
spect to the verb.
6. The path of the constituent to the verb
phrase.
7. Preposition of the constituent, NULL if it
doesn?t exist.
3 Maximum entropy model
The maximum entropy approach became the pre-
ferred approach of probabilistic model builders
for its flexibility and its novel approach to
smoothing (Ratnaparakhi, 1999).
Many classification tasks are most naturally
handled by representing the instance to be classi-
fied as a vector of features. We represent features
as binary functions of two arguments, f(a,H),
where ?a? is the observation or the class and ?H? is
the history. For example, a feature fi(a, H) is true
if ?a? is Ram and ?H? is ?AGENT of a verb?. In a
log linear model, the probability function P (a|H)
with a set of features f1, f2, ....fj that connects ?a?
to the history ?H?, takes the following form.
P (a|H) = e
?
i ?i(a,H)?fi(a,H)
Z(H)
Here ?i?s are weights between negative and
positive infinity that indicate the relative impor-
tance of a feature: the more relevant the feature to
the value of the probability, the higher the abso-
lute value of the associated lambda. Z(H), called
the partition function, is the normalizing constant
(for a fixed H).
4 Description of our system
Our approach labels the semantic roles in two
stages, (1) argument identification and (2) ar-
gument classification. As input to our sys-
tem, we use full syntactic information (Collins,
1999), Named-entities, Verb senses and Propbank
frames. For our experiments, we use Zhang Le?s
Maxent Toolkit 1, and the L-BFGS parameter esti-
mation algorithm with Gaussian prior smoothing
(Chen and Rosenfield, 1999).
4.1 Argument identification
The first task in this stage is to find the candidate
arguments and their boundaries using a parser.
We use Collins parser to infer a list of candidate
arguments for every predicate. The following are
some of the sub-stages in this task.
? Convert the CFG tree given by Collins parser
to a dependency tree.
? Eliminate auxilliary verbs etc.
? Mark the head of relative clause as an argu-
ment of the verb.
1http://www.nlplab.cn/zhangle/maxent toolkit.html
166
? If a verb is modified by another verb, the
syntactic arguments of the superior verb
are considered as shared arguments between
both the verbs.
? If a prepositional phrase attached to a verb
contains more than one noun phrase, attach
the second noun phrase to the verb.
The second task is to filter out the constituents
which are not really the arguments of the pred-
icate. Given our approach towards argument
classification, we also need information about
whether an argument is mandatory or optional.
Hence, in this stage the constituents are marked
using three labels, (1) MANDATORY argument,
(2) OPTIONAL argument and (3) NULL, using a
maximum entropy classifier. For example, a sen-
tence ?John was playing football in the evening?,
?John? is marked MANDATORY, ?football? is
marked MANDATORY and ?in the evening? is
marked OPTIONAL.
For training, the Collins parser is run on the
training data and the syntactic arguments are
identified. Among these arguments, the ones
which do not exist in the propbank annotation of
the training data are marked as null. Among the
remaining arguments, the arguments are marked
as mandatory or optional according to the prop-
bank frame information. Mandatory roles are
those appearing in the propbank frames of the
verb and its sense, the rest are marked as optional.
A propbank frame contains information as illus-
trated by the following example:
If Verb = play, sense = 01,
then the roles A0, A1 are MANDATORY.
4.2 Argument classification
Argument classification is done in two steps. In
the first step, the propbank sub-categorization
frames are used to assign the semantic roles to the
mandatory arguments in the order specified by the
sub-categorization frames. Sometimes, the num-
ber of mandatory arguments of a verb in the sen-
tence may be less than the number of roles which
can be assigned by the sub-categorization frame.
For example, in the sentence
?MAN1 MAN2 V MAN3 OPT1?, roles could
be assigned in the following two possible ways by
the sub-categorization frame ?A0 v A1? of verb
V1.
? A0[MAN1] MAN2 V1 A1[MAN3] OPT1
? MAN1 A0[MAN2] V A1[MAN3] OPT1
In the second step, the task is to label the un-
labelled mandatory arguments and the arguments
which are marked as optional. This is done by
marking these arguments with the most probable
semantic role which is not one of the frame ele-
ments of the sub-categorization frame ?A0 v A1?.
In the above example, the unlabelled mandatory
arguments and the optional arguments cannot be
labelled as either A0 or A1. Hence, after this step,
the following might be the role-labelling for the
sentence ?MAN1 MAN2 V1 MAN3 OPT1?.
? A0[MAN1] AM-TMP[MAN2] V1
A1[MAN3] AM-LOC[OPT1]
? AM-MNC[MAN1] A0[MAN2] V1
A1[MAN3] AM-LOC[OPT1]
The best possible sequence of semantic roles
(R?) is decided by the taking the product of prob-
abilities of individual assignments. This also dis-
ambiguates the ambiguity in the assignment of
mandatory roles. The individual probabilities are
computed using the maximum entropy model.
For a sequence ~R, the product of the probabilities
is defined as
P (~R) = ?Ri?~RP (Ri|Argi)
The best sequence of semantic roles R? is de-
fined as
R? = argmax P (~R)
For training the maximum entropy model, the
outcomes are all the possible semantic roles. The
list of sub-categorization frames for a verb is ob-
tained from the training data using information
about mandatory roles from the propbank. The
propbank sub-categorization frames are also ap-
pended to this list.
We present our results in the next section.
167
Precision Recall F?=1
Development 71.88% 64.76% 68.14
Test WSJ 73.76% 65.52% 69.40
Test Brown 65.25% 55.72% 60.11
Test WSJ+Brown 72.66% 64.21% 68.17
Test WSJ Precision Recall F?=1
Overall 73.76% 65.52% 69.40
A0 85.17% 73.34% 78.81
A1 74.08% 66.08% 69.86
A2 54.51% 48.47% 51.31
A3 52.54% 35.84% 42.61
A4 71.13% 67.65% 69.35
A5 25.00% 20.00% 22.22
AM-ADV 52.18% 47.23% 49.59
AM-CAU 60.42% 39.73% 47.93
AM-DIR 45.65% 24.71% 32.06
AM-DIS 75.24% 73.12% 74.17
AM-EXT 73.68% 43.75% 54.90
AM-LOC 50.80% 43.53% 46.88
AM-MNR 47.24% 49.71% 48.44
AM-MOD 93.67% 91.29% 92.46
AM-NEG 94.67% 92.61% 93.63
AM-PNC 42.02% 43.48% 42.74
AM-PRD 0.00% 0.00% 0.00
AM-REC 0.00% 0.00% 0.00
AM-TMP 74.13% 66.97% 70.37
R-A0 82.27% 80.80% 81.53
R-A1 73.28% 61.54% 66.90
R-A2 75.00% 37.50% 50.00
R-A3 0.00% 0.00% 0.00
R-A4 0.00% 0.00% 0.00
R-AM-ADV 0.00% 0.00% 0.00
R-AM-CAU 0.00% 0.00% 0.00
R-AM-EXT 0.00% 0.00% 0.00
R-AM-LOC 100.00% 57.14% 72.73
R-AM-MNR 25.00% 16.67% 20.00
R-AM-TMP 70.00% 53.85% 60.87
V 97.28% 97.28% 97.28
Table 1: Overall results (top) and detailed results
on the WSJ test (bottom).
5 Results
The results of our approach are presented in table
1.
When we used an approach which uses a sim-
ple maximum entropy model, we obtained an F-
measure of 67.03%. Hence, we show that the
sub-categorization frames help in predicting the
semantic roles of the mandatory arguments, thus
improving the overall performance.
6 Conclusion
In this paper, we propose an approach for in-
ferring semantic role using sub-categorization
frames and maximum entropy model. Using this
approach, we obtained an F-measure of 68.14%
on the development set of the data provided for
the CONLL-2005 shared task.
7 Future work
We have observed that the main limitation of our
system was in argument identification. Currently,
the recall of the arguments inferred from the out-
put of the parser is 75.52% which makes it the up-
per bound of recall of our system. In near future,
we would focus on increasing the upper bound
of recall. In this direction, we would also use
the partial syntactic information. The accuracy
of the first stage of our approach would increase
if we include the mandatory/optional information
for training the parser (Yi and Palmer, 1999).
8 Acknowledgements
We would like to thank Prof. Rajeev Sangal, Dr.
Sushama Bendre and Dr. Dipti Misra Sharma for
guiding us in this project. We would like to thank
Szu-ting for giving some valuable advice.
References
S. Chen and R. Rosenfield. 1999. A gaussian prior for
smoothing maximum entropy models.
M. Collins. 1999. Head driven statistical models for
natural language processing.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles.
Hwang Young Sook Lim, Joon-H and, So-Young Park,
and Hae-Chang Rim. 2004. Semantic role labelling
using maximum entropy model.
Sameer Pradhan, Kadri Hacioglu, Valerie Krugler,
Wayne Ward, James. H. Martin, and Daniel Juraf-
sky. 2003. Support Vector Learning for Semantic
Argument Classification.
Adwait Ratnaparakhi. 1999. Learning to parse natural
language with maximum entropy models.
Cynthia A. Thompson, Roger Levy, and Christo-
pher D. Manning. 2003. A generative model for
semantic role labelling.
Szu-ting Yi and M. Palmer. 1999. The integration of
syntactic parsing and semantic role labeling.
168
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2115?2126,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Prune-and-Score: Learning for Greedy Coreference Resolution
Chao Ma, Janardhan Rao Doppa
?
, J. Walker Orr, Prashanth Mannem
Xiaoli Fern, Tom Dietterich and Prasad Tadepalli
School of Electrical Engineering and Computer Science, Oregon State University
{machao,orr,mannemp,xfern,tgd,tadepall}@eecs.oregonstate.edu
? School of Electrical Engineering and Computer Science, Washington State University
jana@eecs.wsu.edu
Abstract
We propose a novel search-based approach
for greedy coreference resolution, where
the mentions are processed in order and
added to previous coreference clusters.
Our method is distinguished by the use
of two functions to make each corefer-
ence decision: a pruning function that
prunes bad coreference decisions from fur-
ther consideration, and a scoring function
that then selects the best among the re-
maining decisions. Our framework re-
duces learning of these functions to rank
learning, which helps leverage powerful
off-the-shelf rank-learners. We show that
our Prune-and-Score approach is superior
to using a single scoring function to make
both decisions and outperforms sever-
al state-of-the-art approaches on multiple
benchmark corpora including OntoNotes.
1 Introduction
Coreference resolution is the task of clustering a
set of mentions in the text such that all mentions in
the same cluster refer to the same entity. It is one
of the first stages in deep language understanding
and has a big potential impact on the rest of the
stages. Several of the state-of-the-art approaches
learn a scoring function defined over mention pair,
cluster-mention or cluster-cluster pair to guide the
coreference decision-making process (Daum?e II-
I, 2006; Bengtson and Roth, 2008; Rahman and
Ng, 2011b; Stoyanov and Eisner, 2012; Chang et
al., 2013; Durrett et al., 2013; Durrett and Klein,
2013). One common and persistent problem with
these approaches is that the scoring function has to
make all the coreference decisions, which leads to
a highly non-realizable learning problem.
Inspired by the recent success of theHC-Search
Framework (Doppa et al., 2014a) for studying a
variety of structured prediction problems (Lam et
al., 2013; Doppa et al., 2014c), we study a novel
approach for search-based coreference resolution
called Prune-and-Score. HC-Search is a divide-
and-conquer solution that learns multiple compo-
nents with pre-defined roles, and each of them
contribute towards the overall goal by making the
role of the other components easier. The HC-
Search framework operates in the space of com-
plete outputs, and relies on the loss function which
is only defined on the complete outputs to drive it-
s learning. Unfortunately, this method does not
work for incremental coreference resolution since
the search space for coreference resolution con-
sists of partial outputs, i.e., a set of mentions only
some of which have been clustered so far.
We develop an alternative framework to HC-
Search that allows us to effectively learn from par-
tial output spaces and apply it to greedy corefer-
ence resolution. The key idea of our work is to
address the problem of non-realizability of the s-
coring function by learning two different function-
s: 1) a pruning function to prune most of the bad
decisions, and 2) a scoring function to pick the
best decision among those that are remaining. Our
Prune-and-Score approach is a particular instanti-
ation of the general idea of learning nearly-sound
constraints for pruning, and leveraging the learned
constraints to learn improved heuristic function-
s for guiding the search. The pruning constraints
can take different forms (e.g., classifiers, decision-
list, or ranking functions) depending on the search
architecture. Therefore, other coreference resolu-
tion systems (Chang et al., 2013; Durrett and K-
lein, 2013; Bj?orkelund and Kuhn, 2014) can also
benefit from this idea. While our basic idea of two-
level selection might appear similar to the coarse-
to-fine inference architectures (Felzenszwalb and
McAllester, 2007; Weiss and Taskar, 2010), the
details differ significantly. Importantly, our prun-
ing and scoring functions operate sequentially at
2115
each greedy search step, whereas in the cascades
approach, the second level function makes its pre-
diction only when the first level decision-making
is done.
Summary of Contributions. The main contribu-
tions of our work are as follows. First, we moti-
vate and introduce the Prune-and-Score approach
to search-based coreference resolution. Second,
we identify a decomposition of the overall loss
of the Prune-and-Score approach into the pruning
loss and the scoring loss, and reduce the problem
of learning these two functions to rank learning,
which allows us to leverage powerful and efficien-
t off-the-shelf rank learners. Third, we evaluate
our approach on OntoNotes, ACE, and MUC da-
ta, and show that it compares favorably to sever-
al state-of-the-art approaches as well as a greedy
search-based approach that uses a single scoring
function.
The remainder of the paper proceeds as follows.
In Section 2, we dicuss the related work. We intro-
duce our problem setup in Section 3 and then de-
scribe our Prune-and-Score approach in Section 4.
We explain our approaches for learning the prun-
ing and scoring functions in Section 5. Section 6
presents our experimental results followed by the
conclusions in Section 7.
2 Related Work
The work on learning-based coreference resolu-
tion can be broadly classified into three types.
First, the pair-wise classifier approaches learn a
classifier on mention pairs (edges) (Soon et al.,
2001; Ng and Cardie, 2002; Bengtson and Roth,
2008), and perform some form of approximate de-
coding or post-processing using the pair-wise s-
cores to make predictions. However, the pair-wise
classifier approach suffers from several drawback-
s including class imbalance (fewer positive edges
compared to negative edges) and not being able to
leverage the global structure (instead making in-
dependent local decisions).
Second, the global approaches such as Struc-
tured SVMs and Conditional Random Fields
(CRFs) learn a cost function to score a potential
clustering output for a given input set of men-
tions (Mccallum and Wellner, 2003; Finley and
Joachims, 2005; Culotta et al., 2007; Yu and
Joachims, 2009; Haghighi and Klein, 2010; Wick
et al., 2011; Wick et al., 2012; Fernandes et al.,
2012). These methods address some of the prob-
lems with pair-wise classifiers, however, they suf-
fer from the intractability of ?Argmin? inference
(finding the least cost clustering output among ex-
ponential possibilities) that is encountered during
both training and testing. As a result, they resort to
approximate inference algorithms (e.g., MCMC,
loopy belief propagation), which can suffer from
local optima.
Third, the incremental approaches construct the
clustering output incrementally by processing the
mentions in some order (Daum?e III, 2006; De-
nis and Baldridge, 2008; Rahman and Ng, 2011b;
Stoyanov and Eisner, 2012; Chang et al., 2013;
Durrett et al., 2013; Durrett and Klein, 2013).
These methods learn a scoring function to guide
the decision-making process and differ in the form
of the scoring function (e.g., mention pair, cluster-
mention or cluster-cluster pair) and how it is being
learned. They have shown great success and are
very efficient. Indeed, several of the approach-
es that have achieved state-of-the-art results on
OntoNotes fall under this category (Chang et al.,
2013; Durrett et al., 2013; Durrett and Klein,
2013; Bj?orkelund and Kuhn, 2014). However,
their efficiency requirement leads to a highly non-
realizable learning problem. Our Prune-and-Score
approach is complementary to these methods, as
we show that having a pruning function (or a set
of learned pruning rules) makes the learning prob-
lem easier and can improve over the performance
of scoring-only approaches. Also, the models in
(Chang et al., 2013; Durrett et al., 2013) try to
leverage cluster-level information implicitly (vi-
a latent antecedents) from mention-pair features,
whereas our model explicitly leverages the cluster
level information.
Coreference resolution systems can benefit
by incorporating the world knowledge including
rules, constraints, and additional information from
external knowledge bases (Lee et al., 2013; Rah-
man and Ng, 2011a; Ratinov and Roth, 2012;
Chang et al., 2013; Zheng et al., 2013; Hajishirzi
et al., 2013). Our work is orthogonal to this line
of work, but domain constraints and rules can be
incorporated into our model as done in (Chang et
al., 2013).
3 Problem Setup
Coreference resolution is a structured pre-
diction problem where the set of mentions
m
1
,m
2
, ? ? ? ,m
D
extracted from a document cor-
2116
reponds to a structured input x and the structured
output y corresponds to a partition of the men-
tions into a set of clusters C
1
, C
2
, ? ? ? , C
k
. Each
mention m
i
belongs to exactly one of the clusters
C
j
. We are provided with a training set of input-
output pairs drawn from an unknown distribution
D, and the goal is to return a function/predictor
from inputs to outputs. The learned predictor
is evaluated against a non-negative loss function
L : X ?Y?Y 7? <
+
, L(x, y
?
, y) is the loss asso-
ciated with predicting incorrect output y
?
for input
x when the true output is y (e.g., B-Cubed Score).
In this work, we formulate the coreference
resolution problem in a search-based framework.
There are three key elements in this framework:
1) the Search space S
p
whose states correspond
to partial clustering outputs; 2) the Action prun-
ing function F
prune
that is used to prune irrelevant
actions at each state; and 3) the Action scoring
function F
score
that is used to construct a com-
plete clustering output by selecting actions from
those that are left after pruning. S
p
is a 3-tuple
?I, A, T ?, where I is the initial state function, A
gives the set of possible actions in a given state,
and T is a predicate which is true for terminal s-
tates. In our case, s
0
= I(x) corresponds to a s-
tate where every mention is unresolved, and A(s
i
)
consists of actions to place the next mention m
i+1
in each cluster in s
i
or a NEW action which creates
a new cluster for it. Terminal nodes correspond to
states with all mentions resolved.
We focus on greedy search. The decision pro-
cess for constructing an output corresponds to s-
electing a sequence of actions leading from the
initial state to a terminal state using both F
prune
and F
score
, which are parameterized functions
over state-action pairs (F
prune
(?
1
(s, a)) ? < and
F
score
(?
2
(s, a)) ? <), where ?
1
and ?
2
stand for
feature functions. We want to learn the parameters
of both F
prune
and F
score
such that the predicted
outputs on unseen inputs have low expected loss.
4 Greedy Prune-and-Score Approach
Our greedy Prune-and-Score approach for coref-
erence resolution is parameterized by a pruning
function F
prune
: S ? A 7? <, a scoring func-
tion F
score
: S ? A 7? <, and a pruning param-
eter b ? [1, A
max
], where A
max
is the maximum
number of actions at any state s ? S . Given a
set of input mentions m
1
,m
2
, ? ? ? ,m
D
extracted
from a document (input x), and a pruning param-
Algorithm 1 Greedy Prune-and-Score Resolver
Input: x = set of mentions m
1
,m
2
, ? ? ? ,m
D
from
a document D, ?I, A, T ? = Search space defini-
tion, F
prune
= learned pruning function, b = prun-
ing parameter, F
score
= learned scoring function
1: s? I(x) // initial state
2: while not T (s) do
3: A
?
? Top b actions from A(s) according to
F
prune
// prune
4: a
p
? argmax
a?A
?
F
score
(s, a) // score
5: s? Apply a
p
on s
6: end while
7: return coreference output corresponding to s
eter b, our Prune-and-Score approach makes pre-
dictions as follows. The search starts at the ini-
tial state s
0
= I(x) (see Algorithm 1). At each
non-terminal state s, the pruning function F
prune
retains only the top b actions (A
?
) from A(s) (Step
3), and the scoring function F
score
picks the best
scoring action a
p
? A
?
(Step 4) to reach the next
state. When a terminal state is reached its con-
tents are returned as the prediction. Figure 1 illus-
trates the decision-making process of our Prune-
and-Score approach for an example state.
We now formalize the learning objective of our
Prune-and-Score approach. Let y? be the predicted
coreference output for a coreference input-output
pair (x, y
?
). The expected loss of the greedy
Prune-and-Score approach E(F
prune
,F
score
) for a
given pruning function F
prune
and scoring func-
tion F
score
can be defined as follows.
E(F
prune
,F
score
) = E
(x,y
?
)?D
L (x, y?, y
?
)
Our goal is to learn an optimal pair of pruning
and scoring functions
(
F
o
prune
,F
o
score
)
that min-
imizes the expected loss of the Prune-and-Score
approach. The behavior of our Prune-and-Score
approach depends on the pruning parameter b,
which dictates the workload of pruning and scor-
ing functions. For small values of b (aggressive
pruning), pruning function learning may be harder,
but scoring function learning will be easier. Simi-
larly, for large values of b (conservative pruning),
scoring function learning becomes hard, but prun-
ing function learning is easy. Therefore, we would
expect beneficial behavior if pruning function can
aggressively prune (small values of b) with little
loss in accuracy. It is interesting to note that our
Prune-and-Score approach degenerates to existing
incremental approaches that use only the scoring
function for search (Daum?e III, 2006; Rahman and
2117
(a) Text with input set of mentions
Ramallah ( West Bank
2
)
1
10-15 ( AFP
3
) - Eyewitnesses
4
reported that Palestinians
5
demonstrated today Sunday in the West Bank
6
against the Sharm el-Sheikh
7
summit to be
held in Egypt
8
tomorrow Monday. In Ramallah
9
, around 500 people
10
took to the town
11
?s
streets chanting slogans denouncing the summit ...
(b) Illustration of Prune-and-Score approach
1m
9m 3m 4m6m2m
1C
1a 2a 3a 4a 5a 6a 7a
5m
10m 7m 11m
2C 3C 4C 5C 6C
State: s = {C
1
, C
2
, C
3
, C
4
, C
5
, C
6
} Actions: A(s) = {a
1
, a
2
, a
3
, a
4
, a
5
, a
6
, a
7
}Pruning step:
Scoring step:
2.5             2.2               1.9                1.5              1.4              0.7              0.4
4.5             3.1              2.6
2a 1a 7a 5a 6a 3a 4a
1a 2a 7aA?(s) = {a2, a1, a7}
b = 3
Decision: a
1
is the best action for state s
F
prune
values
F
score
values
Figure 1: Illustration of Prune-and-Score approach. (a) Text with input set of mentions. Mentions are highlighted
and numbered. (b) Illustration of decision-making process for mention m
11
. The partial clustering output corre-
sponding to the current state s consists of six clusters denoted by C
1
, C
2
, ? ? ? , C
6
. Highlighted circles correspond
to the clusters. Edges from mention m
11
to each of the six clusters and to itself stand for the set of possible actions
A(s) in state s, and are denoted by a
1
, a
2
, ? ? ? , a
7
. The pruning function F
prune
scores all the actions in A(s) and
only keeps the top 3 actions A
?
= {a
2
, a
1
, a
7
} as specified by the pruning parameter b. The scoring function picks
the best scoring action a
1
? A
?
as the final decision, and mention m
11
is merged with cluster C
1
.
Ng, 2011b) when b =?. Additionally, for b = 1,
our pruning function coincides with the scoring
function.
Analysis of Representational Power. The fol-
lowing proposition formalizes the intuition that t-
wo functions are strictly better than one in expres-
sive power. See Appendix for the proof.
Proposition 1. Let F
prune
and F
score
be func-
tions from the same function space. Then for all
learning problems, min
F
score
E(F
score
,F
score
) ?
min
(F
prune
,F
score
)
E(F
prune
,F
score
). More-
over there exist learning problems for which
min
F
score
E(F
score
,F
score
) can be arbitrarily
worse than min
(F
prune
,F
score
)
E(F
prune
,F
score
).
5 Learning Algorithms
In general, learning the optimal
(
F
o
prune
,F
o
score
)
pair can be intractable due to their potential inter-
dependence. Specifically, when learning F
prune
in the worst case there can be ambiguity about
which of the non-optimal actions to retain, and
for only some of those an effective F
score
can be
found. However, we observe a loss decomposi-
tion in terms of the individual losses due to F
prune
and F
score
, and develop a stage-wise learning ap-
proach that first learns F
prune
and then learns a
corresponding F
score
.
5.1 Loss Decomposition
The overall loss of the Prune-and-Score approach
E (F
prune
,F
score
) can be decomposed into prun-
ing loss 
prune
, the loss due to F
prune
not be-
ing able to retain the optimal terminal state in
the search space; and scoring loss 
score|F
prune
,
the additional loss due to F
score
not guiding the
greedy search to the best terminal state after prun-
ing using F
prune
. Below, we will define these
losses more formally.
Pruning Loss is defined as the expected loss of
the Prune-and-Score approach when we perform
greedy search with F
prune
and F
?
score
, the opti-
mal scoring function. A scoring function is said to
be optimal if at every state s in the search space
2118
Sp
, and for any set of remaining actions A(s), it
can score each action a ? A(s) such that greedy
search can reach the best terminal state (as eval-
uated by task loss function L) that is reachable
from s through A(s). Unfortunately, computing
the optimal scoring function is highly intractable
for the non-decomposable loss functions that are
employed in coreference resolution (e.g., B-Cubed
F1). The main difficulty is that the decision at any
one state has interdependencies with future deci-
sions (see Section 5.5 in (Daum?e III, 2006) for
more details). So we need to resort to some form
of approximate optimal scoring function that ex-
hibits the intended behavior. This is very similar
to the dynamic oracle concept developed for de-
pendency parsing (Goldberg and Nivre, 2013).
Let y
?
prune
be the coreference output corre-
sponding to the terminal state reached from input
x by Prune-and-Score approach when performing
search using F
prune
and F
?
score
. Then the pruning
loss can be expressed as follows.

prune
= E
(x,y
?
)?D
L
(
x, y
?
prune
, y
?
)
Scoring Loss is defined as the additional loss due
to F
score
not guiding the greedy search to the best
terminal state reachable via the pruning function
F
score
(i.e., y
?
prune
). Let y? be the coreference out-
put corresponding to the terminal state reached by
Prune-and-Score approach by performing search
with F
prune
and F
score
for an input x. Then the
scoring loss can be expressed as follows:

score|F
prune
= E
(x,y
?
)?D
L (x, y?, y
?
)? L
(
x, y
?
prune
, y
?
)
The overall loss decomposition of our Prune-and-
Score approach can be expressed as follows.
E (F
prune
,F
score
)
= E
(x,y
?
)?D
L
(
x, y
?
prune
, y
?
)
? ?? ?

prune
+
E
(x,y
?
)?D
L (x, y?, y
?
)? L
(
x, y
?
prune
, y
?
)
? ?? ?

score|F
prune
5.2 Stage-wise Learning
The loss decomposition motivates a learning ap-
proach that targets minimizing the errors of prun-
ing and scoring functions independently. In par-
ticular, we optimize the overall loss of the Prune-
and-Score approach in a stage-wise manner. We
first train a pruning function
?
F
prune
to optimize
the pruning loss component 
prune
and then train
a scoring function
?
F
score
to optimize the scoring
loss 
score|
?
F
prune
conditioned on
?
F
prune
.
?
F
prune
? argmin
F
prune
?F
p

prune
?
F
score
? argmin
F
score
?F
s

score|
?
F
prune
Note that this approach is myopic in the sense that
?
F
prune
is learned without considering the impli-
cations for learning
?
F
score
. Below, we first de-
scribe our approach for pruning function learning,
and then explain our scoring function learning al-
gorithm.
5.3 Pruning Function Learning
In our greedy Prune-and-Score approach, the role
of the pruning function F
prune
is to prune away
irrelevant actions (as specified by the pruning pa-
rameter b) at each search step. More specifically,
we want F
prune
to score actions A(s) at each s-
tate s such that the optimal action a
?
? A(s) is
ranked within the top b actions to minimize 
prune
.
For this, we assume that for any training input-
output pair (x, y
?
) there exists a unique action se-
quence, or solution path (initial state to terminal
state), for producing y
?
from x. More formally, let
(s
?
0
, a
?
0
), (s
?
1
, a
?
1
), ? ? ? , (s
?
D
,?) correspond to the
sequence of state-action pairs along this solution
path, where s
?
0
is the initial state and s
?
D
is the ter-
minal state. The goal is to learn the parameters of
F
prune
such that at each state s
?
i
, a
?
i
? A(s
?
i
) is
ranked among the top b actions.
While we can employ an online-LaSO style ap-
proach (III and Marcu, 2005; Xu et al., 2009) to
learn the parameters of the pruning function, it is
quite inefficient, as it must regenerate the same
search trajectory again and again until it learn-
s to make the right decision. Additionally, this
approach limits applicability of the off-the-shelf
learners to learn the parameters of F
prune
. To
overcome these drawbacks, we apply offline train-
ing.
Reduction to Rank Learning. We reduce the
pruning function learning to a rank learning prob-
lem. This allows us to leverage powerful and effi-
cient off-the-shelf rank-learners (Liu, 2009). The
reduction is as follows. At each state s
?
i
on the so-
lution path of a training example (x, y
?
), we create
an example by labeling optimal action a
?
i
? A(s
?
i
)
as the only relevant action, and then try to learn
2119
a ranking function that can rank actions such that
the relevant action a
?
i
is in the top b actions, where
b is the input pruning paramter. In other word-
s, we have a rank learning problem, where the
learner?s goal is to optimize the Precision at Top-
b. The training approach creates such an exam-
ple for each state s in the solution path. The set
of aggregate imitation examples collected over al-
l the training data is then given to a rank learner
(e.g., LambdaMART (Burges, 2010)) to learn the
parameters of F
prune
by optimizing the Precision
at Top-b loss. See appendix for the pseudocode.
If we can learn a function F
prune
that is con-
sistent with these imitation examples, then the
learned pruning function is guaranteed to keep
the solution path within the pruned space for al-
l the training examples. We can also employ
more advanced imitation learning algorithms in-
cluding DAgger (Ross et al., 2011) and SEARN
(Hal Daum?e III et al., 2009) if we are provid-
ed with an (approximate) optimal scoring function
F
?
score
that can pick optimal actions at states that
are not in the solution path (i.e., off-trajectory s-
tates).
5.4 Scoring Function Learning
Given a learned pruning function F
prune
, we want
to learn a scoring function that can pick the best
action from the b actions that remain after prun-
ing at each state. We formulate this problem in the
framework of imitation learning (Khardon, 1999).
More formally, let (s?
0
, a
?
0
), (s?
1
, a
?
1
), ? ? ? , (s?
?
D
,?)
correspond to the sequence of state-action pairs
along the greedy trajectory obtained by running
the Prune-and-Score approach with F
prune
and
F
?
score
, the optimal scoring function, on a train-
ing example (x, y
?
), where s?
?
D
is the best terminal
state in the pruned space. The goal of our imita-
tion training approach is to learn the parameters
of F
score
such that at each state s?
i
, a
?
i
? A
?
is
ranked higher than all other actions in A
?
, where
A
?
? A(s?
i
) is the set of b actions that remain after
pruning.
It is important to note that the distribution of
states in the pruned space due to F
prune
on the
testing data may be somewhat different from those
on training data. Therefore, we train our scoring
function via cross-validation by training the scor-
ing function on heldout data that was not used to
train the pruning function. This methodology is
commonly employed in Re-Ranking and Stacking
approaches (Collins, 2000; Cohen and de Carval-
ho, 2005).
Our scoring function learning procedure uses
cross validation and consists of the following four
steps. First, we divide the training data D in-
to k folds. Second, we learn k different pruners,
where each pruning function F
i
prune
is learned us-
ing the data from all the folds excluding the i
th
fold. Third, we generate ranking examples for
scoring function learning as described above us-
ing each pruning function F
i
prune
on the data it
was not trained on. Finally, we give the aggregate
set of ranking examples R to a rank learner (e.g.,
SVM-Rank or LambdaMART) to learn the scoring
function F
score
. See appendix for the pseudocode.
Approximate Optimal Scoring Function. If the
learned pruning function is not consistent with the
training data, we will encounter states s?
i
that are
not on the target path, and we will need some su-
pervision for learning in those cases. As discussed
before in Section 5.1, computing an optimal scor-
ing functionF
?
score
is intractable for combinatorial
loss functions that are used for coreference resolu-
tion. So we employ an approximate function from
existing work that is amenable to evaluate partial
outputs (Daum?e III, 2006). It is a variant of the
ACE scoring function that removes the bipartite
matching step from the ACE metric. Moreover
this score is computed only on the partial coref-
erence output corresponding to the ?after state?
s
?
resulting from taking action a in state s, i.e.,
F
?
score
(s, a) = F
?
score
(s
?
). To further simplify the
computation, we give uniform weight to the three
types of costs: 1) Credit for correct linking, 2)
Penalty for incorrect linking, and 3) Penalty for
missing links. Intuitively, this is similar to the
correct-link count computed only on a subgraph.
We direct the reader to (Daum?e III, 2006) for more
details (see Section 5.5).
6 Experiments and Results
In this section, we evaluate our greedy Prune-
and-Score approach on three benchmark corpora
? OntoNotes 5.0 (Pradhan et al., 2012), ACE 2004
(NIST, 2004), and MUC6 (MUC6, 1995) ? and
compare it against the state-of-the-art approaches
for coreference resolution. For OntoNotes data,
we report the results on both gold mentions and
predicted mentions. We also report the results on
gold mentions for ACE 2004 and MUC6 data.
2120
6.1 Experimental Setup
Datasets. For OntoNotes corpus, we employ the
official split for training, validation, and testing.
There are 2802 documents in the training set; 343
documents in the validation set; and 345 docu-
ments in the testing set. The ACE 2004 corpus
contains 443 documents. We follow the (Culot-
ta et al., 2007; Bengtson and Roth, 2008) split
in our experiments by employing 268 documents
for training, 68 documents for validation, and 107
documents (ACE2004-CULOTTA-TEST) for test-
ing. We also evaluate our system on the 128
newswire documents in ACE 2004 corpus for a
fair comparison with the state-of-the-art. The
MUC6 corpus containts 255 documents. We em-
ploy the official test set of 30 documents (MUC6-
TEST) for testing purposes. From the remaining
225 documents, which includes 195 official train-
ing documents and 30 dry-run test documents, we
randomly pick 30 documents for validation, and
use the remaining ones for training.
Evaluation Metrics. We compute three most pop-
ular performance metrics for coreference resolu-
tion: MUC (Vilain et al., 1995), B-Cubed (Bag-
ga and Baldwin, 1998), and Entity-based CEAF
(CEAF
?4
) (Luo, 2005). As it is commonly done
in CoNLL shared tasks (Pradhan et al., 2012), we
employ the average F1 score (CoNLL F1) of these
three metrics for comparison purposes. We evalu-
ate all the results using the updated version
1
(7.0)
of the coreference scorer.
Features. We built
2
our coreference resolver
based on the Easy-first coreference system (Stoy-
anov and Eisner, 2012), which is derived from the
Reconcile system (Stoyanov et al., 2010). We es-
sentially employ the same features as in the Easy-
first system. However, we provide some high-
level details that are necessary for subsequent dis-
cussion. Recall that our features ?(s, a) for both
pruning and scoring functions are defined over
state-action pairs, where each state s consists of
a set of clusters and an action a corresponds to
merging an unprocessed mention m with a clus-
ter C in state s or create one for itself. Therefore,
?(s, a) defines features over cluster-mention pairs
(C,m). Our feature vector consists of three part-
s: a) mention pair features; b) entity pair features;
and c) a single indicator feature to represent NEW
1
http://code.google.com/p/reference-coreference-scorers/
2
See http://research.engr.oregonstate.edu/dral/ for our
software.
action (i.e., mention m starts its own cluster). For
mention pair features, we average the pair-wise
features over all links between m and every men-
tion m
c
in cluster C (often referred to as average-
link). Note that, we cannot employ the best-link
feature representation because we perform offline
training and do not have weights for scoring the
links. For entity pair features, we treat mention
m as a singleton entity and compute features by
pairing it with the entity represented by cluster C
(exactly as in the Easy-first system). The indica-
tor feature will be 1 for the NEW action and 0 for
all other actions.We have a total of 140 features:
90 mention pair features; 49 entity pair features;
and one NEW indicator feature. We believe that
our approach can benefit from employing features
of the mention for the NEW action (Rahman and
Ng, 2011b; Durrett and Klein, 2013). However,
we were constrained by the Reconcile system and
could not leverage these features for the NEW ac-
tion.
Base Rank-Learner. Our pruning and scoring
function learning algorithms need a base rank-
learner. We employ LambdaMART (Burges,
2010), a state-of-the art rank learner from the
RankLib
3
library. LambdaMART is a variant of
boosted regression trees. We use a learning rate
of 0.1, specify the maximum number of boost-
ing iterations (or trees) as 1000 noting that its ac-
tual value is automatically decided based on the
validation set, and tune the number of leaves per
tree based on the validation data. Once we fix
the hyper-parameters of LambdaMART, we train
the final model on all of the training data. Lamb-
daMART uses an internal train/validation split of
the input ranking examples to decide when to stop
the boosting iterations. We fixed this ratio to 0.8
noting that the performance is not sensitive to this
parameter. For scoring function learning, we used
5 folds for the cross-validation training.
Pruning Parameter b. The hyper-parameter b
controls the amount of pruning in our Prune-and-
Score approach. We perform experiments with d-
ifferent values of b and pick the best value based
on the performance on the validation set.
Singleton Mention Filter for OntoNotes Cor-
pus. We employ the Illinois-Coref system (Chang
et al., 2012) to extract system mentions for our
OntoNotes experiments, and observe that the num-
3
http://sourceforge.net/p/lemur/wiki/RankLib/
2121
ber of predicted mentions is thrice the number of
gold mentions. Since the training data provides the
clustering supervision for only gold mentions, it is
not clear how to train with the system mention-
s that are not part of gold mentions. A common
way of dealing with this problem is to treat all the
extra system mentions as singleton clusters (Dur-
rett and Klein, 2013; Chang et al., 2013). Howev-
er, this solution most likely will not work with our
current feature representation (i.e., NEW action is
represented as a single indicator feature). Recall
that to predict these extra system mentions as s-
ingleton clusters with our incremental clustering
approach, the learned model should first predic-
t a NEW action while processing these mention-
s to form a temporary singleton cluster, and then
refrain from merging any of the subsequent men-
tions with that cluster so that it becomes a single-
ton cluster in the final clustering output. Howev-
er, in OntoNotes corpus, the training data does not
include singleton clusters for the gold mentions.
Therefore, only the large number (57%) of system
mentions that are not part of gold mentions will
constitute the set of singleton clusters. This leads
to a highly imbalanced learning problem because
our model needs to learn (the weight of the sin-
gle indicator feature) to predict NEW as the best
action for a large set of mentions, which will bias
our model to predict large number of NEW actions
during testing. As a result, we will generate many
singleton clusters, which will hurt the recall of the
mention detection after post-processing. There-
fore, we aim to learn a singleton mention filter
that will be used as a pre-processor before training
and testing to overcome this problem. We would
like to point out that our filter is complementary to
other solutions (e.g., employing features that can
discriminate a given mention to be anaphoric or
not in place of our single indicator feature, or us-
ing a customized loss to weight our ranking exam-
ples for cost-sensitive training)(Durrett and Klein,
2013).
Filter Learning. The singleton mention filter is
a classifier that will label a given mention as ?s-
ingleton? or not. We represent each mention m
in a document by averaging the mention-pair fea-
tures ?(m,m
?
) of the k-most similar mentions
(obtained by ranking all other mentions m
?
in the
document with a learned ranking functionR given
m) and then learn a decision-tree classifier by opti-
mizing the F1 loss. We learn the mention-ranking
function R by optimizing the recall of positive
pairs for a given k, and employ LambdaMART as
our base ranker. The hyper-parameters are tuned
based on the performance on the validation set.
6.2 Results
We first describe the results of the learned single-
ton mention filter, and then the performance of
our Prune-and-Score approach with and without
the filter. Next, we compare the results of our ap-
proach with several state-of-the-art approaches for
coreference resolution.
Singleton Mention Filter Results. Table 1 shows
the performance of the learned singleton mention
filter with k = 2 noting that the results are ro-
bust for all values of k ? 2. As we can see, the
learned filter improves the precision of the men-
tion detection with only small loss in the recall of
gold mentions.
Mention Detection Accuracy
P R F1
Before- 43.18% 86.99% 57.71%
filtering (16664/38596) (16664/19156)
After- 79.02% 80.98% 79.97%
filtering (15516/19640) (15516/19156)
Table 1: Performance of the singleton mention filter on
the OntoNotes 5.0 development set. The numerators of
the fractions in the brackets show the exact numbers of
mentions that are matched with the gold mentions.
Prune-and-Score Results. Table 2 shows the per-
formance of Prune-and-Score approach with and
without the singleton mention filter. We can see
that the results with filter are much better than the
corresponding results without the filter. These re-
sults show that our approach can benefit from hav-
ing a good singleton mention filter.
Filter settings MUC B
3
CEAF
?4
CoNLL
OntoNotes 5.0 Dev Set w. Predict Ment.
O.S. (w.o. Filter) 66.73 53.40 44.23 54.79
P&S (w.o. Filter) 65.93 52.96 50.24 56.38
P&S (w. Filter) 71.18 58.87 57.88 62.64
Table 2: Performance of Prune-and-Score approach
with and without the singleton mention filter, and Only-
Score approach without the filter.
Table 3 shows the performance of different con-
figurations of our Prune-and-Score approach. As
we can see, Prune-and-Score gives better results
than the configuration where we employ only the
scoring function (b = ?) for small values of b.
2122
MUC B
3
CEAF
?4
CoNLL
P R F1 P R F1 P R F1 Avg-F1
a. Results on OntoNotes 5.0 Test Set with Predicted Mentions
Prune-and-Score 81.03 66.16 72.84 66.90 51.10 57.94 68.75 44.34 53.91 61.56
Only-Scoring 75.95 61.53 67.98 63.94 47.37 54.42 58.54 49.76 53.79 58.73
HOTCoref 67.46 74.3 70.72 54.96 62.71 58.58 52.27 59.4 55.61 61.63
CPL
3
M - - 69.48 - - 57.44 - - 53.07 60.00
Berkeley 74.89 67.17 70.82 64.26 53.09 58.14 58.12 52.67 55.27 61.41
Fernandes et al., 2012 75.91 65.83 70.51 65.19 51.55 57.58 57.28 50.82 53.86 60.65
Stanford 65.31 64.11 64.71 56.54 48.58 52.26 46.67 52.29 49.32 55.43
b. Results on OntoNotes 5.0 Test Set with Gold Mentions
Prune-and-Score 88.10 85.85 86.96 76.82 76.16 76.49 80.90 74.06 77.33 80.26
Only-Scoring 86.96 84.52 85.73 74.51 74.25 74.38 79.04 70.67 74.62 78.24
CPL
3
M - - 84.80 - - 78.74 - - 68.75 77.43
Berkeley 85.73 89.26 87.46 78.23 75.11 76.63 82.89 70.86 76.40 80.16
Stanford 89.94 78.17 83.64 81.75 68.95 74.81 73.97 61.20 66.98 75.14
c. Results on ACE2004 Culotta Test Set with Gold Mentions
Prune-and-Score 85.57 72.68 78.60 90.09 77.02 83.04 74.64 86.02 79.42 80.35
Only-Scoring 82.75 69.25 75.40 88.54 74.22 80.75 73.69 85.22 78.58 78.24
CPL
3
M - - 78.29 - - 82.20 - - 79.26 79.91
Stanford 82.91 69.90 75.85 89.14 74.05 80.90 75.67 77.45 76.55 77.77
d. Results on ACE2004 Newswire with Gold Mentions
Prune-and-Score 89.72 75.72 82.13 90.89 76.15 82.87 72.43 86.83 78.69 81.23
Only-Scoring 86.92 76.49 81.37 88.10 75.83 81.51 73.15 84.31 78.05 80.31
Easy-first - - 80.1 - - 81.8 - - - -
Stanford 84.75 75.34 79.77 87.50 74.59 80.53 73.32 81.49 77.19 79.16
e. Results on MUC6 Test Set with Gold Mentions
Prune-and-Score 89.53 82.75 86.01 86.48 76.18 81.00 60.74 80.33 68.68 78.56
Only-Scoring 86.77 80.96 83.76 81.72 72.99 77.11 57.56 75.38 64.91 75.26
Easy-first - - 88.2 - - 77.5 - - - -
Stanford 91.19 69.54 78.91 91.07 63.39 74.75 62.43 69.62 65.83 73.16
Table 4: Comparison of Prune-and-Score with state-of-the-art approaches. Metric values reflect version 7 of
CoNLL scorer.
The performance is clearly better than the degen-
erate case (b = ?) over a wide range of b values,
suggesting that it is not necessary to carefully tune
the parameter b.
Pruning param. b MUC B
3
CEAF
?4
CoNLL
OntoNotes 5.0 Dev Set w. Predict Ment.
2 69.12 56.80 56.30 60.74
3 70.50 57.89 57.24 61.88
4 71.00 58.65 57.41 62.35
5 71.18 58.87 57.88 62.64
6 70.93 58.66 57.85 62.48
8 70.12 58.13 57.37 61.87
10 70.24 58.34 56.27 61.61
20 67.97 57.73 56.63 60.78
? 67.03 56.31 55.56 59.63
Table 3: Performance of Prune-and-Score approach
with different values of the pruning parameter b. For
b =?, Prune-and-Score becomes an Only-Scoring al-
gorithm.
Comparison to State-of-the-Art. Table 4
shows the results of our Prune-and-Score ap-
proach compared with the following state-of-the-
art coreference resolution approaches: HOTCoref
system (Bj?orkelund and Kuhn, 2014); Berkeley
system with the FINAL feature set (Durrett and K-
lein, 2013); CPL
3
M system (Chang et al., 2013);
Stanford system (Lee et al., 2013); Easy-first sys-
tem (Stoyanov and Eisner, 2012); and Fernan-
des et al., 2012 (Fernandes et al., 2012). On-
ly Scoring is the special case of our Prune-and-
Score approach where we employ only the scoring
function. This corresponds to existing incremen-
tal approaches (Daum?e III, 2006; Rahman and Ng,
2011b). We report the best published results for
CPL
3
M system, Easy-first, and Fernandes et al.,
2012. We ran the publicly available software to
generate the results for Berkeley and Stanford sys-
tems with the updated CoNLL scorer. We include
the results of Prune-and-Score for best b on the de-
velopment set with singleton mention filter for the
comparison. In Table 4, ?-? indicates that we could
not find published results for those cases. We see
2123
that results of the Prune-and-Score approach are
comparable to or better than the state-of-the-art in-
cluding Only-Scoring.
7 Conclusions and Future Work
We introduced the Prune-and-Score approach for
greedy coreference resolution whose main idea
is to learn a pruning function along with a scor-
ing function to effectively guide the search. We
showed that our approach improves over the meth-
ods that only learn a scoring function, and gives
comparable or better results than several state-of-
the-art coreference resolution systems.
Our Prune-and-Score approach is a particular
instantiation of the general idea of learning nearly-
sound constraints for pruning, and leveraging the
learned constraints to learn improved heuristic
functions for guiding the search (See (Chen et
al., 2014) for another instantiation of this idea for
multi-object tracking in videos). Therefore, oth-
er coreference resolution systems (Chang et al.,
2013; Durrett and Klein, 2013; Bj?orkelund and
Kuhn, 2014) can also benefit from this idea. One
way to further improve the peformance of our
approach is to perform a search in the Limited
Discrepancy Search (LDS) space (Doppa et al.,
2014b) using the learned functions.
Future work should apply this general idea to
other natural language processing tasks including
dependency parsing (Nivre et al., 2007) and in-
formation extraction (Li et al., 2013). We would
expect more beneficial behavior with the prun-
ing constraints for problems with large action sets
(e.g., labeled dependency parsing). It would be in-
teresting and useful to generalize this approach to
search spaces where there are multiple target paths
from the initial state to the terminal state, e.g., as
in the Easy-first framework.
Acknowledgments
Authors would like to thank Veselin Stoyanov
(JHU) for answering several questions related to
the Easy-first and Reconcile systems; Van Dang
(UMass, Amherst) for technical discussions relat-
ed to the RankLib library; Kai-Wei Chang (UIUC)
for the help related to the Illinois-Coref mention
extractor; and Greg Durrett (UC Berkeley) for his
help with the Berkeley system. This work was
supported in part by NSF grants IIS 1219258, I-
IS 1018490 and in part by the Defense Advanced
Research Projects Agency (DARPA) and the Air
Force Research Laboratory (AFRL) under Con-
tract No. FA8750-13-2-0033. Any opinions,
findings and conclusions or recommendations ex-
pressed in this material are those of the author(s)
and do not necessarily reflect the views of the NS-
F, the DARPA, the Air Force Research Laboratory
(AFRL), or the US government.
References
Amit Bagga and Breck Baldwin. 1998. Algorithms for
scoring coreference chains. In In The First Interna-
tional Conference on Language Resources and Eval-
uation Workshop on Linguistics Coreference, pages
563?566.
Eric Bengtson and Dan Roth. 2008. Understanding the
value of features for coreference resolution. In Pro-
ceedings of Empirical Methods in Natural Language
Processing (EMNLP), pages 294?303.
Anders Bj?orkelund and Jonas Kuhn. 2014. Learn-
ing structured perceptrons for coreference resolution
with latent antecedents and non-local features. In
Proceedings of the 52nd Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), pages 47?57, Baltimore, Maryland,
June. Association for Computational Linguistics.
Christopher Burges. 2010. From RankNet to Lamb-
daRank to LambdaMART: An overview. Microsoft
Technical Report, (MSR-TR-2010).
Kai-Wei Chang, Rajhans Samdani, Alla Rozovskaya,
Mark Sammons, and Dan Roth. 2012. Illinois-
Coref: The UI system in the CoNLL-2012 shared
task. In Joint Conference on EMNLP and CoNLL
- Shared Task, pages 113?117, Jeju Island, Korea,
July. Association for Computational Linguistics.
Kai-Wei Chang, Rajhans Samdani, and Dan Roth.
2013. A constrained latent variable model for
coreference resolution. In Proceedings of Em-
pirical Methods in Natural Language Processing
(EMNLP), pages 601?612.
Sheng Chen, Alan Fern, and Sinisa Todorovic. 2014.
Multi-object tracking via constrained sequential la-
beling. In To appear in Proceedings of IEEE Con-
ference on Computer Vision and Pattern Recognition
(CVPR).
William W. Cohen and Vitor Rocha de Carvalho. 2005.
Stacked sequential learning. In Proceedings of In-
ternational Joint Conference on Artificial Intelli-
gence (IJCAI), pages 671?676.
Michael Collins. 2000. Discriminative reranking for
natural language parsing. In Proceedings of Inter-
national Conference on Machine Learning (ICML),
pages 175?182.
2124
Aron Culotta, Michael L. Wick, and Andrew Mc-
Callum. 2007. First-order probabilistic models
for coreference resolution. In Proceedings of Hu-
man Language Technology Conference of the North
American Chapter of the Association of Computa-
tional Linguistics (HLT-NAACL), pages 81?88.
Hal Daum?e III. 2006. Practical Structured Learning
Techniques for Natural Language Processing. Ph.D.
thesis, University of Southern California, Los Ange-
les, CA.
Pascal Denis and Jason Baldridge. 2008. Specialized
models and ranking for coreference resolution. In
Proceedings of Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 660?669.
Janardhan Rao Doppa, Alan Fern, and Prasad Tadepal-
li. 2014a. HC-Search: A learning framework for
search-based structured prediction. Journal of Arti-
ficial Intelligence Research (JAIR), 50:369?407.
Janardhan Rao Doppa, Alan Fern, and Prasad Tade-
palli. 2014b. Structured prediction via output s-
pace search. Journal of Machine Learning Research
(JMLR), 15:1317?1350.
Janardhan Rao Doppa, Jun Yu, Chao Ma, Alan Fern,
and Prasad Tadepalli. 2014c. HC-Search for multi-
label prediction: An empirical study. In Proceed-
ings of AAAI Conference on Artificial Intelligence
(AAAI).
Greg Durrett and Dan Klein. 2013. Easy victories and
uphill battles in coreference resolution. In Proceed-
ings of the Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP), pages 1971?
1982.
Greg Durrett, David Leo Wright Hall, and Dan Klein.
2013. Decentralized entity-level modeling for coref-
erence resolution. In Proceedings of Association of
Computational Linguistics (ACL) Conference, pages
114?124.
Pedro F. Felzenszwalb and David A. McAllester. 2007.
The generalized A* architecture. Journal of Artifi-
cial Intelligence Research (JAIR), 29:153?190.
Eraldo Rezende Fernandes, C??cero Nogueira dos San-
tos, and Ruy Luiz Milidi?u. 2012. Latent structure
perceptron with feature induction for unrestricted
coreference resolution. International Conference on
Computational Natural Language Learning (CoNL-
L), pages 41?48.
Thomas Finley and Thorsten Joachims. 2005. Su-
pervised clustering with support vector machines.
In Proceedings of International Conference on Ma-
chine Learning (ICML), pages 217?224.
Yoav Goldberg and Joakim Nivre. 2013. Training
deterministic parsers with non-deterministic oracles.
Transactions of the Association for Computational
Linguistics, 1:403?414.
Aria Haghighi and Dan Klein. 2010. Coreference res-
olution in a modular, entity-centered model. In Pro-
ceedings of Human Language Technology Confer-
ence of the North American Chapter of the Associa-
tion of Computational Linguistics (HLT-NAACL).
Hannaneh Hajishirzi, Leila Zilles, Daniel S. Weld,
and Luke S. Zettlemoyer. 2013. Joint corefer-
ence resolution and named-entity linking with multi-
pass sieves. In Proceedings of Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP), pages 289?299.
Hal Daum?e III, John Langford, and Daniel Marcu.
2009. Search-based structured prediction. Machine
Learning Journal (MLJ), 75(3):297?325.
Hal Daum?e III and Daniel Marcu. 2005. Learning
as search optimization: Approximate large margin
methods for structured prediction. In ICML.
Roni Khardon. 1999. Learning to take actions. Ma-
chine Learning Journal (MLJ), 35(1):57?90.
Michael Lam, Janardhan Rao Doppa, Xu Hu, Sinisa
Todorovic, Thomas Dietterich, Abigail Reft, and
Marymegan Daly. 2013. Learning to detect basal
tubules of nematocysts in sem images. In ICCV
Workshop on Computer Vision for Accelerated Bio-
sciences (CVAB). IEEE.
Heeyoung Lee, Angel X. Chang, Yves Peirsman,
Nathanael Chambers, Mihai Surdeanu, and Dan Ju-
rafsky. 2013. Deterministic coreference resolu-
tion based on entity-centric, precision-ranked rules.
Computational Linguistics, 39(4):885?916.
Qi Li, Heng Ji, and Liang Huang. 2013. Joint event
extraction via structured prediction with global fea-
tures. In Proceedings of the 51st Annual Meeting
of the Association for Computational Linguistics (A-
CL), pages 73?82.
Tie-Yan Liu. 2009. Learning to rank for information
retrieval. Foundations and Trends in Information
Retrieval, 3(3):225?331.
Xiaoqiang Luo. 2005. On coreference resolution per-
formance metrics. In Proceedings of the Confer-
ence on Human Language Technology and Empir-
ical Methods in Natural Language Processing, HLT
?05, pages 25?32, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Andrew Mccallum and Ben Wellner. 2003. To-
ward conditional models of identity uncertainty with
application to proper noun coreference. In Pro-
ceedings of Neural Information Processing Systems
(NIPS), pages 905?912. MIT Press.
MUC6. 1995. Coreference task definition. In Pro-
ceedings of the Sixth Message Understanding Con-
ference (MUC-6), pages 335?344.
2125
Vincent Ng and Claire Cardie. 2002. Improving
machine learning approaches to coreference resolu-
tion. In Proceedings of Association of Computation-
al Linguistics (ACL) Conference, pages 104?111.
NIST. 2004. The ACE evaluation plan.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas
Chanev, G?ulsen Eryigit, Sandra K?ubler, Svetoslav
Marinov, and Erwin Marsi. 2007. Maltparser:
A language-independent system for data-driven de-
pendency parsing. Natural Language Engineering,
13(2):95?135.
Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,
Olga Uryupina, and Yuchen Zhang. 2012. Conll-
2012 shared task: Modeling multilingual unrestrict-
ed coreference in ontonotes. In Proceedings of the
Joint Conference on EMNLP and CoNLL: Shared
Task, pages 1?40.
Altaf Rahman and Vincent Ng. 2011a. Coreference
resolution with world knowledge. In Proceedings
of Association of Computational Linguistics (ACL)
Conference, pages 814?824.
Altaf Rahman and Vincent Ng. 2011b. Narrowing the
modeling gap: A cluster-ranking approach to coref-
erence resolution. Journal of Artificial Intelligence
Research (JAIR), 40:469?521.
Lev-Arie Ratinov and Dan Roth. 2012. Learning-
based multi-sieve co-reference resolution with
knowledge. In Proceedings of Empirical Methods
in Natural Language Processing (EMNLP) Confer-
ence, pages 1234?1244.
St?ephane Ross, Geoffrey J. Gordon, and Drew Bagnell.
2011. A reduction of imitation learning and struc-
tured prediction to no-regret online learning. Jour-
nal of Machine Learning Research - Proceedings
Track, 15:627?635.
Wee Meng Soon, Daniel Chung, Daniel Chung Yong
Lim, Yong Lim, and Hwee Tou Ng. 2001. A ma-
chine learning approach to coreference resolution of
noun phrases.
Veselin Stoyanov and Jason Eisner. 2012. Easy-first
coreference resolution. In Proceedings of Inter-
national Conference on Computational Linguistics
(COLING), pages 2519?2534.
Veselin Stoyanov, Claire Cardie, Nathan Gilbert, Ellen
Riloff, David Buttler, and David Hysom. 2010.
Coreference resolution with reconcile. In Proceed-
ings of Association of Computational Linguistics (A-
CL) Conference, pages 156?161.
Marc B. Vilain, John D. Burger, John S. Aberdeen,
Dennis Connolly, and Lynette Hirschman. 1995.
A model-theoretic coreference scoring scheme. In
MUC, pages 45?52.
David Weiss and Benjamin Taskar. 2010. Structured
prediction cascades. Journal of Machine Learning
Research - Proceedings Track, 9:916?923.
Michael L. Wick, Khashayar Rohanimanesh, Kedar
Bellare, Aron Culotta, and Andrew McCallum.
2011. SampleRank: Training factor graphs with
atomic gradients. In Proceedings of International
Conference on Machine Learning (ICML).
Michael L. Wick, Sameer Singh, and Andrew McCal-
lum. 2012. A discriminative hierarchical model for
fast coreference at large scale. In Proceedings of As-
sociation of Computational Linguistics (ACL) Con-
ference, pages 379?388.
Yuehua Xu, Alan Fern, and Sung Wook Yoon. 2009.
Learning linear ranking functions for beam search
with application to planning. Journal of Machine
Learning Research (JMLR), 10:1571?1610.
Chun-Nam John Yu and Thorsten Joachims. 2009.
Learning structural SVMs with latent variables. In
Proceedings of International Conference on Ma-
chine Learning (ICML).
Jiaping Zheng, Luke Vilnis, Sameer Singh, Jinho D.
Choi, and Andrew McCallum. 2013. Dynamic
knowledge-base alignment for coreference resolu-
tion. In Conference on Computational Natural Lan-
guage Learning (CoNLL).
2126
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1597?1606,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Partial Parsing from Bitext Projections
Prashanth Mannem and Aswarth Dara
Language Technologies Research Center
International Institute of Information Technology
Hyderabad, AP, India - 500032
{prashanth,abhilash.d}@research.iiit.ac.in
Abstract
Recent work has shown how a parallel
corpus can be leveraged to build syntac-
tic parser for a target language by project-
ing automatic source parse onto the target
sentence using word alignments. The pro-
jected target dependency parses are not al-
ways fully connected to be useful for train-
ing traditional dependency parsers. In this
paper, we present a greedy non-directional
parsing algorithm which doesn?t need a
fully connected parse and can learn from
partial parses by utilizing available struc-
tural and syntactic information in them.
Our parser achieved statistically signifi-
cant improvements over a baseline system
that trains on only fully connected parses
for Bulgarian, Spanish and Hindi. It also
gave a significant improvement over pre-
viously reported results for Bulgarian and
set a benchmark for Hindi.
1 Introduction
Parallel corpora have been used to transfer in-
formation from source to target languages for
Part-Of-Speech (POS) tagging, word sense disam-
biguation (Yarowsky et al, 2001), syntactic pars-
ing (Hwa et al, 2005; Ganchev et al, 2009; Jiang
and Liu, 2010) and machine translation (Koehn,
2005; Tiedemann, 2002). Analysis on the source
sentences was induced onto the target sentence via
projections across word aligned parallel corpora.
Equipped with a source language parser and a
word alignment tool, parallel data can be used to
build an automatic treebank for a target language.
The parse trees given by the parser on the source
sentences in the parallel data are projected onto the
target sentence using the word alignments from
the alignment tool. Due to the usage of automatic
source parses, automatic word alignments and dif-
ferences in the annotation schemes of source and
target languages, the projected parses are not al-
ways fully connected and can have edges missing
(Hwa et al, 2005; Ganchev et al, 2009). Non-
literal translations and divergences in the syntax
of the two languages also lead to incomplete pro-
jected parse trees.
Figure 1 shows an English-Hindi parallel sen-
tence with correct source parse, alignments and
target dependency parse. For the same sentence,
Figure 2 is a sample partial dependency parse pro-
jected using an automatic source parser on aligned
text. This parse is not fully connected with the
words banaa, kottaige and dikhataa left without
any parents.
para bahuta hai
The cottage built on the hill looks very beautiful
pahaada banaa huaa kottaige sundara dikhataa
Figure 1: Word alignment with dependency
parses for an English-Hindi parallel sentence
To train the traditional dependency parsers (Ya-
mada and Matsumoto, 2003; Eisner, 1996; Nivre,
2003), the dependency parse has to satisfy four
constraints: connectedness, single-headedness,
acyclicity and projectivity (Kuhlmann and Nivre,
2006). Projectivity can be relaxed in some parsers
(McDonald et al, 2005; Nivre, 2009). But these
parsers can not directly be used to learn from par-
tially connected parses (Hwa et al, 2005; Ganchev
et al, 2009).
In the projected Hindi treebank (section 4) that
was extracted from English-Hindi parallel text,
only 5.9% of the sentences had full trees. In
1597
Spanish and Bulgarian projected data extracted by
Ganchev et al (2009), the figures are 3.2% and
12.9% respectively. Learning from data with such
high proportions of partially connected depen-
dency parses requires special parsing algorithms
which are not bound by connectedness. Its only
during learning that the constraint doesn?t satisfy.
For a new sentence (i.e. during inference), the
parser should output fully connected dependency
tree.
para bahuta haipahaada banaa huaa kottaige sundara dikhataa
on cottage very beautifulbuild lookhill PastPart. Be.Pres.
Figure 2: A sample dependency parse with partial
parses
In this paper, we present a dependency pars-
ing algorithm which can train on partial projected
parses and can take rich syntactic information as
features for learning. The parsing algorithm con-
structs the partial parses in a bottom-up manner by
performing a greedy search over all possible rela-
tions and choosing the best one at each step with-
out following either left-to-right or right-to-left
traversal. The algorithm is inspired by earlier non-
directional parsing works of Shen and Joshi (2008)
and Goldberg and Elhadad (2010). We also pro-
pose an extended partial parsing algorithm that can
learn from partial parses whose yields are partially
contiguous.
Apart from bitext projections, this work can be
extended to other cases where learning from par-
tial structures is required. For example, while
bootstrapping parsers high confidence parses are
extracted and trained upon (Steedman et al, 2003;
Reichart and Rappoport, 2007). In cases where
these parses are few, learning from partial parses
might be beneficial.
We train our parser on projected Hindi, Bulgar-
ian and Spanish treebanks and show statistically
significant improvements in accuracies between
training on fully connected trees and learning from
partial parses.
2 Related Work
Learning from partial parses has been dealt in dif-
ferent ways in the literature. Hwa et al (2005)
used post-projection completion/transformation
rules to get full parse trees from the projections
and train Collin?s parser (Collins, 1999) on them.
Ganchev et al (2009) handle partial projected
parses by avoiding committing to entire projected
tree during training. The posterior regularization
based framework constrains the projected syntac-
tic relations to hold approximately and only in ex-
pectation. Jiang and Liu (2010) refer to align-
ment matrix and a dynamic programming search
algorithm to obtain better projected dependency
trees. They deal with partial projections by break-
ing down the projected parse into a set of edges
and training on the set of projected relations rather
than on trees.
While Hwa et al (2005) requires full projected
parses to train their parser, Ganchev et al (2009)
and Jiang and Liu (2010) can learn from partially
projected trees. However, the discriminative train-
ing in (Ganchev et al, 2009) doesn?t allow for
richer syntactic context and it doesn?t learn from
all the relations in the partial dependency parse.
By treating each relation in the projected depen-
dency data independently as a classification in-
stance for parsing, Jiang and Liu (2010) sacrifice
the context of the relations such as global struc-
tural context, neighboring relations that are crucial
for dependency analysis. Due to this, they report
that the parser suffers from local optimization dur-
ing training.
The parser proposed in this work (section 3)
learns from partial trees by using the available
structural information in it and also in neighbor-
ing partial parses. We evaluated our system (sec-
tion 5) on Bulgarian and Spanish projected depen-
dency data used in (Ganchev et al, 2009) for com-
parison. The same could not be carried out for
Chinese (which was the language (Jiang and Liu,
2010) worked on) due to the unavailability of pro-
jected data used in their work. Comparison with
the traditional dependency parsers (McDonald et
al., 2005; Yamada and Matsumoto, 2003; Nivre,
2003; Goldberg and Elhadad, 2010) which train on
complete dependency parsers is out of the scope of
this work.
3 Partial Parsing
A standard dependency graph satisfies four graph
constraints: connectedness, single-headedness,
acyclicity and projectivity (Kuhlmann and Nivre,
2006). In our work, we assume the dependency
graph for a sentence only satisfies the single-
1598
a)
parapahaada banaa huaa kottaige bahuta sundara dikhataa hai
hill on build PastPart. cottage very beautiful look Be.Pres.
b)
para bahuta haipahaada banaa huaa kottaige sundara dikhataa
c)
para haibanaa huaa kottaige sundara dikhataapahaada
bahuta
d)
haibanaa huaa kottaige sundara dikhataapahaada
bahutapara
e)
haibanaa kottaige sundara dikhataapahaada
bahutapara huaa
f)
banaa kottaige sundara dikhataapahaada
bahutapara huaa hai
g)
sundara
bahuta haipahaada
para
banaa kottaige dikhataa
huaa
h)
haipahaada
para
sundara
bahuta
banaa kottaige dikhataa
huaa
Figure 3: Steps taken by GNPPA. The dashed arcs indicate the unconnected words in unConn. The
dotted arcs indicate the candidate arcs in candidateArcs and the solid arcs are the high scoring arcs that
are stored in builtPPs
headedness, acyclicity and projectivity constraints
while not necessarily being connected i.e. all the
words need not have parents.
Given a sentence W=w0 ? ? ? wn with a set of
directed arcs A on the words in W , wi ? wj de-
notes a dependency arc from wi to wj , (wi,wj) 
A. wi is the parent in the arc and wj is the child in
the arc. ??? denotes the reflexive and transitive clo-
sure of the arc. wi
??? wj says that wi dominates
wj , i.e. there is (possibly empty) path from wi to
wj .
A node wi is unconnected if it does not have
an incoming arc. R is the set of all such uncon-
nected nodes in the dependency graph. For the
example in Figure 2, R={banaa, kottaige,
dikhataa}. A partial parse rooted at node wi
denoted by ?(wi) is the set of arcs that can be tra-
versed from node wi. The yield of a partial parse
?(wi) is the set of nodes dominated by it. We
use pi(wi) to refer to the yield of ?(wi) arranged
in the linear order of their occurrence in the sen-
tence. The span of the partial tree is the first and
last words in its yield.
The dependency graph D can now be rep-
resented in terms of partial parses by D =
(W,R, %(R)) where W={w0 ? ? ? wn} is the sen-
tence, R={r1 ? ? ? rm} is the set of unconnected
nodes and %(R)= {?(r1) ? ? ? ?(rm)} is the set of
partial parses rooted at these unconnected nodes.
w0 is a dummy word added at the beginning of
W to behave as a root of a fully connected parse.
A fully connected dependency graph would have
only one element w0 in R and the dependency
graph rooted at w0 as the only (fully connected)
parse in %(R).
We assume the combined yield of %(R) spans
the entire sentence and each of the partial parses in
%(R) to be contiguous and non-overlapping with
one another. A partial parse is contiguous if its
yield is contiguous i.e. if a node wj  pi(wi), then
all the words between wi and wj also belong to
pi(wi). A partial parse ?(wi) is non-overlapping if
the intersection of its yield pi(wi) with yields of all
other partial parses is empty.
3.1 Greedy Non-directional Partial Parsing
Algorithm (GNPPA)
Given the sentence W and the set of unconnected
nodes R, the parser follows a non-directional
greedy approach to establish relations in a bottom
up manner. The parser does a greedy search over
all the possible relations and picks the one with
1599
the highest score at each stage. This process is re-
peated until parents for all the nodes that do not
belong to R are chosen.
Algorithm 1 lists the outline of the greedy non-
directional partial parsing algorithm (GNPPA).
builtPPs maintains a list of all the partial
parses that have been built. It is initialized
in line 1 by considering each word as a sep-
arate partial parse with just one node. can-
didateArcs stores all the arcs that are possi-
ble at each stage of the parsing process in a
bottom up strategy. It is initialized in line 2
using the method initCandidateArcs(w0 ? ? ? wn).
initCandidateArcs(w0 ? ? ? wn) adds two candidate
arcs for each pair of consecutive words with each
other as parent (see Figure 3b). If an arc has one
of the nodes in R as the child, it isn?t included in
candidateArcs.
Algorithm 1 Partial Parsing Algorithm
Input: sentence w0 ? ? ? wn and set of partial tree roots un-
Conn={r1 ? ? ? rm}
Output: set of partial parses whose roots are in unConn
(builtPPs = {?(r1) ? ? ? ?(rm)})
1: builtPPs = {?(r1) ? ? ? ?(rn)} ? {w0 ? ? ? wn}
2: candidateArcs = initCandidateArcs(w0 ? ? ? wn)
3: while candidateArcs.isNotEmpty() do
4: bestArc = argmax
ci  candidateArcs
score(ci,??w )
5: builtPPs.remove(bestArc.child)
6: builtPPs.remove(bestArc.parent)
7: builtPPs.add(bestArc)
8: updateCandidateArcs(bestArc,
candidateArcs, builtPPs, unConn)
9: end while
10: return builtPPs
Once initialized, the candidate arc with the
highest score (line 4) is chosen and accepted
into builtPPs. This involves replacing the best
arc?s child partial parse ?(arc.child) and parent
partial parse ?(arc.parent) over which the arc
has been formed with the arc ?(arc.parent) ?
?(arc.child) itself in builtPPs (lines 5-7). In Figure
3f, to accept the best candidate arc ?(banaa) ?
?(pahaada), the parser would remove the nodes
?(banaa) and ?(pahaada) in builtPPs and add
?(banaa) ? ?(pahaada) to builtPPs (see Fig-
ure 3g).
After the best arc is accepted, the candidateArcs
has to be updated (line 8) to remove the arcs that
are no longer valid and add new arcs in the con-
text of the updated builtPPs. Algorithm 2 shows
the update procedure. First, all the arcs that end
on the child are removed (lines 3-7) along with
the arc from child to parent. Then, the immedi-
ately previous and next partial parses of the best
arc in builtPPs are retrieved (lines 8-9) to add pos-
sible candidate arcs between them and the partial
parse representing the best arc (lines 10-23). In
the example, between Figures 3b and 3c, the arcs
?(kottaige) ? ?(bahuta) and ?(bahuta)
? ?(sundara) are first removed and the arc
?(kottaige) ? ?(sundara) is added to can-
didateArcs. Care is taken to avoid adding arcs that
end on unconnected nodes listed in R.
The entire GNPPA parsing process for the ex-
ample sentence in Figure 2 is shown in Figure 3.
Algorithm 2 updateCandidateArcs(bestArc, can-
didateArcs, builtPPs, unConn)
1: baChild = bestArc.child
2: baParent = bestArc.parent
3: for all arc  candidateArcs do
4: if arc.child = baChild or
(arc.parent = baChild and
arc.child = baParent) then
5: remove arc
6: end if
7: end for
8: prevPP = builtPPs.previousPP(bestArc)
9: nextPP = builtPPs.nextPP(bestArc)
10: if bestArc.direction == LEFT then
11: newArc1 = new Arc(prevPP,baParent)
12: newArc2 = new Arc(baParent,prevPP)
13: end if
14: if bestArc.direction == RIGHT then
15: newArc1 = new Arc(nextPP,baParent)
16: newArc2 = new Arc(baParent,nextPP)
17: end if
18: if newArc1.parent /? unConn then
19: candidateArcs.add(newArc1)
20: end if
21: if newArc2.parent /? unConn then
22: candidateArcs.add(newArc2)
23: end if
24: return candidateArcs
3.2 Learning
The algorithm described in the previous section
uses a weight vector ??w to compute the best arc
from the list of candidate arcs. This weight vec-
tor is learned using a simple Perceptron like algo-
rithm similar to the one used in (Shen and Joshi,
2008). Algorithm 3 lists the learning framework
for GNPPA.
For a training sample with sentence w0 ? ? ? wn,
projected partial parses projectedPPs={?(ri) ? ? ?
?(rm)}, unconnected words unConn and weight
vector ??w , the builtPPs and candidateArcs are ini-
tiated as in algorithm 1. Then the arc with the
highest score is selected. If this arc belongs to
the parses in projectedPPs, builtPPs and candi-
dateArcs are updated similar to the operations in
1600
a)
para haipahaada banaa huaa kottaige bahuta sundara dikhataa
hill on build PastPart. cottage very beautiful look Be.Pres.
b)
para haipahaada banaa huaa kottaige bahuta sundara dikhataa
c)
hai
bahuta
pahaada para banaa huaa kottaige sundara dikhataa
d)
hai
para bahuta
pahaada banaa huaa kottaige sundara dikhataa
Figure 4: First four steps taken by E-GNPPA. The blue colored dotted arcs are the additional candidate
arcs that are added to candidateArcs
algorithm 1. If it doesn?t, it is treated as a neg-
ative sample and a corresponding positive candi-
date arc which is present both projectedPPs and
candidateArcs is selected (lines 11-12).
The weights of the positive candidate arc are in-
creased while that of the negative sample (best arc)
are decreased. To reduce over fitting, we use aver-
aged weights (Collins, 2002) in algorithm 1.
Algorithm 3 Learning for Non-directional Greedy
Partial Parsing Algorithm
Input: sentence w0 ? ? ? wn, projected partial parses project-
edPPs, unconnected words unConn, current ??w
Output: updated ??w
1: builtPPs = {?(r1) ? ? ? ?(rn)} ? {w0 ? ? ? wn}
2: candidateArcs = initCandidateArcs(w0 ? ? ? wn)
3: while candidateArcs.isNotEmpty() do
4: bestArc = argmax
ci  candidateArcs
score(ci,??w )
5: if bestArc ? projectedPPs then
6: builtPPs.remove(bestArc.child)
7: builtPPs.remove(bestArc.parent)
8: builtPPs.add(bestArc)
9: updateCandidateArcs(bestArc,
candidateArcs, builtPPs, unConn)
10: else
11: allowedArcs = {ci | ci  candidateArcs && ci 
projectedArcs}
12: compatArc = argmax
ci  allowedArcs
score(ci,??w )
13: promote(compatArc,??w )
14: demote(bestArc,??w )
15: end if
16: end while
17: return builtPPs
3.3 Extended GNPPA (E-GNPPA)
The GNPPA described in section 3.1 assumes that
the partial parses are contiguous. The exam-
ple in Figure 5 has a partial tree ?(dikhataa)
which isn?t contiguous. Its yield doesn?t con-
tain bahuta and sundara. We call such non-
contiguous partial parses whose yields encompass
the yield of an other partial parse as partially con-
tiguous. Partially contiguous parses are common
in the projected data and would not be parsable by
the algorithm 1 (?(dikhataa)? ?(kottaige)
would not be identified).
para bahuta haipahaada banaa huaa kottaige sundara dikhataa
hill on build cottage very beautiful lookPastPart. Be.Pres.
Figure 5: Dependency parse with a partially con-
tiguous partial parse
In order to identify and learn from relations
which are part of partially contiguous partial
parses, we propose an extension to GNPPA. The
extended GNPAA (E-GNPPA) broadens its scope
while searching for possible candidate arcs given
R and builtPPs. If the immediate previous or
the next partial parses over which arcs are to
be formed are designated unconnected nodes, the
parser looks further for a partial parse over which
it can form arcs. For example, in Figure 4b, the
arc ?(para) ? ?(banaa) can not be added to
the candidateArcs since banaa is a designated
unconnected node in unConn. The E-GNPPA
looks over the unconnected node and adds the arc
?(para) ? ?(huaa) to the candidate arcs list
candidateArcs.
E-GNPPA differs from algorithm 1 in lines 2
and 8. The E-GNPPA uses an extended initializa-
tion method initCandidateArcsExtended(w0) for
1601
Parent and Child par.pos, chd.pos, par.lex, chd.lex
Sentence Context
par-1.pos, par-2.pos, par+1.pos, par+2.pos, par-1.lex, par+1.lex
chd-1.pos, chd-2.pos, chd+1.pos, chd+2.pos, chd-1.lex, chd+1.lex
Structural Info
leftMostChild(par).pos, rightMostChild(par).pos, leftSibling(chd).pos,
rightSibling(chd).pos
Partial Parse Context previousPP().pos, previousPP().lex, nextPP().pos, nextPP().lex
Table 1: Information on which features are defined. par denotes the parent in the relation and chd the
child. .pos and .lex is the POS and word-form of the corresponding node. +/-i is the previous/next
ith word in the sentence. leftMostChild() and rightMostChild() denote the left most and right most
children of a node. leftSibling() and rightSibling() get the immediate left and right siblings of a node.
previousPP() and nextPP() return the immediate previous and next partial parses of the arc in builtPPs at
the state.
candidateArcs in line 2 and an extended proce-
dure updateCandidateArcsExtended to update the
candidateArcs after each step in line 8. Algorithm
4 shows the changes w.r.t algorithm 2. Figure 4
presents the steps taken by the E-GNPPA parser
for the example parse in Figure 5.
Algorithm 4 updateCandidateArcsExtended
( bestArc, candidateArcs, builtPPs,unConn )
? ? ? lines 1 to 7 of Algorithm 2 ? ? ?
prevPP = builtPPs.previousPP(bestArc)
while prevPP ? unConn do
prevPP = builtPPs.previousPP(prevPP)
end while
nextPP = builtPPs.nextPP(bestArc)
while nextPP ? unConn do
nextPP = builtPPs.nextPP(nextPP)
end while
? ? ? lines 10 to 24 of Algorithm 2 ? ? ?
3.4 Features
Features for a relation (candidate arc) are defined
on the POS tags and lexical items of the nodes in
the relation and those in its context. Two kinds
of context are used a) context from the input sen-
tence (sentence context) b) context in builtPPs i.e.
nearby partial parses (partial parse context). In-
formation from the partial parses (structural info)
such as left and right most children of the par-
ent node in the relation, left and right siblings of
the child node in the relation are also used. Ta-
ble 1 lists the information on which features are
defined in the various configurations of the three
language parsers. The actual features are combi-
nations of the information present in the table. The
set varies depending on the language and whether
its GNPPA or E-GNPPA approach.
While training, no features are defined on
whether a node is unconnected (present in un-
Conn) or not as this information isn?t available
during testing.
4 Hindi Projected Dependency Treebank
We conducted experiments on English-Hindi par-
allel data by transferring syntactic information
from English to Hindi to build a projected depen-
dency treebank for Hindi.
The TIDES English-Hindi parallel data con-
taining 45,000 sentences was used for this pur-
pose 1 (Venkatapathy, 2008). Word alignments
for these sentences were obtained using the widely
used GIZA++ toolkit in grow-diag-final-and mode
(Och and Ney, 2003). Since Hindi is a morpho-
logically rich language, root words were used in-
stead of the word forms. A bidirectional English
POS tagger (Shen et al, 2007) was used to POS
tag the source sentences and the parses were ob-
tained using the first order MST parser (McDon-
ald et al, 2005) trained on dependencies extracted
from Penn treebank using the head rules of Ya-
mada and Matsumoto (2003). A CRF based Hindi
POS tagger (PVS. and Gali, 2007) was used to
POS tag the target sentences.
English and Hindi being morphologically and
syntactically divergent makes the word alignment
and dependency projection a challenging task.
The source dependencies are projected using an
approach similar to (Hwa et al, 2005). While
they use post-projection transformations on the
projected parse to account for annotation differ-
ences, we use pre-projection transformations on
the source parse. The projection algorithm pro-
1The original data had 50,000 parallel sentences. It was
later refined by IIIT-Hyderabad to remove repetitions and
other trivial errors. The corpus is still noisy with typographi-
cal errors, mismatched sentences and unfaithful translations.
1602
duces acyclic parses which could be unconnected
and non-projective.
4.1 Annotation Differences in Hindi and
English
Before projecting the source parses onto the tar-
get sentence, the parses are transformed to reflect
the annotation scheme differences in English and
Hindi. While English dependency parses reflect
the PTB annotation style (Marcus et al, 1994),
we project them to Hindi to reflect the annotation
scheme described in (Begum et al, 2008). The
differences in the annotation schemes are with re-
spect to three phenomena: a) head of a verb group
containing auxiliary and main verbs, b) preposi-
tions in a prepositional phrase (PP) and c) coordi-
nation structures.
In the English parses, the auxiliary verb is the
head of the main verb while in Hindi, the main
verb is the head of the auxiliary in the verb group.
For example, in the Hindi parse in Figure 1,
dikhataa is the head of the auxiliary verb hai.
The prepositions in English are realized as post-
positions in Hindi. While prepositions are the
heads in a preposition phrase, post-positions are
the modifiers of the preceding nouns in Hindi. In
pahaada para (on the hill), hill is the head
of para. In coordination structures, while En-
glish differentiates between how NP coordination
and VP coordination structures behave, Hindi an-
notation scheme is consistent in its handling. Left-
most verb is the head of a VP coordination struc-
ture in English whereas the rightmost noun is the
head in case of NP coordination. In Hindi, the con-
junct is the head of the two verbs/nouns in the co-
ordination structure.
These three cases are identified in the source
tree and appropriate transformations are made to
the source parse itself before projecting the rela-
tions using word alignments.
5 Experiments
We carried out all our experiments on paral-
lel corpora belonging to English-Hindi, English-
Bulgarian and English-Spanish language pairs.
While the Hindi projected treebank was obtained
using the method described in section 4, Bulgar-
ian and Spanish projected datasets were obtained
using the approach in (Ganchev et al, 2009). The
datasets of Bulgarian and Spanish that contributed
to the best accuracies for Ganchev et al (2009)
Statistic Hindi Bulgarian Spanish
N(Words) 226852 71986 133124
N(Parent==-1) 44607 30268 54815
P(Parent==-1) 19.7 42.0 41.1
N(Full trees) 593 1299 327
N(GNPPA) 30063 10850 19622
P(GNPPA) 16.4 26.0 25.0
N(E-GNPPA) 35389 12281 24577
P(E-GNPPA) 19.3 29.4 30.0
Table 2: Statistics of the Hindi, Bulgarian and Spanish
projected treebanks used for experiments. Each of them has
10,000 randomly picked parses. N(X) denotes number of X
and P(X) denotes percentage of X. N(Words) is the number
of words. N(Parents==-1) is the number of words without a
parent. N(Full trees) is the number of parses which are fully
connected. N(GNPPA) is the number of relations learnt by
GNPPA parser and N(E-GNPPA) is the number of relations
learnt by E-GNPPA parser. Note that P(GNPPA) is calculated
as N(GNPPA)/(N(Words) - N(Parents==-1)).
were used in our work (7 rules dataset for Bulgar-
ian and 3 rules dataset for Spanish). The Hindi,
Bulgarian and Spanish projected dependency tree-
banks have 44760, 39516 and 76958 sentences re-
spectively. Since we don?t have confidence scores
for the projections on the sentences, we picked
10,000 sentences randomly in each of the three
datasets for training the parsers2. Other methods
of choosing the 10K sentences such as those with
the max. no. of relations, those with least no. of
unconnected words, those with max. no. of con-
tiguous partial trees that can be learned by GNPPA
parser etc. were tried out. Among all these, ran-
dom selection was consistent and yielded the best
results. The errors introduced in the projected
parses by errors in word alignment, source parser
and projection are not consistent enough to be ex-
ploited to select the better parses from the entire
projected data.
Table 2 gives an account of the randomly cho-
sen 10k sentences in terms of the number of words,
words without parents etc. Around 40% of the
words spread over 88% of sentences in Bulgarian
and 97% of sentences in Spanish have no parents.
Traditional dependency parsers which only train
from fully connected trees would not be able to
learn from these sentences. P(GNPPA) is the per-
centage of relations in the data that are learned by
the GNPPA parser satisfying the contiguous par-
tial tree constraint and P(E-GNPPA) is the per-
2Exactly 10K sentences were selected in order to compare
our results with those of (Ganchev et al, 2009).
1603
Parser
Hindi Bulgarian Spanish
Punct NoPunct Punct NoPunct Punct NoPunct
Baseline 78.70 77.39 51.85 55.15 41.60 45.61
GNPPA 80.03* 78.81* 77.03* 79.06* 65.49* 68.70*
E-GNPPA 81.10*? 79.94*? 78.93*? 80.11*? 67.69*? 70.90*?
Table 3: UAS for Hindi, Bulgarian and Spanish with the baseline, GNPPA and E-GNPPA parsers trained
on 10k parses selected randomly. Punct indicates evaluation with punctuation whereas NoPunct indicates
without punctuation. * next to an accuracy denotes statistically significant (McNemar?s and p < 0.05)
improvement over the baseline. ? denotes significance over GNPPA
centage that satisfies the partially contiguous con-
straint. E-GNPPA parser learns around 2-5% more
no. of relations than GNPPA due to the relaxation
in the constraints.
The Hindi test data that was released as part of
the ICON-2010 Shared Task (Husain et al, 2010)
was used for evaluation. For Bulgarian and Span-
ish, we used the same test data that was used in
the work of Ganchev et al (2009). These test
datasets had sentences from the training section of
the CoNLL Shared Task (Nivre et al, 2007) that
had lengths less than or equal to 10. All the test
datasets have gold POS tags.
A baseline parser was built to compare learning
from partial parses with learning from fully con-
nected parses. Full parses are constructed from
partial parses in the projected data by randomly
assigning parents to unconnected parents, similar
to the work in (Hwa et al, 2005). The uncon-
nected words in the parse are selected randomly
one by one and are assigned parents randomly to
complete the parse. This process is repeated for all
the sentences in the three language datasets. The
parser is then trained with the GNPPA algorithm
on these fully connected parses to be used as the
baseline.
Table 3 lists the accuracies of the baseline,
GNPPA and E-GNPPA parsers. The accuracies
are unlabeled attachment scores (UAS): the per-
centage of words with the correct head. Table
4 compares our accuracies with those reported in
(Ganchev et al, 2009) for Bulgarian and Spanish.
5.1 Discussion
The baseline reported in (Ganchev et al, 2009)
significantly outperforms our baseline (see Table
4) due to the different baselines used in both the
works. In our work, while creating the data for
the baseline by assigning random parents to un-
connected words, acyclicity and projectivity con-
Parser Bulgarian Spanish
Ganchev-Baseline 72.6 69.0
Baseline 55.15 45.61
Ganchev-Discriminative 78.3 72.3
GNPPA 79.06 68.70
E-GNPPA 80.11 70.90
Table 4: Comparison of baseline, GNPPA and E-
GNPPA with baseline and discriminative model
from (Ganchev et al, 2009) for Bulgarian and
Spanish. Evaluation didn?t include punctuation.
straints are not enforced. Ganchev et al (2009)?s
baseline is similar to the first iteration of their dis-
criminative model and hence performs better than
ours. Our Bulgarian E-GNPPA parser achieved a
1.8% gain over theirs while the Spanish results are
lower. Though their training data size is also 10K,
the training data is different in both our works due
to the difference in the method of choosing 10K
sentences from the large projected treebanks.
The GNPPA accuracies (see table 3) for all the
three languages are significant improvements over
the baseline accuracies. This shows that learning
from partial parses is effective when compared to
imposing the connected constraint on the partially
projected dependency parse. Even while project-
ing source dependencies during data creation, it
is better to project high confidence relations than
look to project more relations and thereby intro-
duce noise.
The E-GNPPA which also learns from partially
contiguous partial parses achieved statistically sig-
nificant gains for all the three languages. The
gains across languages is due to the fact that in
the 10K data that was used for training, E-GNPPA
parser could learn 2 ? 5% more relations over
GNPPA (see Table 2).
Figure 6 shows the accuracies of baseline and E-
1604
 
30
 
40
 
50
 
60
 
70
 
80  0
 
1
 
2
 
3
 
4
 
5
 
6
 
7
 
8
 
9
 
10
Unlabeled Accuracy
Thousa
nds of 
senten
ces
Bulgari
an Hindi Spanis
h
hn-bas
eline
bg-bas
eline
es-bas
eline
Figure 6: Accuracies (without punctuation) w.r.t
varying training data sizes for baseline and E-
GNPPA parsers.
GNPPA parser for the three languages when train-
ing data size is varied. The parsers peak early with
less than 1000 sentences and make small gains
with the addition of more data.
6 Conclusion
We presented a non-directional parsing algorithm
that can learn from partial parses using syntac-
tic and contextual information as features. A
Hindi projected dependency treebank was devel-
oped from English-Hindi bilingual data and ex-
periments were conducted for three languages
Hindi, Bulgarian and Spanish. Statistically sig-
nificant improvements were achieved by our par-
tial parsers over the baseline system. The partial
parsing algorithms presented in this paper are not
specific to bitext projections and can be used for
learning from partial parses in any setting.
References
R. Begum, S. Husain, A. Dhwaj, D. Sharma, L. Bai,
and R. Sangal. 2008. Dependency annotation
scheme for indian languages. In In Proceedings of
The Third International Joint Conference on Natural
Language Processing (IJCNLP), Hyderabad, India.
Michael John Collins. 1999. Head-driven statistical
models for natural language parsing. Ph.D. thesis,
University of Pennsylvania, Philadelphia, PA, USA.
AAI9926110.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: theory and experi-
ments with perceptron algorithms. In Proceedings
of the ACL-02 conference on Empirical methods in
natural language processing - Volume 10, EMNLP
?02, pages 1?8, Morristown, NJ, USA. Association
for Computational Linguistics.
Jason M. Eisner. 1996. Three new probabilistic mod-
els for dependency parsing: an exploration. In Pro-
ceedings of the 16th conference on Computational
linguistics - Volume 1, pages 340?345, Morristown,
NJ, USA. Association for Computational Linguis-
tics.
Kuzman Ganchev, Jennifer Gillenwater, and Ben
Taskar. 2009. Dependency grammar induction via
bitext projection constraints. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP: Vol-
ume 1 - Volume 1, ACL-IJCNLP ?09, pages 369?
377, Morristown, NJ, USA. Association for Compu-
tational Linguistics.
Yoav Goldberg and Michael Elhadad. 2010. An effi-
cient algorithm for easy-first non-directional depen-
dency parsing. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, HLT ?10, pages 742?750, Morristown, NJ,
USA. Association for Computational Linguistics.
Samar Husain, Prashanth Mannem, Bharath Ambati,
and Phani Gadde. 2010. Icon 2010 tools contest on
indian language dependency parsing. In Proceed-
ings of ICON 2010 NLP Tools Contest.
Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara
Cabezas, and Okan Kolak. 2005. Bootstrapping
parsers via syntactic projection across parallel texts.
Nat. Lang. Eng., 11:311?325, September.
Wenbin Jiang and Qun Liu. 2010. Dependency parsing
and projection based on word-pair classification. In
Proceedings of the 48th Annual Meeting of the As-
sociation for Computational Linguistics, ACL ?10,
pages 12?20, Morristown, NJ, USA. Association for
Computational Linguistics.
P. Koehn. 2005. Europarl: A parallel corpus for statis-
tical machine translation. In MT summit, volume 5.
Citeseer.
Marco Kuhlmann and Joakim Nivre. 2006. Mildly
non-projective dependency structures. In Proceed-
ings of the COLING/ACL on Main conference poster
sessions, pages 507?514, Morristown, NJ, USA. As-
sociation for Computational Linguistics.
Mitchell P. Marcus, Beatrice Santorini, and Mary A.
Marcinkiewicz. 1994. Building a large annotated
corpus of english: The penn treebank. Computa-
tional Linguistics, 19(2):313?330.
R. McDonald, K. Crammer, and F. Pereira. 2005. On-
line large-margin training of dependency parsers. In
Proceedings of the Annual Meeting of the Associa-
tion for Computational Linguistics (ACL).
1605
Jens Nilsson and Joakim Nivre. 2008. Malteval:
an evaluation and visualization tool for dependency
parsing. In Proceedings of the Sixth International
Language Resources and Evaluation (LREC?08),
Marrakech, Morocco, may. European Language
Resources Association (ELRA). http://www.lrec-
conf.org/proceedings/lrec2008/.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan Mc-
donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007. The CoNLL 2007 shared task on de-
pendency parsing. In Proceedings of the CoNLL
Shared Task Session of EMNLP-CoNLL 2007, pages
915?932, Prague, Czech Republic. Association for
Computational Linguistics.
Joakim Nivre. 2003. An Efficient Algorithm for Pro-
jective Dependency Parsing. In Eighth International
Workshop on Parsing Technologies, Nancy, France.
Joakim Nivre. 2009. Non-projective dependency pars-
ing in expected linear time. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP, pages
351?359, Suntec, Singapore, August. Association
for Computational Linguistics.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Avinesh PVS. and Karthik Gali. 2007. Part-Of-Speech
Tagging and Chunking using Conditional Random
Fields and Transformation-Based Learning. In Pro-
ceedings of the IJCAI and the Workshop On Shallow
Parsing for South Asian Languages (SPSAL), pages
21?24.
Roi Reichart and Ari Rappoport. 2007. Self-training
for enhancement and domain adaptation of statisti-
cal parsers trained on small datasets. In Proceed-
ings of the 45th Annual Meeting of the Associa-
tion of Computational Linguistics, pages 616?623,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Libin Shen and Aravind Joshi. 2008. LTAG depen-
dency parsing with bidirectional incremental con-
struction. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Process-
ing, pages 495?504, Honolulu, Hawaii, October. As-
sociation for Computational Linguistics.
L. Shen, G. Satta, and A. Joshi. 2007. Guided learn-
ing for bidirectional sequence classification. In Pro-
ceedings of the 45th Annual Meeting of the Associa-
tion for Computational Linguistics (ACL).
Mark Steedman, Miles Osborne, Anoop Sarkar,
Stephen Clark, Rebecca Hwa, Julia Hockenmaier,
Paul Ruhlen, Steven Baker, and Jeremiah Crim.
2003. Bootstrapping statistical parsers from small
datasets. In Proceedings of the tenth conference on
European chapter of the Association for Computa-
tional Linguistics - Volume 1, EACL ?03, pages 331?
338, Morristown, NJ, USA. Association for Compu-
tational Linguistics.
Jrg Tiedemann. 2002. MatsLex - a multilingual lex-
ical database for machine translation. In Proceed-
ings of the 3rd International Conference on Lan-
guage Resources and Evaluation (LREC?2002), vol-
ume VI, pages 1909?1912, Las Palmas de Gran Ca-
naria, Spain, 29-31 May.
Sriram Venkatapathy. 2008. Nlp tools contest - 2008:
Summary. In Proceedings of ICON 2008 NLP Tools
Contest.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statis-
tical Dependency Analysis with Support Vector Ma-
chines. In In Proceedings of IWPT, pages 195?206.
David Yarowsky, Grace Ngai, and Richard Wicen-
towski. 2001. Inducing multilingual text analysis
tools via robust projection across aligned corpora.
In Proceedings of the first international conference
on Human language technology research, HLT ?01,
pages 1?8, Morristown, NJ, USA. Association for
Computational Linguistics.
1606
Proceedings of the Fifth Law Workshop (LAW V), pages 134?142,
Portland, Oregon, 23-24 June 2011. c?2011 Association for Computational Linguistics
Empty Categories in Hindi Dependency Treebank: Analysis and Recovery
Chaitanya GSK
Intl Institute of Info. Technology
Hyderabad, India
chaitanya.gsk
@research.iiit.ac.in
Samar Husain
Intl Institute of Info. Technology
Hyderabad, India
samar
@research.iiit.ac.in
Prashanth Mannem
Intl Institute of Info. Technology
Hyderabad, India
prashanth
@research.iiit.ac.in
Abstract
In this paper, we first analyze and classify the
empty categories in a Hindi dependency tree-
bank and then identify various discovery pro-
cedures to automatically detect the existence
of these categories in a sentence. For this we
make use of lexical knowledge along with the
parsed output from a constraint based parser.
Through this work we show that it is possi-
ble to successfully discover certain types of
empty categories while some other types are
more difficult to identify. This work leads to
the state-of-the-art system for automatic inser-
tion of empty categories in the Hindi sentence.
1 Introduction
Empty categories play a crucial role in the annota-
tion framework of the Hindi dependency treebank1
(Begum et al, 2008; Bharati et al, 2009b). They
are inserted in a sentence in case the dependency
analysis does not lead to a fully connected tree. In
the Hindi treebank, an empty category (denoted by
a NULL node) always has at least one child. These
elements have essentially the same properties (e.g.
case-marking, agreement, etc.) as an overtly real-
ized element and they provide valuable information
(such as predicate-argument structure, etc.). A dif-
ferent kind of motivation for postulating empty cate-
gories comes from the demands of natural lan- guage
processing, in particular parsing. There are several
types of empty categories in the Hindi dependency
1The dependency treebank is part of a Multi Representa-
tional and Multi-Layered Treebank for Hindi/Urdu (Palmer et
al., 2009).
treebank serving different purposes. The presence
of these elements can be crucial for correct auto-
matic parsing. Traditional parsing algorithms do
not insert empty categories and require them to be
part of the input. The performance of such parser
will be severely affected if one removes these ele-
ments from the input data. Statistical parsers like
MaltParser (Nivre, 2003), MSTParser (McDonald,
2005), as well as Constraint Based Hybrid Parser
(CBHP) (Bharati et al, 2009a) produce incorrect
parse trees once the empty categories are removed
from the input data. Hence there is a need for auto-
matic detection and insertion of empty categories in
the Hindi data. Additionally, it is evident that suc-
cessful detection of such nodes will help the annota-
tion process as well.
There have been many approaches for the recov-
ery of empty categories in the treebanks like Penn
treebank, both ML based (Collins, 1997; Johnson,
2002; Dienes and Dubey, 2003a,b; Higgins, 2003)
and rule based (R Campbell, 2004). Some ap-
proaches such as Yang and Xue (2010) follow a post
processing step of recovering empty categories after
parsing the text.
In this paper we make use of lexical knowledge
along with the parsed output from a constraint based
parser to successfully insert empty category in the
input sentence, which may further be given for pars-
ing or other applications. Throughout this paper, we
use the term recovery (of empty categories) for the
insertion of different types of empty categories into
the input sentence.
The paper is arranged as follows, Section 2 dis-
cusses the empty nodes in the treebank and classifies
134
NULL NP tokens 69
NULL VG tokens 68
NULL CCP tokens 32
Sentences with more than
one empty category in them 159
Table 1: Empty categories in Hindi Tree bank
them based on their syntactic type. In section 3 we
provide an algorithm to automatically recover these
elements. Section 4 shows the performance of our
system and discusses the results. We conclude the
paper in section 5.
2 An overview of Empty Categories in
Hindi dependency Treebank
Begum et al, (2008) proposed a dependency frame-
work in which an empty node is introduced dur-
ing the annotation process only if its presence is
required to build the dependency tree for the sen-
tence (Figures 1, 2, 3) 2. Empty categories such as
those discussed in Bhatia et al (2010) which would
be leaf nodes in the dependency tree are not part
of the dependency structure and are added during
Propbanking3. Consequently, the empty categories
in Hindi treebank do not mark displacement as in
Penn treebank (Marcus et al, 1993) rather, they rep-
resent undisplaced syntactic elements which happen
to lack phonological realization. In the Hindi depen-
dency treebank, an empty category is represented by
a ?NULL? word. Sentences can have a missing VG
or NP or CCP 4. These are represented by ?NULL?
token and are marked with the appropriate Part-of-
speech tag along with marking the chunk tag such
as NULL NP, NULL VGF, NULL CCP, etc. in Ta-
ble 2
2Due to space constraints, sentences in all the figures only
show chunk heads. Please refer to examples 1 to 6 for entire
sentences with glosses
3These empty categories are either required to correctly cap-
ture the argument structure during propbanking or are required
to successfully convert the dependency structure to phrase struc-
ture (Xia et al, 2009)
4VG is Verb Group, NP is Noun Phrase and CCP is Conjunct
Phrase.
Type of empty Inst- Chunk tag
categories ances (CPOS)
Empty subject 69 NULL NP
Backward gapping 29 NULL VG
Forward gapping 21 NULL VG
Finite verb ellipses 18 NULL VG
Conjunction ellipses
(verbs) 20 NULL CCP
Conjunction ellipses
(nouns) 12 NULL CCP
Total 169
Table 2: Empty category types.
2.1 Empty category types
From the empty categories recovery point of view,
we have divided the empty categories in the treebank
into six types (Table 2).
The first type of empty category is Empty Subject
(Figure 1), example.1 where a clause ?rava ke
kaaran hi manmohan singh rajaneeti me aaye? is
dependent on the missing subject of the verb ?hai?
(is).
(1) NULL gaurtalab hai ki raao
NULL ?noticeable? ?is? ?that? ?Rao?
ke kaaran hi manmohan sing
?because? ?only? ?Manmohan? ?singh?
raajaniiti me aaye
?politics? ?in? ?came.
?it is noticeable that because of Rao, Manmohan
Singh came in politics?
The second type of empty category is due to
Backward Gapping (Figure 2), example.2 where
the verb is absent in the clause that occurs before a
co-ordinating conjunct.
(2) doosare nambara para misa roosa
?second? ?position? ?on? ?miss? ?Russia?
natasha NULL aur tiisare nambara
?Natasha? NULL ?and? ?third? ?position?
para misa lebanan sendra rahiim .
?on? ?miss? ?Lebanan? ?Sandra? were? .
135
Figure 1: Empty Subject.
Figure 2: Backward Gapping.
Figure 3: Forward Gapping.
Figure 4: Finite verb ellipses.
Figure 5: Conjunction ellipses (verbs).
136
Figure 6: Conjunctuon ellipses (nouns).
?Miss Russia stood second and Miss Lebanan
was third?
The third type of empty category is Forward
Gapping (Figure 3), example 3, which is similar to
the second type but with the clause with the missing
verb occurring after the conjunct rather than before.
The reason for a separate class for forward gapping
is explained in the next section.
(3) divaalii ke dina jua Kele magara
?Diwali? ?GEN? ?day? ?gamble? ?play? ?but?
NULL gar me yaa hotala me
?NULL? ?home? ?in? ?or? ?hotel? ?in?
?Played gamble on Diwali day but was it at home
or hotel?
The fourth type of empty category is due to Finite
verb ellipses (Figure4), example 4, where the main
verb for a sentence is missing.
(4) saath me vahii phevareta khadaa pyaaja
?along? ?in? ?that? ?favorite? ?raw? ?onion?
NULL.
NULL
?Along with this, the same favorite semi-cooked
onion?
The fifth type of empty category is Conjunction
ellipses (Verbs), example 5 (Figure 5).
(5) bacce bare ho-ga-ye-hai NULL
?children? ?big? ?become? ?NULL?
kisii ki baat nahiin maante
?anyone? ?gen? ?advice? ?not? ?accept?
?The children have grown big (and) do not listen
to anyone?
The sixth type of empty category is the Conjunc-
tion ellipses (for nouns), example 6 (Figure 6).
(6) yamunaa nadii me uphaana se
?Yamuna? ?river? ?in? ?storm? ?INST?
sekado ekara gannaa, caaraa,
?thousands? ?acre? ?sugarcane? ?straw?
dhana, NULL sabjii kii phasale
?money? ?NULL? ?vegetable? ?GEN? ?crops?
jala-magna ho-gai-hai .
?drowned? ?happened?
?Because of the storm in the Yamuna river, thou-
sand acres of sugarcane, straw, money, vegetable
crops got submerged?
3 Empty categories recovery Algorithm
Given the limited amount of data available (only 159
sentences with at least one empty category in them
out of 2973 sentences in the Hindi treebank, Table
12 ), we follow a rule based approach rather than us-
ing ML to recover the empty catogories discussed in
the previous section. Interestingly, a rule-based ap-
proach was followed by R Campbell, (2004) that re-
covered empty categories in English resulting in bet-
ter performance than previous empirical approaches.
This work can be extended for ML once more data
becomes available.
The techniques that are used for recovering empty
categories in the Penn treebank (Collins, 1997;
Johnson, 2002;) might not be suitable since the Penn
treebank has all the empty categories as leaf nodes in
the tree unlike the Hindi dependency treebank where
137
for each sentence in the input data
try in Empty Subject
try in Forward Gapping
try in Finite Verb ellipses
for each tree in CBHP parse output
try in Backward Gapping
try in Forward Gapping
try in Finite Verb ellipses
try in Conjunction ellipses (for Verbs)
Table 3: Empty categories Recovery Algorithm.
the empty categories are always internal nodes in the
dependency trees (Figure 2).
In this section we describe an algorithm which
recovers empty categories given an input sentence.
Our method makes use of both the lexical cues as
well as the output of the Constraint Based Hybrid
Parser (CBHP). Table 3 presents the recovery algo-
rithm which first runs on the input sentence and then
on the output of the CBHP.
3.1 Empty Subject
Framing rule 1 requires the formation of a set (Cue-
Set) based on our analysis discussed in the previ-
ous section. It contains all the linguistic cues (lex-
ical items such as gaurtalab ?noticeable?, maloom
?known?, etc). We then scan the input sentence
searching for the cue and insert an empty category
(NULL NP)5 if the cue is found. Table 4 illustates
the process where we search for ?CueSet he ki? or
?CueSet ho ki? phrases. In Table 4, W+1 represents
word next to W, W+2 represents word next to W+1.
3.2 Backward Gapping
To handle backward gapping cases, we take the in-
termediate parse output from CBHP 6 for the whole
data. The reason behind choosing CBHP lies in its
rule based approach. CBHP fails (or rather gives
a visibly different parse) for sentences with miss-
ing verbs. And when it fails to find a verb, CBHP
5We insert a token ?NULL? with NULL NP as CPOS
6CBHP is a two-stage parser. In the 1st stage it parses intra-
clausal relations and inter-clausal relations in the 2nd stage. The
1st stage parse is an intermediate parse.
for each word W in the Sentence
if W  CueSet
if W+1 & W+2 = he or ho & ki
Insert NULL with PRP as POS,
NULL NP as CPOS
Table 4: Rule for identifying Empty Subject.
for each node N in tree T
if head of N = ?
insert N in unattached subtrees[]
for each node X in unattached subtrees[]
while POS(X) is not VG
traverse in the array of unattached subtrees
if ? a conjunct, then recovery=1
if recovery = 1
insert NULL, with VM as POS,
NULL VG as CPOS
Head of NULL = ?
Table 5: Rule for identifying Backward Gapping using
CBHP.
gives unattached subtrees7 (Figure 7, 8, 9 illustrates
the unattached subtrees where the parser is unable to
find a relation between the heads of each unattached
subtree). Similarly whenever the parser expects a
conjunction and the conjunction is absent in the sen-
tence, CBHP again gives the unattached subtrees.
We analyze these unattached sub-trees to see
whether there is a possibility for empty category.
The array, in Table 5 represents all the nodes hav-
ing no heads. POS represents part of speech and
CPOS represents chunk part of speech and ? repre-
sents empty set.
3.3 Forward gapping
The main reason for handling the forward gapping as
a separate case rather than considering it along with
backward gapping is the prototypical SOV word-
order of Hindi, i.e. the verb occurs after subject and
object in a clause or sentence. We take the interme-
diate parse output from the CBHP for the whole data
and when ever a verb is absent in a clause occurring
immediately after a conjunct, we search for a VG af-
7CBHP gives fully connected trees in both the stages. We
have modified the parser so that it gives unattached subtrees
when it fails.
138
for each node N in tree T
if head of N = ?
insert N in unattached subtrees[]
for each node X in unattached subtrees[]
if !? a verb between two conjuncts
if those conjuncts belongs to conjunct set
insert insert NULL with VM as POS,
NULL VG as CPOS
Table 6: Rule for identifying Forward Gapping using
CBHP.
for each word W in the sentence S
if W  CueSet FG
insert NULL with NULL VG as POS
and CPOS
if W = Conjunct
if POS(W-1) = VG
if !? a VG in S-W
insert NULL with VM as POS,
NULL VG as CPOS
Table 7: Rule for identifying Forward Gapping .
ter the conjunct and insert an empty category if the
VG is absent (an example of such cases can be seen
in Figure 7). This procedure is given in Table 6. In
addition, we use the lexical cues (such as ya nahii ?or
not?, ya ?or?) for recovering certain types of empty
categories. CueSet FG is the set that contains the
lexical cues and conjunct set contains lexical cues
like (ki and ya). This procedure is shown in Table 7.
Figure 7: Unattached sub trees in CBHP parse output of
an input sentence (forward gapping).
3.4 Finite Verb ellipses
In the cases where there is no VG at all in the sen-
tence, we insert a NULL VG before the EOS (End-
Of-Sentence) in the input sentence. For this case,
finite verb ellipses can be recovered directly from
if !? a VG in S-W
insert NULL with VM as POS,
NULL VG as CPOS
Table 8: Rule for identifying Finite Verb ellipses in sen-
tence.
for each node N in tree T
if head of N = ?
insert N in unattached subtrees[]
if !? a verb in unattached subtrees[]
if those conjuncts belongs to conjunct set
insert insert NULL with VM as POS,
NULL VG as CPOS
Table 9: Rule for identifying Finite Verb ellipses using
CBHP.
the input sentence using the rule in Table 8 .Also,
in a sentence with a VG, we use CBHP to ascertain
if this VG is the root of the sentence. If its not, we
insert an additional NULL VG. This algorithm will
correctly recover VG in the sentence but the position
can be different from the gold input at times not be-
cause the recovery algorithm is wrong, but there is
no strict rule that says the exact position of empty
category in this case of finite verb ellipse and anno-
tators might choose to insert an empty category at
any position. For example, in Figure 8, we can in-
sert an empty category either after first NP sub tree
or second or the third etc, all these possibilities are
accepted syntactically. For simplicity purposes, we
insert the empty category just before the EOS. This
procedure is shown in Table 9.
3.5 Conjunction ellipses (for verbs)
We again use the intermediate parsed output of
CBHP for this type. Whenever there is a miss-
ing conjunction between the two finite clauses, the
clausal sub trees are disconnected from each other
as shown in Figure 9. Hence the rule that should
be applied is to insert a NULL CCP between two
sub trees with VG heads and insert NULL CCP im-
mediately after the first verb in the input sentence.
Table 10 shows this procedure.
139
Figure 8: Unattached Subtrees (Finite verb ellipses).
Figure 9: Unattached Subtrees in the case of conjunction ellipses.
for each node N in tree T
if head of N = ?
insert N in unattached subtrees[]
for each node X in unattached subtrees[]
if X and X+1 are VG?s
insert insert NULL with CC as POS,
NULL CCP as CPOS
Table 10: Rule for identifying Finite Verb ellipses using
CBHP.
4 Results and Discussion
We have presented two sets of results, the overall
empty categories detection along with the accuracies
of individual types of empty categories in Table 11
and Table 12.
The results in Table 12 show that the precision in
recovering many empty categories is close to 90%.
A high precision value of 89.8 for recovery of Empty
subject type is due to the strong lexical cues that
were found during our analysis. CBHP parse out-
put proved helpful in most of the remaining types.
Few cases such as backward gapping and conjunc-
Type of empty Inst- Prec- Recall
categories ances ision
Empty subject 69 89.8 89.8
Backward gapping 29 77.7 48.3
Forward gapping 21 88.8 72.7
Finite verb ellipses 18 78.5 61.1
Conjunction ellipses 20 88.2 75
(verbs)
Conjunction ellipses 12 0 0
(nouns)
Total 169 91.4 69.8
Table 11: Recovery of empty categories in Hindi tree-
bank.
tion ellipses (for nouns) are very difficult to handle.
We see that although CBHP helps in the recovery
process by providing unattached subtrees in many
instances, there are cases such as those of backward
gapping and nominal conjunction ellipses where it
does not help. It is not difficult to see why this is
so. The presence of the 2nd verb in the case of back-
ward gapping fools CBHP into treating it as the main
verb of a normal finite clause. In such a case, the
140
Type of empty Inst- Prec- Recall
categories ances ision
NULL NP tokens 69 89.8 89.8
NULL VG tokens 68 82 60.2
NULL CCP tokens 32 88.2 46.8
Total 159 91.4 69.8
Table 12: Empty categories in Hindi Tree bank
parser ends up producing a fully formed tree (which
of course is a wrong analysis) that is of no use for
us.
Similar problem is faced while handling conjunc-
tion ellipses (for nouns). Here as in the previous
case, CBHP is fooled into treating two coordinat-
ing nominals as independent nouns. We note here
that both the cases are in fact notoriously difficult
to automatically detect because of the presence (or
absence) of any robust linguistic pattern.
These results show that our system can be used to
supplement the annotators effort during treebanking.
We plan to use our system during the ongoing Hindi
treebanking to ascertain it effect. As mentioned ear-
lier, automatic detection of empty categories/nodes
will prove to be indis pensable for parsing a sen-
tence. We also intend to see the effect of our system
during the task of parsing.
5 Conclusion
In this paper we presented an empty category recov-
ery algorithm by analyzing the empty categories in
the Hindi treebank. This, we noticed, uses lexical
cues and parsed output of a constraint based parser.
The results show that our system performs consid-
erably high ( 90%) for many types of empty cate-
gories. Few types, on the other hand, such as back-
ward gapping and nominal coordinating conjunc-
tions were very difficult to handle. Our approach
and analysis will be useful in automatic insertion of
empty nodes during dependency annotation. It will
also benefit data-driven/statistical approaches either
as a post-processing tool or in recovering empty cat-
egories by helping in feature selection for various
machine learning techniques.
Acknowledgments
We would like to thank Prof. Rajeev Sangal for pro-
viding valuable inputs throughout the work.
References
R. Begum, S. Husain, A. Dhwaj, D. Sharma, L. Bai,
and R. Sangal. Dependency annotation scheme for
Indian languages. 2008. In proceedings of Third
International Joint Conference on Natural Language
Processing (IJCNLP), Hyderabad, India
A. Bharati, S. Husain, D. Misra, and R. Sangal. Two
stage constraint based hybrid approach to free word
order language dependency parsing. 2009a. In
Proceedings of the 11th International Conference on
Parsing Technologies (IWPT). Paris.
A. Bharati, D. Sharma, S. Husain, L. Bai, R. Begam, and
R. Sangal. Anncorra: Treebanks for indian languages,
guidelines for annotating hindi treebank. 2009b.
http://ltrc.iiit.ac.in/MachineTrans/research/tb/DS-
guidelines/DS-guidelines-ver2-28-05-09.pdf
A. Bhatia, R. Bhatt, B. Narasimhan, M. Palmer, O. Ram-
bow, D. Sharma, M. Tepper, A. Vaidya, and F. Xia.
Empty Categories in a Hindi Treebank. 2010. In the
Proceedings of the 7th International Conference on
Language Resources and Evaluation (LREC).
R. Campbell. Using linguistic principles to recover
empty categories. 2004. In Proceedings of the 42nd
Annual Meeting of the Association for Computational
Linguistics
A. Chanev. Portability of dependency parsing
algorithms?an application for Italian. 2005. In
Proc. of the fourth workshop on Treebanks and
Linguistic Theories (TLT). Citeseer.
M. Collins. Three generative, lexicalised models for
statistical parsing. 1997. In Proceedings of the 35th
Annual Meeting of the Association for Computational
Linguistics and Eighth Conference of the European
Chapter of the Association for Computational Lin-
guistics.
P. Dienes and A. Dubey. Antecedent recovery: Experi-
ments with a trace tagger. 2003a. In Proceedings of
the 2003 conference on Empirical methods in natural
language processing.
141
P. Dienes and A. Dubey. Deep syntactic processing by
combining shallow methods. 2003b. In Proceedings
of the 41st Annual Meeting on Association for Com-
putational Linguistics-Volume 1.
D. Higgins. A machine-learning approach to the identifi-
cation of WH gaps. 2003. In Proceedings of the tenth
conference on European chapter of the Association
for Computational Linguistics-Volume 2.
X. Fei, O. Rambow, R. Bhatt, M. Palmer, and D. Sharma.
Towards a multi-representational treebank. 2008.
Proc. of the 7th Int?lWorkshop on Treebanks and
Linguistic Theories (TLT-7)
M. Johnson. A simple pattern-matching algorithm
for recovering empty nodes and their antecedents.
2002. In Proceedings of the 40th Annual Meeting on
Association for Computational Linguistics.
M. Marcus, M. Marcinkiewicz, and B. Santorini. Build-
ing a large annotated corpus of English: The Penn
Treebank. 1993. Computational linguistics.
R. McDonald, F. Pereira, K. Ribarov, and J. Hajic?.
Non-projective dependency parsing using spanning
tree algorithms. 2005. In Proceedings of the confer-
ence on Human Language Technology and Empirical
Methods in Natural Language Processing.
J. Nivre. An efficient algorithm for projective depen-
dency parsing. 2003. In Proceedings of the 8th
International Workshop on Parsing Technologies
(IWPT).
M. Palmer, R. Bhatt, B. Narasimhan, O. Rambow,
D. Sharma, and F. Xia. Hindi Syntax: Annotating
Dependency, Lexical Predicate-Argument Structure,
and Phrase Structure. 2009. In The 7th International
Conference on Natural Language Processing.
Y. Yang and N. Xue. Chasing the ghost: recovering
empty categories in the Chinese Treebank. 2010. In
Proceedings of the 23rd International Conference on
Computational Linguistics: Posters.
142
Proceedings of the Sixth Workshop on Innovative Use of NLP for Building Educational Applications, pages 1?9,
Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguistics
Automatic Question Generation using Discourse Cues
Manish Agarwal?, Rakshit Shah? and Prashanth Mannem
Language Technologies Research Center
International Institute of Information Technology
Hyderabad, AP, India - 500032
{manish.agarwal, rakshit.shah, prashanth}@research.iiit.ac.in
Abstract
In this paper, we present a system that au-
tomatically generates questions from natural
language text using discourse connectives. We
explore the usefulness of the discourse con-
nectives for Question Generation (QG) that
looks at the problem beyond sentence level.
Our work divides the QG task into content se-
lection and question formation. Content se-
lection consists of finding the relevant part in
text to frame question from while question for-
mation involves sense disambiguation of the
discourse connectives, identification of ques-
tion type and applying syntactic transforma-
tions on the content. The system is evaluated
manually for syntactic and semantic correct-
ness.
1 Introduction
Automatic QG from sentences and paragraphs has
caught the attention of the NLP community in the
last few years through the question generation work-
shops and the shared task in 2010 (QGSTEC, 2010).
Previous work in this area has concentrated on gen-
erating questions from individual sentences (Varga
and Ha, 2010; Paland et al, 2010; Ali et al, 2010).
Sneiders and E. (2002) used question templates and
Heilman et al (2009) used general-purpose rules
to transform sentences into questions. A notable
exception is Mannem et al (2010) who generated
questions of various scopes (general, medium and
specific) ? 1 from paragraphs instead of individual
?First two authors contributed equally to this work
1General scope - entire or almost entire paragraph, Medium
scope - multiple clauses or sentences, and Specific scope - sen-
sentences. They boil down the QG from paragraphs
task into first identifying the sentences in the para-
graph with general, medium and specific scopes and
then generating the corresponding questions from
these sentences using semantic roles of predicates.
Discourse connectives play a vital role in mak-
ing the text coherent. They connect two clauses
or sentences exhibiting discourse relations such as
temporal, causal, elaboration, contrast, result,
etc. Discourse relations have been shown to be use-
ful to generate questions (Prasad and Joshi, 2008)
but identifying these relations in the text is a difficult
task (Pitler et al, 2009). So in this work, instead of
identifying discourse relations and generating ques-
tions using them, we explore the usefulness of dis-
course connectives for QG. We do this by analyzing
the senses of the connectives that help in QG and
propose a system that makes use of this analysis to
generate questions of the type why, when, give an
example and yes/no.
The two main problems in QG are identifying the
content to ask a question on and finding the corre-
sponding question type for that content. We ana-
lyze the connectives in terms of the content useful
for question generation based on the senses they ex-
hibit. We show that the senses of the connectives
further help in choosing the relevant question type
for the content.
In this paper, we present an end-to-end QG sys-
tem that takes a document as input and outputs all
the questions generated using the selected discourse
connectives. The system has been evaluated man-
ually by two evaluators for syntactic and semantic
tence or less
1
correctness of the generated questions. The over-
all system has been rated 6.3 out of 8 for QGSTEC
development dataset and 5.8 out of 8 for Wikipedia
dataset.
2 Overview
Question Generation involves two tasks, content
selection (the text selected for question generation)
and question formation (transformations on the con-
tent to get the question). Question formation further
has the subtasks of (i) finding suitable question type
(wh-word), (ii) auxiliary and main verb transforma-
tions and (iii) rearranging the phrases to get the final
question.
There are 100 distinct types of discourse connec-
tives listed in PDTB manual (PDTB, 2007). The
most frequent connectives in PDTB are and, or,
but, when, because, since, also, although, for
example, however and as a result. In this paper,
we provide analysis for four subordinating conjunc-
tions, since, when, because and although, and three
adverbials, for example, for instance and as a re-
sult. Connectives such as and, or and also show-
ing conjunction relation have not been found to
be good candidates for generating wh-type ques-
tions and hence have not been discussed in the pa-
per. Leaving aside and, or and also, the selected
connectives cover 52.05 per cent of the total number
of the connectives in QGSTEC-2010 2 dataset and
41.97 per cent in Wikipedia articles. Connective-
wise coverage in both the datasets is shown in Table
1. Though but and however denoting contrast re-
lation occur frequently in the data, it has not been
feasible to generate wh-questions using them.
QGSTEC-2010 Dev. Data Wikipedia Dataset
Connective count % count %
because 20 16.53 36 10.28
since 9 7.44 18 5.14
when 23 19.00 35 10.00
although 4 3.30 22 6.28
as a result 5 4.13 6 1.71
for example 2 1.65 30 8.28
for instance 0 0.00 1 0.28
Total 121 52.05 350 41.97
Table 1: Coverage of the selected discourse connec-
tives in the data
The system goes through the entire document and
2QGSTEC 2010 data set involves Wikipedia, Yahoo An-
swers and OpenLearn articles.
identifies the sentences containing at least one of the
seven discourse connectives. In our approach, suit-
able content for each discourse connective which is
referred to as target argument is decided based on
the properties of discourse connective. The system
finds the question type on the basis of discourse re-
lation shown by discourse connective.
3 Discourse connectives for QG
In this section, we provide an analysis of dis-
course connectives with respect to their target argu-
ments and the question types they take.
3.1 Question type identification
The sense of the discourse connective influences
the question-type (Q-type). Since few discourse
connectives such as when, since and although
among the selected ones can show multiple senses,
the task of sense disambiguation of the connectives
is essential for finding the question type.
Since: The connective can show temporal, causal
or temporal + causal relation in a sentence. Sen-
tence exhibits temporal relation in presence of key-
words like time(7 am), year (1989 or 1980s), start,
begin, end, date(9/11), month (January) etc. If the
relation is temporal then the question-type is when
whereas in case of causal relation it would be why.
1. Single wicket has rarely been played since lim-
ited overs cricket began.
Q-type: when
2. Half-court games require less cardiovascular
stamina , since players need not run back and
forth a full court.
Q-type: why
In examples 1 and 2, 1 is identified to show tem-
poral relation because it has the keyword began
whereas there is no keyword in the context of ex-
ample 2 that gives the hint of temporal relation and
so the relation here is identified as causal.
When: Consider the sentences with connec-
tive when in Figure 1. Although when shows
multiple senses (temporal, temporal+causal and
conditional), we can frame questions by a single
question type, when. Given a new instance of
the connective, finding the correct sense of when
2
Sentence:  The San?Francisco earthquake hit when resources in the field already were stetched. (Temporal)
Sentence:  Venice?s long decline started in the 15th century, when it first made an unsuccessful attempt to hold Thessalonica
Sentence:   Earthquake mainly occurs when the different blocks or plates that make up the Earth?s surface move relative to
Question:   When do earthquake mainly occur ?     
                    each other, causing distortion in the rock. ( Conditional ) 
Question:   When did San?Francisco earthquake hit ? 
against the Ottomans (1423?1430). ( Temporal + Causal ) 
Question:  When did Venice?s long decline start in the 15th century ?
Figure 1: Questions for discourse connective when
Discourse Sense Q-type
connectives
because causal why
since temporal when
causal why
when
causal + temporal
whentemporal
conditional
although contrast yes/ no
concession
as a result result why
for example instantiation give an example
where
for instance instantiation give an instance
where
Table 2: Question type for discourse connectives
becomes unnecessary as a result of using discourse
connectives.
Although: The connective can show concession
or contrast discourse relations. It is difficult to frame
a wh-question on contrast or concession relations.
So, system generates a yes/no type question for al-
though. Moreover, yes/no question-type adds to the
variety of questions generated by the system.
3. Greek colonies were not politically controlled
by their founding cities , although they often
retained religious and commercial links with
them .
Q-type: Yes/No
A yes/no question could have been asked for
connectives but and however denoting a contrast re-
lation but it was not done to preserve the question-
type variety in the final output of the QG system.
Y es/no questions have been asked for occurrences
of although since they occur less frequently than
but and however.
Identifying the question types for other selected
discourse connectives is straight forward because
they broadly show only one discourse relation
(Pitler and Nenkova, 2009). Based on the relations
exhibited by these connectives, Table 2 shows the
question types for each discourse connective.
3.2 Target arguments for discourse connectives
A discourse connective can realize its two argu-
ments, Arg1 and Arg2, structurally and anaphori-
cally. Arg2 is always realized structurally whereas
Arg1 can be either structural or anaphoric (PDTB,
2007; Prasad et al, 2010).
4. [Arg1 Organisms inherit the characteristics of
their parents] because [Arg2 the cells of the
offspring contain copies of the genes in their
parents? cells.](Intra-sentential connective be-
cause)
5. [Arg1 The scorers are directed by the hand sig-
nals of an umpire.] For example, [Arg2 the
umpire raises a forefinger to signal that the
batsman is out (has been dismissed); he raises
both arms above his head if the batsman has
hit the ball for six runs.](Inter-sentential con-
nective for example)
Consider examples 4 and 5. In 4, Arg1 and Arg2
are the structural arguments of the connective be-
cause whereas in 5, Arg2 is the structural argument
and Arg1 is realized anaphorically.
The task of content selection involves finding the
target argument (either Arg1 or Arg2) of the dis-
course connective. Since both the arguments are po-
tential candidates for QG, we analyze the data to
identify which argument makes better content for
each of the connectives. Our system selects one of
the two arguments based on the properties of the dis-
course connectives. Table 3 shows the target argu-
3
Discourse connective Target argument
because Arg1
since Arg1
when Arg1
although Arg1
as a result Arg2
for example Arg1
for instance Arg1
Table 3: Target argument for discourse connectives
ment i.e. either Arg1 or Arg2, which is used as con-
tent for QG.
4 Target Argument Identification
Target argument for a discourse connective can
be a clause(s) or a sentence(s). It could be one or
more sentences in case of inter-sentential3 discourse
connectives, whereas one or more clauses in case of
intra-sentential4 connectives.
Discourse connectives for example and for in-
stance can realize its Arg1 anywhere in the prior dis-
course (Elwell and Baldridge, 2008). So the system
considers only those sentences in which the connec-
tives occur at the beginning of the sentence and the
immediate previous sentence is assumed to be the
Arg1 of the connective (which is the target argument
for QG).
In case of intra-sentential connectives (because,
since, although and when) and as a result (target ar-
gument is Arg2 which would be a clause), identifi-
cation of target argument is done in two steps. The
system first locates the syntactic head or head verb
of the target argument and then extracts it from the
dependency tree of the sentence.
4.1 Locate syntactic head
Approach for locating the syntactic head of tar-
get argument is explained with the help of Figure 2
(generic dependency trees) and an example shown
in Figure 3. Syntactic head of Arg2 is the first fi-
nite verb while percolating up in the dependency tree
starting from the discourse connective. In case of
intra-sentential connectives where Arg1 is the target
argument, the system percolates up until it gets the
second finite verb which is assumed to be target head
3Connectives that realize its Arg1 anaphorically and Arg2
structurally
4Connectives that realize both of its arguments structurally
X          P         Z                   
DC        A
(a)                                            (b)
1   1
   
V                                           V 
2  X          V           Z                 
V2
DC         A                                     Q  
Figure 2: Head selection of the target argument
for intra-sentential connectives (V1,V2: finite verbs;
X,Z: subtrees of V1; A: subtree of V2; P,Q:Not verbs;
DC:discourse connective(child of V2))
of Arg1. Number of percolations entirely depend on
structure and complexity of the sentence. Figure 2
shows two dependency trees (a) and (b). Starting
from the discourse connective DC and percolating
up, the system identifies that the head of Arg2 is V2
and that of Arg1 is V1.
  
 
aux : "is"
played
competitive
badminton
is    indoors     
by     because   flight    is
   wind                shuttlecock
Why is competitive badminton played indoors ?
affected
Because 
(From section 2.1)                      (From section 2.2)
qtype : "Why"                        Target Arg Head : "played"
(section 2.3)
              [Arg2 shuttlecock flight is affected by wind],
[Arg1 competitive badminton is played indoors].(content)
Figure 3: Question Generation process
Since the discourse connective in the example of
Figure 3 is because, the target argument is Arg1
(from Table 2). By percolating up the tree starting
from because, the head of Arg2 is affected and that
of Arg1 is played. Once we locate the head of the
target argument, we find the auxiliary as Mannem
et al (2010) does. For the example in Figure 3, the
auxiliary for question generation is is.
4.2 Target Argument Extraction
The extraction of the target argument is done af-
ter identifying its syntactic head. For as a result,
the target argument, Arg2, is the subtree with head
4
Score Description Example
4 The question is grammatically correct and idiomatic/natural. In which type of animals are phagocytes highly developed?
3 The question is grammatically correct but does not read as In which type of animals are phagocytes, which are importantfluently as we would like. throughout the animal kingdom, highly developed?
2 There are some grammatical errors in the question. In which type of animals is phagocytes, which are important
throughout the animal kingdom, highly developed?
1 The question is grammatically unacceptable. On which type of animals is phagocytes, which are important
throughout the animal kingdom, developed?
Table 4: Evaluation guidelines for syntactic correctness measure
as the head of the connective. For intra-sentential
connectives, the target argument, Arg1, is the tree
remaining after removing the subtree that contains
Arg2.
In Figures 2 (a) and (b) both, a tree with head
V1 and its children, X and Z, is left after removing
Arg2 from dependency trees, which is the content
required for generating the question. Note that in the
tree of Figure 2(b), the child P of the head verb V1 is
removed with its entire subtree that contains Arg2.
Thus, subtree with head V2 is the unwanted part for
the tree in Figure 2(a) whereas subtree with head P
is the unwanted part for the tree in Figure 2(b) when
the target argument is Arg1.
In Figure 3, after removing the unwanted argu-
ment Arg2 (subtree with head affected), the system
gets competitive badminton is played indoors which
is the required clause (content) for question genera-
tion. The next section describes how the content is
transformed into a question.
5 Syntactic Transformations and Question
Generation
The syntactic transformations used in this work
are similar to those by Mannem et al (2010). At this
stage, the system has the question type, auxiliary and
the content. The following set of transformations
are applied on the content to get the final question.
(1) If the auxiliary is present in the sentence itself
then it is moved to the beginning of the sentence;
otherwise auxiliary is added at the beginning of the
sentence. (2) If a wh-question is to be formed, the
question word is added just before the auxiliary. In
case of Yes/No questions, the question starts with
the auxiliary itself as no question word is needed. (3)
A question-mark(?) is added at the end to complete
the question.
Consider the example in Figure 3. Here the con-
tent is competitive badminton is played indoors.
Applying the transformations, the auxiliary is first
moved at the start of the sentence to get is compet-
itive badminton played indoors. Then the question
type Why is added just before the auxiliary is, and
a question-mark is added at the end to get the final
question, Why is competitive badminton played in-
doors ?
Scope: In QGSTEC 2010 the question had to be
assigned a scope, specific, medium or general. The
scope is defined as: general - entire input paragraph,
medium - one or more clauses or sentences and spe-
cific - phrase or less. Questions generated using dis-
course connectives are usually of the scope specific
or medium. Mannem et al (2010) assigned medium
scope to the questions generated using the seman-
tic roles such as ARGM-DIS (result), ARGM-CAU
(causal) and ARGM-PNC (purpose) given by the
SRL. However, most of the times, the scope of the
answer to these questions is just a clause or a sen-
tence and should have been assigned specific scope
instead of medium.
6 Evaluation and Results
Automatic evaluation of any natural language
generated text is difficult. So, our system is eval-
uated manually. The evaluation was performed
by two graduate students with good English profi-
ciency. Evaluators were asked to rate the questions
on the scale of 1 to 4 (4 being the best score) on syn-
tactic and semantic correctness (Evalguide, 2010)
of the question and an overall rating on the scale of
8 (4+4) is assigned to each question.
The syntactic correctness is rated to ensure that
the system can generate grammatical output. In ad-
dition, those questions which read fluently are given
greater score. The syntactic correctness and fluency
is evaluated using the following scores: 4 - gram-
5
Discourse ExampleConnective
because
One-handed backhand players move to the net with greater ease than two-handed players
because the shot permits greater forward momentum and has greater similarities in muscle
memory to the preferred type of backhand volley (one-handed, for greater reach ).
Why do one-handed backhand players move to the net with greater ease than two-handed
players ? (Causal)
since
Half-court games require less cardiovascular stamina, since players need not run back and
forth a full court.
Why do half-court games require less cardiovascular stamina ? (Causal)
Single wicket has rarely been played since limited overs cricket began.
Since when has single wicket rarely been played ? (Temporal)
when A one-point shot can be earned when shooting from the foul line after a foul is made.When can a one-point shot be earned ? (Conditional)
although
A bowler cannot bowl two successive overs, although a bowler can bowl unchanged at
end for several overs.
Can a bowler bowl unchanged at the same end for several overs? (Contrast, concession)
as a result
In the United States sleep deprivation is common with students because almost all schools
begin early in the morning and many of these students either choose to stay up awake late into
the night or cannot do otherwise due to delayed sleep phase syndrome. As a result, students
that should be getting between 8.5 and 9.25 hours of sleep are getting only 7 hours.
Why are students that should be getting between 8.5 and 9.25 hours of sleep getting
only 7 hours? (Result)
As a result of studies showing the effects of sleep-deprivation on grades , and the different
sleep patterns for teenagers , a school in New Zealand , changed its start time to 10:30,
in 2006, to allow students to keep to a schedule that allowed more sleep.
Why did a school in New Zealand change its start time ? (Result)
for example
Slicing also causes the shuttlecock to travel much slower than the arm movement suggests.
For example, a good cross court sliced drop shot will use a hitting action that suggests a straight
clear or smash, deceiving the opponent about both the power and direction of the shuttlecock.
Give an example where slicing also causes the shuttlecock to travel much slower than
the arm movement suggests. (Instantiation)
for instance
If the team that bats last scores enough runs to win, it is said to have ?won by n wickets?,
where n is the number of wickets left to fall. For instance a team that passes its opponents?
score having only lost six wickets would have won ?by four wickets?.
Give an instance where if the team that bats last scores enough runs to win, it is said to have
?won by n wickets?,where n is the number of wickets left to fall. (Instantiation)
Table 5: Examples
matically correct and idiomatic/natural, 3 - gram-
matically correct, 2 - some grammar problems, 1 -
grammatically unacceptable. Table 4 shows syntac-
tic correctness measure with examples.
The semantic correctness is evaluated using the
following scores: 4 - semantically correct and id-
iomatic/natural, 3 - semantically correct and close to
the text or other questions, 2 - some semantic issues,
1 - semantically unacceptable.
Table 5 shows questions generated by the system
for each connective. The results of our system on
QGSTEC-2010 development dataset are shown in
Table 6. The overall system is rated 6.3 out of 8 on
this dataset and the total number of questions gen-
erated for this dataset is 61. The instances of the
connectives were less in the QGSTEC-2010 devel-
opment dataset. So, the system is further tested on
five Wikipedia articles (football, cricket, basketball,
badminton and tennis) for effective evaluation. Re-
sults on this dataset are presented in Table 7. Overall
rating of the system is 5.8 out of 8 for this dataset
and 150 are the total number of questions generated
for this dataset. The ratings presented in the Tables 6
and 7 are the average of the ratings given by both the
evaluators. The inter-evaluator agreement (Cohen?s
kappa coefficient) for the QGSTEC-2010 develop-
6
ment dataset for syntactic correctness measure is 0.6
and is 0.5 for semantic correctness measure, and in
case of Wikipedia articles the agreement is 0.7 and
0.6 for syntactic and semantic correctness measures
respectively.
Discourse No. of Syntactic Semantic Overall
connective questions Correctness(4) Correctness(4) Rating(8)
because 20 3.6 3.6 7.2
since 9 3.8 3.2 7
when 23 2.3 2.2 4.5
although 4 4 3.8 7.8
as a result 5 4 4 8
Overall 61 3.2 3.1 6.3
Table 6: Results on QGSTEC-2010 development
dataset
Discourse No. of Syntactic Semantic Overall
connective questions Correctness(4) Correctness(4) Rating(8)
because 36 3.3 3.2 6.5
since 18 3.1 3 6.1
when 35 2.4 2.0 4.4
although 22 3.1 2.8 5.9
as a result 6 3.6 3.2 6.8
for example 16 3.1 2.9 6.0
for instance 2 4 3 7
Overall 135 3.0 2.8 5.8
Table 7: Results on the Wikipedia data(cricket, foot-
ball, basketball, badminton, tennis)
On analyzing the data, we found that the
Wikipedia articles have more complex sentences
(with unusual structure as well as more number of
clauses) than QGSTEC-2010 development dataset.
As a result, the system?s performance consistently
drops for all the connectives in case of Wikipedia
dataset.
No comparable evaluation was done as none of
the earlier works in QG exploited the discourse con-
nectives in text to generate questions.
7 Error Analysis
An error analysis was carried out on the system?s
output and the four most frequent types of errors are
discussed in this section.
7.1 Coreference resolution
The system doesn?t handle coreference resolution
and as a result of this, many questions have been
rated low for semantic correctness by the evalua-
tors. Greater the number of pronouns in the ques-
tion, lesser is the semantic rating of the question.
6. They grow in height when they reach shallower
water, in a wave shoaling process.
Question: When do they grow in height?
Although the above example 6 is syntactically
correct, such questions are rated semantically low
because the context is not sufficient to answer the
question due to the pronouns in it. 13.54% of
the generated questions on the Wikipedia dataset
have pronouns without their antecedents, making the
questions semantically insufficient.
7.2 Parsing Errors
Sometimes the parser fails to give a correct parse
for the sentences with complex structure. In such
cases, the system generates a question that is unac-
ceptable. Consider the examples below.
7. In a family who know that both parents are car-
riers of CF , either because they already have a
CF child or as a result of carrier testing , PND
allows the conversion of a probable risk of the
disease affecting an unborn child to nearer a
certainty that it will or will not be affected.
Question: Why do in a family who know that
both parents are carriers of CF , either or will
not be affected ?
In example 7 above, the sentence has a com-
plex structure containing paired connective, either-
or, where the argument of either has because and
that of or has as a result in it. Here the question is
formed using because which is correct neither syn-
tactically nor semantically due to the complex nature
of the sentence. 9.38% sentences in the datasets are
complex with either three or more discourse connec-
tives.
7.3 Errors due to the inter-sentential
connectives
For inter-sentential connectives, system considers
only those sentences in which the connectives occur
at the beginning of the sentence and the immediate
previous sentence is assumed to be the Arg1 of the
connective (which is the target argument for QG).
But this assumption is not always true. Of the total
number of instances of these connectives, 52.94%
(for Wikipedia dataset) connectives occur at the be-
ginning of the sentences. Consider the paragraph be-
low.
8. A game point occurs in tennis whenever the
7
player who is in the lead in the game needs only
one more point to win the game. The termi-
nology is extended to sets (set point), matches
(match point), and even championships (cham-
pionship point). For example, if the player who
is serving has a score of 40-love, the player has
a triple game point (triple set point, etc.) as the
player has three consecutive chances to win the
game.
Here in example 8, the third sentence in which the
example is specified is related to the first sentence
but not the immediately previous sentence. For these
connectives, the assumption that immediate previ-
ous sentence is Arg1 is false 14.29% of the times.
7.4 Fluency issues
The system does not handle the removal of pred-
icative adjuncts. So the questions with optional
phrases in it are rated low for syntactic correctness
measure.
8 Conclusions and Future Work
Our QG system generates questions using dis-
course connectives for different question types. In
this work, we present an end-to-end system that
takes a document as input and outputs all the ques-
tions for selected discourse connectives. The system
has been evaluated for syntactic and semantic sound-
ness of the question by two evaluators. We have
shown that some specific discourse relations are im-
portant such as causal, temporal and result than
others from the QG point of view. This work also
shows that discourse connectives are good enough
for QG and that there is no need for full fledged dis-
course parsing. In the near future, we plan to im-
plement coreference resolution and sentences with
more than two connectives. We aim to improve the
system with respect to the sentence complexity and
also incorporate other discourse connectives.
Acknowledgements
We would like to thank Suman Yelati and Sudheer
Kolachina from LTRC, IIIT-Hyderabad for their
helpful discussions and pointers during the course of
this work. Thanks also to the anonymous reviewers
for useful feedback.
References
2010 Question generation shared task and evaluation
challenge, http://questiongeneration.org/QG2010
Andrea Varga and Le An Ha 2010 WLV: A Question
Generation System for the QGSTEC 2010 Task B,
Proceedings of QG2010: The Third Workshop on
Question Generation
Santanu Paland Tapabrata Mondal, Partha Pakray,
Dipankar Das and Sivaji Bandyopadhyay 2010
QGSTEC System Description JUQGG: A Rule based
approach , Proceedings of QG2010: The Third
Workshop on Question Generation
Husam Ali, Yllias Chali, and Sadid A. Hasan 2010
Automation of Question Generation From Sentences,
Proceedings of QG2010: The Third Workshop on
Question Generation
Eriks Sneiders 2002. Automated question answering
using question templates that cover the conceptual
model of the database. In Proceedings of the 6th
International Conference on Applications of Natural
Language to Information Systems (pp. 235-239).
Michael Heilman, Noah A. Smith. 2009 Question gener-
ation via overgenerating transformations and ranking
Technical Report CMU-LTI-09-013, Carnegie Mellon
University.
Prashanth Mannem, Rashmi Prasad and Aravind Joshi
2010 Question Generation from Paragraphs at
UPenn: QGSTEC System Discription, Proceedings of
QG2010: The Third Workshop on Question Genera-
tion
Rashmi Prasad and Aravind Joshi 2008 A Discourse-
based Approach to Generating why-Questions from
text, Proceedings of the Workshop on the Question
Generation Shared Task and Evaluation Challenge
Arlington, VA, September 2008
Emily Pitler, Annie Louis and Ani Nenkova 2009 Auto-
matic sense prediction for implicit discourse relations
in text , ACL ?09 Proceedings of the Joint Conference
of the 47th Annual Meeting of the ACL and the
4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP: Volume 2 - Volume 2
2007 PDTB 2.0 Annotation Manual,
http://www.seas.upenn.edu/ pdtb/PDTBAPI/pdtb-
annotation-manual.pdf
8
Emily Pitler and Ani Nenkova 2009 Using syntax to
Disambiguate Explicit Discourse Connectives in Text,
ACLShort ?09 Proceedings of the ACL-IJCNLP 2009
Conference Short Papers
Rashmi Prasad and Aravind Joshi and Bonnie Webber
2010 Exploiting Scope for Shallow discourse Parsing,
LREC2010
Robert Elwell and Jason Baldridge 2008 Discourse
connective argument identification with connective
specific rankers, In Proceedings of ICSC-2008
Evaluation guidelines 2010 In QGSTEC-2010 Task B
evaluation guidelines, http://www.question genera-
tion.org/QGSTEC2010/ uploads/QG-fromSentences-
v2.doc
9
Proceedings of the Sixth Workshop on Innovative Use of NLP for Building Educational Applications, pages 56?64,
Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguistics
Automatic Gap-fill Question Generation from Text Books
Manish Agarwal and Prashanth Mannem
Language Technologies Research Center
International Institute of Information Technology
Hyderabad, AP, India - 500032
{manish.agarwal,prashanth}@research.iiit.ac.in
Abstract
In this paper, we present an automatic question
generation system that can generate gap-fill
questions for content in a document. Gap-fill
questions are fill-in-the-blank questions with
multiple choices (one correct answer and three
distractors) provided. The system finds the in-
formative sentences from the document and
generates gap-fill questions from them by first
blanking keys from the sentences and then de-
termining the distractors for these keys. Syn-
tactic and lexical features are used in this pro-
cess without relying on any external resource
apart from the information in the document.
We evaluated our system on two chapters of
a standard biology textbook and presented the
results.
1 Introduction
Gap-fill questions are fill-in-the-blank questions,
where one or more words are removed from a
sentence/paragraph and potential answers are listed.
These questions, being multiple choice ones, are
easy to evaluate. Preparing these questions manu-
ally will take a lot of time and effort. This is where
automatic gap-fill question generation (GFQG)
from a given text is useful.
1. A bond is the sharing of a pair of va-
lence electrons by two atoms.
(a) Hydrogen (b) Covalent (c) Ionic (d) Double
(correct answer: Covalent)
In a gap-fill question (GFQ) such as the one
above, we refer to the sentence with the gap as the
question sentence (QS) and the sentence in the text
that is used to generate the QS as the gap-fill sen-
tence (GFS). The word(s) which is removed from a
GFS to form the QS is referred to as the key while
the three alternatives in the question are called as
distractors, as they are used to distract the students
from the correct answer.
Previous works in GFQG (Sumita et al, 2005;
John Lee and Stephanie Seneff, 2007; Lin et al,
2007; Pino et al, 2009; Smith et al, 2010) have
mostly worked in the domain of English language
learning. Gap-fill questions have been generated to
test student?s knowledge of English in using the cor-
rect verbs (Sumita et al, 2005), prepositions (John
Lee and Stephanie Seneff, 2007) and adjectives (Lin
et al, 2007) in sentences. Pino et al (2009) and
Smith et al (2010) have generated GFQs to teach
and evaluate student?s vocabulary.
In this paper, we move away from the domain
of English language learning and work on generat-
ing gap-fill questions from the chapters of a biol-
ogy textbook used for Advanced Placement (AP) ex-
ams. The aim is to go through the textbook, identify
informative sentences1 and generate gap-fill ques-
tions from them to aid students? learning. The sys-
tem scans through the text in the chapter and iden-
tifies the informative sentences in it using features
inspired by summarization techniques. Questions
from these sentences (GFSs) are generated by first
choosing a key in each of these and then finding ap-
propriate distractors for them from the chapter.
Our GFQG system takes a document with its title
as an input and produces a list of gap-fill questions as
1A sentence is deemed informative if it has the relevant
course knowledge which can be questioned.
56
output. Unlike previous works (Brown et al, 2005;
Smith et al, 2010) it doesn?t use any external re-
source for distractor selection, making it adaptable
to text from any domain. Its simplicity makes it use-
ful not only as an aid for teachers to prepare gap-fill
questions but also for students who need an auto-
matic question generator to aid their learning from a
textbook.
2 Data Used
A Biology text book Campbell Biology, 6th Edi-
tion has been used for work in this paper. We have
reported results of our system on 2 chapters (the
structure and function of macromolecules and an
introduction to metabolism ) of unit 1. Each chapter
contains sections and subsections with their respec-
tive topic headings. Number of subsections, sen-
tences, words per sentence in each chapter are (25,
416, 18.3) and (32, 423, 19.5) respectively. Each
subsection is taken as a document. The chapters are
divided into documents and each document is used
for GFQG independently.
3 Approach
Given a document, the gap-fill questions are gen-
erated from it in three stages: sentence selection,
key selection and distractor selection. Sentence se-
lection involves identifying informative sentences in
the document which can be used to generate a gap-
fill question. These sentences are then processed in
the key selection stage to identify the key on which
to ask the question. In the final stage, the distrac-
tors for the selected key are identified from the given
chapter by searching for words with the same con-
text as that of the key.
In each stage, the system identifies a set of candi-
dates (i.e. all sentences in the document in stage I,
words in the previously selected sentence in stage II
and words in the chapter in stage III) and extracts a
set of features relevant to the task. Weighted sum of
extracted features (see equation 1) is used to score
these candidates, with the weights for the features
in each of the three steps assigned heuristically. A
small development data has been used to tune the
feature weights.
score =
n
?
i=0
wi ? fi (1)
In equation 1, fi denotes the feature and wi denotes
the weight of the feature fi. The overall architecture
of the system is shown in Figure 1.
Sentence
Selection
sentence (GFS)
Gap?fill
selection
      &
Distractors
 selection
GAP?FILL
Question
GFSs
Document
Chapter
  Key 
Figure 1: System architecture
In earlier approaches to generating gap-fill ques-
tions (for English language learning), the keys in a
text were gathered first (or given as input in some
cases) and all the sentences containing the key were
used to generate the question. In domains where
language learning is not the aim, a gap-fill question
needs an informative sentence and not just any sen-
tence with the desired key present in it. For this rea-
son, in our work, sentence selection is performed be-
fore key selection.
3.1 Sentence Selection
A good GFS should be (1) informative and (2)
gap-fill question-generatable. An informative sen-
tence in a document is one which has relevant
knowledge that is useful in the context of the docu-
ment. A sentence is gap-fill question-generatable if
there is sufficient context within the sentence to pre-
dict the key when it is blanked out. An informative
sentence might not have enough context to generate
a question from and vice versa.
The sentence selection module goes through all
the sentences in the documents and extracts a set of
features from each of them. These features are de-
fined in such a way that the two criterion defined
above are accounted for. Table 1 gives a summary
of the features used.
First sentence: f(si) is a binary feature to check
whether the sentence si is the first sentence of the
document or not. Upon analysing the documents in
the textbook, it was observed that the first sentence
in the document usually provides a summary of the
document. Hence, f(si) has been used to make use
of the summarized first sentence of the document.
57
Feature Symbol Description Criterion
f(si) Is si the first sentence of the document? I
sim(si) No. of tokens common in si and title / length(si) I, G
abb(si) Does si contain any abbreviation? I
super(si) Does si contain a word in its superlative degree? I
pos(si) si?s position in the document (= i) G
discon(si) Is si beginning with a discourse connective? G
l(si) Number of words in si G
nouns(si) No. of nouns in si / length(si) G
pronouns(si) No. of pronouns in si / length(si) G
Table 1: Feature set for Sentence Selection (si: ith sen-
tence of the document; I: to capture informative sen-
tences; G: to capture the potential candidate for gener-
ating a GFQs)
Common tokens: sim(si) is the count of words
(nouns and adjectives) that the sentence and the title
of the document have in common. A sentence with
words from the title in it is important and is a good
candidate to ask a question using the common words
as the key.
2. The different states of potential energy that
electrons have in an atom are called energy
levels, or electron shells. (Title: The Energy
Levels of Electrons)
For example sentence 2, value of the feature is
3/19 (common words:3, sentence length:19) and
generating gap-fill question using energy, levels or
electrons as the key will be useful.
Abbreviations and Superlatives: abb(si),
super(si) features capture those sentences which
contain abbreviations and words in superlative de-
gree respectively. The binary features determine the
degree of the importance of a sentence in terms of
the presence of abbreviations and superlatives.
3. In living organisms, most of the strongest
chemical bonds are covalent ones.
For example, in sentence 3, presence of strongest
makes sentence more informative and useful for
generating a gap-fill question.
Sentence position: pos(si) is position of the
sentence si, in the document (= i). Since topic of
the document is elaborated in the middle of the
document, the sentences occurring in the middle of
the document are less important for the GFSs than
those which occur either at the start or the end of the
document. In order to use the above observation,
the module uses this feature.
Discourse connective at the beginning:
discon(si)?s value is 1 if first word of si is a
discourse connective2 and 0 otherwise. Discourse
connective at the beginning of a sentence indicates
that the sentence might not have enough context for
a QS to be understood by the students.
4. Because of this, it is both an amine and a car-
boxylic acid.
In example 4, after selecting amine and car-
boxylic as a key, QS will be left with insufficient
context to answer. Thus binary feature, discon(si),
is used.
Length: l(si) is the number of words in the
sentence. It is important to note that a very short
sentence might generate an unanswerable question
because of short context and a very long sentence
might have enough context to make the question
generated from it trivial.
Number of nouns and pronouns: Features
nouns(si) and pronouns(si) represent the amount
of context present in a sentence. More number of
pronouns in a sentence reduces the contextual infor-
mation, instead more number of nouns increases the
number of potential keys to ask a gap-fill question
on.
Four sample GFSs are shown in Table 3 with their
document?s titles.
3.2 Key Selection
For each sentence selected in the previous stage,
the key selection stage identifies the most appropri-
ate key from the sentence to ask the question on.
Previous works in this area, Smith et al (2010)
take keys as an input and, Karamanis et al (2006)
and Mitkov et al (2006) select keys on the basis of
term frequency and regular expressions on nouns.
Then they search for sentences which contain that
particular key in it. Since their approaches generate
gap-fill questions only with one blank, they could
end up with a trivial GFQ, especially in case of con-
junctions.
2because, since, when, thus, however, although, for example
and for instance connectives have been included.
58
   
(A)
  DT       JJS       NNS       IN    NN         NNS      VBP       JJ         NNS   CC        JJ    NNS  
potential keys selection
     [The  strongest   kind]     of    [chemical  bonds]    are    [covalent  bond   and    ionic    bond].
    [The  strongest  kind]  of   [ chemical  bonds]  are  [ covalent  bond  and   ionic  bond] .(B)  
Figure 2: Generating potential key?s list, (key-list) of strongest, chemical and covalent + ionic.
5. Somewhere in the transition from molecules to
cells, we will cross the blurry boundary be-
tween nonlife and life.
For instance in example sentence 5, selecting only
one of non-life and life makes the question trivial.
This is an other reason for performing sentence se-
lection before key selection. Our system can gen-
erate GFQs with multiple blanks unlike previous
works described above.
Our approach of key selection from a GFS is two
step process. In the first step the module generates
a list of potential keys from the GFS (key-list) and
in the second step it selects the best key from this
key-list.
3.2.1 Key-list formation
A list of potential keys is created in this step using
the part of speech (POS) tags of words and chunks
of the sentence in the following manner:
1. Each sequence of words in all the noun chunks
is pushed into key-list. In figure 2(A), the three
noun chunks the strongest kind, chemical bond
and covalent bond and ionic bond are pushed
into the key-list.
2. For each sequence in the key-list, the most im-
portant word(s) is selected as the potential key
and the other words are removed. The most im-
portant word in a noun chunk in the context of
GFQG in biology domain is a cardinal, adjec-
tive and noun in that order. In case where there
are multiple nouns, the first noun is chosen as
the potential key. If the noun chunk is a NP
coordination, both the conjuncts are selected as
a single potential key making it a case of mul-
tiple gaps in QS. In Figure 2(B) potential keys
strongest, chemical and covalent + ionic are se-
lected from the noun chunks by taking the order
of importance into account.
An automatic POS tagger and a noun chunker has
been used to process the sentences selected in the
first stage. It was observed that if words of a key
are spread across a chunk then there might not be
enough context left in QS to answer the question.
The noun chunk boundaries ensure that the sequence
of words in the potential keys are not disconnected.
6. Hydrogen has 1 valence electron in the first
shell, but the shell?s capacity is 2 electrons.
Any element of the key-list which occurs more
than once in the GFS is discarded as a potential key
as it more often than not generates a trivial question.
For example, in sentence 6 selecting any one of the
two electron as a key generates an easy gap-fill ques-
tion.
7. In contrast , trypsin , a digestive enzyme resid-
ing in the alkaline environment of the intestine
, has an optimal pH of .
(a) 6 (b) 7 (c) 8 (d) 9 (correct answer: 8)
If cardinals are present in a GFS, the first one is cho-
sen as its key directly and a gap-fill question has been
generated (see example 7).
3.2.2 Best Key selection
In this step three features, term(keyp),
title(keyp) and height(keyp), described in Ta-
ble 2, are used to select the best key from the key-list.
Feature Symbol Description
term(keyp)
Number of occurrences of the
keyp in the document.
title(keyp)
Does title contain
keyp ?
height(keyp)
height of the keyp in the
syntactic tree of the sentence.
Table 2: Feature set for key selection (potential key,
keyp is an element of key-list)
Term frequency: term(keyp) is number of oc-
currences of the keyp in the document. term(keyp)
59
is considered as a feature to give preference to the
potential keys with high frequency.
In title: title(keyp) is a binary feature to check
whether keyp is present in the title of the document
or not. A common word of GFS and the title of the
document serves as a better key for gap-fill question
than the ones that are not present in both.
Height: height(keyp) denotes the height 3 of the
keyp in the syntactic tree of the sentence. Height
gives an indirect indication of the importance of the
word. It also denotes the amount of text in the sen-
tence that modifies the word under consideration.
                                  
                                
                                      
      F(0)                G(0)                  
D (1)                E (0)     
A(3)                                  
C(2)B(0)
Figure 3: Height feature: node (height)
An answerable question should have enough con-
text left after the key blanked out. A word with
greater height in dependency tree gets more score
since there is enough context from its dependent
words in the syntactic tree to predict the word. For
example in Figure 3, node C?s height is two and the
words in the dashed box in its subtree provide the
context to answer a question on C.
The score of each potential key is normalized by
the number of words present in it and the best key is
chosen based on the scores of potential keys in key-
list. Table 3 shows the selected keys (red colored)
for sample GFSs.
3.3 Distractor Selection
Karamanis et al (2006) defines a distractor as,
an appropriate distractor is a concept semantically
close to the key which, however, cannot serve as the
right answer itself.
For distractor selection, Brown et al (2005) and
Smith et al (2010) used WordNet, Kunichika et
3The height of a tree is the length of the path from the deep-
est node in the tree to the root.
No. Selected keys (red colored)
1
An electron having a certain discrete amount of energy is
something like a ball on a staircase.
(The Energy Levels of Electrons)
2
Lipids are the class of large biological molecules that does not
include polymer.
(Lipids?Diverse Hydrophobic Molecules)
3
A DNA molecule is very long and usually consists of hundreds
or thousands of genes.
(Nucleic acids store and transmit hereditary information)
4
The fatty acid will have a kink in its tail wherever a double bond
occurs.
(Fats store large amounts of energy)
Table 3: Selected keys for each sample GFS
al. (2002) used their in-house thesauri to retrieve
similar or related words (synonyms, hypernyms, hy-
ponyms, antonyms, etc.). However, their approaches
can?t be used for those domains which don?t have
ontologies. Moreover, Smith et al (2010) do not se-
lect distractors based on the context of the keys. For
example, in the sentences 8 and 9, the key book oc-
curs in two different senses but same set of distrac-
tors will be generated by them.
8. Book the flight.
9. I read a book.
Feature Symbol Description
context(distractorp , measure of contextual similarity
keys) of distractorp and the keys
in which they are present
sim(distractorp , Dice coefficient score between
keys) GFS and the sentence
containing the distractorp
diff(distractorp , difference in term frequencies
keys) of distractorp and keys
in the chapter
Table 4: Feature set for distractor selection (keys is the
selected key for a GFS, distractorp is the potential dis-
tractor for the keys)
So a distractor should come from the same con-
text and domain, and should be relevant. It is also
clear from the above discussion that only term fre-
quency formula alone will not work for selection
of distractors. Our module uses features, shown in
Table 4, to select three distractors from the set of
all potential distractors. Potential distractors are the
words in the chapter which have the same POS tag
as that of the key.
60
Contextual similarity: context(distractorp,
keys) gets the contextual similarity score of a
potential distractor and the keys on the basis
of context in which they occur in their respective
sentences. Value of the feature depends on how
similar are the key and the potential distractor
contextually. The previous two and next two words
along with their POS tags are compared to calculate
the score.
Sentence Similarity: sim(distractorp, keys)
feature value represents similarity of the sentences
in which the keys and the distractorp occur in.
Dice Coefficient (Dice, 1945) (equation 2) has been
used to assign weights to those potential distractors
which come from sentences similar to GFS because
a distractor coming from a similar sentence will be
more relevant.
dice coefficient(s1, s2) =
2? commontokens
l(s1) + l(s2)
(2)
Difference in term frequencies: Feature,
diff (distractorp, keys) is used to find distractors
with comparable importance to the key. Term fre-
quency of a word represents its importance in the
text and words with comparable importance might
be close in their semantic meanings. So, a smaller
difference in the term frequencies is preferable.
key Distractors
energy charge, mass, water
polymer acid, glucose, know
DNA RNA, branch, specific
kink available, start, method
Table 5: Selected distractors for selected keys, shown in
Table 3
10. Electrons have a negative charge, the unequal
sharing of electrons in water causes the oxy-
gen atom to have a partial negative charge and
each hydrogen atom a partial positive charge.
A word that is present in the GFS would not be
selected as a distractor. For example in sentence 10,
if system selects oxygen as a key then hydrogen will
not be considered as a distractor. Table 5 shows
selected three distractors for each selected keys.
4 Evaluation and Results
Two chapters of the biology book are selected for
testing and top 15% candidates are selected by three
modules (sentence selection, key selection and dis-
tractor selection). The modules were manually eval-
uated independently by two biology students with
good English proficiency. Since in current system
any kind of post editing or manual work is avoided,
comparison of efficiency in manual and automatic
generation is not needed unlike Mitkov and Ha et
al. (2003).
4.1 Sentence Selection
The output of the sentence selection module is
a list of sentences. The evaluators check if each
of these sentences are good GFSs (informative and
gap-fill question-generatable) or not and binary
scoring is done. Evaluators are asked to evaluate
selected sentences independently, whether they are
useful for learning and answerable, or not.The cov-
erage of the selected sentences w.r.t the document
has not been evaluated.
Chapter-5 Chapter-6 Total
No. of 390 423 813Sentences
No. of 55 65 120Selected Sentences
No. of Good 51 59 110GFSs (Eval-1)
No. of Good 44 51 95GFSs (Eval-2)
Table 6: Evaluation of Sentence Selection
Evaluator-1 and 2 rated 91.66% and 79.16% of
sentences as good potential candidates for gap-fill
question respectively with 0.7 inter evaluator agree-
ment (Cohen?s kappa coefficient). Table 6 shows
the results of sentence selection for individual
chapters. Upon analysing the bad GFSs, we found
two different sources of errors. The first source is
the feature first sentence and the second is lack of
used in sentence selection module.
First sentence: Few documents in the data had
either a general statement or a summary of the pre-
vious section as the first sentence and the first sen-
tence feature contributed to their selection as GFS
even though they aren?t good GFSs.
11. An understanding of energy is as important
for students of biology as it is for students of
physics, chemistry and engineering.
For example, the system generated a gap-fill
61
question on example 11 which isn?t a good GFS at
all even though it occurs as the first sentence in the
document.
Less no. of features: Features like common to-
kens, superlative and abbreviation, discourse con-
nective at the beginning and number of pronouns
was useful in selecting informative sentences from
the documents. However, in absence of these fea-
tures in the document, module has selected the GFSs
on the basis of only two features, length and position
of the sentence. In those cases Evaluators rated few
GFSs as bad.
12. Here is another example of how emergent prop-
erties result from a specific arrangement of
building components.
For example, sentence 12 rated as a bad GFS by
the evaluators. So more features are need to be to
used to avoid this kind of errors.
13. A molecule has a characteristic size and shape.
Apart from these we also found few cases where
the context present in the GFS wasn?t sufficient to
answer the question although those sentences were
informative. In the above example 13, size and
shape were selected as the key that makes gap-fill
question unanswerable because of short context.
4.2 Key Selection
Our evaluation characterizes a key into two cat-
egories namely good (G) and bad (B). Evaluator-1
and 2 found that 94.16% and 84.16% of the keys
are good respectively with inter evaluator agreement
0.75. Table 7 shows the results of keys selection for
individual chapters.
Chap-5 Chap-6 Total
G B G B G B
Eval-1 50 5 63 2 113 7
Eval-2 50 5 51 14 101 19
Table 7: Evaluation of Key(s) Selection: Chap: Chap-
ter, Eval: Evaluator, G and B are for good and bad key
respectively
14. Carbon has a total of 6 electrons , with 2 in the
first electron shell and 4 in the second shell.
We observed that selection of first cardinal as key
is not always correct. For example, in sentence 14
selection of 6 as the key generated trivial GFQ.
4.3 Distractors Selection
Our system generates four alternatives for each
gap-fill question, out of which three are distrac-
tors. To evaluate the distractors? quality, evaluators
are asked to substitute the distractor in the gap and
check the readability and semantic meaning of the
QS to classify the distractor as good or bad. Eval-
uators rate 0, 1, 2 or 3 depending on the number of
good distractors in the GFQ (for example, questions
that are rated 2 have two good distractors and one
bad distractor).
15. An electron having a certain discrete amount of
is something like a ball on a staircase.
(a) charge (b) energy (c) mass (d) water
(Class: 3)
16. Lipids are the class of large biological
molecules that does not include .
(a) acid (b)polymer (c) glucose (d) know
(Class: 2)
17. A molecule is very long and usually
consists of hundreds or thousands of genes.
(a) DNA (b) RNA (c) specific (d) branch
(Class: 1)
18. The fatty acid will have a in its tail
wherever a double bond occurs .
(a) available (b) method (c) kink (d) start
(Class: 0)
Examples of gap-fill questions generated by our
system are shown above (red colored alternatives are
good distractors, blue colored ones are the correct
answers for the questions and the black ones are bad
distractors).
Chap-5 Chap-6 Total
Class 0 1 2 3 0 1 2 3 0 1 2 3
Eval-1 21 19 12 3 8 31 21 5 29 50 33 8
Eval-2 20 19 13 3 9 25 28 3 29 44 41 6
Table 8: Evaluation of Distractor Selection (Before any
corrections)
Table 8 shows the human evaluated results for
individual chapter. According to both evaluator-
1 and evaluator-2, 75.83% of the cases the system
finds useful gap-fill questions with 0.67 inter evalu-
ator agreement. Useful gap-fill questions are those
which have at least one good distractor. 60.05% and
67.72% test items are answered correctly by Evalu-
ator 1 and 2 respectively.
62
We observed that when a key has more than one
word, distractors? quality reduces because every to-
ken in a distractor must be comparably relevant.
Small chapter size also effects the number of good
distractors because distractors are selected from the
chapter text.
In our work, as we only considered syntactic and
lexical features for distractor selection, the selected
distractors could be semantically conflicting with
themselves or with the key. For example, due to
the lack of semantic features in our method a hyper-
nym of the key could find way into the distractors
list thereby providing a confusing list of distractors
to the students. In the example question 1 in section
1, chemical which is the hypernym of covalent and
ionic could prove confusing if its one of the choices
for the answer. Semantic similarity measures need
to be used to solve this problem.
5 Related work
Given the distinct domains in which our system
and other systems were deployed, a direct com-
parison of evaluation scores could be misleading.
Hence, in this section we compare our approach with
previous approaches in this area.
Smith et al (2010) and Pino et al (2009) used
gap-fill questions for vocabulary learning. Smith et
al. (2010) present a system, TEDDCLOG, which au-
tomatically generates draft test items from a corpus.
TEDDCLOG takes the key as input. It finds dis-
tractors from a distributional thesaurus. They got
53.33% (40 out of 75) accuracy after post editing
(editing either in carrier sentence (GFS) or in dis-
tractors) in the generated gap-fill questions.
Pino et al (2009) describe a baseline technique to
generate cloze questions (gap-fill questions) which
uses sample sentences from WordNet. They then re-
fine this technique with linguistically motivated fea-
tures to generate better questions. They used the
Cambridge Advanced Learners Dictionary (CALD)
which has several sample sentences for each sense
of a word for stem selection (GFS). The new strat-
egy produced high quality cloze questions 66% of
the time.
Karamanis et al (2006) report the results of a pi-
lot study on generating Multiple-Choice Test Items
(MCTI) from medical text which builds on the work
of Mitkov et al (2006). Initially key set is enlarged
with NPs featuring potential key terms as their heads
and satisfying certain regular expressions. Then sen-
tences having at least one key are selected and the
terms with the same semantic type in UMLS are se-
lected as distractors. In their manual evaluation, the
domain experts regarded a MCTI as unusable if it
could not be used in a test or required too much revi-
sion to do so. The remaining items were considered
to be usable and could be post edited by the experts
to improve their content and readability or replace
inappropriate distractors. They have reported 19%
usable items generated from their system and after
post editing stems accuracy jumps to 54%.
However, our system takes a document and pro-
duces a list of GFQs by selecting informative sen-
tences from the document. It doesn?t use any exter-
nal resources for distractors selection and finds them
in the chapter only that makes it adaptable for those
domains which do not have ontologies.
6 Conclusions and Future Work
Our GFQG system, selects most informative sen-
tences of the chapters and generates gap-fill ques-
tions on them. Syntactic features helped in quality of
gap-fill questions. We look forward to experiment-
ing on larger data by combining the chapters. Eval-
uation of course coverage by our system and use of
semantic features will be part of our future work.
Acknowledgements
We would like to thank Avinesh Polisetty and
Sudheer Kolachina from LTRC, IIIT-Hyderabad for
their helpful discussions and pointers during the
course of this work. Thanks also to the anonymous
reviewers for useful feedback.
References
Eiichiro Sumita, Fumiaki Sugaya, and Seiichi Yamamoto
2005. Measuring Non-native Speakers Proficiency of
English by Using a Test with Automatically-Generated
Fill-in-the-Blank Questions, 2nd Wkshop on Building
Educational Applications using NLP, Ann Arbor.
John Lee and Stephanie Seneff. 2007. Automatic Gen-
eration of Cloze Items for Prepositions, CiteSeerX
- Scientific Literature Digital Library and Search
Engine [http://citeseerx.ist.psu.edu/oai2] (United
States).
Lin, Y. C., Sung, L. C., Chen and M. C. 2007. An
63
Automatic Multiple-Choice Question Generation
Scheme for English Adjective Understanding, CCE
2007 Workshop Proc. of Modeling, Management and
Generation of Problems / Questions in eLearning, pp.
137-142.
Juan Pino, Michael Heilman and Maxine Eskenazi.
2009. A Selection Strategy to Improve Cloze Question
Quality, Wkshop on Intelligent Tutoring Systems for
Ill-Defined Domains. 9th Int. Conf. on ITS.
Simon Smith, P.V.S Avinesh and Adam Kilgarriff. 2010.
Gap-fill Tests for Language Learners: Corpus-Driven
Item Generation .
Jonathan C. Brown, Gwen A. Frishkoff, Maxine Es-
kenazi. 2005. Automatic Question Generation for
Vocabulary Assessment, Proc. of HLT/EMNLP ?05,
pp. 819-826.
Nikiforos Karamanis, Le An Ha and Ruslan Mitkov.
2006 Generating Multiple-Choice Test Iterms from
Medical Text: A Pilot Study, In Proceedings of INLG
2006, Sydney, Australia.
Ruslan Mitkov, Le An Ha and Nikiforos Karamanis.
2006 A computer-aided environment for generating
multiple-choice test items, Natural Language Engi-
neering 12(2): 177-194
Hidenobu Kunichika, Minoru Urushima,Tsukasa Hi-
rashima and Akira Takeuchi. 2002. A Computational
Method of Complexity of Questions on Contents of
English Sentences and its Evaluation, In: Proc. of
ICCE 2002, Auckland, NZ, pp. 97101 (2002).
Lee Raymond Dice. 1945. Measures of the Amount of
Ecologic Association Between Species
Ruslan Mitkov and Le An Ha. 2003 Computer-aided
generation of multiple-choice tests, Proceedings
of the HLT/NAACL 2003 Workshop on Building
educational applications using Natural Language
Processing. Edmonton, Canada, 17-22.
64
Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically Rich Languages, pages 91?96,
Seattle, Washington, USA, 18 October 2013. c?2013 Association for Computational Linguistics
A Statistical Approach to Prediction of Empty Categories in Hindi
Dependency Treebank
Puneeth Kukkadapu, Prashanth Mannem
Language Technologies Research Center
IIIT Hyderabad, India
{puneeth.kukkadapu,prashanth}@research.iiit.ac.in
Abstract
In this paper we use statistical dependency
parsing techniques to detect NULL or Empty
categories in the Hindi sentences. We have
currently worked on Hindi dependency tree-
bank which is released as part of COLING-
MTPIL 2012 Workshop. Earlier Rule based
approaches are employed to detect Empty
heads for Hindi language but statistical learn-
ing for automatic prediction is not explored.
In this approach we used a technique of in-
troducing complex labels into the data to pre-
dict Empty categories in sentences. We have
also discussed about shortcomings and diffi-
culties in this approach and evaluated the per-
formance of this approach on different Empty
categories.
1 Introduction
Hindi is a morphologically rich and a relatively
free word order language (MoR-FWO). Parsing is
a challenging task for such MoR-FWO languages
like Turkish, Basque, Czech, Arabic, etc. be-
cause of their non-configurable nature. Previous re-
search showed that the dependency based annota-
tion scheme performs better than phrase based an-
notation scheme for such languages (Hudson, 1984;
Bharati et al, 1995). Dependency annotation for
Hindi is based on Paninian framework for building
the treebank (Begum et al, 2008). In recent years
data driven parsing on Hindi has shown good re-
sults, the availability of annotated corpora is a defi-
nite factor for this improvement (Nivre et al, 2006;
McDonald et al, 2005; Martins et al, 2009; Man-
nem and Dara, 2011). Other approaches such as
rule-based and hybrid of rule-based and data-driven
(Bharati et al, 2009a) for Hindi language have also
been tried out. In the shared task for Hindi Pars-
ing organized with COLING workshop Singla et al
(2012) achieved best results for Gold-Standard data
with 90.99% (Labeled Attachment Score or LAS)
and 95.87% (Unlabeled Attachment Score or UAS).
Empty category is a nominal element which does
not have any phonological content and is therefore
unpronounced. Empty categories are annotated in
sentences to ensure a linguistically plausible struc-
ture. Empty categories play a crucial role in the an-
notation framework of the Hindi dependency tree-
bank (Begum et al, 2008; Bharati et al, 2009b). If
dependency structure of a sentence do not form a
fully connected tree then Empty category (denoted
by NULL in Hindi Treebank) is inserted in the sen-
tence. In the Hindi treebank, an Empty category has
at least one child. Traditional parsing algorithms do
not insert Empty categories and require the Empty
categories to be part of the input. These Empty
categories are manually annotated in the treebank.
In real time scenarios, like translation between lan-
guages, it is not possible to add the Empty cate-
gories into the sentences manually. So we require an
approach which can identify the presence of these
Empty categories and insert into appropriate posi-
tions in the sentence.
Figure 1 shows an Example of a Hindi sentence
annotated with a NULL category. The English trans-
lation for this sentence is, ?Its not fixed what his big
bank will do?. The aim of this paper is to investigate
the problem of automatically predicting the Empty
categories in the sentences using the statistical de-
91
Figure 1: An Example of a Hindi sentence annotated with a NULL category.
pendency parsing technique and to shed some light
on the challenges of this problem. As the data-driven
parsing on Hindi language has achieved good results
(Singla et al, 2012), we try to use this approach to
predict Empty categories in the sentence. In this
approach the information about NULL categories is
encoded into the label set of the structure. In these
experiments we have used only Projective sentences
from the treebank. Non-projectivity makes it diffi-
cult to identify the exact position of NULLs during
introduction of NULLs in the sentence.
The rest of the paper is divided into the follow-
ing sections: Section 2 discusses about the related
work. Section 3 gives an overview of the Hindi data
we have used for our experiments. Section 4 con-
tains the details of our approach and section 5 dis-
cusses about experiments, parser, results and discus-
sion. We conclude the paper in section 6 with a sum-
mary and the future work.
2 Related Work
Previous work related to Empty categories predic-
tion on Hindi data is done by Gsk et al (2011) which
is a rule based approach for detection of Empty cate-
gories and also presented detailed analysis of differ-
ent types of Empty categories present in the Hindi
treebank. They used hand-crafted rules in order
to identify each type of Empty category. As this
is a rule based approach it becomes language spe-
cific. There are many approaches for the recov-
ery of empty categories in the treebanks like Penn
treebank, both ML based (Collins, 1997; Johnson,
2002; Seeker et al, 2012), and rule based (Camp-
bell, 2004). Some approaches such as Yang and
Xue (2010) follow a post processing step of recov-
ering empty categories after parsing the text. Gsk
et al (2011) have discussed about different types
of Empty categories in Hindi Treebank in detailed
manner. The main types of Empty categories are:
? Empty Subject where a clause is dependent on
missing subject (NP) of the verb, denoted as
NULL NP or NULL PRP.
? Backward Gapping where the verb (VM) is
absent in the clause that occurs before a co-
ordinating conjunct, denoted as NULL VM
? Forward Gapping where the verb (VM) is
absent in the clause that occurs after a co-
ordinating conjunct, denoted as NULL VM.
? Conjunction Ellipses where the Conjunction
(CC) is absent in the sentence, denoted as
NULL CC.
3 Data
We have used COLING-MTPIL workshop 2012
data for our experiments. This was released by the
organizers as part of the shared task in two differ-
ent settings. One being the manually annotated data
with POS tags, chunks and other information such as
gender, number, person etc. whereas the other one
contains only automatic POS tags without any other
information. We have used Gold standard data with
92
Type of NULL No. of Instances
NULL VM 247
NULL CC 184
NULL NP 71
NULL PRP 25
Table 1: Empty categories in Training + Development
Dataset of Hindi treebank.
Type of NULL No. of instances
NULL VM 26
NULL CC 36
NULL NP 9
NULL PRP 4
Table 2: Empty categories in Testing Dataset of Hindi
treebank.
all features provided for our experiments. Train-
ing set contains 12,041 sentences, development data
set consists of 1233 sentences and testing data set
consists of 1828 sentences. In our experiments we
have worked with only projective sentences. We
have combined the training and development data
sets into one data set and used as training in the final
experiments.
Training and Development data together consists
of 544 NULL instances (in 436 sentences) of 10,690
sentences. The major types of Empty categories
present in the training data are of type NULL CC,
NULL VM, NULL NN and NULL PRP categories.
Table 1 and Table 2 show the number of instances of
each category. Testing data consists of 80 instances
(72 sentences) of 1455 sentences.
4 Approach
There are 3 main steps involved in this process.
4.1 Pre-Processing
In the first step, we encode information about pres-
ence of Empty categories in a sentence into the
dependency relation label set of the sentence. If
NULLs are present in a sentence, we remove the
NULLs from the respective sentence in the treebank.
In a sentence the dependents or children of a NULL
category are attached to the parent of the NULL cat-
egory and their respective labels are combined with
dependency label of NULL category which indicates
the presence of NULL and also says that such words
or tokens are children of NULL category. Instead of
just combining the labels we also add a sense of di-
rection to the complex label which indicates whether
the position of NULL is to the right or left of this
token in the sentence and subsequently NULLs are
also detached from its parent node. Therefore a
complex label in a sentence indicates the presence
of a NULL category in the sentence.
Example: Null-label r dep-label is a generic
type of a complex label. In this format ?r? indicates
that a NULL instance is to the right of this token.
Null-label is the dependency relation label joining
the Null instance and its parent and dep-label is the
dependency relation label joining the current token
or word to its parent which is a NULL instance.
Figure 2 illustrates this step.
4.2 Data-driven parsing
In the second step a Data-driven parser is trained
using the training data (with complex dependency
relation labels) and when this parser model is used
on the test data it predicts the complex labels in the
output. In this approach we have tried out different
data-driven parsers such as Malt (Nivre et al, 2006),
Turbo (Martins et al, 2010) and MST (McDonald
et al, 2005) for this experiment which were shown
earlier to be performing better for Hindi Parsing by
Kukkadapu et al (2012) and found that Malt parser
performs better than the rest on this data with com-
plex labels.
4.3 Post-processing
In the final step, Post-processing is applied on the
output predicted by the parser in the above step. In
this step presence of NULLs are identified using the
complex labels and their position in the sentence
is identified using sense of direction in these labels
(i.e., whether NULL instance is to the left ?l? or right
?r? of this token). During the insertion of NULLs
into the sentence Projectivity of the sentence must
be preserved. Keeping this constraint intact and us-
ing the direction information from the dependency
relation labels, NULLs are introduced into the sen-
tence. Figure 2 illustrates this step.
The advantage in using statistical approach rather
than a rule based approach to predict NULLs is, it
93
Figure 2: Process
can be easily used to predict NULLs in other MoR-
FWO languages. The problem with this approach
is, it can?t handle Empty categories occurring as
Leaf nodes (or Terminal nodes in the dependency
tree) and as Root nodes. As we have mentioned
earlier, the dependency annotation scheme of Hindi
language does not allow for Empty categories to oc-
cur as Leaf nodes (or Terminal nodes). But if these
Empty categories occur as Root nodes in the depen-
dency tree then such cases are not disturbed in our
approach.
5 Experiments and Results
5.1 Parser settings
As mentioned earlier we had used Malt parser for
our experiments. Malt Parser implements the tran-
sition based approach to dependency parsing which
has two components:
1) A transition system for mapping sentences into
dependency trees.
2) A classifier for predicting the next transition for
every possible system configuration.
Malt parser provides two learning algorithms
LIBSVM and LIBLINEAR. It also provides various
options for parsing algorithms and we have exper-
imented on nivre-eager, nivre-standard and stack-
proj parsing algorithms. Nivre-eager has shown
good results in our experiments.
5.2 Features and Template
Feature model is the template, which governs the
learning from the given training data. We observed
feature model used by Kosaraju et al (2010) per-
forms best.
In order to get best results in the second step
(Data-driven parsing) we have experimented with
Type of NULL Category Recall
NULL VM 50
NULL CC 69.45
NULL NN 88.89
NULL PRP 50
Table 3: Empty categories Predicted by this approach on
test data.
various features provided in the data. Kosaraju et al
(2010) and Husain et al (2010) showed the best fea-
tures that can be used in FEATS column in CoNLL-
X format. These features are vibhakti (post posi-
tional marker), TAM (tense, aspect and modality),
chunk features like chunk head, chunk distance and
chunk boundary information have proved to be ef-
fective in parsing of Hindi language and our results
on overall accuracy of data is consistent with their
results.
5.3 Results and Discussion
The Results obtained on the test dataset are shown
below and Recall on each Empty category are given
in Table 3:
The Results obtained by using this approach on
the test set including all the Empty category types is
as follows:
Precision = 84.9
Recall = 69.23
F-measure = 76.26
In computation of the above results the exact po-
sition of NULLs in the sentence are not considered.
These values indicate the efficiency of the system
in identifying the presence of the Empty categories
in the system. However, this approach inserted the
94
NULLs in exact positions with a Precision of more
than 85%, i.e., of all the NULL instances it has in-
serted correctly, it has inserted 85% of them in exact
positions in the sentences.
The approach was able to insert NULL NP to-
kens with good accuracy but it had a tough time pre-
dicting NULL VM tokens. This was also consistent
with Gsk et al (2011) conclusions about Empty cat-
egories in Hindi treebank.
In case of NULL VM categories we have ob-
served some inconsistency in the annotation of these
sentences. In these sentences which have multiple
clauses with main verb (VM) token missing, certain
sentences are annotated with NULL VM for each
clause where main verb (VM) token is missing and
certain sentences are annotated with one NULL VM
for all the clauses with main verb (VM) missing.
This may be a reason for accuracy drop in predict-
ing NULL VM tokens. The main reason for low ac-
curacy as we have observed is that the output pre-
dicted by the parser is low for these complex labels.
The test data consists of 202 complex labels whereas
the parser has been able to predict only 102 of them,
which is a huge drop in accuracy for complex labels.
The overall accuracy of parser on the test data (only
projective sentences) has been high 91.11%(LAS),
95.86%(UAS) and 92.65%(LS). The low accuracy
of the parser on complex labels may be due to less
number of these instances compared to size of the
corpus. Another reason may be due to the introduc-
tion of complex labels the size of label set has in-
creased significantly and it may be difficult for the
parser to learn the rare labels.
6 Conclusion and Future work
In this paper, we presented a statistical approach to
Empty category prediction using Data-driven pars-
ing. We have used state-of-the-art parser for Hindi
language with an accuracy above 90% and have
achieved a decent F-score of 76.26 in predicting
Empty categories. We look to try out this approach
for other MoR-FWO languages and compare the
performances on different languages. We need to
identify Features which would help in identifying
NULL CC category and also should try this ap-
proach on a big data set with a significant number
of instances of NULLs and also look to extend this
approach to Non-Projective sentences.
References
Rafiya Begum, Samar Husain, Arun Dhwaj, Dipti Misra
Sharma, Lakshmi Bai, and Rajeev Sangal. 2008. De-
pendency annotation scheme for indian languages. In
Proceedings of IJCNLP.
A. Bharati, V. Chaitanya, R. Sangal, and KV Ramakrish-
namacharyulu. 1995. Natural language processing: A
Paninian perspective. Prentice-Hall of India.
Akshar Bharati, Samar Husain, Dipti Misra, and Rajeev
Sangal. 2009a. Two stage constraint based hybrid ap-
proach to free word order language dependency pars-
ing. In Proceedings of the 11th International Confer-
ence on Parsing Technologies, pages 77?80. Associa-
tion for Computational Linguistics.
Akshara Bharati, Dipti Misra Sharma, Samar Husain,
Lakshmi Bai, Rafiya Begam, and Rajeev Sangal.
2009b. Anncorra: Treebanks for indian languages,
guidelines for annotating hindi treebank.
Richard Campbell. 2004. Using linguistic principles to
recover empty categories. In Proceedings of the 42nd
annual meeting on association for computational lin-
guistics, page 645. Association for Computational Lin-
guistics.
Michael Collins. 1997. Three generative, lexicalised
models for statistical parsing. In Proceedings of the
eighth conference on European chapter of the Associ-
ation for Computational Linguistics, pages 16?23. As-
sociation for Computational Linguistics.
Chaitanya Gsk, Samar Husain, and Prashanth Mannem.
2011. Empty categories in hindi dependency treebank:
Analysis and recovery. In Proceedings of the 5th Lin-
guistic Annotation Workshop, pages 134?142. Associ-
ation for Computational Linguistics.
R.A. Hudson. 1984. Word grammar. Blackwell Oxford.
Samar Husain, Prashanth Mannem, Bharat Ram Ambati,
and Phani Gadde. 2010. The icon-2010 tools contest
on indian language dependency parsing. Proceedings
of ICON-2010 Tools Contest on Indian Language De-
pendency Parsing, ICON, 10:1?8.
Mark Johnson. 2002. A simple pattern-matching al-
gorithm for recovering empty nodes and their an-
tecedents. In Proceedings of the 40th Annual Meeting
on Association for Computational Linguistics, pages
136?143. Association for Computational Linguistics.
P. Kosaraju, S.R. Kesidi, V.B.R. Ainavolu, and
P. Kukkadapu. 2010. Experiments on indian language
dependency parsing. Proceedings of the ICON10 NLP
Tools Contest: Indian Language Dependency Parsing.
Puneeth Kukkadapu, Deepak Kumar Malladi, and
Aswarth Dara. 2012. Ensembling various dependency
95
parsers: Adopting turbo parser for indian languages.
In 24th International Conference on Computational
Linguistics, page 179.
P. Mannem and A. Dara. 2011. Partial parsing from bi-
text projections. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies, pages 1597?
1606.
A.F.T. Martins, N.A. Smith, and E.P. Xing. 2009. Con-
cise integer linear programming formulations for de-
pendency parsing. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and the 4th
International Joint Conference on Natural Language
Processing of the AFNLP: Volume 1-Volume 1, pages
342?350.
A.F.T. Martins, N.A. Smith, E.P. Xing, P.M.Q. Aguiar,
and M.A.T. Figueiredo. 2010. Turbo parsers: Depen-
dency parsing by approximate variational inference.
In Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, pages 34?
44.
R. McDonald, F. Pereira, K. Ribarov, and J. Hajic?. 2005.
Non-projective dependency parsing using spanning
tree algorithms. In Proceedings of the conference on
Human Language Technology and Empirical Meth-
ods in Natural Language Processing (EMNLP), pages
523?530.
J. Nivre, J. Hall, and J. Nilsson. 2006. Maltparser: A
data-driven parser-generator for dependency parsing.
In Proceedings of LREC, volume 6, pages 2216?2219.
Wolfgang Seeker, Richa?rd Farkas, Bernd Bohnet, Hel-
mut Schmid, and Jonas Kuhn. 2012. Data-driven de-
pendency parsing with empty heads. In Proceedings
of COLING 2012: Posters, pages 1081?1090, Mum-
bai, India, December. The COLING 2012 Organizing
Committee.
Karan Singla, Aniruddha Tammewar, Naman Jain, and
Sambhav Jain. 2012. Two-stage approach for
hindi dependency parsing using maltparser. Training,
12041(268,093):22?27.
Yaqin Yang and Nianwen Xue. 2010. Chasing the ghost:
recovering empty categories in the chinese treebank.
In Proceedings of the 23rd International Conference
on Computational Linguistics: Posters, pages 1382?
1390. Association for Computational Linguistics.
96
Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically Rich Languages, pages 119?128,
Seattle, Washington, USA, 18 October 2013. c?2013 Association for Computational Linguistics
Context Based Statistical Morphological Analyzer and its Effect on Hindi
Dependency Parsing
Deepak Kumar Malladi and Prashanth Mannem
Language Technologies Research Center
International Institute of Information Technology
Hyderabad, AP, India - 500032
{deepak.malladi, prashanth}@research.iiit.ac.in
Abstract
This paper revisits the work of (Malladi and
Mannem, 2013) which focused on building
a Statistical Morphological Analyzer (SMA)
for Hindi and compares the performance of
SMA with other existing statistical analyzer,
Morfette. We shall evaluate SMA in vari-
ous experiment scenarios and look at how it
performs for unseen words. The later part
of the paper presents the effect of the pre-
dicted morph features on dependency parsing
and extends the work to other morphologically
rich languages: Hindi and Telugu, without any
language-specific engineering.
1 Introduction
Hindi is one of the widely spoken language in the
world with more than 250 million native speakers1.
Language technologies could play a major role in re-
moving the digital divide that exists between speak-
ers of various languages. Hindi, being a morpho-
logically rich language with a relatively free word
order (Mor-FOW), poses a variety of challenges for
NLP that may not be encountered when working on
English.
Morphological analysis is the task of analyzing
the structure of morphemes in a word and is gen-
erally a prelude to further complex tasks such as
parsing, machine translation, semantic analysis etc.
These tasks need an analysis of the words in the
sentence in terms of lemma, affixes, parts of speech
(POS) etc.
1http://www.ethnologue.com/statistics/size
NLP for Hindi has suffered due to the lack
of a high coverage automatic morphological ana-
lyzer. For example, the 2012 Hindi Parsing Shared
Task (Sharma et al, 2012) held with COLING-
2012 workshop had a gold-standard input track and
an automatic input track, where the former had
gold-standard morphological analysis, POS tags and
chunks of a sentence as input and the automatic track
had only the sentence along with automatic POS
tags as input. The morphological information which
is crucial for Hindi parsing was missing in the au-
tomatic track as the existing analyzer had limited
coverage. Parsing accuracies of gold-standard input
track were significantly higher than that of the other
track. But in the real scenario NLP applications,
gold information is not provided. Even Ambati et
al. (2010b) and Bharati et al (2009a) have exploited
the role of morpho-syntactic features in Hindi de-
pendency parsing. Hence we need a high coverage
and accurate morphological analyzer.
2 Related work
Previous efforts on Hindi morphological analysis
concentrated on building rule based systems that
give all the possible analyses for a word form ir-
respective of its context in the sentence. The
paradigm based analyzer (PBA) by Bharati et al
(1995) is one of the most widely used applications
among researchers in the Indian NLP community.
In paradigm based analysis, words are grouped into
a set of paradigms based on the inflections they take.
Each paradigm has a set of add-delete rules to ac-
count for its inflections and words belonging to a
paradigm take the same inflectional forms. Given a
119
L G N P C T/V
? ? ? ? ? ?
xeSa
(country)
xeSa m sg 3 d 0
xeSa m pl 3 d 0
xeSa m sg 3 o 0
cAhie
(want)
cAha any sg 2h ie
cAha any pl 2h eM
L-lemma, G-gender, N-number, P-person
C-case, T/V-TAM or Vibhakti
Table 1: Multiple analyses given by the PBA for the
words xeSa and cAhie
word, the PBA identifies the lemma, coarse POS tag,
gender, number, person, case marker, vibhakti2 and
TAM (tense, aspect, modality). Being a rule-based
system, the PBA takes a word as input and gives all
the possible analyses as output. (Table 1 presents an
example). It doesn?t pick the correct analysis for a
word in its sentential context.
Goyal and Lehal?s analyser (2008), which is a re-
implementation of the PBA with few extensions, has
not done any comparative evaluation. Kanuparthi
et al (2012) built a derivational morphological ana-
lyzer for Hindi by introducing a layer over the PBA.
It identifies 22 derivational suffixes which helps in
providing derivational analysis for the word whose
suffix matches with one of these 22 suffixes.
The large scale machine translation projects3 that
are currently under way in India use shallow parser
built on PBA and an automatic POS tagger. The
shallow parser prunes the morphological analyses
from PBA to select the correct one using the POS
tags from the tagger. Since it is based on PBA, it
suffers from similar coverage issues for out of vo-
cabulary (OOV) words.
The PBA, developed in 1995, has a limited vo-
cabulary and has received only minor upgrades since
then. Out of 17,666 unique words in the Hindi Tree-
bank (HTB) released during the 2012 Hindi Parsing
Shared Task (Sharma et al, 2012), the PBA does
not have entries for 5,581 words (31.6%).
Apart from the traditional rule-based approaches,
Morfette (Chrupa?a et al, 2008) is a modular, data-
2Vibhakti is a Sanskrit grammatical term that encompasses
post-positionals and case endings for nouns, as well as inflec-
tion and auxiliaries for verbs (Pedersen et al, 2004).
3http://sampark.iiit.ac.in/
Data #Sentences #Words
Training 12,041 268,096
Development 1,233 26,416
Test 1,828 39,775
Table 2: HTB statistics
driven, probabilistic system which learns to perform
joint morphological tagging and lemmatization from
morphologically annotated corpora. The system is
composed of two learning modules, one for mor-
phological tagging and one for lemmati- zation, and
one decoding module which searches for the best se-
quence of pairs of morphological tags and lemmas
for an input sequence of wordforms.
Malladi and Mannem (2013) have build a Statis-
tical Morphological Analyzer (SMA) with minimal
set of features but they haven?t compared their sys-
tem with Morfette. In our work we shall discuss
in detail about SMA with more concentration on
evaluating the system in various scenarios and shall
extend the approach to other morphologically rich
languages. Later we evaluate the effect of the pre-
dicted morph features (by SMA) on Hindi depen-
dency parsing.
3 Hindi Dependency Treebank (HTB)
A multi layered and multi representational tree-
bank for Hindi is developed by annotating with
morpho-syntactic (morphological analyses, POS
tags, chunk) and syntacto-semantic (dependency re-
lations labeled in the computational paninian frame-
work) information. A part of the HTB (constituting
of 15,102 sentences) was released for Hindi Pars-
ing Shared Task. Table 2 shows the word counts of
training, development and test sections of HTB.
With the existing morph analyzer (PBA) perform-
ing poorly on OOV words and the availability of an
annotated treebank, Malladi and Mannem (2013) set
out to build a high-coverage automatic Hindi morph
analyzer by learning each of the seven morpholog-
ical attributes separately from the Hindi Treebank.
During this process, it was realized that vibhakti
and TAM can be better predicted using heuristics on
fine-grained POS tags than by training on the HTB.
In the rest of the section, we discuss the meth-
ods used by SMA to predict each of the seven mor-
120
MorphFeature Values
Gender masculine, feminine, any, none
Number singular, plural, any, none
Person 1, 1h, 2, 2h, 3, 3h, any, none
CaseMarker direct, oblique, any, none
Table 3: Morph features and the values they take
source target gloss
k i y A k a r a do
l a d a k e l a d a k A boy
l a d a k I l a d a k I girl
l a d a k I y A M l a d a k I girl
Table 4: Sample parallel corpus for lemma prediction
phological attributes and their effect on Hindi depen-
dency parsing. Table 3 lists the values that each of
the morph attributes take in HTB.
4 Statistical Morphological Analyzer
(SMA)
The output of a morphological analyzer depends on
the language that it is developed for. Analyzers for
English (Goldsmith, 2000) predict just the lemmas
and affixes mainly because of its restricted agree-
ment based on semantic features such as animacy
and natural gender. But in Hindi, agreement de-
pends on lexical features such as grammatical gen-
der, number, person and case. Hence, it is crucial
that Hindi analyzers predict these along with TAM
and vibhakti which have been found to be useful for
syntactic parsing (Ambati et al, 2010b; Bharati et
al., 2009a).
Hindi has syntactic agreement (of GNP and case)
of two kinds: modifier-head agreement and noun-
verb agreement. Modifiers, including determiners,
agree with their head noun in gender, number and
case, and finite verbs agree with some noun in the
sentence in gender, number and person (Kachru,
2006). Therefore, apart from lemma and POS tags,
providing gender, number and person is also crucial
for syntactic parsing.4
4While nouns, pronouns and adjectives have both GNP and
case associated with them, verbs only have GNP. TAM is valid
only for verbs and vibhakti (post-position) is only associated
with nouns and pronouns.
4.1 Lemma prediction
The PBA uses a large vocabulary along with
paradigm tables consisting of add-delete rules to find
the lemma of a given word. All possible add-delete
rules are applied on a given word form and the re-
sulting lemma is checked against the vocabulary to
find if it is right or not. If no such lemma exists (for
OOV words), it returns the word itself as the lemma.
While the gender, number and person of a word
form varies according to the context (due to syntac-
tic agreement with head words), there are very few
cases where a word form can have more than one
lemma in a context. For example, vaha can either
be masculine or feminine depending on the form that
the verb takes. It is feminine in vaha Gara gayI
(she went home) and masculine in vaha Gara
gayA (he went home). The lemma for vaha can
only be vaha irrespective of the context and also
the lemma for gayI and gayA is jA. This makes
lemma simpler to predict among the morphological
features, provided there is access to a dictionary of
all the word forms along with their lemmas. Unfor-
tunately, such a large lemma dictionary doesn?t ex-
ist. There are 15,752 word types in training, 4,292
word types in development and 5,536 word types
in test sections of HTB respectively. Among these
18.5% of the types in development and 20.2% in test
data are unseen in training data.
SMA analyzer perceives lemma prediction from a
machine translation perspective, with the characters
in the input word form treated as the source sentence
and those in the lemma as the target. The strings
on source and target side are split into sequences
of characters separated by space, as shown in Ta-
ble 4. The phrase based model (Koehn et al, 2007)
in Moses is trained on the parallel data created from
the training part of HTB. The translation model ac-
counts for the changes in the affixes (sequence of
characters) from word form to lemma whereas the
language model accounts for which affixes go with
which stems. In this perspective, the standard MT
experiment of switching source and target to attain
better accuracy would not apply since it is unrea-
sonable to predict the word form from the lemma
without taking the context into account.
Apart from the above mentioned approach, we ap-
ply a heuristic on top of SMA, wherein proper nouns
121
Gender Word Gloss
masculine cAvala, paMKA rice, fan
feminine rela, xAla train, pulse
any jA go
none karIba near
Table 5: Gender value examples
Number Word Gloss
singular ladZake boy-Sg-Oblique
plural ladZake boy-Pl-Direct
any banA make
none karIba near
Table 6: Number value examples
(NNP) take the word form itself as the lemma.
4.2 Gender, Number, Person and Case
Prediction
Unlike lemma prediction, SMA uses SVM (support
vector machine) machine learning algorithm to pre-
dict GNP and case.
Though knowing the syntactic head of a word
helps in enforcing agreement (and thereby accu-
rately predicting the correct GNP), parsing is usu-
ally a higher level task and is not performed be-
fore morphological analysis. Hence, certain cases of
GNP prediction are similar in nature to the standard
chicken and egg problem.
4.2.1 Gender
Gender prediction is tricky in Hindi as even native
speakers tend to make errors while annotating. Gen-
der prediction in English is easy when compared to
Hindi since gender in English is inferred based on
the biological characteristics the word is referring
to. For example, Train has neuter gender in En-
glish whereas in Hindi, it exhibits feminine charac-
teristics. A dictionary of word-gender information
may usually suffice for gender prediction in English
but in Hindi it isn?t the case as gender could vary
based on its agreement with verb/modifier. The val-
ues that gender can take for a word in a given context
are masculine(m), feminine(f ), any (either m or f ) or
none (neither m nor f ). Table 5 gives example for
each gender value.
Nouns inherently carry gender information. Pro-
Case Word Gloss
direct ladZake boy-Pl
oblique ladZake boy-sg
any bAraha twelve (cardinals)
none kaha say
Table 7: Case value examples
nouns (of genitive form), adjectives and verbs inflect
according to the gender of the noun they refer to.
4.2.2 Number
Every noun belongs to a unique number class.
Noun modifiers and verbs have different forms for
each number class and inflect accordingly to match
the grammatical number of the nouns to which they
refer.
Number takes the values singular (sg), plural (pl),
any (either sg or pl) and none (neither sg nor pl). Ta-
ble 6 lists examples for each of the values. In it,
ladZake takes the grammatical number sg (in di-
rect case) or pl (in oblique case) depending on the
context in which it occurs. It may be noted that since
PBA does not consider the word?s context, it outputs
both the values and leaves the disambiguation to the
subsequent stages.
4.2.3 Person
Apart from first, second and third persons, Hindi
also has the honorific forms, resulting in 1h, 2h and
3h. Postpositions do not have person information,
hence none is also a possible value. Apart from the
above mentioned grammatical person values, any is
also a feasible value.
4.2.4 Case Marker
Case markers in Hindi (direct and oblique) are at-
tributed to nouns and pronouns. Table 7 lists few
examples.
Words which inflect for gender, number, person
and case primarily undergo affixation at the end.
Features for GNP & Case Marker
The following features were tried out in building
the models for gender, number, person and case pre-
diction:
? Word level features
? Word
122
? Last 2 characters
? Last 3 characters
? Last 4 characters
? Character N-grams of the word
? Lemma
? Word Length
? Sentence level features
? Lexical category5
? Next word
? Previous word
Combinations of these features have been tried
out to build the SVM models for GNP and case. For
each of these tasks, feature tuning was done sep-
arately. In Malladi and Mannem (2013), a linear
SVM classification (Fan et al, 2008) is used to build
statistical models for GNP and case but we found
that with RBF kernel (non-linear SVM)6 we achieve
better accuracies. Furthermore, the parameters (C,
?) of the RBF kernel are learned using grid search
technique.
4.3 Vibhakti and TAM
Vibhakti and TAM are helpful in identifying the
karaka7 dependency labels in HTB. While nouns
and pronouns take vibhakti, verbs inflect for TAM.
Both TAM and vibhakti occur immediately after the
words in their respective word classes.
Instead of building statistical models for vibhakti
and TAM prediction, SMA uses heuristics on POS
tag sequences to predict the correct value. The POS
tags of words following nouns, pronouns and verbs
give an indication as to what the vibhakti/TAM are.
Words with PSP (postposition) and NST (noun with
spatial and temporal properties) tags are generally
considered as the vibhakti for the preceding nouns
and pronouns. A postposition in HTB is annotated
as PSP only if it is written separately (usane/PRP
vs usa/PRP ne/PSP). For cases where the postposi-
tion is not written separately SMA relies on the tree-
bank data to get the suffix. Similarly, words with
5POS is considered as a sentence level feature since tagging
models use the word ngrams to predict the POS category
6LIBSVM tool is used to build non-linear SVM models for
our experiments (Chang and Lin, 2011).
7karakas are syntactico-semantic relations which are em-
ployed in Paninian framework (Begum et al, 2008; Bharati et
al., 2009b)
VAUX tag form the TAM for the immediately pre-
ceding verb.
The PBA takes individual words as input and
hence does not output the entire vibhakti or TAM
of the word in the sentence. It only identifies these
values for those words which have the information
within the word form (e.g. usakA he+Oblique,
kiyA do+PAST).
In the sentence,
rAma/NNP kA/PSP kiwAba/NN
cori/NN ho/VM sakawA/VAUX
hE/VAUX
PBA identifies rAma?s vibhakti as 0 and ho?s TAM
as 0. Whereas in HTB, vibhakti and TAM of rAma
and ho are annotated as 0 kA and 0 saka+wA hE
respectively. SMA determines this information pre-
cisely and Morfette which can predict other morph
features, is not capable of predicting TAM and Vib-
hakti as these features are specific to Indian lan-
guages.
5 Evaluation Systems
SMA is compared with a baseline system, Morfette
and two versions of the PBA wherever relevant. The
baseline system takes the word form itself as the
lemma and selects the most frequent value for the
rest of the attributes.
Since PBA is a rule based analyzer which gives
more than one analysis for words, we use two ver-
sions of it for comparison. The first system is the
oracle PBA (referred further as O-PBA) which uses
an oracle to pick the best analysis from the list of
all analyses given by the PBA. The second version
of the PBA (F-PBA) picks the first analysis from the
output as the correct analysis.
Morfette can perdict lemma, gender, number, per-
son and case attributes but it cannot predict TAM
and Vibhakti as they do not have a definite set of pre-
defined values unlike other morphological attributes.
6 Experiments and Results
SMA approach to Hindi morphological analysis
is based on handling each of the seven attributes
(lemma, gender, number, person, case, vibhakti and
TAM) separately. However, evaluation is performed
123
Analysis
Test Data - Overall(%) Test Data - OOV of SMA(%)
Baseline F-PBA O-PBA Morfette SMA Baseline F-PBA O-PBA Morfette SMA
L 71.12 83.10 86.69 94.14 95.84 78.10 82.08 82.48 90.30 89.51
G 37.43 72.98 79.59 95.05 96.19 60.22 43.07 44.06 72.03 82.65
N 52.87 72.22 80.50 94.09 95.37 69.60 44.53 47.56 84.89 90.44
P 45.59 74.33 84.13 94.88 96.38 78.30 52.51 53.89 84.76 94.85
C 29.31 58.24 81.20 93.91 95.32 43.60 31.40 47.36 80.21 88.52
V/T 65.40 53.05 59.65 NA 97.04 58.31 33.58 34.56 NA 96.04
L+C 16.46 48.84 72.06 88.56 91.39 32.52 28.50 44.66 72.89 79.09
L+V/T 54.78 44.57 51.71 NA 93.06 53.56 31.73 32.72 NA 86.41
G+N+P 23.05 61.10 73.81 88.36 91.11 47.49 35.75 39.58 62.33 76.52
G+N+P+C 9.72 45.73 70.87 84.43 87.78 21.04 20.91 35.95 55.74 69.99
L+G+N+P 20.27 53.29 66.28 83.44 87.51 44.72 34.63 38.46 57.85 69.13
L+G+N+P+C 8.57 38.25 63.41 79.73 84.25 19.33 19.92 34.89 51.52 63.06
L+G+N+P+C+V/T 1.25 32.53 42.80 NA 82.12 4.02 14.51 18.67 NA 60.07
L-lemma, G-gender, N-number, P-person, C-case, V/T-Vibhakti/TAM
Table 8: Accuracies of SMA compared with F-PBA, O-PBA and baseline systems.
on individual attributes as well as on the combined
output.
SMA builds models for lemma, gender, number,
person and case prediction trained on the training
data of the HTB. All the models are tuned on devel-
opment data and evaluated on test data of the HTB.
Table 8 presents the accuracies of five systems
(baseline, F-PBA, O-PBA, Morfette and SMA) in
predicting the morphological attributes of all the
words in the HTB?s test data and also for OOV
words of SMA (i.e. words that occur in the test sec-
tion but not in training section of HTB)8. The accu-
racies are the percentages of words in the data with
the correct analysis. It may be noted that SMA per-
forms significantly better than the best analyses of
PBA and the baseline system in all the experiments
conducted. As far as Morfette is concerned, it per-
forms on par with SMA in terms of overall accuracy
but for OOV words, except for lemma prediction,
SMA outperforms Morfette by significant margin.
Table 13 lists the accuracies of lemma, gender,
number, person and case for the most frequently oc-
curring POS tags. Table 12 reports the same for
OOV words. The number of OOV words in postpo-
8OOV words for SMA need not be out of vocabulary for
PBA?s dictionaries. Table 8 lists accuracies for OOV words of
SMA. We shall also report accuracies for OOV words of PBA
in the later part of the paper (Table 11).
Metric Exp-1a Exp-2b Exp-3c
LAS 87.75 89.41 89.82
UAS 94.41 94.50 94.81
LA 89.89 91.67 91.96
Table 9: MALT Parser?s accuracies on HTB test data.
Unlabeled Attachment Score (UAS) is the percentage of
words with correct heads. Labeled Accuracy (LA) is the
percentage of words with correct dependency labels. La-
beled Attachment Score (LAS) is the percentage of words
with both correct heads and labels.
aExp-1: Without morph features
bExp-2: With morph features predicted by SMA
cExp-3: With gold morph features (as annotated in HTB)
sition and pronoun categories is quite less and hence
have not been included in the table.
Hindi derivational morph analyzer (Kanuparthi
et al, 2012) and the morph analyzer developed by
Punjab University (Goyal and Lehal, 2008) do not
add much to PBA accuracy since they are devel-
oped with PBA as the base. Out of 334,287 words
in HTB, the derivational morph analyzer identified
only 9,580 derivational variants. For the remaining
words, it gives similar analysis as PBA.
6.1 Lemma
The evaluation metric for lemma?s model is accu-
racy, which is the percentage of predicted lemmas
124
that are correct. The phrase based translation sys-
tem used to predict lemmas achieved an accuracy of
95.84% compared to O-PBA?s 86.69%. For OOV
words, the PBA outputs the word itself as the lemma
whereas the translation-based lemma model is ro-
bust enough to give the analysis.
The translation-based lemma model and O-PBA
report accuracies of 89.51% and 82.48% respec-
tively for OOV words of SMA. In terms of
both overall and OOV accuracies, translation-based
model outperforms PBA. Though SMA performs
better than Morfette in terms of overall accuracy, but
for OOV accuracy Morfette narrowly outperforms
SMA.
The postposition accuracy is significantly worse
than the overall accuracy. This is because the con-
fusion is high among postpositions in HTB. For ex-
ample, out of 14,818 occurrences of ke, it takes the
lemma kA in 7,763 instances and ke in 7,022 cases.
This could be the result of an inconsistency in the an-
notation process of HTB. The accuracies for verbs
are low (when compared to Nouns, Adjectives) as
well mainly because verbs in Hindi take more inflec-
tions than the rest. The accuracy for verbs is even
lower for OOV words (69.23% in Table 12).
6.2 Gender, Number, Person and Case
The accuracies of gender, number, person and case
hover around 95% but the combined (G+N+P) accu-
racy drops to 91.11%. This figure is important if one
wants to enforce agreement in parsing.
The OOV accuracy for person is close to overall
accuracy as most of the OOV words belong to the
3rd person category. It is not the same case for gen-
der and number. Gender particularly suffers a sig-
nificant drop of 14% for OOV words confirming the
theory that gender prediction is a difficult problem
without knowing the semantics of the word.
The number and person accuracies for verbs are
consistently low for OOV words as well as for seen
words. This could be because SMA doesn?t handle
long distance agreement during GNP prediction.
Until now, we reported accuracies for OOV words
of SMA. Table 11 lists accuracies for OOV words
of the PBA (i.e. words which are not analyzed by
the PBA) in the test section of HTB. SMA clearly
outperforms baseline system and also performs bet-
ter than F-PBA and O-PBA as they do not give any
Analysis Accuracy OOV Accuracy
Gender 95.74 80.08
Number 95.29 89.71
Person 96.12 94.06
Case 95.16 88.32
G+N+P 90.92 74.14
G+N+P+C 87.72 68.47
Table 10: Joint Model for Gender, Number, Person, Case
analyses.
In a nutshell, we have evaluated SMA for OOV
words of the PBA as well as for OOV words of
SMA. In both the cases, SMA performed better than
other systems. We shall evaluate SMA in a chal-
lenging scenario wherein training data consists of
the words from the HTB which are analyzed by the
PBA and test data consists of the remaining unana-
lyzed words by the PBA. Thereby, the entire test data
contains only out of vocabulary instances for both
SMA and PBA. Table 14 presents the results of this
new evaluation. The results are almost similar with
that of OOV results shown in Table 8 except for Per-
son. The reason behind that could be, in the training
data there are only 0.1% instances of 3h class but in
test data their presence is quite significant (approx-
imately 10%). The training instances for 3h class
were not sufficient for the model to learn and hence
very few of these instances were identified correctly.
This explains the drop in Person accuracy for this
experiment scenario.
It may be noted that, we have used gold POS tags
for all our experiments related to GNP and case pre-
diction. There are numerous efforts on building POS
taggers for Hindi. The ILMT pos tagger9 is 96.5%
accurate on the test data of the HTB. Table 15 re-
ports the accuracies of gender, number, person and
case using the automatic POS tags predicted by the
ILMT tagger. The results are similar to that of the
experiments conducted with gold POS tags.
Malladi and Mannem (2013) have build separate
models for gender, number, person and case. Table
10 reports the results of Joint Model for these morph
attributes. In terms of accuracy, Joint Model is as
efficient as individual models.
9http://ilmt.iiit.ac.in/
125
Analysis Baseline SMA
Lemma 65.40 95.96
Gender 57.09 95.93
Number 76.79 95.17
Person 65.76 96.42
Case 46.39 95.17
Table 11: Accuracy for OOV words of PBA
Analysis Noun Verb Adjective
Lemma 92.18 69.23 88.35
Gender 80.49 86.15 92.23
Number 92.35 76.92 87.38
Person 96.64 75.38 100.00
Case 88.81 98.46 70.87
Table 12: OOV accuracies for words (by POS tags)
6.3 TAM and Vibhakti
The proposed heuristics for Vibhakti and TAM pre-
diction gave accuracy of 97.04% on test data set of
HTB. On the entire HTB data, SMA achieved accu-
racy of 98.88%. O-PBA gave accuracy of 59.65%
for TAM and Vibhakti prediction on test part of
HTB. The reason behind low performance of O-
PBA is that it identifies the TAM and vibhakti val-
ues for each word separately and doesn?t consider
the neighbouring words in the sentence.
7 Effect on Parsing
The effect of morphological features on parsing is
well documented (Ambati et al, 2010a). Previous
works used gold morphological analysis to prove
their point. In this work, we also evaluated the effect
of automatic morph features (predicted by SMA)
on dependency parsing. MALT parser was trained
Analysis N V PSP JJ PRP
Lemma 98.50 94.28 89.41 97.99 98.78
Gender 93.30 95.34 98.93 98.42 94.24
Number 96.26 89.67 96.45 96.26 88.98
Person 98.58 85.28 99.45 99.57 90.94
Case 94.67 98.95 93.26 83.76 95.90
N:Noun, V:Verb, PSP:postposition, JJ:adjective, PRP:pronoun
Table 13: Overall accuracies for words (by POS tags)
Analysis Baseline SMA
Gender 57.09 73.09
Number 76.79 85.71
Person 65.76 77.93
Case 33.62 89.05
Table 14: Evaluation of SMA in a challenging scenario: train-
ing data consists only of words analyzed by PBA and test data
consists of remaining unanalyzed words.
Analysis Overall OOV
Gender 95.68 80.41
Number 94.97 90.30
Person 96.09 96.17
Case 94.61 88.19
Table 15: Accuracy of SMA with auto POS tags
on gold-standard POS tagged HTB data with and
with out morph features. Table 9 lists the evaluation
scores for these settings. While the unlabeled at-
tachment score (UAS) does not show significant im-
provement, the labeled attachment score (LAS) and
label accuracy (LA) have increased significantly.
Ambati et al (2010a) also reported similar results
with gold-standard morph features. Lemma, case,
vibhakti and TAM features contribute to the increase
in label accuracy because of the karaka labels in
Paninian annotation scheme (Begum et al, 2008).
Table 9 also lists the performance of MALT parser
with gold morph features (as annotated in HTB).
It may be noted that, predicted morph features had
similar effect on hindi dependency parsing as of gold
features which is desirable making SMA usable for
real scenario applications.
8 Extending the work to Telugu and Urdu
We shall look at how SMA performs in prediciting
GNP and case for other morphologically rich Indian
languages: Telugu and Urdu. At this stage, we have
not done any language-dependent engineering effort
Language #Sentences #Words
Urdu 5230 68588
Telugu 1600 6321
Table 16: Telugu and Urdu Treebank Statistics
126
Analysis
Telugu Urdu
Overall OOV Overall OOV
Gender 96.49 89.85 89.14 88.18
Number 90.65 75.13 91.62 91.35
Person 94.82 85.79 93.37 95.53
Case 96.49 89.34 85.49 79.01
Table 17: SMA for other Mor-FOW languages: Telugu and
Urdu
in improving the results rather we want to see how
well the system works for other languages using the
minimalistic feature set employed for Hindi mor-
phological analysis.
Telugu Treebank was released for ICON 2010
Shared Task(Husain et al, 2010) and a modified ver-
sion of that data is used for our experiments. Urdu
Treebank which is still under development at IIIT
Hyderabad10 is used for experiments related to Urdu
morph analysis. Refer table 16 for treebank statis-
tics.
Table 17 shows the evaluation results for Telugu
and Urdu.
9 Conclusion and Future work
In conclusion, SMA is a robust state-of-the-art sta-
tistical morphological analyzer which outperforms
previous analyzers for Hindi by a considerable mar-
gin. SMA achieved an accuracy of 63.06% for
lemma, gender, number, person and case whereas
PBA and Morfette are 34.89% and 51.52% accurate
respectively. With the predicted morphological at-
tributes by SMA, we achieve a labeled attachment
score of 89.41 while without these morphological at-
tributes the parsing accuracy drops to 87.75.
The agreement phenomenon in Hindi provides
challenges in predicting gender, number and person
of words in their sentential context. These can be
better predicted if dependency relations are given as
input. However, the standard natural language anal-
ysis pipeline forbids using parse information during
morphological analysis. This provides an oppor-
tunity to explore joint modelling of morphological
analysis and syntactic parsing for Hindi. We plan to
experiment this as part of our future work.
Performance of Morfette is comparable to SMA
10iiit.ac.in
and for lemma prediction in the case of OOV words,
Morfette outperforms SMA. We plan to build a hy-
brid system whose feature set includes features from
both the systems.
References
Bharat Ram Ambati, Samar Husain, Sambhav Jain,
Dipti Misra Sharma, and Rajeev Sangal. 2010a. Two
methods to incorporate local morphosyntactic features
in hindi dependency parsing. In Proceedings of the
NAACL HLT 2010 First Workshop on Statistical Pars-
ing of Morphologically-Rich Languages, pages 22?30.
Association for Computational Linguistics.
Bharat Ram Ambati, Samar Husain, Joakim Nivre, and
Rajeev Sangal. 2010b. On the role of morphosyn-
tactic features in hindi dependency parsing. In Pro-
ceedings of the NAACL HLT 2010 First Workshop
on Statistical Parsing of Morphologically-Rich Lan-
guages, pages 94?102. Association for Computational
Linguistics.
Rafiya Begum, Samar Husain, Arun Dhwaj, Dipti Misra
Sharma, Lakshmi Bai, and Rajeev Sangal. 2008. De-
pendency annotation scheme for indian languages. In
Proceedings of IJCNLP.
Akshar Bharati, Vineet Chaitanya, Rajeev Sangal, and
KV Ramakrishnamacharyulu. 1995. Natural lan-
guage processing: A Paninian perspective. Prentice-
Hall of India New Delhi.
Akshar Bharati, Samar Husain, Meher Vijay, Kalyan
Deepak, Dipti Misra Sharma, and Rajeev Sangal.
2009a. Constraint based hybrid approach to parsing
indian languages. Proc of PACLIC 23. Hong Kong.
Akshara Bharati, Dipti Misra Sharma, Samar Husain,
Lakshmi Bai, Rafiya Begam, and Rajeev Sangal.
2009b. Anncorra: Treebanks for indian languages,
guidelines for annotating hindi treebank.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM:
A library for support vector machines. ACM Transac-
tions on Intelligent Systems and Technology, 2:27:1?
27:27.
Grzegorz Chrupa?a, Georgiana Dinu, and Josef Van Gen-
abith. 2008. Learning morphology with morfette.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A li-
brary for large linear classification. Journal of Ma-
chine Learning Research, 9:1871?1874.
John Goldsmith. 2000. Linguistica: An automatic mor-
phological analyzer. In Proceedings of 36th meeting
of the Chicago Linguistic Society.
Vishal Goyal and G. Singh Lehal. 2008. Hindi morpho-
logical analyzer and generator. In Emerging Trends in
127
Engineering and Technology, 2008. ICETET?08. First
International Conference on, pages 1156?1159. IEEE.
Samar Husain, Prashanth Mannem, Bharat Ram Ambati,
and Phani Gadde. 2010. The icon-2010 tools contest
on indian language dependency parsing. Proceedings
of ICON-2010 Tools Contest on Indian Language De-
pendency Parsing, ICON, 10:1?8.
Yamuna Kachru. 2006. Hindi, volume 12. John Ben-
jamins Publishing Company.
Nikhil Kanuparthi, Abhilash Inumella, and Dipti Misra
Sharma. 2012. Hindi derivational morphological an-
alyzer. In Proceedings of the Twelfth Meeting of the
Special Interest Group on Computational Morphology
and Phonology, pages 10?16. Association for Compu-
tational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, et al 2007. Moses: Open source toolkit for sta-
tistical machine translation. In Proceedings of the 45th
Annual Meeting of the ACL on Interactive Poster and
Demonstration Sessions, pages 177?180. Association
for Computational Linguistics.
Deepak Kumar Malladi and Prashanth Mannem. 2013.
Statistical morphological analyzer for hindi. In Pro-
ceedings of 6th International Joint Conference on Nat-
ural Language Processing.
Mark Pedersen, Domenyk Eades, Samir K Amin, and
Lakshmi Prakash. 2004. Relative clauses in hindi
and arabic: A paninian dependency grammar analy-
sis. COLING 2004 Recent Advances in Dependency
Grammar, pages 9?16.
Dipti Misra Sharma, Prashanth Mannem, Joseph Van-
Genabith, Sobha Lalitha Devi, Radhika Mamidi, and
Ranjani Parthasarathi, editors. 2012. Proceedings of
the Workshop on Machine Translation and Parsing in
Indian Languages. Mumbai, India, December.
128
