Proceedings of NAACL HLT 2007, Companion Volume, pages 133?136,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
An integrated architecture for speech-input multi-target machine translation
Alicia Pe?rez, M. Ine?s Torres
Dep. of Electricity and Electronics
University of the Basque Country
manes@we.lc.ehu.es
M. Teresa Gonza?lez, Francisco Casacuberta
Dep. of Information Systems and Computation
Technical University of Valencia
fcn@dsic.upv.es
Abstract
The aim of this work is to show the abil-
ity of finite-state transducers to simultane-
ously translate speech into multiple lan-
guages. Our proposal deals with an ex-
tension of stochastic finite-state transduc-
ers that can produce more than one out-
put at the same time. These kind of de-
vices offer great versatility for the inte-
gration with other finite-state devices such
as acoustic models in order to produce a
speech translation system. This proposal
has been evaluated in a practical situation,
and its results have been compared with
those obtained using a standard mono-
target speech transducer.
1 Introduction
Finite-state models constitute an important frame-
work both in syntactic pattern recognition and in
language processing. Specifically, stochastic finite-
state transducers (SFSTs) have proved to be useful
for machine translation tasks within restricted do-
mains; they usually offer high speed during the de-
coding step and they provide competitive results in
terms of error rates (Mohri et al, 2002). Moreover,
SFSTs have proved to be versatile models, which
can be easily integrated with other finite-state mod-
els (Pereira and Riley, 1997).
The article (Casacuberta and Vidal, 2004) ex-
plored an automatic method to learn an SFST from a
bilingual set of samples for machine translation pur-
poses, the so-called GIATI (Grammar Inference and
Alignments for Transducers Inference). It described
how to learn both the structural and the probabilistic
components of an SFST making use of underlying
alignment models.
A multi-target SFST is a generalization of stan-
dard SFSTs, in such a way that every input string
in the source language results in a tuple of output
strings each being associated to a different target
language. An extension of GIATI that allowed to in-
fer a multi-target SFST from a multilingual corpus
was proposed in (Gonza?lez and Casacuberta, 2006).
A syntactic variant of this method (denoted as GI-
AMTI) has been used in this work in order to infer
the models from training samples as it is summa-
rized in section 3.
On the other hand, speech translation has been al-
ready carried out by integrating acoustic models into
a SFST (Casacuberta et al, 2004). Our main goal
in this work is to extend and assess these method-
ologies to accomplish spoken language multi-target
translation. Section 2 deals with this proposal by
presenting a new integrated architecture for speech-
input multi-target translation. Under this approach
spoken language can be simultaneously decoded and
translated into m languages using a unique network.
In section 4, the performance of the system has
been experimentally evaluated over a trilingual task
which aims to translate TVweather forecast into two
languages at the same time.
2 An integrated architecture for
speech-input multi-target translation
The classical architecture for spoken language
multi-target translation involves a speech recogni-
133
tion system in a serial architecture withm decoupled
text-to-text translators. Thus, the whole process in-
volves m + 1 searching stages, a first one for the
speech signal transcription into the source language
text string, and further m for the source language
translation into the m target languages. If we re-
placed the m translators by the multi-target SFST,
the problem would be reduced to 2 searching stages.
Nevertheless, in this paper we propose a natural way
for acoustic models to be integrated in the same net-
work. As a result, the input speech-signal can be
simultaneously decoded and translated into m target
languages just in a single searching stage.
Given the acoustic representation (x) of a speech
signal, the goal of multi-target speech translation
is to find the most likely m target strings (tm);
that is, one string (ti) per target language involved
(i ? {1, . . . ,m}). This approach is summarized
in eq. (1), where the hidden variable s can be in-
terpreted as the transcription of the speech signal:
t?m = argmax
tm
P (tm|x) = argmax
tm
?
s
P (tm, s|x)
(1)
Making use of Bayes? rule, the former expression
turns into:
t?m = argmax
tm
?
s
P (tm, s)P (x|tm, s) (2)
Empirically, there is no loss of generality if we as-
sume that the acoustic signal representation depends
only on the source string: i.e., that P (x|tm, s) is in-
dependent of tm. In this sense, eq. (2) can be rewrit-
ten as:
t?m = argmax
tm
?
s
P (tm, s)P (x|s) (3)
Equation (3) combines a standard acoustic model,
P (x|s), and a multi-target translation model,
P (tm, s), both of whom can be integrated on the fly
during the searching routine. Nevertheless, the outer
maximization is computationally very expensive to
search for the optimal tuple of target strings tm in
an effective way. Thus we make use of the so called
Viterbi approximation, which finds the best path.
3 Inference
Given a multilingual corpus, that is, a finite set of
multilingual samples (s, t1, . . . , tm) ? ?? ? ??1 ?
? ? ? ? ??m, where ti denotes the translation of the
source sentence s (formed by words of the input vo-
cabulary ?) into the i-th target language, which, in
its turn, has a vocabulary ?i, the GIAMTI method
can be outlined as follows:
1. Each multilingual sample is transformed into a
single string from an extended vocabulary (? ?
????1 ? ? ? ? ??
?
m) using a labelling function
(Lm). This transformation searches an ade-
quate monotonous segmentation for each of the
m source-target language pairs. A monotonous
segmentation copes with monotonous align-
ments, that is, j < k ? aj < ak following
the notation of (Brown et al, 1993). Each
source word is then joined with a target phrase
of each language as the corresponding segmen-
tation suggests. Each extended symbol consists
of a word from the source language plus zero
or more words from each target language.
2. Once the set of multilingual samples has been
converted into a set of single extended strings
(z ? ??), a stochastic regular grammar can be
inferred.
3. The extended symbols associated with the
transitions of the automaton are transformed
into one input word and m output phrases
(w/p?1/ . . . /p?m) by the inverse labeling func-
tion (L?m), leading to the required transducer.
In this work, the first step of the algorithm (as
described above), which is the one that handles
the alignment and segmentation routines, relies on
statistical alignments obtained with GIZA++ (Och,
2000). The second step was implemented us-
ing our own language modeling toolkit, which
learns stochastic k-testable in the string-sense gram-
mars (Torres and Varona, 2001), and allows for
back-off smoothing.
4 Experimental results
4.1 Task and corpus
We have implemented a highly practical application
that could be used to translate on-line TV weather
forecasts into several languages, taking the speech
of the presenter as the input and producing as output
text-strings, or sub-titles, in several languages. For
134
this purpose, we used the corpus METEUS (see Ta-
ble 1) which consists of a set of trilingual sentences,
in English, Spanish and Basque, as extracted from
weather forecast reports that had been published on
the Internet. Basque language is a minority lan-
guage, spoken in a small area of Europe and also
within some small American communities (such as
that in Boise, Idaho). In the Basque Country it has
an official status along with Spanish. However both
languages differs greatly in syntax and in semantics.
The differences in the size of the vocabulary (see
Table 1), for instance, are due to the agglutinative
nature of the Basque language.
With regard to the speech test, the input consisted
of the speech signal recorded by 36 speakers, each
one reading out 50 sentences from the test-set in Ta-
ble 1. That is, each sentence was read out by at least
three speakers. The input speech resulted in approx-
imately 3.50 hours of audio signal. Needless to say,
the application that we envisage has to be speaker-
independent if it is to be realistic.
Spanish Basque English
T
ra
in
in
g Sentences 14,615
Different Sent. 7,225 7,523 6,634
Words 191,156 187,462 195,627
Vocabulary 702 1,147 498
Average Length 13.0 12.8 13.3
Te
st
Different Sent. 500
Words 8,706 8,274 9,150
Average Length 17.4 16.5 18.3
Perplexity (3grams) 4.8 6.7 5.8
Table 1: Main features of the METEUS corpus.
4.2 System evaluation
The experimental setup was as follows: the multi-
target SFST was learned from the training set in Ta-
ble 1 using the GIAMTI algorithm described in sec-
tion 1; then, the speech test was translated, and the
output provided by the system in each language was
compared to the corresponding reference sentence.
Additionally, two mono-target SFST were inferred
from the same training set with their outputs for the
aforementioned test to be taken as baseline.
4.2.1 Computational cost
The expected searching time and the amount of
memory that needs to be allocated for a given model
are two key parameters to bear in mind in speech-
input machine translation applications. These values
can be objectively measured based on the size and on
the average branching factor of the model displayed
in Table 2.
multi-target mono-targetS2B S2E
Nodes 52,074 35,034 20,148
Edges 163,146 115,526 69,690
Braching factor 3.30 3.13 3.46
Table 2: Features of multi-target model and the two
decoupled mono-target models (one for Spanish to
Basque translation, referred to as S2B, and the sec-
ond for Spanish to English, S2E).
Adding the states and the edges up for the two
mono-target SFSTs that take part in the decoupled
architecture (see Table 2), we conclude that the de-
coupled model needs a total of 185, 216 edges to be
allocated in memory, which represents an increment
of 13% in memory-space with respect to the multi-
target model.
On the other hand, the multi-target approach of-
fers a slightly smaller branching factor than each
mono-target approach. As a result, fewer paths have
to be explored with the multi-target approach than
with the decoupled one, which means that searching
for a translation can be faster. In fact, experimental
results in Table 3 show that the mono-target archi-
tecture works%11more slowly than the multi-target
one.
multi-target mono-targetS2B S2E S2B+S2E
Time (s) 30,514 24,398 9,501 33,899
Table 3: Time needed to translate the speech-test
into two languages.
Summarizing, in terms of computational cost
(space and time), a multi-target SFST performs bet-
ter than the mono-target decoupled system.
4.2.2 Performance
So far, the capability of the systems have been as-
sessed in terms of time and spatial costs. However,
the quality of the translations they provide is, doubt-
less, the most relevant evaluation criterion. In order
to assess the performance of the system in a quan-
titative manner, the following evaluation parameters
135
were computed for each scenario: bilingual evalua-
tion under study (BLEU), position independent er-
ror rate (PER) and word error rate (WER).
As can be derived from the Speech-input trans-
lation results shown in Table 4, slightly better re-
sults are obtained with the classical mono-target SF-
STs, compared with the multi-target approach. From
Spanish into English the improvement is around
3.4% but from Spanish into Basque, multi-target ap-
proach works better with an improvement of a 0.8%.
multi-target mono-target
S2B S2E S2B S2E
BLEU 39.5 59.0 39.2 61.1
PER 42.2 25.3 41.5 23.6
WER 51.5 33.9 50.5 31.9
Table 4: Speech-input translation results for Spanish
into Basque (S2B) and Spanish into English (S2E)
using a multi-target SFST or two mono-target SF-
STs.
The process of speech signal decoding is itself
introducing some errors. In an attempt to measure
these errors, the text transcription of the recognized
input signal was extracted and compared to the input
reference in terms of WER as shown in Table 5.
multi-target mono-targetS2B S2E
WER 10.7 9.3 9.1
Table 5: Spanish speech decoding results for the
multi-target SFST and the two mono target SFSTs.
5 Concluding remarks and further work
A fully embedded architecture that integrates the
acoustic model into the multi-target translation
model for multiple speech translation has been pro-
posed. Due to the finite-state nature of this model,
the speech translation engine is based on a Viterbi-
like algorithm. The most significant feature of this
approach is its ability to carry out both the recogni-
tion and the translation into multiple languages inte-
grated in a unique model.
In contrast to the classical decoupled systems,
multi-target SFSTs enable the translation from one
source language simultaneously into several target
languages with lower computational costs (in terms
of space and time) and comparable qualitative re-
sults.
In future work we intend to make a deeper study
on the performance of the multi-target system as the
amount of targets increase, since the amount of pa-
rameters to be estimated also increases.
Acknowledgements
This work has been partially supported by the Uni-
versity of the Basque Country and by the Spanish
CICYT under grants 9/UPV 00224.310-15900/2004
and TIC2003-08681-C02-02 respectively.
References
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: Parameter estimation. Computa-
tional Linguistics, 19(2):263?311.
Francisco Casacuberta and Enrique Vidal. 2004. Ma-
chine translation with inferred stochastic finite-state
transducers. Computational Linguistics, 30(2):205?
225.
F. Casacuberta, H. Ney, F. J. Och, E. Vidal, J. M.
Vilar, S. Barrachina, I. Garc??a-Varea, D. Llorens,
C. Mart??nez, S. Molau, F. Nevado, M. Pastor, D. Pico?,
A. Sanchis, and C. Tillmann. 2004. Some approaches
to statistical and finite-state speech-to-speech transla-
tion. Computer Speech and Language, 18:25?47, Jan-
uary.
M. Teresa Gonza?lez and Francisco Casacuberta. 2006.
Multi-Target Machine Translation using Finite-State
Transducers. In Proceedings of TC-Star Speech to
Speech Translation Workshop, pages 105?110.
Mehryar Mohri, Fernando Pereira, and Michael Ri-
ley. 2002. Weighted finite-state transducers in
speech recognition. Computer, Speech and Language,
16(1):69?88, January.
Franz J. Och. 2000. GIZA++: Training of statistical
translation models.
Fernando C.N. Pereira and Michael D. Riley. 1997.
Speech Recognition by Composition of Weighted Fi-
nite Automata. In Emmanuel Roche and Yves Sch-
abes, editors, Finite-State Language Processing, Lan-
guage, Speech and Communication series, pages 431?
453. The MIT Press, Cambridge, Massachusetts.
M. Ine?s Torres and Amparo Varona. 2001. k-tss lan-
guage models in speech recognition systems. Com-
puter Speech and Language, 15(2):127?149.
136
Proceedings of the Second Workshop on Statistical Machine Translation, pages 56?63,
Prague, June 2007. c?2007 Association for Computational Linguistics
Speech-input multi-target machine translation
Alicia Pe?rez, M. Ine?s Torres
Dep. of Electricity and Electronics
University of the Basque Country
manes@we.lc.ehu.es
M. Teresa Gonza?lez, Francisco Casacuberta
Dep. of Information Systems and Computation
Technical University of Valencia
fcn@dsic.upv.es
Abstract
In order to simultaneously translate speech
into multiple languages an extension of
stochastic finite-state transducers is pro-
posed. In this approach the speech trans-
lation model consists of a single network
where acoustic models (in the input) and the
multilingual model (in the output) are em-
bedded.
The multi-target model has been evaluated
in a practical situation, and the results have
been compared with those obtained using
several mono-target models. Experimental
results show that the multi-target one re-
quires less amount of memory. In addition, a
single decoding is enough to get the speech
translated into multiple languages.
1 Introduction
In this work we deal with finite-state models which
constitute an important framework in syntactic pat-
tern recognition for language and speech processing
applications (Mohri et al, 2002; Pereira and Riley,
1997). One of their outstanding characteristics is the
availability of efficient algorithms for both optimiza-
tion and decoding purposes.
Specifically, stochastic finite-state transducers
(SFSTs) have proved to be useful for machine trans-
lation tasks within restricted domains. There are
several approaches implemented over SFSTs which
range from word-based systems (Knight and Al-
Onaizan, 1998) to phrase-based systems (Pe?rez et
al., 2007). SFSTs usually offer high speed during
the decoding step and they provide competitive re-
sults in terms of error rates. In addition, SFSTs have
proved to be versatile models, which can be easily
integrated with other finite-state models, such as a
speech recognition system for speech-input transla-
tion purposes (Vidal, 1997). In fact, the integrated
architecture has proved to work better than the de-
coupled one. Our main goal is, hence, to extend
and assess these methodologies to accomplish spo-
ken language multi-target translation.
As far as multilingual translation is concerned,
there are two main trends in machine translation de-
voted to translate an input string simultaneously into
m languages (Hutchins and Somers, 1992): inter-
lingua and parallel transfer. The former has his-
torically been a knowledge-based technique that re-
quires a deep-analysis effort, and the latter consists
on m decoupled translators in a parallel architec-
ture. These translators can be either knowledge or
example-based. On the other hand, in (Gonza?lez
and Casacuberta, 2006) an example based technique
consisting of a single SFST that cope with multiple
target languages was presented. In that approach,
when translating an input sentence, only one search
through the multi-target SFST is required, instead of
the m independent decoding processes required by
the mono-target translators.
The classical layout for speech-input multi-target
translation includes a speech recognition system in
a serial architecture with m decoupled text-to-text
translators. Thus, this architecture entails a decod-
ing stage of the speech signal into the source lan-
guage text, and m further decoding stages to trans-
late the source text into each of the m target lan-
56
guages. If we supplant the m translators with the
multi-target SFST, the problem would be reduced to
2 searching stages. Nevertheless, in this paper we
propose a natural way for acoustic models to be in-
tegrated in the multilingual network itself, in such
a way that the input speech signal can be simulta-
neously decoded and translated into m target lan-
guages. As a result, due to the fact that there is just
a single searching stage, this novel approach entails
less computational cost.
The remainder of the present paper is structured
as follows: section 2 describes both multi-target SF-
STs and the inference algorithm from training ex-
amples; in section 3 a novel integrated architecture
for speech-input multi-target translation is proposed;
section 4 presents a practical application of these
methods, including the experimental setup and the
results they produced; finally, section 5 summarizes
the main conclusions of this work.
2 Multi-target stochastic finite-state
transducers
A multi-target SFST is a generalization of standard
SFSTs, in such a way that every input string in the
source language results in a tuple of output strings
each being associated to a different target language.
2.1 Definition
A multi-target stochastic finite-state transducer is a
tuple T = ??,?1 . . .?m, Q, q0, R, F, P ?, where:
? is a finite set of input symbols (source vocabu-
lary);
?1 . . .?m are m finite sets of output symbols (tar-
get vocabularies);
Q is a finite set of states;
q0 ? Q is the initial state;
R ? Q?????1 . . .?
?
m?Q is a set of transitions
such as (q, w, p?1, . . . , p?m, q?), which is a tran-
sition from the state q to the state q?, with the
source symbol w and producing the substrings
(p?1, . . . , p?m);
P : R ? [0, 1] is the transition probability distri-
bution;
F : Q ? [0, 1] is the final state probability distri-
bution;
The probability distributions satisfy the stochastic
constraint:
?q ? Q (1)
F (q)+
?
w,p?1,...,p?m,q?
P (q, w, p?1, . . . , p?m, q?) = 1
2.2 Training the multilingual translation model
Both topology and parameters of an SFST can
be learned fully automatically from bilingual ex-
amples making use of underlying alignment mod-
els (Casacuberta and Vidal, 2004). Furthermore,
a multi-target SFST can be inferred from a multi-
lingual set of samples (Gonza?lez and Casacuberta,
2006). Even though in realistic situations multilin-
gual corpora are too scarce, recent works (Popovic?
et al, 2005) show that bilingual corpora covering the
same domain are sufficient to obtain generalized cor-
pora based on which one can subsequently create the
required collections of aligned tuples.
The inference algorithm, GIAMTI (grammatical
inference and alignments for multi-target transducer
inference), requires a multilingual corpus, that is, a
finite set of multilingual samples (s, t1, . . . , tm) ?
?????1?? ? ???
?
m, where ti denotes the translation
of the source sentence s into the i-th target language;
? denotes the source language vocabulary, and ?i
the i-th target language vocabulary; the algorithm
can be outlined as follows:
1. Each multilingual sample is transformed into
a single string from an extended vocabulary
(? ? ? ? ??1 ? ? ? ? ? ?
?
m) using a labeling
function (Lm). This transformation searches an
adequate monotonic segmentation for each of
the m source-target language pairs on the basis
of bilingual alignments such as those given by
GIZA++ (Och, 2000). A monotonic segmen-
tation copes with monotonic alignments, that
is, j < k ? aj < ak following the notation
of (Brown et al, 1993). Each source token,
which can be either a word or a phrase (Pe?rez
et al, 2007), is then joined with a target phrase
of each language as the corresponding segmen-
tation suggests. Each extended symbol consists
of a token from the source language plus zero
57
Alignment #0
0:tenperatura
1:minimoa
2:jeitsiko
3:da
0:
te
mp
er
at
ur
as
1:
mi
ni
ma
s
2:
en
3:
de
sc
en
so
(a) Spanish-Basque
Alignment #0
0:low
1:temperatures
2:falling
0:
te
mp
er
at
ur
as
1:
mi
ni
ma
s
2:
en
3:
de
sc
en
so
(b) Spanish-English
0 1temperaturas | temperatura | NIL 2maximas | maximoak | high temperaturesminimas | minimoak | low temperatures 3en | NIL | NIL 5descenso | jaitsiko da | fallingascenso | igoko da | rising
(c) Multi-target SFST from Spanish into English and Basque.
Figure 1: Example of a trilingual alignment over a trilingual sentence extracted from the task under consid-
eration;the related multi-target SFST (with Spanish as input, and English and Basque as output).
or more words from each target language in
their turn.
2. Once the set of multilingual samples has been
converted into a set of single extended strings
(z ? ??), a stochastic regular grammar can be
inferred. Specifically, in this work we deal with
k-testable in the string-sense grammars (Garc??a
and Vidal, 1990), which are considered to be
a syntactic approach of the n-gram models. In
addition, they allow the integration of several
order models in a single smoothed automa-
ton (Torres and Varona, 2001).
3. The extended symbols associated with the
transitions of the automaton are transformed
into one input token and m output phrases
(w/p?1| . . . |p?m) by the inverse labeling function
(L?m), leading to the required transducer.
Example An illustration of the inference of the
multi-target SFST can be shown over a couple of
simple trilingual sentences from the corpus (where
?B? stands for Basque, ?S? for Spanish and ?E? for
English):
1-B tenperatura maximoa jaitsiko da
1-S temperaturas ma?ximas en descenso
1-E high temperatures falling
2-B tenperatura minimoa igoko da
2-S temperaturas m??nimas en ascenso
2-E low temperatures rising
From the alignments, depicted in Figures 1(a)
and 1(b), an input-language-synchronized
monotonous segmentation can be built (bear in
mind that we are considering Spanish as the input
language). The corresponding extended strings with
the following constituents for the first and second
samples respectively are the following ones:
1 temperaturas|tenperatura|?
m??nimas|minimoa|low temperatures
en|?|?
descenso|jaitsiko da|falling
58
2 temperaturas|tenperatura|?
ma?ximas|maximoa|high temperatures
en|?|?
ascenso|igoko da|rising
Finally, from this representation of the data, the
multi-target SFST can be built as shown in Fig-
ure 1(c).
2.3 Decoding
Given an input string s (a sentence in the source lan-
guage), the decoding module has to search the opti-
mal m output strings tm ? ??1 ? ? ? ? ??
?
m (a sen-
tence in each of the target language) according to the
underlying translation model (T ):
t?m = arg max
tm???1??????
?
m
PT (s, t
m) (2)
Solving equation (2) is a hard computational prob-
lem, however, it can be efficiently computed under
the so called maximum approach as follows:
PT (s, t
m) ? max
?(s,tm)
PT (?(s, t
m)) (3)
where ?(s, tm) is a translation form, that is, a se-
quence of transitions in the multi-target SFST com-
patible with both the input and the m output strings.
?(s, tm) : (q0, w1, p?m1 , q1) ? ? ? (qJ?1, wJ , p?
m
J , qJ)
The input string (s) is a sequence of J input sym-
bols, s = wJ1 , and each of the m output strings
consists of J phrases in its corresponding language
tm = (t1, ? ? ? , tm) = (p?1)J1 , ? ? ? , (p?m)
J
1 . Thus, the
probability supplied by the multi-target SFST to the
translation form is given by:
PT (?(s, t
m)) = F (qJ)
J?
j=1
P (qj?1, wj , p?
m
j , qj)
(4)
In this context, the Viterbi algorithm can be used
to obtain the optimal sequence of states through the
multi-target SFST for a given input string. As a
result, the established m translations are built con-
catenating the (J) output phrases for each language
through the optimal path.
3 An embedded architecture for
speech-input multi-target translation
3.1 Statistical framework
Given the acoustic representation (x) of a speech
signal, the goal of multi-target speech translation
is to find the most likely m target strings (tm);
that is, one string (ti) per target language involved
(i ? {1, . . . ,m}). This approach is summarized
in eq. (5), where the hidden variable s can be in-
terpreted as the transcription of the speech signal:
t?m = arg max
tm
P (tm|x) = arg max
tm
?
s
P (tm, s|x)
(5)
Making use of Bayes? rule, the former expression
turns into:
t?m = arg max
tm
?
s
P (tm, s)P (x|tm, s) (6)
Empirically, there is no loss of generality if we as-
sume that the acoustic signal representation depends
only on the source string, i.e. P (x|tm, s) is inde-
pendent of tm. In this sense, eq. (6) can be rewritten
as:
t?m = arg max
tm
?
s
P (tm, s)P (x|s) (7)
Equation (7) combines a standard acoustic model,
P (x|s), and a multi-target translation model,
P (tm, s), both of whom can be integrated on the fly
during the searching routine as shown in Figure 2.
That is, each acoustic sub-network is only expanded
at decoding time when it is required.
The outer sum is computationally very expensive
to search for the optimal tuple of target strings tm
in an effective way. Thus we make use of the so
called Viterbi approximation, which finds the best
path over the whole transducer.
3.2 Practical issues
The underlying recognizer used in this work is our
own continuous-speech recognition system, which
implements stochastic finite-state models at all lev-
els: acoustic-phonetic, lexical and syntactic, and
which allows to infer them based on samples.
The signal analysis was carried out in a stan-
dard way, based on the classical Mel-cepstrum
parametrization. Each phone-like unit was modeled
59
1 /e/ | NIL | NIL 2/n/ | NIL | NIL
Figure 2: Integration on the fly of acoustic models in one edge of the SFST shown in Figure 1(c)
by a typical left to right hidden Markov model. A
phonetically-balanced Spanish database, called Al-
bayzin (Moreno et al, 1993), was used to train these
models.
The lexical model consisted of the extended to-
kens of the multi-target SFST instead of running
words. The acoustic transcription for each extended
token was automatically obtained on the basis of the
input projection of each unit, that is, the Spanish vo-
cabulary in this case.
Instead of the usual language model, we make use
of the multi-target SFST itself, which had the syn-
tactic structure provided by a k-testable in the strict
sense model, with k=3, and Witten-Bell smoothing.
Note that the SFST implicitly involves both input
and output language models.
4 Experimental results
4.1 Task and corpus
The described general methodology has been put
into practice in a highly practical application that
aims to translate on-line TV weather forecasts into
several languages, taking the speech of the presen-
ter as the input and producing as output text-strings,
or sub-titles, in several languages. For this purpose,
we used the corpus METEUS which consists of a
set of trilingual sentences, in English, Spanish and
Basque, as extracted from weather forecast reports
that had been published on the Internet. Let us no-
tice that it is a real trilingual corpus, which they are
usually quite scarce.
Basque is a pre-Indoeuropean language of still
unknown origin. It is a minority language, spo-
ken in a small area of Europe and also within some
small American communities (such as that in Reno,
Nevada). In the Basque Country (located in the
north of Spain) it has an official status along with
Spanish. However, despite having coexisted for cen-
turies in the same area, they differ greatly both in
syntax and in semantics. Hence, efforts are being
devoted nowadays to machine translation tools in-
volving these two languages (Alegria et al, 2004),
although they are still scarce. With regard to the or-
der of the phrases within a sentence, the most com-
mon one in Basque is Subject plus Objects plus Verb
(even though some alternative structures are also ac-
cepted), whereas in Spanish and English other con-
structions such as Subject plus Verb plus Objects are
more frequent (see Figures 1(a) and 1(b)). Another
difference between Basque and Spanish or English
is that Basque is an extremely inflected language.
In this experiment we intend to translate Span-
ish speech simultaneously into both Basque and En-
glish. Just by having a look at the main features of
the corpus in Table 1, we can realize that there are
substantial differences among these three languages,
in terms both of the size of the vocabulary and of the
amount of running words. These figures reveal the
agglutinant nature of the Basque language in com-
parison with English or Spanish.
Spanish Basque English
T
ra
in
in
g Total sentences 14,615
Different sentences 7,225 7,523 6,634
Words 191,156 187,462 195,627
Vocabulary 702 1,147 498
Average Length 13.0 12.8 13.3
Te
st
Sentences 500
Words 8,706 8,274 9,150
Average Length 17.4 16.5 18.3
Perplexity (3grams) 4.8 6.7 5.8
Table 1: Main features of the METEUS corpus.
With regard to the speech test, the input consisted
of the speech signal recorded by 36 speakers, each
one reading out 50 sentences from the test-set in Ta-
ble 1. That is, each sentence was read out by at least
three speakers. The input speech resulted in approx-
imately 3.50 hours of audio signal. Needless to say,
the application that we envisage has to be speaker-
60
independent if it is to be realistic.
4.2 System evaluation
The performance obtained by the acoustic integra-
tion has been experimentally tested for both multi-
target and mono-target devices. As a matter of com-
parison, text-input translation results are also re-
ported.
The multi-target SFST was learned from the train-
ing set described in Table 1 using the previously de-
scribed GIAMTI algorithm. The 500 test sentences
were then translated by the multi-target SFST. The
translation provided by the system in each language
was compared to the corresponding reference sen-
tence. Additionally, two mono-target SFSTs were
inferred with their outputs for the aforementioned
test to be taken as baseline. The evaluation includes
both computational cost and performance of the sys-
tem.
4.2.1 Computational cost
The expected searching time and the amount of
memory that needs to be allocated for a given model
are two key parameters to bear in mind in speech-
input machine translation applications. These val-
ues can be objectively measured in terms of the size
and on the average branching factor of the model
displayed in Table 2.
multi-target mono-targetS2B S2E
Nodes 52,074 35,034 20,148
Edges 163,146 115,526 69,690
Branching factor 3.30 3.13 3.46
Table 2: Features of multi-target model and the two
decoupled mono-target models (one for Spanish to
Basque translation, referred to as S2B, and the sec-
ond for Spanish to English, S2E).
Adding the edges up for the two mono-target SF-
STs that take part in the decoupled architecture (see
Table 2), we conclude that the decoupled model
needs a total of 185, 216 edges to be allocated in
memory, which represents an increment of 13%
in memory-space with respect to the multi-target
model.
On the other hand, the multi-target approach of-
fers a slightly smaller branching factor than each
mono-target approach. As a result, fewer paths have
to be explored with the multi-target approach than
with the decoupled one, which suggests that search-
ing for a translation might be faster. As a matter of
fact, experimental results in Table 3 show that the
mono-target architecture works 11% more slowly
than the multi-target one for speech-input machine
translation and decoding, and 30% for text to text
translation.
Time (s)
multi-target mono-targetS2B+S2E
Text-input 0.36 0.47
Speech-input 16.9 18.9
Table 3: Average time needed to translate each input
sentence into two languages.
Summarizing, in terms of computational cost
(space and time), a multi-target SFST performs bet-
ter than the mono-target decoupled system.
4.2.2 Performance
So far, the capability of the systems has been as-
sessed in terms of time and spatial costs. However,
the quality of the translations they provide is, doubt-
less, the most relevant evaluation criterion. In or-
der to determine the performance of the system in
a quantitative manner, the following evaluation pa-
rameters were computed for each scenario: bilingual
evaluation under study (BLEU), position indepen-
dent error rate (PER) and word error rate (WER).
Both text and speech-input translation results pro-
vided by the multi-target and the mono-target mod-
els respectively are shown in Table 4.
As can be derived from the translation results,
for text-input translation the classical approach per-
forms slightly better than the multi-target one, but
for speech-input translation from Spanish into En-
glish is the other way around. In any case, the dif-
ferences in performance are marginal.
Comparing the text-input with the speech-input
results we realize that, as could be expected, the pro-
cess of speech signal decoding is itself introducing
some errors. In an attempt to measure these errors,
the text transcription of the recognized input signal
was extracted and compared to the input reference
in terms of WER as shown in the last row of the Ta-
ble 4. Note that even though the input sentences are
the same the three results differ due to the fact that
61
we are making use of different SFST models that de-
code and translate at the same time.
multi-target mono-target
S2B S2E S2B S2E
Te
xt
BLEU 42.7 66.7 43.4 67.8
PER 39.9 19.9 38.2 19.0
WER 48.0 27.5 46.2 26.6
Sp
ee
ch
BLEU 39.5 59.0 39.2 61.1
PER 42.2 25.3 41.5 23.6
WER 51.5 33.9 50.5 31.9
recognition WER 10.7 9.3 9.1
Table 4: Text-input and speech-input translation re-
sults for Spanish into Basque (S2B) and Spanish into
English (S2E) using a multi-target SFST (columns
on the left) or two mono-target SFSTs (columns on
the right). The last row shows Spanish speech de-
coding results using each of the three devices.
In these series of experiments the same task has
been compared with two extremely different lan-
guage pairs under the same conditions. There is a
noticeable difference in terms of quality between the
English and the Basque translations. The underlying
reason might be due to the fact that SFST models
do not capture properly the rich morphology of the
Basque as they have to face long-distance reordering
issues. These differences in the performance of the
system when translating into English or into Basque
have been previously detected in other works (Or-
tiz et al, 2003). In our case, a manual review of the
models and the obtained translations encourage us to
make use of reordering models in future work, since
they have proved to report good results in a similar
framework (Kanthak et al, 2005).
5 Concluding remarks and further work
The main contribution of this paper is the proposal
of a fully embedded architecture for multiple speech
translation. Thus, acoustic models are integrated on
the fly into a multi-target translation model. The
most significant feature of this approach is its abil-
ity to carry out both the recognition and the transla-
tion into multiple languages integrated in a unique
model. Due to the finite-state nature of this model,
the speech translation engine is based on a Viterbi-
like algorithm.
In contrast to the mono-target systems, multi-
target SFSTs enable the translation from one source
language simultaneously into several target lan-
guages with lower computational costs (in terms
of space and time) and comparable qualitative re-
sults. Moreover, the integration of several languages
and acoustic models is straightforward on means of
finite-state devices.
Nevertheless, the integrated architecture needs
more parameters to be estimated. In fact, as the
amount of targets increase the data sparseness might
become a difficult problem to cope with. In future
work we intend to make a deeper study on the per-
formance of the multi-target system with regard to
the amount of parameters to be estimated. In ad-
dition, as the first step of the learning algorithm is
decisive, we are planning to make use of reordering
models in an attempt to face up to with long dis-
tance reordering and in order to homogenize all the
languages involved.
Acknowledgments
This work has been partially supported by the Uni-
versity of the Basque Country and by Spanish CI-
CYT under grants 9/UPV 00224.310-15900/2004,
TIC2003-08681-C02-02, and CICYT es TIN2005-
08660-C04-03 respectively.
References
In?aki Alegria, Olatz Ansa, Xabier Artola, Nerea Ezeiza,
Koldo Gojenola, and Ruben Urizar. 2004. Repre-
sentation and treatment of multiword expressions in
basque. In Takaaki Tanaka, Aline Villavicencio, Fran-
cis Bond, and Anna Korhonen, editors, Second ACL
Workshop on Multiword Expressions: Integrating Pro-
cessing, pages 48?55, Barcelona, Spain, July. Associ-
ation for Computational Linguistics.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and R. L. Mercer. 1993. The mathematics of
statistical machine translation: Parameter estimation.
Computational Linguistics, 19(2):263?311.
Francisco Casacuberta and Enrique Vidal. 2004. Ma-
chine translation with inferred stochastic finite-state
transducers. Computational Linguistics, 30(2):205?
225.
P. Garc??a and E. Vidal. 1990. Inference of k-testable
languages in the strict sense and application to syntac-
tic pattern recognition. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 12(9):920?925.
62
M.T. Gonza?lez and F. Casacuberta. 2006. Multi-Target
Machine Translation using Finite-State Transducers.
In Proceedings of TC-Star Speech to Speech Transla-
tion Workshop, pages 105?110.
John Hutchins and Harold L. Somers. 1992. An In-
troduction to Machine Translation. Academic Press,
Cambridge, MA.
Stephan Kanthak, David Vilar, Evgeny Matusov, Richard
Zens, and Hermann Ney. 2005. Novel reordering ap-
proaches in phrase-based statistical machine transla-
tion. In Proceedings of the ACL Workshop on Building
and Using Parallel Texts, pages 167?174, Ann Arbor,
Michigan, June. Association for Computational Lin-
guistics.
K. Knight and Y. Al-Onaizan. 1998. Translation with
finite-state devices. In 4th AMTA (Association for Ma-
chine Translation in the Americas).
Mehryar Mohri, Fernando Pereira, and Michael Ri-
ley. 2002. Weighted finite-state transducers in
speech recognition. Computer, Speech and Language,
16(1):69?88, January.
A. Moreno, D. Poch, A. Bonafonte, E. Lleida, J. Llisterri,
J. B. Mario, and C. Nadeu. 1993. Albayzin speech
database: Design of the phonetic corpus. In Proc. of
the European Conference on Speech Communications
and Technology (EUROSPEECH), Berl??n, Germany.
Franz J. Och. 2000. GIZA++: Train-
ing of statistical translation models.
http://www.fjoch.com/GIZA++.html.
Daniel Ortiz, Ismael Garc??a-Varea, Francisco Casacu-
berta, Antonio Lagarda, and Jorge Gonza?lez. 2003.
On the use of statistical machine translation techniques
within a memory-based translation system (AME-
TRA). In Proc. of Machine Translation Summit IX,
pages 115?120, New Orleans, USA, September.
Fernando C.N. Pereira and Michael D. Riley. 1997.
Speech Recognition by Composition of Weighted Fi-
nite Automata. In Emmanuel Roche and Yves Sch-
abes, editors, Finite-State Language Processing, Lan-
guage, Speech and Communication series, pages 431?
453. The MIT Press, Cambridge, Massachusetts.
Alicia Pe?rez, M. Ine?s Torres, and Francisco Casacuberta.
2007. Speech translation with phrase based stochas-
tic finite-state transducers. In Proceedings of the 32nd
International Conference on Acoustics, Speech, and
Signal Processing (ICASSP 2007), Honolulu, Hawaii
USA, April 15-20. IEEE.
Maja Popovic?, David Vilar, Hermann Ney, Slobodan
Jovic?ic?, and Zoran S?aric?. 2005. Augmenting a small
parallel text with morpho-syntactic language. In Pro-
ceedings of the ACL Workshop on Building and Us-
ing Parallel Texts, pages 41?48, Ann Arbor, Michigan,
June. Association for Computational Linguistics.
M. Ine?s Torres and Amparo Varona. 2001. k-tss lan-
guage models in speech recognition systems. Com-
puter Speech and Language, 15(2):127?149.
Enrique Vidal. 1997. Finite-state speech-to-speech
translation. In Proc. IEEE International Conference
on Acoustics, Speech, and Signal Processing, vol-
ume 1, pages 111?114, Munich, Germany, April.
63
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 361?365,
Dublin, Ireland, August 23-24, 2014.
IxaMed: Applying Freeling and a Perceptron Sequential Tagger at the
Shared Task on Analyzing Clinical Texts
Koldo Gojenola, Maite Oronoz, Alicia P
?
erez, Arantza Casillas
IXA Taldea (UPV-EHU)
maite.oronoz@ehu.es
http://ixa.si.ehu.es
Abstract
This paper presents the results of the Ix-
aMed team at the SemEval-2014 Shared
Task 7 on Analyzing Clinical Texts.
We have developed three different sys-
tems based on: a) exact match, b) a
general-purpose morphosyntactic analyzer
enriched with the SNOMED CT termi-
nology content, and c) a perceptron se-
quential tagger based on a Global Linear
Model. The three individual systems re-
sult in similar f-score while they vary in
their precision and recall. We have also
tried direct combinations of the individual
systems, obtaining considerable improve-
ments in performance.
1 Introduction
This paper presents the results of the IxaMed team.
The task is focused on the identification (Task A)
and normalization (Task B) of diseases and disor-
ders in clinical reports.
We have developed three different systems
based on: a) exact match, b) a general-
purpose morphosyntactic analyzer enriched with
the SNOMED CT terminology content, and c) a
perceptron sequential tagger based on a Global
Linear Model. The first system can be seen as
a baseline that can be compared with other ap-
proaches, while the other two represent two alter-
native approaches based on knowledge organized
in dictionaries/ontologies and machine learning,
respectively. We also tried direct combinations of
the individual systems, obtaining considerable im-
provements in performance.
These approaches are representative of different
solutions that have been proposed in the literature
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organizers. License details:
http://creativecommons.org/licenses/by/4.0/
(Pradhan et al., 2013), which can be broadly clas-
sified in the following types:
? Knowledge-based. This approach makes use
of large-scale dictionaries and ontologies,
that are sometimes integrated in general tools
adapted to the clinical domain, as MetaMap
(Aronson and Lang, 2010) and cTAKES (Xia
et al., 2013).
? Rule-based. For example, in (Wang and
Akella, 2013) the authors show the use
of a rule-based approach on the output of
MetaMap.
? Statistical techniques. These systems take a
training set as input and apply different vari-
ants of machine learning, such as sequen-
tial taggers based on hidden Markov mod-
els (HMMs) or conditional random fields
(CRFs) (Zuccon et al., 2013; Bodnari et al.,
2013; Gung, 2013; Hervas et al., 2013; Lea-
man et al., 2013).
? Combinations. These approaches try to take
the advantages of different system types, us-
ing methods such as voting or metaclassi-
fiers (Liu et al., 2013).
In the rest of the paper, we will first introduce
the different systems that we have developed in
section 2, presenting the main results in section 3,
and ending with the main conclusions.
2 System Description
The task of detecting diseases and their corre-
sponding concept unique identifiers (CUI) has
been faced using three methods that are described
in the following subsections.
2.1 Exact Match
The system based on Exact Match (EM) simply
obtained a list of terms and their corresponding
361
CUI identifier from the training set and marked
any appearance of those terms in the evaluation
set. This simple method was improved with some
additional extensions:
? Improving precision. In order to reduce the
number of false positives (FP), we applied
first the EM system to the training set it-
self. This process helped to measure FPs,
for example, blood gave 184 FPs and 2 true
positives (TPs). For the sake of not hurting
the recall, we allowed the system to detect
only those terms where TP > FP , that is,
?blood? would not be classified as disorder.
? Treatment of discontinuous terms. For
these terms, our system performed a soft-
matching comparison allowing a limited vari-
ation for the text comprised between the
term elements (for example ?right atrium is
mildly/moderately dilated?). These patterns
were tuned manually.
2.2 Adapting Freeling to the Medical Domain
Freeling is an open-source multilingual language
processing library providing a wide range of ana-
lyzers for several languages (Padr?o et al., 2010),
Spanish and English among others. We had al-
ready adapted Freeling to the medical domain in
Spanish (Oronoz et al., 2013), so we used our pre-
vious experience to adapt the English version to
the same domain. For the sake of clarity, we will
refer to this system as FreeMed henceforth.
The linguistic resources (lexica, grammars,. . . )
in Freeling can be modified, so we took advantage
of this flexibility extending two standard Freel-
ing dictionaries: a basic dictionary of terms con-
sisting of a unique word, and a multiword-term
dictionary. Both of them were enriched with a
dictionary of medical abbreviations
1
and with the
Systematized Nomenclature of Medicine Clinical
Terms (SNOMED CT) version dated 31st of July
of 2013. In addition to the changes in the lexica,
we added regular expressions in the tokenizer to
recognize medical terms as ?Alzheimer?s disease?
as a unique term.
In our approach, the system distinguishes be-
tween morphology and syntax on one side and
semantics on the other side. First, on the mor-
phosyntactic processing, our system only catego-
rizes word-forms using their basic part-of-speech
1
http://www.jdmd.com/abbreviations-glossary.asp
(POS) categories. Next, the semantic distinctions
are applied (the identification of the term as sub-
stance, disorder, procedure,. . . ). Following this
approach, whenever the specific term on the new
domain (biomedicine in this case) was already in
Freeling?s standard dictionaries, the specific en-
tries will not be added to the lexicon. Instead,
medical meanings are added in a later semantic
tagging stage. For example: the widely used term
?fever?, as common noun, was not added to the
lexicon but its semantic class is given in a sec-
ond stage. Only very specific terms not appear-
ing in the lexica as, for instance, ?diskospondyli-
tis? were inserted. This solution helps to avoid
an explosion of ambiguity in the morphosyntactic
analysis and, besides, it enables a clear separation
between morphosyntax and semantics.
In figure 1 the results of both levels of anal-
ysis, morphosyntactic and semantic, are shown.
The linguistic and medical information of medical
texts is stored in the Kyoto Annotation Format or
KAF (Bosma et al., 2009) that is based in the eX-
tended Markup Language (XML). In this example
the term aneurysm is analyzed as NN (meaning
noun) and it is semantically categorized as mor-
phological abnormality and disorder.
SNOMED CT is part of the Metathesaurus,
one of the elements of the Unified Medical Lan-
guage System (UMLS). We used the Metathe-
saurus vocabulary database to extract the map-
ping between SNOMED CT?s concept identifiers
and their corresponding UMLS?s concept unique
identifier (CUI). All the medical terms appearing
in SNOMED CT and analyzed with FreeMed are
tagged with both identifiers. For instance, the term
aneurysm in figure 1 has the 85659009 SNOMED
CT identifier when the term is classified in the
morphological abnormality content hierarchy and
the 432119003 identifier as disorder. Both are
linked to the same concept identifier, C0002940,
in UMLS. This mapping has been used for Task
B, whenever the CUI is the same in all the analy-
sis of the same term.
All the terms from all the 19 content hierarchies
of SNOMED CT were tagged with semantic infor-
mation in the provided texts.
The training corpus was linguistically analyzed
and its format was changed from XML to the for-
mat specified at the shared task. After a manual
inspection of the results and the Gold Standard,
some selection of terms was performed:
362
<term tid=?t241? lemma=?aneurysm? pos=?NN?>
<extRefs>
<extRef resource=?SCT 20130731? reference=?85659009?
reftype=?morphologic abnormality? >
<extRef resource=?UMLS-2010AB? reference=?C0002940?/ >
</extRef>
<extRef resource=?SCT 20130731? reference=?432119003?
reftype=?disorder? >
<extRef resource=?UMLS-2010AB? reference=?C0002940?/>
</extRef>
</extRefs>
</term>
Figure 1: Analysis with augmented information.
? Selection and combination of semantic
classes. All the terms from the disor-
der semantic class (for example ?Hypothy-
roidism?) and from the finding class (for in-
stance ?headache?) are chosen, as well as
some tag combinations (see figure 1). After
analyzing the train corpus we decided to join
into a unique term a body structure immedi-
ately followed by a disorder/finding. In this
way, we identify terms as ?MCA aneurysm?
that are composed of the MCA abbreviation
(meaning ?middle cerebral artery?) and the
inmediately following ?aneurysm? disorder.
? Filtering. Not all the terms from the men-
tioned SNOMED CT hierarchies are identi-
fied as disorders in the Gold Standard. Some
terms are discarded following these criteria:
i) findings describing personal situations (e.g.
?alcoholic?), ii) findings describing current
situations (e.g. ?awake?), iii) findings with
words indicating a negation or normal situ-
ation (e.g. ?stable blood pressure?) and iv)
too general terms (e.g. ?problems?).
The medical terms indicating disorders that are
linked to more than one CUI identifier, were
tagged as CUI-less. That is, we did not perform
any CUI disambiguation.
In subsequent iterations and after analyzing our
misses, new terms and term variations (Hina et
al., 2013) are added to the lexica in Freeling with
the restriction that, at least, one synonym should
appear in SNOMED CT. Thus, equivalent forms
were created for all the terms indicating a cancer,
a tumor, a syndrome, or a specific disease. For in-
stance, variants for the term ?cancer of colon? and
with the same SNOMED CT concept identifier
(number 363406005) are created with the forms
?colon cancer?, ?cancer of the colon? and ?can-
cer in colon?. Some abbreviation variations found
in the Gold Standard are added in the lexica too,
following the same criteria.
2.3 Perceptron Sequential Tagger
This system uses a Global Linear Model (GLM),
a sequential tagger using the perceptron algorithm
(Collins, 2002), that relies on Viterbi decoding of
training examples combined with simple additive
updates. The algorithm is competitive to other op-
tions such as maximum-entropy taggers or CRFs.
The original textual files are firstly processed by
FreeMed, and then the tagger uses all the available
information to assign tags to the text. Each token
contains information about the word form, lemma,
part of speech, and SNOMED CT category.
Our GLM system only deals with Task A, and
it will not tackle the problem of concept normal-
ization, due to time constraints. In this respect, for
Task B the GLM system will simply return the first
SNOMED CT category given by FreeMed. This
does not mean that GLM and FreeMed will give
the same result for Task B, as the GLM system
first categorizes each element as a disease, and it
gives a CUI only when that element is identified.
2.4 Combinations
The previous subsections presented three differ-
ent approaches to the problem that obtain com-
parable scores (see table 1). In the area of auto-
matic tagging, there are several works that com-
bine disparate systems, usually getting good re-
sults. For this reason, we tried the simplest ap-
proach of merging the outputs of the three individ-
ual systems into a single file.
3 Results
Table 1 presents the results of the individual and
combined systems on the development set. Look-
ing at the individual systems on Task A, we can see
that all of them obtain a similar f-score, although
there are important differences in terms of preci-
sion and recall. Contrary to our initial intuition,
the FreeMed system, based on dictionaries and on-
tologies, gives the best precision and the lowest re-
call. In principle, having SNOMED CT as a base,
we could expect that the coverage would be more
complete (attaining the highest recall). However,
the results show that there is a gap between the
writing of the standard SNOMED CT terms and
the terms written by doctors in their notes. On the
other hand, the sequential tagger gives the best re-
363
Task A Task B
Strict Relaxed Strict Relaxed
System Precision Recall F-Score Precision Recall F-Score Accuracy
INDIVIDUAL SYSTEMS
Exact Match (EM) 0.804 0.505 0.620 0.958 0.604 0.740 0.479 0.948
FreeMed 0.822 0.501 0.622 0.947 0.578 0.718 0.240 0.479
GLM 0.715 0.570 0.634 0.908 0.735 0.813 0.298 0.522
COMBINATIONS
FreeMed + EM 0.766 0.652 0.704 0.936 0.754 0.835 0.556 0.855
FreeMed + GLM 0.689 0.668 0.678 0.903 0.790 0.843 0.345 0.518
EM + GLM 0.680 0.679 0.679 0.907 0.819 0.861 0.398 0.598
FreeMed + EM + GLM 0.659 0.724 0.690 0.899 0.845 0.871 0.421 0.584
Table 1: Results of the different systems on the development set.
Task A Task B
Strict Relaxed Strict Relaxed
System Precision Recall F-Score Precision Recall F-Score Accuracy
FreeMed + EM 0.729 0.701 0.715 0.885 0.808 0.845 0.604 0.862
FreeMed + EM + GLM 0.681 0.786 0.730 0.872 0.890 0.881 0.439 0.558
Best system 0.843 0.786 0.813 0.936 0.866 0.900 0.741 0.873
Table 2: Results on the test set.
call. Since the tagger uses both contextual words
and prefixes and suffixes as features for learning,
this method has proven helpful for the recognition
of terms that do not appear in the training data (see
the difference with the EM approach).
Looking at the different combinations in table 1,
we see that two approaches work best, either com-
bining FreeMed and EM, or combining the three
individual systems. The inclusion of GLM results
in the best coverage, but at the expense of preci-
sion. On the other hand, combining FreeMed and
EM gives a better precision but lower coverage.
As pointed out by Collins (2002), the results of
the perceptron tagger are competitive with respect
to other statistical approaches such as CRFs (Zuc-
con et al., 2013; Bodnari et al., 2013; Gung, 2013;
Hervas et al., 2013; Leaman et al., 2013).
Regarding Task B, we can see that the EM sys-
tem is by far the most accurate, while FreeMed
is well below its a priori potential. The reason of
this low result is mainly due to the high ambiguity
found on the output of the SNOMED CT tagger, as
many terms are associated with more than one CUI
and, consequently, are left untagged. This problem
deserves future work on automatic semantic dis-
ambiguation. On the combinations, FreeMed and
EM together give the best result. However, as we
told before, the GLM system was only trained for
Task A, so it is not surprising to see that its results
deteriorate the accuracy in Task B.
We chose these best two combinations for the
evaluation on the test set (using training and de-
velopment for experimentation or training), which
are presented in table 2. Here we can see that re-
sults on the development also hold on the test set.
Given the unsophisticated approach to combine
the systems, we can figure out more elaborated so-
lutions, such as majority or weighted voting, or
even more, the definition of a machine learning
classifier to select the best system for every pro-
posed term. These ideas are left for future work.
4 Conclusions
We have presented the IxaMed approach, com-
posed of three systems that are based on exact
match, linguistic and knowledge repositories, and
a statistical tagger, respectively. The results of in-
dividual systems are comparable, with differences
in precision and recall. We also tested a sim-
ple combination of the systems, which proved to
give significant improvements over each individ-
ual system. The results are competitive, although
still far from the winning system.
For future work, we plan to further improve the
individual systems. Besides, we hope that the ex-
perimentation with new combination approaches
will offer room for improvement.
Acknowledgements
This work was partially supported by the Euro-
pean Commission (325099 and SEP-210087649),
the Spanish Ministry of Science and Innovation
(TIN2012-38584-C06-02) and the Industry of the
Basque Government (IT344-10).
364
References
Alan R Aronson and Francois-Michel Lang. 2010. An
overview of MetaMap: historical perspective and re-
cent advances. Journal of the American Medical In-
formatics Association (JAMIA), 17:229?236.
Andreea Bodnari, Louise Deleger, Thomas Lavergne,
Aurelie Neveol, and Pierre Zweigenbaum. 2013.
A Supervised Named-Entity Extraction System for
Medical Text. In Online Working Notes of the CLEF
2013 Evaluation Labs and Workshop, September.
Wauter Bosma, Piek Vossen, Aitor Soroa, German
Rigau, Maurizio Tesconi, Andrea Marchetti, Mon-
ica Monachini, and Carlo Aliprandi. 2009. KAF: a
Generic Semantic Annotation Format. In Proceed-
ings of the 5th International Conference on Gener-
ative Approaches to the Lexicon GL, pages 17?19,
Septembre.
Michael Collins. 2002. Discriminative Training Meth-
ods for Hidden Markov Models: Theory and Ex-
periments with Perceptron Algorithms. In Proceed-
ings of the 2002 Conference on Empirical Methods
in Natural Language Processing, pages 1?8. Asso-
ciation for Computational Linguistics, July.
James Gung. 2013. Using Relations for Identification
and Normalization of Disorders: Team CLEAR in
the ShARe/CLEF 2013 eHealth Evaluation Lab. In
Online Working Notes of the CLEF 2013 Evaluation
Labs and Workshop, September.
Lucia Hervas, Victor Martinez, Irene Sanchez, and Al-
berto Diaz. 2013. UCM at CLEF eHealth 2013
Shared Task1. In Online Working Notes of the CLEF
2013 Evaluation Labs and Workshop, September.
Saman Hina, Eric Atwell, and Owen Johnson. 2013.
SnoMedTagger: A semantic tagger for medical nar-
ratives. In Conference on Intelligent Text Processing
and Computational Linguistics (CICLING).
Robert Leaman, Ritu Khare, and Zhiyong Lu. 2013.
NCBI at 2013 ShARe/CLEF eHealth Shared Task:
Disorder Normalization in Clinical Notes with
Dnorm. In Online Working Notes of the CLEF 2013
Evaluation Labs and Workshop, September.
Hongfang Liu, Kavishwar Wagholikar, Siddhartha Jon-
nalagadda, and Sunghwan Sohn. 2013. Integrated
cTAKES for Concept Mention Detection and Nor-
malization. In Online Working Notes of the CLEF
2013 Evaluation Labs and Workshop, September.
Maite Oronoz, Arantza Casillas, Koldo Gojenola, and
Alicia Perez. 2013. Automatic Annotation of
Medical Records in Spanish with Disease, Drug
and Substance Names. In Lecture Notes in Com-
puter Science, 8259. Progress in Pattern Recogni-
tion, ImageAnalysis, ComputerVision, and Applica-
tions 18th Iberoamerican Congress, CIARP 2013,
Havana, Cuba, November 20-23.
Lluis Padr?o, Samuel Reese, Eneko Agirre, and Aitor
Soroa. 2010. Semantic Services in Freeling 2.1:
WordNet and UKB. In Global Wordnet Conference,
Mumbai, India.
Sameer Pradhan, Noemie Elhadad, Brett R. South,
David Martinez, Lee Christensen, Amy Vogel,
Hanna Suominen, Wendy W. Chapman, and Guer-
gana Savova. 2013. Task 1: ShARe/CLEF eHealth
Evaluation Lab 2013. In Online Working Notes
of the CLEF 2013 Evaluation Labs and Workshop,
September.
Chunye Wang and Ramakrishna Akella. 2013. UCSCs
System for CLEF eHealth 2013 Task 1. In Online
Working Notes of the CLEF 2013 Evaluation Labs
and Workshop, September.
Yunqing Xia, Xiaoshi Zhong, Peng Liu, Cheng Tan,
Sen Na, Qinan Hu, and Yaohai Huang. 2013. Com-
bining MetaMap and cTAKES in Disorder Recogni-
tion: THCIB at CLEF eHealth Lab 2013 Task 1. In
Online Working Notes of the CLEF 2013 Evaluation
Labs and Workshop, September.
Guido Zuccon, Alexander Holloway, Bevan Koop-
man, and Anthony Nguyen. 2013. Identify Disor-
ders in Health Records using Conditional Random
Fields and Metamap AEHRC at ShARe/CLEF 2013
eHealth Evaluation Lab Task 1. In Online Working
Notes of the CLEF 2013 Evaluation Labs and Work-
shop, September.
365
Proceedings of the 10th International Workshop on Finite State Methods and Natural Language Processing, pages 60?64,
Donostia?San Sebastia?n, July 23?25, 2012. c?2012 Association for Computational Linguistics
First approaches on Spanish medical record classification using
Diagnostic Term to class transduction
A. Casillas(1), A. D??az de Ilarraza(2), K. Gojenola(2), M. Oronoz(2), A. Pe?rez(2)
(1) Dep. Electricity and Electronics
(2) Dep. Computer Languages and Systems
University of the Basque Country (UPV/EHU)
arantza.casillas@ehu.es
Abstract
This paper presents an application of finite-
state transducers to the domain of medicine.
The objective is to assign disease codes to
each Diagnostic Term in the medical records
generated by the Basque Health Hospital Sys-
tem. As a starting point, a set of manually
coded medical records were collected in order
to code new medical records on the basis of
this set of positive samples. Since the texts
are written in natural language by doctors, the
same Diagnostic Term might show alternative
forms. Hence, trying to code a new medical
record by exact matching the samples in the
set is not always feasible due to sparsity of
data. In an attempt to increase the coverage
of the data, our work centered on applying a
set of finite-state transducers that helped the
matching process between the positive sam-
ples and a set of new entries. That is, these
transducers allowed not only exact matching
but also approximate matching. While there
are related works in languages such as En-
glish, this work presents the first results on au-
tomatic assignment of disease codes to medi-
cal records written in Spanish.
1 Introduction
During the last years an exponential increase in
the number of electronic documents in the medi-
cal domain has occurred. The automatic process-
ing of these documents allows to retrieve informa-
tion, helping the health professionals in their work.
There are different sort of valuable data that help to
exploit medical information. Our framework lays
on the classification of Medical Records (MRs) ac-
cording to a standard. In our context, the MRs pro-
duced in a hospital have to be classified with re-
spect to the World Health Organization?s 9th Revi-
sion of the International Classification of Diseases1
(ICD-9). ICD-9 is designed for the classification of
morbidity and mortality information and for the in-
dexing of hospital records by disease and procedure.
The already classified MRs are stored in a database
that serves for further classification purposes. Each
MR consists of two pieces of information:
Diagnostic Terms (DTs): one or more terms that
describe the diseases corresponding to the MR.
Body-text: a description of the patient?s details,
antecedents, symptoms, adverse effects, meth-
ods of administration of medicines etc.
Even though the DTs are within a limited domain,
their description is not subject to a standard. Doc-
tors express the DTs in natural language with their
own style and different degrees of precision. Usu-
ally, a given concept might be expressed by alterna-
tive DTs with variations due to modifiers, abbrevia-
tions, acronyms, dates, names, misspellings or style.
This is a typical problem that arises in natural lan-
guage processing due to the fact that doctors focus
on the patients and not so much on the writing of the
MR. On account of this, there is ample variability in
the presentation of the DTs. Consequently, it is not
a straightforward task to get the corresponding ICD-
codes. That is, the task is by far more complex than
a standard dictionary lookup.
1http://www.cdc.gov/nchs/icd/icd9.htm
60
The Basque Health Hospital System is concerned
with the automatization of this ICD-code assign-
ment task. So far, the hospital processes the daily
produced documents in the following sequence:
1. Automatic: exact match of the DTs in a set of
manually coded samples.
2. Semi-automatic: through semantic match,
ranking the DTs by means of machine-learning
techniques. This stage requires that experts se-
lect amongst the ranked choices.
3. Manual: the documents that were not matched
in the previous two stages are examined by pro-
fessional coders assigning the codes manually.
The goal of this paper is to bypass the variability
associated to natural language descriptions in an at-
tempt to maximize the proportion of automatically
assigned codes, as the Hospital System aims to ex-
pand the use of the automatic codification of MRs
to more hospitals. According to experts, even an in-
crease of 1% in exact match would represent a sig-
nificant improvement allowing to gain time and re-
sources.
Related work can be found in the literature. For
instance, Pestian et al (2007) reported on a shared
task involving the assignment of ICD-codes to radi-
ology reports written in English from a reduced set
of 45 codes. In general it implied the examination of
the full MR (including body-text). In our case, the
number of ICD-codes is above 1,000, although we
restrict ourselves to exact and approximate match
over the diagnoses.
Farkas and Szarvas (2008) used machine learning
for the automatic assignment of ICD-9 codes. Their
results showed that hand-crafted systems could be
reproduced by replacing several laborious steps in
their construction with machine learning models.
Tsuruoka et al (2008) presented a system that
tried to normalize different variants of the terms con-
tained in a medical dictionary, automatically getting
normalizing rules for genes, proteins, chemicals and
diseases in English.
The contribution of this work is: i) to collect
manually coded MRs in Spanish; ii) to approximate
transduction with finite-state (FS) models for auto-
matic MR coding and, iii) to assess the performance
of the proposed FS transduction approaches.
2 Approximate transduction
As it was previously mentioned, there are variations
regarding the DT descriptions due to style, miss-
spells, etc. Table 1 shows several pairs of DT and
ICD-codes within the collected samples that illus-
trate some of those variations.
DT ICD
1 Adenocarcinoma de prostata 185
2 Adenocarcinomas pro?stata. 185
3 Ca. prostata 185
4 CA?NCER DE PROSTATA 185
5 adenocarcinoma de pulmon estadio IV 1629
6 CA pulmo?n estadio 4 1629
7 ADENOCARCINOMA PANCREAS 1579
Table 1: Examples of DTs and their ICD-codes.
There are differences in the use of uppercase/lower
case; omissions of accents; use of both standard and
non-standard abbreviations (e.g. ca. for both ca?ncer
and adenocarcinoma); punctuation marks (inciden-
tal use of full-stop as commas, etc.); omission of
prepositions (see rows 1 and 2); equivalence be-
tween Roman and Arabic numerals (rows 5 and 6).
Due to these variations, our problem can be defined
as an approximate lookup in a dictionary.
2.1 Finite-state models
Foma toolkit was used to build the FS machines and
code the evaluation sets. Foma (Hulden, 2009) is
a freely available2 toolkit that allows to both build
and parse FS automata and transducers. Foma of-
fers a versatile layout that supports imports/exports
from/to other tools such as: Xerox XFST (Beesley
and Karttunen, 2003), AT&T (Mehryar Mohri
and Riley, 2003), OpenFST (Riley et al, 2009).
There are, as well, outstanding alternatives such as
HFST (Linde?n et al, 2010). Refer to (Yli-Jyra? et al,
2006) for a thorough inventory on FS resources.
The FS models in Figure 1 perform the conver-
sions necessary to carry out a soft match between
the dictionary entries and their variants.
? First, we define the transducer Accents that
takes into account the correspondences be-
tween standard letters and their versions using
accent text marks.
2http://code.google.com/p/foma
61
define Accents [a:a?|e:e?|i:??|o:o?|u:u?|...];
define Case [a:A|b:B|c:C|d:D|e:E|f:F|...];
define Spaces [..] (->) " " || [.#. | "."] , .#.;
define Punctuation ["."|"-"|" "]:["."|"-"|" "];
define Plurals [..] -> ([s|es]) || [.#. | "." | " "];
define PluralsI [s|es] (->) "" || [.#. | "." | ","| " "];
define Preps [..] (->) [de |del |con |por ] || " " ;
define Disease [enf|enf.|enfermedad]:[enf|enf.|enfermedad];
define AltCa [tumor|ca|ca.|carcinoma|adenocarcinoma|ca?ncer];
define TagNormCa AltCa:AltCa;
define AltIzq [izquierdo|izquierda|izq|izq.|izqda|izqda.|
izqdo|izqdo.|izda|izda.|izdo|izdo.];
define TagNormIzq AltIzq:AltIzq;
Figure 1: A simplified version in Foma source code of the regular expressions and transducers used to
bypass several sources of distortion within the DTs in order to parse variations of unseen input DTs.
? The expression Case matches uppercase and
lowercase versions of the DTs.
? There is a set of transducers (Spaces,
Punctuation, Plurals and PluralsI)
that deal with the addition or deletion of spaces
and separators (as full-stop, comma, and hy-
phen) between words or at the end of the DT.
? Prepositions. Many DTs can be differen-
tiated by the use or absence of prepositions, al-
though they correspond to the same ICD-code.
For that reason, we designed a transducer that
inserts or deletes the prepositions from a re-
duced set that were identified by inspection of
the training set. In this way, expressions as
?Adenocarcinoma prostata? and ?Adenocarci-
noma de prostata? can be mapped to each other.
? Tag Normalization of synonyms, vari-
ants and abbreviations. The examination of the
DTs in the training set revealed that there were
several terms used indistinctly, including syn-
onyms and different kinds of variants (mascu-
line and feminine) and abbreviations. For ex-
ample, the words adenocarcinoma, adenoca.,
carcinoma, ca, ca. and cancer serve to name
the same disease. There are also multiple vari-
ants of left/right, indicating the location of an
illness, that do not affect the assignment of the
ICD-code (e.g. izquierdo, izq., izda.).
Finally, all the FS transducers were composed
into a single machine that served to overcome all the
sources of distortion together.
3 Experimental results
To begin with, coded MRs produced in the hospi-
tal throughout 12 months were collected summing
up a total of 8,020 MRs as described in Table 2.
Note that there are ambiguities in our data-set since
there are 3,313 different DTs that have resulted in
3,407 (DT, ICD-code) different pairs (as shown in
Table 2). That is, the same DT was not always as-
signed the same ICD-code.
DT ICD-code
entries 8,020
different entries 3,407
different forms 3,313 1,011
Table 2: The data-set of (DT, ICD-code) pairs.
Next, the data-set was shuffled and divided into 3
disjoint sets for training, development and test pur-
poses as shown in Table 3.
train dev test
entries 6,020 1,000 1,000
different entries 2,825 734 728
Table 3: The data-set shuffled and divided into 3 sets
Using the set of mappings derived from the train-
ing set we performed the experiments on the devel-
opment set. After several rounds of tuning the sys-
tem, the resulting system was applied to the test set.
62
PERCENTAGE OF UNCLASSIFIED DTs
TRAIN EVAL-SET exact-match + case-ins. + punct. + plurals +preps. + tag-norm.
train dev 30.6 27.0 25.2 24.4 23.9 23.2
train test 29.8 26.7 25.1 24.8 24.3 23.2
train+dev test 27.7 24.5 23.0 22.9 22.5 21.4
Table 4: Performance of different FS machines in terms of the percentage of unclassified entries. All the
classified entries were correctly classified, yielding, as a result, a precision of 100%.
Given a DT, the goal is to find its corresponding
ICD-code despite the variations. Different FS ap-
proaches (described in Section 2.1) were proposed
to bypass particular sources of noise in the DT. Their
performance was assessed by means of the percent-
age of unclassified DTs, as summarized in Table 4.
Note that the lower the number of unclassified DTs
the better the performance. In each of the three rows
of Table 4 the results of different experimental se-
tups are shown: in the first two rows the training set
was used to build the models and either the devel-
opment or the test set was evaluated in their turn;
in the third row, both the training and the devel-
opment sets were used to build the model and the
test set was evaluated. The impact of adding pro-
gressively the FS machines built to tackle particular
sources of noise is shown by columns. Thus, the re-
sults of the last column represent the performance
of the transducer allowing exact-match search to-
gether with case-insensitive search, bypassing punc-
tuation marks, allowing plurals, bypassing preposi-
tions and allowing tag-normalization. The compo-
sition of each transducer outperforms the previous
result, yielding an improvement on the test of 6 ab-
solute points over the exact-match baseline, from
27.7% to 21.4%. As it can be derived from the
first column of Table 4 the test set contributed to the
training+development set with %27.7 of new DTs.
Overall, the FSMs progressively improved the re-
sults for the three series of experiments carried out
in more than 6%. As a result, less and less DTs are
left unclassified. In other words, the FS machines
tackling different sources of errors contribute to as-
sign ICD-codes to previously unassigned DTs.
A manual inspection over the results associated
to the evaluation of the development set (focus on
the first row of Table 4) showed that all the DTs
were correctly classified according to the training
data. Overall, the resulting transducer was unable
to classify 232 DTs out of 1,000 (see last column
in first row). Among the unclassified DTs, 10 out
of 232 were due to misspellings: e.g. cic atriz
(instead of cicatriz), desprendimineot (instead of
desprendimiento). In fact, spelling correction re-
ported improvements in related tasks (Patrick et al,
2010). The remaining DTs showed wider variations
in their forms, as unexpected degree of specificity
(e.g. named entities), spurious dates or numbers.
4 Conclusions
Medical records in Spanish were collected yielding
a data set of 8,020 DT and ICD-code pairs. While
there are a number of references dealing with En-
glish medical records, there are few for Spanish.
The goal of this work was to build a system that
given a DT it would find its corresponding ICD-
code as in a standard key-value dictionary. Yet, the
DTs are far from being standard since they contain
a number of variations. We proposed the use of sev-
eral FS models to bypass different variants and al-
low to provide ICD-codes even when the exact DT
was not found. Each source of variations was tack-
led with a specific transducer based on handwritten
rules. The composition of each machine improved
the performance of the system gradually, leading to
an improvement up to 6% in accuracy, from 27.7%
unclassified DTs with the exact-match baseline to
21.4% with the tag-normalization transducer.
Future work will focus on the unclassified DTs.
Together with FS models, other strategies shall be
explored. Machine-learning strategies in the field of
information retrieval might help to make the most of
the piece of information that was here discarded (i.e.
the body-text). All in all, regardless of the approach,
the command in this MR classification context is to
get an accuracy of 100%, possibly through the inter-
active inference framework (Toselli et al, 2011).
63
Acknowledgments
Authors would like to thank the Hospital Galdakao-
Usansolo for their contributions and support, in par-
ticular to Javier Yetano, responsible of the Clinical
Documentation Service.
This research was supported by the Department of
Industry of the Basque Government (IT344-10, S-
PE11UN114, GIC10/158 IT375-10), the University
of the Basque Country (GIU09/19) and the Span-
ish Ministry of Science and Innovation (MICINN,
TIN2010- 20218).
References
[Beesley and Karttunen2003] Kenneth R. Beesley and
Lauri Karttunen. 2003. Finite State Morphology.
CSLI Publications,.
[Farkas and Szarvas2008] Richa?rd Farkas and Gyo?rgy
Szarvas. 2008. Automatic construction of rule-based
ICD-9-CM coding systems. BMC Bioinformatics., 9
(Suppl 3): S10.
[Hulden2009] Mans Hulden. 2009. Foma: a Finite-State
Compiler and Library. In EACL (Demos), pages 29?
32. The Association for Computer Linguistics.
[Linde?n et al2010] Krister Linde?n, Miikka Silfverberg,
and Tommi Pirinen. 2010. HFST tools for morphol-
ogy ? an efficient open-source package for construc-
tion of morphological analyzers.
[Mehryar Mohri and Riley2003] Fernando C. N. Pereira
Mehryar Mohri and Michael D. Riley. 2003. AT&T
FSM LibraryTM ? Finite-State Machine Library.
www.research.att.com/sw/tools/fsm.
[Patrick et al2010] Jon Patrick, Mojtaba Sabbagh, Suvir
Jain, and Haifeng Zheng. 2010. Spelling correction
in clinical notes with emphasis on first suggestion ac-
curacy. In 2nd Workshop on Building and Evaluating
Resources for Biomedical Text Mining (BioTxtM2010)
LREC. ELRA.
[Pestian et al2007] John P. Pestian, Chris Brew, Pawel
Matykiewicz, D. J Hovermale, Neil Johnson, K. Bre-
tonnel Cohen, and Wlodzislaw Duch. 2007. A shared
task involving multi-label classification of clinical free
text. In Biological, translational, and clinical lan-
guage processing, pages 97?104, Prague, Czech Re-
public, June. Association for Computational Linguis-
tics.
[Riley et al2009] Michael Riley, Cyril Allauzen, and
Martin Jansche. 2009. OpenFST: An open-source,
weighted finite-state transducer library and its applica-
tions to speech and language. In Proceedings of Hu-
man Language Technologies: The 2009 Annual Con-
ference of the North American Chapter of the Associ-
ation for Computational Linguistics, Companion Vol-
ume: Tutorial Abstracts, pages 9?10, Boulder, Col-
orado, May. Association for Computational Linguis-
tics.
[Toselli et al2011] Alejandro H. Toselli, Enrique Vi-
dal, and Francisco Casacuberta. 2011. Multi-
modal Interactive Pattern Recognition and Applica-
tions. Springer.
[Tsuruoka et al2008] Yoshimasa Tsuruoka, John Mc-
Naught, and Sophia Ananiadou. 2008. Normalizing
biomedical terms by minimizing ambiguity and vari-
ability. BMC Bioinformatics, 9(Suppl 3):S2.
[Yli-Jyra? et al2006] A. Yli-Jyra?, K. Koskenniemi, and
K.. Linde?n. 2006. Common infrastructure for
finite-state based methods and linguistic descriptions.
In Proceedings of International Workshop Towards
a Research Infrastructure for Language Resources.,
Genoa, May.
64
Proceedings of the 10th International Workshop on Finite State Methods and Natural Language Processing, pages 99?107,
Donostia?San Sebastia?n, July 23?25, 2012. c?2012 Association for Computational Linguistics
Finite-state acoustic and translation model composition in statistical
speech translation: empirical assessment
Alicia Pe?rez(1), M. Ine?s Torres(2)
(1)Dep. Computer Languages and Systems
(2)Dep. Electricidad y Electro?nica
University of the Basque Country UPV/EHU
Bilbao (Spain)
(1)alicia.perez@ehu.es
(2)manes.torres@ehu.es
Francisco Casacuberta
Instituto Tecnolo?gico de Informa?tica
Universidad Polite?cnica de Valencia
Valencia (Spain)
fcn@iti.upv.es
Abstract
Speech translation can be tackled by
means of the so-called decoupled ap-
proach: a speech recognition system fol-
lowed by a text translation system. The
major drawback of this two-pass decod-
ing approach lies in the fact that the trans-
lation system has to cope with the er-
rors derived from the speech recognition
system. There is hardly any cooperation
between the acoustic and the translation
knowledge sources. There is a line of re-
search focusing on alternatives to imple-
ment speech translation efficiently: rang-
ing from semi-decoupled to tightly in-
tegrated approaches. The goal of inte-
gration is to make acoustic and transla-
tion models cooperate in the underlying
decision problem. That is, the transla-
tion is built by virtue of the joint ac-
tion of both models. As a side-advantage
of the integrated approaches, the transla-
tion is obtained in a single-pass decod-
ing strategy. The aim of this paper is
to assess the quality of the hypotheses
explored within different speech transla-
tion approaches. Evidence of the perfor-
mance is given through experimental re-
sults on a limited-domain task.
1 Introduction
Statistical speech translation (SST) was typ-
ically implemented as a pair of consecutive
steps in the so-called decoupled approach: with
an automatic speech recognition (ASR) system
placed before to a text-to-text translation sys-
tem. This approach involves two independent
decision processes: first, getting the most likely
string in the source language and next, get-
ting the expected translation into the target lan-
guage. Since the ASR system is not an ideal
device it might make mistakes. Hence, the text
translation system would have to manage with
the transcription errors. Being the translation
models (TMs) trained with positive samples of
well-formed source strings, they are very sensi-
tive to ill-formed strings in the source language.
Hence, it seems ambitious for TMs to aspire to
cope with both well and ill formed sentences in
the source language.
1.1 Related work
Regarding the coupling of acoustic and trans-
lation models, there are some contributions in
the literature that propose the use of semi-
decoupled approaches. On the one hand, in
(Zhang et al, 2004), SST is carried out by
99
an ASR placed before a TM with an addi-
tional stage that would re-score the obtained hy-
potheses within a log-linear framework gather-
ing features from both the ASR system (lexicon
and language model) and the TM (eg. distor-
tion, fertility) and also additional features (POS,
length etc.).
On the other hand, in (Quan et al, 2005), the
N-best hypotheses derived from an ASR sys-
tem were next translated by a TM, finally, a last
stage would re-score the hypotheses and make
a choice. Within the list of the N-best hypothe-
ses typically a number of them include some n-
grams that are identical, hence, the list results to
be an inefficient means of storing data. Alterna-
tively, in (Zhou et al, 2007) the search space
extracted from the ASR system, represented as
a word-graph (WG), was next explored by a TM
following a multilayer search algorithm.
Still, a further approach can be assumed
in order to make the graph-decoding com-
putationally cheaper, that is, confusion net-
works (Bertoldi et al, 2007). Confusion-
networks implement a linear approach of the
word-graphs, however, as a result, dummy hy-
potheses might be introduced and probabili-
ties mis-computed. Confusion networks traded
off between the accuracy and storage ability of
word-graphs for decoding time. Indeed, in (Ma-
tusov and Ney, 2011) an efficient means of do-
ing the decoding with confusion networks was
presented. Note that these approaches follow a
two-pass decoding strategy.
The aforementioned approaches imple-
mented phrase-based TMs within a log-linear
framework. In this context, in (Casacuberta
et al, 2008) a fully integrated approach was
examined. Under this approach, the translation
was carried out in a single-pass decoding,
involving a single decision process in which
acoustic and translations models cooperated.
This integration paradigm, was earlier proposed
in (Vidal, 1997), showing that a single-pass
decoding was enough to carry out SST.
Finally, in (Pe?rez et al, 2010) several SST de-
coding approaches including decoupled, N-best
lists and integrated were compared. Neverthe-
less, the paper focused on the potential scope of
the approaches, comparing the theoretical upper
threshold of their performance.
1.2 Contribution
All the models assessed in this work relay upon
exactly the same acoustic and translation mod-
els. It is the combination of them on which
we are focusing. In brief, the aim of this pa-
per is to compare different approaches to carry
out speech translation decoding. The compari-
son is carried out using exactly the same under-
lying acoustic and translation models in order
to allow to make a fair comparison of the abil-
ities inherent to the decoding strategy. Apart
from the decoupled and semi-decoupled strate-
gies we also focus on the fully-integrated ap-
proach. While the fully integrated approach al-
lows to provide the most-likely hypothesis, we
explored a variant: an integrated architecture
with a re-scoring LM that provided alternatives
derived from the integrated approach and used
re-scoring to make the final decision. Not only
an oracle-evaluation is provided as an upper-
threshold of the experiments but also an experi-
mental set-up to give empirical evidence.
The paper is arranged as follows: Section 2
introduces the formulation of statistical speech
translation (SST); Section 3 describes differ-
ent approaches to put into practice SST, plac-
ing emphasis on the assumptions behind each
of them. Section 4 is devoted to assess experi-
mentally the performance of each approach. Fi-
nally, in Section 5 the concussions drawn from
the experiments are summarized.
100
2 Statistical speech translation
The goal of speech translation, formulated un-
der the probabilistic framework, is to find the
most likely string in the target language (?t)
given the spoken utterance in the source lan-
guage. Speech signal in the source language
is characterized in terms of an array of acoustic
features in the source language, x. The decision
problem involved is formulated as follows:
t? = arg max
t
P (t|x) (1)
In this context, the text transcription in the
source language (denoted as s) is introduced as
a hidden variable and Bayes? rule applied:
t? = arg max
t
?
s
P (x|s, t)P (s, t) (2)
Assuming P (x|s, t) ? P (x|s), and using the
maximum term involved in the sum as an ap-
proach to the sum itself for the sake of compu-
tational affordability, we yield to:
t? ? arg max
t
max
s
P (x|s)P (s, t) (3)
As a result, the expected translation is built
relying upon both a translation model (P (s, t))
and an acoustic model in the source language
(P (x|s)). This approach requires the joint co-
operation of both models to implement the de-
cision problem since the maximum over s con-
cerns both of them.
2.1 Involved models
Being the goal of this paper to compare differ-
ent techniques to combine acoustic and trans-
lation models, it is important to keep constant
the underlying models while varying the strate-
gies to combine them. Before to delve into the
composition strategies and due to the fact that
some combination strategies are based on the
finite-state topology of the models, a summary
of the relevant features of the underlying mod-
els is given in this section.
2.1.1 Translation model
The translation model used in this work
to tackle all the approaches consists of a
stochastic finite-state transducer (SFST) en-
compassing phrases in the source and tar-
get languages together with a probability of
joint occurrence. The SFST (T ) is a tuple
T = ??,?, Q, q0, R, F, P ?, where:
? is a finite set of input symbols;
? is a finite set of output symbols;
Q is a finite set of states;
q0 ? Q is the initial state;
R ? Q ? ?+ ? ?? ? Q is a set of transi-
tions. (q, s?, t?, q?) ? R, represents a tran-
sition from the state q ? Q to the state
q? ? Q, with the source phrase s? ? ?+ and
producing the substring t? ? ??, where t?
might consist of zero or more target words
(|t?| ? 0);
F : Q? [0, 1] is a final state probability;
P : R? [0, 1] is a transition probability;
Subject to the stochastic constraint:
?q ? Q F (q) +
?
s?,t?,q?
P (q, s?, t?, q?) = 1 (4)
For further reading on formulation and prop-
erties of these machines turn to (Vidal et al,
2005).
The SFST can be understood as a statistical
bi-language implemented by means of finite-
state regular grammar (Casacuberta and Vidal,
2004) (in the same way as a stochastic finite-
state automaton can be used to model a sin-
gle language): A = ??, Q, q0, R, F, P ?, being
? ? ?+ ??? a finite-set of bilingual-phrases.
Likewise, bilingual n-gram models can be in-
ferred in practice (Marin?o et al, 2006).
101
2.1.2 Acoustic models
The acoustic model consists of a mapping of
text-transcriptions of lexical units in the source
language and their acoustic representation. That
comprises the composition of: 1) a lexical
model consisting of a mapping between the tex-
tual representation with their phone-like repre-
sentation in terms of a left-to-right sequence;
and 2) an inventory of phone-like units con-
sists of a typical three-state hidden Markov
model (Rabiner, 1989). Thus, acoustic model
lays on the composition of two finite-state mod-
els (depicted in Figure 1).
/T/ /j/ /e/ /l/ /o/
cielo
(a) Phonetic representation of a text lexical unit
/T/
/j/
/e/
(b) HMM phone-like units
Figure 1: Acoustic model requires composing
phone-like units within phonetic representation
of lexical units.
3 Decoding strategies
In the previous section the formulation of SST
was summarized. Let us now turn into prac-
tice and show the different strategies explored
to combine acoustic and translation models to
tackle SST. The approaches accounted are: de-
coupled, semi-decoupled and integrated archi-
tectures. While the former two are imple-
mentable by virtue of alternative TMs, the latter
is achieved thanks to the integration allowed by
finite-state framework. Thus, in order to com-
pare the combination rather than the TMs them-
selves, all of the combinations shall be put in
practice using the same SFST as TM.
3.1 Decoupled approach
Possibly the most widely used approach to
tackle speech translation is the so-called serial,
cascade or decoupled approach. It consists of
a text-to-text translation system placed after an
ASR system. This process is formally stated as:
t? ? arg max
t
max
s
P (x|s)P (s)P (t|s) (5)
In practice, previous expression is imple-
mented in two independent stages as follows:
1st stage: an ASR system would find the
most likely transcription (?s):
s? ? arg max
s
P (x|s)P (s) (6)
2nd stage next, given the expected string in
the source language (?s), a TM would find the
most likely translation:
t? ? arg max
t
P (t|?s) = arg max
t
P (?s, t) (7)
The TM involved in eq.(7) can be based on
either posterior or joint-probability as the dif-
ference between both of them is a normaliza-
tion term that does not intervene in the maxi-
mization process. The second stage has to cope
with expected transcription of speech (?s) which
does not necessarily convey the exact reference
source string (s). That is, the ASR might intro-
duce errors in the source string to be translated
in the next stage. However, the TMs are typ-
ically trained with correct source-target pairs.
Thus, transcription errors are seldom foreseen
even in models including smoothing (Martin et
al., 1999). In addition, TMs are extremely sen-
sitive to the errors in the input, in particular to
substitutions (Vilar et al, 2006).
This architecture represents a suboptimal
means of contending with SST as referred in
eq. (3). This approach barely takes advantage of
the involved knowledge sources, namely, acous-
tic and translation models.
102
3.2 Semi-Decoupled approach
Occasionally, the most probable translation
does not result to be the most accurate one with
respect to a given reference. That is, it might
happen that hypotheses with a slightly lower
probability than that of the expected hypothesis
turn to be more similar to the reference than the
expected hypothesis. This happens due to sev-
eral factors, amongst others, due to the sparsity
of the data with which the model was trained.
In brief, some sort of disparity between the
probability of the hypotheses and their quality
might arise in practice. The semi-decoupled ap-
proach arose to address this issue. Hence, rather
than translating a single transcription hypothe-
sis, a number of them are provided by the ASR
to the TM, and it is the latter that makes the de-
cision giving as a result the most likely transla-
tion. The decoupled approach is implemented
in two steps, and so is it the semi-decoupled ap-
proach. Details on the process are as follows:
1st stage: for a given utterance in the source
language, an ASR system, laying on source
acoustic model and source language model
(LM), would provide a search sub-space. This
sub-space is traced in the search process for the
most likely transcription of speech but without
getting rid of other highly probable hypotheses.
For what us concern, this sub-space is rep-
resented in terms of a graph of words in the
source language (S). The word-graph gath-
ers the hypotheses with a probability within a
threshold with respect to the optimal hypothesis
at each time-frame as it was formulated in (Ney
et al, 1997). The obtained graph is an acyclic
directed graph where the nodes are associated
with word-prefixes of a variable length, and the
edges join the word sequences allowed in the
recognition process with an associated recogni-
tion probability. The edges consist of the acous-
tic and language model probabilities as the ASR
system handles throughout the trellis.
2nd stage: translating the hypotheses within
S (the graph derived in the 1st stage) allows to
take into account alternative translations for the
given spoken utterance. The searching space
being explored is limited by the source strings
conveyed by S . The combination of the recog-
nition probability with the translation probabil-
ity results in a score that accounts both recogni-
tion and translation likelihood:
t? ? arg max
t
max
s?S
P (s)P (s, t) (8)
Thus, acoustic and translation models would
one re-score the other.
All in all, this semi-decoupled approach re-
sults in an extension of the decoupled one.
It accounts alternative transcriptions of speech
in an attempt to get good quality transcrip-
tions (rather than the most probable transcrip-
tion as in the case of the decoupled approach).
Amongst all the transcriptions, those with high
quality are expected to provide the best quality
in the target language. That is, by avoiding er-
rors derived from the transcription process, the
TM should perform better, and thus get transla-
tions of higher quality. Note that finally, a single
translation hypothesis is selected. To do so, the
highest combined probability is accounted.
3.3 Fully-integrated approach
Finite-state framework (by contrast to other
frameworks) makes a tight composition of mod-
els possible. In our case, of acoustic and trans-
lation finite-state models. The fully-integrated
approach, proposed in (Vidal, 1997), encfom-
passed acoustic and translation models within a
single model. To develop the fully-integrated
approach a finite-state acoustic model on the
source language (A) providing the text tran-
scription of a given acoustic utterance (A :
103
X ? S) can be composed with a text transla-
tion model (T ) that provides the translation of a
given text in the source language (T : S ? T )
and give as a result a transducer (Z = A ? T )
that would render acoustic utterances in the
source language to strings in the target lan-
guage. For the sake of efficiency in terms of
spatial cost, the models are integrated on-the-fly
in the same manner as it is done in ASR (Ca-
seiro and Trancoso, 2006).
The way in which integrated architecture ap-
proaches eq. (3) is looking for the most-likely
source-target translation pair as follows:
?(s, t) = arg max
(s,t)
P (s, t)P (x|s) (9)
That is, the search is driven by bilingual phrases
made up of acoustic elements in the source
language integrated within bilingual phrases of
words together with target phrases.
Then, the expected translation would simply
be approached as the target projection of ?(s, t),
the expected source-target string (also known
as the lower projection); and likewise, the ex-
pected transcription is obtained as a side-result
by the source projection (aka upper projection).
It is well-worth mentioning that this approach
implements fairly the eq. (3) without further
assumptions rather than those made in the de-
coding stage such as Viterbi-like decoding with
beam-search. All in all, acoustic and translation
models cooperate to find the expected transla-
tion. Moreover, it is carried out in a single-pass
decoding strategy by contrast to either decou-
pled or semi-decoupled approaches.
3.4 Integrated WG and re-scoring LM
The fully-integrated approach looks for the
single-best hypothesis within the integrated
acoustic-and-translation network. Following
the reasoning of Section 3.2, the most likely
path together with other locally close paths in
the integrated searching space can be extracted
and arranged in terms of a word graph. While
the WG derived in Section 3.2 was in source
language, this one would be bilingual.
Given a bilingual WG, the lower-side net
(WG.l) can be extracted keeping the topol-
ogy and the associated probability distributions
while getting rid of the input string of each tran-
sition, this gives as a result the projection of
the WG in the target language. Next, a target
language model (LM) would help to make the
choice for the most likely hypothesis amongst
those in the WG.l.
t? ? arg max
t
PWG.l(t)PLM (t) (10)
In other words, while in Section 3.2 the trans-
lation model was used to re-score alternative
transcriptions of speech whereas in this ap-
proach a target language models re-scores al-
ternative translations provided by the bilingual
WG. Note that this approach, as well as the
semi-decoupled one, entail a two-pass decoding
strategy. Both rely upon two models: the for-
mer focused on the source language WG, this
one focuses on the target language WG.
4 Experiments
The aim of this section is to assess empir-
ically the performance each of the four ap-
proaches previously introduced: decoupled,
semi-decoupled, fully-integrated and integrated
WG with re-scoring LM. The four approaches
differ on the decoding strategy implemented to
sort out the decision problem, but all of them
rely on the very same knowledge sources (that
is, the same acoustic and translation model).
The main features of the corpus used to carry
out the experimental layout are summarized in
Table 1. The training set was used to infer the
104
TM consisting of an SFST and the test set to as-
sess the SST decoding approaches. The test set
consisted of 500 training-independent pairs dif-
ferent each other, each of them was uttered by
at least 3 speakers.
Spanish Basque
Tr
ain Sentences 15,000Running words 191,000 187,000
Vocabulary 702 1,135
Te
st Sentences 1,800
Hours of speech 3.0 3.5
Table 1: Main features of the Meteus corpus.
The performance of each experiment is as-
sessed through well-known evaluation met-
rics, namely: bilingual evaluation under-study
(BLEU) (Papineni et al, 2002), word error-rate
(WER), translation edit rate (TER).
4.1 Results
The obtained results are given in Table 2. The
performance of the most-likely or single-best
translation derived by either decoupled or fully-
integrated architectures is shown in the first row
of Tables 2a and 2b respectively. The per-
formance of the semi-decoupled and integrated
WG with re-scoring LM is shown in the sec-
ond row. The highest performance achievable
by both the semi decoupled approach and the
integrated WG with re-scoring LM is given in
the third row. To do so, an oracle evaluation of
the alternatives was carried out and the score as-
sociated to the best choice achievable was given
as in (Pe?rez et al, 2010). Since the oracle evalu-
ation provides an upper threshold of the quality
achievable, the scope of each decoupled or in-
tegrated approaches can be assessed regardless
of the underlying decoding algorithms and ap-
proaches. The highest performance achievable
is reflected in the last row of Tables 2a and 2b.
4.2 Discussion
While the results with two-pass decoding strate-
gies (either decoupled or semi-decoupled ap-
proach) require an ASR engine, integrated ap-
proaches have the ability to get both the source
string together with its translation. This is why
we have make a distinction between ASR-WER
in the former and source-WER in the latter.
Nevertheless, our aim focuses on translation
rather than on recognition.
The results show that semi-decoupled ap-
proach outperforms the decoupled one. Simi-
larly, the approach based on the integrated WG
with the re-scoring target LM outperforms the
integrated approach. As a result, exploring dif-
ferent hypotheses and making the selection with
a second model allows to make refined deci-
sions. On the other hand, comparing the first
row of the Table 2a with the first row of the Ta-
ble 2b (or equally the second row of the former
with the second row of the latter), we conclude
that slightly better performance can be obtained
with the integrated approach.
Finally, comparing the third row of both Ta-
ble 2a and Table 2b, the conclusion is that the
eventual quality of the hypotheses within the in-
tegrated approach are significantly better than
those in the semi-decoupled approaches. That
is, what we can learn is that the integrated de-
coding strategy keeps much better hypotheses
than the semi-decoupled one throughout the de-
coding process. Still, while good quality hy-
potheses exist within the integrated approach,
the re-scoring with a target LM used to select
a single hypothesis from the entire network has
not resulted in getting the best possible hypoth-
esis. Oracle evaluation shows that the integrated
approach offers a leeway to achieve improve-
ments in the quality, yet, alternative strategies
have to be explored.
105
ASR target
WER BLEU WER TER
D 1-best 7.9 40.8 50.3 47.7
SD 7.9 42.2 47.6 44.7
SD tgt-oracle 7.5 57.6 36.2 32.8
(a) Decoupled and semi-decoupled
source target
WER BLEU WER TER
I 1-best 9.6 40.9 49.6 46.8
I WG + LM 9.3 42.6 46.7 43.9
I tgt-oracle 6.6 64.0 32.2 28.5
(b) Integrated and integrated WG with LM
Table 2: Assessment of SST approaches decoupled (2a) and integrated (2b) respectively.
5 Conclusions
Different approaches to cope with the SST de-
coding methodology were explored, namely,
decoupled approach, semi-decoupled approach,
fully-integrated approach and integrated ap-
proach with a re-scoring LM. The first two fol-
low a two-pass decoding strategy and focus on
exploring alternatives in the source language;
while the integrated one follows a single-pass
decoding and present tight cooperation between
acoustic and translation models.
All the experimental layouts used exactly the
same translation and acoustic models differing
only on the methodology used to overcome the
decision problem. In this way, we can assert
that the differences lay on the decoding strate-
gies rather than on the models themselves. Note
that implementing all the models in terms of
finite-state models allows to build both decou-
pled and integrated approaches.
Both decoupled and integrated decoding ap-
proaches aim at finding the most-likely transla-
tion under different assumptions. Occasionally,
the most probable translation does not result to
be the most accurate one with respect to a given
reference. On account of this, we turned to ana-
lyzing alternatives and making use of re-scoring
techniques on both approaches in an attempt
to make the most accurate hypothesis emerge.
This resulted in semi-decoupled and integrated-
WG with re-scoring target LM approaches.
What we can learn from the experiments
is that integrating the models allow to keep
good quality hypotheses in the decoding pro-
cess. Nevertheless, the re-scoring model has
not resulted in being able to make the most of
the integrated approach. In other words, there
are better quality hypotheses within the word-
graph rather than that selected by the re-scoring
target LM. Hence, further work should be fo-
cused on other means of selecting hypotheses
from the integrated word-graph.
However, undoubtedly significantly better
performance can be reached from the inte-
grated decoding strategy than from the semi-
decoupled one. It seems as though knowledge
sources modeling the syntactic differences be-
tween source and target languages should be
tackled in order to improve the performance,
particularly in our case, a strategy for further
work could go on the line of the recently tack-
led approach (Durrani et al, 2011).
Acknowledgments
This work was partially funded by the
Spanish Ministry of Science and Innovation:
through the T??mpano (TIN2011-28169-C05-04)
and iTrans2 (TIN2009-14511) projects; also
through MIPRCV (CSD2007-00018) project
within the Consolider-Ingenio 2010 program;
by the Basque Government to PR&ST research
group (GIC10/158, IT375-10), and by the Gen-
eralitat Valenciana under grants ALMPR (Prom-
eteo/2009/01) and GV/2010/067.
106
References
[Bertoldi et al2007] N. Bertoldi, R. Zens, and M.
Federico. 2008. Efficient speech translation by
confusion network decoding. IEEE International
Conference on Acoustics, Speech and Signal Pro-
cessing, pg. 1696?1705
[Casacuberta and Vidal2004] F. Casacuberta and E.
Vidal. 2004. Machine translation with in-
ferred stochastic finite-state transducers. Compu-
tational Linguistics, 30(2): pg. 205?225.
[Casacuberta et al2008] F. Casacuberta, M. Fed-
erico, H. Ney, and E. Vidal. 2008. Recent efforts
in spoken language translation. IEEE Signal Pro-
cessing Magazine, 25(3): pg. 80?88.
[Caseiro and Trancoso2006] D. Caseiro and I. Tran-
coso. 2006. A specialized on-the-fly algo-
rithm for lexicon and language model composi-
tion. IEEE Transactions on Audio, Speech &
Language Processing, 14(4): pg. 1281?1291.
[Durrani et al2011] N. Durrani, H. Schmid, and A.
Fraser. 2011. A joint sequence translation model
with integrated reordering. In 49th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies, pg. 1045?
1054
[Marin?o et al2006] J. B. Marin?o, R. E. Banchs, J. M.
Crego, A. de Gispert, P. Lambert, J. A. R. Fonol-
losa, and M. R. Costa-jussa`. 2006. N-gram-
based machine translation. Computational Lin-
guistics, 32(4): pg. 527?549
[Martin et al1999] S. C. Martin, H. Ney, and
J. Zaplo. 1999. Smoothing methods in maxi-
mum entropy language modeling. IEEE Interna-
tional Conference on Acoustics, Speech, and Sig-
nal Processing , vol. 1, pg. 545?548
[Matusov and Ney2011] E. Matusov and H. Ney.
2011. Lattice-based ASR-MT interface for
speech translation. IEEE Transactions on Audio,
Speech, and Language Processing, 19(4): pg. 721
?732
[Ney et al1997] H. Ney, S. Ortmanns, and I. Lin-
dam. 1997. Extensions to the word graph method
for large vocabulary continuous speech recogni-
tion. IEEE International Conference on Acous-
tics, Speech, and Signal Processing, vol. 3, pg.
1791 ?1794
[Papineni et al2002] K. Papineni, S. Roukos, T.
Ward, and W.-J. Zhu. 2002. Bleu: a method for
automatic evaluation of machine translation. An-
nual Meeting on Association for Computational
Linguistics, pg. 311?318
[Pe?rez et al2010] A. Pe?rez, M. I. Torres, and F.
Casacuberta. 2010. Potential scope of a fully-
integrated architecture for speech translation. An-
nual Conference of the European Association for
Machine Translation, pg. 1?8
[Quan et al2005] V. H. Quan, M. Federico, and M.
Cettolo. 2005. Integrated n-best re-ranking for
spoken language translation. European Conver-
ence on Speech Communication and Technology,
Interspeech, pg. 3181?3184.
[Rabiner1989] L.R. Rabiner. 1989. A tutorial on
hidden markov models and selected applications
in speech recognition. Proceedings of the IEEE,
77(2): pg. 257?286
[Vidal et al2005] E. Vidal, F. Thollard, C. de la
Higuera, F. Casacuberta, and R. C. Carrasco.
2005. Probabilistic finite-state machines - part II.
IEEE Transactions on Pattern Analysis and Ma-
chine Intelligence, 27(7): pg. 1026?1039
[Vidal1997] E. Vidal. 1997. Finite-state speech-to-
speech translation. International Conference on
Acoustic, Speech and Signal Processing, vol. 1,
pg. 111?114
[Vilar et al2006] David Vilar, Jia Xu, Luis Fernando
D?Haro, and H. Ney. 2006. Error Analysis of
Machine Translation Output. International Con-
ference on Language Resources and Evaluation,
pg. 697?702
[Zhang et al2004] R. Zhang, G. Kikui, H. Ya-
mamoto, T. Watanabe, F. Soong, and W. K. Lo.
2004. A unified approach in speech-to-speech
translation: integrating features of speech recog-
nition and machine translation. International
Conference on Computational Linguistics, pg.
1168-1174
[Zhou et al2007] B. Zhou, L. Besacier, and Y. Gao.
2007. On efficient coupling of ASR and SMT for
speech translation. IEEE International Confer-
ence on Acoustics, Speech and Signal Processing,
vol. 4, pg. 101?104
107
Proceedings of the 5th International Workshop on Health Text Mining and Information Analysis (Louhi) @ EACL 2014, pages 85?89,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Adverse Drug Event prediction combining
shallow analysis and machine learning
Sara Santiso
Alicia P
?
erez
Koldo Gojenola
IXA Taldea (UPV-EHU)
Arantza Casillas
Maite Oronoz
IXA Taldea (UPV-EHU)
http://ixa.si.ehu.es
Abstract
The aim of this work is to infer a model
able to extract cause-effect relations be-
tween drugs and diseases. A two-level
system is proposed. The first level car-
ries out a shallow analysis of Electronic
Health Records (EHRs) in order to iden-
tify medical concepts such as drug brand-
names, substances, diseases, etc. Next,
all the combination pairs formed by a
concept from the group of drugs (drug
and substances) and the group of diseases
(diseases and symptoms) are characterised
through a set of 57 features. A supervised
classifier inferred on those features is in
charge of deciding whether that pair rep-
resents a cause-effect type of event.
One of the challenges of this work is the
fact that the system explores the entire
document. The contributions of this pa-
per stand on the use of real EHRs to dis-
cover adverse drug reaction events even in
different sentences. Besides, the work fo-
cuses on Spanish language.
1 Introduction
This work deals with semantic data mining within
the clinical domain. The aim is to automatically
highlight the Adverse Drug Reactions (ADRs) in
EHRs in order to alleviate the work-load to sev-
eral services within a hospital (pharmacy service,
documentation service,. . . ) that have to read these
reports. Event detection was thoroughly tackled in
the Natural Language Processing for Clinical Data
2010 Challenge. Since then, cause-effect event ex-
traction has emerged as a field of interest in the
Biomedical domain (Bj?orne et al., 2010; Mihaila
et al., 2013). The motivation is, above all, practi-
cal. Electronic Health Records (EHRs) are studied
by several services in the hospital, not only by the
doctor in charge of the patient but also by the phar-
macy and documentation services, amongst oth-
ers. There are some attempts in the literature that
aim to make the reading of the reports in English
easier and less time-consuming by means of an au-
tomatic annotation toolkit (Rink et al., 2011; Bot-
sis et al., 2011; Toldo et al., 2012). This work is
a first approach on automatic learning of relations
between drugs causing diseases in Spanish EHRs.
This work presents a system that entails two
stages in cascade: 1) the first one carries out the
annotation of drugs or substances (from now on-
wards both of them shall be referred to as DRUG)
and diseases or symptoms (referred to as DIS-
EASE); 2) the second one determines whether a
given (DRUG, DISEASE) pair of concepts repre-
sents a cause-effect reaction. Note that we are in-
terested in highlighting events involving (DRUG,
DISEASE) pairs where the drug caused an adverse
reaction or a disease. By contrast, often, (DRUG,
DISEASE) pairs would entail a drug prescribed to
combat a disease, but these correspond to a differ-
ent kind of events (indeed, diametrically opposed).
Besides, (DRUG, DISEASE) pairs might represent
other sort of events or they might even be unre-
lated at all. Finally, the system should present the
ADRs marked in a friendly front-end. To this end,
the aim is to represent the text in the framework
provided by Brat (Stenetorp et al., 2012). Figure 1
shows an example, represented in Brat, of some
cause-effect events manually tagged by experts.
There are related works in this field aiming at
a variety of biomedical event extraction, such as
binary protein-protein interaction (Wong, 2001),
biomolecular event extraction (Kim et al., 2011),
and drug-drug interaction extraction (Segura-
Bedmar et al., 2013). We are focusing on a variety
of interaction extraction: drugs causing diseases.
There are previous works in the literature that try
to warn whether a document contains or not this
type of events. There are more recent works that
85
Figure 1: Some cause-effect events manually annotated in the Brat framework.
cope with event extraction within the same sen-
tence, that is, intra-sentence events. By contrast, in
this work we have realised that around 26% of the
events occur between concepts that are in differ-
ent sentences. Moreover, some of them are at very
long distance. Hence, our method aims at provid-
ing all the (DRUG, DISEASE) concepts within the
document that represent a cause-effect relation.
We cope with real discharge EHRs written by
around 400 different doctors. These records are
not written in a template, that is, the EHRs do not
follow a pre-determined structure, and this, by it-
self entails a challenge. The EHRs we are dealing
with are written in a free structure using natural
language, non-standard abbreviations etc. More-
over, we tackle Spanish language, for which little
work has been carried out. In addition, we do not
only aim at single concept-words but also at con-
cepts based on multi-word terms.
2 System overview
The system, as depicted in Figure 2 entails two
stages.
EHR
Stage 1:
ANNOTATING
CONCEPTS
Stage 2:
EXTRACTING
EVENTS
MARKED 
EHR
Figure 2: The ADR event extraction system.
In the first stage, relevant pairs of concepts have
to be identified within an EHR. Concept annota-
tion is accomplished by means of a shallow anal-
yser system (described in section 2.1). Once the
analyser has detected (DRUG, DISEASE) pairs in
a document, all the pairs will be examined by
an inferred supervised classifier (described in sec-
tion 2.2).
2.1 Annotating concepts by shallow analysis
The first stage of the system has to detect and an-
notate two types of semantic concepts: drugs and
diseases. Each concept, as requested by the phar-
macy service, should gather several sub-concepts
stated as follows:
1. DRUG concept:
(a) Generic names for pharmaceutical
drugs: e.g. corticoids;
(b) Brand-names for pharmaceutical drugs:
e.g. Aspirin;
(c) Active ingredients: e.g. vancomycin;
(d) Substances: e.g. dust, rubber;
2. DISEASE concept:
(a) Diseases
(b) Signs
(c) Symptoms
These concepts were identified by means of a
general purpose analyser available for Spanish,
called FreeLing (Padr?o et al., 2010), that had been
enhanced with medical ontologies and dictionar-
ies, such as SNOMED-CT, BotPLUS, ICD-9-CM,
etc. (Oronoz et al., 2013). This toolkit is able
to identify multi-word context-terms, lemmas and
also POS tags. An example of the morphological,
semantic and syntactic analysis, provided by this
parser is given in Figure 3. In the figure two pieces
of information can be distinguished: for exam-
ple, given the word ?secundarios? (meaning sec-
ondaries) 1) the POS tag provided is AQOM corre-
sponding to Qualificative Adjective Ordinal Mas-
culine Singular; and 2) the provided lemma is ?se-
cundario? (secondary). Besides, in a third layer,
the semantic tag is given, that is, the tag ?ENFER-
MEDAD? (meaning disease) involves the multi-
word concept ?HTP severa? (severe pulmonary
hypertension).
86
Figure 3: Lemmas, POS-tags and semantic tags are identified by the clinic domain analyser (diseases in
yellow and drugs or substances in violet).
2.2 Extracting adverse drug reaction events
using inferred classifiers
The goal of the second stage is to determine if a
given (DRUG, DISEASE) pair represents an ADR
event or not. On account of this, we resorted to
supervised classification models. These models
can be automatically inferred from a set of doc-
uments in which the target concepts had been pre-
viously annotated. Hence, first of all, a set of an-
notated data representative for the task is required.
To this end, our starting point is a manually anno-
tated corpus (presented in section 2.2.1). Besides,
in order to automatically learn the classifier, the
(DRUG, DISEASE) pairs have to be described in an
operative way, that is, in terms of a finite-set of
features (see section 2.2.2). The supervised clas-
sification model selected was a type of ensemble
classifier: Random Forests (for further details turn
to section 2.2.3).
2.2.1 Producing an annotated set
A supervised classifier was inferred from an-
notated real EHRs. The annotation was carried
out by doctors from the same hospital that pro-
duced the EHRs. Given the text with the con-
cepts marked on the first stage (turn to section 2.1)
and represented within the framework provided by
Brat
1
, around 4 doctors from the same hospital an-
notated the events. This annotated set would work
as a source of data to get instances that would
serve to train supervised classification models, as
the one referred in section 2.2.
2.2.2 Operational description of events
As it is well-known, the success of the techniques
based on Machine Learning relies upon the fea-
tures used to describe the instances. Hence, we se-
lected the following features that eventually have
1
Brat is the framework a priori selected as the output
front-end shown in Figure 1
proven useful to capture the semantic relations be-
tween ADRs. The features can be organised in the
following sets:
? Concept-words and context-words: to be
precise, we make use of entire terms
including both single-words and multi-
words.
? DRUG concept-word together with
left and right context words (a con-
text up to 3, yielding, thus, 7 fea-
tures).
? DISEASE concept-word together
with left and right context words (7
features).
? Concept-lemmas and context-lemmas
for both drug and disease (14 features
overall)
? Concept-POS and context-POS for both
drug and disease (14 features)
? Negation and speculation: these are
binary valued features to determine
whether the concept words or their con-
text was either negated or speculated (2
features).
? Presence/absence of other drugs in the
context of the target drug and disease (12
features)
? Distance: the number of characters from
the DRUG concept to the DISEASE con-
cept (1 feature).
2.2.3 Inferring a supervised classifier
Given the operational description of a set of
(DRUG, DISEASE) pairs, this stage has to deter-
87
mine if there exists an ADR event (that is, a cause-
effect relation) or not. To do so, we resorted
to Random Forests (RFs), a variety of ensemble
models. RFs combine a number of decision trees
being each tree built on the basis of the C4.5 algo-
rithm (Quinlan, 1993) but with a distinctive char-
acteristic: some randomness is introduced in the
order in which the nodes are generated. Particu-
larly, each time a node is generated in the tree, in-
stead of chosing the attribute that maximizes the
Information Gain, the attribute is randomly se-
lected amongst the k best options. We made use
of the implementation of this algorithm available
in Weka-6.9 (Hall et al., 2009). Ensemble models
were proved useful on drug-drug interaction ex-
traction tasks (Thomas et al., 2011).
3 Experimental results
We count on data consisting of discharge sum-
maries from Galdakao-Usansolo Hospital. The
records are semi-structured in the sense that there
are two main fields: the first one for personal data
of the patient (age, dates relating to admittance)
that were not provided by the hospital for privacy
issues; and the second one, our target, a single
field that contains the antecedents, treatment, clin-
ical analysis, etc. This second field is an unstruc-
tured section (some hospitals rely upon templates
that divide this field into several subfields, provid-
ing it with further structure). The discharge notes
describe a chronological development of the pa-
tient?s condition, the undergone treatments, and
also the clinical tests that were carried out.
Given the entire set of manually annotated doc-
uments, 34% were randomly selected without re-
placement to produce the evaluation set. The re-
sulting partition is presented in Table 1 (where the
train and evaluation sets are referred to as Train
and Eval respectivelly).
Documents Concepts Relations
Train 144 6,105 4,675
Eval 50 2,206 1,598
Table 1: Quantitative description of the data.
All together, there are 194 EHRs manually
tagged with more than 8,000 concepts (entailing
diseases, symptoms, drugs, substances and proce-
dures). From these EHRs all the (DRUG,DISEASE)
pairs are taken into account as event candidates,
and these are referred to as relations in Table 1.
The system was assessed using per-class aver-
aged precision, recall and f1-measure as presented
in Table 2.
Precision Recall F1-measure
0.932 0.849 0.883
Table 2: Experimental results.
Semantic knowledge and contextual features
have proven very relevant to detect cause-effect re-
lations. Particularly, those used to detect the con-
cepts and also negation or speculation of the con-
text in which the concept appear.
A manual inspection was carried out on both the
false positives and false negative predictions and
the following conclusions were drawn:
? The majority of false positives were caused
by i) pairs of concepts at a very long distance;
ii) pairs where one of the elements is related
to past-events undergone while the other el-
ement is in the current treatment prescribed
(e.g. the disease is in the antecedents and the
drug in the current diagnostics).
? The vast majority of false negatives were
due to concepts in the same sentence where
the context-words are irrelevant (e.g. filler
words, determiners, etc.).
4 Concluding Remarks and Future Work
This work presents a system that first identifies rel-
evant pairs of concepts in EHRs by means of a
shallow analysis and next examines all the pairs
by an inferred supervised classifier to determine if
a given pair represents a cause-effect event. A rel-
evant contribution of this work is that we extract
events occurring between concepts that are in dif-
ferent sentences. In addition, this is one of the first
works on medical event extraction for Spanish.
Our aim for future work is to determine whether
the (DRUG, DISEASE) pair represents either a rela-
tion where 1) the drug is to overcome the disease;
2) the drug causes the disease; 3) there is no rela-
tionship between the drug and the disease.
The aim of context features is to capture charac-
teristics of the text surrounding the relevant con-
cepts that trigger a relation. More features could
also be explored such as trigger words, regular pat-
terns, n-grams, etc.
88
Acknowledgments
The authors would like to thank the Pharmacy
and Pharmacovigilance services of Galdakao-
Usansolo Hospital.
This work was partially supported by the Euro-
pean Commission (325099 and SEP-210087649),
the Spanish Ministry of Science and Innovation
(TIN2012-38584-C06-02) and the Industry of the
Basque Government (IT344-10).
References
Jari Bj?orne, Filip Ginter, Sampo Pyysalo, Jun?ichi Tsu-
jii, and Tapio Salakoski. 2010. Complex event ex-
traction at pubmed scale. Bioinformatics [ISMB],
26(12):382?390.
Taxiarchis Botsis, Michael D. Nguyen, Emily Jane
Woo, Marianthi Markatou, and Robert Ball. 2011.
Text mining for the vaccine adverse event reporting
system: medical text classification using informative
feature selection. JAMIA, 18(5):631?638.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA data mining software: An up-
date. SIGKDD Explorations, 11(1):10?18.
Jin-Dong Kim, Sampo Pyysalo, Tomoko Ohta, Robert
Bossy, Ngan Nguyen, and Jun?ichi Tsujii. 2011.
Overview of bionlp shared task 2011. In
Proceedings of the BioNLP Shared Task 2011
Workshop, pages 1?6. Association for Computa-
tional Linguistics.
Claudiu Mihaila, Tomoko Ohta, Sampo Pyysalo, and
Sophia Ananiadou. 2013. Biocause: Annotating
and analysing causality in the biomedical domain.
BMC Bioinformatics, 14:2.
Maite Oronoz, Arantza Casillas, Koldo Gojenola, and
Alicia Perez. 2013. Automatic annotation of
medical records in Spanish with disease, drug and
substance names. In Lecture Notes in Computer
Science, volume 8259, pages 536?547. Springer-
Verlag.
Lluis Padr?o, S. Reese, Eneko Agirre, and Aitor Soroa.
2010. Semantic Services in Freeling 2.1: WordNet
and UKB. In Global Wordnet Conference, Mumbai,
India.
Ross Quinlan. 1993. C4.5: Programs for Machine
Learning. Morgan Kaufmann Publishers, San Ma-
teo, CA.
Bryan Rink, Sanda Harabagiu, and Kirk Roberts.
2011. Automatic extraction of relations between
medical concepts in clinical texts. JAMIA, 18:594?
600.
Isabel Segura-Bedmar, P Mart??nez, and Mar?a Herrero-
Zazo. 2013. Semeval-2013 task 9: Extraction of
drug-drug interactions from biomedical texts (ddiex-
traction 2013). Proceedings of Semeval, pages 341?
350.
Pontus Stenetorp, Sampo Pyysalo, Goran Topi?c,
Tomoko Ohta, Sophia Ananiadou, and Jun?ichi Tsu-
jii. 2012. Brat: A web-based tool for nlp-
assisted text annotation. In In Proceedings of the
Demonstrations Session at EACL 2012.
Philippe Thomas, Mariana Neves, Ill?es Solt,
Domonkos Tikk, and Ulf Leser. 2011. Relation
extraction for drug-drug interactions using ensem-
ble learning. 1st Challenge task on Drug-Drug
Interaction Extraction (DDIExtraction 2011), pages
11?18.
Luca Toldo, Sanmitra Bhattacharya, and Harsha Gu-
rulingappa. 2012. Automated identification of ad-
verse events from case reports using machine learn-
ing. In Workshop on Computational Methods in
Pharmacovigilance.
Limsoon Wong. 2001. A protein interaction extraction
system. In Pacific Symposium on Biocomputing,
volume 6, pages 520?531. Citeseer.
89
