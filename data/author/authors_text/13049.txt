Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 887?896, Prague, June 2007. c?2007 Association for Computational Linguistics
A Probabilistic Approach to Diachronic Phonology
Alexandre Bouchard-Co?te?? Percy Liang? Thomas L. Griffiths? Dan Klein?
?Computer Science Division ?Department of Psychology
University of California at Berkeley
Berkeley, CA 94720
Abstract
We present a probabilistic model of di-
achronic phonology in which individual
word forms undergo stochastic edits along
the branches of a phylogenetic tree. Our ap-
proach allows us to achieve three goals with
a single unified model: (1) reconstruction
of both ancient and modern word forms, (2)
discovery of general phonological changes,
and (3) selection among different phyloge-
nies. We learn our model using a Monte
Carlo EM algorithm and present quantitative
results validating the model.
1 Introduction
Modeling how languages change phonologically
over time (diachronic phonology) is a central topic
in historical linguistics (Campbell, 1998). The ques-
tions involved range from reconstruction of ancient
word forms, to the elucidation of phonological drift
processes, to the determination of phylogenetic re-
lationships between languages. However, this prob-
lem has received relatively little attention from the
computational community. What work there is has
focused on the reconstruction of phylogenies on the
basis of a Boolean matrix indicating the properties
of words in different languages (Gray and Atkinson,
2003; Evans et al, 2004; Ringe et al, 2002; Nakhleh
et al, 2005).
In this paper, we present a novel framework, along
with a concrete model and experiments, for the prob-
abilistic modeling of diachronic phonology. We fo-
cus on the case where the words are etymological
cognates across languages, e.g. French faire and
Spanish hacer from Latin facere (to do). Given
this information as input, we learn a model acting
at the level of individual phoneme sequences, which
can be used for reconstruction and prediction, Our
model is fully generative, and can be used to reason
about a variety of types of information. For exam-
ple, we can observe a word in one or more modern
languages, say French and Spanish, and query the
corresponding word form in another language, say
Italian. This kind of lexicon-filling has applications
in machine translation. Alternatively, we can also
reconstruct ancestral word forms or inspect the rules
learned along each branch of a phylogeny to identify
salient patterns. Finally, the model can be used as a
building block in a system for inferring the topology
of phylogenetic trees. We discuss all of these cases
further in Section 4.
The contributions of this paper are threefold.
First, the approach to modeling language change at
the phoneme sequence level is new, as is the spe-
cific model we present. Second, we compiled a new
corpus1 and developed a methodology for quantita-
tively evaluating such approaches. Finally, we de-
scribe an efficient inference algorithm for our model
and empirically study its performance.
1.1 Previous work
While our word-level model of phonological change
is new, there have been several computational inves-
tigations into diachronic linguistics which are rele-
vant to the present work.
The task of reconstructing phylogenetic trees
1nlp.cs.berkeley.edu/pages/historical.html
887
for languages has been studied by several authors.
These approaches descend from glottochronology
(Swadesh, 1955), which views a language as a col-
lection of shared cognates but ignores the structure
of those cognates. This information is obtained from
manually curated cognate lists such as the data of
Dyen et al (1997).
As an example of a cognate set encoding, consider
the meaning ?eat?. There would be one column for
the cognate set which appears in French as manger
and Italian as mangiare since both descend from the
Latin mandere (to chew). There would be another
column for the cognate set which appears in both
Spanish and Portuguese as comer, descending from
the Latin comedere (to consume). If this were the
only data, algorithms based on this data would tend
to conclude that French and Italian were closely re-
lated and that Spanish and Portuguese were equally
related. However, the cognate set representation has
several disadvantages: it does not capture the fact
that the cognate is closer between Spanish and Por-
tuguese than between French and Spanish, nor do
the resulting models let us conclude anything about
the regular processes which caused these languages
to diverge. Also, the existing cognate data has been
curated at a relatively high cost. In our work, we
track each word using an automatically obtained
cognate list. While our cognates may be noisier,
we compensate by modeling phonological changes
rather than boolean mutations in cognate sets.
There has been other computational work in this
broad domain. Venkataraman et al (1997) describe
an information theoretic measure of the distance be-
tween two dialects of Chinese. Like our approach,
they use a probabilistic edit model as a formaliza-
tion of the phonological process. However, they do
not consider the question of reconstruction or infer-
ence in multi-node phylogenies, nor do they present
a learning algorithm for such models.
Finally, for the specific application of cog-
nate prediction in machine translation, essentially
transliteration, there have been several approaches,
including Kondrak (2002). However, the phenom-
ena of interest, and therefore the models, are ex-
tremely different. Kondrak (2002) presents a model
for learning ?sound laws,? general phonological
changes governing two completely observed aligned
cognate lists. His model can be viewed as a special
la
es it
la
vl
ib
es pt
it
la
it pt
es
la
it es
pt
Topology 1 Topology 2 *Topology 3 *Topology 4
Figure 1: Tree topologies used in our experiments. *Topology
3 and *Topology 4 are incorrect evolutionary tree used for our
experiments on the selection of phylogenies (Section 4.4).
case of ours using a simple two-node topology.
There is also a rich literature (Huelsenbeck et al,
2001) on the related problems of evolutionary biol-
ogy. A good reference on the subject is Felsenstein
(2003). In particular, Yang and Rannala (1997), Mau
and Newton (1997) and Li et al (2000) each inde-
pendently presented a Bayesian model for comput-
ing posteriors over evolutionary trees. A key dif-
ference with our model is that independence across
evolutionary sites is assumed in their work, while
the evolution of the phonemes in our model depends
on the environment in which the change occurs.
2 A model of phonological change
Assume we have a fixed set of word types (cog-
nate sets) in our vocabulary V and a set of languages
L. Each word type i has a word form wil in each lan-
guage l ? L, which is represented as a sequence of
phonemes and might or might not be observed. The
languages are arranged according to some tree topol-
ogy T (see Figure 1 for examples). One might con-
sider models that simultaneously induce the topol-
ogy and cognate set assignments, but let us fix both
for now. We discuss one way to relax this assump-
tion and present experimental results in Section 4.4.
Our generative model (Figure 3) specifies a dis-
tribution over the word forms {wil} for each word
type i ? V and each language l ? L. The genera-
tive process starts at the root language and generates
all the word forms in each language in a top-down
manner. One appealing aspect about our model is
that, at a high-level, it reflects the actual phonolog-
ical process that languages undergo. However, im-
portant phenomena like lexical drift, borrowing, and
other non-phonological changes are not modeled.
888
Our generative model can be summarized as fol-
lows:
For each word i ? V :
?wiROOT ? LanguageModel
For each branch (k ? l) ? T :
??k?l ? Dirichlet(?) [choose edit params.]
?For each word i ? V :
??wil ? Edit(wik, ?k?l) [sample word form]
In the remainder of this section, we describe each
of the steps in the model.
2.1 Language model
For the distributionw ? LanguageModel, we used a
simple bigram phoneme model. The phonemes were
partitioned into natural classes (see Section 4 for de-
tails). A root word form consisting of n phonemes
x1 ? ? ?xn is generated with probability
plm(x1)
n?
j=2
plm(xj | NaturalClass(xj?1)),
where plm is the distribution of the language model.
2.2 Edit model
The stochastic edit model y ? Edit(x, ?) describes
how a single old word form x = x1 ? ? ?xn changes
along one branch of the phylogeny with parameters
? to produce a new word form y. This process is
parameterized by rule probabilities ?k?l, which are
specific to branch (k ? l).
The generative process is as follows: for each
phoneme xi in the old word form, walking from
left to right, choose a rule to apply. There are
three types of rules: (1) deletion of the phoneme,
(2) substitution with another phoneme (possibly the
same one), or (3) insertion of another phoneme, ei-
ther before or after the existing one. The prob-
ability of applying a rule depends on a context
(NaturalClass(xi?1),NaturalClass(xi+1)). Figure 2
illustrates the edits on an example. The context-
dependence allows us to represent phenomena such
as the fact that s is likely to be deleted only in word-
final contexts.
The edit model we have presented approximately
encodes a limited form of classic rewrite-driven seg-
mental phonology (Chomsky and Halle, 1968). One
# C V C V C #
# f o k u s #
# f w O k o #
# C V V C V #
f ? f / # Vo ? w O / C Ck ? k / V Vu ? o / C Cs ? / V #
Edits applied Rules used
Figure 2: An example of edits that were used to transform
the Latin word FOCUS (/fokus/) into the Italian word fuoco
(/fwOko/) (fire) along with the context-specific rules that were
applied.
could imagine basing our model on more modern
phonological theory, but the computational proper-
ties of the edit model are compelling, and it is ade-
quate for many kinds of phonological change.
In addition to simple edits, we can model some
classical changes that appear to be too complex to be
captured by a single left-to-right edit model of this
kind. For instance, bleeding and feeding arrange-
ments occur when one phonological change intro-
duces a new context, which triggers another phono-
logical change, but the two cannot occur simultane-
ously. For example, vowel raising e ? i / c might
be needed before palatalization t ? c / i. Instead
of capturing such an interaction directly, we can
break up a branch into two segments joined at an in-
termediate language node, conflating the concept of
historically intermediate languages with the concept
of intermediate stages in the application of sequen-
tial rules.
However, many complex processes are not well-
represented by our basic model. One problem-
atic case is chained shifts such as Grimm?s law in
Proto-Germanic or the Great Vowel Shift in English.
To model such dependent rules, we would need
to use a more complex prior distributions over the
edit parameters. Another difficult case is prosodic
changes, such as unstressed vowel neutralizations,
which would require a representation of supraseg-
mental features. While our basic model does not
account for these phenomena, extensions within the
generative framework could capture such richness.
3 Learning and inference
We use a Monte Carlo EM algorithm to fit the pa-
rameters of our model. The algorithm iterates be-
tween a stochastic E-step, which computes recon-
889
...
wiA
wiB
wiC wiD
... ...word type i = 1 . . . |V |
eiA?B?A?B
eiB?C?B?C eiB?D ?B?D
Figure 3: The graphical model representation of our model: ?
are the parameters specifying the stochastic edits e, which gov-
ern how the words w evolve. The plate notation indicates the
replication of the nodes corresponding to the evolving words.
structions based on the current edit parameters, and
an M-step, which updates the edit parameters based
on the reconstructions.
3.1 Monte Carlo E-step: sampling the edits
The E-step needs to produce expected counts of how
many times each edit (such as o ? O) was used in
each context. An exact E-step would require sum-
ming over all possible edits involving all languages
in the phylogeny (all unobserved {e}, {w} variables
in Figure 3). Unfortunately, unlike in the case of
HMMs and PCFGs, our model permits no tractable
dynamic program to compute these counts exactly.
Therefore, we resort to a Monte Carlo E-step,
where many samples of the edit variables are col-
lected, and counts are computed based on these sam-
ples. Samples are drawn using Gibbs sampling (Ge-
man and Geman, 1984): for each word form of a
particular language wil, we fix all other variables in
the model and sample wil along with its correspond-
ing edits.
In the E-step, we fix the parameters, which ren-
ders the word types conditionally independent, just
as in an HMM. Therefore, we can process each word
type in turn without approximation.
First consider the simple 4-language topology in
Figure 3. Suppose that the words in languages A,
C and D are fixed, and we wish to infer the word
at language B along with the three corresponding
sets of edits (remember the edits fully determine the
words). There are an exponential number of possi-
ble words/edits, but it turns out that we can exploit
theMarkov structure in the edit model to consider all
such words/edits using dynamic programming, in a
way broadly similar to the forward-backward algo-
rithm for HMMs.
Figure 4 shows the lattice for the dynamic pro-
gram. Each path connecting the two shaded end-
point states represents a particular word form for
language B and a corresponding set of edits. Each
node in the lattice is a state of the dynamic pro-
gram, which is a 5-tuple (iA, iC , iD, c1, c2), where
iA, iC and iD are the cursor positions (represented
by dots in Figure 4) in each of the word forms of
A,C and D, respectively; c1 is the natural class of
the phoneme in the word form for B that was last
generated; and c2 corresponds to the phoneme that
will be generated next.
Each state transition involves applying a rule
to A?s current phoneme (which produces 0?2
phonemes in B) and applying rules to B?s new 0?2
phonemes. There are three types of rules (deletion,
substitution, insertion), resulting in 30+32+34 = 91
types of state transitions. For illustration, Figure 4
shows the simpler case where B only has one child
C. Given these rules, the new state is computed by
advancing the appropriate cursors and updating the
natural classes c1 and c2. The weight of each tran-
sition w(s ? t) is a product of the language model
probability and the rule probabilities that were cho-
sen.
For each state s, the dynamic program computes
W (s), the sum of the weights of all paths leaving s,
W (s) =
?
s?t
w(s ? t)W (t).
To sample a path, we start at the leftmost state,
choose the transition with probability proportional
to its contribution in the sum for computing W (s),
and repeat until we reach the rightmost state.
We applied a few approximations to speed up the
sampling of words, which reduced the running time
by several orders of magnitude. For example, we
pruned rules with low probability and restricted the
890
An example of a dynamic programming lattice
...
...
... ... ... ... ... ... ...
...
patr ? ia
# C V C C# p a t r ? V #a #
patr ? ja
x [T1] p1ed(i ? /C V) x
x [T3] plm(j | C) p1ed(i ? j/C V) p2ed(j ? j/C V) x
x [T11] plm(j | C) plm(i | C) p1ed(i ? j i/C V) p2ed(j ? j/C V) p2ed(i ? /C V) x
. . .
patri ? a
# C V C C# p a t r ? V #a #
patr ? ja
patri ? a
# C V C C C# p a t r j ? V #a #
patrj ? a
patri ? a
# C V C C C V# p a t r j i ? V #a #
patrj ? a
. . .
Types of state transitions (x: ancient phoneme, y: intermediate, z: modern)
x
y
x
y
z
x
y
z
x
y
z z
x
y
z
y
z
x
y
z
y
z
x
y
z
y
z z
x
y
z
y
z
x
y
z
y
z
x
y
z
y
z z
x
y
z z
y
z
x
y
z z
y
z
x
y
z z
y
z z[T1] [T2] [T3] [T4] [T5] [T6] [T7] [T8] [T9] [T10] [T11] [T12] [T13]
Figure 4: The dynamic program involved in sampling an intermediate word form given one ancient and one modern word form.
One lattice node is expanded to show the dynamic program state (represented by the part not grayed out) and three of the many
possible transitions leaving the state. Each transition is labeled with the weight of the transition, which is the product of the relevant
model probabilities. At the bottom, the 13 types of state transitions are shown.
state space of the dynamic program by limiting the
deviation in cursor positions.
3.2 M-step: updating the parameters
The M-step is standard once we have computed
the expected counts of edits in the E-step. For
each branch (k ? l) ? T in the phylogeny,
we compute the maximum likelihood estimate
of the edit parameters {?k?l(x ? ? / c1 c2)}.
For example, the parameter corresponding to
x = /e/, ? = /e s/, c1 = ALVEOLAR, c2 = # is
the probability of inserting a final /s/ after an /e/
which is itself preceded by an alveolar phoneme.
The probability of each rule is estimated as follows:
?k?l(x ? ? / c1 c2) =
#(x ? ? / c1 c2) + ?(x ? ? / c1 c2)? 1?
?? #(x ? ?? / c1 c2) + ?(x ? ?? / c1 c2)? 1
,
where ? is the concentration hyperparameter of the
Dirichlet prior. The value ? ? 1 can be interpreted
as the number of pseudocounts for a rule.
4 Experiments
In this section we show the results of our experi-
ments with our model. The experimental conditions
are summarized in Table 1, with additional informa-
Experiment Topology Heldout
Latin reconstruction (4.2) 1 la:293
Italian reconstruction (4.2) 1 it:117
Sound changes (4.3) 2 None
Phylogeny selection (4.4) 2, 3, 4 None
Table 1: Conditions under which each of the experiments pre-
sented in this section were performed. The topology indices
correspond to those displayed in Figure 1. Note that by condi-
tional independence, the topology used for Spanish reconstruc-
tion reduces to a chain. The heldout column indicates howmany
words, if any, were heldout for edit distance evaluation, and
from which language.
tion on the specifics of the experiments presented in
Section 4.5. We start with a description of the corpus
we created for these experiments.
4.1 Corpus
In order to train and evaluate our system, we
compiled a corpus of Romance cognate words.
The raw data was taken from three sources: the
wiktionary.org website, a Bible parallel cor-
pus (Resnik et al, 1999) and the Europarl corpus
(Koehn, 2002). From an XML dump of the Wik-
tionary data, we extracted multilingual translations,
which provide a list of word tuples in a large num-
ber of languages, including a few ancient languages.
891
The Europarl and the biblical data were processed
and aligned in the standard way, using combined
GIZA++ alignments (Och and Ney, 2003).
We performed our experiments with four lan-
guages from the Romance family (Latin, Italian,
Spanish, and Portuguese). For each of these lan-
guages, we used a simple in-house rule-based sys-
tem to convert the words into their IPA represen-
tations.2 After augmenting our alignments with
the transitive closure3 of the Europarl, Bible and
Wiktionary data, we filtered out non-cognate words
by thresholding the ratio of edit distance to word
length.4 The preprocessing is constraining in that we
require that all the elements of a tuple to be cognates,
which leaves out a significant portion of the data be-
hind (see the row Full entries in Table 2). However,
our approach relies on this assumption, as there is no
explicit model of non-cognate words. An interest-
ing direction for future work is the joint modeling of
phonology with the determination of the cognates,
but our simpler setting lets us focus on the proper-
ties of the edit model. Moreover, the restriction to
full entries has the side advantage that the Latin bot-
tleneck prevents the introduction of too many neol-
ogisms, which are numerous in the Europarl data, to
the final corpus.
Since we used automatic tools for preparing our
corpus rather than careful linguistic analysis, our
cognate list is much noiser in terms of the pres-
ence of borrowed words and phonemeic transcrip-
tion errors compared to the ones used by previous
approaches (Swadesh, 1955; Dyen et al, 1997). The
benefit of our mechanical preprocessing is that more
cognate data can easily be made available, allowing
us to effectively train richer models. We show in the
rest of this section that our phonological model can
indeed overcome this noise and recover meaningful
patterns from the data.
2The tool and the rules we used are available at
nlp.cs.berkeley.edu/pages/historical.html.
3For example, we would infer from an la-es bible align-
ment confessionem-confesio?n (confession) and an es-it Eu-
roparl alignment confesio?n-confessione that the Latin word con-
fessionem and the Italian word confessione are related.
4To be more precise we keep a tuple (w1, w2, . . . , wp) iff
d(wi,wj)
l?(wi,wj)
? 0.7 for all i, j ? {1, 2, . . . , p}, where l? is the mean
length
|wi|+|wj |
2 and d is the Levenshtein distance.
Name Languages Tuples Word forms
Raw sources of data used to create the corpus
Wiktionary es,pt,la,it 5840 11724
Bible la,es 2391 4782
Europarl es,pt 36905 73773
it,es 39506 78982
Main stages of preprocessing of the corpus
Closure es,pt,la,it 40944 106090
Cognates es,pt,la,it 27996 69637
Full entries es,pt,la,it 586 2344
Table 2: Statistics of the dataset we compiled for the evaluation
of our model. We show the languages represented, the number
of tuples and the number of word forms found in each of the
source of data and pre-processing steps involved in the creation
of the dataset we used to test our model. By full entry, we mean
the number of tuples that are jointly considered cognate by our
preprocessing system and that have a word form known for each
of the languages of interest. These last row forms the dataset
used for our experiments.
Language Baseline Model Improvement
Latin 2.84 2.34 9%
Spanish 3.59 3.21 11%
Table 3: Results of the edit distance experiment. The language
column corresponds to the language held-out for evaluation. We
show the mean edit distance across the evaluation examples.
4.2 Reconstruction of word forms
We ran the system using Topology 1 in Figure 1 to
demonstrate the the system can propose reasonable
reconstructions of Latin word forms on the basis of
modern observations. Half of the Latin words at the
root of the tree were held out, and the (uniform cost)
Levenshtein edit distance from the predicted recon-
struction to the truth was computed. Our baseline is
to pick randomly, for each heldout node in the tree,
an observed neighboring word (i.e. copy one of the
modern forms). We stopped EM after 15 iterations,
and reported the result on a Viterbi derivation using
the parameters obtained. Our model outperformed
this baseline by a 9% relative reduction in average
edit distance. Similarly, reconstruction of modern
forms was also demonstrated, with an improvement
of 11% (see Table 3).
To give a qualitative feel for the operation of the
system (good and bad), consider the example in Fig-
ure 5, taken from this experiment. The Latin dentis
/dEntis/ (teeth) is nearly correctly reconstructed as
/dEntes/, reconciling the appearance of the /j/ in the
892
/dEntis/
/djEntes/ /dEnti/
i ? E
E? j E s ?
Figure 5: An example of a Latin reconstruction given the Span-
ish and Italian word forms.
Spanish and the disappearance of the final /s/ in the
Italian. Note that the /is/ vs. /es/ ending is difficult
to predict in this context (indeed, it was one of the
early distinctions to be eroded in vulgar Latin).
While the uniform-cost edit distance misses im-
portant aspects of phonology (all phoneme substitu-
tions are not equal, for instance), it is parameter-free
and still seems to correlate to a large extent with lin-
guistic quality of reconstruction. It is also superior
to held-out log-likelihood, which fails to penalize er-
rors in the modeling assumptions, and to measuring
the percentage of perfect reconstructions, which ig-
nores the degree of correctness of each reconstructed
word.
4.3 Inference of phonological changes
Another use of our model is to automatically recover
the phonological drift processes between known or
partially known languages. To facilitate evaluation,
we continued in the well-studied Romance evolu-
tionary tree. Again, the root is Latin, but we now add
an additional modern language, Portuguese, and two
additional hidden nodes. One of the nodes charac-
terizes the least common ancestor of modern Span-
ish and Portuguese; the other, the least common an-
cestor of all three modern languages. In Figure 1,
Topology 2, these two nodes are labelled vl (Vulgar
Latin) and ib (Proto-Ibero Romance) respectively.
Since we are omitting many other branches, these
names should not be understood as referring to ac-
tual historical proto-languages, but, at best, to col-
lapsed points representing several centuries of evo-
lution. Nonetheless, the major reconstructed rules
still correspond to well known phenomena and the
learned model generally places them on reasonable
branches.
Figure 6 shows the top four general rules for
each of the evolutionary branches in this experiment,
ranked by the number of times they were used in the
derivations during the last iteration of EM. The la,
es, pt, and it forms are fully observed while the
vl and ib forms are automatically reconstructed.
Figure 6 also shows a specific example of the evolu-
tion of the Latin VERBUM (word/verb), along with
the specific edits employed by the model.
While quantitative evaluation such as measuring
edit distance is helpful for comparing results, it is
also illuminating to consider the plausibility of the
learned parameters in a historical light, which we
do here briefly. In particular, we consider rules on
the branch between la and vl, for which we have
historical evidence. For example, documents such
as the Appendix Probi (Baehrens, 1922) provide in-
dications of orthographic confusions which resulted
from the growing gap between Classical Latin and
Vulgar Latin phonology around the 3rd and 4th cen-
turies AD. The Appendix lists common misspellings
of Latin words, from which phonological changes
can be inferred.
On the la to vl branch, rules for word-final dele-
tion of classical case markers dominate the list (rules
ranks 1 and 3 for deletion of final /s/, ranks 2 and
4 for deletion of final /m/). It is indeed likely that
these were generally eliminated in Vulgar Latin. For
the deletion of the /m/, the Appendix Probi contains
pairs such as PASSIM NON PASSI and OLIM NON
OLI. For the deletion of final /s/, this was observed
in early inscriptions, e.g. CORNELIO for CORNE-
LIOS (Allen, 1989). The frequent leveling of the
distinction between /o/ and /u/ (rules ranked 5 and 6)
can be also be found in the Appendix Probi: COLU-
BER NON COLOBER. Note that in the specific ex-
ample shown, the model lowers the orignal /u/ and
then re-raises it in the pt branch due to a latter pro-
cess along that branch.
Similarily, major canonical rules were discovered
in other branches as well, for example, /v/ to /b/
fortition in Spanish, /s/ to /z/ voicing in Italian,
palatalization along several branches, and so on. Of
course, the recovered words and rules are not per-
fect. For example, reconstructed Ibero /tRinta/ to
Spanish /tReinta/ (thirty) is generated in an odd fash-
ion using rules /e/ to /i/ and /n/ to /in/. Moreover,
even when otherwise reasonable systematic sound
changes are captured, the crudeness of our fixed-
granularity contexts can prevent the true context
893
r ? R / many environmentse ? / #i ? / #t ? d / UNROUNDED UNROUNDED
u ? o / many environmentsv ? b / initial or intervocalict ? t e / ALVEOLAR #z ? s / ROUNDED UNROUNDED
/werbum/ (la)
/verbo/ (vl)
/veRbo/ (ib)
/beRbo/ (es) /veRbu/ (pt)
/vErbo/ (it)
s ? / #m ? /u ? o / many environmentsw ? v / # UNROUNDED
u ? o / ALVEOLAR #e ? E / many environmentsi ? / many environmentsi ? e / ALVEOLAR #
a ? 5 / ALVEOLAR #n ? m / UNROUNDED ALVEOLARo ? u / ALVEOLAR #e ? 1 / BILABIAL ALVEOLAR
m ?u ? ow ? v
r ? R
v ? b o ? u
e ? E
Figure 6: The tree shows the system?s hypothesised derivation of a selected Latin word form, VERBUM (word/verb) into the modern
Spanish, Italian and Portuguese pronunciations. The Latin root and modern leaves were observed while the hidden nodes as well as
all the derivations were obtained using the parameters computed by our model after 15 iterations of EM. Nontrivial rules (i.e. rules
that are not identities) used at each stage are shown along the corresponding edge. The boxes display the top four nontrivial rules
corresponding to each of these evolutionary branches, ordered by the number of time they were applied during the last E round of
sampling. Note that since our natural classes are of fixed granularity, some rules must be redundantly discovered, which tends to
flood the top of the rule lists with duplicates of the top few rules. We summarized such redundancies in the above tables.
from being captured, resulting in either rules apply-
ing with low probability in overly coarse environ-
ments or rules being learned redundantly in overly
fine environments.
4.4 Selection of phylogenies
In this experiment, we show that our model can be
used to select between various topologies of phylo-
genies. We first presented to the algorithm the uni-
versally accepted evolutionary tree corresponding to
the evolution of Latin into Spanish, Portuguese and
Italian (Topology 2 in Figure 1). We estimated the
log-likelihood L? of the data under this topology.
Next, we estimated the log-likelihood L? under two
defective topologies (*Topology 3 and *Topology
4). We recorded the log-likelihood ratio L? ? L?
after the last iteration of EM. Note that the two like-
lihoods are comparable since the complexity of the
two models is the same.5
We obtained a ratio of L? ? L? = ?4458 ?
(?4766) = 307 for Topology 2 versus *Topology
3, and ?4877? (?5125) = 248 for Topology 2 ver-
sus *Topology 4 (the experimental setup is described
in Table 1). As one would hope, this log-likelihood
ratio is positive in both cases, indicating that the sys-
tem prefers the true topology over the wrong ones.
While it may seem, at the first glance, that this re-
sult is limited in scope, knowing the relative arrange-
5If a word was not reachable in one of the topology, it was
ignored in both models for the computation of the likelihoods.
ment of all groups of four nodes is actually sufficient
for constructing a full-fledged phylogenetic tree. In-
deed, quartet-based methods, which have been very
popular in the computational biology community,
are precisely based on this fact (Erdos et al, 1996).
There is a rich literature on this subject and approxi-
mate algorithms exist which are robust to misclassi-
fication of a subset of quartets (Wu et al, 2007).
4.5 More experimental details
This section summarizes the values of the parame-
ters we used in these experiments, their interpreta-
tion, and the effect of setting them to other values.
The Dirichlet prior on the parameters can be in-
terpreted as adding pseudocounts to the correspond-
ing edits. It is an important way of infusing par-
simony into the model by setting the prior of the
self-substitution parameters much higher than that
of the other parameters. We used 6.0 as the prior on
the self-substitution parameters, and for all environ-
ments, 1.1 was divided uniformly across the other
edits. As long as the prior on self-substitution is
kept within this rough order of magnitude, varying
them has a limited effect on our results. We also ini-
tialized the parameters with values that encourage
self-substitutions. Again, the results were robust to
perturbation of initialization as long as the value for
self-substitution dominates the other parameters.
The experiments used two natural classes for
vowels (rounded and unrounded), and six natural
894
classes for consonants, based on the place of ar-
ticulation (alveolar, bilabial, labiodental, palatal,
postalveolar, and velar). We conducted experi-
ments to evaluate the effect of using different natural
classes and found that finer ones can help if enough
data is used for training. We defer the meticulous
study of the optimal granularity to future work, as it
would be a more interesting experiment under a log-
linear model. In such a model, contexts of different
granularities can coexist, whereas such coexistence
is not recognized by the current model, giving rise
to many duplicate rules.
We estimated the bigram phoneme model on the
words in the root languages that were not heldout.
Just as in machine translation, the language model
was found to contribute significantly to reconstruc-
tion performance. We tried to increase the weight of
the language model by exponentiating it to a power,
as is often done in NLP applications, but we did
not find that it had any significant impact on per-
formance.
In the reconstruction experiments, when the data
was not reachable by the model, the word used in
the initialization was used as the prediction, and
the evolution of these words were ignored when re-
estimating the parameters. Words were initialized
by picking at random, for each unobserved node, an
observed node?s corresponding word.
5 Conclusion
We have presented a novel probabilistic model of
diachronic phonology and an associated inference
procedure. Our experiments indicate that our model
is able to both produce accurate reconstructions as
measured by edit distance and identify linguisti-
cally plausible rules that account for the phonologi-
cal changes. We believe that the probabilistic frame-
work we have introduced for diachronic phonology
is promising, and scaling it up to richer phylogenetic
may indeed reveal something insightful about lan-
guage change.
6 Acknowledgement
We would like to thank Bonnie Chantarotwong for
her help with the IPA converter and our reviewers
for their comments. This work was supported by
a FQRNT fellowship to the first author, a NDSEG
fellowship to the second author, NSF grant number
BCS-0631518 to the third author, and a Microsoft
Research New Faculty Fellowship to the fourth au-
thor.
References
W. Sidney Allen. 1989. Vox Latina: The Pronunciation
of Classical Latin. Cambridge University Press.
W.A. Baehrens. 1922. Sprachlicher Kommentar zur
vulga?rlateinischen Appendix Probi. Halle (Saale) M.
Niemeyer.
L. Campbell. 1998. Historical Linguistics. The MIT
Press.
N. Chomsky and M. Halle. 1968. The Sound Pattern of
English. Harper & Row.
I. Dyen, J.B. Kruskal, and P. Black.
1997. FILE IE-DATA1. Available at
http://www.ntu.edu.au/education/langs/ielex/IE-
DATA1.
P. L. Erdos, M. A. Steel, L. A. Szekely, and T. J. Warnow.
1996. Local quartet splits of a binary tree infer all
quartet splits via one dyadic inference rule. Technical
report, DIMACS.
S. N. Evans, D. Ringe, and T. Warnow. 2004. Inference
of divergence times as a statistical inverse problem. In
P. Forster and C. Renfrew, editors, Phylogenetic Meth-
ods and the Prehistory of Languages. McDonald Insti-
tute Monographs.
Joseph Felsenstein. 2003. Inferring Phylogenies. Sin-
auer Associates.
S. Geman and D. Geman. 1984. Stochastic relaxation,
Gibbs distributions, and the Bayesian restoration of
images. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 6:721?741.
R. D. Gray and Q. Atkinson. 2003. Language-tree di-
vergence times support the Anatolian theory of Indo-
European origins. Nature.
John P. Huelsenbeck, Fredrik Ronquist, Rasmus Nielsen,
and Jonathan P. Bollback. 2001. Bayesian inference
of phylogeny and its impact on evolutionary biology.
Science.
P. Koehn. 2002. Europarl: A Multilingual Corpus for
Evaluation of Machine Translation.
G. Kondrak. 2002. Algorithms for Language Recon-
struction. Ph.D. thesis, University of Toronto.
895
S. Li, D. K. Pearl, and H. Doss. 2000. Phylogenetic tree
construction using Markov chain Monte Carlo. Jour-
nal of the American Statistical Association.
Bob Mau and M.A. Newton. 1997. Phylogenetic in-
ference for binary data on dendrograms using markov
chain monte carlo. Journal of Computational and
Graphical Statistics.
L. Nakhleh, D. Ringe, and T. Warnow. 2005. Perfect
phylogenetic networks: A new methodology for re-
constructing the evolutionary history of natural lan-
guages. Language, 81:382?420.
F. J. Och and H. Ney. 2003. A Systematic Comparison of
Various Statistical Alignment Models. Computational
Linguistics, 29:19?51.
P. Resnik, Mari Broman Olsen, and Mona Diab. 1999.
The bible as a parallel corpus: Annotating the ?book of
2000 tongues?. Computers and the Humanities, 33(1-
2):129?153.
D. Ringe, T. Warnow, and A. Taylor. 2002. Indo-
european and computational cladistics. Transactions
of the Philological Society, 100:59?129.
M. Swadesh. 1955. Towards greater accuracy in lex-
icostatistic dating. Journal of American Linguistics,
21:121?137.
A. Venkataraman, J. Newman, and J.D. Patrick. 1997.
A complexity measure for diachronic chinese phonol-
ogy. In J. Coleman, editor, Computational Phonology.
Association for Computational Linguistics.
G. Wu, J. A. You, and G. Lin. 2007. Quartet-based
phylogeny reconstruction with answer set program-
ming. IEEE/ACM Transactions on computational bi-
ology, 4:139?152.
Ziheng Yang and Bruce Rannala. 1997. Bayesian phy-
logenetic inference using dna sequences: A markov
chain monte carlo method. Molecular Biology and
Evolution 14.
896
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 314?323,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Sampling Alignment Structure under a Bayesian Translation Model
John DeNero, Alexandre Bouchard-Co?te? and Dan Klein
Computer Science Department
University of California, Berkeley
{denero, bouchard, klein}@cs.berkeley.edu
Abstract
We describe the first tractable Gibbs sam-
pling procedure for estimating phrase pair
frequencies under a probabilistic model of
phrase alignment. We propose and evalu-
ate two nonparametric priors that successfully
avoid the degenerate behavior noted in previ-
ous work, where overly large phrases mem-
orize the training data. Phrase table weights
learned under our model yield an increase in
BLEU score over the word-alignment based
heuristic estimates used regularly in phrase-
based translation systems.
1 Introduction
In phrase-based translation, statistical knowledge
of translation equivalence is primarily captured by
counts of how frequently various phrase pairs occur
in training bitexts. Since bitexts do not come seg-
mented and aligned into phrase pairs, these counts
are typically gathered by fixing a word alignment
and applying phrase extraction heuristics to this
word-aligned training corpus. Alternatively, phrase
pair frequencies can be learned via a probabilistic
model of phrase alignment, but this approach has
presented several practical challenges.
In this paper, we address the two most signifi-
cant challenges in phrase alignment modeling. The
first challenge is with inference: computing align-
ment expectations under general phrase models is
#P-hard (DeNero and Klein, 2008). Previous phrase
alignment work has sacrificed consistency for effi-
ciency, employing greedy hill-climbing algorithms
and constraining inference with word alignments
(Marcu and Wong, 2002; DeNero et al, 2006; Birch
et al, 2006). We describe a Gibbs sampler that con-
sistently and efficiently approximates expectations,
using only polynomial-time computable operators.
Despite the combinatorial complexity of the phrase
alignment space, our sampled phrase pair expecta-
tions are guaranteed to converge to the true poste-
rior distributions under the model (in theory) and do
converge to effective values (in practice).
The second challenge in learning phrase align-
ments is avoiding a degenerate behavior of the gen-
eral model class: as with many models which can
choose between large and small structures, the larger
structures win out in maximum likelihood estima-
tion. Indeed, the maximum likelihood estimate of
a joint phrase alignment model analyzes each sen-
tence pair as one large phrase with no internal struc-
ture (Marcu andWong, 2002). We describe two non-
parametric priors that empirically avoid this degen-
erate solution.
Fixed word alignments are used in virtually ev-
ery statistical machine translation system, if not to
extract phrase pairs or rules directly, then at least
to constrain the inference procedure for higher-level
models. We estimate phrase translation features
consistently using an inference procedure that is not
constrained by word alignments, or any other heuris-
tic. Despite this substantial change in approach, we
report translation improvements over the standard
word-alignment-based heuristic estimates of phrase
table weights. We view this result as an important
step toward building fully model-based translation
systems that rely on fewer procedural heuristics.
2 Phrase Alignment Model
While state-of-the-art phrase-based translation sys-
tems include an increasing number of features,
translation behavior is largely driven by the phrase
pair count ratios ?(e|f) and ?(f |e). These features
are typically estimated heuristically using the counts
c(?e, f?) of all phrase pairs in a training corpus that
are licensed by word alignments:
?(e|f) =
c(?e, f?)
?
e? c(?e
?, f?)
.
314
Gracias
,
lo
har?
de
muy
buen
grado
.
you
do so
Thank , I shall
gladly
.
you
do so
Thank , I shall
gladly
.
Gracias
,
lo
har?
de
muy
buen
grado
.
(a) example word alignment (b) example phrase alignment
Figure 1: In this corpus example, the phrase
alignment model found the non-literal translation
pair ?gladly, de muy buen grado? while heuristically-
combined word alignment models did not. (a) is a grow-
diag-final-and combined IBM Model 4 word alignment;
(b) is a phrase alignment under our model.
In contrast, a generative model that explicitly
aligns pairs of phrases ?e, f? gives us well-founded
alternatives for estimating phrase pair scores. For
instance, we could use the model?s parameters as
translation features. In this paper, we compute the
expected counts of phrase pairs in the training data
according to our model, and derive features from
these expected counts. This approach endows phrase
pair scores with well-defined semantics relative to a
probabilistic model. Practically, phrase models can
discover high-quality phrase pairs that often elude
heuristics, as in Figure 1. In addition, the model-
based approach fits neatly into the framework of sta-
tistical learning theory for unsupervised problems.
2.1 Generative Model Description
We first describe the symmetric joint model of
Marcu and Wong (2002), which we will extend. A
two-step generative process constructs an ordered
set of English phrases e1:m, an ordered set of for-
eign phrases f1:n, and a phrase-to-phrase alignment
between them, a = {(j, k)} indicating that ?ej , fk?
is an aligned pair.
1. Choose a number of components ` and generate
each of ` phrase pairs independently.
2. Choose an ordering for the phrases in the for-
eign language; the ordering for English is fixed
by the generation order.1
1We choose the foreign to reorder without loss of generality.
In this process, m = n = |a|; all phrases in both
sentences are aligned one-to-one.
We parameterize the choice of ` using a geometric
distribution, denoted PG, with stop parameter p$:
P (`) = PG(`; p$) = p$ ? (1 ? p$)
`?1 .
Each aligned phrase pair ?e, f? is drawn from a
multinomial distribution ?J which is unknown. We
fix a simple distortion model, setting the probability
of a permutation of the foreign phrases proportional
to the product of position-based distortion penalties
for each phrase:
P (a|{?e, f?}) ?
?
a?a
?(a)
?(a = (j, k)) = b|pos(ej)?pos(fk)?s| ,
where pos(?) denotes the word position of the start
of a phrase, and s the ratio of the length of the En-
glish to the length of the foreign sentence. This po-
sitional distortion model was deemed to work best
by Marcu and Wong (2002).
We can now state the joint probability for a
phrase-aligned sentence consisting of ` phrase pairs:
P ({?e, f?}, a) = PG(`; p$)P (a|{?e, f?})
?
?e,f?
?J(?e, f?) .
While this model has several free parameters in ad-
dition to ?J, we fix them to reasonable values to fo-
cus learning on the phrase pair distribution.2
2.2 Unaligned Phrases
Sentence pairs do not always contain equal informa-
tion on both sides, and so we revise the generative
story to include unaligned phrases in both sentences.
When generating each component of a sentence pair,
we first decide whether to generate an aligned phrase
pair or, with probability p?, an unaligned phrase.3
Then, we either generate an aligned phrase pair from
?J or an unaligned phrase from ?N, where ?N is a
multinomial over phrases. Now, when generating
e1:m, f1:n and alignment a, the number of phrases
m+ n can be greater than 2 ? |a|.
2Parameters were chosen by hand during development on a
small training corpus. p$ = 0.1, b = 0.85 in experiments.
3We strongly discouraged unaligned phrases in order to
align as much of the corpus as possible: p? = 10?10 in ex-
periments.
315
To unify notation, we denote unaligned phrases as
phrase pairs with one side equal to null: ?e, null? or
?null, f?. Then, the revised model takes the form:
P ({?e, f?},a) = PG(`; p$)P (a|{?e, f?})
?
?e,f?
PM(?e, f?)
PM(?e, f?) = p??N(?e, f?) + (1 ? p?)?J(?e, f?) .
In this definition, the distribution ?N gives non-
zero weight only to unaligned phrases of the form
?e, null? or ?null, f?, while ?J gives non-zero
weight only to aligned phrase pairs.
3 Model Training and Expectations
Our model involves observed sentence pairs, which
in aggregate we can call x, latent phrase segmenta-
tions and alignments, which we can call z, and pa-
rameters ?J and ?N, which together we can call ?.
A model such as ours could be used either for the
learning of the key phrase pair parameters in ?, or
to compute expected counts of phrase pairs in our
data. These two uses are very closely related, but
we focus on the computation of phrase pair expecta-
tions. For exposition purposes, we describe a Gibbs
sampling algorithm for computing expected counts
of phrases under P (z|x, ?) for fixed ?. Such ex-
pectations would be used, for example, to compute
maximum likelihood estimates in the E-step of EM.
In Section 4, we instead compute expectations under
P (z|x), with ? marginalized out entirely.
In a Gibbs sampler, we start with a complete
phrase segmentation and alignment, state z0, which
sets all latent variables to some initial configuration.
We then produce a sequence of sample states zi,
each of which differs from the last by some small
local change. The samples zi are guaranteed (in the
limit) to consistently approximate the conditional
distribution P (z|x, ?) (or P (z|x) later). Therefore,
the average counts of phrase pairs in the samples
converge to expected counts under the model. Nor-
malizing these expected counts yields estimates for
the features ?(e|f) and ?(f |e).
Gibbs sampling is not new to the natural language
processing community (Teh, 2006; Johnson et al,
2007). However, it is usually used as a search pro-
cedure akin to simulated annealing, rather than for
approximating expectations (Goldwater et al, 2006;
Finkel et al, 2007). Our application is also atypical
for an NLP application in that we use an approxi-
mate sampler not only to include Bayesian prior in-
formation (section 4), but also because computing
phrase alignment expectations exactly is a #P-hard
problem (DeNero and Klein, 2008). That is, we
could not run EM exactly, even if we wanted maxi-
mum likelihood estimates.
3.1 Related Work
Expected phrase pair counts under P (z|x, ?) have
been approximated before in order to run EM.
Marcu and Wong (2002) employed local search
from a heuristic initialization and collected align-
ment counts during a hill climb through the align-
ment space. DeNero et al (2006) instead proposed
an exponential-time dynamic program pruned using
word alignments. Subsequent work has relied heav-
ily on word alignments to constrain inference, even
under reordering models that admit polynomial-time
E-steps (Cherry and Lin, 2007; Zhang et al, 2008).
None of these approximations are consistent, and
they offer no method of measuring their biases.
Gibbs sampling is not only consistent in the limit,
but also allows us to add Bayesian priors conve-
niently (section 4). Of course, sampling has liabili-
ties as well: we do not know in advance how long we
need to run the sampler to approximate the desired
expectations ?closely enough.?
Snyder and Barzilay (2008) describe a Gibbs sam-
pler for a bilingual morphology model very similar
in structure to ours. However, the basic sampling
step they propose ? resampling all segmentations
and alignments for a sequence at once ? requires a
#P-hard computation. While this asymptotic com-
plexity was apparently not prohibitive in the case of
morphological alignment, where the sequences are
short, it is prohibitive in phrase alignment, where the
sentences are often very long.
3.2 Sampling with the SWAP Operator
Our Gibbs sampler repeatedly applies each of five
operators to each position in each training sentence
pair. Each operator freezes all of the current state zi
except a small local region, determines all the ways
that region can be reconfigured, and then chooses a
(possibly) slightly different zi+1 from among those
outcomes according to the conditional probability of
each, given the frozen remainder of the state. This
316
frozen region of the state is called a Markov blanket
(denoted m), and plays a critical role in proving the
correctness of the sampler.
The first operator we consider is SWAP, which
changes alignments but not segmentations. It freezes
the set of phrases, then picks two English phrases e1
and e2 (or two foreign phrases, but we focus on the
English case). All alignments are frozen except the
phrase pairs ?e1, f1? and ?e2, f2?. SWAP chooses be-
tween keeping ?e1, f1? and ?e2, f2? aligned as they
are (outcome o0), or swapping their alignments to
create ?e1, f2? and ?e2, f1? (outcome o1).
SWAP chooses stochastically in proportion to
each outcome?s posterior probability: P (o0|m,x, ?)
and P (o1|m,x, ?). Each phrase pair in each out-
come contributes to these posteriors the probability
of adding a new pair, deciding whether it is null, and
generating the phrase pair along with its contribu-
tion to the distortion probability. This is all captured
in a succinct potential function ?(?e, f?) =
{
(1?p$) (1?p?) ?J(?e, f?) ?(?e, f?) e & f non-null
(1?p$) ? p? ? ?N(?e, f?) otherwise
.
Thus, outcome o0 is chosen with probability
P (o0|m,x, ?) =
?(?e1, f1?)?(?e2, f2?)
?(?e1, f1?)?(?e2, f2?) + ?(?e1, f2?)?(?e2, f1?)
.
Operators in a Gibbs sampler require certain con-
ditions to guarantee the correctness of the sampler.
First, they must choose among all possible configu-
rations of the unfrozen local state. Second, imme-
diately re-applying the operator from any outcome
must yield the same set of outcome options as be-
fore.4 If these conditions are not met, the sampler
may no longer be guaranteed to yield consistent ap-
proximations of the posterior distribution.
A subtle issue arises with SWAP as defined:
should it also consider an outcome o2 of ?e1, null?
and ?e2, null? that removes alignments? No part
of the frozen state is changed by removing these
alignments, so the first Gibbs condition dictates that
we must include o2. However, after choosing o2,
when we reapply the operator to positions e1 and
4These are two sufficient conditions to guarantee that the
Metropolis-Hastings acceptance ratio of the sampling step is 1.
(b) FLIP(a) SWAP
(c) TOGGLE
(d) FLIP TWO
(e) MOVE
Figure 2: Each local operator manipulates a small portion
of a single alignment. Relevant phrases are exaggerated
for clarity. The outcome sets (depicted by arrows) of each
possible configuration are fully connected. Certain con-
figurations cannot be altered by certain operators, such as
the final configuration in SWAP. Unalterable configura-
tions for TOGGLE have been omitted for space.
e2, we freeze all alignments except ?e1, null? and
?e2, null?, which prevents us from returning to o0.
Thus, we fail to satisfy the second condition. This
point is worth emphasizing because some prior work
has treated Gibbs sampling as randomized search
and, intentionally or otherwise, proposed inconsis-
tent operators.
Luckily, the problem is not with SWAP, but with
our justification of it: we can salvage SWAP by aug-
menting its Markov blanket. Given that we have se-
lected ?e1, f1? and ?e2, f2?, we not only freeze all
other alignments and phrase boundaries, but also the
number of aligned phrase pairs. With this count held
invariant, o2 is not among the possible outcomes of
SWAP given m. Moreover, regardless of the out-
come chosen, SWAP can immediately be reapplied
at the same location with the same set of outcomes.
All the possible starting configurations and out-
come sets for SWAP appear in Figure 2(a).
317
The boys are
Ellos
comen
Current State
Includes segmentations
and alignments for all
sentence pairs
Markov Blanket
Freezes most of the
segmentations and 
alignments, along with 
the alignment count
Outcomes
An exhaustive set of 
possibilities given 
the Markov blanket
eating
? ?
Apply the FLIP operator 
to English position 1
1
Compute the conditional 
probability of each outcome
2
Finally, select a new state proportional 
to its conditional probability
3
?
Figure 3: The three steps involved in applying the FLIP
operator. The Markov blanket freezes all segmentations
except English position 1 and all alignments except those
for Ellos and The boys. The blanket alo freezes the num-
ber of alignments, which disallows the lower right out-
come.
3.3 The FLIP operator
SWAP can arbitrarily shuffle alignments, but we
need a second operator to change the actual phrase
boundaries. The FLIP operator changes the status of
a single segmentation position5 to be either a phrase
boundary or not. In this sense FLIP is a bilingual
analog of the segmentation boundary flipping oper-
ator of Goldwater et al (2006).
Figure 3 diagrams the operator and its Markov
blanket. First, FLIP chooses any between-word po-
sition in either sentence. The outcome sets for FLIP
vary based on the current segmentation and adjacent
alignments, and are depicted in Figure 2.
Again, for FLIP to satisfy the Gibbs conditions,
we must augment its Markov blanket to freeze not
only all other segmentation points and alignments,
but also the number of aligned phrase pairs. Oth-
erwise, we end up allowing outcomes from which
5A segmentation position is a position between two words
that is also potentially a boundary between two phrases in an
aligned sentence pair.
we cannot return to the original state by reapply-
ing FLIP. Consequently, when a position is already
segmented and both adjacent phrases are currently
aligned, FLIP cannot unsegment the point because
it can?t create two aligned phrase pairs with the one
larger phrase that results (see bottom of Figure 2(b)).
3.4 The TOGGLE operator
Both SWAP and FLIP freeze the number of align-
ments in a sentence. The TOGGLE operator, on the
other hand, can add or remove individual alignment
links. In TOGGLE, we first choose an e1 and f1. If
?e1, f1? ? a or both e1 and f1 are null, we freeze
all segmentations and the rest of the alignments, and
choose between including ?e1, f1? in the alignment
or leaving both e1 and f1 unaligned. If only one of
e1 and f1 are aligned, or they are not aligned to each
other, then TOGGLE does nothing.
3.5 A Complete Sampler
Together, FLIP, SWAP and TOGGLE constitute a
complete Gibbs sampler that consistently samples
from the posterior P (z|x, ?). Not only are these
operators valid Gibbs steps, but they also can form
a path of positive probability from any source state
to any target state in the space of phrase alignments
(formally, the induced Markov chain is irreducible).
Such a path can at worst be constructed by unalign-
ing all phrases in the source state with TOGGLE,
composing applications of FLIP to match the target
phrase boundaries, then applying TOGGLE to match
the target algnments.
We include two more local operators to speed up
the rate at which the sampler explores the hypothesis
space. In short, FLIP TWO simultaneously flips an
English and a foreign segmentation point (to make a
large phrase out of two smaller ones or vice versa),
while MOVE shifts an aligned phrase boundary to
the left or right. We omit details for lack of space.
3.6 Phrase Pair Count Estimation
With our sampling procedure in place, we can now
estimate the expected number of times a given
phrase pair occurs in our data, for fixed ?, using a
Monte-Carlo average,
1
N
N?
i=1
count?e,f?(x, zi)
a.s.
?? E
[
count?e,f?(x, ?)
]
.
318
The left hand side is simple to compute; we count
aligned phrase pairs in each sample we generate.
In practice, we only count phrase pairs after apply-
ing every operator to every position in every sen-
tence (one iteration).6 Appropriate normalizations
of these expected counts can be used either in an M-
step as maximum likelihood estimates, or to com-
pute values for features ?(f |e) and ?(e|f).
4 Nonparametric Bayesian Priors
The Gibbs sampler we presented addresses the infer-
ence challenges of learning phrase alignment mod-
els. With slight modifications, it also enables us to
include prior information into the model. In this sec-
tion, we treat ? as a random variable and shape its
prior distribution in order to correct the well-known
degenerate behavior of the model.
4.1 Model Degeneracy
The structure of our joint model penalizes explana-
tions that use many small phrase pairs. Each phrase
pair token incurs the additional expense of genera-
tion and distortion. In fact, the maximum likelihood
estimate of the model puts mass on ?e, f? pairs that
span entire sentences, explaining the training corpus
with one phrase pair per sentence.
Previous phrase alignment work has primarily
mitigated this tendency by constraining the in-
ference procedure, for example with word align-
ments and linguistic features (Birch et al, 2006),
or by disallowing large phrase pairs using a non-
compositional constraint (Cherry and Lin, 2007;
Zhang et al, 2008). However, the problem lies with
the model, and therefore should be corrected in the
model, rather than the inference procedure.
Model-based solutions appear in the literature as
well, though typically combined with word align-
ment constraints on inference. A sparse Dirichlet
prior coupled with variational EM was explored by
Zhang et al (2008), but it did not avoid the degen-
erate solution. Moore and Quirk (2007) proposed a
new conditional model structure that does not cause
large and small phrases to compete for probabil-
ity mass. May and Knight (2007) added additional
model terms to balance the cost of long and short
derivations in a syntactic alignment model.
6For experiments, we ran the sampler for 100 iterations.
4.2 A Dirichlet Process Prior
We control this degenerate behavior by placing a
Dirichlet process (DP) prior over ?J, the distribution
over aligned phrase pairs (Ferguson, 1973).
If we were to assume a maximum number K of
phrase pair types, a (finite) Dirichlet distribution
would be an appropriate prior. A draw from a K-
dimensional Dirichlet distribution is a list of K real
numbers in [0, 1] that sum to one, which can be in-
terpreted as a distribution overK phrase pair types.
However, since the event space of possible phrase
pairs is in principle unbounded, we instead use a
Dirichlet process. A draw from a DP is a countably
infinite list of real numbers in [0, 1] that sum to one,
which we interpret as a distribution over a countably
infinite list of phrase pair types.7
The Dirichlet distribution and the DP distribution
have similar parameterizations. A K-dimensional
Dirichlet can be parameterized with a concentration
parameter ? > 0 and a base distribution M0 =
(?1, . . . , ?K?1), with ?i ? (0, 1).8 This parameteri-
zation has an intuitive interpretation: under these pa-
rameters, the average of independent samples from
the Dirichlet will converge toM0. That is, the aver-
age of the ith element of the samples will converge
to ?i. Hence, the base distributionM0 characterizes
the sample mean. The concentration parameter ?
only affects the variance of the draws.
Similarly, we can parameterize the Dirichlet pro-
cess with a concentration parameter ? (that affects
only the variance) and a base distribution M0 that
determines the mean of the samples. Just as in the
finite Dirichlet case,M0 is simply a probability dis-
tribution, but now with countably infinite support:
all possible phrase pairs in our case. In practice, we
can use an unnormalized M0 (a base measure) by
appropriately rescaling ?.
In our model, we select a base measure that
strongly prefers shorter phrases, encouraging the
model to use large phrases only when it has suffi-
cient evidence for them. We continue the model:
7Technical note: to simplify exposition, we restrict the dis-
cussion to settings such as ours where the base measure of the
DP has countable support.
8This parametrization is equivalent to the standard pseudo-
counts parametrization of K positive real numbers. The bi-
jection is given by ? =
PK
i=1 ??i and ?i = ??i/?, where
(??1, . . . , ??K) are the pseudo-counts.
319
?J ? DP (M0, ?)
M0(?e, f?) = [Pf (f)PWA(e|f) ? Pe(e)PWA(f |e)]
1
2
Pf (f) = PG(|f |; ps) ?
(
1
nf
)|f |
Pe(e) = PG(|e|; ps) ?
(
1
ne
)|e|
.
.
PWA is the IBM model 1 likelihood of one phrase
conditioned on the other (Brown et al, 1994). Pf
and Pe are uniform over types for each phrase
length: the constants nf and ne denote the vocab-
ulary size of the foreign and English languages, re-
spectively, and PG is a geometric distribution.
Above, ?J is drawn from a DP centered on the ge-
ometric mean of two joint distributions over phrase
pairs, each of which is composed of a monolingual
unigram model and a lexical translation component.
This prior has two advantages. First, we pressure
the model to use smaller phrases by increasing ps
(ps = 0.8 in experiments). Second, we encour-
age good phrase pairs by incorporating IBM Model
1 distributions. This use of word alignment distri-
butions is notably different from lexical weighting
or word alignment constraints: we are supplying
prior knowledge that phrases will generally follow
word alignments, though with enough corpus evi-
dence they need not (and often do not) do so in the
posterior samples. The model proved largely insen-
sitive to changes in the sparsity parameter ?, which
we set to 100 for experiments.
4.3 Unaligned phrases and the DP Prior
Introducing unaligned phrases invites further degen-
erate megaphrase behavior: a sentence pair can be
generated cheaply as two unaligned phrases that
each span an entire sentence. We attempted to place
a similar DP prior over ?N, but surprisingly, this
modeling choice invoked yet another degenerate be-
havior. The DP prior imposes a rich-get-richer prop-
erty over the phrase pair distribution, strongly en-
couraging the model to reuse existing pairs rather
than generate new ones. As a result, common
words consistently aligned to null, even while suit-
able translations were present, simply because each
null alignment reinforced the next. For instance, the
was always unaligned.
Instead, we fix ?N to a simple unigram model that
is uniform over word types. This way, we discour-
age unaligned phrases while focusing learning on ?J.
For simplicity, we reuse Pf (f) and Pe(e) from the
prior over ?J.
?N(?e, f?) =
{
1
2 ? Pe(e) if f = null
1
2 ? Pf (f) if e = null .
The 12 represents a choice of whether the aligned
phrase is in the foreign or English sentence.
4.4 Collapsed Sampling with a DP Prior
Our entire model now has the general form
P (x, z, ?J); all other model parameters have been
fixed. Instead of searching for a suitable ?J,9 we
sample from the posterior distribution P (z|x) with
?J marginalized out.
To this end, we convert our Gibbs sampler into
a collapsed Gibbs sampler10 using the Chinese
Restaurant Process (CRP) representation of the DP
(Aldous, 1985). With the CRP, we avoid the prob-
lem of explicitely representing samples from the
DP. CRP-based samplers have served the commu-
nity well in related language tasks, such as word seg-
mentation and coreference resolution (Goldwater et
al., 2006; Haghighi and Klein, 2007).
Under this representation, the probability of each
sampling outcome is a simple expression in terms
of the state of the rest of the training corpus (the
Markov blanket), rather than explicitly using ?J.
Let zm be the set of aligned phrase pair tokens ob-
served in the rest of the corpus. Then, when ?e, f? is
aligned (that is, neither e nor f are null), the condi-
tional probability for a pair ?e, f? takes the form:
?(?e, f?|zm) =
count?e,f?(zm) + ? ?M0(?e, f?)
|zm| + ?
,
where count?e,f?(zm) is the number of times that
?e, f? appears in zm. We can write this expression
thanks to the exchangeability of the model. For fur-
ther exposition of this collapsed sampler posterior,
9For instance, using approximate MAP EM.
10A collapsed sampler is simply one in which the model pa-
rameters have been marginalized out.
320
025
50
75
100
2007 2008
1 x 1
1 x 2, 2 x 1
2 x 2
2 x 3, 3 x 2
3+ x 3+ 
0
25
50
75
100
1x1 1x2 & 2x1 1x3 & 3x1 2x2 2x3 & 3x2 3x3 and up
Minimal extracted phrases
Sampled phrases
All extracted phrases
Figure 4: The distribution of phrase pair sizes (denoted
English length x foreign length) favors small phrases un-
der the model.
see Goldwater et al (2006).11
The sampler remains exactly the same as de-
scribed in Section 3, except that the posterior con-
ditional probability of each outcome uses a revised
potential function ?DP(?e, f?) =
{
(1?p$) (1?p?) ?(?e, f?) ?(?e, f?) e & f non-null
(1?p$) ? p? ? ?N(?e, f?) otherwise .
?DP is like ?, but the fixed ?J is replaced with the
constantly-updated ? function.
4.5 Degeneracy Analysis
Figure 4 shows a histogram of phrase pair sizes in
the distribution of expected counts under the model.
As reference, we show the size distribution of both
minimal and all phrase pairs extracted from word
alignments using the standard heuristic. Our model
tends to select minimal phrases, only using larger
phrases when well motivated.12
This result alone is important: a model-based
solution with no inference constraint has yielded
a non-degenerate distribution over phrase lengths.
Note that our sampler does find the degenerate solu-
tion quickly under a uniform prior, confirming that
the model, and not the inference procedure, is select-
ing these small phrases.
11Note that the expression for ? changes slightly under con-
ditions where two phrase pairs being changed simultaneously
coincidentally share the same lexical content. Details of these
fringe conditions have been omitted for space, but were in-
cluded in our implementation.
12The largest phrase pair found was 13 English words by 7
Spanish words.
4.6 A Hierarchical Dirichlet Process Prior
We also evaluate a hierarchical Dirichlet process
(HDP) prior over ?J, which draws monolingual dis-
tributions ?E and ?F from a DP and ?J from their
cross-product:
?J ? DP (M
?
0, ?)
M ?0(?e, f?) = [?F(f)PWA(e|f) ? ?E(e)PWA(f |e)]
1
2
?F ? DP (Pf , ?
?)
?E ? DP (Pe, ?
?) .
This prior encourages novel phrase pairs to be com-
posed of phrases that have been used before. In the
sampler, we approximate table counts for ?E and
?F with their expectations, which can be computed
from phrase pair counts (see the appendix of Gold-
water et al (2006) for details). The HDP prior gives
a similar distribution over phrase sizes.
5 Translation Results
We evaluate our new estimates using the baseline
translation pipeline from the 2007 Statistical Ma-
chine Translation Workshop shared task.
5.1 Baseline System
We trained Moses on all Spanish-English Europarl
sentences up to length 20 (177k sentences) using
GIZA++ Model 4 word alignments and the grow-
diag-final-and combination heuristic (Koehn et al,
2007; Och and Ney, 2003; Koehn, 2002), which
performed better than any alternative combination
heuristic.13 The baseline estimates (Heuristic) come
from extracting phrases up to length 7 from the word
alignment. We used a bidirectional lexicalized dis-
tortion model that conditions on both foreign and
English phrases, along with their orientations. Our
5-gram language model was trained on 38.3 million
words of Europarl using Kneser-Ney smoothing. We
report results with and without lexical weighting,
denoted lex.
We tuned and tested on development corpora for
the 2006 translation workshop. The parameters for
each phrase table were tuned separately using min-
imum error rate training (Och, 2003). Results are
13Sampling iteration time scales quadratically with sentence
length. Short sentences were chosen to speed up our experiment
cycle.
321
Phrase Exact
Pair NIST Match
Estimate Count BLEU METEOR
Heuristic 4.4M 29.8 52.4
DP 0.6M 28.8 51.7
HDP 0.3M 29.1 52.0
DP-composed 3.7M 30.1 52.7
HDP-composed 3.1M 30.1 52.6
DP-smooth 4.8M 30.1 52.5
HDP-smooth 4.6M 30.2 52.7
Heuristic + lex 4.4M 30.5 52.9
DP-smooth + lex 4.8M 30.4 53.0
HDP-smooth + lex 4.6M 30.7 53.2
Table 1: BLEU results for learned distributions improve
over a heuristic baseline. Estimate labels are described
fully in section 5.3. The label lex indicates the addition
of a lexical weighting feature.
scored with lowercased, tokenized NIST BLEU, and
exact match METEOR (Papineni et al, 2002; Lavie
and Agarwal, 2007).
The baseline system gives a BLEU score of 29.8,
which increases to 30.5 with lex, as shown in Table
1. For reference, training on all sentences of length
less than 40 (the shared task baseline default) gives
32.4 BLEU with lex.
5.2 Learned Distribution Performance
We initialized the sampler with a configuration de-
rived from the word alignments generated by the
baseline. We greedily constructed a phrase align-
ment from the word alignment by identifying min-
imal phrase pairs consistent with the word align-
ment in each region of the sentence. We then ran
the sampler for 100 iterations through the training
data. Each iteration required 12 minutes under the
DP prior, and 30 minutes under the HDP prior. Total
running time for the HDP model neared two days on
an eight-processor machine with 16 Gb of RAM.
Estimating phrase counts under the DP prior de-
creases BLEU to 28.8, or 29.1 under the HDP prior.
This gap is not surprising: heuristic extraction dis-
covers many more phrase pairs than sampling. Note
that sacrificing only 0.7 BLEU while shrinking the
phrase table by 92% is an appealing trade-off in
resource-constrained settings.
5.3 Increasing Phrase Pair Coverage
The estimates DP-composed and HDP-composed in
Table 1 take expectations of a more liberal count
function. While sampling, we count not only aligned
phrase pairs, but also larger ones composed of two or
more contiguous aligned pairs. This count function
is similar to the phrase pair extraction heuristic, but
never includes unaligned phrases in any way. Expec-
tations of these composite phrases still have a proba-
bilistic interpretation, but they are not the structures
we are directly modeling. Notably, these estimates
outperform the baseline by 0.3 BLEU without ever
extracting phrases from word alignments, and per-
formance increases despite a reduction in table size.
We can instead increase coverage by smooth-
ing the learned estimates with the heuristic counts.
The estimates DP-smooth and HDP-smooth add
counts extracted from word alignments to the sam-
pler?s running totals, which improves performance
by 0.4 BLEU over the baseline. This smoothing bal-
ances the lower-bias sampler counts with the lower-
variance heuristics ones.
6 Conclusion
Our novel Gibbs sampler and nonparametric pri-
ors together address two open problems in learn-
ing phrase alignment models, approximating infer-
ence consistently and efficiently while avoiding de-
generate solutions. While improvements are mod-
est relative to the highly developed word-alignment-
centered baseline, we show for the first time com-
petitive results from a system that uses word align-
ments only for model initialization and smoothing,
rather than inference and estimation. We view this
milestone as critical to eventually developing a clean
probabilistic approach to machine translation that
unifies model structure across both estimation and
decoding, and decreases the use of heuristics.
References
David Aldous. 1985. Exchangeability and related topics.
In E?cole d?e?te? de probabilitie?s de Saint-Flour, Berlin.
Springer.
Alexandra Birch, Chris Callison-Burch, and Miles Os-
borne. 2006. Constraining the phrase-based, joint
probability statistical translation model. In The Con-
322
ference for the Association for Machine Translation in
the Americas.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1994. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, 19:263?311.
Colin Cherry and Dekang Lin. 2007. Inversion transduc-
tion grammar for joint phrasal translation modeling. In
The Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics
Workshop on Syntax and Structure in Statistical Trans-
lation.
John DeNero and Dan Klein. 2008. The complexity of
phrase alignment problems. In The Annual Confer-
ence of the Association for Computational Linguistics:
Short Paper Track.
John DeNero, Dan Gillick, James Zhang, and Dan Klein.
2006. Why generative phrase models underperform
surface heuristics. In The Annual Conference of the
North American Chapter of the Association for Com-
putational Linguistics Workshop on Statistical Ma-
chine Translation.
Thomas S Ferguson. 1973. A bayesian analysis of some
nonparametric problems. In Annals of Statistics.
Jenny Rose Finkel, Trond Grenager, and Christopher D.
Manning. 2007. The infinite tree. In The Annual Con-
ference of the Association for Computational Linguis-
tics.
Sharon Goldwater, Thomas L. Griffiths, and Mark John-
son. 2006. Contextual dependencies in unsupervised
word segmentation. In The Annual Conference of the
Association for Computational Linguistics.
Aria Haghighi and Dan Klein. 2007. Unsupervised
coreference resolution in a nonparametric bayesian
model. In The Annual Conference of the Association
for Computational Linguistics.
Mark Johnson, Thomas Griffiths, and Sharon Goldwa-
ter. 2007. Bayesian inference for PCFGs via Markov
chain Monte Carlo. In The Annual Conference of the
Association for Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In The An-
nual Conference of the Association for Computational
Linguistics.
Philipp Koehn. 2002. Europarl: A multilingual corpus
for evaluation of machine translation.
Alon Lavie and Abhaya Agarwal. 2007. Meteor: An
automatic metric for mt evaluation with high levels
of correlation with human judgments. In The Annual
Conference of the Association for Computational Lin-
guistics Workshop on Statistical Machine Translation.
Daniel Marcu and Daniel Wong. 2002. A phrase-based,
joint probability model for statistical machine trans-
lation. In The Conference on Empirical Methods in
Natural Language Processing.
Jonathan May and Kevin Knight. 2007. Syntactic re-
alignment models for machine translation. In The
Conference on Empirical Methods in Natural Lan-
guage Processing.
Robert Moore and Chris Quirk. 2007. An iteratively-
trained segmentation-free phrase translation model for
statistical machine translation. In The Annual Confer-
ence of the Association for Computational Linguistics
Workshop on Statistical Machine Translation.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29:19?51.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In The Annual Confer-
ence of the Association for Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: A method for automatic eval-
uation of machine translation. In The Annual Confer-
ence of the Association for Computational Linguistics.
Benjamin Snyder and Regina Barzilay. 2008. Unsuper-
vised multilingual learning for morphological segmen-
tation. In The Annual Conference of the Association
for Computational Linguistics.
Yee Whye Teh. 2006. A hierarchical Bayesian language
model based on Pitman-Yor processes. In The Annual
Conference of the Association for Computational Lin-
guistics.
Hao Zhang, Chris Quirk, Robert C. Moore, and
Daniel Gildea. 2008. Bayesian learning of non-
compositional phrases with synchronous parsing. In
The Annual Conference of the Association for Compu-
tational Linguistics.
323
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 65?73,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Improved Reconstruction of Protolanguage Word Forms
Alexandre Bouchard-Co?te?? Thomas L. Griffiths? Dan Klein?
?Computer Science Division ?Department of Psychology
University of California at Berkeley
Berkeley, CA 94720
Abstract
We present an unsupervised approach to re-
constructing ancient word forms. The present
work addresses three limitations of previous
work. First, previous work focused on faith-
fulness features, which model changes be-
tween successive languages. We add marked-
ness features, which model well-formedness
within each language. Second, we introduce
universal features, which support generaliza-
tions across languages. Finally, we increase
the number of languages to which these meth-
ods can be applied by an order of magni-
tude by using improved inference methods.
Experiments on the reconstruction of Proto-
Oceanic, Proto-Malayo-Javanic, and Classical
Latin show substantial reductions in error rate,
giving the best results to date.
1 Introduction
A central problem in diachronic linguistics is the re-
construction of ancient languages from their modern
descendants (Campbell, 1998). Here, we consider
the problem of reconstructing phonological forms,
given a known linguistic phylogeny and known cog-
nate groups. For example, Figure 1 (a) shows a col-
lection of word forms in several Oceanic languages,
all meaning to cry. The ancestral form in this case
has been presumed to be /taNis/ in Blust (1993). We
are interested in models which take as input many
such word tuples, each representing a cognate group,
along with a language tree, and induce word forms
for hidden ancestral languages.
The traditional approach to this problem has been
the comparative method, in which reconstructions
are done manually using assumptions about the rel-
ative probability of different kinds of sound change
(Hock, 1986). There has been work attempting to
automate part (Durham and Rogers, 1969; Eastlack,
1977; Lowe and Mazaudon, 1994; Covington, 1998;
Kondrak, 2002) or all of the process (Oakes, 2000;
Bouchard-Co?te? et al, 2008). However, previous au-
tomated methods have been unable to leverage three
important ideas a linguist would employ. We ad-
dress these omissions here, resulting in a more pow-
erful method for automatically reconstructing an-
cient protolanguages.
First, linguists triangulate reconstructions from
many languages, while past work has been lim-
ited to small numbers of languages. For example,
Oakes (2000) used four languages to reconstruct
Proto-Malayo-Javanic (PMJ) and Bouchard-Co?te? et
al. (2008) used two languages to reconstruct Clas-
sical Latin (La). We revisit these small datasets
and show that our method significantly outperforms
these previous systems. However, we also show that
our method can be applied to a much larger data
set (Greenhill et al, 2008), reconstructing Proto-
Oceanic (POc) from 64 modern languages. In ad-
dition, performance improves with more languages,
which was not the case for previous methods.
Second, linguists exploit knowledge of phonolog-
ical universals. For example, small changes in vowel
height or consonant place are more likely than large
changes, and much more likely than change to ar-
bitrarily different phonemes. In a statistical system,
one could imagine either manually encoding or auto-
matically inferring such preferences. We show that
both strategies are effective.
Finally, linguists consider not only how languages
change, but also how they are internally consistent.
Past models described how sounds do (or, more of-
ten, do not) change between nodes in the tree. To
borrow broad terminology from the Optimality The-
ory literature (Prince and Smolensky, 1993), such
models incorporated faithfulness features, captur-
ing the ways in which successive forms remained
similar to one another. However, each language
has certain regular phonotactic patterns which con-
65
strain these changes. We encode such patterns us-
ing markedness features, characterizing the internal
phonotactic structure of each language. Faithfulness
and markedness play roles analogous to the channel
and language models of a noisy-channel system. We
show that markedness features improve reconstruc-
tion, and can be used efficiently.
2 Related work
Our focus in this section is on describing the prop-
erties of the two previous systems for reconstruct-
ing ancient word forms to which we compare our
method. Citations for other related work, such as
similar approaches to using faithfulness and marked-
ness features, appear in the body of the paper.
In Oakes (2000), the word forms in a given pro-
tolanguage are reconstructed using a Viterbi multi-
alignment between a small number of its descendant
languages. The alignment is computed using hand-
set parameters. Deterministic rules characterizing
changes between pairs of observed languages are ex-
tracted from the alignment when their frequency is
higher than a threshold, and a proto-phoneme inven-
tory is built using linguistically motivated rules and
parsimony. A reconstruction of each observed word
is first proposed independently for each language. If
at least two reconstructions agree, a majority vote
is taken, otherwise no reconstruction is proposed.
This approach has several limitations. First, it is not
tractable for larger trees, since the time complexity
of their multi-alignment algorithm grows exponen-
tially in the number of languages. Second, deter-
ministic rules, while elegant in theory, are not robust
to noise: even in experiments with only four daugh-
ter languages, a large fraction of the words could not
be reconstructed.
In Bouchard-Co?te? et al (2008), a stochastic model
of sound change is used and reconstructions are in-
ferred by performing probabilistic inference over an
evolutionary tree expressing the relationships be-
tween languages. The model does not support gener-
alizations across languages, and has no way to cap-
ture phonotactic regularities within languages. As a
consequence, the resulting method does not scale to
large phylogenies. The work we present here ad-
dresses both of these issues, with a richer model
and faster inference allowing improved reconstruc-
tion and increased scale.
3 Model
We start this section by introducing some notation.
Let ? be a tree of languages, such as the examples
in Figure 3 (c-e). In such a tree, the modern lan-
guages, whose word forms will be observed, are the
leaves of ? . All internal nodes, particularly the root,
are languages whose word forms are not observed.
Let L denote all languages, modern and otherwise.
All word forms are assumed to be strings ?? in the
International Phonological Alphabet (IPA).1
We assume that word forms evolve along the
branches of the tree ? . However, it is not the case
that each cognate set exists in each modern lan-
guage. Formally, we assume there to be a known
list of C cognate sets. For each c ? {1, . . . , C}
let L(c) denote the subset of modern languages that
have a word form in the c-th cognate set. For each
set c ? {1, . . . , C} and each language ` ? L(c), we
denote the modern word form by wc`. For cognate
set c, only the minimal subtree ?(c) containing L(c)
and the root is relevant to the reconstruction infer-
ence problem for that set.
From a high-level perspective, the generative pro-
cess is quite simple. Let c be the index of the cur-
rent cognate set, with topology ?(c). First, a word
is generated for the root of ?(c) using an (initially
unknown) root language model (distribution over
strings). The other nodes of the tree are drawn incre-
mentally as follows: for each edge ` ? `? in ?(c) use
a branch-specific distribution over changes in strings
to generate the word at node `?.
In the remainder of this section, we clarify the ex-
act form of the conditional distributions over string
changes, the distribution over strings at the root, and
the parameterization of this process.
3.1 Markedness and Faithfulness
In Optimality Theory (OT) (Prince and Smolensky,
1993), two types of constraints influence the selec-
tion of a realized output given an input form: faith-
fulness andmarkedness constraints. Faithfulness en-
1The choice of a phonemic representation is motivated by
the fact that most of the data available comes in this form. Dia-
critics are available in a smaller number of languages and may
vary across dialects, so we discarded them in this work.
66
t a
a
?
n g
i
i
s
#
#
#
#
a
?
n
g
/angi/
/a?i/
/ta?i/
/angi/
/a?i/
/ta?i/
?
S
?
I
x
1
x
2
x
3
x
7
y
1
y
2
y
3
y
7
x
4
y
4
y
5
y
6
x
5
x
6
?
n
g
1[Insert]
1[Subst]
1[(n g)@Kw]
1[??g@Kw]
1[??g]
1[(n)@Kw]
1[(g)@Kw]
Language Word form
Proto Oceanic /taNis/
Lau /aNi/
Kwara?ae /angi/
Taiof /taNis/
Table 1: A cognate set from the Austronesian dataset. All
word forms mean to cry.
constrain these changes. We encode such patterns
using markedness features, characterizing the inter-
nal phonotactic structure of each language. Faith-
fulness and marked ess play roles analogous to the
channel and language models of a noisy-channel
system. We show that markedness features greatly
improve reconstruction quality, and we show how to
work with them efficiently.
2 Related Work
Our focus in this section is on describing the prop-
erties of the two previous systems for reconstruct-
ing ancient word forms to which we compare our
method. Citations for other related work, such as
similar approaches to using faithfulness and marked-
ness features, appear in the body of the paper.
In Oakes (2000), the word forms in a given proto-
language are reconstructed using a Viterbi multi-
alignment between a small number of its descendant
languages. The alignment is computed using hand-
set parameters. Deterministic rules characterizing
changes between pairs of observed languages are ex-
tracted from the alignment when their frequency is
higher than a threshold, and a proto-phoneme inven-
tory is built using linguistically motivated rules and
parsimony. A reconstruction of each observed word
is first proposed independently for each language. If
at least two reconstructions agree, a majority vote
is taken, otherwise no reconstruction is proposed.
This approach has several limitations. First, it is
not tractable for larger trees since the complexity of
the multi-alignment algorithm grows exponentially
in the number of languages. Second, determinis-
tic rules, while elegant in theory, are not robust to
noise: even in experiments with only four daughter
languages, a large fraction of the words could not be
reconstructed.
In Bouchard-Co?te? et al (2008), a stochastic model
of sound change is used and reconstructions are in-
ferred by performing probabilistic inference over an
evolutionary tree expressing the relationships be-
tween languages. Use of approximate inference and
stochastic rules addresses some of the limitations of
(Oakes, 2000), but the resulting method is computa-
tionally demanding and consequently does not scale
to large phylogenies. The high computational cost
of probabilistic inference also limits the features that
can be included in the model (omitting global fea-
tures supporting generalizations across languages,
and markedness features within languages). The
work we present here addresses both of these issues,
with faster inference and a richer model allowing in-
creased scale and improved reconstruction.
3 Model
We start this section by introducing some notation.
Let ? be a tree of languages, such as the examples in
Figure 4 (c-e). In such a tree, the modern languages,
whose word forms will be observed, are the leaves
"1 . . . "m. All internal nodes, particularly the root,
are languages " whose word forms are not observed.
Let L denote all languages, modern and otherwise.
All word forms are assumed to be strings ?? in the
International Phonological Alphabet (IPA).1
As a first approximation, we assume that word
forms evolve along the branches of the tree ? . How-
ever, it is not the case that each cognate set exists
in each modern langugage. Formally, we assume
there to be a known list of C cognate sets. For each
c ? {1, . . . , C} let L(c) denote the subset of mod-
ern languages that have a word form in the c-th cog-
nate set. For each set c ? {1, . . . , C} and each lan-
guage " ? L(c), we denote the modern word form
by wc!. For cognate set c, only the minimal subtree
?(c) containing L(c) and the root is relevant to the
reconstruction inference problem for that set.
From a high-level perspective, the generative pro-
cess is quite simple. Let c be the index of the cur-
rent cognate set, with topology ?(c). First, a word
is generated for the root of ?(c) using an (initially
unknown) root language model (distribution over
strings). The other nodes of the tree are drawn in-
crementally as follows: for each edge " ? "? in ?(c)
1The choice of a phonemic representation is motivated by
the fact that most of the data available comes in this form. Dia-
critics are available in a smaller number of languages and may
vary across dialects, so we discarted them in this work.
(a) (b)
(f)
(c)
(d)(e)
..
?
Figure 1: (a) A cognate set from the Austronesian dataset.
All word orms mean to cry. (b-d) The mutation model
used in this paper. (b) The mutation of POc /taNis/ to
Kw. /angi/. (c) Graphical model depicting the dependen-
cie among variables in one step of the mutation Markov
chain. (d) Active features for one step in this process.
(e-f) Comparison of two inference procedures on trees:
Single sequence resampling (e) draws one sequence at a
time, conditio ed on its parent and children, while ances-
try resampling (f) draws an aligned slice from all words
simultaneously. In large trees, the latter is ore efficien
than the former.
courages similarity between the input and output
while markedness favors well-formed output.
Viewed from this perspective, previous comput -
tional approaches to reconstruction are based almost
xclusively n faithf lnes , ex r ssed thr ug a mu-
tation model. Only the words in the language at the
root of the tree, if any, are explicitly encouraged to
be w ll-formed. In ontrast, we incorporate con-
straints on markedness for each language with both
general and branc -specific constraints on faithful-
ness. This is done using a lexicalized stochastic
string transducer (Varadarajan et al, 2008).
We now make precise the conditional distribu-
tions over pairs of evolving strings, referring to Fig-
ure 1 (b-d). Consider a language `? evolving to `
for cognate set c. Assume we have a word form
x = wcl? . The generative process for producing
y = wcl works as follows. First, we consider
x to be composed of characters x1x2 . . . xn, with
the first and last being a special boundary symbol
x1 = # ? ? which is never deleted, mutated, or
created. The process generates y = y1y2 . . . yn in
n chunks yi ? ??, i ? {1, . . . , n}, one for each xi.
The yi?s may be a single character, multiple charac-
ters, or even empty. In the example shown, all three
of these cases occur.
T generat yi, we define a mutation Markov
chain that incrementally adds zero or more charac-
ters to an initially empty yi. First, we decide whether
the current phoneme in the top word t = xi will be
deleted, in which case yi =  as in the example of
/s/ being deleted. If t is not deleted, we chose a sin-
gle substitution character in the bottom word. This
is the case both when /a/ is unchanged and when /N/
substitutes to /n/. We writeS = ??{?} for this set
of outcomes, where ? is the special outcome indi-
cating deletion. Importantly, the probabilities of this
multinomial can depend on both the previous char-
acter gen rated so far (i.e. the rightmost character
p of yi?1) and the current character in the previous
generation string (t). As we will see shortly, this al-
lows modelling markedness and faithfulness at every
branch, jointly. This multinomial decision acts as
the initial distribution of the mutation Markov chain.
We consider insertions only if a deletion was not
selected in the first step. Here, we draw from a
multinomial overS , where this time the special out-
come ? corresponds to stopping insertions, and the
other elements ofS correspond to symbols that are
appende to yi. In this case, the conditioning envi-
ronment is t = xi and the current rightmost symbol
p in yi. Insertions continue until ? is selected. In
the example, w follow the substitution of /N/ to /n/
with an insertion of /g/, followed by a decision to
stop that yi. We will use ?S,t,p,` and ?I,t,p,` to denote
the probabilities ver the substitution and insertion
decisions in the current branch `? ? `.
A similar process generates the word at the root
` of a tree, treating this word as a single string
y1 generated from a dummy ancestor t = x1. In
this case, only the insertion probabilities matter, and
we separately parameterize these probabilities with
?R,t,p,`. There is no actual dependence on t at the
root, but this formulation allows us to unify the pa-
rameterization, with each ??,t,p,` ? R|?|+1 where
? ? {R,S, I}.
3.2 Parameterization
Instead of directly estimating the transition proba-
bilities of the mutation Markov chain (as the param-
eters of a collection of multinomial distributions) we
67
express them as the output of a log-linear model. We
used the following feature templates:
OPERATION identifies whether an operation in the
mutation Markov chain is an insertion, a deletion,
a substitution, a self-substitution (i.e. of the form
x ? y, x = y), or the end of an insertion event.
Examples in Figure 1 (d): 1[Subst] and 1[Insert].
MARKEDNESS consists of language-specific n-
gram indicator functions for all symbols in ?. Only
unigram and bigram features are used for computa-
tional reasons, but we show in Section 5 that this
already captures important constraints. Examples in
Figure 1 (d): the bigram indicator 1[(n g)@Kw] (Kw
stands for Kwara?ae, a language of the Solomon
Islands), the unigram indicators 1[(n)@Kw] and
1[(g)@Kw].
FAITHFULNESS consists of indicators for muta-
tion events of the form 1[x ? y], where x ? ?,
y ? S . Examples: 1[N ? n], 1[N ? n@Kw].
Feature templates similar to these can be found
for instance in Dreyer et al (2008) and Chen (2003),
in the context of string-to-string transduction. Note
also the connection with stochastic OT (Goldwater
and Johnson, 2003; Wilson, 2006), where a log-
linear model mediates markedness and faithfulness
of the production of an output form from an under-
lying input form.
3.3 Parameter sharing
Data sparsity is a significant challenge in protolan-
guage reconstruction. While the experiments we
present here use an order of magnitude more lan-
guages than previous computational approaches, the
increase in observed data also brings with it addi-
tional unknowns in the form of intermediate pro-
tolanguages. Since there is one set of parameters
for each language, adding more data is not sufficient
for increasing the quality of the reconstruction: we
show in Section 5.2 that adding extra languages can
actually hurt reconstruction using previous methods.
It is therefore important to share parameters across
different branches in the tree in order to benefit from
having observations from more languages.
As an example of useful parameter sharing, con-
sider the faithfulness features 1[/p/ ? /b/] and
1[/p/ ? /r/], which are indicator functions for the
appearance of two substitutions for /p/. We would
like the model to learn that the former event (a sim-
ple voicing change) should be preferred over the lat-
ter. In Bouchard-Co?te? et al (2008), this has to be
learned for each branch in the tree. The difficulty is
that not all branches will have enough information
to learn this preference, meaning that we need to de-
fine the model in such a way that it can generalize
across languages.
We used the following technique to address this
problem: we augment the sufficient statistics of
Bouchard-Co?te? et al (2008) to include the current
language (or language at the bottom of the current
branch) and use a single, global weight vector in-
stead of a set of branch-specific weights. Gener-
alization across branches is then achieved by using
features that ignore `, while branch-specific features
depend on `.
For instance, in Figure 1 (d), 1[N ? n] is
an example of a universal (global) feature shared
across all branches while 1[N ? n@Kw] is branch-
specific. Similarly, all of the features in OPERA-
TION, MARKEDNESS and FAITHFULNESS have uni-
versal and branch-specific versions.
3.4 Objective function
Concretely, the transition probabilities of the muta-
tion and root generation are given by:
??,t,p,`(?) = exp{??, f(?, t, p, `, ?)?}Z(?, t, p, `, ?) ? ?(?, t, ?),
where ? ? S , f : {S, I,R}?????L?S ? Rk
is the sufficient statistics or feature function, ??, ??
denotes inner product and ? ? Rk is a weight vector.
Here, k is the dimensionality of the feature space of
the log-linear model. In the terminology of exponen-
tial families, Z and ? are the normalization function
and reference measure respectively:
Z(?, t, p, `, ?) = ?
???S
exp{??, f(?, t, p, `, ??)?}
?(?, t, ?) =
?
???
???
0 if ? = S, t = #, ? 6= #
0 if ? = R, ? = ?
0 if ? 6= R, ? = #
1 o.w.
Here, ? is used to handle boundary conditions.
We will also need the following notation: let
P?(?),P?(?|?) denote the root and branch probabil-ity models described in Section 3.1 (with transition
probabilities given by the above log-linear model),
I(c), the set of internal (non-leaf) nodes in ?(c),
pa(`), the parent of language `, r(c), the root of ?(c)
68
and W (c) = (??)|I(c)|. We can summarize our ob-
jective function as follows:
CX
c=1
log
X
~w?W (c)
P?(wc,r(c))
Y
`?I(c)
P?(wc,`|wc,pa(`)) ? ||?||
2
2
2?2
The second term is a standard L2 regularization
penalty (we used ?2 = 1).
4 Learning algorithm
Learning is done using a Monte Carlo variant of the
Expectation-Maximization (EM) algorithm (Demp-
ster et al, 1977). The M step is convex and com-
puted using L-BFGS (Liu et al, 1989); but the E
step is intractable (Lunter et al, 2003), so we used
a Markov chain Monte Carlo (MCMC) approxima-
tion (Tierney, 1994). At E step t = 1, 2, . . . , we
simulated the chain for O(t) iterations; this regime
is necessary for convergence (Jank, 2005).
In the E step, the inference problem is to com-
pute an expectation under the posterior over strings
in a protolanguage given observed word forms at the
leaves of the tree. The typical approach in biology
or historical linguistics (Holmes and Bruno, 2001;
Bouchard-Co?te? et al, 2008) is to use Gibbs sam-
pling, where the entire string at a single node in the
tree is sampled, conditioned on its parent and chil-
dren. This sampling domain is shown in Figure 1 (e),
where the middle word is completely resampled but
adjacent words are fixed. We will call this method
Single Sequence Resampling (SSR). While concep-
tually simple, this approach suffers from problems
in large trees (Holmes and Bruno, 2001). Con-
sequently, we use a different MCMC procedure,
called Ancestry Resampling (AR) that alleviates
the mixing problems (Figure 1 (f)). This method
was originally introduced for biological applications
(Bouchard-Co?te? et al, 2009), but commonalities be-
tween the biological and linguistic cases make it
possible to use it in our model.
Concretely, the problem with SSR arises when the
tree under consideration is large or unbalanced. In
this case, it can take a long time for information
from the observed languages to propagate to the root
of the tree. Indeed, samples at the root will ini-
tially be independent of the observations. AR ad-
dresses this problem by resampling one thin vertical
slice of all sequences at a time, called an ancestry.
For the precise definition, see Bouchard-Co?te? et al
(2009). Slices condition on observed data, avoiding
the problems mentioned above, and can propagate
information rapidly across the tree.
5 Experiments
We performed a comprehensive set of experiments
to test the new method for reconstruction outlined
above. In Section 5.1, we analyze in isolation the
effects of varying the set of features, the number of
observed languages, the topology, and the number
of iterations of EM. In Section 5.2 we compare per-
formance to an oracle and to three other systems.
Evaluation of all methods was done by computing
the Levenshtein distance (Levenshtein, 1966) be-
tween the reconstruction produced by each method
and the reconstruction produced by linguists. We
averaged this distance across reconstructed words to
report a single number for each method. We show
in Table 2 the average word length in each corpus;
note that the Latin average is much larger, giving
an explanation to the higher errors in the Romance
dataset. The statistical significance of all perfor-
mance differences are assessed using a paired t-test
with significance level of 0.05.
5.1 Evaluating system performance
We used the Austronesian Basic Vocabulary
Database (Greenhill et al, 2008) as the basis for
a series of experiments used to evaluate the per-
formance of our system and the factors relevant to
its success. The database includes partial cognacy
judgments and IPA transcriptions, as well as a few
reconstructed protolanguages. A reconstruction of
Proto-Oceanic (POc) originally developed by Blust
(1993) using the comparative method was the basis
for evaluation.
We used the cognate information provided in
the database, automatically constructing a global
tree2 and set of subtrees from the cognate set in-
dicator matrix M(`, c) = 1[` ? L(c)], c ?
{1, . . . , C}, ` ? L. For constructing the global tree,
we used the implementation of neighbor joining in
the Phylip package (Felsenstein, 1989). We used
a distance based on cognates overlap, dc(`1, `2) =?C
c=1 M(`1, c)M(`2, c). We bootstrapped 1000
2The dataset included a tree, but it was out of date as of
November 2008 (Greenhill et al, 2008).
69
NggelaBugotuTapeAvavaNeveeiNamanNeseSantaAnaNahavaqNatiKwaraaeSol
LauKwameraToloMarshalles
PuloAnnaChuukeseAK
SaipanCaro
Puluwatese
WoleaianPuloAnnan
Carolinian
WoleaiChuukeseNaunaPaameseSou
AnutaVaeakauTau
TakuuTokelauTonganSamoanIfiraMeleM
TikopiaTuvaluNiueFutunaEast
UveaEastRennellese
EmaeKapingamar
SikaianaNukuoroLuangiuaHawaiianMarquesan
TahitianthRurutuanMaoriTuamotuMangareva
Rarotongan
PenrhynRapanuiEas
PukapukaMwotlapMotaFijianBauNamakirNgunaArakiSouth
SaaRagaPeteraraMa
ItEsPtSndJvMadMal POc LaPMJ
Figure 3: Phylogenetic trees for three language families.Clockwise, from the top left: Romance, Austronesian andProto-Malayo-Javanic.
formance of our system and the factors relevant toits success. The database contained, as of Novem-ber 2008, 124,468 lexical items from 587 languagesmostly from the Austronesian language family. Thedatabase includes partial cognacy judgments andIPA transcriptions, as well as a few reconstructedproto-languages. A reconstruction of Proto Oceanic(POc) originally developed by (Blust, 1993) usingthe comparative method was the basis for evaluation.We used the cognate information provided in thedatabase, automatically constructing a global tree2and set of subtrees from the cognate set indicatormatrix M(!, c) = 1[! ? L(c)], c ? {1, . . . , C}, ! ?
L. For constructing the global tree, we used theimplementation of neighbor joining in the Phylippackage (Felsenstein, 1989). The distance ma-trix used the Hamming distance of cognate indi-cators, dc(!1, !2) = ?Cc=1 M(!1, c)M(!2, c). Webootstrapped 1000 samples and formed an accurate(90%) consensus tree. The tree obtained is not bi-nary, but the AR inference algorithm scales linearlyin the branching factor of the tree (in contrast, SSRscales exponentially (Lunter et al, 2003)).The first claim we verified experimentally is thathaving more observed languages aids reconstructionof proto-languages. To test this hypothesis we addedobserved modern languages in increasing order ofdistance dc to the target reconstruction of POc sothat the languages that are most useful for POc re-construction are added first. This prevents the ef-fects of adding a close language after several distant
2The dataset included a tree, but as of November 2008, itwas generated automatically and ?has [not] been updated in awhile.?
0 10 20 30 40 50 60 701.4
1.6
1.8
2
2.2
2.4
2.6
Number of modern languages
Error
Figure 4: Mean distance to the target reconstruction ofproto Oceanic as a function of the number of modern lan-guages used by the inference procedure.
ones being confused with an improvement producedby increasing the number of languages.The results are reported in Figure 4. They con-firm that large-scale inference is desirable for auto-matic proto-language reconstruction: going from 2-to-4, 4-to-8, 8-to-16, 16-to-32 languages all signifi-cantly helped reconstruction. There was still an av-erage edit distance improvement of 0.05 from 32 to64 languages, altough this was not statistically sig-nificant.We then conducted a number of experiments in-tended to assess the robustness of the system, and toidentify the contribution made by different factors itincorporates. First, we ran the system with 20 dif-ferent random seeds and assessed the stability of thesolution found. In each cases, learning was stableand helded performances. See Figure 5.Next, we found that all of the following ablationssignificantly hurts reconstruction: using a flat treein which all languages are equidistant from the re-constructed root and from each other instead of theconsensus tree, dropping the markedness features,disabling sharing across branches and dropping thefaithfulness features. The results of these experi-ments are shown in Table 2.For comparison, we also included in the sametable the performance of a semi-supervised systemtrained by K-fold validation. The system was ran
K time, with disjoint 1 ? K?1 of the POc. wordsgiven to the system (as observations in the graph-
Condition Edit dist.Unsupervised full system 1.87-FAITHFULNESS 2.02-MARKEDNESS 2.18-Sharing 1.99-Topology 2.06Semi-supervised system 1.75
Table 2: Effects of ablation of various aspects of ourunsupervised system on mean edit distance to protoOceanic. -Sharing corresponds to the subset of the fea-tures in OPERATION, FAITHFULNESS and MARKEDNESSthat condition on the current language, -Topolo y corre-sponds to using a flat topology where the only edges inthe tree connect modern languages to proto Oc anic. Thesemi-supervised system i described in text. All dif-ferences (compared to the unsupervised full system) arestatistically significant.
ical model) fo each run. It is semi-supervised inthe sense that gold reconstruction for many internalnodes are not avail bl (such as th common ances-tor of Kw. nd Lau in Fi re 6).3
Figure 6 shows the results of a concrete run over32 languages, zooming in to a pair of the Solomoniclanguages and the cognate set from Table 1. In theexample shown, the reconstruction is as good as theoracle, though off by one character (the final /s/ isnot resent in any of the 32 inputs and thereforeis not reconstructed). The diagrams show, for boththe global and the local features, the expectationsof each substitution superimposed on an IPA soundch rt, as well as a list of the top changes. Darkerlines indicate higher counts. T is run did not usen tural class constraints, but it can be seen that lin-guistically plausibl substitutions are learned. Theglobal features prefer a range of voic ng changes,manner changes, adjace t vowel motion, and so on,including mu ations like /s/ to /h/ which are commonbut poorly repre ented in a naive attribute-based nat-ural class scheme. On the other hand, the features l -cal to the lang ag Kwara?a (Kw.) pick out the sub-set of these change which are active in that branch,such as /s/?/t/ fortition.
3We also tried a fully supervised system where a flat topol-ogy is used so that all of these latent internal nodes are avoided;but it did not perform as well.
0 2 4 6 8 10 12 14 16 18 201.8
2
2.2
2.4
2.6
2.8
3
3.2
3.4
3.6
EM Iteration
Error
Figure 5: Mean distance to the target reconstruction ofPOc as a function of the EM iteration.
5.2 Comparisons against other methods
The first two competing methods, PRAGUE and
BCLKG, are described in Oakes (2000) andBouchard-Co?te? et al (2008) respectively and sum-marized them in Section 1. Neither approach scaleswell to large datasets. In the first case, the bottleneckis the complexity of computing multi-alignmentswithout guide trees and the vanishing probabilitythat independent reconstructions agree. In the sec-ond case, the problem comes from slow mixing ofthe inference algorithm and the unregularized pro-liferation of parameters. For this reason, we built athird baseline that scales well in large datasets.This third baseline, CENTROID, computes thecentroid of the observed word forms in Leven-shtein distance. Let L(x, y) denote the Lev-enshtein distance between word forms x and
y. Ideally, we would like the baseline toreturn argminx????y?O L(x, y), where O ={y1, . . . , y|O|} is the set of observed word forms.Note that the optimum is not changed if we restrictthe minimization to be taken on x ? ?(O)? suchthat m ? |x| ? M where m = mini |yi|,M =maxi |yi| and?(O) is the set of characters occurringin O. Even with this restriction, this optimizationis intractable. As an approximation, we consideredonly strings built by at most k contiguous substringstaken from the word forms in O. If k = 1, then itis equivalent to taking the min over x ? O. At theother end of the spectrum, if k = M , it is exact.This scheme is exponential in k, but since words arerelatively short, we found that k = 2 often finds the
E
r
r
o
r
N. of m d rn lang. EM iteration
100 20300 60
1.4
1.8
2.2
2.6
1.8
2.4
3
3.6
Figure 2: Left: Mean distance to the target reconstruction
of POc as a function of the number of modern languages
used by the inference procedure. Right: Mean distance
and confidenc intervals as a function of th EM it ration,
averag d over 20 random seeds an ran on 4 languages.
samples nd forme a accurate (90%) consen us
tre . The tree obtained is o binary, but the AR
infer ce algorithm scales lin arly in the branching
factor of the tree (in contrast, SSR scale exp nen-
tially (Lunter et al, 2003)).
T e first laim we ver fied experimentally is that
having more observed languages aids reconstruction
of protolanguages. To t t this hypothesis we added
observed mod rn l nguage in increasing order of
distance dc to the target reconstruction of POc so
that the languages that are most useful for POc re-
construction are added first. This prevents the ef-
fects of adding a close language after several distant
ones being confused with an improvement produced
by increasing the number of languages.
The results are reported in Figure 2 (a). They con-
firm that large-scale inference is desirable for au-
tomatic protolanguage reconstruction: reconstruc-
tion improved statistically significantly with each in-
crease except from 32 to 64 languages, where the
average edit distance improvement was 0.05.
We then conducted a number of experiments in-
tended to assess the robustness of the system, and to
identify the contribution made by different factors it
incorporates. First, we ran the system with 20 dif-
ferent random seeds to assess the stability of the so-
lutions found. In each case, learning was stable and
accuracy improved during training. See Figure 2 (b).
Next, we found that all of the following ablations
significantly hurt reconstruction: using a flat tree (in
which all languages are equidistant from the recon-
structed root and from each other) instead of the con-
sensus tree, dropping the markedness features, drop-
Condition Edit dist.
Unsupervised full system 1.87
-FAITHFULNESS 2.02
-MARKEDNESS 2.18
-Sharing 1.99
-Topology 2.06
Semi-supervised system 1.75
Table 1: Effects of ablation of various aspects of our
unsupervised system on mean edit distance to POc.
-Sharing corresponds to the restriction to the subset of the
features in OPERATION, FAITHFULNESS and MARKED-
NESS that are branch-specific, -Topology corresponds to
using a flat topology where the only edges in the tree con-
nect modern languages to POc. The semi-supervised sys-
tem is described in the text. All differences (compared to
the unsupervised full system) are statistically significant.
ping the faithfulness features, and disabling sharing
across branches. The results of these experiments
are shown in Table 1.
For comparison, we also included in the same
table the performance of a semi-supervised system
trained by K-fold validation. The system was ran
K = 5 times, with 1?K?1 of the POc words given
to the system as observations in the graphical model
for each run. It is semi-supervised in the sense that
gold reconstruction for many internal nodes are not
available in the dataset (for example the common an-
cestor of Kwara?ae (Kw.) and Lau in Figure 3 (b)),
so they are still not filled.3
Figure 3 (b) shows the results of a concrete run
over 32 languages, zooming in to a pair of the
Solomonic languages and the cognate set from Fig-
ure 1 (a). In the example shown, the reconstruc-
tion is as good as the ORACLE (described in Sec-
tion 5.2), though off by one character (the final /s/
is not present in any of the 32 inputs and therefore
is not reconstructed). In (a), diagrams show, for
both the global and the local (Kwara?ae) features,
the expectations of each substitution superimposed
on an IPA sound chart, as well as a list of the top
changes. Darker lines indicate higher counts. This
run did not use natural class constraints, but it can
3We also tried a fully supervised system where a flat topol-
ogy is used so that all of these latent internal nodes are avoided;
but it did not perform as well?this is consistent with the
-Topology experiment of Table 1.
70
be seen that linguistically plausible substitutions are
learned. The global features prefer a range of voic-
ing changes, manner changes, adjacent vowel mo-
tion, and so on, including mutations like /s/ to /h/
which are common but poorly represented in a naive
attribute-based natural class scheme. On the other
hand, the features local to the language Kwara?ae
pick out the subset of these changes which are ac-
tive in that branch, such as /s/?/t/ fortition.
5.2 Comparisons against other methods
The first two competing methods, PRAGUE and
BCLKG, are described in Oakes (2000) and
Bouchard-Co?te? et al (2008) respectively and sum-
marized in Section 1. Neither approach scales well
to large datasets. In the first case, the bottleneck is
the complexity of computing multi-alignments with-
out guide trees and the vanishing probability that in-
dependent reconstructions agree. In the second case,
the problem comes from the unregularized prolifera-
tion of parameters and slow mixing of the inference
algorithm. For this reason, we built a third baseline
that scales well in large datasets.
This third baseline, CENTROID, computes the
centroid of the observed word forms in Leven-
shtein distance. Let L(x, y) denote the Lev-
enshtein distance between word forms x and
y. Ideally, we would like the baseline to
return argminx???
?
y?O L(x, y), where O =
{y1, . . . , y|O|} is the set of observed word forms.
Note that the optimum is not changed if we restrict
the minimization to be taken on x ? ?(O)? such
that m ? |x| ? M where m = mini |yi|,M =
maxi |yi| and ?(O) is the set of characters occurring
in O. Even with this restriction, this optimization
is intractable. As an approximation, we considered
only strings built by at most k contiguous substrings
taken from the word forms in O. If k = 1, then it
is equivalent to taking the min over x ? O. At the
other end of the spectrum, if k = M , it is exact.
This scheme is exponential in k, but since words are
relatively short, we found that k = 2 often finds the
same solution as higher values of k. The difference
was in all the cases not statistically significant, so we
report the approximation k = 2 in what follows.
We also compared against an oracle, denoted OR-
ACLE, which returns argminy?OL(y, x?), where x?
is the target reconstruction. We will denote it by OR-
Comparison CENTROID PRAGUE BCLKG
Protolanguage POc PMJ La
Heldout (prop.) 243 (1.0) 79 (1.0) 293 (0.5)
Modern languages 70 4 2
Cognate sets 1321 179 583
Observed words 10783 470 1463
Mean word length 4.5 5.0 7.4
Table 2: Experimental setup: number of held-out proto-
word from (absolute and relative), of modern languages,
cognate sets and total observed words. The split for
BCLKG is the same as in Bouchard-Co?te? et al (2008).
ACLE. This is superior to picking a single closest
language to be used for all word forms, but it is pos-
sible for systems to perform better than the oracle
since it has to return one of the observed word forms.
We performed the comparison against Oakes
(2000) and Bouchard-Co?te? et al (2008) on the same
dataset and experimental conditions as those used in
the respective papers (see Table 2). Note that the
setup of Bouchard-Co?te? et al (2008) provides super-
vision (half of the Latin word forms are provided);
all of the other comparisons are performed in a com-
pletely unsupervised manner.
The PMJ dataset was compiled by Nothofer
(1975), who also reconstructed the corresponding
protolanguage. Since PRAGUE is not guaranteed to
return a reconstruction for each cognate set, only 55
word forms could be directly compared to our sys-
tem. We restricted comparison to this subset of the
data. This favors PRAGUE since the system only pro-
poses a reconstruction when it is certain. Still, our
system outperformed PRAGUE, with an average dis-
tance of 1.60 compared to 2.02 for PRAGUE. The
difference is marginally significant, p = 0.06, partly
due to the small number of word forms involved.
We also exceeded the performance of BCLKG on
the Romance dataset. Our system?s reconstruction
had an edit distance of 3.02 to the truth against 3.10
for BCLKG. However, this difference was not signifi-
cant (p = 0.15). We think this is because of the high
level of noise in the data (the Romance dataset is the
only dataset we consider that was automatically con-
structed rather than curated by linguists). A second
factor contributing to this small difference may be
that the the experimental setup of BCLKG used very
few languages, while the performance of our system
improves markedly with more languages.
71
Nggela
Bugotu
TapeAvava
Neveei
Naman
NeseSantaAna
Nahavaq
NatiKwaraaeSol
LauKwamera
ToloMarshalles
PuloAnna
ChuukeseAK
SaipanCaro
Puluwatese
Woleaian
PuloAnnan
Carolinian
Woleai
Chuukese
Nauna
PaameseSou
AnutaVaeakauTau
Takuu
Tokelau
Tongan
Samoan
IfiraMeleM
Tikopia
Tuvalu
NiueFutunaEast
UveaEast
Rennellese
EmaeKapingamar
Sikaiana
Nukuoro
Luangiua
Hawaiian
Marquesan
Tahitianth
Rurutuan
Maori
Tuamotu
Mangareva
Rarotongan
Penrhyn
RapanuiEas
Pukapuka
Mwotlap
MotaFijianBau
Namakir
Nguna
ArakiSouth
SaaRagaPeteraraMa
ItEsPtSndJvMadMal POc LaPMJ
/a?i/ (Lau)/angi/ (Kw.)
/a?i/
/ta?i/ (POc)
....
Universal
a ?? el ?? rs ?? hk ?? gr ?? l
Kwa
N ?? ng ?? ks ?? te ?? io ?? a
1
Universal
a ?? el ?? rs ?? hk ?? gr ?? l
Kwa
N ?? ng ?? ks ?? te io a
1
? ??
??
?
?
?
? ? ??
?
?
?
???
?
? ?
? ??
?
?
??
?
?
?
?
f
gdb c
n
?
m
j
k
hv
t
s
r
qp ?
z
?
? x
? ??
??
?
?
?
? ? ??
?
?
?
???
?
? ?
? ??
?
?
??
?
?
?
?
f
gdb c
n
?
m
j
k
hv
t
s
r
qp ?
z
?
? x
(a) (b) (c) (d) (e)
Figure 3: (a) A visualization of two learned faithfulness parameters: on the top, from the universal features, on
the bottom, for one particular branch. Each pair of phonemes have a link with grayscale value proportional to the
expectation of a transition between them. The five strongest links are also included at the right. (b) A sample taken
from our POc experiments (see text). (c-e) Phylogenetic trees for three language families: Proto-Malayo-Javanic,
Austronesian and Romance.
We conducted another experiment to verify this
by running both systems in larger trees. Because the
Romance dataset had only three modern languages
transcribed in IPA, we used the Austronesian dataset
to perform the test. The results were all significant in
this setup: while our method went from an edit dis-
tance of 2.01 to 1.79 in the 4-to-8 languages exper-
iment described in Section 5.1, BCLKG went from
3.30 to 3.38. This suggests that more languages can
actually hurt systems that do not support parameter
sharing.
Since we have shown evidence that PRAGUE and
BCLKG do not scale well to large datasets, we
also compared against ORACLE and CENTROID in a
large-scale setting. Specifically, we compare to the
experimental setup on 64 modern languages used to
reconstruct POc described before. Encouragingly,
while the system?s average distance (1.49) does not
attain that of the ORACLE (1.13), we significantly
outperform the CENTROID baseline (1.79).
5.3 Incorporating prior linguistic knowledge
The model also supports the addition of prior lin-
guistic knowledge. This takes the form of feature
templates with more internal structure. We per-
formed experiments with an additional feature tem-
plate:
STRUCT-FAITHFULNESS is a structured version of
FAITHFULNESS, replacing x and y with their natu-
ral classes N?(x) and N?(y) where ? indexes types
of classes, ranging over {manner, place, phonation,
isOral, isCentral, height, backness, roundedness}.
This feature set is reminiscent of the featurized rep-
resentation of Kondrak (2000).
We compared the performance of the system with
and without STRUCT-FAITHFULNESS to check if the
algorithm can recover the structure of natural classes
in an unsupervised fashion. We found that with
2 or 4 observed languages, FAITHFULNESS under-
performed STRUCT-FAITHFULNESS, but for larger
trees, the difference was not significant. FAITH-
FULNESS even slightly outperformed its structured
cousin with 16 observed languages.
6 Conclusion
By enriching our model to include important fea-
tures like markedness, and by scaling up to much
larger data sets than were previously possible, we
obtained substantial improvements in reconstruc-
tion quality, giving the best results on past data
sets. While many more complex phenomena are
still unmodeled, from reduplication to borrowing to
chained sound shifts, the current approach signifi-
cantly increases the power, accuracy, and efficiency
of automatic reconstruction.
Acknowledgments
We would like to thank Anna Rafferty and our re-
viewers for their comments. This work was sup-
ported by a NSERC fellowship to the first author and
NSF grant number BCS-0631518 to the second au-
thor.
72
References
R. Blust. 1993. Central and central-Eastern Malayo-
Polynesian. Oceanic Linguistics, 32:241?293.
A. Bouchard-Co?te?, P. Liang, D. Klein, and T. L. Griffiths.
2008. A probabilistic approach to language change. In
Advances in Neural Information Processing Systems
20.
A. Bouchard-Co?te?, M. I. Jordan, and D. Klein. 2009.
Efficient inference in phylogenetic InDel trees. In Ad-
vances in Neural Information Processing Systems 21.
L. Campbell. 1998. Historical Linguistics. The MIT
Press.
S. F. Chen. 2003. Conditional and joint models for
grapheme-to-phoneme conversion. In Proceedings of
Eurospeech.
M. A. Covington. 1998. Alignment of multiple lan-
guages for historical comparison. In Proceedings of
ACL 1998.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the EM
algorithm. Journal of the Royal Statistical Society. Se-
ries B (Methodological), 39(1):1?38.
M. Dreyer, J. R. Smith, and J. Eisner. 2008. Latent-
variable modeling of string transductions with finite-
state methods. In Proceedings of EMNLP 2008.
S. P. Durham and D. E. Rogers. 1969. An application
of computer programming to the reconstruction of a
proto-language. In Proceedings of the 1969 confer-
ence on Computational linguistics.
C. L. Eastlack. 1977. Iberochange: A program to
simulate systematic sound change in Ibero-Romance.
Computers and the Humanities.
J. Felsenstein. 1989. PHYLIP - PHYLogeny Inference
Package (Version 3.2). Cladistics, 5:164?166.
S. Goldwater and M. Johnson. 2003. Learning OT
constraint rankings using a maximum entropy model.
Proceedings of the Workshop on Variation within Op-
timality Theory.
S. J. Greenhill, R. Blust, and R. D. Gray. 2008. The
Austronesian basic vocabulary database: From bioin-
formatics to lexomics. Evolutionary Bioinformatics,
4:271?283.
H. H. Hock. 1986. Principles of Historical Linguistics.
Walter de Gruyter.
I. Holmes and W. J. Bruno. 2001. Evolutionary HMM:
a Bayesian approach to multiple alignment. Bioinfor-
matics, 17:803?820.
W. Jank. 2005. Stochastic variants of EM: Monte Carlo,
quasi-Monte Carlo and more. In Proceedings of the
American Statistical Association.
G. Kondrak. 2000. A new algorithm for the alignment of
phonetic sequences. In Proceedings of NAACL 2000.
G. Kondrak. 2002. Algorithms for Language Recon-
struction. Ph.D. thesis, University of Toronto.
V. I. Levenshtein. 1966. Binary codes capable of correct-
ing deletions, insertions and reversals. Soviet Physics
Doklady, 10, February.
D. C. Liu, J. Nocedal, and C. Dong. 1989. On the limited
memory BFGS method for large scale optimization.
Mathematical Programming, 45:503?528.
J. B. Lowe and M. Mazaudon. 1994. The reconstruction
engine: a computer implementation of the comparative
method. Comput. Linguist., 20(3):381?417.
G. A. Lunter, I. Miklo?s, Y. S. Song, and J. Hein. 2003.
An efficient algorithm for statistical multiple align-
ment on arbitrary phylogenetic trees. Journal of Com-
putational Biology, 10:869?889.
B. Nothofer. 1975. The reconstruction of Proto-Malayo-
Javanic. M. Nijhoff.
M. P. Oakes. 2000. Computer estimation of vocabu-
lary in a protolanguage from word lists in four daugh-
ter languages. Journal of Quantitative Linguistics,
7(3):233?244.
A. Prince and P. Smolensky. 1993. Optimality theory:
Constraint interaction in generative grammar. Techni-
cal Report 2, Rutgers University Center for Cognitive
Science.
L. Tierney. 1994. Markov chains for exploring posterior
distributions. The Annals of Statistics, 22(4):1701?
1728.
A. Varadarajan, R. K. Bradley, and I. H. Holmes. 2008.
Tools for simulating evolution of aligned genomic re-
gions with integrated parameter estimation. Genome
Biology, 9:R147.
C. Wilson. 2006. Learning phonology with substantive
bias: An experimental and computational study of ve-
lar palatalization. Cognitive Science, 30.5:945?982.
73
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 761?768,
Sydney, July 2006. c?2006 Association for Computational Linguistics
An End-to-End Discriminative Approach to Machine Translation
Percy Liang Alexandre Bouchard-Co?te? Dan Klein Ben Taskar
Computer Science Division, EECS Department
University of California at Berkeley
Berkeley, CA 94720
{pliang, bouchard, klein, taskar}@cs.berkeley.edu
Abstract
We present a perceptron-style discriminative ap-
proach to machine translation in which large feature
sets can be exploited. Unlike discriminative rerank-
ing approaches, our system can take advantage of
learned features in all stages of decoding. We first
discuss several challenges to error-driven discrim-
inative approaches. In particular, we explore dif-
ferent ways of updating parameters given a training
example. We find that making frequent but smaller
updates is preferable to making fewer but larger up-
dates. Then, we discuss an array of features and
show both how they quantitatively increase BLEU
score and how they qualitatively interact on spe-
cific examples. One particular feature we investi-
gate is a novel way to introduce learning into the
initial phrase extraction process, which has previ-
ously been entirely heuristic.
1 Introduction
The generative, noisy-channel paradigm has his-
torically served as the foundation for most of the
work in statistical machine translation (Brown et
al., 1994). At the same time, discriminative meth-
ods have provided substantial improvements over
generative models on a wide range of NLP tasks.
They allow one to easily encode domain knowl-
edge in the form of features. Moreover, param-
eters are tuned to directly minimize error rather
than to maximize joint likelihood, which may not
correspond well to the task objective.
In this paper, we present an end-to-end dis-
criminative approach to machine translation. The
proposed system is phrase-based, as in Koehn et
al. (2003), but uses an online perceptron training
scheme to learn model parameters. Unlike mini-
mum error rate training (Och, 2003), our system is
able to exploit large numbers of specific features
in the same manner as static reranking systems
(Shen et al, 2004; Och et al, 2004). However,
unlike static rerankers, our system does not rely
on a baseline translation system. Instead, it up-
dates based on its own n-best lists. As parameter
estimates improve, the system produces better n-
best lists, which can in turn enable better updates
in future training iterations. In this paper, we fo-
cus on two aspects of the problem of discrimina-
tive translation: the inherent difficulty of learning
from reference translations, and the challenge of
engineering effective features for this task.
Discriminative learning from reference transla-
tions is inherently problematic because standard
discriminative methods need to know which out-
puts are correct and which are not. However, a
proposed translation that differs from a reference
translation need not be incorrect. It may differ
in word choice, literalness, or style, yet be fully
acceptable. Pushing our system to avoid such al-
ternate translations is undesirable. On the other
hand, even if a system produces a reference trans-
lation, it may do so by abusing the hidden struc-
ture (sentence segmentation and alignment). We
can therefore never be entirely sure whether or not
a proposed output is safe to update towards. We
discuss this issue in detail in Section 5, where we
show that conservative updates (which push the
system towards a local variant of the current pre-
diction) are more effective than more aggressive
updates (which try to directly update towards the
reference).
The second major contribution of this work is
an investigation of an array of features for our
model. We show how our features quantitatively
increase BLEU score, as well as how they qual-
itatively interact on specific examples. We first
consider learning weights for individual phrases
and part-of-speech patterns, showing gains from
each. We then present a novel way to parameter-
ize and introduce learning into the initial phrase
extraction process. In particular, we introduce
alignment constellation features, which allow us
to weight phrases based on the word alignment
pattern that led to their extraction. This kind of
761
feature provides a potential way to initially extract
phrases more aggressively and then later down-
weight undesirable patterns, essentially learning a
weighted extraction heuristic. Finally, we use POS
features to parameterize a distortion model in a
limited distortion decoder (Zens and Ney, 2004;
Tillmann and Zhang, 2005). We show that over-
all, BLEU score increases from 28.4 to 29.6 on
French-English.
2 Approach
2.1 Translation as structured classification
Machine translation can be seen as a structured
classification task, in which the goal is to learn
a mapping from an input (French) sentence x to
an output (English) sentence y. Given this setup,
discriminative methods allow us to define a broad
class of features ? that operate on (x,y). For ex-
ample, some features would measure the fluency
of y and others would measure the faithfulness of
y as a translation of x.
However, the translation task in this framework
differs from traditional applications of discrimina-
tive structured classification such as POS tagging
and parsing in a fundamental way. Whereas in
POS tagging, there is a one-to-one correspondence
between the words x and the tags y, the correspon-
dence between x and y in machine translation is
not only much more complex, but is in fact un-
known. Therefore, we introduce a hidden corre-
spondence structure h and work with the feature
vector ?(x,y,h).
The phrase-based model of Koehn et al (2003)
is an instance of this framework. In their model,
the correspondence h consists of (1) the segmen-
tation of the input sentence into phrases, (2) the
segmentation of the output sentence into the same
number of phrases, and (3) a bijection between
the input and output phrases. The feature vec-
tor ?(x,y,h) contains four components: the log
probability of the output sentence y under a lan-
guage model, the score of translating x into y
based on a phrase table, a distortion score, and a
length penalty.1 In Section 6, we vastly increase
the number of features to take advantage of the full
power of discriminative training.
Another example of this framework is the hier-
archical model of Chiang (2005). In this model
the correspondence h is a synchronous parse tree
1More components can be added to the feature vector if
additional language models or phrase tables are available.
over input and output sentences, and features in-
clude the scores of various productions used in the
tree.
Given features ? and a corresponding set of pa-
rameters w, a standard classification rule f is to
return the highest scoring output sentence y, max-
imizing over correspondences h:
f(x;w) = argmax
y,h
w ? ?(x,y,h). (1)
In the phrase-based model, computing the
argmax exactly is intractable, so we approximate
f with beam decoding.
2.2 Perceptron-based training
To tune the parameters w of the model, we use the
averaged perceptron algorithm (Collins, 2002) be-
cause of its efficiency and past success on various
NLP tasks (Collins and Roark, 2004; Roark et al,
2004). In principle, w could have been tuned by
maximizing conditional probability or maximiz-
ing margin. However, these two options require
either marginalization or numerical optimization,
neither of which is tractable over the space of out-
put sentences y and correspondences h. In con-
trast, the perceptron algorithm requires only a de-
coder that computes f(x;w).
Recall the traditional perceptron update rule on
an example (xi,yi) is
w? w + ?(xi,yt)? ?(xi,yp), (2)
where yt = yi is the target output and yp =
f(xi;w) = argmaxyw ? ?(xi,y) is the predic-
tion using the current parameters w.
We adapt this update rule to work with hidden
variables as follows:
w? w + ?(xi,yt,ht)??(xi,yp,hp), (3)
where (yp,hp) is the argmax computation in
Equation 1, and (yt,ht) is the target that we up-
date towards. If (yt,ht) is the same argmax com-
putation with the additional constraint that yt =
yi, then Equation 3 can be interpreted as a Viterbi
approximation to the stochastic gradient
EP (h|xi,yi;w)?(xi,yi,h)?EP (y,h|xi;w)?(xi,y,h)
for the following conditional likelihood objective:
P (yi | xi) ?
?
h
exp(w ? ?(xi,yi,h)).
762
      
    	
 

   
 
 


 




      
	

  

 
  
  
	 
   
      
  
	 
   
	
     

  

 
 

 
	 
  
	
     

  

 
  
 
 	
       
	

	 
   
	
   
 
	


	 
   
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 582?590,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Painless Unsupervised Learning with Features
Taylor Berg-Kirkpatrick Alexandre Bouchard-Co?te? John DeNero Dan Klein
Computer Science Division
University of California at Berkeley
{tberg, bouchard, denero, klein}@cs.berkeley.edu
Abstract
We show how features can easily be added
to standard generative models for unsuper-
vised learning, without requiring complex
new training methods. In particular, each
component multinomial of a generative model
can be turned into a miniature logistic regres-
sion model if feature locality permits. The in-
tuitive EM algorithm still applies, but with a
gradient-based M-step familiar from discrim-
inative training of logistic regression mod-
els. We apply this technique to part-of-speech
induction, grammar induction, word align-
ment, and word segmentation, incorporating
a few linguistically-motivated features into
the standard generative model for each task.
These feature-enhanced models each outper-
form their basic counterparts by a substantial
margin, and even compete with and surpass
more complex state-of-the-art models.
1 Introduction
Unsupervised learning methods have been increas-
ingly successful in recent NLP research. The rea-
sons are varied: increased supplies of unlabeled
data, improved understanding of modeling methods,
additional choices of optimization algorithms, and,
perhaps most importantly for the present work, in-
corporation of richer domain knowledge into struc-
tured models. Unfortunately, that knowledge has
generally been encoded in the form of conditional
independence structure, which means that injecting
it is both tricky (because the connection between
independence and knowledge is subtle) and time-
consuming (because new structure often necessitates
new inference algorithms).
In this paper, we present a range of experiments
wherein we improve existing unsupervised models
by declaratively adding richer features. In particu-
lar, we parameterize the local multinomials of exist-
ing generative models using features, in a way which
does not require complex new machinery but which
still provides substantial flexibility. In the feature-
engineering paradigm, one can worry less about the
backbone structure and instead use hand-designed
features to declaratively inject domain knowledge
into a model. While feature engineering has his-
torically been associated with discriminative, super-
vised learning settings, we argue that it can and
should be applied more broadly to the unsupervised
setting.
The idea of using features in unsupervised learn-
ing is neither new nor even controversial. Many
top unsupervised results use feature-based mod-
els (Smith and Eisner, 2005; Haghighi and Klein,
2006). However, such approaches have presented
their own barriers, from challenging normalization
problems, to neighborhood design, to the need for
complex optimization procedures. As a result, most
work still focuses on the stable and intuitive ap-
proach of using the EM algorithm to optimize data
likelihood in locally normalized, generative models.
The primary contribution of this paper is to
demonstrate the clear empirical success of a sim-
ple and accessible approach to unsupervised learn-
ing with features, which can be optimized by us-
ing standard NLP building blocks. We consider
the same generative, locally-normalized models that
dominate past work on a range of tasks. However,
we follow Chen (2003), Bisani and Ney (2008), and
Bouchard-Co?te? et al (2008), and allow each com-
ponent multinomial of the model to be a miniature
multi-class logistic regression model. In this case,
the EM algorithm still applies with the E-step un-
changed. The M-step involves gradient-based train-
ing familiar from standard supervised logistic re-
gression (i.e., maximum entropy models). By inte-
grating these two familiar learning techniques, we
add features to unsupervised models without any
582
specialized learning or inference.
A second contribution of this work is to show that
further gains can be achieved by directly optimiz-
ing data likelihood with LBFGS (Liu et al, 1989).
This alternative optimization procedure requires no
additional machinery beyond what EM uses. This
approach is still very simple to implement, and we
found that it empirically outperforms EM.
This paper is largely empirical; the underlying op-
timization techniques are known, even if the overall
approach will be novel to many readers. As an em-
pirical demonstration, our results span an array of
unsupervised learning tasks: part-of-speech induc-
tion, grammar induction, word alignment, and word
segmentation. In each task, we show that declaring a
few linguistically motivated feature templates yields
state-of-the-art results.
2 Models
We start by explaining our feature-enhanced model
for part-of-speech (POS) induction. This particular
example illustrates our approach to adding features
to unsupervised models in a well-known NLP task.
We then explain how the technique applies more
generally.
2.1 Example: Part-of-Speech Induction
POS induction consists of labeling words in text
with POS tags. A hidden Markov model (HMM) is a
standard model for this task, used in both a frequen-
tist setting (Merialdo, 1994; Elworthy, 1994) and in
a Bayesian setting (Goldwater and Griffiths, 2007;
Johnson, 2007).
A POS HMM generates a sequence of words in
order. In each generation step, an observed word
emission yi and a hidden successor POS tag zi+1 are
generated independently, conditioned on the current
POS tag zi . This process continues until an absorb-
ing stop state is generated by the transition model.
There are two types of conditional distributions in
the model?emission and transition probabilities?
that are both multinomial probability distributions.
The joint likelihood factors into these distributions:
P?(Y = y,Z = z) = P?(Z1 = z1) ?
|z|
?
i=1
P?(Yi = yi|Zi = zi) ? P?(Zi+1 = zi+1|Zi = zi)
The emission distribution P?(Yi = yi|Zi = zi) is
parameterized by conditional probabilities ?y,z,EMIT
for each word y given tag z. Alternatively, we can
express this emission distribution as the output of a
logistic regression model, replacing the explicit con-
ditional probability table by a logistic function pa-
rameterized by weights and features:
?y,z,EMIT(w) =
exp ?w, f(y, z, EMIT)?
?
y? exp ?w, f(y?, z, EMIT)?
This feature-based logistic expression is equivalent
to the flat multinomial in the case that the feature
function f(y, z, EMIT) consists of all indicator fea-
tures on tuples (y, z, EMIT), which we call BASIC
features. The equivalence follows by setting weight
wy,z,EMIT = log(?y,z,EMIT).1 This formulation is
known as the natural parameterization of the multi-
nomial distribution.
In order to enhance this emission distribution, we
include coarse features in f(y, z, EMIT), in addi-
tion to the BASIC features. Crucially, these features
can be active across multiple (y, z) values. In this
way, the model can abstract general patterns, such
as a POS tag co-occurring with an inflectional mor-
pheme. We discuss specific POS features in Sec-
tion 4.
2.2 General Directed Models
Like the HMM, all of the models we propose are
based on locally normalized generative decisions
that condition on some context. In general, let X =
(Z,Y) denote the sequence of generation steps (ran-
dom variables) where Z contains all hidden random
variables and Y contains all observed random vari-
ables. The joint probability of this directed model
factors as:
Pw(X = x) =
?
i?I
Pw
(
Xi = xi
?
?Xpi(i) = xpi(i)
)
,
where Xpi(i) denotes the parents of Xi and I is the
index set of the variables in X.
In the models that we use, each factor in the above
expression is the output of a local logistic regression
1As long as no transition or emission probabilities are equal
to zero. When zeros are present, for instance to model that an
absorbing stop state can only transition to itself, it is often possi-
ble to absorb these zeros into a base measure. All the arguments
in this paper carry with a structured base measure; we drop it for
simplicity.
583
model parameterized by w:
Pw
`
Xi = d
?
?Xpi(i) = c
?
=
exp?w, f(d, c, t)?
P
d? exp?w, f(d?, c, t)?
Above, d is the generative decision value for Xi
picked by the model, c is the conditioning context
tuple of values for the parents of Xi, and t is the
type of decision being made. For instance, the POS
HMM has two types of decisions: transitions and
emissions. In the emission model, the type t is EMIT,
the decision d is a word and the context c is a tag.
The denominator normalizes the factor to be a prob-
ability distribution over decisions.
The objective function we derive from this model
is the marginal likelihood of the observations y,
along with a regularization term:
L(w) = log Pw(Y = y)? ?||w||22 (1)
This model has two advantages over the more preva-
lent form of a feature-rich unsupervised model, the
globally normalized Markov random field.2 First,
as we explain in Section 3, optimizing our objec-
tive does not require computing expectations over
the joint distribution. In the case of the POS HMM,
for example, we do not need to enumerate an in-
finite sum of products of potentials when optimiz-
ing, in contrast to Haghighi and Klein (2006). Sec-
ond, we found that locally normalized models em-
pirically outperform their globally normalized coun-
terparts, despite their efficiency and simplicity.
3 Optimization
3.1 Optimizing with Expectation Maximization
In this section, we describe the EM algorithm ap-
plied to our feature-rich, locally normalized models.
For models parameterized by standard multinomi-
als, EM optimizes L(?) = log P?(Y = y) (Demp-
ster et al, 1977). The E-step computes expected
counts for each tuple of decision d, context c, and
multinomial type t:
ed,c,t?E?
[
?
i?I
 (Xi =d, Xpi(i) =c, t)
?
?
?
?
Y = y
]
(2)
2The locally normalized model class is actually equivalent
to its globally normalized counterpart when the former meets
the following three conditions: (1) The graphical model is a
directed tree. (2) The BASIC features are included in f . (3) We
do not include regularization in the model (? = 0). This follows
from Smith and Johnson (2007).
These expected counts are then normalized in the
M-step to re-estimate ?:
?d,c,t ?
ed,c,t
?
d? ed?,c,t
Normalizing expected counts in this way maximizes
the expected complete log likelihood with respect to
the current model parameters.
EM can likewise optimize L(w) for our locally
normalized models with logistic parameterizations.
The E-step first precomputes multinomial parame-
ters from w for each decision, context, and type:
?d,c,t(w)?
exp?w, f(d, c, t)?
?
d? exp?w, f(d?, c, t)?
Then, expected counts e are computed accord-
ing to Equation 2. In the case of POS induction,
expected counts are computed with the forward-
backward algorithm in both the standard and logistic
parameterizations. The only change is that the con-
ditional probabilities ? are now functions of w.
The M-step changes more substantially, but still
relies on canonical NLP learning methods. We wish
to choose w to optimize the regularized expected
complete log likelihood:
?(w, e) =
?
d,c,t
ed,c,t log ?d,c,t(w)? ?||w||22 (3)
We optimize this objective via a gradient-based
search algorithm like LBFGS. The gradient with re-
spect to w takes the form
??(w, e) =
?
d,c,t
ed,c,t ??d,c,t(w)? 2? ?w (4)
?d,c,t(w) = f(d, c, t)?
?
d?
?d?,c,t(w)f(d?, c, t)
This gradient matches that of regularized logis-
tic regression in a supervised model: the differ-
ence ? between the observed and expected features,
summed over every decision and context. In the su-
pervised case, we would observe the count of occur-
rences of (d, c, t), but in the unsupervised M-step,
we instead substitute expected counts ed,c,t.
This gradient-based M-step is an iterative proce-
dure. For each different value of w considered dur-
ing the search, we must recompute ?(w), which re-
quires computation in proportion to the size of the
584
parameter space. However, e stays fixed throughout
the M-step. Algorithm 1 outlines EM in its entirety.
The subroutine climb(?, ?, ?) represents a generic op-
timization step such as an LBFGS iteration.
Algorithm 1 Feature-enhanced EM
repeat
Compute expected counts e  Eq. 2
repeat
Compute ?(w, e)  Eq. 3
Compute??(w, e)  Eq. 4
w ? climb(w, ?(w, e),??(w, e))
until convergence
until convergence
3.2 Direct Marginal Likelihood Optimization
Another approach to optimizing Equation 1 is to
compute the gradient of the log marginal likelihood
directly (Salakhutdinov et al, 2003). The gradient
turns out to have the same form as Equation 4, with
the key difference that ed,c,t is recomputed for every
different value of w. Algorithm 2 outlines the proce-
dure. Justification for this algorithm appears in the
Appendix.
Algorithm 2 Feature-enhanced direct gradient
repeat
Compute expected counts e  Eq. 2
Compute L(w)  Eq. 1
Compute ??(w, e)  Eq. 4
w ? climb(w, L(w),??(w, e))
until convergence
In practice, we find that this optimization ap-
proach leads to higher task accuracy for several
models. However, in cases where computing ed,c,t
is expensive, EM can be a more efficient alternative.
4 Part-of-Speech Induction
We now describe experiments that demonstrate the
effectiveness of locally normalized logistic models.
We first use the bigram HMM described in Sec-
tion 2.1 for POS induction, which has two types of
multinomials. For type EMIT, the decisions d are
words and contexts c are tags. For type TRANS, the
decisions and contexts are both tags.
4.1 POS Induction Features
We use the same set of features used by Haghighi
and Klein (2006) in their baseline globally normal-
ized Markov random field (MRF) model. These are
all coarse features on emission contexts that activate
for words with certain orthographic properties. We
use only the BASIC features for transitions. For
an emission with word y and tag z, we use the
following feature templates:
BASIC:  (y = ?, z = ?)
CONTAINS-DIGIT: Check if y contains digit and conjoin
with z:
 (containsDigit(y) = ?, z = ?)
CONTAINS-HYPHEN:  (containsHyphen(x) = ?, z = ?)
INITIAL-CAP: Check if the first letter of y is
capitalized:  (isCap(y) = ?, z = ?)
N-GRAM: Indicator functions for character n-
grams of up to length 3 present in y.
4.2 POS Induction Data and Evaluation
We train and test on the entire WSJ tag corpus (Mar-
cus et al, 1993). We attempt the most difficult ver-
sion of this task where the only information our sys-
tem can make use of is the unlabeled text itself. In
particular, we do not make use of a tagging dictio-
nary. We use 45 tag clusters, the number of POS tags
that appear in the WSJ corpus. There is an identifi-
ability issue when evaluating inferred tags. In or-
der to measure accuracy on the hand-labeled corpus,
we map each cluster to the tag that gives the highest
accuracy, the many-1 evaluation approach (Johnson,
2007). We run all POS induction models for 1000
iterations, with 10 random initializations. The mean
and standard deviation of many-1 accuracy appears
in Table 1.
4.3 POS Induction Results
We compare our model to the basic HMM and a bi-
gram version of the feature-enhanced MRF model of
Haghighi and Klein (2006). Using EM, we achieve
a many-1 accuracy of 68.1. This outperforms the
basic HMM baseline by a 5.0 margin. The same
model, trained using the direct gradient approach,
achieves a many-1 accuracy of 75.5, outperforming
the basic HMM baseline by a margin of 12.4. These
results show that the direct gradient approach can of-
fer additional boosts in performance when used with
a feature-enhanced model. We also outperform the
585
globally normalized MRF, which uses the same set
of features and which we train using a direct gradi-
ent approach.
To the best of our knowledge, our system achieves
the best performance to date on the WSJ corpus for
totally unsupervised POS tagging.3
5 Grammar Induction
We next apply our technique to a grammar induction
task: the unsupervised learning of dependency parse
trees via the dependency model with valence (DMV)
(Klein and Manning, 2004). A dependency parse is
a directed tree over tokens in a sentence. Each edge
of the tree specifies a directed dependency from a
head token to a dependent, or argument token. Thus,
the number of dependencies in a parse is exactly the
number of tokens in the sentence, not counting the
artificial root token.
5.1 Dependency Model with Valence
The DMV defines a probability distribution over de-
pendency parse trees. In this head-outward attach-
ment model, a parse and the word tokens are derived
together through a recursive generative process. For
each token generated so far, starting with the root, a
set of left dependents is generated, followed by a set
of right dependents.
There are two types of multinomial distributions
in this model. The Bernoulli STOP probabilities
?d,c,STOP capture the valence of a particular head. For
this type, the decision d is whether or not to stop
generating arguments, and the context c contains the
current head h, direction ? and adjacency adj. If
a head?s stop probability is high, it will be encour-
aged to accept few arguments. The ATTACH multi-
nomial probability distributions ?d,c,ATTACH capture
attachment preferences of heads. For this type, a de-
cision d is an argument token a, and the context c
consists of a head h and a direction ?.
We take the same approach as previous work
(Klein and Manning, 2004; Cohen and Smith, 2009)
and use gold POS tags in place of words.
3Haghighi and Klein (2006) achieve higher accuracies by
making use of labeled prototypes. We do not use any external
information.
5.2 Grammar Induction Features
One way to inject knowledge into a dependency
model is to encode the similarity between the vari-
ous morphological variants of nouns and verbs. We
encode this similarity by incorporating features into
both the STOP and the ATTACH probabilities. The
attachment features appear below; the stop feature
templates are similar and are therefore omitted.
BASIC:  (a = ?, h = ?, ? = ?)
NOUN: Generalize the morphological variants of
nouns by using isNoun(?):
 (a = ?, isNoun(h) = ?, ? = ?)
 (isNoun(a) = ?, h = ?, ? = ?)
 (isNoun(a) = ?, isNoun(h) = ?, ? = ?)
VERB: Same as above, generalizing verbs instead
of nouns by using isVerb(?)
NOUN-VERB: Same as above, generalizing with
isVerbOrNoun(?) = isVerb(?)? isNoun(?)
BACK-OFF: We add versions of all other features that
ignore direction or adjacency.
While the model has the expressive power to al-
low specific morphological variants to have their
own behaviors, the existence of coarse features en-
courages uniform analyses, which in turn gives bet-
ter accuracies.
Cohen and Smith?s (2009) method has similar
characteristics. They add a shared logistic-normal
prior (SLN) to the DMV in order to tie multinomial
parameters across related derivation events. They
achieve their best results by only tying parame-
ters between different multinomials when the cor-
responding contexts are headed by nouns and verbs.
This observation motivates the features we choose to
incorporate into the DMV.
5.3 Grammar Induction Data and Evaluation
For our English experiments we train and report di-
rected attachment accuracy on portions of the WSJ
corpus. We work with a standard, reduced version of
WSJ, WSJ10, that contains only sentences of length
10 or less after punctuation has been removed. We
train on sections 2-21, and use section 22 as a de-
velopment set. We report accuracy on section 23.
These are the same training, development, and test
sets used by Cohen and Smith (2009). The regular-
ization parameter (?) is tuned on the development
set to maximize accuracy.
For our Chinese experiments, we use the same
corpus and training/test split as Cohen and Smith
586
(2009). We train on sections 1-270 of the Penn Chi-
nese Treebank (Xue et al, 2002), similarly reduced
(CTB10). We test on sections 271-300 of CTB10,
and use sections 400-454 as a development set.
The DMV is known to be sensitive to initializa-
tion. We use the deterministic harmonic initializer
from Klein and Manning (2004). We ran each op-
timization procedure for 100 iterations. The results
are reported in Table 1.
5.4 Grammar Induction Results
We are able to outperform Cohen and Smith?s (2009)
best system, which requires a more complicated
variational inference method, on both English and
Chinese data sets. Their system achieves an accu-
racy of 61.3 for English and an accuracy of 51.9 for
Chinese.4 Our feature-enhanced model, trained us-
ing the direct gradient approach, achieves an accu-
racy of 63.0 for English, and an accuracy of 53.6 for
Chinese. To our knowledge, our method for feature-
based dependency parse induction outperforms all
existing methods that make the same set of condi-
tional independence assumptions as the DMV.
6 Word Alignment
Word alignment is a core machine learning com-
ponent of statistical machine translation systems,
and one of the few NLP tasks that is dominantly
solved using unsupervised techniques. The pur-
pose of word alignment models is to induce a cor-
respondence between the words of a sentence and
the words of its translation.
6.1 Word Alignment Models
We consider two classic generative alignment mod-
els that are both used heavily today, IBM Model 1
(Brown et al, 1994) and the HMM alignment model
(Ney and Vogel, 1996). These models generate a
hidden alignment vector z and an observed foreign
sentence y, all conditioned on an observed English
sentence e. The likelihood of both models takes the
form:
P (y, z|e) =
?
j
p(zj = i|zj?1) ? ?yj ,ei,ALIGN
4Using additional bilingual data, Cohen and Smith (2009)
achieve an accuracy of 62.0 for English, and an accuracy of
52.0 for Chinese, still below our results.
Model Inference Reg Eval
POS Induction ? Many-1
W
SJ
Basic-HMM EM ? 63.1 (1.3)
Feature-MRF LBFGS 0.1 59.6 (6.9)
Feature-HMM EM 1.0 68.1 (1.7)
LBFGS 1.0 75.5 (1.1)
Grammar Induction ? Dir
W
SJ
10
Basic-DMV EM ? 47.8
Feature-DMV EM 0.05 48.3
LBFGS 10.0 63.0
(Cohen and Smith, 2009) 61.3
C
T
B
10
Basic-DMV EM ? 42.5
Feature-DMV EM 1.0 49.9
LBFGS 5.0 53.6
(Cohen and Smith, 2009) 51.9
Word Alignment ? AER
N
IS
T
C
hE
n Basic-Model 1 EM ? 38.0
Feature-Model 1 EM ? 35.6
Basic-HMM EM ? 33.8
Feature-HMM EM ? 30.0
Word Segmentation ? F1
B
R
Basic-Unigram EM ? 76.9 (0.1)
Feature-Unigram EM 0.2 84.5 (0.5)
LBFGS 0.2 88.0 (0.1)
(Johnson and Goldwater, 2009) 87
Table 1: Locally normalized feature-based models outperform
all proposed baselines for all four tasks. LBFGS outperformed
EM in all cases where the algorithm was sufficiently fast to run.
Details of each experiment appear in the main text.
The distortion term p(zj = i|zj?1) is uniform in
Model 1, and Markovian in the HMM. See Liang et
al. (2006) for details on the specific variant of the
distortion model of the HMM that we used. We use
these standard distortion models in both the baseline
and feature-enhanced word alignment systems.
The bilexical emission model ?y,e,ALIGN differen-
tiates our feature-enhanced system from the base-
line system. In the former, the emission model is a
standard conditional multinomial that represents the
probability that decision word y is generated from
context word e, while in our system, the emission
model is re-parameterized as a logistic regression
model and feature-enhanced.
Many supervised feature-based alignment models
have been developed. In fact, this logistic parame-
terization of the HMM has been proposed before and
yielded alignment improvements, but was trained
using supervised estimation techniques (Varea et al,
2002).5 However, most full translation systems to-
5Varea et al (2002) describes unsupervised EM optimiza-
tion with logistic regression models at a high level?their dy-
namic training approach?but provides no experiments.
587
day rely on unsupervised learning so that the models
may be applied easily to many language pairs. Our
approach provides efficient and consistent unsuper-
vised estimation for feature-rich alignment models.
6.2 Word Alignment Features
The BASIC features on pairs of lexical items
provide strong baseline performance. We add
coarse features to the model in order to inject
prior knowledge and tie together lexical items with
similar characteristics.
BASIC:  (e = ?, y = ?)
EDIT-DISTANCE:  (dist(y, e) = ?)
DICTIONARY:  ((y, e) ? D) for dictionary D.
STEM:  (stem(e) = ?, y = ?) for Porter stemmer.
PREFIX:  (prefix(e) = ?, y = ?) for prefixes of
length 4.
CHARACTER:  (e = ?, charAt(y, i) = ?) for index i in
the Chinese word.
These features correspond to several common
augmentations of word alignment models, such as
adding dictionary priors and truncating long words,
but here we integrate them all coherently into a sin-
gle model.
6.3 Word Alignment Data and Evaluation
We evaluate on the standard hand-aligned portion
of the NIST 2002 Chinese-English development set
(Ayan et al, 2005). The set is annotated with sure S
and possible P alignments. We measure alignment
quality using alignment error rate (AER) (Och and
Ney, 2000).
We train the models on 10,000 sentences of FBIS
Chinese-English newswire. This is not a large-scale
experiment, but large enough to be relevant for low-
resource languages. LBFGS experiments are not
provided because computing expectations in these
models is too computationally intensive to run for
many iterations. Hence, EM training is a more ap-
propriate optimization approach: computing the M-
step gradient requires only summing over word type
pairs, while the marginal likelihood gradient needed
for LBFGS requires summing over training sentence
alignments. The final alignments, in both the base-
line and the feature-enhanced models, are computed
by training the generative models in both directions,
combining the result with hard union competitive
thresholding (DeNero and Klein, 2007), and us-
ing agreement training for the HMM (Liang et al,
2006). The combination of these techniques yields
a state-of-the-art unsupervised baseline for Chinese-
English.
6.4 Word Alignment Results
For both IBM Model 1 and the HMM alignment
model, EM training with feature-enhanced models
outperforms the standard multinomial models, by
2.4 and 3.8 AER respectively.6 As expected, large
positive weights are assigned to both the dictionary
and edit distance features. Stem and character fea-
tures also contribute to the performance gain.
7 Word Segmentation
Finally, we show that it is possible to improve upon
the simple and effective word segmentation model
presented in Liang and Klein (2009) by adding
phonological features. Unsupervised word segmen-
tation is the task of identifying word boundaries in
sentences where spaces have been removed. For a
sequence of characters y = (y1, ..., yn), a segmen-
tation is a sequence of segments z = (z1, ..., z|z|)
such that z is a partition of y and each zi is a con-
tiguous subsequence of y. Unsupervised models for
this task infer word boundaries from corpora of sen-
tences of characters without ever seeing examples of
well-formed words.
7.1 Unigram Double-Exponential Model
Liang and Klein?s (2009) unigram double-
exponential model corresponds to a simple
derivational process where sentences of characters
x are generated a word at a time, drawn from a
multinomial over all possible strings ?z,SEGMENT.
For this type, there is no context and the decision is
the particular string generated. In order to avoid the
degenerate MLE that assigns mass only to single
segment sentences it is helpful to independently
generate a length for each segment from a fixed
distribution. Liang and Klein (2009) constrain in-
dividual segments to have maximum length 10 and
generate lengths from the following distribution:
?l,LENGTH = exp(?l1.6) when 1 ? l ? 10. Their
model is deficient since it is possible to generate
6The best published results for this dataset are supervised,
and trained on 17 times more data (Haghighi et al, 2009).
588
lengths that are inconsistent with the actual lengths
of the generated segments. The likelihood equation
is given by:
P (Y = y,Z = z) =
?STOP
|z|
?
i=1
[
(1? ?STOP) ?zi,SEGMENT exp(?|zi|1.6)
]
7.2 Segmentation Data and Evaluation
We train and test on the phonetic version of the
Bernstein-Ratner corpus (1987). This is the same
set-up used by Liang and Klein (2009), Goldwater
et al (2006), and Johnson and Goldwater (2009).
This corpus consists of 9790 child-directed utter-
ances transcribed using a phonetic representation.
We measure segment F1 score on the entire corpus.
We run all word segmentation models for 300 iter-
ations with 10 random initializations and report the
mean and standard deviation of F1 in Table 1.
7.3 Segmentation Features
The SEGMENT multinomial is the important distri-
bution in this model. We use the following features:
BASIC:  (z = ?)
LENGTH:  (length(z) = ?)
NUMBER-VOWELS:  (numVowels(z) = ?)
PHONO-CLASS-PREF:  (prefix(coarsePhonemes(z)) = ?)
PHONO-CLASS-PREF:  (suffix(coarsePhonemes(z)) = ?)
The phonological class prefix and suffix features
project each phoneme of a string to a coarser class
and then take prefix and suffix indicators on the
string of projected characters. We include two ver-
sions of these features that use projections with dif-
ferent levels of coarseness. The goal of these fea-
tures is to help the model learn general phonetic
shapes that correspond to well-formed word bound-
aries.
As is the case in general for our method, the
feature-enhanced unigram model still respects the
conditional independence assumptions that the stan-
dard unigram model makes, and inference is still
performed using a simple dynamic program to com-
pute expected sufficient statistics, which are just seg-
ment counts.
7.4 Segmentation Results
To our knowledge our system achieves the best per-
formance to date on the Bernstein-Ratner corpus,
with an F1 of 88.0. It is substantially simpler than
the non-parametric Bayesian models proposed by
Johnson et al (2007), which require sampling pro-
cedures to perform inference and achieve an F1 of
87 (Johnson and Goldwater, 2009). Similar to our
other results, the direct gradient approach outper-
forms EM for feature-enhanced models, and both
approaches outperform the baseline, which achieves
an F1 of 76.9.
8 Conclusion
We have shown that simple, locally normalized
models can effectively incorporate features into un-
supervised models. These enriched models can
be easily optimized using standard NLP build-
ing blocks. Beyond the four tasks explored in
this paper?POS tagging, DMV grammar induc-
tion, word alignment, and word segmentation?the
method can be applied to many other tasks, for ex-
ample grounded semantics, unsupervised PCFG in-
duction, document clustering, and anaphora resolu-
tion.
Acknowledgements
We thank Percy Liang for making his word segmen-
tation code available to us, and the anonymous re-
viewers for their comments.
Appendix: Optimization
In this section, we derive the gradient of the log marginal likeli-
hood needed for the direct gradient approach. Let w0 be the cur-
rent weights in Algorithm 2 and e = e(w0) be the expectations
under these weights as computed in Equation 2. In order to jus-
tify Algorithm 2, we need to prove that ?L(w0) = ??(w0, e).
We use the following simple lemma: if ?, ? are real-valued
functions such that: (1) ?(w0) = ?(w0) for some w0; (2)
?(w) ? ?(w) on an open set containing w0; and (3), ? and ?
are differentiable at w0; then ??(w0) = ??(w0).
We set ?(w) = L(w) and ?(w) = ?(w, e)?P
z
Pw0(Z =
z|Y = y) log Pw0(Z = z|Y = y). If we can show that ?, ?
satisfy the conditions of the lemma we are done since the second
term of ? depends on w0, but not on w.
Property (3) can be easily checked, and property (2) follows
from Jensen?s inequality. Finally, property (1) follows from
Lemma 2 of Neal and Hinton (1998).
589
References
N. F. Ayan, B. Dorr, and C. Monz. 2005. Combining
word alignments using neural networks. In Empirical
Methods in Natural Language Processing.
N. Bernstein-Ratner. 1987. The phonology of parent-
child speech. K. Nelson and A. van Kleeck.
M. Bisani and H. Ney. 2008. Joint-sequence models for
grapheme-to-phoneme conversion.
A. Bouchard-Co?te?, P. Liang, D. Klein, and T. L. Griffiths.
2008. A probabilistic approach to language change.
In Neural Information Processing Systems.
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and
R. L. Mercer. 1994. The mathematics of statistical
machine translation: Parameter estimation. Computa-
tional Linguistics.
S. F. Chen. 2003. Conditional and joint models for
grapheme-to-phoneme conversion. In Eurospeech.
S. B. Cohen and N. A. Smith. 2009. Shared logistic nor-
mal distributions for soft parameter tying in unsuper-
vised grammar induction. In North American Chapter
of the Association for Computational Linguistics.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the EM
algorithm. Journal of the Royal Statistical Society. Se-
ries B (Methodological).
J. DeNero and D. Klein. 2007. Tailoring word align-
ments to syntactic machine translation. In Association
for Computational Linguistics.
D. Elworthy. 1994. Does Baum-Welch re-estimation
help taggers? In Association for Computational Lin-
guistics.
S. Goldwater and T. L. Griffiths. 2007. A fully Bayesian
approach to unsupervised part-of-speech tagging. In
Association for Computational Linguistics.
S. Goldwater, T. L. Griffiths, and M. Johnson. 2006.
Contextual dependencies in unsupervised word seg-
mentation. In International Conference on Computa-
tional Linguistics/Association for Computational Lin-
guistics.
A. Haghighi and D. Klein. 2006. Prototype-driven learn-
ing for sequence models. In Association for Computa-
tional Linguistics.
A. Haghighi, J. Blitzer, J. DeNero, and D. Klein. 2009.
Better word alignments with supervised ITG models.
In Association for Computational Linguistics.
M. Johnson and S. Goldwater. 2009. Improving non-
parametric Bayesian inference: Experiments on unsu-
pervised word segmentation with adaptor grammars.
In North American Chapter of the Association for
Computational Linguistics.
M. Johnson, T. L. Griffiths, and S. Goldwater. 2007.
Adaptor grammars: a framework for specifying com-
positional nonparametric Bayesian models. In Neural
Information Processing Systems.
M. Johnson. 2007. Why doesnt EM find good HMM
POS-taggers? In Empirical Methods in Natural Lan-
guage Processing/Computational Natural Language
Learning.
D. Klein and C. D. Manning. 2004. Corpus-based in-
duction of syntactic structure: Models of dependency
and constituency. In Association for Computational
Linguistics.
P. Liang and D. Klein. 2009. Online EM for unsuper-
vised models. In North American Chapter of the As-
sociation for Computational Linguistics.
P. Liang, B. Taskar, and D. Klein. 2006. Alignment by
agreement. In North American Chapter of the Associ-
ation for Computational Linguistics.
D. C. Liu, J. Nocedal, and C. Dong. 1989. On the limited
memory BFGS method for large scale optimization.
Mathematical Programming.
M. P. Marcus, M. A. Marcinkiewicz, and B. Santorini.
1993. Building a large annotated corpus of English:
the penn treebank. Computational Linguistics.
B. Merialdo. 1994. Tagging English text with a proba-
bilistic model. Computational Linguistics.
R. Neal and G. E. Hinton. 1998. A view of the EM
algorithm that justifies incremental, sparse, and other
variants. In Learning in Graphical Models. Kluwer
Academic Publishers.
H. Ney and S. Vogel. 1996. HMM-based word alignment
in statistical translation. In International Conference
on Computational Linguistics.
F. J. Och and H. Ney. 2000. Improved statistical align-
ment models. In Association for Computational Lin-
guistics.
R. Salakhutdinov, S. Roweis, and Z. Ghahramani. 2003.
Optimization with EM and expectation-conjugate-
gradient. In International Conference on Machine
Learning.
N. A. Smith and J. Eisner. 2005. Contrastive estimation:
Training log-linear models on unlabeled data. In As-
sociation for Computational Linguistics.
N. A. Smith and M. Johnson. 2007. Weighted and prob-
abilistic context-free grammars are equally expressive.
Computational Linguistics.
I. G. Varea, F. J. Och, H. Ney, and F. Casacuberta. 2002.
Refined lexicon models for statistical machine transla-
tion using a maximum entropy approach. In Associa-
tion for Computational Linguistics.
N. Xue, F-D Chiou, and M. Palmer. 2002. Building a
large-scale annotated Chinese corpus. In International
Conference on Computational Linguistics.
590
