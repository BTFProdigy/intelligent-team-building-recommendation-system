Discriminating the registers and styles in the Modem Greek language 
George Tambouratzis*,  Stella Markantonatou*, Nikolaos Hairetakis*, 
Marina Vassiliou*, Dimitrios Tambouratzis ^,George Carayannis* 
* Institute for Language and Speech Processing 
Epidavrou & Artemidos 6,151 25 Maroussi, Greece 
{giorg_t, marks, nhaire, mvas, gkara} @ilsp.gr 
^ Agricultural University of Athens, 
lera Odos 75, 118 55, Athens, Greece. 
{dtamb@aua.gr} 
Abstract 
This article investigates (a) whether register 
discrimination can successfully exploit linguistic 
information reflecting the evolution of a 
language (such as the diglossia phenomenon of
the Modern Greek language) and (b) what kind 
of linguistic information and which statistical 
techniques may be employed to distinguish 
among individual styles within one register. 
Using clustering techniques and features 
reflecting the diglossia phenomenon, we have 
successfully discriminated registers in Modem 
Greek. However, diglossia information has not 
been shown sufficient to distinguish among 
individual styles within one register. Instead, a 
large number of linguistic features need to be 
studied with methods such as discriminant 
analysis in order to obtain a high degree of 
discrimination accuracy. 
1 Introduction 
The identification of the language style 
characterising the constituent parts of a corpus is 
very important o several appfieations. For 
example, in information retrieval applications, 
where large corpora of texts need to be searched 
efficiently, it is useful to have information about 
the language style used in each text, to improve 
the accuracy of the search (Karlgren, 1999). In 
fact, the criteria regarding language style may 
differ for each search and therefore - due to the 
large number of texts - there is a requirement to
perform style categorisation i an automated 
manner. Such systems normally use statistical 
methods to evaluate the properties of given texts. 
The complexity of the studied properties varies. 
Kilgarriff (1996) employs mainly the frequency- 
of-occurrence of words while Karlgren (1999) 
applies tatistical methods primarily on structural 
and part-of-speech information. 
Baayen et al (1996), who study the topic of 
author identification, apply statistical measures 
and methods on syntactic rewrite rules resulting 
by processing a given set of texts. They report 
that the accuracy thus obtained is higher than 
when applying the same statistical measures to 
the original text. On the other hand, Biber 
(1995) uses Multidimensional Analysis coupled 
with a large number of linguistic features to 
distinguish amongregisters. The underlying idea 
is that, rather than being distinguished on the 
basis of a set of finguistic features, registers are 
distinguished on the basis of combinations of 
weighted linguistic features, the so-called 
"dimensions". 
This article reports on the discrimination oftexts 
in written Modem Greek. The ongoing research 
described here has followed two distinct 
directions. First, we have tried to distinguish 
among registers of written Modern Greek. In a 
second phase, our research has focused on 
distinguishing among individual styles within 
one register and, more specifically, among 
speakers of the Greek Parliament. To achieve 
that, structural, morphological nd part-of-speech 
information is employed. Initially (in section 2) 
emphasis is placed on distinguishing among the 
different registers used. In section 3, the task of 
author identification is tested with selected 
statistical methods. In both sections, we describe 
the set of linguistic features measured, we argue 
for the statistical method employed and we 
comment on the results. Section 4 contains a 
description of future plans for extending this fine 
of research while in section 5 the conclusions of 
this article are provided. 
2 Distinguishing Registers 
To distinguish among registers, we successfully 
exploited a particular feature of Modem Greek, 
namely the contrast between Katharevousa and 
Demotiki. These are variation.,; of Modern Greek 
which correspond (if only roughly) to formal and 
informal speaking. Katharevoma was the official 
language of the Greek State until 1979 when it 
was replaced by Demotiki. By that time, 
Demotiki was the establis\]hed language of 
literature while, in times, it had been the 
language of elementary education. Compared to 
Demotiki, Katharevousa bears an important 
resemblance to Ancient Greek manifested 
explicitly on the morphological level and the use 
of the lexicon. At a second step, we dropped the 
Katharevousa-Demotiki approach and relied on 
part-of-speech information, which is often 
exploited in text categofisafion experiments (for 
instance, see Biber et al 1998). Again, we 
obtained satisfactory esults. 
2.1 Method of work 
The variables used to distinguish among registers 
may be grouped into the following categories: 
1. Morphological variables: These were verbal 
endings quantifying the contrast 
Katharevousa / Demofiki. Although the 
morphological differences between these two 
variations of Greek are not limited to the 
verb paradigm, we focused on the latter since 
it better highlights the contrast under 
consideration (Tainbouratzis et al, 2000). A 
total of 230 verbal endings were selected, 
split into 145 Demotiki and 85 Katharevousa 
endings (see also the Appendix). These 230 
frequencies-of-occurrence were grouped into 
12 variables for use in the, statistical nalysis. 
2. Lexical variables: Certain negation particles 
(ovd~ei?, otu~rere, oo~o4zo6, dryer)clearly 
signify a preference for Katharevousa while 
others (~51Xo~?, #are, Xcopi?) are clear 
indicators of Demotiki. However, the most 
frequently used negation particles (tzt,/a/v, 
~cv) are not characteristic ofeither of the two 
variations. 
3. Structural macro-features: average sentence 
length, number of commas, dashes and 
brackets (total of 4 variables). 
4. After the completion of the experiments with 
variables of type 1-3 (Tambouratzis et al, 
2000), Part-of-Speech (PoS) counts were 
introduced. The PoS categories were 
adjectives, adjunedons, adverbs, articles, 
conjunctions, nouns, pronouns, numerals, 
particles, verbs and a hold-all category (for 
non-classifiable ntries), resulting in 11 
variables expressed as percentages. 
These variables are more similar to the 
characteristics used by Karlgren (1999), and 
differ considerably from those used by Kilgarriff 
(1996) and Baayen et al (1996). For the metrics 
of the first and thkd categories, a custom-built 
program was used running under Linux. This 
program calculated all structural and 
morphological metrics for each text in a single 
pass and the results were processed with the help 
of a spreadsheet package. The metrics of the 
second category were calculated using a custom- 
built program in the C programming language. 
PoS counts were obtained using the ILSP tagger 
(Papageorgiou et al, 2000) coupled with a 
number of custom-built programs to determine 
the actual frequencies-of-occurrence from the 
tagged texts. Finally, the STATGRAPHICS 
package was used for the statistical nalysis. 
The dataset selected consisted of examples from 
three registers: 
(i) fiction (364 Kwords - 24 texts), 
(ii) texts of academic prose referring to 
historical issues, also referred to as the 
history register (361 Kwords - 32 texts) and 
(iii) political speeches obtained from the 
proceedings of the Greek parliament 
sessions, also referred to as the parliament 
register (509 Kwords - 12 texts). 
The texts of registers (I) and (II) were retrieved 
from the ILSP corpus (Gavrilidou et al, 1998), 
all of them dating from the period 1991-1999. 
The texts of register (III) were transcripts of the 
Greek Parliament sessions held during the first 
half of 1999. 
This dataset was processed using both seeded 
and unseeded clustering techniques with between 
3 and 6 clusters. The unseeded approach 
confirmed the existence of distinct natural 
classes, which correspond to the three registers. 
The seeded approach confirmed the ability to 
accurately separate these three registers and to 
cluster their elements together. Initially, a "short" 
data vector containing only the 12 morphological 
variables quantifying the Demofiki/Katharevousa 
contrast was used (Tambouratzis etal. 2000), as 
well as a 16-element vector combining structural 
and morphological characteristics. The seeds for 
the Parliaraent and History registers were chosen 
randomly. The seeds for the Fiction register were 
chosen so that at least one of them would not be 
36 
an "outlier" of the Fiction register. 
Representative results are shown in Table 1 for 
the different vectors and numbers of clusters. In 
each case, the classification rate quoted 
corresponds to the number of text elements 
correctly classified (according to the register of 
the respective seed). 
12-elem. 16-elem. 
6 clusL 95.6% 98.5% 
4 clust. 97.1% 98.5% 
3 clust. 95.6% 97.1% 
Table 1 - Seeded clustering accuracy as a 
function of  the cluster number and vector size. 
The vector size was augmented with PoS 
information, resulting in a 27-element data 
vector. A new set of clustering experiments were 
performed using Ward's method with the 
squared Euclidean distance measure to cluster 
the data in an unseeded manner. Finally, a 15- 
element data vector was used with PoS and 
structural information but without any 
morphological information. The results obtained 
(Table 2) show that PoS information improves 
the clustering performance. 
2.2 Comments on the Results 
Our results strongly suggest hat registers of 
written Modem Greek can be discriminated 
accurately on the basis of the contrast 
Katharevousa / Demotiki manifested with 
morphological variation. Languages with a 
different history may not be suited to such a 
categorisafion method. This is evident in Biber's 
work (1995) for the English language, where a 
variety of grammatical and macro-structural 
linguistic features but no morphological variation 
features were employed. It seems then that 
corpora of languages which are characterised by 
the phenomenon of diglossia, may be 
successfully categorisable on the basis of 
morphological information (or other reflexes of 
diglossia). Such a discrimination method may 
give results as satisfactory as approaches which 
are closer to the Biber (1995) spirit and rely on 
PoS and structural measures (see Tables 1 and 
2). 
Tables 1 and 2 show that the accuracy of 
clustering reaches approximately 99% while the 
seeded clustering approach ad a high degree of 
accuracy, reaching 100% when using 5 clusters. 
For the 27-element vector with both 
morphological and PoS information, perfect 
clustering has been achieved even with 4 
clusters. On the other hand, a successful 
clustering (albeit with a lower level of accuracy) 
is achieved using only structural and PoS 
information. 
It should be noted that the lexical variables used, 
that is the negation particles, did not contribute at 
all (Markantonatou et al, 2000). Furthermore, 
the system performed almost as well with and 
without macro-structure features, the difference 
in accuracy being less than 5%. 
The parliament texts can be claimed to form a 
register whose patterns are closely positioned in 
the pattern space. Of the three registers, the 
literature one presented the highest degree of 
variance, with more than one sub-clusters 
existing as well as outlier elements. This may be 
explained by the fact that the parliament 
proceedings, contrary to literature, undergo 
intensive diting by a small group of specialised 
public servants. 
3 Distinguishing Styles within One 
Register 
In this section, we report on our efforts to 
distinguish among individual styles within one 
register. In particular, we intend to distinguish 
among speakers of the Parliament by studying 
the transcripts of the speeches of five parliament 
members over the period 1997-2000. Each of 
these speakers belongs to one of the five political 
parties that were represented in the Greek 
parliament over that period. Up to date, the 
experiments have been limited to the period 
1999-2000. 
3.1 Method of work 
The number of variables (46 in total) calculated 
for each of the five speakers can be grouped as 
follows: 
. Morphological variables (20 variables): 
? Verbal endings expressing the 
Katharevousa / Demotiki contrast giving 
rise to 12 variables. 
~'7 
12-elem. 
6 clust. 95.5% 
5 clust. 95.5% 
4 clust. 94.1% 
3 clust. 94.1% 
16-elem. 
100.0% 
100.0% 
98.5% 
98.5% 
27-elem. 15-elem 
100.0% 100.0% 
100.0% . 89.6% 
100.0% 83.4% 
98.5% 83.4% 
Table 2 - Unseeded clu~?erin? accuracy as a function of the cluster number and vector size usevL 
* the use of infixes (2 variables) in the past 
tense forms. 
. the person and ntmaber of the verb form 
(6 variables). 
The last two types of variable are expressed 
as percentages normalised over the number 
of verb forms. 
2. Lexical variables (6 variables): 
? Negation particles (623, &v,/aft). 
? Negative words of Katharevousa (ovJei?, 
~iveo). 
? Other words which also express the 
contrast Katharevousa / Demotiki (the 
anaphoric pronouns 'o~oio?' (Kath) and 
'taro' (Dem)), currently resulting in a 
single variable. 
3. Structural macro-features: average sentence 
and word length, number of commas, 
question marks, dashes and brackets, 
resulting in a total of 6 variables. 
4. Structural micro-features (other than 
lexical): 
? Part-of-Speech ounts (10 variables). 
? Use of grammatical categories such as 
the genitive case with nouns and 
adjectives (2 variables). 
5. The year when the speech was presented in
the Parliament and the order of the speech in 
the daily schedule, that is whether it was the 
first speech of the speaker that clay (hereafter 
denoted as "protoloyia") or the second, third 
etc. (resulting in a total of 2 variables). 
6. The identity of the speaker, denoted as the 
speaker Signature (1 variable), which was 
used to determine the desired classification. 
Similarly to the clustering experiments, a set of 
C programs was used to extract automatically the 
values of the aforementioned variables from the 
transcripts. Most of these programs rely on 
measuring the occurrence of di-gram% and more 
generally n-grams, for letters, words and tagsets, 
thus being straight-forward. In the case of 
speaker identification, Discriminant Analysis 
was used, as the clustering approach did not give 
very good results, indicating that the distinction 
among personal styles is weaker than that among 
registers. Even when only 2 speakers were used, 
the clusters formed involved patterns from both 
speaker classes. 
We experimented with two corpora, Corpus I and 
Corpus 11, as described in Table 3. Corpus H is a 
subset of Corpus I. Each of the speeches 
included in Corpus II was delivered as an 
opening speech Cprotoloyia") at a parliament 
session when at least wo of the studied speakers 
delivered speeches. 
An important issue is whether the selected 
variables are strongly correlated. If indeed strong 
correlations do exist, these might be used to 
reduce the dimensionality of the pattern space. 
For the purposes of this analysis, the 46 
independent variables were used (45 in the case 
of Corpus II where only "protoioyiai" exist, since 
then the order variable is constantly equal to 1). 
The number of correlations of all variable pairs 
exceeding iven thresholds i depicted in Figure 
1, for both Corpus I and Corpus 11. According 
to this study, in Corpus IL the percentage of 
variable pairs with an absolute value of 
correlation exceeding 0.5 is approximately 3%, 
indicating a low correlation between the 
parameters. Additionally, out of 990 pairs of 
Corpus 11, only a single one has a correlation 
exceeding 0.8. The correlations for the same 
parameter pairs over the two corpora re similar, 
though as a rule the correlation for Corpus I is 
less that that for Corpus 11, reflecting the larger 
variability of texts in Corpus I. The correlation 
study indicated that most of the parameters are 
not strongly correlated. Thus, a factor analysis 
step is not necessary and the application of the 
diseriminant analysis directly on the original 
variables i  justified. 
Initially, Corpus I (see Table 3) was processed. 
The 46 aforementioned variables were used to 
generate discriminant functions accurately 
recognising the identity of the speaker. To that 
end, three different approaches were used: 
(i) the full model: all variables were used to 
determine the discriminant functions; 
38 
(ii) the forward model: starting from an 
empty model, variables were introduced in 
order to create a reduced model, with a 
small number of variables; 
(iii) the backward model: starting from the full 
model, variables were eliminated to create 
a reduced model. 
In the cases of the forward and backward 
models, the values of the F parameter to both 
enter and delete a variable were set to 4 while the 
maximum number of steps to generate the model 
was set to 50. 
Year 1999-2000 
Speaker Corpus I Corpus II 
A 92 30 
B 45 24 
C 33 21 
D 21 16 
E 150 36 
Table 3 - Comparative composition of 
Corpus I and Corpus 11. 
The performance ofthis model is improved if: 
I. the order in which each particular speech 
was delivered is taken into account: the 
subset of "protoloyiai" is well-defined and 
presents a low variance while the speeches of 
second or lower order have a higher 
variance. 
2. the corpus comprises only sessions where 
more than one speaker has delivered 
speeches. Thus, the more balanced Corpus II 
(Table 3) presents an improved 
discrimination performance. 
For these two corpora, the results of the 
diseriminant analysis are shown in Table 4. The 
discrimination rate obtained with Corpus II is 
much higher than that for Corpus I. In addition, 
smaller models, with 8 variables, may be created 
that correctly classify at least 75% of Corpus II. 
An example of the factors generated and the 
manner in which they separate the pattern space 
is shown in the diagrams of Figure 2. 
3.2 Comments  on the Results 
Though this research is continuing, certain facts 
can be reported with confidence. 
Within the Greek Parliament Proceedings 
register, individual styles can not be classified on 
the basis of morphological features expressing 
the contrast Katharevousa/Demotiki. This may 
be explained by the fact that these texts undergo 
intensive diting towards a well-established sub- 
language. This editing homogenises the 
morphological profile of the texts but, of course, 
does not go as far as homogenising the lexical 
preferences of the various speakers. That is why, 
contrary to the register-clustering experiments, 
lexical variables expressing the particular 
contrast seem to play a role in discriminating 
between speakers and why the use of 
Katharevousa-odented n gative particles, which 
was not important in register discrimination, 
seems to be of some importance in style 
discrimination. The observation that negative 
words play a role in style identification is in 
agreement with the observations ofLabb6 (1983) 
on the French political speech. 
Structural features have turned out to be 
important: the average word length, the use of 
punctuation and question marks and the use of 
certain parts-of-speech such as articles, 
conjunctions, adjuncfions and - especiaUy - 
verbs. Furthermore, the distribution of verbs into 
persons and numbers seems to be important, 
though the exact variables selected differ 
depending on the exact set of speeches used 
(these variables are of course complementary). 
One of the most interesting findings of this 
research is that it is important whether the 
speaker delivers a "protoloyia" or not. 
"Protoloyiai" can be classified at a rate of 95% 
while mixed deliveries result in a lower rate, as 
low as 75%. This may be caused by two factors: 
1. "Protoloyiai" represent longer stretches of 
text, which are more characteristic ofa given 
speaker. 
2. Speakers prepare meticulously for their 
"protoloyiai" while their other deliveries 
represent a more spontaneous type of speech, 
which tends to contain patterns hared by all 
the parliament members. 
Finally, certain additional patterns are emerging 
for each of the speakers. Certain speakers (e.g. 
speaker A) are more consistently recognised than 
others (e.g. speaker B) while speaker B is similar 
to speaker C and speaker D is similar to speaker 
E. This indicates that additional variables may be 
required to improve the classification accuracy 
for all speakers. 
correlation of variables 
m m 
o ~ ' ~ _ - - _ ~ _  : : := 
0.2 0.4 0.6 0.8 
-10  
correlation level 
? ,e - Corpus  I 
Corpus  II 
Figure 1 - Percentage of variable pairs exceedin? a given level of absolute correlation. 
Dataset 
Corpus I
Corpus II 
Corpus II 
(reduced model) 
full 
93.79 % 
(46) 
97.64 % 
(45) 
97.64 %% 
(45) 
Model 
forward 
75.37 % 
(46) 
94.49 % 
(13) 
87.40 % 
(8) 
backward 
78.30 % 
(46) 
92.91% 
(20) 
79.53 %% 
(s) 
observations 
341 
127 
127 
Table 4 - Discrimination rate (the corresponding model size is shown in italics). 
4 Future Plans 5 Conclusions 
As a next step, frequency of use of certain 
lemmata shall be imroduced since visual 
inspection indicates that they may provide good 
discriminatory features. We also plan to 
substitute average lengths (of both words and 
sentences) with the distribution of lengths. 
Furthermore, we intend to introduce certain 
structural measurements such as repetition of 
structures, chains of nominals and the occurrence 
of negation within NP phrasal eousdments. 
Another possible extension involves the 
inclusion of the speech topic. As certain 
speakers' characteristics seem to change through 
time, we plan to process the entire corpus of 
speeches for the target period 1997/-2000. 
Finally, an important issue is the comparison of 
the results obtained in our experiments o these 
generatedby alternative t chniques proposed by 
other researchers. This will allow the deduction 
of more accurate conclusions regarding the 
strengths and the weaknesses of the research 
strategies. 
In this article, ongoing research on register and 
individual style eategorisation f written Modem 
Greek has been reported. A system has been 
proposed for the automatic register 
categorisafion of corpora in Modem Greek 
exploiting the highly inflectional nature of the 
language. The results have been obtained with a 
relatively constrained set of registers; however 
their recognition accuracy is remarkably high, 
exceeding 98% with an unseeded clustering 
approach using between 3and 6 clusters. 
On the front of individual style categorisation, a 
discrimination rate of over 80% was achieved for 
five speakers within the Greek Parliament 
register. Morphological variables were shown to 
be of less importance to this task, while lexieal 
and straetural variables eemed to take over. We 
are planning to introduce several new lexical and 
structural variables in order to achieve better 
discrimination rates and to determine 
discriminating features of the different styles. 
40 
Acknowledgements 
The authors wish to thank the Dept. of Language 
Technology Applications and specifically Dr. 
Harris Papageorgiou and Mr. Prokopis 
Prokopidis in obtaining the lemmatised versions 
of the parliament transcripts. Additionally, the 
authors wish to acknowledge the assistance of 
the Secretariat of the Hellenic Parliament in 
obtaining the session transcripts. 
References 
Baayen, R. H., van Halteren, H. and Tweedie, F. J. 
(1996). Outside the cave of shadows: Using 
syntactic annotation to enhance authorship 
attribution. Literary and Linguistic Computing, 
Vol. 11, No. 3, pp. 121-131. 
Biber, D. (1995) Dimensions of Register Variation: A 
cross-linguistic omparison. Cambridge University 
Press. 
Biber, D., Conrad, S. & Reppen, R. (1998) Corpus 
Linguistics: Investigating Language Structure and 
Use. Cambridge University Press. 
Clairis, C. & Babiniofis, G. (1999) Grammar of 
Modern Greek - I1 Verbs. Ellinika Grammata, 
Athens (in Greek). 
Gavrilidou, M., Labropoulou P., Papakostopouiou N.,
Spiliotopoulou S., Nassos N. (1998) Greek Corpus 
Documentation, Parole LE2-4017/10369, WP2.9- 
WP-ATH-1. 
Holton D., Mackridge, P. & Philippaki-Warburton, I. 
(1997) Greek: A Comprehensive Grammar of the 
Modem Language. Roufledge, London and New 
York. 
Karlgren, J., (1999) Stylistic Experiments in 
Information Retrieval. In T. Strzalkowski (ed.), 
Natural Language Information Retrieval, pp. 147- 
166. Dordrecht: Kluwer. 
Kilgarriff, A. (1996). Which words are parfieuiarly 
characteristic of a text? A survey of statistical 
approaches. In Proc. AISB Workshop on Language 
Engineering for Document Analysis and 
Recognition, Sussex University, April, pp. 33-40. 
Labb~, D. (1983). Francois Mittermna~ Essai sur le 
discours. La Pens~,e Sauvage, Grenoble. 
Markantonatou, S. & Tambouratzis, G. (2000) Some 
quantitative observations regarding the use of 
grammatical negation in Modern Greek. 
Proceedings of the 21 st Annual Meeting of the 
Department of lz~guistics, Faculty of Philosophy 
of the Aristotelian University of Thessaloniki, May 
2000 (in print/in Greek). 
Papageorgiou, H., Prokopidis, P., Giouli, V. & 
Piperidis, S. (2000) A Unified PoS Tagging 
Architecture and its application to Greek. 
Proceedings of the 2nd International Conference on 
Language Resources and Evaluations, Athens, 
Greece, 31 May - 2 June, Vol. 3, pp. 1455-1462. 
Tambouratzis, G., Markantonatou, S., Hairetakis, N. 
& Carayannis, G. (2000) Automatic Style 
Categorisation f Corpora in the Greek Language. 
Proceedings of the 2nd International Conference on 
Language Resources and Evaluations, Athens, 
Greece, 31 May - 2 June, Vol. 1, pp. 135-140. 
APPENDIX 
Characteristics 
Demotiki 
of Katharevousa and 
Diglossia in Modem Greek is due to the contrast 
between Katharevousa and Demotiki and is well- 
manifested on the morphological level. Here we 
concentrate on verb morphology. 
Demotiki tends to have words ending with an 'open' 
syllable. So, 3 r~ Plural verbal endings in -n (1) are 
augmented to -ne (2). 
(1) ~ \[e'leyan\] (Kath ) (=they said) 
(2) 2b/etw \[le'yane\] (Dem) (=they said) 
In Demotik/, Katharevousa's consonant dusters of 
two fricatives or two plosives are convened into 
clusters of one fricative and one plosive (3) - (4) 
(Holton et al, 1997, pp. 14). 
(3) nmtrOtb \[pis0o'\]/n~zortb \[pisto'\] (=to be 
convinced) 
(4) ga2mpOd~ \[kalifSo'\]/xczivffrd~ \[kalifto'\] (--to be 
covered) 
Certain verb classes exhibit thematic vowel 
alternations either following the inflectional paradigm 
of Ancient Greek or Demotiki (5) (Clairis and 
Babiniotis, 1999). 
(5) e~aprdtraz \[eksarta'te\] ( Kath ) l e~odrakrou 
\[eksartiete\] (Dem) (=depends) 
Sometimes Deraotiki uses a verbal root, which is 
similar though not identical to the Katharevousa one 
(6). 
(6) AfioJ \[li'o\] (Kath)/2fivxo Oi" no\] (Dem ) (=to solve) 
Finally, many verbs inherited from Katharevausa 
survive in Demotik/, either having an equivalent - 
mainly colloquial- (7) or not (8) (Clalris and 
Babiniotis, 1999). 
(7) rcpodO~uoa \[profi'0eme\] ( Kath ) / oxoxe6oJ 
\[skope'vo\] (Dem) (=I intend to) 
(8) ztpo~arapaz \[proi'stame\] (=supervise) 
/11 
r~ 
o 
d 
0 Speak~ A " " 
v sp--k~el " .  . 
+ s . .~o  I "=- .  S=__ 
? - y_~,  
? ~ '~v  v 
+ * 
o .~j>,- . , 
:{ ?? ,o .o 
.3 o 0o# 
0 0 O0 
_____~_- - _~___~_ .~.~- - . ___ . _ , __@. - , . _ ___  
-8 -S -4 -2 0 2 4 6 
F=ctm I
0 Spsmlmr A * 
Speaker B ? 
4 " SpeakeT C ? * ? 
+ Speakm O ? . 
0 Speaker E 
2 ~ , " ' -  : "  
% ^ ~.o  o?~ ? . ? 
# + ., o + ++ 0 
Fact0r 2 
o 
6 - - .~- -~m-- . .~ .~=. - - . - . . -=========. , . - . . - - -  
Z~ SptakW B I ? 
Speaker C I 4 ? [ ? * 
+ Speaker O I ~ ? 
0 Spellker E [ * 
2 ~_ ? o 
0 ~ O0 " *. 
0 0 ^o ~ d+o 
o~O'~.  
0 -2 0 ++ + 
z~ 
-4 z~ 
-8 -8 -4 -2 0 2 4 
Facle? 1 
0 
~ o 
6~ 
4 
21 
o ~ 
u. 
-2 
-6 
.,6 
O Speaker A 
? ~ ~sdcer O 
? ,,peakeT C
+ Speaker D 
0 Speaker E 
e ? ? 
Oo  oO;O" : 
# o +'~,+ ~o + 
o 
+ ++ 
0 
e 
? 0 0 
0 
o :. ~ 
Z~ 
Fact0r2 
r 
i 0 Spe~kerA I SpeWer B + 
e Spe;lker C 
+ Speaker O ++ 
0 Spe~ikor E 
+ 
+.#-~ + 
+ ? 
+,  . "  0 0 
+ O+ 00 " " t~ ' : .  " O 0 
O O O " . .  ,~  # 0~000 
00~o_QP o ~ ~"~- 
-2 0 0 0 ZX 0 
0 ~ t , 
8 6 4 -2 0 2 4 6 
Factor I
'2  
3 
~T 
-I 
-2 
-3 
6 
6 r a m  
0 SpsakefA + 
5 z~ Speaker B 
? Speaker C 
4 + Spe=k~ O + + + 
O Speaks E 
3 + 
+ .~+ ? + 
++ +? e00 ? ."" 
0 : "  " "  
? 
Z~ z~O 0 Q> 0 o 
Z~ ~0 0 
-4 -2 0 2 4 
Factor 3 
Figure 2 - Discriminant factors plotted against he patterns for corpus I1. 
42 
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 65?68,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
PRESEMT: Pattern Recognition-based Statistically Enhanced MT 
 
George Tambouratzis, Marina Vassiliou, Sokratis Sofianopoulos 
Institute for Language and Speech Processing, Athena R.C. 
6 Artemidos & Epidavrou Str., Paradissos Amaroussiou, 151 25, Athens, Greece. 
{giorg_t; mvas ; s_sofian}@ilsp.gr 
 
 
 
 
Abstract 
This document contains a brief presentation 
of the PRESEMT project that aims in the de-
velopment of a novel language-independent 
methodology for the creation of a flexible and 
adaptable MT system. 
1. Introduction 
The PRESEMT project constitutes a novel ap-
proach to the machine translation task. This ap-
proach is characterised by (a) introducing cross-
disciplinary techniques, mainly borrowed from 
the machine learning and computational intelli-
gence domains, in the MT paradigm and (b) us-
ing relatively inexpensive language resources. 
The aim is to develop a language-independent 
methodology for the creation of a flexible and 
adaptable MT system, the features of which en-
sure easy portability to new language pairs or 
adaptability to particular user requirements and 
to specialised domains with minimal effort. 
PRESEMT falls within the Corpus-based MT 
(CBMT) paradigm, using a small bilingual paral-
lel corpus and a large TL monolingual corpus. 
Both these resources are collected as far as pos-
sible over the web, to simplify the development 
of resources for new language pairs. 
The main aim of PRESEMT has been to alle-
viate the reliance on specialised resources. In 
comparison, Statistical MT requires large parallel 
corpora for the source and target languages. 
PRESEMT relaxes this requirement by using a 
small parallel corpus, augmented by a large TL 
monolingual corpus. 
2. PRESEMT system structure 
The PRESEMT system is distinguished into 
three stages, as shown in Figure 1: 
1. Pre-processing stage: This is the stage where 
the essential resources for the MT system are 
compiled. It consists of four discrete modules: (a) 
the Corpus creation & annotation module, 
being responsible for the compilation of mono-
lingual and bilingual corpora over the web and 
their annotation; (b) the Phrase aligner module, 
which processes a bilingual corpus to perform 
phrasal level alignment within a language pair; (c) 
the Phrasing model generator that elicits an SL 
phrasing model on the basis of the aforemen-
tioned alignment and employs it as a parsing tool 
during the translation process; (d) the Corpus 
modelling module, which creates semantics-
based TL models used for disambiguation pur-
poses during the translation process. 
2. Main translation engine: The translation in 
PRESEMT is a top-down two-phase process, 
distinguished into the Structure selection mod-
ule, where the constituent phrases of an SL sen-
tence are reordered according to the TL, and the 
Translation equivalent selection module where 
translation disambiguation is resolved and word 
order within phrases is established. Closely inte-
grated to the translation engine, but not part of 
the main translation process, is the Optimisation 
module, which is responsible for automatically 
improving the performance of the two translation 
phases by fine-tuning the values of the various 
system parameters. 
3. Post-processing stage: The third stage is user-
oriented and comprises (i) the Post-processing 
and (ii) the User Adaptation modules. The first 
module allows the user to modify the system-
generated translations towards their requirements. 
The second module enables PRESEMT to adapt 
to this input so that it learns to generate transla-
tions closer to the users? requirements. The post-
processing stage represents work in progress to 
be reported in future publications, the present 
article focussing on the actual strategy for gener-
ating the translation. 
65
3. Processing of the bilingual corpus 
The bilingual corpus contains literal translations, 
to allow the extrapolation of mapping informa-
tion from SL to TL, though this may affect the 
translation quality. The Phrase aligner module 
(PAM) performs offline SL ? TL word and 
phrase alignment within this corpus. PAM serves 
as a language-independent method for mapping 
corresponding terms within a language pair, by 
circumventing the problem of achieving com-
patibility between the outputs of two different 
parsers, one for the SL and one for the TL. PAM 
relies on a single parser for the one language and 
generates an appropriate phrasing model for the 
other language in an automated manner.  
The phrases are assumed to be flat and linguisti-
cally valid. As a parser, any available tool may 
be used (the TreeTagger (Schmid, 1994) is used 
in the present implementation for English). PAM 
processes a bilingual corpus of SL ? TL sentence 
pairs, taking into account the parsing information 
in one language (in the current implementation 
the TL side) and making use of a bilingual lexi-
con and information on potential phrase heads; 
the output being the bilingual corpus aligned at 
word, phrase and clause level. Thus, at a phrasal 
level, the PAM output indicates how an SL struc-
ture is transformed into the TL. For instance, 
based on a sentence pair from the parallel corpus, 
the SL sentence with structure A-B-C-D is trans-
formed into A?-C?-D?-B?, where X is a phrase in 
SL and X? is a phrase in TL. Further PAM details 
are reported in Tambouratzis et al (2011). 
The PAM output in terms of SL phrases is 
then handed over to the Phrasing model genera-
tor (PMG), which is trained to determine the 
phrasal structure of an input sentence. PMG 
reads the SL phrasing as defined by PAM and 
generates an SL phrasing model using a probabil-
istic methodology. This phrasing model is then 
applied in segmenting any arbitrary SL text being 
input to the PRESEMT system for translation. 
PMG is based on the Conditional Random Fields 
model (Lafferty et al, 1999) which has been 
found to provide the highest accuracy. The SL 
text segmented into phrases by PMG is then in-
put to the 1st translation phase. For a new lan-
guage pair, the PAM-PMG chain is implemented 
without any manual correction of outputs. 
4. Organising the monolingual corpus 
The language models created by the Corpus 
modelling module can only serve translation dis-
ambiguation purposes; thus another form of in-
terfacing with the monolingual corpus is essen-
tial for the word reordering task within each 
phrase. The size of the data accessed is very 
large. Typically, a monolingual corpus contains 3 
billion words, 108 sentences and approximately 
109 phrases. Since the models for the TL phrases 
need to be accessed in real-time to allow word 
reordering within each phrase, the module uses 
the phrase indexed representation of the mono-
lingual corpus. This phrase index is created 
based on four criteria: (i) phrase type, (ii) phrase 
head lemma, (iii) phrase head PoS tag and (iv) 
number of tokens in the phrase. 
Indexing is performed by extracting all 
phrases from the monolingual corpus, each of 
which is transformed to the java object instance 
used within the PRESEMT system. The phrases 
are then organised in a hash map that allows mul-
tiple values for each key, using as a key the 4 
aforementioned criteria. Statistical information 
about the number of occurrences of each phrase 
in the corpus is also included. Finally, each map 
is serialised and stored in the appropriate file in 
the PRESEMT path, with each file being given a 
suitable name for easy retrieval. For example, for 
the English monolingual corpus, all verb phrases 
with head lemma ?read? (verb) and PoS tag 
?VV? containing 2 tokens in total are stored in 
the file ?Corpora\EN\Phrases\VC\read_VV?. If 
any of these criteria has a different value, then a 
separate file is created (for instance for verb 
phrases with head ?read? that contain 3 tokens). 
5. Main translation engine 
The PRESEMT translation process entails first 
the establishment of the sentence phrasal struc-
ture and then the resolution of the intra-phrasal 
arrangements, i.e. specifying the correct word 
order and deciding upon the appropriate candi-
date translation. Both phases involve searching 
for suitable matching patterns at two different 
levels of granularity, the first (coarse-grained) 
aiming at defining a TL-compatible ordering of 
phrases in the sentence and the second (fine-
grained) determining the internal structure of 
phrases. While the first phase utilises the small 
bilingual corpus, the second phase makes use of 
the large monolingual corpus. To reduce the 
translation time required, both corpora are proc-
essed in advance and the processed resources are 
stored in such a form as be retrieved as rapidly as 
possible during translation. 
66
5.1 Translation Phase 1: Structure selection 
module 
Each SL sentence input for translation is tagged 
and lemmatised and then it is segmented into 
phrases by the Phrasing model generator on the 
basis of the SL phrasing model previously cre-
ated. For establishing the correct phrase order 
according to the TL, the parallel corpus needs to 
be pre-processed using the Phrase aligner module 
to identify word and phrase alignments between 
the equivalent SL and TL sentences. 
During structure selection, the SL sentence is 
aligned to each SL sentence of the parallel cor-
pus, as processed by the PAM and assigned a 
similarity score using an algorithm from the dy-
namic programming paradigm. The similarity 
score is calculated by taking into account edit 
operations (replacement, insertion or removal) 
needed to be performed in the input sentence in 
order to transform it to the corpus SL sentence. 
Each of these operations has an associated cost, 
considered as a system parameter. The aligned 
corpus sentence that achieves the highest similar-
ity score is the most similar one to the input 
source sentence. This comparison process relies 
on a set of similarity parameters (e.g. phrase type, 
phrase head etc.), the values of which are opti-
mised by employing the optimisation module. 
The implementation is based on the Smith-
Waterman algorithm (Smith and Waterman, 
1981), initially proposed for determining similar 
regions between two protein or DNA sequences. 
The algorithm is guaranteed to find the optimal 
local alignment between the two input sequences 
at clause level. 
5.2 Translation Phase 2: Translation 
equivalent selection module 
After establishing the order of phrases within 
each sentence, the second phase of the translation 
process is initiated, comprising two distinct 
tasks. The first task is to resolve the lexical am-
biguity, by picking one lemma from each set of 
possible translations (as provided by a bilingual 
dictionary). In doing so, this module makes use 
of the semantic similarities between words which 
have been determined by the Corpus Modelling 
module through a co-occurrence analysis on the 
monolingual TL corpus. That way, the best com-
bination of lemmas from the sets of candidate 
translations is determined for a given context. 
In the second task, the most similar phrases to 
the TL structure phrases are retrieved from the 
monolingual corpus to provide local structural 
information such as word-reordering. A match-
ing algorithm selects the most similar from the 
set of the retrieved TL phrases through a com-
parison process, which is viewed as an assign-
ment problem, using the Gale-Shapley algorithm 
(Gale and Shapley, 1962). 
6. Experiments & evaluation results 
To date MT systems based on the PRESEMT 
methodology have been created for a total of 8 
languages, indicating the flexibility of the pro-
posed approach. Table 1 illustrates an indicative 
set of results obtained by running automatic 
evaluation metrics on test data translated by the 
1st PRESEMT prototype for a selection of lan-
guage pairs, due to space restrictions. 
In the case of the language pair English-to-
German, these results are contrasted to the ones 
obtained when translating the same test set with 
Moses (Koehn et al, 2007).It is observed that for 
the English-to-German language pair, PRESEMT 
achieved approximately 50% of the MOSES 
BLEU score and 80% of the MOSES with re-
spect to the Meteor and TER scores. These are 
reasonably competitive results compared to an 
established system such as Moses. Furthermore, 
it should taken into consideration that (a) the 
PRESEMT results were obtained by the 1st sys-
tem prototype, (b) PRESEMT is still under de-
velopment and (c) only one reference translation 
was used per sentence.  
Newer versions of the PRESEMT system, in-
corporating more advanced versions of the dif-
ferent modules are expected to result in substan-
tially improved translation accuracies. In particu-
lar, the second translation phase will be further 
researched. In addition, experiments have indi-
cated that the language modelling module can 
provide additional improvement in the perform-
ance. Finally, refinements in PAM and PMG 
may lead in increased translation accuracies. 
7. Links 
Find out more about the project on the PRE-
SEMT website: www.presemt.eu. Also, the 
PRESEMT prototype may be tried at: 
presemt.cslab.ece.ntua.gr:8080/presemt_interface_test 
Acknowledgments 
The research leading to these results has received 
funding from the European Community's Sev-
enth Framework Programme (FP7/2007-2013) 
under grant agreement n? 248307. 
67
References 
Gale D. and L. S. Shapley. 1962. College Admissions 
and the Stability of Marriage. American Mathe-
matical Monthly, Vol. 69, pp. 9-14. 
Koehn P., H. Hoang, A. Birch, C. Callison-Burch, M. 
Federico, N. Bertoldi, B. Cowan, W. Shen, C. 
Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin, 
and E. Herbst. 2007. Moses: Open Source Toolkit 
for Statistical Machine Translation. In Proceedings 
of the ACL-2007 Demo and Poster Sessions. 
Kuhn H. W. 1955. The Hungarian method for the as-
signment problem. Naval Research Logistics Quar-
terly, Vol. 2, pp.83-97. 
Lafferty J., A. McCallum, F. Pereira. 2001. Condi-
tional Random Fields: Probabilistic Models for 
Segmenting and Labelling Sequence Data. Pro-
ceedings of ICML Conference, pp.282-289. 
Munkres J. 1957. Algorithms for the assignment and 
transportation problems. Journal of the Society for 
Industrial and Applied Mathematics, Vol. 5, pp.32-
38. 
Schmid, H. 1994. Probabilistic Part-of-Speech Tag-
ging Using Decision Trees, Proceedings of Interna-
tional Conference on New Methods in Language 
Processing, Manchester, UK. 
Smith T. F. and M. S. Waterman. 1981. Identification 
of Common Molecular Subsequences. Journal of 
Molecular Biology, 147: 195?197. 
Tambouratzis G., F. Simistira, S. Sofianopoulos, N. 
Tsimboukakis and M. Vassiliou 2011. A resource-
light phrase scheme for language-portable MT, 
Proceedings of the 15th International Conference of 
the European Association for Machine Translation, 
30-31 May 2011, Leuven, Belgium, pp. 185-192. 
 
Table 1 ? PRESEMT Evaluation results for different language pairs. 
Language Pair Sentence set Metrics  
SL TL Number Source BLEU NIST Meteor TER  
English German 189 web 0.1052 3.8433 0.1939 83.233  
German English 195 web 0.1305 4.5401 0.2058 74.804  
Greek English 200 web 0.1011 4.5124 0.2442 79.750  
         
English German 189 web 0.2108 5.6517 0.2497 68.190 Moses 
 
Figure 1 ? PRESEMT system architecture. 
Tagging-
Lemmatising, 
Lexicon look-up & 
Token generation 
Phase 1: Structure selection 
Final 
translation 
Phase 2: Translation equivalent 
selection 
Bilingual 
Lexicon 
TL Monolingual 
Corpus & 
Corpus model 
Post-processing 
& User 
adaptation 
modules 
WEB 
Offline corpora 
creation 
Phrase aligner 
module 
Corpus creation 
& annotation 
module 
Corpus modelling 
module 
Phrasing model 
generator 
SL text 
Bilingual parallel 
corpus 
Bilingual aligned 
corpus 
Phrase reordering of each 
sentence in the SL text 
Word reordering & 
disambiguation 
Optimisation module 
68
Proceedings of the First Workshop on Multilingual Modeling, pages 1?10,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Implementing a language-independent MT methodology 
 
 
Sokratis Sofianopoulos Marina Vassiliou George Tambouratzis 
ILSP / Athena R.C. ILSP / Athena R.C. ILSP / Athena R.C. 
Artemidos 6 & Epidavrou Artemidos 6 & Epidavrou Artemidos 6 & Epidavrou 
Athens, Greece Athens, Greece Athens, Greece 
s_sofian @ilsp.gr mvas@ilsp.gr giorg_t@ilsp.gr 
 
 
 
 
 
 
Abstract 
The current paper presents a language-
independent methodology, which facilitates 
the creation of machine translation (MT) 
systems for various language pairs. This 
methodology is implemented in the 
PRESEMT hybrid MT system. PRESEMT 
has the lowest possible requirements on 
specialised resources and tools, given that 
for many languages (especially less widely 
used ones) only limited linguistic resources 
are available. In PRESEMT, the main 
translation process comprises two phases. 
The first one, Structure selection, 
determines the overall structure of a target 
language (TL) sentence, drawing on 
syntactic information from a small 
bilingual corpus. The second phase, 
Translation equivalent selection, relies on 
models extracted solely from monolingual 
corpora to implement translation 
disambiguation, determine intra-phrase 
word order and handle functional words. 
This paper proposes extracting information 
for disambiguation from the monolingual 
corpus. Experimental results indicate that 
such information substantially contributes 
in improving translation quality. 
1 Introduction 
Currently most language-independent MT 
approaches are based on the statistical machine 
translation (SMT) paradigm (Koehn, 2010). SMT 
has proved to be particularly amenable to new 
language pairs, provided the necessary training 
data are available. The main SMT constraint is the 
need for SL-TL bilingual corpora of a sufficient 
size (at least several hundreds of thousands of 
sentences) to allow the building of accurate 
translation models. Such corpora are hard to find, 
particularly for less widely used languages. 
Furthermore, SMT translation accuracy largely 
depends on the quality of the bilingual corpora as 
well as their relevance to the domain of text to be 
translated. For instance, parliament proceedings 
(among the most widely available corpora) may 
not suffice to train MT systems aimed towards 
technical manuals or news articles. 
Example-Based Machine Translation (EBMT) is 
another MT paradigm, where a set of SL sentences 
are provided together with their TL reference 
translations. Translations are generated by analogy, 
where for an input sentence the most similar SL 
side from the sentence set is determined and the 
corresponding TL side sentence is used to generate 
the translation. Hybrid MT systems combining 
EBMT and SMT techniques have been proposed 
(cf. Groves & Way, 2005 and Phillips, 2011). 
As an alternative to SMT, techniques for 
creating MT systems using more limited but easily 
obtainable resources have been proposed. Even if 
these methods do not achieve an accuracy as high 
as that of SMT, their ability to develop MT 
systems with very limited resources confers to 
them an important advantage. The present article 
focusses on the development of such a 
methodology. 
1
2 MT systems utilising low-cost resources 
A number of methods for the automatic inference 
of templates for the structural transfer from SL to 
TL have been proposed. Notably, Caseli et al 
(2008) have proposed generating resources such as 
bilingual transfer rules and, more importantly, 
shallow transfer rules from parallel corpora. In a 
related set-up, Sanchez-Martinez et al (2009) 
suggest using small parallel corpora only to extract 
transfer rules, assuming that a sufficient bilingual 
dictionary is already available. Sanchez-Martinez 
et al (2009) report that the MT accuracy is 
substantially higher for related languages, the 
proposed method exceeding even SMT systems 
(for which the parallel corpora used, averaging 
approximately one million words each, are found 
to be too small to allow effective linguistic 
modelling). Both aforementioned approaches have 
been combined with the Apertium1 MT system. 
Other MT systems have been proposed to cater 
for the case of low resources. Habash (2003) has 
proposed the Matador system for translation from 
Spanish to English, as a typical example of 
Generation-Heavy Machine Translation (GHMT), 
where resource poverty in the source language is 
addressed by exploiting TL resources. Carbonell et 
al. (2006) propose an MT method that requires no 
parallel text, but relies on a translation model 
utilising a full-form bilingual dictionary and a 
decoder using long-range context via large n-grams. 
Another family of systems using low-cost 
resources encompasses METIS (Dologlou et al, 
2003) and METIS-II (Markantonatou et al, 2009; 
Carl et al, 2008). These rely solely on extensive 
monolingual corpora in order to translate SL texts. 
METIS and METIS-II employ pattern recognition-
based algorithms to determine the translation. 
3 The PRESEMT system in brief 
The architecture of PRESEMT has been 
formulated on the basis of experience collected 
within METIS and METIS-II. However, 
PRESEMT has been substantially modified in 
order to provide a measurable increase in 
translation speed and accuracy. 
More specifically, in terms of resources, 
PRESEMT uses a bilingual dictionary providing 
SL ? TL lexical correspondences. It also uses, as 
                                                           
1
 www.apertium.org 
does METIS-II, an extensive TL monolingual 
corpus, which is compiled automatically via web 
crawling; a small bilingual corpus is yet 
additionally employed, in order to (a) reduce the 
number of possible translations that need to be 
evaluated by the system and (b) define examples of 
SL ? TL structural modifications, thus improving 
the translation quality. The bilingual corpus need 
not cover a particular domain and only numbers a 
few hundred sentences (typically ~200) for 
determining structural equivalences between the 
source and target languages. Hence, in comparison 
to SMT systems, the size of the parallel corpus 
required is reduced by at least three orders of 
magnitude. 
Both the bilingual and the monolingual corpora 
are annotated 2  with lemma and Part-of-Speech 
(PoS) information and, depending on the language, 
with additional morphological features (e.g. case, 
number, tense etc.). Furthermore, they are 
segmented into non-recursive syntactic phrases 
(e.g. noun phrase, verb phrase etc.). The next 
section details the kind of information extracted. 
3.1 Exploiting the corpora 
The processing of the bilingual corpus involves the 
combined use of two modules, the Phrase aligner 
module (PAM) and the Phrasing model generator 
(PMG). Details on PAM and PMG are provided in 
Tambouratzis et al (2011), though their operation 
is summarised here for reasons of completeness. 
Initially, the bilingual corpus is aligned at word 
and phrase level by PAM. PAM aims at 
circumventing incompatibilities of different 
annotation tools, based on a learning-by-example 
principle. It identifies how the SL structure is 
modified towards the TL one, allowing the 
deduction of a phrasing model for the source 
language. To operate, PAM assumes the existence 
of a parser in TL, which provides chunking 
information. Based on lexical information 
combined with statistical data on PoS tag 
correspondences drawn from the bilingual lexicon, 
PAM transfers the parsing scheme from the TL 
side of the corpus (bearing lemma, tag and parsing 
                                                           
2
 For the annotation task readily available tools are employed, 
including statistical taggers and (to some extent) chunkers that 
provide shallow parsing. This alleviates the need for 
developing new linguistic tools. 
2
information3), to the SL side, which is only tagged 
and lemmatised. In other words, the SL side is 
segmented into phrases in accordance to the 
phrasal segmentation provided for the TL side. 
PAM follows a three-step process, involving (a) 
lexicon-based correspondences, (b) alignment 
based on similarity of grammatical features and 
PoS tag correspondence and (c) alignment guided 
by already aligned neighbouring words. In each 
consecutive step, additional SL words are assigned 
to phrases, but with a reduced accuracy, the aim 
being for all words to be assigned to phrases. 
The SL side of the aligned corpus is 
subsequently processed by PMG, with a two-fold 
purpose, namely to (i) deduce a phrasing model 
based on conditional random fields (CRF) 
(Lafferty et al, 2001) and (ii) employ this model 
for parsing any SL text submitted for translation. 
The TL monolingual corpus serves as the basis 
for extracting two models, which are employed 
during the translation process. The first one is used 
solely for disambiguation purposes (cf. subsection 
6.4). The second model provides the micro-
structural information on the translation output to 
support word reordering. It derives from a phrase-
based indexing of the TL monolingual corpus, 
which is performed offline during the pre-
processing stage and is based on (i) phrase type, 
(ii) phrase head and (iii) phrase head PoS tag. 
To implement a fast retrieval, the TL phrases are 
then organised in a hash map that allows the 
storage of multiple values for each key, using as a 
key the three aforementioned criteria. For each 
phrase the number of occurrences within the 
corpus is also retained. Each hash map is serialised 
and stored in a file with a unique name for 
immediate access by the search algorithm. 
The number of files created as a result of this 
process is large, yet each of the files is of small 
size and thus can be loaded quickly. Furthermore, 
the existence of a given word in a phrase does not 
necessarily mean that this phrase will be grouped 
with other phrases containing the same word, since 
the model is based on the phrase head. 
For the experiments reported here, the TL 
monolingual corpus is indexed based on the 
criteria listed above. However, a different indexing 
scheme may prove more effective, and thus 
                                                           
3
 For the experiments reported here, TreeTagger (Schmid, 
1994) was used for the TL processing. 
experiments on the optimal indexing are 
continuing. For instance, the environment of the 
phrase may also be stored (i.e. the type of the 
previous and next phrases) and in this case the 
phrase organisation may be modified. These 
modifications may yield a decrease in 
computational load during translation, by reducing 
the number of phrase comparisons. 
3.2 Main translation engine 
The translation process is split into two phases, 
each of which makes use of only a single type of 
corpus. Phase 1 (Structure selection) uses the 
bilingual corpus to determine, for a given input SL 
sentence, the appropriate TL structure in terms of 
phrase type and order. The output of the Structure 
selection phase is the SL sentence with a TL 
structure, created by reordering the phrases 
according to the parallel corpus, and all words 
replaced by the TL lemmas and tag information as 
retrieved from the bilingual dictionary. 
Phase 2 (Translation equivalent selection) uses 
the monolingual corpus to specify the most likely 
word order within phrases, to handle functional 
words such as articles and prepositions and to 
resolve lexical ambiguities emerging from the 
possible translations provided by the bilingual 
dictionary. Finally, a token generator component 
generates tokens out of lemmas. Therefore, the 
first PRESEMT translation phase is closely related 
to EBMT, while the second phase is reliant upon 
statistical information, resulting in a hybrid nature. 
4 Example of the PRESEMT translation 
process 
In this section the translation process of the 
PRESEMT system is illustrated via a simple 
example. Details on the algorithmic part are 
provided in the subsequent sections. 
Input Sentence: ???????????? ??????? ????? 
????????? ???? ????? ??? ????????? (= ?Good 
neighbourhood relations are established in the 
Balkan countries?) 
Annotation at various levels [tagging & 
lemmatising; PMG-based segmentation to phrases; 
output of the lexicon look-up] 
Input sentence annotation after being input for translation 
Phrase Word Lemma Tag Lexicon 
VC4 ???????????? ???????? vbo3pl {consolidate; 
                                                           
4
 VC: verb chunk, PC: prepositional chunk 
3
Input sentence annotation after being input for translation 
Phrase Word Lemma Tag Lexicon 
establish} 
PC 
??????? 
????? 
????????? 
????? 
????? 
???????? 
nofeplnm 
ajfesgge 
nofesgge 
{relation; 
relationship} 
{nice; decent; 
good} 
{adjacency; 
neighbourhood} 
PC 
???? 
????? 
??? 
????????? 
???? 
???? 
? 
???????? 
asfeplac 
nofeplac 
atneplge 
noneplge 
{on; at; to; into; 
in; upon} 
{country} 
{the} 
{Balkan} 
 
1st translation phase: Establish the correct 
phrase order on the basis of TL. Search the 
bilingual corpus for the most similar SL sentence 
in structural terms, find the corresponding TL one 
and reorder the input sentence accordingly. 
Most similar SL sentence of the bilingual parallel corpus 
Phrase Word Lemma Tag 
VC ???????????? ???????? vb03pl 
PC ???????????? ???????? 
??????????? 
???????? 
nofeplnm 
nomaplge 
PC 
?? 
????? 
????? 
??? 
?? 
?? 
????? 
???? 
? 
?? 
asppsp 
pnfe03plac 
nofeplac 
atffesgge 
abbr 
Corresponding TL sentence of the bilingual parallel corpus 
Phrase Word Lemma Tag 
PC student protests 
student 
protest 
nn 
nns 
VC occur occur vv 
PC 
In 
other 
EU 
countries 
in 
other 
EU 
country 
in 
jj 
np 
nns 
 
Output of 1st transl. phase (expressed as list of 
phrases and lemmas): [{relation; relationship}; 
{nice; decent; good}; {adjacency; neighbourhood} 
PC] [{consolidate; establish} VC] [{on; at; to; into; 
in; upon}; {country}; {the}; {Balkan} PC] 
2nd translation phase: Identify the correct word 
order within each phrase. Disambiguate the 
translations. Generate tokens out of lemmas 
Word reordering results: [{nice; decent; 
good}; {adjacency; neighbourhood}; {relation; 
relationship} PC] [{consolidate; establish} VC] [{on; at; 
to; into; in; upon}; {the}; {Balkan}; {country} PC] 
Disambiguation: [{good}; {neighbourhood}; 
{relation} PC] [{establish} VC] [{in}; {the}; 
{Balkan}; {country} PC] 
Token generation: [{good}; {neighbourhood}; 
{relations} PC] [{are established} VC] [{in}; {the}; 
{Balkan}; {countries} PC] 
Final Translation: [Good neighbourhood 
relations
 PC] [are established VC] [in the Balkan 
countries PC] 
5 Phase 1: Structure selection 
The task of Structure selection is to determine the 
type of TL phrases to which the SL ones translate 
and to order them in the TL sentence. To this end it 
consults the patterns of SL ? TL structural 
modifications to be found in the parallel corpus, 
thus resembling EBMT (Hutchins, 2005). 
Translation phase 1 receives as input an SL 
sentence (termed ISS ? Input Source Sentence), 
bearing lexical translations from the dictionary, 
annotated with tag & lemma information and 
segmented into phrases by PMG. A dynamic 
programming algorithm then determines for each 
ISS the most similar, in terms of phrase structure, 
SL sentence found in the bilingual corpus (termed 
ACS ? Aligned Corpus Sentence)5. 
The similarity is determined by taking into 
account structural information such as phrase type, 
phrase head PoS tag, phrase functional head info 
and phrase head case. The ISS phrases are then 
reordered in accordance to the TL side of the 
chosen ACS by replicating the SL-TL phrase 
alignment mapping. The data flow of the Structure 
selection is depicted in Figure 1. 
The dynamic programming algorithm is 
essentially a monolingual similarity algorithm. The 
most similar SL structure of the bilingual corpus, 
that determines the TL structure of the sentence to 
be translated, is thus selected purely on SL 
properties. The implemented method is based on 
the Smith-Waterman algorithm (Smith and 
Waterman, 1981), initially proposed for alignment 
of DNA and RNA sequences. This algorithm is 
guaranteed to find the optimal local alignment 
between two input sequences. 
                                                           
5
 If the most similar ACS retrieved from the parallel corpus is 
very dissimilar, then ISS does not undergo any reordering. It is 
notable that in our experiments never did such an occasion 
appear, the similarity always reaching a high percentage 
(above 70%). The fact that comparisons involve sentences of 
the same language (SL) ensures a high similarity score. 
4
5.1 Calculating structural similarity 
The structural similarity between ISS and ACS is 
reflected on the similarity score, for the calculation 
of which a two-dimensional matrix is created with 
the ISS along the top row and the ACS along the 
left side. A cell (i,j) represents the similarity of the 
sub-sequence of elements up to the mapping of the 
elements Ei of the ACS and E?j of the ISS, where 
each element corresponds to a phrase. The 
similarity for cell (i,j) is determined by examining 
the predecessor cells located directly to the left (i, 
j-1), directly above (i-1, j) and above-left (i-1, j-1), 
that contain values V1, V2 and V3 respectively, 
and is calculated iteratively as the maximum of the 
three numbers {max(V1, V2, 
V3)+ElementSimilarity(Ei, E?j)}. The similarity of 
two phrases (PhrSim) is calculated as the weighted 
sum of four criteria, namely the similarities of (a) 
the phrase type (PhrTypSim), (b) the phrase head 
PoS tag (PhrHPosSim), (c) the phrase head case 
(PhrHCasSim) and (d) the functional phrase head 
PoS tag (PhrfHPosSim): 
PhrSim(Ei,E?j) = WphraseType*PhrTypSim(Ei,E?j) + 
WheadPoS*PhrHPosSim(Ei,E?j) + 
WheadCase*PhrHCasSim(Ei,E?j) + 
WfheadPoS*PhrfHPosSim(Ei,E?j) 
For normalisation purposes, the sum of the four 
aforementioned weights (whose experimental 
values6  are 0.4, 0.1, 0.1 and 0.4 respectively) is 
equal to 1. The similarity score ranges from 100 to 
0, these limits denoting exact match and total 
dissimilarity between elements Ei and E?j 
respectively. In case of a zero similarity score, a 
penalty weight (-50) is employed, to further 
penalise mapping of dissimilar items. 
When the algorithm has reached the jth element 
of the ISS, the similarity score between the two SL 
sentences is calculated as the value of the 
maximum jth cell. The ACS that achieves the 
highest similarity score is the closest to the input 
SL sentence in terms of phrase structure. 
After determining the similarity between 
sentences, as the final similarity score, the 
comparison matrix indicates the optimal phrase 
alignment between the two SL sentences. By 
combining the SL sentence alignment from the 
algorithm with the alignment information between 
                                                           
6
 An optimisation module has been designed as part of the 
PRESEMT system for defining the optimal values of these 
parameters (cf. subsection 5.3 for more details). 
the ACS and the attached TL sentence, ISS phrases 
are reordered according to the TL side structure. 
To illustrate this approach, an example is 
provided with Greek as SL and English as TL. Let 
us assume the ISS given in (1): 
(1) ?? ??? ??? ???????? ????????? 
???????????? ?? ??? ????????pi??????? ?????????? 
(?The term Machine Translation denotes an 
automated procedure?) 
 
The input sentence is segmented by PMG into 
the structure depicted in (2a); the structure 
elements being exemplified in (2b): 
(2a) pc(as, no_ac) pc(-, no_ac) vp(-, vb) pc(as, no_ac) 
(2b) <Phrase type> (<Phrase fhead PoS tag>, 
<Phrase head PoS tag>_<Phrase head case>) 
An indicative ACS from the aligned corpus is 
given in (3): 
(3) ?? ????????? ????? ??? ????pi????? ?????? 
????????? ??? ??????? ????????? ??????. (?The 
historical roots of the European Union lie in the 
Second World War?) 
The corresponding structural information for (3) 
is: pc(-,no_nm) pc(-,no_ge) vc(vb) pc(as,no_ac). 
 
   Input source sentence (ISS)  
 
  
pc (as, 
no_ac) 
pc (-, 
no_ac) 
vc 
(-, vb) 
pc (-, 
no_ac) 
 
 0 0 0 0 0 
pc(-, 
no_nm) 0 60 80 -20 60 
pc(-, 
no_ge) 0 60 140 40 40 
vc(vb) 0 -50 10 240 140 
A
lig
n
ed
 
co
rp
u
s 
se
n
te
n
ce
 
(A
C
S)
 
pc(as, 
no_ac) 0 100 30 -40 340 
Table 1. Matrix defining phrase correspondence of 
sentences (1) and (3) 
 
Then, the matrix of Table 1 is created to 
calculate the similarity scores between sentences 
(1) and (3) (cells forming the best aligned 
subsequence are highlighted). By choosing for 
each element the maximum similarity, the 
transformation cost is calculated (340 in this case). 
Based on this matrix, ISS is modified in 
accordance to the attached TL structure. 
5
Figure 1. Data flow in Structure selection 
 
6 Phase 2: Translation equivalent 
selection 
Following Phase 1, the issues to be resolved in the 
second phase include (i) word ordering within 
phrases, (ii) handling of functional words and (iii) 
resolution of translation ambiguities. 
6.1 Searching for phrasal equivalents 
The monolingual TL corpus is searched to 
determine the most similar phrase to each phrase in 
the SL sentence, in order to establish the correct 
word order. The similarity measure takes into 
account the phrase type, and the words contained 
in the phrase in terms of lemma, PoS tag and 
morphological features. These factors enter the 
comparison with different weights, whose relative 
magnitudes are subject to an optimisation process. 
The main issue at this stage is to reorder 
appropriately any items within each phrase. This 
entails that the words of a given phrase of the input 
sentence (denoted as ISP ? Input Sentence Phrase), 
and the words of a retrieved TL phrase (denoted as 
MCP ? Monolingual Corpus (TL) Phrase), are 
close to each other in terms of number and type. 
The data flow of the Translation equivalent 
selection is depicted in Figure 2. 
6.2 Establishing correct word order 
When initiating Phase 2 of the translation process, 
the matching algorithm accesses the indexed TL 
phrase corpus to retrieve similar phrases and select 
the most similar one through a comparison process, 
which is viewed as an assignment problem. This 
problem can be solved via algorithms such as the 
Gale-Shapley (Gale and Shapley, 1962; Mairson, 
1992) and Kuhn?Munkres ones (Kuhn, 1955; 
Munkres, 1957). The Kuhn-Munkres approach 
computes an exact solution of the assignment 
problem to determine the optimal matching 
between elements. Experiments with METIS-II 
have shown that the solution of the assignment 
problem is computationally-intensive. 
On the contrary, the Gale-Shapley algorithm 
solves the assignment problem in a reduced time. 
In this approach, the two sides are termed suitors 
(in PRESEMT, the SL side) and reviewers (the TL 
side). The two groups have distinct roles, suitors 
proclaiming their order of preference of being 
assigned to a specific reviewer, via an ordered list. 
Each reviewer selects one of the suitors after 
evaluating them based on the ordered preference 
list, in subsequent steps revising its selection so 
that the resulting assignment is improved. This 
process is suitor-optimal but possibly non-optimal 
from the reviewers? viewpoint. As its complexity is 
substantially lower than that of Kuhn-Munkres, the 
Gale-Shapley algorithm is adopted in PRESEMT 
to limit the computation time. 
For each SL phrase, it is necessary to establish 
the correct word order for all possible TL phrases 
that can be produced by combining the lexical 
equivalents of each word in the phrase. 
After the completion of this comparison process, 
the selected phrase from the monolingual corpus 
serves as a basis for resolving other issues such as 
the handling of functional words (e.g. insertion / 
deletion of articles). In this process, the TL 
information prevails over the SL entries. 
6.3 Optimising the selection process of 
phrasal equivalents 
The search for the most similar phrase depends on 
a set of parameters. Within this set, different types 
of weights are included, such as weights governing 
the similarity of PoS tags, lemmas, phrase types 
and morphological features. The weights from both 
6
translation phases are handled in a unified manner 
by the Optimisation module. Research in earlier 
MT systems has shown that the application of 
Genetic Algorithms (GAs) and multi-objective 
evolutionary algorithms such as SPEA2 (Improved 
Strength Pareto Evolutionary Algorithm) for the 
optimisation of parameters can considerably 
improve the translation quality (Sofianopoulos et 
al., 2010). 
For the experiments presented in the next section, 
manually-defined preliminary weights are used for 
the parameters of both phases. To further improve 
the translation accuracy, an optimisation process is 
studied. This optimisation (which is beyond the 
scope of the present article) provides the prospect 
for a substantial improvement in the accuracy via 
the selection of appropriate parameter values. 
 
 
 
 
Figure 2. Data flow in Translation equivalent selection 
 
6.4 Resolving translation ambiguities 
Translation equivalent selection receives as 
input the output of Structure selection, which 
contains sets of candidate translations for each 
SL lemma. One translation needs to be chosen 
from each set, thus disambiguating amongst the 
possible translations. The disambiguation 
process uses the semantic similarities between 
words as evidenced by the monolingual corpus. 
Different approaches are evaluated for selecting 
the most appropriate translation, including 
Vector Space Modelling (Marsi et al, 2010) and 
Self-Organising Maps, following the work by 
Tsimboukakis et al (2011). 
These disambiguation processes lie beyond 
the scope of the present publication. On the 
contrary, a simpler, corpus-based approach is 
proposed here, which relies on the extraction of 
statistical information with only limited pre-
processing. This method reuses and enhances the 
indexed sets of the monolingual corpus phrases, 
by exploiting information on the frequency of 
occurrence of each TL phrase. When searching 
for the best matching TL phrase for each 
combination of lexical alternatives, the 
frequency of the TL phrase is taken into account. 
Notably, not all combinations are examined for 
lexical disambiguation; instead only the phrase 
mapped to the most frequent TL phrase is 
retained. 
7 Experimental Results 
The evaluation results reported here concern the 
Greek ? English language7 pair and were based 
on the development datasets used in PRESEMT 
for studying the system performance. For each 
SL, these datasets contain 1,000 sentences, 
collected via web-crawling. Sentence length 
ranges from 7 to 40 words. From these datasets, 
200 sentences were randomly chosen, and 
manually translated into each of the target 
languages. The correctness of these reference 
translations was checked independently by 
native speakers. 
                                                           
7
 PRESEMT handles 8 language pairs: SL {Czech, English, 
German, Greek, Norwegian} ? TL {English, German}. 
7
For the current evaluation phase four 
automatic evaluation metrics have been 
employed, i.e. BLEU (Papineni et al, 2002), 
NIST (NIST 2002), Meteor (Denkowski and 
Lavie, 2011) and TER (Snover et al, 2006). 
Table 2 summarises indicative scores obtained. 
 
Number of sentences 40 Source web 
Reference translations 1 Language pair EL ? EN 
Metrics MT system 
BLEU NIST Meteor TER 
PRESEMT 1 0.1297 4.1568 0.2669 79.417 
PRESEMT 2 0.2004 4.9995 0.3294 72.678 
Metis-II 0.1222 3.1655 0.2698 82.878 
Google8 0.5472 7.1360 0.4713 29.963 
Systran9 0.3143 5.4615 0.3857 49.449 
WordLingo10 0.2908 5.1853 0.3728 49.632 
Table 2. Evaluation results 
 
When using the base PRESEMT system with 
the phrase-frequency disambiguation component 
deactivated (denoted as PRESEMT 1), a BLEU 
score of 0.1297 and a Meteor score of 0.2669 are 
obtained. When the disambiguation component 
is activated (PRESEMT 2), these scores increase 
substantially, reaching a BLEU score of just 
over 0.20. The BLEU improvement over 
PRESEMT 1 is 0.07 points (representing a 50% 
improvement), while NIST is increased by 0.85 
and Meteor by over 0.06. TER is reduced by 7 
points, also marking an improvement. 
To put these scores into perspective, a 
comparison is made to MT systems available on 
the Internet, both rule-based (SYSTRAN) and 
SMT ones (Google Translate). In addition, the 
results of METIS-II are quoted, to compare 
PRESEMT with a system based on monolingual 
corpora. As can be seen, web-based MT systems 
produce higher scores for all metrics, with 
Google Translate possessing the best values. 
Yet these scores are, especially in the case of 
Systran and WordLingo, not far off the scores 
obtained for PRESEMT with disambiguation. In 
particular NIST scores are directly comparable 
whilst the Meteor ones are not substantially 
higher. It can be reasonably assumed that due to 
the language-independent methodology without 
                                                           
8
 translate.google.com 
9
 www.systranet.com 
10
 www.worldlingo.com 
direct provision of language-specific 
information, the scores obtained via PRESEMT 
will be lower. Still, it is expected that refined 
versions of the PRESEMT algorithm will allow 
the achievement of higher scores that render its 
performance directly comparable to that of 
Systran and WordLingo, for the given language 
pair. In comparison to METIS-II, PRESEMT 
offers a substantial improvement for all metrics, 
with for instance BLEU and NIST scores 
increased by over 50%. This illustrates the 
improvements conferred by the new translation 
methodology. As noted, PRESEMT is still under 
development and it is anticipated that more 
extensive experiments involving additional 
language pairs will provide improvements in the 
translation quality. 
8 Conclusions 
In the present article the principles and the 
implementation of a novel language-independent 
methodology have been presented. The 
PRESEMT methodology draws on information 
residing in a large monolingual corpus and a 
small bilingual one for creating MT systems 
readily portable to new language pairs. Most of 
this information is extracted in an automated 
manner using pattern recognition techniques. 
First experimental results using objective 
evaluation metrics and comparisons to 
established systems have also been reported. 
These results are promising, especially taking 
into account the fact that several PRESEMT 
modules are still under development and the 
translation process is being refined, in particular 
with respect to the handling of internal phrasal 
structure. These will be reported in future 
articles. 
References 
Michael Carl, Maite Melero, Toni Badia, Vincent 
Vandeghinste, Peter Dirix, Ineke Schuurman, 
Stella Markantonatou, Sokratis Sofianopoulos, 
Marina Vassiliou and Olga Yannoutsou. 2008. 
METIS-II: Low Resources Machine Translation: 
Background, Implementation, Results and 
Potentials. Machine Translation, Vol. 22, No. 1-2, 
pp. 67-99. 
Jaime Carbonell, Steve Klein, David Miller, Michael 
Steinbaum, Tomer Grassiany, and Jochen Frey. 
8
2006. Context-Based Machine Translation. In 
Proceedings of the 7th Conference of the 
Association for Machine Translation in the 
Americas, Cambridge, Massachusetts, USA, pp. 
19-28. 
Helena M. Caseli, Maria das Gracas V. Nunes, and 
Mikel L. Forcada (2008) Automatic Induction of 
Bilingual resources from aligned parallel corpora: 
Application to shallow-transfer machine 
translation. Machine Translation, Vol. 20, pp. 227-
245. 
Michael Denkowski and Alon Lavie. 2011. Meteor 
1.3: Automatic Metric for Reliable Optimization 
and Evaluation of Machine Translation Systems. 
EMNLP 2011 Workshop on Statistical Machine 
Translation, Edinburgh, Scotland, pp. 85-91. 
Ioannis Dologlou, Stella Markantonatou, George 
Tambouratzis, Olga Yannoutsou, Athanasia Fourla 
and Nikos Ioannou. 2003. Using Monolingual 
Corpora for Statistical Machine Translation: The 
METIS System. In Proceedings of the EAMT- 
CLAW?03 Workshop, Dublin, Ireland, pp. 61-68. 
David Gale and Lloyd S. Shapley. 1962. College 
Admissions and the Stability of Marriage. 
American Mathematical Monthly, Vol. 69, pp. 9-
14. 
Declan Groves & Andy Way, 2005. Hybrid data-
driven Models of Machine Translation. Machine 
Translation, Vol 19, pp.301-323. 
Nizar Habash. 2003. Matador: A Large-Scale 
Spanish-English GHMT System. In Proceedings 
of MT Summit IX, New Orleans, LA, pp. 149-156. 
John Hutchins. 2005. Example-Based Machine 
Translation: a Review and Commentary. Machine 
Translation, Vol. 19, pp.197-211. 
Philip Koehn. 2010. Statistical Machine Translation. 
Cambridge University Press, Cambridge. 
Harold W. Kuhn. 1955. The Hungarian method for 
the assignment problem. Naval Research Logistics 
Quarterly, Vol. 2, pp. 83-97. 
John Lafferty, Andrew McCallum and Fernando 
Pereira. 2001. Conditional Random Fields: 
Probabilistic Models for Segmenting and 
Labelling Sequence Data. 28th International 
Conference on Machine Learning, ICML 2011, 
Bellevue, Washington, USA, pp. 282-289. 
Stella Markantonatou, Sokratis Sofianopoulos, Olga 
Giannoutsou and Marina Vassiliou. 2009. Hybrid 
Machine Translation for Low- and Middle- 
Density Languages. Language Engineering for 
Lesser-Studied Languages, S. Nirenburg (ed.), IOS 
Press, pp. 243-274. 
Erwin Marsi, Andr? Lynum, Lars Bungum, and Bj?rn 
Gamb?ck. 2011. Word Translation 
Disambiguation without Parallel Texts. 
International Workshop on Using Linguistic 
Information for Hybrid Machine Translation, 
Barcelona, Spain, pp. 66-74. 
Harry Mairson. 1992. The Stable Marriage Problem. 
The Brandeis Review, 12:1. Available at: 
www.cs.columbia.edu/~evs/intro/stable/writeup.html 
James Munkres. 1957. Algorithms for the assignment 
and transportation problems. Journal of the Society 
for Industrial and Applied Mathematics, Vol. 5, 
pp. 32-38. 
NIST 2002. Automatic Evaluation of Machine 
Translation Quality Using n-gram Co-occurrences 
Statistics. 
Kishore Papineni, Salim Roukos, Todd Ward, and 
Wei-Jing Zhu. 2002. BLEU: A Method for 
Automatic Evaluation of Ma-chine Translation. 
40th Annual Meeting of the Association for 
Computational Linguistics, Philadelphia, USA, pp. 
311-318. 
Aaron Phillips. 2011. CUNEI: Open-source Machine 
Translation with Relevance-based models of each 
translation instance. Machine Translation, Vol. 25, 
pp. 161-177 
Felipe Sanchez-Martinez and Mikel L. Forcada. 
2009. Inferring Shallow-transfer Machine 
translation Rules from Small Parallel Corpora. 
Journal of Artificial Intelligence Research, Vol. 
34, pp. 605-635. 
Helmut Schmid. 1994. Probabilistic Part-of-Speech 
Tagging Using Decision Trees. In Proceedings of 
International Conference on New Methods in 
Language Processing, Manchester, UK, pp. 44-49. 
Temple F. Smith and Michael S. Waterman. 1981. 
Identification of Common Molecular 
Subsequences. Journal of Molecular Biology, Vol. 
147, pp. 195-197. 
Matthew Snover, Bonnie Dorr, Richard Schwartz, 
Linnea Micciulla, and John Makhoul. 2006. A 
Study of Translation Edit Rate with Targeted 
Human Annotation. In Proceedings of the 7th 
Conference of the Association for Machine 
Translation in the Americas, Cambridge, 
Massachusetts, USA, pp. 223-231. 
Sokratis Sofianopoulos, and George Tambouratzis. 
2010. Multiobjective Optimisation of real-valued 
9
Parameters of a Hybrid MT System using Genetic 
Algorithms. Pattern Recognition Letters, Vol. 31, 
pp.1672-1682. 
George Tambouratzis, Fotini Simistira, Sokratis 
Sofianopoulos, Nikos Tsimboukakis, and Marina 
Vassiliou. 2011. A resource-light phrase scheme 
for language-portable MT. 15th International 
Conference of the European Association for 
Machine Translation, Leuven, Belgium, pp. 185-
192. 
Nikos Tsimboukakis, and George Tambouratzis. 
2011. Word map systems for content-based 
document classification. IEEE Transactions on 
Systems, Man & Cybernetics ? Part C, Vol. 41(5), 
pp. 662-673. 
10
Proceedings of the Second Workshop on Hybrid Approaches to Translation, pages 123?130,
Sofia, Bulgaria, August 8, 2013. c?2013 Association for Computational Linguistics
Language-independent hybrid MT with PRESEMT 
 
 
George Tambouratzis Sokratis Sofianopoulos Marina Vassiliou 
ILSP, Athena R.C ILSP, Athena R.C ILSP, Athena R.C 
giorg_t@ilsp.gr s_sofian@ilsp.gr mvas@ilsp.gr 
 
 
 
 
 
 
Abstract 
The present article provides a compre-
hensive review of the work carried out 
on developing PRESEMT, a hybrid lan-
guage-independent machine translation 
(MT) methodology. This methodology 
has been designed to facilitate rapid 
creation of MT systems for uncon-
strained language pairs, setting the low-
est possible requirements on specialised 
resources and tools. Given the limited 
availability of resources for many lan-
guages, only a very small bilingual cor-
pus is required, while language model-
ling is performed by sampling a large 
target language (TL) monolingual cor-
pus. The article summarises implementa-
tion decisions, using the Greek-English 
language pair as a test case. Evaluation 
results are reported, for both objective 
and subjective metrics. Finally, main er-
ror sources are identified and directions 
are described to improve this hybrid MT 
methodology. 
1 Introduction and background 
Currently a large proportion of language-
independent MT approaches are based on the 
statistical machine translation (SMT) paradigm 
(Koehn, 2010). A main benefit of SMT is that it 
is directly amenable to new language pairs, pro-
vided appropriate training data are available for 
extracting translation and language models. The 
main obstacle to the creation of an SMT system 
is the requirement for SL-TL parallel corpora of 
a sufficient size to allow the extraction of mean-
ingful translation models. Such corpora (of the 
order of million sentences) are hard to obtain, 
particularly for less resourced languages. On the 
other hand, the translation accuracy of such sys-
tems largely depends on the quality and size of 
the bilingual corpora, as well as their relevance 
to the domain of text being translated. Even if 
such parallel corpora exist for a language pair, 
they are frequently restricted to a specific do-
main (or a narrow range of domains). As a con-
sequence, these corpora are not suitable for cre-
ating MT systems that focus on other domains. 
For this reason, in SMT, researchers are investi-
gating the extraction of information from mono-
lingual corpora, including lexical translation 
probabilities (Klementiev et al, 2012) and topic-
specific information (Su et al, 2011). 
Alternative techniques for creating MT sys-
tems using less informative but readily available 
resources have been proposed. Even if these 
methods do not provide a translation quality as 
high as SMT, their ability to develop hybrid MT 
systems with very limited specialised resources 
represents an important advantage. Such meth-
ods include automatic inference of templates for 
structural transfer from SL to TL (Caseli et al, 
2008 and Sanchez-Martinez et al, 2009). Simi-
larly, Carbonell et al (2006) propose an MT 
method that needs no parallel text, but relies on a 
lightweight translation model utilising a full-
form bilingual dictionary and a decoder for long-
range context. Other systems using low-cost re-
sources include METIS (Dologlou et al, 2003) 
and METIS-II (Markantonatou et al, 2009; Carl 
et al, 2008), which utilise a bilingual lexicon 
123
and monolingual corpora to translate SL texts. 
METIS/METIS II, which have studied transla-
tion only towards English, employ pattern rec-
ognition algorithms to retrieve the most appro-
priate translation from a monolingual corpus. 
2 The MT methodology in brief 
The MT methodology has been developed 
within the PRESEMT (Pattern REcognition-
based Statistically Enhanced MT) project, 
funded by the European Commission (cf. 
www.presemt.eu). It comprises three stages: 
(i) pre-processing, where the input sentence is 
tagged and lemmatised 
(ii) main translation, where the actual transla-
tion output is generated and 
(iii) post-processing, where the corresponding 
tokens are generated from lemmas. 
The main translation process is split in two 
phases, namely (a) the establishment of the 
translation structure in terms of phrase order and 
(b) the definition of word order and resolution of 
lexical ambiguities at an intra-phrase level.  
In terms of resources, PRESEMT utilises a bi-
lingual lemma dictionary providing SL ? TL 
lexical correspondences. It also employs an ex-
tensive TL monolingual corpus, compiled auto-
matically via web crawling (Pomikalek et al, 
2008) to generate a comprehensive phrase-based 
language model. The provision of the monolin-
gual corpus allows PRESEMT to use only a very 
small bilingual corpus for mapping the transfer 
from SL to TL sentence structures. This bilin-
gual corpus only numbers a few hundred sen-
tences, reducing reliance on costly linguistic re-
sources. The corpus is assembled from available 
parallel corpora, only replacing free translations 
with more literal ones, to allow the accurate ex-
traction of structural modifications. The parallel 
corpus coverage is not studied prior to integra-
tion in PRESEMT, which would have allowed 
an optimisation of translation performance. 
3 Extracting information from corpora 
3.1 Parallel corpus 
Initially, both the bilingual and the monolingual 
corpora are annotated 1  so as to incorporate 
lemma and Part-of-Speech (PoS) information 
and other salient language-specific morphologi-
cal features (e.g. case, number, tense etc.). Fur-
thermore, for the TL side, a shallow parser or 
chunker (hereafter referred to as parser) is used 
to split the sentences into syntactic phrases. As 
the proposed methodology has been developed 
to maximise the use of publicly-available soft-
ware, the user is free to select any desired parser 
for the TL language. 
To avoid either an additional SL side parser or 
potential incompatibilities between the two pars-
ers, the Phrase Aligner module (PAM, Tam-
bouratzis et al, 2011) is implemented. PAM 
transfers the TL side parsing scheme, which en-
compasses lemma, tag and parsing information, 
to the SL side, based on lexical information cou-
pled with statistical data on PoS tag correspon-
dences extracted from the lexicon. The parsing 
scheme includes phrase boundaries and phrase 
labels. PAM follows a 3-step process, involving 
(a) lexicon-based alignment, (b) alignment based 
on similarity of grammatical features and PoS 
tag correspondence and (c) alignment on the 
evidence of already aligned neighbouring words. 
The SL side of the aligned corpus is subse-
quently processed by the Phrasing model genera-
tor (PMG), to create an SL phrasing model 
which will then parse sentences input for transla-
tion. The original PMG implementation (Tam-
bouratzis et al, 2011) has utilised Conditional 
Random Fields (CRF), due to the considerable 
representation capabilities of this model 
(Lafferty et al, 2001). CRF is a statistical mod-
elling method that takes context into account to 
predict labels for sequences of input samples. 
The implementation of an alternative PMG 
methodology (termed PMG-simple) based on 
template-matching principles has also been pur-
sued. PMG-simple locates phrases that match 
                                                           
1
 For the annotation task readily available tools are em-
ployed. For the experiments reported here, TreeTagger 
(Schmid, 1994) has been used for the TL text processing 
and the FBT PoS tagger (Prokopidis et al, 2011) has been 
employed for the processing of the SL text.. 
124
exactly what it has seen before, based on a sim-
ple template-matching algorithm (Duda et al, 
2001). The templates used are the phrases to 
which the SL side sentences of the bilingual cor-
pus have been segmented. In contrast to CRF, 
PMG-simple implements a greedy search (Black, 
2005) without backtracking. Initially all phrases 
are positioned in an ordered list according to 
their likelihood of being accurately detected. 
Starting from the phrase with the highest likeli-
hood, PMG-simple examines if each phrase oc-
curs in the input sentence. If it does and the con-
stituent words are not part of an already estab-
lished phrase, the constituent words are marked 
as parts of this phrase and are no longer consid-
ered in the phrase-matching process. If the 
phrase pattern does not occur, the next in-line 
phrase is considered, until the table is exhausted. 
Comparative results between CRF and PMG-
simple are reported in the results section. 
3.2 Monolingual corpus 
The TL monolingual corpus is processed to ex-
tract two complementary types of information. 
The first type supports disambiguation between 
multiple possible translations, while the second 
determines the order of words in the final trans-
lation and the addition or removal of functional 
words, using a TL phrase model derived from an 
indexing based on (i) phrase type, (ii) phrase 
head lemma and (iii) phrase head PoS tag. 
The TL phrases are then organised in a hash 
map that allows the storage of multiple values 
for each key, using as a key the three aforemen-
tioned criteria. For each phrase the number of 
occurrences within the corpus is retained. Each 
hash map is stored in a separate file to minimise 
access time during translation. 
4 Translation phase 1: Structure selec-
tion 
The Structure selection phase determines the 
type and relative position of TL phrases to which 
the SL ones are translated. To achieve this, 
PRESEMT consults the SL-to-TL structural 
modifications as contained in the PAM-
processed parallel corpus. In that respect, it 
resembles EBMT (Hutchins, 2005). 
Translation phase 1 receives as input an SL 
sentence, annotated with tag & lemma informa-
tion and segmented into phrases by the PMG. A 
dynamic programming algorithm then deter-
mines for each SL side the most similar (in 
terms of phrase structure) SL sentence from the 
bilingual corpus. Similarity is calculated by tak-
ing into account structural information such as 
the phrase type, the PoS tag and case (if applica-
ble) of the phrase head and phrase functional 
head info. The phrases of the input sentence are 
then reordered to generate the translation struc-
ture by combining the phrase alignments estab-
lished by the algorithm and the SL-TL phrase 
alignment information stored in the pair of paral-
lel sentences. 
The dynamic programming algorithm com-
pares structures from the same language. The 
most similar SL structure from the bilingual cor-
pus, that will determine the TL translation struc-
ture, is thus selected purely on SL properties. 
The similarity of two sentences is calculated as a 
weighted internal product between the two sen-
tences, traversing both sentences in parallel from 
their start towards their end. The implemented 
method utilises the Smith-Waterman variant 
(Smith and Waterman, 1981).  
The last step of this phase is the translation of 
words using the bilingual lexicon.2 All transla-
tion alternatives are disambiguated during the 
subsequent translation phase. 
5 Translation Phase 2: Translation 
equivalent selection 
Issues resolved in the second phase are phrase-
internal and include (i) word order within each 
phrase, (ii) introduction or deletion of functional 
words and (iii) selection of the best candidate in 
the case of translation ambiguities. These are 
resolved using the phrase-based indexing of the 
TL monolingual corpus. 
For each phrase of the sentence being trans-
lated, the algorithm searches the TL phrase 
model for similar phrases. If the search is suc-
cessful, all retrieved TL phrases are compared to 
the phrase to be translated. The comparison is 
based on the words included, their tags and lem-
mas and the morphological features. 
                                                           
2
 If an SL word is not included in the lexicon, it is retained 
in the translation in its original SL form. 
125
1. Retrieve the relevant phrases from the TL 
corpus based on the head word 
2. Compare the phrase with all the TL relevant 
phrases and store the one that scores the 
highest similarity score  
3. For any words that the TL model cannot 
disambiguate, use the lemma frequency 
model for selecting the best translation 
4. Return the new translated Phrase instance. 
 
 
Figure 1. Pseudocode for Translation equivalent 
selection 
 
For the purposes of the proposed methodol-
ogy, the stable-marriage algorithm (Gale & 
Shapley, 1962) is applied for calculating the 
similarity and aligning the words of a phrase 
pair. In comparison to other relevant algorithms, 
the Gale-Shapley algorithm, results in poten-
tially non-optimal solutions, but possesses the 
advantage of a substantially lower complexity 
and thus a reduced processing time. 
Using the most similar TL phrase and the 
word alignments generated by the stable-
marriage algorithm, word reordering, translation 
disambiguation and addition or removal of func-
tional words is performed for each phrase of the 
input sentence. The final translation is produced 
by combining all of its translated phrases. 
6 Developing new Language Pairs 
The porting of the proposed methodology to new 
language pairs is straightforward. The summary 
presented herewith is based on the creation of a 
new Greek-to-Italian language pair, and is typi-
cal of porting to new TLs. Initially, the NLP 
tools need to be selected for the new language 
(tagger & lemmatiser, shallow parser). In addi-
tion, a TL monolingual corpus and a bilingual 
lexicon need to be provided. The following steps 
are then taken: 
A. Create a java wrapper class for the Italian 
annotation tools, and provide rules for iden-
tifying heads of phrases. 
B. Tag/lemmatise and chunk the TL corpus, 
which takes less than a day. 
C. Process the chunked Italian corpus to gener-
ate the phrase model. This operation is fully 
automated and performed off-line (e.g. for a 
corpus of 100 million words, approx. 1.5 
days are needed). 
D. For the parallel corpus, train the PAM/PMG 
suite for the relevant language pair (less than 
2 hours needed). 
7 Objective Evaluation Experiments 
The evaluation results reported in this article 
focus on the Greek ? English language pair. Two 
datasets have been used (a development set and 
a test set), each of which comprises 200 sen-
tences, with a length of between 7 and 40 words. 
For every sentence, exactly one reference trans-
lation has been created, by SL-language native 
speakers and then the translation correctness was 
cross-checked by TL-language native speakers. 
 
Number of sentences 200 Source web 
Reference translations 1 Language pair EL?EN 
Metrics MT system 
BLEU NIST Meteor TER 
PRESEMT  0.3254 6.9793 0.3880 51.5330 
METIS-2 0.1222 3.1655 0.2698 82.878 
Systran 0.2930 6.4664 0.3830 49.721 
Bing 0.4600 7.9409 0.4281 37.631 
Google 0.5544 8.8051 0.4665 29.791 
WorldLingo 0.2659 5.9978 0.3666 50.627 
Table 1. Objective metrics results for PRESEMT 
& other MT systems (development set) 
 
To objectively evaluate the translation accu-
racy, four automatic evaluation metrics have 
been chosen, namely BLEU (Papineni et al, 
2002), NIST (NIST 2002), Meteor (Denkowski 
and Lavie, 2011) and TER (Snover et al, 2006). 
When developing the MT methodology, exten-
sive evaluation was carried out at regular inter-
vals (Sofianopoulos et al, 2012). The evolution 
of translation accuracy is depicted within Figure 
2. The falling trend for TER, signifies a continu-
ously improving translation performance. The 
current results for a number of MT systems for 
the development set are reported in Table 1. 
These results show that at the current stage of 
development the proposed approach has a qual-
ity exceeding that of WorldLingo and Systran, 
but is still inferior to Google and Bing. The re-
sults are particularly promising, taking into ac-
count that the proposed methodology has been 
developed for a substantially shorter period than 
the other systems, and has no language-specific 
information injected into it. According to an er-
126
ror analysis carried out, most of the errors are 
due to the lack of syntactic information (e.g. the 
inability to distinguish between object/subject). 
Also a point which can be improved concerns 
the mapping of sentence structures from SL to 
TL. To address this, additional experiments are 
currently under way involving larger monolin-
gual corpora.  
Even without this type of knowledge, the pro-
posed methodology has shown substantial scope 
for improvement, as evidenced by the evolution 
of the objective translation metrics (cf. Figure 
2). It is expected that this trend will be continued 
in future versions of the MT system. 
 
40.0000
45.0000
50.0000
55.0000
60.0000
65.0000
May-12 Jun-12 Jul-12 Aug-12 Sep-12 Oct-12 Nov-12 Dec-12
 
Figure 2. Evolution of translation accuracy re-
flected by TER scores for the PRESEMT system 
together with the associated trend line 
 
Number of sentences 200 Source web 
Reference translations 1 Language pair EL?EN 
Metrics PMG type 
BLEU NIST Meteor TER 
CRF-based 0.3167 6.9127 0.3817 52.509 
PMG-simple 0.3254 6.9793 0.3880 51.533 
Table 2. Effect on PRESEMT translation accu-
racy of using the two distinct PMG variants 
 
Recent activity towards improving translation 
accuracy has focussed on the effect of using dif-
ferent PMG approaches, as summarised in sec-
tion 3. According to Table 2, an improvement in 
all four metrics is achieved using PMG-simple 
instead of CRF. For the limited training set de-
fined by the parallel corpus, PMG-simple ex-
tracts more effectively the phrasing model. An 
improvement of approx. 3% in the BLEU score 
is achieved over the CRF-based system. The 
reduction in TER is almost 2% indicating a siz-
able improvement in translation quality, while 
NIST and METEOR scores are improved by 1% 
and 1.9% respectively. 
8 Subjective Evaluation Results 
To fully evaluate translation quality, both objec-
tive and subjective evaluation have been imple-
mented. The latter type is carried out by humans 
who assess translation quality. 
Human evaluation is considered to be more 
representative of the actual MT quality (Calli-
son-Burch, et al, 2008 & 2011), though on the 
other hand it is time-consuming and laborious. 
Furthermore, it lacks objectivity (single evalua-
tors may not be consistent in assessing a given 
translation through time while two evaluators 
may yield completely different judgements on 
the same text) and must be repeated for every 
new test result.  
For the human evaluation, for each language 
pair, a total of 15 language professionals were 
recruited, who were either language profession-
als, closely associated with MT tasks, or post-
graduate university students in the area of lin-
guistics. Two types of subjective evaluation 
were carried out. The first one involves the ex-
perts grading translations generated by the PRE-
SEMT system regarding their adequacy and flu-
ency. Adequacy refers to the amount of informa-
tion from the SL text that is retained in the trans-
lation, based on a 1-5 scale of scores (with a 
score of 1 corresponding to the worst transla-
tion). Fluency measures whether the translation 
is well-formed, also on a 1-5 scale, with empha-
sis being placed on grammaticality. 
The second type of subjective evaluation in-
volves direct comparison between the transla-
tions generated by PRESEMT and by other es-
tablished MT systems over the same dataset. In 
this case, each evaluator ranks the translations of 
the different systems, these systems being pre-
sented in randomised order to ensure the de-
pendability of the feedback received. 
Subjective evaluation activities were carried 
out during two distinct periods (namely October 
and December 2012), separated by two months. 
The purpose of implementing two sessions has 
been to judge the improvement in the system 
within the intervening period. Thus, two distinct 
versions of the EL-EN MT system correspond-
ing to these two time points were used. For ref-
127
erence, the objective evaluation results obtained 
for the test sentences are listed in Table 3. In 
both cases, the CRF-based PMG was used since 
it was more mature at the time of evaluation.  
A specifically-designed platform has been de-
veloped to support subjective evaluation activi-
ties3. This platform has been used to (a) collect 
the human evaluators? feedback for the different 
language pairs and (b) support the subsequent 
assessment of the results via statistical methods. 
 
Number of sentences 200 Source web 
Reference translations 1 Language pair EL?EN 
Metrics MT system 
BLEU NIST Meteor TER 
PRESEMT 
(phase 1) 0.2627 6.2001 0.3329 60.0420 
PRESEMT 
(phase 2) 0.2666 6.2061 0.3335 59.3360 
Bing 0.4793 8.1357 0.4486 35.7220 
Google 0.5116 8.4549 0.4580 32.6860 
WorldLingo 0.3019 6.3799 0.3814 46.7350 
Table 3. Objective metrics results for PRESEMT 
& other MT systems (test set) 
 
0
20
40
60
80
100
nu
m
be
r o
f c
as
es
1 2 3 4 5
Score scale
adequacy
fluency
 
Figure 3. Histogram of adequacy and fluency 
over all sentences (1st human evaluation phase) 
 
0
20
40
60
80
100
nu
m
be
r o
f c
as
es
1 2 3 4 5
Score scale
adequacy
fluency
 
Figure 4. Histogram of adequacy and fluency 
over all sentences (2nd human evaluation phase) 
 
For the proposed methodology, in phase 1 rel-
atively low values of both adequacy and fluency 
                                                           
3
 www.presemt.eu/presemt_eval/ 
measurements were recorded. By comparing the 
scores in the first and second evaluation phases 
(Figures 3 and 4, respectively), it can be seen 
that both adequacy and fluency histograms move 
towards higher values (notably fluency ratings 
with a score of 3 and adequacy ratings with 
scores of 3 and 4 have substantially higher fre-
quencies). This reflects improved translation 
quality in the later version of the proposed MT 
system in comparison to the earlier one.  
 
Number of sentences 200 Source web 
Reference translations 1 Language pair EL?EN 
Adequacy Fluency MT system 
average stdev. average stdev. 
PRESEMT 
(phase 1)  3.08 0.27 2.17 0.27 
PRESEMT 
(phase 2) 3.14 0.24 2.16 0.25 
Google 4.17 0.39 3.51 0.50 
Bing 3.75 0.77 3.02 0.61 
WorldLingo 3.77 0.45 3.11 0.51 
Table 4. Summary of measurements (in terms of 
average and standard deviation) for fluency and 
adequacy for various MT systems (test set) 
 
In addition, in phase 2 of subjective evalua-
tion, adequacy and fluency measurements were 
collected for the three operational systems used 
as reference systems (namely Google Translate, 
Bing and WorldLingo). These operational sys-
tems have higher adequacy and fluency values 
than PRESEMT, as indicated in Table 4. Fur-
thermore, paired t-tests have confirmed that at a 
0.99 level of significance, these three systems 
have statistically superior subjective measure-
ments to the proposed methodology. To provide 
a reference, for the same set of 200 sentences, 
objective metrics are shown in Table 3 for each 
system. As can be seen the relative order of the 
systems in the subjective evaluations (in terms 
of adequacy and fluency) is confirmed by the 
objective measurements. 
A second subjective evaluation focused on 
ranking comparatively the translations of the 
four studied MT systems. Evaluators were pre-
sented with the outputs of the four systems in 
randomized order, to conceal the identity of each 
system. The evaluators were requested to order 
the four translations from higher to lower quality 
(with 1 denoting the more accurate translation. 
128
To transform this ranking into a single score, the 
individual rankings per evaluator have been ac-
cumulated and normalized over the number of 
evaluators. Then the representative scoring has 
been defined as a weighted sum of frequency of 
a system being ranked as first, second, third and 
fourth best over all evaluators, by multiplying 
with weights of 40, 30, 20 and 10 respectively. 
The average scores of the proposed methodology 
were the lowest, followed by the ranking results 
for WorldLingo. The results of Bing and Google 
are comparable with the Google results giving 
the best results. A statistical analysis was carried 
out using paired t-tests for all six pairings of the 
four systems being studied. This has confirmed 
that the differences in subjective scores are sta-
tistically significant at a level of 0.95. 
To summarise, subjective evaluation has 
shown that the PRESEMT methodology has an 
inferior translation performance in terms of sub-
jective measurements to the three operational 
systems. This can be justified as the proposed 
methodology refrains from utilising language-
specific information as a priori grammatical 
knowledge. Inferior translations also reflect the 
much shorter development time available as well 
as the very limited amount of expensive re-
sources provided. The effect on translation qual-
ity of using pre-existing tools (to ease portability 
to new language pairs) needs to be stressed, as 
no modification of these tools was performed to 
remedy systematic shortcomings identified. For 
the newer MT versions now available, a new 
round of subjective evaluations is planned. It has 
been observed that improvements in objective 
metrics are followed by improved subjective 
evaluation performance. Thus, for these new 
versions, an improved accuracy is expected. 
9 Discussion 
In the present article the principles and imple-
mentation of a novel language-independent MT 
methodology have been presented. This meth-
odology draws on information from a large TL 
monolingual corpus and a very small bilingual 
one. The overwhelming majority of linguistic 
information is extracted in an automated manner 
using pattern recognition techniques. 
Two types of evaluation have been reported, 
these concerning objective and subjective 
evaluations. Experimental results using objective 
metrics through a period of time have indicated a 
rising trend in terms of translation quality. Also, 
it has been shown that by introducing a new 
phrasing model for the sentences to be translated 
a substantial improvement is achieved. Subjec-
tive evaluation activities have indicated a higher 
translation accuracy achieved by other MT sys-
tems. A limiting factor for the PRESEMT meth-
odology is admittedly the requirement for port-
ability to new language pairs. This leads to the 
extraction of knowledge from texts via algo-
rithmic means and the adoption of already exist-
ing linguistic tools, without modifications.  
On the other hand, subsequent versions of the 
proposed MT system have shown a trend of im-
proving translation accuracy. In this respect, ob-
jective evaluation results are promising, espe-
cially taking into account the fact that for several 
aspects, scope for improvement has been identi-
fied. This includes the revision of the structure 
selection phase, where smaller sub-sentential 
structures need to be combined to improve gen-
eralisation. In addition, improvements in the bi-
lingual corpus compilation procedure need to be 
studied. The results of these ongoing experi-
ments will be reported in the future. 
References 
Paul E. Black. 2005. Dictionary of Algorithms and 
Data Structures. U.S. National Institute of Stan-
dards and Technology (NIST). 
Chris Callison-Burch, Cameron Fordyce, Philipp 
Koehn, Christof Monz and Josh Schroeder. 2009. 
Further Meta-Evaluation of Machine Translation. 
Proceedings of the WMT-08 Workshop, Colom-
bus, Ohio. 
Chris Callison-Burch, Philip Koehn, Christof Monz, 
Omar F. Zaidan. 2011. Findings of the 
2011Workshop on Statistical Machine Translation. 
Proceedings of the 6th Workshop on Statistical 
Machine Translation, Edinburgh, UK, pp. 22?64. 
Jaime Carbonell, Steve Klein, David Miller, Michael 
Steinbaum, Tomer Grassiany and Jochen Frey. 
2006. Context-Based Machine Translation. Pro-
ceedings of the 7th AMTA Conference, Cam-
bridge, MA, USA, pp. 19-28. 
Michael Carl, Maite Melero, Toni Badia, Vincent 
Vandeghinste, Peter Dirix, Ineke Schuurman, 
Stella Markantonatou, Sokratis Sofianopoulos, 
129
Marina Vassiliou and Olga Yannoutsou. 2008. 
METIS-II: Low Resources Machine Translation: 
Background, Implementation, Results and Poten-
tials. Machine Translation, 22 (1-2):pp. 67-99. 
Helena M. Caseli, Maria das Gra?as V. Nunes and 
Mikel L. Forcada. 2008. Automatic Induction of 
Bilingual resources from aligned parallel corpora: 
Application to shallow-transfer machine transla-
tion. Machine Translation, 20:pp. 227-245. 
Michael Denkowski and Alon Lavie. 2011. Meteor 
1.3: Automatic Metric for Reliable Optimization 
and Evaluation of Machine Translation Systems. 
EMNLP 2011 Workshop on Statistical Machine 
Translation, Edinburgh, UK, pp. 85-91. 
Ioannis Dologlou, Stella Markantonatou, George 
Tambouratzis, Olga Yannoutsou, Athanasia Fourla 
and Nikos Ioannou. 2003. Using Monolingual 
Corpora for Statistical Machine Translation: The 
METIS System. Proceedings of the EAMT- CLAW 
2003 Workshop, Dublin, Ireland, pp. 61-68. 
Richard O. Duda, Peter E. Hart and David G. Scott. 
2001. Pattern Classification (2nd edition). Wiley 
Interscience, New York, U.S.A. 
David Gale and Lloyd S. Shapley. 1962. College 
Admissions and the Stability of Marriage. Ameri-
can Mathematical Monthly, 69:pp. 9-14. 
John Hutchins. 2005. Example-Based Machine 
Translation: a Review and Commentary. Machine 
Translation, 19:pp. 197-211. 
Alexandre Klementiev, Ann Irvine, Chris Callison-
Burch and David Yarowsky. 2012. Toward Statis-
tical Machine Translation without Parallel Cor-
pora. Proceedings of EACL2012, Avignon, 
France, 23-25 April, pp. 130-140. 
Philip Koehn. 2010. Statistical Machine Translation. 
Cambridge University Press, Cambridge. 
John Lafferty, Andrew McCallum and Fernando 
Pereira. 2001. Conditional Random Fields: Prob-
abilistic Models for Segmenting and Labelling Se-
quence Data. Proceedings of ICML 2011, Belle-
vue, Washington, USA, pp. 282-289. 
Harry Mairson. 1992. The Stable Marriage Problem. 
The Brandeis Review, 12:1. 
Stella Markantonatou, Sokratis Sofianopoulos, Olga 
Giannoutsou and Marina Vassiliou. 2009. Hybrid 
Machine Translation for Low- and Middle- Den-
sity Languages. Language Engineering for Lesser-
Studied Languages, S. Nirenburg (ed.), IOS Press, 
pp. 243-274. 
NIST 2002. Automatic Evaluation of Machine Trans-
lation Quality Using n-gram Co-occurrences Sta-
tistics. 
Kishore Papineni, Salim Roukos, Todd Ward, and 
Wei-Jing Zhu. 2002. BLEU: A Method for Auto-
matic Evaluation of Machine Translation. Pro-
ceedings of the 40th ACL Meeting, Philadelphia, 
USA, pp. 311-318. 
Jan Pomik?lek and Pavel Rychl?. 2008. Detecting co-
derivative documents in large text collections. 
Proceedings of LREC2008, Marrakech, Morrocco, 
pp.1884-1887. 
Prokopis Prokopidis, Byron Georgantopoulos and 
Harris Papageorgiou. 2011. A suite of NLP tools 
for Greek. Proceedings of the 10th ICGL Confer-
ence, Komotini, Greece, pp. 373-383. 
Felipe Sanchez-Martinez and Mikel L. Forcada. 
2009. Inferring Shallow-transfer Machine transla-
tion Rules from Small Parallel Corpora. Journal of 
Artificial Intelligence Research, 34:pp. 605-635. 
Helmut Schmid. 1994. Probabilistic Part-of-Speech 
Tagging Using Decision Trees. Proceedings of In-
ternational Conference on New Methods in Lan-
guage Processing, Manchester, UK, pp. 44-49. 
Temple F. Smith and Michael S. Waterman. 1981. 
Identification of Common Molecular Subse-
quences. Journal of Molecular Biology, 147:195-
197. 
Matthew Snover, Bonnie Dorr, Richard Schwartz, 
Linnea Micciulla and John Makhoul. 2006. A 
Study of Translation Edit Rate with Targeted Hu-
man Annotation. Proceedings of the 7th AMTA 
Conference, Cambridge, MA, USA, pp. 223-231. 
Sokratis Sofianopoulos, Marina Vassiliou and George 
Tambouratzis. 2012. Implementing a language-
independent MT methodology. Proceedings of the 
1st Workshop on Multilingual Modeling (held 
within the ACL-2012 Conference), Jeju, Republic 
of Korea, pp.1-10. 
Jinsong Su, Hua Wu, Haifeng Wang, Yidong Chen, 
Xiaodong Shi, Huailin Dong and Qun Liu. 2011. 
Translation Model Adaptation for Statistical Ma-
chine Translation with Monolingual Topic Infor-
mation. Proceedings of the 50th ACL Meeting, 
Jeju, Republic of Korea, pp. 459?468. 
George Tambouratzis, Fotini Simistira, Sokratis Sofi-
anopoulos, Nikos Tsimboukakis and Marina Vas-
siliou. 2011. A resource-light phrase scheme for 
language-portable MT. Proceedings of the 15th 
EAMT Conference, Leuven, Belgium, pp. 185-
192. 
130
Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 57?66,
October 25, 2014, Doha, Qatar. c?2014 Association for Computational Linguistics
Expanding the Language model in a low-resource hybrid MT system 
 
George Tambouratzis Sokratis Sofianopoulos Marina Vassiliou 
ILSP, Athena R.C ILSP, Athena R.C ILSP, Athena R.C 
giorg_t@ilsp.gr s_sofian@ilsp.gr mvas@ilsp.gr 
 
Abstract 
The present article investigates the fusion of 
different language models to improve transla-
tion accuracy. A hybrid MT system, recently-
developed in the European Commission-
funded PRESEMT project that combines ex-
ample-based MT and Statistical MT princi-
ples is used as a starting point. In this article, 
the syntactically-defined phrasal language 
models (NPs, VPs etc.) used by this MT sys-
tem are supplemented by n-gram language 
models to improve translation accuracy. For 
specific structural patterns, n-gram statistics 
are consulted to determine whether the pat-
tern instantiations are corroborated. Experi-
ments indicate improvements in translation 
accuracy. 
1 Introduction 
Currently a major part of cutting-edge research 
in MT revolves around the statistical machine 
translation (SMT) paradigm. SMT has been in-
spired by the use of statistical methods to create 
language models for a number of applications 
including speech recognition. A number of dif-
ferent translation models of increasing complex-
ity and translation accuracy have been developed 
(Brown et al., 1993). Today, several packages for 
developing statistical language models are avail-
able for free use, including SRI (Stolke et al., 
2011), thus supporting research into statistical 
methods. A main reason for the widespread 
adoption of SMT is that it is directly amenable to 
new language pairs using the same algorithms. 
An integrated framework (MOSES) has been 
developed for the creation of SMT systems 
(Koehn et al., 2007). The more recent develop-
ments of SMT are summarised by Koehn (2010). 
One particular advance in SMT has been the in-
tegration of syntactically motivated phrases in 
order to establish correspondences between 
source language (SL) and target language (TL) 
(Koehn et al., 2003). Recently SMT has been 
enhanced by using different levels of abstraction 
e.g. word, lemma or part-of-speech (PoS), in fac-
tored SMT models so as to improve SMT per-
formance (Koehn & Hoang, 2007). 
The drawback of SMT is that SL-to-TL paral-
lel corpora of the order of millions of tokens are 
required to extract meaningful models for trans-
lation. Such corpora are hard to obtain, particu-
larly for less resourced languages. For this rea-
son, SMT researchers are increasingly investigat-
ing the extraction of information from monolin-
gual corpora, including lexica (Koehn & Knight, 
2002 & Klementiev et al., 2012), restructuring 
(Nuhn et al., 2012) and topic-specific informa-
tion (Su et al., 2011). 
As an alternative to pure SMT, the use of less 
specialised but more readily available resources 
has been proposed. Even if such approaches do 
not provide a translation quality as high as SMT, 
their ability to develop MT systems with very 
limited resources confers to them an important 
advantage. Carbonell et al. (2006) have proposed 
an MT method that requires no parallel text, but 
relies on a full-form bilingual dictionary and a 
decoder using long-range context. Other systems 
using low-cost resources include METIS 
(Dologlou et al., 2003) and METIS-II (Markan-
tonatou et al., 2009), which are based only on 
large monolingual corpora to translate SL texts. 
Another recent trend in MT has been towards 
hybrid MT systems, which combine characteris-
tics from multiple MT paradigms. The idea is 
that by fusing characteristics from different para-
digms, a better translation performance can be 
attained (Wu et al., 2005). In the present article, 
the PRESEMT hybrid MT method using pre-
dominantly monolingual corpora (Sofianopoulos 
et al., 2012 & Tambouratzis et al., 2013) is ex-
tended by integrating n-gram information to im-
prove the translation accuracy. The focus of the 
article is on how to extract, as comprehensively 
as possible, information from monolingual cor-
pora by combining multiple models, to allow a 
higher quality translation. 
A review of the base MT system is performed 
in section 2. The TL language model is then de-
tailed, allowing new work to be presented in sec-
tion 3. More specifically, via an error analysis, n-
gram based extensions are proposed to augment 
57
the language model. Experiments are presented 
in section 4 and discussed in section 5. 
2 The hybrid MT methodology in brief 
The PRESEMT methodology can be broken 
down into the pre-processing stage, the post-
processing stage and two translation steps each 
of which addresses different aspects of the trans-
lation process. The first translation step estab-
lishes the structure of the translation by perform-
ing a structural transformation of the source side 
phrases based on a small bilingual corpus, to 
capture long range reordering. The second step 
makes lexical choices and performs local word 
reordering within each phrase. By dividing the 
translation process in these two steps the chal-
lenging task of both local and long distance reor-
dering is addressed. 
Phrase-based SMT systems give accurate 
translations for language pairs that only require a 
limited number of short-range reorderings. On 
the contrary, when translating between languages 
with free word order, these models prove ineffi-
cient. Instead, reordering models need to be built, 
which require large parallel training data, as 
various reordering challenges must be tackled. 
2.1 Pre-processing 
This involves PoS tagging, lemmatising and 
shallow syntactic parsing (chunking) of the 
source text. In terms of resources, the methodol-
ogy utilises a bilingual lemma dictionary, an ex-
tensive TL monolingual corpus, annotated with 
PoS tags, lemmas and syntactic phrases (chunks), 
and a very small parallel corpus of 200 sen-
tences, with tagged and lemmatised source side 
and tagged, lemmatised and chunked target side. 
The bilingual corpus provides samples of the 
structural transformation from SL to TL. During 
this phase, the translation methodology ports the 
chunking from the TL- to the SL-side, alleviating 
the need for an additional parser in SL. An ex-
ample of the pre-processing stage is shown in 
Figure 1, for a sentence translated from Greek to 
English. For this sentence, the chunk structure is 
shown at the bottom part of Figure 1. 
2.2 Structure Selection 
Structure selection transforms the input text us-
ing the limited bilingual corpus as a structural 
knowledge base, closely resembling the ?transla-
tion by analogy? aspect of EBMT systems (Hut-
chins, 2005). Using available structural informa-
tion, namely the order of syntactic phrases, the 
PoS tag of the head token of each phrase and the 
case of the head token (if available), we retrieve 
the most similar source side sentence from the 
parallel corpus. Based on the alignment informa-
tion from the bilingual corpus between SL and 
TL, the input sentence structure is transformed to 
the structure of the target side translation. 
For the retrieval of the most similar source 
side sentence, an algorithm from the dynamic 
programming paradigm is adopted (Sofianopou-
los et al., 2012), treating the structure selection 
process as a sequence alignment, aligning the 
input sentence to an SL side sentence from the 
aligned parallel corpus and assigning a similarity 
score. The implementation is based on the Smith-
Waterman algorithm (Smith and Waterman, 
1981), initially proposed for similarity detection 
between protein sequences. The algorithm finds 
the optimal local alignment between the two in-
put sequences at clause level. 
The similarity of two clauses is calculated by 
taking into account the edit operations (replace-
ment, insertion or removal) that must be applied 
to the input sentence in order to transform it to a 
source side sentence from the corpus. Each of 
these operations has an associated cost, consid-
ered as a system parameter. The parallel corpus 
sentence that achieves the highest similarity 
score is the most similar one to the input source 
sentence. For the example of Figure 1, the com-
parison of the SL sentence structure to the paral-
lel corpus is schematically depicted in Figure 2. 
The resulting TL sentence structure is shown in 
Figure 3 in terms of phrase types and heads. 
 
  	
