Proceedings of the Workshop on Sentiment and Subjectivity in Text, pages 55?62,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Towards a validated model for affective classification of texts
Michel Ge?ne?reux and Roger Evans
Natural Language Technology Group (NLTG)
University of Brighton, United Kingdom
{M.Genereux,R.P.Evans}@brighton.ac.uk
Abstract
In this paper, we present the results of
experiments aiming to validate a two-
dimensional typology of affective states as
a suitable basis for affective classification
of texts. Using a corpus of English weblog
posts, annotated for mood by their authors,
we trained support vector machine binary
classifiers to distinguish texts on the ba-
sis of their affiliation with one region of
the space. We then report on experiments
which go a step further, using four-class
classifiers based on automated scoring of
texts for each dimension of the typology.
Our results indicate that it is possible to
extend the standard binary sentiment anal-
ysis (positive/negative) approach to a two
dimensional model (positive/negative; ac-
tive/passive), and provide some evidence
to support a more fine-grained classifica-
tion along these two axes.
1 Introduction
We are investigating the subjective use of language
in text and the automatic classification of texts ac-
cording to their subjective characteristics, or ?af-
fect?. Our approach is to view affective states
(such as ?happy?, ?angry?) as locations in Osgood?s
Evaluation-Activation (EA) space (Osgood et al ,
1957), and draws on work in psychology which
has a long history of work seeking to construct a
typology of such affective states (Scherer, 1984).
A similar approach has been used more recently
to describe emotional states that are expressed in
speech (Cowie and Cornelius, 2002; Schro?der and
Cowie, 2005). Our overall aim is to determine
the extent to which such a typology can be vali-
dated and applied to the task of text classification
using automatic methods. In this paper we de-
scribe some initial experiments aimed at validating
a basic two dimensional classification of weblog
data, first with Support Vector Machine (SVM)
binary classifiers, then with Pointwise Mutual In-
formation - Information Retrieval (PMI-IR). The
domain of weblog posts is particularly well-suited
for this task given its highly subjective nature and
the availability of data , including data which has
been author-annotated for ?mood?, which is a rea-
sonable approximation of ?affect?.
Recent attempts to classify weblog posts have
shown modest, but consistent improvements over
a 50% baseline, only slightly worse than human
performance (Mishne, 2005). One important mile-
stone is the elaboration of a typology of affec-
tive states. To devise such a typology, our start-
ing point is Figure 1, which is based on a model
of emotion as a multicomponent process (Scherer,
1984). In this model, the distribution of the af-
fective states is the result of analysing similar-
ity judgments by humans for 235 emotion terms1
using cluster-analysis and multidimensional scal-
ing techniques to map out the structure as a two-
dimensional space. The positioning of words is
not so much controversial as fuzzy; an affective
state such as ?angry? to describe facial expression
in speech may have a slightly different location
than an ?angry? weblog post. In this model, the
well-studied ?sentiment? classification is simply a
specific case (left vs. right halves of the space).
The experiments we describe here seek to go be-
yond this basic distinction. They involve an addi-
tional dimension of affect, the activity dimension,
allowing textual data to be classified into four cat-
egories corresponding to each of the four quad-
1Reduced to less than 100 in Figure 1.
55
Figure 1: Typology of affective states based on (Scherer, 1984)
rants in the space. Ultimately, once scores have
been ?promoted? to real measures, classification
can be more precise; for example, a text is not only
negative and passive, it is more precisely ?depres-
sive?. With such a more precise classification one
might, for example, be able to detect individuals
at risk of suicide. In Experiment 1, we use bi-
nary classifiers to investigate how the four quad-
rants defined by the typology hold together, the
assumption being that if the typology is correct,
the classifiers should perform substantially better
than a random baseline. In Experiment 2, we go
a step closer towards a more fine-grained classifi-
cation by evaluating the performance of an unsu-
pervised automated technique for scoring texts on
both axes. Both these experiments are preliminary
? our long term goal is to be able to validate the
whole typology in terms of computationally effec-
tive classification.
2 Corpus
We have collected from Livejournal2 a total of
346723 weblogs (mood-annotated by authors) in
2http://www.livejournal.com.
English, from which almost half are annotated
with a mood belonging to one of the four quad-
rants, described as follows:
Quadrant1 bellicose, tense, alarmed, envious,
hateful, angry, enraged, defiant, annoyed, jealous,
indignant, frustrated, distressed, disgusted, sus-
picious, discontented, bitter, insulted, distrustful,
startled, contemptuous and impatient.
Quadrant2 apathetic, disappointed, miserable,
dissatisfied, taken aback, worried, languid, feel
guilt, ashamed, gloomy, sad, uncomfortable, em-
barrassed, melancholic, depress, desperate, hes-
itant, bored, wavering, droopy, tired, insecured,
anxious, lonely and doubtful.
Quadrant3 feel well, impressed, pleased,
amourous, astonished, glad, content, hopeful,
solemn, attentive, longing, relaxed, serious,
serene, content, at ease, friendly, satisfied,
calm, contemplative, polite, pensive, peaceful,
conscientious, empathic, reverent and sleepy.
Quadrant4 happy, ambitious, amused, adven-
turous, aroused, astonished, triumphant, excited,
56
conceited, self confident, courageous, feeling su-
perior, enthusiastic, light hearthed, determined,
passionate, expectant, interested, joyous and de-
lighted.
In our experiments, we used 15662 from quad-
rant Q1 (see Figure 1), 54940 from Q2, 49779
from Q3 and 35634 from Q4.
3 Experiment 1: Distinguishing the four
Quadrants
Our hypothesis is that the classification of two dis-
joint sets of moods should yield a classification ac-
curacy significantly above a baseline of 50%. To
verify our hypothesis, we conducted a series of ex-
periments using machine learning to classify we-
blog posts according to their mood, each class cor-
responding to one particular quadrant. We used
Support Vector Machines (Joachims, 2001) with
three basic classic features (unigrams, POS and
stems) to classify the posts as belonging to one
quadrant or one of the three others. For each clas-
sification task, we extracted randomly 1000 test-
ing examples, and trained separately with 2000,
4000, 8000 and 16000 examples. In each case, ex-
amples were divided equally among positive and
negative examples3. The set of features used var-
ied for each of these tasks, they were selected by
thresholding each (distinct) training data set, after
removing words (unigrams) from the categories
poor in affective content (prepositions, determin-
ers, etc.). To qualify as a feature, each unigram,
POS or stem had to occur at least three times in
the training data. The value of each feature corre-
sponds to its number of occurence in the training
examples.
3.1 Results
Our hypothesis is that, if the four quadrants de-
picted in Figure 1 are a suitable arrangement for
affective states in the EA space, a classifier should
perform significantly better than chance (50%).
Table 1 shows the results for the binary classifi-
cation of the quadrants. In this table, the first col-
umn identifies the classification task in the form
?P vs N?, where ?P? stands for positive examples
and ?N? for negative examples. The ?Random? row
shows results for selecting positive and negative
examples randomly from all four quadrants. By
3For instance, 1000 = 500 positives from one QUAD-
RANT + 500 negatives among the other three QUAD-
RANTS.
micro-averaging accuracy for the classification of
each quadrant vs all others (rows 10 to 13), we
obtain at least 60% accuracy for the four binary
classifications of the quadrants4. The first six rows
show evidence that each quadrant forms a distinc-
tive whole, as the classifer can easily decide be-
tween any two of them.
Testing Size of training set
1000 examples 2k 4k 8k 16k
Q1 vs Q3 67% 70% 72% 73%
Q2 vs Q4 61% 64% 65% 67%
Q1 vs Q2 64% 66% 68% 69%
Q2 vs Q3 58% 59% 59% 59%
Q3 vs Q4 59% 60% 60% 61%
Q4 vs Q1 69% 72% 73% 75%
Q1+4 vs Q2+3 56% 58% 58% 61%
Q3+4 vs Q1+2 62% 65% 67% 66%
Random 49% 52% 50% 50%
Q1 vs Q2+3+4 67% 72% 72% 73%
Q2 vs Q1+3+4 59% 60% 63% 63%
Q3 vs Q1+2+4 57% 58% 58% 59%
Q4 vs Q1+2+3 60% 63% 65% 65%
Micro-accuracy 61% 64% 65% 65%
Table 1: Accuracy of binary classification
3.2 Analysis of Results
We introduce now table 2 that shows two thresh-
olds of significance (1% and 5%) for the interpre-
tation of current and coming results. For exam-
ple, if we have 1000 trials with each trial having a
probability of success of 0.5, the likelihood of get-
ting at least 53.7% of the trials right is only 1%.
This gives us a baseline to see how significantly
well above chance a classifier performs. The SVM
algorithm has linearly separated the data for each
quadrant according to lexical and POS content (the
features). The most sensible explanation is that the
features for each class (quadrant) are semantically
related, a piece of information which is relevant
for the model (see section 4). It is safe to conclude
that the results cannot be allocated to chance, that
there is something else at work that explains the
4Micro-averaged accuracy is defined as:
?
i (tpi + tni)
?
i (tpi + tni + fpi + fni)
where tp stands for ?true positive?, fn for ?false negative?,
etc.
57
Trials Prob(Success) 1% 5%
1000 0.50 53.7% 52.6%
750 0.50 54.3% 53.1%
500 0.50 55.2% 53.6%
250 0.50 57.2% 55.2%
1000 0.25 28.2% 27.3%
750 0.25 28.7% 27.6%
500 0.25 29.6% 28.2%
250 0.25 31.6% 29.6%
Table 2: Statistical Significance
accuracies consistently well above a baseline, and
this something else is the typology. These results
show that the abstraction offered by the four quad-
rants in the model seems correct. This is also sup-
ported by the observation that the classifier shows
no improvements over the baseline if trained over
a random selection of examples in the entire space.
4 Experiment 2: Classification using
Semantic Orientation from Association
Our next goal is to be able to classify a text accord-
ing to more than four classes (positive/negative,
active/passive), by undertaking multi-category
classification of texts according to particular re-
gions of the space, (such as ?angry?, ?sad?, etc.). In
order to do that we need a scoring system for each
axis. In the following experiments we explore the
use of such scores and give some insights into how
to transform these scores of affect as measures of
affect.
Using binary classifiers, we have already estab-
lished that if we look at the lexical contents of we-
blog posts tagged according to their mood by their
author, these mood classes tend to cluster accord-
ing to a two-dimensional typology defined by their
semantic orientation: positive or negative (evalu-
ation), active or passive (activity). Beyond aca-
demic importance, the typology really becomes of
practical interest if we can classify the posts us-
ing pre-defined automated scores for both axis.
One strategy of scoring is to extract phrases, in-
cluding single words, which are good indicators
of subjectivity in texts, and score them accord-
ing to how they relate or ?associate? to one or the
other extremity of each axis. This strategy, called
Semantic Orientation (SO) from Association (A)
has been used successfully (Turney and Littman,
2003) to classify texts or adjectives of all sorts ac-
cording to their sentiments (in our typology this
corresponds to the evaluation dimension). Ac-
cording to these scores, a text or adjective can be
said to have, for example, a more or less positive
or negative evaluation. We will use this strategy to
go further in the validation of our model of affec-
tive states by scoring also the activity dimension;
to our knowledge, this is the first time this strat-
egy is employed to get (text) scores for dimen-
sions other than evaluation. In SO-A, we score
the strength of the association between an indica-
tor from the text and a set of positive or negative
words (the paradigms Pwords and Nwords) cap-
turing the very positive/active or negative/passive
semantic orientation of the axis poles. To get the
SO-A of a text, we sum over positive scores for
indicators positively related to Pwords and nega-
tively related to Nwords and negative scores for
indicators positively related to Nwords and nega-
tively related to Pwords. In mathematical terms,
the SO-A of a text is:
Text
?
ind
(
Pwords
?
p
A(ind, p) ?
Nwords
?
n
A(ind, n))
where ind stands for indicator. Note that the quan-
tity of Pwords must be equal to Nwords.
To compute A, (Kamps et al , 2004) focus
on the use of lexical relations defined in Word-
Net5 and define a distance measure between two
terms which amounts to the length of the short-
est path that connects the two terms. This strat-
egy is interesting because it constrains all values
to belong to the [-1,+1] range, but can be applied
only to a finite set of indicators and has yet to
be tested for the classification of texts. (Turney
and Littman, 2003) use Pointwise Mutual Infor-
mation - Information Retrieval (PMI-IR); PMI-IR
operates on a wider variety of multi-words indi-
cators, allowing for contextual information to be
taken into account, has been tested extensively on
different types of texts, and the scoring system can
be potentially normalized between [-1,+1], as we
will soon see. PMI (Church and Hanks, 1990) be-
tween two phrases is defined as:
log2
prob(ph1 is near ph2)
prob(ph1) ? prob(ph2)
PMI is positive when two phrases tend to co-occur
and negative when they tend to be in a comple-
mentary distribution. PMI-IR refers to the fact
5http://wordnet.princeton.edu/.
58
that, as in Informtion Retrieval (IR), multiple oc-
currences in the same document count as just one
occurrence: according to (Turney and Littman,
2003), this seems to yield a better measure of
semantic similarity, providing some resistance to
noise. Computing probabilities using hit counts
from IR, this yields to a value for PMI-IR of:
logn
N ? (hits(ph1 NEAR ph2) + 1/N)
(hits(ph1) + 1) ? (hits(ph2) + 1)
where N is the total number of documents in the
corpus. We are going to use this method for com-
puting A in SO-A, which we call SO-PMI-IR. The
configuration depicted in the remaining of this sec-
tion follows mostly (Turney and Littman, 2003).
Smoothing values (1/N and 1) are chosen so that
PMI-IR will be zero for words that are not in the
corpus, two phrases are considered NEAR if they
co-occur within a window of 20 words, and log2has been replaced by logn, since the natural log ismore common in the literature for log-odds ratio
and this makes no difference for the algorithm.
Two crucial aspects of the method are the choice
of indicators to be extracted from the text to be
classified, as well as the sets of positive and neg-
ative words to be used as paradigms for the eval-
uation and activity dimensions. The five part-of-
speech (POS) patterns from (Turney, 2002) were
used for the extraction of indicators, all involving
at least one adjective or adverb. POS tags were
acquired with TreeTagger (Schmid, 1994)6. Ide-
ally, words used as paradigms should be context
insensitive, i.e their semantic orientation is either
always positive or negative. The adjectives good,
nice, excellent, positive, fortunate, correct, supe-
rior and bad, nasty, poor, negative, unfortunate,
wrong, inferior were used as near pure representa-
tions of positive and negative evaluation respec-
tively, while fast, alive, noisy, young and slow,
dead, quiet, old as near pure representations of ac-
tive and passive activity (Summers, 1970).
Departing from (Turney and Littman, 2003),
who uses the Alta Vista advanced search with ap-
proximately 350 millions web pages, we used the
Waterloo corpus7, with approximately 46 millions
pages. To avoid introducing confusing heuristics,
we stick to the configuration described above, but
(Turney and Littman, 2003) have experimented
with different configuation in computing SO-PMI-
IR.
6(Turney and Littman, 2003) uses (Brill, 1994).
7http://canola1.uwaterloo.ca/.
4.1 The Typology and SO-PMI-IR
We now use the typology with an automated scor-
ing method for semantic orientation. The results
are presented in the form of a Confusion Matrix
(CM). In this and the following matrices, the top-
left cell indicates the overall accuracy8, the POS-
itive (ACTive) and NEGative (PASsive) columns
represent the instances in a predicted class, the
P/T column (where present) indicates the average
number of patterns per text (blog post), E/P indi-
cates the average evaluation score per pattern and
A/P indicates the average activity score per pat-
tern. Each row represents the instances in an ac-
tual class9.
First, it is useful to get a clear idea of how
the SO-PMI-IR experimental setup we presented
compares with (Turney and Littman, 2003) on a
human-annotated set of words according to their
evaluation dimension: the General Inquirer (GI,
(Stone, 1966)) lexicon is made of 3596 words
(1614 positives and 1982 negatives)10. Table 3
summarizes the results. (Turney and Littman,
(U) 76.4% POS NEG E/P
POS(1614) 59.3% 40.7% 1.5
NEG(1982) 9.6% 90.4% -4.3
(T) 82.8% POS NEG E/P
POS(1614) 81.2% 18.8% 3.2
NEG(1982) 15.8% 84.2% -3.6
Table 3: CM for the GI: (U)Us and (T)(Turney and
Littman, 2003)
2003) reports an accuracy of 82.8% while clas-
sifying those words, while our experiment yields
an accuracy of 76.4% for the same words. Their
results show that their classifier errs very slightly
towards the negative pole (as shown by the accura-
cies of both predicted classes) and has a very bal-
anced distribution of the word scores (as shown
by the almost equal but opposite in signs values
of E/Ps). This is some evidence that the paradigm
words are appropriate as near pure representations
of positive and negative evaluation. By contrast,
8Recall that table 2 gives an interpretation of the statistical
signifiance of accuracy, with trials ? 750 and Prob(success)
= 0.5.
9For example, in the comparative evaluation shown in ta-
ble 3, our classifier classified 59.3% of the 1614 positive in-
stances as positive and 40.7% as negative, with an average
score of 1.5 per pattern.
10Note that all moods in the typology present in the GI
have the same polarity for evaluation in both, which is some
evidence in favour of the typology.
59
our classifier appears to be more strongly biased
towards the negative pole, probably due to the use
of different corpora. This bias11should be kept in
mind in the interpretation of the results to come.
The second experiment focuses on the words
from the typology. Table 4 shows the results. The
81.1% POS NEG P/T E/P
POS(43) 60.5% 39.5% 1 0.4
NEG(47) 0.0% 100.0% 1 -6.4
66.7% ACT PAS P/T A/P
ACT(39) 33.3% 66.7% 1 -0.9
PAS(51) 7.8% 92.2% 1 -2.9
Table 4: CM for the Typology affective states
value of 1 under P/T reflects the fact that the ex-
periment amounts, in practical terms, to classify-
ing the annotation of the post (a single word). For
the evaluation dimension, there is another shift to-
wards the negative pole of the axis, which suggests
that words in the typology are distributed not ex-
actly as shown on figure 1, but instead appear to
have a true location shifted towards the negative
pole. The activity dimension also appear to have
a negative (i.e passive) bias. There are two main
possible reasons for that: words in the typology
should be shifted towards the passive pole (as in
the evaluation case), or the paradigm words for the
passive pole are not pure representations of the ex-
tremity of the pole 12.
Having established that our classifier has a neg-
ative bias for both axes, we now turn to the classifi-
cation of the quadrants per se. In the next section,
we used SO-PMI-IR to classify 1000 randomnly
selected blog posts from our corpus, i.e 250 in
each of the four quadrants. Some of these posts
were found to have no pattern and were therefore
not classified, which means that less than 1000
posts were actually classified in each experiment.
We also report on the classification of an impor-
tant subcategory of these moods called the Big Six
emotions.
11Bias can be introduced by the use of a small corpus, inad-
equate paradigm words or typology. In practice, a quick fix
for neutralizing bias would be to normalize the SO-PMI-IR
values by subtracting the average. This work aims at tuning
the model to remove bias introduced by unsound paradigm
words or typology.
12At the time of experimenting, we were not aware
of an equivalent of the GI to independently verify our
paradigm words for activity, but one reviewer pointed out
such a resource, see http://www.wjh.harvard.edu/
?inquirer/spreadsheet_guide.htm.
4.2 Results
Of the 1000 blog posts, there were 938 with at
least one pattern. Table 5 shows the accuracy for
the classification of these posts.
56.8% POS NEG P/T E/P
POS(475) 76.2% 23.8% 10 5.2
NEG(463) 63.1% 36.9% 9 3.5
51.8% ACT PAS P/T A/P
ACT(461) 20.6% 79.4% 8 -4.3
PAS(477) 18.0% 82.0% 11 -4.2
Table 5: CM for all Moods
An important set of emotions found in the liter-
ature (Ekman, 1972) has been termed the Big Six.
These emotions are fear, anger, happiness, sad-
ness, surprise and disgust. We have used a mini-
mally extended set, adding love and desire (Cowie
and Cornelius, 2002), to cover all four quadrants
(we called this set the Big Eight). Fear, anger and
disgust belong to quadrant 1, sadness and surprise
(we have taken it to be a synonym of ?taken aback?
in the typology) belong to quadrant 2, love and
desire (taken to be synonyms of ?amorous? and
?longing? in the typology) belong to quadrant 3
and happy to quadrant 4. Table 6 shows the results
for the classification of the blog posts that were
tagged with one of these emotions. This amounts
to classifying the posts containing only the Big
Eight affective states.
59.0% POS NEG P/T E/P
POS(467) 72.4% 27.6% 9 5.1
NEG(351) 58.7% 41.3% 6 2.3
54.9% ACT PAS P/T A/P
ACT(357) 23.8% 76.2% 8 -4.4
PAS(461) 21.0% 79.0% 8 -4.6
Table 6: CM for the Big Eight
In the remaining two experiments, blog posts
have been classifed using a discrete scoring sys-
tem. Disregarding the real value of SO, each pat-
tern was scored with a value of +1 for a positive
score and -1 for a negative score. This amounts to
counting the number of patterns on each side and
has the advantage of providing a normalized value
for E/T and A/T between -1 and +1. Normalized
values are the first step towards a measure of af-
fect, not merely a score, in the sense that it gives
an estimate of the strength of affect. We have not
60
classified the posts for which the resulting score
was zero, which means that even fewer posts (741)
than the previous experiment were actually evalu-
ated. Table 7 shows the results for all moods and
table 8 for the Big Eight.
55.7% POS NEG P/T E/P
POS(374) 53.2% 46.8% 11 0.03
NEG(367) 41.7% 58.3% 9 -0.11
53.3% ACT PAS P/T A/P
ACT(357) 21.8% 78.2% 8 -0.3
PAS(384) 17.4% 82.6% 12 -0.34
Table 7: CM for all Moods: Discrete scoring
59.8% POS NEG P/T E/P
POS(373) 52.3% 47.7% 10 0.01
NEG(354) 32.2% 67.8% 9 -0.2
52.8% ACT PAS P/T A/P
ACT(361) 25.8% 74.2% 10 -0.3
PAS(366) 20.5% 79.5% 9 -0.4
Table 8: CM for the Big Eight: Discrete scoring
4.3 Analysis of Results
Our concerns about the paradigm words for eval-
uating the activity dimension are clearly revealed
in the classification results. The classifier shows a
heavy negative (passive) bias in all experiments.
The overall accuracy for activity is consistently
below that for evaluation: three of them are not
statistically significant at 1% (51.8%, 53.3% and
52.8%) and two at even 5% (51.8% and 52.8%).
The classifier appears particularly confused in ta-
ble 5, averaging a score for active posts (-4.3)
smaller than for passive posts (-4.2). It is not
impossible that the moods present in the typol-
ogy may have to be shifted towards the passive
dimension, but further research should look first
at finding better paradigm words for activity. A
good starting point for the calibration of the clas-
sifier for activity is the creation of a list of human-
annotated words for activity, comparable in size to
the GI list, combined with an experiment similar
to the one for which results are reported in table 3.
With regards to the evaluation dimension, ta-
bles 5 and 6 reveal a positive bias (despite having a
classifier which has a ?built-in? negative bias, see
section 4.1). Possible explanations for this phe-
nomenon include the use of irony by people in
negative posts, blogs which are expressed in more
positive terms than their annotation would suggest,
and failure to detect ?negative? contexts for pat-
terns ? one example of the latter is provided in
table 9. This phenomena appears to be alleviated
Mood: bored (evaluation-)
Post: gah!! i need new music, any
suggestions? by the way,
GOOD MUSIC.
Patterns: new music [JJ NN] +4.38
GOOD MUSIC [JJ NN] +53.40
Average SO: +57.78 (evaluation+)
Table 9: Missclassified post
by the use of discrete scores (see tables 7 and 8).
One way of refining the scoring system is to re-
duce the effect of scoring antonyms as high as syn-
onyms by not counting co-occurences in the cor-
pus where the word ?not? is in the neighbourhood
(Turney, 2001). Also,
The long-term goal of this research is to be
able to classify texts by locating their normal-
ized scores for evaluation and activity between
-1 and +1, and we have suggested a simple
method of achieving that by averaging over dis-
crete scores. However, by combining individual
results for evaluation and activity for each post13,
we can already classify text into one of the four
quadrants, and we can expect the average accuracy
of this classification to be approximately the prod-
uct of the accuracy for each dimension. Table 10
shows the results for the classification directly into
quadrants of the 727 posts already classified into
halves (E?, A?) in table 8. The overall accuracy
is 31.1% (expected accuracy is 59.8% * 52.8% =
31.6%). There are biases towards Q2 and Q3, but
no clear cases of confusion between two or more
classes.
31.1% Q1 Q2 Q3 Q4
Q1(180) 21.1% 47.8% 22.2% 8.9%
Q2(174) 15.5% 51.1% 25.3% 8.0%
Q3(192) 9.9% 42.2% 40.1% 7.8%
Q4(181) 9.4% 33.7% 44.8% 12.2%
Table 10: CM for Big Eight: Discrete scoring
Finally, our experiments show no correlation
between the length of a post (in number of pat-
terns) and the accuracy of the classification.
13For example, a post with E- and A+ would be classified
in Q1.
61
5 Conclusion and Future Work
In this paper, we have used a machine learning ap-
proach to show that there is a relation between the
semantic content of texts and the affective state
they (wish to) convey, so that a typology of affec-
tive states based on semantic association is a good
description of the distribution of affect in a two-
dimensional space. Using automated methods to
score semantic association, we have demonstrated
a method to compute semantic orientation on both
dimensions, giving some insights into how to go
beyond the customary ?sentiment? analysis. In the
classification experiments, accuracies were always
above a random baseline, although not always sta-
tistically significant. To improve the typology and
the accuracies of classifiers based on it, a better
calibration of the activity axis is the most press-
ing task. Our next steps are experiments aiming
at refining the translation of scores to normalized
measures, so that individual affects can be distin-
guished within a single quadrant. Other interest-
ing avenues are studies investigating how well the
typology can be ported to other textual data do-
mains, the inclusion of a ?neutral? tag, and the
treatment of texts with multiple affects.
Finally, the domain of weblog posts is attractive
because of the easy access to annotated data, but
we have found through our experiments that the
content is very noisy, annotation is not always con-
sistent among ?bloggers?, and therefore classifica-
tion is difficult. We should not underestimate the
positive effects that cleaner data, consistent tag-
ging and access to bigger corpora would have on
the accuracy of the classifier.
Acknowledgement
This work was partially funded by the European
Commission through the Network of Excellence
EPOCH (?Excellence in Processing Open Cultural
Heritage?). Thanks to Peter Turney for the provi-
sion of access to the Waterloo MultiText System.
References
Eric Brill. 1994. Some advances in transformation-
based part of speech tagging. Proc. of 12th National
Conference on AI. pp. 722-727. Menlo Park, CA:
AAAI Press.
Kenneth Ward Church and Patrick Hanks. 1990. Word
association norms, mutual information, and lexicog-
raphy. Computational Linguistics. Vol. 16, No 1.
pages 22?29, MIT Press, Cambridge, MA, USA.
Roddy Cowie and Randolph R. Cornelius. 2002. De-
scribing the emotional states that are expressed in
speech. Speech Communication 1228. Elsevier Sci-
ence B.V.. 20 June 2002, 28 pages.
Paul Ekman. 1972. Universal and cultural differences
in facial expression of emotion. J.K. Cole (Eds),
Nebraska Symposium on Motivation. pp 207-282.
Lincoln, University of Nebraska Press.
Thorsten Joachims. 2001. Learning to Classify Text
Using Support Vector Machines. Kluwer Academic
Publishers.
Jaap Kamps and Robert J. Mokken and Maarten Marx
and Maarten de Rijke. 2004. Using WordNet to
measure semantic orientation of adjectives. Proc.
of LREC 2004. Vol. IV, pages 1115-1118.
Gilad Mishne. 2005. Experiments with mood classifi-
cation in blog posts. In Style2005 - the 1st Work-
shop on Stylistic Analysis Of Text For Information
Access, at SIGIR 2005.
Charles E. Osgood, George J. Suci, and Percy H. Tan-
nenbaum. 1957. The Measurement of Meaning.
University of Illinois.
Klaus R. Scherer. 1984. Emotion as a Multicompo-
nent Process: A model and some cross-cultural data.
In P. Shaver (Ed.) Review of Personality and Social
Psych. Vol. 5 (pp. 37-63). Beverley Hills, CA: Sage.
H. Schmid. 1994. Probabilistic part-of-speech tagging
using decision trees. In International Conf. on New
Methods in Language Processing. Manchester UK.
Marc Schro?der and Roddy Cowie. 2005. Towards
emotion-sensitive multimodal interfaces. Invitated
talk at the Workshop on ?Adapting the interaction
style to affective factors? pp. 235-253. User Mod-
elling 2005, July 25, Edinburgh.
Philip J. Stone and Dexter C. Dunphy and Marshall S.
Smith and Daniel M. Ogilvie. 1966. The General
Inquirer: A Computer Approach to Content Anal-
ysis. MIT Press. http://www.webuse.umd.
edu:9090/.
Gene F. Summers. 1970. Attitude measurement.
Chicago: Rand McNally. pp. 235-253.
Peter Turney. 2001. Mining the Web for Syn-
onyms: PMI-IR versus LSA on TOEFL. Eu-
ropean Conference on Machine Learning.
pp 491?502. citeseer.nj.nec.com/
turney01mining.html.
Peter D. Turney. 2002. Thumbs Up or Thumbs
Down? Semantic Orientation Applied to Unsuper-
vised Classification of Reviews. Proc. of the ACL
2002. Philadelphia, USA. July 8-10, 2002, pp 417-
424.
Peter D. Turney and Michael L. Littman. 2003. Mea-
suring praise and criticism: Inference of semantic
orientation from association. ACM Trans. Inf. Syst.
21(4):315346.
62
Proceedings of the Workshop on Language Technology for Cultural Heritage Data (LaTeCH 2007), pages 41?48,
Prague, 28 June 2007. c?2007 Association for Computational Linguistics
Cultural Heritage Digital Resources: from Extraction to Querying
Michel Ge?ne?reux
Natural Language Technology Group
University of Brighton
Umited Kingdom
M.Genereux@brighton.ac.uk
Abstract
This article presents a method to extract and
query Cultural Heritage (CH) textual dig-
ital resources. The extraction and query-
ing phases are linked by a common on-
tological representation (CIDOC-CRM). A
transport format (RDF) allows the ontol-
ogy to be queried in a suitable query lan-
guage (SPARQL), on top of which an inter-
face makes it possible to formulate queries
in Natural Language (NL). The extraction
phase exploits the propositional nature of
the ontology. The query interface is based
on the Generate and Select principle, where
potentially suitable queries are generated to
match the user input, only for the most se-
mantically similar candidate to be selected.
In the process we evaluate data extracted
from the description of a medieval city
(Wolfenbu?ttel), transform and develop two
methods of computing similarity between
sentences based on WordNet. Experiments
are described that compare the pros and
cons of the similarity measures and evaluate
them.
1 Introduction
The CIDOC-CRM (DOERR, 2005) ontology is an
ISO standard created to describe in a formal lan-
guage the explicit and implicit concepts and rela-
tions underlying the documentation produced in CH.
The ontology aims at accommodating a wide variety
of data from the CH domain, but its sheer complex-
ity may make it difficult for non-experts to learn it
quickly, let alne use it efficiently. For others, it may
even be simpler to find a way to translate automati-
cally their data from the storage mechanism already
in place into CIDOC-CRM. For practitioners unfa-
miliar with strict formalisms, it may be more natural
to describe collections in natural language (e.g. En-
glish), and there is already an unprecedented wealth
of information available on-line in natural language
for almost anything, including CH. Wouldn?t it be
practical to be able to describe a collection of arti-
facts in plain English, with little or no knowledge
of the CIDOC-CRM formalism, and let language
technology take over and produce a CIDOC-CRM
database? The principle behind that idea is based
on the observation that the building blocks of the
CIDOC-CRM ontology, the triples, have a pred-
icative nature, which is structurally consistent with
the way many natural languages are built (DOERR,
2005):
The domain class is analogous to the
grammatical subject of the phrase for
which the property is analogous to the
verb. Property names in the CRM are de-
signed to be semantically meaningful and
grammatically correct when read from do-
main to range. In addition, the inverse
property name, normally given in paren-
theses, is also designed to be semanti-
cally meaningful and grammatically cor-
rect when read from range to domain.
A triple is defined as:
DOMAIN PROPERTY RANGE
41
The domain is the class (or entity) for which a prop-
erty is formally defined. Subclasses of the domain
class inherit that property. The range is the class
that comprises all potential values of a property.
Through inheritance, subclasses of the range class
can also be values for that property. Example 1 is
somewhat artificial, but it illustrates how triples can
be extracted from natural language, where entities
E48 and E53 are Place Name and Place respectively,
while P1 is the property identify.
(1) Rome
DOM E41
E48
identifies
PROP P1
P1
the capital of Italy.
RANGE E1
E53
?Rome identifies the capital of Italy.?
The task of the natural language processing tool is to
map relevant parts of texts to entities and properties
in such a way that triples can be constructed (see also
(SHETH, 2003; SCHUTZ, 2005; DAGAN, 2006)).
In a nutshell, the Noun Clauses (NC) Rome and the
capital of Italy are mapped to Entity 48 and Entity
53, themselves subclasses of the domain E41 and
range E1 respectively, while the Verb Clause (VC)
identifies is mapped to Property P1.
On the other hand, a natural language interface
(ANDROUTSOPOULOS, 1995) to query struc-
turally complex and semantically intertwined data
such as those that can be found in the archaeological
domain can lighten a great deal the tasks of browsing
and searching. This state of affairs is even more true
for people not familiar with formal languages, as is
often the case in archaeology in particular and cul-
tural heritage in general. With the Semantic Web1
in full development and ontologies such as CIDOC-
CRM teaming together to render semantic naviga-
tion a realistic prospect, natural language interfaces
can offer a welcomed simplified view of the under-
lying data.
One of the most important and Semantic Web
oriented conceptual model available today is the
CIDOC-CRM, which is becoming the new standard
model to document CH data: the creation of tools
ready to manage CIDOC-CRM compliant archives
will be one of the most important goals of the com-
ing years (HERMON, 2000). The full implemen-
tation of the CIDOC-CRM model is simplified to-
1http://www.w3.org/2001/sw/
day by a family of languages developed by the
World Wide Web Consortium2 and XML-based (LI,
2006). One of its most important representative is
RDF3, on top of which sits a query language such as
SPARQL4.
2 Extraction
2.1 Methodology
TRIPLEhhhhhhh??
(((((((
NC
NP
Rome
VC
VVZ
identifies
NCaaa!!!
NC
QQ??
DT
the
NN
capital
PC
ll,,
IN
of
NC
NP
Italy
Figure 1: Linguistic parse tree for example 1.
Figure 1 suggests that all pairs of NC separated by
a VC (and possibly other elements) are potentially
valid CIDOC-CRM triples. Part-of-speeches (POS)
and phrasal clauses can be obtained with a POS tag-
ger and chunker5. To validate the triples, we must
first make sure that the predicate is relevant by ex-
tracting the main verb of the verbal clause (VC) and
see if its meaning is similar (synonym) to at least
one of the CIDOC-CRM properties. For example, it
is possible to use the verb describe instead of iden-
tify. Once a set of possible properties is identified,
we must verify if the noun clauses (NC) surround-
ing the property are related to the DOMAIN and the
RANGE of that property. To establish the relation,
the first step is to identify the semantics of each NC
clause. For English, a good indicator of the NC se-
mantics is the rightmost NN in the clause, excluding
any attached PC. The rightmost NN is usually the
most significant: for example, in the NC the mu-
seum artifact, the main focus point is artifact, not
museum. In figure 1 the rightmost NN of the capital
2W3C: http://www.w3.org/
3http://www.w3.org/RDF/
4http://www.w3.org/TR/2006/CR-rdf-sparql-query-
20060406/
5http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/
42
of Italy is capital (excluding the attached PC); this
tells us that we are dealing with an object of type
capital. The second step is to see if the type is a
subclass of the DOMAIN or RANGE. Because en-
tity (E1) is a hypernym of capital, then we conclude
that the clause the capital of Italy is a subclass of
E1:CRM Entity. What if the NC has no NN? One
possibility6 is that the clause is made up of at least
one proper noun (Rome). To establish the type of
a proper noun, we use the Web as corpus and com-
pute a measure of semantic association (CHURCH,
1989) with all CIDOC-CRM classes and choose the
most similar as being the type of the NC clause. This
would yield the following triple:
E41
Rome P1
E1
the capital of Italy
where E1 and E41 are the entities Appellation and
CRM Entity respectively.
2.2 Extracting a triple from free text
The following experiment shows the result of ex-
tracting a triple from a textual description of the me-
dieval city of Wolfenbu?ttel based on the method de-
scribed previously. The document was 3922 words
long with 173 sentences. The system extracted 197
intermediate triples and 79 final triples. Table 1
shows a few processing steps for the following frag-
ment of text:
The street?s particular charm lies in its
broad-faced half-timbered buildings.
In step ?, an intermediate triple is extracted from
texts, then we use synonyms and hypernyms in step
? to find mappings with domains (D), properties (P)
and ranges (R) of the ontology. The final triples
appears in step ?. For example, consist is a syn-
onym for lie, and object is an hypernym of building.
In each case, we extracted from WordNet7 (PED-
ERSEN, 2004) the synonyms and hypernyms of the
three most common uses for each word (verb, noun).
6The other possibility, pronouns, is omitted for simplicity.
7http://wordnet.princeton.edu/
D [The street?s particular charm]
? P lies in
R [its broad-faced
half-timbered buildings]
D [attribute, charm, entity,
language, object]
? P [consist]
R [activity, building, creation,
entity, event, object]
D [e13:Attribute Assignment]
? P p9:consists of
R [e7:Activity]
Table 1: A triple extracted from free text.
3 Querying
3.1 NL Interface to SPARQL Querying
Our approach to the problem of mapping a query in
natural language to a query expressed in a partic-
ular query language (here SPARQL) is to generate
(BURKE, 1997) the most likely candidates and se-
lect the item which shows maximum semantic sim-
ilarity with the input string. We now explain both
steps in turn.
3.1.1 Generation
We start from two parallel grammars describing
both the target query language and one or more nat-
ural languages. Here is an excerpt from one query
language (SPARQL),
SelectQuery ? Select
{
Var+
Star
}
DC? WC SM?
DC ? From Table
WC ? Where? { Filter }
SM ? OrderBy Modifier
Star ? ?*?
Select ? ?select?
From ? ?from?
Table ? ?< OneTable >?
and part of its equivalent in natural language (here
English):
Select ?
{
?select?
?show?
}
?the??
From ? ?from?
43
Star ?
{
?all records?
?everything?
}
OneTable ? ?clients?
Therefore, for a SPARQL query such as select *
from <clients> {}, we are able to generate the
equivalent in natural language: select all records
from clients. The generation space of SPARQL and
natural languages can be very large (in fact it can be
infinitely large in both cases), so generation must be
constrained in some way (it is in fact constrained by
the size of the input string). More specifically, the
grammar generates candidate strings of length to be
contained between a fraction f1 shorter and a frac-
tion f2 longer than the size (in meaningful words) of
the input strings. Meaningful words are limited to be
adjectives (tag J), nouns (tag N), verbs (tag V) and
adverbs (tag RB), partly because they can be com-
pared against each other using WordNet. The val-
ues of f2 is usually less than the value of f1, but the
exact values are to be determined empirically. The
idea behind this is based on the general observation
that queries expressed in natural languages are more
likely to be redundant than underspecified. Let?s
look at example 2, a particular example of a user
query.
(2) Could/MD you/PP show/VVP me/PP all/PDT
the/DT records/NNS you/PP have/VHP
please/VV ./SENT
There are three (show, records and have) meaning-
ful words in 2. Assuming that we have 0.4 and 0.1
for the values of f1 and f2 respectively, the gener-
ator would then be constrained to produce candi-
date strings having a length in the range [3-0.4*3,
3+0.1*3] or [1.8, 3.3], i.e. between two and three
words after rounding. The generative process must
also be informed on possible values employed by
the user for the sake of filtering. For example, in
queries such as Show me everything that has a salary
above 500 and Select people named Smith, the value
of the fields salary and name are respectively speci-
fied as being above 500 and Smith. These values are
used by the generator. They are assume to be found
as symbols (SYM), foreign words (FW), nouns (N),
cardinal numbers (CD) or adjectives (J) in the input
string. The whole generative process can be sum-
marised as follows:
1. Compute the input query strings length I in
meaningful word tokens and detect potential
field values (SYM, FW, N, CD or J)
2. Generates candidate strings of a given language
L with length in the range [I-f1*I, I+f2*I]
3. For each candidate string, generate the equiva-
lent SPARQL query
The candidate strings in language L from step 2 are
passed on to the selection process.
3.1.2 Selection
The selection process is based on a measure of
similarity between the input string and candidates
issued from generation. The two similarity mea-
sures we are presenting are based on an available
semantic resource for English, Wordnet. Both mea-
sures assume that two sentences are semantically
similar if they share words with similar meanings.
This assumption is certainly not true in general but,
in the case of database querying, we can assume
that the use of metaphors, irony or contextualised
expressions is relatively rare. There are different
approaches to compute similarity, but we are con-
strained by the fact that the system must potentially
analyse and compare a large number of sentences
with varying lengths. The so-called Levenshtein dis-
tance or edit distance is a simple option based on
dynamic programming (RISTAD, 1998). It can be
transformed to become a semantic distance, that is,
the semantic distance between two strings is given
by the minimum number of operations needed to
transform one string into the other, where an oper-
ation is an insertion (cost 1), deletion (cost 1), or
substitution of a single word (as opposed to letters
in the original edit-distance). The exact cost of sub-
stitution is given by how dissimilar a pair of words
is according to WordNet (from 0 to 2). Two strings
are therefore similar if they have words semantically
related, with a preference for the same word order.
This last requirement is not always acceptable for
natural language, as can be illustrated by examples 3
and 4, which are clear semantic equivalent, although
a measure based on the Levenhstein distance would
be unduly penalising because of a different word or-
der.
(3) Show me the name and salary of all clients.
44
(4) Look into clients and show me their name and
salary.
However, the edit distance is computationally attrac-
tive and it is not clear whether word-order is such an
important factor when querying database in natural
language.
One way to have more control over word-order
is to built a similarity matrix. A similarity matrix
provides a pairwise measure of similarity between
each word of two sentences. Let?s say we want to
compare the similarity of a user?s sentence 5 with a
candidate query 6 generated by system.
(5) Show me salaries for names Smith.
(6) Select the salary where name is Smith.
The corresponding similarity matrix is shown as ta-
ble 2. Each word is assigned a part-of-speech and
transformed to its base-form to simplify comparison
using WordNet. The similarity values in the table are
Similarity show salary name Smith
select 25 0 0 0
salary 0 100 8 6
name 0 8 100 17
be 33 0 0 0
Smith 0 6 17 100
Table 2: Similarity matrix between two sentences
in the [0,100] range. They are computed using sim-
ple edge counting in WordNet, a technique similar
to computing how two people are genetically related
through their common ancestors (BUDANITSKY,
2001). Only nouns, verbs, adjectives and adverbs
can be semantically related by WordNet, therefore
strings are initially stripped of all other grammatical
categories. For example, table 2 shows that the word
select has a degree of similarity of 25 with show.
This approach does not take on board word-order at
all, and we introduce a slight correction for the value
of each entry in the table: similarity is decreased
when words appear in different positions in a string.
This is a sensible compromise to consider word-
order without undue penalties. This approach can be
expressed as follows: similarity values are decreased
by a maximum of MaxDecrease only when a pair of
words are significantly distant (by factor SigDistant)
in their respective position within each string. This
is expressed by the following formula:
IF | l ? c |L > SigDistant THEN
Sim ? Sim ?
(
1? | l ? c |L ?MaxDecrease
)
where l and c are the line and column numbers re-
spectively and L is the size of the longest string. If
we set the values of SigDistant and MaxDecrease
to 0.2, then table 2 is transformed to 3. In table 3,
Similarity show salary name Smith
select 25 0 0 0
salary 0 100 7 5
name 0 7 100 17
be 28 0 0 0
Smith 0 5 16 100
Table 3: Transf. sim. matrix between two sentences
we can see that the similarity between show and be
has been reduced from 33 to 28. Once we have the
transformed similarity matrix, we can compute the
similarity between the two sentences as such. This
is achieved by the following four steps:
1. Generate all possible squared (k*k) sub-
matrices from the transformed similarity ma-
trix. There are Ckn = n!k!(n?k)! such matrices
where k is the size of the shortest sentence and
n the longest
2. Generate all possible word-pairings for each
sub-matrices. This amounts to selecting ele-
ments being on a different row and column.
There are k! such pairings for each Ckn =n!
k!(n?k)! squared sub-matrices
3. Compute the similarity of each k! word-pairs
for all Ckn sub-matrices by adding their similar-
ity value
4. The similarity of the transformed matrix is
taken to be the same as the highest among the
k! word-pairs * Ckn sub-matrices, divided (nor-
malised) by the size of the longest string n
For our running example in table 3, step 1 yields five
4*4 sub-matrices. For each sub-matrix, there are 24
45
word-pairings (step 2). It is easy to see which word
pairing from table 2 gives the highest similarity: (be-
show,28), (salary-salary,100), (name-name,100) and
(Smith-Smith,100), for a total of 328, normalised
to the length of the longest string (5): 328/5 =
66. For comparison, the semantic similarity dis-
tance between the same two sentences using the edit-
distance is 250, and this must be normalised to the
added length of the shortest and the longest sen-
tence, 250/(5+4) = 28. Since Levenhstein gives us
a distance, we have 1-distance for similarity. The
normalising factor is (longest+shortest = 5+4), since
two strings completely different would necessitate k
replacements and n-k insertions. The maximum cost
is therefore k*2 + (n-k) = k+n.
We can get a flavour of the computational com-
plexities involved in both measures in terms of
the number of semantic similarity computations be-
tween two words (the most costly computation). The
ration between these numbers for Matrix (n!/(n-k)!)
and Edit (k*n) is (n-1)!/k(n-k)!. This ratio is equal
or greater than 1 in all cases except when n=k=2 and
n=k=3, which confirms the expected greater com-
plexity of the Matrix method. For example, when
two strings of 8 words (n=k=8) are compared, com-
plexity is 64 for Edit and 40320 for Matrix.
3.2 Comparative Evaluation
In this experiment8 we aim at evaluating and com-
paring the two (word-based) measures of semantic
similarity between sentences previously described
and based on WordNet. We will refer to these mea-
sures as Edit and Matrix. We need a reference cor-
pus where phrases are paired as paraphrases, so
we used the Microsoft Research Paraphrase Corpus
(QUIRK, 2004), which is described by the authors
as:
. . . a text file containing 5800 pairs of sen-
tences which have been extracted from
news sources on the web, along with hu-
man annotations indicating whether each
pair captures a paraphrase/semantic equiv-
alence relationship.
8Values of parameters for the methods: cost of substitution
= 2, word-pairings are centred, contiguous and do not exceed
7, MaxDecrease=0.2, SigDistant=0.2, method for similarity =
count of edges
One of two levels of quality is assigned to each para-
phrase (0 or 1). For example, phrases 7 are better
paraphrases (annotated ?1?) than 8 (annotated ?0?).
(7) Amrozi accused his brother, whom he called ?the
witness?, of deliberately distorting his evidence./
Referring to him as only ?the witness?, Amrozi accused
his brother of deliberately distorting his evidence.
(8) Yucaipa owned Dominick?s before selling the chain to
Safeway in 1998 for $2.5 billion./ Yucaipa bought
Dominick?s in 1995 for $693 million and sold it to
Safeway for $1.8 billion in 1998.
We selected random subsets of 100 pairs of good
paraphrases (i.e. annotated with ?1?), 100 pairs of
less good paraphrases (annotated with ?0?) and 100
pairs of phrases not paraphrases of each other. We
computed semantic similarity for each subset using
both methods. Results are presented in table 4. For
each method the minimum and maximum values of
similarity are reported. Variance is relatively low
and both methods appear to correlate. As expected,
paraphrases have higher similarity values, with type
?1? values slightly ahead. Moreover, average val-
ues for paraphrases are significantly higher than for
non-paraphrases, which is a sign that both methods
can discriminate between semantically related sen-
tences. When querying databases, we cannot always
Compar. Min Avg Max Var Cor
No(E) 5 12 24 0.2 0.7
No(M) 3 14 30 0.4 0.7
?0?(E) 21 57 86 1.2 0.8
?0?(M) 20 54 84 3.3 0.8
?1?(E) 35 69 94 1.9 0.6
?1?(M) 34 61 84 2.4 0.6
Table 4: Compar. eval. of the (E)dit and (M)atrix
methods for types ?0?, ?1? and (No) paraphrases.
expect a clear front runner, but a continuum of more
or less likely valuable candidates, more in line with
the case of paraphrases ?0?.
2-best pairs In this last experiment, 40 sets of 9
phrases are submitted to each method for evalua-
tion. Each set includes only one pair of paraphrases:
sets 1 to 20 include type ?0? paraphrases, while sets
21 to 40 include type ?1? paraphrases. There was
no indication in the corpus that two phrases were
not paraphrase of each other, so we assumed that
46
phrases not paired as being paraphrases were not.
Therefore, our random selection of non-paraphrases
can be more or less dissimilar. Table 5 show the re-
sults, where underlined similarity scores are those
of the actual paraphrases, and columns BEST and
SECOND give the actual measures of similarity for
the best match (the pair the system thinks are para-
phrases) and its closest follower respectively. We
can see that all 40 paraphrases were selected as the
best by both methods (M and E). Numbers in bold
indicate cases where methods have selected differ-
ent second best. The differences between type ?0?
and ?1? are consistent with those observed in table
4. These are very encouraging results that suggest
both methods could be used in a real system.
S Type 0 Type 1 S
E Best Second Best Second E
T M E M E M E M E T
1 43 54 19 20 59 48 26 17 21
2 39 38 19 14 62 94 24 17 22
3 40 59 32 21 74 90 16 18 23
4 46 65 24 20 57 86 39 21 24
5 51 57 33 31 47 47 25 19 25
6 39 43 19 15 53 54 15 11 26
7 54 70 41 39 46 60 16 15 27
8 50 59 13 9 51 79 12 10 28
9 72 78 17 20 52 62 21 14 29
10 60 67 33 23 56 60 42 29 30
11 56 78 17 15 56 52 27 26 31
12 36 50 15 14 84 79 21 17 32
13 72 80 18 16 48 60 29 27 33
14 66 68 29 25 80 79 16 13 34
15 39 65 15 12 84 87 34 29 35
16 52 58 10 9 52 77 22 14 36
17 75 71 23 21 84 82 21 18 37
18 48 53 22 19 84 87 21 17 38
19 60 60 27 19 69 71 15 13 39
20 84 63 18 14 55 80 18 18 40
Table 5: Similarity scores for each of the 2 most sim-
ilar pairs of phrases as computed by M and E
3.3 Conclusions and Future Work
It is difficult to have a comprehensive evaluation of
the extraction phase through standard metrics (pre-
cision, recall), since there is no benchmark for this
type of analysis. A good benchmark would be a
CIDOC-CRM human-annotated text. Yet we can
give some evidence of the performance of the sys-
tem. In our experiment, we have collected 79 final
triples from a 173 sentences long document describ-
ing buildings and places of interest in a medieval
city. The data was relatively clean, although punc-
tuation was heavily used throughout the document,
confusing the chunker. Despite modest results, there
is no doubt that a system like this gives a head start
to anyone wishing to build a collection using the
CIDOC-CRM ontology. A first pass in the docu-
mentation gives a good idea of what the textual doc-
umentation is about. However, a fuller interpretation
will often involve combining many triples together
to form paths. Because of time restriction, we have
decided to process the three most common meanings
of each word that we looked up in WordNet (avoid-
ing the need to select the correct meaning among
many); this may have the side effect of lowering ac-
curacy. Speed was not an issue without access to the
Web, not an absolute necessity if we have a good
thesaurus for proper nouns. Finally, we have tuned
the CRM to analyse impressions of a city, which is
not a domain for which the CRM is optimally in-
tended. We conjecture that texts about museum cat-
alogues would have yielded better results.
The approach to database querying presented in
this paper demonstrates that more and more seman-
tic resources can be used to render natural language
interfaces more efficient. The semantic web pro-
vides the backbone and the technology to support
complex querying of naturally complex data. Lexi-
cal resources such as WordNet makes it possible to
compute semantic similarity between sentences, al-
lowing researchers to develop original ways to se-
mantic parsing of natural languages. Our experi-
ments show that it is possible to map English queries
to a subset of SPARQL with high level of precision
and recall. The main drawback of the Edit method is
its overemphasis on word-order, making it less suit-
able for some languages (e.g. German). The Ma-
trix method is computationally greedy, and future
research must investigate efficient ways of cutting
down the large search space. Perhaps step 2 should
limit the number of word-pairings by taking only ad-
jacent combinations.
Another improvement might include less uncon-
47
ventional methods for generating the sentences such
as FUF/Surge or the realiser of the LKB system, as
well as the use of a corpus more relevant to CH.
At this point we concede that the generation space
may be problematic as input gets longer, but we con-
jecture that user?s input should in most cases be of
manageable length. Finally, more standards evalu-
ation metrics could serve to situate the two similar-
ity measures that are being presented with regards to
more standard approaches used for the same purpose
(KAUCHAK, 2006).
Finally, we have avoided the issue raised by poly-
semic words by considering only the most common
senses found in WordNet, so the approach would be
well complemented by contribution from the field of
Word-Sense Disambiguation (WSD).
Acknowledgement
This work has been conducted as part of the EPOCH
network of excellence (IST-2002-507382) within the
IST (Information Society Technologies) section of
the Sixth Framework Programme of the European
Commission. Thank you to the reviewers for useful
comments.
References
ANDROUTSOPOULOS I., RITCHIE G., THANISCH P.
(1995). Natural language interfaces to databases - an
introduction. Journal of Language Engineering, 1(1),
29.
BUDANITSKY A., HIRST G. (2001). Semantic dis-
tance in wordnet : an experimental, application-
oriented evaluation of five measures. In NAACL 2001
Workshop on WordNet and Other Lexical Resources,
Pittsburgh.
BURKE R.D., HAMMOND K.J., KULYUKIN V., LYTI-
NEN S.L., TOMURO N., SCHOENBERG. S. Ques-
tion answering from frequently asked question files -
experiences with the faq finder system. AI Magazine,
18(2), 57.
CHURCH K.W., HANKS P. (1989) Word association
norms, mutual information, and lexicography. In
Proc. of the 27th. Annual Meeting of the ACL Van-
vouver, B.V., 1989), pp. 76-83.
CRESCIOLI M., D?ANDREA A., NICCOLUCCI F.
(2002). XML Encoding of Archaeological Unstruc-
tured Data. In G. Burenhault (ed.), Archaeological
Informatics : Pushing the envelope. In Proc. of the
29th CAA Conference, Gotland April 2001, BAR In-
ternational Series 1016, Oxford 2002, 267-275.
DAGAN I., GLICKMAN O., MAGNINI B. (2006). The
PASCAL Recognising Textual Entailment Challenge.
Lecture Notes in Computer Science, Volume 3944, Jan
2006, Pages 177 - 190.
DOERR M. (2005) The CIDOC CRM, an Ontolog-
ical Approach to Schema Heterogeneity. Seman-
tic Interoperability and Integration. Dagstuhl Sem-
inar Proceedings, pp. 1862-4405. Internationales
Begegnungs- und Forschungszentrum fuer Informatik
(IBFI), Schloss Dagstuhl, Germany.
HERMON S., NICCOLUCCI F. (2000). The Impact of
Web-shared Knowledge on Archaeological Scientific
Research. Proc. of Intl CRIS 2000 Conf., Helsinki,
Finland, 2000.
KAUCHAK D., BARZILAY R. (2006). Paraphrasing
for Automatic Evaluation. In Proc. of NAACL/HLT,
2006.
LI Y., YANG H., JAGADISH H. (2006). Construct-
ing a generic natural language interface for an xml
database. International Conference on Extending
Database Technology
PEDERSEN T., PATWARDHAN S.,MICHELIZZI
J.(2004). Wordnet::Similarity - Measuring the
Relatedness of Concepts. In Nineteenth National
Conference on Artificial Intelligence (AAAI-04), San
Jose, CA. (Intelligent Systems Demonstration).
QUIRK C., BROCKETT C., DOLAN W.B. (2004).
Monolingual Machine Translation for Paraphrase
Generation. In Proceedings of the 2004 Conference
on Empirical Methods in Natural Language Process-
ing, Barcelona Spain.
RISTAD E.S., YIANILOS P. N. (1998). Learning string-
edit distance. IEEE Transactions on Pattern Analysis
and Machine Intelligence, 20(5), 522.
SCHUTZ A., BUITELAR P. (2005). RelExt: A Tool
for Relation Extraction in Ontology Extension. In:
Proc. of the 4th International Semantic Web Confer-
ence, Galway, Ireland, Nov. 2005.
SHETH A. (2003) Capturing and applying exist-
ing knowledge to semantic applications. Invited
Talk ?Sharing the Knowledge? - International CIDOC
CRM Symposium. March 2003. Washington DC.
All web references visited on 02-05-2007.
48
Proceedings of the EACL 2009 Demonstrations Session, pages 5?8,
Athens, Greece, 3 April 2009. c?2009 Association for Computational Linguistics
CBSEAS, a Summarization System
Integration of Opinion Mining Techniques to Summarize Blogs
Aure?lien Bossard, Michel Ge?ne?reux and Thierry Poibeau
Laboratoire d?Informatique de Paris-Nord
CNRS UMR 7030 and Universite? Paris 13
93430 Villetaneuse ? France
{firstname.lastname}@lipn.univ-paris13.fr
Abstract
In this paper, we present a novel approach
for automatic summarization. Our system,
called CBSEAS, integrates a new method
to detect redundancy at its very core, and
produce more expressive summaries than
previous approaches. Moreover, we show
that our system is versatile enough to in-
tegrate opinion mining techniques, so that
it is capable of producing opinion oriented
summaries. The very competitive results
obtained during the last Text Evaluation
Conference (TAC 2008) show that our ap-
proach is efficient.
1 Introduction
During the past decade, automatic summarization,
supported by evaluation campaigns and a large re-
search community, has shown fast and deep im-
provements. Indeed, the research in this domain is
guided by strong industrial needs: fast processing
despite ever increasing amount of data.
In this paper, we present a novel approach for
automatic summarization. Our system, called CB-
SEAS, integrates a new method to detect redun-
dancy at its very core, and produce more expres-
sive summaries than previous approaches. The
system is flexible enough to produce opinion ori-
ented summaries by accommodating techniques to
mine documents that express different views or
commentaries. The very competitive results ob-
tained during the last Text Evaluation Conference
(TAC 2008) show that our approach is efficient.
This short paper is structured as follows: we
first give a quick overview of the state of the art.
We then describe our system, focusing on the most
important novel features implemented. Lastly, we
give the details of the results obtained on the TAC
2008 Opinion Pilot task.
2 Related works
Interest in creating automatic summaries has be-
gun in the 1950s (Luhn, 1958). (Edmundson and
Wyllys, 1961) proposed features to assign a score
to each sentence of a corpus in order to rank these
sentences. The ones with the highest scores are
kept to produce the summary. The features they
used were sentence position (in a news article for
example, the first sentences are the most impor-
tant), proper names and keywords in the document
title, indicative phrases and sentence length.
Later on, summarizers aimed at eliminating re-
dundancy, especially for multi-documents summa-
rizing purpose. Identifying redundancy is a criti-
cal task, as information appearing several times in
different documents can be qualified as important.
Among recent approaches, the ?centroid-based
summarization? method developed by (Radev et
al., 2004) consists in identifying the centroid
of a cluster of documents, in other words the
terms which best suit the documents to summa-
rize. Then, the sentences to be extracted are
the ones that contain the greatest number of cen-
troids. Radev implemented this method in an on-
line multi-document summarizer, MEAD.
Radev further improved MEAD using a differ-
ent method to extract sentences: ?Graph-based
centrality? extractor (Erkan and Radev, 2004).
It consists in computing similarity between sen-
tences, and then selecting sentences which are
considered as ?central? in a graph where nodes are
sentences and edges are similarities. Sentence se-
lection is then performed by picking the sentences
which have been visited most after a random walk
on the graph.
The last two systems are dealing with redun-
dancy as a post-processing step. (Zhu et al, 2007),
assuming that redundancy should be the concept
on what is based multi-document summarization,
offered a method to deal with redundancy at the
5
same time as sentence selection. For that purpose,
the authors used a ?Markov absorbing chain ran-
dom walk? on a graph representing the different
sentences of the corpus to summarize.
MMR-MD, introduced by Carbonnel in (Car-
bonell and Goldstein, 1998), is a measure which
needs a passage clustering: all passages consid-
ered as synonyms are grouped into the same clus-
ters. MMR-MD takes into account the similarity
to a query, coverage of a passage (clusters that
it belongs to), content in the passage, similarity
to passages already selected for the summary, be-
longing to a cluster or to a document that has al-
ready contributed a passage to the summary.
The problem of this measure lies in the clus-
tering method: in the literature, clustering is gen-
erally fulfilled using a threshold. If a passage
has a similarity to a cluster centroid higher than
a threshold, then it is added to this cluster. This
makes it a supervised clustering method; an unsu-
pervised clustering method is best suited for au-
tomatic summarization, as the corpora we need
to summarize are different from one to another.
Moreover, sentence synonymy is also dependent
on the corpus granularity and on the user compres-
sion requirement.
3 CBSEAS: A Clustering-Based
Sentence Extractor for Automatic
Summarization
We assume that, in multi-document summariza-
tion, redundant pieces of information are the sin-
gle most important element to produce a good
summary. Therefore, the sentences which carry
those pieces of information have to be extracted.
Detecting these sentences conveying the same in-
formation is the first step of our approach. The de-
veloped algorithm first establishes the similarities
between all sentences of the documents to sum-
marize, then applies a clustering algorithm ? fast
global k-means (Lo?pez-Escobar et al, 2006) ? to
the similarity matrix in order to create clusters in
which sentences convey the same information.
First, our system ranks all the sentences accord-
ing to their similarity to the documents centroid.
We have chosen to build up the documents cen-
troid with the m most important terms, their im-
portance being reflected by the tf/idf of each term.
We then select the n2 best ranked sentences to cre-
ate a n sentences long summary. We do so because
the clustering algorithm we use to detect sentences
for all ejinE
C1 ? ej
for i from 1 to k do
for j from 1 to i
center(Cj)? em|emmaximizes
?
eninCj
sim(em, en)
for all ej in E
ej ? Cl|Clmaximizes sim(center(Cl, ej))
add a new cluster: Ci. It initially contains only its
center, the worst represented element in its cluster.
done
Figure 1: Fast global k-means algorithm
conveying the same information, fast global k-
means, behaves better when it has to group n2
elements into n clusters. The similarity with the
centroid is a weighted sum of terms appearing in
both centroid and sentence, normalized by sen-
tence length.
Similarity between sentences is computed using
a variant of the ?Jaccard? measure. If two terms
are not equal, we test their synonymy/hyperonymy
using the Wordnet taxonomy (Fellbaum, 1998). In
case they are synonyms or hyperonym/hyponym,
these terms are taken into account in the similar-
ity calculation, but weighted respectively half and
quarter in order to reflect that term equality is more
important than term semantic relation. We do this
in order to solve the problem pointed out in (Erkan
and Radev, 2004) (synonymy was not taken into
account for sentence similarity measures) and so
to enhance sentence similarity measure. It is cru-
cial to our system based on redundancy location as
redundancy assumption is dependent on sentence
similarities.
Once the similarities are computed, we cluster
the sentences using fast global k-means (descrip-
tion of the algorithm is in figure 1) using the simi-
larity matrix. It works well on a small data set with
a small number of dimensions, although it has not
yet scaled up as well as we would have expected.
This clustering step completed, we select one
sentence per cluster in order to produce a sum-
mary that contains most of the relevant informa-
tion/ideas in the original documents. We do so by
choosing the central sentence in each cluster. The
central sentence is the one which maximizes the
sum of similarities with the other sentences of its
cluster. It should be the one that characterizes best
the cluster in terms of information vehicled.
6
4 TAC 2008: The Opinion
Summarization Task
In order to evaluate our system, we participated
in the Text Analysis Conference (TAC) that pro-
posed in 2008 an opinion summarization task. The
goal is to produce fluent and well-organized sum-
maries of blogs. These summaries are oriented
by complex user queries, such as ?Why do people
like.....?? or ?Why do people prefer... to...??.
The results were analyzed manually, using the
PYRAMID method (Lin et al, 2006): the PYRA-
MID score of a summary depends on the number
of simple semantic units, units considered as im-
portant by the annotators. The TAC evaluation
for this task also included grammaticality, non-
redundancy, structure/coherence and overall flu-
ency scores.
5 CBSEAS Adaptation to the Opinion
Summarization Task
Blog summarization is very different from a
newswire article or a scientific paper summa-
rization. Linguistic quality as well as reason-
ing structure are variable from one blogger to an-
other. We cannot use generalities on blog struc-
ture, neither on linguistic markers to improve
our summarization system. The other problem
with blogs is the noise due to the use of un-
usual language. We had to clean the blogs in a
pre-processing step: sentences with a ratio num-
ber of frequent words/total number of words below
a given threshold (0.35) were deemed too noisy
and discarded. Frequent words are the one hun-
dred most frequent words in the English language
which on average make up approximately half of
written texts (Fry et al, 2000).
Our system, CBSEAS, is a ?standard? summa-
rization system. We had to adapt it in order to
deal with the specific task of summarizing opin-
ions. All sentences from the set of documents to
summarize were tagged following the opinion de-
tected in the blog post they originated from. We
used for that purpose a two-class (positive or neg-
ative) SVM classifier trained on movie reviews.
The idea behind the opinion classifier is to im-
prove summaries by selecting sentences having
the same opinionated polarity as the query, which
were tagged using a SVM trained on the manually
tagged queries from the training data provided ear-
lier in TAC.
As the Opinion Summarization Task was to pro-
duce a query-oriented summary, the sentence pre-
selection was changed, using the user query in-
stead of the documents centroid. We also changed
the sentence pre-selection ranking measure by
weighting terms according to their lexical cate-
gory; we have chosen to give more weight to
proper names than verbs adjectives, adverbs and
nouns. Indeed, opinions we had to summarize
were mostly on products or people.
While experimenting our system on TAC 2008
training data, we noticed that extracting sentences
which are closest to their cluster center was not
satisfactory. Some other sentences in the same
cluster were best fitted to a query-oriented sum-
mary. We added the sentence ranking used for the
sentence pre-selection to the final sentence extrac-
tor. Each sentence is given a score which is the
distance to the cluster center times the similarity
to the query.
6 TAC 2008 Results on Opinion
Summarization Task
Participants to the Opinion Summarization Task
were allowed to use extra-information given by
TAC organizers. These pieces of information are
called snippets. The snippets contain the relevant
information, and could be used as a stand-alone
dataset. Participants were classified into two dif-
ferent groups: one for those who did not use snip-
pets, and one for those who did. We did not use
snippets at all, as it is a more realistic challenge
to look directly at the blogs with no external help.
The results we present here are those of the partic-
ipants that were not using snippets. Indeed, sys-
tems using snippets obtained much higher scores
than the other systems. We cannot compare our
system to systems using snippets.
Our system obtained quite good results on
the ?opinion task?: the scores can be found on
figure 2. As one can see, our responsiveness
scores are low compared to the others (responsive-
ness score corresponds to the following question:
?How much would you pay for that summary??).
We suppose that despite the grammaticality, flu-
ency and pyramid scores of our summaries, judges
gave a bad responsiveness score to our summaries
because they are too long: we made the choice
to produce summaries with a compression rate of
10% when it was possible, the maximum length
authorized otherwise.
7
Evaluation CBSEAS Mean Best Worst Rank
Pyramid .169 .151 .251 .101 5/20
Grammatic. 5.95 5.14 7.54 3.54 3/20
Non-redun. 6.64 5.88 7.91 4.36 4/20
Structure 3.50 2.68 3.59 2.04 2/20
Fluency 4.45 3.43 5.32 2.64 2/20
Responsiv. 2.64 2.61 5.77 1.68 8/20
Figure 2: Opinion task overall results
Figure 3: Opinion task results
However, we noticed that the quality of our
summaries was very erratic. We assume this is
due to the length of our summaries, as the longest
summaries are the ones which get the worst scores
in terms of pyramid f-score (fig 3). The length of
the summaries is a ratio of the original documents
length. The quality of the summaries would be
decreasing while the number of input sentences is
increasing.
Solutions to fix this problem could be:
? Define a better score for the correspondence
to a user query and remove sentences which
are under a threshold;
? Extract sentences from the clusters that con-
tain more than a predefined number of ele-
ments only.
This would result in improving the pertinence
of the extracted sentences. The users reading the
summaries would also be less disturbed by the
large amount of sentences a too long summary
provides. As the ?opinion summarization? task
was evaluated manually and reflects well the qual-
ity of a summary for an operational use, the con-
clusions of this evaluation are good indicators of
the quality of the summaries produced by our sys-
tem.
7 Conclusion
We presented here a new approach for multi-
document summarization. It uses an unsuper-
vised clustering method to group semantically re-
lated sentences together. It can be compared to
approaches using sentence neighbourhood (Erkan
and Radev, 2004), because the sentences which are
highly related to the highest number of sentences
are those which will be extracted first. How-
ever, our approach is different since sentence se-
lection is directly dependent on redundancy loca-
tion. Also, redundancy elimination, which is cru-
cial in multi-document summarization, takes place
in the same step as sentence selection.
References
Jaime Carbonell and Jade Goldstein. 1998. The use
of MMR, diversity-based reranking for reordering
documents and producing summaries. In SIGIR?98,
pages 335?336, New York, NY, USA. ACM.
Harold P. Edmundson and Ronald E. Wyllys. 1961.
Automatic abstracting and indexing?survey and
recommendations. Commun. ACM, 4(5):226?234.
Gu?nes? Erkan and Dragomir R. Radev. 2004. Lexrank:
Graph-based centrality as salience in text summa-
rization. Journal of Artificial Intelligence Research
(JAIR).
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press.
Edward Bernard Fry, Jacqueline E. Kress, and
Dona Lee Fountoukidis. 2000. The Reading Teach-
ers Book of Lists. Jossey-Bass, 4th edition.
Chin-Yew Lin, Guihong Cao, Jianfeng Gao, and Jian-
Yun Nie. 2006. An information-theoretic approach
to automatic evaluation of summaries. In Proceed-
ings of HLT-NAACL, pages 463?470, Morristown,
NJ, USA.
Sau?l Lo?pez-Escobar, Jesu?s Ariel Carrasco-Ochoa, and
Jose? Francisco Mart??nez Trinidad. 2006. Fast
global -means with similarity functions algorithm.
In IDEAL, volume 4224 of Springer, Lecture Notes
in Computer Science, pages 512?521.
H.P. Luhn. 1958. The automatic creation of literature
abstracts. IBM Journal, 2(2):159?165.
Dragomir Radev et al 2004. MEAD - a platform for
multidocument multilingual text summarization. In
Proceedings of LREC 2004, Lisbon, Portugal.
Xiaojin Zhu, Andrew Goldberg, Jurgen Van Gael, and
David Andrzejewski. 2007. Improving diversity
in ranking using absorbing random walks. In Pro-
ceedings of HLT-NAACL, pages 97?104, Rochester,
USA.
8
Proceedings of the Workshop on Innovative Hybrid Approaches to the Processing of Textual Data (Hybrid2012), EACL 2012, pages 46?51,
Avignon, France, April 23 2012. c?2012 Association for Computational Linguistics
Contrasting objective and subjective Portuguese texts from
heterogeneous sources
Michel Ge?ne?reux
Centro de Lingu??stica da
Universidade de Lisboa (CLUL)
Av. Prof. Gama Pinto, 2
1649-003 Lisboa - Portugal
genereux@clul.ul.pt
William Martinez
Instituto de Lingu??stica
Te?orica e Computacional (ILTEC)
Avenida Elias Garcia, 147 - 5? direito
1050-099 Lisboa - Portugal
william@iltec.pt
Abstract
This paper contrasts the content and form
of objective versus subjective texts. A col-
lection of on-line newspaper news items
serve as objective texts, while parliamen-
tary speeches (debates) and blog posts form
the basis of our subjective texts, all in
Portuguese. The aim is to provide gen-
eral linguistic patterns as used in objec-
tive written media and subjective speeches
and blog posts, to help construct domain-
independent templates for information ex-
traction and opinion mining. Our hybrid
approach combines statistical data along
with linguistic knowledge to filter out ir-
relevant patterns. As resources for subjec-
tive classification are still limited for Por-
tuguese, we use a parallel corpus and tools
developed for English to build our sub-
jective spoken corpus, through annotations
produced for English projected onto a par-
allel corpus in Portuguese. A measure for
the saliency of n-grams is used to extract
relevant linguistic patterns deemed ?objec-
tive? and ?subjective?. Perhaps unsurpris-
ingly, our contrastive approach shows that,
in Portuguese at least, subjective texts are
characterized by markers such as descrip-
tive, reactive and opinionated terms, while
objective texts are characterized mainly by
the absence of subjective markers.
1 Introduction
During the last few years there has been a growing
interest in the automatic extraction of elements re-
lated to feelings and emotions in texts, and to pro-
vide tools that can be integrated into a more global
treatment of languages and their subjective aspect.
Most research so far has focused on English, and
this is mainly due to the availability of resources
for the analysis of subjectivity in this language,
such as lexicons and manually annotated corpora.
In this paper, we contrast the subjective and the
objective aspects of language for Portuguese.
Essentially, our approach will extract linguis-
tic patterns (hopefully ?objective? for newspa-
per news items and ?subjective? for parliamen-
tary speeches and blog posts) by comparing fre-
quencies against a reference corpus. Our method
is relevant for hybrid approaches as it combines
linguistic and statistic information. Our reference
corpus, the Reference Corpus of Contemporary
Portuguese (CRPC)1, is an electronically based
linguistic corpus of around 310 million tokens,
taken by sampling from several types of written
texts (literature, newspapers, science, economics,
law, parliamentary debates, technical and didactic
documents), pertaining to national and regional
varieties of Portuguese. A random selection of
10,000 texts from the entire CRPC will be used
for our experiment. The experiment flow-chart is
shown in Figure 1. We define as objective short
news items from newspapers that reports strictly
a piece of news, without comments or analysis. A
selection of blog post items and short verbal ex-
changes between member of the European parlia-
ment will serve as subjective texts.
2 Previous work
The task of extracting linguistic patterns for data
mining is not new, albeit most research has so
far dealt with English texts. Extracting subjec-
tive patterns represents a more recent and chal-
lenging task. For example, in the Text Analy-
1
http://www.clul.ul.pt/en/resources/
183-reference-corpus-of-contemporary-portuguese-crpc
46
Parliamentary
Speeches
(subjective)
Blog Posts
(subjective)
News Items
(objective)
Term and
Pattern
Extraction
Reference
Corpus
(neutral)
Patterns
Figure 1: Experiment flow-chart.
sis Conference (TAC 2009), it was decided to
withdraw the task of creating summaries of opin-
ions, present at TAC 2008, the organizers having
agreed on the difficulty of extracting subjective el-
ements of a text and organize them appropriately
to produce a summary. Yet, there is already some
relevant work in this area which may be men-
tioned here. For opinions, previous studies have
mainly focused in the detection and the gradation
of their emotional level, and this involves three
main subtasks. The first subtask is to distinguish
subjective from objectives texts (Yu and Hatzi-
vassiloglou, 2003). The second subtask focuses
on the classification of subjective texts into pos-
itive or negative (Turney, 2002). The third level
of refinement is trying to determine the extent to
which texts are positive or negative (Wilson et al,
2004). The momentum for this type of research
came through events such as TREC Blog Opin-
ion Task since 2006. It is also worth mention-
ing recent efforts to reintroduce language and dis-
cursive approaches (e.g. taking into account the
modality of the speaker) in this area (Asher and
Mathieu, 2008). The approaches developed for
automatic analysis of subjectivity have been used
in a wide variety of applications, such as online
monitoring of mood (Lloyd et al, 2005), the clas-
sification of opinions or comments (Pang et al,
2002) and their extraction (Hu an Liu, 2004) and
the semantic analysis of texts (Esuli and Sebas-
tiani, 2006). In (Mihalcea et al, 2007), a bilingual
lexicon and a manually translated parallel corpus
are used to generate a sentence classifier accord-
ing to their level of subjectivity for Romanian.
Although many recent studies in the analysis of
subjectivity emphasize sentiment (a type of sub-
jectivity, positive or negative), our work focuses
on the recognition of subjectivity and objectivity
in general. As stressed in some work (Banea et
al., 2008), researchers have shown that in senti-
ment analysis, an approach in two steps is often
beneficial, in which we first distinguish objective
from subjective texts, and then classify subjective
texts depending on their polarity (Kim and Hovy,
2006). In fact, the problem of distinguishing sub-
jective versus objective texts has often been the
most difficult of the two steps. Improvements in
the first step will therefore necessarily have a ben-
eficial impact on the second, which is also shown
in some work (Takamura et al, 2006).
3 Creating a corpus of Subjective and
Objective Portuguese Texts
To build our subjective spoken corpus (more than
2,000 texts), we used a parallel corpus of English-
Portuguese speeches2 and a tool to automatically
classify sentences in English as objective or sub-
jective (OpinionFinder (Riloff et al, 2003)). We
then projected the labels obtained for the sen-
tences in English on the Portuguese sentences.
The original parallel corpus is made of 1,783,437
pairs of parallel sentences, and after removing
pervasive short sentences (e.g. ?the House ad-
journed at ...?) or pairs of sentences with the ra-
tio of their respective lengths far away from one
(a sign of alignment or translation error), we are
left with 1,153,875 pairs. A random selection of
contiguous 20k pairs is selected for the experi-
ment. The English sentences are submitted to
OpinionFinder, which labels each of them as ?un-
known?, ?subjective? or ?objective?. Opinion-
Finder has labelled 11,694 of the 20k sentences
as ?subjective?. As our experiment aims at com-
paring frequencies between texts, we have auto-
matically created segments of texts showing lex-
ical similarities using Textiling (Hearst, 1997),
leading to 2,025 texts. We haven?t made any at-
tempt to improve or evaluate OpinionFinder and
Textiling performance. This strategy is sensible
as parliamentary speeches are a series of short
opinionated interventions by members on specific
2European Parliament: http://www.statmt.org/
europarl/
47
themes. The 11,694 subjective labels have been
projected on each of the corresponding sentences
of the Portuguese corpus to produce our final spo-
ken corpus3. Note that apart from a bridge (here
a parallel corpus) between the source language
(here English) and the target language (here Por-
tuguese), our approach does not require any man-
ual annotation. Thus, given a bridge between
English and the target language, this approach
can be applied to other languages. The consid-
erable amount of work involved in the creation of
these resources for English can therefore serve as
a leverage for creating similar resources for other
languages.
We decided to include a collection of blog posts
as an additional source of subjective texts. We
gathered a corpus of 1,110 blog posts using Boot-
Cat4, a tool that allows the harvesting and clean-
ing of web pages on the basis of a set of seed
terms5.
For our treatment of objectivity and how news
are reported in Portuguese newspapers, we have
collected and cleaned a corpus of nearly 1500 ar-
ticles from over a dozen major websites (Jornal
de Not??cias, Destak, Visa?o, A Bola, etc.).
After tokenizing and POS-tagging all sen-
tences, we collected all n-grams (n = 1, 2 and
3) along with their corresponding frequency for
each corpus (reference (CRPC), objective (news
items) and subjective (parliamentary speeches and
blog posts)), each gram being a combination of
a token with its part-of-speech tag (e.g. falar V,
?speak V?). The list of POS tags is provided in
appendix A.
3As our subjective spoken corpus has been built entirely
automatically (Opinion Finder and Textiling), it is important
to note that (Ge?ne?reux and Poibeau, 2009) have verified that
such a corpus correlates well with human judgements.
4http://bootcat.sslmit.unibo.it/
5In an attempt to collect as much opinionated pages in
Portuguese as can be, we constraint BootCat to extract pages
written in Portuguese from the following web domains:
communidades.net, blogspot.com, wordpress.
com and myspace.com. We used the following seed
words, more or less strongly related to the Portuguese cul-
ture: ribatejo, camo?es, queijo, vinho, cavaco, europa, sintra,
praia, porto, fado, pasteis, bacalhau, lisboa, algarve, alen-
tejo and coelho.
4 Experiments and Results
4.1 POS and n-grams
In our experiments we have compared all the n-
grams (n = 1, 2 and 3) from the objective and
subjective texts with the n-grams from the ref-
erence corpus. This kind of analysis aims es-
sentially at the identification of salient expres-
sions (with high log-odds ratio scores). The log-
odds ratio method (Baroni and Bernardini, 2004)
compares the frequency of occurrence of each n-
gram in a specialized corpus (news, parliamen-
tary speeches or blogs) to its frequency of oc-
currence in a reference corpus (CRPC). Apply-
ing this method solely on POS, we found that
objective texts used predominantly verbs with an
emphasis on past participles (PPT/PPA, adotado,
?adopted?), which is consistent with the nature
of reported news. In general, we observed that
subjective texts have a higher number of adjec-
tives (ADJ, o?timo, ?optimum?): parliamentary
speeches also include many infinitives (INF, fe-
licitar ?congratulate?), while blogs make use of
interjections (ITJ, uau, ?wow?). Tables 1, 2 and
3 show salient expressions for each type of texts.
These expressions do not always point to a dis-
tinction between subjectivity and objectivity, but
also to topics normally associated with each type
of texts, a situation particularly acute in the case
of parliamentary speeches. Nevertheless, we can
make some very general observations. There
is no clear pattern in news items, except for a
slight tendency towards the use of a quantita-
tive terminology (?save?, ?spend?). Parliamen-
tary speeches are concerned with societal issues
(?socio-economic?, ?biodegradable?) and forms
of politeness (?wish to express/protest?). In blog
posts we find terms related to opinions (?pinch
of salt?), wishes (?I hope you enjoy?), reactions
(?oups?) and descriptions (?creamy?).
4.2 Patterns around NPs
The n-gram approach can provide interesting pat-
terns but it has its limits. In particular, it does not
allow for generalization over larger constituents.
One way to overcome this flaw is to chunk cor-
pora into noun-phrases (NP). This is the approach
taken in (Riloff and Wiebe, 2003) for English. In
Riloff and Wiebe (2003), the patterns for English
involved a very detailed linguistic analysis, such
as the detection of grammatical functions as well
48
PORTUGUESE ENGLISH
detetado PPA ?detected?
empatado PPT ?tied?
castigado PPT ?punished?
ano CN perdido PPA ?lost year?
triunfa ADJ ?triumph?
recec?a?o CN ?recession?
podem V poupar INF ?can save?
vai V salvar INF ?will save?
deviam V hoje ADV ?must today?
ameac?as CN se CL ?threats
concretizem INF materialize?
andam V a DA gastar INF ?go to spend?
ano CN de PREP ?year of
desafios CN challenges?
contratac?o?es CN de PREP ?hiring of
pessoal CN staff?
Table 1: Salient expressions in news.
as active or passive forms. Without the proper re-
sources needed to produce sophisticated linguistic
annotations for Portuguese, we decided to sim-
plify matters slightly by not making distinction
of grammatical function or voice. That is, only
NPs would matter for our analysis. We used the
NP-chunker Yamcha6 trained on 1,000 manually
annotated (NPs and POS-tags) sentences. The
main idea here remains the same and is to find
a set of syntactic patterns that are relevant to each
group of texts, as we did for n-grams previously,
each NP becoming a single 1-gram for this pur-
pose. It is worth mentioning that NP-chunking
becomes particularly challenging in the case of
blogs, which are linguistically heterogeneous and
noisy. Finally, log-odds ratio once again serves
as a discriminative measure to highlight relevant
patterns around NPs. Tables 4, 5 and 6 illustrate
salient expressions from the three specialized cor-
pora, presenting some of them in context.
Although limited to relatively simple syntactic
patterns, this approach reveals a number of salient
linguistic structures for the subjective texts. In
parliamentary speeches, forms of politeness are
clearly predominant (?ladies and <NP>?, ?thank
<NP>? and ?<NP> wish to thank?). Unfortu-
nately, the patterns extracted from blog posts are
6http://chasen.org/?taku/software/
yamcha/. Our evaluation of the trained chunker on
Portuguese texts lead to an accuracy of 86% at word level.
PORTUGUESE ENGLISH
socioecono?micas ADJ ?socio-economic?
biodegradveis ADJ ?biodegradable?
infraestrutural ADJ ?infra-structural?
base CN jur??dica ADJ ?legal basis?
estado-membro ADJ ?member state?
resoluc?a?o CN ?common
comun ADJ resolution?
gostaria V de PREP ?wish to
expressar INF express?
gostaria V de PREP ?wish to
manifestar INF protest?
adoptar INF uma UM ?adopt an ?
abordagem CN approach?
agradecer INF muito ADV ?thank very
sinceramente ADV sincerely?
comec?ar INF por PREP ?start by
felicitar INF congratulate?
senhora CN ?Commissioner?
comissa?ria CN
senhora CN deputada CN ?Deputy?
quitac?a?o CN ?discharge?
governanc?a CN ?governance?
Table 2: Salient expressions in parliamentary
speeches.
pervaded by ?boiler-plate? material that were not
filtered out during the cleaning phase and parasite
the analysis: ?published by <NP>?, ?share on
<NP>? and ?posted by <NP>?. However, opin-
ions (?<NP> is beautiful?) and opinion primer
(?currently, <NP>?) remain present. News items
are still characterized mainly by the absence of
subjective structures (markers), albeit quantitative
expressions can still be found (?spent?).
Obviously, a statistical approach yields a cer-
tain number of irrelevant (or at best ?counter-
intuitive?) expressions: our results are no excep-
tion to this reality. Clearly, in order to reveal
insights or suggest meaningful implications, an
external (human) evaluation of the patterns pre-
sented in this study would paint a clearer picture
of the relevance of our results for information ex-
traction and opinion mining, but we think they
constitute a good starting point.
5 Conclusion and Future Work
We have presented a partly automated approach
to extract subjective and objective patterns in se-
49
PORTUGUESE ENGLISH
direto ADJ ?direct?
cremoso ADJ ?creamy?
crocante ADJ ?crispy?
atuais ADJ ?current?
coletiva ADJ ?collective?
muito ADV legal ADJ ?very legal?
redes CN sociais ADJ ?social networks?
ups ITJ ?oups?
hum ITJ ?hum?
eh ITJ ?eh?
atualmente ADV ?currently?
atrac?o?es CN ?attractions?
tenho V certeza CN ?I am sure?
e? V exatamente ADV ?this is exactly?
cafe? CN da PREP+DA ?morning
manha? CN coffee?
pitada CN de PREP ?pinch of
sal CN salt?
espero V que CJ ?I hope
gostem INF you enjoy?
Table 3: Salient expressions in blogs.
lected texts from the European Parliament, blog
posts and on-line newspapers in Portuguese. Our
work first shows that it is possible to built re-
sources for Portuguese using resources (a paral-
lel corpus) and tools (OpinionFinder) built for En-
glish. Our experiments also show that, despite our
small specialised corpora, the resources are good
enough to extract linguistic patterns that give a
broad characterization of the language in use for
reporting news items and expressing subjectivity
in Portuguese. The approach could be favourably
augmented with a more thorough cleaning phase,
a parsing phase, the inclusion of larger n-grams (n
> 3) and manual evaluation. A fully automated
daily process to collect a large-scale Portuguese
press (including editorials) and blog corpora is
currently being developed.
Acknowledgments
We are grateful to Iris Hendrickx from CLUL for
making available the POS-tagger used in our ex-
periments.
References
Asher N., Benamara F. and Mathieu Y. Distilling opin-
ion in discourse: A preliminary study. In Coling
Some NP-patterns in context
? fiquemos V a` PREP+DA <NP>
?we are waiting for <NP>?
E tambe?m na?o fiquemos a` <espera da
Oposic?a?o> mais interessada em chegar ao
Poder.
?And also we are not waiting for an opposition
more interested in coming to power.?
? revelam V <NP> gasta?mos V
?revealed by <NP> we spent?
O problema e? que, como revelam <os dados
da SIBS, na semana do Natal> gasta?mos
quase 1300 euros por segundo.
?The problem is that as shown by the data of
SIBS, in the Christmas week we spent
nearly 1300 Euros per second.?
? <NP> deviam V hoje ADV
?<NP> must today?
E para evitar males maiores, <todos os
portugueses ( ou quase todos )> deviam hoje
fazer . . .
?And to avoid greater evils, all the Portuguese
(or almost all) should today make . . .
Other NP-patterns
? <NP> gosta?mos V quase ADV
?<NP> spent almost?
? precisa V daqueles PREP+DEM <NP>
?need those <NP>?
Table 4: NP-patterns in news
2008, posters, pages 710, Manchester, UK.
Banea C., Mihalcea R., Wiebe J. and Hassan S. Multi-
lingual subjectivity analysis using machine transla-
tion. In Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP 2008), Hon-
olulu, Hawaii, October 2008.
Baroni M. and Bernardini S. Bootcat : Bootstrapping
corpora and terms from the web. In Proceedings of
LREC 2004, p. 1313-1316.
Esuli A. and Sebastiani F. Determining term subjec-
tivity and term orientation for opinion mining. In
EACL 2006.
Ge?ne?reux M. and Poibeau T. Approche mixte
utilisant des outils et ressources pour l?anglais
pour l?identification de fragments textuels subjec-
tifs franc?ais. In DEFT?09, DE?fi Fouilles de Textes,
Atelier de clo?ture, Paris, June 22nd, 2009.
Hearst M. TextTiling: Segmenting text into multi-
paragraph subtopic passages. In Computational
Linguistics, pages 33?64, 1997.
Hu M. and Liu B. Mining and summarizing customer
reviews. In ACM SIGKDD.
50
Some NP-patterns in context
? tambe?m ADV <NP> gostaria V
?also <NP> would like?
Senhor Presidente , tambe?m <eu> gostaria de
felicitar a relatora, . . .
?Mr President, I would also like to congratulate
the rapporteur, . . .?
? senhoras ADJ e CJ <NP>
?ladies and <NP>?
Senhor Presidente , Senhora Deputada
McCarthy, Senhoras e <Senhores
Deputados>, gostaria de comec?ar . . .
?Mr President, Mrs McCarthy, Ladies and
gentlemen, let me begin . . .?
? agradecer INF a` PREP+DA <NP>
?thank <NP>?
Gostaria de agradecer a` <minha colega,
senhora deputada Echerer>, pela . . .
?I would like to thank my colleague,
Mrs Echerer for . . . ?
Other NP-patterns
? <NP> desejo V agradecer INF
?<NP> wish to thank?
? aguardo V com PREP <NP>
?I look forward to <NP>?
? associar INF aos PREP+DA <NP>
?associate with <NP>?
? considero V , PNT <NP>
?I consider, <NP>?
Table 5: NP-patterns in parliamentary speeches
Kim S.-M. and Hovy E. Identifying and analyzing
judgment opinions. In HLT/NAACL 2006.
Lloyd L., Kechagias D. and Skiena S. Lydia: A system
for large-scale news analysis. In SPIRE 2005.
Mihalcea R., Banea C. and Hassan S. Learning mul-
tilingual subjective language via cross-lingual pro-
jections. In ACL 2007.
Pang B., Lee L. and Vaithyanathan S. Thumbs
up? Sentiment classification using machine learn-
ing techniques. In EMNLP 2002.
Riloff E. and Wiebe J. Learning extraction patterns for
subjective expressions. In Proceedings of EMNLP-
03, 8th Conference on Empirical Methods in Natu-
ral Language Processing, Sapporo, JP.
Riloff E., Wiebe J. and Wilson T. Learning subjective
nouns using extraction pattern bootstrapping. In
W. Daelemans & M. Osborne, Eds., Proceedings of
CONLL-03, 7th Conference on Natural Language
Learning, p. 2532, Edmonton, CA.
Takamura H., Inui T. and Okumura M. Latent vari-
Some NP-patterns in context
? publicada V por PREP <NP>
?published by <NP>?
Publicada por <Joaquim Trincheiras>
em 07:30
?Posted by Joaquim Trenches at 07:30?
? partilhar INF no PREP+DA <NP>
?share on <NP>?
Partilhar no <Twitter> . . .
?Share on Twitter ? . . .
? postado PPA por PREP <NP>
?posted by <NP>?
Postado por <Assuntos de Pol??cia> a`s 13:30.
?Posted by Police Affairs at 13:30.?
Other NP-patterns
? <NP> por PREP la? ADV
?<NP> over there?
? <NP> deixe V <NP>
?<NP> let <NP>?
? atualmente ADV , PNT <NP>
?currently, <NP>?
? <NP> e? V linda ADJ
?<NP> is beautiful?
Table 6: NP-patterns in blogs
able models for semantic orientations of phrases. In
EACL 2006.
Turney P. Thumbs up or thumbs down? Semantic ori-
entation applied to unsupervised classification of re-
views. In ACL 2002.
Wilson T., Wiebe J. and Hwa R. Just how mad are
you? Finding strong and weak opinion clauses. In
Proceedings of AAAI-04, 21st Conference of the
American Association for Artificial Intelligence, p.
761-769, San Jose, US.
Yu H. and Hatzivassiloglou V. Towards answering
opinion questions: Separating facts from opinions
and identifying the polarity of opinion sentences. In
EMNLP 2003.
A List of POS-tags
ADJ (adjectives), ADV (adverbs), CJ (con-
junctions), CL (clitics), CN (common nouns),
DA (definite articles), DEM (demonstratives),
INF (infinitives), ITJ (interjections), NP (noun
phrases), PNT (punctuation marks) PPA/PPT
(past participles), PREP (prepositions), UM
(?um? or ?uma?), V (other verbs).
51
