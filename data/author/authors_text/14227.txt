Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 24?29,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Combining Generative and Discriminative Model Scores for Distant
Supervision
Benjamin Roth, Dietrich Klakow
Saarland University
Spoken Language Systems
Saarbru?cken, Germany
{benjamin.roth|dietrich.klakow}@lsv.uni-saarland.de
Abstract
Distant supervision is a scheme to generate
noisy training data for relation extraction by
aligning entities of a knowledge base with
text. In this work we combine the output of
a discriminative at-least-one learner with that
of a generative hierarchical topic model to re-
duce the noise in distant supervision data. The
combination significantly increases the rank-
ing quality of extracted facts and achieves
state-of-the-art extraction performance in an
end-to-end setting. A simple linear interpo-
lation of the model scores performs better
than a parameter-free scheme based on non-
dominated sorting.
1 Introduction
Relation extraction is the task of finding relational
facts in unstructured text and putting them into a
structured (tabularized) knowledge base. Training
machine learning algorithms for relation extraction
requires training data. If the set of relations is pre-
specified, the training data needs to be labeled with
those relations.
Manual annotation of training data is laborious
and costly, however, the knowledge base may al-
ready partially be filled with instances from the rela-
tions. This is utilized by a scheme known as distant
supervision (DS) (Mintz et al, 2009): text is au-
tomatically labeled by aligning (matching) pairs of
entities that are contained in a knowledge base with
their textual occurrences. Whenever such a match is
encountered, the surrounding context (sentence) is
assumed to express the relation.
This assumption, however, can fail.
Consider the example given in (Taka-
matsu et al, 2012): If the tuple
place_of_birth(Michael Jackson, Gary)
is contained in the knowledge base, one matching
context could be:
Michael Jackson was born in Gary ...
And another possible context:
Michael Jackson moved from Gary ...
Clearly, only the first context indeed expresses the
relation and should be labeled accordingly.
Three basic approaches have been proposed to
deal with noisy distant supervision instances: The
discriminative at-least-one approach (Riedel et al,
2010), that requires that at least one of the matches
for a relation-entity tuple indeed expresses the
relation; The generative approach (Alfonseca et
al., 2012) that separates relation-specific distribu-
tions from noise distributions by using hierarchical
topic models; And the pattern correlation approach
(Takamatsu et al, 2012) that assumes that contexts
which match argument pairs have a high overlap in
argument pairs with other patterns expressing the re-
lation.
In this work we combine 1) a discriminative at-
least-one learner, that requires high scores for both
a dedicated noise label and the matched relation, and
2) a generative topic model that uses a feature-based
representation to separate relation-specific patterns
from background or pair-specific noise. We score
surface patterns and show that combining the two
approaches results in a better ranking quality of re-
lational facts. In an end-to-end evaluation we set a
threshold on the pattern scores and apply the pat-
24
Figure 1: Hierarchical topic models. Intertext model
(left) and feature model (right).
terns in a TAC KBP-style evaluation. Although
the surface patterns are very simple (only strings of
tokens), they achieve state-of-the-art extraction re-
sults.
2 Related Work
2.1 At-Least-One Models
The original form of distant supervision (Mintz et
al., 2009) assumes all sentences containing an entity
pair to be potential patterns for the relation holding
between the entities. A variety of models relax this
assumption and only presume that at least one of the
entity pair occurrences is a textual manifestation of
the relation. The first proposed model with an at-
least-one learner is that of Riedel et al (2010) and
Yao et al (2010). It consists of a factor graph that
includes binary variables for contexts, and groups
contexts together for each entity pair. MultiR (Hoff-
mann et al, 2011) can be viewed as a multi-label
extension of (Riedel et al, 2010). A further exten-
sion is MIMLRE (Surdeanu et al, 2012), a jointly
trained two-stage classification model.
2.2 Hierarchical Topic Model
The hierarchical topic model (HierTopics) by Alfon-
seca et al (2012) models the distant supervision data
by a generative model. For each corpus match of an
entity pair in the knowledge base, the corresponding
surface pattern is assumed to be typical for either the
entity pair, the relation, or neither. This principle is
then used to infer distributions over patterns of one
of the following types:
1. For every entity pair, a pair-specific distribu-
tion.
2. For every relation, a relation-specific distribu-
tion.
3. A general background distribution.
The generative process assumes that for each ar-
gument pair in the knowledge base, all patterns
are generated by first choosing a hidden variable z
which can take on three values,B for background,R
for relation and P for pair. Corresponding vocabu-
lary distributions (?bg, ?rel, ?pair) for generating the
context patterns are chosen according to the value of
z. The Dirichlet-smoothed vocabulary distributions
are shared on the respective levels. Figure 1 shows
the plate diagram of the HierTopics model.
3 Model Extensions and Combination
3.1 Generative Model
We use a feature-based extension (Roth and Klakow,
2013) of Alfonseca et al (2012) to include bigrams
for a more fine-grained representation of the pat-
terns. For including features in the model, the model
is extended with a second layer of hidden variables.
A variable x represents a choice of B,R or P for
every pattern, i.e. there is one variable x for every
pattern. Each feature is generated conditioned on
a second variable z ? {B,R, P}, i.e. there are as
many variables z for a pattern as there are features
for it. First, the hidden variable x is generated, then
all z variables are generated for the corresponding
features (see Figure 1). The values B,R or P of z
depend on the corresponding x by a transition distri-
bution:
P (Zi = z|Xj(i) = x) =
{
psame, if z = x
1?psame
2 , otherwise
where features at indices i are mapped to the corre-
sponding pattern indices by a function j(i); psame
is set to .99 to enforce the correspondence between
pattern and feature topics. 1
3.2 Discriminative Model
As a second feature-based model, we employ a per-
ceptron model that enforces constraints on the labels
for patterns (Roth and Klakow, 2013). The model
consists of log-linear factors for the set of relations
1The hyper-parameters used for the feature-based topic
model are ? = (1, 1, 1) and ? = (.1, .001, .001).
25
Algorithm 1 At-Least-One Perceptron Training
? ? 0
for r ? R do
for pair ? kb pairs(r) do
for s ? sentences(pair) do
for r? ? R \ r do
if P (r|s, ?) ? P (r?|s, ?) then
? ? ? + ?(s, r)? ?(s, r?)
if P (NIL|s, ?) ? P (r?|s, ?) then
? ? ? + ?(s,NIL)? ?(s, r?)
if ?s?sentences(pair) : P (r|s, ?) ? P (NIL|s, ?) then
s? = argmaxs
P (r|s,?)
P (NIL|s,?)
? ? ? + ?(s?, r)? ?(s?, NIL)
R as well as a factor for the NIL label (no relation).
Probabilities for a relation r given a sentence pat-
tern s are calculated by normalizing over log-linear
factors defined as fr(s) = exp (
?
i ?i(s, r)?i), with
?(s, r) the feature vector for sentence s and label
assignment r, and ?r the feature weight vector.
The learner is directed by the following se-
mantics: First, for a sentence s that has a distant
supervision match for relation r, relation r should
have a higher probability than any other relation
r? ? R \ r. As extractions are expected to be
noisy, high probabilities for NIL are enforced
by a second constraint: NIL must have a higher
probability than any relation r? ? R \ r. Third, at
least one DS sentence for an argument pair is ex-
pected to express the corresponding relation r. For
sentences s for an entity pair belonging to relation
r, this can be written as the following constraints:
?s,r? : P (r|s) > P (r?|s) ? P (NIL|s) > P (r?|s)
?s : P (r|s) > P (NIL|s)
The violation of any of the above constraints
triggers a perceptron update. The basic algorithm is
sketched in Algorithm 1.2
3.3 Model Combination
The per-pattern probabilities P (r|pat) are calcu-
lated as in Alfonseca et al (2012) and aggregated
over all pattern occurrences: For the topic model,
the number of times the relation-specific topic has
been sampled for a pattern is divided by n(pat), the
number of times the same pattern has been observed.
Analogously for the perceptron, the number of times
a pattern co-occurs with entity pairs for r is multi-
plied by the perceptron score and divided by n(pat).
2The weight vectors are averaged over 20 iterations.
Figure 2: Score combination by non-dominated sorting:
Circles indicate patterns on the Pareto-frontier, which are
ranked highest. They are followed by the triangles, the
square indicates the lowest ranked pattern in this exam-
ple.
For the patterns of the form [ARG1] context
[ARG2], we compute the following scores:
? Maximum Likelihood (MLE):
n(pat,r)
n(pat)
? Topic Model:
n(pat,topic(r))
n(pat)
? Perceptron:
n(pat,r)
n(pat) ?
P (r|s,?)
P (r|s,?)+P (NIL|s,?)
? Interpolation:
0.5?n(pat,topic(r))
n(pat) +
0.5?n(pat,r)?P (r|s,?)
n(pat)?(P (r|s,?)+P (NIL|s,?))
The topic model and perceptron approaches are
based on plausible yet fundamentally different prin-
ciples of modeling noise without direct supervision.
It is therefore an interesting question how comple-
mentary the models are and how much can be gained
from a combination. As the two models do not use
direct supervision, we also avoid tuning parameters
for their combination.
We use two schemes to obtain a combined rank-
ing from the two model scores: The first is a rank-
ing based on non-dominated sorting by successively
computing the Pareto-frontier of the 2-dimensional
score vectors (Borzsony et al, 2001; Godfrey et
al., 2007). The underlying principle is that all data
points (patterns in our case) that are not dominated
by another point3 build the frontier and are ranked
highest (see Figure 2), with ties broken by linear
3A data point h1 dominates a data point h2 if h1 ? h2 in all
metrics and h1 > h2 in at least one metric.
26
combination. Sorting by computing the Pareto-
frontier has been applied to training machine transla-
tion systems (Duh et al, 2012) to combine the trans-
lation quality metrics BLEU, RIBES and NTER,
each of which is based on different principles. In the
context of machine translation it has been found to
outperform a linear interpolation of the metrics and
to be more stable to non-smooth metrics and non-
comparable scalings. We compare non-dominated
sorting with a simple linear interpolation with uni-
form weights.
4 Evaluation
4.1 Ranking-Based Evaluation
Evaluation is done on the ranking quality according
to TAC KBP gold annotations (Ji et al, 2010) of ex-
tracted facts from all TAC KBP queries from 2009-
2011 and the TAC KBP 2009-2011 corpora. First,
candidate sentences are retrieved in which the query
entity and a second entity with the appropriate type
are contained. Candidate sentences are then used
to provide answer candidates if one of the extracted
patterns matches. The answer candidates are ranked
according to the score of the matching pattern.
The basis for pattern extraction is the noisy DS
training data of a top-3 ranked system in TAC KBP
2012 (Roth et al, 2012). The retrieval component
of this system is used to obtain sentence and an-
swer candidates (ranked according to their respec-
tive pattern scores). Evaluation results are reported
as averages over per-relation results of the standard
ranking metrics mean average precision (map), geo-
metric map (gmap), precision at 5 and at 10 (p@5,
p@10).
The maximum-likelihood estimator (MLE) base-
line scores patterns by the relative frequency they
occur with a certain relation. The hierarchical topic
(hier orig) as described in Alfonseca et al (2012)
increases the scores under most metrics, however
the increase is only significant for p@5 and p@10.
The feature-based extension of the topic model
(hier feat) has significantly better ranking quality.
Slightly better scores are obtained by the at-least-
one perceptron learner. It is interesting to see that the
model combinations both by non-dominated sorting
perc+hier (pareto) as well as uniform interpolation
perc+hier (itpl) give a further increase in ranking
method map gmap p@5 p@10
MLE .253 .142 .263 .232
hier orig .270 .158 .353* .297*
hier feature .318?* .205?* .363* .321*
perceptron .330?* .210?* .379* .337*
perc+hier (pareto) .340?* .220?* .400* .340*
perc+hier (itpl) .344?* .220?* .426?* .353?*
Table 1: Ranking quality of extracted facts. Significance
(paired t-test, p < 0.05) w.r.t. MLE(*) and hier orig(?).
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0  0.2  0.4  0.6  0.8  1
Prec
ision
Recall
Interpolated Precision/Recall
MLEhier orighier featperceptronperc+hier (itpl)
Figure 3: Precision at recall levels.
quality. The simpler interpolation scheme gener-
ally works best. Figure 3 shows the Precision/Recall
curves of the basic models and the linear interpola-
tion. On the P/R curve, the linear interpolation is
equal or better than the single methods on all recall
levels.
4.2 End-To-End Evaluation
We evaluate the extraction quality of the induced
perc+hier (itpl) patterns in an end-to-end setting.
We use the evaluation setting of (Surdeanu et al,
2012) and the results obtained with their pipeline for
MIMLRE and their re-implementation of MultiR as
a point of reference.
In Surdeanu et al (2012) evaluation is done us-
ing a subset of queries from the TAC KBP 2010 and
2011 evaluation. The source corpus is the TAC KBP
source corpus and a 2010 Wikipedia dump. Only
those answers are considered in scoring that are con-
tained in a list of possible answers from their can-
didates (reducing the number of gold answers from
1601 to 576 and thereby considerably increasing the
value of reported recall).
For evaluating our patterns, we take the same
27
queries for testing as Surdeanu et al (2012). As the
document collection, we use the TAC KBP source
collection and a Wikipedia dump from 07/2009 that
was available to us. From this document collec-
tion, we use our retrieval pipeline of Roth et al
(2012) and take those sentences that contain query
entities and slot filler candidates according to NE-
tags. We filter out all candidates that are not con-
tained in the list of candidates considered in (Sur-
deanu et al, 2012), and use the same reduced set
of 576 gold answers as the key. We tune a single
threshold parameter t = .3 on held-out development
data and take all patterns with higher scores. Ta-
ble 2 shows that results obtained with the induced
patterns compare well with state-of-the-art relation
extraction systems.
method Recall Precision F1
MultiR .200 .306 .242
MIMLRE .314 .247 .277
perc+hier (itpl) .248 .401 .307
Table 2: TAC Scores on (Surdeanu et al, 2012) queries.
4.3 Illustration: Top-Ranked Patterns
Figure 4 shows top-ranked patterns for per:title
and org:top members employees, the two rela-
tions with most answers in the gold annotations. For
maximum likelihood estimation the score is 1.0 if
the patterns occurs only with the relation in question
? this includes all cases where the pattern is only
found once in the corpus. While this could be cir-
cumvented by frequency thresholding, we leave the
long tail of the data as it is and let the algorithm deal
with both frequent and infrequent patterns.
One can see that while the maximum likelihood
patterns contain some reasonable relational con-
texts, they are less prototypical and more prone to
distant supervision errors. The patterns scored high
by the proposed combination generalize better, vari-
ation at the top is achieved by re-combining ele-
ments that carry relational meaning (?is an?, ?vice
president?, ?president director?) or are closely cor-
related to the particular relation.
5 Conclusion
We have combined two models based on distinct
principles for noise reduction in distant supervision:
per:title, MLE
[ARG1] , a singing [ARG2]
*[ARG1] Best film : Capote ( as [ARG2]
[ARG1] Nunn ( born October 7 , 1957 in Little Rock , Arkansas
) is an American jazz [ARG2]
*[ARG2] Kevin Weekes , subbing for a rarely rested [ARG1]
[ARG1] Butterfill FRICS ( born February 14 , 1941 , Surrey ) is
a British [ARG2]
per:title, perc+hier (itpl)
[ARG1] , is a Canadian [ARG2]
[ARG1] Hilligoss is an American [ARG2]
[ARG1] , is an American film [ARG2]
[ARG1] , is an American film and television [ARG2]
*[ARG1] for Best [ARG2]
org:top members employees, MLE
[ARG2] remained chairman of [ARG1]
*[ARG2] asks the ball whether he and [ARG1]
[ARG2] was chairman of the [ARG1]
*[ARG1] , Joe Lieberman and [ARG2]
*[ARG1] ?s responsibility to pin down just how the government
decided to front $ 30 billion in taxpayer dollars for the Bear
Stearns deal , ? Chairman [ARG2]
org:top members employees, perc+hier (itpl)
[ARG2] , Vice President of the [ARG1]
[ARG1] Vice president [ARG2]
[ARG1] president director [ARG2]
[ARG1] vice president director [ARG2]
[ARG1] Board member [ARG2]
Figure 4: Top-scored patterns for maximum likelihood
(MLE) and the interpolation (perc+hier itpl) method. In-
exact patterns are marked by *.
a feature-based extension of a hierarchical topic
model, and an at-least-one perceptron. Interpola-
tion increases the quality of extractions and achieves
state-of-the-art extraction performance. A combina-
tion scheme based on non-dominated sorting, that
was inspired by work on combining machine trans-
lation metrics, was not as good as a simple linear
combination of scores. We think that the good re-
sults motivate research into more integrated combi-
nations of noise reduction approaches.
Acknowledgment
Benjamin Roth is a recipient of the Google Europe
Fellowship in Natural Language Processing, and this
research is supported in part by this Google Fellow-
ship.
28
References
Enrique Alfonseca, Katja Filippova, Jean-Yves Delort,
and Guillermo Garrido. 2012. Pattern learning for
relation extraction with a hierarchical topic model. In
Proceedings of the 50th Annual Meeting of the Asso-
ciation for Computational Linguistics: Short Papers-
Volume 2, pages 54?59. Association for Computa-
tional Linguistics.
S Borzsony, Donald Kossmann, and Konrad Stocker.
2001. The skyline operator. In Data Engineering,
2001. Proceedings. 17th International Conference on,
pages 421?430. IEEE.
Kevin Duh, Katsuhito Sudoh, Xianchao Wu, Hajime
Tsukada, and Masaaki Nagata. 2012. Learning to
translate with multiple objectives. In Proceedings of
the 50th Annual Meeting of the Association for Com-
putational Linguistics: Long Papers-Volume 1, pages
1?10. Association for Computational Linguistics.
Parke Godfrey, Ryan Shipley, and Jarek Gryz. 2007. Al-
gorithms and analyses for maximal vector computa-
tion. The VLDB JournalThe International Journal on
Very Large Data Bases, 16(1):5?28.
Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke
Zettlemoyer, and Daniel S Weld. 2011. Knowledge-
based weak supervision for information extraction of
overlapping relations. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, vol-
ume 1, pages 541?550.
Heng Ji, Ralph Grishman, Hoa Trang Dang, Kira Grif-
fitt, and Joe Ellis. 2010. Overview of the tac 2010
knowledge base population track. In Third Text Anal-
ysis Conference (TAC 2010).
Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky.
2009. Distant supervision for relation extraction with-
out labeled data. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and the
4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP: Volume 2-Volume 2,
pages 1003?1011. Association for Computational Lin-
guistics.
Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Modeling relations and their mentions with-
out labeled text. In Machine Learning and Knowledge
Discovery in Databases, pages 148?163. Springer.
Benjamin Roth and Dietrich Klakow. 2013. Feature-
based models for improving the quality of noisy train-
ing data for relation extraction. In Proceedings of the
22nd ACM International Conference on Information
and Knowledge Management (CIKM). ACM.
Benjamin Roth, Grzegorz Chrupala, Michael Wiegand,
Mittul Singh, and Dietrich Klakow. 2012. General-
izing from freebase and patterns using distant supervi-
sion for slot filling. In Proceedings of the Text Analysis
Conference (TAC).
Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati, and
Christopher D Manning. 2012. Multi-instance multi-
label learning for relation extraction. In Proceedings
of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 455?465. Associa-
tion for Computational Linguistics.
Shingo Takamatsu, Issei Sato, and Hiroshi Nakagawa.
2012. Reducing wrong labels in distant supervi-
sion for relation extraction. In Proceedings of the
50th Annual Meeting of the Association for Compu-
tational Linguistics: Long Papers - Volume 1, ACL
?12, pages 721?729, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Limin Yao, Sebastian Riedel, and Andrew McCallum.
2010. Collective cross-document relation extraction
without labelled data. In Proceedings of the 2010 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1013?1023. Association for Com-
putational Linguistics.
29
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 673?682,
Gothenburg, Sweden, April 26-30 2014. c?2014 Association for Computational Linguistics
Automatic Food Categorization from Large Unlabeled Corpora and Its
Impact on Relation Extraction
Michael Wiegand and Benjamin Roth and Dietrich Klakow
Spoken Language Systems
Saarland University
D-66123 Saarbru?cken, Germany
{Michael.Wiegand|Benjamin.Roth|Dietrich.Klakow}@lsv.uni-saarland.de
Abstract
We present a weakly-supervised induc-
tion method to assign semantic informa-
tion to food items. We consider two tasks
of categorizations being food-type classi-
fication and the distinction of whether a
food item is composite or not. The cate-
gorizations are induced by a graph-based
algorithm applied on a large unlabeled
domain-specific corpus. We show that the
usage of a domain-specific corpus is vi-
tal. We do not only outperform a manually
designed open-domain ontology but also
prove the usefulness of these categoriza-
tions in relation extraction, outperforming
state-of-the-art features that include syn-
tactic information and Brown clustering.
1 Introduction
In view of the large interest in food in many parts
of the population and the ever increasing amount
of new dishes/food items, there is a need of au-
tomatic knowledge acquisition. We approach this
task with the help of natural language processing.
We investigate different methods to assign cate-
gories to food items. We focus on two categoriza-
tions, being a classification of food items to cat-
egories of the Food Guide Pyramid (U.S. Depart-
ment of Agriculture, 1992) and a categorization of
whether a food item is composite or not.
We present a semi-supervised graph-based ap-
proach to induce these food categorizations from
an unlabeled domain-specific text corpus crawled
from the Web. The method only requires mini-
mal manual guidance for the initialization of the
algorithm with seed terms. It depends, however,
on an automatically constructed high-quality sim-
ilarity graph. For that we choose a pattern-based
representation that outperforms a distributional-
based representation. For initialization, we ex-
amine some manually compiled seed words and
a very few simple surface patterns to automati-
cally induce such expressions. As a hard baseline,
we compare the effectiveness of using a general-
purpose ontology for the same types of categoriza-
tions. Apart from an intrinsic evaluation, we also
examine the categories in relation extraction.
The contributions of this paper are a method re-
quiring minimal supervision for a comprehensive
classification of food items and a proof of con-
cept that the knowledge that can thus be gained is
beneficial for relation extraction. Even though we
focus on a specific domain, the induction method
can be easily translated to other domains. In par-
ticular, other life-style domains, such as fashion,
cosmetics or home & gardening, show parallels
since comparable textual web data are available
and similar relation types (e.g. that two items fit
together or can be substituted by each other) exist.
Our experiments are carried out on German data
but our findings should carry over to other lan-
guages since the issues we address are (mostly)
language universal. For general accessibility, all
examples are given as English translations.
2 Data & Annotation
2.1 Domain-Specific Text Corpus
In order to generate a dataset for our experiments,
we used a crawl of chefkoch.de1 (Wiegand et al.,
2012b) consisting of 418, 558 webpages of food-
related forum entries. chefkoch.de is the largest
German web portal for food-related issues.
2.2 Food Categorization
As a food vocabulary, we employ a list of 1888
food items: 1104 items were directly extracted
from GermaNet (Hamp and Feldweg, 1997), the
German version of WordNet (Miller et al., 1990).
The items were identified by extracting all hy-
ponyms of the synset Nahrung (English: food). By
1www.chefkoch.de
673
Class Description Size Perc.
MEAT meat and fish (products) 394 20.87
BEVERAGE beverages (incl. alcoholic drinks) 298 15.78
VEGE vegetables (incl. salads) 231 12.24
SWEET sweets, pastries and snack mixes 228 12.08
SPICE spices and sauces 216 11.44
STARCH starch-based side dishes 185 9.80
MILK milk products 104 5.51
FRUIT fruits 94 4.98
GRAIN grains, nuts and seeds 77 4.08
FAT fat 41 2.18
EGG eggs 20 1.06
Table 1: The different food types (gold standard).
consulting the relation tuples from Wiegand et al.
(2012c) a further 784 items were added. We man-
ually annotated this vocabulary w.r.t. two tasks:
2.2.1 Task I: Food Types
The food type categories we chose are mainly in-
spired by the Food Guide Pyramid (U.S. Depart-
ment of Agriculture, 1992) that divides food items
into categories with similar nutritional properties.
This categorization scheme not only divides the
set of food items in many intuitive homogeneous
classes but it is also the scheme that is most com-
monly agreed upon. Table 1 lists the specific cat-
egories we use. For category assignment of com-
plex dishes comprising different food items we ap-
plied a heuristics: we always assign the category
that dominates the dish. A meat sauce, for exam-
ple, would thus be assigned MEAT (even though
there may be other ingredients than meat).
2.2.2 Task II: Dishes vs. Atomic Food Items
In addition to Task I, we include another catego-
rization that divides food items into dishes and
atomic food items (Table 2). By dish, we mainly
understand food items that are composite food
items made of other (atomic) food items. This
categorization is orthogonal to the previous clas-
sification of food items. We refrained from adding
dishes as a further category of food types in ?2.2.1,
as we would have ended up with a very heteroge-
neous class in the set of homogeneous food type
categories. Thus, dishes that differ greatly in nu-
trient content, such as Waldorf salad and chocolate
cake, would have been subsumed by one class.
3 Method
3.1 Graph-based Induction
We propose a semi-supervised graph-based ap-
proach to label food items with their respective
Class Description Examples Perc.
DISH composite food items cake, falafel, meat loaf 32.10
ATOM non-composite food items apple, steak, potato 67.90
Table 2: Distribution of dishes and atomic food
items among the food vocabulary (gold standard).
food categories. The underlying data structure
is a similarity graph connecting different food
items. Food items that belong to the same category
should be connected by highly weighted edges. In
order to infer the labels for each respective food
item, one first needs to specify a small set of seeds
for each category and then apply a graph-based
clustering method that divides the graph into clus-
ters that represent distinct food categories. Our
method is a low-resource approach that can also
be easily adapted to other domains. The only
domain-specific information required are an unla-
beled corpus and a set of seeds.
3.1.1 Construction of the Similarity Graph
To enable a graph-based induction, we generate a
similarity graph that connects similar food items.
For that purpose, a list of domain-independent
similarity-patterns was compiled. Each pattern is a
lexical sequence that connects the mention of two
food items (Table 3). Each pair of food items ob-
served with any of those patterns is connected via
a weighted edge (the different patterns are treated
equally). The weight is the total frequency of all
patterns co-occurring with a particular food pair.
Due to the high precision of our patterns, with
one or a few prototypical seeds we cannot expect
to find all items of a food category within the set
of items to which the seeds are directly connected.
Instead, one also needs to consider transitive con-
nectedness within the graph. For example, in Fig-
ure 1 banana and redberry are not directly con-
nected but they can be reached via pear or rasp-
berry. However, by considering mediate relation-
ships it becomes more difficult to determine the
most appropriate category for each food item since
most food items are connected to food items of dif-
ferent categories (in Figure 1, there are not only
edges between banana and other types of fruits
but there is also some edge to some sweet, i.e.
chocolate). For a unique class assignment, we ap-
ply a robust graph-based clustering algorithm. (It
will figure out that banana, pear, raspberry and
redberry belong to the same category and choco-
late belongs to another category, since it is mostly
674
Patterns food item
1
(or|or rather|instead of|?(?) food item
2
Example {apple: pineapple, pear, fruit, strawberry, kiwi} {steak:
schnitzel, sausage, roast, meat loaf, cutlet}
Table 3: Domain-independent patterns for build-
ing the similarity graph.
Figure 1: Illustration of the similarity graph.
linked to many other food items not being fruits.)
3.1.2 Semi-Supervised Graph Optimization
Our semi-supervised graph optimization (Belkin
and Niyogi, 2004) is a robust algorithm that was
primarily chosen since it only contains few free
parameters to adjust. It is based on two principles:
First, similar data points should be assigned simi-
lar labels, as expressed by a similarity graph of la-
beled and unlabeled data. Second, for labeled data
points the prediction of the learnt classifier should
be consistent with the (actual) gold labels.
We construct a weighted transition matrix W
of the graph by normalization of the matrix with
co-occurrence counts C which we obtain from the
similarity graph (?3.1.1). We use the common
normalization by a power of the degree function
d
i
=
?
j
C
ij
: it defines W
ij
=
C
ij
d
?
i
d
?
j
if i 6= j,
and W
ii
= 0. The normalization weight ? is the
first of two parameters used in our experiments for
semi-supervised graph optimization. For learning
the semi-supervised classifier, we use the method
of Zhou et al. (2004) to find a classifying function
which is sufficiently smooth with respect to both
the structure of unlabeled and labeled points.
Given a set of data points X = {x
1
, . . . , x
n
}
and label set L = {1, . . . , c}, with x
i:1?i?l
labeled
as y
i
? L and x
i:l+1?i?n
unlabeled. For predic-
tion, a vectorial function F : X ? Rc is estimated
assigning a vector F
i
of label scores to every x
i
.
The predicted labeling follows from these scores
as y?
i
= argmax
j?c
F
ij
. Conversely, the gold la-
beling matrix Y is a n ? c matrix with Y
ij
= 1 if
x
i
is labeled as y
i
= j and Y
ij
= 0 otherwise.
Minimizing the cost function Q aims at a trade-
off between information from neighbours and ini-
tial labeling information, controlled by parameter
Patterns Categorization Examples
patt
hearst
Food Types food item is some food type,
food type such as food item, . . .
patt
dishes
Dishes recipe for food item
patt
atom
Atomic Food Items made of|contains food item
Table 4: List of patterns to extract seeds.
? (the second parameter used in our experiments):
Q =
1
2
(
?
n
i,j=1
W
ij
?
?
?
?
?
1
?
?
i
F
i
?
1
?
?
j
F
j
?
?
?
?
?
+ ?
?
n
i=1
?F
i
? Y
i
?
)
where ?
i
is the degree function of W .
The first term in Q is the smoothness constraint,
its minimization leads to adjacent edges having
similar labels. The second term is the fitting con-
straint, its minimization leads to consistency of the
function F with the labeling of the data. The solu-
tion to the above cost function is found by solving
a system of linear equations (Zhou et al., 2004).
As we do not possess development data for this
work, we set the two free parameters ? = 0.5 and
? = 0.01. This setting is used for both induction
tasks and all configurations. It is a setting that pro-
vided reasonable results without any notable bias
for any particular configuration we examine.
3.1.3 Manually vs. Automatically Extracted
Seeds
We explore two types of seed initializations: (a)
a manually compiled list of seed food items and
(b) a small set of patterns (Table 4) by the help of
which such seeds are automatically extracted.
In order to extract seeds for Task I with the
pattern-based approach, we apply the patterns
from Hearst (1992). These patterns have been de-
signed for the acquisition of hyponyms. Task I can
also be regarded as some type of hyponym extrac-
tion. The food types (fruit, meat, sweets) repre-
sent the hypernyms for which we extract seed hy-
ponyms (banana, beef, chocolate).
In order to extract seeds for Task II, we apply
two domain-specific sets of patterns (patt
dish
and
patt
atom
). We rank the food items according to the
frequency of occurring with the respective pattern
set. Since food items may occur in both rankings,
we merge the two rankings in the following way:
score(food item) = #patt
dish
(food it.)?#patt
atom
(food it.)
The top end of this ranking represents dishes
while the bottom end represents atoms.
3.2 Using a General-Purpose Ontology
As a hard baseline, we also make use of the seman-
tic relationships encoded in GermaNet. Our two
675
types of food categorization schemes can be ap-
proximated with the hypernymy graph in that on-
tology: We manually identify nodes that resemble
our food categories (e.g. fruit, meat or dish) and
label any food item that is an immediate or a me-
diate hyponym of these nodes (e.g. apple for fruit)
with the respective category label. The downside
of this method is that a large amount of food items
is missing from the GermaNet-database (?2.2).
3.3 Other Baselines & Post-Processing
In addition to the previous methods we imple-
ment a heuristic baseline (HEUR) that rests on the
observation that German food items of the same
food category often share the same suffix, e.g.
Schokoladenkuchen (English: chocolate cake) and
Apfelkuchen (English: apple pie). For HEUR, we
manually compiled a set of few typical suffixes for
each food type/dish category (ranging from 3 to 8
suffixes per category). For classification of a food
item, we assign the food item the category label
whose suffix matched with the food item.2
We also examine an unsupervised baseline
(UNSUP) that applies spectral clustering on the
similarity graph following von Luxburg (2007):
? Input: a similarity matrix W and the number of categories to detect k.
? The laplacian L is constructed from W . It is the symmetric laplacian
L = I ?D
1/2
WD
1/2
, where D is a diagonal degree matrix.3
? A matrix U ? Rn?k is constructed that contains as columns the first
k eigenvectors u
1
, . . . , u
k
of L.
? The rows of U are interpreted as the new data points. The final cluster-
ing is obtained by k-means clustering of the rows of U .
UNSUP (which is completely parameter-free)
gives some indication about the intrinsic expres-
siveness of the similarity graph as it lacks any
guidance towards the categories to be predicted.
In graph-based food categorization, one can
only make predictions for food items that are con-
nected (be it directly or indirectly) to seed food
items within the similarity graph. To expand labels
to unconnected food items, we apply some post-
processing (POSTP). Similarly to HEUR, it ex-
ploits the suffix-similarity of food items. It assigns
each unconnected food item the label of the food
item (that could be labeled by the graph optimiza-
tion) that shares the longest suffix. Due to their
similar nature, we refrain from applying POSTP
on HEUR as it would produce no changes.
2Unlike German food items, English food items are of-
ten multi-word expressions. Therefore, we assume that for
English, instead of analyzing suffixes the usage of the head
of a multiword expression (i.e. chocolate cake) would be an
appropriate basis for a similar heuristic.
3That is, D
ii
equals to the sum of the ith row.
PLAIN +POSTP
Configuration graph Acc Prec Rec F1 Acc Prec Rec F1
UNSUP X 46.2 43.1 35.7 36.0 56.1 41.0 42.5 38.4
HEUR (plain) 25.5 87.9 32.2 42.9 N/A N/A N/A N/A
HEUR X 56.4 73.6 52.1 54.7 68.7 72.3 64.3 60.7
PAT-Top1 X 52.4 60.2 51.2 52.5 64.5 58.2 62.9 57.4
PAT-Top5 X 61.1 70.7 61.9 64.4 74.5 67.9 76.0 69.7
PAT-Top10 X 60.2 69.6 60.5 62.2 73.4 66.7 74.2 67.3
1-PROTO X 58.0 68.0 58.0 59.5 70.2 64.1 71.0 63.8
5-PROTO X 64.5 76.6 63.7 68.6 78.6 73.8 78.5 75.2
10-PROTO X 65.8 79.0 65.5 71.0 80.2 75.9 80.6 77.7
GermaNet (plain) 52.1 94.0 52.0 65.7 75.4 73.2 75.0 72.4
GermaNet X 68.3 84.7 63.4 71.6 82.7 81.8 77.7 79.1
Table 5: Comparison of different food-type classi-
fiers (graph indicates graph-based optimization).
4 Experiments
We report precision, recall and F-score and accu-
racy.4 For precision, recall and F-score, we list the
macro-averaged score.
4.1 Evaluation of Food Categorization
4.1.1 Detection of Food Types
Table 5 compares different classifiers and configu-
rations for the prediction of food types (against the
gold standard from Table 1). Apart from the pre-
viously described baselines, we consider n man-
ually selected prototypes (n-PROTO) and the top
n food items produced by Hearst-patterns (PAT-
Topn) as seeds for graph-based optimization. The
table shows that the semi-supervised graph-based
approach with these seeds outperforms the base-
lines UNSUP and HEUR. Only as few as 5
prototypical seeds (per category) are required to
obtain performance that is even better than us-
ing plain GermaNet. The table also shows that
post-processing (with our suffix-heuristics) con-
sistently improves performance. Manually choos-
ing prototypes is more effective than instantiating
seeds via Hearst-patterns. The quality of the out-
put of Hearst-patterns degrades from top 10 on-
wards. However, considering that PAT-Topn does
not include any manual intervention, it already
produces decent results. Finally, even GermaNet
can be effectively used as seeds.
4.1.2 Detection of Dishes
Table 6 compares different classifiers for the de-
tection of dishes (against the gold standard from
Table 2). Dishes and atomic food items are very
4All manually labeled resources are available at:
www.lsv.uni-saarland.de/personalPages/
michael/relFood.html
676
PLAIN +POSTP
Configuration graph Acc Prec Rec F1 Acc Prec Rec F1
UNSUP X 54.5 59.6 40.2 37.3 67.9 59.0 50.0 40.6
HEUR (plain) 74.1 84.3 59.9 58.6 N/A N/A N/A N/A
PAT-Top25 X 59.7 72.2 54.6 61.9 74.1 70.1 67.6 68.4
PAT-Top50 X 60.9 74.4 55.6 63.1 75.9 72.7 69.2 70.3
PAT-Top100 X 62.7 77.6 57.2 65.2 78.4 76.5 71.5 73.0
PAT-Top250 X 59.6 71.8 55.1 62.2 74.2 70.3 68.7 69.3
RAND-25 X 61.4 77.1 54.3 61.8 76.1 74.4 67.1 68.4
RAND-50 X 62.6 76.3 60.1 67.2 77.2 74.0 76.8 74.4
RAND-100 X 66.5 82.7 63.0 71.3 83.0 80.8 79.5 80.1
GermaNet (plain) 49.5 81.3 46.5 59.3 79.0 75.9 75.5 75.7
GermaNet X 60.8 79.4 51.3 57.6 75.9 78.2 64.4 65.4
Table 6: Comparison of different classifiers dis-
tinguishing between dishes and atomic food items
(graph indicates graph-based optimization).
PLAIN +POSTP
Configuration graph Acc Prec Rec F1 Acc Prec Rec F1
PAT-Top100 (plain) 9.5 89.5 10.5 18.6 63.6 61.5 63.5 61.3
PAT-Top100 X 62.7 77.6 57.2 65.2 78.4 76.5 71.5 73.0
RAND-100 (plain) 10.6 100.0 12.2 21.4 70.2 69.7 69.0 69.0
RAND-100 X 66.5 82.7 63.0 71.3 83.0 80.8 79.5 80.1
Table 7: Impact of graph-based optimization
(graph) for the detection of dishes.
heterogeneous classes which is why more seeds
are required for initialization. This means that
we cannot look for prototypes. For simplicity,
we resorted to randomly sample seeds from our
gold standard (RAND-n). For HEUR, we could
not find a small and intuitive set of suffixes that
are shared by many atomic food types, therefore
we considered all food types from our vocabulary
whose suffix did not match a typical dish suffix as
atomic. As this leaves no unspecified food items in
our vocabulary, we cannot use the output of HEUR
as seeds for graph-based optimization.
In contrast to the previous experiment, HEUR is
a more robust baseline. But again, post-processing
mostly improves performance, and patterns are not
as good as manual (random) seeds yet the former
are notably better than HEUR w.r.t. F-Score. Un-
like in the food-type classification, graph-based
optimization applied on GermaNet does not result
in some improvement. We assume that the preci-
sion of plain GermaNet with 81.3% is too low.5
Since GermaNet cannot effectively be used as
seeds for the graph-based optimization and post-
processing has already a strong positive effect, we
may wonder how effective the actual graph-based
5For other seeds for which it worked, we usually mea-
sured a precision of 90% or higher.
optimization is for this classification task. Af-
ter all, significantly more seeds are required for
this classification task than for the previous task,
so we need to show that it is not the mere seeds
(+post-processing) that are required for a reason-
able categorization. Table 7 examines two key
configurations with and without graph-based op-
timization. It shows that also for this classification
task, graph-based optimization produces a catego-
rization superior to the mere seeds. Moreover, the
suffix-based post-processing is complementary to
the improvement by the graph-based optimization.
4.1.3 Comparison of Initialization Methods
Table 8 compares for each food type 5 manually
selected prototypical seeds (i.e. 5-PROTO) and
the 5 food items most frequently been observed
with patt
hearst
(Table 4). While the manually cho-
sen seeds represent the spectrum of food items
within each particular class (e.g. for STARCH,
some type of pasta, rice and potato was chosen),
it is not possible to enforce such diversity with
the automatically extracted seeds. However, most
food items are correct. Table 9 displays the 10
most highly ranked dishes and atomic food items
extracted with patt
dish
and patt
atom
(Table 4). Un-
like the previous task (Table 8), we obtain more
heterogeneous seeds within the same class.
4.1.4 Distributional Similarity
Since many recent methods for related tasks, such
as noun classification, are based on so-called dis-
tributional similarity (Riloff and Shepherd, 1997;
Lin, 1998; Snow et al., 2004; Weeds et al., 2004;
Yamada et al., 2009; Huang and Riloff, 2010;
Lenci and Benotto, 2012), we also examine this as
an alternative representation to the pattern-based
similarity graph (Table 3). We represent each food
item as a vector which itself is an aggregate of
the contexts of all mentions of a particular food
item. We weighted the individual (context) words
co-occurring with the food item at a fixed window
size of 5 words with tf-idf. We can now apply
graph-based optimization on the similarity matrix
encoding the cosine similarities between any pos-
sible pair of vectors representing two food items.
As seeds, we use the best configuration (not em-
ploying GermaNet), i.e. 10-PROTO for food type
classification and RAND-100 for the dish classi-
fication. Since, however, the graph clustering is
not actually necessary, as we have a full similar-
ity matrix (rather than a sparse graph) that also al-
677
Class 5 Manually Chosen Seeds (5-PROTO) 5 Hearst-Pattern Seeds (PAT-Top5)
MEAT schnitzel, rissole, bologna, redfish, trout salmon, beef, chicken, turkey hen, poultry
BEVERAGE coffee, tea, water, beer, coke coffee, beer, mineral water, lemonade, tea
VEGE peas, green salad, tomato, cauliflower, carrot zucchini, lamb?s salad, broccoli, leek, cauliflower
SWEET chocolate, torte, popcorn, apple pie, potato crisps wine gum, marzipan, custard, pancake, biscuits
SPICE pepper, cinnamon, salt, gravy, remoulade cinnamon, laurel, clove, tomato sauce, basil
STARCH spaghetti, basmati rice, white bread, potato, french fries au gratin potatoes, jacket potato, potato, pita, jam
MILK yoghurt, gouda, cream cheese, cream, butter milk butter milk, bovine milk, soured milk, goat cheese, sour cream
FRUIT banana, apple, strawberries, apricot, orange banana, strawberries, pear, melon, kiwi
GRAIN hazelnut, pumpkin seed, rye flour, semolina, wheat sesame, spelt, wheat, millet, barley
FAT margarine, lard, colza oil, spread, butter margarine, lard, resolidified butter, coconut oil, tartar
EGG scrambled eggs, fried eggs, chicken egg, omelette, pickled egg yolk, fried eggs, albumen, offal, easter egg
Table 8: Comparison of different seed initializations for the food type categorization task (underlined
food items represent erroneously extracted food items).
lows us to compare any arbitrary pair of food items
directly, we also employ a second classifier (for
comparison) based on the nearest neighbour prin-
ciple. We assign each food item the label of the
most similar seed food item.
Table 10 compares these two classifiers with the
best previous result. It shows that the pattern-
based representation consistently outperforms the
distributional representation. The former may be
sparse but it produces high-precision similarity
links.6 The vector representation, on the other
hand, may not be sparse but it contains a high
degree of noise. The major problem is that not
only vectors of similar food items, such as chips
(fries), potatoes and rice, are similar to each other,
but also vectors of different food items that are
typically consumed with each other (e.g. fish
and chips). This is because of their frequent co-
occurrence (as in collocations like fish & chips).
Unfortunately, these pairs belong to different food
types. For the dish classification, however, the
vector representation is less of a problem.7
The distributional representation works better
with the simple nearest neighbour classifier. We
assume that graph-based optimization adds further
noise to the classification since, unlike the nearest
neighbour which only calculates the direct similar-
ity between two vectors, it also incorporates indi-
rect relationships (which may be more error-prone
than the direct relationships) between food items.
4.1.5 Do we need a domain-specific corpus?
In this section, we want to provide evidence that
apart from the similarity graph and seeds the tex-
tual source for the graph, i.e. our domain-specific
6By the label propagation within the graph-based opti-
mization, the sparsity problem is also mitigated.
7Fish and chips are both atoms, so in the dish classifica-
tion, it is no mistake to consider them similar food items.
Class 10 Seeds Extracted with Patterns (PAT-Top10)
DISH cookies, cake, praline, bread dumpling, jam, biscuit, cheese
cake, black-and-whites, onion tart, pasta salad
ATOM marzipan, flour, potato, olive oil, water, sugar, cream, choco-
late, milk, tomato
Table 9: Illustration of seed initialization for the
distinction between dishes and atomic food items.
Task Similarity Classifier Acc F1
Food Type distributional nearest neighbour 53.4 51.1
distributional graph 25.6 25.6
pattern-based graph 80.2 77.7
Dish distributional nearest neighbour 76.8 75.2
distributional graph 71.5 71.2
pattern-based graph 83.0 80.1
Table 10: Impact of the similarity representation.
corpus (chefkoch.de), is also important. For that
purpose, we compare our current corpus against
an open-domain corpus. We consider the German
version of Wikipedia since this resource also con-
tains encyclopedic knowledge about food items.
Table 11 compares the graph-based induction. As
in the previous section, we only consider the best
previous configuration. The table clearly shows
that our domain-specific text corpus is a more ef-
fective resource for our purpose than Wikipedia.
4.2 Evaluation for Relation Extraction
We now examine whether automatic food cate-
gorization can be harnessed for relation extrac-
tion. The task is to detect instances of the relation
types SuitsTo, SubstitutedBy and IngredientOf in-
troduced Wiegand et al. (2012b) (repeated in Ta-
ble 12) and motivated in Wiegand et al. (2012a).
These relation types are highly relevant for cus-
tomer advice/product recommendation. In partic-
ular, SuitsTo and SubstitutedBy are fairly domain-
independent relation types. Customers want to
678
know which items can be used together (SuitsTo),
be it two food items that can be used as a meal
or two fashion items that can be worn together.
Substitutes are also relevant for situations in which
item A is out of stock but item B can be offered as
an alternative. Therefore, insights from this work
should carry over to other domains.
We randomly extracted 1500 sentences from
our text corpus (?2.1) in which (at least) two food
items co-occur. Each food pair mention was man-
ually assigned one label. In addition to the three
relation types from above, we introduce the la-
bel Other for cases in which either another rela-
tion between the target food items is expressed or
the co-occurrence is co-incidental. On a subset of
200 sentences, we measured a substantial inter-
annotation agreement of Cohen?s ? = 0.67 (Lan-
dis and Koch, 1977).
We train a supervised classifier and incorporate
the knowledge induced from our domain-specific
corpus as features. We chose Support Vector Ma-
chines with 5-fold cross-validation using SVMlight-
multi-class (Joachims, 1999).
Table 13 displays all features that we examine
for supervised classification. Most features are
widely used throughout different NLP tasks. One
special feature brown takes into consideration the
output of Brown clustering (Brown et al., 1992)
which like our graph-based optimization produces
a corpus-driven categorization of words. Simi-
lar to UNSUP, this method is unsupervised but it
considers the entire vocabulary of our text corpus
rather than only food items. Therefore, this in-
formation can be considered as a generalization
of all contextual words. Such type of informa-
tion has been shown to be useful for named-entity
recognition (Turian et al., 2010) and relation ex-
traction (Plank and Moschitti, 2013).
For syntactic parsing, Stanford Parser (Rafferty
and Manning, 2008) was used. For Brown cluster-
ing, the SRILM-toolkit (Stolcke, 2002) was used.
Following Turian et al. (2010), we induced 1000
clusters (from our domain-specific corpus ?2.1).
4.2.1 Why should food categories be helpful
for relation extraction?
All relation types we consider comprise pairs of
two food items which makes these relation types
likely to be confused. Contextual information may
be used for disambiguation but there may also be
frequent contexts that are not sufficiently informa-
tive. For example, 25% of the instances of Ingre-
PLAIN +POSTP
Task Corpus graph Acc F1 Acc F1
Food Type
Wikipedia X 40.3 49.4 61.4 59.8
chefkoch.de X 65.8 71.0 80.2 77.7
Dish
Wikipedia X 50.4 53.1 75.4 71.1
chefkoch.de X 66.5 71.3 83.0 80.1
Table 11: Comparison of Wikipedia and domain-
specific corpus as a source for the similarity graph.
dientOf follow the lexical pattern food item
1
with
food item
2
(1). However, the same pattern also
covers 15% of the instances of SuitsTo (2).
(1) We had a stew with red lentils. (Relation: IngredientOf)
(2) We had salmon with broccoli. (Relation: SuitsTo)
The food type information we learned from our
text corpus might tell us which of the food items
are dishes. Only in (1), there is a dish, i.e. stew.
So, one may infer that the presence of dishes is
indicative of IngredientOf rather than SuitsTo.
food item
1
and food item
2
is another ambigu-
ous context. It cannot only be observed with the
relation SuitsTo, as in (3) (66% of all instantia-
tions of that pattern), but also SubstitutedBy (20%
of all mentions of that relation match that pattern),
as in (4). For SuitsTo, two food items that belong
to two different classes (e.g. MEAT and STARCH
or MEAT and VEGE) are quite characteristic. For
SubstitutedBy, the two food items are very often of
the same category of the Food Guide Pyramid.
(3) I very often eat fish and chips. (Relation: SuitsTo)
(4) For these types of dishes you can offer both Burgundy wine and
Champagne. (Relation: SubstitutedBy)
Since the second ambiguous context involves
the two general relation types SuitsTo and Substi-
tutedBy, resolving this ambiguity with automati-
cally induced type information has some signifi-
cance for other domains. In particular, for other
life-style domains, domain-specific type informa-
tion could be obtained following our method from
?3.1. The disambiguation rule that two entities of
the same type imply SubstitutedBy otherwise they
imply SuitsTo should also be widely applicable.
4.2.2 Results
Table 14 displays the performance of the different
feature sets for relation extraction. The features
designed from graph-based induction (i.e. graph)
work slightly better than GermaNet. The perfor-
mance of patt is not impressively high. However,
one should consider that patt can be used directly
without a supervised classifier (as each pattern is
679
Relation Description Example Freq. Perc.
SuitsTo food items that are typically consumed together My kids love the simple combination of fish fingers
with mashed potatoes.
633 42.20
SubstitutedBy similar food items commonly consumed in the same situations We usually buy margarine instead of butter. 336 22.40
IngredientOf ingredient of a particular dish Falafel is made of chickpeas. 246 16.40
Other other relation or co-occurrence of food items are co-incidental On my shopping list, I?ve got bread, cauliflower, ... 285 19.00
Table 12: The different relation types and their respective frequency on our dataset.
Features Description
patt lexical surface patterns used in Wiegand et al. (2012b)
word bag-of-words features: all words within the sentence
brown features using Brown clustering: all features from word but
words are replaced by induced clusters
pos part-of-speech sequence between target food items and tags
of the words immediately preceding and following them
synt path from syntactic parse tree from first target food item to
second target food item
conj conjunctive features: patt with brown classes of target food
items; pos sequence with brown classes of target food items;
synt with brown classes of target food items
graph semantic food information induced by graph optimization
(config.: 10-PROTO(+POSTP) and RAND-100(+POSTP))
germanet semantic food information derived from (plain) GermaNet
Table 13: Description of the feature set.
designed for a particular relation type, one can
read off from the matching pattern which class is
predicted). word is slightly better but, unlike patt,
it is dependent on supervised learning.
The only feature that individually manages to
significantly outperform word is graph. The tra-
ditional features (i.e. pos, synt and brown) only
produce some mild improvement when added
jointly to word along some conjunctive fea-
tures. When graph is added to this feature set
(i.e. word+patt+pos+synt+brown+conj), we ob-
tain another significant improvement. In con-
clusion, the information we induced from our
domain-specific corpus cannot be obtained by
other NLP-features, including other state-of-the-
art induction methods such as Brown clustering.
5 Related Work
While many of the previous works on noun catego-
rization also address the task of hypernym classifi-
cation (Hearst, 1992; Caraballo, 1999; Widdows,
2003; Kozareva et al., 2008; Huang and Riloff,
2010; Lenci and Benotto, 2012) and some include
examples involving food items (Widdows and
Dorow, 2002; Cederberg and Widdows, 2003),
only van Hage et al. (2005) and van Hage et al.
(2006) specifically focus on the classification of
food items. van Hage et al. (2005) deal with on-
tology mapping whereas van Hage et al. (2006)
explore part-whole relations.
Features Acc Prec Rec F1
germanet 45.3 41.3 37.2 37.3
graph 46.0 39.4 39.7 38.6
patt 59.8 49.8 41.1 38.7
word 60.1 56.9 54.5 55.1
word+patt 60.3 57.3 54.9 55.5
word+brown 59.5 56.1 54.6 54.9
word+synt 60.3 57.7 55.4 56.0
word+pos 59.8 56.6 54.6 55.1
word+germanet 61.3 58.6 56.0 56.7
word+graph 62.9 59.2 57.6 58.1?
word+patt+brown+synt+pos 60.4 57.3 56.2 56.5
word+patt+brown+synt+pos+conj 61.7 59.0 57.8 58.2?
word+patt+brown+synt+pos+conj+germanet 63.1 60.2 58.6 59.1?
word+patt+brown+synt+pos+conj+graph 64.7 62.1 60.3 60.9??
statistical significance testing (paired t-test): better than word ? at p < 0.1/
? at p < 0.05; ? better than word+patt+brown+synt+pos+conj at p < 0.05
Table 14: Comparison of various features (Ta-
ble 13) for (unrestricted) relation extraction.
The task of data-driven lexicon expansion has
also been explored before (Kanayama and Na-
sukawa, 2006; Das and Smith, 2012), however,
our paper presents the first attempt to carry out
a comprehensive categorization for the food do-
main. For the first time, we also show that type
information can effectively improve the extraction
of very common relations. For the twitter domain,
the usage of type information based on cluster-
ing has already been found effective for supervised
learning (Bergsma et al., 2013).
6 Conclusion
We presented an induction method to assign se-
mantic information to food items. We considered
two types of categorizations being food-type infor-
mation and information about whether a food item
is composite or not. The categorization is induced
by graph-based optimization applied on a large
unlabeled domain-specific text corpus. We pro-
duce categorizations that outperform a manually
compiled resource. The usage of such a domain-
specific corpus based on a pattern-based represen-
tation is vital and largely outperforms other text
corpora or a distributional representation. The in-
duced knowledge improves relation extraction.
680
Acknowledgements
This work was performed in the context of the Software-
Cluster project SINNODIUM. Michael Wiegand was funded
by the German Federal Ministry of Education and Research
(BMBF) under grant no. 01IC12SO1X. Benjamin Roth is
a recipient of the Google Europe Fellowship in Natural Lan-
guage Processing, and this research is supported in part by
this Google Fellowship. The authors would like to thank
Stephanie Ko?ser for annotating the dataset presented in this
paper.
References
Mikhail Belkin and Partha Niyogi. 2004. Semi-
supervised learning on Riemannian manifolds. Ma-
chine Learning, 56(1-3):209?239.
Shane Bergsma, Mark Dredze, Benjamin Van
Durme, Theresa Wilson, and David Yarowsky.
2013. Broadly Improving User Classification
via Communication-Based Name and Location
Clustering on Twitter. In Proceedings of the Human
Language Technology Conference of the North
American Chapter of the ACL (HLT/NAACL), pages
1010?1019, Atlanta, GA, USA.
Peter F. Brown, Peter V. deSouza, Robert L. Mer-
cer, Vincent J. Della Pietra, and Jenifer C. Lai.
1992. Class-based n-gram models of natural lan-
guage. Computational Linguistics, 18:467?479.
Sharon A. Caraballo. 1999. Automatic construction
of a hypernym-labeled noun hierarchy from text. In
Proceedings of the Annual Meeting of the Associ-
ation for Computational Linguistics (ACL), pages
120?126, College Park, MD, USA.
Scott Cederberg and Dominic Widdows. 2003. Us-
ing LSA and Noun Coordination Information to Im-
prove the Precision and Recall of Automatic Hy-
ponymy Extraction. In Proceedings of the Confer-
ence on Computational Natural Language Learn-
ing (CoNLL), pages 111?118, Edmonton, Alberta,
Canada.
Dipanjan Das and Noah A. Smith. 2012. Graph-
Based Lexicon Expansion with Sparsity-Inducing
Penalties. In Proceedings of the Human Lan-
guage Technology Conference of the North Ameri-
can Chapter of the ACL (HLT/NAACL), pages 677?
687, Montre?al, Quebec, Canada.
Birgit Hamp and Helmut Feldweg. 1997. GermaNet -
a Lexical-Semantic Net for German. In Proceedings
of ACL workshop Automatic Information Extraction
and Building of Lexical Semantic Resources for NLP
Applications, pages 9?15, Madrid, Spain.
Marti A. Hearst. 1992. Automatic Acquisition of
Hyponyms from Large Text Corpora. In Pro-
ceedings of the International Conference on Com-
putational Linguistics (COLING), pages 539?545,
Nantes, France.
Ruihong Huang and Ellen Riloff. 2010. Inducing
Domain-specific Semantic Class Taggers from (al-
most) Nothing. In Proceedings of the Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 275?285, Uppsala, Sweden.
Thorsten Joachims. 1999. Making Large-Scale SVM
Learning Practical. In B. Scho?lkopf, C. Burges,
and A. Smola, editors, Advances in Kernel Meth-
ods - Support Vector Learning, pages 169?184. MIT
Press.
Hiroshi Kanayama and Tetsuya Nasukawa. 2006.
Fully Automatic Lexicon Expansion for Domain-
oriented Sentiment Analysis. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 355?363, Syd-
ney, Australia.
Zornitsa Kozareva, Ellen Riloff, and Eduard Hovy.
2008. Semantic Class Learning from the Web
with Hyponym Pattern Linkage Graphs. In Pro-
ceedings of the Annual Meeting of the Association
for Computational Linguistics (ACL), pages 1048?
1056, Columbus, OH, USA.
J. Richard Landis and Gary G. Koch. 1977. The
Measurement of Observer Agreement for Categor-
ical Data. Biometrics, 33(1):159?174.
Alessandro Lenci and Guilia Benotto. 2012. Identi-
fying hypernyms in distributional semantic spaces.
In Proceedings of the Joint Conference on Lexical
and Computational Semantics (*SEM), pages 75?
79, Montre?al, Quebec, Canada.
Dekang Lin. 1998. Automatic Retrieval and Cluster-
ing of Similar Words. In Proceedings of the Annual
Meeting of the Association for Computational Lin-
guistics and International Conference on Computa-
tional Linguistics (ACL/COLING), pages 768?774,
Montreal, Quebec, Canada.
George Miller, Richard Beckwith, Christiane Fell-
baum, Derek Gross, and Katherine Miller. 1990.
Introduction to WordNet: An On-line Lexical
Database. International Journal of Lexicography,
3:235?244.
Barbara Plank and Alessandro Moschitti. 2013. Em-
bedding Semantic Similarity in Tree Kernels for Do-
main Adapation of Relation Extraction. In Pro-
ceedings of the Annual Meeting of the Association
for Computational Linguistics (ACL), pages 1498?
1507, Sofia, Bulgaria.
Anna Rafferty and Christopher D. Manning. 2008.
Parsing Three German Treebanks: Lexicalized and
Unlexicalized Baselines. In Proceedings of the ACL
Workshop on Parsing German (PaGe), pages 40?46,
Columbus, OH, USA.
681
Ellen Riloff and Jessica Shepherd. 1997. A
Corpus-Based Approach for Building Semantic
Lexicons. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 117?124, Providence, RI, USA.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2004.
Learning Syntactic Patterns for Automatic Hyper-
nym Discovery. In Advances in Neural Informa-
tion Processing Systems (NIPS), Vancouver, British
Columbia, Canada.
Andreas Stolcke. 2002. SRILM - An Extensible Lan-
guage Modeling Toolkit. In Proceedings of the In-
ternational Conference on Spoken Language Pro-
cessing (ICSLP), pages 901?904, Denver, CO, USA.
Joseph Turian, Lev Ratinov, and Yoshua Bengio.
2010. Word Representations: A Simple and General
Method for Semi-supervised Learning. In Proceed-
ings of the Annual Meeting of the Association for
Computational Linguistics (ACL), pages 384?394,
Uppsala, Sweden.
Human Nutrition Information Service U.S. Depart-
ment of Agriculture. 1992. The Food Guide Pyra-
mid. Home and Garden Bulletin 252, Washington,
D.C., USA.
Willem Robert van Hage, Sophia Katrenko, and Guus
Schreiber. 2005. A Method to Combine Linguis-
tic Ontology-Mapping Techniques. In Proceedings
of International Semantic Web Conference (ISWC),
pages 732 ? 744, Galway, Ireland. Springer.
Willem Robert van Hage, Hap Kolb, and Guus
Schreiber. 2006. A Method for Learning Part-
Whole Relations. In Proceedings of International
Semantic Web Conference (ISWC), pages 723 ? 735,
Athens, GA, USA. Springer.
Ulrike von Luxburg. 2007. A Tutorial on Spectral
Clustering. Statistics and Computing, 17:395?416.
Julie Weeds, David Weir, and Diana McCarthy. 2004.
Characterising Measures of Lexical Distributional
Similarity. In Proceedings of the International Con-
ference on Computational Linguistics (COLING),
pages 1015?1021, Geneva, Switzerland.
Dominic Widdows and Beate Dorow. 2002. A
Graph Model for Unsupervised Lexical Acquisition.
In Proceedings of the International Conference on
Computational Linguistics (COLING), pages 1093?
1099, Taipei, Taiwan.
Dominic Widdows. 2003. Unsupervised methods for
developing taxonomies by combining syntactic and
statistical information. In Proceedings of the Hu-
man Language Technology Conference of the North
American Chapter of the ACL (HLT/NAACL), pages
197?204, Edmonton, Alberta, Canada.
Michael Wiegand, Benjamin Roth, and Dietrich
Klakow. 2012a. Knowledge Acquisition with Nat-
ural Language Processing in the Food Domain: Po-
tential and Challenges. In Proceedings of the ECAI-
Workshop on Cooking with Computers (CWC),
pages 46?51, Montpellier, France.
Michael Wiegand, Benjamin Roth, and Dietrich
Klakow. 2012b. Web-based Relation Extraction
for the Food Domain. In Proceedings of the In-
ternational Conference on Applications of Natu-
ral Language Processing to Information Systems
(NLDB), pages 222?227, Groningen, the Nether-
lands. Springer.
Michael Wiegand, Benjamin Roth, Eva Lasarcyk,
Stephanie Ko?ser, and Dietrich Klakow. 2012c. A
Gold Standard for Relation Extraction in the Food
Domain. In Proceedings of the Conference on
Language Resources and Evaluation (LREC), pages
507?514, Istanbul, Turkey.
Ichiro Yamada, Kentaro Torisawa, Jun?ichi Kazama,
Kow Kuroda, Masaki Murata, Stijn De Saeger, Fran-
cis Bond, and Asuka Sumida. 2009. Hypernym Dis-
covery Based on Distributional Similarity and Hi-
erarchical Structures. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 929?927, Singapore.
Dengyong Zhou, Olivier Bousquet, Thomas Navin Lal,
Jason Weston, and Bernhard Scho?lkopf. 2004.
Learning with Local and Global Consistency. In
Advances in Neural Information Processing Systems
(NIPS), Vancouver and Whistler, British Columbia,
Canada.
682
Proceedings of the Demonstrations at the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 89?92,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
RelationFactory: A Fast, Modular and Effective System for Knowledge
Base Population
Benjamin Roth
?
Tassilo Barth
?
Grzegorz Chrupa?a
*
Martin Gropp
?
Dietrich Klakow
?
?
Spoken Language Systems, Saarland University, 66123 Saarbr?ucken, Germany
*
Tilburg University, PO Box 90153, 5000 LE Tilburg, The Netherlands
?
{beroth|tbarth|mgropp|dietrich.klakow}@lsv.uni-saarland.de
*
g.chrupala@uvt.nl
Abstract
We present RelationFactory, a highly ef-
fective open source relation extraction sys-
tem based on shallow modeling tech-
niques. RelationFactory emphasizes mod-
ularity, is easily configurable and uses a
transparent pipelined approach.
The interactive demo allows the user to
pose queries for which RelationFactory re-
trieves and analyses contexts that contain
relational information about the query en-
tity. Additionally, a recall error analy-
sis component categorizes and illustrates
cases in which the system missed a correct
answer.
1 Introduction and Overview
Knowledge base population (KBP) is the
task of finding relational information in large
text corpora, and structuring and tabulariz-
ing that information in a knowledge base.
Given an entity (e.g. of type PERSON) with
an associated relational schema (a set of re-
lations, e.g. city of birth(PERSON,
CITY), schools attended(PERSON,
ORGANIZATION), spouse(PERSON,
PERSON)), all relations about the entity that
are expressed in a text corpus would be rele-
vant, and the correct answers would have to be
extracted.
The TAC KBP benchmarks
1
are an effort to for-
malize this task and give researchers in the field
the opportunity to evaluate their algorithms on a
set of currently 41 relations. In TAC KBP, the
task and evaluation setup is established by well-
defined information needs about query entities of
types PERSON and ORGANIZATION (e.g. who is
the spouse of a person, how many employees
1
http://www.nist.gov/tac/about/
does an organization have). A perfect system
would have to return all relevant information (and
only this) contained in the text corpus. TAC KBP
aims at giving a realistic picture of not only pre-
cision but also recall of relation extraction sys-
tems on big corpora, and is therefore an advance-
ment over many other evaluations done for rela-
tion extraction that are often precision oriented
(Suchanek et al., 2007) or restrict the gold key to
answers from a fixed candidate set (Surdeanu et
al., 2012) or to answers contained in a data base
(Riedel et al., 2010). Similar to the classical TREC
evaluation campaigns in document retrieval, TAC
KBP aims at approaching a true recall estimate by
pooling, i.e. merging the answers of a timed-out
manual search with the answers of all participat-
ing systems. The pooled answers are then evalu-
ated by human judges.
It is a big advantage of TAC KBP that the end-
to-end setup (from the query, through retrieval of
candidate contexts and judging whether a relation
is expressed, to normalizing answers and putting
them into a knowledge base) is realistic. At the
same time, the task is very complex and may in-
volve too much work overhead for researchers
only interested in a particular step in relation ex-
traction such as matching and disambiguation of
entities, or judging relational contexts. We there-
fore introduce RelationFactory, a fast, modular
and effective relation extraction system, to the re-
search community as open source software.
2
Rela-
tionFactory is based on distantly supervised classi-
fiers and patterns (Roth et al., 2013), and was top-
ranked (out of 18 systems) in the TAC KBP 2013
English Slot-filling benchmark (Surdeanu, 2013).
In this demo, we give potential users the possi-
bility to interact with the system and to get a feel
for use cases, strengths and limitations of the cur-
rent state of the art in knowledge base population.
2
https://github.com/beroth/
relationfactory
89
The demo illustrates how RelationFactory arrives
at its conclusions and where future potentials in
relation extraction lie. We believe that Relation-
Factory provides an easy start for researchers in-
terested in relation extraction, and we hope that
it may serve as a baseline for new advances in
knowledge base population.
2 System Philosophy and Design
Principles
The design principles of RelationFactory conform
to what is known as the Unix philosophy.
3
For Re-
lationFactory this philosophy amounts to a set of
modules that solve a certain step in the pipeline
and can be run (and tested) independently of the
other modules. For most modules, input and out-
put formats are column-based text representations
that can be conveniently processed with standard
Linux tools for easy diagnostics or prototyping.
Data representation is compact: the system is de-
signed in a way that each module ideally outputs
one new file. Because of modularization and sim-
ple input and output formats, RelationFactory al-
lows for easy extensibility, e.g. for research that
focuses solely on novel algorithms at the predic-
tion stage.
The single modules are connected by a make-
file that controls the data flow and allows for easy
parallelization. RelationFactory is highly config-
urable: new relations can be added without chang-
ing any of the source code, only by changing con-
figuration files and adding or training respective
relational models.
Furthermore, RelationFactory is designed to be
highly scalable: Thanks to feature hashing, large
amounts of training data can be used in a memory-
friendly way. Predicting relations in real-time is
possible using shallow representations. Surface
patterns, ngrams and skip-ngrams allow for highly
accurate relational modeling (Roth et al., 2013),
without incurring the cost of resource-intensive
processing, such as parsing.
3
One popular set of tenets (Gancarz, 2003) summarizes
the Unix philosophy as:
1. Small is beautiful.
2. Make each program do one thing well.
3. Build a prototype as soon as possible.
4. Choose portability over efficiency.
5. Store data in flat text files.
6. Use software leverage to your advantage.
7. Use shell scripts to increase leverage and portability.
8. Avoid captive user interfaces.
9. Make every program a filter.
Figure 1: TAC KBP: Given a set of queries, return
a correct, complete and non-redundant response
with relevant information extracted from the text
corpus.
Figure 2: Data flow of the relation extraction sys-
tem: The candidate generation stage retrieves pos-
sible relational contexts. The candidate validation
stage predicts whether relations actually hold and
produces a valid response.
3 Component Overview
A simplified input and output to RelationFactory
is shown in Figure 1. In general, the pipeline
is divided in a candidate generation stage, where
documents are retrieved and candidate sentences
are identified, and the candidate validation stage,
which predicts and generates a response from the
retrieved candidates (see Figure 2).
In a first step, the system generates aliases for
the query using statistical and rule-based expan-
sion methods, for example:
Query Expansion
Adam Gadahn Azzam the American, Adam Yahiye Gadahn, Gadahn
STX Finland Kvaerner Masa Yards, Aker Finnyards, STX Finland Ltd
The expansions are used for retrieving docu-
ments from a Lucene index. All those sen-
90
tences are retained where the query (or one of
the query aliases) is contained and the named-
entity tagger has identified another entity with
the type of a potential answer for one of the
sought relations. The system is easily con-
figurable to include matching of non-standard
named-entity types from lists. RelationFac-
tory uses lists obtained from Freebase (www.
freebase.com) to match answer candidates
for the types CAUSE-OF-DEATH, JOB-TITLE,
CRIMINAL-CHARGES and RELIGION.
The candidate sentences are output line-by-line
and processed by one of the validation modules,
which determine whether actually one of the rela-
tions is expressed. RelationFactory currently uses
three standard validation modules: One based on
SVM classifiers, one based on automatically in-
duced and scored patterns, and one based on man-
ually crafted patterns. The validation modules
function as a filter to the candidates file. They
do not have to add a particular formatting or con-
form to other requirements of the KBP task such
as establishing non-redundancy or finding the cor-
rect offsets in the text corpus. This is done by
other modules in the pipeline, most notably in
the post-processing step, where statistical meth-
ods and heuristics are applied to produce a well-
formed TAC KBP response.
4 User Perspective
From a user perspective, running the system is as
easy as calling:
./run.sh system.config
The configuration file contains all information
about the general run configuration of the system,
such as the query file to use, the format of the re-
sponse file (e.g. TAC 2012 or TAC 2013 format),
the run directory that will contain the response,
and the Lucene index with the corpus. Optional
configuration can control non-standard validation
modules, and special low or high-recall query ex-
pansion schemes.
The relevant parts of the configuration file for a
standard 2013 TAC KBP run would look like the
following:
query /TAC_EVAL/2013/query.xml
goal response2013
rundir /TAC_RUNS/run2013/
index /TAC_CORPORA/2013/index
rellist /CFG/rellist2013
relations.config /CFG/relations2013.config
The last two lines refer to relation-specific con-
figuration files: The list of relations to use and in-
formation about them. Changing these files (and
adding respective models) allows for inclusion of
further relations. The relation-specific configura-
tion file contains information about the query en-
tity type, the expected answer named-entity tag
and whether a list of answers is expected (com-
pared to relations with just one correct answer):
per:religion enttype PER
per:religion argtag RELIGION
per:religion listtype false
org:top_members_employees enttype ORG
org:top_members_employees argtag PERSON
org:top_members_employees listtype true
RelationFactory comes with batteries included:
The models and configurations for TAC KBP 2013
work out-of-the-box and can easily be used as a
relation extraction module in a bigger setting or as
a baseline for new experiments.
4
5 Illustrating RelationFactory
In TAC KBP 2013, 6 out of 18 systems achieved
an F1 score of over 30%. RelationFactory as
the top-performing system achieved 37.28% com-
pared to 68.49% achieved by human control an-
notators (Surdeanu, 2013). These numbers clearly
show that current systems have just gone halfway
toward achieving human-like performance on an
end-to-end relation extraction task.
The aim of the RelationFactory demo is to il-
lustrate what the current challenges in TAC KBP
are. The demonstration interface therefore not
only shows the answers generated for populating
a potential knowledge base, but also what text was
used to justify the extraction.
The real-time performance of RelationFactory
allows for trying arbitrary queries and changing
the configuration files and immediately seeing the
effects. Different expansion schemes, validation
modules and patterns can be turned on and off, and
intuitions can be obtained about the bottlenecks
and error sources of relation extraction. The demo
also allows for seeing the effect of extracting infor-
mation from different corpora: a Wikipedia corpus
and different TAC KBP corpora, such as newswire
and web text.
4
Training models for new relations requires is a bigger
effort and includes generation of distant supervision train-
ing data by getting argument pairs from relational patterns
or a knowledge base like Freebase. RelationFactory includes
some training scripts but since they are typically run once
only, they are significantly less documented.
91
Figure 3: Screenshot of the RelationFactory demo user interface.
RelationFactory contains a number of diagnos-
tic tools: With a gold key for a set of queries, error
classes can be broken down and examples for cer-
tain error classes can be shown. For example, the
diagnostic tool for missed recall performs the fol-
lowing checks:
1. Is document retrieved?
2. Is query matched? This determines whether a sen-
tence is considered for further processing.
3. Is answer in query sentence? Whether the answer is
in one of the sentences with the query. Our system only
can find answers when this is the case, as there is no co-
reference module included.
4. Do answer tags overlap with gold answer?
5. Do they overlap exactly?
6. Other (validation). If all previous checks are passed,
the candidate has correctly been generated by the can-
didate generation stage, but the validation modules
have failed to predict the relation.
On the TAC KBP 2013 queries, the resulting re-
call error analysis is:
error class missing recall
Doc not retrieved 5.59%
Query not matched 10.37%
Answer not in query sentence 16.63%
Answer tag inexact 5.36%
Answer not tagged 24.85%
Other (validation) 37.17%
The demonstration tool allows for inspection of
instances of each of the error classes.
6 Conclusion
This paper illustrates RelationFactory, a modular
open source knowledge-base population system.
We believe that RelationFactory will become es-
pecially valuable for researchers in the field of re-
lation extraction that focus on one particular prob-
lem of knowledge-base-population (such as entity
expansion or relation prediction) and want to inte-
grate their algorithms in an end-to-end setting.
Acknowledgments
Benjamin Roth is a recipient of the Google Europe
Fellowship in Natural Language Processing, and
this research is supported in part by this Google
Fellowship. Tassilo Barth was supported in part
by IARPA contract number W911NF-12-C-0015.
References
Mike Gancarz. 2003. Linux and the Unix philosophy.
Digital Press.
Sebastian Riedel, Limin Yao, and Andrew McCal-
lum. 2010. Modeling relations and their men-
tions without labeled text. In Machine Learning and
Knowledge Discovery in Databases, pages 148?163.
Springer.
Benjamin Roth, Tassilo Barth, Michael Wiegand, Mit-
tul Singh, and Dietrich Klakow. 2013. Effective slot
filling based on shallow distant supervision methods.
In Proceedings of the Sixth Text Analysis Conference
(TAC 2013).
Fabian M Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: a core of semantic knowl-
edge. In Proceedings of the 16th international con-
ference on World Wide Web, pages 697?706. ACM.
Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati,
and Christopher D Manning. 2012. Multi-instance
multi-label learning for relation extraction. In Pro-
ceedings of the 2012 Conference on Empirical Meth-
ods in Natural Language Processing and Natural
Language Learning (EMNLP-CoNLL), pages 455?
465. ACL.
Mihai Surdeanu. 2013. Overview of the tac2013
knowledge base population evaluation: English slot
filling and temporal slot filling. In Proceedings of
the Sixth Text Analysis Conference (TAC 2013).
92
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 100?105,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Unsupervised Parsing for Generating Surface-Based
Relation Extraction Patterns
Jens Illig
University of Kassel
Wilhelmsh?oher Allee 73
D-34121 Kassel, Germany
illig@cs.uni-kassel.de
Benjamin Roth and Dietrich Klakow
Saarland University
D-66123 Saarbr?ucken, Germany
{benjamin.roth, dietrich.klakow}
@lsv.uni-saarland.de
Abstract
Finding the right features and patterns for
identifying relations in natural language is
one of the most pressing research ques-
tions for relation extraction. In this pa-
per, we compare patterns based on super-
vised and unsupervised syntactic parsing
and present a simple method for extract-
ing surface patterns from a parsed training
set. Results show that the use of surface-
based patterns not only increases extrac-
tion speed, but also improves the quality
of the extracted relations. We find that, in
this setting, unsupervised parsing, besides
requiring less resources, compares favor-
ably in terms of extraction quality.
1 Introduction
Relation extraction is the task of automatically de-
tecting occurrences of expressed relations between
entities in a text and structuring the detected in-
formation in a tabularized form. In natural lan-
guage, there are infinitely many ways to creatively
express a set of semantic relations in accordance to
the syntax of the language. Languages vary across
domains and change over time. It is therefore im-
possible to statically capture all ways of express-
ing a relation.
Most relation extraction systems (Bunescu and
Mooney, 2005; Snow et al., 2005; Zhang et al.,
2006; Mintz et al., 2009; Alfonseca et al., 2012;
Min et al., 2012) generalize semantic relations
by taking into account statistics about the syntac-
tic construction of sentences. Usually supervised
parsers are applied for parsing sentences.
Statistics are then utilized to machine-learn how
textual mentions of relations can be identified.
Many researchers avoid the need for expensive
corpora with manually labeled relations by apply-
ing a scheme called distant supervision (Mintz et
al., 2009; Roth et al., 2013) which hypothesizes
that all text fragments containing argument co-
occurrences of known semantic relation facts in-
deed express these relations. Still, systems rely-
ing on supervised parsers require training from an-
notated treebanks, which are expensive to create,
and highly domain- and language dependent when
available.
An alternative is unsupervised parsing, which
automatically induces grammars by structurally
analyzing unlabeled corpora. Applying unsuper-
vised parsing thus avoids the limitation to lan-
guages and domains for which annotated data is
available. However, induced grammars do not
match traditional linguistic grammars. In most of
the research on parsing, unsupervised parsers are
still evaluated based on their level of correspon-
dence to treebanks. This is known to be prob-
lematic because there are several different ways of
linguistically analyzing text, and treebank anno-
tations also contain questionable analyses (Klein,
2005). Moreover, it is not guaranteed that the syn-
tactic analysis which is most conforming to a gen-
eral linguistic theory is also best suited in an ex-
trinsic evaluation, such as for relation extraction.
In this work, we apply a supervised and an un-
supervised parser to the relation extraction task by
extracting statistically counted patterns from the
resulting parses. By utilizing the performance of
the overall relation extraction system as an indirect
measure of a parser?s practical qualities, we get a
task-driven evaluation comparing supervised and
unsupervised parsers. To the best of our knowl-
edge, this is the first work to compare general-
purpose unsupervised and supervised parsing on
the application of relation extraction. Moreover,
we introduce a simple method to obtain shallow
patterns from syntactic analyses and show that, be-
sides eliminating the need to parse text during sys-
tem application, such patterns also increase extrac-
tion quality. We discover that, for this method, un-
100
supervised parsing achieves better extraction qual-
ity than the more expensive supervised parsing.
1.1 Related Work
Unsupervised and weakly supervised training
methods have been applied to relation extraction
(Mintz et al., 2009; Banko et al., 2007; Yates
and Etzioni, 2009) and similar applications such
as semantic parsing (Poon and Domingos, 2009)
and paraphrase acquisition (Lin and Pantel, 2001).
However, in such systems, parsing is commonly
applied as a separately trained subtask
1
for which
supervision is used.
H?anig and Schierle (2009) have applied unsu-
pervised parsing to a relation extraction task but
their task-specific data prohibits supervised pars-
ing for comparison.
Unsupervised parsing is traditionally only eval-
uated intrinsically by comparison to gold-standard
parses. In contrast, Reichart and Rappoport (2009)
count POS token sequences inside sub-phrases for
measuring parsing consistency. But this count is
not clearly related to application qualities.
2 Methodology
A complete relation extraction system consists of
multiple components. Our system follows the ar-
chitecture described by Roth et al. (2012). In
short, the system retrieves queries in the form
of entity names for which all relations captured
by the system are to be returned. The en-
tity names are expanded by alias-names extracted
from Wikipedia link anchor texts. An information
retrieval component retrieves documents contain-
ing either the name or one of the aliases. Further
filtering retains only sentences where a named en-
tity tagger labeled an occurrence of the queried
entity as being of a suitable type and furthermore
found a possible entity for the relation?s second ar-
gument. For each candidate sentence, a classifier
component then identifies whether one of the cap-
tured relation types is expressed and, if so, which
one it is. Postprocessing then outputs the classi-
fied relation according to task-specific format re-
quirements. Here, we focus on the relation type
classifier.
1
An exception is the joint syntactic and semantic (super-
vised) parsing model inference by Henderson et al. (2013)
2.1 Pattern Extraction
For our relation extraction system, we use a simple
pattern matching framework. Whenever at least
one candidate sentence containing two entities A
and B matches one of the patterns extracted for a
certain relation type R, the classifier states that R
holds between A and B.
We experimented with two types of patterns.
First, we simply parsed the training set and ex-
tracted shortest dependency path patterns. These
patterns search for matches on the parse tree.
Following Lin and Pantel (2001), the shortest
path connecting two arguments in a dependency
graph has been widely used as a representation
of relation instance mentions. The general idea
is that shortest paths skip over irrelevant op-
tional parts of a sentence such as in $1, who
... founded $2 where the shortest path pattern
$1?founded?$2 matches although an irrel-
evant relative clause appears between the argu-
ments $1 and $2. Similar representations have
been used by Mintz et al. (2009), Alfonseca et al.
(2012) and Snow et al. (2005).
In a second set of experiments, we used the
shortest dependency paths in parsed training sen-
tences to generate surface-based patterns. These
patterns search for matches directly on plain text
and therefore do no longer rely on parsing at appli-
cation time. The patterns are obtained by turning
the shortest paths between relational arguments in
the parsed training data into token sequences with
gaps. The token sequences consist of all words
in the sentence that appear on the shortest depen-
dency path. Argument positions in the surface pat-
terns are specified by special tokens $1 and $2.
At all places, where there are one or more tokens
which are not on the shortest dependency path but
which are surrounded either by tokens on the de-
pendency path or by arguments, an asterisk repre-
sents up to four unspecified tokens. For the short-
est path $1?,?who?$2 connecting Friedman
and economist in the DMV parse depicted in Fig-
ure 1, this method generates the pattern $1,
*
$2 who. As can be seen, such patterns can cap-
ture a conjunction of token presence conditions to
the left, between, and to the right of the arguments.
In cases where argument entities are not parsed as
a single complete phrase, we generate patterns for
each possible combination of outgoing edges from
the two arguments. We dismiss patterns generated
for less than four distinct argument entity pairs of
101
Milton Friedman , a conservative economist who died in 2006 at age 94 , received the Nobel Prize for economics in 1976 .
nn
nsubj
punct
det
amod
appos
nsubj
rcmod
prep
pobj
prep
pobj
num
punct
MALT root
det
nn
dobj
prep
pobj
prep
pobj
punct
DMV root
Figure 1: Comparison of a DMV (above text) and a MALT parse (below text) of the same sentence.
the same relation type. For each pattern, we cal-
culate the precision on the training set and retain
only patterns above a certain precision threshold.
2.2 Supervised and Unsupervised Parsing
Typical applications which require syntactic anal-
yses make use of a parser that has been trained un-
der supervision of a labeled corpus conforming to
a linguistically engineered grammar. In contrast,
unsupervised parsing induces a grammar from fre-
quency structures in plain text.
Various algorithms for unsupervised parsing
have been developed in the past decades. Head-
den (2012) gives a rather recent and extensive
overview of unsupervised parsing models. For our
work, we use the Dependency Model with Valence
(DMV) by Klein and Manning (2004). Most of
the more recent unsupervised dependency pars-
ing research is based on this model. DMV is a
generative head-outward parsing model which is
trained by expectation maximization on part-of-
speech (POS) sequences of the input sentences.
Starting from a single root token, head tokens gen-
erate dependants by a probability conditioned on
the direction (left/right) from the head and the
head?s token type. Each head node generates to-
kens until a stop event is generated with a prob-
ability dependent on the same criteria plus a flag
whether some dependant token has already been
generated in the same direction.
For comparison of unsupervised and supervised
parsing, we apply the (Nivre, 2003) determinis-
tic incremental parsing algorithm Nivre arc-eager,
the default algorithm of the MALT framework
2
(Nivre et al., 2007). In this model, for each word
token, an SVM classifier decides for a parser state
transition, which, in conjunction with other deci-
sions, determines where phrases begin and end.
2
http://www.maltparser.org as of Nov. 2013
3 Experiments
We used the plain text documents of the English
Newswire and Web Text Documents provided for
TAC KBP challenge 2011 (Ji et al., 2011). We
automatically annotated relation type mentions in
these documents by distant supervision using the
online database Freebase
3
, i.e. for all relation
types of TAC KBP 2011, we took relation triples
from Freebase and, applying preprocessing as de-
scribed in Section 2, we retrieved sentences men-
tioning both arguments of some Freebase relation
with matching predicted entity types. We hypothe-
size that all sentences express the respective Free-
base relation. This way we retrieved a distantly
supervised training set of 480 622 English sen-
tences containing 92468 distinct relation instances
instantiating 41 TAC KBP relation types.
3.1 Training and Evaluation
From our retrieved set of sentences, we took those
with a maximum length of 10 tokens and trans-
formed them to POS sequences. We trained DMV
only on this dataset of short POS sequences, which
we expect to form mentions of a modeled relation.
Therefore, we suspect that DMV training assigns
an increased amount of probability mass to depen-
dency paths along structures which are truly re-
lated to these relations. We used the DMV imple-
mentation from Cohen and Smith (2009)
4
.
For the supervised Nivre arc-eager parser we
used MALT (Nivre et al., 2007) with a pre-trained
Penn Treebank (Marcus et al., 1993) model
5
. As
a baseline, we tested left branching parses i.e.
3
http://www.freebase.com as of Nov. 2013
4
publicly available at http://www.ark.cs.cmu.
edu/DAGEEM/ as of Nov. 2013 (parser version 1.0).
5
http://www.maltparser.org/mco/
english_parser/engmalt.linear-1.7.mco
as of Nov. 2013
102
 0
 0.02
 0.04
 0.06
 0.08
 0.1
 0.12
 0.14
 0.16
 0.18
 0.2
 0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
m
icr
o-a
ve
rag
e K
BP
 F 1
threshold on pattern-precision
lbranch
dmv surface
dmv dep-graph
malt surface
malt dep-graph
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0  0.05  0.1  0.15  0.2  0.25  0.3  0.35  0.4
pre
cis
ion
recall
lbranch
dmv surface
dmv dep-graph
malt surface
malt dep-graph
Figure 2: micro-averaged F
1
and precision&recall results for varied training precision thresholds
pattern set (+additional DMV pattern) precision recall F
1
MALT generated patterns only .1769 .2010 .1882
+p:title $1 * $2 of +0.73% +8.40% +4.14%
+p:title $1 , * $2 of +0.90% +4.22% +2.39%
+o:state of hqs $1 * in * , $2 +1.35% +1.59% +1.43%
+p:title $1 , * $2 who +0.90% +1.35% +1.22%
+o:parents $1 , * by $2 +0.62% +1.35% +1.06%
+o:city of hqs $1 , * in $2 , +1.01% +1.04% +1.00%
+p:origin $2 ?s $1 won the +0.84% +1.04% +0.95%
+p:employee of $1 * $2 ?s chief +0.28% +1.04% +0.79%
+o:website $1 : $2 +0.28% +1.04% +0.79%
Table 1: DMV patterns improving MALT results
the most, when added to the MALT patternset
dependency trees solely consisting of head-to-
dependent edges from the right to the left
6
.
All the extracted sentences were parsed and pat-
terns were extracted from the parses. The patterns
were then applied to the corpus and their precision
was determined according to Freebase. With dif-
ferent cut-off values on training precision, the full
relation extraction pipeline described in Section 2
was evaluated with respect to the Slot Filling test
queries of TAC KBP 2011.
3.2 Results
Figure 2 (left) depicts F
1
-measured testset results
for pattern sets with varying training precision
thresholds. Figure 2 (right) shows a precision re-
call plot of the same data points.
As can be seen in Figure 2 (left), flattening
graph patterns to surface-based patterns increased
the overall F
1
score. The curve for MALT gen-
erated surface patterns in Figure 2 (right) shows
no increase in precision towards low recall levels
where only the highest-training-precision patterns
are retained. This indicates a lack of precision
6
Since for such parses the shortest path is the complete
observed word sequence between the two relation arguments,
surface and parse-tree patterns become equal.
in MALT-based surface patterns. In contrast, the
corresponding DMV-based graph increases mono-
tonically towards lower recall levels, which is re-
flected by the highest F
1
score (Figure 2, left).
Table 1 shows the increases in evaluation score
of those DMV-generated patterns which help most
to more precisely identify relations when added to
the set of all MALT-generated patterns (sorted by
F
1
score). Figure 1 compares the syntactic analy-
ses of MALT and DMV for an example sentence
where DMV generates one of the listed patterns.
The numbers of Table 1 indicate that such patterns
are missing without alternatives in the pattern set
gained from supervised parsing.
4 Conclusion
We have presented a simple method for generat-
ing surface-based patterns from parse trees which,
besides avoiding the need for parsing test data,
also increases extraction quality. By comparing
supervised and unsupervised parsing, we further-
more found that unsupervised parsing not only
eliminates the dependency on expensive domain-
specific training data, but also produce surface-
based extraction patterns of increased quality. Our
results emphasize the need for task-driven evalu-
ation of unsupervised parsing methods and show
that there exist indicative structures for relation ex-
traction beyond widely agreed-on linguistic syntax
analyses.
5 Acknowledgements
Benjamin Roth is a recipient of the Google Europe
Fellowship in Natural Language Processing, and
this research is supported in part by this Google
Fellowship.
103
References
Enrique Alfonseca, Katja Filippova, Jean-Yves Delort,
and Guillermo Garrido. 2012. Pattern learning for
relation extraction with a hierarchical topic model.
In Proceedings of the 50th Annual Meeting of the As-
sociation for Computational Linguistics: Short Pa-
pers - Volume 2, ACL ?12, pages 54?59, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Michele Banko, Michael J. Cafarella, Stephen Soder-
land, Matt Broadhead, and Oren Etzioni. 2007.
Open information extraction from the web. In Pro-
ceedings of the 20th International Joint Conference
on Artifical Intelligence, IJCAI?07, pages 2670?
2676, San Francisco, CA, USA. Morgan Kaufmann
Publishers Inc.
Razvan C. Bunescu and Raymond J. Mooney. 2005.
A shortest path dependency kernel for relation ex-
traction. In Proceedings of the conference on Hu-
man Language Technology and Empirical Methods
in Natural Language Processing, HLT ?05, pages
724?731, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Shay B. Cohen and Noah A. Smith. 2009. Shared lo-
gistic normal distributions for soft parameter tying in
unsupervised grammar induction. In Proceedings of
Human Language Technologies: The 2009 Annual
Conference of the North American Chapter of the
Association for Computational Linguistics, NAACL
?09, pages 74?82, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Christian H?anig and Martin Schierle. 2009. Rela-
tion extraction based on unsupervised syntactic pars-
ing. In Gerhard Heyer, editor, Text Mining Ser-
vices, Leipziger Beitr?age zur Informatik, pages 65?
70, Leipzig, Germany. Leipzig University.
William Headden. 2012. Unsupervised Bayesian Lexi-
calized Dependency Grammar Induction. Ph.D. the-
sis, Brown University.
James Henderson, Paola Merlo, Ivan Titov, and
Gabriele Musillo. 2013. Multi-lingual joint parsing
of syntactic and semantic dependencies with a latent
variable model. Computational Linguistics, 39(4).
Heng Ji, Ralph Grishman, and Hoa Dang. 2011.
Overview of the TAC2011 knowledge base popula-
tion track. In TAC 2011 Proceedings Papers.
Dan Klein and Christopher D. Manning. 2004.
Corpus-based induction of syntactic structure: mod-
els of dependency and constituency. In ACL, ACL
?04, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Dan Klein. 2005. The Unsupervised Learning of Natu-
ral Language Structure. Ph.D. thesis, Stanford Uni-
versity.
Dekang Lin and Patrick Pantel. 2001. DIRT: Discov-
ery of Inference Rules from Text. In Proceedings
of the Seventh ACM SIGKDD International Con-
ference on Knowledge Discovery and Data Mining
(KDD?01), pages 323?328, New York, NY, USA.
ACM Press.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of english: the penn treebank. Comput.
Linguist., 19(2):313?330, June.
Bonan Min, Xiang Li, Ralph Grishman, and Sun Ang.
2012. New york university 2012 system for kbp
slot filling. In Proceedings of the Fifth Text Analysis
Conference (TAC 2012). National Institute of Stan-
dards and Technology (NIST), November.
M. Mintz, S. Bills, R. Snow, and D. Jurafsky. 2009.
Distant supervision for relation extraction without
labeled data. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and the
4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP: Volume 2-Volume
2, pages 1003?1011. Association for Computational
Linguistics.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas
Chanev, G?ulsen Eryigit, Sandra K?ubler, Svetoslav
Marinov, and Erwin Marsi. 2007. Maltparser:
A language-independent system for data-driven de-
pendency parsing. Natural Language Engineering,
13(2):95?135.
Joakim Nivre. 2003. An efficient algorithm for pro-
jective dependency parsing. In Proceedings of the
8th International Workshop on Parsing Technologies
(IWPT), pages 149?160.
Hoifung Poon and Pedro Domingos. 2009. Unsuper-
vised semantic parsing. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing: Volume 1 - Volume 1, EMNLP
?09, pages 1?10, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Roi Reichart and Ari Rappoport. 2009. Automatic se-
lection of high quality parses created by a fully un-
supervised parser. In Proceedings of the Thirteenth
Conference on Computational Natural Language
Learning, CoNLL ?09, pages 156?164, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Benjamin Roth, Grzegorz Chrupala, Michael Wiegand,
Singh Mittul, and Klakow Dietrich. 2012. General-
izing from freebase and patterns using cluster-based
distant supervision for tac kbp slotfilling 2012. In
Proceedings of the Fifth Text Analysis Conference
(TAC 2012), Gaithersburg, Maryland, USA, Novem-
ber. National Institute of Standards and Technology
(NIST).
Benjamin Roth, Tassilo Barth, Michael Wiegand, and
Dietrich Klakow. 2013. A survey of noise reduction
104
methods for distant supervision. In Proceedings of
the 2013 workshop on Automated knowledge base
construction, pages 73?78. ACM.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2005.
Learning syntactic patterns for automatic hypernym
discovery. In Lawrence K. Saul, Yair Weiss, and
L?eon Bottou, editors, Advances in Neural Informa-
tion Processing Systems 17, pages 1297?1304. MIT
Press, Cambridge, MA.
Alexander Yates and Oren Etzioni. 2009. Unsuper-
vised methods for determining object and relation
synonyms on the web. J. Artif. Int. Res., 34(1):255?
296, March.
Min Zhang, Jie Zhang, Jian Su, and Guodong Zhou.
2006. A composite kernel to extract relations be-
tween entities with both flat and structured features.
In Proceedings of the 21st International Conference
on Computational Linguistics and the 44th annual
meeting of the Association for Computational Lin-
guistics, ACL-44, pages 825?832, Stroudsburg, PA,
USA. Association for Computational Linguistics.
105
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1138?1147,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Topic Models for Word Sense Disambiguation and
Token-based Idiom Detection
Linlin Li, Benjamin Roth, and Caroline Sporleder
Saarland University, Postfach 15 11 50
66041 Saarbru?cken, Germany
{linlin, beroth, csporled}@coli.uni-saarland.de
Abstract
This paper presents a probabilistic model
for sense disambiguation which chooses
the best sense based on the conditional
probability of sense paraphrases given a
context. We use a topic model to decom-
pose this conditional probability into two
conditional probabilities with latent vari-
ables. We propose three different instanti-
ations of the model for solving sense dis-
ambiguation problems with different de-
grees of resource availability. The pro-
posed models are tested on three different
tasks: coarse-grained word sense disam-
biguation, fine-grained word sense disam-
biguation, and detection of literal vs. non-
literal usages of potentially idiomatic ex-
pressions. In all three cases, we outper-
form state-of-the-art systems either quan-
titatively or statistically significantly.
1 Introduction
Word sense disambiguation (WSD) is the task of
automatically determining the correct sense for a
target word given the context in which it occurs.
WSD is an important problem in NLP and an es-
sential preprocessing step for many applications,
including machine translation, question answering
and information extraction. However, WSD is a
difficult task, and despite the fact that it has been
the focus of much research over the years, state-
of-the-art systems are still often not good enough
for real-world applications. One major factor that
makes WSD difficult is a relative lack of manu-
ally annotated corpora, which hampers the perfor-
mance of supervised systems.
To address this problem, there has been a
significant amount of work on unsupervised
WSD that does not require manually sense-
disambiguated training data (see McCarthy (2009)
for an overview). Recently, several researchers
have experimented with topic models (Brody and
Lapata, 2009; Boyd-Graber et al, 2007; Boyd-
Graber and Blei, 2007; Cai et al, 2007) for sense
disambiguation and induction. Topic models are
generative probabilistic models of text corpora in
which each document is modelled as a mixture
over (latent) topics, which are in turn represented
by a distribution over words.
Previous approaches using topic models for
sense disambiguation either embed topic features
in a supervised model (Cai et al, 2007) or rely
heavily on the structure of hierarchical lexicons
such as WordNet (Boyd-Graber et al, 2007). In
this paper, we propose a novel framework which
is fairly resource-poor in that it requires only 1)
a large unlabelled corpus from which to estimate
the topics distributions, and 2) paraphrases for the
possible target senses. The paraphrases can be
user-supplied or can be taken from existing re-
sources.
We approach the sense disambiguation task by
choosing the best sense based on the conditional
probability of sense paraphrases given a context.
We propose three models which are suitable for
different situations: Model I requires knowledge
of the prior distribution over senses and directly
maximizes the conditional probability of a sense
given the context; Model II maximizes this condi-
tional probability by maximizing the cosine value
of two topic-document vectors (one for the sense
and one for the context). We apply these models
to coarse- and fine-grained WSD and find that they
outperform comparable systems for both tasks.
We also test our framework on the related task
of idiom detection, which involves distinguishing
literal and nonliteral usages of potentially ambigu-
ous expressions such as rock the boat. For this
task, we propose a third model. Model III cal-
culates the probability of a sense given a context
according to the component words of the sense
1138
paraphrase. Specifically, it chooses the sense type
which maximizes the probability (given the con-
text) of the paraphrase component word with the
highest likelihood of occurring in that context.
This model also outperforms state-of-the-art sys-
tems.
2 Related Work
There is a large body of work on WSD, cover-
ing supervised, unsupervised (word sense induc-
tion) and knowledge-based approaches (see Mc-
Carthy (2009) for an overview). While most su-
pervised approaches treat the task as a classifica-
tion task and use hand-labelled corpora as train-
ing data, most unsupervised systems automatically
group word tokens into similar groups using clus-
tering algorithms, and then assign labels to each
sense cluster. Knowledge-based approaches ex-
ploit information contained in existing resources.
They can be combined with supervised machine-
learning models to assemble semi-supervised ap-
proaches.
Recently, a number of systems have been pro-
posed that make use of topic models for sense
disambiguation. Cai et al (2007), for example,
use LDA to capture global context. They com-
pute topic models from a large unlabelled corpus
and include them as features in a supervised sys-
tem. Boyd-Graber and Blei (2007) propose an un-
supervised approach that integrates McCarthy et
al.?s (2004) method for finding predominant word
senses into a topic modelling framework. In ad-
dition to generating a topic from the document?s
topic distribution and sampling a word from that
topic, the enhanced model also generates a distri-
butional neighbour for the chosen word and then
assigns a sense based on the word, its neighbour
and the topic. Boyd-Graber and Blei (2007) test
their method on WSD and information retrieval
tasks and find that it can lead to modest improve-
ments over state-of-the-art results.
In another unsupervised system, Boyd-Graber
et al (2007) enhance the basic LDA algorithm by
incorporating WordNet senses as an additional la-
tent variable. Instead of generating words directly
from a topic, each topic is associated with a ran-
dom walk through the WordNet hierarchy which
generates the observed word. Topics and synsets
are then inferred together. While Boyd-Graber
et al (2007) show that this method can lead to
improvements in accuracy, they also find that id-
iosyncracies in the hierarchical structure of Word-
Net can harm performance. This is a general prob-
lem for methods which use hierarchical lexicons
to model semantic distance (Budanitsky and Hirst,
2006). In our approach, we circumvent this prob-
lem by exploiting paraphrase information for the
target senses rather than relying on the structure
of WordNet as a whole.
Topic models have also been applied to the re-
lated task of word sense induction. Brody and
Lapata (2009) propose a method that integrates a
number of different linguistic features into a single
generative model.
Topic models have been previously consid-
ered for metaphor extraction and estimating the
frequency of metaphors (Klebanov et al, 2009;
Bethard et al, 2009). However, we have a differ-
ent focus in this paper, which aims to distinguish
literal and nonliteral usages of potential idiomatic
expressions. A number of methods have been ap-
plied to this task. Katz and Giesbrecht (2006)
devise a supervised method in which they com-
pute the meaning vectors for the literal and non-
literal usages of a given expression in the trainning
data. Birke and Sarkar (2006) use a clustering al-
gorithm which compares test instances to two au-
tomatically constructed seed sets (one literal and
one nonliteral), assigning the label of the closest
set. An unsupervised method that computes co-
hesive links between the component words of the
target expression and its context have been pro-
posed (Sporleder and Li, 2009; Li and Sporleder,
2009). Their system predicts literal usages when
strong links can be found.
3 The Sense Disambiguation Model
3.1 Topic Model
As pointed out by Hofmann (1999), the starting
point of topic models is to decompose the con-
ditional word-document probability distribution
p(w|d) into two different distributions: the word-
topic distribution p(w|z), and the topic-document
distribution p(z|d) (see Equation 1). This allows
each semantic topic z to be represented as a multi-
nominal distribution of words p(w|z), and each
document d to be represented as a multinominal
distribution of semantic topics p(z|d). The model
introduces a conditional independence assumption
that document d and word w are independent con-
1139
ditioned on the hidden variable, topic z.
p(w|d) =
?
z
p(z|d)p(w|z) (1)
LDA is a Bayesian version of this framework with
Dirichlet hyper-parameters (Blei et al, 2003).
The inference of the two distributions given an
observed corpus can be done through Gibbs Sam-
pling (Geman and Geman, 1987; Griffiths and
Steyvers, 2004). For each turn of the sampling,
each word in each document is assigned a seman-
tic topic based on the current word-topic distribu-
tion and topic-document distribution. The result-
ing topic assignments are then used to re-estimate
a new word-topic distribution and topic-document
distribution for the next turn. This process re-
peats until convergence. To avoid statistical co-
incidence, the final estimation of the distributions
is made by the average of all the turns after con-
vergence.
3.2 The Sense Disambiguation Model
Assigning the correct sense s to a target word w
occurring in a context c involves finding the sense
which maximizes the conditional probability of
senses given a context:
s = argmax
si
p(si|c) (2)
In our model, we represent a sense (si) as a col-
lection of ?paraphrases? that capture (some aspect
of) the meaning of the sense. These paraphrases
can be taken from an existing resource such as
WordNet (Miller, 1995) or supplied by the user
(see Section 4).
This conditional probability is decomposed by
incorporating a hidden variable, topic z, intro-
duced by the topic model. We propose three varia-
tions of the basic model, depending on how much
background information is available, i.e., knowl-
edge of the prior sense distribution available and
type of sense paraphrases used. In Model I and
Model II, the sense paraphrases are obtained from
WordNet, and both the context and the sense para-
phrases are treated as documents, c = dc and
s = ds.
WordNet is a fairly rich resource which pro-
vides detailed information about word senses
(glosses, example sentences, synsets, semantic re-
lations between senses, etc.). Sometimes such de-
tailed information may not be available, for in-
stance for languages for which such a resource
does not exist or for expressions that are not
very well covered in WordNet, such as idioms.
For those situations, we propose another model,
Model III, in which contexts are treated as docu-
ments while sense paraphrases are treated as se-
quences of independent words.1
Model I directly maximizes the conditional
probability of the sense given the context, where
the sense is modeled as a ?paraphrase document?
ds and the context as a ?context document? dc.
The conditional probability of sense given context
p(ds|dc) can be rewritten as a joint probability di-
vided by a normalization factor:
p(ds|dc) =
p(ds, dc)
p(dc)
(3)
This joint probability can be rewritten as a gen-
erative process by introducing a hidden variable z.
We make the conditional independence assump-
tion that, conditioned on the topic z, a paraphrase
document ds is generated independently of the
specific context document dc:
p(ds, dc) =
?
z
p(ds)p(z|ds)p(dc|z) (4)
We apply the same process to the conditional
probability p(dc|z). It can be rewritten as:
p(dc|z) =
p(dc)p(z|dc)
p(z)
(5)
Now, the disambiguation model p(ds|dc) can be
rewritten as a prior p(ds) times a topic function
f(z):
p(ds|dc) = p(ds)
?
z
p(z|dc)p(z|ds)
p(z)
(6)
As p(z) is a uniform distribution according to
the uniform Dirichlet priors assumption, Equation
6 can be rewritten as:
p(ds|dc) ? p(ds)
?
z
p(z|dc)p(z|ds) (7)
Model I:
argmax
dsi
p(dsi)
?
z
p(z|dc)p(z|dsi) (8)
Model I has the disadvantage that it requires
information about the prior distribution of senses
1The idea is that these key words capture the meaning of
the idioms.
1140
p(ds), which is not always available. We use sense
frequency information from WordNet to estimate
the prior sense distribution, although it must be
kept in mind that, depending on the genre of the
texts, it is possible that the distribution of senses
in the testing corpus may diverge greatly from the
WordNet-based estimation. If there is no means
for estimating the prior sense distribution of an
experimental corpus, generally a uniform distri-
bution must be assumed. However, this assump-
tion does not hold, as the true distribution of word
senses is often highly skewed (McCarthy, 2009).
To overcome this problem, we propose Model
II, which indirectly maximizes the sense-context
probability by maximizing the cosine value of two
document vectors that encode the document-topic
frequencies from sampling, v(z|dc) and v(z|ds).
The document vectors are represented by topics,
with each dimension representing the number of
times that the tokens in this document are assigned
to a certain topic.
Model II:
argmax
dsi
cos(v(z|dc), v(z|dsi)) (9)
If the prior distribution of senses is known, Model
I is the best choice. However, Model II has to be
chosen instead when this knowledge is not avail-
able. In our experiments, we test the performance
of both models (see Section 5).
If the sense paraphrases are very short, it is diffi-
cult to reliably estimate p(z|ds). In order to solve
this problem, we treat the sense paraphrase ds as
a ?query?, a concept which is used in information
retrieval. One model from information retrieval
takes the conditional probability of the query given
the document as a product of all the conditional
probabilities of words in the query given the doc-
ument. The assumption is that the query is gener-
ated by a collection of conditionally independent
words (Song and Croft, 1999).
We make the same assumption here. How-
ever, instead of taking the product of all the condi-
tional probabilities of words given the document,
we take the maximum. There are two reasons for
this: (i) taking the product may penalize longer
paraphrases since the product of probabilities de-
creases as there are more words; (ii) we do not
want to model the probability of generating spe-
cific paraphrases, but rather the probability of gen-
erating a sense, which might only be represented
by one or two words in the paraphrases (e.g., the
potentially idiomatic phrase ?rock the boat? can be
paraphrased as ?break the norm? or ?cause trou-
ble?. A similar topic distribution to that of the
individual words ?norm? or ?trouble? would be
strong supporting evidence of the corresponding
idiomatic reading.). We propose Model III:
argmax
qsi
max
wi?qs
?
z
p(wi|z)p(z|dc) (10)
where qs is a collection of words contained in the
sense paraphrases.
3.3 Inference
One possible inference approach is to combine the
context documents and sense paraphrases into a
corpus and run Gibbs sampling on this corpus.
The problem with this approach is that the test set
and sense paraphrase set are relatively small, and
topic models running on a small corpus are less
likely to capture rich semantic topics. One sim-
ple explanation for this is that a small corpus usu-
ally has a relatively small vocabulary, which is less
representative of topics, i.e., p(w|z) cannot be es-
timated reliably.
In order to overcome this problem, we infer the
word-topic distribution from a very large corpus
(Wikipedia dump, see Section 4). All the follow-
ing inference experiments on the test corpus are
based on the assumption that the word-topic dis-
tribution p(w|z) is the same as the one estimated
from the Wikipedia dump. Inference of topic-
document distributions for context and sense para-
phrases is done by fixing the word-topic distribu-
tion as a constant.
4 Experimental Setup
We evaluate our models on three different tasks:
coarse-grained WSD, fine-grained WSD and lit-
eral vs. nonliteral sense detection. In this section
we discuss our experimental set-up. We start by
describing the three datasets for evaluation and an-
other dataset for probability estimation. We also
discuss how we choose sense paraphrases and in-
stance contexts.
Data We use three datasets for evaluation. The
coarse-grained task is evaluated on the Semeval-
2007 Task-07 benchmark dataset released by Nav-
igli et al (2009). The dataset consists of 5377
words of running text from five different articles:
the first three were obtained from the WSJ cor-
pus, the fourth was the Wikipedia entry for com-
puter programming, and the fifth was an excerpt of
1141
Amy Steedman?s Knights of the Art, biographies
of Italian painters. The proportion of the non news
text, the last two articles, constitutes 51.87% of the
whole testing set. It consists of 1108 nouns, 591
verbs, 362 adjectives, and 208 adverbs. The data
were annotated with coarse-grained senses which
were obtained by clustering senses from the Word-
Net 2.1 sense inventory based on the procedure
proposed by Navigli (2006).
To determine whether our model is also suitable
for fine-grained WSD, we test on the data provided
by Pradhan et al (2009) for the Semeval-2007
Task-17 (English fine-grained all-words task).
This dataset is a subset of the set from Task-07. It
comprises the three WSJ articles from Navigli et
al. (2009). A total of 465 lemmas were selected as
instances from about 3500 words of text. There are
10 instances marked as ?U? (undecided sense tag).
Of the remaining 455 instances, 159 are nouns and
296 are verbs. The sense inventory is from Word-
Net 2.1.
Finally, we test our model on the related sense
disambiguation task of distinguishing literal and
nonliteral usages of potentially ambiguous expres-
sions such as break the ice. For this, we use the
dataset from Sporleder and Li (2009) as a test set.
This dataset consists of 3964 instances of 17 po-
tential English idioms which were manually anno-
tated as literal or nonliteral.
A Wikipedia dump2 is used to estimate the
multinomial word-topic distribution. This dataset,
which consists of 320,000 articles,3 is significantly
larger than SemCor, which is the dataset used by
Boyd-Graber et al (2007). All markup from the
Wikipedia dump was stripped off using the same
filter as the ESA implementation (Sorg and Cimi-
ano, 2008), and stopwords were filtered out using
the Snowball (Porter, October 2001) stopword list.
In addition, words with a Wikipedia document fre-
quency of 1 were filtered out. The lemmatized
version of the corpus consists of 299,825 lexical
units.
The test sets were POS-tagged and lemmatized
using RASP (Briscoe and Carroll, 2006). The in-
ference processes are run on the lemmatized ver-
sion of the corpus. For the Semeval-2007 Task 17
English all-words, the organizers do not supply the
part-of-speech and lemma information of the tar-
get instances. In order to avoid the wrong predic-
2We use the English snapshot of 2009-07-13
3All articles of fewer than 100 words were discarded.
tions caused by tagging or lemmatization errors,
we manually corrected any bad tags and lemmas
for the target instances.4
Sense Paraphrases For word sense disam-
biguation tasks, the paraphrases of the sense keys
are represented by information from WordNet 2.1.
(Miller, 1995). To obtain the paraphrases, we use
the word forms, glosses and example sentences
of the synset itself and a set of selected reference
synsets (i.e., synsets linked to the target synset by
specific semantic relations, see Table 1). We ex-
cluded the ?hypernym reference synsets?, since in-
formation common to all of the child synsets may
confuse the disambiguation process.
For the literal vs. nonliteral sense detection
task, we selected the paraphrases of the nonlit-
eral meaning from several online idiom dictionar-
ies. For the literal senses, we used 2-3 manu-
ally selected words with which we tried to cap-
ture (aspects of) the literal meaning of the expres-
sion.5 For instance, the literal ?paraphrases? that
we chose for ?break the ice? were ice, water and
snow. The paraphrases are shorter for the idiom
task than for the WSD task, because the mean-
ing descriptions from the idiom dictionaries are
shorter than what we get from WordNet. In the
latter case, each sense can be represented by its
synset as well as its reference synsets.
Instance Context We experimented with differ-
ent context sizes for the disambiguation task. The
five different context settings that we used for the
WSD tasks are: collocations (1w), ?5-word win-
dow (5w), ?10-word window (10w), current sen-
tence, and whole text. Because the idiom corpus
also includes explicitly marked paragraph bound-
aries, we included ?paragraph? as a sixth type of
context size for the idiom sense detection task.
5 Experiments
As mentioned above, we test our proposed sense
disambiguation framework on three tasks. We
start by describing the sampling experiments for
4This was done by comparing the predicted sense keys
and the gold standard sense keys. We only checked instances
for which the POS-tags in the predicted sense keys are not
consistent with those in the gold standard. This was the case
for around 20 instances.
5Note that we use the word ?paraphrase? in a fairly wide
sense in this paper. Sometimes it is not possible to obtain ex-
act paraphrases. This applies especially to the task of distin-
guishing literal from nonliteral senses of multi-word expres-
sions. In this case we take as paraphrases some key words
which capture salient aspects of the meaning.
1142
POS Paraphrase reference synsets
N hyponyms, instance hyponyms, member holonyms, substance holonyms, part holonyms,
member meronyms, part meronyms, substance meronyms, attributes, topic members,
region members, usage members, topics, regions, usages
V Troponyms, entailments, outcomes, phrases, verb groups, topics, regions, usages, sentence frames
A similar, pertainym, attributes, related, topics, regions, usages
R pertainyms, topics, regions, usages
Table 1: Selected reference synsets from WordNet that were used for different parts-of-speech to obtain
word sense paraphrase. N(noun), V(verb), A(adj), R(adv).
estimating the word-topic distribution from the
Wikipedia dump. We used the package provided
by Wang et al (2009) with the suggested Dirich-
let hyper-parameters 6. In order to avoid statistical
instability, the final result is averaged over the last
50 iterations. We did four rounds of sampling with
1000, 500, 250, and 125 topics respectively. The
final word-topic distribution is a normalized con-
catenate of the four distributions estimated in each
round. In average, the sampling program run on
the Wikipedia dump consumed 20G memory, and
each round took about one week on a single AMD
Dual-Core 1000MHZ processor.
5.1 Coarse-Grained WSD
In this section we first describe the landscape of
similar systems against which we compare our
models, then present the results of the comparison.
The systems that participated in the SemEval-2007
coarse-grained WSD task (Task-07) can be di-
vided into three categories, depending on whether
training data is needed and whether other types
of background knowledge are required: What we
call Type I includes all the systems that need an-
notated training data. All the participating sys-
tems that have the mark TR fall into this cate-
gory (see Navigli et al (2009) for the evaluation
for all the participating systems). Type II con-
sists of systems that do not need training data but
require prior knowledge of the sense distribution
(estimated sense frequency). All the participating
systems that have the mark MFS belong to this cat-
egory. Systems that need neither training data nor
prior sense distribution knowledge are categorized
as Type III.
We make this distinction based on two princi-
ples: (i) the cost of building a system; (ii) the
portability of the established resource. Type III
is the cheapest system to build, while Type I and
6They were set as: ? = 50#topics and ? = 0.01.
Type II both need extra resources. Type II has
an advantage over Type I since the prior knowl-
edge of the sense distribution can be estimated
from annotated corpora (e.g.: SemCor, Senseval).
In contrast, training data in Type I may be sys-
tem specific (e.g.: different input format, different
annotation guidelines). McCarthy (2009) also ad-
dresses the issue of performance and cost by com-
paring supervised word sense disambiguation sys-
tems with unsupervised ones.
We exclude the system provided by one of
the organizers (UoR-SSI) from our categorization.
The reason is that although this system is claimed
to be unsupervised, and it performs better than
all the participating systems (including the super-
vised systems) in the SemEval-2007 shared task, it
still needs to incorporate a lot of prior knowledge,
specifically information about co-occurrences be-
tween different word senses, which was obtained
from a number of resources (SSI+LKB) includ-
ing: (i) SemCor (manually annotated); (ii) LDC-
DSO (partly manually annotated); (iii) collocation
dictionaries which are then disambiguated semi-
automatically. Even though the system is not
?trained?, it needs a lot of information which is
largely dependent on manually annotated data, so
it does not fit neatly into the categories Type II or
Type III either.
Table 2 lists the best participating systems of
each type in the SemEval-2007 task (Type I:
NUS-PT (Chan et al, 2007); Type II: UPV-WSD
(Buscaldi and Rosso, 2007); Type III: TKB-UO
(Anaya-Sa?nchez et al, 2007)). Our Model I be-
longs to Type II, and our Model II belongs to Type
III.
Table 2 compares the performance of our mod-
els with the Semeval-2007 participating systems.
We only compare the F-score, since all the com-
pared systems have an attempted rate7 of 1.0,
7Attempted rate is defined as the total number of disam-
biguated output instances divided by the total number of input
1143
which makes both the precision and recall rates the
same as the F-score. We focus on comparisons be-
tween our models and the best SemEval-2007 par-
ticipating systems within the same type. Model I is
compared with UPV-WSD, and Model II is com-
pared with TKB-UO. In addition, we also compare
our system with the most frequent sense baseline
which was not outperformed by any of the systems
of Type II and Type III in the SemEval-2007 task.
Comparison on Type III is marked with ?, while
comparison on Type II is marked with ?. We find
that Model II performs statistically significantly
better than the best participating system of the
same type TKB-UO (p<<0.01, ?2 test). When
encoded with the prior knowledge of sense distri-
bution, Model I outperforms by 1.36% the best
Type II system UPV-WSD, although the differ-
ence is not statistically significant. Furthermore,
Model I also quantitatively outperforms the most
frequent sense baseline BLmfs, which, as men-
tioned above, was not beat by any participating
systems that do not use training data.
We also find that our model works best for
nouns. The unsupervised Type III model Model
II achieves better results than the most frequent
sense baseline on nouns, but not on other parts-
of-speech. This is in line with results obtained
by previous systems (Griffiths et al, 2005; Boyd-
Graber and Blei, 2008; Cai et al, 2007). While the
performance on verbs can be increased to outper-
form the most frequent sense baseline by including
the prior sense probability, the performance on ad-
jectives and adverbs remains below the most fre-
quent sense baseline. We think that there are three
reasons for this: first, adjectives and adverbs have
fewer reference synsets for paraphrases compared
with nouns and verbs (see Table 1); second, adjec-
tives and adverbs tend to convey less key semantic
content in the document, so they are more difficult
to capture by the topic model; and third, adjectives
and adverbs are a small portion of the test set, so
their performances are statistically unstable. For
example, if ?already? appears 10 times out of 20
adverb instances, a system may get bad result on
adverbs only because of its failure to disambiguate
the word ?already?.
Paraphrase analysis Table 2 also shows the
effect of different ways of choosing sense para-
phrases. MII+ref is the result of including the ref-
erence synsets, while MII-ref excludes the refer-
instances.
System Noun Verb Adj Adv All
UoR-SSI 84.12 78.34 85.36 88.46 83.21
NUS-PT 82.31 78.51 85.64 89.42 82.50
UPV-WSD 79.33 72.76 84.53 81.52 78.63?
TKB-UO 70.76 62.61 78.73 74.04 70.21?
MII?ref 78.16 70.39 79.56 81.25 76.64
MII+ref 80.05 70.73 82.04 82.21 78.14?
MI+ref 79.96 75.47 83.98 86.06 79.99?
BLmfs 77.44 75.30 84.25 87.50 78.99?
Table 2: Model performance (F-score) on the
coarse-grained dataset (context=sentence). Para-
phrases with/without reference synsets (+ref/-ref).
Context Ate. Pre. Rec. F1
?1w 91.67 75.05 68.80 71.79
?5w 99.29 77.14 76.60 76.87
?10w 100 77.92 77.92 77.92
text 100 76.86 76.86 76.86
sent. 100 78.14 78.14 78.14
Table 3: Model II performance on different con-
text size. attempted rate (Ate.), precision (Pre.),
recall (Rec.), F-score (F1).
ence synsets. As can be seen from the table, in-
cluding all reference synsets in sense paraphrases
increases performance. Longer paraphrases con-
tain more information, and they are statistically
more stable for inference.
We find that nouns get the greatest perfor-
mance boost from including reference synsets, as
they have the largest number of different types of
synsets. We also find the ?similar? reference synset
for adjectives to be very useful. Performance on
adjectives increases by 2.75% when including this
reference synset.
Context analysis In order to study how the con-
text influences the performance, we experiment
with Model II on different context sizes (see Ta-
ble 3). We find sentence context is the best size for
this disambiguation task. Using a smaller context
not only reduces the precision, but also reduces the
recall rate, which is caused by the all-zero topic as-
signment by the topic model for documents only
containing words that are not in the vocabulary.
As a result, the model is unable to disambiguate.
The context based on the whole text (article) does
not perform well either, possibly because using the
full text folds in too much noisy information.
1144
System F-score
RACAI 52.7 ?4.5
BLmfs 55.91?4.5
MI+ref 56.99?4.5
Table 4: Model performance (F-score) for the fine-
grained word sense disambiguation task.
5.2 Fine-grained WSD
We saw in the previous section that our frame-
work performs well on coarse-grained WSD. Fine-
grained WSD, however, is a more difficult task. To
determine whether our framework is also able to
detect subtler sense distinctions, we tested Model I
on the English all-words subtask of SemEval-2007
Task-17 (see Table 4).
We find that Model I performs better than both
the best unsupervised system, RACAI (Ion and
Tufis?, 2007) and the most frequent sense baseline
(BLmfs), although these differences are not sta-
tistically significant due to the small size of the
available test data (465).
5.3 Idiom Sense Disambiguation
In the previous section, we provided the results
of applying our framework to coarse- and fine-
grained word sense disambiguation tasks. For
both tasks, our models outperform the state-of-
the-art systems of the same type either quantita-
tively or statistically significantly. In this section,
we apply Model III to another sense disambigua-
tion task, namely distinguishing literal and nonlit-
eral senses of ambiguous expressions.
WordNet has a relatively low coverage for id-
iomatic expressions. In order to represent non-
literal senses, we replace the paraphrases obtained
automatically from WordNet by words selected
manually from online idiom dictionaries (for the
nonliteral sense) and by linguistic introspection
(for the literal sense). We then compare the topic
distributions of literal and nonliteral senses.
As the paraphrases obtained from the idiom dic-
tionary are very short, we treat the paraphrase
as a sequence of independent words instead of
as a document and apply Model III (see Sec-
tion 3). Table 5 shows the results of our pro-
posed model compared with state-of-the-art sys-
tems. We find that the system significantly out-
performs the majority baseline (p<<0.01, ?2 test)
and the cohesion-graph based approach proposed
by Sporleder and Li (2009) (p<<0.01, ?2 test).
The system also outperforms the bootstrapping
System Precl Recl Fl Acc.
Basemaj - - - 78.25
co-graph 50.04 69.72 58.26 78.38
boot. 71.86 66.36 69.00 87.03
Model III 67.05 81.07 73.40 87.24
Table 5: Performance on the literal or nonliteral
sense disambiguation task on idioms. literal pre-
cision (Precl), literal recall (Recl), literal F-score
(Fl), accuracy(Acc.).
system by Li and Sporleder (2009), although not
statistically significantly. This shows how a lim-
ited amount of human knowledge (e.g., para-
phrases) can be added to an unsupervised system
for a strong boost in performance ( Model III com-
pared with the cohesion-graph and the bootstrap-
ping approaches).
For obvious reasons, this approach is sensitive
to the quality of the paraphrases. The paraphrases
chosen to characterise (aspects of) the meaning of
a sense should be non-ambiguous between the lit-
eral or idiomatic meaning. For instance, ?fire? is
not a good choice for a paraphrase of the literal
reading of ?play with fire?, since this word can
be interpreted literally as ?fire? or metaphorically
as ?something dangerous?. The verb component
word ?play? is a better literal paraphrase.
For the same reason, this approach works well
for expressions where the literal and nonliteral
readings are well separated (i.e., occur in different
contexts), while the performance drops for expres-
sions whose literal and idiomatic readings can ap-
pear in a similar context. We test the performance
on individual idioms on the five most frequent id-
ioms in our corpus8 (see Table 6). We find that
?drop the ball? is a difficult case. The words ?fault?,
?mistake?, ?fail? or ?miss? can be used as the nonlit-
eral paraphrases. However, it is also highly likely
that these words are used to describe a scenario in
a baseball game, in which ?drop the ball? is used
literally. In contrast, the performance on ?rock the
boat? is much better, since the nonliteral reading
of the phrases ?break the norm? or ?cause trouble?
are less likely to be linked with the literal reading
?boat?. This may also be because ?boat? is not of-
ten used metaphorically in the corpus.
As the topic distribution of nouns and verbs
exhibit different properties, topic comparisons
across parts-of-speech do not make sense. We
8We tested only on the most frequent idioms in order to
avoid statistically unreliable observations.
1145
Idiom Acc.
drop the ball 75.86
play with fire 91.17
break the ice 87.43
rock the boat 95.82
set in stone 89.39
Table 6: Performance on individual idioms.
make the topic distributions comparable by mak-
ing sure each type of paraphrase contains the same
sets of parts-of-speech. For instance, we do not
permit combinations of literal paraphrases which
only consist of nouns and nonliteral paraphrases
which only consist of verbs.
6 Conclusion
We propose three models for sense disambigua-
tion on words and multi-word expressions. The
basic idea of these models is to compare the topic
distribution of a target instance with the candidate
sense paraphrases and choose the most probable
one. While Model I and Model III model the
problem in a probabilistic way, Model II uses a
vector space model by comparing the cosine val-
ues of two topic vectors. Model II and Model III
are completely unsupervised, while Model I needs
the prior sense distribution. Model I and Model
II treat the sense paraphrases as documents, while
Model III treats the sense paraphrases as a collec-
tion of independent words.
We test the proposed models on three tasks. We
apply Model I and Model II to the WSD tasks due
to the availability of more paraphrase information.
Model III is applied to the idiom detection task
since the paraphrases from the idiom dictionary
are smaller. We find that all models outperform
comparable state-of-the-art systems either quanti-
tatively or statistically significantly.
By testing our framework on three different
sense disambiguation tasks, we show that the
framework can be used flexibly in different ap-
plication tasks. The system also points out a
promising way of solving the granularity problem
of word sense disambiguation, as new application
tasks which need different sense granularities can
utilize this framework when new paraphrases of
sense clusters are available. In addition, this sys-
tem can also be used in a larger context such as
figurative language identification (literal or figu-
rative) and sentiment detection (positive or nega-
tive).
Acknowledgments
This work was funded by the DFG within the
Cluster of Excellence ?Multimodal Computing
and Interaction?.
References
H. Anaya-Sa?nchez, A. Pons-Porrata, R. Berlanga-
Llavori. 2007. TKB-UO: using sense clustering for
WSD. In SemEval ?07: Proceedings of the 4th Inter-
national Workshop on Semantic Evaluations, 322?
325.
S. Bethard, V. T. Lai, J. H. Martin. 2009. Topic model
analysis of metaphor frequency for psycholinguistic
stimuli. In CALC ?09: Proceedings of the Workshop
on Computational Approaches to Linguistic Creativ-
ity, 9?16, Morristown, NJ, USA. Association for
Computational Linguistics.
J. Birke, A. Sarkar. 2006. A clustering approach
for the nearly unsupervised recognition of nonliteral
language. In Proceedings of EACL-06.
D. M. Blei, A. Y. Ng, M. I. Jordan. 2003. Latent
dirichlet alocation. Journal of Machine Learning
Reseach, 3:993?1022.
J. Boyd-Graber, D. Blei. 2007. PUTOP: turning
predominant senses into a topic model for word
sense disambiguation. In Proceedings of the Fourth
International Workshop on Semantic Evaluations
(SemEval-2007), 277?281.
J. Boyd-Graber, D. Blei. 2008. Syntactic topic models.
Computational Linguistics.
J. Boyd-Graber, D. Blei, X. Zhu. 2007. A topic
model for word sense disambiguation. In Proceed-
ings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), 1024?1033.
T. Briscoe, J. Carroll. 2006. Evaluating the accuracy
of an unlexicalized statistical parser on the PARC
DepBank. In Proceedings of the COLING/ACL on
Main conference poster sessions, 41?48.
S. Brody, M. Lapata. 2009. Bayesian word sense
induction. In Proceedings of the 12th Conference
of the European Chapter of the ACL (EACL 2009),
103?111.
A. Budanitsky, G. Hirst. 2006. Evaluating wordnet-
based measures of lexical semantic relatedness.
Computational Linguistics, 32(1):13?47.
D. Buscaldi, P. Rosso. 2007. UPV-WSD: Combining
different WSD methods by means of Fuzzy Borda
Voting. In SemEval ?07: Proceedings of the 4th
International Workshop on Semantic Evaluations,
434?437.
J. Cai, W. S. Lee, Y. W. Teh. 2007. Improving word
sense disambiguation using topic features. In Pro-
ceedings of the 2007 Joint Conference on Empirical
1146
Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), 1015?1023.
Y. S. Chan, H. T. Ng, Z. Zhong. 2007. NUS-PT: ex-
ploiting parallel texts for word sense disambiguation
in the English all-words tasks. In SemEval ?07: Pro-
ceedings of the 4th International Workshop on Se-
mantic Evaluations, 253?256.
S. Geman, D. Geman. 1987. Stochastic relaxation,
Gibbs distributions, and the Bayesian restoration
of images. In Readings in computer vision: is-
sues, problems, principles, and paradigms, 564?
584. Morgan Kaufmann Publishers Inc., San Fran-
cisco, CA, USA.
T. L. Griffiths, M. Steyvers. 2004. Finding scientific
topics. Proceedings of the National Academy of Sci-
ences, 101(Suppl. 1):5228?5235.
T. L. Griffiths, M. Steyvers, D. M. Blei, J. B. Tenen-
baum. 2005. Integrating topics and syntax. In In
Advances in Neural Information Processing Systems
17, 537?544. MIT Press.
T. Hofmann. 1999. Probabilistic latent semantic in-
dexing. In SIGIR ?99: Proceedings of the 22nd an-
nual international ACM SIGIR conference on Re-
search and development in information retrieval,
50?57.
R. Ion, D. Tufis?. 2007. Racai: meaning affinity mod-
els. In SemEval ?07: Proceedings of the 4th Inter-
national Workshop on Semantic Evaluations, 282?
287, Morristown, NJ, USA. Association for Compu-
tational Linguistics.
G. Katz, E. Giesbrecht. 2006. Automatic identifi-
cation of non-compositional multi-word expressions
using latent semantic analysis. In Proceedings of the
ACL/COLING-06 Workshop on Multiword Expres-
sions: Identifying and Exploiting Underlying Prop-
erties.
B. B. Klebanov, E. Beigman, D. Diermeier. 2009. Dis-
course topics and metaphors. In CALC ?09: Pro-
ceedings of the Workshop on Computational Ap-
proaches to Linguistic Creativity, 1?8, Morristown,
NJ, USA. Association for Computational Linguis-
tics.
L. Li, C. Sporleder. 2009. Contextual idiom detection
without labelled data. In Proceedings of EMNLP-
09.
D. McCarthy, R. Koeling, J. Weeds, J. Carroll. 2004.
Finding predominant word senses in untagged text.
In Proceedings of the 42nd Meeting of the Associa-
tion for Computational Linguistics (ACL?04), Main
Volume, 279?286.
D. McCarthy. 2009. Word sense disambiguation:
An overview. Language and Linguistics Compass,
3(2):537?558.
G. A. Miller. 1995. WordNet: a lexical database for
english. Commun. ACM, 38(11):39?41.
R. Navigli, K. C. Litkowski, O. Hargraves. 2009.
SemEval-2007 Task 07: Coarse-grained English all-
words task. In Proceedings of the 4th International
Workshop on Semantic Evaluation (SemEval-2007).
R. Navigli. 2006. Meaningful clustering of senses
helps boost word sense disambiguation perfor-
mance. In Proceedings of the 44th Annual Meeting
of the Association for Computational Liguistics joint
with the 21st International Conference on Computa-
tional Liguistics (COLING-ACL 2006).
M. Porter. October 2001. Snowball: A lan-
guage for stemming algorithms. http:
//snowball.tartarus.org/texts/
introduction.html.
S. S. Pradhan, E. Loper, D. Dligach, M. Palmer. 2009.
SemEval-2007 Task 07: Coarse-grained English all-
words task. In Proceedings of the 4th International
Workshop on Semantic Evaluation (SemEval-2007).
F. Song, W. B. Croft. 1999. A general language model
for information retrieval (poster abstract). In Re-
search and Development in Information Retrieval,
279?280.
P. Sorg, P. Cimiano. 2008. Cross-lingual information
retrieval with explicit semantic analysis. In In Work-
ing Notes for the CLEF 2008 Workshop.
C. Sporleder, L. Li. 2009. Unsupervised recognition of
literal and non-literal use of idiomatic expressions.
In Proceedings of EACL-09.
Y. Wang, H. Bai, M. Stanton, W.-Y. Chen, E. Y. Chang.
2009. Plda: Parallel latent dirichlet alocation for
large-scale applications. In Proc. of 5th Interna-
tional Conference on Algorithmic Aspects in Infor-
mation and Management. Software available at
http://code.google.com/p/plda.
1147
Proceedings of the Workshop on Negation and Speculation in Natural Language Processing, pages 60?68,
Uppsala, July 2010.
A Survey on the Role of Negation in Sentiment Analysis
Michael Wiegand
Saarland University
Saarbru?cken, Germany
michael.wiegand@lsv.uni-saarland.de
Alexandra Balahur
University of Alicante
Alicante, Spain
abalahur@dlsi.ua.es
Benjamin Roth and Dietrich Klakow
Saarland University
Saarbru?cken, Germany
benjamin.roth@lsv.uni-saarland.de
dietrich.klakow@lsv.uni-saarland.de
Andre?s Montoyo
University of Alicante
Alicante, Spain
montoyo@dlsi.ua.es
Abstract
This paper presents a survey on the role of
negation in sentiment analysis. Negation
is a very common linguistic construction
that affects polarity and, therefore, needs
to be taken into consideration in sentiment
analysis.
We will present various computational ap-
proaches modeling negation in sentiment
analysis. We will, in particular, focus
on aspects, such as level of representation
used for sentiment analysis, negation word
detection and scope of negation. We will
also discuss limits and challenges of nega-
tion modeling on that task.
1 Introduction
Sentiment analysis is the task dealing with the
automatic detection and classification of opinions
expressed in text written in natural language.
Subjectivity is defined as the linguistic expression
of somebody?s opinions, sentiments, emotions,
evaluations, beliefs and speculations (Wiebe,
1994). Subjectivity is opposed to objectivity,
which is the expression of facts. It is important to
make the distinction between subjectivity detec-
tion and sentiment analysis, as they are two sep-
arate tasks in natural language processing. Sen-
timent analysis can be dependently or indepen-
dently done from subjectivity detection, although
Pang and Lee (2004) state that subjectivity de-
tection performed prior to the sentiment analysis
leads to better results in the latter.
Although research in this area has started only re-
cently, the substantial growth in subjective infor-
mation on the world wide web in the past years
has made sentiment analysis a task on which con-
stantly growing efforts have been concentrated.
The body of research published on sentiment anal-
ysis has shown that the task is difficult, not only
due to the syntactic and semantic variability of
language, but also because it involves the extrac-
tion of indirect or implicit assessments of objects,
by means of emotions or attitudes. Being a part
of subjective language, the expression of opinions
involves the use of nuances and intricate surface
realizations. That is why the automatic study of
opinions requires fine-grained linguistic analysis
techniques and substantial efforts to extract fea-
tures for machine learning or rule-based systems,
in which subtle phenomena as negation can be ap-
propriately incorporated.
Sentiment analysis is considered as a subsequent
task to subjectivity detection, which should ideally
be performed to extract content that is not factual
in nature. Subsequently, sentiment analysis aims
at classifying the sentiment of the opinions into
polarity types (the common types are positive and
negative). This text classification task is also re-
ferred to as polarity classification.
This paper presents a survey on the role of nega-
tion in sentiment analysis. Negation is a very com-
mon linguistic construction that affects polarity
and, therefore, needs to be taken into considera-
tion in sentiment analysis. Before we describe the
computational approaches that have been devised
to account for this phenomenon in sentiment anal-
ysis, we will motivate the problem.
2 Motivation
Since subjectivity and sentiment are related to ex-
pressions of personal attitudes, the way in which
this is realized at the surface level influences the
manner in which an opinion is extracted and its
polarity is computed. As we have seen, sentiment
analysis goes a step beyond subjectivity detection,
60
including polarity classification. So, in this task,
correctly determining the valence of a text span
(whether it conveys a positive or negative opinion)
is equivalent to the success or failure of the auto-
matic processing.
It is easy to see that Sentence 1 expresses a posi-
tive opinion.
1. I like+ this new Nokia model.
The polarity is conveyed by like which is a polar
expression. Polar expressions, such as like or hor-
rible, are words containing a prior polarity. The
negation of Sentence 1, i.e. Sentence 2, using the
negation word not, expresses a negative opinion.
2. I do [not like+]? this new Nokia model.
In this example, it is straightforward to notice the
impact of negation on the polarity of the opinion
expressed. However, it is not always that easy
to spot positive and negative opinions in text. A
negation word can also be used in other expres-
sions without constituting a negation of the propo-
sition expressed as exemplified in Sentence 3.
3. Not only is this phone expensive but it is also heavy and
difficult to use.
In this context, not does not invert the polarity of
the opinion expressed which remains negative.
Moreover, the presence of an actual negation word
in a sentence does not mean that all its polar opin-
ions are inverted. In Sentence 4, for example, the
negation does not modify the second polar expres-
sion intriguing since the negation and intriguing
are in separate clauses.
4. [I do [not like+]? the design of new Nokia model] but
[it contains some intriguing+ new functions].
Therefore, when treating negation, one must be
able to correctly determine the scope that it has
(i.e. determine what part of the meaning expressed
is modified by the presence of the negation).
Finally, the surface realization of a negation is
highly variable, depending on various factors,
such as the impact the author wants to make on
the general text meaning, the context, the textual
genre etc. Most of the times, its expression is far
from being simple (as in the first two examples),
and does not only contain obvious negation words,
such as not, neither or nor. Research in the field
has shown that there are many other words that in-
vert the polarity of an opinion expressed, such as
diminishers/valence shifters (Sentence 5), connec-
tives (Sentence 6), or even modals (Sentence 7).
5. I find the functionality of the new phone less practical.
6. Perhaps it is a great phone, but I fail to see why.
7. In theory, the phone should have worked even under
water.
As can be seen from these examples, modeling
negation is a difficult yet important aspect of sen-
timent analysis.
3 The Survey
In this survey, we focus on work that has presented
novel aspects for negation modeling in sentiment
analysis and we describe them chronologically.
3.1 Negation and Bag of Words in Supervised
Machine Learning
Several research efforts in polarity classification
employ supervised machine-learning algorithms,
like Support Vector Machines, Na??ve Bayes Clas-
sifiers or Maximum Entropy Classifiers. For these
algorithms, already a low-level representation us-
ing bag of words is fairly effective (Pang et al,
2002). Using a bag-of-words representation, the
supervised classifier has to figure out by itself
which words in the dataset, or more precisely fea-
ture set, are polar and which are not. One either
considers all words occurring in a dataset or, as
in the case of Pang et al (2002), one carries out
a simple feature selection, such as removing infre-
quent words. Thus, the standard bag-of-words rep-
resentation does not contain any explicit knowl-
edge of polar expressions. As a consequence of
this simple level of representation, the reversal
of the polarity type of polar expressions as it is
caused by a negation cannot be explicitly modeled.
The usual way to incorporate negation modeling
into this representation is to add artificial words:
i.e. if a word x is preceded by a negation word,
then rather than considering this as an occurrence
of the feature x, a new feature NOT x is created.
The scope of negation cannot be properly modeled
with this representation either. Pang et al (2002),
for example, consider every word until the next
punctuation mark. Sentence 2 would, therefore,
result in the following representation:
8. I do not NOT like NOT this NOT new NOT Nokia
NOT model.
The advantage of this feature design is that a plain
occurrence and a negated occurrence of a word are
61
reflected by two separate features. The disadvan-
tage, however, is that these two contexts treat the
same word as two completely different entities.
Since the words to be considered are unrestricted,
any word ? no matter whether it is an actual po-
lar expression or not ? is subjected to this nega-
tion modification. This is not only linguistically
inaccurate but also increases the feature space with
more sparse features (since the majority of words
will only be negated once or twice in a corpus).
Considering these shortcomings, it comes to no
surprise that the impact of negation modeling on
this level of representation is limited. Pang et al
(2002) report only a negligible improvement by
adding the artificial features compared to plain bag
of words in which negation is not considered.
Despite the lack of linguistic plausibility, super-
vised polarity classifiers using bag of words (in
particular, if training and testing are done on the
same domain) offer fairly good performance. This
is, in particular, the case on coarse-grained clas-
sification, such as on document level. The suc-
cess of these methods can be explained by the
fact that larger texts contain redundant informa-
tion, e.g. it does not matter whether a classifier
cannot model a negation if the text to be classi-
fied contains twenty polar opinions and only one
or two contain a negation. Another advantage
of these machine learning approaches on coarse-
grained classification is their usage of higher order
n-grams. Imagine a labeled training set of docu-
ments contains frequent bigrams, such as not ap-
pealing or less entertaining. Then a feature set us-
ing higher order n-grams implicitly contains nega-
tion modeling. This also partially explains the ef-
fectiveness of bigrams and trigrams for this task as
stated in (Ng et al, 2006).
The dataset used for the experiments in (Pang et
al., 2002; Ng et al, 2006) has been established as
a popular benchmark dataset for sentiment analy-
sis and is publicly available1.
3.2 Incorporating Negation in Models that
Include Knowledge of Polar Expressions
- Early Works
The previous subsection suggested that appropri-
ate negation modeling for sentiment analysis re-
quires the awareness of polar expressions. One
way of obtaining such expressions is by using a
1http://www.cs.cornell.edu/people/
pabo/movie-review-data
polarity lexicon which contains a list of polar ex-
pressions and for each expression the correspond-
ing polarity type. A simple rule-based polarity
classifier derived from this knowledge typically
counts the number of positive and negative polar
expressions in a text and assigns it the polarity
type with the majority of polar expressions. The
counts of polar expressions can also be used as
features in a supervised classifier. Negation is typ-
ically incorporated in those features, e.g. by con-
sidering negated polar expressions as unnegated
polar expressions with the opposite polarity type.
3.2.1 Contextual Valence Shifters
The first computational model that accounts for
negation in a model that includes knowledge of
polar expressions is (Polanyi and Zaenen, 2004).
The different types of negations are modeled via
contextual valence shifting. The model assigns
scores to polar expressions, i.e. positive scores to
positive polar expressions and negative scores to
negative polar expressions, respectively. If a polar
expression is negated, its polarity score is simply
inverted (see Example 1).
clever (+2) ? not clever (?2) (1)
In a similar fashion, diminishers are taken into
consideration. The difference is, however, that
the score is only reduced rather than shifted to the
other polarity type (see Example 2).
efficient (+2)? rather efficient (+1) (2)
Beyond that the model also accounts for modals,
presuppositional items and even discourse-based
valence shifting. Unfortunately, this model is
not implemented and, therefore, one can only
speculate about its real effectiveness.
Kennedy and Inkpen (2005) evaluate a nega-
tion model which is fairly identical to the one pro-
posed by Polanyi and Zaenen (2004) (as far as
simple negation words and diminishers are con-
cerned) in document-level polarity classification.
A simple scope for negation is chosen. A polar
expression is thought to be negated if the negation
word immediately precedes it. In an extension of
this work (Kennedy and Inkpen, 2006) a parser is
considered for scope computation. Unfortunately,
no precise description of how the parse is used
for scope modeling is given in that work. Neither
is there a comparison of these two scope models
measuring their respective impacts.
62
Final results show that modeling negation is im-
portant and relevant, even in the case of such sim-
ple methods. The consideration of negation words
is more important than that of diminishers.
3.2.2 Features for Negation Modeling
Wilson et al (2005) carry out more advanced
negation modeling on expression-level polarity
classification. The work uses supervised machine
learning where negation modeling is mostly en-
coded as features using polar expressions. The
features for negation modeling are organized in
three groups:
? negation features
? shifter features
? polarity modification features
Negation features directly relate to negation ex-
pressions negating a polar expression. One feature
checks whether a negation expression occurs in a
fixed window of four words preceding the polar
expression. The other feature accounts for a polar
predicate having a negated subject. This frequent
long-range relationship is illustrated in Sentence 9.
9. [No politically prudent Israeli]
subject
could
support
polar pred
either of them.
All negation expressions are additionally disam-
biguated as some negation words do not function
as a negation word in certain contexts, e.g. not to
mention or not just.
Shifter features are binary features checking the
presence of different types of polarity shifters. Po-
larity shifters, such as little, are weaker than ordi-
nary negation expressions. They can be grouped
into three categories, general polarity shifters,
positive polarity shifters, and negative polarity
shifters. General polarity shifters reverse polarity
like negations. The latter two types only reverse
a particular polarity type, e.g. the positive shifter
abate only modifies negative polar expressions as
in abate the damage. Thus, the presence of a pos-
itive shifter may indicate positive polarity. The set
of words that are denoted by these three features
can be approximately equated with diminishers.
Finally, polarity modification features describe
polar expressions of a particular type modify-
ing or being modified by other polar expressions.
Though these features do not explicitly contain
negations, language constructions which are sim-
ilar to negation may be captured. In the phrase
[disappointed? hope+]?, for instance, a negative
polar expression modifies a positive polar expres-
sion which results in an overall negative phrase.
Adding these three feature groups to a feature
set comprising bag of words and features count-
ing polar expressions results in a significant im-
provement. In (Wilson et al, 2009), the experi-
ments of Wilson et al (2005) are extended by a
detailed analysis on the individual effectiveness of
the three feature groups mentioned above. The re-
sults averaged over four different supervised learn-
ing algorithms suggest that the actual negation fea-
tures are most effective whereas the binary polar-
ity shifters have the smallest impact. This is con-
sistent with Kennedy and Inkpen (2005) given the
similarity of polarity shifters and diminishers.
Considering the amount of improvement that is
achieved by negation modeling, the improvement
seems to be larger in (Wilson et al, 2005). There
might be two explanations for this. Firstly, the
negation modeling in (Wilson et al, 2005) is con-
siderably more complex and, secondly, Wilson et
al. (2005) evaluate on a more fine-grained level
(i.e. expression level) than Kennedy and Inkpen
(2005) (they evaluate on document level). As al-
ready pointed out in ?3.1, document-level polar-
ity classification contains more redundant infor-
mation than sentence-level or expression-level po-
larity classification, therefore complex negation
modeling on these levels might be more effective
since the correct contextual interpretation of an in-
dividual polar expression is far more important2.
The fine-grained opinion corpus used in (Wilson
et al, 2005; Wilson et al, 2009) and all the re-
sources necessary to replicate the features used in
these experiments are also publicly available3.
3.3 Other Approaches
The approaches presented in the previous sec-
tion (Polanyi and Zaenen, 2004; Kennedy and
Inkpen, 2005; Wilson et al, 2005) can be consid-
ered as the works pioneering negation modeling
in sentiment analysis. We now present some more
recent work on that topic. All these approaches,
however, are heavily related to these early works.
2This should also explain why most subsequent works
(see ?3.3) have been evaluated on fine-grained levels.
3The corpus is available under:
http://www.cs.pitt.edu/mpqa/
databaserelease and the resources
for the features are part of OpinionFinder:
http://www.cs.pitt.edu/mpqa/
opinionfinderrelease
63
3.3.1 Semantic Composition
In (Moilanen and Pulman, 2007), a method to
compute the polarity of headlines and complex
noun phrases using compositional semantics is
presented. The paper argues that the principles of
this linguistic modeling paradigm can be success-
fully applied to determine the subsentential polar-
ity of the sentiment expressed, demonstrating it
through its application to contexts involving senti-
ment propagation, polarity reversal (e.g. through
the use of negation following Polanyi and Zae-
nen (2004) and Kennedy and Inkpen (2005)) or
polarity conflict resolution. The goal is achieved
through the use of syntactic representations of sen-
tences, on which rules for composition are de-
fined, accounting for negation (incrementally ap-
plied to constituents depending on the scope) us-
ing negation words, shifters and negative polar ex-
pressions. The latter are subdivided into differ-
ent categories, such that special words are defined,
whose negative intensity is strong enough that they
have the power to change the polarity of the entire
text spans or constituents they are part of.
A similar approach is presented by Shaikh et al
(2007). The main difference to Moilanen and
Pulman (2007) lies in the representation format
on which the compositional model is applied.
While Moilanen and Pulman (2007) use syntac-
tic phrase structure trees, Shaikh et al (2007) con-
sider a more abstract level of representation be-
ing verb frames. The advantage of a more abstract
level of representation is that it more accurately
represents the meaning of the text it describes.
Apart from that, Shaikh et al (2007) design a
model for sentence-level classification rather than
for headlines or complex noun phrases.
The approach by Moilanen and Pulman (2007) is
not compared against another established classifi-
cation method whereas the approach by Shaikh et
al. (2007) is evaluated against a non-compositional
rule-based system which it outperforms.
3.3.2 Shallow Semantic Composition
Choi and Cardie (2008) present a more lightweight
approach using compositional semantics towards
classifying the polarity of expressions. Their
working assumption is that the polarity of a phrase
can be computed in two steps:
? the assessment of polarity of the constituents
? the subsequent application of a set of previously-
defined inference rules
An example rule, such as:
Polarity([NP1]? [IN] [NP2]?) = + (3)
may be applied to expressions, such as
[lack]?NP1 [of]IN [crime]?NP2 in rural areas.
The advantage of these rules is that they restrict
the scope of negation to specific constituents
rather than using the scope of the entire target
expression.
Such inference rules are very reminiscent of
polarity modification features (Wilson et al,
2005), as a negative polar expression is modified
by positive polar expression. The rules presented
by Choi and Cardie (2008) are, however, much
more specific, as they define syntactic contexts of
the polar expressions. Moreover, from each con-
text a direct polarity for the entire expression can
be derived. In (Wilson et al, 2005), this decision
is left to the classifier. The rules are also similar
to the syntactic rules from Moilanen and Pulman
(2007). However, they involve less linguistic
processing and are easier to comprehend4 . The
effectiveness of these rules are both evaluated in
rule-based methods and a machine learning based
method where they are anchored as constraints
in the objective function. The results of their
evaluation show that the compositional methods
outperform methods using simpler scopes for
negation, such as considering the scope of the
entire target expression. The learning method
incorporating the rules also slightly outperforms
the (plain) rule-based method.
3.3.3 Scope Modeling
In sentiment analysis, the most prominent work
examining the impact of different scope models
for negation is (Jia et al, 2009). The scope de-
tection method that is proposed considers:
? static delimiters
? dynamic delimiters
? heuristic rules focused on polar expressions
Static delimiters are unambiguous words, such as
because or unless marking the beginning of an-
other clause. Dynamic delimiters are, however,
4It is probably due to the latter, that these rules have
been successfully re-used in subsequent works, most promi-
nently Klenner et al (2009).
64
ambiguous, e.g. like and for, and require disam-
biguation rules, using contextual information such
as their pertaining part-of-speech tag. These de-
limiters suitably account for various complex sen-
tence types so that only the clause containing the
negation is considered.
The heuristic rules focus on cases in which po-
lar expressions in specific syntactic configurations
are directly preceded by negation words which re-
sults in the polar expression becoming a delimiter
itself. Unlike Choi and Cardie (2008), these rules
require a proper parse and reflect grammatical re-
lationships between different constituents.
The complexity of the scope model proposed
by Jia et al (2009) is similar to the ones of
the compositional models (Moilanen and Pulman,
2007; Shaikh et al, 2007; Choi and Cardie, 2008)
where scope modeling is exclusively incorporated
in the compositional rules.
Apart from scope modeling, Jia et al (2009) also
employ a complex negation term disambiguation
considering not only phrases in which potential
negation expressions do not have an actual negat-
ing function (as already used in (Wilson et al,
2005)), but also negative rhetorical questions and
restricted comparative sentences.
On sentence-level polarity classification, their
scope model is compared with
? a simple negation scope using a fixed window size
(similar to the negation feature in (Wilson et al, 2005))
? the text span until the first occurrence of a polar expres-
sion following the negation word
? the entire sentence
The proposed method consistently outperforms
the simpler methods proving that the incorpora-
tion of linguistic insights into negation modeling
is meaningful. Even on polarity document re-
trieval, i.e. a more coarse-grained classification
task where contextual disambiguation usually
results in a less significant improvement, the
proposed method also outperforms the other
scopes examined.
There have only been few research efforts in
sentiment analysis examining the impact of scope
modeling for negation in contrast to other research
areas, such as the biomedical domain (Huang and
Lowe, 2007; Morante et al, 2008; Morante and
Daelemans, 2009). This is presumably due to the
fact that only for the biomedical domain, publicly
available corpora containing annotation for the
scope of negation exist (Szarvas et al, 2008). The
usability of those corpora for sentiment analysis
has not been tested.
3.4 Negation within Words
So far, negation has only be considered as a phe-
nomenon that affects entire words or phrases.
The word expressing a negation and the words
or phrases being negated are disjoint. There are,
however, cases in which both negation and the
negated content which can also be opinionated
are part of the same word. In case, these words
are lexicalized, such as flaw-less, and are conse-
quently to be found a polarity lexicon, this phe-
nomenon does not need to be accounted for in sen-
timent analysis. However, since this process is (at
least theoretically) productive, fairly uncommon
words, such as not-so-nice, anti-war or offensive-
less which are not necessarily contained in lexical
resources, may emerge as a result of this process.
Therefore, a polarity classifier should also be able
to decompose words and carry out negation mod-
eling within words.
There are only few works addressing this particu-
lar aspect (Moilanen and Pulman, 2008; Ku et al,
2009) so it is not clear how much impact this type
of negation has on an overall polarity classification
and what complexity of morphological analysis is
really necessary. We argue, however, that in syn-
thetic languages where negation may regularly be
realized as an affix rather than an individual word,
such an analysis is much more important.
3.5 Negation in Various Languages
Current research in sentiment analysis mainly fo-
cuses on English texts. Since there are signifi-
cant structural differences among the different lan-
guages, some particular methods may only cap-
ture the idiosyncratic properties of the English lan-
guage. This may also affect negation modeling.
The previous section already stated that the need
for morphological analyses may differ across the
different languages.
Moreover, the complexity of scope modeling may
also be language dependent. In English, for ex-
ample, modeling the scope of a negation as a
fixed window size of words following the oc-
currence of a negation expression already yields
a reasonable performance (Kennedy and Inkpen,
2005). However, in other languages, for example
German, more complex processing is required as
the negated expression may either precede (Sen-
65
tence 10) or follow (Sentence 11) the negation ex-
pression. Syntactic properties of the negated noun
phrase (i.e. the fact whether the negated polar ex-
pression is a verb or an adjective) determine the
particular negation construction.
10. Peter mag den Kuchen nicht.
Peter likes the cake not.
?Peter does not like the cake.?
11. Der Kuchen ist nicht ko?stlich.
The cake is not delicious.
?The cake is not delicious.?
These items show that, clearly, some more ex-
tensive cross-lingual examination is required in or-
der to be able to make statements of the general
applicability of specific negation models.
3.6 Bad and Not Good are Not the Same
The standard approach of negation modeling sug-
gests to consider a negated polar expression, such
as not bad, as an unnegated polar expression with
the opposite polarity, such as good. Liu and Seneff
(2009) claim, however, that this is an oversimpli-
fication of language. Not bad and good may have
the same polarity but they differ in their respec-
tive polar strength, i.e. not bad is less positive
than good. That is why, Liu and Seneff (2009)
suggest a compositional model in which for indi-
vidual adjectives and adverbs (the latter include
negations) a prior rating score encoding their in-
tensity and polarity is estimated from pros and
cons of on-line reviews. Moreover, compositional
rules for polar phrases, such as adverb-adjective or
negation-adverb-adjective are defined exclusively
using the scores of the individual words. Thus,
adverbs function like universal quantifiers scaling
either up or down the polar strength of the specific
polar adjectives they modify. The model indepen-
dently learns what negations are, i.e. a subset of
adverbs having stronger negative scores than other
adverbs. In short, the proposed model provides
a unifying account for intensifiers (e.g. very), di-
minishers, polarity shifters and negation words. Its
advantage is that polarity is treated composition-
ally and is interpreted as a continuum rather than
a binary classification. This approach reflects its
meaning in a more suitable manner.
3.7 Using Negations in Lexicon Induction
Many classification approaches illustrated above
depend on the knowledge of which natural lan-
guage expressions are polar. The process of ac-
quiring such lexical resources is called lexicon in-
duction. The observation that negations co-occur
with polar expressions has been used for inducing
polarity lexicons on Chinese in an unsupervised
manner (Zagibalov and Carroll, 2008). One ad-
vantage of negation is that though the induction
starts with just positive polar seeds, the method
also accomplishes to extract negative polar expres-
sions since negated mentions of the positive po-
lar seeds co-occur with negative polar expressions.
Moreover, and more importantly, the distribution
of the co-occurrence between polar expressions
and negations can be exploited for the selection of
those seed lexical items. The model presented by
Zagibalov and Carroll (2008) relies on the obser-
vation that a polar expression can be negated but it
occurs more frequently without the negation. The
distributional behaviour of an expression, i.e. sig-
nificantly often co-occurring with a negation word
but significantly more often occurring without a
negation word makes up a property of a polar ex-
pression. The data used for these experiments are
publicly available5 .
3.8 Irony ? The Big Challenge
Irony is a rhetorical process of intentionally using
words or expressions for uttering meaning that is
different from the one they have when used liter-
ally (Carvalho et al, 2009). Thus, we consider
that the use of irony can reflect an implicit nega-
tion of what is conveyed through the literal use of
the words. Moreover, due to its nature irony is
mostly used to express a polar opinion.
Carvalho et al (2009) confirm the relevance of
(verbal) irony for sentiment analysis by an error
analysis of their present classifier stating that a
large proportion of misclassifications derive from
their system?s inability to account for irony.
They present predictive features for detecting
irony in positive sentences (which are actually
meant to have a negative meaning). Their find-
ings are that the use of emoticons or expressions
of gestures and the use of quotation marks within
a context in which no reported speech is included
are a good signal of irony in written text. Although
the use of these clues in the defined patterns helps
to detect some situations in which irony is present,
they do not fully represent the phenomenon.
5http://www.informatics.sussex.ac.uk/
users/tz21/coling08.zip
66
A data-driven approach for irony detection on
product-reviews is presented in (Tsur et al, 2010).
In the first stage, a considerably large list of simple
surface patterns of ironic expressions are induced
from a small set of labeled seed sentences. A pat-
tern is a generalized word sequence in which con-
tent words are replaced by a generic CW symbol.
In the second stage, the seed sentences are used to
collect more examples from the web, relying on
the assumption that sentences next to ironic ones
are also ironic. In addition to these patterns, some
punctuation-based features are derived from the
labeled sentences. The acquired patterns are used
as features along the punctuation-based features
within a k nearest neighbour classifier. On an in-
domain test set the classifier achieves a reasonable
performance. Unfortunately, these experiments
only elicit few additional insights into the general
nature of irony. As there is no cross-domain eval-
uation of the system, it is unclear in how far this
approach generalizes to other domains.
4 Limits of Negation Modeling in
Sentiment Analysis
So far, this paper has not only outlined the impor-
tance of negation modeling in sentiment analysis
but it has also shown different ways to account for
this linguistic phenomenon. In this section, we
present the limits of negation modeling in senti-
ment analysis.
Earlier in this paper, we stated that negation mod-
eling depends on the knowledge of polar expres-
sions. However, the recognition of genuine polar
expressions is still fairly brittle. Many polar ex-
pressions, such as disease are ambiguous, i.e. they
have a polar meaning in one context (Sentence 12)
but do not have one in another (Sentence 13).
12. He is a disease to every team he has gone to.
13. Early symptoms of the disease are headaches, fevers,
cold chills and body pain.
In a pilot study (Akkaya et al, 2009), it has al-
ready been shown that applying subjectivity word
sense disambiguation in addition to the feature-
based negation modeling approach of Wilson et al
(2005) results in an improvement of performance
in polarity classification.
Another problem is that some polar opinions are
not lexicalized. Sentence 14 is a negative prag-
matic opinion (Somasundaran and Wiebe, 2009)
which can only be detected with the help of exter-
nal world knowledge.
14. The next time I hear this song on the radio, I?ll throw
my radio out of the window.
Moreover, the effectiveness of specific negation
models can only be proven with the help of cor-
pora containing those constructions or the type of
language behaviour that is reflected in the mod-
els to be evaluated. This presumably explains why
rare constructions, such as negations using con-
nectives (Sentence 6 in ?2), modals (Sentence 7
in ?2) or other phenomena presented in the con-
ceptual model of Polanyi and Zaenen (2004), have
not yet been dealt with.
5 Conclusion
In this paper, we have presented a survey on
the role of negation in sentiment analysis. The
plethora of work presented on the topic proves that
this common linguistic construction is highly rel-
evant for sentiment analysis.
An effective negation model for sentiment analy-
sis usually requires the knowledge of polar expres-
sions. Negation is not only conveyed by common
negation words but also other lexical units, such as
diminishers. Negation expressions are ambiguous,
i.e. in some contexts do not function as a nega-
tion and, therefore, need to be disambiguated. A
negation does not negate every word in a sentence,
therefore, using syntactic knowledge to model the
scope of negation expressions is useful.
Despite the existence of several approaches to
negation modeling for sentiment analysis, in or-
der to make general statements about the effective-
ness of specific methods systematic comparative
analyses examining the impact of different nega-
tion models (varying in complexity) with regard to
classification type, text granularity, target domain,
language etc. still need to be carried out.
Finally, negation modeling is only one aspect that
needs to be taken into consideration in sentiment
analysis. In order to fully master this task, other
aspects, such as a more reliable identification of
genuine polar expressions in specific contexts, are
at least as important as negation modeling.
Acknowledgements
Michael Wiegand was funded by the BMBF project NL-
Search under contract number 01IS08020B. Alexandra Bal-
ahur was funded by Ministerio de Ciencia e Innovacio?n -
Spanish Government (grant no. TIN2009-13391-C04-01),
and Conselleria d?Educacio?n-Generalitat Valenciana (grant
no. PROMETEO/2009/119 and ACOMP/2010/286).
67
References
C. Akkaya, J. Wiebe, and R. Mihalcea. 2009. Subjec-
tivity Word Sense Disambiguation. In Proceedings
of EMNLP.
P. Carvalho, L. Sarmento, M. J. Silva, and
E. de Oliveira. 2009. Clues for Detecting
Irony in User-Generated Contents: Oh...!! It?s ?so
easy? ;-). In Proceedings of CIKM-Workshop TSA.
Y. Choi and C. Cardie. 2008. Learning with Compo-
sitional Semantics as Structural Inference for Sub-
sentential Sentiment Analysis. In Proceedings of
EMNLP.
Y. Huang and H. J. Lowe. 2007. A Novel Hybrid Ap-
proach to Automated Negation Detection in Clinical
Radiology Reports. JAMIA, 14.
L. Jia, C. Yu, and W. Meng. 2009. The Effect of Nega-
tion on Sentiment Analysis and Retrieval Effective-
ness. In Proceedings of CIKM.
A. Kennedy and D. Inkpen. 2005. Sentiment Classifi-
cation of Movie Reviews Using Contextual Valence
Shifters. In Proceedings of FINEXIN.
A. Kennedy and D. Inkpen. 2006. Sentiment Classifi-
cation of Movie Reviews Using Contextual Valence
Shifters. Computational Intelligence, 22.
M. Klenner, S. Petrakis, and A. Fahrni. 2009. Robust
Compositional Polarity Classification. In Proceed-
ings of RANLP.
L. Ku, T. Huang, and H. Chen. 2009. Using Morpho-
logical and Syntactic Structures for Chinese Opinion
Analysis. In Proceedings ACL/IJCNLP.
J. Liu and S. Seneff. 2009. Review Sentiment Scoring
via a Parse-and-Paraphrase Paradigm. In Proceed-
ings of EMNLP.
K. Moilanen and S. Pulman. 2007. Sentiment Con-
struction. In Proceedings of RANLP.
K. Moilanen and S. Pulman. 2008. The Good, the Bad,
and the Unknown. In Proceedings of ACL/HLT.
R. Morante and W. Daelemans. 2009. A Metalearning
Approach to Processing the Scope of Negation. In
Proceedings of CoNLL.
R. Morante, A. Liekens, and W. Daelemans. 2008.
Learning the Scope of Negation in Biomedical
Texts. In Proceedings of EMNLP.
V. Ng, S. Dasgupta, and S. M. Niaz Arifin. 2006. Ex-
amining the Role of Linguistic Knowledge Sources
in the Automatic Identification and Classification of
Reviews. In Proceedings of COLING/ACL.
B. Pang and L. Lee. 2004. A Sentimental Education:
Sentiment Analysis Using Subjectivity Summariza-
tion Based on Minimum Cuts. In Proceedings of
ACL.
B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs
up? Sentiment Classification Using Machine Learn-
ing Techniques. In Proceedings of EMNLP.
L. Polanyi and A. Zaenen. 2004. Context Valence
Shifters. In Proceedings of the AAAI Spring Sym-
posium on Exploring Attitude and Affect in Text.
M. A. M. Shaikh, H. Prendinger, and M. Ishizuka.
2007. Assessing Sentiment of Text by Semantic De-
pendency and Contextual Valence Analysis. In Pro-
ceedings of ACII.
S. Somasundaran and J. Wiebe. 2009. Recogniz-
ing Stances in Online Debates. In Proceedings of
ACL/IJCNLP.
G. Szarvas, V. Vincze, R. Farkas, and J. Csirik. 2008.
The BioScope Corpus: Annotation for Negation,
Uncertainty and Their Scope in Biomedical Texts.
In Proceedings of BioNLP.
O. Tsur, D. Davidov, and A. Rappoport. 2010.
ICWSM - A Great Catchy Name: Semi-Supervised
Recognition of Sarcastic Sentences in Online Prod-
uct Reviews. In Proceeding of ICWSM.
J. Wiebe. 1994. Tracking Point of View in Narrative.
Computational Linguistics, 20.
T. Wilson, J. Wiebe, and P. Hoffmann. 2005. Recog-
nizing Contextual Polarity in Phrase-level Sentiment
Analysis. In Proceedings of HLT/EMNLP.
T. Wilson, J. Wiebe, and P. Hoffmann. 2009. Rec-
ognizing Contextual Polarity: An Exploration for
Phrase-level Analysis. Computational Linguistics,
35:3.
T. Zagibalov and J. Carroll. 2008. Automatic Seed
Word Selection for Unsupervised Sentiment Classi-
fication of Chinese Text. In Proceedings of COL-
ING.
68
