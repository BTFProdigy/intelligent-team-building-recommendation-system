Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 795?802, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Morphology and Reranking for the Statistical Parsing of Spanish
Brooke Cowan
MIT CSAIL
brooke@csail.mit.edu
Michael Collins
MIT CSAIL
mcollins@csail.mit.edu
Abstract
We present two methods for incorporat-
ing detailed features in a Spanish parser,
building on a baseline model that is a lex-
icalized PCFG. The first method exploits
Spanish morphology, and achieves an F1
constituency score of 83.6%. This is an
improvement over 81.2% accuracy for the
baseline, which makes little or no use of
morphological information. The second
model uses a reranking approach to add
arbitrary global features of parse trees to
the morphological model. The reranking
model reaches 85.1% F1 accuracy on the
Spanish parsing task. The resulting model
for Spanish parsing combines an approach
that specifically targets morphological in-
formation with an approach that makes
use of general structural features.
1 Introduction
Initial methods for statistical parsing were mainly
developed through experimentation on English data
sets. Subsequent research has focused on apply-
ing these methods to other languages. There has
been widespread evidence that new languages ex-
hibit linguistic phenomena that pose considerable
challenges to techniques originally developed for
English; because of this, an important area of cur-
rent research concerns how to model these phenom-
ena more accurately within statistical approaches. In
this paper, we investigate this question within the
context of parsing Spanish. We describe two meth-
ods for incorporating detailed features in a Spanish
parser, building on a baseline model that is a lexical-
ized PCFG originally developed for English.
Our first model uses morphology to improve
the performance of the baseline model. English
is a morphologically-impoverished language, while
most of the world?s languages exhibit far richer mor-
phologies. Spanish is one of these languages. For
instance, the forms of Spanish nouns, determiners,
and adjectives reflect both number and gender; pro-
nouns reflect gender, number, person, and case. Fur-
thermore, morphological constraints may be mani-
fested at the syntactic level: certain constituents of a
noun phrase are constrained to agree in number and
gender, and a verb is constrained to agree in num-
ber and person with its subject. Hence, morphol-
ogy gives us important structural cues about how the
words in a Spanish sentence relate to one another.
The mechanism we employ for incorporating mor-
phology into the PCFG model (the Model 1 parser
in (Collins, 1999)) is the modification of its part-of-
speech (POS) tagset; in this paper, we explain how
this mechanism allows the parser to better capture
morphological constraints.
All of the experiments in this paper are carried
out using a freely-available Spanish treebank pro-
duced by the 3LB project (Navarro et al, 2003).
This resource contains around 3,500 hand-annotated
trees encoding ample morphological information.
We could not use all of this information and ade-
quately train the resulting parameters due to lim-
ited training data. Hence, we used development
data to test the performance of several models, each
incorporating a subset of morphological informa-
tion. The highest-accuracy model on the devel-
opment set uses the mode and number of verbs,
as well as the number of adjectives, determiners,
nouns, and pronouns. On test data, it reaches
F1 accuracy of 83.6%/83.9%/79.4% for labeled
constituents, unlabeled dependencies, and labeled
dependencies, respectively. The baseline model,
which makes almost no use of morphology, achieves
81.2%/82.5%/77.0% in these same measures.
We use the morphological model from the afore-
mentioned experiments as a base parser in a second
set of experiments. Here we investigate the efficacy
of a reranking approach for parsing Spanish by using
795
arbitrary structural features. Previous work in sta-
tistical parsing (Collins and Koo, 2005) has shown
that applying reranking techniques to the n-best out-
put of a base parser can improve parsing perfor-
mance. Applying an exponentiated gradient rerank-
ing algorithm (Bartlett et al, 2004) to the n-best out-
put of our morphologically-informed Spanish pars-
ing model gives us similar improvements. Using the
reranking model combined with the morphological
model raises performance to 85.1%/84.7%/80.2%
F1 accuracy for labeled constituents, unlabeled de-
pendencies, and labeled dependencies.
2 Related Work
The statistical parsing of English has surpassed 90%
accuracy in the precision and recall of labeled con-
stituents (e.g., (Collins, 1999; Charniak and John-
son, 2005)). A recent proliferation of treebanks in
various languages has fueled research in the pars-
ing of other languages. For instance, work has
been done in Chinese using the Penn Chinese Tree-
bank (Levy and Manning, 2003; Chiang and Bikel,
2002), in Czech using the Prague Dependency Tree-
bank (Collins et al, 1999), in French using the
French Treebank (Arun and Keller, 2005), in Ger-
man using the Negra Treebank (Dubey, 2005; Dubey
and Keller, 2003), and in Spanish using the UAM
Spanish Treebank (Moreno et al, 2000). The best-
reported F1 constituency scores from this work for
each language are 79.9% (Chinese (Chiang and
Bikel, 2002)), 81.0% (French (Arun and Keller,
2005), 76.2% (German (Dubey, 2005)), and 73.8%
(Spanish (Moreno et al, 2000)). The authors in
(Collins et al, 1999) describe an approach that gives
80% accuracy in recovering unlabeled dependencies
in Czech.1
The project that is arguably most akin to the work
presented in this paper is that on Spanish parsing
(Moreno et al, 2000). However, a direct compari-
son of scores is complicated by the fact that we have
used a different corpus as well as larger training and
test sets (2,800- vs. 1,500-sentence training sets, and
700- vs. 40-sentence test sets).
1Note that cross-linguistic comparison of results is compli-
cated: in addition to differences in corpus annotation schemes
and sizes, there may be significant differences in linguistic char-
acteristics.
Category Attributes
Adjective gender, number, participle
Determiner gender, number, person, possessor
Noun gender, number
Verb gender, number, person, mode, tense
Preposition gender, number, form
Pronoun gender, number, person, case, possessor
Table 1: A list of the morphological features from which we
created our models. For brevity, we only list attributes with at
least two values. See (Civit, 2000) for a comprehensive list of
the morphological attributes included in the Spanish treebank.
3 Models
This section details our two approaches for adding
features to a baseline parsing model. First, we de-
scribe how morphological information can be added
to a parsing model by modifying the POS tagset.
Second, we describe an approach that reranks the
n-best output of the morphologically-rich parser, us-
ing arbitrary, general features of the parse trees as
additional information.
3.1 Adding Morphological Information
The mechanism we employ for incorporating mor-
phological information is the modification of the
POS tagset of a lexicalized PCFG2 ? the Model 1
parser described in (Collins, 1999) (hereafter
Model 1). Each POS tagset can be thought of as a
particular morphological model or a subset of mor-
phological attributes. Table 1 shows the complete set
of morphological features we considered for Span-
ish. There are 22 morphological features in total in
this table; different POS sets can be created by de-
ciding whether or not to include each of these 22
features; hence, there are 222 different morpholog-
ical models we could have created. For instance,
one particular model might capture the modal infor-
mation of verbs. In this model, there would be six
POS tags for verbs (one for each of indicative, sub-
junctive, imperative, infinitive, gerund, and partici-
ple) instead of just one. A model that captured both
the number and mode of verbs would have 18 verbal
POS tags, assuming three values (singular, plural,
and neutral) for the number feature.
The Effect of the Tagset on Model 1 Modifying
the POS tagset alows Model 1 to better distinguish
2Hand-crafted head rules are used to lexicalize the trees.
796
S(corri?,v)
NP(gatos,n) VP(corri?,v)
Figure 1: An ungrammatical dependency: the plural noun gatos
is unlikely to modify the singular verb corrio?.
events that are unlikely from those that are likely, on
the basis of morphological evidence. An example
will help to illustrate this point.
Model 1 relies on statistics conditioned on lexi-
cal headwords for practically all parameters in the
model. This sensitivity to headwords is achieved by
propagating lexical heads and POS tags to the non-
terminals in the parse tree. Thus, any statistic based
on headwords may also be sensitive to the associated
POS tag. For instance, consider the subtree in Fig-
ure 1. Note that this structure is ungrammatical be-
cause the subject, gatos (cats), is plural, but the verb,
corrio? (ran), is singular. In Model 1, the probability
of generating the noun phrase (NP) with headword
gatos and headtag noun (n) is defined as follows:3
P (gatos, n, NP | corrio?, v, S, VP) =
P1(n, NP | corrio?, v, S, VP)?
P2(gatos | n, NP, corrio?, v, S, VP)
The parser smooths parameter values using backed-
off statistics, and in particular smooths statistics
based on headwords with coarser statistics based on
POS tags alone. This allows the parser to effectively
use POS tags as a way of separating different lexi-
cal items into subsets or classes depending on their
syntactic behavior. In our example, each term is es-
timated as follows:
P1(n, NP | corrio?, v, S, VP) =
?1,1P?1,1(n, NP | corrio?, v, S, VP) +
?1,2P?1,2(n, NP | v, S, VP) +
?1,3P?1,3(n, NP | S, VP)
and
P2(gatos | n, NP, corrio?, v, S, VP) =
?2,1P?2,1(gatos | n, NP, corrio?, v, S, VP) +
?2,2P?2,2(gatos | n, NP, v, S, VP) +
?2,3P?2,3(gatos | n)
3Note that the parsing model includes other features such as
distance which we omit from the parameter definition for the
sake of brevity.
Here the P?i,j terms are maximum likelihood es-
timates derived directly from counts in the train-
ing data. The ?i,j parameters are defined so that
?1,1+?1,2+?1,3 = ?2,1+?2,2+?2,3 = 1. They con-
trol the relative contribution of each level of back-off
to the final estimate.
Note that thus far our example has not included
any morphological information in the POS tags. Be-
cause of this, we will see that there is a danger of
the estimates P1 and P2 both being high, in spite
of the dependency being ungrammatical. P1 will be
high because all three estimates P?1,1, P?1,2 and P?1,3
will most likely be high. Next, consider P2. Of the
three estimates P?2,1, P?2,2, and P?2,3, only P?2,1 retains
the information that the noun is plural and the verb
is singular. Thus P2 will be sensitive to the morpho-
logical clash between gatos and corrio? only if ?2,1 is
high, reflecting a high level of confidence in the es-
timate of P?2,3. This will only happen if the context
?corrio?, v, S, VP? is seen frequently enough for ?2,1
to take a high value. This is unlikely, given that this
context is quite specific. In summary, the impover-
ished model can only capture morphological restric-
tions through lexically-specific estimates based on
extremely sparse statistics.
Now consider a model that incorporates morpho-
logical information ? in particular, number infor-
mation ? in the noun and verb POS tags. gatos will
have the POS pn, signifying a plural noun; corrio?
will have the POS sv, signifying a singular verb.
All estimates in the previous equations will reflect
these POS changes. For example, P1 will now be
estimated as follows:
P1(pn, NP | corrio?, sv, S, VP) =
?1,1P?1,1(pn, NP | corrio?, sv, S, VP) +
?1,2P?1,2(pn, NP | sv, S, VP) +
?1,3P?1,3(pn, NP | S, VP)
Note that the two estimates P?1,1 and P?1,2 include
an (unlikely) dependency between the POS tags pn
and sv. Both of these estimates will be 0, assum-
ing that a plural noun is never seen as the subject of
a singular verb. At the very least, the context ?sv,
S, VP? will be frequent enough for P?1,2 to be a re-
liable estimate. The value for ?1,2 will therefore be
high, leading to a low estimate for P1, thus correctly
assigning low probability to the ungrammatical de-
797
pendency. In summary, the morphologically-rich
model can make use of non-lexical statistics such as
P?1,2(pn, NP | sv, S, VP) which contain dependen-
cies between POS tags and which will most likely
be estimated reliably by the model.
3.2 The Reranking Model
In the reranking model, we use an n-best version of
the morphologically-rich parser to generate a num-
ber of candidate parse trees for each sentence in
training and test data. These parse trees are then
represented through a combination of the log prob-
ability under the initial model, together with a large
number of global features. A reranking model uses
the information from these features to derive a new
ranking of the n-best parses, with the hope of im-
proving upon the baseline model. Previous ap-
proaches (e.g., (Collins and Koo, 2005)) have used
a linear model to combine the log probability un-
der a base parser with arbitrary features derived from
parse trees. There are a variety of methods for train-
ing the parameters of the model. In this work, we
use the algorithm described in (Bartlett et al, 2004),
which applies the large-margin training criterion of
support vector machines (Cortes and Vapnik, 1995)
to the reranking problem.
The motivation for the reranking model is that a
wide variety of features, which can essentially be
sensitive to arbitrary context in the parse trees, can
be incorporated into the model. In our work, we in-
cluded all features described in (Collins and Koo,
2005). As far as we are aware, this is the first time
that a reranking model has been applied to parsing
a language other than English. One goal was to in-
vestigate whether the improvements seen on English
parsing can be carried across to another language.
We have found that features in (Collins and Koo,
2005), initially developed for English parsing, also
give appreciable gains in accuracy when applied to
Spanish.
4 Data
The Spanish 3LB treebank is a freely-available re-
source with about 3,500 sentence/tree pairs that we
have used to train our models. The average sen-
tence length is 28 tokens. The data is taken from
38 complete articles and short texts. Roughly 27%
Non-Terminal Significance
aq adjective
cc conjunction
COORD coordinated phrase
ESPEC determiner
GRUP base noun phrase
GV verb phrase
MORF impersonal pronoun
p pronoun
PREP base prepositional phrase
RELATIU relative pronoun phrase
s adjectival phrase
SN noun phrase
SP prepositional phrase
SADV adverbial phrase
S sentence
sps preposition
v verb
Table 2: The non-terminals and preterminals from the Spanish
3LB corpus used in this paper.
of the texts are news articles, 27% scientific articles,
14% narrative, 11% commentary, 11% sports arti-
cles, 6% essays, and 5% articles from weekly maga-
zines. The trees contain information about both con-
stituency structure and syntactic functions.
4.1 Preprocessing
It is well-known that tree representation influences
parsing performance (Johnson, 1998). Prior to train-
ing our models, we made some systematic modifica-
tions to the corpus trees in an effort to make it eas-
ier for Model 1 to represent the linguistic phenom-
ena present in the trees. For the convenience of the
reader, Table 2 gives a key to the non-terminal labels
in the 3LB treebank that are used in this section and
the remainder of the paper.
Relative and Subordinate Clauses Cases of rela-
tive and subordinate clauses appearing in the corpus
trees have the basic structure of the example in Fig-
ure 2a. Figure 2b shows the modifications we im-
pose on such structures. The modified structure has
the advantage that the SBAR selects the CP node as
its head, making the relative pronoun que the head-
word for the root of the subtree. This change allows,
for example, better modeling of verbs that select for
particular complementizers. In addition, the new
subtree rooted at the S node now looks like a top-
level sentence, making sentence types more uniform
in structure and easier to model statistically. Addi-
tionally, the new structure differentiates phrases em-
798
RELATIU?CP
a
p
quien
SP?CP
sps
PREP?CP
consideraban
v
GV
todos
SN
GRUP
p
CP
SBAR?S
S
todos
PREP
sps
a
SN
GRUP
p consideraban
v
GV
S
SP
RELATIU
p
quien
(a)
(b)
Figure 2: Figure (a) is the original structure from the 3LB tree-
bank for the phrase a quien todos consideraban or whom ev-
eryone considered. We transform structures like (a) into (b) by
inserting SBAR and CP nodes, and by marking all non-terminals
below the CP with a -CP tag.
bedded in the complementizers of SBARs from those
used in other contexts, allowing relative pronouns
like quien in Figure 2 to surface as lexical head-
words when embedded in larger phrases beneath the
CP node.4
Coordination In the treebank, coordinated con-
stituents and their coordinating conjunction are
placed as sister nodes in a flat structure. We enhance
the structure of such subtrees, as in Figure 3. Our
structure helps to rule out unlikely phrases such as
cats and dogs and; the model trained with the orig-
inal treebank structures will assign non-zero proba-
bility to ill-formed structures such as these.
5 Experiments
Our models were trained using a training set con-
sisting of 80% of the data (2,801 sentence/tree pairs,
75,372 words) available to us in the 3LB treebank.
We reserved the remaining 20% (692 sentences,
19,343 words) to use as unseen data in a test set.
We selected these subsets with two criteria in mind:
first, respecting the boundaries of the texts by plac-
ing articles in their entirety into either one subset or
the other; and second, maintaining, in each subset,
the same proportion of genres found in the original
set of trees. During development, we used a cross-
4This is achieved through our head rules.
(a)
(b)
civilesparlamentarios y
parlamentarios
COORD
y civiles
s
s?CC1
s s?CC2
s
aq
s
COORD ss
aq cc aq
aq
cc
Figure 3: In the 3LB corpus, phrases involving coordination,
are represented with a flat structure as in (a). For coordination
involving a non-terminal X (X = s in the example), we insert
new nodes X-CC1 and X-CC2 to form the structure in (b).
validation approach on the training set to test differ-
ent models. We divided the 2,800 training data trees
into 14 different development data sets, where each
of these data sets consisted of 2,600 training sen-
tences and 200 development sentences. We took the
average over the results of the 14 splits to gauge the
effectiveness of the model being tested.
To evaluate our models, we considered the recov-
ery of labeled and unlabeled dependencies as well as
labeled constituents. Unlabeled dependencies cap-
ture how the words in a sentence depend on one an-
other. Formally, they are tuples {headchild index,
modifier index}, where the indices indicate position
in the sentence. Labeled dependencies include the
labels of the modifier, headchild, and parent non-
terminals as well. The root of the tree has a special
dependency: {head index} in the unlabeled case and
{TOP, headchild index, root non-terminal} in the la-
beled case. The labeled constituents in a tree are all
of the non-terminals and, for each, the positions of
the words it spans. We use the standard definitions
of precision, recall, and F-measure.5
5When extracting dependencies, we replaced all non-
punctuation POS labels with a generic label TAG to avoid con-
flating tagging errors with dependency errors. We also included
the structural changes that we imposed during preprocessing.
Results for constituent precision and recall were computed af-
ter we restored the trees to the original treebank structure.
799
Labeled Dep Unlabeled Dep Labeled Const
<=70 words <=40 Words
Model Prec/Rec Gain Prec/Rec Gain Prec Rec Prec Rec
1 Baseline 76.0 ? 82.1 ? 81.6 80.4 82.6 81.4
2 n(P,N,V) 78.4 2.4 83.6 1.5 83.1 82.5 84.1 83.4
3 n(A,D,N,P,V) 78.2 2.2 83.5 1.4 83.3 82.4 84.2 83.3
4 n(V) 77.8 1.8 82.9 0.8 82.3 81.6 83.1 82.2
5 m(V) 78.4 2.4 83.1 1.0 82.8 82.0 83.8 82.9
6 t(V) 77.6 1.6 82.7 0.6 82.4 81.4 83.2 82.3
7 p(V) 78.1 2.1 83.3 1.2 82.9 82.0 83.8 82.8
8 g(V) 76.3 0.3 82.2 0.1 81.6 80.6 82.7 81.7
9 n(A,D,N,V,P)+m(V) 79.0 3.0 84.0 1.9 83.9 83.2 84.7 84.1
10 n(P,N,V)+m(V) 78.9 2.9 83.7/83.8 1.6/1.7 83.6 82.8 84.6 83.7
11 n(A,D,N,V,P)+m(V)+p(V) 78.7 2.7 83.6 1.5 83.6 82.9 84.4 83.8
12 n(A,D,N,V,P)+p(V) 78.4 2.4 83.5/83.6 1.4/1.5 83.3 82.6 84.2 83.5
13 n(A,D,N,V,P)+g(A,D,N,V,P) 78.1 2.1 83.2 1.1 83.1 82.5 83.9 83.4
Table 3: Results after training morphological models during development. When precision and recall differ in labeled or unlabeled
dependencies, both scores are shown. Row 1 shows results on a baseline model containing almost no morphological information.
The subsequent rows represent a subset of the models with which we experimented: n(P,N,V) uses number for pronouns, nouns,
and verbs; n(A,D,N,P,V) uses number for adjectives, determiners, nouns, pronouns, and verbs; n(V) uses number for verbs; m(V)
uses mode for verbs; t(V) uses tense for verbs; p(V) uses person for verbs; g(V) uses gender for verbs; the models in rows 9?12
are combinations of these models, and in row 13, n(A,D,N,V,P) combines with g(A,D,N,V,P), which uses gender for adjectives,
determiners, nouns, verbs, and pronouns. The results of the best-performing model are in bold.
Labeled Dep Unlabeled Dep Labeled Const
<=70 words <=40 Words
Model Prec/Rec Prec/Rec Prec Rec Prec Rec
1 Baseline 77.0 82.5 81.7 80.8 83.1 82.0
2 n(A,D,N,V,P)+m(V) 79.4 83.9 83.9 83.4 85.1 84.4
3 RERANK 80.2 84.7 85.2 85.0 86.3 85.9
Table 4: Results after running the morphological and reranking models on test data. Row 1 is our baseline model. Row 2 is the
morphological model that scored highest during development. Row 3 gives the accuracy of the reranking approach, when applied
to n-best output from the model in Row 2.
5.1 The Effects of Morphology
In our first experiments, we trained over 50 mod-
els, incorporating different morphological informa-
tion into each in the way described in Section 3.1.
Prior to running the parsers, we trained the POS tag-
ger described in (Collins, 2002). The output from
the tagger was used to assign a POS label for un-
known words. We only attempted to parse sentences
under 70 words in length.
Table 3 describes some of the models we tried
during development and gives results for each. Our
baseline model, which we used to evaluate the ef-
fects of using morphology, was Model 1 (Collins,
1999) with a simple POS tagset containing almost
no morphological information. The morphologi-
cal models we show are meant to be representative
of both the highest-scoring models and the perfor-
mance of various morphological features. For in-
stance, we found that, in general, gender had only a
slight impact on the performance of the parser. Note
that gender is not a morphological attribute of Span-
ish verbs, and that the inclusion of verbal features,
particularly number, mode, and person, generated
the strongest-performing models in our experiments.
Table 4 shows the results of running two mod-
els on the test set: the baseline model and the best-
performing morphological model from the develop-
ment stage. This model uses the number and mode
of verbs, as well as the number of adjectives, deter-
miners, nouns, and pronouns.
The results in Tables 3 and 4 show that adding
some amount of morphological information to a
parsing model is beneficial. We found, however, that
adding more information does not always lead to im-
proved performance (see, for example, rows 11 and
13 in Table 3). Presumably this is because the tagset
grows too large.
Table 5 takes a closer look at the performance
800
of the best-performing morphological model in the
recovery of particular labeled dependencies. The
breakdown shows the top 15 dependencies in the
gold-standard trees across the entire training set.
Collectively, these dependencies represent around
72% of the dependencies seen in this data.
We see an extraordinary gain in the recovery of
some of these dependencies when we add morpho-
logical information. Among these are the two in-
volving postmodifiers to verbs. When examining the
output of the morphological model, we found that
much of this gain is due to the fact that there are two
non-terminal labels used in the treebank that specify
modal information of verbs they dominate (infiniti-
vals and gerunds): with insufficient morphological
information, the baseline parser was unable to dis-
tinguish regular verb phrases from these more spe-
cific verb phrases.
Some dependencies are particularly difficult for
the parser, such as that in which SBAR modifies
a noun ({GRUP TAG SBAR R}). We found that
around 20% of cases of this type in the training set
involve structures like el proceso de negociones que
(in English the process of negotiation that). This
type of structure is inherently difficult to disam-
biguate. In Spanish, such structures may be more
common than in English, since phrases involving
nominal modifiers to nouns, like negotiation pro-
cess, are always formed as noun + de + noun.
5.2 Experiments with Reranking
In the reranking experiments, we follow the proce-
dure described in (Collins and Koo, 2005) for cre-
ation of a training set with n-best parses for each
sentence. This method involves jack-knifing the
data: the training set of 2,800 sentences was parsed
in 200-sentence chunks by an n-best morphologi-
cal parser trained on the remaining 2,600 sentences.
This ensured that each sentence in the training data
had n-best output from a baseline model that was
not trained on that sentence. We used the optimal
morphological model (n(A,D,N,V,P)+m(V)) to gen-
erate the n-best lists, and we used the feature set de-
scribed in (Collins and Koo, 2005). The test results
are given in Table 4.6
6Note that we also created development sets for develop-
ment of the reranking approach, and for cross-validation of the
single parameter C in approach of (Bartlett et al, 2004).
Dependency Count Model Prec/Rec
Determiner modifier 9680 BL 95.0/95.4
SN GRUP ESPEC L (15.5%) M 95.4/95.7
Complement of SP 9052 BL 92.4/92.9
SP PREP SN R (14.5%) M 93.2/93.9
SP modifier to noun 4500 BL 83.9/78.1
GRUP TAG SP R (7.2%) M 82.9/79.9
Subject 3106 BL 77.7/86.1
S GV SN L (5.0%) M 83.1/87.5
Sentential head 2758 BL 75.0/75.0
TOP S (4.4%) M 79.7/79.7
S modifier under SBAR 2728 BL 83.3/82.1
SBAR CP S R (4.4%) M 86.0/84.7
SP modifier to verb 2685 BL 62.4/78.8
S GV SP R (4.3%) M 72.6/82.5
SN modifier to verb 2677 BL 71.6/75.6
S GV SN R (4.3%) M 81.0/83.0
Adjective postmodifier 2522 BL 76.3/83.6
GRUP TAG s R (4.0%) M 76.4/83.5
Adjective premodifier 980 BL 79.2/80.0
GRUP TAG s L (1.6%) M 80.1/79.3
SBAR modifier to noun 928 BL 62.2/60.6
GRUP TAG SBAR R (1.4%) M 61.3/60.8
Coordination 895 BL 65.2/72.7
S-CC2 S coord L (1.4%) M 66.7/74.2
Coordination 870 BL 52.4/56.1
S-CC1 S-CC2 S L (1.4%) M 60.3/63.6
Impersonal pronoun 804 BL 93.3/96.4
S GV MORF L (1.3%) M 92.0/95.6
SN modifier to noun 736 BL 47.3/39.5
GRUP TAG SN R (1.2%) M 51.7/50.8
Table 5: Labeled dependency accuracy for the top 15 depen-
dencies (representing around 72% of all dependencies) in the
gold-standard trees across all training data. The first column
shows the type and subtype, where the subtype is specified as
the 4-tuple {parent non-terminal, head non-terminal, modifier
non-terminal, direction}; the second column shows the count
for that subtype and the percent of the total that it represents
(where the total is 62,372) . The model BL is the baseline, and
M is the morphological model n(A,D,N,V,P)+m(V).
5.3 Statistical Significance
We tested the significance of the labeled precision
and recall results in Table 4 using the sign test.
When applying the sign test, for each sentence in
the test data we calculate the sentence-level F1 con-
stituent score for the two parses being compared.
This indicates whether one model performs better
on that sentence than the other model, or whether
the two models perform equally well, information
used by the sign test. All differences were found to
be statistically significant at the level p = 0.01.7
7When comparing the baseline model to the morphological
model on the 692 test sentences, F1 scores improved on 314
sentences, and became worse on 164 sentences. When com-
paring the baseline model to the reranked model, 358/157 sen-
801
6 Conclusions and Future Work
We have developed a statistical parsing model for
Spanish that performs at 85.1% F1 constituency ac-
curacy. We find that an approach that explicitly
represents some of the particular features of Span-
ish (i.e., its morphology) does indeed help in pars-
ing. Moreover, this approach is compatible with
the reranking approach, which uses general fea-
tures that were first developed for use in an En-
glish parser. In fact, our best parsing model com-
bines both the language-specific morphological fea-
tures and the non-specific reranking features. The
morphological features are local, being restricted to
dependencies between words in the parse tree; the
reranking features are more global, relying on larger
portions of parse structures. Thus, we see our final
model as combining the strengths of two comple-
mentary approaches.
We are curious to know the extent to which a
close analysis of the dependency errors made by the
baseline parser can be corrected by the development
of features tailored to addressing these problems.
Some preliminary investigation of this suggests that
we see much higher gains when using generic fea-
tures than these more specific ones, but we leave a
thorough investigation of this to future work. An-
other avenue for future investigation is to try using a
more sophisticated baseline model such as Collins?
Model 2, which incorporates both subcategorization
and complement/adjunct information. Finally, we
would like to use the Spanish parser in an applica-
tion such as machine translation.
Acknowledgements
We would like to thank Xavier Carreras for point-
ing us to the Spanish 3LB treebank and Montserrat
Civit for providing access to the data and answering
questions about it. We also gratefully acknowledge
the support of the National Science Foundation un-
der grants 0347631 and 0434222.
tences had improved/worse parses. When comparing the mor-
phological model to the reranked model, 199/106 sentences had
improved/worse parses.
References
Abhishek Arun and Frank Keller. 2005. Lexicalization in
crosslinguistic probabilistic parsing: the case of French.
ACL 2005, Ann Arbor, MI.
Peter Bartlett, Michael Collins, Ben Taskar, and David
McAllester. 2004. Exponentiated gradient algorithms for
large-margin structured classification. Proceedings of NIPS
2004.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-fine
n-best parsing and MaxEnt discriminative reranking. ACL
2005, Ann Arbor, MI.
David Chiang and Daniel M. Bikel. 2002. Recovering latent
information in treebanks. Proceedings of COLING-2002,
pages 183?189.
Montserrat Civit Torruella. 2000. Gu??a para la anotacio?n mor-
fosinta?ctica del corpus CLiC-TALP. X-Tract Working Paper,
WP-00/06.
Michael Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. University of Pennsylvania.
Michael Collins, Jan Hajic, Lance Ranshaw, and Christoph Till-
man. 1999. A statistical parser for Czech. ACL 99.
Michael Collins. 2002. Discriminative training methods for
hidden Markov models: theory and experiments with per-
ceptron algorithms. EMNLP 2002.
Michael Collins and Terry Koo. 2005. Discriminative Rerank-
ing for Natural Language Parsing. Computational Linguis-
tics, 31(1):25?69.
C. Cortes and V. Vapnik. 1995. Support Vector Networks. Ma-
chine Learning, 20:273?297.
Amit Dubey and Frank Keller. 2003. Probabilistic parsing for
German using sister-head dependencies. ACL 2003, pp. 96?
103.
Amit Dubey. 2005. What to do when lexicalization fails: pars-
ing German with suffix analysis and smoothing. ACL 2005,
Ann Arbor, MI.
Mark Johnson. 1998. PCFG Models of Linguistic Tree Repre-
sentations. Computational Linguistics, 24(4):613?632.
Roger Levy and Christopher Manning. 2003. Is it harder to
parse Chinese, or the Chinese Treebank? ACL 2003, pp.
439?446.
Antonio Moreno, Ralph Grishman, Susana Lo?pez, Fernando
Sa?nchez, and Satoshi Sekine. 2000. A treebank of Span-
ish and its application to parsing. The Proceedings of the
Workshop on Using Evaluation within HLT Programs: Re-
sults and Trends, Athens, Greece.
Borja Navarro, Montserrat Civit, Ma. Anto`nia Mart??, Raquel
Marcos, and Bele?n Ferna?ndez. 2003. Syntactic, semantic
and pragmatic annotation in Cast3LB. Shallow Processing
of Large Corpora (SProLaC), a Workshop of Corpus Lin-
guistics, 2003, Lancaster, UK.
802
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 177?180,
Prague, June 2007. c?2007 Association for Computational Linguistics 
Moses: Open Source Toolkit for Statistical Machine Translation 
Philipp Koehn 
Hieu Hoang  
Alexandra Birch 
Chris Callison-Burch 
University of Edin-
burgh1 
Marcello Federico 
Nicola Bertoldi 
ITC-irst2 
Brooke Cowan 
Wade Shen 
Christine Moran 
MIT3 
Richard Zens 
RWTH Aachen4 
Chris Dyer 
University of Maryland5 
 
Ond?ej Bojar 
Charles University6 
Alexandra Constantin 
Williams College7 
Evan Herbst 
Cornell8 
1 pkoehn@inf.ed.ac.uk, {h.hoang, A.C.Birch-Mayne}@sms.ed.ac.uk, callison-burch@ed.ac.uk. 
2{federico, bertoldi}@itc.it. 3 brooke@csail.mit.edu, swade@ll.mit.edu, weezer@mit.edu. 4 
zens@i6.informatik.rwth-aachen.de. 5 redpony@umd.edu. 6 bojar@ufal.ms.mff.cuni.cz. 7 
07aec_2@williams.edu. 8 evh4@cornell.edu 
 
Abstract 
We describe an open-source toolkit for sta-
tistical machine translation whose novel 
contributions are (a) support for linguisti-
cally motivated factors, (b) confusion net-
work decoding, and (c) efficient data for-
mats for translation models and language 
models. In addition to the SMT decoder, 
the toolkit also includes a wide variety of 
tools for training, tuning and applying the 
system to many translation tasks.  
1 Motivation 
Phrase-based statistical machine translation 
(Koehn et al 2003) has emerged as the dominant 
paradigm in machine translation research. How-
ever, until now, most work in this field has been 
carried out on proprietary and in-house research 
systems. This lack of openness has created a high 
barrier to entry for researchers as many of the 
components required have had to be duplicated. 
This has also hindered effective comparisons of the 
different elements of the systems. 
By providing a free and complete toolkit, we 
hope that this will stimulate the development of the 
field. For this system to be adopted by the commu-
nity, it must demonstrate performance that is com-
parable to the best available systems. Moses has 
shown that it achieves results comparable to the 
most competitive and widely used statistical ma-
chine translation systems in translation quality and 
run-time (Shen et al 2006). It features all the ca-
pabilities of the closed sourced Pharaoh decoder 
(Koehn 2004). 
Apart from providing an open-source toolkit 
for SMT, a further motivation for Moses is to ex-
tend phrase-based translation with factors and con-
fusion network decoding. 
The current phrase-based approach to statisti-
cal machine translation is limited to the mapping of 
small text chunks without any explicit use of lin-
guistic information, be it morphological, syntactic, 
or semantic. These additional sources of informa-
tion have been shown to be valuable when inte-
grated into pre-processing or post-processing steps. 
Moses also integrates confusion network de-
coding, which allows the translation of ambiguous 
input. This enables, for instance, the tighter inte-
gration of speech recognition and machine transla-
tion. Instead of passing along the one-best output 
of the recognizer, a network of different word 
choices may be examined by the machine transla-
tion system. 
Efficient data structures in Moses for the 
memory-intensive translation model and language 
model allow the exploitation of much larger data 
resources with limited hardware. 
177
 2 Toolkit 
The toolkit is a complete out-of-the-box trans-
lation system for academic research. It consists of 
all the components needed to preprocess data, train 
the language models and the translation models. It 
also contains tools for tuning these models using 
minimum error rate training (Och 2003) and evalu-
ating the resulting translations using the BLEU 
score (Papineni et al 2002).  
Moses uses standard external tools for some of 
the tasks to avoid duplication, such as GIZA++ 
(Och and Ney 2003) for word alignments and 
SRILM for language modeling.  Also, since these 
tasks are often CPU intensive, the toolkit has been 
designed to work with Sun Grid Engine parallel 
environment to increase throughput.  
In order to unify the experimental stages, a 
utility has been developed to run repeatable ex-
periments. This uses the tools contained in Moses 
and requires minimal changes to set up and cus-
tomize. 
The toolkit has been hosted and developed un-
der sourceforge.net since inception. Moses has an 
active research community and has reached over 
1000 downloads as of 1st March 2007.  
The main online presence is at  
http://www.statmt.org/moses/ 
where many sources of information about the 
project can be found. Moses was the subject of this 
year?s Johns Hopkins University Workshop on 
Machine Translation (Koehn et al 2006). 
The decoder is the core component of Moses. 
To minimize the learning curve for many research-
ers, the decoder was developed as a drop-in re-
placement for Pharaoh, the popular phrase-based 
decoder. 
In order for the toolkit to be adopted by the 
community, and to make it easy for others to con-
tribute to the project, we kept to the following 
principles when developing the decoder: 
? Accessibility 
? Easy to Maintain 
? Flexibility 
? Easy for distributed team development 
? Portability 
It was developed in C++ for efficiency and fol-
lowed modular, object-oriented design. 
3 Factored Translation Model 
Non-factored SMT typically deals only with 
the surface form of words and has one phrase table, 
as shown in Figure 1. 
i am buying you a green cat
using phrase dictionary:
i
 am buying
you
a
green
cat
je
ach?te
vous
un
vert
chat
a une
je vous ach?te un chat vert
Translate:
 
In factored translation models, the surface 
forms may be augmented with different factors, 
such as POS tags or lemma. This creates a factored 
representation of each word, Figure 2.  
1 1 1 / sing /
                                 
je vous achet un chat
PRO PRO VB ART NN
je vous acheter un chat
st st st present masc masc
? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?
1 1 / 1 sing sing
i buy you a cat
PRO VB PRO ART NN
i tobuy you a cat
st st present st
? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?  
 
Mapping of source phrases to target phrases 
may be decomposed into several steps. Decompo-
sition of the decoding process into various steps 
means that different factors can be modeled sepa-
rately. Modeling factors in isolation allows for 
flexibility in their application. It can also increase 
accuracy and reduce sparsity by minimizing the 
number dependencies for each step. 
For example, we can decompose translating 
from surface forms to surface forms and lemma, as 
shown in Figure 3. 
Figure 2. Factored translation 
Figure 1. Non-factored translation 
178
  
Figure 3. Example of graph of decoding steps 
By allowing the graph to be user definable, we 
can experiment to find the optimum configuration 
for a given language pair and available data.  
The factors on the source sentence are consid-
ered fixed, therefore, there is no decoding step 
which create source factors from other source fac-
tors. However, Moses can have ambiguous input in 
the form of confusion networks. This input type 
has been used successfully for speech to text 
translation (Shen et al 2006). 
Every factor on the target language can have its 
own language model. Since many factors, like 
lemmas and POS tags, are less sparse than surface 
forms, it is possible to create a higher order lan-
guage models for these factors. This may encour-
age more syntactically correct output. In Figure 3 
we apply two language models, indicated by the 
shaded arrows, one over the words and another 
over the lemmas. Moses is also able to integrate 
factored language models, such as those described 
in (Bilmes and Kirchhoff 2003) and (Axelrod 
2006). 
4 Confusion Network Decoding 
Machine translation input currently takes the 
form of simple sequences of words. However, 
there are increasing demands to integrate machine 
translation technology into larger information 
processing systems with upstream NLP/speech 
processing tools (such as named entity recognizers, 
speech recognizers, morphological analyzers, etc.). 
These upstream processes tend to generate multiple, 
erroneous hypotheses with varying confidence. 
Current MT systems are designed to process only 
one input hypothesis, making them vulnerable to 
errors in the input.  
In experiments with confusion networks, we 
have focused so far on the speech translation case, 
where the input is generated by a speech recog-
nizer. Namely, our goal is to improve performance 
of spoken language translation by better integrating 
speech recognition and machine translation models. 
Translation from speech input is considered more 
difficult than translation from text for several rea-
sons. Spoken language has many styles and genres, 
such as, formal read speech, unplanned speeches, 
interviews, spontaneous conversations; it produces 
less controlled language, presenting more relaxed 
syntax and spontaneous speech phenomena. Fi-
nally, translation of spoken language is prone to 
speech recognition errors, which can possibly cor-
rupt the syntax and the meaning of the input. 
There is also empirical evidence that better 
translations can be obtained from transcriptions of 
the speech recognizer which resulted in lower 
scores. This suggests that improvements can be 
achieved by applying machine translation on a 
large set of transcription hypotheses generated by 
the speech recognizers and by combining scores of 
acoustic models, language models, and translation 
models. 
Recently, approaches have been proposed for 
improving translation quality through the process-
ing of multiple input hypotheses. We have imple-
mented in Moses confusion network decoding as 
discussed in (Bertoldi and Federico 2005), and de-
veloped a simpler translation model and a more 
efficient implementation of the search algorithm. 
Remarkably, the confusion network decoder re-
sulted in an extension of the standard text decoder. 
5 Efficient Data Structures for Transla-
tion Model and Language Models 
With the availability of ever-increasing 
amounts of training data, it has become a challenge 
for machine translation systems to cope with the 
resulting strain on computational resources. Instead 
of simply buying larger machines with, say, 12 GB 
of main memory, the implementation of more effi-
cient data structures in Moses makes it possible to 
exploit larger data resources with limited hardware 
infrastructure. 
A phrase translation table easily takes up giga-
bytes of disk space, but for the translation of a sin-
gle sentence only a tiny fraction of this table is 
needed. Moses implements an efficient representa-
tion of the phrase translation table. Its key proper-
ties are a prefix tree structure for source words and 
on demand loading, i.e. only the fraction of the 
phrase table that is needed to translate a sentence is 
loaded into the working memory of the decoder. 
179
 For the Chinese-English NIST  task, the mem-
ory requirement of the phrase table is reduced from 
1.7 gigabytes to less than 20 mega bytes, with no 
loss in translation quality and speed (Zens and Ney 
2007). 
The other large data resource for statistical ma-
chine translation is the language model. Almost 
unlimited text resources can be collected from the 
Internet and used as training data for language 
modeling. This results in language models that are 
too large to easily fit into memory. 
The Moses system implements a data structure 
for language models that is more efficient than the 
canonical SRILM (Stolcke 2002) implementation 
used in most systems. The language model on disk 
is also converted into this binary format, resulting 
in a minimal loading time during start-up of the 
decoder.  
An even more compact representation of the 
language model is the result of the quantization of 
the word prediction and back-off probabilities of 
the language model. Instead of representing these 
probabilities with 4 byte or 8 byte floats, they are 
sorted into bins, resulting in (typically) 256 bins 
which can be referenced with a single 1 byte index. 
This quantized language model, albeit being less 
accurate, has only minimal impact on translation 
performance (Federico and Bertoldi 2006). 
6 Conclusion and Future Work 
This paper has presented a suite of open-source 
tools which we believe will be of value to the MT 
research community. 
We have also described a new SMT decoder 
which can incorporate some linguistic features in a 
consistent and flexible framework. This new direc-
tion in research opens up many possibilities and 
issues that require further research and experimen-
tation. Initial results show the potential benefit of 
factors for statistical machine translation, (Koehn 
et al 2006) and (Koehn and Hoang 2007). 
References 
Axelrod, Amittai. "Factored Language Model for Sta-
tistical Machine Translation." MRes Thesis. 
Edinburgh University, 2006. 
Bertoldi, Nicola, and Marcello Federico. "A New De-
coder for Spoken Language Translation Based 
on Confusion Networks." Automatic Speech 
Recognition and Understanding Workshop 
(ASRU), 2005. 
Bilmes, Jeff A, and Katrin Kirchhoff. "Factored Lan-
guage Models and Generalized Parallel Back-
off." HLT/NACCL, 2003. 
Koehn, Philipp. "Pharaoh: A Beam Search Decoder for 
Phrase-Based Statistical Machine Translation 
Models." AMTA, 2004. 
Koehn, Philipp, Marcello Federico, Wade Shen, Nicola 
Bertoldi, Ondrej Bojar, Chris Callison-Burch, 
Brooke Cowan, Chris Dyer, Hieu Hoang, 
Richard Zens, Alexandra Constantin, Christine 
Corbett Moran, and Evan Herbst. "Open 
Source Toolkit for Statistical Machine Transla-
tion". Report of the 2006 Summer Workshop at 
Johns Hopkins University, 2006. 
Koehn, Philipp, and Hieu Hoang. "Factored Translation 
Models." EMNLP, 2007. 
Koehn, Philipp, Franz Josef Och, and Daniel Marcu. 
"Statistical Phrase-Based Translation." 
HLT/NAACL, 2003. 
Och, Franz Josef. "Minimum Error Rate Training for 
Statistical Machine Translation." ACL, 2003. 
Och, Franz Josef, and Hermann Ney. "A Systematic 
Comparison of Various Statistical Alignment 
Models." Computational Linguistics 29.1 
(2003): 19-51. 
Papineni, Kishore, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. "BLEU: A Method for Automatic 
Evaluation of Machine Translation." ACL, 
2002. 
Shen, Wade, Richard Zens, Nicola Bertoldi, and 
Marcello Federico. "The JHU Workshop 2006 
Iwslt System." International Workshop on Spo-
ken Language Translation, 2006. 
Stolcke, Andreas. "SRILM an Extensible Language 
Modeling Toolkit." Intl. Conf. on Spoken Lan-
guage Processing, 2002. 
Zens, Richard, and Hermann Ney. "Efficient Phrase-
Table Representation for Machine Translation 
with Applications to Online MT and Speech 
Recognition." HLT/NAACL, 2007. 
180
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 232?241,
Sydney, July 2006. c?2006 Association for Computational Linguistics
A Discriminative Model for Tree-to-Tree Translation
Brooke Cowan
MIT CSAIL
brooke@csail.mit.edu
Ivona Kuc?erova?
MIT Linguistics Department
kucerova@mit.edu
Michael Collins
MIT CSAIL
mcollins@csail.mit.edu
Abstract
This paper proposes a statistical, tree-
to-tree model for producing translations.
Two main contributions are as follows:
(1) a method for the extraction of syn-
tactic structures with alignment informa-
tion from a parallel corpus of translations,
and (2) use of a discriminative, feature-
based model for prediction of these target-
language syntactic structures?which we
call aligned extended projections, or
AEPs. An evaluation of the method on
translation from German to English shows
similar performance to the phrase-based
model of Koehn et al (2003).
1 Introduction
Phrase-based approaches (Och and Ney, 2004)
to statistical machine translation (SMT) have re-
cently achieved impressive results, leading to sig-
nificant improvements in accuracy over the origi-
nal IBM models (Brown et al, 1993). However,
phrase-based models lack a direct representation
of syntactic information in the source or target lan-
guages; this has prompted several researchers to
consider various approaches that make use of syn-
tactic information.
This paper describes a framework for tree-to-
tree based statistical translation. Our goal is to
learn a model that maps parse trees in the source
language to parse trees in the target language.
The model is learned from a corpus of transla-
tion pairs, where each sentence in the source or
target language has an associated parse tree. We
see two major benefits of tree-to-tree based trans-
lation. First, it is possible to explicitly model the
syntax of the target language, thereby improving
grammaticality. Second, we can build a detailed
model of the correspondence between the source
and target parse trees, with the aim of constructing
translations that preserve the meaning of source
language sentences.
Our translation framework involves a process
where the target-language parse tree is broken
down into a sequence of clauses, and each clause
is then translated separately. A central concept we
introduce in the translation of clauses is that of an
aligned extended projection (AEP). AEPs are de-
rived from the concept of an extended projection
in lexicalized tree adjoining grammars (LTAG)
(Frank, 2002), with the addition of alignment in-
formation that is based on work in synchronous
LTAG (Shieber and Schabes, 1990). A key con-
tribution of this paper is a method for learning
to map German clauses to AEPs using a feature-
based model with a perceptron learning algorithm.
We performed experiments on translation from
German to English on the Europarl data set. Eval-
uation in terms of both BLEU scores and human
judgments shows that our system performs sim-
ilarly to the phrase-based model of Koehn et al
(2003).
1.1 A Sketch of the Approach
This section provides an overview of the transla-
tion process. We will use the German sentence wir
wissen da? das haupthemmnis der vorhersehbare
widerstand der hersteller war as a running exam-
ple. For this example we take the desired transla-
tion to be we know that the main obstacle has been
the predictable resistance of manufacturers.
Translation of a German sentence proceeds in
the following four steps:
Step 1: The German sentence is parsed and then
broken down into separate parse structures for a
sequence of clauses. For example, the German ex-
ample above is broken into a parse structure for
the clause wir wissen followed by a parse struc-
ture for the subordinate clause da?. . .war. Each
of these clauses is then translated separately, using
steps 2?3 below.
Step 2: An aligned extended projection (AEP)
is predicted for each German clause. To illustrate
this step, consider translation of the second Ger-
man clause, which has the following parse struc-
ture:
232
s-oc kous-cp da?
np-sb 1 art das
nn haupthemmnis
np-pd 2 art der
adja vorhersehbare
nn widerstand
np-ag art der
nn hersteller
vafin-hd war
Note that we use the symbols 1 and 2 to identify
the two modifiers (arguments or adjuncts) in the
clause, in this case a subject and an object.
A major part of the AEP is a parse-tree frag-
ment, that is similar to a TAG elementary tree (see
also Figure 2):
SBAR
that S
NP VP
V
has
VP
V
been
NP
Following the work of Frank (2002), we will refer
to a structure like this as an extended projection
(EP). The EP encapsulates the core syntactic struc-
ture in the English clause. It contains the main
verb been, as well as the function words that and
has. It also contains a parse tree ?spine? which has
the main verb been as one of its leaves, and has the
clause label SBAR as its root. In addition, it spec-
ifies positions for arguments in the clause?in this
case NPs corresponding to the subject and object.
An AEP contains an EP, as well as alignment
information about where the German modifiers
should be placed in the extended projection. For
example, the AEP in this case would contain the
tree fragment shown above, together with an align-
ment specifying that the modifiers 1 and 2 from
the German parse will appear in the EP as subject
and object, respectively.
Step 3: The German modifiers are translated
and placed in the appropriate positions within the
AEP. For example, the modifiers das haupthemm-
nis and der vorhersehbare widerstand der her-
steller would be translated as the main obstacle,
and the predictable resistance of manufacturers,
respectively, and then placed into the subject and
object positions in the AEP.
Step 4: The individual clause translations are
combined to give a final translation. For example,
the translations we know and that the main obsta-
cle has been . . . would be concatenated to give we
know that the main obstacle has been . . ..
The main focus of this paper will be Step 2: the
prediction of AEPs from German clauses. AEPs
are detailed structural objects, and their relation-
ship to the source-language clause can be quite
complex. We use a discriminative feature-based
model, trained with the perceptron algorithm, to
incrementally predict the AEP in a sequence of
steps. At each step we define features that allow
the model to capture a wide variety of dependen-
cies within the AEP itself, or between the AEP and
the source-language clause.
1.2 Motivation for the Approach
Our approach to tree-to-tree translation is mo-
tivated by several observations. Breaking the
source-language tree into clauses (Step 1) consid-
erably simplifies the difficult problem of defining
an alignment between source and target trees. Our
impression is that high-quality translations can be
produced in a clause-by-clause fashion.1 The use
of a feature-based model for AEP prediction (Step
2) allows us to capture complex syntactic corre-
spondences between English and German, as well
as grammaticality constraints on the English side.
In this paper, we implement the translation of
modifiers (Step 3) with the phrase-based system
of Koehn et al (2003). The modifiers in our data
set are generally small chunks of text such as NPs,
PPs, and ADJPs, which by definition do not in-
clude clauses or verbs. In our approach, we use
the phrase-based system to generate n-best lists of
candidate translations and then rerank the trans-
lations based on grammaticality, i.e., using crite-
ria that judge how well they fit the position in the
AEP. In future work, we might use finite state ma-
chines in place of a reranking approach, or recur-
sively apply the AEP approach to the modifiers.
Stitching translated clauses back together (Step
4) is a relatively simple task: in a substantial ma-
jority of cases, the German clauses are not embed-
ded, but instead form a linear sequence that ac-
counts for the entire sentence. In these cases we
can simply concatenate the English clause trans-
lations to form the full translation. Embedded
clauses in German are slightly more complicated,
but it is not difficult to form embedded structures
in the English translations.
Section 5.2 of this paper describes the features
1Note that we do not assume that all of the translations
in the training data have been produced in a clause-by-clause
fashion. Rather, we assume that good translations for test
examples can be produced in this way.
233
we use for AEP prediction in translation from
German to English. Many of the features of the
AEP prediction model are specifically tuned to the
choice of German and English as the source and
target languages. However, it should be easy to
develop new feature sets to deal with other lan-
guages or treebanking styles. We see this as one
of the strengths of the feature-based approach.
In the work presented in this paper, we focus on
the prediction of clausal AEPs, i.e., AEPs associ-
ated with main verbs. One reason for this is that
clause structures are particularly rich and com-
plex from a syntactic perspective. This means that
there should be considerable potential in improv-
ing translation quality if we can accurately predict
these structures. It also means that clause-level
AEPs are a good test-bed for the discriminative
approach to AEP prediction; future work may con-
sider applying these methods to other structures
such as NPs, PPs, ADJPs, and so on.
2 Related Work
There has been a substantial amount of previous
work on approaches that make use of syntactic in-
formation in statistical machine translation. Wu
(1997) and Alshawi (1996) describe early work on
formalisms that make use of transductive gram-
mars; Graehl and Knight (2004) describe meth-
ods for training tree transducers. Melamed (2004)
establishes a theoretical framework for general-
ized synchronous parsing and translation. Eisner
(2003) discusses methods for learning synchro-
nized elementary tree pairs from a parallel corpus
of parsed sentences. Chiang (2005) has recently
shown significant improvements in translation ac-
curacy, using synchronous grammars. Riezler and
Maxwell (2006) describe a method for learning
a probabilistic model that maps LFG parse struc-
tures in German into LFG parse structures in En-
glish.
Yamada and Knight (2001) and Galley et al
(2004) describe methods that make use of syn-
tactic information in the target language alone;
Quirk et al (2005) describe similar methods that
make use of dependency representations. Syntac-
tic parsers in the target language have been used
as language models in translation, giving some
improvement in accuracy (Charniak et al, 2001).
The work of Gildea (2003) involves methods that
make use of syntactic information in both the
source and target languages.
Other work has attempted to incorporate syntac-
S
NP-A VP
V
know
SBAR-A
SBAR-A
IN
that
S
NP-A VP
V
has
VP
V
been
NP-A
NP
D
the
N
obstacle
Figure 1: Extended projections for the verbs know and been,
and for the noun obstacle. The EPs were taken from the parse
tree for the sentence We know that the main obstacle has been
the predictable resistance of manufacturers.
tic information through reranking approaches ap-
plied to n-best output from phrase-based systems
(Och et al, 2004). Another class of approaches
has shown improvements in translation through re-
ordering, where source language strings are parsed
and then reordered, in an attempt to recover a word
order that is closer to the target language (Collins
et al, 2005; Xia and McCord, 2004).
Our approach is closely related to previous
work on synchronous tree adjoining grammars
(Shieber and Schabes, 1990; Shieber, 2004), and
the work on TAG approaches to syntax described
by Frank (2002). A major departure from previous
work on synchronous TAGs is in our use of a dis-
criminative model that incrementally predicts the
information in the AEP. Note also that our model
may include features that take into account any
part of the German clause.
3 A Translation Architecture Based on
Aligned Extended Projections
3.1 Background: Extended Projections (EPs)
Extended projections (EPs) play a crucial role in
the lexicalized tree adjoining grammar (LTAG)
(Joshi, 1985) approach to syntax described by
Frank (2002). In this paper we focus almost ex-
clusively on extended projections associated with
main verbs; note, however, that EPs are typically
associated with all content words (nouns, adjec-
tives, etc.). As an example, a parse tree for the
sentence we know that the main obstacle has been
the predictable resistance of manufacturers would
make use of EPs for the words we, know, main, ob-
stacle, been, predictable, resistance, and manufac-
turers. Function words (in this sentence that, the,
has, and of) do not have EPs; instead, as we de-
scribe shortly, each function word is incorporated
in an EP of some content word.
Figure 1 has examples of EPs. Each one is
an LTAG elementary tree which contains a sin-
234
gle content word as one of its leaves. Substitution
nodes (such as NP-A or SBAR-A) in the elemen-
tary trees specify the positions of arguments of the
content words. Each EP may contain one or more
function words that are associated with the con-
tent word. For verbs, these function words include
items such as modal verbs and auxiliaries (e.g.,
should and has); complementizers (e.g., that);
and wh-words (e.g., which). For nouns, function
words include determiners and prepositions.
Elementary trees corresponding to EPs form the
basic units in the LTAG approach described by
Frank (2002). They are combined to form a full
parse tree for a sentence using the TAG operations
of substitution and adjunction. For example, the
EP for been in Figure 1 can be substituted into the
SBAR-A position in the EP for know; the EP for
obstacle can be substituted into the subject posi-
tion of the EP for been.
3.2 Aligned Extended Projections (AEPs)
We now build on the idea of extended projections
to give a detailed description of AEPs. Figure 2
shows examples of German clauses paired with the
AEPs found in training data.2 The German clause
is assumed to have n (where n ? 0) modifiers. For
example, the first German parse in Figure 2 has
two arguments, indexed as 1 and 2. Each of these
modifiers must either have a translation in the cor-
responding English clause, or must be deleted.
An AEP consists of the following parts:
STEM: A string specifying the stemmed form
of the main verb in the clause.
SPINE: A syntactic structure associated with
the main verb. The structure has the symbol V
as one of its leaf nodes; this is the position of
the main verb. It includes higher projections of
the verb such as VPs, Ss, and SBARs. It also in-
cludes leaf nodes NP-A in positions correspond-
ing to noun-phrase arguments (e.g., the subject
or object) of the main verb. In addition, it may
contain leaf nodes labeled with categories such
as WHNP or WHADVP where a wh-phrase may be
placed. It may include leaf nodes corresponding
to one or more complementizers (common exam-
ples being that, if, so that, and so on).
VOICE: One of two alternatives, active or
passive, specifying the voice of the main verb.
2Note that in this paper we consider translation from Ger-
man to English; in the remainder of the paper we take English
to be synonymous with the target language in translation and
German to be synonymous with the source language.
SUBJECT: This variable can be one of three
types. If there is no subject position in the SPINE
variable, then the value for SUBJECT is NULL.
Otherwise, SUBJECT can either be a string, for
example there,3 or an index of one of the n modi-
fiers in the German clause.
OBJECT: This variable is similar to SUBJECT,
and can also take three types: NULL, a specific
string, or an index of one of the n German modi-
fiers. It is always NULL if there is no object posi-
tion in the SPINE; it can never be a modifier index
that has already been assigned to SUBJECT.
WH: This variable is always NULL if there is no
wh-phrase position within the SPINE; it is always
a non-empty string (such as which, or in which) if
a wh-phrase position does exist.
MODALS: This is a string of verbs that consti-
tute the modals that appear within the clause. We
use NULL to signify an absence of modals.
INFL: The inflected form of the verb.
MOD(i): There are n modifier variables
MOD(1), MOD(2), . . ., MOD(n) that spec-
ify the positions for German arguments that have
not already been assigned to the SUBJECT or
OBJECT positions in the spine. Each variable
MOD(i) can take one of five possible values:
? null: This value is chosen if and only if
the modifier has already been assigned to the
subject or object position.
? deleted: This means that a translation of
the i?th German modifier is not present in the
English clause.
? pre-sub: The modifier appears after any
complementizers or wh-phrases, but before
the subject of the English clause.
? post-sub: The modifier appears after the
subject of the English clause, but before the
modals.
? in-modals: The modifier appears after the
first modal in the sequence of modals, but be-
fore the second modal or the main verb.
? post-verb: The modifier appears some-
where after the main verb.
3This happens in the case where there exists a subject in
the English clause which is not aligned to a modifier in the
German clause. See, for instance, the second example in Fig-
ure 2.
235
German Clause English AEP
s-oc kous-cp da?
np-sb 1 art das
nn haupthemmnis
np-pd 2 art der
adja vorhersehbare
nn widerstand
np-ag art der
nn hersteller
vafin-hd war
Paraphrase: that [np-sb the
main obstacle] [np-pd the
predictable resistance of man-
ufacturers] was
STEM: be
SPINE:
SBAR-A IN that
S NP-A
VP V
NP-A
VOICE: active
SUBJECT: 1
OBJECT: 2
WH: NULL
MODALS: has
INFL: been
MOD1: null
MOD2: null
s pp-mo 1 appr zwischen
piat beiden
nn gesetzen
vvfin-hd bestehen
adv-mo 2 also
np-sb 3 adja erhebliche
adja rechtliche
$, ,
adja praktische
kon und
adja wirtschaftliche
nn unterschiede
Paraphrase: [pp-mo between
the two pieces of legislation]
exist so [np-sb significant
legal, practical and economic
differences]
STEM: be
SPINE:
S NP-A
VP V
NP-A
VOICE: active
SUBJECT: ?there?
OBJECT: 3
WH: NULL
MODALS: NULL
INFL: are
MOD1: post-verb
MOD2: pre-sub
MOD3: null
s-rc prels-sb die
vp pp-mo 1 appr an
pdat jenem
nn tag
pp-mo 2 appr in
ne tschernobyl
vvpp-hd gezu?ndet
vafin-hd wurde
Paraphrase: which [pp-mo on
that day] [pp-mo in cher-
nobyl] released were
STEM: release
SPINE:
SBAR WHNP
SG-A VP V
VOICE: passive
SUBJECT: NULL
OBJECT: NULL
WH: which
MODALS: was
INFL: released
MOD1: post-verb
MOD2: post-verb
Figure 2: Three examples of German parse trees, together
with their aligned extended projections (AEPs) in the train-
ing data. Note that in the second example the correspondence
between the German clause and its English translation is not
entirely direct. The subject in the English is the expletive
there; the subject in the German clause becomes the object
in English. This is a typical pattern for the German verb
bestehen. The German PP zwischen ... appears at the start
of the clause in German, but is post-verbal in the English.
The modifier also?whose English translation is so?is in an
intermediate position in the German clause, but appears in the
pre-subject position in the English clause.
4 Extracting AEPs from a Corpus
A crucial step in our approach is the extraction
of training examples from a translation corpus.
Each training example consists of a German clause
paired with an English AEP (see Figure 2).
In our experiments, we used the Europarl cor-
pus (Koehn, 2005). For each sentence pair from
this data, we used a version of the German parser
described by Dubey (2005) to parse the German
component, and a version of the English parser
described by Collins (1999) to parse the English
component. To extract AEPs, we perform the fol-
lowing steps:
NP and PP Alignment To align NPs and PPs,
first all German and English nouns, personal
and possessive pronouns, numbers, and adjectives
are identified in each sentence and aligned using
GIZA++ (Och and Ney, 2003). Next, each NP in
an English tree is aligned to an NP or PP in the
corresponding German tree in a way that is consis-
tent with the word-alignment information. That is,
the words dominated by the English node must be
aligned only to words dominated by the German
node, and vice versa. Note that if there is more
than one German node that is consistent, then the
one rooted at the minimal subtree is selected.
Clause alignment, and AEP Extraction The
next step in the training process is to identify
German/English clause pairs which are transla-
tions of each other. We first break each English
or German parse tree into a set of clauses; see
Appendix A for a description of how we iden-
tify clauses. We retain only those training ex-
amples where the English and German sentences
have the same number of clauses. For these re-
tained examples, define the English sentence to
contain the clause sequence ?e1, e2, . . . , en?, and
the German sentence to contain the clause se-
quence ?g1, g2, . . . , gn?. The clauses are ordered
according to the position of their main verbs in
the original sentence. We create n candidate pairs
?(e1, g1), (e2, g2), . . . , (en, gn)? (i.e., force a one-
to-one correspondence between the two clause se-
quences). We then discard any clause pairs (e, g)
which are inconsistent with the NP/PP alignments
for that sentence.4
4A clause pair is inconsistent with the NP/PP alignments
if it contains an NP/PP on either the German or English side
which is aligned to another NP/PP which is not within the
clause pair.
236
Note that this method is deliberately conserva-
tive (i.e., high precision, but lower recall), in that it
discards sentence pairs where the English/German
sentences have different numbers of clauses. In
practice, we have found that the method yields a
large number of training examples, and that these
training examples are of relatively high quality.
Future work may consider improved methods for
identifying clause pairs, for example methods that
make use of labeled training examples.
An AEP can then be extracted from each
clause pair. The EP for the English clause is
first extracted, giving values for all variables ex-
cept for SUBJECT, OBJECT, and MOD(1), . . . ,
MOD(n). The values for the SUBJECT, OBJECT,
and MOD(i) variables are derived from the align-
ments between NPs/PPs, and an alignment of
other clauses (ADVPs, ADJPs, etc.) derived from
GIZA++ alignments. If the English clause has a
subject or object which is not aligned to a German
modifier, then the value for SUBJECT or OBJECT
is taken to be the full English string.
5 The Model
5.1 Beam search and the perceptron
In this section we describe linear history-based
models with beam search, and the perceptron al-
gorithm for learning in these models. These meth-
ods will form the basis for our model that maps
German clauses to AEPs.
We have a training set of n examples, (xi, yi)
for i = 1 . . . n, where each xi is a German parse
tree, and each yi is an AEP. We follow previous
work on history-based models, by representing
each yi as a series of N decisions ?d1, d2, . . . dN ?.
In our approach, N will be a fixed number for any
input x: we take the N decisions to correspond to
the sequence of variables STEM, SPINE, . . .,
MOD(1), MOD(2), . . ., MOD(n) described
in section 3. Each di is a member of a set Di
which specifies the set of allowable decisions at
the i?th point (for example, D2 would be the set
of all possible values for SPINE). We assume a
function ADVANCE(x, ?d1, d2, . . . , di?1?) which
maps an input x together with a prefix of decisions
d1 . . . di?1 to a subset ofDi. ADVANCE is a func-
tion that specifies which decisions are allowable
for a past history ?d1, . . . , di?1? and an input x. In
our case the ADVANCE function implements hard
constraints on AEPs (for example, the constraint
that the SUBJECT variable must be NULL if no
subject position exists in the SPINE). For any in-
put x, a well-formed decision sequence for x is a
sequence ?d1, . . . , dN ? such that for i = 1 . . . n,
di ? ADVANCE(x, ?d1, . . . , di?1?). We define
GEN(x) to be the set of all decision sequences (or
AEPs) which are well-formed for x.
The model that we will use is a
discriminatively-trained, feature-based model. A
significant advantage to feature-based mod-
els is their flexibility: it is very easy to
sensitize the model to dependencies in the
data by encoding new features. To define a
feature-based model, we assume a function
??(x, ?d1, . . . , di?1?, di) ? Rd which maps a deci-
sion di in context (x, ?d1, . . . , di?1?) to a feature
vector. We also assume a vector ?? ? Rd of param-
eter values. We define the score for any partial or
complete decision sequence y = ?d1, d2, . . . , dm?
paired with x as:
SCORE(x, y) = ?(x, y) ? ?? (1)
where ?(x, y) = ?mi=1 ??(x, ?d1, . . . , di?1?, di).
In particular, given the definitions above, the out-
put structure F (x) for an input x is the highest?
scoring well?formed structure for x:
F (x) = arg max
y?GEN(x)
SCORE(x, y) (2)
To decode with the model we use a beam-search
method. The method incrementally builds an AEP
in the decision order d1, d2, . . . , dN . At each
point, a beam contains the top M highest?scoring
partial paths for the first m decisions, where M
is taken to be a fixed number. The score for any
partial path is defined in Eq. 1. The ADVANCE
function is used to specify the set of possible deci-
sions that can extend any given path in the beam.
To train the model, we use the averaged per-
ceptron algorithm described by Collins (2002).
This combination of the perceptron algorithm with
beam-search is similar to that described by Collins
and Roark (2004).5 The perceptron algorithm is a
convenient choice because it converges quickly ?
usually taking only a few iterations over the train-
ing set (Collins, 2002; Collins and Roark, 2004).
5.2 The Features of the Model
The model?s features allow it to capture depen-
dencies between the AEP and the German clause,
as well as dependencies between different parts
of the AEP itself. The features included in ??
5Future work may consider alternative algorithms, such
as those described by Daume? and Marcu (2005).
237
1 main verb
2 any verb in the clause
3 all verbs, in sequence
4 spine
5 tree
6 preterminal label of left-most child of subject
7 terminal label of left-most child of subject
8 suffix of terminal label of right-most child of subject
9 preterminal label of left-most child of object
10 terminal label of left-most child of object
11 suffix of terminal label of right-most child of object
12 preterminal label of the negation word nicht (not)
13 is either of the strings es gibt (there is/are)
or es gab (there was/were) present?
14 complementizers and wh-words
15 labels of all wh-nonterminals
16 terminal labels of all wh-words
17 preterminal label of a verb in first position
18 terminal label of a verb in first position
19 terminal labels of all words in any relative pronoun
under a PP
20 are all of the verbs at the end?
21 nonterminal label of the root of the tree
22 terminal labels of all words constituting the subject
23 terminal labels of all words constituting the object
24 the leaves dominated by each node in the tree
25 each node in the context of a CFG rule
26 each node in the context of the RHS of a CFG rule
27 each node with its left and right sibling
28 the number of leaves dominated by each node
in the tree
Table 1: Functions of the German clause used for making
features in the AEP prediction model.
can consist of any function of the decision history
?d1, . . . , di?1?, the current decision di, or the Ger-
man clause. In defining features over AEP/clause
pairs, we make use of some basic functions which
look at the German clause and the AEP (see Ta-
bles 1 and 2). We use various combinations of
these basic functions in the prediction of each de-
cision di, as described below.
STEM: Features for the prediction of STEM
conjoin the value of this variable with each of the
functions in lines 1?13 of Table 1. For example,
one feature is the value of STEM conjoined with
the main verb of the German clause. In addition,
?? includes features sensitive to the rank of a can-
didate stem in an externally-compiled lexicon.6
SPINE: Spine prediction features make use of
the values of the variables SPINE and STEM from
the AEP, as well as functions of the spine in lines
1?7 of Table 2, conjoined in various ways with
the functions in lines 4, 12, and 14?21 of Table 1.
Note that the functions in Table 2 allow us to look
6The lexicon is derived from GIZA++ and provides, for a
large number of German main verbs, a ranked list of possible
English translations.
1 does the SPINE have a subject?
2 does the SPINE have an object?
3 does the SPINE have any wh-words?
4 the labels of any complementizer nonterminals
in the SPINE
5 the labels of any wh-nonterminals in the SPINE
6 the nonterminal labels SQ or SBARQ in the SPINE
7 the nonterminal label of the root of the SPINE
8 the grammatical category of the finite verbal form
INFL (i.e., infinitive, 1st-, 2nd-, or 3rd-person pres,
pres participle, sing past, plur past, past participle)
Table 2: Functions of the English AEP used for making fea-
tures in the AEP prediction model.
at substructure in the spine. For instance, one of
the features for SPINE is the label SBARQ or SQ,
if it exists in the candidate spine, conjoined with
a verbal preterminal label if there is a verb in the
first position of the German clause. This feature
captures the fact that German yes/no questions be-
gin with a verb in the first position.
VOICE: Voice features in general combine val-
ues of VOICE, SPINE, and STEM, with the func-
tions in lines 1?5, 22, and 23 of Table 1.
SUBJECT: Features used for subject prediction
make use of the AEP variables VOICE and STEM.
In addition, if the value of SUBJECT is an index
i (see section 3), then ?? looks at the nontermi-
nal label of the German node indexed by i as well
as the surrounding context in the German clausal
tree. Otherwise, ?? looks at the value of SUBJECT.
These basic features are combined with the func-
tions in lines 1, 3, and 24?27 of Table 1.
OBJECT: We make similar features to those for
the prediction of SUBJECT. In addition, ?? can
look at the value predicted for SUBJECT.
WH: Features for WH look at the values of WH
and SPINE, conjoined with the functions in lines
1, 15, and 19 of Table 1.
MODALS: For the prediction of MODALS, ??
looks at MODALS, SPINE, and STEM, conjoined
with the functions in lines 2?5 and 12 of Table 1.
INFL: The features for INFL include the values
of INFL, MODALS, and SUBJECT, and VOICE,
and the function in line 8 of Table 2.
MOD(i): For the MOD(i) variables, ?? looks
at the value of MODALS, SPINE and the current
MOD(i), as well as the nonterminal label of the
root node of the German modifier being placed,
and the functions in lines 24 and 28 of Table 1.
238
6 Deriving Full Translations
As we described in section 1.1, the translation of a
full German sentence proceeds in a series of steps:
a German parse tree is broken into a sequence of
clauses; each clause is individually translated; and
finally, the clause-level translations are combined
to form the translation for a full sentence. The first
and last steps are relatively straightforward. We
now show how the second step is achieved?i.e.,
how AEPs can be used to derive English clause
translations from German clauses.
We will again use the following translation
pair as an example: da? das haupthemmnis der
vorhersehbare widerstand der hersteller war./that
the main obstacle has been the predictable resis-
tance of manufacturers.
First, an AEP like the one at the top of Fig-
ure 2 is predicted. Then, for each German mod-
ifier which does not have the value deleted, an
English translation is predicted. In the example,
the modifiers das haupthemmnis and der vorherse-
hbare widerstand der hersteller would be trans-
lated to the main obstacle, and the predictable re-
sistance of manufacturers, respectively.
A number of methods could be used for trans-
lation of the modifiers. In this paper, we use the
phrase-based system of Koehn et al (2003) to
generate n-best translations for each of the mod-
ifiers, and we then use a discriminative rerank-
ing algorithm (Bartlett et al, 2004) to choose be-
tween these modifiers. The features in the rerank-
ing model can be sensitive to various properties of
the candidate English translation, for example the
words, the part-of-speech sequence or the parse
tree for the string. The reranker can also take into
account the original German string. Finally, the
features can be sensitive to properties of the AEP,
such as the main verb or the position in which the
modifier appears (e.g., subject, object, pre-sub,
post-verb, etc.) in the English clause. See
Appendix B for a full description of the features
used in the modifier translation model. Note that
the reranking stage allows us to filter translation
candidates which do not fit syntactically with the
position in the English tree. For example, we can
parse the members of the n-best list, and then learn
a feature which strongly disprefers prepositional
phrases if the modifier appears in subject position.
Finally, the full string is predicted. In our
example, the AEP variables SPINE, MODALS,
and INFL in Figure 2 give the ordering <that
SUBJECT has been OBJECT>. The AEP
and modifier translations would be combined
to give the final English string. In gen-
eral, any modifiers assigned to pre-sub,
post-sub, in-modals or post-verb are
placed in the corresponding position within the
spine. For example, the second AEP in Fig-
ure 2 has a spine with ordering <SUBJECT
are OBJECT>; modifiers 1 and 2 would be
placed in positions pre-sub and post-verb,
giving the ordering <MOD2 SUBJECT are
OBJECT MOD1>. Note that modifiers assigned
post-verb are placed after the object. If mul-
tiple modifiers appear in the same position (e.g.,
post-verb), then they are placed in the order
seen in the original German clause.
7 Experiments
We applied the approach to translation from Ger-
man to English, using the Europarl corpus (Koehn,
2005) for our training data. This corpus contains
over 750,000 training sentences; we extracted over
441,000 training examples for the AEP model
from this corpus, using the method described in
section 4. We reserved 35,000 of these training
examples as development data for the model. We
used a set of features derived from the those de-
scribed in section 5.2. This set was optimized us-
ing the development data through experimentation
with several different feature subsets.
Modifiers within German clauses were trans-
lated using the phrase-based model of Koehn et
al. (2003). We first generated n-best lists for each
modifier. We then built a reranking model?see
section 6?to choose between the elements in the
n-best lists. The reranker was trained using around
800 labeled examples from a development set.
The test data for the experiments consisted of
2,000 sentences, and was the same test set as that
used by Collins et al (2005). We use the model
of Koehn et al (2003) as a baseline for our ex-
periments. The AEP-driven model was used to
translate all test set sentences where all clauses
within the German parse tree contained at least
one verb and there was no embedding of clauses?
there were 1,335 sentences which met these crite-
ria. The remaining 665 sentences were translated
with the baseline system. This set of 2,000 trans-
lations had a BLEU score of 23.96. The baseline
system alone achieved a BLEU score of 25.26 on
the same set of 2,000 test sentences. We also ob-
tained judgments from two human annotators on
239
100 randomly-drawn sentences on which the base-
line and AEP-based outputs differed. For each ex-
ample the annotator viewed the reference transla-
tion, together with the two systems? translations
presented in a random order. Annotator 1 judged
62 translations to be equal in quality, 16 transla-
tions to be better under the AEP system, and 22
to be better for the baseline system. Annotator 2
judged 37 translations to be equal in quality, 32 to
be better under the baseline, and 31 to be better
under the AEP-based system.
8 Conclusions and Future Work
We have presented an approach to tree-to-
tree based translation which models a new
representation?aligned extended projections?
within a discriminative, feature-based framework.
Our model makes use of an explicit representation
of syntax in the target language, together with con-
straints on the alignments between source and tar-
get parse trees.
The current system presents many opportuni-
ties for future work. For example, improve-
ment in accuracy may come from a tighter in-
tegration of modifier translation into the over-
all translation process. The current method?
using an n-best reranking model to select the best
candidate?chooses each modifier independently
and then places it into the translation. We in-
tend to explore an alternative method that com-
bines finite-state machines representing the n-best
output from the phrase-based system with finite-
state machines representing the complementiz-
ers, verbs, modals, and other substrings of the
translation derived from the AEP. Selecting mod-
ifiers using this representation would correspond
to searching the finite-state network for the most
likely path. A finite-state representation has many
advantages, including the ability to easily incorpo-
rate an n-gram language model.
Future work may also consider expanded defi-
nitions of AEPs. For example, we might consider
AEPs that include larger chunks of phrase struc-
ture, or we might consider AEPs that contain more
detailed information about the relative ordering of
modifiers. There is certainly room for improve-
ment in the accuracy with which AEPs are pre-
dicted in our data; the feature-driven approach al-
lows a wide range of features to be tested. For ex-
ample, it would be relatively easy to incorporate a
syntactic language model (i.e., a prior distribution
over AEP structures) induced from a large amount
of English monolingual data.
Appendix A: Identification of Clauses
In the English parse trees, we identify clauses as
follows. Any non-terminal labeled by the parser
of (Collins, 1999) as SBAR or SBAR-A is labeled
as a clause root. Any node labeled by the parser as
S or S-A is also labeled as the root of a clause, un-
less it is directly dominated by a non-terminal la-
beled SBAR or SBAR-A. Any node labeled SG or
SG-A by the parser is labeled as a clause root, un-
less (1) the node is directly dominated by SBAR or
SBAR-A; or (2) the node is directly dominated by
a VP, and the node is directly preceded by a verb
(POS tag beginning with V) or modal (POS tag be-
ginning with M). Any node labeled VP is marked
as a clause root if (1) the node is not directly dom-
inated by a VP, S, S-A, SBAR, SBAR-A, SG, or
SG-A; or (2) the node is directly preceded by a
coordinating conjunction (i.e., a POS tag labeled
as CC).
In German parse trees, we identify any nodes
labeled as S or CS as clause roots. In addition,
we mark any node labeled as VP as a clause root,
provided that (1) it is preceded by a coordinating
conjunction, i.e., a POS tag labeled as KON; or (2)
it has one of the functional tags -mo, -re or -sb.
Appendix B: Reranking Modifier
Translations
The n-best reranking model for the translation of
modifiers considers a list of candidate translations.
We hand-labeled 800 examples, marking the ele-
ment in each list that would lead to the best trans-
lation. The features of the n-best reranking algo-
rithm are combinations of the basic features in Ta-
bles 3 and 4.
Each list contained the n-best translations pro-
duced by the phrase-based system of Koehn et al
(2003). The lists also contained a supplementary
candidate ?DELETED?, signifying that the mod-
ifier should be deleted from the English transla-
tion. In addition, each candidate derived from the
phrase-based system contributed one new candi-
date to the list signifying that the first word of
the candidate should be deleted. These additional
candidates were motivated by our observation that
the optimal candidate in the n-best list produced
by the phrase-based system often included an un-
wanted preposition at the beginning of the string.
240
1 candidate string
2 should the first word of the candidate be deleted?
3 POS tag of first word of candidate
4 POS tag of last word of candidate
5 top nonterminal of parse of candidate
6 modifier deleted from English translation?
7 first candidate on n-best list
8 first word of candidate
9 last word of candidate
10 rank of candidate in n-best list
11 is there punctuation at the beginning, middle,
or end of the string?
12 if the first word of the candidate should be deleted,
what is the string that is deleted?
13 if the first word of the candidate should be deleted,
what is the POS tag of the word that is deleted?
Table 3: Functions of the candidate modifier translations used
for making features in the n-best reranking model.
1 the position of the modifier (0?4) in AEP
2 main verb
3 voice
4 subject prediction
5 German input string
Table 4: Functions of the German input string and predicted
AEP output used for making features in the n-best reranking
model.
Acknowledgements
We would like to thank Luke Zettlemoyer, Regina Barzilay,
Ed Filisko, and Ben Snyder for their valuable comments and
help during the writing of this paper. Thanks also to Jason
Rennie and John Barnett for providing human judgments of
the translation output. This work was funded by NSF grants
IIS-0347631, IIS-0415030, and DMS-0434222, as well as a
grant from NTT, Agmt. Dtd. 6/21/1998.
References
H. Alshawi. 1996. Head automata and bilingual tiling: trans-
lation with minimal representations. ACL 96.
P. Bartlett, M. Collins, B. Taskar, and D. McAllester. 2004.
Exponentiated gradient algorithms for large-margin struc-
tured classification. Proceedings of NIPS 2004.
P. Brown, S. Della Pietra, V. Della Pietra, and R. Mercer.
1993. The mathematics of statistical machine translation.
Computational Linguistics, 22(1):39?69.
E. Charniak, K. Knight, and K. Yamada. 2001. Syntax-based
language models for statistical machine translation. ACL
01.
D. Chiang. 2005. A hierarchical phrase-based model for
statistical machine translation. ACL 05.
M. Collins. 1999. Head-Driven Statistical Models for Natu-
ral Language Parsing. University of Pennsylvania.
M. Collins. 2002. Discriminative training methods for hid-
den markov models: theory and experiments with percep-
tron algorithms. EMNLP 02.
M. Collins and B. Roark. 2004. Incremental parsing with the
perceptron algorithm. ACL 04.
M. Collins, P. Koehn, and I. Kuc?erova?. 2005. Clause restruc-
turing for statistical machine translation. ACL 05.
H. Daume? III and D. Marcu. 2005. Learning as search op-
timization: approximate large margin methods for struc-
tured prediction. ICML 05.
A. Dubey. 2005. What to do when lexicalization fails: pars-
ing German with suffix analysis and smoothing. ACL 05.
J. Eisner. 2003. Learning non-isomorphic tree mappings for
machine translation. ACL 03, Companion Volume.
R. Frank. 2002. Phrase Structure Composition and Syntactic
Dependencies. Cambridge, MA: MIT Press.
M. Galley, M. Hopkins, K. Knight, and D. Marcu. 2004.
What?s in a translation rule? HLT-NAACL 04.
D. Gildea. 2003. Loosely tree-based alignment for machine
translation. ACL 03.
J. Graehl and K. Knight. 2004. Training tree transducers.
NAACL-HLT 04.
A. Joshi. 1985. How much context-sensitivity is necessary
for characterizing structural descriptions ? tree-adjoining
grammar. Cambridge University Press.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical phrase
based translation. HLT-NAACL 03.
P. Koehn. 2005. Europarl: A parallel corpus for statistical
machine translation. MT Summit 05.
I. D. Melamed 2004. Statistical machine translation by pars-
ing. ACL 04.
F. J. Och, D. Gildea, S. Khudanpur, A. Sarkar, K. Yamada,
A. Fraser, S. Kumar, L. Shen, D. Smith, K. Eng, V. Jain,
Z. Jin, D. Radev. 2004. A smorgasbord of features for
statistical machine translation. HLT/NAACL 04
F. J. Och and H. Ney. 2004. The alignment template ap-
proach to statistical machine translation. Computational
Linguistics, 30(4):417?449.
F. J. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational Lin-
guistics, 29(1):19?51.
C. Quirk, A. Menezes, and C. Cherry. 2005. Depen-
dency tree translation: syntactically informed phrasal
SMT. EACL 05.
S. Riezler and J. Maxwell. 2006. Grammatical machine
translation. In NLT-NAACL 06.
S. Shieber. 2004. Synchronous grammars as tree transduc-
ers. In Proceedings of the Seventh International Workshop
on Tree Adjoining Grammar and Related Formalisms.
S. Shieber and Y. Schabes. 1990. Synchronous tree-
adjoining grammars. In Proceedings of the 13th Interna-
tional Conference on Computational Linguistics.
D. Wu. 1997. Stochastic inversion transduction grammars
and bilingual parsing of parallel corpora. Computational
Linguistics, 23(3):377?403.
F. Xia and M. McCord. 2004. Improving a statistical MT
system with automatically learned rewrite patterns. COL-
ING 04.
K. Yamada and K. Knight. 2001. A syntax-based statistical
translation model. ACL 01.
241
