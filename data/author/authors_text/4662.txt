Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 529?536
Manchester, August 2008
Linguistically-based sub-sentential alignment for terminology extraction
from a bilingual automotive corpus
Lieve Macken and Els Lefever and Veronique Hoste
Language and Translation Technology Team
Ghent University College
Belgium
{Lieve.Macken, Els.Lefever, Veronique.Hoste}@hogent.be
Abstract
We present a sub-sentential alignment
system that links linguistically motivated
phrases in parallel texts based on lexical
correspondences and syntactic similarity.
We compare the performance of our sub-
sentential alignment system with different
symmetrization heuristics that combine the
GIZA++ alignments of both translation di-
rections. We demonstrate that the aligned
linguistically motivated phrases are a use-
ful means to extract bilingual terminology
and more specifically complex multiword
terms.
1 Introduction
This research has been carried out in the frame-
work of a customer project for PSA Peugeot
Citro?en. The final goal of the project is a re-
duction and terminological unification process of
PSA?s database, which contains all text strings that
are used for compiling user manuals. French being
the source language, all French entries have been
translated to some extent into the twenty different
languages that are part of the customer?s portfolio.
Two sub-projects have been defined:
1. automatic terminology extraction for all lan-
guages taking French as the pivot language
2. improved consistency of the database entries,
e.g. through the automatic replacement of
synonyms by the preferred term (decided in
(1))
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
This paper presents a novel terminology extrac-
tion method applied to the French-English part of
the database.
There is a long tradition of research into
bilingual terminology extraction (Kupiec, 1993),
(Gaussier, 1998). In most systems, candidate terms
are first identified in the source language based on
predefined PoS patterns ? for French, N N, N Prep
N, and N Adj are typical patterns. In a second step,
the translation candidates are extracted from the
bilingual corpus based on word alignments. In re-
cent work, Itagaki et al (2007) use the phrase table
derived from the GIZA++ alignments to identify
the translations.
We use a different and more flexible approach.
We developed a sub-sentential alignment system
that links linguistically motivated phrases in paral-
lel texts based on lexical correspondences and syn-
tactic similarity. Rather than predefining terms as
sequences of PoS patterns, we first generate candi-
date terms starting from the aligned phrases. In a
second step, we use a general purpose corpus and
the n-gram frequency of the automotive corpus to
determine the specificity of the terms.
The remainder of this paper is organized as fol-
lows: Section 2 describes the corpus. In Section 3,
we present our linguistically-based sub-sentential
alignment system and in Section 4 we describe
how we use the aligned phrases for terminology
extraction.
2 Automotive corpus
For developing our terminology extraction mod-
ule, we have used the French-English sentence-
aligned database that contains 363,651 entries.
These entries can be full sentences, parts of sen-
tences, as well as isolated words and are aligned
across languages by means of a unique ID. The
529
PoS tagging Lemmatisation PoS after Lemmas
error rate error rate update after update
French 4.50 % 2.29 % 1.92 % 1.22 %
English 5.16 % 3.13 % 2.66 % 3.03 %
Table 1: Part-of-Speech tagging and lemmatisation
error rate on the test sentences
average sentence length of a database entry is 9
words.
2.1 Linguistic annotation
In order to ensure consistent processing of the lan-
guages in the corpus (e.g. Italian, Spanish, Ger-
man), we have used the freely availabe TreeTag-
ger tool (Schmid, 1994) for performing tokeni-
sation, part-of-speech tagging and lemmatisation
of the corpus. In order to evaluate the domain-
adaptability of the tagger, we have manually val-
idated the quality of the TreeTagger output for a
training set of 12,200 tokens (about 1,200 sen-
tences). We have used this validated set to derive
a list of hard coded PoS tags (e.g. the French word
vis can be a noun or verb, but is always a noun
in our corpus) as well as post-processing rules
for remediating erroneous PoS tags. We addition-
ally annotated 350 test sentences (about 3,500 to-
kens). Table 1 shows the error rate figures for PoS-
tagging and lemmatisation before and after updat-
ing the default output of the TreeTagger tool.
We further enriched the corpora with chunk in-
formation. During text chunking, syntactically re-
lated words are combined into non-overlapping
chunks based on PoS information. We devel-
oped rule-based chunkers for English and French.
The rule-based chunkers contain distituency rules,
i.e. the rules add a chunk boundary when two part-
of-speech codes cannot occur in the same con-
stituent. The following example shows a French-
English sentence pair divided in non-overlapping
chunks:
Fr: valable | uniquement | pour la ceinture | de
s?ecurit?e avant lat?erale | du c?ot?e passager
En: applies | only | to the outer seat belt | on the
passenger side
We manually indicated chunk boundaries in the
350-sentences test corpus and evaluated the rule-
based chunkers by running the CoNLL-evalscript
(Tjong Kim Sang and Buchholz, 2000). We ob-
tained precision scores of 89% and 87% and recall
scores of 91% and 91% for French and English re-
spectively.
# Words # Sentence pairs
Short (< 8 words) 4,496 404
Medium (8-19 words) 4,493 212
Long (> 19 words) 4,498 97
Total 13,487 713
Development corpus 4,423 231
Table 2: Number of words and sentence pairs in
the three test corpora and the development corpus
2.2 Test corpora
As we expect that sentence length has an impact
on the alignment performance, we created three
test corpora with varying sentence length. We dis-
tinguished short sentences (2-7 words), medium-
length sentences (8-19 words) and long sentences
(> 19 words). Each test corpus contains approxi-
mately 4,500 words.
We also compiled a development corpus con-
taining sentences of varying sentence length to de-
bug the system and to determine the value of the
thresholds used in the system. The formal charac-
teristics of the test corpora and the training corpus
are given in Table 2.
3 Sub-sentential alignment
Sub-sentential alignments ? and the underlying
word alignments ? are used in the context of
Machine Translation to create phrase tables for
phrase-based statistical machine translation sys-
tems (Koehn et al, 2007). A stand-alone sub-
sentential alignment module however, is also use-
ful for human translators if incorporated in CAT-
tools, e.g. sophisticated bilingual concordance sys-
tems, or in sub-sentential translation memory sys-
tems (Gotti et al, 2005). A quite obvious applica-
tion of a sub-sentential alignment system is the cre-
ation of bilingual dictionaries and terminology ex-
traction from bilingual corpora (Melamed, 2000),
(Itagaki et al, 2007).
In the context of statistical machine translation,
GIZA++ is one of the most widely used word
alignment toolkits. GIZA++ implements the IBM
models and is used in Moses (Koehn et al, 2007)
to generate the initial source-to-target and target-
to-source word alignments after which some sym-
metrization heuristics combine the alignments of
both translation directions.
We present an alternative ? linguistically-based
? approach, that starts from a lexical probabilistic
bilingual dictionary generated by IBM Model One.
530
3.1 Architecture
The basic idea behind our approach is that ? at least
for European languages ? translations conveying
the same meaning use to a certain extent the same
building blocks from which this meaning is com-
posed: i.e. we assume that to a large extent noun
and prepositional phrases, verb phrases and adver-
bial phrases in one language directly map to simi-
lar constituents in the other language
1
. The extent
to which our basic assumption holds depends on
the translation strategy that was used. Text types
that are typically translated in a more literal way
(e.g. user manuals) will contain more direct corre-
spondences.
We conceive our sub-sentential aligner as a cas-
cade model consisting of two phases. The objec-
tive of the first phase is to link anchor chunks,
i.e. chunks that can be linked with a very high pre-
cision. Those anchor chunks are linked based on
lexical clues and syntactic similarity. In the sec-
ond phase, we will try to model the more complex
translational correspondences based on observed
translation shift patterns. The anchor chunks of the
first phase will be used to limit the search space in
the second phase.
As the application at hand is terminology ex-
traction, we are interested in alignments with very
high precision. As the automotive corpus contains
rather literal translations, we expect that a high per-
centage of anchor chunks can be retrieved using
only the first phase of our approach.
The sub-sentential alignment system takes as
input sentence-aligned texts, together with addi-
tional linguistic annotations (part-of-speech codes
and lemmas) for the source and the target text.
In the first step of the process, the source and
target sentences are divided into chunks based on
PoS information, and lexical correspondences are
retrieved from a bilingual dictionary. During an-
chor chunk alignment, the sub-sentential aligner
links chunks based on lexical correspondences and
chunk similarity.
3.2 Bilingual Dictionary
We used the Perl implementation of IBM Model
One that is part of the Microsoft Bilingual Sen-
tence Aligner (Moore, 2002) to derive a bilingual
dictionary from a parallel corpus. IBM Model One
1
The more syntax-aware SMT systems assume that to a
certain extent syntactic relationships in one language directly
map to syntactic relationships in the other, which Hwa (2002)
calls the Direct Correspondence Assumption.
is a purely lexical model: it only takes into account
word frequencies of source and target sentences
2
.
The IBMmodels allow only 1:n word mappings,
and are therefore asymmetric. To overcome this
problem, we ran the model in two directions: from
French to English and from English to French. To
get high-accuracy links, only the words pairs oc-
curring in both the French-English and English-
French word lists were retained, and the probabil-
ities were averaged. To get rid of the noise pro-
duced by the translation model, only the entries
with an averaged value of at least 0.1 were re-
tained. This value was set experimentally
3
.
The resulting bilingual dictionary contains
28,990 English-French word pairs. The bilingual
dictionary is used to create the lexical link matrix
for each sentence pair.
3.3 Lexical Link Matrix
For each source and target word in each sentence
pair, all translations for the word form and the
lemma are retrieved from the bilingual dictionary.
In the process of building the lexical link ma-
trix, function words are neglected. Given the fre-
quency of function words in a sentence, linking
function words based on lexical information alone,
often results in erroneous alignments. For that
reason no lexical links are created for the follow-
ing word classes: determiners, prepositions, co-
ordinating conjunctions, possessive pronouns and
punctuation symbols.
For all content words, if a source word occurs in
the set of possible translations of a target word, or
if a target word occurs in the set of possible transla-
tions of the source words, a lexical link is created.
Identical strings in source and target language are
also linked.
3.4 Anchor chunks
Anchor chunk alignment comprises two steps. In
a first step, we select candidate anchor chunks; in
a second step we test the syntactic similarity of the
candidate anchor chunks.
3.4.1 Selecting candidate anchor chunks
The candidate anchor chunks are selected based on
the information available in the lexical link matrix.
2
The higher numbered IBM Models build on IBM Model
One and take into account word order (distortion) and model
the probability that a source word aligns to n target words
(fertility).
3
Lowering this threshold significantly decreased precision
scores of the sub-sentential alignment system.
531
For each source chunk a candidate target chunk is
constructed. The candidate target chunk is built by
concatenating all target chunks from a begin index
until an end index. The begin index points to the
first target chunk with a lexical link to the source
chunk under consideration. The end index points
to the last target chunk with a lexical link to the
source chunk under consideration. In this way, 1:1
and 1:n candidate target chunks are built.
The process of selecting candidate chunks as de-
scribed above, is performed a second time starting
from the target sentence. In this way additional n:1
candidates are constructed.
3.4.2 Testing chunk similarity
For each selected candidate pair, a similarity test
is performed. Chunks are considered to be similar
if at least a certain percentage of words of source
and target chunk(s) are either linked by means of
a lexical link or can be linked on the basis of cor-
responding part-of-speech codes. All word classes
can be linked based on PoS codes.
In addition to linking words based on PoS codes,
a small set of predefined language-dependent rules
were implemented to handle function words. For
example:
? Extra function words (determiners and prepo-
sitions) in source or target language are linked
together with their noun to the noun?s transla-
tion.
? In French, the preposition de is contracted
with the definitive articles le and les to du and
des respectively. The contracted determiners
are linked to an English preposition and de-
terminer.
The percentage of words that have to be linked was
empirically set at 85%.
3.5 Remaining chunks
In a second step, chunks consisting of one function
word ? mostly punctuation marks and conjunctions
? are linked based on corresponding part-of-speech
codes if its left or right neighbour on the diagonal
is an anchor chunk. Corresponding final punctua-
tion marks are also linked.
In a final step, additional candidates are con-
structed by selecting non-anchor chunks in the
source and target sentence that have correspond-
ing left and right anchor chunks as neigbours. The
anchor chunks of the first step are used as contex-
tual information to link n:m chunks or chunks for
which no lexical link was found in the lexical link
matrix.
In Figure 1, the chunks [Fr: gradient] ? [En: gra-
dient] and the final punctuation mark have been
retrieved in the first step as anchor chunk. In the
last step, the n:m chunk [Fr: de remont?ee p?edale
d? embrayage] ? [En: of rising of the clutch pedal]
is selected as candidate anchor chunk because it is
enclosed within anchor chunks.
Figure 1: n:m candidate chunk: ?A? stands for an-
chor chunks, ?L? for lexical links, ?P? for words
linked on the basis of corresponding PoS codes and
?R? for words linked by language-dependent rules.
As the contextual clues (the left and right neig-
bours of the additional candidate chunks are an-
chor chunks) provide some extra indication that the
chunks can be linked, the similarity test for the fi-
nal candidates was somewhat relaxed: the percent-
age of words that have to be linked was lowered
to 0.80 and a more relaxed PoS matching function
was used:
? Verbs and nouns can be linked
Fr: pour permettre de vidanger proprement le circuit
En: to permit clean draining of the system
? Adjectives and nouns can be linked
Fr: l? entr
?
ee d? air
En: incoming air
? Past participles can be linked to past tense
4
3.6 Evaluation
All translational correspondences were manually
indicated in the three test corpora (see section 2.2).
4
The English PoS tagger often tags a past participle erro-
neously as a past tense.
532
We adapted the annotation guidelines of Macken
(2007) to the French-English language pair, and
used three different types of links: regular links
for straightforward correspondences, fuzzy links
for translation-specific shifts of various kinds, and
null links for words for which no correspondence
could be indicated. Figure 2 shows an example.
Figure 2: Manual reference: regular links are indi-
cated by x?s, fuzzy links and null links by 0?s
To evaluate the system?s performance, we used the
evaluation methodology of Och and Ney (2003).
Och and Ney distinguished sure alignments (S)
and possible alignments (P) and introduced the fol-
lowing redefined precision and recall measures:
precision =
|A ? P |
|A|
, recall =
|A ? S|
|S|
(1)
and the alignment error rate (AER):
AER(S, P ;A) = 1 ?
|A ? P | + |A ? S|
|A| + |S|
(2)
We consider all regular links of the manual ref-
erence as sure alignments and all fuzzy and null
links as possible alignments to compare the output
of our system with the manual reference.
We trained statistical translation models using
Moses. Moses uses the GIZA++ toolkit (IBM
Model 1-4) in both translation directions (source
to target, target to source) and allows for different
symmetrization heuristics to combine the align-
ments of both translation directions. We used three
different heuristics: grow-diag-final (default), in-
tersection and union.
SHORT MEDIUM LONG
p r e p r e p r e
? .99 .83 .10 .98 .73 .16 .99 .77 .13
? .95 .92 .07 .91 .86 .11 .91 .89 .10
Gdf .95 .91 .07 .93 .85 .11 .94 .88 .09
Ling. .96 .93 .06 .94 .88 .09 .92 .87 .10
Table 3: Precision (p), recall (r) and align-
ment error rate (e) for three symmetrization
heuristics based on the GIZA++ alignments
(intersection(?), union (?), Grow-diag-final
(Gdf)) vs the linguistically-based system (Ling.)
for the three test corpora
Table 3 compares the alignment quality of our
linguistically-based system with the purely statisti-
cal approaches. Overall, the results confirm our as-
sumption that shorter sentences are easier to align
than longer sentences. As expected, the intersec-
tion heuristic aligns words with a very high preci-
sion (98-99%). We further observe that the align-
ment error rate of the linguistically-based system
is the lowest for the short and medium-length sen-
tences, but that on the long sentences the default
symmetrization heuristic yields the best results.
Manual inspection of the alignments revealed that
in some long sentences, the linguistically-based
system misaligns repeated terms in long sentences,
a phenomenon that occured frequently in the long
sentence corpus. As expected, the linguistically-
based system scores better on function words.
Overall, on this data set, the linguistically-based
system yields results that are comparable to the re-
sults obtained by the complex and computationally
expensive chain of IBM models.
4 Terminology extraction
As described in Section 1, we generate candidate
terms starting from the aligned anchor chunks. In
a second step, we use a general purpose corpus and
the n-gram frequency of the automotive corpus to
determine the specificity of the terms.
4.1 Generating candidate terms
English and French use a different compounding
strategy. In English, the most frequently used com-
pounding strategy is the concatenation of nouns,
while in French prepositional phrases are concate-
nated. The following example illustrates the dif-
ferent compounding strategy:
Fr: une proc?edure d? initialisation du calculateur
de bo??te de vitesses automatique
533
En: an automatic gearbox ECU initialisation pro-
cedure
We start from the anchor chunks as they are the
minimal chunks that could be linked together. We
implemented two heuristics to generate additional
French candidate terms: a first heuristic strips off
adjectives and a second heuristic considers each N
+ PP pattern as candidate term.
For each French candidate term, the English
translation is constructed on the basis of the word
alignments. The following candidate terms are
generated for our example:
1 proc?edure d? initialisation
du calculateur de bo??te de
vitesses automatique
automatic gearbox ECU
initialisation procedure
2 proc?edure d? initialisation
du calculateur de bo??te de
vitesses
gearbox ECU initialisa-
tion procedure
3 proc?edure d? initialisation initialisation procedure
4 initialisation du calcula-
teur
ECU initialisation
5 calculateur de bo??te de
vitesses
gearbox ECU
6 bo??te de vitesses automa-
tique
automatic gearbox
7 bo??te de vitesses gearbox
8 proc?edure procedure
9 initialisation initialisation
10 calculateur ECU
11 automatique automatic
4.2 Filtering of candidate terms
As our terminology extraction module is meant to
generate a bilingual automotive lexicon, every en-
try in our lexicon should refer to a concept or ac-
tion that is relevant in an automotive context. This
also means we want to include the minimal se-
mantical units (e.g. seat belt) as well as the larger
semantical units (e.g. outer front seat belt) of a
parent-child term relationship. In order to decide
on which terms should be kept in our lexicon, we
have combined two algorithms: Log-Likelihood
for single word entries and Mutual Expectation
Measure for multiword entries.
4.2.1 Log-Likelihood Measure
In order to detect single word terms that are dis-
tinctive enough to be kept in our bilingual lexi-
con, we have applied the Log-Likelihood measure
(LL). This metric considers frequencies of words
weighted over two different corpora (in our case
a technical automotive corpus and the more gen-
eral purpose corpus ?Le Monde?), in order to as-
sign high LL-values to words having much higher
or lower frequencies than expected. Daille (1995)
has determined empirically that LL is an accurate
measure for finding the most surprisingly frequent
words in a corpus. Low LL values on the other
hand allow to retrieve common vocabulary with
high frequencies in both corpora. We have cre-
ated a frequency list for both corpora and calcu-
lated the Log-Likelihood values for each word in
this frequency list. In the formula below, N cor-
responds to the number of words in the corpus,
whereas the observed values O correspond to the
real frequencies of a word in the corpus. The for-
mula for calculating both the expected values (E)
and the Log-Likelihood have been described in de-
tail by (Rayson and Garside, 2000).
E
i
=
N
i
?
i
O
i
?
i
N
i
(3)
We used the resulting Expected values for calcu-
lating the Log-Likelihood:
?2ln? = 2
?
i
O
i
ln(
O
i
E
i
) (4)
Manual inspection of the Log-Likelihood fig-
ures confirmed our hypothesis that more domain-
specific terms in our corpus got high LL-values.
As we are mainly interested in finding distinc-
tive terms in the automotive corpus, we have only
kept those terms showing positive Expected Val-
ues in our domain-specific corpus combined with
user-defined Log-Likelihood values. Examples of
French-English translation pairs that are filtered
out using the LL values are:
Fr: tout ? En: entire
Fr: propre ? En: clean
Fr: interdits ? En: prohibited
Fr: nombre ? En: number
4.2.2 Mutual Expectation Measure
Dias and Kaalep (2003) have developed the Mu-
tual Expectation measure for evaluating the degree
of cohesiveness between words in a text. We have
applied this metric on our list of multiword terms,
to exclude multiword terms which components do
not occur together more often than expected by
chance. In a first step, we have calculated all n-
gram frequencies (up to 8-grams) for our English
and French sentences. We use these frequencies to
derive the Normalised Expectation (NE) values for
all multiword entries, as specified by the formula
of Dias and Kaalep:
534
NE =
prob(n? gram)
1
n
?
prob(n? 1 ? grams)
(5)
The Normalised Expectation value expresses the
cost, in terms of cohesiveness, of the possible
loss of one word in an n-gram. The higher the
frequency of the n-1-grams, the smaller the NE,
and the smaller the chance that it is a valid mul-
tiword expression. As simple n-gram frequency
also seems to be a valid criterion for multiword
term identification (Daille, 1995), the NE values
are multiplied by the n-gram frequency to obtain
the final Mutual Expectation (ME) value.
We have calculated Mutual Expectation values
for all French and English multiword terms and
filtered out incomplete or erroneous terms having
very low ME values. The following example has
been filtered out:
Fr: permettant d?alimenter le circuit d?eau arri`ere
En: to supply the rear water circuit
Incomplete term:
eau arri`ere - rear water (should be Fr: circuit
d?eau arri`ere - En: rear water circuit)
4.3 Evaluation of the Terminology Extraction
Module
To evaluate the terminology extraction module,
we used all sentences of the three test corpora
(see Section 2.2). We compared the performance
of our algorithm with the output of a commer-
cial state-of-the-art terminology extraction pro-
gram SDL MultiTerm Extract
5
. MultiTerm first
extracts source language terms and identifies in
a separate step the term translations. MultiTerm
makes use of basic vocubulary lists to exclude gen-
eral vocabulary words from the candidate term list.
We ran MultiTerm Extract with the default settings
on 70,000 aligned sentences
6
of the automotive
corpus. The extracted terms of our system have
been filtered by applying Log-Likelihood thresh-
olds (for single word terms) and Mutual Expec-
tation thresholds (for multiword terms). Tabel 4
shows the number of terms after each reduction
phase.
The output of both systems has been manually
labeled taking into account the following guide-
lines:
5
www.translationzone.com/en/products/sdlmultitermextract
6
70,000 sentences was the maximum size of the corpus
that could be processed within MultiTerm Extract.
# extracted # entries # entries
entries after after
ME filtering LL filtering
Anchor chunk approach 2778 2688 2549
Multiterm Extract 1337 N/A N/A
Table 4: Figures after Log-Likelihood and Mutual
Expectation reduction
Anchor chunk approach Correct Not correct Maybe correct
Multiwords 78.5% 19% 2.5%
Single words 89.5% 9.5% 1%
All terms 83% 15% 2%
Multiterm Extract Correct Not correct Maybe correct
Multiwords 51% 48.5% 0.5%
Single words 83% 16% 1%
All terms 66% 33.5% 0.5%
Table 5: Results Term Extraction Module
? judge the quality of the bilingual entry as a
whole, meaning that the French and English
terms should express the same concept
? each entry should form a semantic unit and
refer to an existing concept or action in the
automotive context
During manual validation, the following three
labels have been used: OK (valid entry), NOK (not
a valid entry) and MAYBE (when the annotator
was not sure about the correct label). Table 5
lists the results of both our system and MultiTerm
Extract and illustrates that our linguistically
based alignment approach works particularly well
for the extraction of more complex multiword
expressions.
Error analysis on the errors made by the anchor
chunk approach revealed the following error types:
1. compounds that are only partially retrieved
in one of the two languages:
ceinture outer seat belt
(valable uniquement pour
la ceinture de s
?
ecurit
?
e
avant lat?erale)
(applies only to the outer
seat belt)
2. fuzzy word links (different grammatical
and syntactical structures, paraphrases etc)
that result in bad lexicon entries:
fusibles no fuse
(montage avec vide-
poches inf?erieur fixe sans
rangement des fusibles)
(fitting with fixed lower
storage compartment with
no fuse storage)
3. translation errors in the parallel corpus:
automatique additional
(tableau de commande
climatisation automa-
tique)
(additional air condition-
ing unit control panel)
535
4. ambiguous words that cause PoS and
chunking errors (in the corpus avant is usu-
ally used as an adjective, but in the example it
has a prepositional function as avant de):
c?ables avant cables
(rep?erer la position des
c?ables avant de les
d?eclipper)
(mark the position of the
cables before unclipping
them)
5 Conclusions and future work
We presented a sub-sentential alignment system
that links linguistically motivated phrases in paral-
lel texts based on lexical correspondences and syn-
tactic similarity. Overall, the obtained alignment
scores are comparable to the scores of the state-of-
the-art statistical approach that is used in Moses.
The results show that the aligned linguistically
motivated phrases are a useful means to extract
bilingual terminology for French-English. In the
short term, we will test our methodology on other
language pairs, i.e. French-Dutch, French-Spanish
and French-Swedish. We will also compare our
work with other bilingual term extraction pro-
grams.
6 Acknowledgement
We would like to thank PSA Peugeot Citro?en for
funding this project.
References
Daille, B. 1995. Study and implementation of com-
bined techniques for automatic extraction of termi-
nology. In Klavans, J. and P. Resnik, editors, The
Balancing Act: Combining Symbolic and Statistical
Approaches to Language, pages 49?66. MIT Press,
Cambridge, Massachusetts; London, England.
Dias, G. and H. Kaalep. 2003. Automatic Extraction of
Multiword Units for Estonian: Phrasal Verbs. Lan-
guages in Development, 41:81?91.
Gaussier, E. 1998. Flow Network Models for Word
Alignment and Terminology Extraction from Bilin-
gual Corpora . In 36th Annual Meeting of the Associ-
ation for Computational Linguistics and 17th Inter-
national Conference on Computational Linguistics
(Proceedings of COLING-ACL ?98), pages 444?450,
Universit?e de Montr?eal, Montreal, Quebec, Canada.
Gotti, F., P. Langlais, E. Macklovitch, D. Bourigault,
B. Robichaud, and C. Coulombe. 2005. 3GTM: a
third-generation translation memory. In Proceedings
of the 3rd Computational Linguistics in the North-
East (CLiNE) Workshop, Gatineau, Qu?ebec.
Hwa, R., P. Resnik, A. Weinberg, and O. Kolak. 2002.
Evaluating translational correspondence using anno-
tation projection. In Proceedings of the 40th An-
nual Meeting of the Association for Computational
Linguistics (ACL), pages 392?399, Philadelphia, PA,
USA.
Itagaki, M., T. Aikawa, and X. He. 2007. Auto-
matic Validation of Terminology Consistency with
Statistical Method. In Maegaard, Bente, editor,
Machine Translation Summit XI, pages 269?274,
Copenhagen, Denmark. European Associaton for
Machine Translation.
Koehn, P., H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open Source Toolkit
for Statistical Machine Translation. In Proceedings
of the ACL 2007 Demo and Poster Sessions, pages
177?180, Prague, Czech Republic.
Kupiec, J. 1993. An algorithm for finding noun phrase
correspondences in bilingual corpora. In Proceed-
ings of the 31st Annual Meeting of the Association
for Computational Linguistics.
Macken, L. 2007. Analysis of translational corre-
spondence in view of sub-sentential alignment. In
Proceedings of the METIS-II Workshop on New Ap-
proaches to Machine Translation, pages 97?105,
Leuven, Belgium.
Melamed, I. Dan. 2000. Models of translational equiv-
alence among words. Computational Linguistics,
26(2):221?249.
Moore, R. C. 2002. Fast and accurate sentence align-
ment of bilingual corpora. In Proceedings of the 5th
Conference of the Association for Machine Transla-
tion in the Americas, Machine Translation: from re-
search to real users, pages 135?244, Tiburon, Cali-
fornia.
Och, F. J. and H. Ney. 2003. A systematic comparison
of various statistical alignment models. Computa-
tional Linguistics, 29(1):19?51.
Rayson, P. and R. Garside. 2000. Comparing cor-
pora using frequency profiling. In Proceedings of
the workshop on Comparing Corpora, 38th annual
meeting of the Association for Computational Lin-
guistics (ACL 2000), pages 1?6.
Schmid, H. 1994. Probabilistic part-of-speech tagging
using decision trees. In International Conference on
New Methods in Language Processing, Manchester,
UK.
Tjong Kim Sang, Erik F. and Sabine Buchholz.
2000. Introduction to the CoNLL-2000 Shared Task:
Chunking. In CoNLL-2000 and LLL-2000, pages
127?132, Lisbon, Portugal.
536
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 105?108,
Prague, June 2007. c?2007 Association for Computational Linguistics
AUG: A combined classification and clustering approach for web people
disambiguation
Els Lefever and Ve?ronique Hoste
LT3 Language and Translation Technology
Ghent University Association
Groot-Brittannie?laan 45, 9000 Gent
els.lefever@hogent.be
veronique.hoste@hogent.be
Timur Fayruzov
Computational Web Intelligence
Ghent University Association
Krijgslaan 281, 9000 Gent
Timur.Fayruzov@UGent.be
Abstract
This paper presents a combined super-
vised and unsupervised approach for multi-
document person name disambiguation.
Based on feature vectors reflecting pairwise
comparisons between web pages, a classifi-
cation algorithm provides linking informa-
tion about document pairs, which leads to
initial clusters. In addition, two different
clustering algorithms are fed with matrices
of weighted keywords. In a final step the
?seed? clusters are combined with the results
of the clustering algorithms. Results on the
validation data show that a combined classi-
fication and clustering approach doesn?t al-
ways compare favorably to those obtained
by the different algorithms separately.
1 Introduction
Finding information about people on the World
Wide Web is one of the most popular activities of
Internet users. Given the high ambiguity of person
names and the increasing amount of information on
the web, it becomes very important to organize this
large amount of information into meaningful clus-
ters referring each to one single individual.
The problem of resolving name ambiguity on
the Internet has been approached from different an-
gles. Mann and Yarowsky (2003) have proposed a
Web based clustering technique relying on a fea-
ture space combining biographic facts and associ-
ated names, whereas Bagga and Baldwin (1998)
have looked for coreference chains within each doc-
ument, take the context of these chains for creating
summaries about each entity and convert these sum-
maries into a bag of words. Documents get clustered
using the standard vector space model. Other re-
searchers have taken this search for distinctive key-
words one step further and tried to come up with
?concepts? describing the documents. Fleischman
and Hovy (2004) introduce the ?maximum entropy
model?: a binary classifier determines whether two
concept-instance pairs refer to the same individ-
ual. Pedersen (2006) presented an unsupervised ap-
proach using bigrams in the contexts to be clustered,
thus aiming at a concept level semantic space instead
of a word level feature space.
For the semeval contest, we approached the task
from a double supervised and unsupervised perspec-
tive. For the supervised classification, the task was
redefined in the form of feature vectors containing
disambiguating information on pairs of documents.
In addition to this, different clustering approaches
were applied on matrices of keywords. These results
were then merged by taking the classification output
as basic ?seed? clusters, which were then enhanced
by the results from the clustering experiments.
In the remainder of this paper, Section 2 intro-
duces the data sets and describes the construction of
the feature vectors and the keyword matrices. The
classification and clustering experiments, and the
final combination of the different outputs are dis-
cussed in Section 3. Section 4 gives an overview of
the results on the test data and Section 5 summarizes
the main findings of the paper.
105
2 Data sets and feature construction
The data we have used for training our system were
made available in the framework of the SemEval
(task 13: Web People Search) competition (Artiles
et al, 2007). As preliminary training corpus (re-
ferred to as ?trial data? in our article), we used the
WePS corpus (Web People Search corpus), available
at http://nlp.uned.es/weps. For the real training set,
this trial set was expanded in order to cover differ-
ent degrees of ambiguity (very common names, un-
common names and celebrity names which tend to
monopolize search results). The training corpus is
composed of 40 sets of 100 web pages, each set
corresponding to the first 100 results for a person
name query. The documents were manually clus-
tered. Documents that couldn?t be clustered prop-
erly have been put in a ?discarded? section. Test
data have been constructed in a similar way (30 sets
of 100 web pages).
The content of the web pages has been prepro-
cessed by means of a memory-based shallow parser
(MBSP) (Daelemans and van den Bosch, 2005).
From the MBSP, we used the regular expression
based tokenizer, the part-of-speech tagger and text
chunker using the memory-based tagger MBT. On
the basis of the preprocessed data we construct a rich
feature space that combines biographic facts and dis-
tinctive characteristics for a given person, a list of
weighted keywords and meta data information about
the web page.
2.1 Feature vector construction
The following biographic facts and related named
entities were extracted from the preprocessed data.
Information on date and place of birth, and on date
and place of death were extracted by means of a rule-
based component. Furthermore, three named en-
tity features were extracted on the basis of the shal-
low syntactic information provided by the memory-
based shallow parser and additional gazetteer infor-
mation. Furthermore, a ?name? feature was aimed
at the extraction of further interesting name infor-
mation (E.g other surnames, family names) on the
person in focus, leading to the extraction of for ex-
ample ?Ann Hill Carter Lee? and ?Jo Ann Hill? for
the document collection on ?Ann Hill?. The ?loca-
tion? feature informs on the overlap between all lo-
cations named in the different documents. In a simi-
lar way, the ?NE? feature returns the inter-document
overlap between all other named entities.
Starting with the assumption that overlapping
URL and email addresses usually point to the same
individual, we have also extracted URL, email and
domain addresses from the web pages. Therefore we
have combined pattern matching rules and markup
information (HTML <href> tag). The link of the
document itself has been added to the set of URL
links. Some filtering on the list has been performed
concerning length (to exclude garbage) and content
(to exclude non-distinctive URL addresses such as
index.html). Pair-wise comparison of documents
with respect to overlapping URL, email and domain
names resulted in 3 binary features.
Another binary feature we have extracted is the
location, based on our simple supposition that if
two documents are hosted in the same city, they
most probably refer to the same person (but not
vice versa). For converting IP-addresses to city lo-
cations, we have used MaxMind GeoIP(tm) open
source database2, which was sufficient for our needs.
2.2 A bag of weighted keywords
The input source for extracting our distinctive key-
words is double: both the entire (preprocessed) con-
tent of the web pages as well as snippets and titles of
documents are used. Keywords extracted from snip-
pets and titles get a predefined -rather high- score,
as we consider them quite important. For determin-
ing the keyword relevance of the words extracted
from the content of the web pages, we have applied
Term Frequency Inverse Document Frequency (TF-
IDF) (Berger et al, 2000).
Once all scores are calculated, all weighted key-
words get stored in a matrix, which serve as input
for the clustering experiments. The calculated key-
word weight is also used, in case of overlapping key-
words, as a feature in our pairwise comparison vec-
tor. In case two keywords occurring in two different
documents are identical or recognized as synonyms
(information we obtain by using WordNet3), we sum
up the different weights of these keywords and store
this value in the feature vector.
2http://www.maxmind.com/app/geolitecity
3http://wordnet.princeton.edu/
106
3 Classification and Clustering algorithms
3.1 Classification
For the classification experiments, we used the ea-
ger RIPPER rule learner (Cohen, 1995) which in-
duces a set of easily understandable if-then classi-
fication rules for the minority class and a default
rule for the remaining class. The ruler learner was
trained and validated on the trial and training data.
Given the completely different class distribution of
the trial and training data, viz. 10.6% positive in-
stances in the trial data versus 66.7% in the train-
ing data, we decided to omit the trial data and opti-
mize the learner on the basis of the more balanced
training data set. There was an optimization of the
class ordering parameter, the two-valued negative
tests parameter, the hypothesis simplification param-
eter, the example coverage parameter, the parameter
expressing the number of optimization passes and
the loss ratio parameter. The predicted positive pair-
wise classifications were then combined using a for
coreference resolution developed counting mecha-
nism (Hoste, 2005).
3.2 Clustering Algorithms
We experimented with several clustering algorithms
and settings on the trial and training data to de-
cide on our list of parameter settings. We validated
the following three clustering algorithms. First,
we compared output from k-means and hierarchical
clustering algorithms. Next to that, we have run ex-
periments for agglomerative clustering4 . with differ-
ent parameter combinations (2 similarity measures
and 5 clustering functions). All clustering experi-
ments take the weighted keywords matrix as input.
Based on the validation experiments, hierarchical
and agglomerative clustering were further evaluated
to find out the optimal parameter settings. For hier-
archical clustering, this led to the choice of the co-
sine distance metric, single-link hierarchical cluster-
ing and a 50% cluster size. For agglomerative clus-
tering, clustering accuracy was very dependent on
the structure of the document set. This has made us
use different strategies for clustering sets containing
?famous? and ?non famous? people. As a distinction
criterion we have chosen the presence/non-presence
4http://glaros.dtc.umn.edu/gkhome/views/cluto
of the person in Wikipedia. We started with the as-
sumption that sets containing famous people (found
in Wikipedia) most probably contain a small amount
of bigger clusters than sets describing ?ordinary?
persons. According to this assumption, two differ-
ent parameter sets were used for clustering. For
Wikipedia people we have used the correlation co-
efficient and g1 clustering type, for ordinary people
we have used the cosine similarity measure and sin-
gle link clustering. For both categories the number
of target output clusters equals (number of RIPPER
output clusters + the number of documents*0.2).
Although the clustering results with the best set-
tings for hierarchical and agglomerative clustering
were very close with regard to F-score (combining
purity and inverse purity, see (Artiles et al, 2007)
for a more detailed description), manual inspection
of the content of the clusters has revealed big dif-
ferences between the two approaches. Clusters that
are output by our hierarchical algorithm look more
homogeneous (higher purity), whereas inverse pu-
rity seems better for the agglomerative clustering.
Therefor we have decided to take the best of two
worlds and combined resulting clusters of both al-
gorithms.
3.3 Merging of clustering results
Classification and clustering with optimal settings
resulted in three sets of clusters, one based on pair-
wise similarity vectors and two based on keyword
matrices. Since the former set tends to have better
precision, which seems logical because more evi-
dent features are used for classification, we used this
set as ?seed? clusters. The two remaining sets were
used to improve recall.
Merging was done in the following way: first we
compare the initial set with the result of the agglom-
erative clustering by trying to find the biggest inter-
section. We remove the intersection from the small-
est cluster and add both clusters to the final set. The
resulting set of clusters is further improved by us-
ing the result of the hierarchical clustering. Here we
apply another combining strategy: if two documents
form one cluster in the initial set, but are in separate
clusters in the other set, we merge these two clusters.
Table 1 lists all results of the separate clustering al-
gorithms as well as the final clustering results for
the Wikipedia person names. Second half of the ta-
107
Person Name Ripper agglom. hierarch. merged
Wikipedia
Alexander Macomb .69/.63 .64/.56 .57/.47 .79/.80
David Lodge .69/.65 .69/.64 .43/.33 .79/.85
George Clinton .65/.62 .64/.59 .54/.45 .75/.80
John Kennedy .67/.62 .70/.66 .49/.39 .76/.80
Michael Howard .56/.54 .63/.62 .65/.58 .62/.75
Paul Collins .54/.57 .64/.62 .63/.56 .55/.62
Tony Abbott .63/.59 .67/.63 .62/.54 .77/.83
Average Scores .73/.76 .67/.72 .62/.60 .66/.75
all Training Data
Table 1: Results on Training Data
ble shows the average results for the separate and
combined algorithms. The first score always refers
to F? = 0.5, the second score refers to F? = 0.2.
The average scores, that were calculated on the
complete training set, show that RIPPER outperforms
the combined clusters.
4 Results on the test data
4.1 Final settings
For our classification algorithm, we have finally not
kept the best settings for the training data, as this
led to an alarming over-assignment of the positive
class, thus linking nearly every document to each
other. Therefore, we were forced to define a more
strict rule set. For the clustering algorithms, we have
used the optimal parameter settings as described in
Section 3.
4.2 Test results
Table 2 lists the results for the separate and merged
clustering for SET 1 in the test data (participants
in the ACL conference) and the average for all al-
gorithms. The average score, that has been calcu-
lated on the complete test set, shows that the com-
bined clusters outperform the separate algorithms
for F? = 0.2, but the hierarchical algorithm out-
performs the others for F? = 0.5. Table 3 lists the
average results for purity, inverse purity and the F-
measures.
5 Conclusions
We proposed and validated a combined classifica-
tion and clustering approach for resolving web peo-
ple ambiguity. In future work we plan to experiment
with clustering algorithms that don?t require a prede-
fined number of clusters, as our tests revealed a big
impact of the cluster size on our results. We will also
Person Name Ripper agglom. hierarch. merged
ACL
Chris Brockett .49/.39 .74/.69 .70/.61 .79/.80
Dekang Lin .69/.58 .76/.67 .59/.47 .93/.89
Frank Keller .48/.41 .68/.75 .64/.62 .56/.71
James Curran .53/.50 .64/.77 .75/.78 .54/.72
Jerry Hobbs .50/.39 .02/.01 .58/.47 .74/.70
Leon Barrett .47/.40 .67/.74 .65/.66 .57/.73
Mark Johnson .45/.42 .55/.70 .65/.77 .44/.65
Robert Moore .39/.37 .60/.71 .66/.68 .46/.65
Sharon Goldwater .60/.49 .72/.61 .40/.29 .91/.86
Stephen Clark .41/.42 .53/.67 .68/.75 .46/.67
Average Scores .49/.45 .58/.63 .69/.69 .61/.74
all Test Data
Table 2: Results on Test Data
Test set Purity Inverse F = F =
Purity ? = 0.5 ? = 0.2
Set1 .57 .85 .64 .73
Set2 .45 .91 .58 .73
Set3 .48 .89 .60 .73
Global .50 .88 .60 .73
Table 3: Purity/Inverse Purity Results on Test Data
experiment with meta-learning, other merging tech-
niques and evaluation metrics. Furthermore, we will
investigate the impact of intra-document and inter-
document coreference resolution on web people dis-
ambiguation.
6 References
J. Artiles and J. Gonzalo and S. Sekine. 2007. The SemEval-
2007 WePS Evaluation: Establishing a benchmark for the Web
People Search Task, Proceedings of Semeval 2007, Association
for Computational Linguistics.
A. Bagga and B. Baldwin. 1998. Entity-based cross-document
co-referencing using the vector space model, Proceedings of
the 17th international conference on Computational linguistics,
75?85.
A. Berger and R. Caruana and D. Cohn and D. Freitag and V.
Mittal. 2000. Bridging the Lexical Chasm: Statistical Ap-
proaches to Answer Finding, Proc. Int. Conf. Reasearch and
Development in Information Retrieval, 192?199.
William W. Cohen. 1995. Fast Effective Rule Induction,
Proceedings of the 12th International Conference on Machine
Learning, 115?123. Tahoe City, CA.
Walter Daelemans and Antal van den Bosch. 2005. Memory-
Based Language Processing. Cambridge University Press.
Veronique Hoste. 2005. Optimization Issues in Machine Learn-
ing of Coreference Resolution. Phd dissertation, Antwerp Uni-
versity.
M.B. Fleischman and E. Hovy. 2004. Multi-document per-
son name resolution, Proceedings of 42nd Annual Meeting of
the Association for Computational Linguistics (ACL), Reference
Resolution Workshop.
G. Mann and D. Yarowsky. 2003. Unsupervised personal name
disambiguation, Proceedings of CoNLL-2003, 33?40. Edmon-
ton, Canada.
T. Pedersen and A. Purandare and A. Kulkarni. 2006. Name
Discrimination by Clustering Similar Contexts, Proceedings of
the World Wide Web Conference (WWW).
108
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 496?504,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Language-independent bilingual terminology extraction from a
multilingual parallel corpus
Els Lefever1,2, Lieve Macken1,2 and Veronique Hoste1,2
1LT3
School of Translation Studies
University College Ghent
Groot-Brittannie?laan 45
9000 Gent, Belgium
2Department of Applied Mathematics
and Computer Science
Ghent University
Krijgslaan281-S9
9000 Gent, Belgium
{Els.Lefever, Lieve.Macken, Veronique.Hoste}@hogent.be
Abstract
We present a language-pair independent
terminology extraction module that is
based on a sub-sentential alignment sys-
tem that links linguistically motivated
phrases in parallel texts. Statistical filters
are applied on the bilingual list of candi-
date terms that is extracted from the align-
ment output.
We compare the performance of both
the alignment and terminology extrac-
tion module for three different language
pairs (French-English, French-Italian and
French-Dutch) and highlight language-
pair specific problems (e.g. different com-
pounding strategy in French and Dutch).
Comparisons with standard terminology
extraction programs show an improvement
of up to 20% for bilingual terminology ex-
traction and competitive results (85% to
90% accuracy) for monolingual terminol-
ogy extraction, and reveal that the linguis-
tically based alignment module is particu-
larly well suited for the extraction of com-
plex multiword terms.
1 Introduction
Automatic Term Recognition (ATR) systems are
usually categorized into two main families. On the
one hand, the linguistically-based or rule-based
approaches use linguistic information such as PoS
tags, chunk information, etc. to filter out stop
words and restrict candidate terms to predefined
syntactic patterns (Ananiadou, 1994), (Dagan and
Church, 1994). On the other hand, the statistical
corpus-based approaches select n-gram sequences
as candidate terms that are filtered by means of
statistical measures. More recent ATR systems
use hybrid approaches that combine both linguis-
tic and statistical information (Frantzi and Anani-
adou, 1999).
Most bilingual terminology extraction systems
first identify candidate terms in the source lan-
guage based on predefined source patterns, and
then select translation candidates for these terms
in the target language (Kupiec, 1993).
We present an alternative approach that gen-
erates candidate terms directly from the aligned
words and phrases in our parallel corpus. In a sec-
ond step, we use frequency information of a gen-
eral purpose corpus and the n-gram frequencies
of the automotive corpus to determine the term
specificity. Our approach is more flexible in the
sense that we do not first generate candidate terms
based on language-dependent predefined PoS pat-
terns (e.g. for French, N N, N Prep N, and N
Adj are typical patterns), but immediately link lin-
guistically motivated phrases in our parallel cor-
pus based on lexical correspondences and syntac-
tic similarity.
This article reports on the term extraction ex-
periments for 3 language pairs, i.e. French-Dutch,
French-English and French-Italian. The focus was
on the extraction of automative lexicons.
The remainder of this paper is organized as fol-
lows: Section 2 describes the corpus. In Section 3
we present our linguistically-based sub-sentential
alignment system and in Section 4 we describe
how we generate and filter our list of candidate
terms. We compare the performance of our sys-
tem with both bilingual and monolingual state-of-
the-art terminology extraction systems. Section 5
concludes this paper.
496
2 Corpus
The focus of this research project was on the au-
tomatic extraction of 20 bilingual automative lex-
icons. All work was carried out in the framework
of a customer project for a major French automo-
tive company. The final goal of the project is to
improve vocabulary consistency in technical texts
across the 20 languages in the customer?s portfo-
lio. The French database contains about 400,000
entries (i.e. sentences and parts of sentences with
an average length of 9 words) and the translation
percentage of the database into 19 languages de-
pends on the target market.
For the development of the alignment and termi-
nology extraction module, we created three paral-
lel corpora (Italian, English, Dutch) with French
as a central language. Figures about the size of
each parallel corpus can be found in table 1.
Target Lang. # Sentence pairs # words
French Italian 364,221 6,408,693
French English 363,651 7,305,151
French Dutch 364,311 7,100,585
Table 1: Number of sentence pairs and total num-
ber of words in the three parallel corpora
2.1 Preprocessing
We PoS-tagged and lemmatized the French, En-
glish and Italian corpora with the freely available
TreeTagger tool (Schmid, 1994) and we used Tad-
Pole (Van den Bosch et al, 2007) to annotate the
Dutch corpus.
In a next step, chunk information was added
by a rule-based language-independent chunker
(Macken et al, 2008) that contains distituency
rules, which implies that chunk boundaries are
added between two PoS codes that cannot occur
in the same constituent.
2.2 Test and development corpus
As we presume that sentence length has an impact
on the alignment performance, and thus on term
extraction, we created three test sets with vary-
ing sentence lengths. We distinguished short sen-
tences (2-7 words), medium-length sentences (8-
19 words) and long sentences (> 19 words). Each
test corpus contains approximately 9,000 words;
the number of sentence pairs per test set can be
found in table 2. We also created a development
corpus with sentences of varying length to debug
the linguistic processing and the alignment mod-
ule as well as to define the thresholds for the sta-
tistical filtering of the candidate terms (see 4.1).
# Words # Sentence pairs
Short (< 8 words) +- 9,000 823
Medium (8-19 words) +- 9,000 386
Long (> 19 words) +- 9,000 180
Development corpus +-5,000 393
Table 2: Number of words and sentence pairs in
the test and development corpora
3 Sub-sentential alignment module
As the basis for our terminology extraction sys-
tem, we used the sub-sentential alignment sys-
tem of (Macken and Daelemans, 2009) that links
linguistically motivated phrases in parallel texts
based on lexical correspondences and syntactic
similarity. In the first phase of this system, anchor
chunks are linked, i.e. chunks that can be linked
with a very high precision. We think these anchor
chunks offer a valid and language-independent al-
ternative to identify candidate terms based on pre-
defined PoS patterns. As the automotive corpus
contains rather literal translations, we expect that a
high percentage of anchor chunks can be retrieved.
Although the architecture of the sub-sentential
alignment system is language-independent, some
language-specific resources are used. First, a
bilingual lexicon to generate the lexical correspon-
dences and second, tools to generate additional
linguistic information (PoS tagger, lemmatizer and
a chunker). The sub-sentential alignment system
takes as input sentence-aligned texts, together with
the additional linguistic annotations for the source
and the target texts.
The source and target sentences are divided into
chunks based on PoS information, and lexical cor-
respondences are retrieved from a bilingual dic-
tionary. In order to extract bilingual dictionaries
from the three parallel corpora, we used the Perl
implementation of IBM Model One that is part of
the Microsoft Bilingual Sentence Aligner (Moore,
2002).
In order to link chunks based on lexical clues
and chunk similarity, the following steps are taken
for each sentence pair:
1. Creation of the lexical link matrix
2. Linking chunks based on lexical correspon-
dences and chunk similarity
497
3. Linking remaining chunks
3.1 Lexical Link Matrix
For each source and target word, all translations
for the word form and the lemma are retrieved
from the bilingual dictionary. In the process of
building the lexical link matrix, function words are
neglected. For all content words, a lexical link is
created if a source word occurs in the set of pos-
sible translations of a target word, or if a target
word occurs in the set of possible translations of
the source words. Identical strings in source and
target language are also linked.
3.2 Linking Anchor chunks
Candidate anchor chunks are selected based on the
information available in the lexical link matrix.
The candidate target chunk is built by concatenat-
ing all target chunks from a begin index until an
end index. The begin index points to the first target
chunk with a lexical link to the source chunk un-
der consideration. The end index points to the last
target chunk with a lexical link to the source chunk
under consideration. This way, 1:1 and 1:n candi-
date target chunks are built. The process of select-
ing candidate chunks as described above, is per-
formed a second time starting from the target sen-
tence. This way, additional n:1 candidates are con-
structed. For each selected candidate pair, a simi-
larity test is performed. Chunks are considered to
be similar if at least a certain percentage of words
of source and target chunk(s) are either linked by
means of a lexical link or can be linked on the basis
of corresponding part-of-speech codes. The per-
centage of words that have to be linked was em-
pirically set at 85%.
3.3 Linking Remaining Chunks
In a second step, chunks consisting of one function
word ? mostly punctuation marks and conjunc-
tions ? are linked based on corresponding part-of-
speech codes if their left or right neighbour on the
diagonal is an anchor chunk. Corresponding final
punctuation marks are also linked.
In a final step, additional candidates are con-
structed by selecting non-anchor chunks in the
source and target sentence that have correspond-
ing left and right anchor chunks as neigbours. The
anchor chunks of the first step are used as contex-
tual information to link n:m chunks or chunks for
which no lexical link was found in the lexical link
matrix.
In Figure 1, the chunks [Fr: gradient] ? [En:
gradient] and the final punctuation mark have been
retrieved in the first step as anchor chunk. In the
last step, the n:m chunk [Fr: de remonte?e pe?dale
d? embrayage] ? [En: of rising of the clutch pedal]
is selected as candidate anchor chunk because it is
enclosed within anchor chunks.
Figure 1: n:m candidate chunk: ?A? stands for an-
chor chunks, ?L? for lexical links, ?P? for words
linked on the basis of corresponding PoS codes
and ?R? for words linked by language-dependent
rules.
As the contextual clues (the left and right neig-
bours of the additional candidate chunks are an-
chor chunks) provide some extra indication that
the chunks can be linked, the similarity test for
the final candidates was somewhat relaxed: the
percentage of words that have to be linked was
lowered to 0.80 and a more relaxed PoS matching
function was used.
3.4 Evaluation
To test our alignment module, we manually indi-
cated all translational correspondences in the three
test corpora. We used the evaluation methodology
of Och and Ney (2003) to evaluate the system?s
performance. They distinguished sure alignments
(S) and possible alignments (P) and introduced the
following redefined precision and recall measures
(where A refers to the set of alignments):
precision =
|A ? P |
|A|
, recall =
|A ? S|
|S|
(1)
and the alignment error rate (AER):
AER(S, P ;A) = 1?
|A ? P |+ |A ? S|
|A|+ |S|
(2)
498
Table 3 shows the alignment results for the three
language pairs. (Macken et al, 2008) showed that
the results for French-English were competitive to
state-of-the-art alignment systems.
SHORT MEDIUM LONG
p r e p r e p r e
Italian .99 .93 .04 .95 .89 .08 .95 .89 .07
English .97 .91 .06 .95 .85 .10 .92 .85 .12
Dutch .96 .83 .11 .87 .73 .20 .87 .67 .24
Table 3: Precision (p), recall (r) and alignment er-
ror rate (e) for our sub-sentential alignment sys-
tem evaluated on French-Italian, French-English
and French-Dutch
As expected, the results show that the align-
ment quality is closely related to the similarity be-
tween languages. As shown in example (1), Ital-
ian and French are syntactically almost identical
? and hence easier to align, English and French
are still close but show some differences (e.g dif-
ferent compounding strategy and word order) and
French and Dutch present a very different lan-
guage structure (e.g. in Dutch the different com-
pound parts are not separated by spaces, separable
verbs, i.e. verbs with prefixes that are stripped off,
occur frequently (losmaken as an infinitive versus
maak los in the conjugated forms) and a different
word order is adopted).
(1) Fr: de?clipper le renvoi de ceinture de se?curite?.
(En: unclip the mounting of the belt of safety)
It: sganciare il dispositivo di riavvolgimento della
cintura di sicurezza.
(En: unclip the mounting of the belt of satefy)
En: unclip the seat belt mounting.
Du: maak de oprolautomaat van de autogordel los.
(En: clip the mounting of the seat-belt un)
We tried to improve the low recall for French-
Dutch by adding a decompounding module to our
alignment system. In case the target word does
not have a lexical correspondence in the source
sentence, we decompose the Dutch word into its
meaningful parts and look for translations of the
compound parts. This implies that, without de-
compounding, in example 2 only the correspon-
dences doublure ? binnenpaneel, arc ? dakverste-
viging and arrie`re ? achter will be found. By de-
composing the compound into its meaningful parts
(binnenpaneel = binnen + paneel, dakversteviging
= dak + versteviging) and retrieving the lexical
links for the compound parts, we were able to link
the missing correspondence: pavillon ? dakverste-
viging.
(2) Fr: doublure arc pavillon arrie`re.
(En: rear roof arch lining)
Du: binnenpaneel dakversteviging achter.
We experimented with the decompounding mod-
ule of (Vandeghinste, 2008), which is based on
the Celex lexical database (Baayen et al, 1993).
The module, however, did not adapt well to the
highly technical automotive domain, which is re-
flected by its low recall and the low confidence
values for many technical terms. In order to adapt
the module to the automotive domain, we imple-
mented a domain-dependent extension to the de-
compounding module on the basis of the devel-
opment corpus. This was done by first running the
decompounding module on the Dutch sentences to
construct a list with possible compound heads, be-
ing valid compound parts in Dutch. This list was
updated by inspecting the decompounding results
on the development corpus. While decomposing,
we go from right to left and strip off the longest
valid part that occurs in our preconstructed list
with compound parts and we repeat this process
on the remaining part of the word until we reach
the beginning of the word.
Table 4 shows the impact of the decompound-
ing module, which is more prominent for short
and medium sentences than for long sentences. A
superficial error analysis revealed that long sen-
tences combine a lot of other French ? Dutch
alignment difficulties next to the decompounding
problem (e.g. different word order and separable
verbs).
SHORT MEDIUM LONG
p r e p r e p r e
Dutch
no dec .95 .76 .16 .88 .67 .24 .88 .64 .26
dec .96 .83 .11 .87 .73 .20 .87 .67 .24
Table 4: Precision (p), recall (r) and alignment er-
ror rate (e) for French-Dutch without and with de-
compounding information
4 Term extraction module
As described in Section 1, we generate candi-
date terms from the aligned phrases. We believe
these anchor chunks offer a more flexible approach
499
because the method is language-pair independent
and is not restricted to a predefined set of PoS pat-
terns to identify valid candidate terms. In a second
step, we use a general-purpose corpus and the n-
gram frequency of the automotive corpus to deter-
mine the specificity of the candidate terms.
The candidate terms are generated in several
steps, as illustrated below for example (3).
(3) Fr: Tableau de commande de climatisation automa-
tique
En: Automatic air conditioning control panel
1. Selection of all anchor chunks (minimal
chunks that could be linked together) and lex-
ical links within the anchor chunks:
tableau de commande control panel
climatisation air conditioning
commande control
tableau panel
2. combine each NP + PP chunk:
commande de climatisa-
tion automatique
automatic air condition-
ing control
tableau de commande de
climatisation automatique
automatic air condition-
ing control panel
3. strip off the adjectives from the anchor
chunks:
commande de climatisa-
tion
air conditioning control
tableau de commande de
climatisation
air conditioning control
panel
4.1 Filtering candidate terms
To filter our candidate terms, we keep following
criteria in mind:
? each entry in the extracted lexicon should re-
fer to an object or action that is relevant for
the domain (notion of termhood that is used
to express ?the degree to which a linguis-
tic unit is related to domain-specific context?
(Kageura and Umino, 1996))
? multiword terms should present a high de-
gree of cohesiveness (notion of unithood that
expresses the ?degree of strength or stability
of syntagmatic combinations or collocations?
(Kageura and Umino, 1996))
? all term pairs should contain valid translation
pairs (translation quality is also taken into
consideration)
To measure the termhood criterion and to fil-
ter out general vocabulary words, we applied
Log-Likelihood filters on the French single-word
terms. In order to filter on low unithood values,
we calculated the Mutual Expectation Measure for
the multiword terms in both source and target lan-
guage.
4.1.1 Log-Likelihood Measure
The Log-Likehood measure (LL) should allow us
to detect single word terms that are distinctive
enough to be kept in our bilingual lexicon (Daille,
1995). This metric considers word frequencies
weighted over two different corpora (in our case a
technical automotive corpus and the more general
purpose corpus ?Le Monde?1), in order to assign
high LL-values to words having much higher or
lower frequencies than expected. We implemented
the formula for both the expected values and the
Log-Likelihood values as described by (Rayson
and Garside, 2000).
Manual inspection of the Log-Likelihood fig-
ures confirmed our hypothesis that more domain-
specific terms in our corpus were assigned high
LL-values. We experimentally defined the thresh-
old for Log-Likelihood values corresponding to
distinctive terms on our development corpus. Ex-
ample (4) shows some translation pairs which are
filtered out by applying the LL threshold.
(4) Fr: cependant ? En: however ? It: tuttavia ? Du:
echter
Fr: choix ? En: choice ? It: scelta ? Du: keuze
Fr: continuer ? En: continue ? It: continuare ? Du:
verdergaan
Fr: cadre ? En: frame ? It: cornice ? Du: frame
(erroneous filtering)
Fr: alle?gement ? En: lightening ? It: alleggerire ?
Du: verlichten (erroneous filtering)
4.1.2 Mutual Expectation Measure
The Mutual Expectation measure as described by
Dias and Kaalep (2003) is used to measure the
degree of cohesiveness between words in a text.
This way, candidate multiword terms whose com-
ponents do not occur together more often than ex-
pected by chance get filtered out. In a first step,
we have calculated all n-gram frequencies (up to
8-grams) for our four automotive corpora and then
used these frequencies to derive the Normalised
1http://catalog.elra.info/product info.php?products id=438
500
Expectation (NE) values for all multiword entries,
as specified by the formula of Dias and Kaalep:
NE =
prob(n? gram)
1
n
?
prob(n? 1? grams)
(3)
The Normalised Expectation value expresses the
cost, in terms of cohesiveness, of the possible loss
of one word in an n-gram. The higher the fre-
quency of the n-1-grams, the smaller the NE, and
the smaller the chance that it is a valid multiword
expression. The final Mutual Expectation (ME)
value is then obtained by multiplying the NE val-
ues by the n-gram frequency. This way, the Mu-
tual Expectation between n words in a multiword
expression is based on the Normalised Expecta-
tion and the relative frequency of the n-gram in
the corpus.
We calculated Mutual Expectation values for all
candidate multiword term pairs and filtered out in-
complete or erroneous terms having ME values be-
low an experimentally set threshold (being below
0.005 for both source and target multiword or be-
low 0.0002 for one of the two multiwords in the
translation pair). The following incomplete can-
didate terms in example (5) were filtered out by
applying the ME filter:
(5) Fr: fermeture embout - En: end closing - It:
chiusura terminale - Du: afsluiting deel
(should be: Fr: fermeture embout de brancard - En:
chassis member end closing panel - It: chiusura ter-
minale del longherone - Du: afsluiting voorste deel
van langsbalk)
4.2 Evaluation
The terminology extraction module was tested on
all sentences from the three test corpora. The out-
put was manually labeled and the annotators were
asked to judge both the translational quality of the
entry (both languages should refer to the same ref-
erential unit) as well as the relevance of the term
in an automotive context. Three labels were used:
OK (valid entry), NOK (not a valid entry) and
MAYBE (in case the annotator was not sure about
the relevance of the term).
First, the impact of the statistical filtering was
measured on the bilingual term extraction. Sec-
ondly, we compared the output of our system with
the output of a commercial bilingual terminology
extraction module and with the output of a set of
standard monolingual term extraction modules.
Since the annotators labeled system output, the
reported scores all refer to precision scores. In fu-
ture work, we will develop a gold standard corpus
which will enable us to also calculate recall scores.
4.2.1 Impact of filtering
Table 5 shows the difference in performance for
both single and multiword terms with and with-
out filtering. Single-word filtering seems to have a
bigger impact on the results than multiword filter-
ing. This can be explained by the fact that our can-
didate multiword terms are generated from anchor
chunks (chunks aligned with a very high preci-
sion) that already answer to strict syntactical con-
straints. The annotators also mentioned the diffi-
culty of judging the relevance of single word terms
for the automotive domain (no clear distinction be-
tween technical and common vocabulary).
NOT FILTERED FILTERED
OK NOK MAY OK NOK MAY
FR-EN
Sing w 82% 17% 1% 86.5% 12% 1.5%
Mult w 81% 16.5% 2.5% 83% 14.5% 2.5%
FR-IT
Sing w 80.5% 19% 0.5% 84.5% 15% 0.5%
Mult w 69% 30% 1.0% 72% 27% 1.0%
FR-DU
Sing w 72% 25% 3% 75% 22% 3%
Mult w 83% 15% 2% 84% 14% 2%
Table 5: Impact of statistical filters on Single and
Multiword terminology extraction
4.2.2 Comparison with bilingual terminology
extraction
We compared the three filtered bilingual lexi-
cons (French versus English-Italian-Dutch) with
the output of a commercial state-of-the-art termi-
nology extraction program SDL MultiTerm Ex-
tract2. MultiTerm is a statistically based system
that first generates a list of candidate terms in the
source language (French in our case) and then
looks for translations of these terms in the target
language. We ran MultiTerm with its default set-
tings (default noise-silence threshold, default stop-
word list, etc.) on a large portion of our parallel
corpus that also contains all test sentences3. We
ran our system (where term extraction happens on
a sentence per sentence basis) on the three test
sets.
2www.translationzone.com/en/products/sdlmultitermextract
370,000 sentences seemed to be the maximum size of
the corpus that could be easily processed within MultiTerm
Extract.
501
Table 6 shows that even after applying statistical
filters, our term extraction module retains a much
higher number of candidate terms than MultiTerm.
# Extracted terms # Terms after filtering MultiTerm
FR-EN 4052 3386 1831
FR-IT 4381 3601 1704
FR-DU 3285 2662 1637
Table 6: Number of terms before and after apply-
ing Log-Likelihood and ME filters
Table 7 lists the results of both systems and
shows the differences in performance for single
and multiword terms. Following observations can
be made:
? The performance of both systems is compa-
rable for the extraction of single word terms,
but our system clearly outperforms Multi-
Term when it comes to the extraction of more
complex multiword terms.
? Although the alignment results for French-
Italian were very good, we do not achieve
comparable results for Italian multiword ex-
traction. This can be due to the fact that the
syntactic structure is very similar in both lan-
guages. As a result, smaller syntactic chunks
are linked. However one can argue that, just
because of the syntactic resemblance of both
languages, the need for complex multiword
terms is less prominent in closely related lan-
guages as translators can just paste smaller
noun phrases together in the same order in
both languages. If we take the following ex-
ample for instance:
de?poser ? l? embout ? de brancard
togliere ? il terminale ? del sotto-
porta
we can recompose the larger compound
l?embout de brancard or il terminale del sot-
toporta by translating the smaller parts in the
same order (l?embout ? il terminale and de
brancard ? del sottoporta
? Despite the worse alignment results for
Dutch, we achieve good accuracy results on
the multiword term extraction. Part of that
can be explained by the fact that French and
Dutch use a different compounding strategy:
whereas French compounds are created by
concatenating prepositional phrases, Dutch
usually tends to concatenate noun phrases
(even without inserting spaces between the
different compound parts). This way we can
extract larger Dutch chunks that correspond
to several French chunks, for instance:
Fr: feu re?gulateur ? de pression
carburant.
Du: brandstofdrukregelaar.
ANCHOR CHUNK APPROACH MULTITERM
OK NOK MAY OK NOK MAY
FR-EN
Sing w 86.5% 12% 1.5% 77% 21% 2%
Mult w 83% 14.5% 2.5% 47% 51% 2%
Total 84.5% 13.5% 2 % 64% 34% 2%
FR-IT
Sing w 84.5% 15% 0.5% 85% 14% 1%
Mult w 72% 27% 1.0% 65% 34% 1%
Total 77.5% 22% 1% 76.5% 22.5% 1%
FR-DU
Sing w 75% 22% 3% 64.5% 33% 2.5%
Mult w 84% 14% 2% 49.5% 49.5% 1%
Total 79.5% 20% 2.5% 58% 40% 2%
Table 7: Precision figures for our term extraction
system and for SDL MultiTerm Extract
4.2.3 Comparison with monolingual
terminology extraction
In order to have insights in the performance of
our terminology extraction module, without con-
sidering the validity of the bilingual terminology
pairs, we contrasted our extracted English terms
with state-of-the art monolingual terminology sys-
tems. As we want to include both single words and
multiword terms in our technical automotive lex-
icon, we only considered ATR systems which ex-
tract both categories. We used the implementation
for these systems from (Zhang et al, 2008) which
is freely available at1.
We compared our system against 5 other ATR
systems:
1. Baseline system (Simple Term Frequency)
2. Weirdness algorithm (Ahmad et al, 2007)
which compares term frequencies in the tar-
get and reference corpora
3. C-value (Frantzi and Ananiadou, 1999)
which uses term frequencies as well as
unit-hood filters (to measure the collocation
strength of units)
1http://www.dcs.shef.ac.uk/?ziqizhang/resources/tools/
502
4. Glossex (Kozakov et al, 2004) which uses
term frequency information from both the tar-
get and reference corpora and compares term
frequencies with frequencies of the multi-
word components
5. TermExtractor (Sclano and Velardi, 2007)
which is comparable to Glossex but intro-
duces the ?domain consensus? which ?sim-
ulates the consensus that a term must gain in
a community before being considered a rele-
vant domain term?
For all of the above algorithms, the input auto-
motive corpus is PoS tagged and linguistic filters
(selecting nouns and noun phrases) are applied to
extract candidate terms. In a second step, stop-
words are removed and the same set of extracted
candidate terms (1105 single words and 1341 mul-
tiwords) is ranked differently by each algorithm.
To compare the performance of the ranking algo-
rithms, we selected the top terms (300 single and
multiword terms) produced by all algorithms and
compared these with our top candidate terms that
are ranked by descending Log-likelihood (calcu-
lated on the BNC corpus) and Mutual Expectation
values. Our filtered list of unique English automo-
tive terms contains 1279 single words and 1879
multiwords in total. About 10% of the terms do
not overlap between the two term lists. All can-
didate terms have been manually labeled by lin-
guists. Table 8 shows the results of this compari-
son.
SINGLE WORD TERMS MULTIWORD TERMS
OK NOK MAY OK NOK MAY
Baseline 80% 19.5% 0.5% 84.5% 14.5% 1%
Weirdness 95.5% 3.5% 1% 96% 2.5% 1.5%
C-value 80% 19.5% 0.5% 94% 5% 1%
Glossex 94.5% 4.5% 1% 85.5% 14% 0.5%
TermExtr. 85% 15% 0% 79% 20% 1%
AC 85.5% 14.5% 0% 90% 8% 2%
approach
Table 8: Results for monolingual Term Extraction
on the English part of the automotive corpus
Although our term extraction module has been tai-
lored towards bilingual term extraction, the results
look competitive to monolingual state-of-the-art
ATR systems. If we compare these results with
our bilingual term extraction results, we can ob-
serve that we gain more in performance for mul-
tiwords than for single words, which might mean
that the filtering and ranking based on the Mutual
Expectation works better than the Log-Likelihood
ranking.
An error analysis of the results leads to the fol-
lowing insights:
? All systems suffer from partial retrieval of
complex multiwords (e.g. ATR management
ecu instead of engine management ecu, AC
approach chassis leg end piece closure in-
stead of chassis leg end piece closure panel).
? We manage to extract nice sets of multiwords
that can be associated with a given concept,
which could be nice for automatic ontology
population (e.g. AC approach gearbox cas-
ing, gearbox casing earth, gearbox casing
earth cable, gearbox control, gearbox control
cables, gearbox cover, gearbox ecu, gearbox
ecu initialisation procedure, gearbox fixing,
gearbox lower fixings, gearbox oil, gearbox
oil cooler protective plug).
? Sometimes smaller compounds are not ex-
tracted because they belong to the same syn-
tactic chunk (E.g we extract passenger com-
partment assembly, passenger compartment
safety, passenger compartment side panel,
etc. but not passenger compartment as such).
5 Conclusions and further work
We presented a bilingual terminology extraction
module that starts from sub-sentential alignments
in parallel corpora and applied it on three differ-
ent parallel corpora that are part of the same auto-
motive corpus. Comparisons with standard termi-
nology extraction programs show an improvement
of up to 20% for bilingual terminology extraction
and competitive results (85% to 90% accuracy) for
monolingual terminology extraction. In the near
future we want to experiment with other filtering
techniques, especially to measure the domain dis-
tinctiveness of terms and work on a gold standard
for measuring recall next to accuracy. We will
also investigate our approach on languages which
are more distant from each other (e.g. French ?
Swedish).
Acknowledgments
We would like to thank PSA Peugeot Citroe?n for
funding this project.
503
References
K. Ahmad, L. Gillam, and L. Tostevin. 2007. Uni-
versity of surrey participation in trec8: Weirdness
indexing for logical document extrapolation and
rerieval (wilder). In Proceedings of the Eight Text
REtrieval Conference (TREC-8).
S. Ananiadou. 1994. A methodology for automatic
term recognition. In Proceedings of the 15th con-
ference on computational linguistics, pages 1034?
1038.
R.H. Baayen, R. Piepenbrock, and H. van Rijn. 1993.
The celex lexical database on cd-rom.
I. Dagan and K. Church. 1994. Termight: identifying
and translating technical terminology. In Proceed-
ings of Applied Language Processing, pages 34?40.
B. Daille. 1995. Study and implementation of com-
bined techniques for automatic extraction of termi-
nology. In J. Klavans and P. Resnik, editors, The
Balancing Act: Combining Symbolic and Statistical
Approaches to Language, pages 49?66. MIT Press,
Cambridge, Massachusetts; London, England.
G. Dias and H. Kaalep. 2003. Automatic extraction
of multiword units for estonian: Phrasal verbs. Lan-
guages in Development, 41:81?91.
K.T. Frantzi and S. Ananiadou. 1999. the c-value/nc-
value domain independent method for multiword
term extraction. journal of Natural Language Pro-
cessing, 6(3):145?180.
K. Kageura and B. Umino. 1996. Methods of au-
tomatic term recognition: a review. Terminology,
3(2):259?289.
L. Kozakov, Y. Park, T.-H Fin, Y. Drissi, Y.N. Do-
ganata, and T. Confino. 2004. Glossary extraction
and knowledge in large organisations via semantic
web technologies. In Proceedings of the 6th Inter-
national Semantic Web Conference and he 2nd Asian
Semantic Web Conference (Se-mantic Web Chal-
lenge Track).
J. Kupiec. 1993. An algorithm for finding noun phrase
correspondences in bilingual corpora. In Proceed-
ings of the 31st Annual Meeting of the Association
for Computational Linguistics.
L. Macken and W. Daelemans. 2009. Aligning lin-
guistically motivated phrases. In van Halteren H.
Verberne, S. and P.-A. Coppen, editors, Selected Pa-
pers from the 18th Computational Linguistics in the
Netherlands Meeting, pages 37?52, Nijmegen, The
Netherlands.
L. Macken, E. Lefever, and V. Hoste. 2008.
Linguistically-based sub-sentential alignment for
terminology extraction from a bilingual automotive
corpus. In Proceedings of the 22nd International
Conference on Computational Linguistics (Coling
2008), pages 529?536, Manchester, United King-
dom.
R. C. Moore. 2002. Fast and accurate sentence align-
ment of bilingual corpora. In Proceedings of the 5th
Conference of the Association for Machine Trans-
lation in the Americas, Machine Translation: from
research to real users, pages 135?244, Tiburon, Cal-
ifornia.
F. J. Och and H. Ney. 2003. A systematic comparison
of various statistical alignment models. Computa-
tional Linguistics, 29(1):19?51.
P. Rayson and R. Garside. 2000. Comparing cor-
pora using frequency profiling. In Proceedings of
the workshop on Comparing Corpora, 38th annual
meeting of the Association for Computational Lin-
guistics (ACL 2000), pages 1?6.
H. Schmid. 1994. Probabilistic part-of-speech tagging
using decision trees. In International Conference on
New Methods in Language Processing, Manchester,
UK.
F. Sclano and P. Velardi. 2007. Termextractor: a web
application to learn the shared terminology of emer-
gent web communities. In Proceedings of the 3rd
International Conference on Interoperability for En-
terprise Software and Applications (I-ESA 2007).
A. Van den Bosch, G.J. Busser, W. Daelemans, and
S. Canisius. 2007. An efficient memory-based mor-
phosyntactic tagger and parser for dutch. In Selected
Papers of the 17th Computational Linguistics in the
Netherlands Meeting, pages 99?114, Leuven, Bel-
gium.
V. Vandeghinste. 2008. A Hybrid Modular Machine
Translation System. LoRe-MT: Low Resources Ma-
chine Translation. Ph.D. thesis, Centre for Compu-
tational Linguistics, KULeuven.
Z. Zhang, J. Iria, C. Brewster, and F. Ciravegna. 2008.
A comparative evaluation of term recognition algo-
rithms. In Proceedings of the sixth international
conference of Language Resources and Evaluation
(LREC 2008).
504
Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 82?87,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
SemEval-2010 Task 3: Cross-lingual Word Sense Disambiguation
Els Lefever1,2 and Veronique Hoste1,2
1LT3, Language and Translation Technology Team, University College Ghent
Groot-Brittannie?laan 45, 9000 Gent, Belgium
2Department of Applied Mathematics and Computer Science, Ghent University
Krijgslaan 281 (S9), 9000 Gent, Belgium
{Els.Lefever, Veronique.Hoste}@hogent.be
Abstract
We propose a multilingual unsupervised Word
Sense Disambiguation (WSD) task for a sample of
English nouns. Instead of providing manually sense-
tagged examples for each sense of a polysemous
noun, our sense inventory is built up on the basis of
the Europarl parallel corpus. The multilingual setup
involves the translations of a given English polyse-
mous noun in five supported languages, viz. Dutch,
French, German, Spanish and Italian.
The task targets the following goals: (a) the man-
ual creation of a multilingual sense inventory for a
lexical sample of English nouns and (b) the eval-
uation of systems on their ability to disambiguate
new occurrences of the selected polysemous nouns.
For the creation of the hand-tagged gold standard,
all translations of a given polysemous English noun
are retrieved in the five languages and clustered by
meaning. Systems can participate in 5 bilingual
evaluation subtasks (English - Dutch, English - Ger-
man, etc.) and in a multilingual subtask covering all
language pairs.
As WSD from cross-lingual evidence is gaining
popularity, we believe it is important to create a mul-
tilingual gold standard and run cross-lingual WSD
benchmark tests.
1 Introduction
The Word Sense Disambiguation (WSD) task,
which consists in selecting the correct sense of a
given word in a given context, has been widely
studied in computational linguistics. For a recent
overview of WSD algorithms, resources and appli-
cations, we refer to Agirre and Edmonds (2006)
and Navigli (2009). Semantic evaluation competi-
tions such as Senseval1 and its successor Semeval
revealed that supervised approaches to WSD
usually achieve better results than unsupervised
methods (Ma`rquez et al, 2006). The former use
machine learning techniques to induce a classifier
from manually sense-tagged data, where each
occurrence of a polysemous word gets assigned a
sense label from a predefined sense inventory such
as WordNet (Fellbaum, 1998). These supervised
methods, however, heavily rely on large sense-
tagged corpora which are very time consuming and
expensive to build. This phenomenon, well known
as the knowledge acquisition bottleneck (Gale et
al., 1992), explains the modest use and success of
supervised WSD in real applications.
Although WSD has long time been studied as a
stand-alone NLP task, there is a growing feeling
in the WSD community that WSD should prefer-
ably be integrated in real applications such as
Machine Translation or multilingual information
retrieval (Agirre and Edmonds, 2006). Several
studies have demonstrated that for instance Sta-
tistical Machine Translation (SMT) benefits from
incorporating a dedicated WSD module (Chan et al,
2007; Carpuat and Wu, 2007). Using translations
from a corpus instead of human-defined sense
labels is one way of facilitating the integration of
WSD in multilingual applications. It also implic-
1http://www.senseval.org/
82
itly deals with the granularity problem as finer
sense distinctions are only relevant as far as they
are lexicalized in the translations. Furthermore,
this type of corpus-based approach is language-
independent, which makes it a valid alternative
for languages lacking sufficient sense inventories
and sense-tagged corpora, although one could
argue that the lack of parallel corpora for certain
language pairs might be problematic as well. The
methodology to deduce word senses from parallel
corpora starts from the hypothesis that the different
sense distinctions of a polysemous word are often
lexicalized cross-linguistically. For instance, if we
query the English noun ?bill? in the English-Dutch
Europarl, the following top four translations are
retrieved: ?rekening? (Eng.: ?invoice?) (198 occur-
rences), ?kosten? (Eng.: ?costs?) (100 occ.), ?Bill?
(96 occ.) and ?wetsvoorstel? (Eng.: ?piece of
legislation?) (77 occ.). If we make the simplifying
assumption for our example that (i) these are the
only Dutch translations of our focus word and that
(ii) all sense distinctions of ?bill? are lexicalized
in Dutch, we can infer that the English noun ?bill?
has at most four different senses. These different
senses in turn can be grouped in case of synonymy.
In the Dutch-French Europarl, for example, both
?rekening? and ?kosten?, are translated by the
French ?frais?, which might indicate that both
Dutch words are synonymous.
Several WSD studies are based on the idea of
cross-lingual evidence. Gale et al (1993) use a
bilingual parallel corpus for the automatic creation
of a sense-tagged data set, where target words in the
source language are tagged with their translation
of the word in the target language. Diab and
Resnik (2002) present an unsupervised approach
to WSD that exploits translational correspondences
in parallel corpora that were artificially created by
applying commercial MT systems on a sense-tagged
English corpus. Ide et al (2002) use a multilingual
parallel corpus (containing seven languages from
four language families) and show that sense dis-
tinctions derived from translation equivalents are at
least as reliable as those made by human annotators.
Moreover, some studies present multilingual WSD
systems that attain state-of-the-art performance in
all-words disambiguation (Ng et al, 2003). The
proposed Cross-lingual Word Sense Disambigua-
tion task differs from earlier work (e.g. Ide et al
(2002)) through its independence from an externally
defined sense set.
The remainder of this paper is organized as follows.
In Section 2, we present a detailed description of
the cross-lingual WSD task. It introduces the par-
allel corpus we used, informs on the development
and test data and discusses the annotation procedure.
Section 3 gives an overview of the different scoring
strategies that will be applied. Section 4 concludes
this paper.
2 Task set up
The cross-lingual Word Sense Disambiguation task
involves a lexical sample of English nouns. We pro-
pose two subtasks, i.e. systems can either partici-
pate in the bilingual evaluation task (in which the
answer consists of translations in one language) or
in the multilingual evaluation task (in which the an-
swer consists of translations in all five supported lan-
guages). Table 1 shows an example of the bilingual
sense labels for two test occurrences of the English
noun bank in our parallel corpus which will be fur-
ther described in Section 2.1. Table 2 presents the
multilingual sense labels for the same sentences.
... giving fish to people living on the [bank] of the
river
Language Sense label
Dutch (NL) oever/dijk
French (F) rives/rivage/bord/bords
German (D) Ufer
Italian (I) riva
Spanish (ES) orilla
The [bank] of Scotland ...
Language Sense label
Dutch (NL) bank/kredietinstelling
French (F) banque/e?tablissement de cre?dit
German (D) Bank/Kreditinstitut
Italian (I) banca
Spanish (ES) banco
Table 1: Example of bilingual sense labels for the English
noun bank
83
... giving fish to people living on the [bank] of the
river
Language Sense label
NL,F,D,I,ES oever/dijk,
rives/rivage/bord/bords,
Ufer, riva, orilla
The [bank] of Scotland ...
Language Sense label
NL,F,D,I,ES bank/kredietinstelling, banque/
e?tablissement de cre?dit, Bank/
Kreditinstitut, banca, banco
Table 2: Example of multi-lingual sense labels for the
English noun bank
2.1 Corpus and word selection
The document collection which serves as the basis
for the gold standard construction and system
evaluation is the Europarl parallel corpus2, which
is extracted from the proceedings of the European
Parliament (Koehn, 2005). We selected 6 languages
from the 11 European languages represented in
the corpus: English (our target language), Dutch,
French, German, Italian and Spanish. All sentences
are aligned using a tool based on the Gale and
Church (1991) algorithm. We only consider the 1-1
sentence alignments between English and the five
other languages (see also Tufis et al (2004) for
a similar strategy). These 1-1 alignments will be
made available to all task participants. Participants
are free to use other training corpora, but additional
translations which are not present in Europarl will
not be included in the sense inventory that is used
for evaluation.
For the competition, two data sets will be developed.
The development and test sentences will be selected
from the JRC-ACQUIS Multilingual Parallel Cor-
pus3. The development data set contains 5 poly-
semous nouns, for which we provide the manually
built sense inventory based on Europarl and 50 ex-
ample instances, each annotated with one sense label
(cluster that contains all translations that have been
grouped together for that particular sense) per target
2http://www.statmt.org/europarl/
3http://wt.jrc.it/lt/Acquis/
language. The manual construction of the sense in-
ventory will be discussed in Section 2.2. The test
data contains 50 instances for 20 nouns from the test
data as used in the Cross-Lingual Lexical Substitu-
tion Task4. In this task, annotators and systems are
asked to provide as many correct Spanish transla-
tions as possible for an English target word. They
are not bound to a predefined parallel corpus, but
can freely choose the translations from any available
resource. Selecting the target words from the set of
nouns thats will be used for the Lexical Substitution
Task should make it easier for systems to participate
in both tasks.
2.2 Manual annotation
The sense inventory for the 5 target nouns in the de-
velopment data and the 20 nouns in the test data is
manually built up in three steps.
1. In the first annotation step, the 5 translations
of the English word are identified per sentence
ID. In order to speed up this identification,
GIZA++ (Och and Ney, 2003) is used to gen-
erate the initial word alignments for the 5 lan-
guages. All word alignments are manually ver-
ified.
In this step, we might come across multiword
translations, especially in Dutch and German
which tend to glue parts of compounds together
in one orthographic unit. We decided to keep
these translations as such, even if they do not
correspond exactly to the English target word.
In following sentence, the Dutch translation
witboek corresponds in fact to the English com-
pound white paper, and not to the English tar-
get word paper:
English: the European Commission
presented its white paper
Dutch: de presentatie van het
witboek door de Europese Com-
missie
Although we will not remove these compound
translations from our sense inventory, we will
make sure that the development and test sen-
tences do not contain target words that are part
4http://lit.csci.unt.edu/index.php/Semeval 2010
84
of a larger multiword unit, in order not to dis-
advantage systems that do not deal with decom-
pounding.
2. In the second step, three annotators per lan-
guage will cluster the retrieved translations per
target language. On the basis of the sentence
IDs, the translations in all languages will be au-
tomatically coupled. Only translations above a
predefined frequency threshold are considered
for inclusion in a cluster. Clustering will hap-
pen in a trilingual setting, i.e. annotators al-
ways cluster two target languages simultane-
ously (with English being the constant source
language)5.
After the clustering of the translations, the an-
notators perform a joint evaluation per lan-
guage in order to reach a consensus clustering
for each target language. In case the annota-
tors do not reach a consensus, we apply soft-
clustering for that particular translation, i.e. we
assign the translation to two or more different
clusters.
3. In a last step, there will be a cross-lingual con-
flict resolution in which the resulting cluster-
ings are checked cross-lingually by the human
annotators.
The resulting sense inventory is used to annotate the
sentences in the development set and the test set.
This implies that a given target word is annotated
with the appropriate sense cluster. This annotation
is done by the same native annotators as in steps
2 and 3. The goal is to reach a consensus cluster
per sentence. But again, if no consensus is reached,
soft-clustering is applied and as a consequence, the
correct answer for this particular test instance con-
sists of one of the clusters that were considered for
soft-clustering.
The resulting clusters are used by the three native
annotators to select their top 3 translations per
sentence. These potentially different translations
are kept to calculate frequency information for all
answer translations (discussed in section 3).
5The annotators will be selected from the master students
at the ?University College Ghent ? Faculty of Translation? that
trains certified translators in all six involved languages.
Table 3 shows an example of how the translation
clusters for the English noun ?paper? could look
like in a trilingual setting.
3 System evaluation
As stated before, systems can participate in two
tasks, i.e. systems can either participate in one or
more bilingual evaluation tasks or they can partici-
pate in the multilingual evaluation task incorporat-
ing the five supported languages. The evaluation of
the multilingual evaluation task is simply the aver-
age of the system scores on the five bilingual evalu-
ation tasks.
3.1 Evaluation strategies
For the evaluation of the participating systems we
will use an evaluation scheme which is inspired
by the English lexical substitution task in SemEval
2007 (McCarthy and Navigli, 2007). The evaluation
will be performed using precision and recall (P and
R in the equations that follow). We perform both a
best result evaluation and a more relaxed evaluation
for the top five results.
Let H be the set of annotators, T be the set of test
items and hi be the set of responses for an item i ? T
for annotator h ? H . Let A be the set of items from
T where the system provides at least one answer and
ai : i ? A be the set of guesses from the system for
item i. For each i, we calculate the multiset union
(Hi) for all hi for all h ? H and for each unique
type (res) in Hi that has an associated frequency
(freqres). In the formula of (McCarthy and Navigli,
2007), the associated frequency (freqres) is equal
to the number of times an item appears in Hi. As
we define our answer clusters by consensus, this fre-
quency would always be ?1?. In order to overcome
this, we ask our human annotators to indicate their
top 3 translations, which enables us to also obtain
meaningful associated frequencies (freqres) (?1? in
case the translation is not chosen by any annotator,
?2? in case a translation is picked by 1 annotator, ?3?
if picked by two annotators and ?4? if chosen by all
three annotators).
Best result evaluation For the best result evalu-
ation, systems can propose as many guesses as the
system believes are correct, but the resulting score is
85
divided by the number of guesses. In this way, sys-
tems that output a lot of guesses are not favoured.
P =
?
ai:i?A
?
res?ai
freqres
|ai|
|Hi|
|A| (1)
R =
?
ai:i?T
?
res?ai
freqres
|ai|
|Hi|
|T | (2)
Relaxed evaluation For the more relaxed evalu-
ation, systems can propose up to five guesses. For
this evaluation, the resulting score is not divided by
the number of guesses.
P =
?
ai:i?A
?
res?ai
freqres
|Hi|
|A| (3)
R =
?
ai:i?T
?
res?ai
freqres
|Hi|
|T | (4)
3.2 Baseline
We will produce two, both frequency-based, base-
lines. The first baseline, which will be used for the
best result evaluation, is based on the output of the
GIZA++ word alignments on the Europarl corpus
and just returns the most frequent translation of a
given word. The second baseline outputs the five
most frequent translations of a given word accord-
ing to the GIZA++ word alignments. This baseline
will be used for the relaxed evaluation. As a third
baseline, we will consider using a baseline based on
EuroWordNet6, which is available in the five target
languages.
4 Conclusions
We presented a multilingual unsupervised Word
Sense Disambiguation task for a sample of English
nouns. The lack of supervision refers to the con-
struction of the sense inventory, that is built up on
the basis of translations retrieved from the Europarl
corpus in five target languages. Systems can partici-
pate in a bilingual or multilingual evaluation and are
asked to provide correct translations in one or five
6http://www.illc.uva.nl/EuroWordNet
target languages for new instances of the selected
polysemous target nouns.
References
E. Agirre and P. Edmonds, editors. 2006. Word Sense
Disambiguation. Text, Speech and Language Tech-
nology. Springer, Dordrecht.
M. Carpuat and D. Wu. 2007. Improving statistical
machine translation using word sense disambiguation.
In Proceedings of the 2007 Joint Conference on Em-
pirical Methods in Natural Language Processing and
Computational Natural Language Learning (EMNLP-
CoNLL), pages 61?72, Prague, Czech Republic.
Y.S. Chan, H.T. Ng, and D. Chiang. 2007. Word sense
disambiguation improves statistical machine transla-
tion. In Proceedings of the 45th Annual Meeting of the
Association of Computational Linguistics, pages 33?
40, Prague, Czech Republic.
M. Diab and P. Resnik. 2002. An unsupervised method
for word sense tagging using parallel corpora. In Pro-
ceedings of ACL, pages 255?262.
C. Fellbaum. 1998. WordNet: An Electronic Lexical
Database. MIT Press.
William A. Gale and Kenneth W. Church. 1991. A pro-
gram for aligning sentences in bilingual corpora. In
Computational Linguistics, pages 177?184.
W.A. Gale, K. Church, and D. Yarowsky. 1992. Esti-
mating upper and lower bounds on the performance
of word-sense disambiguation programs. In Proceed-
ings of the 30th Annual Meeting of the Association for
Computational Linguistics, pages 249?256.
W.A. Gale, K.W. Church, and D. Yarowsky. 1993. A
method for disambiguating word senses in a large cor-
pus. In Computers and the Humanities, volume 26,
pages 415?439.
N. Ide, T. Erjavec, and D. Tufis. 2002. Sense dis-
crimination with parallel corpora. In Proceedings of
ACL Workshop on Word Sense Disambiguation: Re-
cent Successes and Future Directions, pages 54?60.
P. Koehn. 2005. Europarl: A parallel corpus for statis-
tical machine translation. In Proceedings of the MT
Summit.
L. Ma`rquez, G. Escudero, D. Mart?`nez, and G. Rigau.
2006. Supervised corpus-based methods for WSD. In
E. Agirre and P. Edmonds, editors, Word Sense Disam-
biguation: Algorithms and Applications, pages 167?
216. Eds Springer, New York, NY.
D. McCarthy and R. Navigli. 2007. Semeval-2007 task
10: English lexical substitution task. In Proceedings
of the 4th International Workshop on Semantic Evalu-
ations (SemEval-2007), pages 48?53.
86
R. Navigli. 2009. Word sense disambiguation: a survey.
In ACM Computing Surveys, volume 41, pages 1?69.
H.T. Ng, B. Wang, and Y.S. Chan. 2003. Exploiting
parallel texts for word sense disambiguation: An em-
pirical study. In Proceedings of the 41st Annual Meet-
ing of the Association for Computational Linguistics,
pages 455?462, Santa Cruz.
F.J. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1):19?51.
Dan Tufis?, Radu Ion, and Nancy Ide. 2004. Fine-Grained
Word Sense Disambiguation Based on Parallel Cor-
pora, Word Alignment, Word Clustering and Aligned
Wordnets. In Proceedings of the 20th International
Conference on Computational Linguistics (COLING
2004), pages 1312?1318, Geneva, Switzerland, Au-
gust. Association for Computational Linguistics.
English ?paper? Dutch French Italian
Cluster 1 boek, verslag, wetsvoorstel livre, document, libro
green paper kaderbesluit paquet
Cluster 2 document, voorstel, paper document, rapport, travail documento, rapporto
present a paper nota, stuk, notitie publication, note testo, nota
proposition, avis
Cluster 3 krant, dagblad journal, quotidien giornale, quotidiano,
read a paper weekblad hebdomadaire settimanale, rivista
Cluster 4 papier papier carta, cartina
reams of paper
Cluster 5 papieren, papier papeterie, papetie`re cartastraccia, cartaceo
of paper, paper prullenmand papier cartiera
industry, paper basket
Cluster 6 stembiljet, bulletin, vote scheda, scheda di voto
voting paper, stembriefje
ballot paper
Cluster 7 papiertje papier volant foglio, foglietto
piece of paper
Cluster 8 papier, administratie paperasse, paperasserie carta, amministrativo
excess of paper, administratief papier, administratif burocratico, cartaceo
generate paper bureaucratie
Cluster 9 in theorie, op papier, en the?orie, in teoria,
on paper papieren, bij woorden conceptuellement di parole
Cluster 10 op papier e?crit, dans les textes, nero su bianco, (di natura)
on paper de nature typographique, par voie tipografica, per iscritto,
e?pistolaire, sur (le) papier cartaceo, di parole
Cluster 11 agenda, zittingstuk, ordre du jour, ordine del giorno
order paper stuk ordre des votes
Table 3: translation clusters for the English noun ?paper?
87
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 317?322,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
ParaSense or How to Use Parallel Corpora for Word Sense Disambiguation
Els Lefever1,2, Ve?ronique Hoste1,2,3 and Martine De Cock2
1LT3, Language and Translation Technology Team, University College Ghent
Groot-Brittannie?laan 45, 9000 Gent, Belgium
2Dept. of Applied Mathematics and Computer Science, Ghent University
Krijgslaan 281 (S9), 9000 Gent, Belgium
3Dept. of Linguistics, Ghent University
Blandijnberg 2, 9000 Gent, Belgium
Abstract
This paper describes a set of exploratory ex-
periments for a multilingual classification-
based approach to Word Sense Disambigua-
tion. Instead of using a predefined monolin-
gual sense-inventory such as WordNet, we use
a language-independent framework where the
word senses are derived automatically from
word alignments on a parallel corpus. We built
five classifiers with English as an input lan-
guage and translations in the five supported
languages (viz. French, Dutch, Italian, Span-
ish and German) as classification output. The
feature vectors incorporate both the more tra-
ditional local context features, as well as bi-
nary bag-of-words features that are extracted
from the aligned translations. Our results
show that the ParaSense multilingual WSD
system shows very competitive results com-
pared to the best systems that were evaluated
on the SemEval-2010 Cross-Lingual Word
Sense Disambiguation task for all five target
languages.
1 Introduction
Word Sense Disambiguation (WSD) is the NLP
task that consists in selecting the correct sense of
a polysemous word in a given context. Most state-
of-the-art WSD systems are supervised classifiers
that are trained on manually sense-tagged corpora,
which are very time-consuming and expensive to
build (Agirre and Edmonds, 2006) . In order to over-
come this acquisition bottleneck (sense-tagged cor-
pora are scarce for languages other than English),
we decided to take a multilingual approach to WSD,
that builds up the sense inventory on the basis of
the Europarl parallel corpus (Koehn, 2005). Using
translations from a parallel corpus implicitly deals
with the granularity problem as finer sense distinc-
tions are only relevant as far as they are lexicalized
in the target translations. It also facilitates the in-
tegration of WSD in multilingual applications such
as multilingual Information Retrieval (IR) or Ma-
chine Translation (MT). Significant improvements
in terms of general MT quality were for the first time
reported by Carpuat and Wu (2007) and Chan et al
(2007). Both papers describe the integration of a
dedicated WSD module in a Chinese-English statis-
tical machine translation framework and report sta-
tistically significant improvements in terms of stan-
dard MT evaluation metrics.
Several studies have already shown the validity
of using parallel corpora for sense discrimination
(e.g. (Ide et al, 2002)), for bilingual WSD mod-
ules (e.g. (Gale and Church, 1993; Ng et al, 2003;
Diab and Resnik, 2002; Chan and Ng, 2005; Da-
gan and Itai, 1994)) and for WSD systems that use
a combination of existing WordNets with multilin-
gual evidence (Tufis? et al, 2004). The research de-
scribed in this paper is novel as it presents a truly
multilingual classification-based approach to WSD
that directly incorporates evidence from four other
languages. To this end, we build further on two
well-known research ideas: (1) the possibility to
use parallel corpora to extract translation labels and
features in an automated way and (2) the assump-
tion that incorporating evidence from multiple lan-
guages into the feature vector will be more infor-
mative than a more restricted set of monolingual or
bilingual features. Furthermore, our WSD system
does not use any information from external lexical
resources such as WordNet (Fellbaum, 1998) or Eu-
roWordNet (Vossen, 1998).
317
2 Experimental Setup
Starting point of the experiments was the six-lingual
sentence-aligned Europarl corpus that was used in
the SemEval-2010 ?Cross-Lingual Word Sense Dis-
ambiguation? (CLWSD) task (Lefever and Hoste,
2010b). The task is a lexical sample task for twenty
English ambiguous nouns that consists in assign-
ing a correct translation in the five supported tar-
get languages (viz. French, Italian, Spanish, Ger-
man and Dutch) for an ambiguous focus word in a
given context. In order to detect the relevant transla-
tions for each of the twenty ambiguous focus words,
we ran GIZA++ (Och and Ney, 2003) with its de-
fault settings for all focus words. This word align-
ment output was then considered to be the label for
the training instances for the corresponding classi-
fier (e.g. the Dutch translation is the label that is used
to train the Dutch classifier). By considering this
word alignment output as oracle information, we re-
defined the CLWSD task as a classification task.
To train our five classifiers (English as input lan-
guage and French, German, Dutch, Italian and Span-
ish as focus languages), we used the memory-based
learning (MBL) algorithm implemented in TIMBL
(Daelemans and Hoste, 2002), which has success-
fully been deployed in previous WSD classification
tasks (Hoste et al, 2002). We performed heuris-
tic experiments to define the parameter settings for
the classifier, leading to the selection of the Jef-
frey Divergence distance metric, Gain Ratio feature
weighting and k = 7 as number of nearest neigh-
bours. In future work, we plan to use an optimized
word-expert approach in which a genetic algorithm
performs joint feature selection and parameter op-
timization per ambiguous word (Daelemans et al,
2003).
For our feature vector creation, we combined a set
of English local context features and a set of binary
bag-of-words features that were extracted from the
aligned translations.
2.1 Training Feature Vector Construction
We created two experimental setups. The first
training set incorporates the automatically generated
word alignments as labels. We applied an automatic
post-processing step on these word alignments in or-
der to remove leading and trailing determiners and
prepositions. In future work, we will investigate
other word alignment strategies and measure the im-
pact on the classification scores. The second training
set uses manually verified word alignments as labels
for the training instances. This second setup is then
to be considered as the upper bound on the current
experimental setup.
All English sentences were preprocessed
by means of a memory-based shallow parser
(MBSP) (Daelemans and van den Bosch, 2005) that
performs tokenization, Part-of-Speech tagging and
text chunking. The preprocessed sentences were
used as input to build a set of commonly used WSD
features related to the English input sentence:
? features related to the focus word itself being
the word form of the focus word, the lemma,
Part-of-Speech and chunk information
? local context features related to a window of
three words preceding and following the focus
word containing for each of these words their
full form, lemma, Part-of-Speech and chunk in-
formation
In addition to these well known monolingual fea-
tures, we extracted a set of binary bag-of-words fea-
tures from the aligned translation that are not the
target language of the classifier (e.g. for the Dutch
classifier, we extract bag-of-words features from the
Italian, Spanish, French and German aligned trans-
lations). In order to extract useful content words,
we first ran Part-of-Speech tagging and lemmatisa-
tion by means of the Treetagger (Schmid, 1994) tool.
Per ambiguous focus word, a list of content words
(nouns, adjectives, verbs and adverbs) was extracted
that occurred in the aligned translations of the En-
glish sentences containing the focus word. One bi-
nary feature per selected content word was then cre-
ated per ambiguous word: ?0? in case the word does
not occur in the aligned translation of this instance,
and ?1? in case the word does occur in the aligned
translation of the training instance.
2.2 Test Feature Vector Construction
For the creation of the feature vectors for the test in-
stances, we follow a similar strategy as the one we
used for the creation of the training instances. The
first part of the feature vector contains the English
318
local context features that were also extracted for
the training instances. For the construction of the
bag-of-words features however, we need to adopt a
different approach as we do not have aligned trans-
lations for the English test instances at our disposal.
We decided to deploy a novel strategy that uses
the Google Translate API1 to automatically gener-
ate a translation for all English test instances in the
five supported languages. Online machine transla-
tions tools have already been used before to create
artificial parallel corpora that were used for NLP
tasks such as for instance Named Entity Recogni-
tion (Shah et al, 2010).
In a next step the automatically generated transla-
tion was preprocessed in the same way as the train-
ing translations (Part-of-Speech-tagged and lemma-
tized). The resulting lemmas were then used to con-
struct the same set of binary bag-of-words features
that were stored for the training instances of the am-
biguous focus word.
3 Evaluation
To evaluate our five classifiers, we used the sense in-
ventory and test set of the SemEval ?Cross-Lingual
Word Sense Disambiguation? task. The sense inven-
tory was built up on the basis of the Europarl corpus:
all retrieved translations of a polysemous word were
manually grouped into clusters, which constitute dif-
ferent senses of that given word. The test instances
were selected from the JRC-ACQUIS Multilingual
Parallel Corpus2 and BNC3. To label the test data,
native speakers provided their top three translations
from the predefined clusters of Europarl translations,
in order to assign frequency weights to the set of
gold standard translations. A more detailed descrip-
tion of the construction of the data set can be found
in Lefever and Hoste (2010a).
As evaluation metrics, we used both the SemEval
BEST precision metric from the CLWSD task as
well as a straightforward accuracy measure. The
SemEval metric takes into account the frequency
weights of the gold standard translations: transla-
tions that were picked by different annotators get
a higher weight. For the BEST evaluation, systems
1http://code.google.com/apis/language/
2http://wt.jrc.it/lt/Acquis/
3http://www.natcorp.ox.ac.uk/
can propose as many guesses as the system believes
are correct, but the resulting score is divided by the
number of guesses. In this way, systems that out-
put a lot of guesses are not favoured. For a more
detailed description of the SemEval scoring scheme,
we refer to McCarthy and Navigli (2007). Follow-
ing variables are used for the SemEval precision for-
mula. Let H be the set of annotators, T the set of test
items and hi the set of responses for an item i ? T
for annotator h ? H . Let A be the set of items from
T where the system provides at least one answer and
ai : i ? A the set of guesses from the system for
item i. For each i, we calculate the multiset union
(Hi) for all hi for all h ? H and for each unique
type (res) in Hi that has an associated frequency
(freqres).
Prec =
?
ai:i?A
P
res?ai
freqres
|ai|
|Hi|
|A|
(1)
The second metric we use is a straightforward ac-
curacy measure, that divides the number of correct
answers by the total amount of test instances.
As a baseline, we selected the most frequent lem-
matized translation that resulted from the automated
word alignment (GIZA++). We also compare our
results with the two winning SemEval-2 systems
for the Cross-Lingual Word Sense Disambiguation
task, UvT-WSD (that only participated for Dutch
and Spanish) and T3-COLEUR. The UvT-WSD sys-
tem (van Gompel, 2010), that also uses a k-nearest
neighbor classifier and a variety of local and global
context features, obtained the best scores for Span-
ish and Dutch in the SemEval CLWSD competi-
tion. Although we also use a memory-based learner,
our method is different from this system in the way
the feature vectors are constructed. Next to the
incorporation of similar local context features, we
also include evidence from multiple languages in
our feature vector. For French, Italian and Ger-
man however, the T3-COLEUR system (Guo and
Diab, 2010) outperformed the other systems in the
SemEval competition. This system adopts a differ-
ent approach: during the training phase a monolin-
gual WSD system processes the English input sen-
tence and a word alignment module is used to ex-
tract the aligned translation. The English senses to-
gether with their aligned translations (and probabil-
319
ity scores) are then stored in a word sense transla-
tion table, in which look-ups are performed during
the testing phase. This system also differs from the
Uvt-WSD and ParaSense systems in the sense that
the word senses are derived from WordNet, whereas
the other systems do not use any external resources.
The results for all five classifiers are listed in two
tables. Table 1 gives an overview of the SemEval-
2010 weighted precision scores, whereas Table 2
shows the more straightforward accuracy figures.
Both tables list the scores averaged over all twenty
test words for the baseline (most frequent word
alignment), the best SemEval system (for a given
language) and the two ParaSense setups: one that ex-
clusively uses automatically generated word align-
ments, and one that uses the verified word alignment
labels. For both setups we trained three flavors of
the ParaSense system (1: local context + translation
features, 2: translation features and 3: local context
features).
The classification results show that for both se-
tups all three flavors of the ParaSense system easily
beat the baseline. Moreover, the ParaSense system
clearly outperforms the winning SemEval systems,
except for Spanish where the scores are similar. As
all systems, viz. the two SemEval systems as well
as the three flavors of the ParaSense system, were
trained on the same Europarl data, the scores illus-
trate the potential advantages of using a multilingual
approach. Although we applied a very basic strategy
for the selection of our bag-of-words translation fea-
tures (we did not perform any filtering on the trans-
lations except for Part-of-Speech information), we
observe that for three languages the full feature vec-
tor outperforms the classifier that uses the more tra-
ditional WSD local context features. For Dutch, the
classifier that merely uses translation features even
outperforms the classifier that uses the local context
features. In previous research (Lefever and Hoste,
2011), we showed that the classifier using evidence
from all different languages was constantly better
than the ones using less or no multilingual evidence.
In addition, the scores also degraded relatively to the
number of translation features that was used. As we
used a different set of translation features for the lat-
ter pilot experiments (we only used the translations
of the ambiguous words instead of the full bag-of-
words features we used for the current setup), we
need to confirm this trend with more experiments
using the current feature sets.
Another important observation is that the classifi-
cation scores degrade when using the automatically
generated word alignments, but only to a minor ex-
tent. This clearly shows the viability of our setup.
Further experiments with different word alignment
settings and symmetrisation methods should allow
us to further improve the results with the automat-
ically generated word alignments. Using the non-
validated labels makes the system very flexible and
language-independent, as all steps in the feature vec-
tor creation can be run automatically.
4 Conclusion
We presented preliminary results for a multilingual
classification-based approach to Word Sense Dis-
ambiguation. In addition to the commonly used
monolingual local context features, we also incor-
porate bag-of-word features that are built from the
aligned translations. Although there is still a lot of
room for improvement on the feature base, our re-
sults show that the ParaSense system clearly outper-
forms state-of-the-art systems for all languages, ex-
cept for Spanish where the results are very similar.
As all steps are run automatically, this multilingual
approach could be an answer for the acquisition bot-
tleneck, as long as there are parallel corpora avail-
able for the targeted languages. Although large mul-
tilingual corpora are still rather scarce, we strongly
believe there will be more parallel corpora available
in the near future (large companies and organiza-
tions disposing of large quantities of parallel text,
internet corpora such as the ever growing Wikipedia
corpus, etc.). Another line of research could be the
exploitation of comparable corpora to acquire addi-
tional training data.
In future work, we want to run additional exper-
iments with different classifiers (SVM) and apply
a genetic algorithm to perform joint feature selec-
tion, parameter optimization and instance selection.
We also plan to expand our feature set by including
global context features (content words from the En-
glish sentence) and to examine the relationship be-
tween the performance and the number (and nature)
of languages that is added to the feature vector. In
addition, we will apply semantic analysis tools (such
320
French Italian Spanish Dutch German
Baseline 20.71 14.03 18.36 15.69 13.16
T3-COLEUR 21.96 15.55 19.78 10.71 13.79
UvT-WSD 23.42 17.70
Non-verified word alignment labels
ParaSense1 (full feature vector) 24.54 18.03 22.80 18.56 16.88
ParaSense2 (translation features) 23.92 16.77 22.58 17.70 15.98
ParaSense3 (local context features) 24.09 19.89 23.21 17.57 16.55
Verified word alignment labels
ParaSense1 (full feature vector) 24.60 19.64 23.10 18.61 17.41
ParaSense2 (translation features) 24.29 19.15 22.94 18.25 16.90
ParaSense3 (local context features) 24.79 21.31 23.56 17.70 17.54
Table 1: SemEval precision scores averaged over all twenty test words
French Italian Spanish Dutch German
Baseline 63.10 47.90 53.70 59.40 52.30
T3-COLEUR 66.88 50.73 59.83 40.01 54.20
UvT-WSD 70.20 64.10
Non-verified word alignment labels
ParaSense1 (full feature vector) 75.20 63.40 68.20 68.10 66.20
ParaSense2 (translation features) 73.20 58.30 67.60 65.90 63.60
ParaSense3 (local context features) 73.50 65.50 69.40 63.90 61.90
Verified word alignment labels
ParaSense1 (full feature vector) 75.70 63.20 68.50 68.20 67.80
ParaSense2 (translation features) 74.70 61.30 68.30 66.80 66.20
ParaSense3 (local context features) 75.20 67.30 70.30 63.30 66.10
Table 2: Accuracy percentages averaged over all twenty test words
as LSA) on our multilingual bag-of-words sets in
order to detect latent semantic topics in the multi-
lingual feature base. Finally, we want to evaluate
to which extent the integration of our WSD output
helps practical applications such as Machine Trans-
lation or Information Retrieval.
Acknowledgments
We thank the anonymous reviewers for their valu-
able remarks. This research was funded by the Uni-
versity College Research Fund.
References
E. Agirre and P. Edmonds, editors. 2006. Word Sense
Disambiguation. Algorithms and Applications. Text,
Speech and Language Technology. Springer, Dor-
drecht.
M. Carpuat and D. Wu. 2007. Improving statistical
machine translation using word sense disambiguation.
In Proceedings of the 2007 Joint Conference on Em-
pirical Methods in Natural Language Processing and
Computational Natural Language Learning (EMNLP-
CoNLL), pages 61?72, Prague, Czech Republic.
Y.S. Chan and H.T. Ng. 2005. Scaling Up Word Sense
Disambiguation via Parallel Texts. In Proceedings of
the 20th National Conference on Artificial Intelligence
(AAAI 2005), pages 1037?1042, Pittsburgh, Pennsyl-
vania, USA.
Y.S. Chan, H.T. Ng, and D. Chiang. 2007. Word sense
disambiguation improves statistical machine transla-
tion. In Proceedings of the 45th Annual Meeting of the
Association of Computational Linguistics, pages 33?
40, Prague, Czech Republic.
W. Daelemans and V. Hoste. 2002. Evaluation of Ma-
chine Learning Methods for Natural Language Pro-
cessing Tasks. In Proceedings of the third Interna-
tional Conference on Language Resources and Eval-
uation (LREC?02), pages 755?760.
W. Daelemans and A. van den Bosch. 2005. Memory-
based Language Processing. Cambridge University
Press.
W. Daelemans, V. Hoste, F. De Meulder, and B. Naudts.
2003. Combined optimization of feature selection and
321
algorithm parameters in machine learning of language.
Machine Learning, pages 84?95.
I. Dagan and A. Itai. 1994. Word sense disambiguation
using a second language monolingual corpus. Compu-
tational Linguistics, 20(4):563?596.
M. Diab and P. Resnik. 2002. An Unsupervised Method
for Word Sense Tagging Using Parallel Corpora. In
Proceedings of ACL, pages 255?262.
C. Fellbaum. 1998. WordNet: An Electronic Lexical
Database. MIT Press.
W.A. Gale and K.W. Church. 1993. A program for align-
ing sentences in bilingual corpora. Computational
Linguistics, 19(1):75?102.
W. Guo and M. Diab. 2010. COLEPL and COLSLM: An
Unsupervised WSD Approach to Multilingual Lexical
Substitution, Tasks 2 and 3 SemEval 2010. In Pro-
ceedings of the 5th International Workshop on Seman-
tic Evaluation, pages 129?133, Uppsala, Sweden. As-
sociation for Computational Linguistics.
V. Hoste, I. Hendrickx, W. Daelemans, and A. van den
Bosch. 2002. Parameter Optimization for Machine-
Learning of Word Sense Disambiguation. Natural
Language Engineering, Special Issue on Word Sense
Disambiguation Systems, 8:311?325.
N. Ide, T. Erjavec, and D. Tufis?. 2002. Sense discrimi-
nation with parallel corpora. . In ACL-2002 Workhop
on Word Sense Disambiguation: Recent Successes and
Future Directions, pages 54?60, Philadelphia.
Ph. Koehn. 2005. Europarl: a parallel corpus for statisti-
cal machine translation. In Tenth Machine Translation
Summit, pages 79?86, Phuket, Thailand.
E. Lefever and V. Hoste. 2010a. Construction
of a Benchmark Data Set for Cross-Lingual Word
Sense Disambiguation. In Nicoletta Calzolari, Khalid
Choukri, Bente Maegaard, Joseph Mariani, Jan Odijk,
Stelios Piperidis, and Daniel Tapias, editors, Proceed-
ings of the seventh International Conference on Lan-
guage Resources and Evaluation (LREC?10), Valletta,
Malta, May. European Language Resources Associa-
tion (ELRA).
E. Lefever and V. Hoste. 2010b. SemEval-2010 Task
3: Cross-Lingual Word Sense Disambiguation. In
Proceedings of the 5th International Workshop on Se-
mantic Evaluation, ACL 2010, pages 15?20, Uppsala,
Sweden.
E. Lefever and V. Hoste. 2011. Examining the Validity
of Cross-Lingual Word Sense Disambiguation. In Pro-
ceedings of the Conference on Computational Linguis-
tics and Intelligent Text Processing (CICLing 2011),
Tokyo, Japan.
D. McCarthy and R. Navigli. 2007. SemEval-2007 Task
10: English Lexical Substitution Task. In Proceedings
of the 4th International Workshop on Semantic Eval-
uations (SemEval-2007), pages 48?53, Prague, Czech
Republic.
H.T. Ng, B. Wang, and Y.S. Chan. 2003. Exploiting par-
allel texts for word sense disambiguation: An empiri-
cal study. In 41st Annual Meeting of the Association
for Computational Linguistics (ACL), pages 455?462,
Sapporo, Japan.
F. J. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1):19?51.
H. Schmid. 1994. Probabilistic part-of-speech tagging
using decision trees. In Proceedings of the Interna-
tional Conference on new methods in Language Pro-
cessing, Manchester, UK.
R. Shah, B. Lin, A. Gershman, and R. Frederking. 2010.
SYNERGY: A Named Entity Recognition System for
Resource-scarce Languages such as Swahili using On-
line Machine Translation. In Proceedings of the
Second Workshop on African Language Technology
(AFLAT 2010), Valletta, Malt.
D. Tufis?, R. Ion, and N. Ide. 2004. Fine-Grained
Word Sense Disambiguation Based on Parallel Cor-
pora, Word Alignment, Word Clustering and Aligned
Wordnets. In Proceedings of the 20th International
Conference on Computational Linguistics (COLING
2004), pages 1312?1318, Geneva, Switzerland, Au-
gust. Association for Computational Linguistics.
M. van Gompel. 2010. UvT-WSD1: A Cross-Lingual
Word Sense Disambiguation System. In Proceedings
of the 5th International Workshop on Semantic Evalu-
ation, pages 238?241, Uppsala, Sweden. Association
for Computational Linguistics.
P. Vossen, editor. 1998. EuroWordNet: a multilingual
database with lexical semantic networks. Kluwer Aca-
demic Publishers, Norwell, MA, USA.
322
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 15?20,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
SemEval-2010 Task 3: Cross-Lingual Word Sense
Disambiguation
Els Lefever
1 ,2
and Veronique Hoste
1 ,2
1
LT3, Language and Translation Technology Team, University College Ghent, Belgium
2
Department of Applied Mathematics and Computer Science, Ghent University, Belgium
{Els.Lefever,Veronique.Hoste}@hogent.be
Abstract
The goal of this task is to evaluate
the feasibility of multilingual WSD on
a newly developed multilingual lexi-
cal sample data set. Participants were
asked to automatically determine the
contextually appropriate translation of
a given English noun in five languages,
viz. Dutch, German, Italian, Spanish
and French. This paper reports on the
sixteen submissions from the five dif-
ferent participating teams.
1 Introduction
Word Sense Disambiguation, the task of se-
lecting the correct sense of an ambiguous word
in a given context, is a well-researched NLP
problem (see for example Agirre and Edmonds
(2006) and Navigli (2009)), largely boosted
by the various Senseval and SemEval editions.
The SemEval-2010 Cross-lingual Word Sense
Disambiguation task focuses on two bottle-
necks in current WSD research, namely the
scarcity of sense inventories and sense-tagged
corpora (especially for languages other than
English) and the growing tendency to eval-
uate the performance of WSD systems in a
real application such as machine translation
and cross-language information retrieval (see
for example Agirre et al (2007)).
The Cross-lingual WSD task aims at the de-
velopment of a multilingual data set to test the
feasibility of multilingual WSD. Many studies
have already shown the validity of this cross-
lingual evidence idea (Gale et al, 1993; Ide et
al., 2002; Ng et al, 2003; Apidianaki, 2009),
but until now no benchmark data sets have
been available. For the SemEval-2010 compe-
tition we developed (i) a sense inventory in
which the sense distinctions were extracted
from the multilingual corpus Europarl
1
and
(ii) a data set in which the ambiguous words
were annotated with the senses from the mul-
tilingual sense inventory. The Cross-Lingual
WSD task is a lexical sample task for English
nouns, in which the word senses are made up of
the translations in five languages, viz. Dutch,
French, Italian, Spanish and German. Both
the sense inventory and the annotated data
set were constructed for a sample of 25 nouns.
The data set was divided into a trial set of 5
ambiguous nouns and a test set of 20 nouns.
The participants had to automatically deter-
mine the contextually appropriate translation
for a given English noun in each or a subset
of the five target languages. Only translations
present in Europarl were considered as valid
translations.
The remainder of this article is organized as
follows. Section 2 focuses on the task descrip-
tion and gives a short overview of the construc-
tion of the sense inventory and the annotation
of the benchmark data set with the senses from
the multilingual sense inventory. Section 3
clarifies the scoring metrics and presents two
frequency-based baselines. The participating
systems are presented in Section 4, while the
results of the task are discussed in Section 5.
Section 6 concludes this paper.
2 Task setup
2.1 Data sets
Two types of data sets were used in the
Cross-lingual WSD task: (a) a parallel corpus
on the basis of which the gold standard sense
inventory was created and (b) a collection of
English sentences containing the lexical sam-
ple words annotated with their contextually
appropriate translations in five languages.
1
http://www.statmt.org/europarl/
15
Below, we provide a short summary of the
complete data construction process. For a
more detailed description, we refer to Lefever
and Hoste (2009; 2010).
The gold standard sense inventory was
derived from the Europarl parallel corpus
2
,
which is extracted from the proceedings of the
European Parliament (Koehn, 2005). We se-
lected 6 languages from the 11 European lan-
guages represented in the corpus, viz. English
(our target language), Dutch, French, Ger-
man, Italian and Spanish. All data were al-
ready sentence-aligned using a tool based on
the Gale and Church (1991) algorithm, which
was part of the Europarl corpus. We only con-
sidered the 1-1 sentence alignments between
English and the five other languages. These
sentence alignments were made available to
the task participants for the five trial words.
The sense inventory extracted from the paral-
lel data set (Section 2.2) was used to annotate
the sentences in the trial set and the test set,
which were extracted from the JRC-ACQUIS
Multilingual Parallel Corpus
3
and BNC
4
.
2.2 Creation of the sense inventory
Two steps were taken to obtain a multilingual
sense inventory: (1) word alignment on the
sentences to find the set of possible transla-
tions for the set of ambiguous nouns and (2)
clustering by meaning (per target word) of the
resulting translations.
GIZA++ (Och and Ney, 2003) was used to
generate the initial word alignments, which
were manually verified by certified translators
in all six involved languages. The human an-
notators were asked to assign a ?NULL? link
to words for which no valid translation could
be identified. Furthermore, they were also
asked to provide extra information on com-
pound translations (e.g. the Dutch word In-
vesteringsbank as a translation of the English
multiword Investment Bank), fuzzy links, or
target words with a different PoS (e.g. the verb
to bank).
The manually verified translations were
clustered by meaning by one annotator. In
order to do so, the translations were linked
2
http://www.statmt.org/europarl/
3
http://wt.jrc.it/lt/Acquis/
4
http://www.natcorp.ox.ac.uk/
across languages on the basis of unique
sentence IDs. After the selection of all
unique translation combinations, the transla-
tions were grouped into clusters. The clus-
ters were organized in two levels, in which
the top level reflects the main sense categories
(e.g. for the word coach we have (1) (sports)
manager, (2) bus, (3) carriage and (4) part
of a train), and the subclusters represent the
finer sense distinctions. Translations that cor-
respond to English multiword units were iden-
tified and in case of non-apparent compounds,
i.e. compounds which are not marked with a
?-?, the different compound parts were sepa-
rated by ?? in the clustering file (e.g. the Ger-
man Post??kutsche). All clustered translations
were also manually lemmatized.
2.3 Sense annotation of the test data
The resulting sense inventory was used to an-
notate the sentences in the trial set (20 sen-
tences per ambiguous word) and the test set
(50 sentences per ambiguous word). In total,
1100 sentences were annotated. The annota-
tors were asked to (a) pick the contextually ap-
propriate sense cluster and to (b) choose their
three preferred translations from this cluster.
In case they were not able to find three ap-
propriate translations, they were also allowed
to provide fewer. These potentially differ-
ent translations were used to assign frequency
weights (shown in example (2)) to the gold
standard translations per sentence. The ex-
ample (1) below shows the annotation result in
both German and Dutch for an English source
sentence containing coach.
(1) SENTENCE 12. STRANGELY , the na-
tional coach of the Irish teams down the
years has had little direct contact with the
four provincial coaches .
German 1: Nationaltrainer
German 2: Trainer
German 3: Coach
Dutch 1: trainer
Dutch 2: coach
Dutch 3: voetbaltrainer
For each instance, the gold standard that
results from the manual annotation contains
a set of translations that are enriched with
16
frequency information. The format of both
the input file and gold standard is similar to
the format that will be used for the Sem-
Eval Cross-Lingual Lexical Substitution task
(Sinha and Mihalcea, 2009). The following
example illustrates the six-language gold stan-
dard format for the trial sentence in (1). The
first field contains the target word, PoS-tag
and language code, the second field contains
the sentence ID and the third field contains the
gold standard translations in the target lan-
guage, enriched with their frequency weight:
(2) coach.n.nl 12 :: coach 3; speler-trainer 1;
trainer 3; voetbaltrainer 1;
coach.n.fr 12 :: capitaine 1; entra??neur 3;
coach.n.de 12 :: Coach 1; Fu?baltrainer 1;
Nationaltrainer 2; Trainer 3;
coach.n.it 12 :: allenatore 3;
coach.n.es 12 :: entrenador 3;
3 Evaluation
3.1 Scoring
To score the participating systems, we use an
evaluation scheme which is inspired by the
English lexical substitution task in SemEval
2007 (McCarthy and Navigli, 2007). We per-
form both a best result evaluation and a more
relaxed evaluation for the top five results. The
evaluation is performed using precision and re-
call (Prec and Rec in the equations below),
and Mode precision (M
P
) and Mode recall
(M
R
), where we calculate precision and re-
call against the translation that is preferred by
the majority of annotators, provided that one
translation is more frequent than the others.
For the precision and recall formula we use
the following variables. Let H be the set of
annotators, T the set of test items and h
i
the
set of responses for an item i ? T for annota-
tor h ? H. For each i ? T we calculate the
mode (m
i
) which corresponds to the transla-
tion with the highest frequency weight. For
a detailed overview of the M
P
and M
R
cal-
culations, we refer to McCarthy and Navigli
(2007). Let A be the set of items from T (and
TM) where the system provides at least one
answer and a
i
: i ? A the set of guesses from
the system for item i. For each i, we calculate
the multiset union (H
i
) for all h
i
for all h ? H
and for each unique type (res) in H
i
that has
an associated frequency (freq
res
). In order to
assign frequency weights to our gold standard
translations, we asked our human annotators
to indicate their top 3 translations, which en-
ables us to also obtain meaningful associated
frequencies (freq
res
) viz. ?1? in case a transla-
tion is picked by 1 annotator, ?2? if picked by
two annotators and ?3? if chosen by all three
annotators.
Best result evaluation For the best re-
sult evaluation, systems can propose as many
guesses as the system believes are correct, but
the resulting score is divided by the number of
guesses. In this way, systems that output a lot
of guesses are not favoured.
Prec =
?
a
i
:i?A
?
res?a
i
freq
res
|a
i
|
|H
i
|
|A|
(1)
Rec =
?
a
i
:i?T
?
res?a
i
freq
res
|a
i
|
|H
i
|
|T |
(2)
Out-of-five (Oof) evaluation For the
more relaxed evaluation, systems can propose
up to five guesses. For this evaluation, the
resulting score is not divided by the number
of guesses.
Prec =
?
a
i
:i?A
?
res?a
i
freq
res
|H
i
|
|A|
(3)
Rec =
?
a
i
:i?T
?
res?a
i
freq
res
|H
i
|
|T |
(4)
3.2 Baselines
We produced two frequency-based baselines:
1. For the Best result evaluation, we select
the most frequent lemmatized translation
that results from the automated word
alignment process (GIZA++).
2. For the Out-of-five or more relaxed eval-
uation, we select the five most fre-
quent (lemmatized) translations that re-
sult from the GIZA++ alignment.
Table 1 shows the baselines for the Best
evaluation, while Table 2 gives an overview
per language of the baselines for the Out-of-
five evaluation.
17
Prec Rec M
P
M
R
Spanish 18.36 18.36 23.38 23.38
French 20.71 20.71 15.21 15.21
Italian 14.03 14.03 11.23 11.23
Dutch 15.69 15.69 8.71 8.71
German 13.16 13.16 6.95 6.95
Table 1: Best Baselines
Prec Rec M
P
M
R
Spanish 48.41 48.41 42.62 42.62
French 45.99 45.99 36.45 36.45
Italian 34.51 34.51 29.70 29.70
Dutch 37.43 37.43 24.58 24.58
German 32.89 32.89 29.80 29.80
Table 2: Out-of-five Baselines
4 Systems
We received sixteen submissions from five dif-
ferent participating teams. One group tack-
led all five target languages, whereas the other
groups focused on four (one team), two (one
team) or one (two teams) target language(s).
For both the best and the Out-of-five evalua-
tion tasks, there were between three and seven
participating systems per language.
The OWNS system identifies the nearest
neighbors of the test instances from the train-
ing data using a pairwise similarity measure
(weighted sum of the word overlap and se-
mantic overlap between two sentences). They
use WordNet similarity measures as an ad-
ditional information source, while the other
teams merely rely on parallel corpora to ex-
tract all lexical information. The UvT-WSD
systems use a k-nearest neighbour classifier
in the form of one word expert per lemma?
Part-of-Speech pair to be disambiguated. The
classifier takes as input a variety of local
and global context features. Both the FCC-
WSD and T3-COLEUR systems use bilingual
translation probability tables that are derived
from the Europarl corpus. The FCC-WSD
system uses a Naive Bayes classifier, while
the T3-COLEUR system uses an unsupervised
graph-based method. Finally, the UHD sys-
tems build for each target word a multilin-
gual co-occurrence graph based on the target
word?s aligned contexts found in parallel cor-
pora. The cross-lingual nodes are first linked
by translation edges, that are labeled with the
translations of the target word in the corre-
sponding contexts. The graph is transformed
into a minimum spanning tree which is used
to select the most relevant words in context to
disambiguate a given test instance.
5 Results
For the system evaluation results, we show
precision (Prec), recall (Rec), Mode precision
(M
P
) and Mode recall (M
R
). We ranked all
system results according to recall, as was done
for the Lexical Substitution task. Table 3
shows the system ranking on the best task,
while Table 4 shows the results for the Oof
task.
Prec Rec M
P
M
R
Spanish
UvT-v 23.42 24.98 24.98 24.98
UvT-g 19.92 19.92 24.17 24.17
T3-COLEUR 19.78 19.59 24.59 24.59
UHD-1 20.48 16.33 28.48 22.19
UHD-2 20.2 16.09 28.18 22.65
FCC-WSD1 15.09 15.09 14.31 14.31
FCC-WSD3 14.43 14.43 13.41 13.41
French
T3-COLEUR 21.96 21.73 16.15 15.93
UHD-2 20.93 16.65 17.78 14.15
UHD-1 20.22 16.21 17.59 14.56
OWNS2 16.05 16.05 14.21 14.21
OWNS1 16.05 16.05 14.21 14.21
OWNS3 12.53 12.53 14.21 14.21
OWNS4 10.49 10.49 14.21 14.21
Italian
T3-COLEUR 15.55 15.4 10.2 10.12
UHD-2 16.28 13.03 14.89 9.46
UHD-1 15.94 12.78 12.34 8.48
Dutch
UvT-v 17.7 17.7 12.05 12.05
UvT-g 15.93 15.93 10.54 10.54
T3-COLEUR 10.71 10.56 6.18 6.16
German
T3-COLEUR 13.79 13.63 8.1 8.1
UHD-1 12.2 9.32 11.05 7.78
UHD-2 12.03 9.23 12.91 9.22
Table 3: Best System Results
Beating the baseline seems to be quite chal-
lenging for this WSD task. While the best sys-
tems outperform the baseline for the best task,
18
Prec Rec M
P
M
R
Spanish
UvT-g 43.12 43.12 43.94 43.94
UvT-v 42.17 42.17 40.62 40.62
FCC-WSD2 40.76 40.76 44.84 44.84
FCC-WSD4 38.46 38.46 39.49 39.49
T3-COLEUR 35.84 35.46 39.01 38.78
UHD-1 38.78 31.81 40.68 32.38
UHD-2 37.74 31.3 39.09 32.05
French
T3-COLEUR 49.44 48.96 42.13 41.77
OWNS1 43.11 43.11 38.29 38.29
OWNS2 38.74 38.74 37.73 37.73
UHD-1 39.06 32 37.00 26.79
UHD-2 37.92 31.38 37.66 27.08
Italian
T3-COLEUR 40.7 40.34 38.99 38.70
UHD-1 33.72 27.49 27.54 21.81
UHD-2 32.68 27.42 29.82 23.20
Dutch
UvT-v 34.95 34.95 24.62 24.62
UvT-g 34.92 34.92 19.72 19.72
T3-COLEUR 21.47 21.27 12.05 12.03
German
T3-COLEUR 33.21 32.82 33.60 33.56
UHD-1 27.62 22.82 25.68 21.16
UHD-2 27.24 22.55 27.19 22.30
Table 4: Out-of-five System Results
this is not always the case for the Out-of-five
task. This is not surprising though, as the Oof
baseline contains the five most frequent Eu-
roparl translations. As a consequence, these
translations usually contain the most frequent
translations from different sense clusters, and
in addition they also contain the most generic
translation that often covers multiple senses of
the target word.
The best results are achieved by the UvT-
WSD (Spanish, Dutch) and ColEur (French,
Italian and German) systems. An interest-
ing feature that these systems have in com-
mon, is that they extract all lexical informa-
tion from the parallel corpus at hand, and do
not need any additional data sources. As a
consequence, the systems can easily be applied
to other languages as well. This is clearly il-
lustrated by the ColEur system, that partici-
pated for all supported languages, and outper-
formed the other systems for three of the five
languages.
In general, we notice that Spanish and
French have the highest scores, followed by
Italian, whereas Dutch and German seem to be
more challenging. The same observation can
be made for both the Oof and Best results,
except for Italian that performs worse than
Dutch for the latter. However, given the low
participation rate for Italian, we do not have
sufficient information to explain this different
behaviour on the two tasks. The discrepancy
between the performance figures for Spanish
and French on the one hand, and German and
Dutch on the other hand, seems more readily
explicable. A likely explanation could be the
number of classes (or translations) the systems
have to choose from. As both Dutch and Ger-
man are characterized by a rich compound-
ing system, these compound translations also
result in a higher number of different trans-
lations. Figure 1 illustrates this by listing
the number of different translations (or classes
in the context of WSD) for all trial and test
words. As a result, the broader set of trans-
lations makes the WSD task, that consists
in choosing the most appropriate translation
from all possible translations for a given in-
stance, more complicated for Dutch and Ger-
man.
6 Concluding remarks
We believe that the Cross-lingual Word Sense
Disambiguation task is an interesting contri-
bution to the domain, as it attempts to ad-
dress two WSD problems which have received
a lot of attention lately, namely (1) the scarcity
of hand-crafted sense inventories and sense-
tagged corpora and (2) the need to make WSD
more suited for practical applications.
The system results lead to the following ob-
servations. Firstly, languages which make ex-
tensive use of single word compounds seem
harder to tackle, which is also reflected in the
baseline scores. A possible explanation for
this phenomenon could lie in the number of
translations the systems have to choose from.
Secondly, it is striking that the systems with
the highest performance solely rely on paral-
lel corpora as a source of information. This
would seem very promising for future multi-
lingual WSD research; by eliminating the need
19
Figure 1: Number of different translations per word for Dutch, French, Spanish, Italian and
German.
for external information sources, these sys-
tems present a more flexible and language-
independent approach to WSD.
References
E. Agirre and P. Edmonds, editors. 2006. Word
Sense Disambiguation. Text, Speech and Lan-
guage Technology. Springer, Dordrecht.
E. Agirre, B. Magnini, O. Lopez de Lacalle,
A. Otegi, G. Rigau, and P. Vossen. 2007.
Semeval-2007 task01: Evaluating wsd on cross-
language information retrieval. In Proceedings
of CLEF 2007 Workshop, pp. 908 - 917. ISSN:
1818-8044. ISBN: 2-912335-31-0.
M. Apidianaki. 2009. Data-driven semantic anal-
ysis for multilingual wsd and lexical selection in
translation. In Proceedings of the 12th Confer-
ence of the European Chapter of the Association
for Computational Linguistics (EACL), Athens,
Greece.
W.A. Gale and K.W. Church. 1991. A program
for aligning sentences in bilingual corpora. In
Computational Linguistics, pages 177?184.
W.A. Gale, K.W. Church, and D. Yarowsky. 1993.
A method for disambiguating word senses in a
large corpus. In Computers and the Humanities,
volume 26, pages 415?439.
N. Ide, T. Erjavec, and D. Tufis. 2002. Sense dis-
crimination with parallel corpora. In Proceed-
ings of ACL Workshop on Word Sense Disam-
biguation: Recent Successes and Future Direc-
tions, pages 54?60.
P. Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In Proceedings
of the MT Summit.
E. Lefever and V. Hoste. 2009. Semeval-2010
task 3: Cross-lingual word sense disambigua-
tion. In Proceedings of the NAACL-HLT 2009
Workshop: SEW-2009 - Semantic Evaluations,
pages 82?87, Boulder, Colorado.
E. Lefever and V. Hoste. 2010. Construction of a
benchmark data set for cross-lingual word sense
disambiguation. In Proceedings of the seventh
international conference on Language Resources
and Evaluation., Malta.
D. McCarthy and R. Navigli. 2007. Semeval-2007
task 10: English lexical substitution task. In
Proceedings of the 4th International Workshop
on Semantic Evaluations (SemEval-2007), pages
48?53, Prague, Czech Republic.
R. Navigli. 2009. Word sense disambiguation: a
survey. In ACM Computing Surveys, volume 41,
pages 1?69.
H.T. Ng, B. Wang, and Y.S. Chan. 2003. Exploit-
ing parallel texts for word sense disambiguation:
An empirical study. In Proceedings of the 41st
Annual Meeting of the Association for Compu-
tational Linguistics, pages 455?462, Santa Cruz.
F.J. Och and H. Ney. 2003. A systematic com-
parison of various statistical alignment models.
Computational Linguistics, 29(1):19?51.
McCarthy D. Sinha, R. D. and R. Mihalcea. 2009.
Semeval-2010 task 2: Cross-lingual lexical sub-
stitution. In Proceedings of the NAACL-HLT
2009 Workshop: SEW-2009 - Semantic Evalua-
tions, Boulder, Colorado.
20
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 158?166, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
SemEval-2013 Task 10: Cross-lingual Word Sense Disambiguation
Els Lefever1,2 and Ve?ronique Hoste1,3
1LT3, Language and Translation Technology Team, University College Ghent
Groot-Brittannie?laan 45, 9000 Gent, Belgium
2Department of Applied Mathematics, Computer Science and Statistics, Ghent University
Krijgslaan 281 (S9), 9000 Gent, Belgium
3Department of Linguistics, Ghent University
Blandijnberg 2, 9000 Gent, Belgium
{Els.Lefever,Veronique.Hoste}@hogent.be
Abstract
The goal of the Cross-lingual Word Sense Disam-
biguation task is to evaluate the viability of multilin-
gual WSD on a benchmark lexical sample data set.
The traditional WSD task is transformed into a mul-
tilingual WSD task, where participants are asked to
provide contextually correct translations of English
ambiguous nouns into five target languages, viz.
French, Italian, English, German and Dutch. We re-
port results for the 12 official submissions from 5
different research teams, as well as for the ParaSense
system that was developed by the task organizers.
1 Introduction
Lexical ambiguity remains one of the major prob-
lems for current machine translation systems. In
the following French sentence ?Je cherche des ide?es
pour manger de l?avocat?1, the word ?avocat? is
clearly referring to the fruit, whereas both Google
Translate2 as well as Babelfish3 translate the word
as ?lawyer?. Although ?lawyer? is a correct transla-
tion of the word ?avocat?, it is the wrong translation
in this context. Other language technology applica-
tions, such as Question Answering (QA) systems or
information retrieval (IR) systems, also suffer from
the poor contextual disambiguation of word senses.
Word sense disambiguation (WSD) is still consid-
ered one of the most challenging problems within
1English translation: ?I?m looking for ideas to eat avocado?.
2http://translate.google.com
3http://be.bing.com/translator/
language technology today. It requires the construc-
tion of an artificial text understanding as the sys-
tem should detect the correct word sense based on
the context of the word. Different methodologies
have been investigated to solve the problem; see for
instance Agirre and Edmonds (2006) and Navigli
(2009) for a detailed overview of WSD algorithms
and evaluation.
This paper reports on the second edition of
the ?Cross-Lingual Word Sense Disambiguation?
(CLWSD) task, that builds further on the insights we
gained from the SemEval-2010 evaluation (Lefever
and Hoste, 2010b) and for which new test data were
annotated. The task is an unsupervised Word Sense
Disambiguation task for English nouns, the sense
label of which is composed of translations in dif-
ferent target languages (viz. French, Italian, Span-
ish, Dutch and German). The sense inventory is
built up on the basis of the Europarl parallel corpus;
all translations of a polysemous word were manu-
ally grouped into clusters, which constitute different
senses of that given word. For the test data, native
speakers assigned a translation cluster(s) to each test
sentence and gave their top three translations from
the predefined list of Europarl translations, in order
to assign weights to the set of gold standard transla-
tions.
The decision to recast the more traditional mono-
lingual WSD task into a cross-lingual WSD task was
motivated by the following arguments. Firstly, using
multilingual unlabeled parallel corpora contributes
to clearing the data acquisition bottleneck for WSD,
because using translations as sense labels excludes
the need for manually created sense-tagged corpora
158
and sense inventories such as WordNet (Fellbaum,
1998) or EuroWordNet (Vossen, 1998). Moreover,
as there is fairly little linguistic knowledge involved,
the framework can be easily deployed for a variety
of different languages. Secondly, a cross-lingual ap-
proach also deals with the sense granularity prob-
lem; finer sense distinctions are only relevant as far
as they get lexicalized in different translations of
the word. If we take the English word ?head? as
an example, we see that this word is always trans-
lated as ?hoofd? in Dutch (both for the ?chief? and
for the ?body part? sense of the word). At the same
time, the subjectivity problem is tackled that arises
when lexicographers have to construct a fixed set of
senses for a particular word that should fit all possi-
ble domains and applications. In addition, the use
of domain-specific corpora allows to derive sense
inventories that are tailored towards a specific tar-
get domain or application and to train a dedicated
CLWSD system using these particular sense inven-
tories. Thirdly, working immediately with transla-
tions instead of more abstract sense labels allows to
bypass the need to map abstract sense labels to cor-
responding translations. This makes it easier to inte-
grate a dedicated WSD module into real multilingual
applications such as machine translation (Carpuat
and Wu, 2007) or information retrieval (Clough and
Stevenson, 2004).
Many studies have already shown the validity of a
cross-lingual approach to Word Sense Disambigua-
tion (Brown et al, 1991; Gale and Church, 1993;
Ng et al, 2003; Diab, 2004; Tufis? et al, 2004;
Chan and Ng, 2005; Specia et al, 2007; Apidi-
anaki, 2009). The Cross-lingual WSD task con-
tributes to this research domain by the construction
of a dedicated benchmark data set where the am-
biguous words were annotated with the senses from
a multilingual sense inventory extracted from a par-
allel corpus. This benchmark data sets allows a de-
tailed comparison between different approaches to
the CLWSD task.
The remainder of this paper is organized as fol-
lows. Section 2 focuses on the task description and
briefly recapitalizes the construction of the sense in-
ventory and the annotation procedure of the test sen-
tences. Section 3 presents the participating systems
to the task, whereas Section 4 gives an overview of
the experimental setup and results. Section 5 con-
cludes this paper.
2 Task set up
The ?Cross-lingual Word Sense Disambiguation?
(CLWSD) task was organized for the first time in the
framework of SemEval-2010 (Lefever and Hoste,
2010b) and resulted in 16 submissions from five
different research teams. Many additional research
teams showed their interest and downloaded the trial
data, but did not manage to finish their systems in
time. In order to gain more insights into the com-
plexity and the viability of cross-lingual WSD, we
proposed a second edition of the task for SemEval-
2013 for which new test data were annotated.
The CLWSD task is an unsupervised Word Sense
Disambiguation task for a lexical sample of twenty
English nouns. The sense label of the nouns is com-
posed of translations in five target languages (viz.
Spanish, French, German, Italian and Dutch) and
the sense inventory is built up on the basis of the
Europarl parallel corpus4. This section briefly de-
scribes the data construction process for the task.
For a more detailed description of the gold stan-
dard creation and data annotation process, we refer
to Lefever and Hoste (2010a; 2010b).
2.1 Sense inventory
The starting point for the gold standard sense inven-
tory creation was the parallel corpus Europarl. We
selected six languages from Europarl (English and
the five target languages) and only considered the 1-
1 sentence alignments between English and the five
target languages5. In order to obtain the multilingual
sense inventory we:
1. performed word alignment on the parallel cor-
pus in order to find all possible translations for
our set of ambiguous focus nouns
2. clustered the resulting translations by meaning
and manually lemmatized all translations
The resulting sense inventory was then used to an-
notate the sentences in the test set that was devel-
oped for the SemEval-2013 CLWSD task.
4http://www.statmt.org/europarl/
5This six-lingual sentence-aligned subcor-
pus of Europarl can be downloaded from
http://lt3.hogent.be/semeval/.
159
2.2 Test data
For the creation of the test data set, we manually se-
lected 50 sentences per ambiguous focus word from
the part of the ANC corpus that is publicly avail-
able6. In total, 1000 sentences were annotated us-
ing the sense inventory that was described in Sec-
tion 2.1. Three annotators per target language were
asked to first select the correct sense cluster and next
to choose the three contextually most appropriate
translations from this sense cluster. They could also
provide fewer translations in case they could not find
three good translations for this particular occurrence
of the test word. These translations were used to
(1) compose the set of gold standard translations per
test instance and (2) to assign frequency weights to
all translations in the gold standard (e.g. translations
that were chosen by all three annotators get a fre-
quency weight of 3 in the gold standard).
2.3 Evaluation tasks
Two subtasks were proposed for the Cross-lingual
WSD task: a best evaluation and an Out-of-five eval-
uation task. For the best evaluation, systems can
propose as many guesses as the system believes are
correct, but the score is divided by the number of
guesses. In case of the Out-of-five evaluation, sys-
tems can propose up to five guesses per test instance
without being penalized for wrong translation sug-
gestions. Both evaluation tasks are explained in
more detail in Section 4.1.
3 Systems
3.1 Systems participating to the official
CLWSD evaluation campaign
Five different research teams participated to the
CLWSD task and submitted up to three different
runs of their system, resulting in 12 different sub-
missions for the task. All systems took part in both
the best and the Out-of-five evaluation tasks. These
systems took very different approaches to solve the
task, ranging from statistical machine translation,
classification and sense clustering to topic model
based approaches.
The XLING team (Tan and Bond, 2013) submit-
ted three runs of their system for all five target lan-
guages. The first version of the system presents a
6http://www.americannationalcorpus.org/
topic matching and translation approach to CLWSD
(TnT run), where LDA is applied on the Europarl
sentences containing the ambiguous focus word in
order to train topic models. Each sentence in the
training corpus is assigned a topic that contains a
list of associated words with the topic. The topic
of the test sentence is then inferred and compared
to the matching training sentences by means of the
cosine similarity between the training and test vec-
tors. WordNet (WN) is used as a fallback in case
the system returns less than 5 answers. The second -
and best performing - flavor of the system (SnT run)
calculates the cosine similarity between the context
words of the test and training sentences. The out-
put of the system then contains the translation that
results from running word alignment on the focus
word in the training corpus. As a fallback, Word-
Net is again used. The WN senses are sorted by fre-
quency in the SemCor corpus and the correspond-
ing translation is selected from the aligned WordNet
in the target language. The third run of the system
(merged) combines the output from the other two
flavors of the system.
The LIMSI system (Apidianaki, 2013) applies an
unsupervised CLWSD method that was proposed in
(Apidianaki, 2009) for three target languages, viz.
Spanish, Italian and French. First, word alignment
is applied on the parallel corpus and three bilingual
lexicons are built, containing for each focus word
the translations in the three target languages. In a
next step, a vector is built for each translation of the
English focus word, using the cooccurrences of the
word in the sentences in which it gets this particu-
lar translation. A clustering algorithm then groups
the feature vectors using the Weighted Jaccard mea-
sure. New instances containing the ambiguous focus
word are then compared to the training feature vec-
tors and assigned to one of the sense clusters. In
case the highest-ranked translation in the cluster has
a score below the threshold, the system falls back to
the most frequent translation.
Two very well performing systems take a
classification-based approach to the CLWSD task:
the HLTDI and WSD2 systems. The HLTDI sys-
tem (Rudnick et al, 2013) performs word alignment
on the intersected Europarl corpus to locate train-
ing instances containing the ambiguous focus words.
The first flavor of the system (l1) uses a maxent clas-
160
sifier that is trained over local context features. The
L2 model (l2 run) also adds translations of the fo-
cus word into the four other target languages to the
feature vector. To disambiguate new test instances,
these translations into the four other languages are
estimated using the classifiers built in the first ver-
sion of the system (l1). The third system run (mrf )
builds a Markov network of L1 classifiers in order to
find the best translation into all five target languages
jointly. The nodes of this network correspond to the
distribution produced by the L1 classifiers, while the
edges contain pairwise potentials derived from the
joint probabilities of translation labels occurring to-
gether in the training data.
Another classification-based approach is pre-
sented by the WSD2 system (van Gompel and
van den Bosch, 2013), that uses a k-NN classifier
to solve the CLWSD task. The first configuration
of the system (c1l) uses local context features for a
window of three words containing the focus word.
Parameters were optimized on the trial data. The
second flavor of the system (c1lN) uses the same
configuration of the system, but without parameter
optimization. The third configuration of the system
(var) is heavily optimized on the trial data, selecting
the winning configuration per trial word and evalua-
tion metric. In addition to the local context features,
also global bag-of-word context features are consid-
ered for this version of the system.
A completely different approach is taken by the
NRC-SMT system (Carpuat, 2013), that uses a sta-
tistical machine translation approach to tackle the
CLWSD task. The baseline version of the system
(SMTbasic) represents a standard phrase-based SMT
baseline, that is trained only on the intersected Eu-
roparl corpus. Translations for the test instances are
extracted from the top hypothesis (for the best eval-
uation) or from the 100-best list (for the Out-of-five
evaluation). The optimized version of the system
(SMTadapt2) is trained on the Europarl corpus and
additional news data, and uses mixture models that
are developed for domain adaptation in SMT.
In addition to the five systems that participated to
the official evaluation campaign, the organizers also
present results for their ParaSense system, which is
described in the following section.
3.2 ParaSense system
The ParaSense system (Lefever et al, 2013)
is a multilingual classification-based approach to
CLWSD. A combination of both local context in-
formation and translational evidence is used to dis-
criminate between different senses of the word, the
underlying hypothesis being that using multilingual
information should be more informative than only
having access to monolingual or bilingual features.
The local context features contain the word form,
lemma, part-of-speech and chunk information for a
window of seven words containing the ambiguous
focus word. In addition, a set of bag-of-words fea-
tures is extracted from the aligned translations that
are not the target language of the classifier. Per
ambiguous focus word, a list of all content words
(nouns, adjectives, adverbs and verbs) that occurred
in the linguistically preprocessed aligned transla-
tions of the English sentences containing this word,
were extracted. Each content word then corresponds
to exactly one binary feature per language. For the
construction of the translation features for the train-
ing set, we used the Europarl aligned translations.
As we do not dispose of similar aligned transla-
tions for the test instances for which we only have
the English test sentences at our disposal, we used
the Google Translate API7 to automatically gener-
ate translations for all English test instances in the
five target languages.
As a classifier, we opted for the k Nearest neigh-
bor method as implemented in TIMBL (Daelemans
and van den Bosch, 2005). As most classifiers can
be initialized with a wide range of parameters, we
used a genetic algorithm to optimize the parameter
settings for our classification task.
4 Results
4.1 Experimental set up
Test set The lexical sample contains 50 English
sentences per ambiguous focus word. All instances
were manually annotated per language, which re-
sulted in a set of gold standard translation labels per
instance. For the construction of the test dataset, we
refer to Section 2.
7http://code.google.com/apis/language/
161
Evaluation metric The BEST precision and recall
metric was introduced by (McCarthy and Navigli,
2007) in the framework of the SemEval-2007 com-
petition. The metric takes into account the frequency
weights of the gold standard translations: transla-
tions that were picked by different annotators re-
ceived a higher associated frequency which is incor-
porated in the formulas for calculating precision and
recall. For the BEST precision and recall evaluation,
the system can propose as many guesses as the sys-
tem believes are correct, but the resulting score is
divided by the number of guesses. In this way, sys-
tems that output many guesses are not favored and
systems can maximize their score by guessing the
most frequent translation from the annotators. We
also calculate Mode precision and recall, where pre-
cision and recall are calculated against the transla-
tion that is preferred by the majority of annotators,
provided that one translation is more frequent than
the others.
The following variables are used for the BEST pre-
cision and recall formulas. Let H be the set of an-
notators, T the set of test words and hi the set of
translations for an item i ? T for annotator h ? H .
Let A be the set of words from T where the system
provides at least one answer and ai the set of guesses
from the system for word i ? A. For each i, we cal-
culate the multiset union (Hi) for all hi for all h ? H
and for each unique type (res) in Hi that has an as-
sociated frequency (freqres). Equation 1 lists the
BEST precision formula, whereas Equation 2 lists
the formula for calculating the BEST recall score:
Precision =
?
ai:i?A
?
res?ai
freqres
|ai|
|Hi|
|A|
(1)
Recall =
?
ai:i?T
?
res?ai
freqres
|ai|
|Hi|
|T |
(2)
Most Frequent translation baseline As a base-
line, we selected the most frequent lemmatized
translation that resulted from the automated word
alignment (GIZA++) for all ambiguous nouns in the
training data. This baseline is inspired by the most
frequent sense baseline often used in WSD evalu-
ations. The main difference between the most fre-
quent sense baseline and our baseline is that the lat-
ter is corpus-dependent: we do not take into account
the overall frequency of a word as it would be mea-
sured based on a large general purpose corpus, but
calculate the most frequent sense (or translation in
this case) based on our training corpus.
4.2 Experimental results
For the system evaluation results, we show preci-
sion and Mode precision figures for both evaluation
types (best and Out-of-five). In our case, precision
refers to the number of correct translations in rela-
tion to the total number of translations generated by
the system, while recall refers to the number of cor-
rect translations generated by the classifier. As all
participating systems predict a translation label for
all sentences in the test set, precision and recall will
give identical results. As a consequence, we do not
list the recall and Mode recall figures that are in this
case identical to the corresponding precision scores.
Table 1 lists the averaged best precision scores
for all systems, while Table 2 gives an overview
of the best Mode precision figures for all five tar-
get languages, viz. Spanish (Es), Dutch (Nl), Ger-
man (De), Italian (It) and French (Fr). We list scores
for all participating systems in the official CLWSD
evaluation campaign, as well as for the organiz-
ers? system ParaSense, that is not part of the offi-
cial SemEval competition. The best results for the
best precision evaluation are achieved by the NRC-
SMTadapt2 system for Spanish and by the WSD2
system for the other four target languages, closely
followed by the HLTDI system. The latter two sys-
tems also obtain the best results for the best Mode
precision metric.
Table 3 lists the averaged Out-of-five precision
scores for all systems, while Table 4 gives an
overview of the Out-of-five Mode precision figures
for all five target languages, viz. Spanish (Es), Dutch
(Nl), German (De), Italian (It) and French (Fr). For
the Out-of-five evaluation, where systems are al-
lowed to generate up to five unique translations with-
out being penalized for wrong translations, again the
HLTDI and WSD2 systems obtain the best classifi-
cation performance.
Although the winning systems use different ap-
proaches (statistical machine translation and classi-
162
fication algorithms), they have in common that they
only use a parallel corpus to extract disambiguating
information, and do not use external resources such
as WordNet. As a consequence, this makes the sys-
tems very flexible and language-independent. The
ParaSense system, that incorporates translation in-
formation from four other languages, outperforms
all other systems, except for the best precision met-
ric in Spanish, where the NRC-SMT system obtains
the overall best results. This confirms the hypothe-
sis that a truly multilingual approach to WSD, which
incorporates translation information from multiple
languages into the feature vector, is more effective
than only using monolingual or bilingual features.
A possible explanation could be that the differences
between the different languages that are integrated
in the feature vector enable the system to refine
the obtained sense distinctions. We indeed see that
the ParaSense system outperforms the classification-
based bilingual approaches which exploit similar in-
formation (e.g. training corpora and machine learn-
ing algorithms).
Es Nl De It Fr
Baseline
23.23 20.66 17.43 20.21 25.74
results for the HLTDI system
hltdi-l1 29.01 21.53 19.50 24.52 27.01
hltdi-l2 28.49 22.36 19.92 23.94 28.23
hltdi-mrf 29.36 21.61 19.76 24.62 27.46
results for the XLING system
merged 11.09 4.91 4.08 6.93 9.57
snt 19.59 9.89 8.13 12.74 17.33
tnt 18.60 7.40 5.29 10.70 16.48
results for the LIMSI system
limsi 24.70 21.20 24.56
results for the NRC-SMT system
basic 27.24
adapt2 32.16
results for the WSD2 system
c1l 28.40 23.14 20.70 25.43 29.88
c1lN 28.65 23.61 20.82 25.66 30.11
var 23.31 17.17 16.20 20.38 25.89
results for the PARASENSE system
31.72 25.29 24.54 28.15 31.21
Table 1: BEST precision scores averaged over all twenty
test words for Spanish (Es), Dutch (Nl), German (De),
Italian (It) and French (Fr).
Es Nl De It Fr
Baseline
27.48 24.15 15.30 19.88 20.19
results for the HLTDI system
hltdi-l1 36.32 25.39 24.16 26.52 21.24
hltdi-l2 37.11 25.34 24.74 26.65 21.07
hltdi-mrf 36.57 25.72 24.01 26.26 21.24
results for the XLING system
merged 24.31 8.54 5.82 7.54 11.63
snt 21.36 9.56 10.36 11.27 11.57
tnt 24.31 8.54 5.82 7.54 11.63
results for the LIMSI system
limsi 32.09 23.06 22.16
results for the NRC-SMT system
basic 32.28
adapt2 36.2
results for the WSD2 system
c1l 33.89 26.32 24.73 31.61 26.62
c1lN 33.70 27.96 24.27 30.67 25.27
var 27.98 18.74 21.74 20.69 16.71
results for the PARASENSE system
40.26 30.29 25.48 30.11 26.33
Table 2: BEST Mode precision scores averaged over all
twenty test words for Spanish (Es), Dutch (Nl), German
(De), Italian (It) and French (Fr).
Es Nl De It Fr
Baseline
53.07 43.59 38.86 42.63 51.36
results for the HLTDI system
hltdi-l1 61.69 46.55 43.66 53.57 57.76
hltdi-l2 59.51 46.36 42.32 53.05 58.20
hltdi-mrf 9.89 5.69 4.15 3.91 7.11
results for the XLING system
merged 43.76 24.30 19.83 33.95 38.15
snt 44.83 27.11 23.71 32.38 38.44
tnt 39.52 23.27 19.13 33.28 35.30
results for the LIMSI system
limsi 49.01 40.25 45.37
results for the NRC-SMT system
basic 37.98
adapt2 41.65
results for the WSD2 system
c1l 58.23 47.83 43.17 52.22 59.07
c1lN 57.62 47.62 43.24 52.73 59.80
var 55.70 46.85 41.46 51.18 59.19
Table 3: OUT-OF-FIVE precision scores averaged over all
twenty test words for Spanish (Es), Dutch (Nl), German
(De), Italian (It) and French (Fr).
163
Es Nl De It Fr
Baseline
57.35 41.97 44.35 41.69 47.42
results for the HLTDI system
hltdi-l1 64.65 47.34 53.50 56.61 51.96
hltdi-l2 62.52 44.06 49.03 54.06 53.57
hltdi-mrf 11.39 5.09 3.14 3.87 7.79
results for the XLING system
merged 48.63 23.64 24.64 31.74 30.11
snt 50.04 27.30 30.57 29.17 32.45
tnt 44.96 22.98 23.54 29.61 28.02
results for the LIMSI system
limsi 51.41 47.21 39.54
results for the NRC-SMT system
basic 42.92
adapt2 45.38
results for the WSD2 system
c1l 63.75 45.27 50.11 54.13 57.57
c1lN 63.80 44.53 50.26 54.37 56.40
var 61.51 41.82 49.23 54.73 54.97
Table 4: OUT-OF-FIVE Mode precision scores averaged
over all twenty test words for Spanish (Es), Dutch (Nl),
German (De), Italian (It) and French (Fr).
In general, we notice that French and Spanish
have the highest scores, while Dutch and German
seem harder to tackle. Italian is situated some-
where in between the Romance and Germanic lan-
guages. This trend confirms the results that were ob-
tained during the first SemEval Cross-lingual WSD
task (Lefever and Hoste, 2010b). As pointed out af-
ter the first competition, the discrepancy between the
scores for the Romance and Germanic languages can
probably be explained by the number of classes (or
translations in this case) the systems have to choose
from. Germanic languages are typically charac-
terized by a very productive compounding system,
where compounds are joined together in one ortho-
graphic unit, which results in a much higher number
of different class labels. As the Romance languages
typically write compounds in separate orthographic
units, they dispose of a smaller number of different
translations for each ambiguous noun.
We can also notice large differences between the
scores for the individual words. Figure 1 illustrates
this by showing the best precision scores in Span-
ish for the different test words for the best run per
system. Except for some exceptions (e.g. coach in
the NRC-SMT system), most system performance
scores follow a similar curve. Some words (e.g.
match, range) are particularly hard to disambiguate,
while others obtain very high scores (e.g. mission,
soil). One possible explanation for the very good
scores for some words (e.g. soil) can be attributed
to a very generic translation which accounts for all
senses of the word even though there might be more
suitable translations for each of the senses depend-
ing on the context. Because the manual annota-
tors were able to select three good translations for
each test instance, the most generic translation is of-
ten part of the gold standard translations. This is
also reflected in the high baseline scores for these
words. For the words performing badly in most sys-
tems, an inspection of the training data properties
revealed two possible explanations for these poor
classification results. Firstly, there seems to be a
link with the number of training instances, corre-
sponding to the frequency of the word in the train-
ing corpus. Both for coach and match, two words
consistently performing bad in all systems, there are
very few training examples in the corpus (66 and
109 respectively). This could also explain why the
NRC-SMT system, that also uses additional paral-
lel data, achieves better results for coach than all
other systems. Secondly, the ambiguity or number
of valid translations per word in the training data
also seems to play a role in the classification results.
Both job and range appear very hard to classify cor-
rectly, and both words are very ambiguous, with no
fewer than 121 and 125 translations, respectively, to
choose from in Spanish.
5 Conclusion
The Cross-lingual Word Sense Disambiguation task
attempts to address three important challenges for
WSD, namely (1) the data acquisition bottleneck,
which is caused by the lack of manually created re-
sources, (2) the sense granularity and subjectivity
problem of the existing sense inventories and (3) the
need to make WSD more suited for practical appli-
cations. The task contributes to the WSD research
domain by the construction of a dedicated bench-
mark data set that allows to compare different ap-
proaches to the Cross-lingual WSD task.
The evaluation results lead to the following ob-
servations. Firstly, languages which make exten-
164
Figure 1: Spanish best precision scores for all systems per ambiguous focus word.
sive use of single word compounds seem harder
to tackle, which can probably be explained by the
higher number of translations these classifiers have
to choose from. Secondly, we can notice large dif-
ferences between the performances of the individual
test words. For the words that appear harder to dis-
ambiguate, both the number of training instances as
well as the ambiguity of the word seem to play a role
for the classification performance. Thirdly, both the
ParaSense system as well as the two winning sys-
tems from the competition extract all disambiguat-
ing information from the parallel corpus and do not
use any external resources. As a result, these sys-
tems are very flexible and can be easily extended to
other languages and domains. In addition, the good
scores of the ParaSense system, that incorporates in-
formation from four additional languages, confirms
the hypothesis that a truly multilingual approach is
an effective way to tackle the CLWSD task.
Acknowledgments
We would like to thank all annotators for their hard
work.
References
Eneko Agirre and Philip Edmonds. 2006. Word Sense
Disambiguation. Algorithms and Applications. Text,
Speech and Language Technology. Springer.
M. Apidianaki. 2009. Data-driven semantic analysis for
multilingual WSD and lexical selection in translation.
In Proceedings of the 12th Conference of the European
Chapter of the Association for Computational Linguis-
tics (EACL), pages 77?85, Athens, Greece.
Marianna Apidianaki. 2013. LIMSI : Cross-lingual
Word Sense Disambiguation using Translation Sense
Clustering. In Proceedings of the 7th International
Workshop on Semantic Evaluation (SemEval 2013), in
conjunction with the Second Joint Conference on Lex-
ical and Computational Semantcis (*SEM 2013), At-
lanta, USA.
P.F. Brown, S.A.D. Pietra, V.J.D. Pietra, and R.L. Mer-
cer. 1991. Word-sense disambiguation using statisti-
cal methods. In Proceedings of the 29th Annual Meet-
ing of the Association for Computational Linguistics,
pages 264?270, Berkeley, California.
M. Carpuat and D. Wu. 2007. Improving statistical
machine translation using word sense disambiguation.
In Proceedings of the 2007 Joint Conference on Em-
pirical Methods in Natural Language Processing and
Computational Natural Language Learning (EMNLP-
CoNLL), pages 61?72, Prague, Czech Republic.
Marine Carpuat. 2013. NRC: A Machine Translation
Approach to Cross-Lingual Word Sense Disambigua-
tion (SemEval-2013 Task 10). In Proceedings of the
7th International Workshop on Semantic Evaluation
(SemEval 2013), in conjunction with the Second Joint
Conference on Lexical and Computational Semantcis
(*SEM 2013), Atlanta, USA.
165
Y.S. Chan and H.T. Ng. 2005. Scaling Up Word Sense
Disambiguation via Parallel Texts. In Proceedings of
the 20th National Conference on Artificial Intelligence
(AAAI 2005), pages 1037?1042, Pittsburgh, Pennsyl-
vania, USA.
P. Clough and M. Stevenson. 2004. Cross-language in-
formation retrieval using eurowordnet and word sense
disambiguation. In Advances in Information Retrieval,
26th European Conference on IR Research (ECIR),
pages 327?337, Sunderland, UK.
W. Daelemans and A. van den Bosch. 2005. Memory-
based Language Processing. Cambridge University
Press.
M. Diab. 2004. Word Sense Disambiguation within a
Multilingual Framework. Phd, University of Mary-
land, USA.
C. Fellbaum. 1998. WordNet: An Electronic Lexical
Database. MIT Press.
W.A. Gale and K.W. Church. 1993. A program for align-
ing sentences in bilingual corpora. Computational
Linguistics, 19(1):75?102.
E. Lefever and V. Hoste. 2010a. Construction
of a Benchmark Data Set for Cross-Lingual Word
Sense Disambiguation. In Nicoletta Calzolari, Khalid
Choukri, Bente Maegaard, Joseph Mariani, Jan Odijk,
Stelios Piperidis, and Daniel Tapias, editors, Proceed-
ings of the seventh International Conference on Lan-
guage Resources and Evaluation (LREC?10), Valletta,
Malta, May. European Language Resources Associa-
tion (ELRA).
E. Lefever and V. Hoste. 2010b. SemEval-2010 Task
3: Cross-Lingual Word Sense Disambiguation. In
Proceedings of the 5th International Workshop on Se-
mantic Evaluation, ACL 2010, pages 15?20, Uppsala,
Sweden.
E. Lefever, V. Hoste, and M. De Cock. 2013. Five
languages are better than one: an attempt to bypass
the data acquisition bottleneck for wsd. In In Alexan-
der Gelbukh (ed.), CICLing 2013, Part I, LNCS 7816,
pages 343?354. Springer-Verlag Berlin Heidelberg.
D. McCarthy and R. Navigli. 2007. SemEval-2007 Task
10: English Lexical Substitution Task. In Proceedings
of the 4th International Workshop on Semantic Eval-
uations (SemEval-2007), pages 48?53, Prague, Czech
Republic.
R. Navigli. 2009. Word Sense Disambiguation: a Sur-
vey. ACM Computing Surveys, 41(2):1?69.
H.T. Ng, B. Wang, and Y.S. Chan. 2003. Exploiting par-
allel texts for word sense disambiguation: An empiri-
cal study. In 41st Annual Meeting of the Association
for Computational Linguistics (ACL), pages 455?462,
Sapporo, Japan.
Alex Rudnick, Can Liu, and Michael Gasser. 2013.
HLTDI: CL-WSD Using Markov Random Fields for
SemEval-2013 Task 10. In Proceedings of the 7th
International Workshop on Semantic Evaluation (Se-
mEval 2013), in conjunction with the Second Joint
Conference on Lexical and Computational Semantcis
(*SEM 2013), Atlanta, USA.
L. Specia, M.G.V. Nunes, and M. Stevenson. 2007.
Learning Expressive Models for Word Sense Disam-
biguation. In Proceedings of the 45th Annual Meeting
of the Association of Computational Linguistics, pages
41?48, Prague, Czech Republic.
Liling Tan and Francis Bond. 2013. XLING: Match-
ing Query Sentences to a Parallel Corpus using Topic
Models for WSD. In Proceedings of the 7th Inter-
national Workshop on Semantic Evaluation (SemEval
2013), in conjunction with the Second Joint Confer-
ence on Lexical and Computational Semantcis (*SEM
2013), Atlanta, USA.
D. Tufis?, R. Ion, and N. Ide. 2004. Fine-Grained
Word Sense Disambiguation Based on Parallel Cor-
pora, Word Alignment, Word Clustering and Aligned
Wordnets. In Proceedings of the 20th International
Conference on Computational Linguistics (COLING
2004), pages 1312?1318, Geneva, Switzerland, Au-
gust. Association for Computational Linguistics.
Maarten van Gompel and Antal van den Bosch. 2013.
Parameter optimisation for Memory-based Cross-
Lingual Word-Sense Disambiguation. In Proceedings
of the 7th International Workshop on Semantic Eval-
uation (SemEval 2013), in conjunction with the Sec-
ond Joint Conference on Lexical and Computational
Semantcis (*SEM 2013), Atlanta, USA.
P. Vossen, editor. 1998. EuroWordNet: a multilingual
database with lexical semantic networks. Kluwer Aca-
demic Publishers, Norwell, MA, USA.
166
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 36?44,
Dublin, Ireland, August 23-24, 2014.
SemEval-2014 Task 5: L2 Writing Assistant
Maarten van Gompel, Iris Hendrickx,
Antal van den Bosch
Centre for Language Studies,
Radboud University Nijmegen,
The Netherlands
proycon@anaproy.nl,
i.hendrickx@let.ru.nl,
a.vandenbosch@let.ru.nl
Els Lefever and V
?
eronique Hoste
LT3,
Language and Translation Technology Team,
Ghent University,
Belgium
els.lefever@ugent.be,
veronique.hoste@ugent.be
Abstract
We present a new cross-lingual task for
SemEval concerning the translation of
L1 fragments in an L2 context. The
task is at the boundary of Cross-Lingual
Word Sense Disambiguation and Machine
Translation. It finds its application in the
field of computer-assisted translation, par-
ticularly in the context of second language
learning. Translating L1 fragments in an
L2 context allows language learners when
writing in a target language (L2) to fall
back to their native language (L1) when-
ever they are uncertain of the right word
or phrase.
1 Introduction
We present a new cross-lingual and application-
oriented task for SemEval that is situated in the
area where Word Sense Disambiguation and Ma-
chine Translation meet. Finding the proper trans-
lation of a word or phrase in a given context is
much like the problem of disambiguating between
multiple senses.
In this task participants are asked to build a
translation/writing assistance system that trans-
lates specifically marked L1 fragments in an L2
context to their proper L2 translation. This type
of translation can be applied in writing assistance
systems for language learners in which users write
in a target language, but are allowed to occasion-
ally back off to their native L1 when they are un-
certain of the proper lexical or grammatical form
in L2. The task concerns the NLP back-end rather
than any user interface.
Full-on machine translation typically concerns
the translation of complete sentences or texts from
This work is licensed under a Creative
Commons Attribution 4.0 International Licence:
http://creativecommons.org/licenses/by/4.0/
L1 to L2. This task, in contrast, focuses on smaller
fragments, side-tracking the problem of full word
reordering.
We focus on the following language combi-
nations of L1 and L2 pairs: English-German,
English-Spanish, French-English and Dutch-
English. Task participants could participate for all
language pairs or any subset thereof.
2 Task Description
We frame the task in the context of second lan-
guage learning, yielding a specific practical appli-
cation.
Participants build a translation assistance sys-
tem rather than a full machine translation system.
The L1 expression, a word or phrase, is translated
by the system to L2, given the L2 context already
present, including right-side context if available.
The aim here, as in all translation, is to carry the
semantics of the L1 fragment over to L2 and find
the most suitable L2 expression given the already
present L2 context.
Other than a limit on length (6 words), we do
not pose explicit constraints on the kinds of L1
fragments allowed. The number of L1 fragments
is limited to one fragment per sentence.
The task addresses both a core problem of
WSD, with cross-lingual context, and a sub-
problem of Phrase-based Statistical Machine
Translation; that of finding the most suitable trans-
lation of a word or phrase. In MT this would be
modelled by the translation model. In our task
the full complexity of full-sentential translation
is bypassed, putting the emphasis on the seman-
tic aspect of translation. Our task has specific
practical applications and a specific intended au-
dience, namely intermediate and advanced second
language learners, whom one generally wants to
encourage to use their target language as much as
possible, but who may often feel the need to fall
back to their native language.
36
Currently, language learners are forced to fall
back to a bilingual dictionary when in doubt. Such
dictionaries do not take the L2 context into ac-
count and are generally more constrained to single
words or short expressions. The proposed applica-
tion would allow more flexible context-dependent
lookups as writing progresses. The task tests how
effectively participating systems accomplish this.
The following examples illustrate the task for
the four language pairs we offer:
? Input (L1=English,L2=Spanish): ?Todo ello,
in accordance con los principios que siempre
hemos apoyado.?
Desired output: ?Todo ello, de conformidad
con los principios que siempre hemos apoy-
ado.?
? Input (L1-English, L2=German): ?Das,
was wir heute machen, is essentially ein
?
Argernis.?
Desired output: ?Das, was wir heute machen,
ist im Grunde genommen ein
?
Argernis.?
? Input (L1=French,L2=English): ?I rentre `a
la maison because I am tired.?
Desired output: ?I return home because I am
tired.?
? Input (L1=Dutch, L2=English): ?Workers
are facing a massive aanval op their employ-
ment and social rights.?
Desired output: ?Workers are facing a mas-
sive attack on their employment and social
rights.?
The task can be related to two tasks that were
offered in previous years of SemEval: Lexical
Substitution (Mihalcea et al., 2010) and most no-
tably Cross-lingual Word Sense Disambiguation
(Lefever and Hoste, 2013).
When comparing our task to the Cross-Lingual
Word-Sense Disambiguation task, one notable dif-
ference is the fact that our task concerns not just
words, but also phrases. Another essential differ-
ence is the nature of the context; our context is in
L2 instead of L1. Unlike the Cross-Lingual Word
Sense Disambiguation task, we do not constrain
the L1 words or phrases that may be used for trans-
lation, except for a maximum length which we set
to 6 tokens, whereas Lefever and Hoste (2013)
only tested a select number of nouns. Our task
emphasizes a correct meaning-preserving choice
of words in which translations have to fit in the
L2 context. There is thus a clear morphosyntactic
aspect to the task, although less prominent than
in full machine translation, as the remainder of
the sentence, already in L2, does not need to be
changed. In the Cross-Lingual Word Sense Dis-
ambiguation tasks, the translations/senses were
lemmatised. We deliberately chose a different path
that allows for the envisioned application to func-
tion directly as a translation assistance system.
A pilot study was conducted to test the feasibil-
ity of the proposed translation system (van Gom-
pel and van den Bosch, 2014). It shows that L2
context information can be a useful cue in transla-
tion of L1 fragments to L2, improving over a non-
context-informed baseline.
3 Data
We did not provide training data for this task, as
we did not want to bias participating systems by
favouring a particular sort of material and method-
ology. Moreover, it would be a prohibitively large
task to manually collect enough training data of
the task itself. Participants were therefore free to
use any suitable training material such as parallel
corpora, wordnets, or bilingual lexica.
Trial and test data has been collected for the
task, both delivered in a simple XML format that
explicitly marks the fragments. System output of
participants adheres to the same format. The trial
set, released early on in the task, was used by par-
ticipants to develop and tune their systems on. The
test set corresponds to the final data released for
the evaluation period; the final evaluation was con-
ducted on this data.
The trial data was constructed in an automated
fashion in the way described in our pilot study
(van Gompel and van den Bosch, 2014). First a
phrase-translation table is constructed from a par-
allel corpus. We used the Europarl parallel corpus
(Koehn, 2005) and the Moses tools (Koehn et al.,
2007), which in turn makes use of GIZA++ (Och
and Ney, 2000). Only strong phrase pairs (ex-
ceeding a set threshold) were retained and weaker
ones were pruned. This phrase-translation table
was then used to create input sentences in which
the L2 fragments are swapped for their L1 coun-
terparts, effectively mimicking a fall-back to L1 in
an L2 context. The full L2 sentence acts as refer-
ence sentence. Finally, to ensure all fragments are
correct and sensible, a manual selection from this
37
automatically generated corpus constituted the fi-
nal trial set.
In our pilot study, such a data set, even with-
out the manual selection stage, proved adequate to
demonstrate the feasibility of translating L1 frag-
ments in an L2 context (van Gompel and van den
Bosch, 2014). One can, however, rightfully argue
whether such data is sufficiently representative for
the task and whether it would adequately cover in-
stances where L2 language learners might experi-
ence difficulties and be inclined to fall back to L1.
We therefore created a more representative test set
for the task.
The actual test set conforms to much more
stringent constraints and was composed entirely
by hand from a wide variety of written sources.
Amongst these sources are study books and gram-
mar books for language learners, short bilingual
on-line stories aimed at language learners, gap-
exercises and cloze tests, and contemporary writ-
ten resources such as newspapers, novels, and
Wikipedia. We aimed for actual learner corpora,
but finding suitable learner corpora with sufficient
data proved hard. For German we could use the
the Merlin corpus (Abel et al., 2013). In example
(a) we see a real example of a fragment in a fall-
back language in an L2 context from the Merlin
corpus.
(a) Input: Das Klima hier ist Tropical und wir haben fast
keinen Winter
Reference: Das Klima hier ist tropisch und wir haben
fast keinen Winter.
For various sources bilingual data was avail-
able. For the ones that were monolingual (L2)
we resorted to manual translation. To ensure our
translations were correct, these were later indepen-
dently verified, and where necessary corrected by
native speakers.
A large portion of the test set comes from off-
line resources because we wanted to make sure
that a substantial portion of the test set could not
be found verbatim on-line. This was done to pre-
vent systems from solving the actual problem by
just attempting to just look up the sources through
the available context information.
Note that in general we aimed for the European
varieties of the different languages. However, for
English we did add the US spelling variants as al-
ternatives. A complete list of all sources used in
establishing the test set is available on our web-
site
1
.
We created a trial set and test set/gold standard
of 500 sentence pairs per language pair. Due to
the detection of some errors at a later stage, some
of which were caused by the tokenisation pro-
cess, we were forced to remove some sentences
from the test set and found ourselves slightly be-
low our aim for some of the language pairs. The
test set was delivered in both tokenised
2
and unto-
kenised form. The trial set was delivered only in
tokenised form. Evaluation was conducted against
the tokenised version, but our evaluation script
was designed to be as lenient as possible regard-
ing differences in tokenisation. We explicitly took
cases into account where participant?s tokenisers
split contractions (such as Spanish ?del? to ?de?
+ ?el?), whereas our tokeniser did not.
For a given input fragment, it may well be possi-
ble that there are multiple correct translations pos-
sible. In establishing our test set, we therefore paid
special attention to adding alternatives. To ensure
no alternatives were missed, all participant output
was aggregated in one set, effectively anonymis-
ing the systems, and valid but previously missed
alternatives were added to the gold standard.
4 Evaluation
Several metrics are available for automatic eval-
uation. First, we measure the absolute accuracy
a = c/n, where c is the number of fragment
translations from the system output that precisely
match the corresponding fragments in the refer-
ence translation, and n is the total number of trans-
latable fragments, including those for which no
translation was found. We also introduce a word-
based accuracy, which unlike the absolute accu-
racy gives some credits to mismatches that show
partial overlap with the reference translation. It as-
signs a score according to the longest consecutive
matching substring between output fragment and
reference fragment and is computed as follows:
wac =
|longestsubmatch(output, reference)|
max(|output|, |reference|)
(1)
The system with the highest word-based accu-
racy wins the competition. All matching is case-
sensitive.
1
https://github.com/proycon/semeval2014task5
2
Using ucto, available at https://github.com/proycon/ucto
38
Systems may decide not to translate fragments
if they cannot find a suitable translation. A recall
metric simply measures the number of fragments
for which the system generated a translation, re-
gardless of whether that translation is correct or
not, as a proportion of the total number of frag-
ments.
In addition to these task-specific metrics, stan-
dard MT metrics such as BLEU, NIST, METEOR
and error rates such as WER, PER and TER, are
included in the evaluation script as well. Scores
such as BLEU will generally be high (> 0.95)
when computed on the full sentence, as a large
portion of the sentence is already translated and
only a specific fragment remains to be evaluated.
Nevertheless, these generic metrics are proven in
our pilot study to follow the same trend as the
more task-specific evaluation metrics, and will be
omitted in the result section for brevity.
It regularly occurs that multiple translations are
possible. As stated, in the creation of the test set
we have taken this into account by explicitly en-
coding valid alternatives. A match with any alter-
native in the reference counts as a valid match. For
word accuracy, the highest word accuracy amongst
all possible alternatives in the reference is taken.
Likewise, participant system output may contain
multiple alternatives as well, as we allowed two
different types of runs, following the example of
the Cross-Lingual Lexical Substitution and Cross-
Lingual Word Sense Disambiguation tasks:
? Best - The system may only output one, its
best, translation;
? Out of Five - The system may output up
to five alternatives, effectively allowing 5
guesses. Only the best match is counted. This
metric does not count how many of the five
are valid.
Participants could submit up to three runs per
language pair and evaluation type.
5 Participants
Six teams submitted systems, three of which par-
ticipated for all language pairs. In alphabetic or-
der, these are:
1. CNRC - Cyril Goutte, Michel Simard, Ma-
rine Carpuat - National Research Council
Canada ? All language pairs
2. IUCL - Alex Rudnick, Liu Can, Levi King,
Sandra K?ubler, Markus Dickinson - Indiana
University (US) ? all language pairs
3. UEdin - Eva Hasler - University of Ed-
inburgh (UK) ? all language pairs except
English-German
4. UNAL - Sergio Jim?enez, Emilio Silva - Uni-
versidad Nacional de Colombia ? English-
Spanish
5. Sensible - Liling Tan - Universit?at des Saar-
landes (Germany) and Nanyang Technolog-
ical University (Singapore) ? all language
pairs
6. TeamZ - Anubhav Gupta - Universit?e de
Franche-Comt?e (France) ? English-Spanish,
English-German
Participants implemented distinct methodolo-
gies and implementations. One obvious avenue of
tackling the problem is through standard Statisti-
cal Machine Translation (SMT). The CNRC team
takes a pure SMT approach with few modifica-
tions. They employ their own Portage decoder and
directly send an L1 fragment in an L2 context, cor-
responding to a partial translation hypothesis with
only one fragment left to decode, to their decoder
(Goutte et al., 2014). The UEdin team applies a
similar method using the Moses decoder, marking
the L2 context so that the decoder leaves this con-
text as is. In addition they add a context similarity
feature for every phrase pair in the phrase transla-
tion table, which expresses topical similarity with
the test context. In order to properly decode, the
phrase table is filtered per test sentence (Hasler,
2014). The IUCL and UNAL teams do make use
of the information from word alignments or phrase
translation tables, but do not use a standard SMT
decoder. The IUCL system combines various in-
formation sources in a log-linear model: phrase
table, L2 Language Model, Multilingual Dictio-
nary, and a dependency-based collocation model,
although this latter source was not finished in time
for the system submission (Rudnick et al., 2014).
The UNAL system extracts syntactic features as a
means to relate L1 fragments with L2 context to
their L2 fragment translations, and uses memory-
based classifiers to achieve this (Silva-Schlenker
et al., 2014). The two systems on the lower end of
the result spectrum use different techniques alto-
gether. The Sensible team approaches the problem
39
by attempting to emulate the manual post-editing
process human translators employ to correct MT
output (Tan et al., 2014), whereas TeamZ relies on
Wiktionary as the sole source (Gupta, 2014).
6 Results
The results of the six participating teams can be
viewed in consensed form in Table 1. This table
shows the highest word accuracy achieved by the
participants, in which multiple system runs have
been aggregated. A ranking can quickly be dis-
tilled from this, as the best score is marked in
bold. The system by the University of Edinburgh
emerges as the clear winner of the task. The full
results of the various system runs by the six par-
ticipants are shown in Tables 2 and 3, two pages
down, all three aforementioned evaluation metrics
are reported there and the systems are sorted by
word accuracy per language pair and evaluation
type.
Team en-es oof en-de oof
CNRC 0.745 0.887 0.717 0.868
IUCL 0.720 0.847 0.722 0.857
UEdin 0.827 0.949 - -
UNAL 0.809 0.880 - -
Sensible 0.351 0.231 0.233 0.306
TeamZ 0.333 0.386 0.293 0.385
fr-en oof nl-en oof
CNRC 0.694 0.839 0.610 0.723
IUCL 0.682 0.800 0.679 0.753
UEdin 0.824 0.939 0.692 0.811
UNAL - - - -
Sensible 0.116 0.14 0.152 0.171
TeamZ - - - -
Table 1: Highest word accuracy per team, per lan-
guage pair, and per evaluation type (out-of-five is
include in the ?oof? column). The best score in
each column is marked in bold.
For the lowest-ranking participants, the score is
negatively impacted by the low recall; their sys-
tems could not find translations for a large number
of fragments.
Figures 1 (next page) and 2 (last page) show the
results for the best evaluation type for each sys-
tem run. Three bars are shown; from left to right
these represent accuracy (blue), word-accuracy
(green) and recall (red). Graphs for out-of-five
evaluation were omitted for brevity, but tend to fol-
low the same trend with scores that are somewhat
higher. These scores can be viewed on the result
website at http://github.com/proycon/
semeval2014task5/. The result website also
holds the system output and evaluation scripts with
which all graphs and tables can be reproduced.
We observe that the best scoring team in the
task (UEdin), as well as the CNRC team, both em-
ploy standard Statistical Machine Translation and
achieve high results. From this we can conclude
that standard SMT techniques are suitable for this
task. Teams IUCL and UNAL achieve similarly
good results, building on word and phrase align-
ment data as does SMT, yet not using a traditional
SMT decoder. TeamZ and Sensible, the two sys-
tems ranked lowest do not rely on any techniques
from SMT. To what extent the context-informed
measures of the various participants are effective
can not be judged from this comparison, but can
only be assessed in comparison to their own base-
lines. For this we refer to the system papers of the
participants.
7 Discussion
We did not specify any training data for the task.
The advantage of this is that participants were free
to build a wider variety of systems from various
sources, rather than introducing a bias towards for
instances statistical systems. The disadvantage,
however, is that a comparison of the various sys-
tems does not yield conclusive results regarding
the merit of their methodologies. Discrepancies
might at least be partly due to differences in train-
ing data, as it is generally well understood in MT
that more training data improves results. The base-
lines various participants describe in their system
papers provide more insight to the merit of their
approaches than a comparison between them.
In the creation of the test set, we aimed to mimic
intermediate to high-level language learners. We
also aimed at a fair distribution of different part-
of-speech categories and phrasal length. The dif-
ficulty of the task differs between language pairs,
though not intentionally so. We observe that the
Dutch-English set is the hardest and the Spanish-
English is the easiest in the task. One of the par-
ticipants implicitly observes this through measure-
ment of the number of Out-of-Vocabulary words
(Goutte et al., 2014). This implies that when com-
paring system performance between different lan-
guage pairs, one can not simply ascribe a lower
result to a system having more difficulty with said
40
Figure 1: English to Spanish (top), English to German (middle) and French to English (bottom). The
three bars, left-to-right, represent Accuracy (blue), Word Accuracy (green) and Recall (red).
41
System Acc W.Acc. Recall
English-Spanish (best)
UEdin-run2 0.755 0.827 1.0
UEdin-run1 0.753 0.827 1.0
UEdin-run3 0.745 0.82 1.0
UNAL-run2 0.733 0.809 0.994
UNAL-run1 0.721 0.794 0.994
CNRC-run1 0.667 0.745 1.0
CNRC-run2 0.651 0.735 1.0
IUCL-run1 0.633 0.72 1.0
IUCL-run2 0.633 0.72 1.0
Sensible-wtmxlingyu 0.239 0.351 0.819
TeamZ-run1 0.223 0.333 0.751
Sensible-wtm 0.145 0.175 0.470
Sensible-wtmxling 0.141 0.171 0.470
English-Spanish (out-of-five)
UEdin-run3 0.928 0.949 1.0
UEdin-run1 0.924 0.946 1.0
UEdin-run2 0.92 0.944 1.0
CNRC-run1 0.843 0.887 1.0
CNRC-run2 0.837 0.884 1.0
UNAL-run1 0.823 0.88 0.994
IUCL-run1 0.781 0.847 1.0
IUCL-run2 0.781 0.847 1.0
Sensible-wtmxlingyu 0.263 0.416 0.819
TeamZ-run1 0.277 0.386 0.751
Sensible-wtm 0.173 0.231 0.470
Sensible-wtmxling 0.169 0.228 0.470
English-German (best)
IUCL-run2 0.665 0.722 1.0
CNRC-run1 0.657 0.717 1.0
CNRC-run2 0.645 0.702 1.0
TeamZ-run1 0.218 0.293 0.852
IUCL-run1 0.198 0.252 1.0
Sensible-wtmxlingyu 0.162 0.233 0.878
Sensible-wtm 0.16 0.184 0.647
Sensible-wtmxling 0.152 0.178 0.647
English-German (out-of-five)
CNRC-run1 0.834 0.868 1.0
CNRC-run2 0.828 0.865 1.0
IUCL-run2 0.806 0.857 1.0
TeamZ-run1 0.307 0.385 0.852
IUCL-run1 0.228 0.317 1.0
Sensible-wtmxlingyu 0.18 0.306 0.878
Sensible-wtm 0.182 0.256 0.647
Sensible-wtmxling 0.174 0.25 0.647
Table 2: Full results for English-Spanish and
English-German.
language pair. This could rather be an intrinsic
property of the test set or the distance between the
languages.
Distance in syntactic structure between lan-
guages also defines the limits of this task. Dur-
ing composition of the test set it became clear that
backing off to L1 was not always possible when
syntax diverged to much. An example of this is
separable verbs in Dutch and German. Consider
the German sentence ?Er ruft seine Mutter an?
(translation: ?He calls his mother?). Imagine
System Acc W.Acc. Recall
French-English (best)
UEdin-run1 0.733 0.824 1.0
UEdin-run2 0.731 0.821 1.0
UEdin-run3 0.723 0.816 1.0
CNRC-run1 0.556 0.694 1.0
CNRC-run2 0.533 0.686 1.0
IUCL-run1 0.545 0.682 1.0
IUCL-run2 0.545 0.682 1.0
Sensible-wtmxlingyu 0.081 0.116 0.321
Sensible-wtm 0.055 0.067 0.210
Sensible-wtmxling 0.055 0.067 0.210
French-English (out-of-five)
UEdin-run2 0.909 0.939 1.0
UEdin-run1 0.905 0.938 1.0
UEdin-run3 0.907 0.937 1.0
CNRC-run1 0.739 0.839 1.0
CNRC-run2 0.731 0.834 1.0
IUCL-run1 0.691 0.8 1.0
IUCL-run2 0.691 0.8 1.0
Sensible-wtmxlingyu 0.085 0.14 0.321
Sensible-wtmxling 0.061 0.09 0.210
Sensible-wtm 0.061 0.089 0.210
Dutch-English (best)
UEdin-run1 0.575 0.692 1.0
UEdin-run2 0.567 0.688 1.0
UEdin-run3 0.565 0.688 1.0
IUCL-run1 0.544 0.679 1.0
IUCL-run2 0.544 0.679 1.0
CNRC-run1 0.45 0.61 1.0
CNRC-run2 0.444 0.609 1.0
Sensible-wtmxlingyu 0.115 0.152 0.335
Sensible-wtm 0.092 0.099 0.214
Sensible-wtmxling 0.088 0.095 0.214
Dutch-English (out-of-five)
UEdin-run1 0.733 0.811 1.0
UEdin-run3 0.727 0.808 1.0
UEdin-run2 0.725 0.808 1.0
IUCL-run1 0.634 0.753 1.0
IUCL-run2 0.634 0.753 1.0
CNRC-run1 0.606 0.723 1.0
CNRC-run2 0.602 0.721 1.0
Sensible-wtmxlingyu 0.123 0.171 0.335
Sensible-wtm 0.099 0.115 0.214
Sensible-wtmxling 0.096 0.112 0.214
Table 3: Full results for French-English and
Dutch-English.
a German language learner wanting to compose
such a sentence but wanting to fall back to En-
glish for the verb ?to call?, which would translate
to German as ?anrufen?. The possible input sen-
tence may still be easy to construe: ?Er calls seine
Mutter?, but the solution to this problem would
require insertion at two different points, whereas
the task currently only deals with a substitution of
a single fragment. The reverse is arguably even
more complex and may stray too far from what
a language learner may do. Consider an English
language learner wanting to fall back to her na-
42
tive German, struggling with the English transla-
tion for ?anrufen?. She may compose a sentence
such as ?He ruft his mother an?, which would
require translating two dependent fragments into
one.
We already have interesting examples in the
gold standard, such as example (b), showing syn-
tactic word-order changes confined to a single
fragment.
(b) Input: I always wanted iemand te zijn , but now I
realize I should have been more specific.
Reference: I always wanted to be somebody , but
now I realize I should have been more specific.
Participant output (aggregated): to be a person; it to
be; someone to his; to be somebody; person to be;
someone to; someone to be; to be anybody; to anyone;
to be someone; a person to have any; to be someone
else
Another question we can ask, but have not in-
vestigated, is whether a language learner would
insert the proper morphosyntactic form of an L1
word given the L2 context, or whether she may
be inclined to fall back to a normal form such
as an infinitive. Especially in the above case of
separable verbs someone may be more inclined to
circumvent the double fragments and provide the
input: ?He anrufen his mother?, but in simpler
cases the same issue arises as well. Consider an
English learner falling back to her native Croatian,
a Slavic language which heavily declines nouns.
If she did not know the English word ?book? and
wanted to write ?He gave the book to him?, she
could use either the Croatian word ?knjigu? in its
accusative declension or fall back to the normal
form ?knjiga?. A proper writing assistant system
would have to account for both options.
We can analyse which of the sentences in the
test data participants struggled with most. First
we look at the number of sentences that produce
an average word accuracy of zero, measured per
sentence over all systems and runs in the out-of-
five metric. This means no participant was close
to the correct output. There were 6 such sentences
in English-Spanish, 17 in English-German, 6 in
French-English, and 32 in Dutch-English.
A particularly difficult context from the Span-
ish set is when a subjunctive verb form was re-
quired, but an indicative verb form was submit-
ted by the systems, such as in the sentence: ?Es-
pero que los frenos del coche funcionen bien.?.
Though this may be deduced from context (the
word ?Espero?, expressing hope yet doubt, be-
ing key here), it is often subtle and hard to cap-
ture. Another problematic case that recurs in the
German and Dutch data sets is compound nouns.
The English fragment ?work motivation? should
translate into the German compound ?Arbeitsmo-
tivation? or ?Arbeitsmoral?, yet participants were
not able to find the actual compound noun. Beside
compound nouns, other less frequent multi-word
expressions are also amongst the difficult cases.
Sparsity or complete absence in training data of
these expressions is why systems struggle here.
Another point of discussion is the fact that we
enriched the test set by adding previously unavail-
able alternative translations from an aggregated
pool of system output. This might draw criticism
for possibly introducing a bias, also considering
the fact that the decision to include a particular al-
ternative for a given context is not always straight-
forward and at times subjective. We, however,
contend that this is the best way to ensure that
valid system output is not discarded and reduce the
number of false negatives. The effect of this mea-
sure has been an increase in (word) accuracy for
all systems, without significant impact on ranking.
8 Conclusion
In this SemEval task we showed that systems can
translate L1 fragments in an L2 context, a task
that finds application in computer-assisted trans-
lation and computer-assisted language learning.
The localised translation of a fragment in a cross-
lingual context makes it a novel task in the field.
Though the task has its limits, we argue for its
practical application in a language-learning set-
ting: as a writing assistant and dictionary replace-
ment. Six contestants participated in the task,
and used an ensemble of techniques from Statis-
tical Machine Translation and Word Sense Disam-
biguation. Most of the task organizers? time went
into manually establishing a gold standard based
on a wide variety of sources, most aimed at lan-
guage learners, for each of the four language pairs
in the task. We have been positively surprised by
the good results of the highest ranking systems.
9 Acknowledgements
We would like to thank Andreu van Hooft and
Sarah Schulz for their manual correction work,
and Sean Banville, Geert Joris, Bernard De Clerck,
Rogier Crijns, Adriane Boyd, Detmar Meurers,
Guillermo Sanz Gallego and Nils Smeuninx for
helping us with the data collection.
43
Figure 2: Dutch to English.
References
Andrea Abel, Lionel Nicolas, Jirka Hana, Barbora
?
Stindlov?a, Katrin Wisniewski, Claudia Woldt, Det-
mar Meurers, and Serhiy Bykh. 2013. A trilingual
learner corpus illustrating european reference lev-
els. In Proceedings of the Learner Corpus Research
Conference, Bergen, Norway, 27-29 September.
Cyril Goutte, Michel Simard, and Marine Carpuat.
2014. CNRC-TMT: Second language writing as-
sistant system description. In Proceedings of the
8th International Workshop on Semantic Evaluation
(SemEval-2014), Dublin, Ireland.
Anubhav Gupta. 2014. Team Z: Wiktionary as L2
writing assistant. In Proceedings of the 8th Interna-
tional Workshop on Semantic Evaluation (SemEval-
2014), Dublin, Ireland.
Eva Hasler. 2014. UEdin: Translating L1 phrases in
L2 context using context-sensitive smt. In Proceed-
ings of the 8th International Workshop on Semantic
Evaluation (SemEval-2014), Dublin, Ireland.
Philipp Koehn, Hieu Hoang, Alexandra Birch,
Chris Callison-Burch, Marcello Federico,
Nicola Bertoldi, Brooke Cowan, Wade Shen,
Christine Moran, Richard Zens, Chris Dyer,
Ohttp://www.aclweb.org/anthology/P/P07/P07
2045ndrej Bojar, Alexandra Constantin, and Evan
Herbst. 2007. Moses: Open source toolkit for
statistical machine translation. In Proceedings of
the 45th Annual Meeting of the Association for
Computational Linguistics Companion Volume
Proceedings of the Demo and Poster Sessions,
pages 177?180, Prague, Czech Republic, June.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In In Proceedings
of the Machine Translation Summit X ([MT]?05).,
pages 79?86.
Els Lefever and Veronique Hoste. 2013. SemEval-
2013 Task 10: Cross-Lingual Word Sense Disam-
biguation. In Proceedings of the 7th International
Workshop on Semantic Evaluation (SemEval 2013),
in conjunction with the Second Joint Conference on
Lexical and Computational Semantics.
Rada Mihalcea, Ravi Sinha, and Diana McCarthy.
2010. Semeval 2010 task 2: Cross-lingual lex-
ical substitution. In Proceedings of the 5th
International Workshop on Semantic Evaluations
(SemEval-2010), Uppsala, Sweden.
Franz Josef Och and Hermann Ney. 2000. Giza++:
Training of statistical translation models. Technical
report, RWTH Aachen, University of Technology.
Alex Rudnick, Levi King, Can Liu, Markus Dickinson,
and Sandra K?ubler. 2014. IUCL: Combining infor-
mation sources for semeval task 5. In Proceedings
of the 8th International Workshop on Semantic Eval-
uation (SemEval-2014), Dublin, Ireland.
Emilio Silva-Schlenker, Sergio Jimenez, and Julia Ba-
quero. 2014. UNAL-NLP: Cross-lingual phrase
sense disambiguation with syntactic dependency
trees. In Proceedings of the 8th International
Workshop on Semantic Evaluation (SemEval-2014),
Dublin, Ireland.
Liling Tan, Anne Schumann, Jos?e Martinez, and Fran-
cis Bond. 2014. Sensible: L2 translation assistance
by emulating the manual post-editing process. In
Proceedings of the 8th International Workshop on
Semantic Evaluation (SemEval-2014), Dublin, Ire-
land.
Maarten van Gompel and Antal van den Bosch. 2014.
Translation assistance by translation of L1 frag-
ments in an L2 context. In To appear in Proceedings
of ACL 2014.
44
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 406?410,
Dublin, Ireland, August 23-24, 2014.
LT3: Sentiment Classification in User-Generated Content Using a Rich
Feature Set
Cynthia Van Hee, Marjan Van de Kauter, Orph
?
ee De Clercq, Els Lefever and V
?
eronique Hoste
LT
3
, Language and Translation Technology Team
Department of Translation, Interpreting and Communication ? Ghent University
Groot-Brittanni?elaan 45, 9000 Ghent, Belgium
Firstname.Lastname@UGent.be
Abstract
This paper describes our contribution to the
SemEval-2014 Task 9 on sentiment analysis in
Twitter. We participated in both strands of the
task, viz. classification at message-level (subtask
B), and polarity disambiguation of particular text
spans within a message (subtask A). Our experi-
ments with a variety of lexical and syntactic fea-
tures show that our systems benefit from rich fea-
ture sets for sentiment analysis on user-generated
content. Our systems ranked ninth among 27 and
sixteenth among 50 submissions for task A and B
respectively.
1 Introduction
Over the past few years, Web 2.0 applications
such as microblogging services, social network-
ing sites, and short messaging services have con-
siderably increased the amount of user-generated
content produced online. Millions of people rely
on these services to send messages, share their
views or gather information about others. Si-
multaneously, companies, marketeers and politi-
cians are anxious to detect sentiment in UGC since
these messages might contain valuable informa-
tion about the public opinion. This explains why
sentiment analysis has been a research area of
great interest in the last few years (Wiebe et al.,
2005; Wilson et al., 2005; Pang and Lee, 2008;
Mohammad and Yang, 2011). Though first studies
focussed more on product or movie reviews, we
see that analyzing sentiment in UGC is currently
becoming increasingly popular. The main differ-
ence between these two sources of information is
that the former is rather long and contains quite
formal language whereas the latter one is gener-
ally very brief and noisy and thus represents some
different challenges (Maynard et al., 2012).
In this paper, we describe our contribution to
the SemEval-2014 Task 9: Sentiment Analysis in
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
Twitter (Rosenthal et al., 2014), which was a rerun
of SemEval-2013 Task 2 (Nakov et al., 2013) and
consisted of two subtasks:
? Subtask A - Contextual Polarity
Disambiguation: Given a message contain-
ing a marked instance of a word or phrase,
determine whether that instance is positive,
negative or neutral in that context.
? Subtask B - Message Polarity
Classification: Given a message, classify
whether the message is of positive, negative,
or neutral sentiment. For messages convey-
ing both a positive and negative sentiment,
whichever is the stronger sentiment should be
chosen.
The datasets for training, development and test-
ing were provided by the task organizers. The
training datasets consisted of Twitter messages
on a variety of topics. The test sets con-
tained regular tweets (Twitter2013, Twitter2014),
tweets labeled as sarcastic (TwitterSarcasm), SMS
messages (SMS2013), and blog posts (LiveJour-
nal2014). For both subtasks, the possible polar-
ity labels were positive, negative, neutral, and ob-
jective. The datasets for subtask B contained an
additional label, i.e. objective-OR-neutral. Ta-
ble 1 presents an overview of all provided datasets.
For each task and test dataset, two runs could be
submitted: a constrained run using the provided
training data only, and an unconstrained one us-
ing additional training data. For both tasks, we
created a constrained model based on supervised
learning, relying on additional lexicons and us-
ing the test datasets of SemEval-2013 as develop-
ment data. Evaluation was based on averaged F-
measure, considering averaged F-positive and F-
negative.
406
Dataset Subtask A Subtask B
Training
Training data 26,928 9,684
Development data 1,135 1,654
Total training data 28,063 11,338
Dev-test (test SemEval-2013)
Tweets 4,435 3,813
SMS messages 2,334 2,094
Test SemEval-2014
Tweets + SMS messages + 10,681 8,987
blog posts + sarcastic tweets
Table 1: Number of labeled instances contained
by the training, development (test data SemEval-
2013), and SemEval-2014 test sets.
2 System Description
Our main goal was to develop, for each polarity
classification task, a classifier to label a message
or an instance of that message as either positive,
negative, or neutral. We ran several experiments to
identify the most discriminative classifier features.
This section gives an overview of the pipeline we
developed and which features were implemented.
2.1 Linguistic Preprocessing
First, we performed manual cleaning on the
datasets to replace non-UTF-8 characters, and we
tokenized all messages using the Carnegie Mellon
University Twitter Part-of-Speech Tagger (Gimpel
et al., 2011). Subsequently, we Part-of-Speech
tagged all instances using the CMU Twitter Part-
of-Speech Tagger (Gimpel et al., 2011), and per-
formed dependency parsing using a caseless pars-
ing model of the Stanford parser (de Marneffe et
al., 2006). Besides that, we also tagged all named
entities using the Twitter NLP tools (Ritter et al.,
2011) for Named Entity Recognition. As a final
preprocessing step, we decided to combine the la-
bels neutral, objective and neutral-OR-objective,
thus recasting the task as a three-way classifica-
tion task.
2.2 Feature Extraction
We implemented a number of lexical and syntactic
features that represent every phrase (subtask A) or
message (subtask B) within a feature vector:
N-gram features
? Word token n-gram features: a binary value
for every token unigram, bigram, and trigram
found in the training data.
? Character n-gram features: a binary value
for every character trigram, and fourgram
(within word tokens) found in the training
data.
? Normalized n-gram features: n-grams that
consisted of URLs and mentions or @-
replies were replaced by http://someurl and
by @someuser, respectively. We also nor-
malized commonly used abbreviations
1
to
their full written form (e.g. h8? hate).
Word shape features
? Character flooding: the number of word to-
kens with a character repeated more than two
times (e.g. sooooooo join).
? Punctuation flooding: the number of con-
tiguous sequences of exclamation/question
marks (e.g. GRADUATION?!?!).
? Punctuation of the last token: a binary value
indicating whether the last word token of
a message contains a question/exclamation
mark (e.g. Going to Helsinki tomorrow or on
the day after tomorrow, yay!).
? The number of capitalized words (e.g. SO
EXCITED).
? The number of hashtags (e.g. #win).
Lexicon features: As sentiment lexicons we
consulted existing resources: AFINN (Nielsen,
2011), General Inquirer (Stone et al., 1966),
MPQA (Wilson et al., 2005), NRC Emotion (Mo-
hammad and Turney, 2010; Mohammad and
Yang, 2011), Bing Liu (Hu and Liu, 2004), and
Bounce (K?okciyan et al., 2013) ? the latter three
are Twitter-specific. Additionally, we created a list
of emoticons extracted from the SemEval-2014
training data. Based on these resources, the fol-
lowing features were extracted:
? The number of positive, negative, and neutral
lexicon words averaged over text length
? The overall polarity, which is the sum of the
values of identified sentiment words
These features were extracted by 1) looking at all
tokens in the instance, and 2) looking at hash-
tag tokens only (e.g. win from #win). We also
considered negation cues by flipping the polarity
1
These were extracted from an existing list of chat abbre-
viations (http://www.chatslang.com/terms/abbreviations).
407
sign of a sentiment word if it occurred in a nega-
tion relation (e.g. @ 2Shades maybe 3rd team bro,
he?s not better than trey Burke from Michigan).
Negation relations were identified using the output
of the dependency parser. In the example above,
the positive polarity of the sentiment word better
is flipped into negative since it occurs in a relation
with not.
Syntactic features:
? Part-of-Speech ? 25 tags, including Twitter-
specific tags such as # (hashtags), @ (at-
mentions), ~ (retweets), U (URLs or e-mail
addresses), and E (emoticons): binary (tag
occurs in the tweet or not), ternary (tag oc-
curs zero, one, or two or more times), abso-
lute (number of occurrences), and frequency
(frequency of the tag).
? Dependency relations ? four binary values for
every dependency relation found in the train-
ing data. The first value indicates the pres-
ence of the lexicalized dependency relations
in the test data. Additionally, as proposed
by (Joshi and Penstein-Ros?e, 2009), the de-
pendency relation features are generalized in
three ways: by backing off the head word to
its PoS-tag, by backing off the modifier word
to its PoS-tag, and by backing off both the
head and modifier word.
Named entity features: This feature group con-
sists of four features: binary (tweet contains NEs
or not), absolute (number of NEs), absolute tokens
(number of tokens that are part of an NE), and fre-
quency tokens (frequency of NE tokens).
PMI features: PMI (pointwise mutual informa-
tion) values indicating the association of a word
with positive and negative sentiment. The higher
the PMI value, the stronger the word-sentiment as-
sociation. For each unigram and bigram in the
training data, PMI values were extracted from
the word-sentiment association lexicon created by
NRC Canada (Mohammad et al., 2013). A sec-
ond PMI feature was considered for each unigram
based on the word-sentiment associations found in
the SemEval-2014 training dataset. PMI values
were calculated as follows:
PMI(w) = PMI(w, positive)? PMI(w, negative)
(1)
As the equation shows, the association score of a
word with negative sentiment is subtracted from
the word?s association score with positive senti-
ment.
2.3 Optimizing the Classification Results
The core of our approach consisted in evaluating
the aforementioned features and selecting those
feature groups contributing most to the classifica-
tion results. To this end, we trained an SVM clas-
sifier using the LIBSVM package (Chang and Lin,
2001) and created models for various feature com-
binations. A linear kernel and a cost value of 1
were chosen as parameter settings for all further
experiments after cross-validation on the training
data. Our experimental setup consisted of three
steps: 1) training an SVM on the original train-
ing data provided by the task organizers (no de-
velopment data was used), 2) generating a model,
and 3) applying and evaluating the model on the
development data (Twitter and SMS test data of
SemEval-2013). We started our experiments with
sentiment lexicon and n-gram features only, and
gradually added other feature groups to identify
the most contributive features. Tables 2 and 3 re-
veal the obtained F-scores for each step.
Features Dev Twitter Dev SMS
lexicons 0.6855 0.6402
n-grams 0.8482 0.8229
n-grams + lexicons 0.8628 0.8489
+ normalization n-grams 0.8632 (+ 0.0004) 0.8502 (+ 0.0013)
+ Part-of-Speech 0.8646 (+ 0.0014) 0.8582 (+ 0.0080)
+ negation 0.8650 (+ 0.0004) 0.8654 (+ 0.0072)
+ word shape 0.8649 (- 0.0001) 0.8650 (- 0.0004)
+ named entity 0.8642 (- 0.0007) 0.8660 (+ 0.0010)
+ dependency 0.8642 (=) 0.8660 (=)
+ PMI 0.8610 (- 0.0032) 0.8654 (- 0.0006)
Table 2: F-scores obtained after adding other fea-
tures for the Twitter and SMS development data
(test data SemEval-2013) ? subtask A.
Features Dev Twitter Dev SMS
lexicons 0.5342 0.5119
n-grams 0.5896 0.5628
n-grams + lexicons 0.6442 0.6040
+ normalization n-grams 0.6414 (- 0.0028) 0.6084 (+ 0.0044)
+ Part-of-Speech 0.6466 (+ 0.0052) 0.6333 (+ 0.0249)
+ negation 0.6542 (+ 0.0076) 0.6384 (+ 0.0051)
+ word shape 0.6581 (+ 0.0039) 0.6394 (+ 0.0010)
+ named entity 0.6559 (- 0.0022) 0.6399 (+ 0.0005)
+ dependency 0.6467 (- 0.0092) 0.6430 (+ 0.0031)
+ PMI 0.6525 (+ 0.0058) 0.6525 (+ 0.0095)
Table 3: F-scores obtained after adding other fea-
tures for the Twitter and SMS development data
(test data SemEval-2013) ? subtask B.
As can be inferred from the tables, F-scores
408
SMS2013 Twitter2013 LiveJournal2014 Twitter2014 Twitter2014 Sarcasm
Task A 85.26 (7/27) 86.28 (8/27) 80.44 (13/27) 81.02 (9/27) 70.76 (13/27)
Task B 64.78 (7/50) 65.56 (14/50) 68.56 (20/50) 65.47 (16/50) 47.76 (22/50)
Table 4: F-scores and rankings of our systems across the various data genres for subtask A (Contextual
Polarity Disambiguation) and subtask B (Message Polarity Classification).
were already relatively high (~0.8559 for subtask
A and ~0.6241 for subtask B) for the combined
lexicon and n-gram features (on average 0.8559
for subtask A and 0.6241 for subtask B), which we
therefore consider as a our baseline setup. Con-
sidering the results for both subtasks and data
genres, we conclude that n-grams, sentiment lex-
icons, and PoS-tags were the most contributive
feature groups, whereas named entity and depen-
dency features did not improve the overall classi-
fication performance. However, using all feature
groups (n-grams, lexicons, normalized n-grams,
Part-of-Speech features, negation features, word
shape features, named entity features, dependency
features, and PMI features) improved the classi-
fication results (reaching an averaged F= 0.8632
for subtask A, and F= 0.6525 for subtask B) com-
pared to classification based on lexicon (averaged
F= 0.6629 for subtask A, and F= 0.5231 for sub-
task B) or n-gram features only (averaged F=
0.8356 for subtask A, and F= 0.5762 for subtask
B). Based on these results, we conclude that using
the full feature set for the classification of unseen
data appears to be a promising approach, consid-
ering that it achieves good performance and that it
would not tune the training model to a particular
data genre.
For further optimization of the classification re-
sults, we performed feature selection in the fea-
ture groups by using a genetic algorithm approach
which can explore different areas of the search
space in parallel. In order to do so, we made use
of the Gallop (Genetic Algorithms for Linguistic
Learner Optimization) python package (Desmet
et al., 2013). This enabled us to select the most
contributive features from every feature group: n-
gram features at token and character level, lexi-
con features from General Inquirer, Liu, AFINN,
and Bounce, character flooding and token capital-
ization features, Part-of-Speech features (binary,
ternary, and absolute), named entity features (bi-
nary, absolute tokens, and frequency tokens), and
PMI features based on the NRC lexicon. None of
the dependency relation features were selected.
3 Results
We submitted sentiment labels for the Contextual
Polarity Disambiguation (subtask A) and for the
Message Polarity Classification (subtask B). Our
competition results are reported in Table 4. Rank-
ings for each dataset are added between brack-
ets. The results reveal that our systems achieved
good performance in the polarity classification of
unseen data across the various genres and tasks.
Overall, we achieved our best classification per-
formance on the Twitter2013 test set, obtaining an
F-score of 86.28, while the best performance for
this data genre is an F-score of 90.14. We saw a
drop in performance on the Twitter2014 Sarcasm
test set. This is consistent with most other teams
as sarcastic language is hard to handle in senti-
ment analysis. Considering the rankings, we con-
clude that we performed particularly well on the
SMS test dataset of SemEval-2013 for both sub-
tasks, ranking seventh for this genre. Our systems
ranked ninth among 27 submissions and sixteenth
among 50 submissions for subtasks A and B re-
spectively.
4 Conclusions and Future Work
Using a rich feature set proves to be beneficial for
automatic sentiment analysis on user-generated
content. Feature selection experiments revealed
that features based on n-grams, sentiment lexi-
cons, and PoS-tags were most contributive for
both classification tasks, while dependency fea-
tures did not contribute to overall classification
performance. As future work it will be interesting
to study the impact of normalization of the data on
the classification performance.
Based on a shallow error analysis, we believe
that including additional classification features
may also be promising: modifiers other than nega-
tion cues (diminishers, increasers, modal verbs,
etc.) that affect the polarity intensity, emoticon
flooding, and pre- and suffixes that indicate emo-
tion (un-, dis-, -less, etc.). Additionally, lemma-
tization and hashtag segmentation on the training
data could also improve classification results.
409
References
Chih-Chung Chang and Chih-Jen Lin, 2001. LIBSVM:
a library for support vector machines.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating Typed
Dependency Parses from Phrase Structure Parses. In
Proc. of LREC?06).
Bart Desmet, V?eronique Hoste, David Verstraeten, and
Jan Verhasselt. 2013. Gallop Documentation.
Technical Report LT3 13-03, University of Ghent.
Kevin Gimpel, Nathan Schneider, Brendan O?Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein,
Michael Heilman, Dani Yogatama, Jeffrey Flanigan,
and Noah A. Smith. 2011. Part-of-speech tagging
for Twitter: Annotation, features, and experiments.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies: Short Papers - Volume 2,
HLT ?11, pages 42?47, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the tenth
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, KDD04, pages
168?177, New York, NY. ACM.
Mahesh Joshi and Carolyn Penstein-Ros?e. 2009. Gen-
eralizing dependency features for opinion mining.
In Proceedings of the ACL-IJCNLP 2009 Confer-
ence Short Papers, ACLShort ?09, pages 313?316,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Nadin K?okciyan, Arda C?elebi, Arzucan
?
Ozg?ur, and
Suzan
?
Usk?udarli. 2013. Bounce: Sentiment classi-
fication in Twitter using rich feature sets. In Second
Joint Conference on Lexical and Computational Se-
mantics (*SEM), Volume 2: Proceedings of the Sev-
enth International Workshop on Semantic Evalua-
tion (SemEval 2013), pages 554?561, Atlanta, Geor-
gia, USA. ACL.
Diane Maynard, Kalina Bontcheva, and Dominic Rout.
2012. Challenges in developing opinion mining
tools for social media. In Proc. of the LREC work-
shop NLP can u tag #usergeneratedcontent?!
Saif Mohammad and Peter Turney. 2010. Emotions
Evoked by Common Words and Phrases: Using Me-
chanical Turk to Create an Emotion Lexicon. In
Proceedings of the NAACL-HLT 2010 Workshop on
Computational Approaches to Analysis and Genera-
tion of Emotion in Text, LA, California.
Saif Mohammad and Tony Yang. 2011. Tracking
Sentiment in Mail: How Genders Differ on Emo-
tional Axes. In Proceedings of the 2nd Workshop on
Computational Approaches to Subjectivity and Sen-
timent Analysis (WASSA 2011), pages 70?79, Port-
land, Oregon. ACL.
Saif M. Mohammad, Svetlana Kiritchenko, and Xiao-
dan Zhu. 2013. Nrc-canada: Building the state-
of-the-art in sentiment analysis of tweets. CoRR,
abs/1308.6242.
Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva,
Veselin Stoyanov, Alan Ritter, and Theresa Wilson.
2013. Semeval-2013 task 2: Sentiment analysis in
Twitter. In Second Joint Conference on Lexical and
Computational Semantics (*SEM), Volume 2: Pro-
ceedings of the Seventh International Workshop on
Semantic Evaluation (SemEval 2013), pages 312?
320, Atlanta, Georgia, USA, June. Association for
Computational Linguistics.
Finn Nielsen. 2011. A new anew: Evaluation of a
word list for sentiment analysis in microblogs. In
Proceedings of the ESWC2011 Workshop on Mak-
ing Sense of Microposts: Big things come in small
packages.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Found. Trends Inf. Retr., 2(1-
2):1?135, January.
Alan Ritter, Sam Clark, Mausam, and Oren Etzioni.
2011. Named entity recognition in tweets: An ex-
perimental study. In Proceedings of the Conference
on Empirical Methods in Natural Language Pro-
cessing, EMNLP ?11, pages 1524?1534, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Sara Rosenthal, Preslav Nakov, Alan Ritter, and
Veselin Stoyanov. 2014. SemEval-2014 Task 9:
Sentiment Analysis in Twitter. In Preslav Nakov and
Torsten Zesch, editors, Proceedings of the 8th In-
ternational Workshop on Semantic Evaluation, Se-
mEval ?14, Dublin, Ireland.
Philip J. Stone, Dexter C. Dunphy, Marshall S. Smith,
and Daniel M. Ogilvie. 1966. The General Inquirer:
A Computer Approach to Content Analysis. MIT
Press.
Janyce Wiebe, Theresa Wilson, and Claire Cardie.
2005. Annotating expressions of opinions and
emotions in language. Computer Intelligence,
39(2):165?210.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of the con-
ference on Human Language Technology and Em-
pirical Methods in Natural Language Processing,
HLT05, pages 347?354, Stroudsburg, PA. ACL.
410
Proceedings of SSST-5, Fifth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 52?60,
ACL HLT 2011, Portland, Oregon, USA, June 2011. c?2011 Association for Computational Linguistics
An Evaluation and Possible Improvement Path for Current SMT Behavior
on Ambiguous Nouns
Els Lefever1,2 and Ve?ronique Hoste1,2,3
1LT3, Language and Translation Technology Team, University College Ghent
Groot-Brittannie?laan 45, 9000 Gent, Belgium
2Dept. of Applied Mathematics and Computer Science, Ghent University
Krijgslaan 281 (S9), 9000 Gent, Belgium
3Dept. of Linguistics, Ghent University
Blandijnberg 2, 9000 Gent, Belgium
Abstract
Mistranslation of an ambiguous word can have
a large impact on the understandability of a
given sentence. In this article, we describe
a thorough evaluation of the translation qual-
ity of ambiguous nouns in three different se-
tups. We compared two statistical Machine
Translation systems and one dedicated Word
Sense Disambiguation (WSD) system. Our
WSD system incorporates multilingual infor-
mation and is independent from external lex-
ical resources. Word senses are derived auto-
matically from word alignments on a parallel
corpus. We show that the two WSD classifiers
that were built for these experiments (English?
French and English?Dutch) outperform the
SMT system that was trained on the same cor-
pus. This opens perspectives for the integra-
tion of our multilingual WSD module in a sta-
tistical Machine Translation framework, in or-
der to improve the automated translation of
ambiguous words, and by consequence make
the translation output more understandable.
1 Introduction
Word Sense Disambiguation (WSD) is the NLP
task that consists in assigning a correct sense to
an ambiguous word in a given context. Tradition-
ally, WSD relies on a predefined monolingual sense-
inventory such as WordNet (Fellbaum, 1998) and
WSD modules are trained on corpora, which are
manually tagged with senses from these inventories.
A number of issues arise with these monolingual su-
pervised approaches to WSD. First of all, there is a
lack of large sense-inventories and sense-tagged cor-
pora for languages other than English. Furthermore,
sense inventories such as WordNet contain very fine-
grained sense distinctions that make the sense dis-
ambiguation task very challenging (even for human
annotators), whereas very detailed sense distinctions
are often irrelevant for practical applications. In ad-
dition to this, there is a growing feeling in the com-
munity that WSD should be used and evaluated in
real application such as Machine Translation (MT)
or Information Retrieval (IR) (Agirre and Edmonds,
2006).
An important line of research consists in the de-
velopment of dedicated WSD modules for MT. In-
stead of assigning a sense label from a monolin-
gual sense-inventory to the ambiguous words, the
WSD system has to predict a correct translation for
the ambiguous word in a given context. In (Vick-
rey et al, 2005), the problem was defined as a word
translation task. The translation choices of ambigu-
ous words are gathered from a parallel corpus by
means of word alignment. The authors reported
improvements on two simplified translation tasks:
word translation and blank filling. The evaluation
was done on an English-French parallel corpus but
is confronted with the important limitation of hav-
ing only one valid translation (the aligned transla-
tion in the parallel corpus) as a gold standard trans-
lation. Cabezas and Resnik (2005) tried to improve
an SMT system by adding additional translations to
the phrase table, but were confronted with tuning
problems of this dedicated WSD feature. Specia
(2006) used an inductive logic programming-based
WSD system which was tested on seven ambigu-
ous verbs in English-Portuguese translation. The lat-
ter systems already present promising results for the
use of WSD in MT, but really significant improve-
ments in terms of general machine translation qual-
52
ity were for the first time obtained by Carpuat and
Wu (2007) and Chan et al (2007). Both papers
describe the integration of a dedicated WSD mod-
ule in a Chinese-English statistical machine trans-
lation framework and report statistically significant
improvements in terms of standard MT evaluation
metrics.
Stroppa et al (2007) take a completely dif-
ferent approach to perform some sort of implicit
Word Sense Disambiguation in MT. They introduce
context-information features that exploit source sim-
ilarity, in addition to target similarity that is modeled
by the language model, in an SMT framework. For
the estimation of these features that are very simi-
lar to the typical WSD local context features (left
and right context words, Part-of-Speech of the focus
phrase and context words), they use a memory-based
classification framework.
The work we present in this paper is different
from previous research in two aspects. Firstly,
we evaluate the performance of two state-of-the-art
SMT systems and a dedicated WSD system on the
translation of ambiguous words. The comparison is
done against a manually constructed gold-standard
for two language pairs, viz. English?French and
English?Dutch. Although it is crucial to measure the
general translation quality after integrating a dedi-
cated WSD module in the SMT system, we think it is
equally interesting to conduct a dedicated evaluation
of the translation quality on ambiguous nouns. Stan-
dard SMT evaluation metrics such as BLEU (Pap-
ineni et al, 2002) or edit-distance metrics (e.g. Word
Error Rate) measure the global overlap of the trans-
lation with a reference, and are thus not very sen-
sitive to WSD errors. The mistranslation of an am-
biguous word might be a subtle change compared to
the reference sentence, but it often drastically affects
the global understanding of the sentence.
Secondly, we explore the potential benefits of a
real multilingual approach to WSD. The idea to use
translations from parallel corpora to distinguish be-
tween word senses is based on the hypothesis that
different meanings of a polysemous word are often
lexicalized across languages (Resnik and Yarowsky,
2000). Many WSD studies have incorporated this
cross-lingual evidence idea and have successfully
applied bilingual WSD classifiers (Gale and Church,
1993; Ng et al, 2003; Diab and Resnik, 2002) or
systems that use a combination of existing Word-
Nets with multilingual evidence (Tufis? et al, 2004).
Our WSD system is different in the sense that it is
independent from a predefined sense-inventory (it
only uses the parallel corpus at hand) and that it
is truly multilingual as it incorporates information
from four other languages (French, Dutch, Span-
ish, Italian and German depending on the target lan-
guage of the classifier). Although our classifiers are
still very preliminary in terms of the feature set and
parameters that are used, we obtain interesting re-
sults on our test sample of ambiguous nouns. We
therefore believe our system can have a real added
value for SMT, as it can easily be trained for differ-
ent language pairs on exactly the same corpus which
is used to train the SMT system, which should make
the integration a lot easier.
The remainder of this paper is organized as fol-
lows. Section 2 introduces the two machine transla-
tion systems we evaluated, while section 3 describes
the feature construction and learning algorithm of
our multilingual WSD system. Section 4 gives an
overview of the experimental setup and results. We
finally draw conclusions and present some future re-
search in Section 5.
2 Statistical Machine Translation Systems
For our experiments, we analyzed the behavior
of two phrase-based statistical machine translation
(SMT) systems on the translation of ambiguous
nouns. SMT generates translations on the basis
of statistical models whose parameters are derived
from the analysis of sentence-aligned parallel text
corpora. Phrase-based SMT is considered as the
dominant paradigm in MT research today. It com-
bines a phrase translation model (which is based on
the noisy channel model) and a phrase-based de-
coder in order to find the most probable translation e
of a foreign sentence f (Koehn et al, 2003). Usually
the Bayes rule is used to reformulate this translation
probability:
argmaxep(e|f) = argmaxep(f |e)p(e)
This allows for a language model p(e) that guar-
antees the fluency and grammatical correctness of
the translation, and a separate translation model
p(f |e) that focusses on the quality of the transla-
53
tion. Training of both the language model (on mono-
lingual data) as well as the translation model (on
bilingual text corpora) requires large amounts of text
data.
Research has pointed out that adding more train-
ing data, both for the translation as for the lan-
guage models, results in better translation quality,
(Callison-Burch et al, 2009). Therefore it is impor-
tant to notice that our comparison of the two SMT
systems is somewhat unfair, as we compared the
Moses research system (that was trained on the Eu-
roparl corpus) with the Google commercial system
that is trained on a much larger data set. It remains
an interesting exercise though, as we consider the
commercial system as the upper bound of how far
current SMT can get in case it has unlimited access
to text corpora and computational resources.
2.1 Moses
The first statistical machine translation system we
used is the off-the-shelf Moses toolkit (Koehn et al,
2007). As the Moses system is open-source, well
documented, supported by a very lively users fo-
rum and reaches state-of-the-art performance, it has
quickly been adopted by the community and highly
stimulated development in the SMT field. It also fea-
tures factored translation models, which enable the
integration of linguistic and other information at the
word level. This makes Moses a good candidate to
experiment with for example a dedicated WSD mod-
ule, that requires more enhanced linguistic informa-
tion (such as lemmas and Part-of-Speech tags).
We trained Moses for English?French and English?
Dutch on a large subsection of the Europarl corpus
(See Section 3 for more information on the corpus),
and performed some standard cleaning. Table 1 lists
the number of aligned sentences after cleaning the
bilingual corpus, and the number of uni-, bi- and tri-
grams that are comprised by the language model.
2.2 Google
In order to gain insights in the upper bounds for
current SMT, we also analyzed the output of the
Google Translate API1 for our set of ambiguous
nouns. Google Translate currently supports 57 lan-
guages. As both the amount of parallel and mono-
1http://code.google.com/apis/language/
translate/overview.html
French Dutch
Number of bilingual sentence pairs
872.689 873.390
Number of ngrams
unigrams 103.027 173.700
bigrams 1.940.925 2.544.554
trigrams 2.054.906 1.951.992
Table 1: Statistics resulting from the Moses training
phase
lingual training data as well as the computer power
are crucial for statistical MT, Google (that disposes
of large computing clusters and a network of data
centers for Web search) has very valuable assets at
its disposal for this task. We can only speculate
about the amount of resources that Google uses to
train its translation engine. Part of the training data
comes from transcripts of United Nations meetings
(in six official languages) and those of the Euro-
pean Parliament (Europarl corpus). Google research
papers report on a distributed infrastructure that is
used to train on up to two trillion tokens, which re-
sult in language models containing up to 300 billion
ngrams (Brants et al, 2007).
3 ParaSense
This section describes the ParaSense WSD system:
a multilingual classification-based approach to
Word Sense Disambiguation. Instead of using
a predefined monolingual sense-inventory such
as WordNet, we use a language-independent
framework where the word senses are derived
automatically from word alignments on a parallel
corpus. We used the sentence-aligned Europarl
corpus (Koehn, 2005) for the construction of our
WSD module. The following six languages were
selected: English (our focus language), Dutch,
French, German, Italian and Spanish. We only
considered the 1-1 sentence alignments between
English and the five other languages. This way we
obtained a six-lingual sentence-aligned subcorpus
of Europarl, that contains 884.603 sentences per
language. For our experiments we used the lexical
sample of twenty ambiguous nouns that was also
used in the SemEval-2010 ?Cross-Lingual Word
Sense Disambiguation? (CLWSD) task (Lefever
and Hoste, 2010b), which consists in assigning a
54
correct translation in five supported target languages
(viz. French, Italian, Spanish, German and Dutch)
for an ambiguous focus word in a given context.
In order to detect all relevant translations
for the twenty ambiguous focus words, we ran
GIZA++ (Och and Ney, 2003) with its default set-
tings on our parallel corpus. The obtained word
alignment output was then considered to be the clas-
sification label for the training instances for a given
classifier (e.g. the French translation resulting from
the word alignment is the label that is used to train
the French classifier). This way we obtained all
class labels (or oracle translations) for all training
instances for our five classifiers (English as an input
language and French, German, Dutch, Italian and
Spanish as target languages). For the experiments
described in this paper, we focused on the English?
French and English?Dutch classifiers.
We created two experimental setups. The first
training set contains the automatically generated
word alignment translations as labels. A postpro-
cessing step was applied on these translations in or-
der to automatically filter leading and trailing deter-
miners and prepositions from the GIZA++ output.
For the creation of the second training set, we man-
ually verified all word alignment correspondences
of the ambiguous words. This second setup gives
an idea of the upperbound performance in case the
word alignment output could be further improved for
our ambiguous nouns.
3.1 Classifier
To train our WSD classifiers, we used the memory-
based learning (MBL) algorithms implemented in
TIMBL (Daelemans and van den Bosch, 2005),
which has successfully been deployed in previous
WSD classification tasks (Hoste et al, 2002). We
performed very basic heuristic experiments to de-
fine the parameter settings for the classifier, leading
to the selection of the Jeffrey Divergence distance
metric, Gain Ratio feature weighting and k = 7 as
number of nearest neighbours. In future work, we
plan to use an optimized word-expert approach in
which a genetic algorithm performs joint feature se-
lection and parameter optimization per ambiguous
word (Daelemans et al, 2003).
3.2 Feature Construction
For the feature vector construction, we combine lo-
cal context features that were extracted from the En-
glish sentence and a set of binary bag-of-words fea-
tures that were extracted from the aligned transla-
tions in the four other languages (that are not the
target language of the classifier).
3.2.1 Local Context Features
We extract the same set of local context features
from both the English training and test instances. All
English sentences were preprocessed by means of a
memory-based shallow parser (MBSP) (Daelemans
and van den Bosch, 2005) that performs tokeniza-
tion, Part-of-Speech tagging and text chunking. The
preprocessed English instances were used as input
to build a set of commonly used WSD features:
? features related to the focus word itself being
the word form of the focus word, the lemma,
Part-of-Speech and chunk information,
? local context features related to a window of
three words preceding and following the focus
word containing for each of these words their
full form, lemma, Part-of-Speech and chunk in-
formation
These local context features are to be considered
as a basic feature set. The Senseval evaluation ex-
ercises have shown that feeding additional informa-
tion sources to the classifier results in better system
performance (Agirre and Martinez, 2004). In fu-
ture experiments we plan to integrate a.o. lemma
information on the surrounding content words and
semantic analysis (e.g. Singular Value Decomposi-
tion (Gliozzo et al, 2005)) in order to detect latent
correlations between terms.
3.2.2 Translation Features
In addition to the commonly deployed local con-
text features, we also extracted a set of binary bag-
of-words features from the aligned translations that
are not the target language of the classifier (e.g.
for the French classifier, we extract bag-of-words
features from the Italian, Spanish, Dutch and Ger-
man aligned translations). We preprocessed all
aligned translations by means of the Treetagger
tool (Schmid, 1994) that outputs Part-of-Speech and
55
lemma information. Per ambiguous focus word, a
list of all content words (nouns, adjectives, adverbs
and verbs) that occurred in the aligned translations
of the English sentences containing this word, was
extracted. This resulted in one binary feature per se-
lected content word per language. For the construc-
tion of the translation features for the training set,
we used the Europarl aligned translations.
As we do not dispose of similar aligned trans-
lations for our test instances (where we only have
the English test sentences at our disposal), we had
to adopt a different strategy. We decided to use
the Google Translate API to automatically generate
translations for all English test instances in the five
target languages. This automatic translation pro-
cess can be done using whatever machine translation
tool, but we chose the Google API because of its
easy integration. Online machine translation tools
have already been used before to create artificial
parallel corpora that were used for NLP tasks such
as for instance Named Entity Recognition (Shah et
al., 2010). Similarly, Navigli and Ponzetto (2010)
used the Google Translate API to enrich BabelNet, a
wide-coverage multilingual semantic network, with
lexical information for all languages.
Once the automatic aligned translations were gen-
erated, we preprocessed them in the same way as we
did for the aligned training translations. In a next
step, we again selected all content words from these
translations and constructed the binary bag-of-words
features.
4 Evaluation
To evaluate the two machine translation systems as
well as the ParaSense system on their performance
on the lexical sample of twenty ambiguous words,
we used the sense inventory and test set of the Sem-
Eval Cross-Lingual Word Sense Disambiguation
task. The sense inventory was built up on the ba-
sis of the Europarl corpus: all retrieved translations
of a polysemous word were manually grouped into
clusters, which constitute different senses of that
given word. The test instances were selected from
the JRC-ACQUIS Multilingual Parallel Corpus2 and
BNC3. There were in total 50 test instances for each
2http://wt.jrc.it/lt/Acquis/
3http://www.natcorp.ox.ac.uk/
of the twenty ambiguous words in the sample. To la-
bel the test data, native speakers assigned three valid
translations from the predefined clusters of Europarl
translations to each test instance. A more detailed
description of the construction of the data set can
be found in (Lefever and Hoste, 2010a). As eval-
uation metric, we used a straightforward accuracy
measure that divides the number of correct answers
by the total amount of test instances. As a baseline,
we selected the most frequent lemmatized transla-
tion that resulted from the automated word align-
ment (GIZA++).
The output of the ParaSense WSD module con-
sists of a lemmatized translation of the ambiguous
focus word in the target language. The output of
the two statistical machine translation systems,
however, is a translation of the full English input
sentence. Therefore we manually selected the
translation of the ambiguous focus word from the
full translation, and made sure the translation was
put in its base form (masculine singular form for
nouns and adjectives, infinitive form for verbs).
Table 2 lists the accuracy figures for the baseline,
two flavors of the ParaSense system (with and with-
out correction of the word alignment output), Moses
and Google for English?French and English?Dutch.
A first conclusion is that all systems beat the
most frequent sense baseline. As expected, the
Google system (where there was no limitation on
the training data) achieves the best results, but for
French the considerable difference in training size
only leads to modest performance gains compared
to the ParaSense System. Another interesting obser-
vation is that the ParaSense system that uses manu-
ally verified translation labels hardly beats the sys-
tem that uses automatically generated class labels.
This is promising as it makes the manual interven-
tions on the data superfluous and leads to a fully au-
tomatic system development process.
Figure 1 illustrates the accuracy figures for French
for all three systems (for the ParaSense system we
used the flavor that incorporates the non-validated
translation labels) on all individual test words.
The three curves follow a similar pattern, except
for some words where Moses (mood, scene, side) or
both Moses and ParaSense (figure) perform worse.
As the curves show, some words (e.g. coach, figure,
56
Figure 1: Accuracy figures per system for all 20 test words
French Dutch
Baseline 63% 59%
ParaSense system
Non Corrected 75% 68%
word alignment labels
Corrected word 76% 68%
alignment labels
SMT Systems
Moses 71% 63%
Google 78% 74%
Table 2: Accuracy figures averaged over all twenty test
words
match, range) are particularly hard to disambiguate,
while others obtain very high scores (e.g. letter, mis-
sion, soil). The almost perfect scores for the latter
can be explained by the fact that these words all have
a very generic translation in French (respectively let-
tre, mission, sol) that can be used for all senses of
the word, although there might be more suited trans-
lations for each of the senses depending on the con-
text. As the manual annotators could pick three good
translations for each test instance, the most generic
translation often figures between the gold standard
translations.
The low scores for some other words can often be
explained through the relationship with the number
of training instances (corresponding to the frequency
Number of Number of
Instances Translations
coach 66 11
education 4380 55
execution 489 26
figure 2298 167
job 7531 184
letter 1822 75
match 109 21
mission 1390 46
mood 100 26
paper 3650 94
post 998 68
pot 63 27
range 1428 145
rest 1739 80
ring 143 46
scene 284 50
side 3533 261
soil 287 16
strain 134 40
test 1368 92
Table 3: Number of instances and classes for all twenty
test words in French
of the word in the training corpus) and the ambigu-
ity (number of translations) per word. As is shown
in Table 3, both for coach and match there are very
few examples in the corpus, while figure and range
57
are very ambiguous (respectively 167 and 145 trans-
lations to choose from).
The main novelty of our ParaSense system lies in
the application of a multilingual approach to per-
form WSD, as opposed to the more classical ap-
proach that only uses monolingual local context fea-
tures. Consequently we also ran a set of additional
experiments to examine the contribution of the dif-
ferent translation features to the WSD performance.
Table 4 shows the accuracy figures for French and
Dutch for a varying number of translation features
including the other four languages: Italian, Span-
ish, French and Dutch for the French classifier or
French for the Dutch classifier. The scores clearly
confirm the validity of our hypothesis: the classifiers
using translation features are constantly better than
the one that merely uses English local context fea-
tures. For French, the other two romance languages
seem to contribute most: the classifier that uses Ital-
ian and Spanish bag-of-words features achieves the
best performance (75.50%), whereas the classifier
that incorporates German and Dutch translations ob-
tains the worst scores (71.90%). For Dutch, the in-
terpretation of the scores is less straightforward: the
Italian-German combination achieves the best result
(69%), but the difference with the other classifiers
that use two romance languages (Italian-Spanish:
67.70% and Italian-French: 67.20%) is less salient
than for French. In order to draw final conclusions
on the contribution of the different languages, we
probably first need to optimize our feature base and
classification parameters. For the current experi-
ments, we use very sparse bag-of-words features that
can be optimized in different ways (e.g. feature se-
lection, reduction of the bag-of-words features by
applying semantic analysis such as Singular Value
Decomposition, etc.).
5 Conclusion
We presented a thorough evaluation of two statis-
tical Machine Translation systems and one dedi-
cated WSD system on a lexical sample of English
ambiguous nouns. Our WSD system incorporates
both monolingual local context features and bag-
of-words features that are built from aligned trans-
lations in four additional languages. The best re-
sults are obtained by Google, the SMT system that
French Dutch
Baseline 63.10 59.40
All four translation features
It, Es, De, Nl/Fr 75.20 68.10
Three translation features
It, Es, De 75.00 67.80
Es, De, Nl/Fr 74.70 66.30
It, De, Nl/Fr 75.20 68.20
It, Es, Nl/Fr 75.30 67.90
Average 75.05 67.55
Two translation features
Es, De 74.70 67.80
It, De 75.10 69.00
De, Nl/Fr 71.90 68.00
It, Es 75.50 67.70
Es, Nl/Fr 74.20 68.10
It, Nl/Fr 75.30 67.20
Average 74.45 67.96
One translation feature
De 74.50 66.50
Es 75.20 68.40
It 74.90 66.70
Nl/Fr 73.80 66.20
Average 74.60 66.95
No translation features
None 73.50 63.90
Table 4: Accuracy figures for French and Dutch for a
varying number of translation features including the other
four languages viz. Italian (It), Spanish (Es), German
(De) and French (Fr) or Dutch (Nl)
is built with no constraints on data size or compu-
tational resources. Although there is still a lot of
room for improvement on the feature base and op-
timization of the WSD classifiers, our results show
that the ParaSense system outperforms Moses that is
built with the same training corpus.
We also noticed large differences among the test
words, often related to the number of training in-
stances and the number of translations the classifier
(or decoder) has to choose from.
Additional experiments with the ParaSense sys-
tem incorporating a number of varying translations
features allow us to confirm the validity of our hy-
pothesis. The classifiers that use the multilingual
bag-of-words features clearly outperform the clas-
sifier that only uses local context features.
In future work, we want to expand our feature set
and apply a genetic algorithm to perform joint fea-
ture selection, parameter optimization and instance
58
selection. In addition, we will apply semantic anal-
ysis tools (such as SVD or LSA) on our multilingual
bag-of-words sets in order to detect latent semantic
topics in the multilingual feature base. Finally, we
want to evaluate to which extent the integration of
our WSD output helps the decoder to pick the cor-
rect translation in a real SMT framework.
References
E. Agirre and P. Edmonds, editors. 2006. Word Sense
Disambiguation. Algorithms and Applications. Text,
Speech and Language Technology. Springer, Dor-
drecht.
E. Agirre and D. Martinez. 2004. Smoothing and Word
Sense Disambiguation. In Proceedings of EsTAL -
Espan?a for Natural Language Processing, Alicante,
Spain.
Th. Brants, A.C. Popat, P. Xu, F.J. Och, and J. Dean.
2007. Large Language Models in Machine Transla-
tion. In Proceedings of the 2007 Joint Conference
on Empirical methods in Natural Language Process-
ing and Computational Natural Language Learning,
pages 858?867.
C. Cabezas and P. Resnik. 2005. Using wsd tech-
niques for lexical selection in statistical machine trans-
lation. Technical report, Institute for Advanced Com-
puter Studies, University of Maryland.
C. Callison-Burch, Ph. Koehn, Ch. Monz, and
J. Schroeder. 2009. Findings of the 2009 Workshop
on Statistical Machine Translation. In Proceedings of
the 4th EACL Workshop on Statistical Machine Trans-
lation, pages 1?28, Athens, Greece.
M. Carpuat and D. Wu. 2007. Improving statistical
machine translation using word sense disambiguation.
In Proceedings of the 2007 Joint Conference on Em-
pirical Methods in Natural Language Processing and
Computational Natural Language Learning (EMNLP-
CoNLL), pages 61?72, Prague, Czech Republic.
Y.S. Chan, H.T. Ng, and D. Chiang. 2007. Word sense
disambiguation improves statistical machine transla-
tion. In Proceedings of the 45th Annual Meeting of the
Association of Computational Linguistics, pages 33?
40, Prague, Czech Republic.
W. Daelemans and A. van den Bosch. 2005. Memory-
based Language Processing. Cambridge University
Press.
W. Daelemans, V. Hoste, F. De Meulder, and B. Naudts.
2003. Combined optimization of feature selection and
algorithm parameters in machine learning of language.
Machine Learning, pages 84?95.
M. Diab and P. Resnik. 2002. An Unsupervised Method
for Word Sense Tagging Using Parallel Corpora. In
Proceedings of ACL, pages 255?262.
C. Fellbaum. 1998. WordNet: An Electronic Lexical
Database. MIT Press.
W.A. Gale and K.W. Church. 1993. A program for align-
ing sentences in bilingual corpora. Computational
Linguistics, 19(1):75?102.
A.M. Gliozzo, C. Giuliano, and C. Strapparava. 2005.
Domain Kernels for Word Sense Disambiguation. In
59
43nd Annual Meeting of the Association for Computa-
tional Linguistics. (ACL-05).
V. Hoste, I. Hendrickx, W. Daelemans, and A. van den
Bosch. 2002. Parameter Optimization for Machine-
Learning of Word Sense Disambiguation. Natural
Language Engineering, Special Issue on Word Sense
Disambiguation Systems, 8:311?325.
P. Koehn, F.J. Och, and D. Marcu. 2003. Statistical
Phrase-based translation. In HLT-NAACL 2003: con-
ference combining Human Language Technology con-
ference series and the North American Chapter of the
Association for Computational Linguistics conference
series, pages 48?54, Edmonton, Canada.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open Source Toolkit for
Statistical Machine Translation. In Proceedings of the
ACL 2007 Demo and Poster Sessions, pages 177?180,
Prague, Czech Republic.
P. Koehn. 2005. Europarl: a parallel corpus for statisti-
cal machine translation. In Tenth Machine Translation
Summit, pages 79?86, Phuket, Thailand.
E. Lefever and V. Hoste. 2010a. Construction
of a Benchmark Data Set for Cross-Lingual Word
Sense Disambiguation. In Nicoletta Calzolari, Khalid
Choukri, Bente Maegaard, Joseph Mariani, Jan Odijk,
Stelios Piperidis, and Daniel Tapias, editors, Proceed-
ings of the seventh International Conference on Lan-
guage Resources and Evaluation (LREC?10), Valletta,
Malta, May. European Language Resources Associa-
tion (ELRA).
E. Lefever and V. Hoste. 2010b. SemEval-2010 Task
3: Cross-Lingual Word Sense Disambiguation. In
Proceedings of the 5th International Workshop on Se-
mantic Evaluation, ACL 2010, pages 15?20, Uppsala,
Sweden.
R. Navigli and S.P. Ponzetto. 2010. BabelNet: Building
a very large multilingual semantic network. In Pro-
ceedings of the 48th Annual Meeting of the Associ-
ation for Computational Linguistics, pages 216?225,
Uppsala, Sweden.
H.T. Ng, B. Wang, and Y.S. Chan. 2003. Exploiting par-
allel texts for word sense disambiguation: An empiri-
cal study. In 41st Annual Meeting of the Association
for Computational Linguistics (ACL), pages 455?462,
Sapporo, Japan.
F. J. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1):19?51.
K. Papineni, S. Roukos, T. Ward, and Zhu W.-J. 2002.
BLEU: a method for automatic evaluation of machine
translation. In Proceedings of the 40th Annual Meet-
ing of the Association for Computational Linguistics.
Ph. Resnik and D. Yarowsky. 2000. Distinguishing sys-
tems and distinguishing senses: New evaluation meth-
ods for word sense disambiguation. Natural Language
Engineering, 5(3):113?133.
H. Schmid. 1994. Probabilistic part-of-speech tagging
using decision trees. In Proceedings of the Interna-
tional Conference on new methods in Language Pro-
cessing, Manchester, UK.
R. Shah, B. Lin, A. Gershman, and R. Frederking. 2010.
SYNERGY: A Named Entity Recognition System for
Resource-scarce Languages such as Swahili using On-
line Machine Translation. In Proceedings of the
Second Workshop on African Language Technology
(AFLAT 2010), Valletta, Malt.
L. Specia. 2006. A Hybrid Relational Approach for
WSD - First Results. In Proceedings of the COL-
ING/ACL 2006 Student Research Workshop, pages 55?
60, Sydney, Australia.
N. Stroppa, A. van den Bosch, and A. Way. 2007.
Exploiting source similarity for smt using context-
informed features. In Proceedings of the 11th Con-
ference on Theoretical and Methodological Issues in
Machine Translation (TMI 2007).
D. Tufis?, R. Ion, and N. Ide. 2004. Fine-Grained
Word Sense Disambiguation Based on Parallel Cor-
pora, Word Alignment, Word Clustering and Aligned
Wordnets. In Proceedings of the 20th International
Conference on Computational Linguistics (COLING
2004), pages 1312?1318, Geneva, Switzerland, Au-
gust. Association for Computational Linguistics.
D. Vickrey, L. Biewald, M. Teyssier, and D. Koller. 2005.
Word-sense disambiguation for machine translation.
In Proceedings of EMNLP05, pages 771?778.
60
