Implicit Ambiguity Resolution Using Incremental Clustering in 
Korean-to-English Cross-Language Information Retrieval 
 
Kyung-Soon Lee1, Kyo Kageura1, Key-Sun Choi2 
1 NII (National Institute of Informatics) 
2-1-2 Hitotsubashi, Chiyoda-ku, 
Tokyo, 101-8430, Japan 
{kslee, kyo}@nii.ac.jp 
2 Division of Computer Science, KAIST 
373-1 Kusung Yusong 
Daejeon, 305-701, Korea 
kschoi@cs.kaist.ac.kr 
 
Abstract  
This paper presents a method to implicitly 
resolve ambiguities using dynamic 
incremental clustering in Korean-to-English 
cross-language information retrieval. In the 
framework we propose, a query in Korean is 
first translated into English by looking up 
Korean-English dictionary, then documents 
are retrieved based on the vector space 
retrieval for the translated query terms. For 
the top-ranked retrieved documents, 
query-oriented document clusters are 
incrementally created and the weight of each 
retrieved document is re-calculated by using 
clusters. In experiment on TREC-6 CLIR 
test collection, our method achieved 28.29% 
performance improvement for translated 
queries without ambiguity resolution for 
queries. This corresponds to 97.27% of the 
monolingual performance for original 
queries. When we combine our method with 
query ambiguity resolution, our method 
even outperforms the monolingual retrieval. 
1 Introduction 
This paper describes a method of applying 
dynamic incremental clustering to the implicit 
resolution of query ambiguities in 
Korean-to-English cross-language information 
retrieval. The method uses the clusters of 
retrieved documents as a context for 
re-weighting each retrieved document and for 
re-ranking the retrieved documents. 
Cross-language information retrieval (CLIR) 
enables users to retrieve documents written in a 
language different from a query language. The 
methods used in CLIR fall into two categories:  
statistical approaches and translation approaches. 
Statistical methods establish cross-lingual 
associations without language translation 
(Dumais et al 1997; Rehder et al 1997; Yang et 
al, 1998). They require large-scale bilingual 
corpora. In translation approach, either queries 
or documents are translated. Though document 
translation is possible when high quality 
machine translation systems are available (Kwon 
et al 1997; Oard and Hackett, 1997), it is not 
very practical. Query translation methods (Hull 
and Grefenstette, 1996; Davis, 1996; Eichmann 
et al 1998; Yang et al 1998; Jang et al 1999; 
Chun, 2000) based on bilingual dictionaries, 
multilingual ontology or thesaurus are much 
more practical. Many researches adopt 
dictionary-based query translation because it is 
simpler and practical, given the wide availability 
of bilingual or multilingual dictionaries. In order 
to achieve a high performance CLIR using 
dictionary-based query translation, however, it is 
necessary to solve the problem of increased 
ambiguities of query terms. One way of 
resolving query ambiguities is to use the 
statistics, such as mutual information (Church 
and Hanks, 1990), to measure associations of 
query terms, on the basis of existing corpora 
(Jang et al 1999). 
Document clusters, widely adopted in various 
applications such as browsing and viewing of 
document results (Hearst and Pedersen, 1996) or 
topic detection (Allan et al 1998), also reflect 
the association of terms and documents. Lee et 
al (2001) showed that incorporating a document 
re-ranking method based on document clusters 
into the vector space retrieval achieved the 
significant improvement in monolingual IR, as it 
contributed to resolving ambiguities caused by 
polysemous query terms. 
The noise or ambiguity produced by 
dictionary-based query translation in CLIR is 
much larger than the polysemous ambiguities in 
monolingual IR. For example, a Korean term 
???[eun-haeng]? is a polysemous term with 
two meanings: ?bank? and ?ginkgo?. The English 
term ?bank? itself is polysemous, so the 
translated query ends up having magnified 
ambiguities. We will show that the method we 
propose, i.e. implicit ambiguity resolution using 
incremental clustering, is highly effective in 
dealing with the increased query ambiguities in 
CLIR. 
2 Implicit ambiguity resolution using 
incremental clustering 
Figure 1 shows the overall architecture of our 
system which incorporates implicit ambiguity 
resolution method based on query-oriented 
document clusters. In the system, a query in 
Korean is first translated into English by looking 
up dictionaries, and documents are retrieved 
based on the vector space retrieval for the 
translated query. For the top-ranked retrieved 
documents, document clusters are incrementally 
created and the weight of each retrieved 
document is re-calculated by using clusters with 
preference. This phase is the core of our implicit 
ambiguity resolution method. Below, we will 
describe each module in the system. 
2.1 Dictionary-based query translation and 
ambiguities 
Queries are written in natural language in 
Korean. We first apply morphological analysis 
and part-of-speech (POS) tagging to a query, 
and select keywords based on the POS 
information. For each keyword, we look up 
Korean-English dictionaries, and all the English 
translations in the dictionaries are chosen as 
query terms. We used a general-purpose 
bilingual dictionary and technical bilingual 
dictionaries (Chun, 2000). All in all, they have 
282,511 Korean entries and 505,003 English 
translations. 
Since a term can have multiple translations, 
the list of translated query terms can contain 
terms of different meanings as well as synonyms. 
While synonyms can improve retrieval 
effectiveness, terms with different meanings 
produced from the same original term can 
degrade retrieval performance tremendously. 
At this stage, we can apply statistical 
ambiguity resolution method based on mutual 
information. In the experiment below, we will 
examine two cases, i.e. with and without 
ambiguity resolution at this stage. 
2.2 Document retrieval based on vector space 
retrieval model 
For the query, documents are retrieved based on 
the vector space retrieval method. This method 
simply checks the existence of query terms, and 
calculates similarities between the query and 
documents. The query-document similarity of 
each document is calculated by vector inner 
product of the query and document vectors: 
di
t
i
qi wwdqsimD ?= ?
=1
),(              (1) 
where query and document weight, qiw and diw , 
are calculated by ntc-ltn weighting scheme  
which yields the best retrieval result in Lee et al
(2001) among several weighting schemes used 
in SMART system (Salton, 1989). 
As the translated query can contain noises, 
non-relevant documents may have higher ranks 
than relevant documents. 
Figure 1. System architecture of implicit 
ambiguity resolution by incremental clustering. 
English Query with ambiguities
TREC AP-news collection
Korean-English 
Dictionaries
Korean Query
retrieved top N docs
Dictionary-based
Query Translation
Vector Space Retrieval
Each  document view
re-ranked results
?
Incremental Clusters
Document context view
Reflecting context
to each document 
2.3 Query-oriented incremental clustering for 
implicit ambiguity resolution 
In order to exclude non-relevant documents 
from higher ranks, we take top N documents to 
create clusters incrementally and dynamically, 
and use similarities between the clusters and the 
query to re-rank the documents. Basic idea is: 
Each cluster created by clustering of retrieved 
documents can be seen as giving a context of the 
documents belonging to the cluster; by 
calculating the similarity between each cluster 
and the query, therefore, we can spot the 
relevant context of the query; documents that 
belong to more relevant context or cluster are 
likely to be relevant to the query. 
It should be noted here that the static global 
clustering is not practical in the current setup, 
because it takes much computational time and 
the document space is too sparse (see Anick and 
Vaithyanathan (1997) for the comparison of 
static and dynamic clustering). 
2.3.1 Dynamic incremental centroid clustering 
We make clusters based on incremental centroid 
method. There are a few variations in the 
agglomerative clustering method. The 
agglomerative centroid method joins the pair of 
clusters with the most similar centroid at each 
stage (Frakes and Baeza-Yates, 1992).  
Incremental centroid clustering method is 
straightforward. The input document of 
incremental clustering proceeds according to the 
ranks of the top-ranked N documents resulted 
from vector space retrieval for a query. 
Document and cluster centroid are represented 
in vectors. For the first input document (rank 1), 
create one cluster whose member is itself. For 
each consecutive document (rank 2, ..., N), 
compute cosine similarity between the document 
and each cluster centroid in the already created 
clusters. If the similarity between the document 
and a cluster is above a threshold, then add the 
document to the cluster as a member and update 
cluster centroid. Otherwise, create a new cluster 
with this document. Note that one document can 
be a member of several clusters as shown in 
Figure 2 (sold lines show that the document 
belongs to the cluster). 
2.3.2 Cluster preference 
Similarities between the clusters and the query, 
or query-cluster similarities, are calculated by 
the combination of the query inclusion ratio and 
vector inner product between the query vector 
and the centroid vectors of the clusters. 
ci
t
i
qi
q wwq
ccqsimC ??= ?
=1
),(          (2) 
where |q| is the number of terms in the query, 
|cq| is the number of query terms included in a 
cluster centroid, |cq|/|q| is the query inclusion 
ratio for the cluster. The documents included in 
the same cluster have the same query-cluster 
similarity. 
Cluster preferences are influenced by the 
query inclusion ratio, which prefers the cluster 
whose centroid includes more various query 
terms. Thus incorporating this information into 
the weighting of each document means adding 
information which is related to the behavior of 
terms in documents as well as the association of 
terms and documents into the evaluation of the 
relevance of each document; it therefore has the 
effect of ambiguity resolution. 
2.4 Reflecting cluster information to the 
documents 
Using the query-cluster similarity, we 
re-calculate the relevance of each document 
according to the following equation: 
 
),(),(),( cqsimCMAXdqsimDdqsim cd??= (3) 
 
where simD(q,d) is a query-document similarity 
by vector space retrieval as defined in equation 
(1) and simC(q,c) is a query-cluster similarity of 
a document d defined in equation (2). Since each 
document can be a member of several clusters, 
we assign the highest query-cluster similarity 
value to the document. The new document 
similarity, sim(q,d), is calculated by 
multiplication of a query-cluster similarity and a 
query-document similarity. Based on this new 
Figure 2. Incremental centroid clustering in order 
of the top-ranked N documents 
 
similarity sim(q,d), we re-rank the retrieved 
documents. In the equation, we tried to use 
weighted sum of a query-document similarity 
and a query-cluster similarity. The combination 
by multiplication showed better performances 
than that of weighted sum. 
Through this procedure, we can effectively 
take into account the contexts of all the terms in 
a document as well as of the query terms. Thus, 
even if a document which has a low 
query-document similarity can have a high 
query-cluster similarity thanks to the effect of 
neighboring documents in the same cluster. The 
reverse can be true as well. 
3 Experiments  
3.1 Experimental environment 
We evaluated our method on TREC-6 CLIR test 
collection which contains 242,918 English 
documents (AP news from 1988 to 1990) and 24 
English queries. English queries are translated to 
Korean queries manually. We use title field of 
queries which consist of three fields such as title, 
description and narrative. 
In dictionary-based query translation, one 
query term has multiple translations. Table 3 
shows the degree of ambiguities. 
The number of Korean query terms 47 
The number of translated terms 149 
The average number of translations 3.2 
Table 1. The degree of ambiguities for 24 queries. 
In our experiment, we only use 14 queries 
which consist of more than one term to observe 
real effects of our method. This is because, if a 
query consists of more than one term, human 
can select the correct meaning of the term by its 
neighbours. But if a query consists of one term 
such as ?bank? and it is polysemous, no one can 
resolve ambiguities without considering 
additional external information. The rest 10 
queries which consist of one term are used to 
decide a threshold in incremental clustering. 
We use SMART system (Salton, 1989) 
developed at Cornell as a vector space retrieval. 
3.2 Results 
The retrieval effectiveness was evaluated using 
the 11-point average precision metric. 
We compared our method with original 
English queries, with translated queries with 
ambiguities, and with translated queries with the 
best translation after disambiguation. The 
followings are the brief descriptions for 
comparison methods: 
1) monolingual: the performance of vector 
space retrieval system for original English 
queries as the monolingual baseline. 
2) tall_base: the performance of vector space 
retrieval system for translated English 
queries which have all possible translations 
in bilingual dictionaries without ambiguity 
resolution. 
3) tall_rerank: the performance of proposed 
method using dynamic incremental clusters 
for the retrieved documents of tall_base. 
4) tone_base: the performance of vector space 
retrieval system for translated queries with 
the best translations for each query term 
after ambiguity resolution based on mutual 
information. 
5) tone_rerank: the performance of proposed 
method using dynamic incremental clusters 
for the retrieved documents of tone_base. 
 
?tall_rerank? and ?tone_rerank? use our 
implicit disambiguation method. The number of 
top N documents used in dynamic incremental 
clustering is 300 and thresholds for incremental 
centroid clustering are set as 0.41 which are 
learned from training 10 queries with one term 
in both tall_rerank and tone_rerank. 
The main objective of this paper is to observe 
the performance change by incremental clusters 
for translated queries with ambiguities (tall_base 
and tall_rerank). 
 
Comparison 11-pt avg. C/M Change 
 precision (%) (%) 
1) monolingual 0.2858 100 - 
2) tall_base 0.2167 75.82 - 
3) tall_rerank 0.2780 97.27 +28.29 
4) tone_base 0.2559 89.54 - 
5) tone_rerank 0.3026 105.87 +18.25 
Table 2. The retrieval effectiveness for comparison 
methods. 
To observe the effect of clusters, we 
compared the results after disambiguation based 
on mutual information (tone_base and 
tone_rerank). We selected the best translation 
based on mutual information among all 
translation terms. Mutual information MI(x,y) is 
defined as following (Church and Hanks, 1990): 
)()(
),(log)()(
),(log),( 22 yfxf
yxfN
ypxp
yxpyxMI ?==  (4) 
where f(x) and f(y) are frequency of term x and  
term y, respectively. Co-occurrence frequency of 
term x and term y, f(x,y), is taken in window size 
6 for AP 1988 news documents. 
 
The 11-point average precision value, 
corresponding result to monolingual (C/M), and 
performance change are summarized in Table 2. 
The retrieval effectiveness of tall_rerank is 
0.2780, corresponding to 97.27% of 
monolingual performance. The performance of 
tone_rerank yields 0.3026 (105.87%). This is 
even better than the monolingual performance. 
The performance of our implicit ambiguity 
resolution method for all translations 
(tall_rerank) shows 8.63% improvement 
compared with that of ambiguity resolution 
based on mutual information (tone_base). The 
proposed method achieved 28% improvement 
for all translation queries and 18% for best 
translation queries compared with the vector 
space retrieval.  Our method after 
disambiguation (tone_rerank) using mutual 
information improved about 39.6% over vector 
space retrieval for all translations queries 
(tall_base). 
The cluster-based implicit disambiguation 
method, therefore, is more effective for 
performance improvement than the simple query 
disambiguation method based on mutual 
information; if used together, it shows yet 
further improvement. 
3.3 Result analysis 
We examined the effects of our method for a 
query with ambiguities increased after bilingual 
dictionary-based term translation. 
The Korean query is ????[ja-dong-cha] 
??[gong-gi] ??[o-yeom]? whose original 
English query is ?automobile air pollution?. The 
translated query with all the possible translations 
in Korean-English dictionaries for this query is 
as follows: 
In this query, the term ???? is polysemous 
which has several meanings such as <air>, 
<atmosphere>, <jackstone>, <co-occurrence>, 
and <bowl>. This is the cause of degrading 
system performance. 
 
146 clusters were created for the retrieved 300 
documents of this query. The token number of 
documents in the clusters was 435. The 
distribution of cluster members is shown in 
Figure 3. Most non-relevant documents had a 
tendency to make singleton cluster, and most 
relevant documents made large group clusters. 
 
We examined inside the clusters how to see 
cluster give effects to resolve ambiguity and 
reflect context. Cluster C4 in Figure 3 has 60 
members, which contains 56 relevant documents 
and 4 non-relevant documents, among 209 
relevant documents for this query. This cluster 
centroid includes following terms related to the 
query: 
car 0.069 
automobile 0.127 
air 0.082 
atmosphere 0.018 
pollution 0.196 
contamination 0.064 
 
??? 
[ja-dong-cha] 
car, automobile, autocar, 
motorcar 
?? 
[gong-gi] 
air, atmosphere, empty vessel, 
bowl, jackstone, pebble, marbles 
??[o-yeom]  contamination, pollution 
C2
C4 C17
0
10
20
30
40
50
60
70
80
90
100
0 20 40 60 80 100 120 140
cluster ID
# o
f m
em
ber
# o
f m
em
ber
Figure 3. The distribution of cluster members 
for the query with translation ambiguities. 
Although this centroid includes a noise term 
?atmosphere?, its weight is low. The other terms 
are appropriate to the query; they are synonyms. 
Since all of the query terms are included in the 
centroid, query inclusion ratio is 1 and all 
synonyms affect positively to the vector inner 
product value. Therefore, since this cluster 
preference is high, the ranks of all documents in 
this cluster changed higher. The cluster 
performed as a context of the documents 
relevant to the query. Cluster C85 is a singleton 
whose centroid includes one of three query 
terms: 
bowl 0.101 
marble 0.191 
Since query inclusion ratio is low, the cluster 
preference is low. Therefore this cluster?s effect 
is weak to the document. 
 
Figure 4 presents the rank changes, calculated 
by subtracting ranks by our method (tall_rerank) 
from those by vector space retrieval (tall_base) 
for each relevant document of the ambiguous 
query. The ranks of most documents are 
changed higher through cluster analysis, 
although the ranks of some documents are 
changed lower. Figure 5 shows recall/precision 
curves for the performances of original English 
query (monolingual; 0.6783 in 11-pt avg. 
precision), translated query without 
disambiguation (tall_base; 0.5635), and our 
method (tall_rerank; 0.6622). For increased 
query ambiguity, we could achieve 97.62% 
performance compared to the monolingual 
retrieval.  
These results indicate that cluster analysis 
help to resolve ambiguity. Thus, we could 
effectively take into account the context of all 
the terms in a document as well as the query 
terms. 
4 Conclusion 
We have proposed the method of applying 
dynamic incremental clustering to the implicit 
resolution of query ambiguities in 
Korean-to-English cross-language information 
retrieval. The method used the clusters of 
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
0.00.10.20.30.40.50.60.70.80.91.0
recall
pr
ec
isi
on
monolingual
tall_base
tall_rerank
 Figure 5. The performance comparison for the ambiguous query. 
Ra
nk
 ch
an
ge
s  
(ra
nk
 of
 ta
ll_
ba
se
  ?
ran
k o
f t
all
_re
ran
k )
-120
-80
-40
0
40
80
120
160
1 1
7 7
1 9
1 8
8
2 9
9 9
9
5 4
7 1
1
6 0
4 0
7
7 2
6 8
3
9 1
4 2
5
1 0
0 6
2 9
1 1
0 8
2 2
1 1
8 1
0 2
1 2
4 9
9 5
1 2
8 7
4 9
1 3
3 2
5 4
1 3
9 7
7 0
1 4
2 5
8 4
1 5
2 7
6 2
1 6
9 3
2 3
1 7
4 0
9 4
1 7
7 5
7 2
1 7
8 6
9 2
1 7
9 7
1 6
1 8
2 6
5 9
1 8
6 1
1 5
1 9
6 0
7 7
1 9
6 9
8 5
2 0
7 8
1 2
2 2
4 2
5 6
2 2
7 4
6 4
2 2
9 4
7 5
2 3
3 7
0 8
Relevant document ID for the query
Ra
nk
 ch
an
ge
s  
(ra
nk
 of
 ta
ll_
ba
se
  ?
ran
k o
f t
all
_re
ran
k )
 
Figure 4. The rank changes of tall_rerank from rank of tall_base for each relevant document of the query.  
retrieved documents as a context for 
re-weighting each retrieved document and for 
re-ranking the retrieved documents. 
Our method was evaluated on TREC-6 CLIR 
test collection. This method achieved 28.29% 
performance improvement for translated queries 
without ambiguity resolution. This corresponds 
to 97.27% of the monolingual performance. 
When our method was used with the query 
ambiguity resolution method based on mutual 
information, it showed 105.87% performance 
improvement of the monolingual retrieval. 
These results indicate that cluster analysis help 
to resolve ambiguity greatly, and each cluster 
itself provide a context for a query. 
Our method is a language independent model 
which can be applied to any language retrieval. 
We expect that our method will further 
improve the results, although further research is 
needed on combining a method to improve recall 
such as query expansion and relevance feedback. 
 
References 
Allan, J. Carbonell, J., Doddington, G. Yamron. J. 
and Yang, Y. (1998) Topic Detection and Tracking 
Pilot Study: Final Report. In Proc. of the DARPA 
Broadcast News Transcription and Understanding 
Workshop, pp.194-218. 
Anick, P.G. and Vaithyanathan, S. (1997) Exploiting 
Clustering and Phrases for Context-Based 
Information Retrieval. In Proc. of 20th ACM 
SIGIR Conference (SIGIR?97). 
Chun, J.H. (2000) Resolving Ambiguity and English 
Query Supplement using Parallel Corpora on 
Korean-English CLIR system. MS thesis, Dept. of 
Computer Science, KAIST (in Korean). 
Church, K.W. and Hanks P. (1990) Word Association 
Norms Mutual Information and Lexicography. 
Computational Linguistics, 16(1), pp.23-29.` 
Davis, M. (1996) New experiments in cross-language 
text retrieval at NMSU's computing research lab. In 
Proc. of the fifth Text Retrieval Conference 
(TREC-5). 
Dumais, S.T., Letsche, T.A., Littman, M.L. and 
Landauer, T.K. (1997) Automatic cross-language 
retrieval using latent semantic indexing. In Proc. of 
AAAI Symposium on Cross-Language Text and 
Speech Retrieval. 
Eichmann, D., Ruiz, M.E. and Srinivasan, P. (1998) 
Cross-Language Information Retrieval with the 
UMLS Metathesaurus. . In Proc. of the 21th ACM 
SIGIR Conference (SIGIR?98). 
Frakes, W.B., and Baeza-Yates, R. (1992) 
Information Retrieval: data structures & algorithms. 
New Jersey: Prentice Hall, pp.435-436. 
Gilarranz, J., Gonzalo, J. and Verdejo, F. (1997) An 
Approach to Conceptual Text Retrieval Using the 
EuroWordNet Multilingual Semantic Database. In 
Proc. of AAAI Spring Symposium on 
Cross-Language Text and Speech Retrieval. 
Hearst, M.A. and Pedersen, J.O. (1996) Reexamining 
the Cluster Hypothesis: Scatter/Gather on Retrieval 
Results. In Proc. of 19th ACM SIGIR Conference 
(SIGIR?96). 
Hull, D.A. and Grefenstette, G. (1996) Querying 
across languages: a dictionary-based approach to 
multilingual information retrieval. In Proc. of the 
19th ACM SIGIR Conference (SIGIR?96). 
Jang, M.G., Myaeng, S.H. and Park, S.H. (1999) 
Using Mutual Information to Resolve Query 
Translation Ambiguities and Query Term 
Weighting. In Proc. of the 37th Annual Meeting of 
the Association for Computational Linguistics. 
Kwon, O-W., Kang, I.S., Lee, J-H and Lee, G.B.  
(1997) Cross-Language Text Retrieval Based on 
Document Translation Using Japanese-to-Korean 
MT system. In Proc. of NLPRS'97, pp. 101-106. 
Lee, K.S., Park, Y.C., Choi, K.S. (2001) Re-ranking 
model based on document clusters. Information 
Processing and Management, 37(1), pp. 1-14. 
Oard, D.,W. and Hackett, P. (1997) Document 
Translation for the Cross-Language Text Retrieval 
at the University of Maryland. In  Proc. of the 
Sixth Text REtrieval Conference (TREC-6). 
Rehder, B., Littman, M.L., Dumais, S. and Landauer, 
T.K. (1997) Automatic 3-language cross-language 
information retrieval with latent semantic indexing. 
In Proc. of the Sixth Text REtrieval Conference 
(TREC-6). 
Salton, G. (1989) Automatic Text Processing: The 
Transformation, Analysis, and Retrieval of 
Information by Computer. Addison-Wesley, 
Reading, Pennsylvania. 
Voorhees, E.M. (1986) Implementing agglomerative 
hierarchic clustering algorithms for use in 
document retrieval. Information Processing & 
Management, 22(6), pp. 465-476. 
Yang, Y., Carbonell, J.G., Brown, R.D. and 
Frederking, R.E. (1998) Translingual Information 
Retrieval: Learning from Bilingual Corpora. AI 
Journal special issue, pp. 323-345. 
Unsupervised Named Entity Classification Models  
and their Ensembles 
 
Jae-Ho Kim*, In-Ho Kang, Key-Sun Choi* 
 
Korea Advanced Institute of Science and Technology (KAIST) / 
Korea Terminology Research Center for Language and Knowledge Engineering* (KORTERM) 
373-1, Guseong-dong, Yuseong-gu 
Daejeon, KOREA, 305-701 
{jjaeh@world, ihkang@csone, kschoi@world}.kaist.ac.kr 
 
Abstract  
This paper proposes an unsupervised 
learning model for classifying named 
entities. This model uses a training set, built 
automatically by means of a small-scale 
named entity dictionary and an unlabeled 
corpus. This enables us to classify named 
entities without the cost for building a large 
hand-tagged training corpus or a lot of rules. 
Our model uses the ensemble of three 
different learning methods and repeats the 
learning with new training examples 
generated through the ensemble learning. 
The ensemble of various learning methods 
brings a better result than each individual 
learning method. The experimental result 
shows 73.16% in precision and 72.98% in 
recall for Korean news articles.  
1 Introduction 
Named entity extraction is an important step for 
various applications in natural language 
processing. Named entity extraction involves 
identifying named entities in the text and 
classifying their types such as person, 
organization, location, time expressions, 
numeric expressions, and so on (Sekine and 
Eriguchi, 2000).  
One might think the named entities can be 
classified easily using dictionaries because most 
of named entities are proper nouns, but this is 
wrong opinion. As time passes, new proper 
nouns are created continuously. Therefore it is 
impossible to add all those proper nouns to a 
dictionary. Even though named entities are 
registered in the dictionary it is not easy to 
decide their senses. They have a semantic 
(sense) ambiguity that a proper noun has 
different senses according to the context (Nina 
Wacholder, et al, 1997). For example, ?United 
States? refers either to a geographical area or to 
the political body which governs this area. The 
semantic ambiguity is occured frequently in 
Korean (Seon, et al 2001). Let us illustrate this.  
Example 1 : Location 
Let?s meet at KAIST. 
 
KAIST       e-seo   man-na-ja .  
(PN:KAIST)  (PP:at)  (V:meet) 
 
Example 2 : Organization 
KAIST announced the list of successful candidates.
 
KAIST      e-seo     hab-gyeok-ja  
(PN:KAIST)  (PP)  (N:successful candidates) 
 
myeong-dan  eul   bal-pyo-haet-da .  
(N:list)      (PP)  (V:announced) 
 
PN : proper noun, N : noun, PP : postposition, V : verb 
In the above examples, ?KAIST? has different 
categories although same postposition, ?e-seo?, 
followed. The classification of named entities in 
Korean is a little more difficult than in English. 
There are two main approaches to classify 
named entities. The first approach employs 
hand-crafted rules. It costs too much to maintain 
rules because rules and dictionaries have to be 
changed according to the application. The 
second belongs to a supervised learning 
approach, which  employs a statistical method. 
As it is more robust and requires less human 
intervention, several statistical methods based on 
a hidden Markov model (Bikel et al, 1997), a 
Maximum Entropy model (Borthwich et al, 
1998) and a Decision Tree model (B?chet et al 
2000) have been studied. The supervised 
learning approach requires a hand-tagged 
training corpus, but it can not achieve a good 
performance without a large amount of data 
because of data sparseness problem. For 
example, Borthwich (1999) showed the 
performance of 83.45% in Precision and 77.42% 
in F-measure for identifying and classifying the 
8 IREX (IREX committee, 1999) categories, 
with 294,000 tokens IREX training corpus. It 
takes a lot of time and labor to build a large 
corpus like this.  
This paper proposes an unsupervised learning 
model that uses a small-scale named entity 
dictionary and an unlabeled corpus for 
classifiying named entities. Collins and Singer 
(1999) opened the possibility of using an 
unlabeled corpus to classify named entities. 
They showed that the use of unlabeled data can 
reduce the requirements for supervision to just 7 
simple seed rules. They used natural redundancy 
in the data : for many named-entity instances, 
both the spelling of the name and the context in 
which it appears are sufficient to determine its 
type.  
Our model considers syntactic relations in a 
sentence to resolve the semantic ambiguity and 
uses the ensemble of three different learning 
methods to improve the performance. They are  
Maximum Entropy Model, Memory-based 
Learning and Sparse Network of Winnows 
(Roth, 1998). 
This model classifies proper nouns appeared 
in the documents into person, organization and 
location on the assumption that the boundaries 
of proper nouns were already recognized. 
2 The System for NE Classification 
This section describes a system that classifies 
named entities by using a machine learning 
algorithm. The system consists of four modules 
as shown in Figure 1. 
First, we builds a training set, named entity 
tagged corpus, automatically. This set will be 
used to predict the categories of named entities 
within target documents received as the input of 
the system. 
The second module extracts syntactic 
relations from the training set and target 
documents. They are encoded to the format of 
training and test examples for machine learning.  
In the third module, each learning for 
classification is progressed independently by 
three learning methods. Three results generated 
by each learner are combined into one result. 
Finally, the system decides the category by 
using a rule for the test examples that did not be 
labeled yet. And then the system outputs a 
named entity tagged corpus. 
Extracting Syntactic Relations
Building 
a Training Set
Training Set
Target Documents
Ensemble Learning
Post-Processing
NE Tagged Corpus
Input
Output
Figure 1. System Architecture 
2.1 Building a Training Set 
The system requires a training set which has 
categories in order to get knowledge for the 
classification. We build a training set 
automatically using a named entity dictionary 
and a POS tagged corpus, and then use it instead 
of a hand-tagged set in machine learning.  
We randomly extract 1500 entries per each 
category (person, location, and organization) 
from a Proper Noun dictionary made by 
KORTERM and then reconstruct the named 
entity dictionary. The Proper Noun dictionary 
has about 51,000 proper nouns classified into 41 
categories (person, animal, plant and etc.). We 
do not extract homonyms to reduce the 
ambiguity. In order to show that it is possible to 
classify named entities with a small-scale 
dictionary, we limit the number of entries to be 
1500.  
We label the target word, proper noun or 
capital alphabet, appeared in the POS tagged 
corpus 1  by means of the NE dictionary 
mentioned above. The corpus is composed of 
                                                     
1 We used a KAIST POS tagged corpus 
one million eojeols2. It is not easy to classify 
named entity correctly only with a dictionary, 
since named entity has the semantic ambiguity. 
So we have to consider the context around the 
target word.  
In order to consider the context, we use 
co-occurrence information between the category 
(c) of a target word (tw) and a head word (hw) 
appeared on the left of the target word or the 
right of the target word. We modify categories 
labeled by the NE dictionary by following 
process. 
1. We extract pairs [c, hw] from the corpus 
labeled by means of the dictionary. 
2. If hw is occurred with several different 
categories, we suppose tw occurred with 
hw may have an ambiguity and then we 
remove the category label of tw. 
3. We make rules for predicting the category 
of tw from pairs [c, hw] and apply them to 
the corpus. The rule is that tw occurred 
with hw has a c.  
4. We extract sentences including the 
labeled target word in the corpus. 
In the step 3, 9 rules are made. We label the c 
for unlabeled target word occurred with hw if 
the pair [c, hw] is found more than a threshold. 
We set the threshold to be 10. Sentences 
including the 4,504 labeled target word are made 
as a tringing set in this process (Table 1). 
Table 1. The number of the target words in a 
training set 
State # of target words
Candidates in the corpus 37,831 
Labeled by the dictionary 3,899 
Removed by the ambiguity 778 
Added by 9 rules 1,383 
Total 4,504 
2.2 Extracting Syntactic Relations 
In order to predict the category, most of machine 
learning systems usually consider two words on 
the left and two ones on the right of a target 
word as a context (Uchimoto and et al 2000, 
                                                     
2 Korean linguistic units that is separated by blank or 
punctuation 
Petasis and et al 2000). However this method 
have some problems.  
If some words that are not helpful to predict 
the category are near the target word, they can 
cause an incorrect prediction. In the following 
example, ?Kim? can be predicted as an 
organization instead of a person because of a left 
word ?Jeong-bu? (the government).  
 
Example  
The goverment supports KIA on the premise that 
the chairman Kim submits a resignation. 
 
Jeong-bu        neun Kim  hoi-jang     i 
(N:the goverment)  (PP) (PN) (N:the chairman) (PP) 
 
sa-pyo        reul  je-chul-han-da neun 
(N:a resignation) (PP)   (V :submit)     (PP)  
 
jeon-je       ro  KIA  reul  ji-won-han-da. 
(N :the premise)(PP) (PN)   (PP)   (V :support) 
 
PN : proper noun, N : noun, PP : postposition, V : verb 
  
The system cannot consider important words 
that are out of the limit of the context. In the 
former example, the word ?je-chul-han-da? 
(submit) is an important feature for predicting 
the category of ?Kim?. If a Korean functional 
word is counted as one window, we cannot get 
this information within right 4 windows. Even if 
we do not count the functional words, 
sometimes it is neccessary to consider larger 
windows than 2 windows like above example. 
We notice that words that modify the target 
word or are modified by the target word are 
more helpful to the prediction than any other 
words in the sentence. So we extract the 
syntactic relations like Figure 2 as the context.  
BLANK Kim hoi-jang(chairman)
je-chul-han-da
(present)i
modifier targetword modifiee predicatejosa
BLANK KIA BLANK ji-won-han-da(support)reul
 
Figure 2. Syntactic relations for the target word 
The modifier is a word modifying the target 
word and the modifiee is one modified by the 
target word. Josa3 is a postposition that follows 
the target word and te predicate is a verb that 
predicates the target word. The ?BLANK? label 
represents that there is no word which 
corresponds to the slot of the templet. These 
syntactic relations are extracted by a simple 
heuristic parser. We will show that these 
syntactic relations bring to a better result 
through an experiment in the section 3. 
These syntactic relations seem to be language 
specific. Josa represents a case for the target 
word. If case information is extracted in a 
sentence, these syntactic relations like Figure 2 
are also made in other languages. 
As machine learner requires training and test 
examples represented in a feature-vector format, 
syntactic relations are encoded as Figure 3. 
 
Feature-vector format  
lexical morpheme (w) Modifier POS tag (t) 
lexical morpheme (w) Target word POS tag (t) 
lexical morpheme (w) Modifiee POS tag (t) 
Josa lexical morpheme (w) 
Predicate lexical morpheme (w) 
Category  Label tag 
 
Training example : [w, t, w, t, w, t, w, w, person] 
Test example    : [w, t, w, t, w, t, w, w, Blank] 
 
Figure 3: The format of an example for learning 
2.3 Ensemble Learning 
The ensemble of several classifiers can be 
improve the performance. Errors made by the 
minority can be removed through the ensemble 
of classifiers (Thomas G. Dietterich, 1997). In 
the base noun phrase identification, Tjong Kim 
Sang, et al (2000) showed that the result 
combined by seven different machine learning 
algorithms outperformed the best individual 
result. 
In our module, machine learners train with the 
training examples and then classify the named 
entities in the test examples. This process is 
shown in Figure 4. 
                                                     
3 Josa, attached to a nominal, is a postpositional 
particle in Korean. 
Ending Condition is satisfied?
Yes
No
Labeled Test Examples
Machine Learners learn with training examples
The classification for the test examples is progressed 
by three different learners independently
Three results are combined by combining techniques
Training examples are modified 
(labeled examples in the combined result + initial training examples)
Loop
Figure 4. The process of the Ensemble Learning 
This ensemble learning has two characteristics. 
One is that the classification is progressed by 
three different learners independently and those 
results are combined into one result. The other is 
that the learning is repeated with new training 
examples generated through the learning. It 
enables the system to receive an incremental 
feedback. 
Through the this learning method, we can get 
larger and more precise training examples for 
predicting the categories. It is important in an 
unsupervisd learning model because there is no 
labeled data for learning. 
2.3.1 Machine Learning algorithms 
We use three learning methods : Memory-based 
Learning, Sparse Network of Winnows, 
Maximum Entropy Model. We describe these 
methods briefly in this section.  
Memory-based Learning stores the training 
examples and classifies new examples by 
choosing the most frequent classification among 
training examples which are closest to a new 
example. Examples are represented as sets of 
feature-value pairs. Each feature receives a 
weight which is based on the amount of 
information which it provides for computing the 
classification of the examples in the training data. 
We use the TiMBL (Daelemans, et al, 1999), a 
Memory-Based Learning software package. 
Sparse Network of Winnows learning 
architecture is a sparse network of linear units. 
Nodes in the input layer of the network represent 
simple relations over the input example and 
things being used as the input features. Each 
linear unit is called a target node and represents 
classifications which are interested in the input 
examples. Given training examples, each input 
example is mapped into a set of features which 
are active (present) in it; this representation is 
presented to the input layer of SNoW and 
propagated to the target nodes. We use SnoW 
(Carlson, et al, 1999), Sparse Network of 
Winnows software package. 
Maximum Entropy Model (MEM) is 
especially suited for integrating evidences from 
various information sources. MEM allows the 
computation of p(f|h) for any f in the space of 
possible futures, F, and for every h in the space 
of possible histories, H. Futures are defined as 
the possible classification and a history is all of 
the conditioning data which enable us to make a 
decision in the space of futures. The 
computation of p(f|h) is dependent on a set of 
features which are binary functions of the 
histroy and future. A feature is represented as 
following.  
?
?
?
?
?
?
?
?
?
?
=
otherwise 0
  future  the  of  one  is  f  and     
condition  some  meets  h if1
),( fhg  
Given a set of features and some training 
examples, a weighing parameter i?  for every 
feature ig  is computed. This allows us to 
compute the conditional probability as follows : 
)()|(
),(
hZhfP
i
fhg
i
i
?
??
=
    
 
??=
f i
fhg
i
ihZ ),()( ??     
We use MEMT, Maximum Entropy Modeling 
Toolkit (Ristad, 1998), to compute the parameter 
for the features. 
2.3.2 Combining Techniques 
We use three different voting mechanisms to 
combine results generated by three learners. 
The first method is a majority voting. Each 
classification receives the same weight and the 
most frequent classification is chosen. The 
ending condition is satisfied when there is no 
difference between a result combined in this 
loop and one combined in the former loop.  
The second method is a probability voting. 
MEMT and SNoW propose the probabilities for 
all category, but Timbl proposes only one 
appropriate category for one test example. We 
set the probability for the category Timbl 
proposes to be 0.6 and for the others to be 0.2. 
For each category, we multiply probabilities 
proposed by 3 learners and then choose N 
examples that have the largest probability. In the 
next learning we set N = N + 100. When N is 
larger than a threshold, the ending condition is 
satisfied and the learning is over. We set it to be 
3/4 of the number of test examples.  
The last method is a mixed voting. We use 
two voting methods mentioned above one after 
another. First, we use probability voting. After 
the learning is over we use majority voting. The 
threshold of the probability voting is 1/2 of the 
number of test examples here.  
2.4 Post-Processing 
After the learning, the system modifies test 
examples by using a rule, one sense per 
discourse. One sense per discourse means that 
the sense of a target word is highly consistent 
within any given document. David Yarowsky 
(1995) showed it was accurate in the word sense 
disambiguation. We label the examples that are 
not labeled yet as the category of the labeled 
word in the discourse as following example and 
we output named entity tagged corpus. 
 
Example  
 
after the ensemble learning 
 
... ... KIA<type=organization> reul ji-won-han-da.  
KIA neon ... ...  
 
after post-processing 
 
... ... KIA<type=organization> reul ji-won-han-da. 
KIA<type=organization> neon ... ...  
   
3 Experimental Results 
We used Korean news articles that consist of 
24,647 eojeols and contain 2,580 named entities 
as a test set. The number of named entities 
which belong to each category is shown in Table 
2. When even a human could not classify named 
entities, ?Unknown? is labeled and it is ignored 
for the evaluation. ?Other? is used for the word 
outside the three categories.  
Table 3 shows the result of the classification. 
The first row shows the result of the 
classification using only a NE dictionary. The 
recall (14.84%) is very low because the system 
uses a small-scale dictionary. The precision 
(91.56%) is not 100% because of the semantic 
ambiguity. It means that it is necessary to refine 
classifications created by a dictionary. 
We build a training set with a NE dictionary 
and a POS tagged corpus and refine it with 
co-occurrence information. The second row 
shows the result of the classification using this 
training set without learning. We can observe 
that the quality of the training set is improved 
thanks to our refining method. 
A Mixed Voting shows the best results. It 
improves the performance by taking good 
characteristics of a majority voting and 
probability voting. 
Table 2. The number of named entities which 
belong to each category in the test set 
Category # of NEs Category # of NEs
Person 459 Other 307 
Organization 814 Unknown 242 
Location 758 Total 2,580 
Table 3. The result of the classification 
Method Precision  Recall F-measure
Dictionary 
based 91.56% 14.84% 25.54% 
Training set 
b1ased 94.32% 20.64% 33.87% 
Majority 
Voting 69.70% 65.74% 67.68% 
Probability 
Voting 75.90% 63.45% 69.12% 
Mixed 
Voting 73.16% 72.98% 73.07% 
We extract the syntatic relations and make 5 
windows (modifier, target word, modifiee, josa, 
predicate) as a context. We conduct a 
comparative experiment using the Uchimoto?s 
method, 5 windows (two words before/after the 
target word) and then we show that our method 
brings to a better result (Table 4). 
Table 4. Comparison with two kinds of window 
size 
Windows Precision  Recall F-measure
Uchimoto?s 66.86% 69.94% 68.37% 
Ours 73.16% 72.98% 73.07% 
We try to perform the co-training similar to 
one of Colins and Singer in the same 
experimental environment. We extract 
contextual rules from our 5 windows because we 
does not have a full parser. The learning is 
started from 417 spelling seed rules made by the 
NE dictionary. We use two independent context 
and spelling rules in turn. Table 5 shows that our 
method improve the recall much more on the 
same conditions. 
Table 5. Comparison with two kinds of 
unsupervised learning method 
Method Precision  Recall F-measure
Co-trianing 84.62% 37.63% 52.09% 
Ours 73.16% 72.98% 73.07% 
Through the ensemble of various learning 
methods, we get larger and more precise training 
examples for the classification. Table 6 shows 
that the ensemble learning brings a better result 
than each individual learning method. 
Table 6. The comparison of an ensemble learning 
and each individual learning 
Learner Precision  Recall F-measure
MEMT 65.19% 61.54% 63.31% 
SNoW 66.93% 70.53% 68.68% 
Timbl 64.14% 67.59% 65.82% 
Ensemble 73.16% 72.98% 73.07% 
Three learners can use different kinds of 
features instead of same features. We conduct a 
comparative experiment as following. As 
features, SNoW uses a modifier and a target 
word, Timbl uses a modifiee and a target word, 
and MEMT uses a josa, a predicate and a target 
word. Table 7 shows that the learning using 
different kinds of features has the low 
performance because of the lack of information. 
Table 7. The comparison with the learnings using 
different features 
Features Precision  Recall F-measure
Seperated  61.69% 49.85% 55.14% 
Same  73.16% 72.98% 73.07% 
The system repeats the learning with new 
training examples generated through the 
ensemble learning. We can see that this loop 
brings to the better result as shown in Table 8.  
After the learning, we apply the rule, a sense 
per discourse. ?Post? in Table 8 indicates the 
performance after this post-processing. It The 
post-processing improves the performance a 
little. 
Table 8. The improvement of the performance 
through the repeated learning 
Method Loop Precision Recall F-measure
1st 94.35% 20.76% 34.03% 
19th 76.72% 59.97% 67.32% Probability Voting Post 75.90% 63.45% 69.12% 
We extracted the syntactic relations by using a 
simple heuristic parser. Because this parser does 
not deal with complex sentences, the failure of 
parsing causes the lack of information or wrong 
learning. Most of errors are actually occurred by 
it, therefore we need to improve the performance 
of the parser.  
4 Conclusion 
We proposed an unsupervised learning model 
for classifying the named entities. This model 
used a training set, built automatically by a 
small-scale NE dictionary and an unlabeled 
corpus, instead of a hand-tagged training set for 
learning. The experimental result showed 
73.16% in precision and 72.98% in recall for 
Korean news articles. This means that it is 
possible to classify named entities without the 
cost for building a large hand-tagged training 
corpus or a lot of rules.  
The learning for classification was progressed 
by the ensemble of three different learning 
methods. Then the ensemble of various learning 
methods brings a better result than each 
individual learning method. 
References  
B?chet, Fr?d?ric, Alexis Nasr and Franck Genet, 2000. 
"Tagging Unknown Proper Names Using Decision 
Trees", In proceedings of the 38th Annual Meeting of the 
Association for Computational Linguistics. 
Bikel, Daniel M., Scott Miller, Richard Schwartz and Ralph 
Weischedel, 1997. "Nymble: a High-Performance 
Learning Name-finder", In Proceedings of the Fifth 
Conference on Applied Natural Language Processing. 
Borthwick, Andrew, John Sterling, Eugene Agichtein and 
Ralph Grishman, 1998. "NYU: Description of the MENE 
Named Entity System as Used in MUC-7", In 
Proceedings of the Seventh Message Understanding 
Conference (MUC-7). 
Borthwick, 1999. ?A Japanese Named Entity Recognizer 
Constructed by a Non-Speaker of Japanese?, IREX. 
Proceedings of the IREX workshop. 
Carlson, Andrew J., Chad M. Cumby, Jeff L. Rosen and 
Dan Roth, 1999. "SNoW User Guide", University of 
Illinois. http://l2r.cs.uiuc.edu/~cogcomp/ 
Collins, Michael and Yoram Singer. 1999. "Unsupervised 
models for named entity classification", In proceedings 
of the Joint SIGDAT Conference on Empirical Methods 
in Natural Language Processing and Very Large 
Corpora. 
Daelemans, Walter, Jakub Zavrel, Ko van der Sloot and 
Antal van den Bosch, 1999. "TiMBL: Tilburg Memory 
Based Learner, version 4.0, Reference Guide", ILK 
Technical Report 01-04. http://ilk.kub.nl/  
Dietterich, T. G., 1997. ?Machine-Learning Research: Four 
Current Dirctions?, AI Magazine 18(4): 97 
IREX Committee (ed.), 1999. Proc. the IREX Workshop. 
http://cs.nyu.edu/cs/projects/proteus/irex 
Petasis, Georgios, Alessandro Cucchiarelli, Paola Velardi, 
Georgios Paliouras, Vangelis Karkaletsis and 
Constantine D. Spyropoulos, 2000. "Automatic 
adaptation of Proper Noun Dictionaries through 
cooperation of machine learning and probabilistic 
methods", Proceedings of the 23rd ACM SIGIR 
Conference on R&D in IR (SIGIR). 
Ristad, Eric Sven, 1998. "Maximum Entropy Modeling 
Toolkit". 
Roth, Dan, 1998. ?Learning to resolve natural language 
ambiguities: A unified approach?, In Proc. National 
Conference on Artificial Intelligence. 
Sekine, Satoshi and Yoshio Eriguchi. 2000. "Japanese 
Named Entity Extraction Evaluation", In the proceedings 
of the 18th COLING. 
Seon, Choong-Nyoung, Youngjoong Ko, Jeong-Seok Kim 
and Jungyun Seo, 2001. ?Named Entity Recognition 
using Machine Learning Methods and Pattern-Selection 
Rules?, Proceedings of the Sixth Natural Language 
Processing Pacific Rim Symposium. 
Tjong Kim Sang, Erik F., Walter Daelemans, Herv? D?jean, 
Rob Koeling, Yuval Krymolowski, Vasin Punyakanok, 
Dan Roth, 2000. ?Applying System Combination to Base 
Noun Phrase Identification?, In the proceedings of the 
18th COLING. 
Uchimoto, Kiyotaka, Qing Ma, Masaki Murata, Hiromi 
Ozaku and Hitoshi Isahara, 2000. ?Named Entity 
Extraction Based on A Maximum Entropy Model and 
Transformation Rules", In proceedings of the 38th 
Annual Meeting of the Association for Computational 
Linguistics. 
Wacholder, Nina, Yael Ravin and Misook Choi (1997) 
"Disambiguation of Proper Names in Text", Proceedings 
of the 5th Applied Natural Language Processing 
Conference. 
Yarowsky, David, 1995. "Unsupervised Word Sense 
Disambiguation Rivaling Supervised Methods", In 
Proceedings of the 33rd Annual Meeting of the 
Association for Computational Linguistics. 
 Word Sense Disambiguation using Static and Dynamic Sense 
Vectors 
 
Jong-Hoon Oh, and Key-Sun Choi 
Computer Science Division, Dept. of EECS, Korea Advanced Institute of Science & Technology 
(KAIST) / Korea Terminology Research Center for Language and Knowledge Engineering 
(KORTERM), 373-1, Guseong-dong, Yuseong-gu, Daejeon, 305-701, Korea 
Email: {rovellia,kschoi}@world.kaist.ac.kr 
 
Abstract  
It is popular in WSD to use contextual 
information in training sense tagged data. 
Co-occurring words within a limited 
window-sized context support one sense 
among the semantically ambiguous ones of 
the word. This paper reports on word sense 
disambiguation of English words using 
static and dynamic sense vectors. First, 
context vectors are constructed using 
contextual words 1  in the training sense 
tagged data. Then, the words in the context 
vector are weighted with local density. 
Using the whole training sense tagged data, 
each sense of a target word2 is represented 
as a static sense vector in word space, which 
is the centroid of the context vectors. Then 
contextual noise is removed using a 
automatic selective sampling. A automatic 
selective sampling method use information 
retrieval technique, so as to enhance the 
discriminative power. In each test case, a 
automatic selective sampling method 
retrieves N relevant training samples to 
reduce noise. Using them, we construct 
another sense vectors for each sense of the 
target word. They are called dynamic sense 
vectors because they are changed according 
to a target word and its context. Finally, a 
word sense of a target word is determined 
using static and dynamic sense vectors. The 
English SENSEVAL test suit is used for this 
experimentation and our method produces 
relatively good results. 
                                                     
1
 ?Contextual words? is defined as a list of content 
words in context. 
2
 In this paper, a target word ?Wt? is a semantically 
1. Introduction 
It is popular in WSD to use contextual 
information in training data (Agirre, et al, 
19963; Escudero, et al, 2000; Gruber, 1991; 
Schutze, 1998). Co-occurring words within a 
limited window-sized context support one sense 
among the semantically ambiguous ones of the 
word. The problem is to find the most effective 
patterns in order to capture the right sense. It is 
true that they have similar context and 
co-occurrence information when words are used 
with the same sense (Rigau, et al, 1997). It is 
also true that contextual words nearby an 
ambiguous word give more effective patterns or 
features than those far from it (Chen, et al, 
1998). In this paper, we represent each sense of 
a word as a vector in word space. First, 
contextual words in the training sense tagged 
data4 are represented as context vectors. Then, 
                                                                               
ambiguous word in a given context of ?Wt?. This 
context may consist of several sentences and it is 
represented by ?contextual words?. 
3
 Agirre et al, (1996) defines a term ?conceptual 
density? based on how many nodes are hit between 
WordNet node and target words+contexts. Unlike 
?Conceptual density?, ?local density? used in this 
paper does not use any semantic net like WordNet 
but use only the contextual words surrounding the 
given target word.. 
4
 In this paper, the English SENSEVAL-2 data for 
the lexical sample task is used as training sense 
tagged data. It is sampled from BNC-2, the Penn 
Treebank (comprising components from the Wall 
Street Journal, Brown, and IBM manuals) and so on. 
All items in the lexical sample are specific to one 
word class; noun, verb or adjective. Training sense 
tagged data is composed of training samples that 
support a certain sense of a target word. They contain 
 the words in the context vector are weighted 
with local density. Then, each sense of a target 
word can be represented as a sense vector, which 
is the centroid of the context vectors in word 
space. 
However, if training samples contain noise, it is 
difficult to capture effective patterns for WSD 
(Atsushi, et al, 1998). Word occurrences in the 
context are too diverse to capture the right 
pattern for WSD. It means that the dimension of 
contextual words will be very large when we 
will use all words in the training samples for 
WSD. To avoid the problems, we use an 
automatized hybrid version of selective 
sampling that will be called ?automatic selective 
sampling?. This automatization is based on 
cosine similarity for the selection.  For a given 
target word and its context, this method retrieves 
N-best relevant training samples using the cosine 
similarity. Using them, we can construct another 
sense vectors for each sense of the target word. 
The relevant training samples are retrieved by 
comparing cosine similarities between given 
contexts and indexed context vectors of training 
samples. The ?automatic selective sampling? 
method makes it possible to use traning samples 
which have higher discriminative power.  
 
This paper is organized as follows: section 2 
shows details of our method. Section 3 deals 
with experiments. Conclusion and future works 
are drawn in sections 4. 
2 Word Sense Disambiguation 
Method 
2.1 Overall System Description 
Figure 1 shows the overall system description. 
The system is composed of a training phase and 
a test phase. In the training phase, words in the 
limited context window of training samples, 
which contains a target word and its sense, are 
extracted and the words are weighted with local 
density concept (section 2.2). Then, context 
vectors, which represent each training sample, 
are indexed and static sense vectors for each 
                                                                               
a target word , its sense and its context. But the sense 
of contexual words is not annotated in the training 
samples (SENSEVAL-2, 2001) 
 
sense are constructed. A static sense vector is the 
centroid of context vectors of training samples 
where a target word is used as a certain sense 
(section 2.3). For example, two sense vectors of 
?bank? can be constructed using context vectors 
of training samples where ?bank? is used as 
?business establishment? and those where ?bank? 
is used as ?artificial embankment?. Each context 
vector is indexed for  ?automatic selective 
sampling?. 
Training samples
Word extraction in 
local context
Term weighting with 
local density
Indexing context 
vector
Context vectors
for each training 
sample
Constructing static 
sense vectors for 
each sense
Index for 
each training 
sample
Static sense 
vectors for 
each sense
Automatic selective 
sampling
Estimating a word 
sense
Dynamic sense 
vectors for the given 
context
Test samples
Word Senses
Constructing
Dynamic vectors
Retrieved N training 
samples
Training Testing
Morphological
analyzer
 
Fig. 1 The overall system description 
In the test phase, contextual words are extracted 
with the same manner as in the training phase 
(section 2.5). Then, the ?automatic selective 
sampling? module retrieves top-N training 
samples. Cosine similarity between indexed 
context vectors of training samples, and the 
context vector of a given test sample provides 
relevant training samples. Then we can make 
another sense vectors for each sense using the 
retrieved context vectors. Since, the sense 
vectors produced by the automatic selective 
sampling method are changed according to test 
samples and their context, we call them dynamic 
sense vectors in this paper (section 2.4) (Note 
that, the sense vectors produced in the training 
phase are not changed according to test samples. 
Thus, we call them static sense vectors.) 
The similarities between dynamic sense vectors, 
and a context vector of a test sample, and those 
between static sense vectors and the context 
vector of the test sample are estimated by cosine 
measure. The sense with the highest similarity is 
selected as the relevant word sense. 
 
Our proposed method can be summarized as 
follows 
Training Phase 
 1) Constructing context vectors using 
contextual words in training sense 
tagged data. 
2) Local density to weight terms in 
context vectors. 
3) Creating static sense vectors, which 
are the centroid of the context 
vectors. 
Test Phase 
1) Constructing context vectors using 
contextual words in test data. 
2) Automatic selective sampling of 
training vectors in each test case to 
reduce noise. 
3) Creating dynamic sense vectors, 
which are the centroid of the 
training vectors for each sense. 
4) Estimating word senses using static 
and dynamic sense vectors. 
2.2 Representing Training Samples as a 
Context Vector with Local Density 
In WSD, context must reflect various contextual 
characteristics5. If the window size of context is 
too large, the context cannot contain relevant 
information consistently (Kilgarriff et al, 2000). 
Words in this context window6 can be classified 
into nouns, verbs, and adjectives. The classified 
words within the context window are assumed to 
show the co-occurring behaviour with the target 
word. They provide a supporting vector for a 
certain sense. Contextual words nearby a target 
word give more relevant information to decide 
its sense than those far from it. Distance from a 
target word is used for this purpose and it is 
calculated by the assumption that the target 
words in the context window have the same 
sense (Yarowsky, 1995).  
Each word in the training samples can be 
weighted by formula (1). Let Wij(tk) represent a 
weighting function for a term tk, which appears 
in the jth training sample for the ith sense, tfijk 
                                                     
5  POS, collocations, semantic word associations, 
subcategorization information, semantic roles, 
selectional preferences and frequency of senses are 
useful for WSD (Agirre et al, 2001). 
6  Since, the length of context window was 
considered when SENSEVAL-2 lexical sample data 
were constructed, we use a training sample itself as 
context window. 
represent the frequency of a term tk in the jth 
training sample for the ith sense, dfik  represent 
the number of training samples for the ith sense 
where a term tk appears, Dijk represent the 
average distance of a term tk from the target 
word in the jth training sample for the ith sense, 
and Ni represent the number of training samples 
for the ith sense.  
 
ij
ijk
kij Z
Z
tW =)(     (1) 
where,  
???
?
???
?
???=
ik
ik
ijk
ijkijk N
N
DF
df
D
tfZ 1  
?? ==
sensesall
ikk
sensesall
i dfDFNN
__
   ,
 
( )?
=
=
termof
k
ijkij ZZ
__#
1
2
 
In formula (1), Z is a normalization factor, 
which forces all values of Wij(tk) to fall into 
between 0 and 1, inclusive (Salton et al, 1983). 
Formula (1) is a variation of tf-idf. We regard 
each training sample as indexed documents, 
which we want to retrieve and a test sample as a 
query in information retrieval system. Because 
we know a target word in training samples and 
test samples, we can restrict search space into 
training samples, which contain the target word 
when we find relevant samples. We also take 
into account distance from the target word.  
Dijk and dfik in formula (1) support a local 
density concept. In this paper, ?local density? of 
a target word ?Wt? is defined by the density 
among contextual words of ?Wt? in terms of their 
in-between distance and relative frequency. First, 
the distance factor is one of the important clues 
because contextual words surrounding a target 
word frequently support a certain sense: for 
example, ?money? in ?money in a bank?. 
Second, if contextual words frequently co-occur 
with a target word of a certain sense, they may 
be a strong evidence to decide what word sense 
is correct. Therefore, contextual words, which 
more frequently appear near a target word and 
appear with a certain sense of a target word, 
have a higher local density. 
With the local density concept, context of 
training samples can be represented by a vector 
 with context words and their weight, such that 
(wij(t1),wij(t2),?.,wij(tn)). When Wij(tk) is 1, it 
means that tk is strong evidence for the ith sense. 
(Zijk are much larger than others.) 
2.3 Constructing Static Sense Vectors  
Now, we can represent each training sample as 
context vectors using contextual words such that 
vij=(wij(t1),wij(t2),?.,wij(tn)) where vij represents a 
context vector of the jth training sample for the ith 
sense and wij(tk) is the weight of a term tk 
calculated by formula (1). 
||
||
1
i
N
j
ij
i N
v
SV
i?
=
=
    (2) 
Context vectors 
for Sense 1 
Context vectors 
for Sense 2
Context vector 
for Sense n
?
2SV
1SV
nSV
Fig.2 A graphical representation of static sense 
vectors 
 
Throughout clustering the context vectors, each 
sense can be represented as sense vectors. Let Ni 
represent the number of training samples for the 
ith sense, and vij represent the context vector of 
the jth training sample for the ith sense. The static 
sense vector for the ith sense, SVi, can be 
represented by formula (2) (Park, 1997). In 
formula (2), SVi is the centroid of context 
vectors of training samples for the ith sense as 
shown in figure 2. In figure 2, there are n senses 
and context vectors, which represent each 
training sample. We can categorize each context 
vector according to a sense of a target word. 
Then, each sense vectors are acquired using 
formula (2). Because the sense vectors are not 
changed according to test samples, we call them 
a static sense vector in this paper (note that sense 
vectors, which we will describe in section 2.4, 
are changed depending on the context of test 
samples). 
2.4 Automatic selective sampling: Dynamic 
Sense Vectors  
It is important to capture effective patterns and 
features from the training sense tagged data in 
WSD. However, if there is noise in the training 
sense tagged data, it makes difficult to 
disambiguate word senses effectively. To reduce 
its negative effects, we use a automatic selective 
sampling method using cosine similarity. Figure 
3 shows the process of a automatic selective 
sampling method. The upper side shows 
retrieval process and the lower side shows a 
graphical representation of dynamic sense 
vectors.  
Sense 1 Sense 2 Sense n
..
Retrieved 
Training 
Samples
..
..
A target word
DSV1 DSV2 DSVn
?
?
A Context vector for a test sample
Indexed 
Training 
Samples 
Context vectors 
for Sense 1
Context vectors 
for Sense 2
Context vectors 
for Sense n
?2DSV
1DSV
nDSV A context vector 
of a test sample
Retrieved top-N 
training sample
Fig. 3 A graphical representation of an automatic 
selective sampling method  
 
For example, let ?bank? have two senses 
(?business establishment?, ?artificial 
embankment?). Now, there are indexed training 
samples for the two senses. Then top-N training 
samples can be acquired for a given test sample 
containing a target word ?bank?. The retrieved 
 training samples can be clustered as Dynamic 
Sense Vectors according to a sense of their 
target word. Since, the sense vectors produced 
by a automatic selective sampling method are 
changed according to the context vector of a test 
sample, we call them dynamic sense vectors in 
this paper.  
Let RTi represent the number of training samples 
for the ith sense in the retrieved top-N, and vij 
represent a context vector of the jth training 
sample for the ith sense in the top-N. The 
dynamic sense vector for the ith sense of a target 
word, DSVi, is formulated by formula (3). In 
formula (3), DSVi means the centroid of the 
retrieved context vectors of training samples for 
the ith sense as shown in the lower side of 
figure.3 
||
||
1
i
RT
j
ij
i RT
v
DSV
i?
=
=
   (3) 
2.5 Context Vectors of a Test Sample 
Contextual words in a test sample are extracted 
as the same manner as in the training phase. The 
classified words in the limited window size ? 
nouns, verbs, and adjectives ? offer components 
of context vectors. When a term tk appears in the 
test sample, the value of tk in a context vector of 
the test sample will be 1, in contrary, when tk 
does not appear in the test sample, the value of tk 
in a context vector of the test sample will be 0. 
Let contextual words of a test sample be ?bank?, 
?river? and ?water?, and dimension of context 
vector be (?bank?, ?commercial?, ?money?, 
?river?, ?water?). Then we can acquire a context 
vector, CV =(1,0,0,1,1), from the test sample. 
Henceforth we will denote CVi as a context 
vector for the ith test sample. 
2.6 Estimating a Word Sense: Comparing 
Similarity 
We described the method for constructing static 
sense vectors, dynamic sense vectors and 
context vectors of a test sample. Next, we will 
describe the method for estimating a word sense 
using them. The similarity in information 
retrieval area is the measure of how alike two 
documents are, or how alike a document and a 
query are. In a vector space model, this is 
usually interpreted as how close their 
corresponding vector representations are to each 
other. A popular method is to compute the 
cosine of the angle between the vectors (Salton 
et al, 1983). Since our method is based on a 
vector space model, the cosine measure (formula 
(4)) will be used as the similarity measure. 
Throughout comparing similarity between SVi 
and CVj and between DSVi and CVj for the ith 
sense and the jth test sample, we can estimate the 
relevant word sense for the given context vector 
of the test sample. Formula (5) shows a 
combining method of sim(SVi,CVj) and 
sim(DSVi,CVj). Let CVj represent the context 
vector of the jth test sample, si represent the ith 
sense of a target word, and Score(si,CVj) 
represent score between the ith  sense and the 
context vector of the jth test sample. 
 
??
?
==
=
=
N
i i
N
i i
N
i ii
wv
wv
wvsim
1
2
1
2
1),(  (4) 
where, N represents the dimension of the vector 
space, v and w represent vectors.  
 
 
),()1(
),(
),(maxarg
ji
ji
ji
s
CVDSVsim
CVSVsim
CVsScore
i
??
+?
=
?
?
  (5) 
where ?  is a weighting parameter.  
  
Because the value of cosine similarity falls into 
between 0 and 1, that of Score(si,CVj) also exists 
between 0 and 1. When similarity value is 1 it 
means perfect consensus, in contrary, when 
similarity value is 0 it means there is no part of 
agreement at all. After all, the sense having 
maximum similarity by formula (5) is decided as 
the answer.  
3. Experiment 
3.1 Experimental Setup 
In this paper, we compared six systems as 
follows. 
The system that assigns a word sense 
which appears most frequently in the 
training samples (Baseline) 
The system by the Na?ve Bayesian 
method (A) (Gale, et al, 1992) 
 The system that is trained by 
co-occurrence information directly 
without changing. (only with term 
frequency) (B) 
The system with local density and 
without automatic selective sampling 
(C)  
The system with automatic selective 
sampling and without local density (D)  
The system with local density and 
automatic selective sampling (E) 
System A was used to compare our method with 
the other method. System B, C, D, and E will 
show the performance of each component in our 
proposed method. To evaluate performance in 
the condition of ?without local density (system B 
and D)?, we weight each word with its frequency 
in the context of training samples.  
The test suit used is the English lexical samples 
released for SENSEVAL-2 in 2001. This test 
suit supplies training sense tagged data and test 
data for noun, verb and adjective 
(SENSEVAL-2, 2001). 
Cross-validation on training sense tagged data is 
used to determine the parameters ? ?  in 
formula (5) and top-N in constructing dynamic 
sense vectors. We divide training sense tagged 
data into ten folds with the equal size, and 
determine each parameter, which makes the best 
result in average from ten-fold validation. The 
values, we used, are 2.0=? , and 50 =N . 
The results were evaluated by precision rates 
(Salton, et al, 1983). The precision rate is 
defined as the proportion of the correct answers 
to the generated results. 
3.2 Experimental Results 
 Noun Verb Adjective Total 
Baseline 50.97% 40.34% 58.04% 47.60% 
A 44.04% 32.48% 43.43% 39.09% 
B 24.33% 21.31% 26.92% 23.50% 
C 44.44% 33.81% 45.38% 40.15% 
D 65.47% 49.64% 66.84% 59.09% 
E 66.89% 53.74% 70.74% 62.07% 
Table 1. Experimental results  
Table 1 shows experimental results. In the result , 
all systems and baseline show higher 
performance on noun and adjective than verb. 
This indicates that the disambiguation of verb is 
more difficult than others in this test suit. In 
analysing errors, we found that we did not 
consider important information for 
disambiguating verb senses such as adverbs, 
which can be used as idioms with the verbs. For 
example, ?carry out?, ?pull out? and so on. It is 
necessary to handle them for more effective 
WSD.  
System B, C, D, and E show how effective local 
density and dynamic vectors are in WSD. The 
performance increase was shown about 70% 
with local density (system C) and about 150% 
with dynamic vectors (system D), when they are 
compared with system B ? without local density 
and dynamic vectors. This shows that local 
density is more effective than term frequency. 
This also shows that automatic selective 
sampling of training samples in each test sample 
is very important. 
Combining local density and dynamic vectors 
(system E), we acquire about 62% performance. 
Our method also shows higher performance than 
baseline and system A (the Na?ve Bayesian 
method) ? about 30% for baseline and about 
58% for system A. 
As a result of this experiment, we proved that 
co-occurrence information throughout the local 
density and the automatic selective sampling is 
more suitable and discriminative in WSD. This 
techniques lead up to 70% ~ 150% performance 
improvement in the experimentation comparing 
the system without local density and automatic 
selective sampling. 
4. Conclusion 
This paper reported about word sense 
disambiguation for English words using static 
and dynamic sense vectors. Content words ? 
noun, verb, and adjective ? in the context were 
selected as contextual words. Local density was 
used to weight words in the contextual window. 
Then we constructed static sense vectors for 
each sense. A automatic selective sampling 
method was used to construct dynamic sense 
vectors, which had more discriminative power, 
by reducing the negative effects of noise in the 
training sense tagged data. The answer was 
decided by comparing similarity. Our method is 
simple but effective for WSD.  
Our method leads up to 70~150% precision 
improvement in the experimentation comparing 
 the system without local density and automatic 
selective sampling. We showed that our method 
is simple but effective. Our method was 
somewhat language independent, because our 
method used only POS information. Syntactic 
and semantic features such as dependency 
relations, approximated word senses of 
contextual words and so on may be useful to 
improve the performance of our method. 
References  
Agirre, E. and G. Rigau (1996) Word Sense 
Disambiguation using Conceptual Density, 
Proceedings of 16th International Conference on 
Computational Linguistics(COLING96), 
Copenhagen, Denmark. 
Agirre, E. and D. Martinez, (2001) Knowledge 
Sources for Word Sense Disambiguation, 
Proceedings of the Fourth International 
Conference (TSD 2001). 
Fujii, Atsushi , Kentaro Inui, Takenobu Tokunaga, 
and Hozumi Tanaka, (1998) Selective Sampling for 
Example-based Word Sense Disambiguation, 
Computational Linguistics, 24(4), pp. 573-597. 
Escudero, G., L. M?rquez and G. Rigau (2000) 
Boosting Applied to Word Sense Disambiguation, 
Proceedings of the 11th European Conference on 
Machine Learning (ECML 2000) Barcelona, Spain. 
2000. Lecture Notes in Artificial Intelligence 1810. 
R. L. de M?ntaras and E. Plaza (Eds.). Springer 
Verlag. 
Gale, William A., Kenneth W. Church, and David 
Yarowsky (1992) A Method for Disambiguating 
Word Senses in a Large Corpus. Computers and 
Humanities, 26, 415-439. 
Gruber, T. R. (1991) Subject-Dependent 
Co-occurrence and Word Sense Disambiguation, 
Proceedings of 29th Annual Meeting of the 
Association for Computational Linguistics. 
Schutze, Hinrich (1998) Automatic Word Sense 
Discrimination. Computational Linguistics, 24(1), 
97-123.  
Chen , Jen Nan and Jason S. Chang (1998) A 
Concept-based Adaptive Approach to Word Sense 
Disambiguation, Proceedings of 36th Annual 
Meeting of the Association for Computational 
Linguistics and  17th International Conference on 
Computational Linguistics (COLING/ACL-98) pp 
237-243. 
Kilgarriff, A. and J. Rosenzweig, (2000) English 
SENSEVAL: Report and Results, Proceedings of 
2nd International Conference on Language 
Resources & Evaluation (LREC 2000), Athens.  
Park,Y.C (1997) ?Building word knowledge for 
information retrieval using statistical information?, 
Ph.D. thesis, Department of Computer Science, 
Korea Advanced Institute of Science and 
Technology. 
Rigau, G., J. Atserias and E. Agirre, (1997) 
Combining Unsupervised Lexical Knowledge 
Methods for Word Sense Disambiguation, 
Proceedings of joint 35th Annual Meeting of the 
Association for Computational Linguistics and 8th 
Conference of the European Chapter of the 
Association for Computational Linguistics 
(ACL/EACL?97), Madrid, Spain. 
Salton, G. and M.  McGill, (1983) Introduction to 
Modern Information Retrieval, McGraw-Hill, New 
York. 
SENSEVAL-2 (2001)http://www.sle.sharp.co.uk/ 
senseval2/ 
Yarowsky, D. (1995) Unsupervised Word Sense 
Disambiguation Rivaling Supervised Methods, In 
Proceedings of the 33rd Annual Meeting of the 
Association for Computational Linguistics, 
Cambridge, MA, 189-196. 
 An English-Korean Transliteration Model Using Pronunciation and 
Contextual Rules 
 
Jong-Hoon Oh, and Key-Sun Choi 
Computer Science Division, Dept. of EECS, Korea Advanced Institute of Science & Technology 
(KAIST) / Korea Terminology Research Center for Language and Knowledge Engineering 
(KORTERM), 373-1, Kusong-dong, Yusong-gu, Taejon, 305-701, Korea 
Email: {rovellia,kschoi}@world.kaist.ac.kr 
 
Abstract  
There is increasing concern about 
English-Korean (E-K) transliteration 
recently. In the previous works, direct 
converting methods from English 
alphabets to Korean alphabets were a 
main research topic. In this paper, we 
present an E-K transliteration model using 
pronunciation and contextual rules. Unlike 
the previous works, our method uses 
phonetic information such as phoneme 
and its context. We also use word 
formation information such as English 
words of Greek origin. With them, our 
method shows significant performance 
increase about 31% in word accuracy.  
1.Introduction 
In Korean, many technical terms in a domain 
specific text, especially science and engineering 
are from foreign origin. Sometimes they are 
written in their original forms and sometimes 
they are transliterated into Korean words in 
various forms. This makes difficult to handle 
them in natural language processing. Especially 
information retrieval, words with the same 
meanings are treated as different ones because of 
their different forms.  
One possible solution can be a dictionary, which 
contains English words and their possible 
transliterated forms. However, this is not a 
practical solution because technical terms, which 
mainly cause the problem, usually have rich 
productivity. The other solution can be 
automatic transliteration. There have been works 
on automatic transliteration from English to 
other languages ? English to Japanese (Kang et 
al., 1996; Knight et al, 1997), and English to 
Korean (Kang et al, 2000; Kang et al, 2001; 
Kim et al, 1999; Lee et al, 1998).  
In E-K transliteration, direct converting methods 
from English alphabet to Korean alphabet were a 
main research topic (Kang et al, 2000; Kang et 
al., 2001; Kim et al, 1999; Lee et al, 1998). In 
the works, machine learning techniques such as 
a decision tree and a neural network were used.  
However, transliteration is more phonetic 
process than orthographic process: ?h? in the 
Johnson does not make any Korean character 
(Knight et al, 1997). Therefore, patterns for E-K 
transliteration acquired from English/Korean 
alphabets as in the previous works, may not be 
effective. In the previous works, they did not 
consider origin of English ? pure English (e.g., 
board), English words with Greek origin (e.g., 
hernia) and so on In E-K transliteration, origin 
of English words determine the way of 
transliteration. Our method uses phonetic 
information such as phoneme and its context as 
well as orthography. English words of Greek 
origin are also considered in transliteration. 
This paper organized as follows. In section 2, we 
survey related works. In section 3, we will 
describe the details of our method. In section 4, 
the results of experiments are represented. 
Finally, the conclusion follows in section 5. 
2. Related works 
2.1 Probability based transliteration 
(Lee et al, 1998) used formula (1) to generate a 
transliterated Korean word ?K? for a given 
English word ?E?. Lee et al (1998) defined a 
pronunciation unit. It is a chunk of graphemes or 
alphabets that can be mapped to phoneme. They 
divided an English word into pronunciation units 
 (PUs) for transliteration. For example, an 
English word ?board (/B AO R D/)? can be 
divided into ?b/B/: oa/AO/: r/R/: d/D/?1 ? ?b?, 
?oa?, ?r? and ?d? are PUs. An English word ?E? 
was represented as ?E=epu1,epu2,?,epun? where 
epui was the ith PU. Sequences of Korean PUs, 
K1,K2,?,Km, where  ?Ki= kpui1,kpui2,?,kpuin? 
were generated according to epui. Lee et al 
(1998) considered all possible English PU 
sequences and corresponding Korean PU 
sequences for a given English word, because its 
pronunciation was not determined. For example, 
?data? can have PU sequences such as ?d :at :a?, 
?da :ta?, ?d :a :t :a? and so on. If the total number 
of English PU in E is N and the average number 
of kpui generated by epui is M, the total number 
of generated Korean PU sequences will be about 
N*M. Then he selected the best result among 
them as a Korean transliteration word. 
)|()(maxarg)|(maxarg KEpKpEKp
KK
= (1) 
?
=
?
?
n
i
ii kpukpupkpupKP
2
11 )|()()(      (2) 
?
=
?
n
i
ii kpuepupKEP
1
)|()|(       (3) 
Kim et al, (1999) used the same formula as 
Lee?s (1998) except P(E|K) (formula(4)). He 
used additional information ? Korean PUs kpui-1 
and kpui+1 ? and used a neural network to 
approximate P(E|K). 
?
=
+??
n
i
iiii kpukpukpuepupKEP
1
11 ),,|()|(  (4) 
Probability based transliteration showed about 
40% precision on E-K transliteration with 1,500 
E-K pairs for training and 150 E-K pairs for 
testing. 
2.2 Decision Tree based transliteration 
Kang, et al (2000; 2001) proposed an English 
alphabet-to-Korean alphabet conversion method 
based on a decision tree. This method used six 
attribute values ? left three English alphabets 
and right three English alphabets ? for 
determining Korean alphabets corresponding to 
English alphabets. For each English alphabet, its 
corresponding decision trees are constructed. 
Table 1 shows an example of transliteration for 
an English word ?data?. In table 1, (E) represents 
                                                     
1
 Henthforce, ?:? will be used as a PU boundary 
a current English alphabet, K represents 
generated Korean alphabets by decision trees. 
L3 L2 L1 (E) R1 R2 R3  K 
< < < d a t a  ?d? 
< < d a t a >  ?e-i? 
< d a t a > >  ?t? 
d a t a > > >  ?a? 
Table 1. An example of decision tree based 
transliteration 
This method showed about 49% precision for 
6,185 E-K pairs for training and 1,000 E-K pairs 
for testing. 
 
Though the previous works showed relatively 
good results, they also showed some limitations. 
Because they focused on a converting method 
from English alphabet to Korean alphabet, they 
did not consider phonetic features such as 
phoneme and word formation features such as 
origin of English. This makes some errors when 
pronunciation and origin of English were 
important clues for transliteration - ?Mcdonald? 
(pronunciation is needed) and ?amylase? (origin 
of English word is needed). 
3. An English-Korean Transliteration 
Model using Pronunciation and 
Contextual Rules 
3.1 Overall System Description 
Figure1 shows the overall system description. 
Our method is composed of two phases ? 
alignment (section 3.2) and transliteration 
(section 3.3, 3.4, 3.5 and 3.6).  
First an English pronunciation unit2 (hearafter, 
EPU) and its corresponding phoneme are 
aligned. EPU-to-Phoneme alignment is to find 
out the most phonetically probable 
correspondence between an English 
pronunciation unit and phoneme. EPU to 
phoneme aligned results acquired from the 
alignment algorithm offer training data for 
estimating pronunciation of English words, 
which are not registered in a pronunciation 
dictionary, for example ?zinkenite?. Second, 
English words are transliterated into Korean 
words through several steps. Using an English 
                                                     
2
 The term ?pronunciation unit? will be used as the same 
meaning as in the Lee?s (Lee et al, 1998) 
 pronunciation dictionary (P-DIC), we can assign 
pronunciation to a given English word. When it 
is not registered in P-DIC, we investigate that it 
has a complex word form (section 3.3). For 
detecting a complex word form, we divide a 
given English word into two words 
(word+word)3 using entries of P-DIC. If both of 
them are in P-DIC, we can assign pronunciation 
to the given word otherwise we should estimate 
pronunciation (section 3.5). Then, we check 
whether the English word is from Greek origin 
or not (section 3.4). Because a way of E-K 
transliteration for the English words of Greek 
origin is different from that for pure English 
words, it is important to detect them. 
Pronunciation for English words, which are not 
registered in a P-DIC, is estimated (section 3.5) 
in the next step. Finally, Korean transliterated 
words are generated using conversion rules 
(section 3.6). The right side of figure 1 shows a 
transliteration example for an English word, 
?cutline?. 
Pronunciation
dictionary English  words
Dictionary Search
Detecting Complex Word forms
Detecting English words
of Greek origin
no
no
Estimating
pronunciation 
for E-class
Phoneme to Korean conversion
yes
yes
Estimating
pronunciation 
for G-class
noyes
EPU-P 
alignment
EPU-P 
Alignment
results
Training data
for estimating
pronunciation 
( E and G  
class) 
Detecting
English words
of Greek origin
Korean transliterated words
cutline
keo-teu-la-in
Complex word forms ?
(Yes)
Registered in 
a pronunciation
dictionary? (No)
Pronunciation to 
Korean conversion
[C/K]:[u/AH]:[T/T]
[L/L]:[I/AY]:[ne/N]
Fig. 1 Overall system description 
3.2 EPU-to-Phoneme Alignment 
EPU-to-Phoneme (hereafter, EPU-P) alignment 
is to find out the most phonetically probable 
correspondence between an English 
pronunciation unit and phoneme. For example, 
one of the possible alignment for an English 
word ?board? and its pronunciation ?/B AO R 
D/?4 is as follows. 
                                                     
3
 ?broadcasting? may be divided into three words : ?broad?, 
?cast? and ?ing?. But from the training corpus and 
pronunciation dictionary, all of complex word is divided 
into two words like ?broad? and ?casting?. 
4 (www.cs.cmu.edu/~laura/pages/arpabet.ps):  ARPAbet 
symbol will be used for representing phonemes. ARPAbet 
English b oa r d 
 | | | | 
Pronunciation /B/ /AO/ /R/ /D/ 
Table 2. One possible alignment between English 
word ?board? and its pronunciation 
For automatic EPU-P alignment, we used the 
modified version of Kang?s E-K alignment 
algorithm (Kang et al, 2000; Kang et al, 2001). 
It is based on Covington?s algorithm (Covington, 
1996). Covington views an alignment as a way 
of stepping through two words ? a word in one 
side and a word in the other side ? while 
performing ?match? or ?skip? operation on each 
step. Kang added ?forward bind? and ?backward 
bind? operations to consider one-to-many, 
many-to- one and many-to-many alignments 
Operation Condition Penalty 
Similar C/CP 0 
V/VP 0 
V/SVP or C/SVP 30 
Dissimilar C/CP 240 
Match 
V/CP or C/VP 250 
Similar C/CP 0 
V/VP 0 
V/SVP or C/SVP 30 
Dissimilar C/CP 190 
Bind 
V/CP or C/VP 200 
Table 3. Penalty metrics: C, V, CP, VP, and SVP 
represent consonants, vowels, consonant 
phonemes, vowel phonemes 5  and semi-vowel 
phonemes respectively. 
English b o a r D Total 
Operation M M < M M Penalty 
Pronunciation B AO < R D  
Penalty +0 +0 +0 +0 +0 0 
Table 4. The best alignment result for an English 
word ?board?. ?M? represents ?match?, and ?<? 
represents ?backward bind?. 
Unlike the previous alignment algorithm, we 
combine ?skip? and ?bind? operations because 
the ?skip? operation can be replaced with the 
?bind? operation. This makes all PUs to be 
mapped into phoneme. It means that our 
algorithm does not allow null-to-phoneme 
alignment or PU-to-null alignment. All the valid 
alignments that are possible by ?match?, and 
?bind? operations can be generated. Alignment 
                                                                               
is one of the method for coding phonemes into ASCII 
chracters. 
5
 In this paper, vowel pronunciation includes diphthongs. 
 may be interpreted as finding the best result 
among them. To find the best result, a penalty 
scheme is used ? the best alignment result is one 
that has the least penalty values. Since Kang?s 
method focused on an E-K character alignment, 
a penalty scheme and an E-K 
character-matching table were restricted to an 
E-K alignment. Instead of Kang?s E-K character 
penalty scheme, we developed an EPU-P penalty 
scheme and an EPU-P matching table using 
manually aligned EPU-P data. We assume that 
all vowels can be aligned with all vowel 
phonemes without penalty. Table 3 shows our 
penalty metrics and table 4 shows an example of 
EPU-P alignment. 
We aligned about 120,000 English word and 
Pronunciation pairs in ?The CMU Pronouncing 
Dictionary?. For evaluating performance of the 
alignment, we randomly selected 100 results. 
The performnance of EPU-P alignment is 99%. 
3.3 Dealing with a Complex word form 
Some English words are not in P-DIC, because 
they are in a complex word form. In this paper, 
we define words in a complex word form as 
those composed of two base nouns in P-DIC. 
When a given word is not in P-DIC, it is 
segmented into all possible two words. If the 
two words are in P-DIC, we can assign their 
pronunciation. For example, ?cutline? can be 
segmented into ?c+utline?, ?cu+tline?, ?cut+line? 
and so on. ?cut+line? is the correct segmentation 
of ?cutline?, because ?cut? and ?line? are in the 
P-DIC. If words are not in P-DIC and they are 
not in a complex word form, we should estimate 
their pronunciation. The details of estimating 
pronunciation will be described in the section 
3.5. 
3.4 Detecting English words of Greek origin  
In Korean, there are two methods for E-K 
transliteration ? ?written word transliteration? 
and ?spoken word transliteration? (Lee et al, 
1998). The two methods use similar mechanism 
for consonant transliteration. However, ?written 
word transliteration? uses its character and 
?spoken word transliteration? uses its phoneme 
when they transliterate vowels. For example, ?a? 
in ?piano? can be transliterated into ?pi-a-no? 
with its character and ?pi-e-no? with its phoneme. 
Since, a vowel in a pure English word is usually 
transliterated using its phoneme and that in an 
English word of Greek origin is usually 
transliterated with its character in E-K 
transliteration- for example, ?hernia? 
(he-reu-ni-a), ?acacia? (a-ka-si-a), ?adenoid? 
(a-de-no-i-deu) and so on -, it is important to 
detect them. We use suffix and prefix patterns 
for detecting English words of Greek origin 
(Luschnig, 2001) 6  and table 5 7  shows the 
patterns. If words have the affixes in table 5, we 
determine them as words of Greek origin 
otherwise pure English words. 
Suffix -ic, -tic, -ac, -ics, -ical, -oid, -ite, -ast, 
-isk, -iscus, -ia, -sis, -me, -ma 
Prefix amphi-, ana-, anti-, apo-, dia-, dys-, ec-, 
ecto-, enantio-, endo-, epi-, cata-, cat-, 
meta-, met-, palin-, pali-, para-, par-, 
peri-, pros-, hyper-, hypo-, hyp- 
Table 5. Suffix and prefix patterns for detecting 
English words of Greek origin. 
3.5 Estimating Pronunciation  
Estimating pronunciation is composed of two 
steps. Using aligned EPU-P pairs as training 
data, we can find EPUs in the given English 
word (Chunking EPU) and assign their 
appropriate phoneme (EPU-to-Phoneme 
assignment). For dealing with English words of 
Greek origin, we categorize EPU-P aligned data 
into pure English words (E-class) and English 
words of Greek origin (G-class). Then we 
construct the ?Chunking EPU? module and the 
?EPU-to-Phoneme assignment? module for each 
class.  
?Chunking EPU? is to find out boundaries of 
EPUs in English words. For example, we can 
find EPUs in ?board? as ?b:oa:r:d?. For chunking 
EPU, we used C4.5 (Quilan, 1993) with ten 
attributes ? left five alphabets and right five 
alphabets and the setting shows the best result 
among various settings such as eight attributes 
(left four and right four - 87.2% ) and so on. 8. 
                                                     
6
 
38 Grek affixes out of 249 Latin and Greek affixes in 
120 categories described in (John, 1953) are used. 63 out of 
the 120 categories share the meaning though their form is 
somewhat different  
7
 In this paper, some Greek affixes are not used, because 
they such as prefix ?a-?, ?an-?, and postfix ?-y?, ?-m? may 
cause error. 
8
 C4.5 is one of the popular method for recognizing 
boundary of chunks. Unlike Kang et al, (2000)?s method, 
 We use 90% of EPU-P aligned data as training 
data and 10% of those as test data. Our 
?Chunking EPU? module shows 91.7% 
precision. 
)|()(maxarg)|(maxarg PEpPpEPp
PP
=   (5) 
?
=
?
?
n
i
ii pppppPP
2
11 )|()()(       (6) 
?
=
?
n
i
ii pepupPEP
1
)|()|(       (7) 
Then we can assign phoneme to each EPU. For 
the given EPU sequence ?E=epu1,epu2,?,epun? 
and its possible phoneme sequences P1,..,Pm 
where ?Pi=pi1,pi2,?,pin?, the ?EPU-to-Phoneme 
assignment? task is to find out the most probable 
phomene sequence ?Pi=pi1,pi2,?,pin?. It can be 
represented as formula (5). p(P) and p(E|P) are 
approximated as formula (6) and (7). 
3.6 Phoneme-to-Korean Conversion 
Our Phoneme-to-Korean (P-K) conversion 
method is based on English-to-Korean Standard 
Conversion Rule (EKSCR) (Ministry, 1995). 
EKSCR is composed of nine general rules and 
five rules for specific cases ? each rule contains 
several sub-rules. It describes a transliteration 
method from English alphabets or phonemes to 
Korean alphabets. It uses English phoneme as a 
transliteration condition ? if a phoneme is A 
then transliterate into a Korean alphabet B. 
However, EKSCR does not contain enough rules 
to generate correct Korean words for 
corresponding English words, because it mainly 
focuses on a way of mapping from one English 
phoneme to one Korean character without 
context of phonemes and PUs. For example, an 
English word ?board? and its pronunciation ?/B 
AO R D/?, are transliterated into ?bo-reu-deu? by 
EKSCR ? the correct transliteration is ?bo-deu?. 
In E-K transliteration, the phoneme ?R? before 
consonant phonemes and after vowel phonemes 
is rarely transliterated into Korean characters 
(Note that the phoneme ?R? in English words of 
Greek origin is transliterated into a Korean 
                                                                               
our method produces EPU and it phoneme. This makes 
possible for a E-K conversion method (in section 3.6) to 
use context of EPU and its phoneme. Because an 
alphabet-to-alphabet mapping method did not use EPU and 
its phoneme, it may show some errors when phoneme and 
its context are the most importnat clues, for example, 
?Mcdonald?. 
consonant ?r? frequently.) These contextual rules 
are very important to generate correct Korean 
transliterated words. 
 
We capture contextual rules by observing errors 
in the results, which are generated by applying 
EKSCR to 200 randomly selected words from 
the CMU pronunciation dictionary. The selected 
words are not in the test data in the experiment. 
Among the generated rules, we selected 27 
contextual rules with high frequency (above 5). 
Table 6 shows some rules and their conditions in 
which rules will be fired. There are three 
conditions ? ?Context?, ?TPU (Target PU)?, and 
?TP (Target Phoneme)?. In context condition, 
?[]?, ?{}?, C, VP, and CP represent phoneme, 
pronunciation unit, consonant, vowel phonemes 
and consonant phonemes respectively. The rule 
with context condition, ?[R] after VP and before 
CP?, is not fired for the English words of Greek 
origin. Except it, all rules are applied to both 
classes (E-class and G-class). 
Condition 
Context  TPU TP 
Korean 
Characters 
C+ {le} ?le? AH L ?eul? 
{or} in the end of 
a word 
?or? ER ?eo? 
{or} in a word ?or? ER ?eu? 
{sm} in the end 
of a word 
?sm? S AH M ?jeum? 
[R] after VP and 
before CP 
?r? ?R? ?eu? 
Table 6. Some contextual rules 
4.Experiment 
4.1 Experimental Setup 
We use two data sets for an accuracy test. Test 
Set I (Lee et al, 1998) is composed of 1,650 
E-K pairs. Since, the test set was used as a 
common testbed for (Lee et al, 1998; Kim et al, 
1999; Kang et al, 2000; Kang et al, 2001), we 
use them as a testbed for comparison between 
our method and other methods. For comparison, 
1,500 pairs are used as training data for other 
methods and 150 pairs are used as test data for 
our method and other methods. Test set II (Kang 
et al, 2000) consists of 7,185 E-K pairs ? the 
number of training data is 6,185 and that of test 
data is 1,000. We use Test set II to compare our 
 method with (Kang et al, 2000), which shows 
the best result among the previous works. 
Evaluation is performed by word accuracy (W. 
A.) and character accuracy (C.A.), which were 
used as the evaluation measure in the previous 
works (Lee and Choi 1998; Kim and Choi 1999; 
Kang and Choi 2000). 
wordsgeneratedof
wordscorrectofAW
  #
  #
.. =   (8) 
L
sdiLAC )(.. ++?=    (9) 
where L represents the length of the original 
string, and di, , and s  represent the number 
of insertion, deletion and substitution 
respectively. If )( sdiL ++< , we consider it 
as zero (Hall and Dowling, 1980). 
We perform the three experiments as follows. 
Comparison Test: Comparison between 
our method and the previous works 
Dictionary Test: Performance of 
transliteration for words in a 
pronunciation dictionary and that for 
others 
Component Test: Effectiveness of each 
component 
4.2 Experimental results 
4.2.1 Comparison Test 
Method C.A W.A 
[Lee et al, 1998] 69.3% 40.7%9 
[Kim et al, 1999] 79.0% 35.1% 
[Kang et al, 2000] 78.1% 37.6% 
Our method 90.82% 56.0% 
Table 7 Comparison test results for Test set I 
Method C.A W.A 
[Kang et al, 2000, 2001] 81.8% 48.7% 
Our method 92.86% 63.0% 
Table 8 Comparison test results for Test set II. 
Table 7 and 8 show results of comparison test 
for Test set I and Test set II respectively. In the 
tables our method shows higher performance 
especially in W.A. Moreover, our method shows 
higher performance in C.A. It means that the 
generated words by our method are more similar 
to the correct transliteration, when they are not 
the correct answer. 
                                                     
9
 with 20 higher rank results. 
4.2.2 Dictionary Test 
For the dictionary test, we use test data of Test 
set II. In the result, ?registered? words show 
higher performance. It can be analysed that 
contextual rules are constructed using registered 
words in a P-DIC and estimating pronunciation 
module makes some errors. However, ?not 
registered? words also show relatively good 
performance. 
 C.A W.A # of words 
Registered 93.49% 67.83% 687 
Not 
registered 
91.47% 52.40% 313 
Table 9. Dictionary test results. 
4.2.3 Component Test 
For the component test, we use words, which are 
?not registered? in Test set II. Components, 
which are tested in ?Component test? are 
?Dealing with words in a complex word 
form?[C], ?Detecting English words of Greek 
origin? [G], and ?Contextual rules? [R]. In the 
result, [G] and [R] show good results in contrary 
to [C]. There are so few words in complex word 
forms that [C] does not show significant 
performance improvement though the 
performance is relatively good ?about 70% W.A. 
for 43 words (43 words out of total 313 words). 
For the effective comparison, it will be 
necessary to consider the number of words, 
which each component handles. Our method 
shows better performance than ?W/O 
[R]?(EKSCR). It indicates that contextual rules 
are important. 
Method C.A. W.A. 
W/O [C], [G], and [R] 87.90% 23.96% 
W/O [G] 88.45% 36.10% 
W/O [C] 91.99% 50.16% 
W/O [R] 89.78% 44.41% 
[C]+[G]+[R] (proposed) 91.47% 52.40% 
Table 10. Component test results. 
4.3. Discussion 
The previous works focused on an 
alphabet-to-alphabet mapping method. Because, 
how the transliteration is more phonetic than 
orthographic, without phonetic infomation10 it 
                                                     
10
 Hangul alphabet has phonetic as well as orthographic. It 
may be adopted to our method as phoneme. Because one 
 may be difficult to acquire more relevant result. 
In the result, ?crepe(keu-le-i-peu/ keu-le-pe) 11?, 
?dealer (dil-leo/ di-eol-leo)?, ?diode (da-i-o-deu/ 
di-o-deu)?, and ?pheromone (pe-ro-mon/ 
pe-eo-o-mon)? etc. produce errors in the 
previous works because they are transliterated 
into Korean with pronunciation and the patterns 
can not be acquired from an 
alphabet-to-alphabet mapping method. For 
example, ?e? before ?p? in ?crepe? is 
transliterated into Korean chracters ?e-i? but it is 
usually transliterated into ?e? in training data. 
Origin of English word also contributes 
performance improvement. For example, words 
such as ?hittite (hi-ta-i-teu /ha-i-ta-i-teu)?, 
?hernia (he-leu-ni-a/ heo-ni-a)?, ?cafeteria 
(ka-pe-te-li-a/ ka-pi-te-ri-a)?.  In summary, E-K 
transliteration is not an alphabet-to-alphabet 
mapping problem but a problem that can be 
solved with mixed use of alphabet, phoneme, 
and word formation information. 
In the experiments, we find that vowel  
transliteration is the main reason of errors rather 
than consonant transliteration in E-K 
transliteration. Especially, ?AH? is the most 
ambiguous phoneme because it can be several 
Korean characters such as ?eo?, ?e?, ?u?, and so 
on. To improve performance of E-K 
transliteration, more specific rules may be 
necessary to handle vowel transliteration.  
5. Conclusion 
We propose an English-Korean transliteration 
model using pronunciation and contextual rules. 
Unlike the previous works, our method use 
phonetic and orthographic information for 
transliteration. With them our method showed 
significant performance increase about 31%. We 
also showed that origin of English words was 
important in E-K transliteration.  
In future works, a study is attempting to develop 
a method for handling English of various foreign 
origin, which this paper did not handle. To 
improve accuracy, contextual rules must be 
added using larger data. Our method may be 
useful to many NLP applications such as 
                                                                               
EPU may produce many phonemes, it may be difficult to 
acquire a good result without context of phoneme and EPU. 
11
 English word (correct transliteration / transliteration by 
the previous works) 
automatic bi-lingual dictionary construction, 
information retrieval, machine translation, 
speech recognition and so on. 
References  
Brown, P. F. and et al (1990), ?A Statistical 
Approach to Machine Translation,? Computational 
Linguistics, Vol 16 (2), June. 
Covington, M. A., (1996). ?An algorithm to align 
words for historical comparison?, Computational 
Linguistics, 22. 
Hall, P., and G. Dowling, (1980), ?Approximate 
string matching,? Computing Surveys, 12(4), 
381-402. 
John Hough, (1953) ?Scientific Terminology?  New 
York: Rhinehart & Company, Inc. 
Kang, Y. and A. A. Maciejewski, (1996). ?An 
algorithm for Generating a Dictionary of Japanese 
Scientific Terms?, Literary and Linguistic 
Computing, 11(2). 
Kang B.J. and K-S. Choi (2000), ?Automatic 
Transliteration and Back-transliteration by 
Decision Tree Learning?, In Proceedings of the 
2nd International Conference on Language 
Resources and Evaluation, Athens, Greece. 
Kang B.J. and  Key-Sun Choi, (2001) ?Two 
approaches for the resolution of word mismatch 
problem caused by English words and foreign 
words in Korean information retrieval?, 
International journal of computer processing of 
oriental language vol 14/No 2, 109-131 
Kim J.J., J.S. Lee, and K-S. Choi., (1999). 
?Pronunciation unit based automatic 
English-Korean transliteration model using neural 
network?, In Proceedings of Korea Cognitive 
Science Association (in Korean) 
Knight, K. and J. Graehl, (1997). ?Machine 
Transliteration?. In Proceedings. of the 35th 
Annual Meetings of the Association for 
Computational Linguistics (ACL) Madrid, Spain. 
Lee, J. S. and K. S. Choi, 1998. English to Korean 
Statistical transliteration for information retrieval. 
Computer Processing of Oriental Languages, 
12(1):17-37. 
Luschnig, C.A.E. (2001) English word origin, 
http://www.ets.uidaho.edu/luschnig/EWO 
Ministry of culture and tourism, Republic of Korea, 
?English-to-Korean Standard conversion rule?, 
1995 (in Korean) 
Quinlan,J.R. (1993), ?C4.5: Programs for Machine 
Learning?, Morgan Kauffman. 
	








 	

Automatic Partial Parsing Rule Acquisition
Using Decision Tree Induction
Myung-Seok Choi, Chul Su Lim, and Key-Sun Choi
Korea Advanced Institute of Science and Technology, 373-1 Guseong-dong,
Yuseong-gu, Daejeon 305-701, Republic of Korea
{mschoi, cslim}@kaist.ac.kr kschoi@cs.kaist.ac.kr
Abstract. Partial parsing techniques try to recover syntactic informa-
tion efficiently and reliably by sacrificing completeness and depth of anal-
ysis. One of the difficulties of partial parsing is finding a means to extract
the grammar involved automatically. In this paper, we present a method
for automatically extracting partial parsing rules from a tree-annotated
corpus using decision tree induction. We define the partial parsing rules
as those that can decide the structure of a substring in an input sentence
deterministically. This decision can be considered as a classification; as
such, for a substring in an input sentence, a proper structure is chosen
among the structures occurred in the corpus. For the classification, we
use decision tree induction, and induce partial parsing rules from the
decision tree. The acquired grammar is similar to a phrase structure
grammar, with contextual and lexical information, but it allows building
structures of depth one or more. Our experiments showed that the pro-
posed partial parser using the automatically extracted rules is not only
accurate and efficient, but also achieves reasonable coverage for Korean.
1 Introduction
Conventional parsers try to identify syntactic information completely. These
parsers encounter difficulties when processing unrestricted texts, because of un-
grammatical sentences, the unavoidable incompleteness of lexicon and grammar,
and other reasons like long sentences. Partial parsing is an alternative technique
developed in response to these problems. This technique aims to recover syn-
tactic information efficiently and reliably from unrestricted texts by sacrificing
completeness and depth of analysis, and relying on local information to resolve
ambiguities [1].
Partial parsing techniques can be roughly classified into two groups. The first
group of techniques involves partial parsing via finite state machines [2,3,9,10].
These approaches apply the sequential regular expression recognizer to an in-
put sentence. When multiple rules match an input string at a given position,
 This research was supported in part by the Ministry of Science and Technology, the
Ministry of Culture and Tourism, and the Korea Science and Engineering Foundation
in Korea.
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 143?154, 2005.
c
? Springer-Verlag Berlin Heidelberg 2005
144 M.-S. Choi, C.S. Lim, and K.-S. Choi
the longest-matching rule is selected. Therefore, these parsers always produce
a single best analysis and operate very fast. In general, these approaches use
a hand-written regular grammar. As would be expected, manually writing a
grammar is both very time consuming and prone to have inconsistencies.
The other group of partial parsing techniques is text chunking, that is, recog-
nition of non-overlapping and non-recursive cores of major phrases (chunks), by
using machine learning techniques [4,7,8,13,15,17]. Since Ramshaw and Mar-
cus [15] first proposed formulating the chunking task as a tagging task, most
chunking methods have followed this word-tagging approach. In base noun phrase
chunking, for instance, each word is marked with one of three chunk tags: I (for
a word inside an NP), O (for outside of an NP), and B (for between the end of
one NP and the start of another) as follows1:
In ( early trading ) in ( Hong Kong ) ( Monday ), ( gold ) was quoted
at ( $ 366.50 ) ( an ounce ).
InO earlyI tradingI inO HongI KongI MondayB ,O goldI wasO quotedO
atO $I 366.50I anB ounceI .O
With respect to these approaches, there have been several studies on automat-
ically extracting chunking rules from large-scale corpora using transformation-
based learning [15], error-driven pruning [7], the ALLiS top-down inductive sys-
tem [8]. However, it is not yet clear how these approaches could be extended
beyond the chunking task.
In this paper, we present a method of automatically extracting partial pars-
ing rules from a tree-annotated corpus using the decision tree method. Our goal
is to extract rules with higher accuracy and broader coverage. We define the
partial parsing rules as those that can establish the structure of a substring in
an input sentence deterministically. This decision can be considered as a classifi-
cation; as such, for a substring in an input sentence, a proper structure is chosen
among the structures occurred in the corpus, as extended from the word-tagging
approach of text chunking. For the classification, we use decision tree induction
with features of contextual and lexical information. In addition, we use negative
evidence, as well as positive evidence, to gain higher accuracy. For general re-
cursive phrases, all possible substrings in a parse tree are taken into account by
extracting evidence recursively from a parse tree in a training corpus. We induce
partial parsing rules from the decision tree, and, to retain only those rules that
are accurate, verify each rule through cross-validation.
In many cases, several different structures are assigned to the same substring
in a tree-annotated corpus. Substrings for coordination and compound nouns are
typical examples of such ambiguous cases in Korean. These ambiguities can pre-
vent us from extracting partial parsing rules that cover the substrings with more
than one substructure and, consequently, can cause the result of partial parsing
to be limited to a relatively shallow depth. In this work, we address this problem
by merging substructures with ambiguity using an underspecified representation.
1 This example is excerpted from Tjong Kim Sang [17].
Automatic Partial Parsing Rule Acquisition 145
This underspecification leads to broader coverage without deteriorating either
the determinism or the precision of partial parsing.
The acquired grammar is similar to a phrase structure grammar, with con-
textual and lexical information, but it allows building structures of depth one or
more. It is easy to understand; it can be easily modified; and it can be selectively
added to or deleted from the grammar. Partial parsing with this grammar pro-
cesses an input sentence deterministically using longest-match heuristics. The
acquired rules are then recursively applied to construct higher structures.
2 Automatic Rule Acquisition
To start, we define the rule template, the basic format of a partial parsing rule,
as follows:
left context | substring | right context ?? substructure
This template shows how the substring of an input sentence, surrounded by the
left context and the right context, constructs the substructure. The left context
and the right context are the remainder of an input sentence minus the substring.
For automatic learning of the partial parsing rules, the lengths of the left context
and the right context are restricted to one respectively. Note that applying a
partial parsing rule results in a structure of depth one or more. In other words,
the rules extracted by this rule template reduce a substring into a subtree, as
opposed to a single non-terminal; hence, the resultant rules can be applied more
specifically and strictly.
c4.5
tree-annotated
corpus
partial parsing
rules
rule candidate
extraction
tree
underspecification
contextualization
& lexicalization
verification
refinement
Fig. 1. Procedure for extracting partial parsing rules
Figure 1 illustrates the procedure for the extraction of partial parsing rules.
First, we extract all possible rule candidates from a tree-annotated corpus, com-
pliant with the rule template. The extracted candidates are grouped according
146 M.-S. Choi, C.S. Lim, and K.-S. Choi
to their respective substrings. Next, using the decision tree method, these candi-
dates are enriched with contextual and lexical information. The contextualized
and lexicalized rules are verified through cross-validation to retain only those
rules that are accurate. The successfully verified accurate rules become the final
partial parsing rules. Remaining rules that cannot be verified are forwarded to
the tree underspecification step, which merges tree structures with hard ambi-
guities. As seen in Fig. 1, the underspecified candidates return to the refinement
step. The following subsections describe each step in detail.
2.1 Extracting Candidates
From the tree-annotated corpus, we extract all the possible candidates for partial
parsing rules in accordance with the rule template. Scanning input sentences an-
notated with its syntactic structure one by one, we can extract the substructure
corresponding to every possible substring at each level of the syntactic struc-
ture. We define level 0 as part-of-speech tags in an input sentence, and level n
as the nodes whose maximum depth is n. If no structure precisely corresponds
to a particular substring, then a null substructure is extracted, which represents
negative evidence.
Figure 2 shows an example sentence2 with its syntactic structure3 and some
of the candidates for the partial parsing rules extracted from the left side of
the example. In this figure, the first partial parsing rule candidate shows how
the substring ?npp? can be constructed into the substructure ?NP?. Snull denotes
negative evidence.
The extracted rule candidates are gathered and grouped according to their
respective substrings. Figure 34 shows the candidate groups. In this figure, G1
and G2 are the group names, and the number in the last column refers to the
frequency that each candidate occurs in the training corpus. Group G1 and G2
have 2 and 3 candidates, respectively. When a particular group has only one
candidate, the candidate can always be applied to a corresponding substring
2 ?NOM? refers to the nominative case and ?ACC? refers to the accusative case. The
term ?npp? denotes personal pronoun; ?jxt? denotes topicalized auxiliary particle;
?ncn? denotes non-predicative common noun; ?jco? denotes objective case particle;
?pvg? denotes general verb; ?ef? denotes final ending; and ?sf? denotes full stop symbol.
For a detailed description of the KAIST corpus and its tagset, refer to Lee [11]. The
symbol ?+? is not a part-of-speech, but rather a delimiter between words within a
word phrase.
3 In Korean, a word phrase, similar to bunsetsu in Japanese, is defined as a spacing unit
with one or more content words followed by zero or more functional words. A content
word indicates the meaning of the word phrase in a sentence, while a functional
word?a particle or a verbal-ending?indicates the grammatical role of the word
phrase. In the KAIST corpus used in this paper, a functional word is not included in
the non-terminal that the preceding content word belongs to, following the restricted
representation of phrase structure grammar for Korean [12]. For example, a word
phrase ?na/npp + neun/jxt? is annotated as ?(NP na/npp ) + neun/jxt?, as in
Fig. 2.
Automatic Partial Parsing Rule Acquisition 147
NP NP VP
VP
VP
S
na/npp+neun/jxt
I-NOM
sagwa/ncn+reul/jco
apple-ACC
meok/pvg + neunda/ef + ./sf
eat
|npp |
|npp + jxt |
|npp + jxt ncn |
?
|NP + jxt NP + jco VP   |
?
|NP + jco VP |
?
|VP + ef + sf |
S
null
S
null
NP + jxt NP + jco VP
VP
VP
NP
VP
S
Fig. 2. An example sentence and the extracted candidates for partial parsing rules
|etm nbn + jcs paa |
S
null
AUXP
G
1
G
2
|VP + ecs VP + ecs VP|
S
null
66
151
123
170
487
freq.
VP + ecs VP + ecs VP
VP
VP
VP + ecs VP + ecs VP
VP
VP
Fig. 3. Groups of partial parsing rules candidates
deterministically. In contrast, if there is more than one candidate in a particular
group, those candidates should be enriched with contextual and lexical informa-
tion to make each candidate distinct for proper application to a corresponding
substring.
2.2 Refining Candidates
This step refines ambiguous candidates with contextual and lexical information
to make them unambiguous.
First, each candidate needs to be annotated with contextual and lexical in-
formation occurring in the training corpus, as shown in Fig. 4. In this figure, we
can see that a substring with lexical information such as ?su/nbn? unambigu-
ously constitutes the substructure ?AUXP?. We use the decision tree method,
C4.5 [14], to select the important contextual and lexical information that can
facilitate the establishment of unambiguous partial parsing rules. The features
used in the decision tree method are the lexical information of each terminal or
4 The term ?etm? denotes adnominalizing ending; ?nbn? denotes non-unit bound noun;
?jcs? denotes subjective case particle; ?paa? denotes attributive adjective; ?ecs? denotes
subordinate conjunctive ending; and ?AUXP? denotes auxiliary phrase.
148 M.-S. Choi, C.S. Lim, and K.-S. Choi
sal/pvg + | r/etm su/nbn + ga/jcs iss/paa | + da/ef ? AUXP
i/jp + | r/etm su/nbn + ga/jcs eop/paa | + da/ef ? AUXP
nolla/pvg + | n/etm jeok/nbn + i/jcs iss/paa | + da/ef ? Snull
wanjeonha/paa + | n/etm geot/nbn + i/jcs eop/paa | + go/ecc? Snull
kkeutna/pvg + | n/etm geut/nbn + i/jcs ani/paa | + ra/ecs ? Snull
ik/pvg + | neun/etm geut/nbn + i/jcs jot/paa | + da/ef ? Snull
ha/xsv + | r/etm nawi/nbn + ga/jcs eop/paa | + da/ef ? Snull
Fig. 4. Annotated candidates for the G1 group rules
nbn = su(way):
paa = iss(exist)
paa = eop(not exist)
paa = man(much) S
null
AUXP
AUXP
  
Fig. 5. A section of the decision tree
non-terminal for the substring, and the parts-of-speech and lexical information
for the left context and the right context. Lexical information of a non-terminal
is defined as the part-of-speech and lexical information of its headword.
Figure 5 shows a section of the decision tree learned from our example sub-
string. The deterministic partial parsing rules in Fig. 6 are extracted from the
decision tree. As shown in Fig. 6, only the lexical entries for the second and the
fourth morphemes in the substring are selected as additional lexical informa-
tion, and none of the contexts is selected in this case. We should note that the
rules induced from the decision tree are ordered. Since these ordered rules do
not interfere with those from other groups, they can be modified without much
difficulty.
| etm su/nbn + jcs iss/paa | ?? AUXP
| etm su/nbn + jcs eop/paa | ?? AUXP
| etm su/nbn + jcs man/paa| ?? Snull
Fig. 6. Partial parsing rules extracted from a section of the decision tree in Fig. 5
Automatic Partial Parsing Rule Acquisition 149
After we enrich the partial parsing rules using the decision tree method, we
verify them by estimating the accuracy of each rule to filter out less deterministic
rules. We estimate the error rates (%) of the rule candidates via a 10-fold cross
validation on the training corpus. The rule candidates of the group with an error
rate that is less than the predefined threshold, ?, can be extracted to the final
partial parsing rules. The candidates in the group G2 in Fig. 3 could not be
extracted as the final partial parsing rules, because the estimated error rate of
the group was higher than the threshold. The candidates in G2 are set aside
for tree underspecification processing. Using the threshold ?, we can control the
number of the final partial parsing rules and the ratio of the precision/recall
trade-off for the parser that adopts the extracted partial parsing rules.
2.3 Dealing with Hard Ambiguities: The Underspecified
Representation
The group G2 in Fig. 3 has one of the attachment ambiguities, namely, consecu-
tive subordinate clauses. Figure 7 shows sections of two different trees extracted
from a tree-annotated corpus. The two trees have identical substrings, but are an-
alyzed differently. This figure exemplifies how an ambiguity relates to the lexical
association between verb phrases, which is difficult to annotate in rules. There
are many other syntactic ambiguities, such as coordination and noun phrase
bracketing, that are difficult to resolve with local information. The resolution
usually requires lexical co-occurrence, global context, or semantics. Such am-
biguities can deteriorate the precision of partial parsing or limit the result of
partial parsing to a relatively shallow depth.
Rule candidates with these ambiguities mostly have several different struc-
tures assigned to the same substrings under the same non-terminals. In this
paper, we refer to them as internal syntactic ambiguities. We manually exam-
ined the patterns of the internal syntactic ambiguities, which were found in the
KAIST corpus as they could not be refined automatically due to low estimated
accuracies. During the process, we observed that few internal syntactic ambigu-
ities could be resolved with local information.
In this paper, we handle internal syntactic ambiguities by merging the candi-
dates using tree intersection and making them underspecified. This underspeci-
fied representation enables an analysis with broader coverage, without deterio-
VPVP VP
cheonsaui ttange ga + seo/ecs
go to the land of angels - as
jal sarabo + ryeogo/ecs
live well - in order to
aesseuda
make effort
VP
VP
(a)
jibe ga + seo/ecs
go home - as
TVreul bo + ryeogo/ecs
watch TV - in order to
gabangeul chaenggida
pack one?s bag
VPVP VP
VP
VP
(b)
Fig. 7. Examples of internal syntactic ambiguities
150 M.-S. Choi, C.S. Lim, and K.-S. Choi
G
2
|VP + ecs VP + ecs VP|
S
null
VP
Fig. 8. Underspecified candidates
rating the determinism or the precision of partial parsing. Since only different
structures under the same non-terminal are merged, the underspecification does
not harm the structure of higher nodes. Figure 8 shows the underspecified can-
didates of group G2. In this figure, the first two rules in G2 are reduced to the
merged ?VP?. Underspecified candidates are also enriched with contextual and
lexical information using the decision tree method, and they are verified through
cross-validation, as described in Sect. 2.2. The resolution of internal syntactic
ambiguities is forwarded to a module beyond the partial parser. If necessary,
by giving all possible structures of underspecified parts, we can prevent a later
processing from re-analyzing the parts. Any remaining candidates that are not
selected as the partial parsing rules after all three steps are discarded.
3 Experimental Results
We have performed experiments to show the usefulness of automatically ex-
tracted partial parsing rules. For our evaluations, we implemented a naive par-
tial parser, using TRIE indexing to search the partial parsing rules. The input
of the partial parser is a part-of-speech tagged sentence and the result is usually
the sequence of subtrees. At each position in an input sentence, the parser tries
to choose a rule group using longest-match heuristics. Then, if any matches are
found, the parser applies the first-matching rule in the group to the correspond-
ing substring, because the rules induced from the decision tree are ordered.
In our experiments, we used the KAIST tree-annotated corpus [11]. The
training corpus contains 10,869 sentences (289,362 morphemes), with an average
length of 26.6 morphemes. The test corpus contains 1,208 sentences, with an
average length of 26.0 morphemes. The validation corpus, used for choosing the
threshold, ?, contains 1,112 sentences, with an average length of 20.1 morphemes,
and is distinct from both the training corpus and the test corpus.
The performance of the partial parser was evaluated using PARSEVAL mea-
sures [5]. The F measure, a complement of the E measure [16], is used to combine
precision and recall into a single measure of overall performance, and is defined
as follows:
F? =
(?2 + 1) ? LP ? LR
?2 ? LP + LR
In the above equation, ? is a factor that determines the weighting of precision
and recall. Thus, ? < 1 is used to weight precision heavier than recall, ? > 1
is used to weight recall heavier than precision, and ? = 1 is used to weight
precision and recall equally.
Automatic Partial Parsing Rule Acquisition 151
Table 1. Precision/Recall with respect to the threshold, ?, for the validation corpus
? # of rules precision recall F?=0.4
6 18,638 95.5 72.9 91.6
11 20,395 95.1 75.1 91.7
16 22,650 94.2 78.0 91.6
21 25,640 92.6 83.3 91.2
26 28,180 92.0 84.7 90.9
Table 2. Experimental results of the partial parser for Korean
Grammar precision recall F?=0.4 F?=1
baseline 73.0 72.0 72.9 72.5
depth 1 rule only 95.2 68.3 90.3 79.6
not underspecified 95.7 71.6 91.4 81.9
underspecified 95.7 73.6 91.9 83.2
underspecified (in case ?=26) 92.2 83.5 90.9 87.6
PCFG 80.0 81.5 80.2 80.7
Lee [11] 87.5 87.5 87.5 87.5
The parsing result can be affected by the predefined threshold, ? (described
in Sect. 2.2), which can control both the accuracy of the partial parser and
the number of the extracted rules. Table 1 shows the number of the extracted
rules and how precision and recall trade off for the validation corpus as the
threshold, ?, is varied. As can be seen, a lower threshold, ?, corresponds to a
higher precision and a lower recall. A higher threshold corresponds to a lower
precision and a higher recall. For a partial parser, the precision is generally
favored over the recall. In this paper, we used a value of 11 for ?, where the
precision was over 95% and f?=0.4 was the highest. The value of this threshold
should be set according to the requirements of the relevant application.
Table 2 presents the precision and the recall of the partial parser for the test
corpus when the threshold, ?, was given a value of 11. In the baseline gram-
mar, we selected the most probable structure for a given substring from each
group of candidates. The ?depth 1 rule only? grammar is the set of the rules
extracted along with the restriction, stating that only a substructure of depth
one is permitted in the rule template. The ?underspecified? grammar is the final
version of our partial parsing rules, and the ?not underspecified? grammar is
the set of the rules extracted without the underspecification processing. Both
PCFG and Lee [11] are statistical full parsers of Korean, and Lee enriched the
grammar using contextual and lexical information to improve the accuracy of a
parser. Both of them were trained and tested on the same corpus as ours was
for comparison. The performance of both the ?not underspecified? grammar and
the ?underspecified? grammar was greatly improved compared to the baseline
grammar and PCFG, neither of which adopts contextual and lexical informa-
tion in their rules. The ?not underspecified? grammar performed better than
152 M.-S. Choi, C.S. Lim, and K.-S. Choi
the ?depth 1 rule only? grammar. This indicates that increasing the depth of a
rule is helpful in partial parsing, as in the case of a statistical full parsing, Data-
Oriented Parsing [6]. Comparing the ?underspecified? grammar with the ?not
underspecified? grammar, we can see that underspecification leads to broader
coverage, that is, higher recall. The precision of the ?underspecified? grammar
was above 95%. In other words, when a parser generates 20 structures, 19 out
of 20 structures are correct. However, its recall dropped far beyond that of the
statistical full parser [11]. When we set ? to a value of 26, the underspecified
grammar slightly outperformed that of the full parser in terms of f?=1, although
the proposed partial parser does not always produce one complete parse tree5.
It follows from what has been said thus far that the proposed parser has the
potential to be a high-precision partial parser and approach the performance
level of a statistical full parser, depending on the threshold ?.
The current implementation of our parser has a O(n2mr) worst case time
complexity for a case involving a skewed binary tree, where n is the length of
the input sentence and mr is the number of rules. Because mr is the constant,
much more than two elements are reduced to subtrees of depth one or more in
each level of parsing, and, differing from full parsing, the number of recursions
in the partial parsing seems to be limited6, we can parse in near-linear time.
Figure 9 shows the time spent in parsing as a function of the sentence length7.
 0
 5
 10
 15
 20
 25
 30
 35
 40
 0  10  20  30  40  50  60  70
p a
rs
in
g  
tim
e (m
s )
sent.length
Fig. 9. Time spent in parsing
Lastly, we manually examined the first 100 or so errors occurring in the test
corpus. In spite of underspecification, the errors related to conjunctions and
5 In the test corpus, the percentage that our partial parser (?=26) produced one
complete parse tree was 70.9%. When ?=11, the percentage was 35.9%.
6 In our parser, the maximum number of recursion was 10 and the average number of
recursion was 4.47.
7 This result was obtained using a Linux machine with Pentium III 700MHz processor.
Automatic Partial Parsing Rule Acquisition 153
attachments were the most frequent. The errors of conjunctions were mostly
caused by substrings not occurring in the training corpus, while the cases of
attachments lacked contextual or lexical information for a given substring. These
errors can be partly resolved by increasing the size of the corpus, but it seems
that they cannot be resolved completely with partial parsing. In addition, there
were errors related to noun phrase bracketing, date/time/unit expression, and
either incorrectly tagged sentences or inherently ambiguous sentences. For date,
time, and unit expressions, manually encoded rules may be effective with partial
parsing, since they appear to be used in a regular way. We should note that
many unrecognized phrases included expressions not occurring in the training
corpus. This is obviously because our grammar cannot handle unseen substrings;
hence, alleviating the sparseness in the sequences will be the goal of our future
research.
4 Conclusion
In this paper, we have proposed a method of automatically extracting the par-
tial parsing rules from a tree-annotated corpus using a decision tree method. We
consider partial parsing as a classification; as such, for a substring in an input
sentence, a proper structure is chosen among the structures occurred in the cor-
pus. Highly accurate partial parsing rules can be extracted by (1) allowing rules
to construct a subtree of depth one or more; (2) using decision tree induction,
with features of contextual and lexical information for the classification; and (3)
verifying induced rules through cross-validation. By merging substructures with
ambiguity in non-deterministic rules using an underspecified representation, we
can handle syntactic ambiguities that are difficult to resolve with local infor-
mation, such as coordination and noun phrase bracketing ambiguities. Using a
threshold, ?, we can control the number of the partial parsing rules and the ratio
of the precision/recall trade-off of the partial parser. The value of this thresh-
old should be set according to the requirements of the relevant application. Our
experiments showed that the proposed partial parser using the automatically
extracted rules is not only accurate and efficient, but also achieves reasonable
coverage for Korean.
References
1. Abney, S.P.: Part-of-speech tagging and partial parsing. Corpus-Based Methods in
Language and Speech. Kluwer Academic Publishers (1996)
2. Abney, S.P.: Partial parsing via finite-state cascades. Proceedings of the ESSLLI
?96 Robust Parsing Workshop (1996) 8?15
3. A??t-Mokhtar, S., Chanod, J.P.: Incremental finite-state parsing. Proceedings of
Applied Natural Language Processing (1997) 72?79
4. Argamon-Engelson, S., Dagan, I., Krymolowski, Y.: A memory-based approach to
learning shallow natural language patterns. Journal of Experimental and Theoret-
ical AI 11(3) (1999) 369?390
154 M.-S. Choi, C.S. Lim, and K.-S. Choi
5. Black, E., Abney, S., Flickenger, D., Gdaniec, C., Grishman, R., Harrison, P.,
Hindle, D., Ingria, R., Jelinek, F., Klavans, J., Liberman, M., Marcus, M., Roukos,
S., Santorini, B., Strzalkowski, T.: A procedure for quantitatively comparing the
syntactic coverage of English grammars. Proceedings of the DARPA Speech and
Natural Language Workshop (1991) 306?311
6. Bod, R.: Enriching Linguistics with Statistics: Performance Models of Natural
Language. Ph.D Thesis. University of Amsterdam (1995)
7. Cardie, C., Pierce, D.: Error-driven pruning of treebank grammars for base noun
phrase identification. Proceedings of 36th Annual Meeting of the Association for
Computational Linguistics and 17th International Conference on Computational
Linguistics (1998) 218?224
8. De?jean, H.: Learning rules and their exceptions. Journal of Machine Learning Re-
search 2 (2002) 669?693
9. Hindle, D.: A parser for text corpora. Computational Approaches to the Lexicon.
Oxford University (1995) 103?151
10. Hobbs, J.R., Appelt, D., Bear, J., Israel, D., Kameyama, M., Stickel, M., Tyson, M.:
Fastus: A cascaded finite-state transducer for extracting information from natural-
language text. Finite-State Language Processing. The MIT Press (1997) 383?406
11. Lee, K.J.: Probabilistic Parsing of Korean based on Language-Specific Properties.
Ph.D. Thesis. KAIST, Korea (1998)
12. Lee, K.J., Kim, G.C., Kim, J.H., Han, Y.S.: Restricted representation of phrase
structure grammar for building a tree annotated corpus of Korean. Natural Lan-
guage Engineering 3(2) (1997) 215?230
13. Mun?oz, M., Punyakanok, V., Roth, D., Zimak, D.: A learning approach to shallow
parsing. Proceedings of the 1999 Joint SIGDAT Conference on Empirical Methods
in Natural Language Processing and Very Large Copora (1999) 168?178
14. Quinlan, J.R.: C4.5: Programs for Machine Learning. Morgan Kaufmann Publish-
ers (1993)
15. Ramshaw, L.A., Marcus, M.P.: Text chunking using transformation-based learning.
Proceedings of Third Wordkshop on Very Large Corpora (1995) 82?94
16. van Rijsbergen, C.: Information Retrieval. Buttersworth (1975)
17. Tjong Kim Sang, E.F.: Memory-based shallow parsing. Journal of Machine Learn-
ing Research 2 (2002) 559?594
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 302 ? 313, 2005. 
? Springer-Verlag Berlin Heidelberg 2005 
Classifying Chinese Texts in Two Steps 
Xinghua Fan1, 2, 3, Maosong Sun1, Key-sun Choi3, and Qin Zhang2 
1
 State Key Laboratory of Intelligent Technology and Systems, Tsinghua University,  
Beijing 100084, China 
fanxh@tsinghua.org.cn, sms@tsinghua.edu.cn 
2
 State Intellectual Property Office of P.R. China, Beijing, 100088, China 
zhangqin@sipo.gov.cn 
3
 Computer Science Division, Korterm, KAIST, 373-1 Guseong-dong Yuseong-gu, 
Daejeon 305-701, Korea 
kschoi@cs.kaist.ac.kr 
Abstract. This paper  proposes a two-step method for Chinese text categoriza-
tion (TC). In the first step, a Na?ve Bayesian classifier is used to fix the fuzzy 
area between two categories, and, in the second step, the classifier with more 
subtle and powerful features is used to deal with documents in the fuzzy area, 
which are thought of being unreliable in the first step. The preliminary experi-
ment validated the soundness of this method. Then, the method is extended 
from two-class TC to multi-class TC. In this two-step framework, we try to fur-
ther improve the classifier by taking the dependences among features into con-
sideration in the second step, resulting in a Causality Na?ve Bayesian Classifier. 
1   Introduction 
Text categorization (TC) is a task of assigning one or multiple predefined category 
labels to natural language texts. To deal with this sophisticated task, a variety of sta-
tistical classification methods and machine learning techniques have been exploited 
intensively[1], including the Na?ve Bayesian (NB) classifier [2], the Vector Space 
Model (VSM)-based classifier [3], the example-based classifier [4], and the Support 
Vector Machine [5]. 
Text  filtering is a basic type of text categorization (two-class TC). It can find 
many real-life applications [6], a typical one is the ill information filtering, such as 
erotic information and garbage information filtering on the web, in e-mails and in 
short messages of mobile phone. It is obvious that this sort of information should be 
carefully controlled. On the other hand, the filtering performance using the existing 
methodologies is still not satisfactory in general. The reason lies in that there exist a 
number of documents with high degree of ambiguity, from the TC point of view, in a 
document collection, that is, there is a fuzzy area across the border of two classes (for 
the sake of expression, we call the class consisting of the ill information-related texts, 
or, the negative samples, the category of TARGET, and, the class consisting of the ill 
information-not-related texts, or, the positive samples, the category of Non-
TARGET). Some documents in one category may have great similarities with some 
other documents in the other category, for example, a lot of words concerning love 
 Classifying Chinese Texts in Two Steps 303 
story and sex are likely appear in both negative samples and positive samples if the 
filtering target is erotic information. We observe that most of the classification errors 
come from the documents falling into the fuzzy area between two categories. 
The idea of this paper is inspired by the fuzzy area between categories.  A two-step 
TC method is thus proposed: in the first step, a classifier is used to fix the fuzzy area 
between categories; in the second step, a classifier (probably the same as that in the 
first step) with more subtle and powerful features is used to deal with documents in 
the fuzzy area which are thought of being unreliable in the first step. Experimental 
results validate the soundness of this method. Then we extend it from two-class TC to 
multi-class TC. Furthermore, in this two-step framework, we try to improve the clas-
sifier by taking the dependences among features into consideration in the second step, 
resulting in a Causality Na?ve Bayesian Classifier. 
This paper is organized as follows: Section 2 describes the two-step method in the 
context of two-class Chinese TC; Section 3 extends it to multi-class TC; Section 4 
introduces the Causality Na?ve Bayesian Classifier; and Section 5 is conclusions.  
2   Basic Idea: A Two-Step Approach to Text Categorization 
2.1   Fix the Fuzzy Area Between Categories by the Na?ve Bayesian Classifier 
We use the Na?ve Bayesian Classifier to fix the fuzzy area in the first step. For a 
document represented by a binary-valued vector d = (W1, W2, ?, W|D|), the two-class 
Na?ve Bayesian Classifier is given as follows: 
???
===
?++=
=
||
1 2
2
||
1 1
1
||
1 2
1
2
1
2
1
1
log
1
log
1
1
log}Pr{
}Pr{
log
}Pr{
}Pr{
log)(
D
k k
k
k
D
k k
k
k
D
k k
k
-p
p
W
-p
p
W
-p
-p
c
c
|dc
|dcdf
 
 
(1) 
where Pr{
?
} is the probability that event {
?
} occurs, ci  is category i, and 
pki=Pr{Wk=1|ci} (i=1,2). If f(d) ?0, the document d will be assigned the category label 
c1, otherwise, c2.   
Let:  
?
=
+=
||
1 2
1
2
1
1
1log}Pr{
}Pr{log
D
k k
k
-p
-p
c
cCon  (2) 
?
=
=
||
1 1
1
1
log
D
k k
k
k
-p
p
WX  (3) 
?
=
=
||
1 2
2
1
log
D
k k
k
k
-p
pWY  (4) 
304 X. Fan et al 
where Con is a constant relevant only to the training set, X and Y are the measures 
that the document d belongs to categories c1 and c2 respectively.  
We rewrite (1) as: 
ConYXdf +?=)(  (5) 
Apparently,  f(d)=0 is the separate line in a two-dimensional space with X and Y 
being X-coordinate and Y-coordinate. In this space, a given document d can be 
viewed as a point (x, y), in which the values of x and y are calculated according to (3) 
and (4).  
As shown in Fig.1, the distance from the point (x, y) to the separate line will be: 
)(
2
1 ConyxDist +?=  (6) 
 
 
Fig. 1. Distance from point (x, y) to the separate line 
Fig. 2 illustrates the distribution of a training set (refer to Section 2.2) regarding 
Dist in the two-dimensional space, with the curve on the left for the negative samples, 
and the curve on the right for the positive samples. As can be seen in the figure, most 
of the misclassified documents, which unexpectedly across the separate line, are near 
the line. The error rate of the classifier is heavily influenced by this area, though the 
documents falling into this area only constitute a small portion of the training set.  
 
Fig. 2. Distribution of the training set in the two-dimensional space 
 Classifying Chinese Texts in Two Steps 305 
Thus, the space can be partitioned into reliable area and unreliable area: 
??
??
?
<
>
??
  reliable is   to  label  theAssigning                 
 reliable is    to  label  theAssigning        
 unreliable is for Decision          
22
11
12
dc,     DistDist
 dc     ,         DistDist
d, DistDistDist
 (7) 
where Dist1 and Dist2 are constants determined by experiments, Dist1 is positive real 
number and Dist2 is negative real number. 
In the second step, more subtle and powerful features will be designed in particular 
to tackle the unreliable area identified in the first step. 
2.2   Experiments on the Two-Class TC 
The dataset used here is composed of 12,600 documents with 1,800 negative samples 
of TARGET and 10,800 positive samples of Non-TARGET. It is split into 4 parts 
randomly, with three parts as training set and one part as test set. All experiments in 
this section are performed in 4-fold cross validation.  
CSeg&Tag3.0, a Chinese word segmentation and POS tagging system developed 
by Tsinghua University, is used to perform the morphological analysis for Chinese 
texts. In the first step, Chinese words with parts-of-speech verb, noun, adjective and 
adverb are considered as features. The original feature set is further reduced to a much 
smaller one according to formula (8) or (9). A Na?ve Bayesian Classifier is then ap-
plied to the test set. In the second step, only the documents that are identified unreli-
able in terms of (7) in the first step are concerned. This time, bigrams of Chinese 
words with parts-of-speech verb and noun are used as features, and the Na?ve Bayes-
ian Classifier is re-trained and applied again. 
?
=
=
n
i ik
ik
ikk
ct
,ct
,ct,ctMI
1
1 }Pr{}Pr{
}Pr{
log}Pr{)(  (8) 
?
=
=
n
i ik
ik
k
ct
,ct
,ctMI
1
2 }Pr{}Pr{
}Pr{
log)(  (9) 
where tk stands for the kth feature, which may be a Chinese word or a word bigram, 
and ci is the ith predefined category.  
We try five methods as follows.  
Method-1: Use Chinese words as features, reduce features with (9), and classify 
documents directly without exploring the two-step strategy.  
Method-2: same as Method-1 except feature reduction with (8).  
Method-3: same as Method-1 except Chinese word bigrams as features.  
Method-4: Use the mixture of Chinese words and Chinese word bigrams as fea-
tures, reduce features with (8), and classify documents directly.  
Method-5: (i.e., the proposed method): Use Chinese words as features in the first 
step and then use word bigrams as features in the second step, reduce features with 
(8), and classify the documents in two steps. 
306 X. Fan et al 
Note that the proportion of negative samples and positive samples is 1:6. Thus if 
all the documents in the test set is arbitrarily set to positive, the precision will reach 
85.7%. For this reason, only the experimental results for negative samples are consid-
ered in evaluation, as given in Table 1. For each method, the number of features is set 
by the highest point in the curve of the classifier performance with respect to the 
number of features (For the limitation of space, we omit all the curves here). The 
numbers of features set in five methods are 4000, 500, 15000, 800 and 500+3000 (the 
first step + the second step) respectively. 
Table 1.  Performance comparisons of the five methods in two-class TC 
 
Comparing Method-1 and Method-2, we can see that feature reduction formula (8) 
is superior to (9). Moreover, the number of features determined in the former is less 
than that in the latter (500 vs. 4000). Comparing Method-2, Method-3 and Method-4, 
we can see that Chinese word bigrams as features have better discriminating capabil-
ity meanwhile with more serious data sparseness: the performances of Method-3 and 
Method-4 are higher than that of Method-2, but the number of features used in 
Method-3 is more than those used in Method-2 and Method-4 (15000 vs. 500 and 
800). Table 1 shows that the proposed method (Methond-5) has the best performance 
(95.54% F1) and good efficiency. It integrates the merit of words and word bigrams. 
Using words as features in the first step aims at its better statistical coverage, -- the 
500 selected features in the first step can treat a majority of documents, constituting 
63.13% of the test set. On the other hand, using word bigrams as features in the sec-
ond step aims at its better discriminating capability, although the number of features 
becomes comparatively large (3000). Comparing Method-5 with Method-2, Method-3 
and Method-4, we find that the two-step approach is superior to either using only one 
kind of features (word or word bigram) in the classifier, or using the mixture of two 
kinds of features in one step. 
3   Extending the Two-Step Approach to the Multi-class TC 
We extend the two-step method presented in Section 2 to handle the multi-class TC 
now. The idea is to transfer the multi-class TC to the two-class TC. Similar to two-
class TC, the emphasis is still on the misclassified documents given by a classifier, 
though we use a modified multi-class Na?ve Bayesian Classifier here. 
 Classifying Chinese Texts in Two Steps 307 
3.1   Fix the Fuzzy Area Between Categories by the Multi-class Bayesian      
Classifier 
For a document represented by a binary-valued vector d = (W1, W2, ?, W|D|), the 
multi-class Na?ve Bayesian Classifier can be re-written as: 
??
==?
? ++=
||
1
||
1
)
1
log)1(log}{Prlog(maxarg
D
k ki
ki
k
D
k
kii
Cc -p
p
 W-p cc
i
 (10) 
where Pr{
?
} is the probability that event {
?
} occurs, pki=Pr{Wk=1|ci}, (i=1,2, ?, |C|), C is the number of predefined categories. Let: 
 ??
==
++=
||
1
||
1 1
log)1(log}{Prlog
D
k ki
ki
k
D
k
kiii
-p
p
 W-p cMV  (11) 
         )(maximummax_ iCcF MVMV i?=  (12) 
         
Cc
iS
i
MVMV
?
= )imum(second_maxmax_  (13) 
where MVi stands for the likelihood of assigning a label ci?C to the document d,  
MVmax_F and MVmax_S are the maximum and the second maximum over all MVi 
(i?|C|) respectively. We approximately rewrite (10) as: 
SF MVMVdf max_max_)( ?=           (14) 
We try to transfer the multi-class TC described by (10) into a two-class TC de-
scribed by (14). Formula (14) means that the binary-valued multi-class Na?ve Bayes-
ian Classifier can be approximately regarded as searching a separate line in a two-
dimensional space with MVmax_F being the X-coordinate and MVmax_S being the Y-
coordinate. The distance from a given document, represented as a point (x, y) with the 
values of x and y calculated according to (12) and (13) respectively, to the separate 
line in this two-dimensional space will be:  
      y)(xDist ?=
2
1
    (15) 
The value of Dist directly reflects the degree of confidence of assigning the label c* 
to the document d. 
The distribution of a training set (refer to Section 3.2) regarding Dist in this two-
dimensional space, and, consequently, the fuzzy area for the Na?ve Bayesian Classi-
fier, are observed and identified, similar to its counterpart in Section 2.2.  
3.2   Experiments on the Multi-class TC 
We construct a dataset, including 5 categories and the total of 17756 Chinese docu-
ments. The document numbers of five categories are 4192, 6968, 2080, 3175 and 
308 X. Fan et al 
1800 respectively, among which the last three categories have the high degree of 
ambiguity each other. The dataset is split into four parts randomly, one as the test set 
and the other three as the training set. We again run the five methods described in 
Section 2.2 on this dataset. The strategy of determining the number of features also 
follows that used in Section 2.2. The experimentally determined numbers of features 
regarding the five methods are 8000, 400, 5000, 800 and 400 + 9000 (the first step +  
the second step) respectively. 
The average precision, average recall and average F1 over the five categories are 
used to evaluate the experimental results, as shown in Table 2. 
Table 2.  Performance comparisons of the five methods in multi-class TC 
 
We can see from Table 2 that the very similar conclusions as that in the two-class 
TC in Section 2.2 can be obtained here: 
1) Formula (8) is superior to (9) in feature reduction. This comes from the per-
formance comparison between Method-2 and Method-1: the former has higher per-
formance and higher efficiency that the latter (the average F1, 97.20% vs. 91.48%, and 
the number of features used, 400 vs. 8000). 
2) Word bigrams as features have better discriminating capability than words as 
features, along with more serious data sparseness. The performances of Method-3 and 
Method-4, which use Chinese word bigrams and the mixture of words and word bi-
grams as features respectively, are higher than that of Method-2, which only uses 
Chinese words as features. But the number of features used in Method-3 is much 
more than those used in Method-2 and Method-4 (5000 vs. 400 and 800). 
3) The proposed method (Methond-5) has the best performances and acceptable ef-
ficiency. In term of the average F1, the performance is improved from the baseline 
91.48% (Method-1) to 98.56% (Method-5). In the first step in Method-5, the number 
of feature set is small (only 400), but a majority of documents can be treated by it. 
The number of features exploited in Method-5 is the highest among the five methods 
(9000), but it is still acceptable. 
4   Using Dependences Among Features in Two-Step 
Categorization  
In this section, a two-step text categorization method taking the dependences among 
features into account is presented. We do the same task with the Na?ve Bayesian Clas-
sifier in the first step, exactly same as what we did in Section 2 and Section 3. In the 
 Classifying Chinese Texts in Two Steps 309 
second step, each document identified unreliable in the first step are further processed 
by exploring the dependences among features. This is realized by a model named the 
Causality Na?ve Bayesian Classifier. 
4.1   The Causality Na?ve Bayesian Classifier (CNB) 
The Causality Na?ve Bayesian Classifier (CNB) is an improved Na?ve Bayesian Clas-
sifier. It contains two additional parts, i.e., the k-dependence feature list and the fea-
ture causality diagram. The former is used to represent the dependence relation among 
features, and the latter is used to estimate the probability distribution of a feature 
dynamically while taking its dependences into account.  
K-Dependence Feature List (K-DFL): CNB allows each feature node Y to have a 
maximum of k features nodes as parents that constitute the k-dependence feature list 
representing the dependences among features. In other words, ?(Y) = {Yd, C}, where 
Yd is the set of at most k features nodes, C is the category node, and ?(C) =?. 
Note that we can build a K-DFL for each feature under each class ct, which repre-
sents different dependence relations under different class.  
Obviously, there exists a 0-dependence feature list for every feature in the Na?ve 
Bayesian Classifier, from the definition of K-DFL. 
The algorithm of constructing K-DFL is as follows: Given the maximum depend-
ence number k, mutual information threshold ? and the class ct. For each feature Y, 
repeat the follow steps. 1) Compute class conditional mutual information MI(Yi, Yj| 
ct), for every pair of features Yi and Yj, where i?j. 2) Construct the set Si={ Yj | 
MI(Yi, Yj| ct) > ?}. 3) Let m= min (k, | Si|), select the top m features as K-DFL  
from Si. 
Feature Causality Diagram (FCD): CNB allows each feature Y, which occurs in a 
given document, to have a Feature Causality Diagram (FCD). FCD is a double-layer 
directed diagram, in which the first layer has only the feature node Y, and the second 
layer allows to have multiple nodes that include the class node C and the correspond-
ing dependence node set S of Y. Here, S=Sd?SF, Sd is the K-DFL node set of Y and 
SF={Xi| Xi is a feature node that occurs in the given document. There exists a directed 
arc from every node Xi at the second layer to the node Y at the first layer. The arc is 
called causality link event Li which represents the causality intensity between node Y 
and Xi, and the probability of Li is pi=Pr{Li}=Pr{Y=1|Xi=1}. The relation among all 
arcs is logical OR. The Feature Causality Diagram can be considered as a sort of 
simplified causality diagram [9][10]. 
Suppose feature Y?s FCD is G, and it parent node set S={X1, X2,?,Xm } (m?1) in 
G, we can estimate the conditional probability as follows while considering the de-
pendences among features: 
? ?
=
?
==
?+===?===
m
i
i
j
ji
i
i ppp
2
1
1
1
m
1
m1 )1(}LPr{ G}|1Pr{Y1}X,1,X|1Pr{Y UL  (16) 
Note that when m=1, C}|1Pr{YG}|1Pr{Y1}X|1Pr{Y 1 ====== . 
310 X. Fan et al 
Causality Na?ve Bayesian Classifier (CNB): For a document represented by a bi-
nary-valued vector d=(X1 ,X2 , ?,X|d|), divide the features into two sets X1 and X2, 
X1= {Xi| Xi=1} and X2= {Xj| Xj=0}. The Causality Na?ve Bayesian Classifier can be 
written as: 
}))c|{XPrlog(1}G|logPr{X}(logPr{cmax argc*
||
1
||
1
tj
ji
iit
Cct
??
==?
?++=
21 XX
 (17) 
4.2   Experiments on CNB 
As mentioned earlier, the first step remains unchanged as that in Section 2 and Sec-
tion 3. The difference is in the second step: for the documents identified unreliable in 
the first step, we apply the Causality Na?ve Bayesian Classifier to handle them. 
We use two datasets in the experiments. one is the two-class dataset described in 
Section 2.2, called Dataset-I, and the other one is the multi-class dataset described in 
Section 3.2, called Dataset-I. 
To evaluate CNB and compare all methods presented in this paper, we experiment 
the following methods:  
1) Na?ve Bayesian Classifier (NB), i.e., the method-2 in Section 2.2;  
2) CNB without exploring the two-step strategy;  
3) The two-step strategy: NB and CNB in the first and second step (TS-CNB);  
4) Limited Dependence Bayesian Classifier (DNB) [11];  
5) Method-5 in Section 2.2 and Section 3.2 (denoted TS-DF here).  
Experimental results for two-class Dataset-I and multi-class Dataset-II are listed in 
Table3 and Table 4. The data for NB and TS-DF are derived from the corresponding 
columns of Table 1 and Table 2. The parameters in CNB and TS-CNB are that the 
dependence number k=1 and 5, the threshold?= 0.0545 and 0.0045 for Dataset-I and 
Dataset-II respectively. The parameters in DNB are that dependence number k=1and 
3, the threshold?= 0.0545 and 0.0045 for Dataset-I and Dataset-II respectively. 
Table 3.  Performance comparisons in two-class Dataset-I 
 
Table 3 and Table 4 demonstrate that 1) The  performance of the Na?ve Bayesian 
Classifier can be improved by taking the dependences among features into account, as 
evidenced by the fact that CNB, TS-CNB and DNB outperform NB. By tracing the 
experiment, we find an interesting phenomenon, as expected: for the documents  
 Classifying Chinese Texts in Two Steps 311 
identified reliable by NB, CNB cannot improve it, but for those identified unreliable 
by NB, CNB can improve it. The reason should be even though NB and CNB use the 
same features, but CNB uses the dependences among features additionally. 2) CNB 
and TS-CNB have the same capability in effectiveness, but TS-CNB has a higher 
computational efficiency. As stated earlier, TS-CNB uses NB to classify documents in 
the reliable area and then uses CNB to classify documents in the unreliable area. At 
the first glance, the efficiency of TS-CNB seems lower than that of using CNB only 
because the former additionally uses NB in the first step, but in fact, a majority of 
documents (e.g., 63.13% of the total documents in dataset-I) fall into the reliable area 
and are then treated by NB successfully (obviously, NB is higher than CNB in effi-
ciency) in the first step, so they will never go to the second step, resulting in a higher 
computational efficiency of TS-CNB than CNB. 3) The performances of CNB, TS-
CNB and DNB are almost identical, among which, the efficiency of TS-CNB is the 
highest. And, the efficiency of CNB is higher than that of DNB, because CNB uses a 
simpler network structure than DNB, with the same learning and inference formalism. 
4) TS-DF has the highest performance among the all. Meanwhile, the ranking of 
computational efficiency (in descending order) is NB, TS-DF, TS-CNB, CNB,  
and DNB.  
Table 4.  Performance comparisons in multi-class Dataset-II 
 
5   Related Works 
Combining multiple methodologies or representations has been studied in several 
areas of information retrieval so far, for example, retrieval effectiveness can be im-
proved by using multiple representations [12]. In the area of text categorization in 
particular, many methods of combining different classifiers have been developed. For 
example, Yang et al [13] used simple equal weights for normalized score of each 
classifier output so as to integrate multiple classifiers linearly in the domain of Topic 
Detection and Tracking; Hull at al. [14] used linear combination for probabilities or 
log odds scores of multiple classifier output in the context of document filtering. Lar-
key et al [15] used weighted linear combination for system ranks and scores of multi-
ple classifier output in the medical document domain; Li and Jain [16] used voting 
and classifier selection technique including dynamic classifier selection and adaptive 
classifier. Lam and Lai [17] automatically selected a classifier for each category based 
on the category-specific statistical characteristics. Bennett et al [18] used voting, 
classifier-selection techniques and a hierarchical combination method with  
reliability indicators. 
312 X. Fan et al 
6   Conclusions 
The issue of how to classify Chinese documents characterized by high degree ambi-
guity from text categorization?s point of view is a challenge. For this issue, this paper 
presents two solutions in a uniform two-step framework, which makes use of the 
distributional characteristics of misclassified documents, that is, most of the misclas-
sified documents are near to the separate line between categories. The first solution is 
a two-step TC approach based on the Na?ve Bayesian Classifier. The second solution 
is to further introduce the dependences among features into the model, resulting in a 
two-step approach based on the so-called Causality Na?ve Bayesian Classifier. Ex-
periments show that the second solution is superior to the Na?ve Bayesian Classifier, 
and is equal to CNB without exploring two-step strategy in performance, but has a 
higher computational efficiency than the latter. The first solution has the best per-
formance in all the experiments, outperforming all other methods (including the sec-
ond solution): in the two-class experiments, its F1 increases from the baseline 82.67% 
to the final 95.54%, and in the multi-class experiments, its average F1 increases from 
the baseline 91.48% to the final 98.56%. 
In addition, the other two conclusions can be drawn from the experiments: 1) Us-
ing Chinese word bigrams as features has a better discriminating capability than using 
words as features, but more serious data sparseness will be faced; 2) formula (8) is 
superior to (9) in feature reduction in both the two-class and multi-class Chinese text 
categorization. 
It is worth point out that we believe the proposed method is in principle language 
independent, though all the experiments are performed on Chinese datasets. 
Acknowledgements 
The research is supported in part by the National 863 Project of China under grant 
number 2001AA114210-03, 2003 Korea-China Young Scientists Exchange Program, 
the Tsinghua-ALVIS Project co-sponsored by the National Natural Science Founda-
tion of China under grant number 60520130299 and EU FP6, and the National Natu-
ral Science Foundation of China under grant number 60321002.  
References 
1. Sebastiani, F. Machine Learning in Automated Text Categorization. ACM Computing Sur-
veys, 34(1):1-47, 2002. 
2. Lewis, D. Naive Bayes at Forty: The Independence Assumption in Information Retrieval. 
In Proceedings of ECML-98, 4-15, 1998. 
3. Salton, G. Automatic Text Processing: The Transformation, Analysis, and Retrieval of In-
formation by Computer. Addison-Wesley, Reading, MA, 1989. 
4. Mitchell, T.M. Machine Learning. McCraw Hill, New York, NY, 1996. 
5. Yang, Y., and Liu, X. A Re-examination of Text Categorization Methods. In Proceedings 
of SIGIR-99, 42-49,1999. 
6. Xinghua Fan. Causality Reasoning and Text Categorization, Postdoctoral Research Report 
of Tsinghua University, P.R. China, April 2004. (In Chinese) 
 Classifying Chinese Texts in Two Steps 313 
7. Dumais, S.T., Platt, J., Hecherman, D., and Sahami, M. Inductive Learning Algorithms 
and Representation for Text Categorization. In Proceedings of CIKM-98, Bethesda, MD, 
148-155, 1998. 
8. Sahami, M., Dumais, S., Hecherman, D., and Horvitz, E. A. Bayesian Approach to Filter-
ing Junk E-Mail. In Learning for Text Categorization: Papers from the AAAI Workshop, 
55-62, Madison Wisconsin. AAAI Technical Report WS-98-05, 1998. 
9. Xinghua Fan. Causality Diagram Theory Research and Applying It to Fault Diagnosis of 
Complexity System, Ph.D. Dissertation of Chongqing University, P.R. China, April 2002. 
(In Chinese) 
10. Xinghua Fan, Zhang Qin, Sun Maosong, and Huang Xiyue. Reasoning Algorithm in 
Multi-Valued Causality Diagram, Chinese Journal of Computers, 26(3), 310-322, 2003. 
(In Chinese) 
11. Sahami, M. Learning Limited Dependence Bayesian Classifiers. In Proceedings of the 
Second International Conference on Knowledge Discovery and Data Mining, Portland, 
335-338, 1996. 
12. Rajashekar, T. B. and Croft, W. B. Combining Automatic and Manual Index Representa-
tions in Probabilistic Retrieval. Journal of the American society for information science, 
6(4): 272-283,1995. 
13. Yang, Y., Ault, T. and Pierce, T. Combining Multiple Learning Strategies for Effective 
Cross Validation. In Proceedings of  ICML 2000, 1167?1174, 2000. 
14. Hull, D. A., Pedersen, J. O. and H. Schutze. Method Combination for Document Filtering. 
In Proceedings of SIGIR-96, 279?287, 1996. 
15. Larkey, L. S. and Croft, W. B. Combining Classifiers in Text Categorization. In Proceed-
ings of SIGIR-96, 289-297, 1996. 
16. Li, Y. H., and Jain, A. K. Classification of Text Documents. The Computer Journal, 41(8): 
537-546, 1998. 
17. Lam, W., and Lai, K.Y. A Meta-learning Approach for Text Categorization. In Proceed-
ings of SIGIR-2001, 303-309, 2001. 
18. Bennett, P. N., Dumais, S. T., and Horvitz, E. Probabilistic Combination of Text Classifi-
ers Using Reliability Indicators: Models and Results. In Proceedings of SIGIR-2002, 11-
15, 2002. 
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 450 ? 461, 2005. 
? Springer-Verlag Berlin Heidelberg 2005 
An Ensemble of Grapheme and Phoneme  
for Machine Transliteration 
Jong-Hoon Oh and Key-Sun Choi 
Department of Computer Science, KAIST/KORTERM/BOLA, 
373-1 Guseong-dong, Yuseong-gu, Daejeon, 305-701, Republic of Korea 
{rovellia, kschoi}@world.kaist.ac.kr 
Abstract. Machine transliteration is an automatic method to generate characters 
or words in one alphabetical system for the corresponding characters in another 
alphabetical system. There has been increasing concern on machine translitera-
tion as an assistant of machine translation and information retrieval. Three ma-
chine transliteration models, including ?grapheme-based model?, ?phoneme-
based model?, and ?hybrid model?, have been proposed. However, there are 
few works trying to make use of correspondence between source grapheme and 
phoneme, although the correspondence plays an important role in machine 
transliteration. Furthermore there are few works, which dynamically handle 
source grapheme and phoneme. In this paper, we propose a new transliteration 
model based on an ensemble of grapheme and phoneme. Our model makes use 
of the correspondence and dynamically uses source grapheme and phoneme. 
Our method shows better performance than the previous works about 15~23% 
in English-to-Korean transliteration and about 15~43% in English-to-Japanese 
transliteration. 
1   Introduction 
Machine transliteration is an automatic method to generate characters or words in one 
alphabetical system for the corresponding characters in another alphabetical system. 
For example, English word data is transliterated into Korean ?deita? 1 and Japanese 
?deeta?. Transliteration is used to phonetically translate proper names and technical 
terms especially from languages in Roman alphabets to languages in non-Roman 
alphabets such as from English to Korean, Japanese, and Chinese and so on. There 
has been increasing concern on machine transliteration as an assistant of Machine 
Translation (MT) [2], [10], mono-lingual information retrieval (MLIR) [8], [11] and 
cross-lingual information retrieval (CLIR) [6]. In the area of MLIR and CLIR, ma-
chine transliteration bridges the gap between a transliterated localized form and its 
original form by generating all possible transliterated forms from each original form. 
Especially for CLIR, machine transliteration gives a help to query translation where 
proper names and technical terms frequently appear in source language queries. In the 
area of MT, machine transliteration prevents translation failure when translations of 
                                                          
1
  In this paper, target language transliterations are represented with their Romanization form in 
a quotation mark (??) .  
 An Ensemble of Grapheme and Phoneme for Machine Transliteration 451 
proper names and technical terms are not registered in a translation dictionary. A 
machine transliteration system, therefore, may affect the performance of MT, MLIR, 
and CLIR system. 
Three machine transliteration models have been studied: called ?grapheme2-based 
transliteration model (?G)? [7], [8], [9], [11], [12], [13], ?phoneme3-based translit-
eration model (?P)? [10], [12], and ?hybrid transliteration model (?H)? [2], [4], 
[12]. ?G and ?P are classified in terms of units to be transliterated. ?G is referred to 
the direct model because it directly transforms source language graphemes to target 
language graphemes without any phonetic knowledge of source language words. ?P is 
called the pivot model because it makes use of phonemes as a pivot during a translit-
eration process. Therefore ?P usually needs two steps; the first step is to produce 
phonemes from source language graphemes, and the second step is to produce target 
language graphemes from phonemes. ?H combines ?G and ?P with the linear interpo-
lation style. Hereafter, we will use a source grapheme for a source language grapheme 
and a target grapheme for a target language grapheme. 
Though transliteration is the phonetic process (?P) rather than the orthographic one 
(?G) [10], we should consider both source grapheme and phoneme to achieve high 
performance in machine transliteration because the standard transliterations are not 
restricted to phoneme-based transliterations4. However, many previous works make 
use of either source grapheme or phoneme. They simplify a machine transliteration 
problem into either ?G or ?P assuming that one of ?G and ?P is able to cover all trans-
literation behaviors. However, transliteration is a complex process, which does not 
rely on either source grapheme or phoneme. For example, the standard Korean trans-
literations of amylase and data are grapheme-based transliteration ?amillaaje? and 
phoneme-based transliteration ?deiteo?, respectively. A machine transliteration model, 
therefore, should reflect the dynamic transliteration behaviors in order to produce the 
correct transliterations.  
?H has the limited power for producing the correct transliterations because it just 
combines ?G and ?P with the linear interpolation style. ?H does not consider corre-
spondence between source grapheme and phoneme during the transliteration process. 
However the correspondence plays important roles in machine transliteration. For 
example, phoneme /AH/5 produces high ambiguities since it can be mapped to almost 
every single vowels in source language and target language (the underlined grapheme 
corresponds to /AH/: cinema, hostel, holocaust in English, ?sinema?, ?hostel?, ?hol-
lokoseuteu? in their Korean counterparts, and ?sinema?, ?hoseuteru?, ?horokoosuto? in 
                                                          
2
  Graphemes refer to the basic units (or the smallest contrastive units) of written language: for 
example, English has 26 graphemes or letters, Korean has 24, and German has 30. 
3
  Phonemes are the simplest significant unit of sound (or the smallest contrastive units of the 
spoken language): for example, the /M/, /AE/, and /TH/ in math. 
4
  In an English-to-Korean transliteration test set [14], we find that about 60% are phoneme-
based transliterations, while about 30% are grapheme-based ones. The others are translitera-
tions generated by combining ?G and ?P. 
5
  ARPAbet symbol will be used for representing phonemes. ARPAbet is one of the methods 
used for coding phonemes into ASCII characters (www.cs.cmu.edu/~laura/pages/arpabet.ps). 
In this paper, we will denote phonemes and pronunciation with two slashes like so : /AH/.  
Pronunciation represented in this paper is based on The CMU Pronunciation Dictionary and 
The American Heritage(r) Dictionary of the English Language.  
452 J.-H. Oh and K.-S. Choi 
their Japanese counterparts). If we know the correspondence between source graph-
eme and phoneme in this context, then we can more easily infer the correct translitera-
tion of /AH/, since a target grapheme of /AH/ usually depends on a source grapheme 
corresponding to /AH/. Korean transliterations of source grapheme a is various such 
as ?a?, ?ei?, ?o?, ?eo? and so on. Like the previous example, correspondence makes it 
possible to reduce transliteration ambiguities like Table 1. In Table 1, the underlined 
source grapheme a in the example column is pronounced as the phoneme in the pho-
neme column. The correct Korean transliterations of source grapheme a can be more 
easily found, like in the Korean grapheme column, by means of phonemes in the 
phoneme column. 
Table 1. Examples of Korean graphemes derived from source grapheme a and its correspond-
ing phoneme: the underline indicates source graphemes corresponding to each phoneme in the 
phoneme column 
Korean grapheme  Phoneme  Example 
?a? /AA/ adagio,  safari, vivace 
?ae? /AE/ advantage, alabaster, travertine 
?ei? /EY/ chamber, champagne, chaos 
?i? /IH/ advantage, average, silage 
?o? /AO/ allspice, ball, chalk 
In this paper, we propose a new machine transliteration model based on an ensem-
ble of source grapheme and phoneme, symbolized as ?C (?correspondence-based 
transliteration model?). ?C has two strong points over ?G, ?P, and ?H. First, ?C can 
produce transliterations by considering correspondence between source grapheme and 
phoneme. As described above, correspondence is very useful for reducing translitera-
tion ambiguities. From the viewpoint of reducing the ambiguities, ?C has an advan-
tage over ?G, ?P, and ?H because ?C can more easily reduce the ambiguities by con-
sidering the correspondence. Second, ?C can dynamically handle source grapheme 
and phoneme according to their contexts. Because of this property, ?C can produce 
grapheme-based transliterations as well as phoneme-based transliterations. It can also 
produce a transliteration, where one part is a grapheme-based transliteration and the 
other part is a phoneme-based transliteration. For example, the Korean transliteration 
of neomycin, ?neomaisin?, where ?neo? is a grapheme-based transliteration and 
?maisin? is a phoneme-based transliteration. 
2   Correspondence-Based Machine Transliteration Model 
Correspondence-based transliteration model (?C) is composed of two component 
functions (?C: ?p??t). In this paper, we refer to ?p as a function for ?producing pro-
nunciation? and ?t as a function for ?producing target grapheme?. First, ?p pro-
duces pronunciation and then ?t produces target graphemes with correspondence be-
tween source grapheme and phoneme produced by ?p. The goal of the ?p is to produce 
the most probable sequence of phonemes corresponding to source graphemes. For 
 An Ensemble of Grapheme and Phoneme for Machine Transliteration 453 
example, ?p produces /B/, /AO/, /~/6, /R/, and /D/ for each source grapheme, b, o, a, r, 
and d in board (see ?The result of ?p? in the right side of Fig 1). In this step, pronun-
ciation is generated through two ways; pronunciation dictionary search and pro-
nunciation estimation. A pronunciation dictionary contains the correct pronunciation 
corresponding to English words. Therefore, English words are first investigated 
whether they are registered in the dictionary otherwise their pronunciation is esti-
mated by pronunciation estimation. The goal of ?t is to produce the most probable 
sequence of target graphemes with correspondence between source grapheme and 
phoneme, which is the result of ?p. For example, ?t produces ?b?, ?o?, ?~?, ?~?, and 
?deu? using the result of ?p, b-/B/, o-/AO/, a-/~/, r-/R/, and d-/D/ (see ?The result of ?t? 
in the right side of Fig 1). Finally, the target language transliteration, such as the Ko-
rean transliteration ?bodeu? for board, can be acquired by concatenating the sequence 
of target graphemes in the result of ?t. 
English word
Training Data 
for PE
Training Data for
 t
Dictionary searchi ti r  r
 tt
Transliterations
Pronunciation 
Dictionary
Pronunciation 
Estimation
r ti  
ti ti
board
/D//R//~//AO//B/
draob
/D//R//~//AO//B/
?deu?~~?o??b?
draob
 p
 p
 tt
Result of  p
The result of  p r t f p
The result of  t r lt f t
?bodeu?
 p: Producing Pronunciation
 t : Producing Target Grapheme
p: r i  r i ti
t : r i  t r
 
Fig. 1. The overall system architecture 
Table 2. Feature types used for correspondence-based transliteration model: where S is a set of 
source graphemes (e.g. English alphabets), P is a set of phonemes defined in ARPABET, T is a 
set of target graphemes. Note that fS,GS is a symbol for indicating both fS and fGS. fP,GP is a sym-
bol for indicating both fP and fGP. 
Feature Type Description Possible feature values 
fS,GS fS Source graphemes Source grapheme in S; 26 alphabets 
for English  
 fGS Source grapheme type Consonant (C), and Vowel (V) 
fP,GP fP Phonemes  Phonemes in P (/AA/, /AE/, etc.) 
 fGP Phoneme type Consonant (C), Vowel (V), Semi-
vowel (SV) and silence (/~/) 
 fT Target graphemes Target graphemes in T 
Pronunciation estimation in ?p and ?t are trained by machine learning algorithms. 
To train each component function, we need features that represent training instance 
                                                          
6
  In this paper, ?/~/? represents silence and ?~? represents null target grapheme. 
454 J.-H. Oh and K.-S. Choi 
and data. Table 2 shows five feature types, fS,  fP, fGS, fGP, and fT that our model uses. 
Depending on component functions, different feature types are used. For example, 
?p(si) uses (fS, fGS, fP) and ?t(si, ?p(si)) does (fS,  fP, fGS, fGP, fT). 
2.1   Producing Pronunciation (?p) 
Producing pronunciation (?p:S?P) is a function that finds phonemes in a set P for 
each source grapheme, where P is a set of phonemes defined in ARPABET, and S is a 
set of source graphemes (e.g. English alphabets). The results of this step can be repre-
sented as a sequence of correspondences between source grapheme and phoneme. We 
will denote it as GP={gp1,gp2,?,gpn; gpi=(si,?p(si))} where si is the ith source graph-
eme of SW=s1,s2,...,sn. Producing pronunciation is composed of two steps. The first 
step involves a search in the pronunciation dictionary, which contains English words 
and their pronunciation. This paper uses The CMU Pronouncing Dictionary7, which 
contains 120,000 English words and their pronunciation. The second step involves 
pronunciation estimation. If an English word is not registered in the pronunciation 
dictionary, we must estimate its pronunciation.  
Table 3. An example of pronunciation estimation for b in board 
Feature type L3 L2 L1 C0 R1 R2 R3 ?p(C0) 
fS $ $ $ b o a r 
fGS $ $ $ C V V C 
/B/ 
fP $ $ $      
Let SW=s1,s2,...,sn be an English word, and PSW= p1,p2,...,pn be SW?s pronunciation, 
where si represents the ith grapheme and pi=?p(si). Pronunciation estimation is a task to 
find the most relevant phoneme among a set of all possible phonemes, which can be 
derived from source grapheme si. Table 3 shows an example of pronunciation estima-
tion for b in board. In Table 3, L1~L3 and R1~R3 represent the left contexts and right 
contexts, respectively. C0 means the current context (or focus). ?p(C0) means the esti-
mated phoneme of C0. $ is a symbol for representing the start of words. The result can 
be interpreted as follows. The most relevant phoneme of b, /B/, can be produced with 
the context, fS, fGS, and fP in contexts of L1~L3, C0, and R1~R3. Other phonemes for o, 
a, r, and d in board are produced in the same manner. Thus, we can get the pronuncia-
tion of board as /B AO R D/ by concatenating the phoneme sequence. 
2.2   Producing Target Graphemes (?t) 
Producing target graphemes (?t:S?P?T) is a function that finds the target grapheme 
in T for each gpi that is a result of ?p. A result of this step, GT, is represented by a 
sequence of gpi and its corresponding target graphemes generated by ?t, like GT={gt1, 
gt2 ,?, gtn; gti=(gpi,?t(gpi))}. 
                                                          
7
 Available at http://www.speech.cs.cmu.edu/cgi-bin/cmudict 
 An Ensemble of Grapheme and Phoneme for Machine Transliteration 455 
Table 4. An example of ?t for b in board 
Feature type  L3 L2 L1 C0 R1 R2 R3 ?t(C0) 
fS $ $ $ b o a r ?b? 
fP $ $ $ /B/ /AO/ /~/ /R/  
fGS $ $ $ C V V C  
fGP $ $ $ C V /~/ C  
fT $ $ $      
Let SW=s1,s2,...,sn be a source language word, PSW= p1,p2,...,pn be SW?s pronuncia-
tion and TSW= t1, t2,...,tn be a target language word of SW, where si, ?p(si)=pi and ?t(gpi) 
= ti represent the ith source grapheme, phoneme corresponding to si, and target graph-
eme corresponding to gpi, respectively. ?t finds the most probable target grapheme 
among a set of all possible target graphemes, which can be derived from gpi. ?t pro-
duces target graphemes with source grapheme (fS), phoneme (fP), source grapheme type 
(fGS), phoneme type (fGP) and ?t?s previous output (fT) in the context window. Table 4 
shows an example of ?t for b in board. ?t produces the most probable sequence of tar-
get graphemes (e.g. Korean), like ?t(gp1)= ?b?, ?t(gp2)= ?o?, ?t(gp3)=?~?, ?t(gp4)=?~?, 
and ?t(gp5)=?deu? for board. Finally, the target language transliteration of board as 
?bodeu? can be acquired by concatenating the sequence of produced target graphemes.  
3   Machine Learning Algorithms for Each Component Function 
In this section we will describe a way of modeling component functions using three 
machine learning algorithms (maximum entropy model, decision tree, and memory-
based learning).  
3.1   Maximum Entropy Model 
The maximum entropy model (MEM) is a widely used probability model that can 
incorporate heterogeneous information effectively [3]. In the maximum entropy 
model, an event ev is usually composed of a target event (te) and a history event (he), 
say ev=<te, he>. Event ev is represented by a bundle of feature functions, fei(ev), 
which represent the existence of a certain characteristic in event ev. A feature function 
is a binary valued function. It is activated (fei(ev)=1) when it meets its activating 
condition, otherwise it is deactivated (fei(ev)=0) [3].  
?p and ?t based on the maximum entropy model can be represented as formula (1). 
History events in each component function are made from the left, right and current 
context. For example, history events for ?t are composed of fS,GS (i-3,i+3), fP,GP (i-3,i+3), and 
fT (i-3,i-1) where i is a index of the current source grapheme and phoneme to be translit-
erated and fX(l,m) represents features of feature type fX located from position l to posi-
tion m. Target events are a set of target graphemes (phonemes) derived from history 
events of ?t (?p). Given history events, ?t (?p) finds the most probable target grapheme 
(phoneme), which maximizes formula (1). One important thing in designing a model 
456 J.-H. Oh and K.-S. Choi 
based on the maximum entropy model is to determine feature functions which effec-
tively support certain decision of the model. Our basic philosophy of feature function 
design for each component function is that context information collocated with the 
unit of interest is an important factor. With the philosophy, we determined the history 
events (or activating conditions) of the feature functions by combinations of features 
in feature types. Possible feature combinations for history events are between features 
in the same feature type and between features in different feature types. The used 
feature combinations in each component function are listed in Table 5. 
Table 5. Used feature combinations for history events  
?p ?t 
Between features in the same feature 
type 
Between features in different feature 
types 
z fS,GS  and fP 
Between features in the same feature 
type 
Between features in different feature 
types  
z fS,GS  and fP,GP  
z fS,GS  and fT 
z fP,GP  and fT 
In formula (1), history events of ?p and ?t are defined by the conditions described 
in Table 5. Target events of ?
 t are all possible target graphemes derived from its his-
tory events; while those of ?p are all possible phonemes derived from its history 
events. In order to model each component function based on MEM, Zhang?s maxi-
mum entropy modeling tool is used [16].  
),|(maxarg)(
),,|(maxarg))(,(
3,3,1,3
3,3,3,3,1,3
+???
+?+???
=
=
iiGSSiiPiip
iiGPPiiGSSiiTiipit
ffpps
ffftpss
?
??
 (1) 
3.2   Decision Tree 
Decision tree learning is one of the most widely used and well-known methods for 
inductive inference [15]. ID3, which is a greedy algorithm and constructs decision 
trees in a top-down manner, adopts a statistical measure called information gain that 
measures how well a given feature (or attribute) separates training examples accord-
ing to their target class [15]. We use C4.5 [15], which is a well-known tool for deci-
sion tree learning and implementation of Quinlan?s ID3 algorithm.  
Training data for each component function is represented by features of feature 
types in the context of L3~L1, C0, and R1~R3 as described in Table 3. Fig. 2 shows a 
fraction of our decision trees for ?p and ?t in English-to-Korean transliteration (note 
that the left side represents the decision tree for ?p and the right side represents the 
decision tree for ?t). A set of the target classes in the decision tree for ?p will be a set 
of phonemes and that for
 
?t will be a set of target graphemes. In Fig. 2, rectangles 
indicate a leaf node and circles indicate a decision node. In order to simplify our  
 An Ensemble of Grapheme and Phoneme for Machine Transliteration 457 
examples, we just use fS and fP in Fig. 2. Intuitively, the most effective feature for ?p 
and
 
?t may be located in C0 among L3~L1, C0, and R1~R3 because the correct out-
puts of ?p and ?t strongly depend on source grapheme or phoneme in the C0 position. 
As we expected, the most effective feature in the decision trees is located in the C0 
position like C0(fS) for ?p and C0(fP) for ?t (Note that the first feature to be tested is 
the most effective feature). In Fig. 2, the decision tree for ?p outputs phoneme /AO/ 
for the instance x(SP) by retrieving the decision nodes C(fS)=o, R1(fS)=a, and R2(fS)=r 
represented with ?*?.  With the similar manner, the decision tree for ?t produces target 
grapheme (Korean grapheme) ?o? for the instance x(SPT) by retrieving the decision 
nodes from C0(fP)=/AO/ to R1(fP)=/~/ represented with ?*?.  
C0(fS):o(*)(
R1(fS): yS  R1(fS): e or q( S)    R1(fS): a(*)(  
/OW// / /OY// / /AA// /
R1(fS): xS  ??
R2(fS): d(fS): R2(fS): r(*)(f ): ( ) R2(fS): others(fS): t rR2(fS): $(fS): 
/OW// /OW///AO/(*)/AO/(*)
R1(fS): bfS : 
L2(fS): af )  L2(fS): r( : rL2(fS): $f )  ??
fS
Feature typex(SP)
?
/AO/draob$$
?pR3R2R1C0L1L2L3
Decision tree for ?p 
C0(fP): /AO/ (*))   
C0(fS): aS): C0(fS): e(  C0(fS): o(*)f )  
?o?? ? ?a?? ? ?eu?? ?
C0(fS): othersfS)  ??
R1(fP): /R/(f ): / / R1(fS): /~/(*)(f ): / /( ) R1(fP): others(f ): t r
?o??o? (*)?o? (*)
C0(fS): i(  
L2(fS): a(  L2(fS): rS : rL2(fS): $(  ??
?o?
?
draob$$fS
x(SPT)
fP
Feature type
/D//R//~//AO//B/$$
?tR3R2R1C0L1L2L3
Decision tree for ?ti i  t
 
Fig. 2. Decision tree for ?p and?t 
3.3   Memory-Based Learning 
Memory-based learning (MBL) is an example-based learning method. It is also called 
instance-based learning and case-based learning method.  It is based on a k-nearest 
neighborhood algorithm [1], [5]. MBL represents a training data as a vector. In the train-
ing phase, MBL puts all training data as examples in memory, and clusters some exam-
ples with a k-nearest neighborhood principle. It then outputs a target class using similar-
ity-based reasoning between test data and examples in the memory. Let test data be x 
and a set of examples in a memory be Y, the similarity between x and Y is estimated by a 
distance function, ?(x,Y). MBL selects an example yi or a cluster of examples that are 
most similar to x, then assign a target class of the example to x?s class. We use a mem-
ory-based learning tool called TiMBL (Tilburg memory-based learner) version 5.0 [5].  
Training data for each component function is represented by features of feature 
types in the context of L3~L1, C0, and R1~R3 as described in Table 4. Fig. 3 shows 
examples of ?p and ?t based on MBL in English-to-Korean transliteration. In order to 
simplify our examples, we just use fS and fP in Fig. 3. All training data are represented 
with their features in the context of L3~L1, C0, and R1~R3 and their target classes for 
?p and ?t. They are stored in the memory through a training phase. Feature weighting 
for dealing with features of differing importance is also performed in the training 
phase. In Fig. 3, ?p based on MBL outputs the phoneme /AO/ for x(SP) by comparing 
the similarities between x(SP) and Y using distance metric ?(x(SP),Y). With the simi-
lar manner, ?t based on MBL outputs the target grapheme ?o?. 
458 J.-H. Oh and K.-S. Choi 
x(SP)
/AO/
?
draob$$fS
Feature type ?pR3R2R1C0L1L2L3
Training instances in a memory (?p)i    
0.51/UW/$tuobaefS8
0.16/AO/$$waskcfS7
0.75/W/draode$fS6
0.73/AO/sraoc$$fS5
0.81/OW/$taob$$fS4
0.81/OW/tsaob$$fS3
0.38/OW/$$$obahfS2
0.93/AO/draoba$fS1*
yi Feature 
type
?p(C0) ?(x(SP),yi)R3R2R1C0L1L2L3
/D//R//~//W//D//~/$fP
/S//R//~//OW//K/$$fP
0.31?u?$tuobaefS4
$/T//~//UW//B//~//IY/fP
3
2
1*
yi
fS
fS
fP
fS
Feature 
type
0.55?u?draode$
0.63?o?sraoc$$
?o?
?t(C0)
/D//R//~//AO//B//AH/$
0.89draoba$
? (x(SPT),yi)R3R2R1C0L1L2L3
x(SPT)
?o?
?
draob$$fS
fP
Feature type
/D//R//~//AO//B/$$
?tR3R2R1C0L1L2L3
Training instances in a memory (?t) i  t
 
Fig. 3. Memory-based learning for ?p and ?t 
4   Experiments 
We perform experiments for English-to-Korean and English-to-Japanese translitera-
tion. English-to-Korean test set (EKSet) [14] consists of 7,185 English-Korean pairs ? 
the number of training data is 6,185 and that of test data is 1,000. EKSet contains no 
transliteration variations. English-to-Japanese test set (EJSet), which is an English-
katakana pair in EDICT8, consists of 10,398 ? 1,000 for test and the rest for training. 
EJSet contains transliteration variations, like (micro, ?maikuro?) and (micro, ?mi-
kuro?); the average number of Japanese transliterations for an English word is 1.15. 
Evaluation is performed by word accuracy (W. A.) in formula (2).  
wordsgeneratedof
wordscorrectofAW
  #
  #
.. =  (2) 
We perform two experiments called ?Comparison test? and ?Context window 
size test?. In the ?Comparison test?, we compare our ?C with the previous works. In 
?Context window size test?, we evaluate the performance of our transliteration model 
depending on context window size. 
4.1   Experimental Results 
Table 6 shows results of ?Comparison test?. MEM, DT, and MBL represent ?C 
based on maximum entropy model, decision tree, and memory-based learning, respec-
tively. GDT [8], GPC [9], GMEM [7] and HWFST [4], which are one of the best 
machine transliteration methods in English-to-Korean transliteration and English-to-
Japanese transliteration, are compared with ?C. Table 7 shows the key feature of each 
method in the viewpoint of information type (SG, PH, COR) and information usage 
(Context size, POut). Information type indicates that each transliteration method be-
longs to which transliteration model. For example, GDT, GPC, and GMEM will be-
long to ?G because they use only the source grapheme; while HWFST belongs to ?H. 
Information usage gives information about what kinds of information each translitera-
tion method can deal with. From the viewpoint of information type, phoneme and 
correspondence, which most previous works do not consider, is the key point of the 
performance gap between our method and the previous works.  
                                                          
8
 http://www.csse.monash.edu.au/~jwb/j_edict.html 
 An Ensemble of Grapheme and Phoneme for Machine Transliteration 459 
Table 6. Evaluation results of ?Comparison test? 
Method EKSet  EJSet  
  W.A Chg % W.A Chg % 
GDT 51.4% 23.2% 50.3% 43.5% 
GPC  55.1% 17.6% 53.2% 35.7% 
GMEM  55.9% 16.4% 56.2% 28.5% 
HWFST 58.3% 14.7% 62.5% 15.5% 
DT  62.0% 7.3% 66.8% 8.1% 
MEM  63.3% 5.4% 67.0% 7.8% 
MBL  66.9% 0% 72.2% 0% 
Table 7. Key features of our machine transliteration model and the previous works: SG, PH, 
COR and POut represent source grapheme, phoneme, correspondence and previous output, 
respectively 
Method  SG PH COR Context size POut  
GDT  O X X <-3, +3> X 
GPC  O X X Unbounded O 
GMEM  O X X <-3, +3> O 
HWFST O O X - - 
Ours  O O O <-3, +3> O 
From the viewpoint of information usage, if a transliteration model adopts wide 
context window and considers previous outputs, it tends to show better performance. 
For example, GMEM that satisfies the conditions gives more accurate results  
than GDT which does not satisfy one of them. Because machine transliteration is 
sensitive to context, wider contexts give more powerful transliteration ability to 
machine transliteration systems. Note that the previous works, however, limit their 
context window size to 3, because the context window size over 3 degrades the  
performance [8] or does not change the performance of their transliteration model 
[9]. Determining reasonable context window size, therefore, is very important for 
machine transliteration.  
For ?Context window size test?, we use ?C based on MBL, which shows the best 
performance among three machine learning algorithms in Table 6. Experiments are 
performed by changing the context window size from 1 to 5. Table 8 shows results of 
context window size test. The results indicate that the best performance is shown 
when the context window size is 3. When the context window size is 1, there are 
many cases where the correct transliterations are not produced due to lack of informa-
tion. For example, in order to produce the correct target grapheme of t in -tion, we 
need the right three graphemes of t, -ion. When the context window size is over 3, it is 
difficult to generalize the training data because of increase of variety of the training 
data. With the two reasons, our system shows the best performance when the context 
window size is 3. Table 8 also shows that context size should be at least 2 to avoid 
significant decrease of performance due to lack of contextual information. 
460 J.-H. Oh and K.-S. Choi 
Table 8.  Evaluation results of ?Context window size test? 
Context Size EKSet EJSet 
1 54.5% 62.7% 
2 63.3% 70.0% 
3 66.9% 72.2% 
4 63.9% 70.7% 
5 63.8% 69.3% 
In summary, our method shows significant performance improvement, about 
15%~23%, in English-to-Korean transliteration, and about 15%~ 43% in English-to-
Japanese transliteration. Experiments show that a good transliteration system should 
consider; 1) source grapheme and phoneme along with their correspondence simulta-
neously and 2) reasonable context size and previous output. Our transliteration model 
satisfies the two conditions, thus it shows higher performance than the previous works.  
5   Conclusion  
This paper has described a correspondence-based machine transliteration model (?C). 
Unlike the previous transliteration models, ?C uses correspondence between source 
grapheme and phoneme. The correspondence makes it possible for ?C to effectively 
produce both grapheme-based transliterations and phoneme-based transliterations. 
Moreover, the correspondence helps ?C to reduce transliteration ambiguities more 
easily. Experiments show that ?C is more powerful transliteration model than the 
previous transliteration models (?C shows significant performance improvement, 
about 15%~23%, in English-to-Korean transliteration, and about 15%~ 43% in Eng-
lish-to-Japanese transliteration).  
In future work, we will apply our transliteration model to English-to-Chinese trans-
literation model. In order to prove usefulness of our method in NLP applications, we 
need to apply our system to applications such as automatic bi-lingual dictionary con-
struction, information retrieval, machine translation, speech recognition and so on. 
Acknowledgement 
This work was supported by the Korea Ministry of Science and Technology, the Ko-
rea Ministry of Commerce, Industry and Energy, and the Korea Science and Engi-
neering Foundation (KOSEF). 
References 
1. Aha, D. W. Lazy learning: Special issue editorial. Artificial Intelligence Review, 11:710, 
(1997). 
2. Al-Onaizan Y. and Kevin Knight, ?Translating Named Entities Using Monolingual and 
Bilingual Resources?, In the Proceedings of  ACL 2002, (2002) 
 An Ensemble of Grapheme and Phoneme for Machine Transliteration 461 
3. Berger, A., S. Della Pietra, and V. Della Pietra. , A maximum entropy approach to natural 
language processing. Computational Linguistics, 22(1), (1996), 39?71 
4. Bilac Slaven and Hozumi Tanaka. "Improving Back-Transliteration by Combining Infor-
mation Sources". In Proc. of IJC-NLP2004, (2004) 542?547 
5. Daelemans, W., Jakub Zavrel, Ko van der Sloot, and Antal van den Bosch, 2002, Timble 
TiMBL: Tilburg Memory Based Learner, version 4.3, Reference Guide, ILK Technical 
Report 02-10, (2002). 
6. Fujii, Atsushi and Tetsuya, Ishikawa. Japanese/English Cross-Language Information Re-
trieval: Exploration of Query Translation and Transliteration. Computers and the Humani-
ties, Vol.35, No.4, (2001) 389?420 
7. Goto, I., N. Kato, N. Uratani and T. Ehara, Transliteration Considering Context Informa-
tion Based on the Maximum Entropy Method, In Proceedings of MT-Summit IX, (2003) 
8. Kang B.J. and K-S. Choi, "Automatic Transliteration and Back-transliteration by Decision 
Tree Learning", In Proceedings of the 2nd International Conference on Language Re-
sources and Evaluation, (2000) 
9. Kang, I.H. and G.C. Kim, "English-to-Korean Transliteration using Multiple Unbounded 
Overlapping Phoneme Chunks", In Proceedings of the 18th International Conference on 
Computational Linguistics, (2000). 
10. Knight, K. and J. Graehl, "Machine Transliteration". In Proceedings. of the 35th Annual 
Meetings of the Association for Computational Linguistics (ACL), (1997)  
11. Lee, J. S. and K. S. Choi, English to Korean Statistical transliteration for information re-
trieval. Computer Processing of Oriental Languages, 12(1), (1998), 17-37. 
12. Lee, J.S., An English-Korean transliteration and Retransliteration model for Cross-lingual 
information retrieval, PhD Thesis, Computer Science Dept., KAIST, (1999) 
13. Li Haizhou, Min Zhang and Jian Su , A Joint Source-Channel Model for Machine Trans-
literation , ACL 2004, (2004), 159?166 
14. Nam, Y.S., Foreign dictionary, Sung-An-Dang publisher, (1997) 
15. Quinlan, J.R., "C4.5: Programs for Machine Learning", Morgan Kauffman, (1993) 
16. Zhang, Le. Maximum Entropy Modeling Toolkit for Python and C++. 
http://www.nlplab.cn/zhangle/, (2004) 
Automatic Extraction of English-Korean Translations for Constitu-
ents of Technical Terms 
Jong-Hoon Oh and Key-Sun Choi 
Department of Computer Science, Division of EECS, KAIST/KORTERM/BOLA 
373-1 Guseong-dong, Yuseong-gu, Daejeon, 305-701, Republic of Korea  
{rovellia,kschoi}@world.kaist.ac.kr 
 
Abstract*
Technical terms are linguistic realiza-
tion of a domain concept and their con-
stituents are a component used for 
representing the concept. Many techni-
cal terms are usually multi-word terms 
and their meaning can be inferred from 
their constituents. Because a term con-
stituent is usually a morphological unit 
rather than a conceptual unit in Korean 
technical terms, we need to first iden-
tify conceptual units and then to re-
solve the proper meaning of the 
conceptual units in order to properly 
translate technical terms. For natural 
language applications to properly han-
dle technical terms, it is necessary to 
give information about conceptual units 
and their meaning including homonym, 
synonym and domain dependency. In 
this paper, we propose a term constitu-
ent alignment algorithm, which extracts 
such information from bilingual techni-
cal term pairs. Our algorithm regards 
English term constituents as a concep-
tual unit and then finds its Korean 
counterpart. Our method shows about 
6.1% AER. 
1 Introduction 
Technical terms are linguistic realization of a 
domain specific concept and their constituents 
are a component used for representing the con-
cept (Sager, 1997). Technical terms can be clas-
sified into single-word terms, and complex term 
                                                          
                                                          
* The first author's current affiliation is with Computational 
Linguistics Group, National Institute of Information and 
Communications Technology, 3-5 Hikaridai, Seika-cho, 
Soraku-gun, Kyoto, 619-0289 Japan 
(or multi-word term) according to the number of 
their constituents. Single-word terms have one 
term constituent while complex terms have more 
than one term constituent. Many Korean techni-
cal terms are usually complex terms and their 
meaning can be inferred from their constituents 
(Sager, 1997).  Therefore it is helpful to identify 
constituents of technical terms and their mean-
ing in order to understand the meaning of the 
technical terms and to translate the technical 
term from one?s language to the other. However, 
a term constituent is usually a morphological 
unit rather than a conceptual unit1  in Korean 
technical terms. Due to the mismatch between a 
term constituent and a conceptual unit, we need 
to first identify conceptual units which is a 
chunk of term constituents representing a do-
main specific concept (?chunking conceptual 
units?) and then to resolve the proper meaning 
of the conceptual unit (?resolving meanings?) in 
order to properly understand the meaning of 
technical terms and to translate them.  
In the ?chunking conceptual units? stage, it is 
necessary to determine whether one term con-
stituent represents a concept or not. The decision 
depends on contexts of term constituents. For 
example, a Korean technical term, ?seong? can 
be a conceptual unit by itself when it represents 
sex. But ?seong? in the context of ?hyang-
chuk+seong / bun-yeol+jo-jik? 2  (representing 
adaxial meristem) should be recognized as a 
conceptual unit along with its neighborhood 
?hyang-chuk? such as ?hyang-chuk+seong? (ad-
axial). If ?seong? is recognized as a conceptual 
unit by itself in the context, like ?hyang-chuk 
(adaxial) / seong (sex) / bun-yeol+jo-jik (meris-
1 In this paper, a conceptual unit is defined as the linguistic 
unit representing a domain specific concept. 
2 In this paper, Romanized Korean transcriptions are repre-
sented in the quotation mark. In the transcriptions, ?+? 
represents the boundary of term constituents, ?-? represents 
the syllable boundary and ?/? represents the boundary of 
conceptual units. 
67
tem)?, we can neither understand the designated 
meaning of ?hyang-chuk+seong / bun-yeol+jo-
jik? (?meristem of a leaf cell in the adaxial 
area?) nor properly translate it.  
In the ?resolving meanings? stage, homonym, 
synonym, and domain dependency of conceptual 
units should be considered. Sino-Korean affixes 
are frequently used for coining Korean technical 
terms and are used as a conceptual unit like sin-
gle words. Moreover, they are usually homonym. 
For example, a suffix ?-gi? is used as a term con-
stituent in a biology domain with four senses 
like group (?), period (?), stage (?), and or-
gan (?). Therefore, disambiguating the sense of 
such affixes is very important for understanding 
a Korean technical term. 
Many Korean technical terms are from for-
eign origin. These technical terms become Ko-
rean technical terms with various translation 
ways ? 1) translation with pure Korean words, 
2) translation with Sino-Korean words, 3) trans-
literation, 4) combinations of the three ways. 
Moreover, each translation way produces some 
variations. For example, abdominal is translated 
into three different Korean terms like ?bok-bu?, 
?bok?, and ?bae?, but they indicate the same 
meaning; in other words, they are synonym. ab-
dominal is translated into two Sino-Korean 
terms like ?bok-bu (??)? and ?bok (?)?, and 
one pure Korean term, ?bae?. Capturing syno-
nym, therefore, is important for understanding 
meaning of technical terms. 
Depending on domain of technical terms, 
translations of conceptual units can be different. 
For example, the meaning of cell in chemistry, 
physics, and electricity is usually ?A single unit 
that converts radiant energy into electric energy?, 
while that in biology is usually ?The smallest 
structural unit of an organism?. In each case, 
cell is differently translated into Korean terms 
?jeon-ji? (in chemistry, physics, and electricity 
domain), and ?se-po? (biology domain). 
For natural language applications to properly 
handle technical terms, it is necessary to give 
information about conceptual units and their 
meaning including homonym, synonym and 
domain dependency. In this paper, we propose a 
term constituent alignment algorithm, which 
extracts such information from bilingual techni-
cal term pairs. In our algorithm, one or more 
than one English term constituents are regarded 
as a conceptual unit. Therefore, the main objec-
tive of our algorithm is to recognize conceptual 
units of Korean technical terms corresponding to 
an English term constituent in English-Korean 
translation pairs of technical terms.  
The recognized bilingual conceptual units 
give contextual information, which supports de-
cision whether certain term constituent tends to 
be used as a conceptual unit by itself or not. 
Homonym and synonym can be handled by find-
ing the correspondence between English and 
Korean conceptual units. Because English and 
Korean conceptual units indicating the same 
concept will be linked to each other, we can eas-
ily find homonym and synonym from the rela-
tions. For example, the homonym ?gi? will be 
linked to four different English conceptual units. 
In the same manner, we can capture three rela-
tions between the English conceptual unit ab-
dominal and its counterparts ?bok-bu?, ?bok?, 
and ?bae?. The three Korean counterparts can be 
clustered as synonyms by means of their corre-
sponding English conceptual unit, like {?bok-
bu?, ?bok?, ?bae?}. Moreover, domain depend-
ency of conceptual units can be handled by the 
relations because extracted relations for certain 
English conceptual unit, which has domain de-
pendency, will be different depending on do-
mains.  
This paper organized as follows. In section 2, 
we will describe the related works. Section 3 
shows details of our method. Section 4 deals 
with experiments. Conclusion and future works 
are drawn in sections 5. 
2 Related Works 
One of the well-known alignment techniques is 
the one based on statistical machine translation 
models. It was initially proposed by (Brown et 
al., 1993) and, more recently, have been inten-
sively studied by several research groups (Ger-
mann et al, 2001; Och et al, 2003). It is used 
for finding sentence, phrase, and word-level cor-
respondences from parallel texts. It can be 
formulated as equation (1). For the give source 
text, S, it finds the most probable alignment set, 
A, and target text, T.  
?
?
=
Aa
SaTpSTp )|,()|(    (1) 
Brown (Brown et al, 1993) proposed five 
alignment models, called IBM Model, for an 
English-French alignment task based on equa-
68
tion (1). Equation (2) describes the IBM Model 
1. It is modeled by two assumptions - P(F|E) 
depends on word translation probability t(fj|ei) 
and one English word was aligned to one French 
word (1:1 alignment). t(fj|ei) is estimated by EM 
algorithm.  
??
= =
=
m
j
l
i
ijml eftCEFp
1 1
, )|()|(  (2) 
where, m represents the length of F, l represents 
the length of E, and Cl,m is a constant value de-
termined by l (the length of E) and m (the length 
of F).  
IBM Model 2 considers distortion (How 
likely is a source language word in position i to 
align to a target language word in position j). 
IBM Model 3 adopts fertility (How likely is a 
source language word to align to k target lan-
guage words) as its parameter for 1:n alignment. 
IBM Model 4 and 5 make use of relative distor-
tion, word classes and variables to avoid defi-
ciency.  
There is another stream of studies on align-
ment. (Chen et al, 1993; Gale et al, 1993) pro-
posed sentence alignment techniques based on 
dynamic programming, using sentence length 
and lexical mapping information. (Haruno et al, 
1996; Kay et al, 1993) applied iterative refine-
ment algorithms to sentence level alignment 
tasks.  
In this paper, we propose an alignment algo-
rithm between English and Korean conceptual 
units (or between English and Korean term con-
stituents) in English-Korean technical term pairs 
based on IBM Model (Brown et al, 1993). 
Unlike IBM Model, our alignment model can 
deal with n:1 alignment. While the IBM Model 
aimed to word-level alignment of parallel texts, 
our method focuses on word- and morphology-
level alignment of English-Korean term pairs. 
Moreover, our algorithm reflects the translation 
properties of English-to-Korean technical term 
pairs in a bilingual dictionary. 
3 Term Constituent Alignment 
For term constituent alignment, we use biology, 
chemistry and physics dictionaries where term 
constituents are manually segmented and their 
part-of-speech is manually assigned. For exam-
ple, the Korean counterpart of crop growth rate 
is ?jak-mul + seng-jang + yul? and its three term 
constituents are ?jak-mul?, ?seng-jang?, and ?yul? 
where the first two are a noun and the last one is 
a suffix.  
The problem can be defined as finding corre-
spondence between English and Korean term 
constituents as described in equation (3). For a 
given English term E=e1,?,en, composed of n 
English term constituents and its corresponding 
Korean term K=k1,?,km, composed of m Korean 
term constituents, the task is to find alignment 
set, A={a1,....,at;ap=(ei,i+w(p),, kj(p))}, maximizing 
probability P(A|K,E), where ei is the ith term 
constituent of E, kj is the jth term constituent of 
K, and ap represents the pth alignment relation 
between English and Korean term constituents. 
Note that ap=(ei,i+w(p), kj(p)) (w ? 0) represents an 
alignment relation between English term con-
stituents ei ,?,ei+w  and Korean term constituent 
kj. For example, there are two alignment rela-
tions for English term female sex hormone and 
Korean term ?ja-seong + ho-leu-mon?, like a1 
=(e1,2(1)=female sex, k1(1)=?ja-seong?) and a2 
=(e1(2)=hormone, k2(2)=?ho-leu-mon?) 
),|(maxarg* EKAPA
A
=   (3) 
3.1  Statistical Modeling 
In this section, first, we describe two translation 
properties (or constraints), derived from analysis 
of the alignment tendency between English-
Korean term constituents and then describe how 
to apply these properties to statistical modeling 
of term constituent alignment.  
We randomly sample 20% data of English-
Korean term pairs in each technical dictionary 
and finds two properties ?Cross alignment ap-
pears in some conditions?3 and ?Null Alignment 
hardly appears?4 by analyzing the sampled data. 
Constraint 1: Cross alignment is partly al-
lowed. 
Let algnment units in a source language be si, 
sj(i<j), where i and j are the index of the source 
language, and those in a target language be tq, tr 
(q<r), where q and r are the index in the target 
language. Then alignment ai=(si,tr), and 
aj=(sj,tq) are called cross alignment. Because a 
sentence structure of Korean is different from 
                                                          
3 Among analyzed data, 1.3% for biology, 0.1% for physics 
and 5.65% for chemistry show cross alignment. 
4 Among analyzed data, 0.8% for biology, 0.2% for physics 
and 0.1% for chemistry show null alignment. 
69
that of English, cross-alignment between Eng-
lish and Korean words frequently occurs in par-
allel sentences (Shin et al, 1995). For alignment 
between term constituents, however, most 
alignment relations are derived from sequential 
alignment because technical terms, which are 
usually noun phrases, share the similar structure, 
say modifier and modifee, in both languages. 
Sometimes there is cross-alignment because of 
the preposition in an English term such as of. In 
that case, we allow cross-alignment. For exam-
ple, there is a cross-alignment relation such as a1 
= (e2 = blood, k1 = ?hyeol-aek?) and a2 = (e1 = 
clotting, k2 = ?eung-go?) between the English 
term clotting of blood and its Korean translation 
?hyeol-aek + eung-go?. Note that we do not con-
sider the preposition of as an alignment unit in 
that case. English-Korean term pairs represent-
ing a name of chemical compounds usually 
show cross-alignment and 1:1 alignment. To 
deal with this case, we allow cross-alignment 
when the number of English term constituents 
and that of Korean term constituents are same. 
With the constraint 1, sequential alignment is 
performed except the above two cases.  
Constraint 2:  Null Alignment is not allowed. 
Constraint 2 means that all English and Korean 
term constituents should be aligned. Because, 
term pairs consist of an English term and its 
translated Korean term, we assume that all con-
stituents should be aligned. Null alignment 
means that an alignment unit in one side is 
aligned to nothing in the other side. For example, 
for Dutch elm disease and ?ne-deol-lan-deu 
(Dutch) / neu-leup-na-mu (elm) / che-gwan 
(sieve tube) / byeong (disease)?, there is no Eng-
lish term constituent to be aligned to the Korean 
term constituent ?che-gwan (sieve tube)?. Be-
cause, null alignment, however, does not fre-
quently appear in term constituent alignment 
(only the 0.1%~0.8% data among analyzed data), 
we do not consider null alignment in our algo-
rithm. 
),,,|(),|(
),|(
1
)(,)( tmnjiaekap
EKAP
t
l
lwiiljl?
=
+ ?=  (4) 
),|()|(
),,|(
),|(
,,
)(,)()(
)(,)(
wiij
t
j
w
wiij
t
lwiilj
w
lj
t
l
lwiiljl
ekkpekp
ekkap
ekap
++
+
+
??
=  (5) 
By the constraints, equation (3) can be repre-
sented as equation (4). In equation (4), n, m, and 
t represent the number of English term constitu-
ents, the number of Korean term constituents 
and the number of alignment relations between 
term constituents. In equation (4), a(i|j,n,t) 
represents position information, which is a bi-
nary-valued function and supports the constraint 
1. a(i|j,n,m,t) = 0 when ap= a(ei,i+w(p),kj(p)) is 
cross-alignment, which is not allowed by con-
straint 1, otherwise a(i|j,n,m,t) = 1. 
In equation (4), p(al|kj(l),ei,i+w(l)) are estimated 
by equation (5). In equation (5), kj(l) is repre-
sented by kwj and ktj where kwj and ktj are lexical 
information and part of speech information of 
the jth Korean term constituent, respectively.  
3.2 Parameter Estimation with EM Algo-
rithm 
Parameters, p(ktj|ei,i+w) and p(kwj| ktj, ei,i+w), in 
equation (5) are estimated with EM (Expecta-
tion-Maximization) algorithm. EM algorithm is 
the technique for parameter estimation of ge-
neric statistical distributions in presence of in-
complete data (Dempster et al, 1997). The main 
goal of EM is to obtain the estimated parameters 
that give maximum likelihood to the input (in-
complete) data. The basic idea underlying the 
EM algorithm is to iterate through a series of 
expectation (E-step) and maximization (M-step) 
steps where the estimation of the parameters of 
the model is progressively refined until conver-
gence (Lopez et al, 1999).  
In this paper, parameters are estimated 
through two steps, called ?initial parameter es-
timation? and ?iterative parameter estimation?. 
In the initial parameter estimation step, the ini-
tial parameters are determined by seed data. 
Seed data, which contains alignment relations 
derived from E=e1,?,en  and E?s Korean transla-
tion K=k1,?,km, where n =1 or m = 1, was se-
lected among data for term constituent 
alignment. In the condition of n = 1 or m = 1, 
English technical terms or Korean technical 
terms are a conceptual unit by itself. In other 
words, alignment relations can be directly ex-
tracted from the English-Korean term pairs if 
70
there is only one English term constituent or 
only one Korean term constituent. With the seed 
data we can get the initial alignment relation set 
A(0) and then the initial parameter ?(0) is esti-
mated with A(0), where A(k) represents the 
alignment relation set and ?(k) represents the 
estimated parameter set derived from the kth it-
eration. Note that A={a1,....,at;ap=(ei,i+w(p),kj(p))} 
and ?={p(ktj|ei,i+w), p(kwj| ktj, ei,i+w)}.  
In the iterative parameter estimation step, 
A(k) is determined by ?(k-1) in E-step and ?
(k) is estimated by A(k) in M-step using the 
whole data until ?(k) converges. E-step and M-
step can be represented as equation (6) 
))1(;,|(maxarg)(: ?=? kKEApkAstepE A ?
))(|(maxarg)(: kApkstepM ?? ?=?  (6) 
p(ktj|ei) and p(kwj| ktj, ei) are estimated in the 
kth iteration as equation (7) and (8), respectively. 
In order to prevent zero probability, the Laplace 
smoothing method (Manning et al, 1999) is ap-
plied to equation (7) and (8). 
))(;(||
))(;,(1
))(;|(
,
,
, kAeCE
kAekC
kAekp
wii
wiij
t
wiij
t
+
+
+ +
+=       (7) 
))(;,(||||
))(;,,(1
))(;,|(
,
,
, kAekCET
kAekkC
kAekkp
wiij
t
wiij
t
j
w
wiij
t
j
w
+
+
+ ++
+= (8) 
where C(x) represents frequency of x, |E| repre-
sents the number of unique English term con-
stituents in A(k), |T| represents the number of 
unique POS tags of Korean term constituents in 
A(k). 
4 Experiments 
For experiments we use three kinds of technical 
dictionary. They are biology, chemistry, and 
physics technical dictionaries where Korean 
term constituents are manually analyzed. The 
characteristics of experimental data are summa-
rized as Table 1 (Ministry, 2002).  
Domain Seed data Test data Total  
Biology  8,163 5,668 13,831
Physics 2,757 8,047 10,804
Chemistry  5,353 10,024 15,377
Table 1. Characteristics experimental data (the 
number of bilingual term pairs) 
We compare our model with IBM Model 2 
(IBM-2), and IBM Model 4 (IBM-4) imple-
mented by GIZA++ (Och et al, 2003). We 
evaluate results with the alignment error rate 
(AER) of Och and Ney (Och et al, 2003), which 
measures agreement at the level of pairs of term 
constituents.5
||||
||21
GA
GAAER +
???=    (9) 
where A is the set of term constituent pairs 
aligned by the automatic system, and G is the set 
aligned in the gold standard. 
4.1 Experimental results 
Table 2 shows evaluation results for IBM-2, 
IBM-4 and our proposed method. In the results 
precision and AER of our proposed method is 
higher than those of IBM-4. But recall of our 
proposed method is lower than that of IBM-4. 
IBM-4 has strong points in handling cross-
alignment and null alignment while our model 
has strong points in handling n:1 alignment. The 
difference between our model and IBM-4 causes 
the performance gap. Because most alignment 
type found in the gold standard is 1:1 alignment 
and 1:n alignment rather than cross-alignment, 
null alignment, and n:1 alignment as described 
in Table 3, the performance gap between our 
method and IBM-4 is not so big. IBM-2 shows 
the worst performance because it can not deal 
with 1:n alignment. In other words, IBM-2 does 
not consider fertility as its parameter for estimat-
ing the translation probability. Note that 1:n 
alignment in the gold standard is about 
18%~22% (see Table 3).  
Domain IBM-2 IBM-4 Proposed 
Biology 25.0% 7.4% 6.5% 
Physics 30.0% 9.6% 5.2% 
Chemistry 28.7% 7.6% 6.5% 
Table 2. Experimental Results  
Type Biology  Physics Chem. 
Null alignment 0.6% 0.2% 0.2% 
Cross alignment 2.1% 0.2% 4.4% 
n:1 alignment 2.1% 1.6% 1.2% 
1:n alignment 16.5% 21.4% 19.0% 
1:1 alignment 78.7% 76.7% 75.3% 
Table 3. Alignment types found in the gold 
standard 
When we analyze errors caused by our 
method, errors are mainly caused by n:1 align-
ment and cross-alignment. In order to produce 
relevant alignment results for n:1 alignment, we 
need information indicating that more than one 
                                                          
5 While (Och et al, 2003) differentiates sure and possible 
hand-annotated alignment, our gold-standard comes in only 
one variety. 
71
English term constituents are used as a concep-
tual unit. Due to lack of the information, our 
model has limitation on recovering errors caused 
by n:1 alignment. It is necessary to use domain 
specific corpus as a way of relaxing the problem. 
Cross alignment, which our model does not al-
low due to constrain 1, makes errors. Due to the 
cross alignment, the performance of our method 
in chemistry and biology is lower than that in 
physics, where there are few cross alignments in 
the gold standard. 
5 Conclusion 
In this paper, we have described an alignment 
algorithm between English and Korean term 
constituents. Our alignment algorithm can han-
dle cross alignment, n:1 alignment and 1:n 
alignment between term constituents. Our 
method shows about 94.7% precision, 93.2% 
recall and 6.1% alignment error rate. However, 
there are scopes to improve performance still 
further. Constraints should be relaxed in order to 
generalize our model and overcome errors 
caused by them.  
Our method can be applied to handle techni-
cal terms in three aspects. First, alignment re-
sults produced by our alignment algorithm help 
a machine translation system to consistently 
translate new English technical terms to Korean 
terms by considering domain of the technical 
terms. Second, alignment results between term 
constituents can be used for constructing term 
formation patterns or word formation patterns. 
Because relations between conceptual units can 
be extracted from the alignment results, we can 
construct concept-level term formation patterns 
using them. Third, the alignment results can be 
used as a resource for recognizing term varia-
tions. Because alignment relations acquired by 
our alignment model offer information about 
homonym, synonym and domain dependency, 
term variations related to certain term constitu-
ent can be recognized using them. 
Acknowledgement 
This work was supported by the Korea Ministry 
of Science and Technology, the Korea Ministry 
of Commerce, Industry and Energy, and the Ko-
rea Science and Engineering Foundation 
(KOSEF). 
References 
Brown P.F., V.S.A. Della Petra, V.J. Della Pietra and 
R.L. Mercer, ?The mathematics of statistical ma-
chine translation: parameter estimation?, Compu-
tational Linguistics, Vol. 19 No 2, (1993) 263?
311 
Chen, S, F., Aligning Sentences in Bilingual Corpora 
Using Lexical information, in proceedings of 31st 
ACL, (1993) 9?16 
Dempster A.P., N.M. Laird, and D.B. Rubin. Maxi-
mum likelihood from incomplete data via the EM 
algorithm. Journal of Royal Statistical Society, 
39(1):138, (1977) 
Gale, W. A. And Church K.W.  A program for al-
ingning sentences in Bilingual Corpora, Computa-
tional linguistics, vol 19, no 1, (1993), 75?102 
Germann, U. M.Jahr, Knight, K., Marcu, D. And 
Yamada, K. Fast Decoding and Optimal Decoding 
for Machine translation, in proceedings of 39th 
ACL, (2001) 228?235  
Haruno M., and Yamazaki, T. High-performance 
Bilingual Text alignment using Statistical and Dic-
tionary information, in proceedings of 34th ACL, 
(1996) 131?138 
Kay, M. and Roscheisen, M. Text-Translation Align-
ment, Computational Linguistics, Vol 19, No 1, 
(1993) 121?142  
L?pez de Teruel P. E., Jos? M. Garc?a and Manuel E. 
Acacio. The Parallel EM Algorithm and its Appli-
cations in Computer Vision. Parallel and Distrib-
uted Processing Techniques and Applications, 
(1999). 
Manning, C.D. and H. Schutze, Foundations of statis-
tical natural language processing, MIT Press 
(1999) 
Ministry of Culture and Tourism, "Forming the foun-
dation of Terminology Standardization?, 
http://www.korterm.or.kr/, (2002) 
Och, Franz Josef and Hermann Ney. A Systematic 
Comparison of Various Statistical Alignment 
Models, Computational Linguistics, Vol 29 (1), 
(2003), 19?51  
Sager, J.C. ?Section 1.2.1 Term formation?, in Hand-
book of terminology management Vol.1, John 
Benjamins publishing company, (1997) 
Shin Jung Ho and Key-Sun Choi (1995), Aligning a 
parallel Korean-English corpus at word and phrase 
level, Proceedings of the 3rd Natural Language 
Processing Pacific Rim Symposium (NLPRS'95), 
(1995)  223?227 
72
 
	 
	
		
	
	
	
	
	




	

	Term Recognition Using Technical Dictionary Hierarchy 
 
Jong-Hoon Oh, KyungSoon Lee, and Key-Sun Choi 
Computer Science Dept., Advanced Information TechnologyResearch Center (AITrc), and 
Korea Terminology Research Center for Language and Knowledge Engineering (KORTERM) 
Korea Advanced Institute of Science & Technology (KAIST)  
Kusong-Dong, Yusong-Gu Taejon, 305-701 Republic of Korea  
{rovellia,kslee,kschoi}@world.kaist.ac.kr  
 
 
 
Abstract  
In recent years, statistical approaches on 
ATR (Automatic Term Recognition) have 
achieved good results. However, there are 
scopes to improve the performance in 
extracting terms still further. For example, 
domain dictionaries can improve the 
performance in ATR. This paper focuses on 
a method for extracting terms using a 
dictionary hierarchy. Our method produces 
relatively good results for this task. 
Introduction 
In recent years, statistical approaches on ATR 
(Automatic Term Recognition) (Bourigault, 
1992; Dagan et al 1994; Justeson and Katz, 
1995; Frantzi, 1999) have achieved good results. 
However, there are scopes to improve the 
performance in extracting terms still further. For 
example, the additional technical dictionaries 
can be used for improving the accuracy in 
extracting terms. Although, the hardship on 
constructing an electronic dictionary was major 
obstacles for using an electronic technical 
dictionary in term recognition, the increasing 
development of tools for building electronic 
lexical resources makes a new chance to use 
them in the field of terminology. From these 
endeavour, a number of electronic technical 
dictionaries (domain dictionaries) have been 
acquired.  
Since newly produced terms are usually made 
out of existing terms, dictionaries can be used as 
a source of them. For example, ?distributed 
database? is composed of ?distributed? and 
?database? that are terms in a computer science 
domain. Further, concepts and terms of a domain 
are frequently imported from related domains. 
For example, the term ?Geographical 
Information System (GIS)? is used not only in a 
computer science domain, but also in an 
electronic domain. To use these properties, it is 
necessary to build relationships between 
domains. The hierarchical clustering method 
used in the information retrieval offers a good 
means for this purpose. A dictionary hierarchy 
can be constructed by the hierarchical clustering 
method. The hierarchy helps to estimate the 
relationships between domains. Moreover the 
estimated relationships between domains can be 
used for weighting terms in the corpus. For 
example, a domain of electronics may have a 
deep relationship to that of computer science. As 
a result, terms in the dictionary of electronics 
domain have a higher probability to be terms of 
computer science domain than terms in the 
dictionary of others do (Felber, 1984).  
The recent works on ATR identify the 
candidate terms using shallow syntactic 
information and score the terms using statistical 
measure such as frequency. The candidate terms 
are ranked by the score and are truncated by the 
thresholds. However, the statistical method 
solely may not give accurate performance in 
case of small sized corpora or very specialized 
domains, where the terms may not appear 
repeatedly in the corpora. 
In our approach, a dictionary hierarchy is 
used to avoid these limitations. In the next 
section, we describe the overall method 
description. In section 2, section 3, and section 4, 
we describe primary methods and its details. In 
section 5, we describe experiments and results 
1 Method Description 
 
The description of the proposed method is 
shown in figure 1. There are three main steps in 
our method. In the first stage, candidate terms 
that are complex nominal are extracted by a 
linguistic filter and a dictionary hierarchy is 
constructed. In the second stage, candidate terms 
are scored by each weighting scheme. In 
dictionary weighing scheme, candidate terms are 
scored based on the kind of domain dictionary 
where terms appear. In statistical weighting 
scheme, terms are scored by their frequency in 
the given corpus. In transliterated word 
weighting scheme, terms are scored by the 
number of transliterated foreign words in the 
terms. In the third stage, each weight is 
normalized and combined to Term weight 
(Wterm), and terms are extracted by Term weight.   
Figure 1. The method description 
2 Dictionary Hierarchy 
2.1 Resource 
Field 
Agrochemical, Aerology, Physics, Biology, 
Mathematics, Nutrition, Casting, Welding, 
Dentistry, Medical, Electronical engineering, 
Computer science, Electronics, Chemical 
engineering, Chemistry.... and so on. 
Table 1. The fragment of a list: dictionaries of 
domains used for constructing the hierarchy. 
A dictionary hierarchy is constructed using 
bi-lingual dictionaries (English to Korean) of the 
fifty-seven domains. Table 1 lists the domains 
that are used for constructing the dictionary 
hierarchy. The dictionaries belong to domains of 
science and technology. Moreover, terms that do 
not appear in any dictionary (henceforth we call 
them unregistered terms) are complemented by a 
domain tagged corpus. We use a corpus, called 
ETRI-KEMONG test collection, with the 
documents of seventy-six domains to 
complement unregistered terms and to eliminate 
common term.  
2.2 Constructing Dictionary Hierarchy  
The clustering method is used for constructing 
a dictionary hierarchy. The clustering is a 
statistical technique to generate a category 
structure using the similarity between 
documents (Anderberg, 1973). Among the 
clustering methods, a reciprocal nearest 
neighbor (RNN) algorithm (Murtaugh, 1983) 
based on a hierarchical clustering model is used, 
since it joins the cluster minimizing the increase 
in the total within-group error sum of squares at 
each stage and tends to make a symmetric 
hierarchy (Lorr, 1983). The algorithm to form a 
cluster can be described as follows:  
 
1. Determine all inter-object (or 
inter-dictionary) dissimilarity. 
2. Form cluster from two closest objects 
(dictionaries) or clusters. 
3. Recalculate dissimilarities between new 
cluster created in the step2 and other 
object (dictionary) or cluster already 
made. (all other inter-point dissimilarities 
are unchanged). 
4. Return to Step2, until all objects 
(including cluster) are in the one cluster. 
 
In the algorithm, all objects are treated as a 
vector such as Di = (xi1, xi2, ... , xiL ). In the step 
1, inter-object dissimilarity is calculated based 
on the Euclidian distance. In the step2, the 
closest object is determined by a RNN. For 
given object i and object j, we can define that 
there is a RNN relationship between i and j 
when the closest object of i is object j and the 
closest object of j is object i. This is the reason 
why the algorithm is called a RNN algorithm. A 
dictionary hierarchy is constructed by the 
algorithm, as shown in figure 2. There are ten 
domains in the hierarchy ? this is a fragment of 
whole hierarchy. 
 
Technical
Dictionaries
Domain 
tagged
Documents 
?.A CB D ?.
Constructing  
hierarchy
POS-tagged
Corpus Linguistic filter
Abbreviation and
Translation pairs
extraction
Candidate term
Frequency based
Weighing
Transliterated
Word detection
Transliterated word
Based Weighting
Complement 
Unregistered Term
Scoring by hierarchy
Eliminate
Common Word
Dictionary based 
Weighting
Statistical
Weight
Transliterated
Word Weight
Dictionary
Weight
Term Recognition
 
Figure 2. The fragment of whole dictionary 
hierarchy : The hierarchy shows that domains 
clustered in the terminal node such as chemical 
engineering and chemistry are highly related. 
2.3 Scoring Terms Using Dictionary 
Hierarchy 
The main idea for scoring terms using the 
hierarchy is based on the premise that terms in 
the dictionaries of the target domain and terms 
in the dictionary of the domain related to the 
target domain act as a positive indicator for 
recognizing terms. Terms in the dictionaries of 
the domains that are not related to the target 
domain act as a negative indicator for 
recognizing terms. We apply the premise for 
scoring terms using the hierarchy. There are 
three steps to calculate the score. 
 
1. Calculating the similarity between the 
domains using the formula (2.1) (Maynard 
and Ananiadou, 1998) 
 
where  
Depthi: the depth of the domaini node in the 
hierarchy 
Commonij: the depth of the deepest node 
sharing between the domaini and the 
domainj in the path from the root. 
 
In the formula (2.1), the depth of the node 
is defined as a distance from the root ? the 
depth of a root is 1. For example, let the 
parent node of C1 and C8 be the root of 
hierarchy in figure 2. The similarity between 
?Chemistry? and ?Chemical engineering? is 
calculated as shown below in table 2: 
 
Domain Chemistry Chemical 
Engineering 
Path from 
the root 
Root->C8-> 
C9->Chemistry 
Root->C8->C9-> 
Chemical 
Engineering 
Depthi 4 4 
Common ij 3 3 
Similarity 
ij 
2*3/(4+4) =0.75 2*3/(4+4) =0.75 
Table 2. Similarityij  calculation: The table shows 
an example in caculating similarity using formula 
(2.1). In the example, Chemical engineering 
domain and Chemistry domain are used. Path, 
Depth, and Common are calculated according to 
figure 1. Then similarity between domains are 
determined to 0.75. 
2.Term scoring by distance between a target 
domain and domains where terms appear: 
 
  
where  
N: the number of dictionaries where a 
term appear  
Similarityti: the similarity between the 
target domain and the domain dictionary 
where a term appears  
 
For example, in figure 2, let the target 
domain be physics and a term ?radioactive? 
appear in physics, chemistry and astronomy 
domain dictionaries. Then similarity between 
physics and the domains where the term 
?radioactive? appears can be estimated by 
formula (2.1) as shown below. Finally, 
Score(radioactive) is calculated by formula 
(2.2) ? score is (0.4+1+0.7)/3.:  
 
N 3 
similarity physics-chemistry 0.4 
similarity physics-physics 1 
similarity physics-astronomy 0.7 
Score(radioactive) 2.1*1/3 = 0.7  
Table 3. Scoring terms based on similarity 
between domains 
 
3. Complementing unregistered terms and 
common terms by domain tagged corpora.  
 
)1.2(2
ji
ij
ij depthdepth
Commonsimilarity
+
?
=
)2.2(1)(
1
?
=
=
N
i
tisimilarityNtermScore
 
where 
W: the number of words in the term ??? 
dofi: the number of domain that words in 
the term appear in the domain tagged 
corpus. 
 
Consider two exceptional possible cases. First, 
there are unregistered terms that are not 
contained in any dictionaries. Second, some 
commonly used terms can be used to describe a 
special concept in a specific domain dictionary.  
Since an unregistered term may be a newly 
created term of domains, it should be considered 
as a candidate term. In contrast with an 
unregistered term, common terms should be 
eliminated from candidate terms. Therefore, the 
score calculated in the step 2 should be 
complemented for these purposes. In our method, 
the domain tagged corpus (ETRI 1997) is used. 
Each word in the candidate terms ? they are 
composed of more than one word ? can appear 
in the domain tagged corpus. We can count the 
number of domains where the word appears. If 
the number is large, we can determine that the 
word have a tendency to be a common word. If 
the number is small, we can determine that the 
word have a high probability to be a valid term. 
In this paper, the score calculated by the 
dictionary hierarchy is called Dictionary Weight 
(WDic). 
3. Statistical Method 
The statistical method is divided into two 
elements. The first element, the Statistical 
Weight, is based on the frequencies of terms. 
The second element, the Transliterated word 
Weight, which is based on the number of 
transliterated foreign word in the candidate term. 
This section describes the above two elements.  
3.1. Statistical Weight: Frequency Based 
Weight 
In the Statistical Weight, not only 
abbreviation pairs and translation pairs in a 
parenthetical expression but also frequencies of 
terms are considered. Abbreviation pairs and 
translation pairs are detected using the following 
simple heuristics: 
 
For a given parenthetical expression A(B), 
1. Check on a fact that A and B are 
abbreviation pairs. The capital letter of A is 
compared with that of B. If the half of the 
capital letter are matched for each other 
sequentially, A and B are determined to 
abbreviation pairs (Hisamitsu et. al, 1998). 
For example, ?ISO? and ?International 
Standardization Organization? is detected as 
an abbreviation in a parenthetical expression 
?ISO (International Standardization 
Organization)?. 
 
2. Check on a fact that A and B are translation 
pairs. Using the bi-lingual dictionary, it is 
determined. 
 
After detecting abbreviation pairs and 
translation pairs, the Statistical Weight (WStat) of 
the terms is calculated by the formula (3.1). 
 
where  
?: a candidate term 
|?|: the length of a term??? 
S (?): abbreviation and translation pairs of 
??? 
T(?): The set of candidate terms that nest 
??? 
f(?): the frequency of ?? ? 
C(T(?)): The number of elements in T(?) 
 
In the formula (3.1), the nested relation is 
defined as follows: let A and B be a candidate 
term. If A contains B, we define that A nests B.  
The formula implies that abbreviation pairs 
and translation pairs related to ??? is counted as 
well as ??? itself and productivity of words in 
the nested expression containing ??? gives more 
weight, when the generated expression contains 
???. Moreover, formula (1) deals with a single- 
word term, since an abbreviation such as GUI 
(Graphical User Interface) is single word term 
and English multi-word term usually translated 
to Korean single-word term ? (e.g. distributed 
database => bunsan deitabeisu) 
)3.2(*)1)(()( 1W
dof
ScoreW
W
i
i
Dic
?
=+= ??
( )
??
?
?
??
?
?
?
??
?
?
?
??
?
?
?
??
?
?
?
??
?
?
?
+?
?
=
?
?
?
??
?
??
}{)(
)(
}{)(
)1.3())((
)(
)(
)(
)(
???
??
???
?
?
??
???
?
S
T
S
Stat
otherwiseTC
f
f
nestedisiff
W
3.2 Transliterated word Weight: By 
Automatic Extraction of Transliterated 
words 
Technical terms and concepts are created in 
the world that must be translated or transliterated. 
Transliterated terms are one of important clues 
to identify the terms in the given domain. We 
observe dictionaries of computer science and 
chemistry domains to investigate the 
transliterated foreign words. In the result of 
observation, about 53% of whole entries in a 
dictionary of a computer science domain are 
transliterated foreign words and about 48% of 
whole entries in a dictionary of a chemistry 
domain are transliterated foreign words. Because 
there are many possible transliterated forms and 
they are usually unregistered terms, it is difficult 
to detect them automatically.  
In our method, we use HMM (Hidden Markov 
Model) for this task (Oh, et al, 1999). The main 
idea for extracting a foreign word is that the 
composition of foreign words would be different 
from that of pure Korean words, since the 
phonetic system for the Korean language is 
different from that of the foreign language. 
Especially, several English consonants that 
occur frequently in English words, such as 
?p?, ?t?, ?c?, and ?f?, are transliterated into Korean 
consonants ?p?, ?t?, ?k?, and ?p? respectively. 
Since these consonants of Korean are not used in 
pure Korean words frequently, this property can 
be used as an important clue for extracting a 
foreign word from Korean. For example, in a 
word, ?si-seu-tem? (system), the syllable ?tem? 
have a high probability to be a syllable of 
transliterated foreign word, since the consonant 
of ?t? in the syllable ?tem? is usually not used in 
a pure Korean word. Therefore, the consonant 
information which is acquired from a corpus can 
be used to determine whether a syllable in the 
given term is likely to be the part of a foreign 
word or not.  
Using HMM, a syllable is tagged with ?K? or 
?F?. A syllable tagged with ?K? means that it is 
part of a pure Korean word. A syllable tagged 
with ?F? means that it is part of a transliterated 
word. For example, ?si-seu-tem-eun (system is)? 
is tagged with  ?si/F + seu/F + tem/F + eun/K?. 
We use consonant information to detect a 
transliterated word like lexical information in 
part-of-speech-tagging. The formula (3.2) is 
used for extracting a transliterated word and the 
formula (3.3) is used for calculating the 
Transliterated Word Weight (WTrl). The formula 
(3.3) implies that terms have more transliterated 
foreign words than common words do. 
 
where  
si: i-th consonant in the given word. 
ti: i-th tag (?F? or ?K?) of the syllable in the 
given word. 
 
 
where  
|?| is the number of words in the term ? 
trans(?) is the number of transliterated 
words in the term ? 
4.Term Weighting 
The three individual weights described above 
are combined according to the following 
formula (4.1) called Term Weight (WTerm) for 
identifying the relevant terms.  
 
Where 
?: a candidate term ??? 
f,g,h : normalization function 
?+?+? = 1 
 
In the formula (4.1), the three individual 
weights are normalized by the function f, g, and 
h respectively and weighted parameter ?,?, and 
?. The parameter ?,?, and ? are determined by 
experiment with the condition ?+?+? = 1. Each 
value which is used in this paper is ?=0.6, ? 
=0.1, and ?=0.3 respectively. 
 
)3.3()()(
?
?
?
transWTrl =
)2.3()|(),|(
)|()()()|(
13
21
121
??
???
???
???
?
=
??
==
??
n
i
ii
n
i
iii tsptttp
ttptpSPSTP
)1.4())(())((
))(()(
????
???
StatTrl
Dicterm
WhWg
WfW
?+?
+?=
5. Experiment 
The proposed method is tested on a corpus of 
computer science domains, called the KT test 
collection. The collection contains 4,434 
documents and 67,253 words and contains 
documents about the abstract of the paper (Park. 
et al, 1996). It was tagged with a part-of-speech 
tagger for evaluation. We examined the 
performance of the Dictionary Weight (WDic) to 
show its usefulness. Moreover, we examined 
both the performance of the C-value that is 
based on the statistical method (Frantzi. et al, 
1999) and the performance of the proposed 
method. 
5.1 Evaluation Criteria 
Two domain experts manually carry out the 
assessment of the list of terms extracted by the 
proposed method. The results are accepted as the 
valid term when both of the two experts agree on 
them. This prevents the evaluation from being 
carried out subjectively, when one expert 
assesses the results. The results are evaluated by 
a precision rate. A precision rate means that the 
proportion of correct answers to the extracted 
results by the system. 
5.2 Evaluation by Dictionary Weight 
(WDic) 
In this section, the evaluation is performed 
using only WDic to show the usefulness of a 
dictionary hierarchy to recognize the relevant 
terms The Dictionary Weight is based on the 
premise that the information of the target 
domain is a good indicator for identifying terms. 
The term in the dictionaries of the target domain 
and the domain related to the target domain acts 
as a positive indicator for recognizing terms. 
The term in the dictionaries of the domains, 
which are not related to the target domain acts as 
a negative indicator for recognizing terms. The 
dictionary hierarchy is constructed to estimate 
the similarity between one domain and another. 
 
 Top 10% Bottom 10% 
The Valid Term 94% 54.8% 
Non-Term 6% 45.2% 
Table 4.  terms and non-terms by Dictionary 
Weight 
The result, depicted in table 4, can be 
interpreted as follows: In the top 10% of the 
extracted terms, 94% of them are the valid terms 
and 6% of them are non-terms. In the bottom 
10% of the extracted terms, 54.8% of them are 
the valid terms and 45.2% of them are non-terms. 
This means that the relevant terms are much 
more than non-terms in the top 10% of the result, 
while non-terms are much more than the 
relevant terms in the bottom 10% of the result.  
 
The results are summarized as follow:  
 
!"According as a term has a high 
Dictionary Weight (WDic), it is apt 
to be valid. 
!"More valid terms have a high 
Dictionary Weight (WDic) than 
non-terms do 
 
5.3 Overall Performance 
Table 5 and figure 3 show the performance of 
the proposed method and of the C-value method. 
By dividing the ranked lists into 10 equal 
sections, the results are compared. Each section 
contains the 1291 terms and is evaluated 
independently. 
 
C-value The proposed 
method 
Section # of 
term 
Precision # of 
term 
Precision 
1 1181 91.48% 1241 96.13% 
2 1159 89.78% 1237 95.82% 
3 1207 93.49% 1213 93.96% 
4 1192 92.33% 1174 90.94% 
5 1206 93.42% 1154 89.39% 
6 981 75.99% 1114 86.29% 
7 934 72.35% 1044 80.87% 
8 895 69.33% 896 69.40% 
9 896 69.40% 780 60.42% 
10 578 44.77% 379 29.36% 
Table 5.  Precision rates of C-value and the 
proposed method : Section contain 1291 terms and 
precision is evaluated independently. For example, 
in section 1, since there are 1291 candidate terms 
and 1241 relevant terms by the proposed method, 
the precision rate in section 1 is 96.13% . 
The result can be interpreted as follows. In the 
top sections, the proposed method shows the 
higher precision rate than the C-value does. The 
distribution of valid terms is also better for the 
proposed method, since there is a downward 
tendency from section 1 to section 10. This 
implies that the terms with higher weight scored 
by our method have a higher probability to be 
valid terms. Moreover, the precision rate of our 
method shows the rapid decrease from section 6 
to section 10. This indicates that most of valid 
terms are located in the top sections. 
20%
30%
40%
50%
60%
70%
80%
90%
100%
1 2 3 4 5 6 7 8 9 10
Section
Pre
cis
ion
The Proposed method C-value
Figure 2. The performance of C-value and the 
proposed method in each section 
The results can be summarized as follow : 
 
!"The proposed method extracts a valid 
term more accurate than C-value does. 
!"Most of the valid terms are in the top 
section extracted by the proposed 
method. 
Conclusion 
In this paper, we have described a method for 
term extraction using a dictionary hierarchy. It is 
constructed by clustering method and is used for 
estimating the relationships between domains. 
Evaluation shows improvement over the C-value. 
Especially, our approach can distinguish the 
valid terms efficiently ? there are more valid 
terms in the top sections and less valid terms in 
the bottom sections. Although the method 
targets Korean, it can be applicable to English 
by slight change on the Tweight (WTrl).  
However, there are many scopes for further 
extensions of this research. The problems of 
non-nominal terms (Klavans and Kan, 1998), 
term variation (Jacquemin et al, 1997), and  
relevant contexts (Maynard and Ananiadou, 
1998), can be considered for improving the 
performance. Moreover, it is necessary to apply 
our method to practical NLP systems, such as an 
information retrieval system and a 
morphological analyser. 
Acknowledgements 
KORTERM is sponsored by the Ministry of Culture 
and Tourism under the program of King Sejong 
Project. Many fundamental researches are supported 
by the fund of Ministry of Science and Technology 
under a project of plan STEP2000. And this work 
was partially supported by the KOSEF through the 
?Multilingual Information Retrieval? project at the 
AITrc. 
References  
Anderberg, M.R. (1973) Cluster Analysis for 
Applications. New York: Academic 
Bourigault, D. (1992) Surface grammatical analysis 
for the extraction of terminological noun phrases. 
In Proceedings of the 14th International Conference 
on Computational Linguistics, COLING?92 pp. 
977-981. 
Dagan, I. and K. Church. (1994) Termight: 
Identifying and terminology In Proceedings of the 
4th Conference on Applied Natural Language 
Processing, Stuttgart/Germany, 1994. Association 
for Computational Linguistics. 
ETRI (1997) Etri-Kemong set 
Felber Helmut (1984) Terminology Manual, 
International Information Centre for Terminology 
(Infoterm) 
Frantzi, K.T. and S.Ananiadou (1999) The 
C-value/NC-value domain independent method for 
multi-word term extraction. Journal of Natural 
Language Processing, 6(3) pp. 145-180 
Hisamitsu, Toru and Yoshiki Niwa (1998) Extraction 
of useful terms from parenthetical expressions by 
using simple rules and statistical measures. In First 
Workshop on Computational Terminology 
Computerm?98, pp 36-42 
Jacquemin, C., Judith L.K. and Evelyne, T. (1997) 
Expansion of Muti-word Terms for indexing and 
Retrieval Using Morphology and Syntax, 35th 
Annual Meeting of the Association for 
Computational Linguistics, pp 24-30 
Justeson, J.S. and S.M. Katz (1995) Technical 
terminology : some linguistic properties and an 
algorithm for identification in text. Natural 
Language Engineering, 1(1) pp. 9-27  
Klavans, J. and Kan M.Y (1998) Role of Verbs in 
Document Analysis, In Proceedings of the 17th 
International Conference on Computational 
Linguistics, COLING?98 pp. 680-686. 
Lauriston, A. (1996) Automatic Term Recognition : 
performance of Linguistic and Statistical 
Techniques. Ph.D. thesis, University of Manchester 
Institute of Science and Technology. 
Lorr, M. (1983) Cluster Analysis and Its Application, 
Advances in Information System Science,8 , 
pp.169-192 
Murtagh, F. (1983) A Survey of Recent Advances in 
Hierarchical Clustering Algorithms, Computer 
Journal, 26, 354-359 
Maynard, D. and Ananiadou, S. (1998) Acquiring 
Context Information for Term Disambiguation In 
First Workshop on Computational Terminology 
Computerm?98, pp 86-90 
Oh, J.H. and K.S. Choi (1999) Automatic extraction 
of a transliterated foreign word using hidden 
markov model , In Proceedings of the 11th Korean 
and Processing of Korean Conference pp. 137-141 
(In Korean). 
Park, Y.C., K.S. Choi, J.K.Kim and Y.H. Kim (1996). 
Development of the KT test collection for 
researchers in information retrieval. In the 23th 
KISS Spring Conference (in Korean) 
  
 
	
ffAutomatic clustering of collocation 
 for detecting practical sense boundary 
Saim Shin 
KAIST 
KorTerm 
BOLA 
miror@world.kaist.ac.kr 
Key-Sun Choi 
KAIST 
KorTerm 
BOLA 
kschoi@world.kaist.ac.kr 
 
Abstract 
This paper talks about the deciding practical 
sense boundary of homonymous words. The 
important problem in dictionaries or thesauri 
is the confusion of the sense boundary by each 
resource. This also becomes a bottleneck in 
the practical language processing systems. 
This paper proposes the method about 
discovering sense boundary using the 
collocation from the large corpora and the 
clustering methods. In the experiments, the 
proposed methods show the similar results 
with the sense boundary from a corpus-based 
dictionary and sense-tagged corpus. 
1 Introduction 
There are three types of sense boundary 
confusion for the homonyms in the existing 
dictionaries. One is sense boundaries? overlapping: 
two senses are overlapped from some semantic 
features. Second, some senses in the dictionary are 
null (or non-existing) in the used corpora. 
Conversely, we have to generate more senses 
depending on the corpora, and we define these 
senses with practical senses. Our goal in this study 
is to revise sense boundary in the existing 
dictionaries with practical senses from the large-
scaled corpus. 
  The collocation from the large-scaled corpus 
contains semantic information. The collocation for 
ambiguous words also contains semantic 
information about multiple senses for this 
ambiguous word. This paper uses the ambiguity of 
collocation for the homonyms. With the clustering 
algorithms, we extract practical sense boundary 
from the collocations. 
This paper explains the collocation ambiguity in 
chapter 2, defines the extracted collocation and 
proposes the used clustering methods and the 
labeling algorithms in chapter 3. After explaining 
the experimental results in chapter 4, this paper 
comes to the conclusion in chapter 5. 
2 Collocation and Senses 
2.1 Impractical senses in dictionary 
In (Patrick and Lin, 2002), senses in dictionary ? 
especially in WordNet ? sometimes don?t contain 
the senses appearing in the corpus. Some senses in 
the manual dictionary don?t appear in the corpus. 
This situation means that there exist differences 
between the senses in the manual dictionaries and 
practical senses from corpus. These differences 
make problems in developing word sense 
disambiguation systems and applying semantic 
information to language processing applications.  
The senses in the corpus are continuously 
changed. In order to reflect these changes, we must 
analyze corpus continuously. This paper discusses 
about the analyzing method in order to detect 
practical senses using the collocation. 
2.2 Homonymous collocation  
The words in the collocation also have their 
collocation. A target word for collocation is called 
the ?central word?, and a word in a collocation is 
referred to as the ?contextual word?. ?Surrounding 
words? mean the collocation for all contextual 
words. The assumption for extracting sense 
boundary is like this: the contextual words used in 
the same sense of the central word show the 
similar pattern of context. If collocation patterns 
between contextual words are similar, it means that 
the contextual words are used in a similar context - 
where used and interrelated in same sense of the 
central word - in the sentence. If contextual words 
are clustered according to the similarity in 
collocations, contextual words for homonymous 
central words can be classified according to the 
senses of the central words. (Shin and Choi, 2004) 
The following is a mathematical representation 
used in this paper. A collocation of the central 
word x, window size w and corpus c is expressed 
with function f: V N C ? 2PC/V. In this formula, V 
means a set of vocabulary, N is the size of the 
contextual window that is an integer, and C means 
a set of corpus. In this paper, vocabulary refers to 
all content words in the corpus. Function f shows 
all collocations. C/V means that C is limited to V as 
well as that all vocabularies are selected from a 
given corpus and 2PC/VP is all sets of C/V. In the 
equation (1), the frequency of x is m in c. We can 
also express m=|c/x|. The window size of a 
collocation is 2w+1. 
}),,{()( xIiixxg ?=  is a word sense assignment 
function that gives the word senses numbered i of 
the word x. Ix is the word sense indexing function 
of x that  gives an  index to each sense of the word 
x. All contextual words xi?j of a central word x have 
their own contextual words in their collocation, 
and they also have multiple senses. This problem is 
expressed by the combination of g and f as follows: 
??
??
?
??
??
?
=
++??
++??
)(),...,(),,(),(),...,(
..........
)(),...,(),1,(),(),...,(
)),,((
11
11
1111
w
hhxh
w
h
w
hhh
w
h
d
mmmm
i
xgxgIxxgxg
xgxgxxgxg
cwxfgh o
 
(1) 
 
In this paper, the problem is that the collocation 
of the central word is ordered according to word 
senses. Figure 1 show the overall process for this 
purpose. 
 
Figure 1 Processing for detecting sense 
boundary 
3 Automatic clustering of collocation 
For extracting practical senses, the contextual 
words for a central word are clustered by analyzing 
the pattern of the surrounding words. With this 
method, we can get the collocation without sense 
ambiguity, and also discover the practical sense 
boundary. 
In order to extract the correct sense boundary 
from the clustering phase, it needs to remove the 
noise and trivial collocation. We call this process 
normalization, and it is specifically provided as [8]. 
The statistically unrelated words can be said that 
the words with high frequency appear regardless of 
their semantic features. After deciding the 
statistically unrelated words by calculating tf?idf 
values, we filtered them from the original 
surrounding words. The second normalization is 
using LSI (Latent Semantic Indexing). Throughout 
the LSI transformation, we can remove the 
dimension of the context vector and express the 
hidden features into the surface of the context 
vector. 
3.1 Discovering sense boundary 
We discovered the senses of the homonyms with 
clustering the normalized collocation. The 
clustering classifies the contextual words having 
similar context ? the contextual words having 
similar pattern of surrounding words - into same 
cluster. Extracted clusters throughout the clustering 
symbolize the senses for the central words and 
their collocation. In order to extract clusters, we 
used several clustering algorithms. Followings are 
the used clustering methods: 
z K-means clustering (K) (Ray and Turi, 1999) 
z Buckshot (B) (Jensen, Beitzel, Pilotto, 
Goharian and Frieder, 2002) 
z Committee based clustering (CBC) (Patrick 
and Lin, 2002) 
z Markov clustering (M1, M2) 1  (Stijn van 
Dongen, 2000) 
z Fuzzy clustering (F1, F2)2 (Song, Cao and 
Bruza, 2003) 
Used clustering methods cover both the 
popularity and the variety of the algorithms ? soft 
and hard clustering and graph clustering etc. In all 
clustering methods, used similarity measure is the 
cosine similarity between two sense vectors for 
each contextual word. 
We extracted clusters with these clustering 
methods, tried to compare their discovered senses 
and the manually distributed senses. 
3.2 Deciding final sense boundary 
After clustering the normalized collocation, we 
combined all clustering results and decided the 
optimal sense boundary for a central word. 
},...,,{
},...,,...,{
)),((
},...,{)),,((
10
0
1 1
xmxxx
ni
i
x
md
x
dxdd
xssS
dddD
dxnumm
hhScwxfgh
iii
=
=
=
==o
 
(2) 
 
In equation (2), we define equation (1) as Sxdi, 
this means extracted sense boundary for a central 
word x with di. The elements of D are the applied 
clustering methods, and Sx is the final combination 
results of all clustering methods for x. 
                                                     
1 M1and M2 have different translating methods between context and graph. 
2 F1and F2 are different methods deciding initial centers. 
This paper proposes the voting of applied 
clustering methods when decides final sense 
boundary like equation (3). 
xi
Dd
SdwnumxNum
i
==
?
)},({)( max  (3)
We determined the number of the final sense 
boundary for each central word with the number of 
clusters that the most clustering algorithms were 
extracted. 
After deciding the final number of senses, we 
mapped clusters between clustering methods. By 
comparing the agreement, the pairs of the 
maximum agreement are looked upon the same 
clusters expressing the same sense, and agreement 
is calculated like equation (4), which is the 
agreement between k-th cluster with i-th clustering 
method and l-th cluster with j-th clustering method 
for central word x. 
}{}{
}{}{
x
ldj
x
kd
x
ldj
x
kd
hh
hh
agreement
i
i
U
I=
(4) 
))},,(({max),( cwxfghwSVot
i
k
i
d
Vx Dd
x o?
? ?
= (5) 
)1,...,1,1(
21 ???= nx a
n
a
n
a
n
S wN
w
N
w
N
zr (6) 
The final step is the assigning elements into the 
final clusters. In equation (5), all contextual words 
w are classified into the maximum results of 
clustering methods. New centers of each cluster are 
recalculated with the equation (6) based on the 
final clusters and their elements. 
Figure 2 represents the clustering result for the 
central word ?chair?. The pink box shows the 
central word ?chair? and the white boxes show the 
selected contextual words. The white and blue area 
means the each clusters separated by the clustering 
methods. The central word ?chair? finally makes 
two clusters. The one located in blue area contains 
the collocation for the sense about ?the position of 
professor?. Another cluster in the white area is the 
cluster for the sense about ?furniture?. The words 
in each cluster are the representative contextual 
words which similarity is included in ranking 10. 
4 Experimental results  
We extracted sense clusters with the proposed 
methods from the large-scaled corpus, and 
compared the results with the sense distribution of 
the existing thesaurus. Applied corpus for the 
experiments for English and Korean is Penn tree 
bank3 corpus and KAIST4 corpus.  
                                                     
3 http://www.cis.upenn.edu/~treebank/home.html 
4 http://kibs.kaist.ac.kr 
 
Figure 2  The clustering example for 'chair' 
For evaluation, we try to compare clustering 
results and sense distribution of dictionary. In case 
of English, used dictionary is WordNet 1.75 - Fine-
grained (WF) and coarse-grained distribution 
(WC). The coarse-grained senses in WordNet are 
adjusted sense based on corpus for SENSEVAL 
task. In order to evaluate the practical word sense 
disambiguation systems, the senses in the WordNet 
1.7 are adjusted by the analyzing the appearing 
senses from the Semcor. For the evaluation of 
Korean we used Korean Unabridged Dictionary 
(KD) for fine-grained senses and Yonsei 
Dictionary (YD) for corpus-based senses. 
Table 1 shows the clustering results by each 
clustering algorithms. The used central words are 
786 target homonyms for the English lexical 
samples in SENSEVAL26. The numbers in Table 1 
shows the average number of clusters with each 
clustering method shown chapter 3 by the part of 
speech. WC and WF are the average number of 
senses by the part of speech. 
In Table 1 and 2, the most clustering methods 
show the similar results. But, CBC extracts more 
clusters comparing other clustering methods. 
Except CBC other methods extract similar sense 
distribution with the Coarse-grained WordNet 
(WC). 
 
 Nouns Adjectives Verbs All 
K 3 3.046 3.039 3.027 
B 3.258 3.218 3.286 3.266 
CBC 6.998 3.228 5.008 5.052 
F1 3.917 2.294 3.645 3.515 
F2 4.038 5.046 3.656 4.013 
Final 3.141 3.08 3.114 3.13 
WC 3.261 2.887 3.366 3.252 
WF 8.935 8.603 9.422 9.129 
Table 1  The results of English 
                                                     
5 http://www.cogsci.princeton.edu/~wn/ 
6 http://www.cs.unt.edu/~rada/senseval/ 
 K B C F1 F2 M1 
Nouns 2.917 2.917 5.5 2.833 2.583 4.083 
 KD YD M2
Nouns 11.25 3.333 3.833
Table 2  The results of Korean 
Table 3 is the evaluating the correctness of the 
elements of cluster. Using the sense-tagged 
collocation from English test suit in SENSEVAL27, 
we calculated the average agreement for all central 
words by each clustering algorithms. 
K B C F1 F2 
98.666 98.578 90.91 97.316 88.333
Table 3 The average agreement by clustering 
methods 
As shown in Table 3, overall clustering methods 
record high agreement. Among the various 
clustering algorithms, the results of K-means and 
buckshot are higher than other algorithms. In the 
K-means and fuzzy clustering, the deciding 
random initial shows higher agreements. But, 
clustering time in hierarchical deciding is faster 
than random deciding 
5 Conclusion 
This paper proposes the method for boundary 
discovery of homonymous senses. In order to 
extract practical senses from corpus, we use the 
collocation from the large corpora and the 
clustering methods.  
In these experiments, the results of the proposed 
methods are different from the fine-grained sense 
distribution - manually analyzed by the experts. 
But the results are similar to the coarse-grained 
results ? corpus-based sense distribution. Therefore, 
these experimental results prove that we can 
extract practical sense distribution using the 
proposed methods. 
For the conclusion, the proposed methods show 
the similar results with the corpus-based sense 
boundary. 
For the future works, using this result, it?ll be 
possible to combine these results with the practical 
thesaurus automatically. The proposed method can 
apply in the evaluation and tuning process for 
existing senses. So, if overall research is 
successfully processed, we can get a automatic 
mechanism about adjusting and constructing 
knowledge base like thesaurus which is practical 
and containing enough knowledge from corpus. 
There are some related works about this research. 
Wortchartz is the collocation dictionary with the 
assumption that Collocation of a word expresses 
                                                     
7 English lexical sample for the same central words 
the meaning of the word (Heyer, Quasthoff and 
Wolff, 2001). (Patrick and Lin, 2002) tried to 
discover senses from the large-scaled corpus with 
CBC (Committee Based Clustering) algorithm.. In 
this paper, used context features are limited only 
1,000 nouns by their frequency. (Hyungsuk, Ploux 
and Wehrli, 2003) tried to extract sense differences 
using clustering in the multi-lingual collocation. 
6 Acknowledgements 
This work has been supported by Ministry of 
Science and Technology in Korea. The result of 
this work is enhanced and distributed through 
Bank of Language Resources supported by grant 
No. R21-2003-000-10042-0 from Korea Science & 
Technology Foundation. 
References  
Ray S. and Turi R.H. 1999. Determination of 
Number of Clusters in K-means Clustering and 
Application in Colour Image Segmentation, In 
?The 4th International Conference on Advances 
in Pattern Recognition and Digital Techniques?, 
Calcuta. 
Heyer G., Quasthoff U. and Wolff C. 2001. 
Information Extraction from Text Corpora, In 
?IEEE Intelligent Systems and Their 
Applications?, Volume 16, No. 2. 
Patrick Pantel and Dekang Lin. 2002. Discovering 
Word Senses from Text, In ?ACM Conference 
on Knowledge Discovery and Data Mining?, 
pages  613?619, Edmonton. 
Hyungsuk Ji, Sabine Ploux and Eric Wehrli. 2003, 
Lexical Knowledge Representation with 
Contexonyms, In ?The 9th Machine Translation?, 
pages 194-201, New Orleans 
Eric C.Jensen, Steven M.Beitzel, Angelo J.Pilotto, 
Nazli Goharian, Ophir Frieder. 2002, 
Parallelizing the Buckshot Algorithm for 
Efficient Document Clustering, In ?The 2002 
ACM International Conference on Information 
and Knowledge Management, pages 04-09, 
McLean, Virginia, USA. 
Stijn van Dongen. 2000, A cluster algorithm for 
graphs, In ?Technical Report INS-R0010?, 
National Research Institute for Mathematics and 
Computer Science in the Netherlands. 
Song D., Cao G., and Bruza P.D. 2003, Fuzzy K-
means Clustering in Information Retrieval, In 
?DSTC Technical Report?. 
Saim Shin and Key-Sun Choi. 2004, Automatic 
Word Sense Clustering using Collocation for 
Sense Adaptation, In ?Global WordNet 
conference?, pages 320-325, Brno, Czech. 
Question-Answering Based on Virtually Integrated Lexical  
Knowledge Base 
Key-Sun Choi 
KAIST,Korterm 
Daejeon  
305-701 Korea 
kschoi@cs.ka
ist.ac.kr 
Jae-Ho Kim 
KAIST,Korterm
Daejeon 
305-701 Korea
jjaeh@world.
kaist.ac.kr
Masaru 
Miyazaki 
NHK STRL 
Tokyo 157-8510
Japan 
miyazaki.m-
fk@nhk.or.jp
Jun Goto 
NHK STRL 
Human Science 
Tokyo 157-8510 
Japan 
 goto.j-
fw@nhk.or.jp 
Yeun-Bae Kim
NHK STRL 
Human Science
Tokyo 157-8510
Japan 
 kimu.y-
go@nhk.or.jp
 
Abstract 
This paper proposes an algorithm for cau-
sality inference based on a set of lexical 
knowledge bases that contain information 
about such items as event role, is-a hier-
archy, relevant relation, antonymy, and 
other features. These lexical knowledge 
bases have mainly made use of lexical 
features and symbols in HowNet. Several 
types of questions are experimented to 
test the effectiveness of the algorithm here 
proposed. Particularly in this paper, the 
question form of ?why? is dealt with to 
show how causality inference works. 
1 Introduction 
A virtually linked knowledge base is designed to 
utilize a pre-constructed knowledge base in a dy-
namic mode when it is in actual use. 
An open-domain question answering architec-
ture must consist of various components and 
processes (Pas?a, 2001) that include WordNet-
like resources, part of speech tagging, parsing, 
named entity recognition, question processing, 
passage retrieval, answer extraction, and answer 
justification. Consider a question like the follow-
ing: ?Why do doctors cure patients?? 
The answer may be obtained by commonsense 
knowledge as follows: 
1. A patient suffered from a 
disease. 
2. A doctor cures the disease. 
3. The doctor cures at hospi-
tal. 
4. Doctor is an occupation. 
5. So the doctor cures the 
patient. 
These sentences are transformed into proposi-
tional forms, as illustrated below: 
6. sufferFrom(patient,disease) 
7. cure(doctor,disease) 
8. cure(doctor,at-hospital) 
9. occupation(doctor) 
10. cure(doctor,patient) 
Linguistic knowledge bases like WordNet 
(Miller, 1995), EDR dictionary (Yokoi, 1995) and 
HowNet (Dong, 1999) have been used to interpret 
these sentences. 
Moldovan et al (2002) generated lexical chains 
from WordNet in order to trace these topically re-
lated paths and thereby to search for causal expla-
nations. A conceptual word Cj inside of a gloss 
under a synset Ci is linked to the synset Cj.  
HowNet (Dong et al 1999) is a linguistic 
knowledge base that is designed to have the defini-
tion of words and concepts as well as event role 
and role-filling entities. Commonsense knowledge 
like naive physics is also built up through event 
role relation like the relation of sufferFrom requir-
ing cure. 
HowNet is modularized into separate knowl-
edge spaces for entity hierarchy, event hierarchy, 
antonymy, syntax, attributes, etc. Relations be-
tween various concepts (e.g., part-of, relevance, 
location) are defined implicitly in the definition of 
each concept. 
This paper will focus on building an algorithm 
that allows for searching for some topical paths in 
order to find causal explanations for questions like 
?Why do doctors cure patients?? or ?Why do pa-
tients pay money?? as illustrated in Figure 1. 
patient doctor occupation money
$cure *cure earn $earn
#occupation
converse
agent=patient
possession=money
target=?
agent=?
possession=money
source=patient
entity
syn
event
*pay $pay
give take
(1)
(2) (3)
(4)
(5)
(6)(7)
(8)
(9)  
Figure 1: A Snapshot of a virtually integrated 
knowledge base for the question: ?Why do patients 
pay money to doctors?? 
 
In the following sections, issues on the virtual in-
tegration of knowledge bases, their algorithms and 
experimentations are presented.  
2 Underlined Knowledge Bases and Vir-
tual Integration 
In Figure 1, each marked numbering has the fol-
lowing meaning: 
(1) Entity hierarchy: entity is the top node in 
the hierarchy of entities. 
(2) entity is the hypernym of patient, doctor, 
occupation, and money in the line (3). 
(3) Concepts or word entries are listed in this 
line. All concepts and word entries repre-
sent their definition by a list of concepts 
and marked pointers. 
(4) A concept (or word) in (3) features defini-
tional relations to a list of concepts. For 
example, a doctor definition is composed 
of two concepts and their marking point-
ers: #occupation and *cure. Pointers in 
HowNet represent relations between two 
concepts or word entries, e.g., ?#? means 
?relevant? and ?*? does ?agent?. 
(5) syn refers to the syntactic relation in the 
question ?Why do patients pay money to 
doctors?? 
(6) converse refers to the converse relation be-
tween events, e.g., give and take. 
(7) Event hierarchy: For example, the hy-
pernym for pay is give and the hypernym 
of give is event. 
(8) Event role: Now, event roles are partially 
filled with entities, e.g., patient and 
money. 
(9) Event role shift: The agent of give is 
equalized to the source of take. 
An overview of each component of the knowl-
edge base is in Figure 2, where three word entries 
why, patient, and money are in the dictionary. 
The four concept facets of entity, role, event, and 
converse are described in this example, mainly as 
part of linguistic knowledge. 
 
pay
give take
agent=
possession=
target=
agent=
possession=
source=
Alter-possession
patient
doctor occupation money
cure
*cure earn $earn
#occupation
entity
give take
converse
event
earn
human
pay
why
role
question
cause
dictionary Conceptfacets
 
Figure 2: HowNet Architecture in Example. 
 
Some issues on ontology integration have been 
discussed from various points of view. Pinto et al 
(1999) classified the notions of ontology integra-
tion into three types: integration, merging and 
use/application. The term virtually integrated 
means the view of ontology-based use/application. 
This paper presents issues on and arguments for 
linguistic knowledge base and commonsense 
knowledge in (Lenat, Miller and Yokoi, 1995). 
One of the arguments was whether linguistic 
knowledge could be separated from commonsense 
knowledge, but it was agreed that both types of 
knowledge were essentially required for natural 
language processing. 
This paper was motivated by the desire to make 
inferences using a lexical knowledge base, thus 
successfully carrying out a kind of commonsense 
reasoning. 
3 Interpretation of Lexical Knowledge 
Consider the following three sentences: 
11. Doctors cure patients. 
12. Doctors earn money. 
13. Patients pay money. 
One major concern is finding connectability 
among words and concepts. As shown in Figure 2, 
the following facts are derived: 
14. Doctor is relevant to oc-
cupation. 
15. Occupation allows you to 
earn money. 
Because there exists a converse relation be-
tween give and take, their hyponyms earn and pay 
also fall under converse relation. It is something 
like the following social commonsense as shown in 
Figure 2: ?If someone X pays money to the other Y, 
Y earns money from X.? 
We humans now understand the reason for 
?why patients pay money.? The answer is that 
?doctors cure patients as their occupation allowing 
them to earn money.? 
The following is a valid syllogism where Y is 
being instantiated to doctor: 
 
If ?X pays money to Y? is 
equivalent to ?Y earns money 
from X?1, and ?a doctor earns 
money from X?, then ?X pays 
money to the doctor?. 
 
Consider the next syllogism: If ?a doctor 
cures X? and ?doctor is an occupa-
tion? and Axiom 1, then ?the doc-
tor earns money from X?. 
Axiom 1 is needed to make such a syllogism 
that ?If Y cures X and Y is an occu-
pation, then Y earns money from 
X.? Then our challenge is to find out this Axiom 
1 from the lexical knowledge bases. It is a com-
monsense and thus there is a gap in the lexical 
knowledge base. 
The following is a list of questions derived 
from the three sentences of 11, 12 and 13 which 
are designed to discover such axioms (or rules) 
from a set of lexical knowledge bases: ?Why do 
                                                           
1 It is a converse relation. 
doctors cure patient??, ?Why do doctors earn 
money??, and ?Why do patients pay money to doc-
tors?? 
4 Connectability: Similarity Measure  
Consider the query ?Why do doctors cure pa-
tients?? Tracing Figure 2 back through Figure 1 
leads to obtaining logical forms from 6 through 10. 
The best connectable path is planned from the first 
word of the question. 
For each pair of words, the function called 
"similar(*,*)" will be estimated to choose the next 
best tracing concepts (or words).  similar's mis-
sions are summarized as (1) checking the connect-
ability between two nodes2, (2) selecting the best 
sense of the node,3 (3) selecting the best tracing 
candidate node in the next step. Finally, following 
the guidance by similar allows us to explain the 
question. 
4.1 Observation and Evidence of Topical Re-
latedness 
Let's try to follow the steps 6-10 given in the logi-
cal forms. In the question ?Why do doctors cure 
patient?? that focuses on three words doctor, cure, 
and patient, we can trace some key words given in 
example sentences as follows: patient ~ disease ~ 
cure ~ doctor ~ occupation ~ earn ~ pay ~ pa-
tient. 
What kind of lexical relations are relevant to 
each pair of words or concepts? Their observation 
can be summarized as follows: 
A) The relation between patient ~ disease is a 
role relation of ?sufferFrom(patient, dis-
ease)?. 
B) A sequence of cure ~ doctor ~ occupation 
~ earn lets us infer the relation among 
cure ~ earn, which are closely linked by 
their relevance relation to occupation. 
Furthermore, earn and cure shares a 
common subject of these two events. 
C) The sequence of earn ~ pay is the result of 
a converse event relation between earn 
and pay. 
D) pay ~ patient: The agent of pay is a ge-
neric human. In other words, pay is a hy-
                                                           
2 A node means either concept or word. 
3 It is similar with word sense disambiguation. 
ponym for the act of human, one of whose 
hyponym is patient. 
Consider again the match between the tracing 
sequences of concepts and the knowledge base. 
Going into more details, notations with footnotes 
will be given to each example. At this point, we 
will give names and formalization based on the 
observed characteristics. 
A) Feature comparison: To find the role re-
lation among patient ~ disease, search the 
definition of entities (referring to patient 
and disease) in ways that two entities share 
the same event concept (referring to 
cure):4 
patient ? human?$cure ?*sufferFrom. 
disease ?  medical?$cure ? undesired. 
B) Interrelation: To find the event interrela-
tion among cure ~ earn, two possible 
paths are presented as follows. 
? First, inverse interrelation: Two event's 
role entities can be found by searching all of 
entities using *earn ~ *cure that share the 
same subject, and using *earn ~ $cure 
where the subject of earn is the object of 
cure. 
? Second, sister interrelation: The following 
logical form can be derived from Figure 2:5 
doctor ? *cure ? #occupation. 
occupation ? earn. 
Because cure and occupation is in the defi-
nition of doctor, a probable logical implica-
tion can be derived as follows:6 
*cure ? ~#occupation. 
C) Converse/antonymy: earn and pay have 
their respective hypernyms take and give. 
There exists a converse relation between 
these two hypernyms. 
                                                           
4 According to HowNet convention, ?$? represents patient, 
target, possession, or content of an event, and ?*? represents 
agent, experiencer, or instrument. ??? means implies or has 
features. 
5 ?#? means ?relevant?. 
6 ?~? means ?very probable?. 
D) Inheritance: The relation among pay ~ 
patient is represented as follows:7 
humanpatient
acthuman
actpay
p
p
*?  
4.2 Rationale of Connectability 
In the former section, we summarized four charac-
teristics8 of causality (relatedness)-based path find-
ing: feature comparison, interrelation, 
converse/antonymy in their hypernym?s level, and 
inheritance. Among search spaces available, it is 
necessary to find out a measure of guiding the op-
timal9 path tracing. 
We will call such a measure similar which will 
be defined according to the four characteristics just 
mentioned. Further details about the calculation 
formula will be presented again later. 
A) For ?feature comparison?, the measure fea-
ture similar(X,Y) defines the notion of 
similarity between the features in X and Y.  
B) There are two interrelations in the last sec-
tion. 
? For ?inverse interrelation?', inverse simi-
lar(X,Y) calculates how much similarity ex-
ists between X? and Y? in a manner that X? 
= {Z | Z ? ?X}, where ?X is an abstraction 
of role-marked concepts like *X, $X, #X, 
etc. Thus inverse similar(X,Y) = simi-
lar(X?,Y?). 
? For ?sister interrelation?, the measure sister 
similar(X,Y) means the following two situa-
tions: First, X and Y are features to define 
one concept (say, W). Second, one of them, 
say, Y's definitional feature concepts (refer-
ring to Z) are similar with X such that X and 
Z are similar if W ? X ?Y and Y ? Z. 
C) Converse or antonymy: The converse re-
lation converse(X,Y) can be found by the 
measure feature similar. converse(X,Y) is 
formulated by X ? ?Y and Y ? ?X where 
? = converse. 
                                                           
7 ? YX p ? means ?Y is hypernym of X?.  
8 Their exhaustiveness should be discussed later. 
9 ?optimal? will not be discussed. 
D) Using inheritance property in the concept 
hierarchy, relations between hypernym of 
concepts X and Y are inherited to X and Y 
in a way that X and Y is similar if there 
exist X? and Z such that 'XX p , Z ? ?X?, 
and ZY p  where ? is a pointer or null. 
This inheritance tracing can be determined 
by how much similar X and Y are in terms 
of their path upward based on the relation 
of hypernym. We will define path similar. 
But tracing the path upward following hy-
pernym links is to be described later ac-
cording to the algorithm. 
A measure called similar will be defined based 
on the discussion in this section. Then an algorithm 
is introduced through this measure with an exam-
ple. 
5 Measures 
In the last section, we discussed four kinds of the 
measure similar. 
? path similar, 
? feature similar, 
? inverse similar, 
? sister similar. 
For feature, inverse, and sister similar func-
tions, path similar is used as a basis of calculation. 
They are different with respect to both their search 
method and the depth of expanding features. fea-
ture similar finds similar features by using path 
similar. inverse similar(X,Y) searches for entries 
that contain X and Y as features and then use the 
path similar. In the same way, sister similar finds 
sister concepts, expands them, and finally meas-
ures using the path similar. 
Since path similar plays a key role in all these 
search and measure processes, its role will be ex-
plained in the next subsection. Other measures are 
only dealt with as part of the algorithm. 
5.1 Similarity Based on Hierarchy and Fea-
ture 
The mission of the measuring function simi-
lar(X,Y) is to calculate their relevancy between 
two concepts or words whether they are of type 
entity, event, or of some other type. 
If X and Y belong to different types of knowl-
edge plane (e.g., entity and event), it is hard to 
compare their hypernym path upward to the top 
concept. However, if different types of concepts 
have any relevance to (connect) causality, we will 
use feature similar or inverse similar after find-
ing the same type of concepts to calculate the path 
similar. Now we will explain the above by using 
two pairs of concept type: entity-entity and entity-
event, without loss of generality. 
First, pathsimilar(entity X, entity Y) is de-
fined as follows: 
)()(
)()(2
),(
YpathXpath
YpathXpath
YXrpathsimila
++
++
+
??=  
where path+(X) is the ordered list of hypernym for 
X by descending order from the top concept. For 
example, 
path+(doctor)  
= [entity...animate...human.doctor] 
path+(patient)  
= [entity...animate...human.patient] 
Because |path+(X)| counts the number of nodes on 
the path, pathsimilar(doctor,patient) = 2? 
6/(7+7)=0.857. 
Second, pathsimilar(entity N, event V) is de-
fined as follows: 
pathsimilar(N,V)  
= Max pathsimilar(N.feature,V) 
where N.feature means the feature list in the defi-
nition of N. The following is an illustrative exam-
ple for the definition: 
money ? $earn,*buy,#sell, $setAside, 
it is equivalent to the following:  
money.feature=[$earn,*buy,#sell,$setAside]. 
So pathsimilar(money,earn)=pathsimilar(earn,earn) 
=1. According to this Max function, the selection 
priorities for the path can be specified. 
Third, pathsimilar(event V, entity N) is de-
fined by inverse similar as follows: pathsimi-
lar(V,N) = Max pathsimilar(V.inverse, N).  For 
example, pathsimilar(cure, doctor) = Max path-
similar(cure.inverse, doctor) = Max pathsimi-
lar({doctor, medical worker, medicine, patient}, 
doctor). 
Fourth, pathsimilar(event X, event Y) shares 
the same formula with pathsimilar(entity X, en-
tity Y) shown before. But, we can give another 
inverse pathsimilar(event X, event Y) = Max 
pathsimilar(X.inverse, Y.inverse). 
5.2 Logical Implication and Expansion Depth 
All of the relations in Figure 2 are translated into 
logical form (see below). As shown in ?Interpreta-
tion as Abduction? (Hobbs et al 1988), ?abductive 
inference is inference to the best explanation?. 
These relations showed ?the interpretation of a text 
is the minimal explanation of why the text would 
be true? based on the abductive inference. By the 
same token, ?the interpretation of a question is the 
minimal explanation of why the question would be 
true? based on a set of lexical knowledge bases. 
Before proceeding to our algorithm, an example 
will be applied to abductive inference briefly as a 
set of logical forms as well as a diagram in Figure 
3. 
16. doctor ? human, #occupation, 
*cure, medical. 
17. medicine ? *cure. 
18. disease ? $cure. 
19. cure ? medical, 
{agent,patient,content}. 
20. medical ? #cure. 
21. converse(pay,earn) ? 
agent=source,  
target=agent. 
22. patient ? human,$cure. 
23. occupation ? affairs, earn. 
24. cause(cure,sufferFrom) ? 
patient=experiencer, 
content=content. 
25. possibleConsequence(cure, 
beRecovered) ?  
patient=experiencer,  
content=stateIni. 
 
While pursuing the path tracing enabling mini-
mal explanation, now we are going to propose       
a connectability measure similar such as 
?weighted abduction? (Hobbs et al 1988). As 
?likelihood estimation? is useful to consider a 
?bounded conditioning? (Russell & Norvig, 1995) 
in a belief network, the ?expansion depth? of simi-
lar will be useful for the explanation path tracing 
for the purpose of the minimal explanation of the 
question. 
commercial
$earn
*buy
#sell
$setAside
patient pay moneywhy
human
*sufferFrom
$cure
agent
content
source
payer*
money 
advanced$
doctor
give
hypernym
take
hypernym
occupation affirs
earn
human
#occupation
*cure
medical
inverse
converse
 
Figure 3: Virtual Linking for Causality 
 
The ?expansion depth level? of similar has two 
kinds of utilities: one is to find the minimal expla-
nation, and the other is to be dynamically adapt-
able to the level of interaction. This level of 
similar is defined as a function simi-
lar(Level)(X,Y) for X and Y, concepts or words in 
the following manner: 
? similar(0)=pathsimilar: they use only them-
selves and their hypernym path from X and 
Y.  
? similar(1)=feature_similar: they use their 
features that are expanded one more than 
similar(0). 
? similar(2)=inverse_similar 
? similar(3)=sister_similar 
=inverse_similar?feature_similar. 
Depending on what level of similar is chosen, 
the search paths may be changed. A snapshot up to 
similar(2) is given in Figure 4. 
 
 
Figure 4: Snapshot for similar(2). 
human
* sufferFrom
$cure 
doctorcure patient why
human
#occupation
*cure
medical
agent 
patient 
content 
medical 
medicine* 
disease& 
medical# 
 ne* 
se$ 
l# 
6 Tracing Algorithms 
6.1 Algorithm Crossover 
The overall algorithm 10  flow depends on simi-
lar(Level) as in the next program. 
Algorithm Crossover 
For Level=0...N until stopping 
condition is satisfied: 
   Expand the trace  
by similar(Level) 
For example, when Level=1, the algorithm cross-
over finds a very primitive answer to the question 
?Why do doctors cure patients?? We will expand 
other features of doctor except for cure because 
cure has a syntactic relation between doctor and 
patient. 
As shown in the logical forms (16~24) intro-
duced in the previous section, this algorithm in 
Level=1 can find the following concepts as a re-
sult: medical, human, cure ($cure, *cure). 
When Level=2, the algorithm crossover will 
seek higher-order relations (like the hypothesis) 
from the concept (by inverse_similar), con-
verse/antonymy relations (by feature_similar), 
and    event relations (if any, for use in knowing 
the cause or consequence relation). Consider again 
our example "Why do doctors cure patients?" by 
using the previous section's logical forms. The re-
sults are as follows: 
*cure = {doctor, medicine} 
$cure = {patient, disease} 
*sufferFrom = {patient} 
$sufferFrom = {disease} 
Its generated meaning may be ?If a doctor cures a 
patient, the patient is recovered from disease.            
Because patients suffer from diseases, doctors cure 
the patients. Patients are recovered after getting 
cured.? 
6.2 Stopping Condition 
Stopping conditions for the algorithm crossover 
are as follows: 
(1) Event roles are filled up. 
(2) If no event is found in the feature defini-
tion, increase similar level. 
                                                           
10 This algorithm will be called ?crossover?. 
(3) [weak stopping condition] When there is 
no event, one of the other features is com-
monly shared between two concepts. For 
example, medical is a common feature be-
tween doctor and cure. 
6.3 Hypernym Climbing 
In section 4.2, inheritance was discussed for the 
purpose of finding a relation among pay ~ patient. 
After trying to make Level=2 in section 5.2, we 
have been motivated to find the interrelation be-
tween hypernyms. The algorithm crossover is up-
dated. 
Algorithm Crossover+ 
For Level=0..N until stopping 
condition is satisfied: 
Expand the trace  
by similar(Level) 
If Level >= 2, then 
repeat climb up hypernym 
until it matches with  
the higher relation. 
6.4 Algorithm Crossover++ 
Consider again the question "Why do patients pay 
money to doctors?" As shown in Figure 1, the best 
trace is $cure ~ *cure ~ *earn ~ $pay. It provides 
an explanation for the statement that ?patients are 
cured by doctors ~ doctors earn money ~ patients 
pay money to doctors?. This minimal explanation 
is observed by switching over the role pointers ? 
whenever tracing is performed. For example, 
$cure was switched over to *cure. This extended 
version of algorithm is called Crossover++. 
7 Evaluation 
By the algorithm Crossover?s, the behavior of 
?why?-type questions are investigated by extract-
ing the answer paths as follows. 
Q: Why does patient pay money?  
Path: patient ~ $cure ~ doctor ~ #occupation ~ 
$earn ~ money 
Q: Why does researcher read textbook? 
Path: researcher ~ #knowledge ~ #information ~ 
readings ~ textbook 
Paths between two concepts can now be found 
by simply checking the presence of a path among 
the concepts reached from an initial concept. Table 
1 and Table 2 show examples of the number of 
paths as a function of path size.  
 
Reached concepts path size Source 
concept 1 2 3 
cure 275 593 24854 
eat 268 605 24903 
study 276 358 23172 
food 532 650 18066 
human 6713 3686 51171 
money 328 1312 19827 
Table 1: Examples of destination concepts reached 
starting from one source concept  
 
Paths number length Concept1 Concept2 1 2 3 
cure human 0 78 26 
pay money 0 7 3 
human money 0 3 7 
food human 0 0 28 
read write 0 4 6 
earn pay 0 0 7 
Table 2: The number of paths between pairs of 
concepts 
8 Discussion 
HowNet (Dong et al 1999-2003) has already de-
fined the words and concepts using the features of 
concepts. Each event role is also defined under the 
notion of feature. On the other hand, WordNet 
(Miller, 1995) consists of synsets and their glosses. 
Moldovan et al (2002) showed a lexical chain to 
use words in glosses in order to trace the topically 
related paths.  
Their search boundary is restricted to the 
shapes: V, W, VW, and WW. In this paper, cross-
over* is shown to be flexible and search for a more 
probable explanation. 
9 Conclusion 
In this paper, we have attempted to show how to 
link pre-existing lexical knowledge bases to one 
another. The major issue was to generate a path to 
give explanation paths for answering the ?why?-
type question. While observing the causality path 
behavior, we proposed the measure similar and 
also the algorithm crossover. It is compared with 
the ?weighted abduction? (Hobbs et al 1988) and 
?lexical chain? (Moldovan et al 2002). 
With the ability to provide explanations de-
pending on the level of the measure similar, our 
proposed algorithm adapts itself to the user knowl-
edge level and well as to the type of interactive 
questions to enable more detailed level of ques-
tion-answering.  
References 
Zhen Dong and Q. Dong.  1999-2003. Hownet, 
http://www.keenage.com/ 
Jerry R. Hobbs, Mark Stickel, Douglas Appelt and 
Paul Martin. 1988. Interpretation as Abduction, 
Proceedings of the Conference on 26th Annual 
Meeting of the Assocation for Computational Lin-
guistics. 
Doug Lenat, George Miller, and Toshio Yokoi. 1995. 
CYC, WordNet, and EDR: Critiques and Re-
sponses, Communications of the ACM, 38(11):45-
48. 
Bernardo Magnini and Manuela Speranza. 2002. 
Merging Global and Specialized Linguistic On-
tologies, Proceedings of Ontolex 2002 (Workshop 
held in conjunction with LREC-2002), Las Palmas. 
George Miller. 1995. WordNet: a lexical database. 
Communications of the ACM, 38(11):39-41. 
Dan Moldovan and Adrian Novischi. 2002. Lexical 
Chains for Question Answering, Proceedings of 
COLING 2002, Taipei. 
Takanoa Ogino and Masahiro Kobayashi. 2000. Verb 
Patterns extracted from EDR Concept Description, 
IPSJ SIGNotes Natural Language Abstract, 
No.138 ? 006:39-46. 
Alexandru Marius Pas?a. 2001. High-Performance, 
Open-Domain Question Answering from Large 
Text Collections. Ph.D Dissertation, Southern 
Methodist University. 
H. Sofia Pinto, Asunci?n G?mez-P?rez and Jo?o P. 
Martins. 1999. Some Issues on Ontology Integra-
tion, Proceedings of the IJCAI-99 workshop on 
Ontologies and Problem-Solving Methods (KRR5), 
Stockholm. 
Stuart Russell and Peter Norvig. 1995. Artificial 
Intelligence: A Modern Approach. Prentice-Hall. 
Toshio Yokoi. 1995. The EDR Electronic Dictionary. 
Communications of the ACM, 38(11). 
A Statistical Model for Hangeul-Hanja Conversion in Terminology Domain 
Jin-Xia HUANG, Sun-Mee BAE, Key-Sun CHOI 
Department of Computer Science 
Korea Advanced Institute of Science and Technology/KORTERM/BOLA 
373-1 Guseong-dong, Yuseong-gu, Daejeon, 305-701 
{hgh, sbae, kschoi}@world.kaist.ac.kr 
 
Abstract 
Sino-Korean words, which are historically 
borrowed from Chinese language, could be 
represented with both Hanja (Chinese 
characters) and Hangeul (Korean characters) 
writings. Previous Korean Input Method 
Editors (IMEs) provide only a simple 
dictionary-based approach for Hangeul-Hanja 
conversion. This paper presents a sentence-
based statistical model for Hangeul-Hanja 
conversion, with word tokenization included 
as a hidden process. As a result, we reach 
91.4% of character accuracy and 81.4% of 
word accuracy in terminology domain, when 
only very limited Hanja data is available.  
1 Introduction 
More than one half of the Korean words are 
Sino-Korean words (Chang, 1996). These words 
are historically borrowed from Chinese language, 
could be represented with both Hanja and Hangeul 
writings. Hanja writing is rarely used in modern 
Korean language, but still plays important roles in 
the word sense disambiguation (WSD) and word 
origin tracing, especially in the terminology, 
proper noun and compound noun domain.  
Automatic Hangeul-Hanja conversion is very 
difficult for system because of several reasons. 
There are 473 Hangeul characters (syllables) have 
Hanja correspondences, map to 4888 common 
Hanja characters (Kim, 2003). Each of these 
Hangeul characters could correspond to from one 
to sixty-four Hanja characters, so it is difficult to 
system to select the correct Hanja correspondence. 
Besides that, the sino-Korean Hangeul 
characters/words could be also native Korean 
characters/words according to their meaning. For 
example, ?
  (susul): stamen, operation, fringe?) 
could correspond to a native Korean word ?
 
 
(stamen)?, a sino-Korean word ?   (operation)?, 
and a mixed word ? 

 (fringe)? (Bae, 2000). It 
means in Hangeul-Hanja conversion, the same 
word may be either converted to Hanja or remain 
as Hangeul writing. In addition, compound sino-
Korean words could be written in both with-space 
and without-space formats even after part of 
speech (POS) tagging, because the space using is 
very flexible in Korean language. For example, 
? 
	  (Hanja bienhuan) (Hanja conversion)? 
could be in both ? 	 ? and ? 
	   ? 
writing formats. It means a compound word 
tokenization should be included as a pre-
processing in Hangeul-Hanja conversion. 
Automatic Hangeul-Hanja conversion also suffers 
from another problem, that there are no enough 
Hanja corpora for statistical approach. In modern 
Korean language, only few sino-Korean words are 
written in Hanja writing generally, and the same 
sino-Korean word with the same meaning could be 
in either Hangeul or Hanja writing even in the 
same text.  
This paper presents a sentence-based statistical 
model for Hangeul-Hanja conversion. The model 
includes a transfer model (TM) and a language 
model (LM), in which word tokenization is 
included as a hidden process for compound word 
tokenization. To find answer for the issues like 
adapt the model to character or word level, or limit 
the conversion target to only noun or expand it to 
other Part of Speech (POS) tags, a series of 
experiments has been performed. As a result, our 
system shows significant better result with only 
very limited Hanja data, when we compare it to the 
dictionary-based conversion approach used in 
commercial products.  
In the following of this paper: Section 2 
discusses related works. Section 3 describes our 
model. Section 4 discusses several factors 
considered in the model implementation and 
experiment design. Section 5 gives the evaluation 
approaches and a series of experiment results. 
Section 6 presents our conclusion. 
2 Related Works 
There are several related areas according to the 
tasks and approaches. First is previous Korean 
Hanja, Japanese Kanji (Chinese characters in 
Japanese language) and Chinese Pinyin input 
methods, the second one is English-Korean 
transliteration.   
Korean IME (Haansoft, 2002; Microsoft, 2002) 
supports word-based Hangeul-to-Hanja conversion. 
It provides all possible Hanja correspondences to 
all Hanja-related-Hangeul words in user selected 
range, without any candidate ranking and sino-
Korean word recognition. User has to select sino-
Korean words and pick out the correct Hanja 
correspondence. Word tokenization is performed 
by left-first longest match method; no context nor 
statistical information is considered in the 
correspondence providing, except last-used-first 
approach in one Korean IME (Microsoft, 2002).  
A multiple-knowledge-source based Hangeul-
Hanja conversion method was also proposed (Lee, 
1996). It was a knowledge based approach which 
used case-frame, noun-noun collocation, co-
occurrence pattern between two nouns, last-used-
first and frequency information to distinguish the 
sense of the sino-Korean words and select the 
correct Hanja correspondence for the given 
Hangeul writing. Lee (1996) reported that for 
practical using, there should be enough knowledge 
base, including case-frame dictionary, collocation 
base and co-occurrence patterns to be developed.  
There are several methods were proposed for 
Japanese Kana-Kanji conversion, including last-
used-first, most-used-first, nearby character, 
collocation and case frame based approaches. The 
word co-occurrence pattern (Yamashita, 1988) and 
case-frame based approach (Abe, 1986) were 
reported with a quite high precision. The 
disadvantages include, there should be enough big 
knowledge-base developed before, and syntactic 
analyzer was required for the case frame based 
approach.  
Chinese Pinyin conversion is a similar task with 
Hangeul-Hanja conversion, except that all Pinyin 
syllables are converted to Chinese characters. To 
convert Pinyin P to Chinese characters H, Chen 
and Lee (2000) used Bayes law to maximize 
Pr(H|P), in which a LM Pr(H) and a typing model 
Pr(P|H) are included. The typing model reflects 
online typing error, and also measures if the input 
is an English or Chinese word. As the report, the 
statistical based Pinyin conversion method showed 
better result than the rule and heuristic based 
Pinyin conversion method.  
Hangeul-Hanja conversion normally does not  
need to convert online input. So we assume the 
user input is perfect, and employ a transfer model 
instead of the typing model in Chen and Lee 
(2000).  
The third related work is transliteration. In 
statistical based English-Korean transliteration, to 
convert English word E to Korean word K, a model 
could use Korean LM Pr(K) and TM Pr(E|K) (Lee, 
1999; Kim et.al, 1999) to maximize Pr(K|E), or use 
English LM Pr(E) and TM Pr(K|E) to maximize 
Pr(E,K) (Jung et, al., 2000). 
3 The Model 
Different from previous Hangeul-Hanja 
conversion method in Korean IMEs, our system 
uses statistical information in both sino-Korean 
word recognition and the best Hanja 
correspondence selection. There are two sub-
models included in the model, one is Hangeul-
Hanja TM, and the other one is Hanja LM. They 
provide a unified approach to the whole conversion 
processing, including compound word tokenization, 
sino-Korean word recognition, and the correct 
Hanja correspondence selection.  
Let S be a Hangeul string (block) not longer than 
a sentence. For any hypothesized Hanja conversion 
T, the task is finding the most likely T*, which is a 
most likely sequence of Hanja and/or Hangeul 
characters/words, so as to maximize the highest 
probability Pr(S, T): T* = argmaxTPr(S, T).  
Pr(S, T) could be transfer probability Pr(T|S) 
itself. And like the model in Pinyin IME (Chen and 
Lee, 2000), we also try to use a Hanja LM Pr(T), to 
measure the probabilities of hypothesized Hanja 
and/or Hangeul sequences. The model is also a 
sentence-based model, which chooses the probable 
Hanja/Hangeul word according to the context. 
Now the model has two parts, TM Pr(T|S), and LM 
Pr(T). We have:  
)Pr()|Pr(maxarg),Pr(* TSTTST
T
==  (1) 
T is a word sequence which composed by t1, t2, 
?, tn, where  ti could be either Hanja or Hangeul 
word/character. We can see the model in equation 
(1) does not follow the bayes law. It is only a 
combination model of TM and LM, in which TM 
reflects transfer probability, and LM reflects 
context information. Using linear interpolated 
bigram as LM, the model in equation (1) can be 
rewritten as equation 2.  
?
=
?
?+?
n
i
iiiii tttstTS
1
1 ))Pr()1()|Pr(){|Pr(),Pr( ??
 
(2
) 
Word tokenization is also a hidden process in 
model (2), so both T=t1, t2, ?,tn and T?=t?1,t?2,?t?m 
can be the correspondences of given source 
sentence S. In practice, a Viterbi algorithm is used 
to search the best T* sequence.  
We do not use the noisy channel model 
Pr(T|S)=argmaxTPr(S|T)Pr(T) to get T*, because 
most of the Hanja characters has only one Hangeul 
writing, so that most of the Pr(S|T) tend to be 1. So 
if we use the noisy channel model in Hangeul-
Hanja conversion, the model would be weakened 
to Hanja LM Pr(T) in most of the cases.  
4 Implementation 
There are several factors should be considered in 
the model implementation. For example, we could 
adapt the model to character-level or word-level; 
we could adopt a TM weight as an interpolation 
coefficient, and find out the suitable weight for 
best result; we can also consider about utilizing 
Chinese corpus to try to overcome the sparsness 
problem of Hanja data. We can also limit the sino-
Korean candidates to only noun words, or expand 
the candidates to noun, verb, modifier and affix 
and so on, to see what kind of POS-tag-restriction 
is better for the Hangeul-Hanja conversion.  
We adopt previous dictionary-based approach as 
our base-line system. To get the higher precision in 
the base-line experiments, we also want to check if 
the big dictionary or small dictionary would be 
better for the Hangeul-Hanja conversion.  
4.1 Word Level or Character Level 
There are two kinds of levels in the model 
implementation. In word level implementation, the 
si in equation (2) is a Hangeul word. In character 
level implementation, si is a sequence of Hangeul 
characters.  
In word level implementation, there is no word 
tokenization after POS tagging, so unknown word 
or compound word is considered as one word 
without further tokenization. The advantage of 
word level implementation is, there is no noisy 
caused by tokenization error. Its disadvantage is 
that, the system is weak for the unknown and 
compound word conversion.  
To the contrary, in character level 
implementation, word tokenization are performed 
as a hidden process of the model. There are several 
reasons for why word tokenization is required even 
after POS tagging. First, it is because the morph 
analysis dictionary is different from the Hangeul-
Hanja word dictionary, so the compound word in 
the morph dictionary still could be unknown word 
in Hangeul-Hanja dictionary. Second, there are 
some unknown words even after POS tagging, and 
this situation is quite serious in terminology or 
technical domain. Character level implementation 
will tokenize a given word to all possible character 
strings, and try to find out the best tokenization 
way by finding the most likely T* via equation (2).  
Obviously, character level implementation is 
better than word level implementation for 
unknown and compound word conversion, but it 
also raises the risk of bringing too much noise 
because of the tokenization error. We have to 
distinguish which one is better through the 
experiment.  
4.2 Transfer Model Weight 
Our model in equation 2 is not derived from 
Bayes law. We just use the conditional probability 
Pr(T|S) to reflect the Hangeul-Hanja conversion 
possibility, and assume Hanja LM Pr(S) would be 
helpful for the output smoothing. The model is 
only a combination model, so we need a 
interpolation coefficient ? - a TM weight, to get 
the best combination way of the model. Get the log 
of the equation, the equation (2) can be rewritten as 
equation (3).  
)}}Pr()1()|Pr(log{)1(
 ))|log(Pr({maxarg
1
1
*
iii
n
i
ii
T
ttt
stT
???
?
?+?+
?
?
=

 (3) 
where, ? = [0,1] is the TM weight.  
When ? takes a value between 0 to 1, it?s a 
combination model. When ?=1, the model is a 
TM; and when ?=0, the model is a LM. 
To the LM, we test both unigram and bigram in 
word level experiment. The interpolated bigram in 
equation (3) is used for character level 
implementation.  
4.3 Language Resource Utilization 
There is no much Hanja data could be used for 
Hangeul-Hanja conversion. So we treat Hangeul-
Hanja word dictionary as a Dictionary corpus, 
which is 5.3Mbytes in our experiment, to get 
unigram, bigram and transfer probability. The 
extracted data from dictionary is called dictionary 
data D.  
Second, we extract user data U from a very small 
user corpus (0.28Mbytes in our open test), which is 
in the same domain with the testing data.  
Finally, we assume that Chinese corpus is 
helpful for the Hangeul-Hanja conversion because 
of the historical relation between them, although 
they may not exactly the same words in the two 
language. We convert the code of the Hanja words 
to Chinese ones (GB in our experiment) to get 
Chinese data D (unigram and bigram) for the 
Hanja words from Chinese corpus, which is 
270Mbytes corpus in news domain (TREC9, 2000).  
We want to know how much these different data 
D, U, C can help for Hangeul-Hanja conversion, 
and testify that through experiment.  
4.4 POS Tag Constraint  
We compare two cases to see the influence of 
the POS tag constraint in sino-Korean recognition. 
The first case is only treat Noun as potential sino-
Korean, and in the other case we extend noun to 
other possible POS tags, including noun, verb, 
modification, suffix, and affix. The sign, foreign, 
junction words are excluded from the potential 
sino-Korean candidates. It is because these words 
would never be sino-Korean in practice. A POS 
tagger is employed for the pre-processing of our 
system. 
Actually, most of the sino-Korean words that 
need Hanja writing are noun words, but in practice, 
the POS tagger normally shows tagging errors. 
Such kind of tagging error is much more serious in 
terminology and technical domain. It is one of the 
reasons why we want to expand the noun words to 
other possible POS tags. Another reason is, the 
more restricted the POS tag constraint is, the lower 
the coverage is, although the higher precision 
could be expected. So we should have a test to see 
if the constraint should be more restrict or less.  
4.5 Dictionary Size 
We develop a dictionary-based conversion 
system as our base line system. This dictionary-
based system follows the approach used in the 
previous Korean IMEs. The difference is our 
system uses POS tagger, and gives the best 
candidate for all sino-Korean words, when 
previous IMEs only provide all possible candidates 
without ranking and let user to select the correct 
one.  
Intuitively, the bigger the dictionary is, the better 
the conversion result would be. But generally, the 
word in bigger dictionary has more candidates, so 
it is still possible that bigger dictionary will low 
down the conversion performance. So we want to 
distinguish which one is better for Hangeul-Hanja 
in practical using.  
We used two dictionaries in the experiments, one 
contains 400k Hangeul-Hanja word entries, and 
one contains 60k Hangeul-Hanja word entries. 
5 Experiment 
This chapter shows the experiments on the 
model in equation 3 and some different 
implementations we have discussed above.  
There are two parts in the experiments, first one 
is mostly related to word level model 
implementation, in which the basic issues like 
language resource utilization and POS tag 
restriction, and some word level related issues like 
bigram or unigram for LM in word level are tested. 
The second part is mostly character level related.  
Several evaluation standards are employed in the 
experiments. The adopted standards and  
evaluation approaches are reported in the first 
section of the experiments.  
5.1 Evaluation Standard and Approach 
We use several evaluation standards in the 
experiments. To reflect the readability from the 
user viewpoint, we adopt word and phrase 
(sentence) level accuracy, precision and recall; to 
compare the automatic conversion result with the 
standard result ? from the developer viewpoint, 
Dice-coefficient based similarity calculation is 
employed also; to compare with previous Chinese 
Pinyin input method, a character based accuracy 
evaluation is also adopted.  
An automatic evaluation and analysis system is 
developed to support large scale experiments. The 
system compares the automatic result to the 
standard one, and performs detailed error analysis 
using a decision tree.  
5.2 Word Level Experiment 
In this part, the basic issues like language 
resource utilization and POS tag restriction, and 
the word level related issues, like bigram or 
unigram for LM are performed.  
The objects of the first experiment are, firstly, 
compare a simple LM based statistical approach 
with the base line - dictionary based approach; 
secondly, see if large dictionary is better than small 
dictionary in dictionary based conversion; thirdly, 
see if Chinese corpus does help to the Hangeul-
Hanja conversion.  
A small dictionary based conversion (Dic), large 
dictionary based conversion (BigDic), a unigram 
(Unigram) and a bigram based (Bigram) word 
level conversion, are performed to compared to the 
each other.  
The small dictionary Dic has 56,000 Hangeul-
Hanja entries; while the large dictionary BigDic 
contains 280,000 Hangeul-Hanja entries. The 
unigram and bigram are extracted from Chinese 
data C. The test set is a small test set with 90 terms 
(180 content words) from terminology domain. 
Word level precision and recall with F1-measure 
are employed as evaluation standard.  
 
 Dic BigDic unigram bigram 
P 57.1% 50.0% 78.6% 78.6% 
R 25.7% 44.0% 70.6% 70.6% 
F1 35.4% 46.8% 74.4% 74.4% 
Table 1. Base line (small dic vs. large dic) vs. 
Statistical approach (unigram vs. bigram) 
From the result shows in table 1, we can get the 
conclusions that, 1) compare to the small 
dictionary, large dictionary reaches better F1-
measure because of the enhancement in recall, 
although the precision is slightly low downed 
because of more Hanja candidates for given 
Hangeul entry; 2) Statistical approach shows 
obvious better result than the dictionary based 
approach, although it is only a very simple LM; 3) 
Chinese data does help to the Hangeul-Hanja 
conversion. We have to evaluation its impact by 
comparing it with other Hanja data in further 
experiments. 4) Bigram shows similar result with 
unigram in word level conversion, it shows that 
data sparseness problem is still very serious.  
The objects of the second experiment include 
the evaluation on different POS tag constraints and 
the comparison between different language 
resources.  
First is evaluation on different POS tag 
constraints. Let the system employs unigram based 
Hangeul-Hanja conversion approach, which uses 
dictionary data D (word unigram from large 
dictionary at here). Our experiment wants to 
compare the case of only considering noun as 
potential sino-Korean words (?Dn? in table 2), 
with the case of extending the POS tags to verb, 
modification and affix (?De? in table 2). Second 
evaluation is comparison on different language 
resources. As we have mentioned above, D is data 
from large dictionary (word unigram is used at 
here), U is data from very small user corpus, and C 
is data from Chinese corpus. We want to compare 
the different combination of these language 
resources. In the second evaluation, extended POS 
tag constraint is employed.  
The experiment uses a test set with 5,127 terms 
(12,786 content words, 4.67 Hanja candidates per 
sino-Korean word in average) in computer science 
and electronic engineering domain. User data U is 
from user corpus, which is the same with the test 
set at here (so it is a closed test). In evaluation, a 
dice-coefficient based similarity evaluation 
standard is employed.  
 
 Dn De U C DC DU UC DUC 
Sim 0.71 0.75 0.81 0.72 0.75 0.82 0.82 0.81 
Table 2. POS tag constraint and language resource 
evaluation 
From the table 2, we can see that, 1) the 
extended POS tag constraint (?De? in table 2) 
shows better result than the noun POS tag 
constraint (?Dn?); 2) User data U shows better 
result than dictionary data D (?U?   ?De?, ?UC? 
  ?DC? in table 2), and dictionary data D shows 
better result than Chinese data C (?De?   ?C?), 
although Chinese corpus (where C is from) is 
270MB, and much larger than the Hangeul-Hanja 
dictionary (5.3MB here, where D is from). It 
shows that the effect of Chinese data is quite 
limited in despite of its usefulness.  
The object of the third experiment is to find 
out which TM weight ? is better for the word 
model.  
 
 ?=0 ?=0.5 ?=1 
P 78.6% 76.52% 84.80% 
R 70.6% 70.70% 77.31% 
F1 74.4% 73.4% 80.8% 
Table 3. TM weight in word model 
Let ? to be 0, 0.5, 1, and so the model in 
equation (3) is LM, combined model, and TM, 
with the same environment of the second 
experiment, we get the result in table 3. Word level 
precision and recall with F1-measure is evaluated. 
We can see the TM with ?=1 shows the best result.  
5.3 Character Level Experiment 
In the character level experiments, first, we 
compare the character level model with base line 
dictionary based approach; Second, compare the 
character level model with the word level model; 
Third, to find out the best TM weight for the 
character level model.  
This part of experiments uses a new test set, 
which has 1,000 terms in it (2,727 content words; 
3.9 Hanja candidates per sino-Korean word in 
average). The user data U has 12,000 Hangeul-
Hanja term pairs in it. U is from the same domain 
of the test set (computer science and electronic 
engineering domain at here), but there is no 
overlap with the test set (so it is a opened test).  
Several different evaluation standards are 
employed. As the first column of table 4, ?CA?, 
?WA? and ?SA? mean character, word, sentence 
(terms) accuracy, respectively. ?Sim? is the 
similarity based evaluation, and F1 is the value of 
word level F1-measure which is from word 
precision/recall evaluation.  
 
% Dic wD1 wDUC1 D.5 DU0 DU.2 DU.5 DU.8 DU1
CA 62.9 69.1 75.0 73.1 81.0 89.3 90.2 91.0 91.4
WA 49.9 73.8 75.3 64.6 72.4 77.1 82.3 82.1 81.4
SA 18.8 43.4 51.2 34.7 48.2 67.0 67.5 67.1 68.1
Sim 68.4 75.5 79.7 77.9 82.5 90.4 91.2 91.7 92.1
F1 39.0 65.6 69.7 51.2 60.8 75.7 75.9 75.9 76.2
Table 4: Character level model vs. word level 
model vs. base line (dictionary based approach) 
The first row of table 4 shows the Hangeul-
Hanja conversion approach with the employed data 
and TM weight ?. ?Dic? is the base line dictionary 
based approach; ?w? means word level model; ?D? 
means dictionary data (extracted from the large 
dictionary with 400,000 Hangeul-Hangeul and 
Hangeul-Hanja entries), U means user data 
described above, C means Chinese data. The 
digital value like ?.5? is TM weight. So, as an 
example, ?wDUC1? means word model with ?=1 
and using all data resources D, U and C; ?DU.2? 
means character model with ?=0.2 and using data 
D and U. 
From the table 4, we can get the conclusions that, 
1) all statistical model based approaches shows 
obviously better performance than the base line 
dictionary based approach ?Dic? (Dic   others). 
2) In most cases, character models show better 
results than word model (DUx   wDUCw1). But 
when there is no user data, word mode is better 
than character model (wD1  D.5). 3) Among 
character models, the TM with ?=1 shows the best 
result (?DU1?   ?DU.x?). 4) User data has 
positive impact on the performance (?Dw1   
DUCw1?, ?D.5   DU.5?), and it is especially 
important to the character model (?D.5   DU.5?). 
It is because character model may cause more 
noise because of word tokenization error when 
there is no user data. 
From the table 4, we can see the best result is 
gotten from character based TM with using 
dictionary and user data D, U (?DU1?). The best 
character accuracy is 91.4%, when the word 
accuracy is 81.4%. The character accuracy is lower 
than the typing and language model based Chinese 
Pinyin IME, which was 95% in Chen & Lee (2000).  
But consider that in our experiment, there is almost 
no Hanja data except dictionary, and also consider 
the extra difficulty from terminology domain, this 
comparision result is quite understandable. Our 
experiment also shows that, compare to using only 
LM like it in Chen & Lee (2000), TM shows 
significantly better result in character accuracy 
(from 81.0% to 91.4% in our experiment: ?DU0? 
  ?DU1?, table 4). Our user evaluation also 
shows that, to the terminology domain, the 
automatic conversion result from the system shows 
even better quality than the draft result from 
untrained human translator.  
 
5.4 Different Evaluation Standards 
Figure 1 shows the trends of different evaluation 
standards in the same experiment shown in table 4. 
We can see character accuracy ?CA? shows similar 
trend with similarity based standard ?Sim?, while 
word accuracy ?WA? and sentence (terms) 
accuracy ?SA? show similar trends with F1-
measure ?F1?, in which ?F1?is based on word 
precision and recall.  
From the user viewpoint, word/sentence 
accuracy and F1-measure reflects readability better 
than character accuracy. It is because, if there is a 
character wrongly converted in a word, it affects 
the readability of whole word but not only that 
character?s. However, character accuracy is more 
important to the system evaluation, especially for 
the character level model implementation. It is 
because the character accuracy can reflect the 
system performance in full detail than the word or 
sentence (term) based one. 





Determining the Specificity of Terms based on Information Theoretic 
Measures 
Pum-Mo Ryu and Key-Sun Choi 
Dept. EECS/KORTERM KAIST 
373-1 Guseong-dong Yuseong-gu 
305-701 Daejeon 
Korea 
pmryu@world.kaist.ac.kr, kschoi@world.kaist.ac.kr 
 
 
Abstract 
This paper introduces new specificity 
determining methods for terms based on 
information theoretic measures. The 
specificity of terms represents the quantity of 
domain specific information that is contained 
in the terms. Compositional and contextual   
information of terms are used in proposed 
methods. As the methods don?t rely on domain 
dependent information, they can be applied to 
other domains without extra processes.  
Experiments showed very promising results 
with the precision 82.0% when applied to the 
terms in MeSH thesaurus. 
1 Introduction 
The specificity of terms represents the quantity of 
domain specific information contained in the terms. 
If a term has large quantity of domain specific 
information, the specificity of the term is high. The 
specificity of a term X is quantified to positive real 
number as equation (1). 
( )Spec X R+?                           (1) 
The specificity is a kind of necessary condition 
for term hierarchy, i.e., if X1 is one of ancestors of 
X2, then Spec(X1) is less than Spec(X2). Thus this 
condition can be applied to automatic construction 
or evaluation of term hierarchy. The specificity 
also can be applied to automatic term recognition. 
Many domain specific terms are multiword 
terms. When domain specific concepts are 
represented as multiword terms, the terms are 
classified into two categories based on composition 
of unit words. In the first category, new terms are 
created by adding modifiers to existing terms. For 
example ?insulin-dependent diabetes mellitus? was 
created by adding modifier ?insulin-dependent? to 
its hypernym ?diabetes mellitus? as in Table 1. In 
English, the specific level terms are very 
commonly compounds of the generic level term 
and some modifier (Croft, 2004). In this case, 
compositional information is important to get 
meaning of the terms. In the second category, new 
terms are independent of existing terms. For 
example, ?wolfram syndrome? is semantically 
related to its ancestor terms as in Table 1. But it 
shares no common words with its ancestor terms. 
In this case, contextual information is important to 
get meaning of the terms. 
 
Node Number Terms 
C18.452.297 diabetes mellitus 
C18.452.297.267 insulin-dependent diabetes mellitus 
C18.452.297.267.960 wolfram syndrome 
Table 1 Subtree of MeSH1 thesaurus. Node 
numbers represent hierarchical structure of terms 
Contextual information has been mainly used to 
represent the meaning of terms in previous works. 
(Grefenstette, 1994) (Pereira, 1993) and 
(Sanderson, 1999) used contextual information to 
find hyponymy relation between terms. (Caraballo, 
1999) also used contextual information to 
determine the specificity of nouns. Contrary, 
compositional information of terms has not been 
commonly discussed. We propose new specificity 
measuring methods based on both compositional 
and contextual information. The methods are 
formulated as information theory like measures. 
This paper consists as follow; new specificity 
measuring methods are introduced in section 2, and 
the experiments and evaluation on the methods are 
discussed in section 3, finally conclusions are 
drawn in section 4. 
2 Specificity Measuring Methods 
In this section, we describe information theory like 
methods to measure the specificity of terms. Here, 
we call information theory like methods, because 
some probability values used in these methods are 
                                                     
1 MeSH is available at http://www.nlm.nih.gov/mesh. 
MeSH 2003 was used in this research. 
CompuTerm 2004 Poster Session  -  3rd International Workshop on Computational Terminology 87
not real probability, rather they are relative weight 
of terms or words.  
In information theory, when a low probability 
message occurs on channel output, the quantity of 
surprise is large, and the length of bits to represent 
the message becomes long. Thus the large quantity 
of information is gained by the message (Haykin, 
1994). If we regard the terms in corpus as the 
messages of channel output, the information 
quantity of the terms can be measured using 
information theory. A set of target terms is defined 
as equation (2) for further explanation. 
{ |1 }kT t k n= ? ?                         (2) 
where tk is a term. In next step, a discrete random 
variable X is defined as equation (3). 
{ |1 }   ( ) Prob( )k k kX x k n p x X x= ? ? = =    (3) 
where xk is an event of tk is observed in corpus, 
p(xk) is the probability of xk. The information 
quantity, I(xk), gained after observing xk, is used as 
the specificity of tk as equation (4). 
( ) ( ) log ( )k k kSpec t I x p x? = ?              (4) 
By equation (4), we can measure the specificity 
of tk, by estimating p(xk). We describe three 
estimating methods for p(xk) in following sections. 
2.1 Compositional Information based Method 
(Method 1) 
By compositionality, the meaning of a term can be 
strictly predicted from the meaning of the 
individual words (Manning, 1999). This method is 
divided into two steps: In the first step, the 
specificity of each word is measured independently. 
In the second step, the specificity of composite 
words is summed up. For detail description, we 
assume that tk consists of one or more words as 
equation (5). 
1 2...k mt w w w=                           (5) 
where wi is i-th word in tk. In next step, a discrete 
random variable Y is defined as equation (6). 
{ |1 }   ( ) Prob( )i i iY y i m p y Y y= ? ? = =       (6) 
where yi is an event of wi occurs in term tk, p(yi) is 
the probability of yi. Information quantity, I(xk), in 
equation (4) is redefined as equation (7) based on 
previous assumption. 
1
( ) ( ) log ( )
m
k i i
i
I x p y p y
=
= ??                 (7) 
where I(xk) is average information quantity of all 
words in tk. In this mechanism, p(yi) of informative 
words should be smaller than that of non 
informative words. Two information sources, word 
frequency, tf.idf are used to estimate p(yi)  
independently. 
We assume that if a term is composed of low 
frequency words, the term have large quantity of 
domain information. Because low frequency words 
appear in limited number of terms, they have high 
discriminating ability. On this assumption, p(yi) in 
equation (7) is estimated as relative frequency of wi 
in corpus. In this estimation, P(yi) for low 
frequency words becomes small. 
tf.idf is widely used term weighting scheme in 
information retrieval (Manning, 1999). We assume 
that if a term is composed of high tf.idf words, the 
term have domain specific information. On this 
assumption, p(yi) in equation (7) is estimated as 
equation (8). 
( )( ) ( ) 1
( )
i
i MLE i
j
j
tf idf wp y p w
tf idf w
?? = ? ??        (8) 
where tf?idf(w) is tf.idf value of w. In this equation, 
p(yi) of high tf.idf words becomes small. 
If the modifier-head structure is known, the 
specificity of the term is calculated incrementally 
starting from head noun. In this manner, the 
specificity of the term is always larger than that of 
the head term. This result answers to the 
assumption that more specific term has higher 
specificity. We use simple nesting relations 
between terms to analyze modifier-head structure 
as follows (Frantzi, 2000): 
 
Definition 1 If two terms X and Y are terms in 
same semantic category and X is nested in Y as 
W1XW2, then X is head term, and W1 and W2 are 
modifiers of X. 
 
For example, because ?diabetes mellitus? is 
nested in ?insulin dependent diabetes mellitus? and 
two terms are all disease names, ?diabetes 
mellitus? is head term and ?insulin dependent? is 
modifier. The specificity of Y is measured as 
equation (9). 
1 2( ) ( ) ( ) ( )Spec Y Spec X Spec W Spec W? ?= + ? + ?    (9) 
where Spec(X), Spec(W1), and Spec(W2) are the 
specificity of X, W1, W2 respectively. ?  and ?  are 
weighting schemes for the specificity of modifiers. 
They are found by experimentally. 
2.2 Contextual Information based Method 
(Method 2)  
There are some problems that are hard to address 
using compositional information alone. Firstly, 
although two disease names, ?wolfram syndrome? 
and ?insulin-dependent diabetes mellitus?, share 
CompuTerm 2004 Poster Session  -  3rd International Workshop on Computational Terminology88
many common features in semantic level, they 
don?t share any common words in lexical level. In 
this case, it is unreasonable to compare two 
specificity values based on compositional 
information. Secondly, when several words are 
combined into one term, there are additional 
semantic components that are not predicted by unit 
words. For example, ?wolfram syndrome? is a kind 
of ?diabetes mellitus?. We can not predict the 
meaning of ?diabetes mellitus? from two separate 
words ?wolfram? and ?syndrome?. Thus we use 
contextual information to address these problems. 
General terms are frequently modified by other 
words in corpus. Because domain specific terms 
have sufficient information in themselves, they are 
rarely modified by other words, (Caraballo, 1999). 
Under this assumption, we use probability 
distribution of modifiers as contextual information. 
Collecting sufficient modifiers from given corpus 
is very important in this method. To this end, we 
use Conexor functional dependency parser 
(Conexor, 2004) to analyze the structure of 
sentences. Among many dependency functions 
defined in the parser, ?attr? and ?mod? functions 
are used to extract modifiers from analyzed 
structures. This method can be applied the terms 
that are modified by other words in corpus. 
Entropy of modifiers for a term is defined as 
equation (10). 
( ) ( , ) log ( , )mod k i k i k
i
H t p mod t p mod t= ??    (10) 
where p(modi,tk) is the probability of modi modifies 
tk and it is estimated as relative frequency of modi 
in all modifiers of tk. The entropy calculated by 
equation (10) is the average information quantity 
of all (modi,tk) pairs. Because domain specific 
terms have simple modifier distributions, the 
entropy of the terms is low. Therefore inversed 
entropy is assigned to I(xk) in equation (4) to make 
specific terms get large quantity of information. 
1
( ) max( ( )) ( )k mod i mod ki nI x H t H t? ?? ?           (11) 
where the first term of approximation is the 
maximum modifier entropy of all terms. 
2.3 Hybrid Method (Method 3) 
In this section, we describe hybrid method to 
overcome shortcomings of previous two methods. 
In this method the specificity is measured as 
equation (12). 
1( ) 1 1( ) (1 )( )
( ) ( )
k
Cmp k Ctx k
I x
I x I x
? ?
?
+ ?
      (12) 
where ICmp(xk) and ICtx(xk) are information quantity 
measured by method1 and method 2 respectively. 
They are normalized value between 0 and 1. 
(0 1)? ?? ?  is weight of two values. If 0.5? = , the 
equation is harmonic mean of two values. 
Therefore I(xk) becomes large when two values are 
equally large. 
3 Experiments and Evaluation 
In this section, we describe our experiments and 
evaluate proposed methods.  
We select a subtree of MeSH thesaurus for the 
experiment. ?metabolic diseases(C18.452)? node is 
root of the subtree, and the subtree consists of 436 
disease names which are target terms for 
specificity measuring. We used MEDLINE 2 
database corpus (170,000 abstracts, 20,000,000 
words) to extract statistical information. 
Each method was evaluated by two criteria, 
coverage and precision. Coverage is the fraction of 
the terms which have the specificity by given 
method. Method 2 gets relatively lower coverage 
than method 1, because method 2 can measure the 
specificity only when both the terms and their 
modifiers occur in corpus. Method 1 can measure 
the specificity whenever parts of composite words 
appear in corpus. Precision is the fraction of 
correct specificity relations values as equation (13). 
#   ( , )   
#    ( , )
of R p c with correct specificityp
of all R p c
=   (13) 
where R(p,c) is a parent-child relation in MeSH 
thesaurus. If child term c has larger specificity than 
that of parent term p, then the relation is said to 
have correct specificity. We divided parent-child 
relations into two types. Relations where parent 
term is nested in child term are categorized as type 
I. Other relations are categorized as type II. There 
are 43 relations in type I and 393 relations in type 
II. The relations in type I always have correct 
specificity provided modifier-head information 
method described in section 2.1 is applied. 
We tested prior experiment for 10 human 
subjects to find out the upper bound of precision. 
The subjects are all medical doctors of internal 
medicine, which is closely related division to 
?metabolic diseases?. They were asked to identify 
parent-child relationship for given term pairs. The 
average precisions of type I and type II were 
96.6% and 86.4% respectively. We set these values 
as upper bound of precision for suggested methods. 
                                                     
2  MEDLINE is a database of biomedical articles 
serviced by National Library of Medicine, USA. 
(http://www.nlm.nih.gov) 
CompuTerm 2004 Poster Session  -  3rd International Workshop on Computational Terminology 89
The specificity of terms was measured with 
method 1, method 2, and method 3 as Table 2. 
Two additional methods, based on term frequency 
and term tf.idf, were experimented to compare 
compositionality based methods and term based 
methods. 
Method 1 showed better performance than term 
based methods. This result illustrate basic 
assumption of this paper that specific concepts are 
created by adding information to existing concepts, 
and new concepts are expressed as new terms by 
adding modifiers to existing terms. Word tf.idf 
based method showed better precision than word 
frequency based method. This result illustrate that 
tf.idf of words is more informative than frequency 
of words. 
Method 3 showed the best precision, 82.0%, 
because the two methods interacted 
complementary. In hybrid method, the weight 
value 0.8? =  indicates that compositional 
information is more informative than contextual 
information for the specificity of domain specific 
terms.  
One reason of the errors is that the names of 
some internal nodes in MeSH thesaurus are 
category names rather disease names. For example, 
as ?acid-base imbalance (C18.452.076)? is name 
of disease category, it doesn't occur as frequently 
as other real disease names. Other predictable 
reason is that we didn?t consider various surface 
forms of same term. For example, although 
?NIDDM? is acronym of ?non insulin dependent 
diabetes mellitus?, the system counted two terms 
separately. Therefore the extracted statistics can?t 
properly reflect semantic level information. 
4 Conclusion 
This paper proposed specificity measuring 
methods for terms based on information theory like 
measures using compositional and contextual 
information of terms. The methods are 
experimented on the terms in MeSH thesaurus. 
Hybrid method showed the best precision of 82.0%, 
because two methods complemented each other. 
As the proposed methods don't use domain 
dependent information, they can be adapted to 
other domains without extra processes. 
In the future, we will modify the system to 
handle various term formations such as abbreviated 
form. Finally we will apply the proposed methods 
to the terms of other specific domains. 
5 Acknowledgements 
This work was supported in part by Ministry of 
Science & Technology, Ministry of Culture & 
Tourism of Korean government, and Korea 
Science & Engineering Foundation. 
References  
Caraballo, S. A., Charniak, E. 1999. Determining 
the Specificity of Nouns from Text. Proceedings 
of the Joint SIGDAT Conference on Empirical 
Methods in Natural Language Processing and 
Very Large Corpora 
Conexor. 2004. Conexor Functional Dependency 
Grammar Parser. http://www.conexor.com 
Croft, W. 2004. Typology and Universals. 2nd ed. 
Cambridge Textbooks in Linguistics, Cambridge 
Univ. Press 
Frantzi, K., et. al. 2000. Automatic recognition of 
multi-word terms: the C-value/NC-value method. 
Journal of Digital Libraries, vol. 3, num. 2 
Grefenstette, G. 1994. Explorations in Automatic 
Thesaurus Discovery. Kluwer Academic 
Publishers 
Haykin, S. 1994. Neural Network. IEEE Press 
Manning, C. D. and Schutze, H. 1999. 
Foundations of Statistical Natural Language 
Processing. The MIT Presss 
Pereira, F., Tishby, N., and Lee, L. 1993. 
Distributational clustering of English words. 
Proceedings of ACL 
Sanderson, M. 1999. Deriving concept hierarchies 
from text. Proceedings of ACM S1GIR 
Precision 
Methods 
Type I Type II Total 
Coverage
Human subjects(Average) 96.6 86.4 87.4  
Term frequency 100.0 53.5 60.6 89.5 
Term tf?idf 52.6 59.2 58.2 89.5 
Word Freq. 37.2 72.5 69.0 100.0 
Word Freq.+Structure (?=?=0.2) 100.0 72.8 75.5 100.0 
Word tf?idf 44.2 75.3 72.2 100.0 
Compositional 
Information 
Method 
(Method 1) Word tf?idf +Structure (?=?=0.2) 100.0 76.6 78.9 100.0 
Contextual Information Method (Method 2) (mod cnt>1) 90.0 66.4 70.0 70.2 
Hybrid Method (Method 3)  (tf?idf + Struct, ?=0.8) 95.0 79.6 82.0 70.2 
Table 2. Experimental results (%) 
CompuTerm 2004 Poster Session  -  3rd International Workshop on Computational Terminology90
Proceedings of the 2nd Workshop on Ontology Learning and Population, pages 41?48,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Taxonomy Learning using Term Specificity and Similarity 
 
Pum-Mo Ryu 
 
Computer Science Division, KAIST 
KORTERM/BOLA 
Korea 
pmryu@world.kaist.ac.kr 
Key-Sun Choi 
 
Computer Science Division, KAIST 
KORTERM/BOLA 
Korea 
kschoi@cs.kaist.ac.kr 
 
 
 
Abstract 
Learning taxonomy for technical terms is 
difficult and tedious task, especially 
when new terms should be included. The 
goal of this paper is to assign taxonomic 
relations among technical terms. We pro-
pose new approach to the problem that 
relies on term specificity and similarity 
measures. Term specificity and similarity 
are necessary conditions for taxonomy 
learning, because highly specific terms 
tend to locate in deep levels and semanti-
cally similar terms are close to each other 
in taxonomy. We analyzed various fea-
tures used in previous researches in view 
of term specificity and similarity, and ap-
plied optimal features for term specificity 
and similarity to our method.  
1 Introduction 
Taxonomy is a collection of controlled vocabu-
lary terms organized into a hierarchical structure. 
Each term in a taxonomy is one or more parent-
child relationships to other terms in the taxon-
omy. Taxonomies are useful artifacts for orga-
nizing many aspects of knowledge. As compo-
nents of ontologies, taxonomies can provide an 
organizational model for a domain (domain on-
tology), or a model suitable for specific tasks 
(task ontologies) (Burgun & Bodenreider, 2001). 
However their wide usage is still hindered by 
time-consuming, cost-ineffective building proc-
esses. 
The main paradigms of taxonomy learning are 
on the one hand pattern based approaches and on 
the other hand distributional hypothesis based 
approaches. The former is approaches based on 
matching lexico-syntactic patterns which convey 
taxonomic relations in a corpus (Hearst, 1992; 
Iwanska et al, 2000), and the latter is statistical 
approaches based on the distribution of context 
in corpus (Cimiano et al, 2005; Yamamoto et al, 
2005; Sanderson & Croft, 1999). The former fea-
tures a high precision and low recall compared to 
the latter. The quality of learned relations is 
higher than those of statistical approaches, while 
the patterns are rarely applied in real corpus. It is 
also difficult to improve performance of pattern 
based approaches because they are simple and 
clear. So, many researches have been focused on 
raising precision of statistical approaches. 
We introduce new distributional hypothesis 
based taxonomy learning method using term 
specificity and term similarity. Term specificity 
is a measure of information quantity of terms in 
given domain. When a term has much domain 
information, the term is highly specific to the 
domain, and vice versa (Ryu & Choi, 2005). Be-
cause highly specific terms tend to locate in low 
level in domain taxonomy, term specificity can 
be used as a necessary condition for taxonomy 
learning. Term similarity is degree of semantic 
overlap among terms. When two terms share 
many common characteristics, they are semanti-
cally similar to each other. Term similarity can 
be another necessary condition for taxonomy 
learning, because semantically similar terms lo-
cate near by in given domain taxonomy. The two 
conditions are generally valid for terms in a taxo-
nomic relation, while terms satisfying the condi-
tions do not always have taxonomic relation. So 
they are necessary conditions for taxonomy 
learning. 
Based on these conditions, it is highly prob-
able that term t1 is an ancestor of term t2 in do-
main taxonomy TD, when t1 and t2 are semanti-
cally similar enough and the specificity of t1 is 
lower than that of t2 in D as in Figure 1. However, 
t1 is not an ancestor of t3 even though the speci-
41
ficity of t1 is lower than that of t3 because t1 is not 
similar to t3 on the semantic level. 
 
t1
t2 t3
Similarity
Specificity 
high
low
Depth
high
low
 
Figure 1. Term specificity and term similarity in 
a domain taxonomy TD 
 
The strength of this method lies in its ability to 
adopt different optimal features for term specific-
ity and term similarity. Most of current re-
searches relied on single feature such as adjec-
tives of terms, verb-argument relation, or co-
occurrence ratio in documents according to their 
methods. Firstly, we analyze characteristics of 
features for taxonomy learning in view of term 
specificity and term similarity to show that the 
features embed characteristics of specificity and 
similarity, and finally apply optimal features to 
our method.  
Additionally we tested inside information of 
terms to measure term specificity and similarity. 
As multiword terms cover the larger part of tech-
nical terms, lexical components are featuring 
information representing semantics of terms 
(Cerbah, 2000). 
The remainder of this paper is organized fol-
lows. Characteristics of term specificity are de-
scribed in Section 2, while term similarity and its 
features are addressed in Section 3. Our taxon-
omy learning method is discussed in Section 4. 
Experiment and evaluation are discussed in Sec-
tion 5, and finally, conclusions are drawn in Sec-
tion 6. 
2 Term Specificity 
Specificity is degree of detailed information of 
an object about given target object. For example, 
if an encyclopedia contains detailed information 
about ?IT domain?, then the encyclopedia is ?IT 
specific encyclopedia?. In this context, specificity 
is a function of objects and target object to real 
number. Traditionally term specificity is widely 
used in information retrieval systems to weight 
index terms in documents (S. Jones, 1972; Ai-
zawa, 2003; Wong & Yao, 1992). In information 
retrieval context, term specificity is function of 
index terms and documents. On the other hand, 
term specificity is the function of terms and tar-
get domains in taxonomy learning context (Ryu 
& Choi 2005). Term specificity to a domain is 
quantified to a positive real number as shown in 
Eq. (1). 
 
( | )Spec t D R+?                                              (1) 
 
where t is a term, and Spec(t|D) is the specificity 
of t in a given domain D. We simply use Spec(t) 
instead of Spec(t|D) assuming a particular do-
main D in this paper.  
Understanding the relation between domain 
concepts and their lexicalization methods is 
needed, before we describe term specificity 
measuring methods. Domain specific concepts 
can be distinguished by a set of what we call 
?characteristics?. More specific concepts are cre-
ated by adding characteristics to the set of char-
acteristics of existing concepts. Let us consider 
two concepts: C1 and C2. C1 is an existing con-
cept and C2 is a newly created concept by com-
bining new characteristics to the characteristic 
set of C1. In this case, C1 is an ancestor of C2 
(ISO, 2000). When domain specific concepts are 
lexicalized as terms, the terms' word-formation is 
classified into two categories based on the com-
position of component words. In the first cate-
gory, new terms are created by adding modifiers 
to existing terms. Figure 2 shows a subtree of 
financial ontology. For example ?current asset? 
was created by adding the modifier ?current? to 
its hypernym ?asset?. In this case, inside informa-
tion is a good evidence to represent the charac-
teristics. In the second category, new terms are 
created independently of existing terms. For ex-
ample, ?cache?, ?inventory?, and ?receivable? 
share no common words with their hypernyms 
?current asset? and ?asset?. In this case, outside 
information is used to differentiate the character-
istics of the terms. 
 
asset
current asset fixed asset
cache inventory receivable intangibleasset  
Figure 2. Subtree of financial ontology 
 
There are many kinds of inside and outside in-
formation to be used in measuring term specific-
ity. Distribution of adjective-term relation and 
verb-argument dependency relation are colloca-
tion based statistics. Distribution of adjective-
term relation refers to the idea that specific nouns 
are rarely modified, while general nouns are fre-
42
quently modified in text. This feature has been 
discussed to measure specificity of nouns in 
(Caraballo, 1999; Ryu & Choi, 2005) and to 
build taxonomy of Japanese nouns (Yamamoto et 
al., 2005). Inversed specificity of a term can be 
measured by entropy of adjectives as shown Eq. 
(2). 
 
1( ) ( | ) log ( | )adj
adj
Spec t P adj t P adj t? = ??              (2) 
 
where P(adj|t), the probability that adj modifies t, 
is estimated as freq(adj,t)/freq(t). The entropy is 
the average information quantity of all (adj,t) 
pairs for term t. Specific terms have low entropy, 
because their adjective distributions are simple. 
For verb-argument distribution, we assume 
that domain specific terms co-occur with selected 
verbs which represent special characteristics of 
terms while general terms are associated with 
multiple verbs. Under this assumption, we make 
use of syntactic dependencies between verbs ap-
pearing in the corpus and their arguments such as 
subjects and objects. For example, ?inventory?1, 
in Figure 2, shows a tendency to be objects of 
specific verbs like ?increase? and ?reduce?. This 
feature was used in (Cimiano et al, 2005) to 
learn concept hierarchy. Inversed specificity of a 
term can be measured by entropy of verb-
argument relations as Eq. (3). 
 
1( ) ( | ) log ( | )
arg
arg
v arg arg
v
Spec t P t v P t v? = ??             (3) 
 
where P(t|varg), the probability that t is argument 
of varg, is estimated as freq(t,varg)/freq(varg). The 
entropy is the average information quantity of all 
(t,varg) pairs for term t. 
Conditional probability of term co-occurrence 
in documents was used in (Sanderson & Croft, 
1999) to build term taxonomy. This statistics is 
based on the assumption that, for two terms, ti 
and tj, ti is said to subsume tj if the following two 
conditions hold, 
 
P(ti|tj) = 1 and P(tj|ti)<1                                     (4) 
 
In other words, ti subsumes tj if the documents 
which tj occurs in are a subset of the documents 
which ti occurs in, therefore ti can be parent of tj 
in taxonomy. Although a good number of term 
pairs are found that adhere to the two subsump-
                                                 
1 ?Inventory? consists of a list of goods and materials held 
available in stock (http://en.wikipedia.org/wiki/Inventory). 
tion conditions, it is noticed that many are just 
failing to be included because a few occurrences 
of the subsumed term, tj, does not co-occur with 
ti. Subsequently, the conditions are relaxed and 
subsume function is defined as Eq. (5). In case of 
P(ti|tj)>P(tj|ti), subsume(ti,tj) returns 1, otherwise 
returns 0. 
 
1  if ( | ) ( | )
( , )
0  otherwise                  
i j j i
i j
P t t P t t
subsume t t
>?= ??
           (5) 
 
We apply this function to calculate term speci-
ficity as shown Eq. (6) where a term is specific 
when it is subsumed by most of other terms. 
Specificity of t is determined by the ratio of 
terms that subsume t over all co-occurring terms. 
 
1
( , )
( )
jj n
coldoc
subsume t t
Spec t
n
? ?= ?                      (6) 
 
where n is number of terms co-occurring terms 
with t. 
Finally, inside-word information is important 
to compute specificity for multiword terms. Con-
sider a term t that consists of two words like t = 
w1w2. Two words, w1 and w2, have their unique 
characteristics and the characteristics are 
summed up to the characteristic of t. Mutual in-
formation is used to estimate the association be-
tween a term and its component words. Let 
T={t1,?,tN} be a set of terms found in a corpus, 
and W={w1,?,wM} be a set of component words 
composing the terms in T. Assume a joint prob-
ability distribution P(ti,wj), probability of wj is a 
component of ti, is given for ti and wj. Mutual 
information between ti and wj compares the prob-
ability of observing ti and wj together and the 
probability of observing ti and wj independently. 
The mutual information represents the reduction 
of uncertainty about ti when wj is observed. The 
summed mutual information between ti and W, as 
in Eq. (7), is total reduction of uncertainty about 
ti when all component words are observed. 
 
( , )
( ) log
( ) ( )
j
i j
in i
w W i j
P t w
Spec t
P t P w?
= ?                            (7) 
 
This equation indicates that wj which is highly 
associated to ti contributes specificity of ti. For 
example, ?debenture bond? is more specific con-
cept than ?financial product?. Intuitively, ?deben-
ture? is highly associated to ?debenture bond? 
43
compared with ?bond? to ?debenture bond? or 
?financial?, ?product? to ?financial product?. 
3 Term Similarity 
We evaluate four statistical and lexical features, 
related to taxonomy learning, in view of term 
similarity. Three statistical features have been 
used in existing taxonomy learning researches. 
(Sanderson & Croft, 1999) used conditional 
probability of co-occurring terms in same docu-
ment in taxonomy learning process as shown in 
Eq. (4). This feature can be used to measure 
similarity of terms. If two terms co-occur in 
common documents, they are semantically simi-
lar to each other. Based on this assumption, we 
can calculate term similarity by comparing the 
frequency of co-occurring ti and tj together and 
the frequency of occurring ti and tj independently, 
as Eq. (8). 
 
2* ( , )
( , )
( ) ( )
i j
coldoc i j
i j
df t t
Sim t t
df t df t
= +
                           (8) 
 
where df(ti,tj) is number of documents in which 
both ti and tj co-occur, df(ti) is number of docu-
ments in which ti occurs.  
(Yamamoto et al, 2005) used adjective pat-
terns to make characteristics vectors for terms in 
Complementary Similarity Measure (CSM). Al-
though CSM was initially designed to extract 
superordinate-subordinate relations, it is a simi-
larity measure by itself. They proposed two CSM 
measures; one is for binary images in which val-
ues in feature vectors are 0 or 1, and the other is 
for gray-scale images in which values in feature 
vectors are 0 through 1. We adapt gray-scale 
measure in similarity calculation, because it 
showed better performance in their research. 
(Cimiano et al, 2005) applied Formal Concept 
Analysis (FCA) to extract taxonomies from a 
text corpus. They modeled the context of a term 
as a vector representing syntactic dependencies. 
Similarity based on verb-argument dependencies 
is calculated using cosine measure as Eq. (9). 
 
2 2
( | ) ( | )
( , )
( | ) ( | )
arg
arg
arg arg
i arg j argv V
v i j
i arg j arg
v V v V
P t v P t v
Sim t t
P t v P t v
?
? ?
= ?? ?
 (9) 
 
where P(t|varg), the probability that t is argument 
of varg, is estimated as freq(t,varg)/freq(varg). 
Above three similarity measures are valid when 
terms, ti and tj, appear in corpus one or more 
times. 
The last similarity measure is based on inside 
information of terms. Because many domain 
terms are multiword terms, component words are 
clues for term similarity. If two terms share 
many common words, they share common char-
acteristics in given domain. For example, four 
words ?asset?, ?current asset?, ?fixed asset? and 
?intangible asset? share characteristics related to 
?asset? as in Figure 2. This similarity measure is 
shown in Eq. (10). 
 
2* ( , )
( , )
| | | |
i j
in i j
i j
cwc t t
Sim t t
t t
= +
                                (10) 
 
where |t| is word count of t, and cwc(ti,tj) is 
common word count in ti and tj. Simin(ti,tj) is 
valid when cwc(ti,tj)>0. Because cwc(ti,tj)=0 for 
most of term pairs, it is difficult to catch reliable 
results for all possible term pairs. 
4 Taxonomy Learning Process 
We model taxonomy learning process as a se-
quential insertion of new terms to current taxon-
omy. New taxonomy starts with empty state, and 
changes to rich taxonomic structure with the re-
peated insertion of terms as depicted in Figure 3. 
Terms to be inserted are sorted by term specific-
ity values. Term insertion based on the increas-
ing order of term specificity is natural, because 
the taxonomy grows from top to down with term 
insertion process in increasing specificity se-
quence. 
 
?
SpecificityHigh Low
Specificity
High
Low
Term sequence
Taxonomy
tnew
tnew
 
Figure 3. Terms are inserted to taxonomy in the 
sequence of specificity 
 
According to above assumption, our system 
selects possible hypernyms of a new term, tnew in 
current taxonomy as following steps: 
 
? Step 1: Select n-most similar terms to tnew 
from current taxonomy 
? Step 2: Select candidate hypernyms of tnew 
from n-most similar terms. Specificity of 
candidate hypernyms is less than that of tnew. 
44
? Step 3: Insert tnew as hyponyms of candidate 
hypernyms 
For example, suppose t2, t4, t5 and t6, are four 
most similar terms to tnew in Figure 4. Two terms 
t2 and t4 are selected as candidate hypernyms of 
tnew, because specificity of the terms is less than 
specificity of tnew. 
 
t1
t2 t3
t4 t5 t6
t7 t8 t9
tnew
t10
Spec(t1) = 1.0
Spec(t3) = 1.5Spec(t2) = 1.5
Spec(t4) = 2.0 Spec(t5) = 3.0
Spec(t7) = 4.0 Spec(t8) = 3.5
Spec(t6) = 2.4
Spec(t9) = 2.5
Spec(tnew) = 2.3
Spec(t10) = 3.0
S
pecificity
High
Low
 
Figure 4. Selection of candidate hypernyms of 
tnew from taxonomy using term specificity and 
similarity 
5 Experiment and Evaluation 
We applied our taxonomy learning method to set 
of terms in existing taxonomy. We removed all 
relations from the taxonomy, and made new 
taxonomic relations among the terms. The 
learned taxonomy was then compared to original 
taxonomy. Our experiment is composed of four 
steps. Firstly, we calculated term specificity us-
ing specificity measures discussed in chapter 2, 
secondly, we calculated term similarity using 
similarity measures described in chapter 3, 
thirdly, we applied the best specificity and simi-
larity features to our taxonomy building process, 
and finally, we evaluated our method and com-
pared with other taxonomy learning methods. 
Finance ontology 2  which was developed 
within the GETESS project (Staab et al, 1999) 
was used in our experiment. We slightly modi-
fied original ontology. We unified different ex-
pressions of same concept to identical expression. 
For example, 'cd-rom drive' and 'cdrom drive' are 
unified as 'cd-rom drive' because the former is 
more usual expression than the latter. We also 
removed terms that are not descendents of 'root' 
node to make the taxonomy have single root 
node. The taxonomy consists of total 1,819 
nodes and 1,130 distinct nodes. Maximum and 
average depths are 15 and 5.5 respectively, and 
                                                 
2 The ontology can be downloaded at http://www.aifb.uni-
karlsruhe.de/WBS/pci/FinanceGoldStandard.isa. P. Cimiano 
and his colleagues added English labels for the originally 
German labeled nodes (Cimiano et al, 2005) 
maximum and average children nodes are 32 and 
3.5 respectively. 
We considered Reuters215783 corpus, over 3.1 
million words in title and body fields. We parsed 
the corpus using Connexor functional depend-
ency parser4 and extracted various statistics: term 
frequency, distribution of adjectives, distribution 
of co-occurring frequency in documents, and 
verb-argument distribution. 
5.1 Term Specificity 
Term specificity was evaluated based on three 
criteria: recall, precision and F-measure. Recall 
is the fraction of the terms that have specificity 
values by the given measuring method. Precision 
is the fraction of relations with correct specificity 
values. F-measure is a harmonic mean of preci-
sion and recall into a single measure of overall 
performance. Precision (Pspec), recall (Rspec), F-
measure (Fspec) is defined as follows: 
 
#     
#    
#   ( , )   
#   ( , )
spec
valid
spec
valid
of terms with specificity
R
of all terms
of R p c with correct specificity
P
of R p c
=
=
  (11) 
 
where Rvalid(p,c) is a valid parent-child relation in 
original taxonomy, and a relation is valid when 
the specificity of two terms are measured by the 
given method. If the specificity of child term, c, 
is larger than that of parent term, p, then the rela-
tion is correct. 
We tested four specificity measuring methods 
discussed in section 2 and the result is shown in 
Table 1. Specadj showed the highest precision as 
we anticipated. Because domain specific terms 
have sufficient information in themselves; they 
are rarely modified by other words in real text. 
However, Specadj showed the lowest recall for 
data sparseness problem. As mentioned above, it 
is hard to collect sufficient adjectives for domain 
specific terms from text. Specvarg showed the 
lowest precision. This result indicates that distri-
bution of verb-argument relation is less corre-
lated to term specificity. Specin showed the high-
est recall because it measures term specificity 
using component words contrary to other meth-
ods. Speccoldoc showed comparable precision and 
recall. 
                                                 
3 
http://www.daviddlewis.com/resources/testcollections/reute
rs21578/ 
4 http://www.connexor.com/ 
45
We harmonized Specin and Specadj to Specin/adj 
as described in (Ryu & Choi, 2005) to take ad-
vantages of both inside and outside information. 
Harmonic mean of two specificity values was 
used in Specin/adj method. Specin/adj showed the 
highest F-measure because precision was higher 
than that of Specin and recall was equal to that of 
Specin. 
 
Table 1. Precision, recall and F-measure for term 
specificity 
Method Precision Recall F-measure
Specadj 0.795 0.609  0.689 
Specvarg 0.663 0.702  0.682 
Speccoldoc 0.717 0.702  0.709 
Specin 0.728 0.907  0.808 
Specin/adj 0.731 0.907  0.810 
5.2 Term Similarity 
We evaluated similarity measures by comparing 
with taxonomy based similarity measure. (Bu-
danitsky & Hirst, 2006) calculated correlation 
coefficients (CC) between human similarity rat-
ings and the five WordNet based similarity 
measures. Among the five computational meas-
ures, (Leacock & Chodorow, 1998)?s method 
showed the highest correlation coefficients, even 
though all of the measures showed similar rang-
ing from 0.74 to 0.85. This result means that tax-
onomy based similarity is highly correlated to 
human similarity ratings. We can indirectly 
evaluate our similarity measures by comparing to 
taxonomy based similarity measure, instead of 
direct comparison to human rating. If applied 
similarity measure is qualified, the calculated 
similarity will be highly correlated to taxonomy 
based similarity. Leacock and Chodorow pro-
posed following formula for computing the 
scaled semantic similarity between terms t1 and t2 
in taxonomy. 
 
1 2
1 2
( , )
( , ) log
2 max ( )LC
t Taxonomy
len t t
Sim t t
depth t
?
= ? ?
             (12) 
 
where the denominator includes the maximum 
depth of given taxonomy, and len(t1, t2) is num-
ber of edges in the shortest path between word t1 
and t2 in the taxonomy.  
Besides CC with ontology based similarity 
measures, recall of a similarity measures is also 
important evaluation factor. We defined recall of 
similarity measure, RSim, as the fraction of the 
term pairs that have similarity values by the 
given measuring method as Eq. (13). 
 
#     
#     Sim
similarity measured term pairs
R
all possible term pairs
=           (13) 
 
We also defined F-measure for a similarity 
measure, Fsim, as harmonic means of CC and Rsim. 
Because CC is a kind of precision, Fsim is overall 
measure of precision and recall. 
We calculated term similarity between all pos-
sible term pairs in finance ontology using the 
measures described in section 3. Additionally we 
introduced new similarity measure Simin/varg 
which is combined similarity of Simvarg and Simin. 
Simvarg and Simin between two terms are harmo-
nized to Simin/varg. We also calculated SimLC 
based on finance ontology, and calculated CC 
between SimLC and results of other measures. 
Figure 5 shows variation of CC and recall as 
threshold of similarity changes from 0.0 to 1.0 
for five similarity measures. Threshold is directly 
proportional to CC and inversely proportional to 
recall in ideal case. We normalized all similarity 
values to [0.0, 1.0] in each measure. CC grows as 
threshold increases in Simcoldoc and Simvarg as we 
expected. CC of CSM measure, Simcsm, increased 
as threshold increased and decreased when 
threshold is over 0.6. For example two terms ?as-
set? and ?current asset? are very similar to each 
other based on SimLC measure, because edge 
count between two terms is one in finance ontol-
ogy. The former can be modified many adjec-
tives such as ?intangible?, ?tangible?, ?new? and 
?estimated?, while the latter is rarely modified by 
other adjectives in corpus because it was already 
extended from ?asset? by adding adjective ?cur-
rent?. Therefore, semantically similar terms do 
not always have similar adjective distributions. 
CC between Simin and SimLC showed high curve 
in low threshold, but downed as threshold in-
creased. Similarity value above 0.6 is insignifi-
cant, because it is hard to be over 0.6 using Eq. 
(10). For example, similarity between ?executive 
board meeting? and ?board meeting? is 0.8, the 
maximum similarity in our test set. The average 
of inside-word similarity is 0.41. 
Simvarg showed higher recall than other meas-
ures. This means that verb-argument relation is 
more abundant than other features in corpus. 
SimIn showed the lowest recall because we could 
get valid similarity using Eq. (10). Simvarg 
showed higher F-measure when threshold is over 
0.2. This result illustrate that verb-argument rela-
tion is adequate feature to similarity calculation. 
46
The combined similarity measure, Simin/varg, 
complement shortcomings of SimIn and Simvarg. 
SimIn showed high CC but low recall. Contrarily 
Simvarg showed low CC but high recall. Simin/varg 
showed the highest F-measure. 
5.3 Taxonomy learning 
In order to evaluate our approach we need to as-
sess how good the automatically learned tax-
onomies reflect a given domain. The goodness is 
evaluated by the similarity of automatically 
learned taxonomy to reference taxonomy. We 
used (Cimiano et al, 2005)?s ontology evaluation 
method in which lexical recall (LRTax), precision 
(PTax) and F-measure (FTax) of learned taxonomy 
are defined based on the notion of taxonomy 
overlap. LRTax is defined as the ratio of number 
of common terms in learned taxonomy and refer-
ence taxonomy over number of terms in refer-
ence taxonomy. PTax is defined as ratio of taxon-
omy overlap of learned taxonomy to reference 
taxonomy. FTax is harmonic mean of LRTax and 
PTax. 
 
0.0
0.2
0.4
0.6
0.8
1.0
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
Threshold
C
C
0.00
0.04
0.08
0.12
0.16
0.20
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
Threshold
F
 m
ea
su
re
Sim(coldoc) Sim(CSM) Sim(varg)
Sim(In) Sim(In/Varg)
0.00
0.04
0.08
0.12
0.16
0.20
0.24
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
Threshold
Re
ca
ll
 
Figure 5 Correlation coefficient between SimLC 
and other similarity measures. Recall and F-
measure of similarity measures 
We generated four taxonomies, Tcoldoc, Tcsm, 
Tfca, Tspec/sim, using four taxonomy learning meth-
ods: term co-occurring method, CSM method, 
FCA method and our method. We applied Spe-
cin/adj in specificity measuring and Simin/varg in 
similarity calculation because they showed the 
highest F-measure. In our method, the most 
probable one term was selected as hypernym of 
newly inserted term in each learning step.  
Figure 6 shows variations of lexical recall, 
precision and F-measure of four methods as 
threshold changes. Threshold in each method 
represent different information to each other. 
Threshold in Tcsm is variation of CSM values. 
Threshold in Tcoldoc is variation of probability of 
two terms co-occur in a document. Threshold in 
Tfca is normalized frequency of contexts. Thresh-
old in Tspec/sim, is variation of similarity. 
Tspec/sim showed the highest lexical recall. 
Lexical recall is tightly related to recall in simi-
larity measures. Simin/varg showed the highest re-
call in similarity measures. Tfca and Tcsm showed 
higher precision than other taxonomies. It is as-
sumed that  precision  of  taxonomy  depends  on 
 
0
0.2
0.4
0.6
0.8
1
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
Threshold
Le
xi
ca
l R
ec
al
l
0
0.2
0.4
0.6
0.8
1
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
Threshold
F-
M
ea
su
re
CSM COLDOC SPEC/SIM FCA
0
0.2
0.4
0.6
0.8
1
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
Threshold
P
re
ci
si
on
 
Figure 6. Lexical recall, precision and F-measure 
of taxonomy learning methods 
 
47
the precision of specificity measures and the CC 
of similarity measures. In actual case, Simvarg 
showed the most plausible curve in CC and Spe-
cadj showed the highest precision in specificity. 
Verb-argument relation and adjective-term rela-
tion are used in FCA and CSM methods respec-
tively. Tspec/sim and Tcoldoc showed higher F-
measure curve than other two taxonomies due to 
high lexical recall. Although our method showed 
plausible F-measure, it showed the lowest preci-
sion. So other combination of similarity and 
specificity measures are needed to improve pre-
cision of learned taxonomy. 
6 Conclusion 
We have presented new taxonomy learning 
method with term similarity and specificity taken 
from domain-specific corpus. It can be applied to 
different domains as it is; and, if we have a syn-
tactic parser available, to different languages. We 
analyzed the features used in previous researches 
in view of term specificity and similarity. In this 
analysis, we found that the features embed the 
characteristics of both conditions. 
Compared to previous approaches, our method 
has advantages in that we can use different fea-
tures for term specificity and similarity. It makes 
easy to analyze errors in taxonomy learning step, 
whether the wrong relations are caused by speci-
ficity errors or by similarity errors. The main 
drawback of our method, as it is now, is that the 
effect of wrong located terms in upper level 
propagates to lower levels.  
Until now, researches on automatic ontology 
learning especially taxonomic relation showed 
very low precision. Human experts? intervention 
is inevitable in automatic learning process to 
make applicable taxonomy. Future work is to 
make new model where human experts and sys-
tem work interactively in ontology learning 
process in order to balance cost and precision. 
Reference 
S. Caraballo, E. Charniak. 1999. Determining the 
Specificity of Nouns from Text. Proceedings of the 
1999 Joint SIGDAT Conference on Empirical 
Methods in Natural Language Processing and Very 
Large Corpora, pp. 63-70 
P. Cimiano, A. Hotho, S.Staab. 2005. Learning Con-
cept Hierarchies from Text Corpora using Formal 
Concept Analysis. Journal of AI Research, Vol. 24, 
pp. 305-339 
M. Hearst. 1992. Automatic Acquisition of Hypo-
nyms from Large Text Corpora. Proceedings of the 
14th International Conference on Computational 
Linguistics 
L. Iwanska, N. Mata and K. Kruger. 2000. Fully 
automatic acquisition of taxonomic knowledge 
from large corpora of texts. In Iwanska, L. & 
Shapiro, S. (Eds.), Natural Language Processing 
and Knowledge Processing, pp. 335-345, 
MIT/AAAI Press. 
E. Yamamoto, K. Kanzaki and H. Isahara. 2005. Ex-
traction of Hierarchies Based on Inclusion of Co-
occurring Words with Frequency Information. 
Proceedings of 9th International Joint Conference 
on Artificial Intelligence, pp. 1160-1167 
A. Burgun, O. Bodenreider. 2001. Aspects of the 
Taxonomic Relation in the Biomedical Domain, 
Proceedings of International Conference on For-
mal Ontology in Information Systems, pp. 222-233 
Mark Sanderson and Bruce Croft. 1999. Deriving 
concept hierarchies from text. Proceedings of the 
22th Annual ACM S1GIR Conference on Research 
and Development in Information Retrieval, pp. 
206-213, 1999 
Karen Sparck Jones. 1972. Exhausitivity and Speci-
ficity Journal of Documentation Vol. 28, Num. 1, 
pp. 11-21 
S.K.M. Wong, Y.Y. Yao. 1992. An Information-
Theoretic Measure of Term Specificity, Journal of 
the American Society for Information Science, Vol. 
43, Num. 1. pp.54-61 
ISO 704. 2000. Terminology work-Principle and 
methods. ISO 704 Second Edition 
A. Aizawa. 2003. An information-theoretic perspec-
tive of tf-idf measures. Journal of Information 
Processing and Management, vol. 39 
Alexander Budanitsky, Graeme Hirst. 2006 Evaluat-
ing WordNet-based Measures of Lexical Semantic 
Relatedness. Computational Linguistics. Vol. 32 
NO. 1, pp. 13-47(35) 
Claudia Leacock, Martin Chodorow. 1998. Combin-
ing local context and WordNet similarity for word 
sense identification. In Christian Fellbaum, editor, 
WordNet: An Electronic Lexical Database. The 
MIT Press, pp. 265-283 
Pum-Mo Ryu, Key-Sun Choi. 2005. An Information-
Theoretic Approach to Taxonomy Extraction for 
Ontology Learning, In P. Buitelaar et al (eds.), On-
tology Learning from Text: Methods, Evaluation 
and Applications, Vol. 123, Frontiers in Artificial 
Intelligence and Applications, IOS Press 
Farid Cerbah. 2000. Exogeneous and Endogeneous 
Approaches to Semantic Categorization of Un-
known Technical Terms. Proceedings of the 18th 
International Conference on Computational Lin-
guistics, vol. 1, pp. 145-151 
48
Proceedings of the 7th Workshop on Asian Language Resources, ACL-IJCNLP 2009, pages 179?186,
Suntec, Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Word Segmentation Standard in Chinese, Japanese and Korean 
 
Key-Sun Choi 
KAIST 
Daejeon Korea 
kschoi@kaist.ac.kr 
Hitoshi Isahara 
NICT 
Kyoto Japan 
isahara@nict.go.jp 
Kyoko Kanzaki
NICT 
Kyoto Japan 
kanzaki@nict.go.jp
Hansaem Kim
National Inst. 
Korean Lang.
Seoul  Korea
thesis00@korea.kr
Seok Mun Pak 
Baekseok Univ. 
Cheonan Korea 
smpark@bu.ac.kr 
Maosong Sun 
Tsinghua Univ.
Beijing China 
sms@tsinghua.edu.cn
 
Abstract 
Word segmentation is a process to divide a 
sentence into meaningful units called ?word 
unit? [ISO/DIS 24614-1]. What is a word 
unit is judged by principles for its internal in-
tegrity and external use constraints. A word 
unit?s internal structure is bound by prin-
ciples of lexical integrity, unpredictability 
and so on in order to represent one syntacti-
cally meaningful unit. Principles for external 
use include language economy and frequency 
such that word units could be registered in a 
lexicon or any other storage for practical re-
duction of processing complexity for the fur-
ther syntactic processing after word segmen-
tation. Such principles for word segmentation 
are applied for Chinese, Japanese and Korean, 
and impacts of the standard are discussed. 
1 Introduction 
Word segmentation is the process of dividing of 
sentence into meaningful units. For example, 
?the White House? consists of three words but 
designates one concept for the President?s resi-
dence in USA. ?Pork? in English is translated 
into two words ?pig meat? in Chinese, Korean 
and Japanese. In Japanese and Korean, because 
an auxiliary verb must be followed by main verb, 
they will compose one word unit like ?tabetai? 
and ?meoggo sipda? (want to eat). So the word 
unit is defined by a meaningful unit that could be 
a candidate of lexicon or of any other type of 
storage (or expanded derived lexicon) that is use-
ful for the further syntactic processing. A word 
unit is more or less fixed and there is no syntactic 
interference in the inside of the word unit. In the 
practical sense, it is useful for the further syntac-
tic parsing because it is not decomposable by 
syntactic processing and also frequently occurred 
in corpora.  
There are a series of linguistic annotation 
standards in ISO: MAF (morpho-syntactic anno-
tation framework), SynAF (syntactic annotation 
framework), and others in ISO/TC37/SC4 1 . 
These standards describe annotation methods but 
not for the meaningful units of word segmenta-
tion. In this aspect, MAF and SynAF are to anno-
tate each linguistic layer horizontally in a stan-
dardized way for the further interoperability. 
Word segmentation standard would like to rec-
ommend what word units should be candidates to 
be registered in some storage or lexicon, and 
what type of word sequences called ?word unit? 
should be recognized before syntactic processing. 
In section 2, principles of word segmentation 
will be introduced based on ISO/CD 24614-1. 
Section 3 will describe the problems in word 
segmentation and what should be word units in 
each language of Chinese, Japanese and Korean. 
The conclusion will include what could be 
shared among three languages for word segmen-
tation. 
2 Word Segmentation: Framework and 
Principles 
Word unit is a layered pre-syntactical unit. That 
means that a word unit consists of the smaller 
word units. But the maximal word unit is fre-
quently occurred in corpora under the constraints 
that the syntactic processing will not refer the 
internal structure of the word unit 
Basic atoms of word unit are word form, mor-
pheme including bound morpheme, and non-
lexical items like punctuation mark, numeric 
string, foreign character string and others as 
shown in Figure 1. Usually we say that ?word? is 
lemma or word form. Word form is a form that a 
lexeme takes when used in a sentence. For ex-
ample, strings ?have?, ?has?, and ?having? are 
word forms of the lexeme HAVE, generally dis-
tinguished by the use of capital letters. [ISO/CD 
24614-1] Lemma is a conventional form used to 
represent a lexeme, and lexeme is an abstract 
unit generally associated with a set of word 
forms sharing a common meaning. 
                                                 
1 Refer to http://www.tc37sc4.org/ for documents 
MAF, SynAF and so on. 
179
 
Figure 1. Configuration of Word Unit 
 
BNF of word unit is as follows: 
<word unit> ::= <word form> | <morpheme> | 
<non lexical items> | <word unit>, 
where <word unit> is recursively defined be-
cause a longer word unit contains smaller word 
units. 
Bunsetsu in Japanese is the longest word unit, 
which is an example of layered maximized pre-
syntactical unit. Eojeol in Korean is a spacing 
unit that consists of one content word (noun, 
verb, adjective or adverb) plus a sequence of 
functional elements. Such language-specific 
word units will be described in section 3.  
Principles for word segmentation will set the 
criteria to validate each word unit, to recognize 
its internal structure and non-lexical word unit, to 
be a candidate of lexicon, and other consistency 
to be necessitated by practical applications for 
any text in any language. The meta model of 
word segmentation will be explained in the 
processing point of view, and then their prin-
ciples of word units in the following subsections.  
2.1 Metamodel of Word Segmentation 
A word unit has a practical unit that will be later 
used for syntactic processing. While the word 
segmentation is a process to identify the longest 
word unit and its internal structure such that the 
word unit is not the object to interfere any syn-
tactic operation, ?chunking? is to identify the 
constituents but does not specify their internal 
structure. Syntactic constituent has a syntactic 
role, but the word unit is a subset of syntactic 
constituent. For example, ?blue sky? could be a 
syntactic constituent but not a word unit. Figure 
2 shows the meta model of word segmentation. 
[ISO CD 24614-1]  
2.2 Principles of Word Unit Validation 
Principles for validating a word unit can be ex-
plained by two perspectives: one is linguistic one 
and the other is processing-oriented practical 
perspective.   
In ISO 24614-1, principles from a linguistic 
perspective, there are five principles: principles 
of (1) bound morpheme, (2) lexical integrity, (3) 
unpredictability, (4) idiomatization, and (5) un-
productivity.  
First, bound morpheme is something like ?in? 
of ? inefficient? . The principle of bound mor-
pheme says that each bound morpheme plus 
word will make another word. Second, principle 
of lexical integrity means that any syntactic 
processing cannot refer the internal structure of 
word (or word unit). From the principle, we can 
say that the longest word unit is the maximal 
meaningful unit before syntactic processing. 
Third, another criterion to recognize a word is 
the principle of unpredictability. If we cannot 
infer the real fact from the word, we consider it 
as a word unit. For example, we cannot image 
what is the colour of blackboard because some is 
green, not black. [ISO 24614-1] The fourth prin-
ciple is that the idiom should be one word, which 
could be a subsidiary principle that follows the 
principle of unpredictability. In the last principle, 
unproductivity is a stronger definition of word; 
there is no pattern to be copied to generate an-
other word from this word formation. For exam-
ple, in ????  (white vegetable) in Chinese, 
there is no other coloured vegetable like ?blue 
vegetable.? 
Another set of principles is derived from the 
practical perspective. There are four principles: 
frequency, Gestalt, prototype and language 
economy. Two principles of frequency and lan-
guage economy are to recognize the frequent 
occurrence in corpora. Gestalt and prototype 
principles are the terms in cognitive science 
about what are in our mental lexicon, and what 
are perceivable words.  
Principle of language economy is to say about 
very practical processing efficiency: ?if the in-
clusion of a word (unit) candidate into the lex-
icon can decrease the difficulty of linguistic 
analysis for it, then it is likely to be a word 
(unit).? 
Gestalt principle is to perceive as a whole. ?It 
supports some perceivable phrasal compounds 
into the lexicon even though they seem to be free 
combinations of their perceivable constituent 
parts,? [ISO/CD 24614-1] where the phrasal 
compound is frequently used linguistic unit and 
its meaning is predictable from its constituent 
elements. Similarly, principle of prototype pro-
180
vides a rationale for including some phrasal 
compounds into lexicon, and phrasal compounds 
serve as prototypes in a productive word forma-
tion pattern, like ?apple pie? with the pattern 
?fruit + pie? into the lexicon.  
 
 
Figure 2. Meta model of word segmentation proess 
[ISO/CD 24614-1] 
2.3 Principles for Word Unit Formation 
As a result of word segmentation of sentence, we 
will get word units. These principles will de-
scribe the internal structure of word unit. They 
have four principles: granularity, scope maximi-
zation of affixations, scope maximization of 
compounding, and segmentation for other strings. 
In the principle of granularity, a word unit has its 
internal structure, if necessary for various appli-
cation of word segmentation.  
Principles of scope maximization of affixa-
tions and compounding are to recognize the 
longest word unit as one word unit even if it is 
composed of stem + affixes or compound of 
word units. For example, ?unhappy? or ?happy? 
is one word unit respectively. ?Next generation 
Internet? is one word unit. The principle of seg-
mentation for other strings is to recognize non-
lexical strings as one word unit if it carries some 
syntactic function, for example, 2009 in ?Now 
this year is 2009.?  
3 Word Segmentation for Chinese, Jap-
anese and Korean 
If the word is derived from Chinese characters, 
three languages have common characteristics. If 
their word in noun consists of two or three Chi-
nese characters, they will be one word unit if 
they are ?tightly combined and steadily used.? 
Even if it is longer length, it will be a word unit 
if it is fixed expression or idiom. But if the first 
character is productive with the following num-
eral, unit word or measure word, it will be seg-
mented. If the last character is productive in a 
limited manner, it forms a word unit with the 
preceding word, for example, ????? (Tokyo 
Metropolis), ?8?? (August) or ????? (acce-
lerator). But if it is a productive suffix like plural 
suffix and noun for locality, it is segmented in-
dependently in Chinese word segmentation rule, 
for example, ???|?? (friends), ???|??? 
(north of the Yangtzi River ) or ???|?? (on 
the table) in Chinese. They may have different 
phenomena in each language. 
Negation character of verb and adjective is 
segmented independently in Chinese, but they 
form one word unit in Japanese. For example, 
?yasashikunai? (?????, not kind) is one 
word unit in Japanese, but ??|?? (not to write),  
??| ?? (cannot),  ??|??? (did not research) 
and ?? | ??? (not completed) will be seg-
mented independently in Chinese. In Korean, 
?chinjeolhaji anhta? (???? ??, not kind) 
has one space inserted between two eojeols but it 
could be one word unit. ?ji anhta? makes nega-
tion of adjectival stem ?chinjeolha?.  
We will carefully investigate what principles 
of word units will be applied and what kind of 
conflicts exists. Because the motivation of word 
segmentation standard is to recommend what 
word units should be registered in a type of lex-
icon (where it is not the lexicon in linguistics but 
any kind of practical indexed container for word 
units), it has two possibly conflicting principles. 
For example, principles of unproductivity, fre-
quency, and granularity could cause conflicts 
because they have different perspectives to de-
fine a word unit.  
3.1 Word Segmentation for Chinese 
For convenience of description, the specification 
in this section follows the convention that classi-
fies words into 13 types: noun, verb, adjective, 
pronoun, numeral, measure word, adverb, prepo-
sition, conjunction, auxiliary word, modal word, 
exclamation word, imitative word. 
3.1.1 Noun 
There is word unit specification for common 
nouns as follows: 
- Two-character word or compact two-character 
noun phrase, e.g., ??(beef) ??(steel) 
181
- Noun phrase with compact structure, if violate 
original meaning after segmentation, e.g., ??
?? (Active power) 
- Phrase consisting of adjective with noun, e.g., 
?? (green leave) 
- The meaning changed phrase consisting of ad-
jective, e.g., ???(young wife)  
- Prefix with noun, e.g., ??(elder brother) ?
? (old eagle) ??? (nonmetal)  ???
(ultrasonic) 
- Noun segmentation unit with following postfix 
(e.g. ? ? ? ? ? ? ? ? ?), e.g., ???
(scientist) 
- Noun segmentation unit with prefix and postfix, 
e.g., ???(superconductance) 
- Time noun or phrase, e.g., ??(May), 11 ? 
42 ? 8 ?(forty two minutes and eight seconds 
past eleven), ??(the day before yesterday), 
??(First day of a month in the Chinese lunar 
calendar )   
But the followings are independent word 
units for noun of locality (e.g., ??|? (on the 
table), ??|?? (north of the Yangtzi River)), 
and the ??? suffix referring to from a plural of 
front noun (e.g., ?? ?(friends) ) except ??
??, ?????(pals),  ?????(guys), etc. Prop-
er nouns have similar specification. 
3.1.2 Verb 
The following verb forms will be one word unit 
as: 
- Various forms of reiterative verb, e.g., ??
(look at), ????(come and go) 
- Verb?object structural word, or compact and 
stably used verb phrase, e.g., ??(meeting)  ?
?(dancing) 
- Verb?complement structural word (two-
character), or stably used Verb-complement 
phrase (two-character), e.g., ??(improve) 
- Adjective with noun word, and compact, and 
stably used adjective with noun phrase, e.g., ?
?(make trouble) ,  ??(talk nonsense) 
- Compound directional verb, e.g., ??(go out)  
??(come in). 
But the followings are independent word 
units: 
- ?AAB, ABAB? or ?A? A, A? A, A?? A?, 
e.g., ??|??(have a discuss), ?|?|? (have 
a good chat) 
- Verb delimited by a negative meaning charac-
ters, e.g., ?|?(not to write)   ?|?(cannot)    
?|??(did not research)    ?|??(not com-
pleted) 
- ?Verb + a negative meaning Chinese character 
+  the same verb" structure, e.g., ??|?|?(say 
or not say)?? 
- Incompact or verb?object structural phrase with 
many similar structures, e.g., ? |?(Eat fish)    
?|??(learn skiing) 
- ?2with1? or ?1with2? structural verb- comple-
ment phrase, e.g., ??|?(clean up), ?|??
(speak clearly),  ??|??(explain clearly) 
- Verb-complement word for phrase, if inserted 
with ?? or ??, e.g., ?|?|? (able to knock 
down), and compound directional verb of direc-
tion, e.g., ?|?|?(able to go out) 
- Phrase formed by verb with directional verb, 
e.g., ?|?(send), ?|?|?(run out) 
- Combination of independent single verbs with-
out conjunction, e.g., ?|?(cover with), ?|?,  
?|? (listen, speaking, read and write) 
- Multi-word verb without conjunction, e.g., ?
?|??(investigate and research) 
3.1.3 Adjective 
One word unit shall be recognized in the follow-
ing cases: 
- Adjective in reiterative form of ?AA, AABB, 
ABB, AAB, A+"?"+AB?, e.g., ??(big), ?
???(careless), except the adjectives in rei-
terative form of ?ABAB?, e.g., ?? |??
(snowy white)     
- Adjective phrase in from of ?? A? B??
A? B?? A? B?? A? B?? A? B?, 
e.g., ????(wholeheartedly) 
- Two single-character adjectives with word fea-
tures varied, ?? (long-short)  ?? (deep-
shallow)  ??(big-small) 
- Color adjective word or phrase, e.g., ??(light 
yellow)   ???(olive green) 
But the followings are segmented as indepen-
dent word units: 
- Adjectives in parataxis form and maintaining 
original adjective meaning, e.g., ? |???
(size), ?? |??(glory) 
- Adjective phrase in positive with negative form 
to indicate question, e.g., ??| ?| ??(easy 
or not easy), except the brachylogical one like 
????(easy or not). 
3.1.4 Pronoun 
The followings shall be one word unit: 
182
- Single-character pronoun with ???, e.g.,?? 
(we) 
- ??????? with unit word ??? or ???
????????, e.g., ??(this) 
- Interrogative adjective or phrase, e.g., ??
(how many) 
But, the following will be independent word 
units: 
- ???????  with numeral , unit word or 
noun word segmentation unit, e.g., ? |? ?
(these 10 days), ?| ?(that person) 
- Pronoun of ???????????????, 
etc. shall be segmented from followed measure 
word or noun, e.g., ?| ? (each country), ?| 
?(each type). 
3.1.5 Numeral 
The followings will be one word unit: 
- Chinese digit word, e.g., ?????????
???(180,040,723) 
- ???? percent in fractional number, e.g., ? 
???(third fifth) 
- Paratactic numerals indicating approximate 
number, e.g., ?? ??(eight or nine kg) 
On the other hand, Independent word unit cas-
es are as follows: 
- Numeral shall be segmented from measure 
word, e.g., ?| ?(three) 
- Ordinal number of ???  shall be segmented 
from followed numeral, e.g., ? ? (first) 
- ?????????????, used after adjec-
tive or verb for indicating approximate number. 
3.1.6 Measure word 
Reiterative measure word and compound meas-
ure word or phrase is a word unit, e.g., ??
(every year), ?? man/year. 
3.1.7 Adverb 
Adverb is one word unit. But????????
???, etc. acting as conjunction shall be seg-
mented, e.g., ? ? ? ?(sweet yet savory). 
3.1.8 Preposition 
It is one word unit. For example, ??(be born 
in), and  ????(according to the regulations). 
3.1.9 Conjunction 
Conjunction shall be deemed as segmentation 
unit. 
3.1.10 Auxiliary word 
Structural auxiliary word ????????? 
and tense auxiliary word ??????? are one 
word unit, e.g., ? |? |?  (his book), ? |?
(watched). But the auxiliary word ??? shall be 
segmented from its followed verb, e.g., ?  ?
(what one thinks).  
3.1.11 Modal word 
It is one word unit, e.g., ???? (How are 
you?). 
3.1.12 Exclamation word 
Exclamation word shall be deemed as segmenta-
tion unit. For example, ??????? (How 
beautiful it is!) 
3.1.13 Imitative word 
It is one word unit like ???? (tinkle).  
3.2 Word Segmentation for Japanese 
For convenience of description, the specification 
in this section follows the convention that classi-
fies words into mainly 10 types: meishi (noun), 
doushi (verb), keiyoushi (adjective), rentaishi 
(adnominal noun: only used in adnominal usage), 
fukushi (adverb), kandoushi (exclamation), set-
suzoushi (conjunction), setsuji (affix), joshi (par-
ticle), and jodoushi (auxiliary verb). These parts 
of speech are divided into more detailed classes 
in terms of grammatical function. 
The longest "word segmentation" corresponds 
to ?Bunsetsu? in Japanese. 
3.2.1 Noun 
When a noun is a member constituting a sentence, 
it is usually followed by a particle or auxiliary 
verb (e.g. ???? (neko_ga) which is composed 
from ?Noun + a particle for subject marker?). 
Also, if a word like an adjective or adnominal 
noun modifies a noun, then a modifier (adjec-
tives, adnominal noun, adnominal phrases) and a 
modificand (a noun) are not segmented. 
3.2.2 Verb 
A Japanese verb has an inflectional ending. The 
ending of a verb changes depending on whether 
it is a negation form, an adverbial form, a base 
form, an adnominal form, an assumption form, or 
an imperative form. Japanese verbs are often 
used with auxiliary verbs and/or particles, and a 
verb with auxiliary verbs and/or particles is con-
sidered as a word segmentation unit (e.g. ???
??? ? (aruki_mashi_ta) is composed from 
?Verb + auxiliary verb for politeness + auxiliary 
verb for tense?). 
3.2.3 Adjective 
A Japanese adjective has an inflectional ending. 
Based on the type of inflectional ending, there 
183
are two kinds of adjectives, "i_keiyoushi" and 
"na_keiyoushi". However, both are treated as 
adjectives. 
In terms of traditional Japanese linguistics, 
?keiyoushi? refers to ?i_keiyoushi?(e.g. ???, 
utsukushi_i) and ?keiyoudoushi?(e.g. ??? , 
shizuka_na) refers to ?na_keiyoushi.? In terms of 
inflectional ending of ?na_keiyoushi,? it is some-
times said to be considered as ?Noun + auxiliary 
verb (da)?. 
The ending of an adjective changes depending 
on whether it is a negation form, an adverbial 
form, a base form, an adnominal form, or an as-
sumption form. Japanese adjectives in predica-
tive position are sometimes used with auxiliary 
verbs and/or particles, and they are considered as 
a word segmentation unit. 
3.2.4 Adnominal noun 
An adnominal noun does not have an inflectional 
ending; it is always used as a modifier. An ad-
nominal noun is considered as one segmentation 
unit. 
3.2.5 Adverb 
An adverb does not have an inflectional ending; 
it is always used as a modifier of a sentence or a 
verb. It is considered as one segmentation unit. 
3.2.6 Conjunction 
A conjunction is considered as one segmentation 
unit. 
3.2.7 Exclamation 
An exclamation is considered as one segmenta-
tion unit. 
3.2.8 Affix 
A prefix and a suffix used as a constituent of a 
word should not be segmented as a word unit. 
3.2.9 Particle 
Particles can be divided into two main types. 
One is a case particle which serves as a case 
marker. The other is an auxiliary particle which 
appears at the end of a phrase or a sentence. 
Auxiliary particles represent a mood and a 
tense. 
Particles should not be segmented from a word. 
A particle is always used with a word like a noun, 
a verb, or an adjective, and they are considered 
as one segmentation unit. 
3.2.10 Auxiliary verb 
Auxiliary verbs represent various semantic func-
tions such as a capability, a voice, a tense, an 
aspect and so on. An auxiliary verb appears at 
the end of a phrase or a sentence. Some linguist 
consider ??? (da), which is Japanese copura, as 
a specific category such as ???(hanteishi).   
An auxiliary verb should not be segmented 
from a word. An auxiliary verb is always used 
with a word like a noun, a verb, or an adjective, 
and is considered as one segmentation unit. 
3.2.11 Idiom and proverb 
Proverbs, mottos, etc. should be segmented if 
their original meanings are not violated after 
segmentation. For example: 
Kouin yano  gotoshi 
noun  noun+particle auxiliary verb 
time  arrow  like (flying) 
Time flies fast. 
3.2.12 Abbreviation 
An abbreviation should not be segmented. 
3.3 Word Segmentation for Korean 
For convenience of description, the specification 
in this section follows the convention that classi-
fies words into 12 types: noun, verb, adjective, 
pronoun, numeral, adnominal, adverb, exclama-
tion, particle, auxiliary verb, auxiliary adjective, 
and copula. The basic parts of speech can be di-
vided into more detailed classes in terms of 
grammatical function. Classification in this paper 
is in accord with the top level of MAF. 
In addition, we treat some multi-Eojeol units 
as the word unit for practical purpose. Korean 
Eojeol is a spacing unit that consists of one con-
tent word (like noun, verb) and series of func-
tional elements (particles, word endings). Func-
tional elements are not indispensable. Eojeol is 
similar with Bunsetsu from some points, but an 
Eojeol is recognized by white space in order to 
enhance the readability that enables to use only 
Hangul alphabet in the usual writing.  
3.3.1 Noun 
When a noun is a member constituting a sentence, 
it is usually followed by a particle. (e.g. 
???_?? (saja_ga, ?a lion is?) which is com-
posed from ?Noun + a particle for subject mark-
er?). Noun subsumes common noun, proper noun, 
and bound noun.  
If there are two frequently concatenated Eoje-
ols that consist of modifier (an adjective or an 
adnominal) and modificand (a noun), they can be 
one word unit according to the principle of lan-
guage economy. Other cases of noun word unit 
are as follows: 
1) Compound noun that consists of two more 
nouns can be a word unit. For example, 
184
???? (son_mok, ?wrist?) where son+mok 
= ?hand?+?neck?. 
2) Noun phrase that denotes just one concept 
can be a word unit. For example, ???? 
??? (yesul_ui jeondang, ?sanctuary of the 
arts? that is used for the name of concert 
hall). 
3.3.2 Verb  
A Korean verb has over one inflectional endings. 
The endings of a verb can be changed and at-
tached depending on grammatical function of 
verb (e.g. ??/??/?/?/?/?/?? (break 
[+emphasis] [+polite] [+past] [+conjectural] final 
ending [+polite]). Compound verb (verb+verb, 
noun+verb, adverb+verb) can be a segmentation 
unit by right. For example, ?????? (dola-
ga-da, ?pass away?) is literally translated into 
?go+back? (verb+verb).  ????? (bin-na-da, 
?be shiny?) is derived from ?light + come out? 
(noun+verb). ?????? (baro-jap-da, ?cor-
rect?) is one word unit but it consists of 
?rightly+hold? (adverb+verb).  
3.3.3 Adjective 
A Korean adjective has inflectional endings like 
verb. For example, in ???/?/?/?/?/?? 
(pretty [+polite] [+past] [+conjectural] final end-
ing [+polite]), one adjective has five endings. 
Compound adjective can be a segmentation unit 
by right. (e.g. ????? (geom-buk-da, ?be 
blackish red?)) 
3.3.4 Adnominal  
An adnominal is always used as a modifier for 
noun. Korean adnominal is not treated as noun 
unlike Japanese one. (e.g. ?? ?? (sae jip, ?new 
house?)? which consist of ?adnominal + noun?). 
3.3.5 Adverb 
An adverb does not have an inflectional ending; 
it is always used as a modifier of a sentence or a 
verb. In Korean, adverb includes conjunction. It 
is considered as one segmentation unit. Com-
pound adverb can be a segmentation unit by right. 
Examples are ???? (bam-nat, ?day and night?), 
and ???? (gotgot, ?everywhere? whose literal 
translation is ?where where?). 
3.3.6 Pronoun 
A pronoun is considered as one segmentation 
unit. Typical examples are ??? (na, ?I?), ??? 
(neo, ?you?), and ???? (uri, ?we). Suffix of 
plural ??? (deul, ?-s?) can be attached to some 
of pronouns in Korean. (e.g. ????? (neohui-
deul, ?you+PLURAL?), ???? (geu-deul, ?they? 
= ?he/she+PLURAL?)). 
3.3.7 Numeral 
A numeral is considered as one segmentation 
unit: e.g. ???? (hana, ?one?), ???? (cheojjae, 
?first?). Also figures are treated as one unit like 
?2009?? (the year 2009). 
3.3.8 Exclamation 
An exclamation is considered as one segmenta-
tion unit. 
3.3.9 Particle 
Korean particles can not be segmented from a 
word just like Japanese particles. A particle is 
always used with a word like a noun, a verb, or 
an adjective, but it is considered as one segmen-
tation unit. 
Particles can be divided into two main types. 
One is a case particle that serves as a case marker. 
The other is an auxiliary particle that appears at 
the end of a phrase or a sentence. Auxiliary par-
ticle represents a mood and a tense. 
3.3.10 Auxiliary verb 
A Korean auxiliary verb represents various se-
mantic functions such as a capability, a voice, a 
tense, an aspect and so on.  
Auxiliary verb is only used with a verb plus 
endings with special word ending depending on 
the auxiliary verb. For example, ???? (boda, 
?try to?), an auxiliary verb has the same inflec-
tional endings but it should follow a main verb 
with a connective ending ??? (eo) or ??? (?go?).  
Consider ?try to eat? in English where ?eat? is a 
main verb, and ?try? is an auxiliary verb with 
specialized connective ?to?. In this case, we need 
two Korean Eojeols that corresponds to ?eat + 
to? and ?try?. Because ?to? is a functional ele-
ment that is attached after main verb ?eat?, it 
constitutes one Eojeol. It causes a conflict be-
tween Eojeol and word unit. That means every 
Eojeol cannot be a word unit. What are the word 
units and Eojeols in this case? There are two 
choices: (1) ?eat+to? and ?try?, (2) ?eat?+ ?to 
try?. According to the definition of Eojeol, (1) is 
correct for two concatenated Eojeols. But if the 
syntactic processing is preferable, (2) is more 
likely to be a candidate of word units.  
3.3.11 Auxiliary adjective 
Unlike Japanese, there is auxiliary adjective in 
Korean. Function and usage of it are very similar 
to auxiliary verb. Auxiliary adjective is consi-
dered as one segmentation unit. 
185
Auxiliary verb can be used with a verb or ad-
jective plus endings with special word ending 
depending on the auxiliary adjective. For exam-
ple, in ??? ??? (meokgo sipda, ?want to 
eat?), sipda is an auxiliary adjective whose mean-
ing is ?want? while meok is a main verb ?want? 
and go corresponds to ?to?; so meokgo is ?to eat?.  
3.3.12 Copula 
A copula is always used for changing the func-
tion of noun. After attaching the copula, noun 
can be used like verb. It can be segmented for 
processing. 
3.3.13 Idiom and proverb 
Proverbs, mottos, etc. should be segmented if 
their original meanings are not violated after 
segmentation like Chinese and Japanese. 
3.3.14 Ending 
Ending is attached to the root of verb and adjec-
tive. It means honorific, tense, aspect, modal, etc. 
There are two endings: prefinal ending and fi-
nal ending. They are functional elements to 
represent honorific, past, or future functions in 
prefinal position, and declarative (-da) or con-
cessive (-na)-functions in final ending. Ending is 
not a segmentation unit in Korean. It is just a unit 
for inflection. 
3.3.15 Affix 
A prefix and a suffix used as a constituent of a 
word should not be segmented as a word unit. 
4 Conclusion 
Word segmentation standard is to recommend 
what type of word sequences should be identified 
as one word unit in order to process the syntactic 
parsing. Principles of word segmentation want to 
provide the measure of such word units. But 
principles of frequency and language economy 
are based on a statistical measure, which will be 
decided by some practical purpose.  
Word segmentation in each language is 
somewhat different according to already made 
word segmentation regulation, even violating one 
or more principles of word segmentation. In the 
future, we have to discuss the more synchronized 
word unit concept because we now live in a mul-
ti-lingual environment. For example, consider 
figure 3. Its English literal translation is ?white 
vegetable and pig meat?, where ?white vegeta-
ble? (??) is an unproductive word pattern and 
forms one word unit without component word 
units, and ?pig meat? in Chinese means one Eng-
lish word ?pork?. So ?pig meat? (??) is the 
longest word unit in this case. But in Japanese 
and Korean, ?pig meat? in Chinese characters 
cannot be two word units, because each compo-
nent character is not used independently. 
 
Figure 3. Basic segmentation and word segmenta-
tion [ISO/CD 24614-1] 
What could be shared among three languages 
for word segmentation? The common things are 
not so much among CJK. The Chinese character 
derived nouns are sharable for its word unit 
structure, but not the whole. On the other hand, 
there are many common things between Korean 
and Japanese. Some Korean word endings and 
Japanese auxiliary verbs have the same functions. 
It will be an interesting study to compare for the 
processing purpose.  
The future work will include the role of word 
unit in machine translation. If the corresponding 
word sequences have one word unit in one lan-
guage, it is one symptom to recognize one word 
unit in other languages. It could be ?principle of 
multi-lingual alignment.?   
The concept of ?word unit? is to broaden the 
view about what could be registered in lexicon of 
natural language processing purpose, without 
much linguistic representation. In the result, we 
would like to promote such language resource 
sharing in public, not just dictionaries of words 
in usual manner but of word units. 
Acknowledgement 
This work has been supported by ISO/TC37, 
KATS and Ministry of Knowledge Economy 
(ROKorea), CNIS and SAC (China), JISC (Ja-
pan) and CSK (DPRK) with special contribution 
of Jeniffer DeCamp (ANSI) and Kiyong Lee. 
References 
ISO CD24614-1, Language Resource Management ? 
Word segmentation of written texts for monolin-
gual and multilingual information processing ? Part 
1: Basic concepts and general principles, 2009.  
ISO WD24614-2, ? Part 2: Word segmentation for 
Chinese, Japanese and Korean, 2009. 
186
Proceedings of the 6th Workshop on Ontologies and Lexical Resources (Ontolex 2010), pages 48?57,
Beijing, August 2010
Intrinsic Property-based Taxonomic Relation Extraction from Category
Structure
DongHyun Choi and Eun-Kyung Kim and Sang-Ah Shim and Key-Sun Choi
Semantic Web Research Center
KAIST
cdh4696, kekeeo, sashim, kschoi@world.kaist.ac.kr
Abstract
We propose a novel algorithm to ex-
tract taxonomic (or isa/instanceOf ) rela-
tions from category structure by classi-
fying each category link. Previous algo-
rithms mainly focus on lexical patterns of
category names to classify whether or not
a given category link is an isa/instanceOf.
In contrast, our algorithm extracts intrin-
sic properties that represent the definition
of given category name, and uses those
properties to classify each category link.
Experimental result shows about 5 to 18 %
increase in F-Measure, compared to other
existing systems.
1 Introduction
1.1 Problem Description
Taxonomies are a crucial component of many ap-
plications, including document clustering (Hotho
et al, 2003) and database search (Byron et al,
1997). Due to their importance, many studies
have examined methods of extracting taxonomic
relations automatically - either from unstructured
text (Cimiano et al, 2005; Cimiano(2) et al,
2005), or from structured data such as Wikipedia
category structures (Ponzetto and Strube, 2007;
Nastase and Strube, 2008; Suchanek et al, 2007).
Many researchers have attempted to obtain tax-
onomic relations from unstructured text to con-
stuct a taxonomy, but in most cases such a system
shows poor precision and low recall. Approaches
to extracting taxonomic relations from structured
data show relatively high performance, but to ob-
tain a taxonomy these require huge amounts of
structured data. Recently, as large amounts of
structured data such as the infoboxes and category
structures of Wikipedia or DBpedia (Auer et al,
2007) have become available, an obstacle to this
approach has been removed.
Although a category structure does contain
some kind of hierarchical structure, in many cases
it cannot be considered as an isa/instanceOf hier-
archy. For example, the article ?Pioneer 111? on
Wikipedia is categorized under ?Radio frequency
propagation?, which is related to the ?Pioneer 11?
but is obviously not a taxonomical parent of ?Pio-
neer 11?.
In this paper, we propose a method for extract-
ing taxonomic relations from a given category
structure. More precisely, for a category link in
the given category structure, the algorithm deter-
mines whether the link could be considered an
isa/instanceOf relation, or if the link simply rep-
resents a broader term/narrower term/related term
relation. For a given category link <A, B>, in
which A is the upper category name and B is the
lower category/article name, we attempt to get the
definition of B to classify the link. More precisely,
we analyze the upper categories of B from the
given category structure, to get tokens that rep-
resents the definition of B. Once we get the to-
kens, we compare the tokens with the name of A,
to classify the given category link. We call the
tokens that represent the definition of B ?intrin-
sic tokens? of B; a more precise definition will be
presented in section 3.1.
To show the validity of this approach, the algo-
rithm is applied to Wikipedia?s category structure,
1Pioneer 11 was the probe for second mission of the Pio-
neer program (after its sister probe Pioneer 10) to investigate
Jupiter and the outer solar system.
48
to obtain taxonomic relations there. Wikipedia?s
category structure consists of categories, article
titles and links between them. A Wikipedia arti-
cle represents one document, and a category is the
grouping of those articles by non-categorization-
expert users. Each category has its own name,
which is assigned by these users.
Although Wikipedia?s category structure is
built by non-experts, it can be thought of as reli-
able since it is refined by many people, and it con-
tains 35,904,116 category links between 764,581
categories and 6,301,594 articles, making it a per-
fect target for an experimental taxonomic relation
extraction algorithm.
After describing related works in section 2, our
detailed algorithm is proposed in section 3, and its
experimental results are discussed in section 4. In
section 5, we make some conclusions and propos-
als for future work.
2 Related Works
Methods of taxonomic relation extraction can be
divided into two broad categories depending on
the input: unstructured or structured data. The ex-
traction of taxonomic relations from unstructured
text is mainly carried out using lexical patterns on
the text. The Hearst pattern (Hearst, 1992) is used
in many pattern-based approaches, such as Cimi-
ano (2005).
In addition, there has been research that at-
tempted to use existing structured data, like the
Wikipedia category structure or the contents of a
thesaurus. The system of Ponzetto (2007) deter-
mines whether or not the given Wikipedia cate-
gory link is an isa/instanceOf relation by applying
a set of rules to the category names, while Nas-
tase (2008) defined lexical patterns on category
names, in addition to Ponzetto (2007). The YAGO
system (Suchanek et al, 2007) attempts to classify
whether the given article-category link represents
an instanceOf relation by checking the plurality
of the upper category name.
The algorithm proposed in this paper focuses
on the structured data, mainly the category struc-
ture, to gather isa/instanceOf relations. The
system gets a category structure as input, and
classifies each category link inside the category
structure according to whether it represents an
isa/instanceOf relation or not.
3 Algorithm Description
In section 3.1, we introduce the necessary defini-
tions for isa/instanceOf relations and the required
terms to describe the algorithm. In section 3.2,
we will discuss the hypotheses based on the defi-
nitions described in section 3.1. Next, two binary
classification algorithms will be proposed based
on the hypotheses, which will determine whether
the given category link is an isa/instanceOf rela-
tion or not.
3.1 Definitions
To define isa and instanceOf relations, Mi-
zoguchi (2004) introduces the concept of intrin-
sic property and other related concepts, which are
shown in the following definitions 1, 2 and 3:
Definition 1: Intrinsic property. The intrinsic
property of a thing is a property which is essen-
tial to the thing and it loses its identity when the
property changes.
Definition 2: The ontological definition of a
class. A thing which is a conceptualization of a set
X can be a class if and only if each element x of X
belongs to the class X if and only if the intrinsic
property of x satisfies the intensional condition of
X. And, then and only then, <x instanceOf X>
holds.
Definition 3: isa relation. isa relation holds
only between classes. <class A isa class B>
holds iff the instance set of A is a subset of the
instance set of B.
In addition, we define the following terms for
algorithm description:
Definition 4: intrinsic token. Token 2 T is an
intrinsic token of B iff T represents the intrinsic
property of B.
For example, when B is ?Pioneer 11?, the in-
trinsic tokens of B are ?spacecraft?, ?escape3?,
?Jupiter?, etc.
2For example, token is a segmented term in category
names of Wikipedia category structure.
3Since the main purpose of Pioneer 11 is to escape from
the solar system and fly into the deep space, we thought ?es-
cape? is the intrinsic token of ?Pioneer 11?. In the same con-
text, ?spacecraft escaping the solar system? is a taxonomical
parent of ?Pioneer 11?.
49
Definition 5: category link. <A, B> is called
category link iff A is a category of B, and that fact
is explicitly stated in the given category structure.
Consider the example of Wikipedia. If B is
an article, <A, B> is called an article-category
link, and if B is a category, <A, B> is called a
category-category link. The article is a catego-
rized terminal object.
Definition 6: category structure. Category
structure is the collection of category links, its
component categories, and categorized terminal
objects.
Definition 7: upper category set. The upper
category set of B is defined as the set of upper
categories of B up to n step in the given category
structure, and it is expressed as U(B, n).
For example, if the two category links <Jupiter
spacecraft, Pioneer 11> and <Jupiter, Jupiter
spacecraft> exist inside the given category struc-
ture, then Jupiter spacecraft is the element of
U(Pioneer 11, 1), while Jupiter is not.
Figure 1 shows the category structure of
U(Pioneer 11, 3),which we refer to throughout
this paper to explain our algorithm.
3.2 Hypotheses
According to the classical Aristotelian view, cat-
egories are discrete entities characterized by a set
of properties shared by their members. Thus, we
make the following lemmas:
Lemma 1: If some objects are grouped into the
same category, then they share at least more than
one property.
According to definition 2, if x is an instanceOf
X, then the intrinsic property of x satisfies the def-
inition of X. Since the intrinsic property is the
property related to the definition of the object, we
can assume that in most categorization systems,
the intrinsic property is the most frequently shared
property among those objects categorized in the
same category.
Lemma 2. Intrinsic properties are shared most
frequently among objects in a category.
Lemma 2 means that, for example, the intrin-
sic token T of B will show up frequently among
the names of upper categories of B. But lemma
2 does NOT mean that non-intrinsic tokens will
not frequently appear among the upper category
names. For example, the elements of U(Pioneer
11, 3) from the Wikipedia category structure con-
tain the token ?spacecraft? 4 times, but it also
contain token ?technology? 3 times. Therefore,
we cannot directly use the token frequency to de-
termine which one is the intrinsic token: rather,
we make another assumption to get the ?intrinsic
score? for each token.
Lemma 3. Intrinsic tokens co-occur frequently
with other intrinsic tokens.
Lemma 3 means that, if T1 is an intrinsic to-
ken of B, and T2 co-occurs with T1 inside the
upper category names of B, then there is a high
probability that T2 is also an intrinsic token of B.
For example, for the category link<Jupiter space-
craft, Pioneer 11>, if the token ?spacecraft? is an
intrinsic token of ?Pioneer 11?, we can assume
that the token ?Jupiter? is also an intrinsic token
of ?Pioneer 11?. Since some intrinsic tokens that
are appropriate as modifiers are not appropriate as
head words ? for example, if the token ?Jupiter?
is used as a modifier, it will be a good intrinsic
token of ?Pioneer 11?, but if it is used as a head
word, choosing it as the intrinsic token of ?Pio-
neer 11? would be bad choice ? thus, we distin-
guish between intrinsic score as head word, and
intrinsic score as modifier. If the intrinsic score
of token T is high for article/category name B,
then it means the probability is high that T is an
intrinsic token of B. We assumed that only the
co-occurrences as head word and its modifier are
meaningful. Corollary 3-1. If a modifier co-
occurs with a head word, and the head word is
frequently an intrinsic token of an object, then the
modifier is an intrinsic token of the object.
Corollary 3-2. If a head word co-occurs with a
modifier, and the modifier is frequently an intrin-
sic token of an object, then the head word is an
intrinsic token of the object.
3.3 Proposed Algorithm
Based on the hypotheses proposed in section 3.2,
we propose two algorithms to get the intrinsic
score of each token in the following sections. The
first algorithm, a counting-based approach, uses
only lemmas 1 and 2, and it will be shown why
this algorithm will not work. The second algo-
rithm, a graph-based approach, uses all of the hy-
50
Figure 1: category structure of U(Pioneer 11, 3) from Wikipedia.
potheses to solve the problem.
For the given category link <A, B>, the intrin-
sic score of each token will be calculated based
on its frequency inside U(B, n) while separately
counting the token?s intrinsic score as modifiers
and the intrinsic score as head word. We here
propose a scoring mechanism based on the HITS
page ranking algorithm (Kleinberg, 1999): For the
given category link <A, B>, we first construct a
?modifier graph? using U(B, n), and then calcu-
late the intrinsic score for each token in U(B, n)
using the HITS algorithm. After that, the intrinsic
score of each token will be used to calculate the
score of <A, B>. If the score is higher than some
predefined threshold, then<A, B> is classified as
an isa/instanceOf link, and otherwise it is not.
3.3.1 Counting-based Approach
This method utilizes lemmas 1 and 2 to get the
intrinsic score for each token, and then uses the
score to determine whether the given category link
is an isa/instanceOf link or not.
To utilize this approach, we first score each to-
ken from U(B, n) by counting the frequency of
each token from the words of U(B, n). Table 1
shows the score of each token from U(Pioneer, 3)
for figure 1.
For the ?Pioneer 11? article, there are seven
category links in Wikipedia?s category struc-
ture: <1973 in space exploration, Pioneer 11>,
<Inactive extraterrestrial probes, Pioneer 11>,
<Jupiter spacecraft, Pioneer 11>, <Pioneer pro-
Token Score
space 6
exploration 5
spacecraft, probe 4
1973, technology, year, radio, solar,
system, nasa
3
vehicle, radio, program, 1970s,
extraterrestrial, transport, Saturn,
Jupiter
2
escape, inactive, frequency, propa-
gation, pioneer, ...
1
Table 1: Score for each token from U(Pioneer 11,
3)
gram, Pioneer 11>, <Radio frequency propaga-
tion, Pioneer 11>, <Saturn spacecraft, Pioneer
11>, and <Spacecraft escaping the Solar System,
Pioneer 11>, as shown in figure 1. The scores
of each link using a counting-based approach are
acquired by adding the scores for each token in ta-
ble 1 that is matched with single term occurrence
in category names. Table 2 shows the result of
counting-based approach.
Although the link <1973 in space exploration,
Pioneer 11> receives the highest score among
those seven links, obviously the link does not rep-
resent isa/instanceOf relation. This shows that
the counting approach does not guarantee accu-
racy. Table 1 shows that non-intrinsic tokens oc-
cur frequently (such as ?technology? in this exam-
51
Article-Category Links Score
<1973 in space exploration,
Pioneer 11>
3+6+5=14
<Spacecraft escaping the So-
lar System, Pioneer 11>
4+1+3+3=11
<Inactive extraterrestrial
probes, Pioneer 11>,
1+2+4=7
<Saturn spacecraft, Pioneer
11>
2+4=6
<Jupiter spacecraft, Pioneer
11>
2+4=6
<Radio frequency propaga-
tion, Pioneer 11>
2+1+1=4
<Pioneer program, Pioneer
11>
1+2=3
Table 2: Scoring each category links using count-
ing approach
ple). We call this an ?overloaded existence? error.
To solve the problems described above, we apply
Lemma 3, Corollary 3-1 and 3-2 to our calcula-
tion, and propose a second algorithm based on a
graph-based approach, which will be explained in
the next section.
3.3.2 Graph-based Approach
In this section, we propose a graph-based ap-
proach to get the intrinsic score of each token. To
do this, we first construct a modifier graph from
the words of U(B, n) for a given category link<A,
B>, with each node representing a token from the
elements of U(B, n), and each edge representing
the co-occurrence of tokens inside each element
of U(B, n). Next, we apply a well-known graph
analysis algorithm to that graph, and get the in-
trinsic scores for each node. Finally, we use the
score of each node to get the score of the given
category link.
Constructing modifier graph Modifier graph
constructed here is defined as a directed graph,
in which each node represents each token inside
U(B, n), and each edge represents a co-occurrence
as modifier-head relation inside each category
name of U(B, n). Using the subset of U(Pioneer
11, 3), we get the modifier graph of figure 2.4
Figure 2: Modifier graph of the subset of
U(Pioneer 11, 3): {Spacecraft escaping the Solar
System, Jupiter spacecraft, 1973 in space explo-
ration, NASA probes, Saturn}
Calculating Intrinsic score After constructing
the modifier graph, we apply the HITS algorithm
to the modifier graph. Since the HITS algorithm
cannot reflect the weight of edges, a modified ver-
sion of the HITS algorithm (Mihalcea and Tarau,
2005) is adopted:
Authority(Vi) =
?
Vj?In(Vi)
eji ? Hub(Vj) (1)
Hub(Vi) =
?
Vj?Out(Vi)
eij ? Authority(Vj) (2)
In(Vi) represents the set of vertices which has
the outgoing edge to Vi, Out(Vi) represents the
set of vertices which has the incoming edge from
Vi, and eij represents the weight of the edge from
Vi to Vj . The algorithm for calculating the scores
is as follows:
1. Initialize the authority and hub score of each
node to one.
2. Calculate hub score of each node using the
formula 2.
3. Calculate authority score of each node using
the formula 1.
4. Normalize authority & hub score so that the
sum of authority score of every node and the sum
of hub score of every node are one.
4We used the full set of U(B, n) to create the modifier
graph for the full scale of experimentation in section 4.
52
5. Iterate from step 2 until the score of every
node converges.
In the modifier graph, Authority score can be
mapped to the intrinsic score of a node(token) as
a head word, and Hub score can be mapped to the
intrinsic score of a node(token) as a modifier.
Scoring Category Link Now, we can score the
input category link. The score of category link
<A, B> is given as follows:
Score(< A,B >)
= Authority(h) +
?
a in mod(A)
Hub(a) (3)
Here, Score(< A,B >) represents the final
score of category link <A, B>, h represents the
head word of A, and mod(A) represents the set
of modifiers of A. Since the score of head word
and modifiers are calculated based on the upper
categories of B, this formula can integrate both
meaning of A and B to classify whether the link is
isa/instanceOf. Table 3 shows the scores of seven
article-category links from table 2, calculated us-
ing the graph-based approach.
Article-Category Links Score
<Spacecraft escaping the Solar
System, Pioneer 11>
0.5972
<1973 in space exploration, Pio-
neer 11>
0.4018
<Jupiter spacecraft, Pioneer 11> 0.2105
<Saturn spacecraft, Pioneer 11> 0.2105
<Inactive extraterrestrial probes,
Pioneer 11>,
0.0440
<Radio frequency propagation, Pi-
oneer 11>
0.0440
<Pioneer program, Pioneer 11> 0.0132
Table 3: Scoring each category links using graph-
based approach
The link <Spacecraft escaping the Solar Sys-
tem, Pioneer 11> gets the highest score, while
the link <1973 in space exploration, Pioneer
11>, which got the highest score using counting-
based approach, gets the second place. That
proves the algorithm?s effectiveness for distin-
guishing isa/instanceOf link from other non-
isa/instanceOf links. But there is still a problem -
although the first-ranked link is a isa/instanceOf
link, the second-ranked is not, while the third
and fourth-ranked links (<Jupiter spacecraft, Pi-
oneer 11>, <Saturn spacecraft, Pioneer 11> are
isa/instanceOf links. To get a better result, we
propose four additional modifications in the next
secton.
3.4 Additional Modifications to the
Graph-based Approach
To better reflect the category structure and the
property of category names to the scoring mech-
anism, the following four modifications can be
made. Each of these modification could be ap-
plied independently to the original algorithm de-
scribed in section 3.3.2.
Authority Impact Factor (I). In most cases,
a category name contains only one head word,
while it contains 2 or more modifiers. As Formula
(3) is just the linear sum of the hub scores of each
modifier and the authority score of the head word,
the resultant score is more affected by hub score,
because the number of modifiers is normally big-
ger than the number of head words. To balance
the effect of hub score and authority score, we in-
troduce authority impact factor I:
Score(< A,B >)
= I ? Authority(h) +
?
a in mod(A)
Hub(a) (4)
The authority impact factor is defined as the aver-
age number of modifiers in the elements of U(B,
n), since normally each category name contains
only one head word.
Dummy Node (D). There are some category
names that contain only one head word and no
modifier, thus making it impossible to create the
modifier graph.5 Thus, for such category names
we introduce dummy nodes to include their infor-
mation into the modifier graph. In figure 3, you
can observe the introduction of the dummy node
?dummy0?.
5For example, in figure 2, we cannot find node ?Saturn?
while U(Pioneer 11, 3) contains category name ?Saturn?
53
Figure 3: Modifier graph of the subset of
U(Pioneer 11, 3), with dummy node.
Category Distance Factor (C). We define the
category distance between category/article A and
B as the minimum number of category links re-
quired to reach B from A by following the cate-
gory links. Category distance factor C of a cat-
egory name A from U(B, n) is the reverse of the
category distance between A and B. We assumed
that, if the distance between A and B is higher,
then it is less probable for A to have the intrinsic
property of B. Based on this assumption, category
distance factor C of category name A is multiplied
by the edge score of an edge generated by cate-
gory name A.
Figure 4 shows the modifier graph of figure 2
that applies the category distance factor. Since
the category distance between ?Pioneer 11? and
?NASA probe? is two, the score of edge (NASA,
probe) is 1/2 = 0.5.
Figure 4: Modifier graph of the subset of
U(Pioneer 11, 3), with category distance factor.
Modifier Number Normalization Factor (W).
In the algorithm of building a modifier graph, the
head word of a category name with many mod-
ifiers has the advantage over the head word of a
category name with few modifiers, as if a cate-
gory name contains n modifiers it will generate
n edges incoming to its head word. To overcome
this problem, we defined the modifier number nor-
malization factor W for each category name: it is
defined as the reverse of the number of modifiers
in the category name, and it is multiplied by the
edge score of an edge, generated by the category
name, of the modifier graph. Figure 5 shows the
modifier graph of figure 2 with the modifier num-
ber normalization factor. Since the category name
?Spacecraft escaping the Solar System? has three
modifiers, the scores of edge (escape, Pioneer 11),
(solar, Pioneer 11) and (system, Pioneer 11) are
1/3 = 0.33.
Figure 5: Modifier graph of the subset of
U(Pioneer 11, 3), with modifier number normal-
ization factor.
Removing roleOf Relation (E). To distinguish
the roleOf relation from taxonomic relation,we in-
troduce a new E. This feature simply classify the
link <A, B> as non-instanceOf if category name
A has endings like -er, -ers, -or, -ors, -ian, -ians.
Since only the terminal node can represent the
name of person in category structure, we applied
this feature to classify only article-category links.
One of the example from Wikipedia which should
be judged as roleOf relation is <La Liga foot-
baller, Cristiano Ronaldo>.
After applying above four modifications, we get
the result in table 4. Now, top 3 links all represent
instanceOf links.
54
Article-Category Links Score
<Spacecraft escaping the Solar
System, Pioneer 11>
2.1416
<Jupiter spacecraft, Pioneer 11> 2.1286
<Saturn spacecraft, Pioneer 11> 2.1286
<1973 in space exploration, Pio-
neer 11>
0.0241
<Pioneer program, Pioneer 11> 0.0062
<Inactive extraterrestrial probes,
Pioneer 11>,
0.0026
<Radio frequency propagation, Pi-
oneer 11>
0.0021
Table 4: Scoring each category links using graph-
based approach with four modifications.
4 Implementation
We implemented a combinatory system that com-
bines the algorithm suggested by this paper with
existing lexical pattern-based algorithms. More
precisely, we set two parameters? and ?, in which
? has a consistently higher value than ?. If score
of the given category link, which is retrieved by
the proposed system, is higher than ?, it is classi-
fied as isa/instanceOf. If the score is higher than ?
but lower or equal to ?, the system uses an exist-
ing lexical pattern-based algorithm to classify the
link. If the score is lower than or equal to ?, it is
classified as not isa/instanceOf.
To test the system, we used Wikipedia?s
category structure, which contains 1,160,248
category-category links and 15,778,801 article-
category links between 505,277 categories and
6,808,543 articles. We extract category links from
the Wikipedia category structure and annotate
them to construct the test corpus. During the pro-
cess of choosing category links, we intentionally
removed category links with names containing
any of the following words: ?stub?, ?wikiproject?,
?wikipedia?, ?template?, ?article?, ?start-class?,
?category?, ?redirect?, ?mediawiki?, ?user?, ?por-
tal?, ?page?, and ?list?. These words are normally
used to represent Wikipedia maintenance pages.
After we remove the links described before, we
randomly choose 3,951 category-category links
and 1,688 article-category links. Two annotators
worked separately to annotate whether or not the
given link is an isa/instanceOf link, and in the
event of conflict they would discuss the case and
make a final decision.
We carried out experiments on category-
category link set and article-category link set sep-
arately, since their characteristics are different.
We assumed that the taxonomic relation in a
category-category link is an isa link, while the tax-
onomic relation in an article-category link is an in-
stanceOf link. To acquire the upper category set,
we set n=3 throughout the experiment. For head
word extraction, the method of Collins (1999) is
used, and for lemmatization we used the Lingpipe
toolkit (Alias-i, 2008).
4.1 Experiments on category-category link
We divided the 3,951 category-category links into
two equally-sized sets, and used one set as a train-
ing set and the other one as a test set. The training
set was used to identify the ? and ? values for
isa link classification: in other words, the ? and
? values that showed the best performance when
applied to training set were selected as the actual
parameters used by the system. As Wikipedia?s
category structure contains a huge number of cat-
egory links, precision is more important than re-
call. As recall cannot be ignored, we chose the
parameters that gave the highest precision on the
training set, while giving a recall of at least 0.7.
Also, we carried out experiments on three base-
line systems.The first one determined every link
as an isa link. The second one applied the head
word matching rule (M) only, which says that for
category-category link<A, B>, if the head words
of A and B are the same, then <A, B> should
be classified as an isa link. The third one applies
the method of Ponzetto (P) (Ponzetto and Strube,
2007). The ruleset of Ponzetto includes Head
word matching rule, Modifier-head word match-
ing rule(Ex. <Crime, Crime Comics>: Head
word of ?Crime? and modifier of ?Crime Comics?
matches: Not isa), and the plurality rule used by
YAGO system(Explained at the next chapter)).
Table 5 shows the baseline results, the results
of existing systems, and our best results on the
test set. Usage of authority score is represented
as A, and usage of hub score is represented as H.
Also,we did experiments on all possible combina-
55
tion of features A, H, I, D, C, W, M, P. For exam-
ple, Comb(AHICDM) means that we used feature
A, H, I, C, D to construct the modifier graph and
score the category link, and for those whose score
is between ? and ? we used head word matching
rule to classify them. At the table, P stands for
Precision, R stands for Recall, and F stands for
F-measure.
Setting P R F
Baseline1 0.7277 1.0 0.8424
Baseline2(M) 0.9480 0.6335 0.7595
Baseline3(P) 0.9232 0.6516 0.7640
Comb1(AHM) 0.9223 0.7350 0.8181
Comb2(AHP) 0.8606 0.7211 0.7847
Comb3(AHICM) 0.9325 0.7302 0.8190
Table 5: Experimental result on test set of
category-category links: Baseline vs. System best
result
As you can observe, the precision of head-word
matching (M) is high, meaning that in many cases
the head word represents the intrinsic property.
Also, its recall shows that for category-category
links, at least more than half of the categories are
categorized using the intrinsic property of the ob-
jects grouped within them, which strongly sup-
ports lemma 2 in section 3.2. The comparison of
setting M and AHM, P and AHP shows that the
intrinsic-property based approach increases recall
of the existing system about 7-10 %, at the cost of
of 2-6 % precision loss. This shows that, rather
than looking only at the given category link and
analyzing patterns on its name, by gathering in-
formation from the upper category set, we were
able to significantly increase recall. However, it
also shows that some ?garbage? information is in-
troduced through the upper category set, resulting
in a 2-6 % precision loss. The best system shows
about a 8-10 % increase in recall, with compara-
bly good precision compared to the two baseline
systems.
4.2 Experiments on article-category link
In a similar manner to the experiments on
category-category links, we divided the 1,688
article-category links into two equally-sized sets,
and used one set as a training set and the other one
as a test set. The training set is used to determine
the parameters for instanceOf link classification.
The parameter setting procedure was the same as
in the experiments on category-category links, ex-
cept that we used the article-category links for the
procedure. In this experiment, we also adapted
three baseline systems. The first system classi-
fies every link as an instanceOf link, the second
system adapts the head word matching rule (M),
and the third system applies the rule from Yago
(Y) (Suchanek et al, 2007), which states that for
article-category link <A, B>, if A is plural then
the link could be classified as an instanceOf rela-
tion.
Setting P R F
Baseline1 0.5261 1.0 0.6894
Baseline2(M) 0.7451 0.0856 0.1535
Baseline3(Y) 0.6036 0.5315 0.5653
Comb1(AHY) 0.6082 0.6718 0.6381
Comb2(ADWEY) 0.7581 0.7410 0.7494
Table 6: Experimental result on test set of article-
category links on some settings
Table 6 shows the baseline results and the best
results of the combinatory system. As you can ob-
serve from the above table, M (head word match-
ing rule) does not work well in article-category
links, although its precision is still high or compa-
rable to that of other methods. Since in most cases
an article represents one instance, in many cases
they have their own name, making the recall of
the head word matching rule extremely low. Also,
the combination system 1 (AHY) shows compa-
rable precision with Y but 14 % higher in reall,
resulting 7 % increse in F-Measure.The best sys-
tem shows about 18 % increase in F-measure, es-
pecially 15 % precision increase and 21 % recall
increase compared to YAGO system.
5 Conclusion and Future work
In this paper, we explored a intrinsic token-based
approach to the problem of classifying whether a
category link is a taxonomic relation or not. Un-
like previous works that classify category links,
we acquired the definition of a lower category
56
name by extracting intrinsic tokens and using
them to score the given category link, rather than
by applying predefined lexical rules to the cat-
egory link. Our intrinsic token-based approach
leads to a significant improvement in F-measure
compared to previous state-of-the-art systems.
One possible future direction for research is au-
tomatic instance population, by using those ex-
tracted intrinsic tokens and gathering taxonomic
relations from the category structure.
Acknowledgments
This work was supported by the Industrial
Strategic Technology Development Program
(10035348, Development of a Cognitive Planning
and Learning Model for Mobile Platforms) funded
by the Ministry of Knowledge Economy(MKE,
Korea).
References
Soumen C. Byron, Byron Dom, Rakesh Agrawal, and
Prabhakar Raghavan. 1997. Using taxonomy, dis-
criminants, and signatures for navigating in text
databases. Proceedings of the international confer-
ence on very large data bases, 446?455.
Philipp Cimiano, Andreas Hotho, and Steffen Staab.
2005. Learning Concept Hierarchies from Text Cor-
pora using Formal Concept Analysis. Journal of Ar-
tificial Intelligence Research, 24:305?339.
Philipp Cimiano, Aleksander Pivk, Lars Schmidt-
Thieme, and Steffen Staab. 2005. Learning Tax-
onomic Relations from Heterogeneous Sources of
Evidence. Ontology Learning from Text: Methods,
Evaluation and Applications, 59?73.
Marti A. Hearst. 1992. Automatic Acquisition of Hy-
ponyms from Large Text Corpora. Proceedings of
the 14th conference on Computational linguistics,
2:539?545.
Andreas Hotho, Steffen Staab, and Gerd Stumme.
2003. Ontologies improve text document cluster-
ing. Proceedings of the IEEE International Confer-
ence on Data Mining, 541?544.
Jon M. Kleinberg. 1999. Authoritative sources in
a hyperlinked environment. Journal of the ACM,
46(5):604?632
Simone P. Ponzetto, and Michael Strube. 2007. Deriv-
ing a Large Scale Taxonomy from Wikipedia. Pro-
ceedings of the AAAI07.
Vivi Nastase, and Michael Strube. 2008. Decoding
Wikipedia category names for knowledge acquisi-
tion . Proceedings of the AAAI08.
Riichiro Mizoguchi. 2004. Part 3: Advanced course
of ontological engineering. New Generation Com-
puting, 22(2): 193?220
Rada Mihalcea, and Paul Tarau. 2005. A Language In-
dependent Algorithm for Single and Multiple Doc-
ument Summarization. Proceedings of IJCNLP
2005.
Ian Niles, and Adam Pease. 2003. Linking Lexi-
cons and Ontologies: Mapping WordNet to the Sug-
gested Upper Merged Ontology. Proceedings of the
IEEE International Conference on Information and
Knowledge Engineering.
Soeren Auer, Christian Bizer, Georgi Kobilarov, Jens
Lehmann, Richard Cyganiak, and Zachary Ives.
2007. DBpedia: A Nucleus for a Web of
Open Data. Lecture Notes in Computer Science,
4825/2007:722?735.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. YAGO: A Core of Semantic
Knowledge Unifying WordNet and Wikipedia Pro-
ceedings of the 16th international conference on
World Wide Web, 697?706.
Michael Collins. 1999. Head-driven Statistical Mod-
els for Natural Language Parsing. University of
Pennsylvania PhD Thesis.
Alias-i. 2008. LingPipe 3.9.1. http://alias-
i.com/lingpipe.
57
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 78?88,
Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational Linguistics
Korean Treebank Transformation for Parser Training
DongHyun Choi
Dept. of Computer Science
KAIST
Korea
cdh4696@world.kaist.ac.kr
Jungyeul Park
Les Editions
an Amzer Vak
France
park@amzer-vak.fr
Key-Sun Choi
Dept. of Computer Science
KAIST
Korea
kschoi@cs.kaist.ac.kr
Abstract
Korean is a morphologically rich language in
which grammatical functions are marked by
inflections and affixes, and they can indicate
grammatical relations such as subject, object,
predicate, etc. A Korean sentence could be
thought as a sequence of eojeols. An eo-
jeol is a word or its variant word form ag-
glutinated with grammatical affixes, and eo-
jeols are separated by white space as in En-
glish written texts. Korean treebanks (Choi
et al, 1994; Han et al, 2002; Korean Lan-
guage Institute, 2012) use eojeol as their fun-
damental unit of analysis, thus representing
an eojeol as a prepreterminal phrase inside
the constituent tree. This eojeol-based an-
notating schema introduces various complex-
ity to train the parser, for example an en-
tity represented by a sequence of nouns will
be annotated as two or more different noun
phrases, depending on the number of spaces
used. In this paper, we propose methods to
transform eojeol-based Korean treebanks into
entity-based Korean treebanks. The methods
are applied to Sejong treebank, which is the
largest constituent treebank in Korean, and the
transformed treebank is used to train and test
various probabilistic CFG parsers. The experi-
mental result shows that the proposed transfor-
mation methods reduce ambiguity in the train-
ing corpus, increasing the overall F1 score up
to about 9 %.
1 Introduction
The result of syntactic parsing is useful for many
NLP applications, such as named entity recogni-
tion (Finkel and Manning, 2009), semantic role la-
beling (Gildea and Jurafsky, 2002), or sentimental
analysis (Nasukawa and Yi, 2003). Currently most
of the state-of-the-art constituent parsers take statis-
tical parsing approach (Klein and Manning, 2003;
Bikel, 2004; Petrov and Klein, 2007), which use
manually annotated syntactic trees to train the prob-
abilistic models of each consituents.
Even though there exist manually annotated Ko-
rean treebank corpora such as Sejong Treebank (Ko-
rean Language Institute, 2012), very few research
projects about the Korean parser, especially using
phrase structure grammars have been conducted. In
this paper, we aim to transform the treebank so that it
could be better used as training data for the already-
existing English constituent parsers.
Most of Korean treebank corpora use eojeols as
their fundamental unit of analysis. An eojeol is
a word or its variant word form agglutinated with
grammatical affixes, and eojeols are separated by
white space as in English written texts (Choi et al,
2011). Figure 1 is one of the example constituent
tree from the Sejong Treebank. As can be observed,
an eojeol is always determined as a prepretermi-
nal phrase 1. But this kind of bracketing guideline
could cause ambiguities to the existing algorithms
for parsing English, because: (1) English does not
have the concept of ?eojeol?, and (2) an eojeol
can contain two or more morphemes with different
grammatical roles. For example, Korean case par-
1A node is a prepreterminal if all the children of this node
are preterminals (Part-Of-Speech tags such as NNP and JKG).
Preterminal is defined to be a node with one child which is itself
a leaf (Damljanovic et al, 2010).
78
Figure 1: An example constituent tree and morphological analysis result from the Sejong treebank
ticles (?josa?) are normally written inside the same
eojeol with their argument nouns, but the whole eo-
jeol is always considered as a prepreterminal noun
phrase in the Korean treebank, as can be seen in the
eojeol Ungaro-GA. Considering that the case parti-
cles in Korean play important role in determining
the syntactic structure of a sentence, this could cause
loss of information during the training phase. More-
over, Emanuel Ungaro is considered as two different
noun phrases, because they simply belong to the two
different eojeols (that is, a space exists between eo-
jeols Emanuel and Ungaro-GA).
In this paper, we propose methods to refine the
Sejong treebank which is currently the largest Ko-
rean treebank corpus. The methods are aimed at de-
creasing the ambiguities during the training phase
of parsers, by separating phrases which are inte-
grated into the same prepreterminal phrase due to
the reason that they happen to be in the same eojeol,
and integrating phrases into the same prepretermi-
nal phrase which are separated because they hap-
pen to be in different eojeols. The refined datasets
are trained and tested against three state-of-the-art
parsers, and the evaluation results for each dataset
are reported.
In section 2, the work about Korean parsers are
briefly introduced. Sejong treebank is described
with more detailed explanation in section 3, while
the methods to transform the treebank are introduced
in section 4. In section 5 the evaluation results of the
transformed treebank using the three existing state-
of-the-art parsers are introduced with an error report,
and we discuss conclusions in section 6.
2 Related Work
There were some trials to build Korean constituent
parsers, but due to the lack of appropriate corpus
those trials were not able to acheive a good re-
sult. (Smith and Smith, 2004) tried to build a Ko-
rean parser by bilingual approach with English, and
achieved labeled precision/recall around 40 % for
Korean. More recently, (Park, 2006) tried to extract
tree adjoining grammars from the Sejong treebank,
and (Oh et al, 2011) build a system to predict a
phrase tag for each eojeol.
Due to the partial free word order and case pari-
cles which can decide the grammatical roles of noun
phrases, there exist some works to build statistical
dependency parsers for Korean. (Chung, 2004) pre-
sented a dependency parsing model using surface
contextual model. (Choi and Palmer, 2011) con-
verted the Sejong treebank into the dependency tree-
bank, and applied the SVM algorithm to learn the
dependency model.
79
NNG General noun IC Interjection JKQ Quotational CP XSV Verb DS
NNP Proper noun MM Adnoun JX Auxiliary PR XSA Adjective DS
NNB Bound noun MAG General adverb JC Conjunctive PR XR Base morpheme
NP Pronoun MAJ Conjunctive adverbEP Prefinal EM SN Number
NR Numeral JKS Subjective CP EF Final EM SL Foreign word
VV Verb JKC Complemental CP EC Conjunctive EM SH Chinese word
VA Adjective JKG Adnomial CP ETN Nominalizing EM NF Noun-like word
VX Auxiliary predicateJKO Objective CP ETM Adnominalizing EMNV Verb-like word
VCP Copula JKB Adverbial CP XPN Noun prefix NA Unknown word
VCN Negation adjective JKV Vocative CP XSN Noun DS SF,SP,SS,SE,SO,SW
Table 1: POS tags used in Sejong treebank (CP: case particle, EM: ending marker, DS: derivational suffix, PR: particle,
SF SP SS SE SO: different types of punctuations, SW: currency symbols and mathematical symbols. Table borrowed
from (Choi and Palmer, 2011))
Apart from the Sejong Treebank, there are few
other Korean treebanks available. The KAIST tree-
bank (Choi et al, 1994) contains constituent trees
about approximately 30K sentences from newspa-
pers, novels and textbooks. Also, the Penn Ko-
rean Treebank (Han et al, 2002) contains 15K
constituent trees constructed from the sentences of
newswire and military domains. The proposed
methods are evaluated using the Sejong treebank be-
cause it is the most recent and the largest Korean
treebank among those which is currently available.
3 Sejong Treebank
The Sejong treebank is the largest constituent
treebank in Korean. It contains approximately
45K manually-annotated constituent trees, and their
sources cover various domains including newspa-
pers, novels and cartoon texts. Figure 1 shows an
example of the Sejong constituent tree.
The tree consists of phrasal nodes and their func-
tional tags as described in table 2. Each eojeol
could contain one or more morphemes with different
POS tags (Table 1 shows the POS tagset). In most
cases, eojeols are determined by white spaces. As
stated in its bracketing guidelines, the Sejong tree-
bank uses eojeols as its fundamental unit of analy-
sis 2. This means that an eojeol is always treated as
one prepreterminal phrase. This could cause confu-
sions to the training system, because an eojeol could
contain many morphemes which have very different
2The bracketing guidelines could be requested from the Se-
jong project, but available only in Korean
grammatical roles, as can be seen in the example
of Ungaro-GA - word Ungaro is a noun, where the
nominative case particle GA suggests that this eojeol
is used as a subject.
Table 2 shows phrase tags and functional tags
used to construct the Sejong treebank. Some phrases
are annotated with functional tags to clarify their
grammatical role inside the sentence. There are
three special phrase tags beside those in table 2:
X indicates phrases containing only case particles
or ending markers, L and R indicate left and right
parenthesis.
Phrase-level tags Functional tags
S Sentence SBJ Subject
Q Quotative clause OBJ Object
NP Noun phrase CMP Complement
VP Verb phrase MOD Modifier
VNP Copula phrase AJT Adjunct
AP Adverb phrase CNJ Conjunctive
DP Adnoun phrase INT Vocative
IP Interjection phrasePRN parenthetical
Table 2: Phrase tags used in Sejong treebank.
4 Transforming Methods: from
Eojeol-based to Entity-based
In this section, we describe the methods to transform
the annotation schema of the Korean treebank from
eojeol-based to entity-based using the examples of
the Sejong treebank.
4.1 Method 1: POS Level Preprocessing
Before starting the actual transforming process, the
system first detects emails, phone numbers and dates
80
based on their unique POS patterns. If the system
detects a sequence of morphemes matching with one
of predefined POS patterns inside an eojeol, then it
groups those morphemes into one entity and tags it
as a noun. This procedure aims to reduce the ambi-
guity of the corpus by reducing many miscellaneous
mrophemes which in fact forms one phone num-
ber, email address or date information into one en-
tity. Figure 2 shows an example of an eojeol whose
five morphemes toghether represent one date, and its
transformation result.
Figure 2: Example of an eojeol containing date: five mor-
phemes are merged into one morpheme representing date.
Also, the morphemes representing chinese char-
acters (POS: SH) and other foreign characters (POS:
SL) are considered as nouns, since they are normally
used to rewrite Korean nouns that have their foreign
origin such as Sino-Korean nouns.
4.2 Method 2: Detecting NPs inside an Eojeol
Although an eojeol is considered to be one prepreter-
minal phrase as a whole, many eojeols contain sep-
arated noun components inside them. For exam-
ple, a noun phrase Ungaro-GA in Figure 3 con-
sists of a separated noun component Ungaro in it,
plus josa GA. The system separates noun compo-
nents from other endings and case particles, creates
a new phrase containing those words and tags it as
an NP. By doing so, the boundaries of the NP are
more clarified - before transforming prepreterminal
NPs could contain case particles and endings, but
after the transformation it is not possible. Also the
internal syntactic structures of phrases are revealed,
providing more information to the parser.
4.3 Method 3: Finding Arguments of Josa
In this step, the system tries to find out the actual ar-
gument of each josa. For example, in figure 4 the
Figure 3: Detecting NP inside an eojeol: Case of a verb
phrase
actual argument of the nominative josa GA is the
whole person name Emanuel Ungaro, not only Un-
garo. The system tries to find out the actual argu-
ment of each josa by using a rather simple heuristic:
1. Traverse the constituent parse tree in bottom-up, right-to-
left manner.
2. If a phrase node is NP, its parent is also NP, and it directly
dominates josa(s), then:
(a) Create a new NP.
(b) Attach the node to that NP, except the josa(s).
(c) Attach all the other children of the parent node to the
newly-created NP.
(d) Remove all the children of the parent, and attach the
new NP and remaining josa part to the parent node.
3. After the procedure ends, find and remove redundant NPs,
if exist.
Figure 4: Example of applying the transformation heuris-
tic
Method 3 is dependent on method 2, since method
2 first determines boundary of NPs which do not in-
clude any case particles.
4.4 Method 4: Integrating a Sequence of
Nouns into One NP
Some of entities represented as sequences of nouns
are considered as two or more separated noun
81
phrases since their components belong to the dif-
ferent eojeols. This could be problematic because
an entity could sometimes be written without any
whitespace between its component nouns. Figure 5
shows one of the case: person name Emanuel Un-
garo is considered as two separated NPs since there
exists a whitespace between a noun Emanual and a
noun Ungaro. In this step, we aim to solve this prob-
lem.
Figure 5: Integrating sequence of nouns representing one
entity into one prepreterminal noun phrase
The system finds out an NP which has two NP
children which dominates only the noun pretermi-
nal children. If the system finds such an NP, then it
removes NP children and attaches their children di-
rectly to the found NP. Figure 5 shows an application
example of the method.
This method is dependent on method 3, since this
method assumes that an NP with its parent also NP
does not have any case particles - which cannot be
hold if method 3 is not applied.
4.5 Method 5: Dealing with Noun
Conjunctions
The system tries to enumerate the noun conjunc-
tions, rather than expressing those conjunctions in
binary format. Current Sejong treebank expresses
noun conjunctions in binary format - that is, to ex-
press the constituent tree for noun conjunctions, the
nonterminal node has one NP child on its left which
contains information about the first item of the con-
junction, and the rest of conjunctions are expressed
on the right child. Figure 63 shows an example of
the Sejong constituent tree expressing the noun con-
junctions, and its transformed version.
3Mike-WA (CNJ) Speaker-GA (NOM) Jangchak-DOI-UH
IT-DA. (?Microphone and speaker are installed.?)
Figure 6: Enumerating Noun Conjunctions
By converting noun conjunctions into rather the
?enumerated? forms, two benefits could be gained:
first, the resultant constituent tree will resemble
more to the Penn-treebank constituent trees. Since
most of the existing English parsers are trained on
the Penn Treebank, we can expect that the enumer-
ated form of conjunctions will more ?fit? to those
parsers. Second, the conjunctions are expressed in
much more explicit format, so the human users can
more easily understand the conjunctive structures in-
side the constituent trees.
4.6 Method 6: Re-tagging Phrase Tags
In this step, the system re-tags some of phrase tags
to clarify their types and to decrease training ambi-
guities. For example, a noun phrase with and with-
out case particles should be distinguished. The sys-
tem re-tags those noun phrases with case particles to
JSP 4 to distinguish them from the pure noun phrases
which consist of only nouns. Also, VP-MOD and
VNP-MOD are re-tagged to DP, since they have very
similar lexical formats with existing DPs. NP-MOD
is converted into JSP-MOD - most of them consist
of a NP with josa JKG, forming possesive cases. S-
MOD remains as S-MOD if its head is JSP-MOD:
4It stands for a ?Josa Phrase?.
82
otherwise, it is also re-tagged to a DP. Figure 75
shows a re-tagging example.
Figure 7: Example of retagging phrase tags: VP-MOD to
DP, NP-MOD to JSP-MOD, and NP-SBJ to JSP-SBJ.
5 Evaluations
In this section, several experiment results using the
standard F1 metric (2PR/(P + R)) are introduced
to show the effect of each transforming method, and
the most frequently shown error cases are explained.
5.1 Experiments using the Sejong Treebank
The proposed transformation methods are applied to
the Sejong treebank, and the converted treebanks are
used to train and test three different well-known sta-
tistical parsers, namely Stanford parser (Klein and
Manning, 2003), Bikel-Collins parser (Bikel, 2012)
and Berkeley parser (Petrov et al, 2006). To figure
out the effect of each method, all six methods are
sequentially applied one by one, and each version of
the treebank is used to train and test each parser. The
baseline treebank is the original Sejong treebank
without any transformations. For the Korean head
word extraction which will be used during parsing,
the head percolation rule of (Choi and Palmer, 2011)
is adapted. According to that paper, particles and
endings were the most useful morphemes to deter-
mine dependencies between eojeols. Based on the
observation, their rules are changed so that they give
the best priorities on those morphemes. We use
the preprocessing method described in (Park, 2006)
for training trees. It replaces symboles with Penn-
Treebank-like tags and corrects wrong morpheme
5See Figure 1 for its transcription and translation.
boundary marks within the eojeol. Methods are ap-
plied cumulatively; for example, symbol ?M 1-6?
means the version of a treebank to which method
1, 2, 3, 4, 5 and 6 are applied cumulatively.6
System Corpus P R F1
Stan.
Baseline 67.88% 61.77% 64.69%
M 1 68.34% 61.93% 64.98%
M 1-2 71.78% 67.50% 69.58%
M 1-3 71.28% 67.91% 69.56%
M 1-4 71.06% 67.08% 69.01%
M 1-5 71.35% 67.27% 69.26%
M 1-6 75.85% 72.07% 73.92%
Bikel.
Baseline 74.81% 70.39% 72.53%
M 1 74.87% 70.45% 72.59%
M 1-2 77.05% 73.84% 75.41%
M 1-3 75.87% 72.88% 74.34%
M 1-4 75.33% 72.10% 73.68%
M 1-5 75.29% 72.18% 73.70%
M 1-6 73.70% 71.05% 72.35%
Berk.
Baseline 75.25% 72.72% 73.96%
M 1 74.54% 71.97% 73.23%
M 1-2 77.27% 75.05% 76.14%
M 1-3 75.60% 73.19% 74.38%
M 1-4 75.69% 73.32% 74.49%
M 1-5 76.53% 74.30% 75.40%
M 1-6 78.60% 76.03% 77.29%
Table 3: Evaluation results of parsers, with various trans-
formed versions of the Sejong treebank.
Table 3 shows the experimental results on each
version of the treebanks using each parser. Since
the corpus covers various domains (i.e. the style of
sentences is not homogeneous.), we perform 10-fold
cross-validation for our experiments. Stan. rep-
resents Stanford parser, Bikel. represents Bikel-
Collins parser, and Berk. means Berkeley parser.
For the Berkeley parser, we set the number of itera-
tion as two for latent annotations. In this set of ex-
periments, only phrase tags are the target of training
and testing, not including functional tags.
As can be observed from the evaluation result, the
performance is improved due to methods 2 and 6
are quite big compared to the effect of other four
6As pointed out by reviewers, we are planning the reversibil-
ity of transformations to be evaluated on the same trees for
meaning comparison.
83
System Corpus P R F1
Stan.
Baseline 71.48% 69.40% 70.43%
M 1 71.89% 69.75% 70.81%
M 1-2 75.90% 73.44% 74.65%
M 1-3 72.32% 69.76% 71.02%
M 1-4 72.37% 69.97% 71.16%
M 1-5 72.80% 70.28% 71.52%
M 1-6 72.32% 69.81% 71.05%
Bikel.
Baseline 69.65% 66.80% 68.19%
M 1 69.73% 66.97% 68.32%
M 1-2 74.33% 71.90% 73.09%
M 1-3 63.94% 64.57% 64.25%
M 1-4 63.95% 65.04% 64.49%
M 1-5 64.09% 65.05% 64.57%
M 1-6 62.94% 64.16% 63.54%
Berk.
Baseline 76.82% 75.28% 76.04%
M 1 76.73% 75.06% 75.89%
M 1-2 79.59% 77.91% 78.74%
M 1-3 75.24% 72.16% 73.67%
M 1-4 75.02% 73.01% 74.00%
M 1-5 75.58% 73.61% 74.58%
M 1-6 74.37% 71.93% 73.13%
Table 4: Evaluation results of parsers, with phrase tags
and functional tags together as learning target.
methods. Especially, the performance increase due
to the method 6 strongly suggests that Sejong phrase
tagsets are not enough to distinguish the types of
phrases effectively. Except those two methods,
only the method 5 increases the overall performance
slightly, and methods 1, 3 and 4 do not have any
significant effect on the performance or even some-
times decrease the overall performance.
Although the usage of functional tags is different
from that of phrase tags, the Sejong treebank has
a very rich functional tag set. Considering the re-
sults of the previous experiments, it is highly likely
that some of phrasal information is encoded into the
functional tags. To prove that, another set of experi-
ments is carried out. In this time, parsers are trained
not only on phrase tags but also on functional tags.
Table 4 shows the evaluation results.
As can be observed, by keeping functional tags
to train and test parsers, the baseline performance
increases 3 to 6 % for the Stanford and Berkeley
parsers. Only the performance of the Bikel parser
is decreased - it is highly possible that the parser
fails to find out the appropriate head word for each
possible tag, because the number of possible tags is
increased greatly by using the functional tags along
with the phrase tags.
In both set of experiments, the method 3 decreases
the overall performance. This strongly suggests that
finding the actual argument of josa directly is quite a
challenging work. The performance drop is consid-
ered mainly because the branching problem at the
higher level of the constituent tree is counted twice
due to the josa.
5.2 Experiments using the Penn Korean
Treebank
To show the effect of the transformation methods
more clearly, the Penn Korean Treebank (Han et al,
2002) is used as another treebank for experimen-
tation: (Chung et al, 2010) describes about major
difficulties of parsing Penn Korean Treebank. The
same three parsers are trained and tested using the
treebank. Due to the different annotation guidelines
and different tagsets, transformation methods 1, 5
and 6 cannot be applied on the treebank. Thus, only
method 2, 3 and 4 are used to transform the treebank.
Table 5 shows the evaluation results.
System Corpus P R F1
Stan.
Baseline 82.84% 80.28% 81.54%
M 2 85.29% 83.25% 84.26%
M 2-3 84.52% 82.71% 83.61%
M 2-4 84.52% 82.92% 83.72%
Bikel.
Baseline 81.49% 78.20% 79.81%
M 2 75.82% 74.47% 75.13%
M 2-3 73.50% 69.66% 71.53%
M 2-4 73.45% 69.66% 71.51%
Berk.
Baseline 85.11% 81.90% 83.47%
M 2 83.40% 81.04% 82.20%
M 2-3 82.36% 80.52% 81.43%
M 2-4 82.97% 81.28% 82.12%
Table 5: Evaluation on Penn Korean Treebank.
The overall performance of training the Penn Ko-
rean treebank is higher than that of the Sejong tree-
bank. There could be two possible explanations.
First one is, since the Penn Korean treebank tries
to follow English Penn treebank guidelines as much
84
as possible, thus annotation guidelines of the Ko-
rean Penn treebank could be much ?familiar? to the
parsers than that of the Sejong treebank. The second
explanation is, since the domain of the Penn Korean
treebank is much more restricted than that of the Se-
jong treebank, the system could be trained for the
specific domain. The best performance was gained
with the Stanford parser, with the treebank trans-
formed by method 2. Actually, (Chung et al, 2010)
also investigated parsing accuracy on the Penn Ko-
rean treebank; the direct comparison could be very
difficult because parsing criteria is different.
5.3 Error Analysis
In this section, some of the parsing error cases are
reported. Berkeley parser trained with the Sejong
treebank is used for error analysis. Both phrase tags
and functional tags are used to train and test the sys-
tem.
5.3.1 Locating Approximate Positions of
Errors
As the first step to analyze the errors, we tried to
figure out at which points of the constituent tree er-
rors frequently occur ? do the errors mainly occur at
the bottom of the trees? Or at the top of the trees?
If we can figure out approximate locations of errors,
then the types of errors could be predicted.
Figure 8: Example of assigning levels to each phrasal
node.
To define the level of each nonterminal node of
the constituent tree, the following rules are used:
? The level of prepreterminal node is 0.
? The levels of other phrasal nodes are defined
as: the maximal level of their children + 1.
? Once the levels of all the phrasal nodes are cal-
culated, normalize the levels so that they have
the values between 0 and 1.
Figure 8 shows an example of constituent tree
with levels assigned to its phrasal nodes. All the
prepreterminal nodes have level value 0, and the top-
most node has level 1.
Figure 9: Performance of the system on each level of the
parse tree
Once the levels are assigned to each constituent
tree, only those constituents with levels larger than
or equal to the predefined threshold ? are used to
evaluate the system. ? are increased from 0 to 1 with
value 0.01. Higher ? value means that the system is
evaluated only for those constituents positioned at
the top level of the constituent tree.
Figure 9 shows the evaluation results. X-axis rep-
resents the value of ?, and Y-axis represents the F1-
score. As can be observed, most of the errors oc-
cur at the mid-level of the constituent trees. Also,
the effects of some methods are explicitly shown
on the graph. For example, method 2 greatly in-
creases the performance at low level of the con-
stituent tree, suggesting improved consistency in de-
temining prepreterminal NP nodes. Also, it is shown
that the proposed methods does not affect the perfor-
mance of mid-level and top-level constituent deci-
sions - this suggests that the future works should be
more focused on providing more information about
those mid-level decision to the treebank annotation.
85
Figure 10: Example of NP boundary detection error. Part
of parse tree as well as name of the enumerated products
are omitted to more clearly show the example itself.
5.3.2 Frequent Error Cases
In this section, four major parsing error cases are
described.
Detecting Boundaries of NP. Although the
method 4 tries to find and gather the sequence of
nouns which actually belong to one NP, it misses
some of the cases. Figure 10 shows such example.
Some parts of the tree are omitted using the notation
?...? to show the example more simply. Although it
is counted as the parser error, the result of the parser
is more likely to be an answer - the number of those
products is 8, not their action. The Sejong treebank
tree is annotated in that way because the number ?8?
and bound noun Gae (?unit?), representing as units,
are separated by a space. To detect such kind of sep-
arated NPs and transform them into one NP will be
our next task.
Finding an AppropriateModifee. Some phrases
modifying other phrases were failed to find their ap-
propriate modifees. Figure 11 shows an example of
such kind of error case.
Detecting an Appropriate Subject of the Sen-
tence. This case frequently occurs when a sentence
is quotated inside the other sentence. In this case,
the subject of quotated sentence is often considered
as the subject of the whole sentence, because the
quotated sentences in Korean are usually first stated
Figure 11: Example of a phrase (JSP-AJT) which failed
to find its right modifee.
and then the subject of the whole sentence shows up.
Figure 12 shows an example of the erroneously de-
tected subject.
The Wrongly-tagged Topmost Node. Some of
Sejong treebank trees have phrases which are not
tagged as S as their topmost nodes. This could cause
confusion during the training. Figure 13 shows such
example.
6 Conclusion and Future Work
Although there exist some manually-annotated
large-enough constituent treebanks such as Sejong
treebank, it was hard to apply the algorithms for En-
glish parsers to Korean treebanks, because they were
annotated in eojeol-based scheme, which concept
does not exist in English. In this paper, we showed
the possibility of acquiring good training and testing
results with the existing parsers trained using the ex-
isting Korean treebanks, if it undergoes some simple
transforming procedures. The error analysis result
shows that, indeed the proposed method improves
the performance of parser at the lower level of con-
stituent tree.
86
Figure 12: Example of a wrongly-detected subject.
Although there exists a performance gain due to
the transforming methods, there are still many gaps
for improvement. The evaluation results and er-
ror analysis results suggests the need to define the
phrase tagset of Sejong treebank in more detail.
Also, the transforming methods themselves are not
perfect yet - we believe still they could be improved
more to increase consistency of the resultant tree-
banks.
We will continuously develop our transforming
methods to improve the parsing result. Furthermore,
we are planning to investigate methods to determine
the appropriate ?detailedness? of phrase tag set, so
that there are no missing information due to too
small number of tags as well as no confusion due
to too many tags.
Acknowledgement
This research was supported by Basic Science
Research Program through the National Research
Foundation of Korea (NRF) funded by the Ministry
of Education, Science and Technology (No. 2011-
Figure 13: Example of the wrongly-tagged topmost node.
Some trees in the treebank have Non-S topmost phrase
nodes.
0026718)
References
Dan Bikel. 2004. On the Parameter Space of Generative
Lexicalized Statistical Parsing Models. Ph.D. thesis,
University of Pennsylvania.
Dan Bikel. 2012. Bikel parser. http://www.cis.
upenn.edu/?dbikel/software.html.
Jinho D. Choi and Martha Palmer. 2011. Statistical de-
pendency parsing in Korean: From corpus generation
to automatic parsing. In The Second Workshop on Sta-
tistical Parsing of Morphologically Rich Languages,
pages 1?11.
Key-Sun Choi, Young S. Han, Young G. Han, and Oh W.
Kwon. 1994. KAIST tree bank project for Korean:
Present and future development. In Proceedings of
the International Workshop on Sharable Natural Lan-
guage Resources, pages 7?14.
Key-Sun Choi, Isahara Hitoshi, and Maosong Sun. 2011.
Language resource management ? word segmentation
of written texts ? part 2: Word segmentation for Chi-
nese, Japanese and Korean. In ISO 24614-2. ISO.
Tagyoung Chung, Matt Post, and Daniel Gildea. 2010.
Factors affecting the accuracy of korean parsing. In
Proceedings of the NAACL HLT 2010 First Workshop
on Statistical Parsing of Morphologically-Rich Lan-
guages, pages 49?57, Los Angeles, CA, USA, June.
Association for Computational Linguistics.
87
Hoojung Chung. 2004. Statistical Korean Dependency
Parsing Model based on the Surface Contextual Infor-
mation. Ph.D. thesis, Korea University.
Danica Damljanovic, Milan Agatonovic, and Hamish
Cunningham. 2010. Identification of the question fo-
cus: Combining syntactic analysis and ontology-based
lookup through the user interaction. In Proceedings of
7th Language Resources and Evaluation Conference
(LREC), pages 361?368.
Jenny Rose Finkel and Christopher D. Manning. 2009.
Joint parsing and named entity recognition. In NAACL
?09 Proceedings of Human Language Technologies:
The 2009 Annual Conference of the North American
Chapter of the Association for Computational Linguis-
tics, pages 326?334.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic la-
beling of semantic roles. Computational Linguistics,
28(3):245?288.
Chung-Hye Han, Na-Rae Han, Eon-Suk Ko, Heejong Yi,
and Martha Palmer. 2002. Penn Korean treebank:
Development and evaluation. In Proceedings of the
16th Pacific Asia Conference on Language, Informa-
tion and Computation.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In Proceedings of the 41st
Meeting of the Association for Computational Linguis-
tics, pages 423?430.
Korean Language Institute. 2012. Sejong treebank.
http://www.sejong.or.kr.
Tetsuya Nasukawa and Jeonghee Yi. 2003. Sentiment
analysis: capturing favorability using natural language
processing. In Proceedings of the 2nd international
conference on Knowledge capture, pages 70?77.
Jin Young Oh, Yo-Sub Han, Jungyeul Park, and Jeong-
Won Cha. 2011. Predicting phrase-level tags using
entropy inspired discriminative models. In 2011 Inter-
national Conference on Information Science and Ap-
plications (ICISA), pages 1?5.
Jungyeul Park. 2006. Extraction of tree adjoining gram-
mars from a treebank for Korean. In Proceedings of
the 21st International Conference on computational
Linguistics and 44th Annual Meeting of the Associa-
tion for Computational Linguistics: Student Research
Workshop, pages 73?78.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Human Language Tech-
nologies 2007: The Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics; Proceedings of the Main Conference, pages
404?411, Rochester, New York, April. Association for
Computational Linguistics.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and in-
terpretable tree annotation. In Proceedings of the
COLING-ACL 2006, pages 433?440.
David A. Smith and Noah A. Smith. 2004. Bilingual
parsing with factored estimation: Using English to
parse Korean. In Proceedings of the EMNLP, pages
49?56.
88
