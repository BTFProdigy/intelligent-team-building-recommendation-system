Unsupervised Construction of Large Paraphrase Corpora: 
Exploiting Massively Parallel News Sources 
Bill DOLAN, Chris QUIRK, and Chris BROCKETT 
Natural Language Processing Group, Microsoft Research  
One Microsoft Way 
Redmond, WA 90852, USA 
{billdol,chrisq,chrisbkt}@microsoft.com 
 
Abstract 
We investigate unsupervised techniques for 
acquiring monolingual sentence-level 
paraphrases from a corpus of temporally and 
topically clustered news articles collected from 
thousands of web-based news sources. Two 
techniques are employed: (1) simple string edit 
distance, and (2) a heuristic strategy that pairs 
initial (presumably summary) sentences from 
different news stories in the same cluster. We 
evaluate both datasets using a word alignment 
algorithm and a metric borrowed from machine 
translation. Results show that edit distance data 
is cleaner and more easily-aligned than the 
heuristic data, with an overall alignment error 
rate (AER) of 11.58% on a similarly-extracted 
test set.  On test data extracted by the heuristic 
strategy, however, performance of the two 
training sets is similar, with AERs of 13.2% 
and 14.7% respectively. Analysis of 100 pairs 
of sentences from each set reveals that the edit 
distance data lacks many of the complex lexical 
and syntactic alternations that characterize 
monolingual paraphrase. The summary 
sentences, while less readily alignable, retain 
more of the non-trivial alternations that are of 
greatest interest learning paraphrase 
relationships.    
 
1 Introduction 
The importance of learning to manipulate 
monolingual paraphrase relationships for 
applications like summarization, search, and dialog 
has been highlighted by a number of recent efforts 
(Barzilay & McKeown 2001; Shinyama et al 
2002; Lee & Barzilay 2003; Lin & Pantel 2001). 
While several different learning methods have 
been applied to this problem, all share a need for 
large amounts of data in the form of pairs or sets of 
strings that are likely to exhibit lexical and/or 
structural paraphrase alternations. One approach1 
                                                    
1
 An alternative approach involves identifying anchor 
points--pairs of words linked in a known way--and 
collecting the strings that intervene. (Shinyama, et al 
2002; Lin & Pantel 2001). Since our interest is in 
that has been successfully used is edit distance, a 
measure of similarity between strings. The 
assumption is that strings separated by a small edit 
distance will tend to be similar in meaning: 
   
The leading indicators measure the economy? 
The leading index measures the economy?. 
 
Lee & Barzilay (2003), for example, use Multi-
Sequence Alignment (MSA) to build a corpus of 
paraphrases involving terrorist acts.  Their goal is 
to extract sentential templates that can be used in 
high-precision generation of paraphrase alter-
nations within a limited domain.  
 Our goal here is rather different: our interest lies 
in constructing a monolingual broad-domain 
corpus of pairwise aligned sentences. Such data 
would be amenable to conventional statistical 
machine translation (SMT) techniques (e.g., those 
discussed in Och & Ney 2003).2 In what follows 
we compare two strategies for unsupervised 
construction of such a corpus, one employing 
string similarity and the other associating sentences 
that may overlap very little at the string level. We 
measure the relative utility of the two derived 
monolingual corpora in the context of word 
alignment techniques developed originally for 
bilingual text.   
We show that although the edit distance corpus is 
well-suited as training data for the alignment 
algorithms currently used in SMT, it is an 
incomplete source of information about paraphrase 
relations, which exhibit many of the characteristics 
of comparable bilingual corpora or free 
translations. Many of the more complex 
alternations that characterize monolingual 
paraphrase, such as large-scale lexical alternations 
and constituent reorderings, are not readily 
                                                                                 
learning sentence level paraphrases, including major 
constituent reorganizations, we do not address this 
approach here.  
2
 Barzilay & McKeown (2001) consider the 
possibility of using SMT machinery, but reject the 
idea because of the noisy, comparable nature of their 
dataset. 
captured by edit distance techniques, which 
conflate semantic similarity with formal similarity.  
We conclude that paraphrase research would 
benefit by identifying richer data sources and 
developing appropriate learning techniques.  
2 Data/Methodology 
Our two paraphrase datasets are distilled from a 
corpus of news articles gathered from thousands of 
news sources over an extended period. While the 
idea of exploiting multiple news reports for 
paraphrase acquisition is not new, previous efforts 
(for example, Shinyama et al 2002; Barzilay and 
Lee 2003) have been restricted to at most two news 
sources. Our work represents what we believe to 
be the first attempt to exploit the explosion of news 
coverage on the Web, where a single event can 
generate scores or hundreds of different articles 
within a brief period of time. Some of these articles 
represent minor rewrites of an original AP or 
Reuters story, while others represent truly distinct 
descriptions of the same basic facts.  The massive 
redundancy of information conveyed with widely 
varying surface strings is a resource begging to be 
exploited. 
Figure 1 shows the flow of our data collection 
process. We begin with sets of pre-clustered URLs 
which point to news articles on the Web, 
representing thousands of different news sources. 
The clustering algorithm takes into account the full 
text of each news article, in addition to temporal 
cues, to produce a set of topically and temporally 
related articles. Our method is believed to be 
independent of the specific clustering technology 
used. The story text is isolated from a sea of 
advertisements and other miscellaneous text 
through use of a supervised HMM.  
Altogether we collected 11,162 clusters in an 8-
month period, assembling 177,095 articles with an 
average of 15.8 articles per cluster.  The clusters 
are generally coherent in topic and focus. Discrete 
events like disasters, business announcements, and 
deaths tend to yield tightly focused clusters, while 
ongoing stories like the SARS crisis tend to 
produce less focused clusters. While exact 
duplicate articles are filtered out of the clusters, 
many slightly-rewritten variants remain. 
 
2.1 Extracting Sentential Paraphrases 
Two separate techniques were employed to 
extract likely pairs of sentential paraphrases from 
these clusters. The first used string edit distance, 
counting the number of lexical deletions and 
insertions needed to transform one string into 
another. The second relied on a discourse-based 
heuristic, specific to the news genre, to identify 
likely paraphrase pairs even when they have little 
superficial similarity. 
 
3 Levenshtein Distance 
A simple edit distance metric (Levenshtein 
1966) was used to identify pairs of sentences 
within a cluster that are similar at the string level.  
First, each sentence was normalized to lower case 
and paired with every other sentence in the cluster. 
Pairings that were identical or differing only by 
punctuation were rejected, as were those where the 
shorter sentence in the pair was less than two thirds 
the length of the longer, this latter constraint in 
effect placing an upper bound on edit distance 
relative to the length of the sentence. Pairs that had 
been seen before in either order were also rejected. 
Filtered in this way, our dataset yields 139K non-
identical sentence pairs at a Levenshtein distance 
of n ? 12. 3  Mean Levenshtein distance was 5.17, 
and mean sentence length was 18.6 words. We will 
refer to this dataset as L12. 
 
3.1.1 First sentences  
The second extraction technique was 
specifically intended to capture paraphrases which 
might contain very different sets of content words, 
word order, and so on. Such pairs are typically 
used to illustrate the phenomenon of paraphrase, 
but precisely because their surface dissimilarity 
renders automatic discovery difficult, they have 
generally not been the focus of previous 
computational approaches.  
In order to automatically identify sentence pairs 
of this type, we have attempted to take advantage 
of some of the unique characteristics of the dataset. 
The topical clustering is sufficiently precise to 
ensure that, in general, articles in the same cluster 
overlap significantly in overall semantic content. 
Even so, any arbitrary pair of sentences from 
different articles within a cluster is unlikely to 
exhibit a paraphrase relationship: 
 
The Phi-X174 genome is short and compact. 
This is a robust new step that allows us to make much 
larger pieces. 
 
To isolate just those sentence pairs that represent 
likely paraphrases without requiring significant 
string similarity, we exploited a common 
journalistic convention: the first sentence or two of 
                                                    
3A maximum Levenshtein distance of 12 was selected 
for the purposes of this paper on the basis of 
experiments with corpora extracted at various edit 
distances.  
a newspaper article typically summarize its 
content. One might reasonably expect, therefore, 
that initial sentences from one article in a cluster 
will be paraphrases of the initial sentences in other 
articles in that cluster. This heuristic turns out to be 
a powerful one, often correctly associating 
sentences that are very different at the string level: 
 
In only 14 days, US researchers have created an 
artificial bacteria-eating virus from synthetic 
genes. 
An artificial bacteria-eating virus has been made from 
synthetic genes in the record time of just two weeks. 
 
Also consider the following example, in which 
related words are obscured by different parts of 
speech: 
   
Chosun Ilbo, one of South Korea's leading newspapers, 
said North Korea had finished developing a new 
ballistic missile last year and was planning to 
deploy it.  
The Chosun Ilbo said development of the new missile, 
with a range of up to %%number%% kilometres 
(%%number%% miles), had been completed and 
deployment was imminent. 
 
A corpus was produced by extracting the first 
two sentences of each article, then pairing these 
across documents within each cluster. We will 
refer to this collection as the F2 corpus.  The 
combination of the first-two sentences heuristic 
plus topical article clusters allows us to take 
advantage of meta-information implicit in our 
corpus, since clustering exploits lexical 
information from the entire document, not just the 
few sentences that are our focus. The assumption 
that two first sentences are semantically related is 
thus based in part on linguistic information that is 
external to the sentences themselves. 
Sometimes, however, the strategy of pairing 
sentences based on their cluster and position goes 
astray. This would lead us to posit a paraphrase 
relationship where there is none: 
 
Terence Hope should have spent most of yesterday in 
hospital performing brain surgery. 
A leading brain surgeon has been suspended from work 
following a dispute over a bowl of soup. 
 
To prevent too high an incidence of unrelated 
sentences, one string-based heuristic filter was 
found useful: a pair is discarded if the sentences do 
not share at least 3 words of 4+ characters. This 
constraint succeeds in filtering out many unrelated 
pairs, although it can sometimes be too restrictive, 
excluding completely legitimate paraphrases:   
 
There was no chance it would endanger our planet, 
astronomers said. 
NASA emphasized that there was never danger of a 
collision. 
 
An additional filter ensured that the word count 
of the shorter sentence is at least one-half that of 
the longer sentence. Given the relatively long 
sentences in our corpus (average length 18.6 
words), these filters allowed us to maintain a 
degree of semantic relatedness between sentences. 
Accordingly, the dataset encompasses many 
paraphrases that would have been excluded under a 
more stringent edit-distance threshold, for 
example, the following non-paraphrase pair that 
contain an element of paraphrase:  
 
A staggering %%number%% million Americans have 
been victims of identity theft in the last five years , 
according to federal trade commission survey out 
this week.  
In the last year alone, %%number%% million people 
have had their identity purloined. 
 
Nevertheless, even after filtering in these ways,  
a significant amount of unfiltered noise remains in 
the F2 corpus, which consisted of 214K sentence 
pairs. Out of a sample of 448 held-out sentence 
pairs, 118 (26.3%) were rated by two independent 
human evaluators as sentence-level paraphrases, 
while 151 (33.7%) were rated as partial 
paraphrases. The remaining ~40% were assessed as 
 
News article clusters: URLs
Download URLs,
Isolate content (HMM),
Sentence separate
Textual content of articles
Select and filter
first sentence pairs
Approximately parallel
monolingual corpus
 
 
 
Figure 1. Data collection 
 
 
unrelated. 4   Thus, although the F2 data set is 
nominally larger than the L12 data set, when the 
noise factor is taken into account, the actual 
number of full paraphrase sentences in this data set 
is estimated to be in the region of 56K sentences, 
with a further estimated 72K sentences containing 
some paraphrase material that might be a potential 
source of alignment.  
Some of these relations captured in this data can 
be complex. The following pair, for example, 
would be unlikely to pass muster on edit distance 
grounds, but nonetheless contains an inversion of 
deep semantic roles, employing different lexical 
items.    
 
The Hartford Courant reported %%day%% that Tony 
Bryant said two friends were the killers.  
A lawyer for Skakel says there is a claim that the 
murder was carried out by two friends of one of 
Skakel's school classmates, Tony Bryan. 
 
The F2 data also retains pairs like the following 
that involve both high-level semantic alternations 
and long distance dependencies:  
 
Two men who robbed a jeweller's shop to raise funds 
for the Bali bombings were each jailed for 
%%number%% years by Indonesian courts today.  
An Indonesian court today sentenced two men to 
%%number%% years in prison for helping 
finance last year's terrorist bombings in Bali by 
robbing a jewelry store. 
 
These examples do not by any means exhaust 
the inventory of complex paraphrase types that are 
commonly encountered in the F2 data. We 
encounter, among other things, polarity 
alternations, including those involving long-
distance dependencies, and a variety of distributed 
paraphrases, with alignments spanning widely 
separated elements. 
 
3.2 Word Error Alignment Rate 
An objective scoring function was needed to 
compare the relative success of the two data 
collection strategies sketched in 2.1.1 and 2.1.2. 
Which technique produces more data? Are the 
types of data significantly different in character or 
utility? In order to address such questions, we used 
word Alignment Error Rate (AER), a metric 
borrowed from the field of statistical machine 
translation (Och & Ney 2003). AER measures how 
accurately an automatic algorithm can align words 
in corpus of parallel sentence pairs, with a human-
                                                    
4
  This contrasts with 16.7% pairs assessed as 
unrelated in a 10,000 pair sampling of the L12 data.   
tagged corpus of alignments serving as the gold 
standard. Paraphrase data is of course monolingual, 
but otherwise the task is very similar to the MT 
alignment problem, posing the same issues with 
one-to-many, many-to-many, and one/many-to-
null word mappings. Our a priori assumption was 
that the lower the AER for a corpus, the more 
likely it would be to yield learnable information 
about paraphrase alternations.  
We closely followed the evaluation standards 
established in Melamed (2001) and Och & Ney 
(2000, 2003). Following Och & Ney?s 
methodology, two annotators each created an 
initial annotation for each dataset, subcategorizing 
alignments as either SURE (necessary) or POSSIBLE 
(allowed, but not required). Differences were then 
highlighted and the annotators were asked to 
review these cases.  Finally we combined the two 
annotations into a single gold standard in the 
following manner: if both annotators agreed that an 
alignment should be SURE, then the alignment was 
marked as sure in the gold-standard; otherwise the 
alignment was marked as POSSIBLE. 
To compute Precision, Recall, and Alignment 
Error Rate (AER) for the twin datasets, we used 
exactly the formulae listed in Och & Ney (2003).  
Let A be the set of alignments in the comparison, S 
be the set of SURE alignments in the gold standard, 
and P be the union of the SURE and POSSIBLE 
alignments in the gold standard.  Then we have:  
 
||
||precision
A
PA ?
=   
 
||
||
  recall
S
SA ?
=
 
 
||
||AER
SA
SAPA
+
?+?
=  
 
 
We held out a set of news clusters from our 
training data and randomly extracted two sets of 
sentence pairs for blind evaluation. The first is a 
set of 250 sentence pairs extracted on the basis of 
an edit distance of 5 ? n ? 20, arbitrarily chosen to 
allow a range of reasonably divergent candidate 
pairs. These sentence pairs were checked by an 
independent human evaluator to ensure that they 
contained paraphrases before they were tagged for 
alignments. The second set comprised 116 
sentence pairs randomly selected from the set of 
first-two sentence pairs. These were likewise hand-
vetted by independent human evaluators. After an 
initial training pass and refinement of the linking 
specification, interrater agreement measured in 
terms of AER5 was 93.1% for the edit distance test 
set versus 83.7% for the F2 test set, suggestive of 
the greater variability in the latter data set.  
3.3 Data Alignment  
Each corpus was used as input to the word 
alignment algorithms available in Giza++ (Och & 
Ney 2000).  Giza++ is a freely available 
implementation of IBM Models 1-5 (Brown et al 
1993) and the HMM alignment (Vogel et al 1996), 
along with various improvements and 
modifications motivated by experimentation by 
Och & Ney (2000).  Giza++ accepts as input a 
corpus of sentence pairs and produces as output a 
Viterbi alignment of that corpus as well as the 
parameters for the model that produced those 
alignments.  
While these models have proven effective at the 
word alignment task (Mihalcea & Pedersen 2003), 
there are significant practical limitations in their 
output. Most fundamentally, all alignments have 
either zero or one connection to each target word. 
Hence they are unable to produce the many-to-
many alignments required to identify 
correspondences with idioms and other phrasal 
chunks. 
To mitigate this limitation on final mappings, 
we follow the approach of Och (2000): we align 
once in the forward direction and again in the 
backward direction.  These alignments can 
subsequently be recombined in a variety of ways, 
                                                    
5
 The formula for AER given here and in Och & Ney 
(2003) is intended to compare an automatic alignment 
against a gold standard alignment. However, when 
comparing one human against another, both comparison 
and reference distinguish between SURE and POSSIBLE 
links. Because the AER is asymmetric (though each 
direction differs by less than 5%), we have presented the 
average of the directional AERs. 
such as union to maximize recall or intersection to 
maximize precision. Och also documents a method 
for heuristically recombining the unidirectional 
alignments intended to balance precision and 
recall. In our experience, many alignment errors 
are present in one side but not the other, hence this 
recombination also serves to filter noise from the 
process. 
4 Evaluation 
Table 1 shows the results of training translation 
models on data extracted by both methods and then 
tested on the blind data. The best overall 
performance, irrespective of test data type, is 
achieved by the L12 training set, with an 11.58% 
overall AER on the 250 sentence pair edit distance 
test set (20.88% AER for non-identical words). 
The F2 training data is probably too sparse and, 
with 40% unrelated sentence pairs, too noisy to 
achieve equally good results; nevertheless the gap 
between the results for the two training data types 
is dramatically narrower on the F2 test data. The 
nearly comparable numbers for the two training 
data sets, at 13.2% and 14.7% respectively, suggest 
that the L12 training corpus provides no 
substantive advantage over the F2 data when tested 
on the more complex test data. This is particularly 
striking given the noise inherent in the F2 training 
data. 
5 Analysis/Discussion 
To explore some of the differences between the 
training sets, we hand-examined a random sample 
of sentence pairs from each corpus type. The most 
common paraphrase alternations that we observed 
fell into the following broad categories: 
 
? Elaboration: Sentence pairs can differ in total 
information content, with an added word, 
phrase or clause in one sentence that has no 
Training Data Type: L12 F2 L12 F2 
Test Data Type: 250 Edit Dist 250 Edit Dist 116 F2 Heuristic 116 F2 Heuristic 
Precision   87.46% 86.44% 85.07% 84.16% 
Recall      89.52% 82.64% 88.70% 86.55% 
AER         11.58% 15.41% 13.24% 14.71% 
Identical word precision   89.36% 88.79% 92.92% 93.41% 
Identical word recall      89.50% 83.10% 93.49% 92.47% 
Identical word AER         10.57% 14.14% 6.80% 7.06% 
Non-Identical word precision   76.99% 71.86% 60.54% 53.69% 
Non-Identical word recall      90.22% 69.57% 59.50% 50.41% 
Non-Identical word AER         20.88% 28.57% 39.81% 47.46% 
 
 
Table 1.  Precision, recall, and alignment error rates (AER) for F2 and L12 
 
counterpart in the other (e.g. the NASDAQ /  
the tech-heavy NASDAQ). 
? Phrasal: An entire group of words in one 
sentence alternates with one word or a phrase 
in the other.  Some are non-compositional 
idioms (has pulled the plug on / is dropping 
plans for); others involve different phrasing 
(electronically / in electronic form, more than 
a million people / a massive crowd). 
? Spelling: British/American sources system-
atically differ in spellings of common words 
(colour / color); other variants also appear 
(email / e-mail). 
? Synonymy:  Sentence pairs differ only in one 
or two words (e.g. charges / accusations), 
suggesting an editor?s hand in modifying a 
single source sentence. 
? Anaphora: A full NP in one sentence  
corresponds to an anaphor in the other (Prime 
Minister Blair / He). Cases of NP anaphora 
(ISS / the Atlanta-based security company) are 
also common in the data, but in quantifying 
paraphrase types we restricted our attention to 
the simpler case of pronominal anaphora.  
? Reordering: Words, phrases, or entire 
constituents occur in different order in two 
related sentences, either because of major 
syntactic differences (e.g. topicalization, voice 
alternations) or more local pragmatic choices 
(e.g. adverb or prepositional phrase placement).  
 
These categories do not cover all possible 
alternations between pairs of paraphrased 
sentences; moreover, categories often overlap in 
the same sequence of words. It is common, for 
example, to find instances of clausal Reordering 
combined with Synonymy. 
Figure 2 shows a hand-aligned paraphrase pair 
taken from the F2 data. This pair displays one 
Spelling alternation (defence / defense), one 
Reordering (position of the ?since? phrase), and 
one example of Elaboration (terror attacks occurs 
in only one sentence).    
To quantify the differences between L12 and F2, 
we randomly chose 100 sentence pairs from each 
dataset and counted the number of times each 
phenomenon was encountered. A given sentence 
pair might exhibit multiple instances of a single 
phenomenon, such as two phrasal paraphrase 
changes or two synonym replacements.  In this 
case all instances were counted. Lower-frequency 
changes that fell outside of the above categories 
were not tallied: for example, the presence or 
absence of a definite article (had authority / had 
the authority) in Figure 2 was ignored.  After 
summing all alternations in each sentence pair, we 
calculated the average number of occurrences of 
each paraphrase type in each data set.  The results 
are shown in Table 2. 
Several major differences stand out between the 
two data sets.  First, the F2 data is less parallel, as 
evidenced by the higher percentage of Elaborations 
found in those sentence pairs. Loss of parallelism, 
however, is offset by greater diversity of 
paraphrase types encountered in the F2 data. 
Phrasal alternations are more than 4x more 
common, and Reorderings occur over 20x more 
frequently.   Thus while string difference methods 
may produce relatively clean training data, this is 
achieved at the cost of filtering out common (and 
interesting) paraphrase relationships. 
 
6 Conclusions and Future Work 
Edit distance identifies sentence pairs that 
exhibit lexical and short phrasal alternations that 
can be aligned with considerable success. Given a 
large dataset and a well-motivated clustering of 
documents, useful datasets can be gleaned even 
without resorting to more sophisticated techniques 
 
 
Figure 2.   Sample human-aligned paraphrase 
 
 L12 F2 
Elaboration 0.83 1.3 
Phrasal 0.14 0.69 
Spelling 0.12 0.01 
Synonym 0.18 0.25 
Anaphora 0.1 0.13 
Reordering 0.02 0.41 
 
 
Table 2.  Mean number of instances of 
paraphrase phenomena per sentence 
 
(such as Multiple Sequence Alignment, as 
employed by Barzilay & Lee 2003).  
However, there is a disparity between the kinds 
of paraphrase alternations that we need to be able 
to align and those that we can already align well 
using current SMT techniques. Based solely on the 
criterion of word AER, the L12 data would seem to 
be superior to the F2 data as a source of paraphrase 
knowledge.  Hand evaluation, though, indicates 
that many of the phenomena that we are interested 
in learning may be absent from this L12 data. 
String edit distance extraction techniques involve 
assumptions about the data that are inadequate, but 
achieve high precision.  Techniques like our F2 
extraction strategies appear to extract a more 
diverse variety of data, but yield more noise.  We 
believe that an approach with the strengths of both 
methods would lead to significant improvement in 
paraphrase identification and generation.   
In the near term, however, the relatively similar 
performances of F2 and L12-trained models on the 
F2 test data suggest that with further refinements, 
this more complex type of data can achieve good 
results. More data will surely help. 
One focus of future work is to build a classifier 
to predict whether two sentences are related 
through paraphrase. Features might include edit 
distance, temporal/topical clustering information, 
information about cross-document discourse 
structure, relative sentence length, and synonymy 
information. We believe that this work has 
potential impact on the fields of summarization, 
information retrieval, and question answering.   
Our ultimate goal is to apply current SMT 
techniques to the problems of paraphrase 
recognition and generation. We feel that this is a 
natural extension of the body of recent 
developments in SMT; perhaps explorations in 
monolingual data may have a reciprocal impact. 
The field of SMT, long focused on closely aligned 
data, is only now beginning to address the   kinds 
of problems immediately encountered in 
monolingual paraphrase (including phrasal 
translations and large scale reorderings).  
Algorithms to address these phenomena will be 
equally applicable to both fields. Of course a 
broad-domain SMT-influenced paraphrase solution 
will require very large corpora of sentential 
paraphrases. In this paper we have described just 
one example of a class of data extraction 
techniques that we hope will scale to this task. 
Acknowledgements 
We are grateful to the Mo Corston-Oliver, Jeff 
Stevenson and Amy Muia of the Butler Hill Group 
for their work in annotating the data used in the 
experiments. We have also benefited from 
discussions with Ken Church, Mark Johnson, 
Daniel Marcu and Franz Och. We remain, 
however, responsible for all content.  
References  
R. Barzilay and K. R. McKeown. 2001. Extracting 
Paraphrases from a parallel corpus. In Proceedings of 
the ACL/EACL. 
R. Barzilay and  L. Lee. 2003. Learning to Paraphrase: 
an unsupervised approach using multiple-sequence 
alignment. In Proceedings of HLT/NAACL. 
P. Brown, S. A. Della Pietra, V.J. Della Pietra and R. L. 
Mercer. 1993. The Mathematics of Statistical 
Machine Translation. Computational Linguistics, 
19(2): 263-311. 
V. Levenshtein. 1966. Binary codes capable of 
correcting deletions, insertions, and reversals. Soviet 
Physice-Doklady, 10:707-710. 
D. Lin and P. Pantel. 2001. DIRT - Discovery of 
Inference Rules from Text. In Proceedings of ACM 
SIGKDD Conference on Knowledge Discovery and 
Data Mining. 
I. D. Melamed. 2001. Empirical Methods for Exploiting 
Parallel Texts.  MIT Press.  
R. Mihalcea and T. Pedersen. 2003 An Evaluation 
Exercise for Word Alignment. In Proceedings of the 
Workshop on Building and Using Parallel Texts: 
Data Driven Machine Translation and Beyond. May 
31, 2003. Edmonton, Canada. 
F. Och and H. Ney. 2000. Improved Statistical 
Alignment Models.  In Proceedings of the 38th 
Annual Meeting of the ACL, Hong Kong, China. 
F. Och and H. Ney. 2003. A Systematic Comparison of 
Various Statistical Alignment Models.  
Computational Linguistics, 29(1):19-52. 
Y. Shinyama, S. Sekine and K. Sudo. 2002. Automatic 
Paraphrase Acquisition from News Articles.  In 
Proceedings of NAACL-HLT. 
S. Vogel, H. Ney and C. Tillmann. 1996. HMM-Based 
Word Alignment in Statistical Translation. In 
Proceedings of the Annual Meeting of the ACL, 
Copenhagen, Denmark.  
 
 
 
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 585?592
Manchester, August 2008
Random Restarts in Minimum Error Rate Training for Statistical
Machine Translation
Robert C. Moore and Chris Quirk
Microsoft Research
Redmond, WA 98052, USA
bobmoore@microsoft.com, chrisq@microsoft.com
Abstract
Och?s (2003) minimum error rate training
(MERT) procedure is the most commonly
used method for training feature weights in
statistical machine translation (SMT) mod-
els. The use of multiple randomized start-
ing points in MERT is a well-established
practice, although there seems to be no
published systematic study of its bene-
fits. We compare several ways of perform-
ing random restarts with MERT. We find
that all of our random restart methods out-
perform MERT without random restarts,
and we develop some refinements of ran-
dom restarts that are superior to the most
common approach with regard to resulting
model quality and training time.
1 Introduction
Och (2003) introduced minimum error rate train-
ing (MERT) for optimizing feature weights in sta-
tistical machine translation (SMT) models, and
demonstrated that it produced higher translation
quality scores than maximizing the conditional
likelihood of a maximum entropy model using the
same features. Och?s method performs a series
of one-dimensional optimizations of the feature
weight vector, using an innovative line search that
takes advantage of special properties of the map-
ping from sets of feature weights to the resulting
translation quality measurement. Och?s line search
is guaranteed to find a global optimum, whereas
more general line search methods are guaranteed
only to find a local optimum.
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
Global optimization along one dimension at a
time, however, does not insure global optimization
in all dimensions. Hence, as Och briefly men-
tions, ?to avoid finding a poor local optimum,?
MERT can be augmented by trying multiple ran-
dom starting points for each optimization search
within the overall algorithm. However, we are not
aware of any published study of the effects of ran-
dom restarts in the MERT optimization search.
We first compare a variant of Och?s method with
and without multiple starting points for the op-
timization search, selecting initial starting points
randomly according to a uniform distribution. We
find that using multiple random restarts can sub-
stantially improve the resulting model in terms of
translation quality as measured by the BLEU met-
ric, but that training time also increases substan-
tially. We next try selecting starting points by a
random walk from the last local optimum reached,
rather than by sampling from a uniform distrib-
ution. We find this provides a slight additional
improvement in BLEU score, and is significantly
faster, although still slower than training without
random restarts.
Finally we look at two methods for speeding up
training by pruning the set of hypotheses consid-
ered. We find that, under some circumstances, this
can speed up training so that it takes very little
additional time compared to the original method
without restarts, with no significant reduction in
BLEU score compared to the best training methods
in our experiments.
2 Och?s MERT procedure
While minimum error rate training for SMT is
theoretically possible by directly applying gen-
eral numerical optimization techniques, such as
the downhill simplex method or Powell?s method
585
(Press, 2002), naive use of these techniques would
involve repeated translation of the training sen-
tences using hundreds or thousands of combina-
tions of feature weights, which is clearly impracti-
cal given the speed of most SMT decoders.
Och?s optimization method saves expensive
SMT decoding time by generating lists of n-best
translation hypotheses, and their feature values ac-
cording to the SMT model, and then optimizing
feature weights just with respect to those hypothe-
ses. In this way, as many different feature weight
settings as necessary can be explored without re-
running the decoder. The translation quality mea-
surement for the training corpus can be estimated
for a given point in feature weight space by find-
ing the highest scoring translation hypothesis, out
of the current set of hypotheses, for each sentence
in the training set. This is typically orders of mag-
nitude faster than re-running the decoder for each
combination of feature weights.
Since the resulting feature weights are opti-
mized only for one particular set of translation hy-
potheses, the decoder may actually produce differ-
ent results when run with those weights. There-
fore Och iterates the process, re-running the de-
coder with the optimized feature weights to pro-
duce new sets of n-best translation hypotheses,
merging these with the previous sets of hypothe-
ses, and re-optimizing the feature weights relative
to the expanded hypothesis sets. This process is re-
peated until no more new hypotheses are obtained
for any sentence in the training set.
Another innovation by Och is a method of nu-
merical optimization that takes advantage of the
fact that, while translation quality metrics may
have continous values, they are always applied to
the discrete outputs of a translation decoder. This
means that any measure of translation quality can
change with variation in feature weights only at
discrete points where the decoder output changes.
Och takes advantage of this through an efficient
procedure for finding all the points along a one-
dimensional line in feature weight space at which
the highest scoring translation hypothesis changes,
given the current set of hypotheses for a particu-
lar sentence. By merging the lists of such points
for all sentences in the training set, he finds all
the points at which the highest scoring hypothesis
changes for any training sentence.
Finding the optimal value of the feature weights
along the line being optimized then requires sim-
ply evaluating the translation quality metric for
each range of values for the feature weights be-
tween two such consecutive points. This can be
done efficiently by tracking incremental changes
in the sufficient statistics for the translation qual-
ity metric as we iterate through the points where
things change. Och uses this procedure as a line
search method in an iterative optimization proce-
dure, until no additional improvement in the trans-
lation quality metric is obtained, given the current
sets of translation hypotheses.
3 Optimization with Random Restarts
Although Och?s line search is globally optimal,
this is not sufficient to guarantee that a series of
line searches will find the globally optimal com-
bination of all feature weights. To avoid getting
stuck at an inferior local optimum during MERT, it
is usual to perform multiple optimization searches
over each expanded set of translation hypotheses
starting from different initial points. Typically, one
of these points is the best point found while op-
timizing over the previous set of translation hy-
potheses.1 Additional starting points are then se-
lected by independently choosing initial values for
each feature weight according to a uniform distri-
bution over a fixed interval, say ?1.0 to +1.0. The
best point reached, starting from either the previ-
ous optimum or one of the random restart points,
is selected as the optimum for the current set of hy-
potheses. This widely-used procedure is described
by Koehn et al (2007, p. 50).
3.1 Preliminary evaluation
In our first experiments, we compared a variant
of Och?s MERT procedure with and without ran-
dom restarts as described above. For our training
and test data we used the English-French subset
of the Europarl corpus provided for the shared task
(Koehn and Monz, 2006) at the Statistical Machine
Translation workshop held in conjunction with the
2006 HLT-NAACL conference. We built a stan-
dard baseline phrasal SMT system, as described
by Koehn et al (2003), for translating from Eng-
lish to French (E-to-F), using the word alignments
and French target language model provided by the
workshop organizers.
We trained a model with the standard eight fea-
tures: E-to-F and F-to-E phrase translation log
1Since additional hypotheses have been added, initiating
an optimization search from this point on the new set of hy-
potheses will often lead to a higher local optimum.
586
probabilities, E-to-F and F-to-E phrase translation
lexical scores, French language model log proba-
bilities, phrase pair count, French word count, and
distortion score. Feature weight optimization was
performed on the designated 2000-sentence-pair
development set, and the resulting feature weights
were evaluated on the designated 2000-sentence-
pair development test set, using the BLEU-4 metric
with one reference translation per sentence.
At each decoding iteration we generated the
100-best translation hypotheses found by our
phrasal SMT decoder. To generate the initial
100-best list, we applied the policy of setting the
weights for features we expected to be positively
correlated with BLEU to 1, the weights for fea-
tures we expected to be negatively correlated with
BLEU to ?1, and the remaining weights to 0. In
this case, we set the initial distortion score weight
to ?1, the phrase count weight to 0, and all other
feature weights to 1.
We made a common modification of the MERT
procedure described by Och, by replacing Pow-
ell?s method (Press, 2002) for selecting the direc-
tions in which to search the feature weight space,
with simple co-ordinate ascent?following Koehn
et al (2007)?repeatedly optimizing one feature
weight at a time while holding the others fixed, un-
til all feature weights are at optimum values, given
the values of the other feature weights. Powell?s
method is not designed to reach a better local op-
timum than co-ordinate ascent, but does have con-
vergence guarantees under certain idealized condi-
tions. However, we have observed informally that,
in MERT, co-ordinate ascent always seems to con-
verge relatively quickly, with Powell?s method of-
fering no clear advantage.
We also modified Och?s termination test slightly.
As noted above, Och suggests terminating the
overall procedure when n-best decoding fails to
produce any hypotheses that have not already been
seen. Without random restarts, this will guarantee
convergence because the last set of feature weights
selected will still be a local optimum.2 However,
if we go through the coordinate ascent procedure
without finding a better set of feature weights, then
we do not have to perform the last iteration of n-
best decoding, because it will necessarily produce
the same n-best lists as the previous iteration, as
2With random restarts, there can be no guarantee of con-
vergence, unless we have a true global optimization method,
or we enumerate all possible hypotheses permitted by the
model.
long as the decoder is deterministic. Thus we can
terminate the overall procedure if either we either
fail to generate any new hypotheses in n-best de-
coding, or the optimium set of feature weights does
not change in the coordinate ascent phase of train-
ing. In fact, we relax the termination test a bit
more than this, and terminate if no feature weight
changes by more than 1.0%.
Without random restarts, we found that MERT
converged in 8 decoding iterations, with the result-
ing model producing a BLEU score of 31.12 on
the development test set. For the experiment with
random restarts, after each iteration of 100-best
decoding, we searched from 20 initial points, 19
points selected by uniform sampling over the in-
terval [?1, 1] for each feature weight, plus the op-
timum point found for the previous set of hypothe-
ses. This procedure converged in 10 decoding it-
erations, with a BLEU score of 32.02 on the de-
velopment test set, an improvement of 0.90 BLEU,
compared to MERT without random restarts.
While the difference in BLEU score with and
without random restarts was substantial, training
with random restarts took much longer. With
our phrasal decoder and our MERT implementa-
tion, optimizing feature weights took 3894 seconds
without random restarts and 12690 seconds with
random restarts.3 We therefore asked the question
whether there was some other way to invest extra
time in training feature weights that might be just
as effective as performing random restarts. The ob-
vious thing to try is using larger n-best lists, so we
re-ran the training without random restarts, using
n-best lists of 200 and 300.
Using n-best lists of 200 produced a noticeable
improvement in training without restarts, converg-
ing in 9 decoding iterations taking 7877 seconds,
and producing a BLEU score of 31.83 on the de-
velopment test set. Using n-best lists of 300 con-
verged in 8 decoding iterations taking 8973 sec-
onds, but the BLEU score on the development test
set fell back to 31.16. Thus, simply increasing the
size of the n-best list does not seem to be a reli-
able method for improving the results obtained by
MERT without random restarts.
4 Random Walk Restarts
In the procedure described above, the initial values
for each feature weight are independently sampled
3Timings are for single-threaded execution using a desk-
top PC with 3.60 GHz Intel Xeon processors.
587
from a uniform distribution over the range [?1, 1].
We have observed anecdotally, however, that if
the selected starting point itself produces a BLEU
score much below the best we have seen so far, co-
ordinate ascent search is very unlikely to take us to
a point that is better than the current best. In order
to bias the selection of restarting points towards
better scores, we select starting points by random
walk from the ending point of the last coordinate
ascent search.
The idea is to perform a series of cautious steps
in feature weight space guided by training set
BLEU. We begin the walk at the ending point of the
last coordinate ascent search; let us call this point
~w
(0)
. Each step updates the feature weights in a
manner inspired by Metropolis-Hastings sampling
(Hastings, 1970). Starting from the current feature
weight vector ~w(i), we sample a small update from
a multivariate Gaussian distribution with mean of
0 and diagonal covariance matrix ?2I . This update
is added to the current value to produce a new po-
tential feature weight vector. The BLEU scores for
the old and the new feature weight vector are com-
pared. The new feature weight vector is always
accepted if the BLEU score on the training set is
improved; however if the BLEU score drops, the
new vector is accepted with a probability that de-
pends on how close the new BLEU score is to the
previous one. After a fixed number of steps, the
walk is terminated, and we produce a value to use
as the initial point for the next round of coordinate
ascent.
There are several wrinkles, however. First, we
prefer that the scores not fall substantially during
the random walk. Therefore we establish a base-
line value of m = BLEU(~w(0)) ? 0.005 (i.e., 1/2
BLEU point below the initial value) and do not al-
low a step to go below this baseline value. To en-
sure this, each step progresses as follows:
~
d
(i)
? GAUSSIAN(0, ?2I)
~v
(i)
= ~w
(i)
+
~
d
(i)
u
(i)
? UNIFORM(0, 1)
~w
(i+1)
=
?
?
?
~v
(i) if BLEU(~v
(i)
)?m
BLEU(~w(i))?m ? u
(i)
~w
(i) otherwise.
With this update rule, we know that ~w(i+1) will
never go below m, since the initial value is not be-
low m, and any step moving below m will result
in a negative ratio and therefore not be accepted.
So far, ?2 is left as a free parameter. An ini-
tial value of 0.001 performs well in our experi-
ence, though in general it may result in steps that
are consistently too small (so that only a very lo-
cal neighborhood is explored) or too large (so that
the vast majority of steps are rejected). Therefore
we devote the first half of the steps to ?burn-in?;
that is, tuning the variance parameter so that ap-
proximately 60% of the steps are accepted. During
burn-in, we compute the acceptance rate after each
step. If it is less than 60%, we multiply ?2 by 0.99;
if greater, we multiply by 1.01.
The final twist is in selection of the point used
for the next iteration of coordinate ascent. Rather
than using the final point of the random walk ~w(n),
we return the feature weight vector that achieved
the highest BLEU score after burn-in: ~w? =
argmax
~w
(i)
,n/2<i?n
BLEU(~w). This ensures that
the new feature weight vector has a relatively high
objective function value yet is likely very different
from the initial point.
4.1 Preliminary evaluation
To evaluate the random walk selection procedure,
we used a similar experimental set-up to the previ-
ous one, testing on the 2006 English-French Eu-
roparl corpus, using n-best lists of 100, and 20
starting points for each coordinate ascent search?
one being the best point found for the previous
hypothesis set, and the other 19 selected by our
random walk procedure. We set the number of
steps to be used in each random walk to 500. This
procedure converged in 6 decoding iterations tak-
ing 8458 seconds, with a BLEU score of 32.13 on
the development test set. This is an improvement
of 0.11 BLEU over the uniform random restart
method, and it also took only 67% as much time.
The speed up was due to the fact that random walk
method converged in 2 fewer decoding iterations,
although the average time per iteration was greater
(1410 seconds vs. 1269 seconds) because of the
extra time needed for the random walk.
5 Hypothesis Set Pruning
MERT with random walk restarts seems to pro-
duce better models than either MERT with uniform
random restarts or with no restarts, but it is still
slower than MERT with no restarts by more than
a factor of 2. The difference between 3894 sec-
onds (1.08 hours) and 8458 seconds (2.35 hours) to
optimize feature weights may not seem important,
given how long the rest of the process of build-
588
ing and training an SMT system takes; however,
to truly optimize an SMT system would actually
require performing feature weight training many
times to find optimum values of hyper-parameters
such as maximum phrase size and distortion limit.
This kind of optimization is rarely done for every
small model change, because of how long feature
weight optimization takes; so it seems well worth
the effort to speed up the optimization process as
much as possible.
To try to speed up the feature weight optimiza-
tion process, we have tried pruning the set of hy-
potheses that MERT is applied to. The time taken
by the random walk and coordinate ascent phases
of MERT with random walk restarts is roughly lin-
ear in the number of translation hypotheses exam-
ined. In the experiment described in Section 4.1,
after the first 100-best decoding iteration there
were 196,319 hypotheses in the n-best lists, and
MERT took 347 seconds. After merging all hy-
potheses from 6 iterations of 100-best decoding
there were 800,580 hypotheses, and MERT took
1380 seconds.
We conjectured that a large proportion of these
hypotheses are both low scoring according to most
submodels and low in measured translation qual-
ity, so that omitting them would make little differ-
ence to the feature weight optimization optimiza-
tion process.4 We attempt to identify such hy-
potheses by extracting some additional informa-
tion from Och?s line search procedure.
Och?s line search procedure takes note of every
hypothesis that is the highest scoring hypothesis
for a particular sentence for some value of the fea-
ture weight being optimized by the line search.
The hypotheses that are never the highest scoring
hypothesis for any combination of feature values
explored effectively play no role in the MERT pro-
cedure. We conjectured that hypotheses that are
never selected as potentially highest scoring in a
particular round of MERT could be pruned from
the hypothesis set without adversely affecting the
quality of the feature weights eventually produced.
We tested two implementations of this type of
hypothesis pruning. In the more conservative im-
plementation, after each decoding iteration, we
note all the hypotheses that are ever ?touched?
(i.e., ever the highest scoring) during the coordi-
nate ascent search either from the initial starting
4Hypotheses that are of low translation quality, but high
scoring according to some submodels, need to be retained so
that the feature weights are tuned to avoid selecting them.
point or from one of the random restarts. Any hy-
pothesis that is never touched is pruned from the
sets of hypotheses that are merged with the results
of subsequent n-best decoding iterations. We refer
to this as ?post-restart? pruning.
In the more aggressive implementation, after
each decoding iteration, we note all the hypothe-
ses that are touched during the coordinate ascent
search from the initial starting point. The hypothe-
ses that are not touched are pruned from the hy-
pothesis set before any random restarts. We refer
to this as ?pre-restart? pruning.
5.1 Preliminary evaluation
We evaluated both post- and pre-restart pruning
with random walk restarts, under the same condi-
tions used to evaluate random walk restarts without
pruning. With post-restart pruning, feature weight
training converged in 8 decoding iterations taking
7790 seconds, with a BLEU score of 32.14 on the
development test set. The last set of restarts of
MERT had 276,134 hypotheses to consider, a re-
duction of more than 65% compared to no prun-
ing. With pre-restart pruning, feature weight train-
ing converged in 7 decoding iterations taking 4556
seconds, with a BLEU score of 32.17 on the devel-
opment test set. The last set of restarts of MERT
had only 64,346 hypotheses to consider, a reduc-
tion of 92% compared to no pruning.
Neither method of pruning degraded translation
quality as measured by BLEU; in fact, BLEU scores
increased by a trivial amount with pruning. Post-
restart pruning speeded up training only slightly,
primarily because it took more decoding iterations
to converge. Time per decoding iteration was re-
duced from 1409 seconds to 974 seconds. Pre-
restart pruning was substantially faster overall, as
well as in terms of time per decoding iteration,
which was 650 seconds.
Additional insight into the differences between
feature weight optimization methods can be gained
by evaluating the feature weight sets produced af-
ter each decoding iteration. Figure 1 plots the
BLEU score obtained on the development test set
as a function of the cumulative time taken to pro-
duce the corresponding feature weights, for each
of the training runs we have described so far.
We observe a clear gap between the results
obtained from random walk restarts and those
from either uniform random restarts or no restarts.
Note in particular that, although the uniform ran-
589
Restart Pruning Num Num Decoding Total MERT Dev-test Conf
Method Method Starts N-best Iterations Seconds Seconds BLEU Level
none none 1 100 8 3894 276 31.12 > 0.999
none none 1 300 8 8973 1173 31.17 > 0.999
none none 1 200 9 7877 717 31.83 0.999
uniform rand none 5 100 7 4294 917 31.95 0.993
uniform rand none 30 100 11 19345 13306 31.98 0.995
uniform rand none 20 100 10 12690 7613 32.02 0.984
uniform rand none 10 100 10 9059 3898 32.02 0.984
random walk pre-restart 30 100 12 9963 3016 32.04 0.999
random walk pre-restart 5 100 11 7696 619 32.10 0.962
random walk none 30 100 7 14055 10887 32.10 0.959
random walk pre-restart 10 100 14 8581 1254 32.10 0.986
random walk post-restart 10 100 9 6985 2236 32.11 0.938
random walk none 20 100 6 8458 5766 32.13 0.909
random walk post-restart 20 100 8 7790 3965 32.14 0.857
random walk none 10 100 8 8338 4280 32.15 0.840
random walk pre-restart 20 100 7 4556 1179 32.17 0.712
random walk post-restart 5 100 10 6103 1114 32.18 0.794
random walk post-restart 30 100 8 9811 6218 32.20 0.554
random walk none 5 100 10 7741 3047 32.21 0.000
Table 1: Extended Evaluation Results.
dom restart method eventually comes within 0.15
BLEU points of the best result using random walk
restarts, it takes far longer to get there. The uni-
form restart run produces noticably inferior BLEU
scores until just before convergence, while with the
random walk method, the BLEU score increases
quite quickly and then stays essentially flat for sev-
eral iterations before convergence.
We also note that there appears to be less real
difference among our three variations on random
walk restarts than there might seem to be from
their times to convergence. Although pre-restart
pruning was much faster to convergence than ei-
ther of the other variants, all three reached approx-
imately the same BLEU score in the same amount
of time, if intermediate points are considered. This
suggests that our convergence test, while more lib-
eral than Och?s, still may be more conservative
than necessary when using random walk restarts.
6 Extended Evaluation
We now extend the previous evaluations in two
ways. First, we repeat all the experiments on opti-
mization with 20 starting points, using 5, 10, and
30 starting points, to see whether we can trade off
training time for translation quality by changing
that parameter setting, and if so, whether any set-
tings seem clearly better than others.
Second, we note that different optimization
methods lead to convergence at different numbers
of decoding iterations. This means that which
method produces the shortest total training time
will depend on the relative time taken by n-best
decoding and the MERT procedure itself (includ-
ing the random walk selection procedure, if that
is used). By co-incidence, these times happened
to be roughly comparable in our experiments.5 In
a situation where decoding is much slower than
MERT, however, the main determinant of overall
training time would be how many decoding iter-
ations are needed. On the other hand, if decod-
ing was made much faster, say, through algorith-
mic improvements or by using a compute cluster,
total training time would be dominated by MERT
proper. We therefore report number of decoding it-
erations to convergence and pure MERT time (ex-
cluding decoding and hypothesis set merging) for
each of our experiments, in addition to total feature
weight training time.
Table 1 reports these three measures of com-
5In our complete set of training experiments encompass-
ing 187 decoding iterations, decoding averaged 521 seconds
per iteration, and MERT (excluding decoding and hypothesis
set merging) averaged 421 seconds per (decoding) iteration.
590
puational effort, plus BLEU score on the devel-
opment test set, sorted by ascending BLEU score,
for 19 variations on MERT: 3 n-best list sizes for
MERT without restarts, and 4 different numbers
of restarts for 4 different versions of MERT with
restarts (uniform random selection, random walk
selection without pruning, random walk selection
with post-restart pruning, and random walk selec-
tion with pre-restart pruning).
The final column of Table 1 is a confidence
score reflecting the estimated probability that the
translation model produced (at convergence) by
the MERT variant for that row of the table is not
as good in terms of BLEU score as the variant that
yielded the highest BLEU score (at convergence)
that we observed in these experiments. These
probabilities were estimated by Koehn?s (2004)
paired bootstrap resampling method, run for at
least 100,000 samples per comparison.
The 11 models obtaining a BLEU score of 31.10
or less are all estimated to be at least 95% likely to
have worse translation quality than the best scor-
ing model. We therefore dismiss these models
from further consideration,6 including all models
trained without random restarts, as well as all mod-
els trained with uniform random restarts, leaving
only models trained with random walk restarts.
With random walk restarts, post-restart prun-
ing remains under consideration at all numbers
of restarts tried. For random walk restarts with-
out pruning, only the model produced by 30 start-
ing points has been eliminated, and for pre-restart
pruning, only the model produced by 20 starts re-
mains under consideration. This suggests that pre-
restart pruning may be too aggressive and, thus,
overly sensitive to the number of restarts.
To get a better picture of the remaining 8 mod-
els, see Figures 2?4. Despite convergence times
ranging from 4456 to 9811 seconds, in Figure 2 we
observe that, if feature weights after each decoding
iteration are considered, the relationships between
training time and BLEU score are remarkably sim-
ilar. In Figure 3, BLEU score varies considerably
up to 3 decoding iterations, but above that, BLEU
scores are very close, and almost flat. In fact, we
see almost no benefit from more than 7 decoding
iterations for any model.
Finally, Figure 4 shows some noticeable differ-
ences between random walk variants in respect to
6We estimate the joint probability that these 11 models are
all worse than our best scoring model to be 0.882, by multi-
plying the confidence scores for all these models.
MERT time proper. Thus, while the choice of ran-
dom walk variant chosen may matter little if de-
coding is slow, it seems that it can have an im-
portant impact if decoding is fast. If we com-
bine the results shown here with the observation
from Figure 3 that there seems to be no benefit to
trying more than 7 decoding iterations, it appears
that perhaps the best trade-off between translation
quality and training time would be obtained by us-
ing post-restart pruning, with 5 starting points per
decoding iteration, cutting off training after 7 iter-
ations. This took a total of 4009 seconds to train,
compared to 7741 seconds for the highest scoring
model on the development test set considered in
Table 1 (produced by random walk restarts with no
pruning, 5 starting points per decoding iteration, at
convergence after 10 iterations).
To validate the proposal to use the suggested
faster training procedure, we compared the two
models under discussion on the 2000 in-domain
sentence pairs for the designated final test set for
our corpus. The model produced by the sug-
gested training procedure resulted in a BLEU score
of 31.92, with the model that scored highest on
the development test set scoring an insignificantly
worse 31.89. In contast, the highest scoring model
of the three trained with no restarts produced a
BLEU score of 31.55 on the final test set, which
was worse than either of the random walk methods
evaluated on the final test set, at confidence levels
exceeding 0.996 according to the paired bootstrap
resampling test.
7 Conclusions
We believe that our results show very convinc-
ingly that using random restarts in MERT im-
proves the BLEU scores produced by the result-
ing models. They also seem to show that starting
point selection by random walk is slightly superior
to uniform random selection. Finally, our exper-
iments suggest that time to carry out MERT can
be significantly reduced by using as few as 5 start-
ing points per decoding iteration, performing post-
restart pruning of hypothesis sets, and cutting off
training after a fixed number of decoding iterations
(perhaps 7) rather than waiting for convergence.
References
Hastings, W. Keith. 1970. Monte Carlo sampling
methods using Markov chains and their applica-
tions. Biometrika 57: 97?109.
591
Koehn, Philipp, and Christof Monz. 2006. Manual
and automatic evaluation of machine translation
between European languages. In Proceedings
of the Workshop on Statistical Machine Trans-
lation, New York City, USA, 102?121.
Koehn, Philipp, Franz Josep Och, and Daniel
Marcu. 2003. Statistical Phrase-Based Trans-
lation. In Proceedings of Human Language
Technology Conference of the North American
Chapter of the Association for Computational
Linguistics, Edmondton, Alberta, Canada, 127?
133.
Koehn, Philipp. 2004. Statistical significance
tests for machine translation evaluation. In Pro-
ceedings of the 2004 Conference on Empiri-
cal Methods in Natural Language Processing,
Barcelona, Spain, 388?395.
Koehn, Philipp, et al 2007. Open Source Toolkit
for Statistical Machine Translation: Factored
Translation Models and Confusion Network De-
coding. Final Report of the 2006 Language En-
gineering Workshop, Johns Hopkins University,
Center for Speech and Language Processing.
Och, Franz Josef. 2003. Minimum error rate train-
ing in statistical machine translation. In Pro-
ceedings of the 41st Annual Meeting of the Asso-
ciation for Computational Linguistics, Sapporo,
Japan, 160?167.
Press, William H., et al 2002. Numerical Recipes
in C++. Cambridge University Press, Cam-
bridge, UK.
30
31
32
33
0 2000 4000 6000 8000 10000 12000 14000
B
L
E
U
s
c
o
r
e
total time in seconds
Figure 1
random walk, pre-
restart pruning, 20 
iterationsrandom walk, post-
restart pruning, 20 
iterationsrandom walk, no 
pruning, 20 iterations
uniform random 
restarts, 20 iterations
no restarts, n-best = 100
no restarts, n-best = 200
no restarts, n-best = 300
30
31
32
33
0 5000 10000
B
L
E
U
s
c
o
r
e
total time in seconds
Figure 2
random walk, pre-restart 
pruning, 20 iterations
random walk, post-
restart pruning, 5 
iterations
random walk, post-
restart pruning, 10 
iterations
random walk, post-
restart pruning, 20 
iterations
random walk, post-
restart pruning, 30 
iterations
random walk, no 
pruning, 5 iterations
random walk, no 
pruning, 10 iterations
random walk, no 
pruning, 20 iterations
30
31
32
33
0 1 2 3 4 5 6 7 8 9 10 11
B
L
E
U
s
c
o
r
e
decoding iterations
Figure 3
random walk, pre-restart 
pruning, 20 iterations
random walk, post-
restart pruning, 5 
iterations
random walk, post-
restart pruning, 10 
iterations
random walk, post-
restart pruning, 20 
iterations
random walk, post-
restart pruning, 30 
iterations
random walk, no 
pruning, 5 iterations
random walk, no 
pruning, 10 iterations
random walk, no 
pruning, 20 iterations
30
31
32
33
0 1000 2000 3000 4000 5000 6000 7000
B
L
E
U
s
c
o
r
e
MERT time in seconds
Figure 4
random walk, pre-restart 
pruning, 20 iterations
random walk, post-restart 
pruning, 5 iterations
random walk, post-restart 
pruning, 10 iterations
random walk, post-restart 
pruning, 20 iterations
random walk, post-restart 
pruning, 30 iterations
random walk, no 
pruning, 5 iterations
random walk, no 
pruning, 10 iterations
random walk, no 
pruning, 20 iterations
592
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 735?744,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Syntactic Models for Structural Word Insertion and Deletion  
Arul Menezes and Chris Quirk 
Microsoft Research 
One Microsoft Way, Redmond, WA 98052, USA 
{arulm, chrisq}@microsoft.com 
 
 
 
 
Abstract 
An important problem in translation neglected 
by most recent statistical machine translation 
systems is insertion and deletion of words, 
such as function words, motivated by linguistic 
structure rather than adjacent lexical context. 
Phrasal and hierarchical systems can only 
insert or delete words in the context of a larger 
phrase or rule. While this may suffice when 
translating in-domain, it performs poorly when 
trying to translate broad domains such as web 
text.  Various syntactic approaches have been 
proposed that begin to address this problem by 
learning lexicalized and unlexicalized rules. 
Among these, the treelet approach uses 
unlexicalized order templates to model 
ordering separately from lexical choice. We 
introduce an extension to the latter that allows 
for structural word insertion and deletion, 
without requiring a lexical anchor, and show 
that it produces gains of more than 1.0% BLEU 
over both phrasal and baseline treelet systems 
on broad domain text. 
1 Introduction 
Among the phenomena that are modeled poorly by 
modern SMT systems is the insertion and deletion 
of words, such as function words, that are 
motivated by the divergent linguistic structure 
between source and target language. To take the 
simplest of examples, the English noun compound 
?file name? would typically be translated into 
Spanish as ?nombre de archivo?, which requires 
the insertion of the preposition ?de?. Conversely, 
when translating from Spanish to English, the ?de? 
must be deleted. At first glance, the problem may 
seem trivial, yet the presence and position of these 
function words can have crucial impact on the 
adequacy and fluency of translation. 
In particular, function words are often used to 
denote key semantic information. They may be 
used to denote case information, in languages such 
as Japanese. Failing to insert the proper case 
marker may render a sentence unreadable or 
significantly change its meaning. Learning these 
operations can be tricky for MT models best suited 
to contiguous word sequences. From a fluency 
standpoint, proper insertion of determiners and 
prepositions can often make the difference between 
laughably awkward output and natural sounding 
translations; consider the output ?it?s a cake piece? 
as opposed to ?it?s a piece of cake?. 
Furthermore, since missing or spurious function 
words can confuse the target language model, 
handling these words properly can have an impact 
beyond the words themselves. 
This paper focuses on methods of inserting and 
deleting words based on syntactic cues, to be used 
in the context of a syntax-informed translation 
system. While the models we build are relatively 
simple and the underlying templates are easy to 
extract, they add significant generalization ability 
to the base translation system, and result in 
significant gains. 
2 Background 
As a motivating example, let us return to the 
English/Spanish pair ?file name? and ?nombre de 
archivo?. In principle, we would want a machine 
translation system to be capable of learning the 
following general transformation: 
 ? NOUN1 NOUN2?  ? NOUN2 de NOUN1? (1) 
Yet even this simple example is beyond the 
capabilities of many common approaches. 
The heavily lexicalized approaches of phrasal 
systems (Koehn et al, 2003), are inherently 
incapable of this generalization. As a proxy, they 
735
acquire phrase pairs such as ?nombre de archivo? 
 ?file name?, ?nombre de?  ?name? and ?de 
archivo?  ?file?. Note that the inserted word is 
attached to adjacent context word(s). When the test 
set vocabulary has significant overlap with the 
training vocabulary, the correct translation can 
often be assembled based on the head or the 
modifying noun. However, as we show in this 
paper, this is woefully inadequate when translating 
truly out-of-domain input. 
In principle, phrase-based translation systems 
may employ insertion phrase pairs such as 
 ?[NULL]?  ?de? (2) 
but the ungrounded nature of this transformation 
makes its use during decoding difficult. Since there 
are no constraints on where such a rule may apply 
and the rule does not consume any input words, the 
decoder must attempt these rules at every point in 
the search. 
The reverse operation 
 ?de?  ?[NULL]? (3) 
is more feasible to implement, though again, there 
is great ambiguity ? a source word may be deleted 
at any point during the search, with identical target 
results. Few systems allow this operation in 
practice. Estimating the likelihood of this operation 
and correctly identifying the contexts in which it 
should occur remain challenging problems. 
Hierarchical systems, such as (Chiang, 2005) in 
principle have the capacity to learn insertions and 
deletions grounded by minimal lexical cues. 
However, the extracted rules use a single non-
terminal. Hence, to avoid explosive ambiguity, 
they are constrained to contain at least one aligned 
pair of words. This restriction successfully limits 
computational complexity at a cost of 
generalization power. 
Syntax-based approaches provide fertile context 
for grounding insertions and deletions. Often we 
may draw a strong correspondence between 
function words in one language and syntactic 
constructions in another. For instance, the syntactic 
approach of Marcu et al (2006) can learn 
unlexicalized rules that insert function words in 
isolation, such as: 
 NP(NN:x0 NN:x1)  x1 de  x0 (4) 
However, as discussed in (Wang, Knight & 
Marcu, 2007), joint modeling of structure and 
lexical choice can exacerbate data sparsity, a 
problem that they attempt to address by tree 
binarization. Nevertheless, as we show below, 
unlexicalized structural transformation rules such 
as (1) and (4) that allow for insertion of isolated 
function words, are essential for good quality 
translation of truly out-of-domain test data.  
In the treelet translation approach (Menezes & 
Quirk, 2007), lexical choice and syntactic re-
ordering are modeled separately using lexicalized 
treelets and unlexicalized order templates. We 
discuss this approach in more detail in Section 4. 
In Section 5, we describe how we extend this 
approach to allow for structural insertion and 
deletion, without the need for content word 
anchors. 
3 Related Work 
There is surprisingly little prior work in this area. 
We previously (Menezes & Quirk, 2005) explored 
the use of deletion operations such as (3) above, 
but these were not grounded in any syntactic 
context, and the estimation was somewhat 
heuristic1. 
The tuple translation model of Crego et al 
(2005), a joint model over source and target 
translations, also provides a means of deleting 
words. In training, sentence pairs such as ?nombre 
de archivo? / ?file name? are first word aligned, 
then minimal bilingual tuples are identified, such 
as ?nombre / name?, ?de / NULL? and ?archivo / 
file?. The tuples may involve deletion of words by 
allowing an empty target side, but do not allow 
insertion tuples with an empty source side. These 
inserted words are bound to an adjacent neighbor. 
An n-gram model is trained over the tuple 
sequences. As a result, deletion probabilities have 
the desirable property of being conditioned on 
adjacent context, yet this context is heavily 
lexicalized, therefore unlikely to generalize well. 
More recently, Li et. al. (2008) describe three 
models for handling ?single word deletion? (they 
discuss, but do not address, word insertion). The 
first model uses a fixed probability of deletion 
                                                          
1
 We assigned channel probabilities based on the sum of the 
Model1 probability of the source word being aligned to NULL 
or one of a list of "garbage collector" words. This exploits the 
property of Model1 that certain high-frequency words tend to 
act as "garbage collectors" for words that should remain 
unaligned. 
736
P(NULL), independent of the source word, 
estimated by counting null alignments in the 
training corpus. The second model estimates a 
deletion probability per-word, P(NULL|w), also 
directly from the aligned corpus, and the third 
model trains an SVM to predict the probability of 
deletion given source language context 
(neighboring and dependency tree-adjacent words 
and parts-of-speech). All three models give large 
gains of 1.5% BLEU or more on Chinese-English 
translation. It is interesting to note that the more 
sophisticated models provide a relatively small 
improvement over the simplest model in-domain, 
and no benefit out-of-domain.  
4  Dependency treelet translation 
As a baseline, we use the treelet translation 
approach (which we previously described in 
Menezes & Quirk, 2007), a linguistically syntax-
based system leveraging a source parser. It first 
unifies lexicalized treelets and unlexicalized 
templates to construct a sentence-specific set of 
synchronous rewrite rules. It then finds the highest 
scoring derivation according to a linear 
combination of models. We briefly review this 
system before describing our current extension. 
4.1 The treelet translation model 
Sentence-specific rewrite rules are constructed by 
unifying information from three sources: a 
dependency parse of the input sentence, a set of 
treelet translation pairs, and a set of unlexicalized 
order templates. Dependency parses are 
represented as trees: each node has a lexical label 
and a part of speech, as well as ordered lists of pre- 
and post-modifiers.  
A treelet represents a connected subgraph of a 
dependency tree; treelet translation pairs consist 
of source and target treelets and a node alignment. 
This alignment is represented by indices: each 
node is annotated with an integer alignment index. 
A source node and a target node are aligned iff they 
have the same alignment index. For instance: 
  ((old1/JJ) man2/NN)  (hombre2 (viejo1)) (5) 
 (man1/NN)  (hombre1) (6) 
Order templates are unlexicalized transduction 
rules that describe the reorderings, insertions and 
deletions associated with a single group of nodes 
that are aligned together. For instance: 
 ((x0:/DT) (x1:/JJ) 1/NN)  ((x0) 1 (x1)) (7) 
 ((x0:/DT) (x1:/JJ) 1/NN)  ((x0) (x1) 1) (8) 
 ((x0:/DT) 1/NN)  ((x0) 1) (9) 
 ((x0:/RB) 1/JJ)  ((x0) 1) (10) 
Each node is either a placeholder or a variable. 
Placeholders, such as 1/NN on the source side or 
1 on the target side, have alignment indices and 
constraints on their parts-of-speech on the source 
side, but are unconstrained lexically (represented 
by the ). These unify at translation time with 
lexicalized treelet nodes with matching parts-of-
speech and alignment.  
Variables, such as x0:/DT on the source side 
and x0: on the target side, also have parts-of-
speech constraints on the source side. Variables are 
used to indicate where rewrite rules are recursively 
applied to translate subtrees. Thus each variable 
label such as x0, must occur exactly once on each 
side. 
In effect, a template specifies how all the 
children of a given source node are reordered 
during translation. If translation were a word-
replacement task, then templates would be just 
simple, single-level tree transducers. However, in 
the presence of one-to-many and many-to-one 
translations and unaligned words,  templates may 
span multiple levels in the tree.  
As an example, order template (7) indicates that 
an NN with two pre-modifying subtrees headed by 
DT and JJ may be translated by using a single 
word translation of the NN, placing the translation 
of the DT subtree as a pre-modifier, and placing 
the translation of the JJ subtree as a post-modifier. 
As discussed below, this template can unify with 
the treelet (6) to produce the following rewrite 
rule: 
 ((x0:DT) (x1:JJ) man/NN)  
 ((x0) hombre (x1)) (11) 
Matching: A treelet translation pair matches an 
input parse iff there is a unique correspondence 
between the source side of the treelet pair and a 
connected subgraph of the input parse.  
An order template matches an input parse iff 
there is a unique correspondence between the 
source side of the template and the input parse, 
with the additional restriction that all children of 
input nodes that correspond to placeholder 
737
template nodes must be included in the 
correspondence. For instance, order template (7) 
matches the parse 
 ((the/DT) (young/JJ) colt/NN) (12) 
but not the parse 
  ((the/DT) (old/JJ) (grey/JJ) mare/NN) (13) 
Finally, an order template matches a treelet 
translation pair at a given node iff, on both source 
and target sides, there is a correspondence between 
the treelet translation nodes and template nodes 
that is consistent with their tree structure and 
alignments. Furthermore, all placeholder nodes in 
the template must correspond to some treelet node. 
Constructing a sentence-specific rewrite rule is 
then a process of unifying each treelet with a 
matching combination of order templates with 
respect to an input parse.  Each treelet node must 
be unified with one and only one order template 
placeholder node. Unifying under these constraints 
produces a rewrite rule that has a one-to-one 
correspondence between variables in source and 
target. For instance, given the input parse: 
 ((the/DT) ((very/RB) old/JJ) man/NN)  (14) 
we can create a rewrite rule from the treelet 
translation pair (5) by unifying it with the order 
template (7), which matches at the node man and 
its descendents, and template (10), which matches 
at the node old, to produce the following sentence-
specific rewrite rule:  
 ((the/DT) ((x1: /RB) old/JJ) man/NN)  
 ((el) hombre ((x1) viejo)) (15) 
Note that by using different combinations of 
order templates, a single treelet can produce 
multiple rewrite rules. Also, note how treelet 
translation pairs capture contextual lexical 
translations but are underspecified with respect to 
ordering, while order templates separately capture 
arbitrary reordering phenomena yet are 
underspecified lexically. Keeping lexical and 
ordering information orthogonal until runtime 
allows for the production of novel transduction 
rules never actually seen in the training corpus, 
leading to improved generalization power. 
 Decoding: Given a set of sentence-specific 
rewrite rules, a standard beam search algorithm is 
used to find the highest scoring derivation. 
Derivations are scored according to a linear 
combination of models. 
4.2 Training 
The process of extracting treelet translation pairs 
and order templates begins with parallel sentences. 
First, the sentence pairs are word segmented on 
both sides, and the source language sentences are 
parsed. Next, the sentence pairs are word aligned 
and the alignments are used to project a target 
language dependency tree. 
Treelet extraction: From each sentence pair , 
with the alignment relation ~, a treelet translation 
pair consisting of the source treelet  	  and the 
target treelet 
 	  is extracted iff: 
(1) There exist    and   
 such that  ~ . 
(2) For all   , and    such that ~,    iff 
  
. 
Order template extraction is attempted starting 
from each node Sroot in the source whose parent is 
not also aligned to the same target word(s). We 
identify Troot, the highest target node aligned to 
Sroot. We initialize the sets S0 as {Sroot} and T0 as 
{Troot}.  We expand S0 to include all nodes 
adjacent to some element of S0 that are (a) 
unaligned, or (b) aligned to some node in T0. The 
converse is applied to T0. This expansion is 
repeated until we reach a fixed point. Together, S0 
and T0 make up the placeholder nodes in the 
extracted order template. We then create one 
variable in the order template for each direct child 
of nodes in S0 and T0 that is not already included in 
the order template. Iff there is a one-to-one word 
alignment correspondence between source and 
target variables, then a template is extracted. This 
restriction leads to clean templates, at the cost of 
excluding all templates involving extraposition. 
5 Insertion/deletion order templates 
In this paper, we extend our previous work to 
allow for insertion and deletion of words, by 
allowing unaligned lexical items as part of the 
otherwise unlexicalized order templates. 
Grounding insertions and deletions in templates 
rather than treelets has two major benefits. First, 
insertion and deletion can be performed even in the 
absence of specific lexical context, leading to 
greater generalization power. Secondly, this 
increased power is tempered by linguistically 
738
informative unlexicalized context. Rather than 
proposing insertions and deletions in any arbitrary 
setting, we are guided by specific syntactic 
phenomena. For instance, when translating English 
noun compounds into Spanish, we often must 
include a preposition; this generalization is 
naturally captured using just parts-of-speech. 
The inclusion of lexical items in order templates 
affects the translation system in only a few places: 
dependency tree projection, order template 
extraction, and rewrite rule construction at runtime. 
Dependency tree projection: During this step of 
the baseline treelet system, unaligned words are by 
default attached low, to the lowest aligned 
neighbor. Although this worked well in 
conjunction with the discriminative order model, it 
prevents unaligned nodes from conditioning on 
relevant context in order templates. Therefore, we 
change the default attachment of unaligned nodes 
to be to the highest aligned neighbor; informal 
experiments showed that this did not noticeably 
impact translation quality in the baseline system. 
For example, consider the source parse and aligned 
target sentence: 
 ((calibrated1/JJ) (camera2/NN) file3/NN) 
 archivo3 de4 c?mara2 calibrado1 (16) 
Using the baseline projection algorithm would 
produce this target dependency tree: 
 (archivo3 ((de4) c?mara2) (calibrado1)) (17) 
Instead, we attach unaligned words high: 
 (archivo3 (de4) (c?mara2) (calibrado1)) (18) 
Order template extraction: In addition to the 
purely unlexicalized templates extracted from each 
training sentence, we also allow templates that 
include lexical items for each unaligned token. For 
each point in the original extraction procedure, 
where S0 or T0 contain unaligned nodes, we now 
extract two templates: The original unlexicalized 
template, and a new template in which only the 
unaligned node(s) contain the specific lexical 
item(s). From the example sentence pair (16), 
using the projected parse (18) we would extract the 
following two templates: 
 ((x0:/JJ) (x1:/NN) 1/NN)   
 (1 (2) (x1) (x0)) (19) 
 ((x0:/JJ) (x1:/NN) 1/NN)   
 (1 (de2) (x1) (x0)) (20) 
Template matching and unification: We extend 
the template matching against the input parse to 
require that any lexicalized source template nodes 
match the input exactly. When matching templates 
to treelet translation pairs, any unaligned treelet 
nodes must be consistent with the corresponding 
template node (i.e. the template node must be 
unlexicalized, or the lexical items must match). On 
the other hand, lexicalized template nodes do not 
need to match any treelet nodes -- insertions or 
deletions may now come from the template alone. 
Consider the following example input parse: 
 ((digital/JJ) (camera/NN)  
     (file/NN) extension/NN) (21) 
The following treelet translation pair provides a 
contextual translation for some of the children, 
including the insertion of one necessary 
preposition: 
 ((file1/NN) extension2/NN)      
     (extension2 (de3) (archivo1)) (22) 
The following order template can provide relative 
ordering information between nodes as well as 
insert the remaining prepositions: 
 ((x0:/JJ) (x1:/NN) (x2:/NN) 1/NN)     
      (1 (de2) (x2) (de3) (x0) (x1)) (23) 
The unification of this template and treelet is 
somewhat complex: the first inserted de is agreed 
upon by both template and treelet, whereas the 
second is inserted by the template alone. This 
results in the following novel rewrite rule: 
 ((x0:/JJ) (x1: /NN) (file) extension)     
      (extension (de) (archivo) (de) (x0) (x1))   (24) 
These relatively minimal changes produce a 
powerful contextualized model of insertion and 
deletion.  
Parameter estimation: The underlying treelet 
system includes a template probability estimated 
by relative frequency. We estimate our lexicalized 
templates in the same way. However early 
experiments showed that this feature alone was not 
enough to allow even common insertions, since the 
probability of even the most common insertion 
templates is much lower than that of unlexicalized 
templates. To improve the modeling capability, we 
included two additional feature functions: a count 
of structurally inserted words, and a count of 
structurally deleted words.  
739
6 Example 
Consider the following English test sentence and 
corresponding Spanish human translation: 
September is National Cholesterol Education 
Month 
Septiembre es el Mes Nacional para la 
Educaci?n sobre el Colesterol 
The baseline treelet system without structural 
insertions translates this sentence as: 
Septiembre es Nacional Colesterol Educaci?n 
Mes 
Not only is the translation missing the appropriate 
articles and prepositions, but also in their absence, 
it fails to reorder the content words correctly. 
Without the missing prepositions, the language 
model does not show a strong preference among 
various orderings of "nacional" "colesterol" 
"educaci?n" and "mes". 
Using structural insertion templates, the highest 
scoring translation of the sentence is now: 
Septiembre es el Mes Nacional de Educaci?n de 
colesterol 
Although the choice of prepositions is not the same 
as the reference, the fluency is much improved and 
the translation is quite understandable. Figure 6.1, 
lists the structural insertion templates that are used 
to produce this translation, and shows how they are 
unified with treelet translation pairs to produce 
sentence-specific rewrite rules, which are in turn 
composed during decoding to produce this 
translation. 
7 Experiments 
We evaluated the translation quality of the system 
using the BLEU metric (Papineni et al, 2002). We 
compared three systems: (a) a standard phrasal 
system using a decoder based on Pharaoh, (Koehn 
et al, 2003), (b) A baseline treelet system using 
unlexicalized order templates and (c) The present 
work, which adds structural insertion and deletion 
templates.  
7.1 Data 
We report results for two language pairs, English-
Spanish and English- Japanese. For English-
Spanish we use two training sets: (a) the Europarl 
corpus provided by the NAACL 2006 Statistical 
Machine Translation workshop (b) a ?general-
domain? data set that includes a broad spectrum of 
data such as governmental data, general web data 
and technical corpora.  
September
NN
is
VB
National
JJ
Cholesterol
NN
Education
NN
Month
NN
Input dependency tree
x2x0 de2 de3 x1
x0:*
JJ
x1:*
NN
x2:*
NN
*1
NN
x2mes x0 de de x1
x0:*
JJ
x1:*
NN
x2:*
NN
month
NN
septiembre es x1
x1:*
NN
is
VB
september
NN
x0 *1 x1
x0:*
NN
*1
VB
x1:*
NN
septiembre es
september
NN
is
VB
mes
month
NN
Treelets Templates Rewrite Rules
el2 el
 
Figure 6.1: Example sentence, matching treelets, structural insertion templates and unified rewrite rules 
740
For English-Japanese we use only the ?general-
domain? data set.  
 Sentence 
pairs 
Tokens Phr 
size 
MERT 
data 
Europarl E-S 730K 15M 7 Europarl 
General E-S 3.7M 41M 4 Web 
General E-J 2.6M 16M 4 Web 
Table 7.1 Training data 
For English-Spanish we report results using the 
four test sets listed in Table 7.2. For English-
Japanese we use only the web test set. The first 
two tests are from the 2006 SMT workshop and the 
newswire test is from the 2008 workshop. The web 
test sets were selected from a random sampling of 
English web sites, with target language translations 
provided by professional translation vendors. All 
test sets have one reference translation.  
 Domain Sentence pairs 
eu-test Europarl  2000 
nc-test News commentary 1064 
News News wire 2051 
Web General web text 5000 
Table 7.2 Test data 
7.2 Models 
The baseline treelet translation system uses all the 
models described in Menezes & Quirk (2007), 
namely:  
? Treelet log probabilities, maximum likelihood 
estimates with absolute discounting.  
? Forward and backward lexical weighting, 
using Model-1 translation log probabilities. 
? Trigram language model using modified 
Kneser-Ney smoothing.  
? Word and phrase count feature functions. 
? Order template log probabilities, maximum 
likelihood estimates, absolute discounting. 
? Count of artificial source order templates.2   
? Discriminative tree-based order model. 
The present work does not use the discriminative 
tree-based order model3 but adds: 
                                                          
2
 When no template is compatible with a treelet, the decoder 
creates an artificial template that preserves source order. This 
count feature allows MERT to deprecate the use of such 
templates. This is analogous to the glue rules of Chiang 
(2005). 
? Count of structural insertions: This counts only 
words inserted via templates, not lexical 
insertions via treelets. 
? Count of structural deletions: This counts only 
words deleted via templates, not lexical 
deletions via treelets. 
The comparison phrasal system was constructed 
using the same alignments and the heuristic 
combination described in (Koehn et al, 2003). 
This system used a standard set of models: 
? Direct and inverse log probabilities, both 
relative frequency and lexical weighting. 
? Word count, phrase count. 
? Trigram language model log probability. 
? Length based distortion model. 
? Lexicalized reordering model. 
7.3 Training 
We parsed the source (English) side of the corpus 
using NLPWIN, a broad-coverage rule-based 
parser able to produce syntactic analyses at varying 
levels of depth (Heidorn, 2000). For the purposes 
of these experiments, we used a dependency tree 
output with part-of-speech tags and unstemmed, 
case-normalized surface words. For word 
alignment we used a training regimen of five 
iterations of Model 1, followed by five iterations of 
a word-dependent HMM model (He, 2007) in both 
directions. The forward and backward alignments 
were combined using a dependency tree-based 
heuristic combination. The word alignments and 
English dependency tree were used to project a 
target tree. From the aligned tree pairs we 
extracted treelet and order template tables. 
For the Europarl systems, we use a 
phrase/treelet size of 7 and train model weights 
using 2000 sentences of Europarl data. For the 
?general-domain? systems, we use a phrase/treelet 
size of 4, and train model weights using 2000 
sentences of web data. 
For any given corpus, all systems used the same 
treelet or phrase size (see Table 7.1) and the same 
trigram language model. Model weights were 
trained separately for each system, data set and 
experimental condition, using minimum error rate 
training to maximize BLEU (Och, 2003). 
                                                                                           
3
 In our experiments, we find that the impact of this model is 
small in the presence of order templates; also, it degrades the 
overall speed of the decoder. 
741
8 Results and Discussion 
Tables 8.1 and 8.4 compare baseline phrasal and 
treelet systems with systems that use various types 
of insertion and deletion templates. 
English-Japanese: As one might expect, the use 
of structural insertion and deletion has the greatest 
impact when translating between languages such as 
English and Japanese that show significant 
structural divergence. In this language pair, both 
insertions and deletions have an impact, for a total 
gain of 1.1% BLEU over the baseline treelet 
system, and 3.6% over the phrasal system. To aid 
our understanding of the system, we tabulated the 
most commonly inserted and deleted words when 
translating from English into Japanese in Tables 
8.2 and 8.3 respectively. Satisfyingly, most of the 
insertions and deletions correspond to well-known 
structural differences between the languages. For 
instance, in English the thematic role of a noun 
phrase, such as subject or object, is typically 
indicated by word order, whereas Japanese uses 
case markers to express this information. Hence, 
case markers such as ??? and ??? need to be 
inserted. Also, when noun compounds are 
translated, an intervening postposition such as ??? 
is usually needed. Among the most common 
deletions are ?the? and ?a?. This is because 
Japanese does not have a notion of definiteness. 
Similarly, pronouns are often dropped in Japanese. 
English-Spanish: We note, in Table 8.4 that 
even between such closely related languages, 
structural insertions give us noticeable 
improvements over the baseline treelet system. On 
the smaller Europarl training corpus the 
improvements range from 0.5% to 1.1% BLEU. 
On the larger training corpus we find that for the 
more in-domain governmental4 and news test sets, 
the effect is smaller or even slightly negative, but 
                                                          
4
 The "general domain" training corpus is a superset of the 
Europarl training set, therefore, the Europarl tests sets are "in-
domain" in both cases. 
on the very broad web test set we still see an 
improvement of about 0.7% BLEU. 
As one might expect, as the training data size 
increases, the generalization power of structural 
insertion and deletions becomes less important 
when translating in-domain text, as more insertions 
and deletions can be handled lexically. 
Nevertheless, the web test results indicate that if 
one hopes to handle truly general input the need 
for structural generalizations remains.  
Unlike in English-Japanese, when translating 
from English to Spanish, structural deletions are 
less helpful. Used in isolation or in combination 
with insertion templates they have a slightly 
negative and/or insignificant impact in all cases. 
We hypothesize that when translating from English 
into Spanish, more words need to be inserted than 
deleted. Conversely, when translating in the 
reverse direction, deletion templates may play a 
bigger role. We were unable to test the reverse 
direction because our syntax-based systems depend 
on a source language parser. In future work we 
hope to address this.  
 % BLEU 
Phrasal 13.41 
Baseline treelet 15.89 
+Deletion only 16.00 
+Insertion only 16.16 
+Deletion and Insertion 17.01 
Table 8.1: English-Japanese system comparisons 
Word Count %age Type 
? 
2844 42% Postposition 
? 
1637 24% Postposition/case marker 
? 
630 9.3% Postposition/case marker 
? 
517 7.6% Punctuation 
? 
476 7.0% Postposition 
?? 
266 3.9% Light verb 
? 
101 1.5% Postposition 
? 
68 1.0% Postposition 
?? 
27 0.40% Light verb 
? 
26 0.38% Punctuation 
? 
19 0.28% Question marker 
Table 8.2: E-J: Most commonly inserted words 
Word Count %age Type 
the 875 59% Definite article 
- 159 11% Punctuation 
a 113 7.7% Indefinite article 
you 53 3.6% Pronoun 
it 53 3.6% Pronoun 
that 26 1.8% Conjunction, Pronoun 
" 23 1.6% Punctuation 
in 16 1.1% Preposition 
. 10 0.68% Punctuation 
's 10 0.68% Possessive 
I 9 0.61% Pronoun 
Table 8.3: E-J: Most commonly deleted words 
 
742
In table 8.5 and 8.6, we list the words most 
commonly inserted and deleted when translating 
the web test using the general English-Spanish 
system. As in English-Japanese, we find that the 
insertions are what one would expect on linguistic 
grounds. However, deletions are used much less 
frequently than insertions and also much less 
frequently than they are in English-Japanese. Only 
53 words are structurally deleted in the 5000 
sentence test set, as opposed to 4728 structural 
insertions. Furthermore, the most common deletion 
is of quotation marks, which is incorrect in most 
cases, even though such deletion is evidenced in 
the training corpus5.  
On the other hand, the next most common 
deletions ?I? and ?it? are linguistically well 
grounded, since Spanish often drops pronouns. 
9 Conclusions and Future Work 
We have presented an extension of the treelet 
translation method to include order templates with 
structural insertion and deletion, which improves 
translation quality under a variety of scenarios, 
particularly between structurally divergent 
languages. Even between closely related 
languages, these operations significantly improve 
the generalizability of the system, providing 
benefit when handling out-of-domain test data. 
Our experiments shed light on a little-studied 
area of MT, but one that is nonetheless crucial for 
high quality broad domain translation. Our results 
affirm the importance of structural insertions, in 
particular, when translating from English into other 
                                                          
5
  In many parallel corpora, quotes are not consistently 
preserved between source and target languages.  
languages, and the importance of both insertions 
and deletions when translating between divergent 
languages. In future, we hope to study translations 
from other languages into English to study the role 
of deletions in such cases.  
References 
Chiang, David. A hierarchical phrase-based model for 
statistical machine translation. ACL 2005.  
Crego, Josep, Jos? Mari?o and Adri? de Gispert. 
Reordered search and tuple unfolding for Ngram-
based SMT. MT Summit 2005. 
He, Xiaodong. Using Word Dependent Transition 
Models in HMM based Word Alignment for 
Statistical Machine Translation. Workshop on 
Statistical Machine Translation, 2007 
de 3509 74% Preposition 
la 555 12% Determiner 
el 250 5.3% Determiner 
se 77 1.6% Reflexive pronoun 
que 63 1.3% Relative pronoun 
los 63 1.3% Determiner 
del 57 1.2% Preposition+Determiner 
, 42 0.89% Punctuation 
a 30 0.63% Preposition 
en 21 0.44% Preposition 
lo 9 0.19% Pronoun 
las 6 0.13% Determiner 
Table 8.5: E-S: Most commonly inserted words 
" 38 72% Punctuation 
I 5 9.4% Pronoun 
it 2 3.8% Pronoun 
, 2 3.8% Punctuation 
- 2 3.8% Punctuation 
Table 8.6: E-S: Most commonly deleted words 
  EU-devtest EU-test NC-test Newswire Web test 
EUROPARL E-S       
 Phrasal 27.9 28.5 24.7 17.7 17.0 
 Baseline treelet 27.65 28.38 27.00 18.46 18.71 
 +Deletion only 27.66 28.39 26.97 18.46 18.64 
 +Insertion only 28.23 28.93 28.10 19.08 19.43 
 +Deletion and Insertion 28.27 29.08 27.82 18.98 19.19 
GENERAL E-S       
 Phrasal 28.79 29.19 29.45 21.12 27.91 
 Baseline treelet 28.67 29.33 32.49 21.90 27.42 
 +Deletion only 28.67 29.27 32.25 21.69 27.47 
 +Insertion only 28.90 29.70 32.53 21.84 28.30 
 +Deletion and Insertion 28.34 29.41 32.66 21.70 27.95 
Table 8.4: English-Spanish system comparisons, %BLEU 
 
743
Heidorn, George. ?Intelligent writing assistance?. In 
Dale et al Handbook of Natural Language 
Processing, Marcel Dekker. 2000 
Koehn, Philipp, Franz Josef Och, and Daniel Marcu. 
Statistical phrase based translation. NAACL 2003. 
Chi-Ho Li, Dongdong Zhang, Mu Li, Ming Zhou, Hailei 
Zhang. An Empirical Study in SourceWord Deletion 
for Phrase-based Statistical Machine Translation. 
Workshop on Statistical Machine Translation, 2008 
Marcu, Daniel, Wei Wang, Abdessamad Echihabi, and 
Kevin Knight. SPMT: Statistical Machine 
Translation with Syntactified Target Language 
Phrases. EMNLP-2006. 
Menezes, Arul, and Chris Quirk. Microsoft Research 
Treelet translation system: IWSLT evaluation. 
International Workshop on Spoken Language 
Translation, 2005 
Menezes, Arul, and Chris Quirk. Using Dependency 
Order Templates to Improve Generality in 
Translation. Workshop on Statistical Machine 
Translation, 2007 
Och, Franz Josef. Minimum error rate training in 
statistical machine translation. ACL 2003. 
Papineni, Kishore, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. BLEU: a method for automatic evaluation 
of machine translation. ACL 2002. 
Wang, Wei, Kevin Knight and Daniel Marcu. 
Binarizing Syntax Trees to Improve Syntax-Based 
Machine Translation Accuracy. EMNLP-CoNLL, 
2007 
744
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 746?755,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Less is More: Significance-Based N-gram Selection
for Smaller, Better Language Models
Robert C. Moore Chris Quirk
Microsoft Research
Redmond, WA 98052, USA
{bobmoore,chrisq}@microsoft.com
Abstract
The recent availability of large corpora
for training N-gram language models has
shown the utility of models of higher or-
der than just trigrams. In this paper, we
investigate methods to control the increase
in model size resulting from applying stan-
dard methods at higher orders. We in-
troduce significance-based N-gram selec-
tion, which not only reduces model size,
but also improves perplexity for several
smoothing methods, including Katz back-
off and absolute discounting. We also
show that, when combined with a new
smoothing method and a novel variant of
weighted-difference pruning, our selection
method performs better in the trade-off be-
tween model size and perplexity than the
best pruning method we found for modi-
fied Kneser-Ney smoothing.
1 Introduction
Statistical language models are potentially useful
for any language technology task that produces
natural-language text as a final (or intermediate)
output. In particular, they are extensively used in
speech recognition and machine translation. De-
spite the criticism that they ignore the structure of
natural language, simple N-gram models, which
estimate the probability of each word in a text
string based on the N?1 preceding words, remain
the most widely-used type of model.
Until the late 1990s, N-gram language models
of order higher than trigrams were seldom used.
This was due, at least in part, to the fact the
amounts of training data available did not produce
significantly better results from higher-order mod-
els. Since that time, however, increasingly large
amounts of language model training data have be-
come available ranging from approximately one
billion words (the Gigaword corpora from the
Linguistic Data Consortium) to trillions of words
(Brants et al, 2007). With these larger resources,
the use of language models based on 5-grams to
7-grams is becoming increasingly common.
The problem we address here is that, even when
relatively modest amounts of training data are
used, high-order N-gram language models esti-
mated by standard techniques can be impractically
large. Hence, we investigate ways of building
high-order N-gram language models without dra-
matically increasing model size. This is, of course,
the same goal behind much previous work on lan-
guage model pruning, including that of Seymore
and Rosenfeld (1996), Stolcke (1998), and Good-
man and Gao (2000). We take a novel approach,
however, which we refer to as significance-based
N-gram selection. We reject a higher-order esti-
mate of the probability of a particular word in a
particular context whenever the distribution of ob-
servations for the higher-order estimate provides
no evidence that the higher-order estimate is bet-
ter than our backoff estimate.
Perhaps our most surprising result is that
significance-based N-gram selection not only re-
duces language model size, but it also improves
perplexity when applied to a number of widely-
used smoothing methods, including Katz backoff
and several variants of absolute discounting.1 In
contrast, experiments applying previous pruning
methods to Katz backoff (Seymore and Rosen-
feld, 1996; Stolcke, 1998) and absolute discount-
ing (Goodman and Gao, 2000) always found the
lowest perplexity model to be the unpruned model.
We tested significance-based selection on only
one smoothing method without obtaining im-
proved perplexity: modified Kneser-Ney (KN)
1For most of the standard smoothing methods mentioned
here, we refer the reader to the excellent comparative study
of smoothing methods by Chen and Goodman (1998). Refer-
ences to the original sources may be found there.
746
smoothing (Chen and Goodman, 1998). This
is unfortunate, because modified KN smoothing
generally seems to have the lowest perplexity of
any known smoothing method for N-gram lan-
guage models; in our tests it had a lower perplex-
ity than any of the other models, with or with-
out significance-based N-gram selection. How-
ever, when we compared modified KN smooth-
ing to our best results applying N-gram selection
to other smoothing methods for multiple N-gram
orders, two of our models outperformed modified
KN in terms of perplexity for a given model size.
Of course, the trade-off between perplexity and
model size for modified KN can also be im-
proved by pruning. So, in a final set of ex-
periments we found the best combinations we
could for pruned modified KN models, and we did
the same for our best model using significance-
based selection. The best pruning method for
the latter turned out to be a novel modifica-
tion of weighted-difference pruning (Seymore and
Rosenfeld, 1996) that was especially convenient
to compute given our method for performing
significance-based N-gram selection. The final re-
sult is that our best model using significance-based
selection and modified weighted difference prun-
ing always had a better size/perplexity trade-off
than pruned modified KN, with up to about 8%
perplexity reduction for a given model size.
2 Significance-Based N-gram Selection
The idea of using a statistical test to decide
whether to use a higher- or lower-order estimate of
an N-gram probablity is not new. It was perhaps
first proposed by Ron, et al (1996), who suggested
using a threshold on relative entropy (Kullback-
Liebler divergence) as an appropriate test to de-
cide whether to extend the context used to predict
the next token in a sequence. Stolcke (1998) used
the same metric in his work on language model
pruning, and he also pointed out that weighted dif-
ference pruning is, in fact, an approximation of
relative entropy pruning. However, while relative
entropy pruning is based on a statistical test, it is
not a significance test. The difference in probabil-
ity represented by a certain relative entropy value
can be statistically significant when measured on
a large corpus, but not significant when measured
on a small corpus.
The primary test we use to choose between
higher- or lower-order estimates of an N-gram
probablity is inspired by an insight of Jedynak and
Khudanpur (2005). They note that, given a set
of y observations of a multinomial distribution,
the observed counts will have the highest proba-
bilty of any possible set of y observations for the
maximum likelihood estimate (MLE) model de-
rived from the relative frequencies of those obser-
vations. In general, however, the MLE model will
not be the only model for which this set of obser-
vations is the most probable set of y observations.
Jedynak and Khudanpur call the set of such mod-
els the maximum likelihood set (MLS) for the ob-
servations.
Jedynak and Khudanpur argue that the obser-
vations alone do not support choosing the MLE
over other members of the MLS. The MLE may
assign the observations a higher probability than
other members of the MLS, but that may be an
accident of what outcomes are possible given the
number of observations. If we flip a coin 9 times
and get 5 heads, is there any reason to believe that
the probability of heads is closer to the MLE 5/9
than it is to 5/10? No, because 5/9 is as close as
we can come to 5/10, given 9 observations.
We apply this insight to the problem of N-
gram selection as follows: For each word w
n
in a context w
1
...w
n?1
with a backoff estimate
for the probability of that word in that context
? p(w
n
|w
2
...w
n?1
),
2 we do not include an explicit
estimate of p(w
n
|w
1
...w
n?1
) in our model, if the
backoff estimate is within the MLS of the counts
for w
1
...w
n
and w
1
...w
n?1
.
This requires finding the MLS of a set of obser-
vations only for binomial distributions (rather than
the general multinomial distributions studied by
Jedynak and Khudanpur), which has a very sim-
ple solution:
MLS(x, y) =
{
p
?
?
?
?
x
y + 1
? p ?
x+ 1
y + 1
}
where x is the count for w
1
...w
n
, y is the count for
w
1
...w
n?1
, and p is a possible backoff probabilty
estimate for p(w
n
|w
1
...w
n?1
). In this case, the
MLS is the set of binomial distributions that have
x successes as their mode given y trials, which is
well-known to be specified by this formula.
We describe this method as ?significance-
based? because we can consider our criterion as
a significance test in which we take the backoff
2
p(w
n
|w
2
...w
n?1
) being the next lower-order estimate,
and ? being the backoff weight for the context w
1
...w
n?1
.
747
probability estimate as the null hypothesis for the
estimate in the higher-order model, and we set the
rejection threshold to the lowest possible value;
we reject the null hypothesis (the backoff probabil-
ity) if there are any outcomes for the given number
of trials that are more likely, according to the null
hypothesis, than the one we observed.
We make a few refinements to this basic idea.
First, we never add an explicit higher-order esti-
mate to our model, if the next lower-order estimate
is not explicitly stored in the model. This enables
us to keep only the next lower-order model avail-
able while performing N-gram selection.
Next, we observe that in some cases the higher-
order estimate for p(w
n
|w
1
...w
n?1
) may not fall
within the MLS for the observed counts, due to
smoothing. In this case, we prefer the backoff
probability estimate if it lies within the MLS or be-
tween the smoothed higher-order estimate and the
MLS. Otherwise, we would reject the backoff es-
timate for being outside the MLS, only to replace
it with a higher-order estimate even farther outside
the MLS.
Finally, we note that the backoff probability es-
timate for an N-gram not observed in the train-
ing data sometimes falls outside the corresponding
MLS, which in the 0-count case simplifies to
MLS(0, y) =
{
p
?
?
?
?
0 ? p ?
1
y + 1
}
When this happens, we include an explicit higher-
order estimate p(w
n
|w
1
...w
n?1
) = 1/(y + 1),
which is the upper limit of the MLS. This is similar
to Rosenfeld and Huang?s (1993) ?confidence in-
terval capping? method for reducing unreasonably
high backoff estimates for unobserved N-grams.
In order to apply this treatment of 0-count N-
grams, we sort the explicitly-stored N-grams for
each backoff context by decreasing probability.
For each higher-order context, to find the 0-count
N-grams subject to the 1/(y + 1) limit, we tra-
verse the sorted list of explicitly-stored N-grams
for its backoff context. When we encounter an N-
gram whose extension to the higher-order context
was not observed in the training data, we give it
an explicit probability of 1/(y+1), if its weighted
backoff probability is greater than that. We stop
the traversal as soon as we encounter an N-gram
for the backoff context that has a weighted backoff
probability less than or equal to 1/(y+1), which in
practice means we actually examine only a small
number of backoff probabilities for each context.
3 Finding Backoff Weights by Iterative
Search
The approach described above is very attractive
from a theoretical perspective, but it has one prac-
tical complication. To decide which N-grams for
each context to explicitly include in the higher-
order model, we need to know the backoff weight
for the context, but we cannot compute the backoff
weight until we know exactly which higher-order
N-grams are included in the model.
We address this problem by iteratively solving
for a backoff weight that yields a normalized prob-
ability distribution. For each context, we guess
an initial value for the backoff weight and keep
track of the sum of the probabilites resulting from
applying our N-gram selection method with that
backoff weight. If the sum is greater than 1.0, by
more than a convergence threshold, we reduce the
estimated backoff weight and iterate. If the sum
is less than 1.0, by more than the threshold, we
increase the estimated weight and iterate.
It is easy to see that, for all standard smooth-
ing methods, the function from backoff weights
to probability sums is piece-wise linear. Within
a region where no decision changes about which
N-grams to include in the model, the probability
sum is a linear function of the backoff weight. At
values of the backoff weight where the set of se-
lected N-grams changes, the function can be dis-
continous. With a little more effort, one can see
that the linear segments overlap with respect to the
probability sum in such a way that there will al-
ways be one or more values of the backoff weight
that make the probability sum equal 1.0, with one
specific exception.
The exception arises because of the capping of
backoff probabilites for unobserved N-grams. It
is possible for there to be a context for which
all observed N-grams are included in the higher-
order model, the probabilities for all unobserved
N-grams are either capped at 1/(y + 1) or effec-
tively 0 due to arithmetic underflow, and the prob-
ability sum is less than 1.0. For some smoothing
methods, the probability sum cannot be increased
in this situation by increasing the backoff weight.
We check for this situation, and if it arises, we
increase the cap on the 0-count probability just
enough to make the probability sum equal 1.0.
That exception aside, we iteratively find back-
off weights as follows: For an initial estimate
of the backoff weight for a context, we compute
748
what the backoff weight would be for the base
smoothing method without N-gram selection. If
that value is less than 1.0, we use it as our ini-
tial estimate, otherwise we use 1.0, which annec-
dotally seems to produce better models than ini-
tial estimates greater than 1.0, in situations where
there are multiple solutions. If the first iteration of
N-gram selection produces a probability sum less
than 1.0, we repeatedly double the estimated back-
off weight until we obtain a sum greater than or
equal to 1.0, or we encounter the special situation
previously described. If the initial probability sum
is greater than 1.0, we repeatedly halve the esti-
mated backoff weight until we obtain a sum less
than or equal to 1.0.
Once we have values for the backoff weight that
produce probability sums on both sides of 1.0, we
have a solution bracketed, and we can use standard
numerical search techniques to find that solution.
At every subsequent iteration, we try a value for
the backoff weight between the largest value we
have tried that produces a sum less than 1.0 and
the smallest value we have tried that produces a
sum greater than 1.0. We stop when the difference
between these values of the backoff weight is less
than a convergence threshold.
We use a combination of simple techniques to
choose the next value of the backoff weight to try.
The primary technique we use is called the ?false
position method?, which basically solves the lin-
ear equation defined by the two current bracketing
values and corresponding probability sums. The
advantage of this method is that, if our bracket-
ing points lie on the same linear segment of our
function, we obtain a solution in one step. The
disadvantage of the method is that it sometimes
approaches the solution by a long sequence of tiny
steps from the same side.
We try to detect the latter situation by keeping
track of the number of consecutive iterations that
make a step in the same direction. If this num-
ber reaches 10, we take the next step by the bi-
section method, which simply tries the value of
the backoff weight halfway between our two cur-
rent bracketing values. In practice, this combined
search method works very well, taking an average
of less than four iterations per backoff weight.
4 Modified Weighted-Difference Pruning
While the N-gram selection method described
above considerably reduces the number of para-
meters in a high-order language model, we may
wish to reduce language model size even more.
The concept of significance-based N-gram selec-
tion to produce smaller models could be extended
by relaxing our criterion for using backoff distrib-
utions in place of explicit higher-order probability
estimates, but true significance tests at more re-
laxed thresholds that are accurate for small counts
are expensive to compute; so we resort to more
conventional language model pruning methods.
In our experiments, we tried four methods for
additional pruning: simple count cutoffs, relative
entropy pruning (REP) (Stolcke, 1998), and two
modified versions of Seymore and Rosenfeld?s
(1996) weighted-difference pruning (WDP). In the
notation we have been using, Seymore and Rosen-
feld?s WDP criterion for using a backoff estimate,
in place of an explicit higher-order estimate, is that
the quantity
K?
(
log(p(w
n
|w
1
...w
n?1
))?
log(?
u
p(w
n
|w
2
...w
n?1
))
)
be less than a pruning threshold, where K is
the Good-Turing-discounted training set count for
w
1
...w
n
, and ?
u
is the backoff weight for the un-
pruned model.
The first of our modified version of WDP uses
the following quantity instead:
p(w
1
...w
n
)?
?
?
?
?
?
log(p(w
n
|w
1
...w
n?1
))?
log(?
p
p(w
n
|w
2
...w
n?1
))
?
?
?
?
?
where p(w
1
...w
n
) is an estimate of the probability
of w
1
...w
n
and ?
p
is the backoff weight for the
pruned model.
We make three modifications to WDP in this
formula. First, we follow a suggestion of Stol-
cke (1998) by replacing the discounted training
set count K of w
1
...w
n
with an estimate the joint
probability of w
1
...w
n
, computed by chaining the
explicit probability estimates, according to our
model, for all N-gram lengths up to n.
The second modification to WDP is that we use
the absolute value of the difference of the log prob-
abilities. By using the signed difference of the log
probabilities, Seymore and Rosenfeld will always
prune a higher-order probability estimate if it is
less than the backoff estimate. But the backoff es-
timate may well be too high. Using the absolute
value of the difference avoids this problem.
749
p(w
n
|w
1
. . . w
n?1
) =
?
?
?
?
?
?
?
?
?
?
?
?
w
1
...w
n?1
C(w
1
...w
n
)?D
n,C(w
1
...w
n
)
C(w
1
...w
n?1
)
+ ?
w
1
...w
n?1
p(w
n
|w
2
. . . w
n?1
)
if C(w
1
. . . w
n
) > 0
?
w
1
...w
n?1
p(w
n
|w
2
. . . w
n?1
) if C(w
1
. . . w
n
) = 0
?
w
1
...w
n?1
= ?
|{w
?
|C(w
1
...w
n?1
w
?
)>0}|
C(w
1
...w
n?1
)
?
w
1
...w
n?1
= 1? ?
w
1
...w
n?1
Figure 1: New language model smoothing method
The final modification is that we compute the
difference in log probability with respect to the
backoff weight for the pruned model rather than
the unpruned model, which we are able to do by
performing the pruning inside our iterative search
for the value of the backoff weight. We do this
because, if the backoff weight is changed signifi-
cantly by pruning, backoff estimates that meet the
pruning criterion with the old backoff weight may
no longer meet the criterion with the new back-
off weight, and vice versa. Since the new backoff
weight is the one that will be used in the pruned
model, that seems to be the one that should be used
to make pruning decisions.
Our second variant of modified WDP is like the
first, but it estimates p(w
1
...w
n
) simply by divid-
ing Seymore and Rosenfeld?s discounted N-gram
count K by the total number of highest-order N-
grams in the training corpus. This is equivalent to
smoothing only the highest-order conditional N-
gram model in estimating p(w
1
...w
n
), estimating
all the lower-order probabilities in the chain by the
corresponding MLE model. We refer to this joint
probability estimate as ?partially-smoothed?, and
the one suggested by Stolcke as ?fully-smoothed?.
5 Evaluation
We carried out three sets of evaluations to test
the new techniques described above. First we
compared the perplexity of full models and mod-
els reduced by significance-based N-gram selec-
tion for seven language model smoothing meth-
ods. For the best three results in that comparison,
we looked at the trade-off between perplexity and
model size over a range of N-gram orders. Finally,
we tried various pruning methods to further reduce
model size, and then compared the best result we
obtained using previous techniques with the best
result we obtained using our new techniques.
5.1 Data and Base Smoothing Methods
For training, parameter optimzation, and test data
we used English text from the WMT-06 Europarl
corpus (Koehn and Monz, 2006). We trained on
the designated 1,003,349 sentences (27,493,499
words) of English language model training data,
and used 2000 sentences each for testing and pa-
rameter optimization, from the English half of the
English-French dev and devtest data sets.
We conducted our experiments on seven lan-
guage model smoothing methods. Five of these
are well-known: (1) interpolated absolute dis-
counting with one discount per N-gram length, es-
timated according to the formula derived by Ney
et al (1994); (2) Katz backoff with Good-Turing
discounts for N-grams occurring 5 times or less;
(3) backoff absolute discounting with Ney et al
formula discounts; (4) backoff absolute discount-
ing with one discount used for all N-gram lengths,
optimized on held-out data; (5) modified interpo-
lated Kneser-Ney smoothing with three discounts
per N-gram length, estimated according to the for-
mulas suggested by Chen and Goodman (1998).
We also experimented with two variants of a
new smoothing method that we have recently de-
veloped. Full details of the new method are given
elsewhere (Moore and Quirk, 2009), but since it is
not well-known, we summarize the method here.
Smoothed N-gram probabilities are defined by the
formulas shown in Figure 1, for all n such that
N ? n ? 2,
3 where N is the greatest N-gram
length used in the model. The novelty of this
model is that, while it is an interpolated model, the
interpolation weights ? for the lower-order model
3For n = 2, we take the expression p(w
n
|w
2
. . . w
n?1
)
to denote a unigram probability estimate p(w
2
).
750
base select percent
Method PP PP change
1 interp-AD-fix 62.6 61.6 -1.6
2 Katz backoff 59.8 56.1 -7.9
3 backoff-AD-fix 59.9 54.3 -9.3
4 backoff-AD-opt 58.8 54.4 -7.5
5 KN-mod-fix 51.6 54.6 +5.8
6 new-fix 56.1 52.1 -7.1
7 new-opt 53.7 52.0 -3.3
Table 1: Perplexity results for N-gram selection
are not constrained to match the backoff weights
? for the lower-order model. This allows the in-
terpolation weights to be set independently of the
discounts D, with the backoff weights being ad-
justed to normalize the resulting distributions.
The motivation for this is to let the D para-
meters correct for potential overestimation of the
probabilities for observed N-grams, while the ?
parameter (which determines the ? and ? interpo-
lation parameters) somewhat independently cor-
rects for quantization errors caused by the fact that
only certain probabilities can be derived from in-
teger observed counts, even after discounting. ? is
interpretable as the estimated mean quantization
error for each distinct count for a given context.
We tested two variants of the new method, (6)
one in which the D parameters and the ? parameter
are set by fixed criteria, and (7) one in which a sin-
gle value for all D parameters and the value of the
? parameter are optimized on held-out data. For
the fixed value of ?, we assume that, since the dis-
tance between possible N-gram counts, after dis-
counting, is approximately 1.0, their mean quan-
tization error would be approximately 0.5. For
the fixed discount parameters, we use three values
for each N-gram length: D
1
for N-grams whose
count is 1, D
2
for N-grams whose count is 2, and
D
3
for N-grams whose count is 3 or more. We
set these values to be the discounts for 1-counts,
2-counts, and 3-counts estimated by the Good-
Turing method. This yields the formula
D
r
= r ? (r + 1)
N
r+1
N
r
,
for 1 ? r ? 3, where N
r
is the number of distinct
N-grams of the length in question occuring r times
in the training set.
In all experiments, the unigram language
model is an un-smoothed, closed-vocabulary MLE
model. We use this unigram model, because there
is no simple, principled way of assigning prob-
abilities to individual out-of-vocabulary (OOV)
words. The only principled solution to this prob-
lem that we are aware of is to use a character-
based model, but this seems overly complicated
for something that is orthogonal to the main points
of this study, and of minor practical importance.
Since we make no provision for OOV words in the
models, OOV words are also omitted from all per-
plexity measurements. Thus, the perplexity num-
bers are systematically lower than they would be
if OOVs were taken into account, but they are all
comparable in this regard.
5.2 Results for Significance-Based N-gram
Selection
Table 1 shows the minimum perplexity (with re-
spect to N-gram order) of language models up to
7-grams for each of the seven smoothing methods
discussed above, with and without significance-
based N-gram selection. N-gram selection im-
proved the perplexity of all models, except for
modified KN. The lowest overall perplexity re-
mains that of the base modified KN method, but
with N-gram selection, the two variants of the new
smoothing method come very close to it.
If we cared only about perplexity, that would be
the end of the story, but we also care about lan-
guage model size. The results in Table 1 were ob-
tained on models estimated using just the counts
needed to cover the parameter optimization and
test sets; so to accurately measure model size, we
trained full language models using base modifed
KN, and the two variants of the new method with
N-gram selection. The resulting sizes of the mod-
els represented in backoff form (in terms of total
number of probability and backoff parameters) are
shown in Figure 2 as function of N-gram length,
from trigrams up to 7-grams for KN and up to
10-grams for the two new models. We see that
beyond 4-grams the model sizes diverge dramati-
cally, with the new models incorporating N-gram
selection leveling off, but the modified KN model
(or any standard model) continuing to grow in size,
apparently linearly in the N-gram order.
In Figure 3, we show the relationship between
perplexity and model size for the same three
models, varying N-gram order. We see that be-
tween about 20 million and 45 million parameters,
both of the new models incorporating significance-
751
020
40
60
80
100
120
140
160
180
3 4 5 6 7 8 9 10
M
illi
on
s o
f m
od
el 
pa
ra
m
et
er
s
N-gram length
No N-gram 
selection
new-opt +               
N-gram selection
new-fix +               
N-gram selection
Figure 2: Model size vs. N-gram length
51
52
53
54
55
56
57
58
59
60
61
0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160
Per
pl
exit
y
Millions of Model Parameters
KN -mod -fix
new-opt +               
N-gram selection
new-fix +               
N-gram selection
Figure 3: Perplexity vs. model size
based N-gram selection seem to outperform mod-
ified KN, and that the best of the three is, in fact,
the new model with fixed parameter values.
5.3 Results for Additional Pruning
We further tested modified KN smoothing, and our
new smoothing method with fixed parameter val-
ues and significance-based N-gram selection, with
additional pruning. We compared several pruning
methods on trigram models: count cutoffs, REP,4
and our two modified versions of WDP.
Figure 4 shows the resulting combinations of
perplexity and model size for REP and modified
WDP at various pruning thresholds, and for count
cutoffs of 1, 2, and 3 for both bigrams and trigrams
(n > 1) and for trigrams only (n > 2), applied to
4Thanks to Asela Gunawardana for the use of his REP
tool.
our new smoothing method with fixed parameter
values, together with significance-based N-gram
selection. Overall, modified WDP with fully-
smoothed joint probability estimates performs the
best. It is has lower perplexity than count cut-
offs at all model sizes tested, and is about equal
to REP at very severe pruning levels and superior
to REP with less pruning. Modified WDP with
fully-smoothed joint probabilities is about equal
to modified WDP with partially-smoothed joint
probabilities at the highest and lowest pruning lev-
els tested, but superior in between.
Figure 4 also shows the result of applying
modified WDP with fully-smoothed joint prob-
abilities to our new smoothing method with-
out significance-based N-gram selection, to test
whether the former subsumes the gains from the
latter. We see that modified WDP does not render
752
60
61
62
63
64
65
66
67
68
69
70
71
72
73
0 1 2 3 4 5 6 7 8 9 10
Per
pl
exit
y
Millions of Model Parameters
count cutoffs for n > 1
count cutoffs for n > 2
relative entropy pruning
modifed WDP w/fully -
smoothed joint probs,    
wo/N -gram selection
modified WDP w/partially -
smoothed joint probs
modified WDP w/fully -
smoothed joint probs
Figure 4: Pruning methods for new smoothing technique with N-gram selection
60
61
62
63
64
65
66
67
68
69
70
71
72
0 1 2 3 4 5 6 7 8 9 10 11
Per
pl
exit
y
Millions of Model Parameters
modified WDP w/fully -
smoothed joint probs
relative entropy pruning
modified WDP w/partially -
smoothed joint probs
count cutoffs for n > 1
count cutoffs for n > 2
Figure 5: Pruning methods for modified KN smoothing
N-gram selection redundant except at very severe
pruning levels, much like REP.
Figure 5 shows the results of applying the
same four pruning methods to KN smoothing.
Count cutoffs clearly perform the best with KN
smoothing. It is interesting to note, however,
that?contrary to the results for our new smooth-
ing method?with KN smoothing, modified WDP
with partially-smoothed joint probabilities is sig-
nificantly better than either REP or modified WDP
with fully-smoothed joint probabilities. We be-
lieve this is due to the fact that the latter two meth-
ods both estimate the joint probabilities by chain-
ing the lower-order conditional probabilities from
the fully-smoothed model, which in the case of
KN smoothing are designed specifically to cover
N-grams that have not been observed, and are poor
estimates for the probabilities of lower-order N-
grams that do occcur in the training data.
Finally, we compared the new smoothing
method with N-gram selection and modified WDP
with fully-smoothed joint probabilities against
modified KN smoothing with count cutoffs, us-
ing combinations of pruning parameter values and
N-gram order that yielded the best size/perplexity
trade-offs. The results are shown in Figure 6. At
all model sizes within the range of these experi-
ments, the new method with significance-based N-
gram selection and modified WDP had lower per-
plexity than modifed KN with count cutoffs?up
to about 8% lower at greater pruning levels.
This experiment also suggests that the
size/perplexity trade-off is easier to optimize
for our new combination of smoothing, N-gram
selection, and modified WDP, than for KN
smoothing with count cut-offs. Table 2 shows the
753
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
0 1 2 3 4 5 6 7 8 9 10 11 12 13
Per
pl
exit
y
Millions of Model Parameters
KN -mod -fix with 
count cutoffs
new-fix with N -gram 
selection and 
modified WDP
Figure 6: Comparison of two best pruned language models
PP N CC n >
69.9 3 4 1
64.7 4 4 1
62.1 4 3 1
59.0 4 2 1
56.5 4 2 2
54.4 4 1 2
53.6 5 1 2
53.4 6 1 2
53.3 7 1 2
Table 2: Optimal pruning parameters for KN-
mod-fix with count cutoffs
perplexity (PP), maximum N-gram length (N),
count cutoff (CC), and N-gram lengths to which
the count cutoffs are applied (n >) for the points
on the curve for pruned KN in Figure 6. Although
some tendencies are discernable, it seems clear
that a significant part of the space of combinations
of N, CC, and ?n >? parameter values must be
searched to find the best points for trading off
perplexity against model size. Table 3 shows
maximum N-gram length and pruning threshold
values for the points on the corresponding curve
for our new approach. Here the situation is much
simpler. The best trade-off points are found by
varying the pruning threshold, and including
in the model all N-grams that pass the pruning
threshold, regardless of N-gram length.
6 Conclusions
We have shown that significance-based N-gram
selection can simultaneously reduce both model
PP N threshold
67.2 10 10?6.5
62.7 10 10?6.75
59.3 10 10?7.0
56.4 10 10?7.25
54.6 10 10?7.5
53.7 10 10?7.75
53.2 10 10?8.0
Table 3: Optimal pruning parameters for new-fix
with N-gram selection and modified WDP
size and perplexity when applied to a number of
language model smoothing methods, including the
widely-used Katz backoff and absolute discount-
ing methods. We are not aware of any other tech-
nique that does this. We also found that, when
combined with a new smoothing method and a
novel variant of weighted difference pruning, our
N-gram selection method outperformed modified
Kneser-Ney smoothing?using the best form of
pruning we found for that approach?with respect
to the trade-off between model size and model
quality.
As our next steps, first, we need to verify that
the results obtained on a moderate-sized train-
ing corpus are repeatable on much larger corpora.
Second, we plan to extend this work to incorpo-
rate language model size reduction by word clus-
tering, which has been shown by Goodman and
Gao (2000) to produce additional gains when com-
bined with previous methods of language model
pruning.
754
References
Brants, Thorsten, Ashok C. Popat, Peng Xu, Franz
J. Och, and Jeffrey Dean. 2007. Large language
models in machine translation. In Proceedings
of EMNLP 2007, 858?867.
Chen, Stanley F., and Joshua Goodman. 1998.
An empirical study of smoothing techniques for
language modeling. Technical Report TR-10-
98, Harvard University.
Goodman, Joshua, and Jianfeng Gao. 2000. Lan-
guage model size reduction by pruning and
clustering. In Proceedings of ICSLP 2000, 110?
113.
Jedynak, Bruno M., and Sanjeev Khudanpur.
2005. Maximum likelihood set for estimating a
probability mass function. Neural Computation
17, 1?23.
Koehn, Philipp, and Christof Monz. 2006. Manual
and automatic evaluation of machine translation
between European languages. In Proceedings
of WMT 2006, 102?121.
Moore, Robert C., and Chris Quirk. 2009. Im-
proved smoothing for N-gram language mod-
els based on ordinary counts. In Proceedings
of ACL-IJCNLP 2009.
Ney, Hermann, Ute Essen, and Reinhard Kneser.
1994. On structuring probabilistic dependen-
cies in stochastic language modelling. Com-
puter Speech and Language, 8, 1?38.
Ron, Dana, Yoram Singer, and Naftali Tishby.
1996. The power of amnesia: learning proba-
bilistic automata with variable memory length.
Machine Learning, 25, 117?149.
Rosenfeld, Ronald, and Xuedong Huang. 1993.
Improvements in stochastic language modeling.
In Proceedings of HLT 1993, 107?111.
Seymore, Kristie, and Ronald Rosenfeld. 1996.
Scalable Trigram Backoff Language Models. In
Proceedings of ICSLP 1996. 232?235.
Stolcke, Andreas. 1998. Entropy-based pruning
of backoff language models. In Proceedings,
DARPA News Transcription and Understanding
Workshop 1998, 270?274.
755
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 9?16,
New York, June 2006. c?2006 Association for Computational Linguistics
Do we need phrases? Challenging the conventional wisdom in Statistical 
Machine Translation 
Chris Quirk and Arul Menezes 
Microsoft Research 
One Microsoft Way 
Redmond, WA  98052  USA 
{chrisq,arulm}@microsoft.com 
 
Abstract 
We begin by exploring theoretical and 
practical issues with phrasal SMT, several 
of which are addressed by syntax-based 
SMT. Next, to address problems not 
handled by syntax, we propose the 
concept of a Minimal Translation Unit 
(MTU) and develop MTU sequence 
models. Finally we incorporate these 
models into a syntax-based SMT system 
and demonstrate that it improves on the 
state of the art translation quality within a 
theoretically more desirable framework. 
1. Introduction 
The last several years have seen phrasal statistical 
machine translation (SMT) systems outperform 
word-based approaches by a wide margin (Koehn 
2003). Unfortunately the use of phrases in SMT is 
beset by a number of difficult theoretical and 
practical problems, which we attempt to 
characterize below. Recent research into syntax-
based SMT (Quirk and Menezes 2005; Chiang 
2005) has produced promising results in 
addressing some of the problems; research 
motivated by other statistical models has helped 
to address others (Banchs et al 2005). We refine 
and unify two threads of research in an attempt to 
address all of these problems simultaneously. 
Such an approach proves both theoretically more 
desirable and empirically superior. 
In brief, Phrasal SMT systems employ phrase 
pairs automatically extracted from parallel 
corpora. To translate, a source sentence is first 
partitioned into a sequence of phrases I = s1?sI. 
Each source phrase si is then translated into a 
target phrase ti. Finally the target phrases are 
permuted, and the translation is read off in order. 
Beam search is used to approximate the optimal 
translation. We refer the reader to Keohn et al 
(2003) for a detailed description. Unless 
otherwise noted, the following discussion is 
generally applicable to Alignment Template 
systems (Och and Ney 2004) as well. 
1.1. Advantages of phrasal SMT 
Non-compositionality 
Phrases capture the translations of idiomatic and 
other non-compositional fixed phrases as a unit, 
side-stepping the need to awkwardly reconstruct 
them word by word. While many words can be 
translated into a single target word, common 
everyday phrases such as the English password 
translating as the French mot de passe cannot be 
easily subdivided. Allowing such translations to 
be first class entities simplifies translation 
implementation and improves translation quality. 
Local re-ordering 
Phrases provide memorized re-ordering decisions. 
As previously noted, translation can be 
conceptually divided into two steps: first, finding 
a set of phrase pairs that simultaneously covers 
the source side and provides a bag of translated 
target phrases; and second, picking an order for 
those target phrases. Since phrase pairs consist of 
memorized substrings of the training data, they 
are very likely to produce correct local re-
orderings. 
Contextual information 
Many phrasal translations may be easily 
subdivided into word-for-word translation, for 
instance the English phrase the cabbage may be 
translated word-for-word as le chou. However we 
note that la is also a perfectly reasonable word-
for-word translation of the, yet la chou is not a 
grammatical French string. Even when a phrase 
appears compositional, the incorporation of 
contextual information often improves translation 
9
quality. Phrases are a straightforward means of 
capturing local context.  
1.2. Theoretical problems with phrasal SMT 
Exact substring match; no discontiguity 
Large fixed phrase pairs are effective when an 
exact match can be found, but are useless 
otherwise. The alignment template approach 
(where phrases are modeled in terms of word 
classes instead of specific words) provides a 
solution at the expense of truly fixed phrases. 
Neither phrasal SMT nor alignment templates 
allow discontiguous translation pairs. 
Global re-ordering 
Phrases do capture local reordering, but provide 
no global re-ordering strategy, and the number of 
possible orderings to be considered is not 
lessened significantly. Given a sentence of n 
words, if the average target phrase length is 4 
words (which is unusually high), then the re-
ordering space is reduced from n! to only (n/4)!: 
still impractical for exact search in most 
sentences. Systems must therefore impose some 
limits on phrasal reordering, often hard limits 
based on distance as in Koehn et al (2003) or 
some linguistically motivated constraint, such as 
ITG (Zens and Ney, 2004). Since these phrases 
are not bound by or even related to syntactic 
constituents, linguistic generalizations (such as 
SVO becoming SOV, or prepositions becoming 
postpositions) are not easily incorporated into the 
movement models. 
Probability estimation 
To estimate the translation probability of a phrase 
pair, several approaches are used, often 
concurrently as features in a log-linear model. 
Conditional probabilities can be estimated by 
maximum likelihood estimation. Yet the phrases 
most likely to contribute important translational 
and ordering information?the longest ones?are 
the ones most subject to sparse data issues. 
Alternately, conditional phrasal models can be 
constructed from word translation probabilities; 
this approach is often called lexical weighting 
(Vogel et al 2003). This avoids sparse data 
issues, but tends to prefer literal translations 
where the word-for-word probabilities are high 
Furthermore most approaches model phrases as 
bags of words, and fail to distinguish between 
local re-ordering possibilities. 
Partitioning limitation 
A phrasal approach partitions the sentence into 
strings of words, making several questionable 
assumptions along the way. First, the probability 
of the partitioning is never considered. Long 
phrases tend to be rare and therefore have sharp 
probability distributions. This adds an inherent 
bias toward long phrases with questionable MLE 
probabilities (e.g. 1/1 or 2/2). 1 
Second, the translation probability of each 
phrase pair is modeled independently. Such an 
approach fails to model any phenomena that reach 
across boundaries; only the target language model 
and perhaps whole-sentence bag of words models 
cross phrase boundaries. This is especially 
important when translating into languages with 
agreement phenomena. Often a single phrase does 
not cover all agreeing modifiers of a headword; 
the uncovered modifiers are biased toward the 
most common variant rather than the one agreeing 
with its head. Ideally a system would consider 
overlapping phrases rather than a single 
partitioning, but this poses a problem for 
generative models: when words are generated 
multiple times by different phrases, they are 
effectively penalized. 
1.3. Practical problem with phrases: size 
In addition to the theoretical problems with 
phrases, there are also practical issues. While 
phrasal systems achieve diminishing returns due 
                                                          
1
 The Alignment Template approach differs slightly here. 
Phrasal SMT estimates the probability of a phrase pair as: 
=
'
)',(
),()|(
t
tscount
tscount
st?  
The Alignment Template method incorporates a loose 
partitioning probability by instead estimating the probability 
as (in the special case where each word has a unique class): 
)(
),()|(
scount
tscount
stp =  
Note that these counts could differ significantly. Picture a 
source phrase that almost always translates into a 
discontiguous phrase (e.g. English not becoming French ne 
? pas), except for the rare occasion where, due to an 
alignment error or odd training data, it translates into a 
contiguous phrase (e.g. French ne parle pas). Then the first 
probability formulation of ne parle pas given not would be 
unreasonably high. However, this is a partial fix since it 
again suffers from data sparsity problems, especially on 
longer templates where systems hope to achieve the best 
benefits from phrases. 
10
to sparse data, one does see a small incremental 
benefit with increasing phrase lengths. Given that 
storing all of these phrases leads to very large 
phrase tables, many research systems simply limit 
the phrases gathered to those that could possibly 
influence some test set. However, this is not 
feasible for true production MT systems, since the 
data to be translated is unknown. 
2. Previous work 
2.1. Delayed phrase construction 
To avoid the major practical problem of phrasal 
SMT?namely large phrase tables, most of which 
are not useful to any one sentence?one can 
instead construct phrase tables on the fly using an 
indexed form of the training data (Zhang and 
Vogel 2005; Callison-Burch et al 2005). 
However, this does not relieve any of the 
theoretical problems with phrase-based SMT. 
2.2. Syntax-based SMT 
Two recent systems have attempted to address the 
contiguity limitation and global re-ordering 
problem using syntax-based approaches. 
Hierarchical phrases 
Recent work in the use of hierarchical phrases 
(Chiang 2005) improves the ability to capture 
linguistic generalizations, and also removes the 
limitation to contiguous phrases. Hierarchical 
phrases differ from standard phrases in one 
important way: in addition to lexical items, a 
phrase pair may contain indexed placeholders, 
where each index must occur exactly once on 
each side. Such a formulation leads to a formally 
syntax-based translation approach, where 
translation is viewed as a parallel parsing problem 
over a grammar with one non-terminal symbol. 
This approach significantly outperforms a phrasal 
SMT baseline in controlled experimentation. 
Hierarchical phrases do address the need for 
non-contiguous phrases and suggest a powerful 
ordering story in the absence of linguistic 
information, although this reordering information 
is bound in a deeply lexicalized form. Yet they do 
not address the phrase probability estimation 
problem; nor do they provide a means of 
modeling phenomena across phrase boundaries. 
The practical problems with phrase-based 
translation systems are further exacerbated, since 
the number of translation rules with up to two 
non-adjacent non-terminals in a 1-1 monotone 
sentence pair of n source and target words is 
O(n6), as compared to O(n2) phrases. 
Treelet Translation 
Another means of extending phrase-based 
translation is to incorporate source language 
syntactic information. In Quirk and Menezes 
(2005) we presented an approach to phrasal SMT 
based on a parsed dependency tree representation 
of the source language. We use a source 
dependency parser and project a target 
dependency tree using a word-based alignment, 
after which we extract tree-based phrases 
(?treelets?) and train a tree-based ordering model. 
We showed that using treelets and a tree-based 
ordering model results in significantly better 
translations than a leading phrase-based system 
(Pharaoh, Koehn 2004), keeping all other models 
identical. 
Like the hierarchical phrase approach, treelet 
translation succeeds in improving the global re-
ordering search and allowing discontiguous 
phrases, but does not solve the partitioning or 
estimation problems. While we found our treelet 
system more resistant to degradation at smaller 
phrase sizes than the phrase-based system, it 
nevertheless suffered significantly at very small 
phrase sizes. Thus it is also subject to practical 
problems of size, and again these problems are 
exacerbated since there are potentially an 
exponential number of treelets. 
2.3. Bilingual n-gram channel models 
To address on the problems of estimation and 
partitioning, one recent approach transforms 
channel modeling into a standard sequence 
modeling problem (Banchs et al 2005). Consider 
the following aligned sentence pair in Figure 1a. 
In such a well-behaved example, it is natural to 
consider the problem in terms of sequence 
models. Picture a generative process that 
produces a sentence pair in left to right, emitting a 
pair of words in lock step. Let M = ? m1, ?, mn ? 
be a sequence of word pairs mi = ? s, t ?. Then one 
can generatively model the probability of an 
aligned sentence pair using techniques from n-
gram language modeling: 
11

=
?
?
=
?
?
=
=
k
i
i
nii
k
i
i
i
mmP
mmP
MPATSP
1
1
1
1
1
)|(
)|(
)(),,(
 
 When an alignment is one-to-one and 
monotone, this definition is sufficient. However 
alignments are seldom purely one-to-one and 
monotone in practice; Figure 1b displays common 
behavior such as one-to-many alignments, 
inserted words, and non-monotone translation. To 
address these problems, Banchs et al (2005) 
suggest defining tuples such that: 
(1) the tuple sequence is monotone, 
(2) there are no word alignment links between 
two distinct tuples, 
(3) each tuple has a non-NULL source side, 
which may require that target words 
aligned to NULL are joined with their 
following word, and 
(4) no smaller tuples can be extracted without 
violating these constraints. 
Note that M is now a sequence of phrase pairs 
instead of word pairs. With this adjusted 
definition, even Figure 1b can be generated using 
the same process using the following tuples: 
m1 = ? the, l? ? 
m2 = ? following example, exemple suivant ? 
m3 = ? renames, change le nom ? 
m4 = ? the, de la ? 
m5 = ? table, table ? 
There are several advantages to such an 
approach. First, it largely avoids the partitioning 
problem; instead of segmenting into potentially 
large phrases, the sentence is segmented into 
much smaller tuples, most often pairs of single 
words. Furthermore the failure to model a 
partitioning probability is much more defensible 
when the partitions are much smaller. Secondly, 
n-gram language model probabilities provide a 
robust means of estimating phrasal translation 
probabilities in context that models interactions 
between all adjacent tuples, obviating the need for 
overlapping mappings. 
These tuple channel models still must address 
practical issues such as model size, though much 
work has been done to shrink language models 
with minimal impact to perplexity (e.g. Stolcke 
1998), which these models could immediately 
leverage. Furthermore, these models do not 
address the contiguity problem or the global 
reordering problem. 
3. Translation by MTUs 
In this paper, we address all four theoretical 
problems using a novel combination of our 
syntactically-informed treelet approach (Quirk 
and Menezes 2005) and a modified version of 
bilingual n-gram channel models (Banchs et al 
2005). As in our previous work, we first parse the 
sentence into a dependency tree. After this initial 
parse, we use a global search to find a candidate 
that maximizes a log-linear model, where these 
candidates consist of a target word sequence 
annotated with a dependency structure, a word 
alignment, and a treelet decomposition.  
We begin by exploring minimal translation 
units and the models that concern them. 
3.1. Minimal Translation Units 
Minimal Translation Units (MTUs) are related to 
the tuples of Banchs et al (2005), but differ in 
several important respects. First, we relieve the 
restriction that the MTU sequence be monotone. 
This prevents spurious expansion of MTUs to 
incorporate adjacent context only to satisfy 
monotonicity. In the example, note that the 
previous algorithm would extract the tuple 
?following example, exemple suivant? even though 
the translations are mostly independent. Their 
partitioning is also context dependent: if the 
sentence did not contain the words following or 
suivant, then ? example, exemple ? would be a 
single MTU. Secondly we drop the requirement 
that no MTU have a NULL source side. While 
some insertions can be modeled in terms of 
adjacent words, we believe more robust models 
can be obtained if we consider insertions as 
 
(a) Monotone aligned sentence pair 
 
 
(b) More common non-monotone aligned sentence pair 
 
Figure 1. Example aligned sentence pairs. 
12
independent units. In the end our MTUs are 
defined quite simply as pairs of source and target 
word sets that follow the given constraints: 
(1) there are no word alignment links between 
distinct MTUs, and 
(2) no smaller MTUs can be extracted without 
violating the previous constraint. 
Since our word alignment algorithm is able to 
produce one-to-one, one-to-many, many-to-one, 
one-to-zero, and zero-to-one translations, these 
act as our basic units. As an example, let us 
consider example (1) once again. Using this new 
algorithm, the MTUs would be: 
m1 = ? the, l? ? 
m2 = ? following, suivant ? 
m3 = ? example, exemple ? 
m4 = ? renames, change le nom ? 
m5 = ? NULL, de ? 
m6 = ? the, la ? 
m7 = ? table, table ? 
A finer grained partitioning into MTUs further 
reduces the data sparsity and partitioning issues 
associated with phrases. Yet it poses issues in 
modeling translation: given a sequence of MTUs 
that does not have a monotone segmentation, how 
do we model the probability of an aligned 
translation pair? We propose several solutions, 
and use each in a log-linear combination of 
models. 
First, one may walk the MTUs in source order, 
ignoring insertion MTUs altogether. Such a 
model is completely agnostic of the target word 
order; instead of generating an aligned source 
target pair, it generates a source sentence along 
with a bag of target phrases. This approach 
expends a great deal of modeling effort in 
regenerating the source sentence, which may not 
be altogether desirable, though it does condition 
on surrounding translations. Also, it can be 
evaluated on candidates before orderings are 
considered. This latter property may be useful in 
two-stage decoding strategies where translations 
are considered before orderings. 
Secondly, one may walk the MTUs in target 
order, ignoring deletion MTUs. Where the source-
order MTU channel model expends probability 
mass generating the source sentence, this model 
expends a probability mass generating the target 
sentence and therefore may be somewhat 
redundant with the target language model. 
Finally, one may walk the MTUs in 
dependency tree order. Let us assume that in 
addition to an aligned source-target candidate 
pair, we have a dependency parse of the source 
side. Where the past models conditioned on 
surface adjacent MTUs, this model conditions on 
tree adjacent MTUs. Currently we condition only 
on the ancestor chain, where parent1(m) is the 
parent MTU of m, parent2(m) is the grandparent 
of m, and so on: 
))(|()(),,( 11 mparentmPMPATSP n
Mm
?
?
??=  
This model hopes to capture information 
completely distinct from the other two models, 
such as translational preferences contingent on the 
head, even in the presence of long distance 
dependencies. Note that it generates unordered 
dependency tree pairs.  
All of these models can be trained from a 
parallel corpus that has been word aligned and the 
source side dependency parsed. We walk through 
each sentence extracting MTUs in source, target, 
and tree order. Standard n-gram language 
modeling tools can be used to train MTU 
language models. 
3.2. Decoding 
We employ a dependency tree-based beam search 
decoder to search the space of translations. First 
the input is parsed into a dependency tree 
  English French English Japanese 
Training Sentences 300,000 500,000 
 Words 4,441,465 5,198,932 7,909,198 9,379,240 
 Vocabulary 63,343 59,290 79,029 95,813 
 Singletons 35,328 29,448 44,111 52,911 
Development test Sentences 200 200 
 Words 3,045 3,456 3,436 4,095 
Test Sentences 2,000 2,000 
 Words 30,010 34,725 35,556 3,855 
 OOV rate 5.5% 4.6% 6.9% 6.8% 
Table 4.1 Data characteristics 
13
structure. For each input node in the dependency 
tree, an n-best list of candidates is produced. 
Candidates consist of a target dependency tree 
along with a treelet and word alignment. The 
decoder generally assumes phrasal cohesion: 
candidates covering a substring (not subsequence) 
of the input sentence produce a potential substring 
(not subsequence) of the final translation. In 
addition to allowing a DP / beam decoder, this 
allows us to evaluate string-based models (such as 
the target language model and the source and 
target order MTU n-gram models) on partial 
candidates. This decoder is unchanged from our 
previous work: the MTU n-gram models are 
simply incorporated as feature functions in the 
log-linear combination. In the experiments section 
the MTU models are referred to as model set (1). 
3.3. Other translation models 
Phrasal channel models 
We can estimate traditional channel models using 
maximum likelihood or lexical weighting: 
? ?
? ?
?
?
? ? ?
? ? ?
?
?
=
=
=
=
)(),(
InverseM1
)(),(
DirectM1
)(),(
InverseMLE
)(),(
DirectMLE
)|(),,(
)|(),,(
)(*,
),(),,(
,*)(
),(),,(
Atreelets s t
Atreelets t s
Atreelets
Atreelets
tspATSf
stpATSf
c
cATSf
c
cATSf
?? ? ?
?? ? ?
??
??
?
??
?
??
 
We use word probability tables p(t | s) and p(s | t) 
estimated by IBM Model 1 (Brown et al 1993). 
Such models can be built over phrases if used in a 
phrasal decoder or over treelets if used in a treelet 
decoder. These models are referred to as set (2). 
Word-based models 
A target language model using modified Kneser-
Ney smoothing captures fluency; a word count 
feature offsets the target LM preference for 
shorter selections; and a treelet/phrase count helps 
bias toward translations using fewer phrases. 
These models are referred to as set (3). 
|)(|),,(
||),,(
)|(),,(
tphrasecoun
wordcount
||
1
1
targetLM
AtreeletsATSf
TATSf
ttPATSf
T
i
i
nii
=
=
= ?
=
?
?
 
Syntactic models 
As in Quirk and Menezes (2005), we include a 
linguistically-informed order model that predicts 
the head-relative position of each node 
independently, and a tree-based bigram target 
language model; these models are referred to as 
set (4). 
?
?
?
?
=
=
Tt
Tt
tparenttPATSf
ATStpositionPATSf
))(|(),,(
),,|)((),,(
treeLM
order
  
4. Experimental setup 
We evaluate the translation quality of the system 
using the BLEU metric (Papineni et al, 02) under 
a variety of configurations. As an additional 
baseline, we compare against a phrasal SMT 
decoder, Pharaoh (Koehn et al 2003).  
4.1. Data 
Two language pairs were used for this 
comparison: English to French, and English to 
Japanese. The data was selected from technical 
software documentation including software 
manuals and product support articles; Table 4.1 
presents the major characteristics of this data. 
4.2. Training 
We parsed the source (English) side of the 
corpora using NLPWIN, a broad-coverage rule-
based parser able to produce syntactic analyses at 
varying levels of depth (Heidorn 2002). For the 
purposes of these experiments we used a 
dependency tree output with part-of-speech tags 
and unstemmed surface words. Word alignments 
were produced by GIZA++ (Och and Ney 2003) 
with a standard training regimen of five iterations 
of Model 1, five iterations of the HMM Model, 
and five iterations of Model 4, in both directions. 
These alignments were combined heuristically as 
described in our previous work. 
We then projected the dependency trees and 
used the aligned dependency tree pairs to extract 
treelet translation pairs, train the order model, and 
train MTU models. The target language models 
were trained using only the target side of the 
corpus. Finally we trained model weights by 
maximizing BLEU (Och 2003) and set decoder 
optimization parameters (n-best list size, timeouts 
14
etc) on a development test set of 200 held-out 
sentences each with a single reference translation. 
Parameters were individually estimated for each 
distinct configuration. 
Pharaoh 
The same GIZA++ alignments as above were 
used in the Pharaoh decoder (Koehn 2004). We 
used the heuristic combination described in (Och 
and Ney 2003) and extracted phrasal translation 
pairs from this combined alignment as described 
in (Koehn et al, 2003). Aside from MTU models 
and syntactic models (Pharaoh uses its own 
ordering approach), the same models were used: 
MLE and lexical weighting channel models, 
target LM, and phrase and word count. Model 
weights were also trained following Och (2003). 
5. Results 
We begin with a broad brush comparison of 
systems in Table 5.1. Throughout this section, 
treelet and phrase sizes are measured in terms of 
MTUs, not words. By default, all systems 
(including Pharaoh) use treelets or phrases of up 
to four MTUs, and MTU bigram models. The first 
results reiterate that the introduction of 
discontiguous mappings and especially a 
linguistically motivated order model (model set 
(4)) can improve translation quality. Replacing 
the standard channel models (model set (2)) with 
MTU bigram models (model set (1)) does not 
appear to degrade quality; it even seems to boost 
quality on EF. Furthermore, the information in the 
MTU models appears somewhat orthogonal to the 
phrasal models; a combination results in 
improvements for both language pairs. 
The experiments in Table 5.2 compare quality 
using different orders of MTU n-gram models. 
(Treelets containing up to four MTUs were still 
used as the basis for decoding; only the order of 
the MTU n-gram models was adjusted.) A 
unigram model performs surprisingly well. This 
supports our intuition that atomic handling of 
non-compositional multi-word translations is a 
major contribution of phrasal SMT. Furthermore 
bigram models increase translation quality 
supporting the claim that local context is another 
contribution. Models beyond bigrams had little 
impact presumably due to sparsity and smoothing. 
Table 5.3 explores the impact of using different 
phrase/treelet sizes in decoding. We see that 
adding MTU models makes translation more 
resilient given smaller phrases. The poor 
performance at size 1 is not particularly 
surprising: both systems require insertions to be 
lexically anchored: the only decoding operation 
allowed is translation of some visible source 
phrase, and insertions have no visible trace. 
6. Conclusions 
In this paper we have teased apart the role of 
 EF EJ 
Phrasal decoder (Pharaoh) 
  Model sets (2),(3) 45.8?2.0 32.9?0.9 
Treelet decoder, without discontiguous mappings 
  Model sets (2),(3) 45.1?2.1 33.2?0.9 
  Model sets (2),(3),(4) 48.4?2.0 34.8?0.9 
Treelet decoder, with discontiguous mappings 
  Model sets (2),(3) 46.4?2.1 34.3?0.9 
  Model sets (2),(3),(4) 48.7?2.1 34.9?0.9 
  Model sets (1),(3),(4) 49.6?2.1 33.9?0.8 
  Model sets (1)-(4) 50.5?2.1 36.2?0.9 
 
Table 5.1. Broad system comparison. 
 EF EJ 
Treelet decoder, model sets (1),(3),(4) 
  MTU unigram 47.8?2.1 33.2?0.9 
  MTU bigram 49.6?2.1 33.9?0.8 
  MTU trigram 49.9?2.0 34.0?0.9 
  MTU 4-gram 49.6?2.1 34.1?0.9 
Treelet decoder, model sets (1)-(4)  
  MTU unigram 48.6?2.1 34.3?1.0 
  MTU bigram 50.5?2.1 36.2?0.9 
  MTU trigram 48.9?2.0 36.1?0.9 
  MTU 4-gram 50.4?2.0 36.2?1.0 
 
Table 5.2. Varying MTU n-gram model order. 
Table 5.3. Varying phrase / treelet size. 
 
 Phrasal decoder 
model sets (2),(3) 
Treelet decoder: MTU bigram 
model sets (1),(3),(4) 
Treelet decoder: MTU bigram 
model sets (1)-(4) 
Size EF EJ EF EJ EF EJ 
1 32.6?1.8 20.5?0.7 26.3?1.3 15.4?0.7 29.8?1.4 16.7?0.7 
2 40.4?1.9 29.7?0.7 48.7?2.1 32.4?0.9 47.7?2.1 33.8?0.8 
3 44.3?2.1 30.7?0.9 48.5?2.0 34.6?0.9 48.5?2.0 35.1?0.9 
4 45.8?2.0 32.9?0.9 49.6?2.1 33.9?0.8 50.5?2.1 36.2?0.9 
 
15
phrases and handled each contribution via a 
distinct model best suited to the task. Non-
compositional translations stay as MTU phrases. 
Context and robust estimation is provided by 
MTU-based n-gram models. Local and global 
ordering is handled by a tree-based model. 
The first interesting result is that at normal 
phrase sizes, augmenting an SMT system with 
MTU n-gram models improves quality; whereas 
replacing the standard phrasal channel models by 
the more theoretically sound MTU n-gram 
channel models leads to very similar 
performance. 
Even more interesting are the results on smaller 
phrases. A system using very small phrases (size 
2) and MTU bigram models matches (English-
French) or at least approaches (English-Japanese) 
the performance of the baseline system using 
large phrases (size 4). While this work does not 
yet obviate the need for phrases, we consider it a 
promising step in that direction. 
An immediate practical benefit is that it allows 
systems to use much smaller phrases (and hence 
smaller phrase tables) with little or no loss in 
quality. This result is particularly important for 
syntax-based systems, or any system that allows 
discontiguous phrases. Given a fixed length limit, 
the number of surface phrases extracted from any 
sentence pair of length n where all words are 
uniquely aligned is O(n), but the number of 
treelets is potentially exponential in the number of 
children; and the number of rules with two gaps 
extracted by Chiang (2005) is potentially O(n3). 
Our results using MTUs suggest that such 
systems can avoid unwieldy, poorly estimated 
long phrases and instead anchor decoding on 
shorter, more tractable knowledge units such as 
MTUs, incorporating channel model information 
and contextual knowledge with an MTU n-gram 
model. 
Much future work does remain. From 
inspecting the model weights of the best systems, 
we note that only the source order MTU n-gram 
model has a major contribution to the overall 
score of a given candidate. This suggests that the 
three distinct models, despite their different walk 
orders, are somewhat redundant. We plan to 
consider other approaches for conditioning on 
context. Furthermore phrasal channel models, in 
spite of the laundry list of problems presented 
here, have a significant impact on translation 
quality. We hope to replace them with effective 
models without the brittleness and sparsity issues 
of heavy lexicalization. 
References 
Banchs, Rafael, Josep Crego, Adri? de Gispert, Patrik 
Lambert, and Jose Mari?o. 2005. Statistical machine 
translation of Euparl data by using bilingual n-grams. In 
Proceedings of ACL Workshop on Building and Using 
Parallel Texts. 
Brown, Peter, Vincent Della Pietra, Stephen Della Pietra, and 
Robert Mercer. 1993. The mathematics of statistical 
machine translation: parameter estimation. Computational 
Linguistics 19(2): 263-311. 
Callison-Burch, Chris, Colin Bannard, and Josh Schroeder. 
2005. Scaling phrase-based machine translation to larger 
corpora and longer phrases. In Proceedings of ACL. 
Chiang, David. 2005. A hierarchical phrase-based model for 
statistical machine translation. In Proceedings of ACL. 
Heidorn, George. 2000. ?Intelligent writing assistance?. In 
Dale et al Handbook of Natural Language Processing, 
Marcel Dekker. 
Koehn, Philipp, Franz Josef Och, and Daniel Marcu. 2003. 
Statistical phrase based translation. In Proceedings of 
NAACL. 
Koehn, Philipp. 2004. Pharaoh: A beam search decoder for 
phrase-based statistical machine translation models. In 
Proceedings of AMTA. 
Och, Franz Josef and Hermann Ney. 2003. A systematic 
comparison of various statistical alignment models. 
Computational Linguistics, 29(1): 19-51. 
Och, Franz Josef and Hermann Ney. 2004. The Alignment 
Template approach to statistical machine translation, 
Computational Linguistics, 30(4):417-450.  
Och, Franz Josef. 2003. Minimum error rate training in 
statistical machine translation. In Proceedings of ACL. 
Papineni, Kishore, Salim Roukos, Todd Ward, and Wei-Jing 
Zhu. 2002. BLEU: a method for automatic evaluation of 
machine translation. In Proceedings of ACL. 
Quirk, Chris and Arul Menezes. 2005. Dependency tree 
translation: syntactically-informed phrasal SMT. In 
Proceedings of ACL. 
Stolcke, Andreas. 1998. Entropy-based pruning of backoff 
language models. In Proceedings of DARPA Broadcast 
News Transcription and Understanding. 
Vogel, Stephan, Ying Zhang, Fei Huang, Alicia Tribble, 
Ashish Venugopal, Bing Zhao, Alex Waibel. 2003. The 
CMU statistical machine translation system. In 
Proceedings of MT Summit. 
Zens, Richard, and Hermann Ney. 2003. A comparative 
study on reordering constraints in statistical machine 
translation. In Proceedings of ACL. 
Zhang, Ying and Stephan Vogel. 2005. An efficient phrase-
to-phrase alignment model for arbitrarily long phrase and 
large corpora. In Proceedings of EAMT. 
16
Proceedings of the 43rd Annual Meeting of the ACL, pages 271?279,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Dependency Treelet Translation: Syntactically Informed Phrasal SMT  
Chris Quirk, Arul Menezes Colin Cherry 
Microsoft Research University of Alberta 
One Microsoft Way Edmonton, Alberta 
Redmond, WA 98052 Canada T6G 2E1 
{chrisq,arulm}@microsoft.com colinc@cs.ualberta.ca 
 
Abstract 
We describe a novel approach to 
statistical machine translation that 
combines syntactic information in the 
source language with recent advances in 
phrasal translation. This method requires a 
source-language dependency parser, target 
language word segmentation and an 
unsupervised word alignment component. 
We align a parallel corpus, project the 
source dependency parse onto the target 
sentence, extract dependency treelet 
translation pairs, and train a tree-based 
ordering model. We describe an efficient 
decoder and show that using these tree-
based models in combination with 
conventional SMT models provides a 
promising approach that incorporates the 
power of phrasal SMT with the linguistic 
generality available in a parser.  
1. Introduction 
Over the past decade, we have witnessed a 
revolution in the field of machine translation 
(MT) toward statistical or corpus-based methods. 
Yet despite this success, statistical machine 
translation (SMT) has many hurdles to overcome. 
While it excels at translating domain-specific 
terminology and fixed phrases, grammatical 
generalizations are poorly captured and often 
mangled during translation (Thurmair, 04).  
1.1. Limitations of string-based phrasal SMT 
State-of-the-art phrasal SMT systems such as 
(Koehn et al, 03) and (Vogel et al, 03) model 
translations of phrases (here, strings of adjacent 
words, not syntactic constituents) rather than 
individual words. Arbitrary reordering of words is 
allowed within memorized phrases, but typically 
only a small amount of phrase reordering is 
allowed, modeled in terms of offset positions at 
the string level. This reordering model is very 
limited in terms of linguistic generalizations. For 
instance, when translating English to Japanese, an 
ideal system would automatically learn large-
scale typological differences: English SVO 
clauses generally become Japanese SOV clauses, 
English post-modifying prepositional phrases 
become Japanese pre-modifying postpositional 
phrases, etc. A phrasal SMT system may learn the 
internal reordering of specific common phrases, 
but it cannot generalize to unseen phrases that 
share the same linguistic structure. 
In addition, these systems are limited to 
phrases contiguous in both source and target, and 
thus cannot learn the generalization that English 
not may translate as French ne?pas except in the 
context of specific intervening words.  
1.2. Previous work on syntactic SMT1 
The hope in the SMT community has been that 
the incorporation of syntax would address these 
issues, but that promise has yet to be realized. 
One simple means of incorporating syntax into 
SMT is by re-ranking the n-best list of a baseline 
SMT system using various syntactic models, but 
Och et al (04) found very little positive impact 
with this approach. However, an n-best list of 
even 16,000 translations captures only a tiny 
fraction of the ordering possibilities of a 20 word 
sentence; re-ranking provides the syntactic model 
no opportunity to boost or prune large sections of 
that search space.  
Inversion Transduction Grammars (Wu, 97), or 
ITGs, treat translation as a process of parallel 
parsing of the source and target language via a 
synchronized grammar. To make this process 
                                                        
1
 Note that since this paper does not address the word alignment problem 
directly, we do not discuss the large body of work on incorporating syntactic 
information into the word alignment process. 
 
271
computationally efficient, however, some severe 
simplifying assumptions are made, such as using 
a single non-terminal label. This results in the 
model simply learning a very high level 
preference regarding how often nodes should 
switch order without any contextual information. 
Also these translation models are intrinsically 
word-based; phrasal combinations are not 
modeled directly, and results have not been 
competitive with the top phrasal SMT systems.  
Along similar lines, Alshawi et al (2000) treat 
translation as a process of simultaneous induction 
of source and target dependency trees using head-
transduction; again, no separate parser is used. 
Yamada and Knight (01) employ a parser in the 
target language to train probabilities on a set of 
operations that convert a target language tree to a 
source language string. This improves fluency 
slightly (Charniak et al, 03), but fails to 
significantly impact overall translation quality. 
This may be because the parser is applied to MT 
output, which is notoriously unlike native 
language, and no additional insight is gained via 
source language analysis.  
Lin (04) translates dependency trees using 
paths. This is the first attempt to incorporate large 
phrasal SMT-style memorized patterns together 
with a separate source dependency parser and 
SMT models. However the phrases are limited to 
linear paths in the tree, the only SMT model used 
is a maximum likelihood channel model and there 
is no ordering model. Reported BLEU scores are 
far below the leading phrasal SMT systems. 
MSR-MT (Menezes & Richardson, 01) parses 
both source and target languages to obtain a 
logical form (LF), and translates source LFs using 
memorized aligned LF patterns to produce a 
target LF. It utilizes a separate sentence 
realization component (Ringger et al, 04) to turn 
this into a target sentence. As such, it does not use 
a target language model during decoding, relying 
instead on MLE channel probabilities and 
heuristics such as pattern size. Recently Aue et al 
(04) incorporated an LF-based language model 
(LM) into the system for a small quality boost. A 
key disadvantage of this approach and related 
work (Ding & Palmer, 02) is that it requires a 
parser in both languages, which severely limits 
the language pairs that can be addressed. 
2. Dependency Treelet Translation 
In this paper we propose a novel dependency tree-
based approach to phrasal SMT which uses tree-
based ?phrases? and a tree-based ordering model 
in combination with conventional SMT models to 
produce state-of-the-art translations.  
Our system employs a source-language 
dependency parser, a target language word 
segmentation component, and an unsupervised 
word alignment component to learn treelet 
translations from a parallel sentence-aligned 
corpus. We begin by parsing the source text to 
obtain dependency trees and word-segmenting the 
target side, then applying an off-the-shelf word 
alignment component to the bitext.  
The word alignments are used to project the 
source dependency parses onto the target 
sentences. From this aligned parallel dependency 
corpus we extract a treelet translation model 
incorporating source and target treelet pairs, 
where a treelet is defined to be an arbitrary 
connected subgraph of the dependency tree. A 
unique feature is that we allow treelets with a 
wildcard root, effectively allowing mappings for 
siblings in the dependency tree. This allows us to 
model important phenomena, such as not ?   
ne?pas. We also train a variety of statistical 
models on this aligned dependency tree corpus, 
including a channel model and an order model.  
To translate an input sentence, we parse the 
sentence, producing a dependency tree for that 
sentence. We then employ a decoder to find a 
combination and ordering of treelet translation 
pairs that cover the source tree and are optimal 
according to a set of models that are combined in 
a log-linear framework as in (Och, 03).  
This approach offers the following advantages 
over string-based SMT systems: Instead of 
limiting learned phrases to contiguous word 
sequences, we allow translation by all possible 
phrases that form connected subgraphs (treelets) 
in the source and target dependency trees. This is 
a powerful extension: the vast majority of 
surface-contiguous phrases are also treelets of the 
tree; in addition, we gain discontiguous phrases, 
including combinations such as verb-object, 
article-noun, adjective-noun etc. regardless of the 
number of intervening words. 
272
Another major advantage is the ability to 
employ more powerful models for reordering 
source language constituents. These models can 
incorporate information from the source analysis. 
For example, we may model directly the 
probability that the translation of an object of a 
preposition in English should precede the 
corresponding postposition in Japanese, or the 
probability that a pre-modifying adjective in 
English translates into a post-modifier in French. 
2.1. Parsing and alignment 
We require a source language dependency parser 
that produces unlabeled, ordered dependency 
trees and annotates each source word with a part-
of-speech (POS). An example dependency tree is 
shown in Figure 1. The arrows indicate the head 
annotation, and the POS for each candidate is 
listed underneath. For the target language we only 
require word segmentation.  
To obtain word alignments we currently use 
GIZA++ (Och & Ney, 03). We follow the 
common practice of deriving many-to-many 
alignments by running the IBM models in both 
directions and combining the results heuristically. 
Our heuristics differ in that they constrain many-
to-one alignments to be contiguous in the source 
dependency tree. A detailed description of these 
heuristics can be found in Quirk et al (04).  
2.2. Projecting dependency trees 
Given a word aligned sentence pair and a source 
dependency tree, we use the alignment to project 
the source structure onto the target sentence. One-
to-one alignments project directly to create a 
target tree isomorphic to the source. Many-to-one 
alignments project similarly; since the ?many? 
source nodes are connected in the tree, they act as 
if condensed into a single node. In the case of 
one-to-many alignments we project the source 
node to the rightmost2 of the ?many? target words, 
and make the rest of the target words dependent 
on it. 
                                                        
2
 If the target language is Japanese, leftmost may be more appropriate. 
Unaligned target words3 are attached into the 
dependency structure as follows: assume there is 
an unaligned word tj in position j. Let i < j and k 
> j be the target positions closest to j such that ti 
depends on tk or vice versa: attach tj to the lower 
of ti or tk. If all the nodes to the left (or right) of 
position j are unaligned, attach tj to the left-most 
(or right-most) word that is aligned. 
The target dependency tree created in this 
process may not read off in the same order as the 
target string, since our alignments do not enforce 
phrasal cohesion. For instance, consider the 
projection of the parse in Figure 1 using the word 
alignment in Figure 2a. Our algorithm produces 
the dependency tree in Figure 2b. If we read off 
the leaves in a left-to-right in-order traversal, we 
do not get the original input string: de d?marrage 
appears in the wrong place. 
A second reattachment pass corrects this 
situation. For each node in the wrong order, we 
reattach it to the lowest of its ancestors such that 
it is in the correct place relative to its siblings and 
parent. In Figure 2c, reattaching d?marrage to et 
suffices to produce the correct order.  
                                                        
3
 Source unaligned nodes do not present a problem, with the exception that if 
the root is unaligned, the projection process produces a forest of target trees 
anchored by a dummy root.  
startup properties and options
Noun Noun Conj Noun
 
Figure 1. An example dependency tree. 
startup properties and options
propri?t?s et options de d?marrage
 
(a) Word alignment. 
 
 
startup properties and options
propri?t?s de d?marrage et options
 
 
 (b) Dependencies after initial projection. 
 
 
startup properties and options
propri?t?s et options de d?marrage
 
(c) Dependencies after reattachment step. 
 
Figure 2. Projection of dependencies. 
273
2.3. Extracting treelet translation pairs 
From the aligned pairs of dependency trees we 
extract all pairs of aligned source and target 
treelets along with word-level alignment linkages, 
up to a configurable maximum size. We also keep 
treelet counts for maximum likelihood estimation.  
2.4. Order model 
Phrasal SMT systems often use a model to score 
the ordering of a set of phrases. One approach is 
to penalize any deviation from monotone 
decoding; another is to estimate the probability 
that a source phrase in position i translates to a 
target phrase in position j (Koehn et al, 03). 
We attempt to improve on these approaches by 
incorporating syntactic information. Our model 
assigns a probability to the order of a target tree 
given a source tree. Under the assumption that 
constituents generally move as a whole, we 
predict the probability of each given ordering of 
modifiers independently. That is, we make the 
following simplifying assumption (where c is a 
function returning the set of nodes modifying t): 
?
?
=
Tt
TStcorderTSTorder ),|))((P(),|)(P(
 
Furthermore, we assume that the position of each 
child can be modeled independently in terms of a 
head-relative position: 
),|),(P(),|))((P(
)(
TStmposTStcorder
tcm
?
?
=  
Figure 3a demonstrates an aligned dependency 
tree pair annotated with head-relative positions; 
Figure 3b presents the same information in an 
alternate tree-like representation. 
We currently use a small set of features 
reflecting very local information in the 
dependency tree to model P(pos(m,t) | S, T): 
? The lexical items of the head and modifier. 
? The lexical items of the source nodes aligned 
to the head and modifier. 
? The part-of-speech ("cat") of the source nodes 
aligned to the head and modifier. 
? The head-relative position of the source node 
aligned to the source modifier. 4 
As an example, consider the children of 
propri?t? in Figure 3. The head-relative positions 
                                                        
4
 One can also include features of siblings to produce a Markov ordering 
model. However, we found that this had little impact in practice. 
of its modifiers la and Cancel are -1 and +1, 
respectively. Thus we try to predict as follows: 
P(pos(m1) = -1 | 
lex(m1)="la", lex(h)="propri?t?", 
lex(src(m1))="the", lex(src(h)="property", 
cat(src(m1))=Determiner, cat(src(h))=Noun, 
position(src(m1))=-2) ? 
P(pos(m2) = +1 | 
lex(m2)="Cancel", lex(h)="propri?t?", 
lex(src(m2))="Cancel", lex(src(h))="property", 
cat(src(m2))=Noun, cat(src(h))=Noun, 
position(src(m2))=-1) 
The training corpus acts as a supervised training 
set: we extract a training feature vector from each 
of the target language nodes in the aligned 
dependency tree pairs. Together these feature 
vectors are used to train a decision tree 
(Chickering, 02). The distribution at each leaf of 
the DT can be used to assign a probability to each 
possible target language position. A more detailed 
description is available in (Quirk et al, 04). 
2.5. Other models 
Channel Models: We incorporate two distinct 
channel models, a maximum likelihood estimate 
(MLE) model and a model computed using 
Model-1 word-to-word alignment probabilities as 
in (Vogel et al, 03). The MLE model effectively 
captures non-literal phrasal translations such as 
idioms, but suffers from data sparsity. The word-
the-2 Cancel-1 property-1 uses these-1 settings+1
la-1 propri?t?-1 Cancel+1 utilise ces-1 param?tres+1
 
(a) Head annotation representation 
 
uses
property-1              settings+1
the-2 Cancel-1                 these-1
la-1             Cancel+1         ces-1
propri?t?-1                        param?tres+1
utilise
 
(b) Branching structure representation. 
 
Figure 3.  Aligned dependency tree pair, annotated with 
head-relative positions 
274
to-word model does not typically suffer from data 
sparsity, but prefers more literal translations.  
Given a set of treelet translation pairs that 
cover a given input dependency tree and produce 
a target dependency tree, we model the 
probability of source given target as the product 
of the individual treelet translation probabilities: 
we assume a uniform probability distribution over 
the decompositions of a tree into treelets.  
Target Model: Given an ordered target language 
dependency tree, it is trivial to read off the surface 
string. We evaluate this string using a trigram 
model with modified Kneser-Ney smoothing.  
Miscellaneous Feature Functions: The log-linear 
framework allows us to incorporate other feature 
functions as ?models? in the translation process. 
For instance, using fewer, larger treelet translation 
pairs often provides better translations, since they 
capture more context and allow fewer possibilities 
for search and model error. Therefore we add a 
feature function that counts the number of phrases 
used. We also add a feature that counts the 
number of target words; this acts as an 
insertion/deletion bonus/penalty.  
3. Decoding 
The challenge of tree-based decoding is that the 
traditional left-to-right decoding approach of 
string-based systems is inapplicable. Additional 
challenges are posed by the need to handle 
treelets?perhaps discontiguous or overlapping?
and a combinatorially explosive ordering space.  
Our decoding approach is influenced by ITG 
(Wu, 97) with several important extensions. First, 
we employ treelet translation pairs instead of 
single word translations. Second, instead of 
modeling rearrangements as either preserving 
source order or swapping source order, we allow 
the dependents of a node to be ordered in any 
arbitrary manner and use the order model 
described in section 2.4 to estimate probabilities. 
Finally, we use a log-linear framework for model 
combination that allows any amount of other 
information to be modeled.  
We will initially approach the decoding 
problem as a bottom up, exhaustive search. We 
define the set of all possible treelet translation 
pairs of the subtree rooted at each input node in 
the following manner: A treelet translation pair x 
is said to match the input dependency tree S iff 
there is some connected subgraph S? that is 
identical to the source side of x. We say that x 
covers all the nodes in S? and is rooted at source 
node s, where s is the root of matched subgraph 
S?.  
We first find all treelet translation pairs that 
match the input dependency tree. Each matched 
pair is placed on a list associated with the input 
node where the match is rooted. Moving bottom-
up through the input dependency tree, we 
compute a list of candidate translations for the 
input subtree rooted at each node s, as follows:  
Consider in turn each treelet translation pair x 
rooted at s. The treelet pair x may cover only a 
portion of the input subtree rooted at s. Find all 
descendents s' of s that are not covered by x, but 
whose parent s'' is covered by x. At each such 
node s'' look at all interleavings of the children of 
s'' specified by x, if any, with each translation t' 
from the candidate translation list5 of each child 
s'. Each such interleaving is scored using the 
models previously described and added to the 
candidate translation list for that input node. The 
resultant translation is the best scoring candidate 
for the root input node. 
As an example, see the example dependency 
tree in Figure 4a and treelet translation pair in 4b. 
This treelet translation pair covers all the nodes in 
4a except the subtrees rooted at software and is. 
                                                        
5
 Computed by the previous application of this procedure to s' during the 
bottom-up traversal. 
installed
software is on
the computer
your
 
 (a) Example input dependency tree. 
installed
on
computer
your
votre
ordinateur
sur
install?s
 
(b) Example treelet translation pair. 
 
Figure 4.  Example decoder structures. 
275
We first compute (and cache) the candidate 
translation lists for the subtrees rooted at software 
and is, then construct full translation candidates 
by attaching those subtree translations to install?s 
in all possible ways. The order of sur relative to 
install?s is fixed; it remains to place the translated 
subtrees for the software and is. Note that if c is 
the count of children specified in the mapping and 
r is the count of subtrees translated via recursive 
calls, then there are (c+r+1)!/(c+1)! orderings. 
Thus (1+2+1)!/(1+1)! = 12 candidate translations 
are produced for each combination of translations 
of the software and is. 
3.1. Optimality-preserving optimizations 
Dynamic Programming 
Converting this exhaustive search to dynamic 
programming relies on the observation that 
scoring a translation candidate at a node depends 
on the following information from its 
descendents: the order model requires features 
from the root of a translated subtree, and the 
target language model is affected by the first and 
last two words in each subtree. Therefore, we 
need to keep the best scoring translation candidate 
for a given subtree for each combination of (head, 
leading bigram, trailing bigram), which is, in the 
worst case, O(V5), where V is the vocabulary size. 
The dynamic programming approach therefore 
does not allow for great savings in practice 
because a trigram target language model forces 
consideration of context external to each subtree.  
Duplicate elimination 
To eliminate unnecessary ordering operations, we 
first check that a given set of words has not been 
previously ordered by the decoder. We use an 
order-independent hash table where two trees are 
considered equal if they have the same tree 
structure and lexical choices after sorting each 
child list into a canonical order. A simpler 
alternate approach would be to compare bags-of-
words. However since our possible orderings are 
bound by the induced tree structure, we might 
overzealously prune a candidate with a different 
tree structure that allows a better target order.  
3.2. Lossy optimizations 
The following optimizations do not preserve 
optimality, but work well in practice. 
N-best lists 
Instead of keeping the full list of translation 
candidates for a given input node, we keep a top-
scoring subset of the candidates. While the 
decoder is no longer guaranteed to find the 
optimal translation, in practice the quality impact 
is minimal with a list size ? 10 (see Table 5.6).  
Variable-sized n-best lists: A further speedup 
can be obtained by noting that the number of 
translations using a given treelet pair is 
exponential in the number of subtrees of the input 
not covered by that pair. To limit this explosion 
we vary the size of the n-best list on any recursive 
call in inverse proportion to the number of 
subtrees uncovered by the current treelet. This has 
the intuitive appeal of allowing a more thorough 
exploration of large treelet translation pairs (that 
are likely to result in better translations) than of 
smaller, less promising pairs.  
Pruning treelet translation pairs 
Channel model scores and treelet size are 
powerful predictors of translation quality. 
Heuristically pruning low scoring treelet 
translation pairs before the search starts allows 
the decoder to focus on combinations and 
orderings of high quality treelet pairs.  
? Only keep those treelet translation pairs with 
an MLE probability above a threshold t. 
? Given a set of treelet translation pairs with 
identical sources, keep those with an MLE 
probability within a ratio r of the best pair.  
? At each input node, keep only the top k treelet 
translation pairs rooted at that node, as ranked 
first by size, then by MLE channel model 
score, then by Model 1 score. The impact of 
this optimization is explored in Table 5.6.  
Greedy ordering 
The complexity of the ordering step at each node 
grows with the factorial of the number of children 
to be ordered. This can be tamed by noting that 
given a fixed pre- and post-modifier count, our 
order model is capable of evaluating a single 
ordering decision independently from other 
ordering decisions. 
One version of the decoder takes advantage of 
this to severely limit the number of ordering 
possibilities considered. Instead of considering all 
interleavings, it considers each potential modifier 
position in turn, greedily picking the most 
276
probable child for that slot, moving on to the next 
slot, picking the most probable among the 
remaining children for that slot and so on. 
The complexity of greedy ordering is linear, 
but at the cost of a noticeable drop in BLEU score 
(see Table 5.4). Under default settings our system 
tries to decode a sentence with exhaustive 
ordering until a specified timeout, at which point 
it falls back to greedy ordering. 
4. Experiments 
We evaluated the translation quality of the system 
using the BLEU metric (Papineni et al, 02) under 
a variety of configurations. We compared against 
two radically different types of systems to 
demonstrate the competitiveness of this approach:  
? Pharaoh: A leading phrasal SMT decoder 
(Koehn et al, 03). 
? The MSR-MT system described in Section 1, 
an EBMT/hybrid MT system.  
4.1. Data 
We used a parallel English-French corpus 
containing 1.5 million sentences of Microsoft 
technical data (e.g., support articles, product 
documentation). We selected a cleaner subset of 
this data by eliminating sentences with XML or 
HTML tags as well as very long (>160 characters) 
and very short (<40 characters) sentences. We 
held out 2,000 sentences for development testing 
and parameter tuning, 10,000 sentences for 
testing, and 250 sentences for lambda training. 
We ran experiments on subsets of the training 
data ranging from 1,000 to 300,000 sentences. 
Table 4.1 presents details about this dataset. 
4.2. Training 
We parsed the source (English) side of the corpus 
using NLPWIN, a broad-coverage rule-based 
parser developed at Microsoft Research able to 
produce syntactic analyses at varying levels of 
depth (Heidorn, 02). For the purposes of these 
experiments we used a dependency tree output 
with part-of-speech tags and unstemmed surface 
words.  
For word alignment, we used GIZA++, 
following a standard training regimen of five 
iterations of Model 1, five iterations of the HMM 
Model, and five iterations of Model 4, in both 
directions.  
We then projected the dependency trees and 
used the aligned dependency tree pairs to extract 
treelet translation pairs and train the order model 
as described above. The target language model 
was trained using only the French side of the 
corpus; additional data may improve its 
performance. Finally we trained lambdas via 
Maximum BLEU (Och, 03) on 250 held-out 
sentences with a single reference translation, and 
tuned the decoder optimization parameters (n-best 
list size, timeouts etc) on the development test set. 
Pharaoh 
The same GIZA++ alignments as above were 
used in the Pharaoh decoder. We used the 
heuristic combination described in (Och & Ney, 
03) and extracted phrasal translation pairs from 
this combined alignment as described in (Koehn 
et al, 03). Except for the order model (Pharaoh 
uses its own ordering approach), the same models 
were used: MLE channel model, Model 1 channel 
model, target language model, phrase count, and 
word count. Lambdas were trained in the same 
manner (Och, 03). 
MSR-MT 
MSR-MT used its own word alignment approach 
as described in (Menezes & Richardson, 01) on 
the same training data. MSR-MT does not use 
lambdas or a target language model. 
5. Results 
We present BLEU scores on an unseen 10,000 
sentence test set using a single reference 
translation for each sentence. Speed numbers are 
the end-to-end translation speed in sentences per 
minute. All results are based on a training set size 
of 100,000 sentences and a phrase size of 4, 
except Table 5.2 which varies the phrase size and 
Table 5.3 which varies the training set size. 
  English French 
Training Sentences 570,562 
 Words 7,327,251 8,415,882 
 Vocabulary 72,440 80,758 
 Singletons 38,037 39,496 
Test Sentences 10,000 
 Words 133,402 153,701 
Table 4.1 Data characteristics 
277
Results for our system and the comparison 
systems are presented in Table 5.1. Pharaoh 
monotone refers to Pharaoh with phrase 
reordering disabled. The difference between 
Pharaoh and the Treelet system is significant at 
the 99% confidence level under a two-tailed 
paired t-test. 
 BLEU Score Sents/min 
Pharaoh monotone 37.06 4286 
Pharaoh 38.83 162 
MSR-MT 35.26 453 
Treelet 40.66 10.1 
Table 5.1 System comparisons  
Table 5.2 compares Pharaoh and the Treelet 
system at different phrase sizes. While all the 
differences are statistically significant at the 99% 
confidence level, the wide gap at smaller phrase 
sizes is particularly striking. We infer that 
whereas Pharaoh depends heavily on long phrases 
to encapsulate reordering, our dependency tree-
based ordering model enables credible 
performance even with single-word ?phrases?. We 
conjecture that in a language pair with large-scale 
ordering differences, such as English-Japanese, 
even long phrases are unlikely to capture the 
necessary reorderings, whereas our tree-based 
ordering model may prove more robust. 
Max. size Treelet BLEU Pharaoh BLEU 
1  37.50 23.18  
2 39.84 32.07  
3 40.36 37.09  
4 (default) 40.66 38.83  
5 40.71 39.41  
6 40.74 39.72  
Table 5.2 Effect of maximum treelet/phrase size 
Table 5.3 compares the same systems at different 
training corpus sizes. All of the differences are 
statistically significant at the 99% confidence 
level. Noting that the gap widens at smaller 
corpus sizes, we suggest that our tree-based 
approach is more suitable than string-based 
phrasal SMT when translating from English into 
languages or domains with limited parallel data. 
We also ran experiments varying different 
system parameters. Table 5.4 explores different 
ordering strategies, Table 5.5 looks at the impact 
of discontiguous phrases and Table 5.6 looks at 
the impact of decoder optimizations such as 
treelet pruning and n-best list size. 
Ordering strategy BLEU  Sents/min  
No order model (monotone) 35.35 39.7 
Greedy ordering 38.85 13.1 
Exhaustive (default) 40.66 10.1 
Table 5.4 Effect of ordering strategies 
 BLEU Score  Sents/min 
Contiguous only 40.08  11.0 
Allow discontiguous 40.66 10.1 
Table 5.5 Effect of allowing treelets that correspond to 
discontiguous phrases 
 BLEU Score  Sents/min  
Pruning treelets   
  Keep top 1 28.58  144.9 
  ? top 3 39.10 21.2 
  ? top 5 40.29 14.6 
  ? top 10 (default) 40.66 10.1 
  ? top 20 40.70 3.5 
  Keep all 40.29 3.2 
N-best list size    
  1-best 37.28 175.4 
  5-best 39.96 79.4 
  10-best 40.42 23.3 
  20-best (default) 40.66 10.1 
  50-best 39.39 3.7 
Table 5.6 Effect of optimizations  
6. Discussion  
We presented a novel approach to syntactically-
informed statistical machine translation that 
leverages a parsed dependency tree representation 
of the source language via a tree-based ordering 
model and treelet phrase extraction. We showed 
that it significantly outperforms a leading phrasal 
SMT system over a wide range of training set 
sizes and phrase sizes. 
Constituents vs. dependencies: Most attempts at 
 1k 3k 10k 30k 100k 300k 
Pharaoh 17.20  22.51  27.70  33.73  38.83  42.75  
Treelet 18.70 25.39 30.96 35.81 40.66 44.32 
Table 5.3 Effect of training set size on treelet translation and comparison system  
 
278
syntactic SMT have relied on a constituency 
analysis rather than dependency analysis. While 
this is a natural starting point due to its well-
understood nature and commonly available tools, 
we feel that this is not the most effective 
representation for syntax in MT. Dependency 
analysis, in contrast to constituency analysis, 
tends to bring semantically related elements 
together (e.g., verbs become adjacent to all their 
arguments) and is better suited to lexicalized 
models, such as the ones presented in this paper.  
7. Future work 
The most important contribution of our system is 
a linguistically motivated ordering approach 
based on the source dependency tree, yet this 
paper only explores one possible model. Different 
model structures, machine learning techniques, 
and target feature representations all have the 
potential for significant improvements.  
Currently we only consider the top parse of an 
input sentence. One means of considering 
alternate possibilities is to build a packed forest of 
dependency trees and use this in decoding 
translations of each input sentence. 
As noted above, our approach shows particular 
promise for language pairs such as English-
Japanese that exhibit large-scale reordering and 
have proven difficult for string-based approaches. 
Further experimentation with such language pairs 
is necessary to confirm this. Our experience has 
been that the quality of GIZA++ alignments for 
such language pairs is inadequate. Following up 
on ideas introduced by (Cherry & Lin, 03) we 
plan to explore ways to leverage the dependency 
tree to improve alignment quality.  
References 
Alshawi, Hiyan, Srinivas Bangalore, and Shona 
Douglas. Learning dependency translation models 
as collections of finite-state head transducers. 
Computational Linguistics, 26(1):45?60, 2000. 
Aue, Anthony, Arul Menezes, Robert C. Moore, Chris 
Quirk, and Eric Ringger. Statistical machine 
translation using labeled semantic dependency 
graphs. TMI 2004. 
Charniak, Eugene, Kevin Knight, and Kenji Yamada. 
Syntax-based language models for statistical 
machine translation. MT Summit 2003. 
Cherry, Colin and Dekang Lin. A probability model to 
improve word alignment. ACL 2003. 
Chickering, David Maxwell. The WinMine Toolkit. 
Microsoft Research Technical Report: MSR-TR-
2002-103. 
Ding, Yuan and Martha Palmer. Automatic learning of 
parallel dependency treelet pairs. IJCNLP 2004. 
Heidorn, George. (2000). ?Intelligent writing 
assistance?. In Dale et al Handbook of Natural 
Language Processing, Marcel Dekker. 
Koehn, Philipp, Franz Josef Och, and Daniel Marcu. 
Statistical phrase based translation. NAACL 2003. 
Lin, Dekang. A path-based transfer model for machine 
translation. COLING 2004. 
 Menezes, Arul and Stephen D. Richardson. A best-
first alignment algorithm for automatic extraction of 
transfer mappings from bilingual corpora. DDMT 
Workshop, ACL 2001. 
Och, Franz Josef and Hermann Ney. A systematic 
comparison of various statistical alignment models, 
Computational Linguistics, 29(1):19-51, 2003.  
Och, Franz Josef. Minimum error rate training in 
statistical machine translation. ACL 2003. 
Och, Franz Josef, et al A smorgasbord of features for 
statistical machine translation. HLT/NAACL 2004. 
Papineni, Kishore, Salim Roukos, Todd Ward, and 
Wei-Jing Zhu. BLEU: a method for automatic 
evaluation of machine translation. ACL 2002. 
Quirk, Chris, Arul Menezes, and Colin Cherry. 
Dependency Tree Translation. Microsoft Research 
Technical Report: MSR-TR-2004-113. 
Ringger, Eric, et al Linguistically informed statistical 
models of constituent structure for ordering in 
sentence realization. COLING 2004. 
 Thurmair, Gregor. Comparing rule-based and 
statistical MT output. Workshop on the amazing 
utility of parallel and comparable corpora, LREC, 
2004. 
 Vogel, Stephan, Ying Zhang, Fei Huang, Alicia 
Tribble, Ashish Venugopal, Bing Zhao, and Alex 
Waibel. The CMU statistical machine translation 
system. MT Summit 2003. 
Wu, Dekai. Stochastic inversion transduction 
grammars and bilingual parsing of parallel corpora. 
Computational Linguistics, 23(3):377?403, 1997. 
Yamada, Kenji and Kevin Knight. A syntax-based 
statistical translation model. ACL, 2001. 
279
Proceedings of ACL-08: HLT, pages 97?105,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Bayesian Learning of Non-compositional Phrases with Synchronous Parsing
Hao Zhang
Computer Science Department
University of Rochester
Rochester, NY 14627
zhanghao@cs.rochester.edu
Chris Quirk
Microsoft Research
One Microsoft Way
Redmond, WA 98052 USA
chrisq@microsoft.com
Robert C. Moore
Microsoft Research
One Microsoft Way
Redmond, WA 98052 USA
bobmoore@microsoft.com
Daniel Gildea
Computer Science Department
University of Rochester
Rochester, NY 14627
gildea@cs.rochester.edu
Abstract
We combine the strengths of Bayesian mod-
eling and synchronous grammar in unsu-
pervised learning of basic translation phrase
pairs. The structured space of a synchronous
grammar is a natural fit for phrase pair proba-
bility estimation, though the search space can
be prohibitively large. Therefore we explore
efficient algorithms for pruning this space that
lead to empirically effective results. Incorpo-
rating a sparse prior using Variational Bayes,
biases the models toward generalizable, parsi-
monious parameter sets, leading to significant
improvements in word alignment. This pref-
erence for sparse solutions together with ef-
fective pruning methods forms a phrase align-
ment regimen that produces better end-to-end
translations than standard word alignment ap-
proaches.
1 Introduction
Most state-of-the-art statistical machine transla-
tion systems are based on large phrase tables ex-
tracted from parallel text using word-level align-
ments. These word-level alignments are most of-
ten obtained using Expectation Maximization on the
conditional generative models of Brown et al (1993)
and Vogel et al (1996). As these word-level align-
ment models restrict the word alignment complex-
ity by requiring each target word to align to zero
or one source words, results are improved by align-
ing both source-to-target as well as target-to-source,
then heuristically combining these alignments. Fi-
nally, the set of phrases consistent with the word
alignments are extracted from every sentence pair;
these form the basis of the decoding process. While
this approach has been very successful, poor word-
level alignments are nonetheless a common source
of error in machine translation systems.
A natural solution to several of these issues is
unite the word-level and phrase-level models into
one learning procedure. Ideally, such a procedure
would remedy the deficiencies of word-level align-
ment models, including the strong restrictions on
the form of the alignment, and the strong inde-
pendence assumption between words. Furthermore
it would obviate the need for heuristic combina-
tion of word alignments. A unified procedure may
also improve the identification of non-compositional
phrasal translations, and the attachment decisions
for unaligned words.
In this direction, Expectation Maximization at
the phrase level was proposed by Marcu and Wong
(2002), who, however, experienced two major dif-
ficulties: computational complexity and controlling
overfitting. Computational complexity arises from
the exponentially large number of decompositions
of a sentence pair into phrase pairs; overfitting is a
problem because as EM attempts to maximize the
likelihood of its training data, it prefers to directly
explain a sentence pair with a single phrase pair.
In this paper, we attempt to address these two is-
sues in order to apply EM above the word level.
97
We attack computational complexity by adopting
the polynomial-time Inversion Transduction Gram-
mar framework, and by only learning small non-
compositional phrases. We address the tendency of
EM to overfit by using Bayesian methods, where
sparse priors assign greater mass to parameter vec-
tors with fewer non-zero values therefore favoring
shorter, more frequent phrases. We test our model
by extracting longer phrases from our model?s align-
ments using traditional phrase extraction, and find
that a phrase table based on our system improves MT
results over a phrase table extracted from traditional
word-level alignments.
2 Phrasal Inversion Transduction
Grammar
We use a phrasal extension of Inversion Transduc-
tion Grammar (Wu, 1997) as the generative frame-
work. Our ITG has two nonterminals: X and
C, where X represents compositional phrase pairs
that can have recursive structures and C is the pre-
terminal over terminal phrase pairs. There are three
rules with X on the left-hand side:
X ? [X X],
X ? ?X X?,
X ? C.
The first two rules are the straight rule and in-
verted rule respectively. They split the left-hand side
constituent which represents a phrase pair into two
smaller phrase pairs on the right-hand side and order
them according to one of the two possible permuta-
tions. The rewriting process continues until the third
rule is invoked. C is our unique pre-terminal for
generating terminal multi-word pairs:
C ? e/f .
We parameterize our probabilistic model in the
manner of a PCFG: we associate a multinomial dis-
tribution with each nonterminal, where each out-
come in this distribution corresponds to an expan-
sion of that nonterminal. Specifically, we place one
multinomial distribution ?X over the three expan-
sions of the nonterminalX , and another multinomial
distribution ?C over the expansions of C. Thus, the
parameters in our model can be listed as
?X = (P??, P[], PC),
where P?? is for the inverted rule, P[] for the straight
rule, PC for the third rule, satisfyingP??+P[]+PC =
1, and
?C = (P (e/f), P (e?/f ?), . . . ),
where
?
e/f P (e/f) = 1 is a multinomial distribu-
tion over phrase pairs.
This is our model in a nutshell. We can train
this model using a two-dimensional extension of the
inside-outside algorithm on bilingual data, assuming
every phrase pair that can appear as a leaf in a parse
tree of the grammar a valid candidate. However, it is
easy to show that the maximum likelihood training
will lead to the saturated solution where PC = 1 ?
each sentence pair is generated by a single phrase
spanning the whole sentence. From the computa-
tional point of view, the full EM algorithm runs in
O(n6) where n is the average length of the two in-
put sentences, which is too slow in practice.
The key is to control the number of parameters,
and therefore the size of the set of candidate phrases.
We deal with this problem in two directions. First
we change the objective function by incorporating
a prior over the phrasal parameters. This has the
effect of preferring parameter vectors in ?C with
fewer non-zero values. Our second approach was
to constrain the search space using simpler align-
ment models, which has the further benefit of signif-
icantly speeding up training. First we train a lower
level word alignment model, then we place hard con-
straints on the phrasal alignment space using confi-
dent word links from this simpler model. Combining
the two approaches, we have a staged training pro-
cedure going from the simplest unconstrained word
based model to a constrained Bayesian word-level
ITG model, and finally proceeding to a constrained
Bayesian phrasal model.
3 Variational Bayes for ITG
Goldwater and Griffiths (2007) and Johnson (2007)
show that modifying an HMM to include a sparse
prior over its parameters and using Bayesian esti-
mation leads to improved accuracy for unsupervised
part-of-speech tagging. In this section, we describe
a Bayesian estimator for ITG: we select parame-
ters that optimize the probability of the data given
a prior. The traditional estimation method for word
98
alignment models is the EM algorithm (Brown et
al., 1993) which iteratively updates parameters to
maximize the likelihood of the data. The drawback
of maximum likelihood is obvious for phrase-based
models. If we do not put any constraint on the dis-
tribution of phrases, EM overfits the data by mem-
orizing every sentence pair. A sparse prior over a
multinomial distribution such as the distribution of
phrase pairs may bias the estimator toward skewed
distributions that generalize better. In the context of
phrasal models, this means learning the more repre-
sentative phrases in the space of all possible phrases.
The Dirichlet distribution, which is parameter-
ized by a vector of real values often interpreted as
pseudo-counts, is a natural choice for the prior, for
two main reasons. First, the Dirichlet is conjugate
to the multinomial distribution, meaning that if we
select a Dirichlet prior and a multinomial likelihood
function, the posterior distribution will again be a
Dirichlet. This makes parameter estimation quite
simple. Second, Dirichlet distributions with small,
non-zero parameters place more probability mass on
multinomials on the edges or faces of the probabil-
ity simplex, distributions with fewer non-zero pa-
rameters. Starting from the model from Section 2,
we propose the following Bayesian extension, where
A ? Dir(B) means the random variable A is dis-
tributed according to a Dirichlet with parameter B:
?X | ?X ? Dir(?X),
?C | ?C ? Dir(?C),
[X X]
?X X?
C
X ? Multi(?X),
e/f | C ? Multi(?C).
The parameters ?X and ?C control the sparsity of
the two distributions in our model. One is the distri-
bution of the three possible branching choices. The
other is the distribution of the phrase pairs. ?C is
crucial, since the multinomial it is controlling has a
high dimension. By adjusting ?C to a very small
number, we hope to place more posterior mass on
parsimonious solutions with fewer but more confi-
dent and general phrase pairs.
Having defined the Bayesian model, it remains
to decide the inference procedure. We chose Vari-
ational Bayes, for its procedural similarity to EM
and ease of implementation. Another potential op-
tion would be Gibbs sampling (or some other sam-
pling technique). However, in experiments in un-
supervised POS tag learning using HMM structured
models, Johnson (2007) shows that VB is more ef-
fective than Gibbs sampling in approaching distribu-
tions that agree with the Zipf?s law, which is promi-
nent in natural languages.
Kurihara and Sato (2006) describe VB for PCFGs,
showing the only need is to change the M step of
the EM algorithm. As in the case of maximum like-
lihood estimation, Bayesian estimation for ITGs is
very similar to PCFGs, which follows due to the
strong isomorphism between the two models. Spe-
cific to our ITG case, the M step becomes:
P? (l+1)[] =
exp(?(E(X ? [X X]) + ?X))
exp(?(E(X) + s?X))
,
P? (l+1)?? =
exp(?(E(X ? ?X X?) + ?X))
exp(?(E(X) + s?X))
,
P? (l+1)C =
exp(?(E(X ? C) + ?X))
exp(?(E(X) + s?X))
,
P? (l+1)(e/f) = exp(?(E(e/f) + ?C))exp(?(E(C) +m?C))
,
where ? is the digamma function (Beal, 2003), s =
3 is the number of right-hand-sides for X , and m is
the number of observed phrase pairs in the data. The
sole difference between EM and VB with a sparse
prior ? is that the raw fractional counts c are re-
placed by exp(?(c + ?)), an operation that resem-
bles smoothing. As pointed out by Johnson (2007),
in effect this expression adds to c a small value that
asymptotically approaches ? ? 0.5 as c approaches
?, and 0 as c approaches 0. For small values of
? the net effect is the opposite of typical smooth-
ing, since it tends to redistribute probably mass away
from unlikely events onto more likely ones.
4 Bitext Pruning Strategy
ITG is slow mainly because it considers every pair of
spans in two sentences as a possible chart element.
In reality, the set of useful chart elements is much
99
smaller than the possible scriptO(n4), where n is
the average sentence length. Pruning the span pairs
(bitext cells) that can participate in a tree (either as
terminals or non-terminals) serves to not only speed
up ITG parsing, but also to provide a kind of ini-
tialization hint to the training procedures, encourag-
ing it to focus on promising regions of the alignment
space.
Given a bitext cell defined by the four boundary
indices (i, j, l,m) as shown in Figure 1a, we prune
based on a figure of merit V (i, j, l,m) approximat-
ing the utility of that cell in a full ITG parse. The
figure of merit considers the Model 1 scores of not
only the words inside a given cell, but also all the
words not included in the source and target spans, as
in Moore (2003) and Vogel (2005). Like Zhang and
Gildea (2005), it is used to prune bitext cells rather
than score phrases. The total score is the product of
the Model 1 probabilities for each column; ?inside?
columns in the range [l,m] are scored according to
the sum (or maximum) of Model 1 probabilities for
[i, j], and ?outside? columns use the sum (or maxi-
mum) of all probabilities not in the range [i, j].
Our pruning differs from Zhang and Gildea
(2005) in two major ways. First, we perform prun-
ing using both directions of the IBM Model 1 scores;
instead of a single figure of merit V , we have two:
VF and VB . Only those spans that pass the prun-
ing threshold in both directions are kept. Second,
we allow whole spans to be pruned. The figure of
merit for a span is VF (i, j) = maxl,m VF (i, j, l,m).
Only spans that are within some threshold of the un-
restricted Model 1 scores VF and VB are kept:
VF (i, j)
VF
? ?s and
VB(l,m)
VB
? ?s.
Amongst those spans retained by this first threshold,
we keep only those bitext cells satisfying both
VF (i, j, l,m)
VF (i, j)
? ?b and
VB(i, j, l,m)
VB(l,m)
? ?b.
4.1 Fast Tic-tac-toe Pruning
The tic-tac-toe pruning algorithm (Zhang and
Gildea, 2005) uses dynamic programming to com-
pute the product of inside and outside scores for
all cells in O(n4) time. However, even this can be
slow for large values of n. Therefore we describe an
Figure 1: (a) shows the original tic-tac-toe score for a
bitext cell (i, j, l,m). (b) demonstrates the finite state
representation using the machine in (c), assuming a fixed
source span (i, j).
improved algorithm with best case n3 performance.
Although the worst case performance is also O(n4),
in practice it is significantly faster.
To begin, let us restrict our attention to the for-
ward direction for a fixed source span (i, j). Prun-
ing bitext spans and cells requires VF (i, j), the score
of the best bitext cell within a given span, as well
as all cells within a given threshold of that best
score. For a fixed i and j, we need to search over
the starting and ending points l and m of the in-
side region. Note that there is an isomorphism be-
tween the set of spans and a simple finite state ma-
chine: any span (l,m) can be represented by a se-
quence of l OUTSIDE columns, followed bym?l+1
INSIDE columns, followed by n ? m + 1 OUT-
SIDE columns. This simple machine has the re-
stricted form described in Figure 1c: it has three
states, L, M , and R; each transition generates ei-
ther an OUTSIDE column O or an INSIDE column
I . The cost of generating an OUTSIDE at posi-
tion a is O(a) = P (ta|NULL) +
?
b 6?[i,j] P (ta|sb);
likewise the cost of generating an INSIDE column
is I(a) = P (ta|NULL) +
?
b?[i,j] P (ta|sb), with
100
O(0) = O(n+ 1) = 1 and I(0) = I(n+ 1) = 0.
Directly computing O and I would take time
O(n2) for each source span, leading to an overall
runtime of O(n4). Luckily there are faster ways to
find the inside and outside scores. First we can pre-
compute following arrays in O(n2) time and space:
pre[0, l] := P (tl|NULL)
pre[i, l] := pre[i? 1, l] + P (tl|si)
suf[n+ 1, l] := 0
suf[i, l] := suf[i+ 1, l] + P (tl|si)
Then for any (i, j), O(a) = P (ta|NULL) +
?
b 6?[i,j] P (ta|sb) = pre[i ? 1, a] + suf[j + 1, a].
I(a) can be incrementally updated as the source
span varies: when i = j, I(a) = P (ta|NULL) +
P (ta|si). As j is incremented, we add P (ta|sj) to
I(a). Thus we have linear time updates for O and I .
We can then find the best scoring sequence using
the familiar Viterbi algorithm. Let ?[a, ?] be the cost
of the best scoring sequence ending at in state ? at
time a:
?[0, ?] := 1 if ? = L; 0 otherwise
?[a, L] := ?[a? 1, L] ?O(a)
?[a,M ] := max
??L,M
{?[a? 1, ?]} ? I(a)
?[a,R] := max
??M,R
{?[a? 1, ?]} ?O(a)
Then VF (i, j) = ?[n + 1, R], using the isomor-
phism between state sequences and spans. This lin-
ear time algorithm allows us to compute span prun-
ing in O(n3) time. The same algorithm may be
performed using the backward figure of merit after
transposing rows and columns.
Having cast the problem in terms of finite state au-
tomata, we can use finite state algorithms for prun-
ing. For instance, fixing a source span we can enu-
merate the target spans in decreasing order by score
(Soong and Huang, 1991), stopping once we en-
counter the first span below threshold. In practice
the overhead of maintaining the priority queue out-
weighs any benefit, as seen in Figure 2.
An alternate approach that avoids this overhead is
to enumerate spans by position. Note that ?[m,R] ?
?n
a=m+1O(a) is within threshold iff there is a
span with right boundary m? < m within thresh-
old. Furthermore if ?[m,M ] ? ?na=m+1O(a) is
 0
 100
 200
 300
 400
 500
 600
 700
 800
 900
 10  20  30  40  50
Pr
un
in
g 
tim
e 
(th
ou
san
ds
 of
 se
co
nd
s)
Average sentence length
Baseline
k-best
Fast
Figure 2: Speed comparison of the O(n4) tic-tac-toe
pruning algorithm, the A* top-x algorithm, and the fast
tic-tac-toe pruning. All produce the same set of bitext
cells, those within threshold of the best bitext cell.
within threshold, thenm is the right boundary within
threshold. Using these facts, we can gradually
sweep the right boundary m from n toward 1 until
the first condition fails to hold. For each value where
the second condition holds, we pause to search for
the set of left boundaries within threshold.
Likewise for the left edge, ?[l,M ] ??ma=l+1 I(a) ?
?n
a=m+1O(a) is within threshold iff there is some
l? < l identifying a span (l?,m) within threshold.
Finally if V (i, j, l,m) = ?[l ? 1, L] ? ?ma=l I(a) ?
?n
a=m+1O(a) is within threshold, then (i, j, l,m)
is a bitext cell within threshold. For right edges that
are known to be within threshold, we can sweep the
left edges leftward until the first condition no longer
holds, keeping only those spans for which the sec-
ond condition holds.
The filtering algorithm behaves extremely well.
Although the worst case runtime is still O(n4), the
best case has improved to n3; empirically it seems to
significantly reduce the amount of time spent explor-
ing spans. Figure 2 compares the speed of the fast
tic-tac-toe algorithm against the algorithm in Zhang
and Gildea (2005).
101
Figure 3: Example output from the ITG using non-compositional phrases. (a) is the Viterbi alignment from the word-
based ITG. The shaded regions indicate phrasal alignments that are allowed by the non-compositional constraint; all
other phrasal alignments will not be considered. (b) is the Viterbi alignment from the phrasal ITG, with the multi-word
alignments highlighted.
5 Bootstrapping Phrasal ITG from
Word-based ITG
This section introduces a technique that bootstraps
candidate phrase pairs for phrase-based ITG from
word-based ITG Viterbi alignments. The word-
based ITG uses the same expansions for the non-
terminal X , but the expansions of C are limited to
generate only 1-1, 1-0, and 0-1 alignments:
C ? e/f,
C ? e/?,
C ? ?/f
where ? indicates that no word was generated.
Broadly speaking, the goal of this section is the same
as the previous section, namely, to limit the set of
phrase pairs that needs to be considered in the train-
ing process. The tic-tac-toe pruning relies on IBM
model 1 for scoring a given aligned area. In this
part, we use word-based ITG alignments as anchor
points in the alignment space to pin down the poten-
tial phrases. The scope of iterative phrasal ITG train-
ing, therefore, is limited to determining the bound-
aries of the phrases anchored on the given one-to-
one word alignments.
The heuristic method is based on the Non-
Compositional Constraint of Cherry and Lin (2007).
Cherry and Lin (2007) use GIZA++ intersections
which have high precision as anchor points in the
bitext space to constraint ITG phrases. We use ITG
Viterbi alignments instead. The benefit is two-fold.
First of all, we do not have to run a GIZA++ aligner.
Second, we do not need to worry about non-ITG
word alignments, such as the (2, 4, 1, 3) permutation
patterns. GIZA++ does not limit the set of permu-
tations allowed during translation, so it can produce
permutations that are not reachable using an ITG.
Formally, given a word-based ITG alignment, the
bootstrapping algorithm finds all the phrase pairs
according to the definition of Och and Ney (2004)
and Chiang (2005) with the additional constraint
that each phrase pair contains at most one word
link. Mathematically, let e(i, j) count the number of
word links that are emitted from the substring ei...j ,
and f(l,m) count the number of word links emit-
ted from the substring fl...m. The non-compositional
phrase pairs satisfy
e(i, j) = f(l,m) ? 1.
Figure 3 (a) shows all possible non-compositional
phrases given the Viterbi word alignment of the ex-
ample sentence pair.
6 Summary of the Pipeline
We summarize the pipeline of our system, demon-
strating the interactions between the three main con-
tributions of this paper: Variational Bayes, tic-tac-
toe pruning, and word-to-phrase bootstrapping. We
102
start from sentence-aligned bilingual data and run
IBM Model 1 in both directions to obtain two trans-
lation tables. Then we use the efficient bidirectional
tic-tac-toe pruning to prune the bitext space within
each of the sentence pairs; ITG parsing will be car-
ried out on only this this sparse set of bitext cells.
The first stage of training is word-based ITG, us-
ing the standard iterative training procedure, except
VB replaces EM to focus on a sparse prior. Af-
ter several training iterations, we obtain the Viterbi
alignments on the training data according to the fi-
nal model. Now we transition into the second stage
? the phrasal training. Before the training starts,
we apply the non-compositional constraints over the
pruned bitext space to further constrain the space
of phrase pairs. Finally, we run phrasal ITG itera-
tive training using VB for a certain number of itera-
tions. In the end, a Viterbi pass for the phrasal ITG is
executed to produce the non-compositional phrasal
alignments. From this alignment, phrase pairs are
extracted in the usual manner, and a phrase-based
translation system is trained.
7 Experiments
The training data was a subset of 175K sentence
pairs from the NIST Chinese-English training data,
automatically selected to maximize character-level
overlap with the source side of the test data. We put
a length limit of 35 on both sides, producing a train-
ing set of 141K sentence pairs. 500 Chinese-English
pairs from this set were manually aligned and used
as a gold standard.
7.1 Word Alignment Evaluation
First, using evaluations of alignment quality, we
demonstrate the effectiveness of VB over EM, and
explore the effect of the prior.
Figure 4 examines the difference between EM and
VB with varying sparse priors for the word-based
model of ITG on the 500 sentence pairs, both af-
ter 10 iterations of training. Using EM, because of
overfitting, AER drops first and increases again as
the number of iterations varies from 1 to 10. The
lowest AER using EM is achieved after the second
iteration, which is .40. At iteration 10, AER for EM
increases to .42. On the other hand, using VB, AER
decreases monotonically over the 10 iterations and
 0.2
 0.25
 0.3
 0.35
 0.4
 0.45
 0.5
 0.55
 0.6
 1e-009  1e-006  0.001  1
A
ER
Prior value
VB
EM
Figure 4: AER drops as ?C approaches zero; a more
sparse solution leads to better results.
stabilizes at iteration 10. When ?C is 1e ? 9, VB
gets AER close to .35 at iteration 10.
As we increase the bias toward sparsity, the AER
decreases, following a long slow plateau. Although
the magnitude of improvement is not large, the trend
is encouraging.
These experiments also indicate that a very sparse
prior is needed for machine translation tasks. Un-
like Johnson (2007), who found optimal perfor-
mance when ? was approximately 10?4, we ob-
served monotonic increases in performance as ?
dropped. The dimensionality of this MT problem is
significantly larger than that of the sequence prob-
lem, though, therefore it may take a stronger push
from the prior to achieve the desired result.
7.2 End-to-end Evaluation
Given an unlimited amount of time, we would tune
the prior to maximize end-to-end performance, us-
ing an objective function such as BLEU. Unfortu-
nately these experiments are very slow. Since we
observed monotonic increases in alignment perfor-
mance with smaller values of ?C , we simply fixed
the prior at a very small value (10?100) for all trans-
lation experiments. We do compare VB against EM
in terms of final BLEU scores in the translation ex-
periments to ensure that this sparse prior has a sig-
103
nificant impact on the output.
We also trained a baseline model with GIZA++
(Och and Ney, 2003) following a regimen of 5 it-
erations of Model 1, 5 iterations of HMM, and 5
iterations of Model 4. We computed Chinese-to-
English and English-to-Chinese word translation ta-
bles using five iterations of Model 1. These val-
ues were used to perform tic-tac-toe pruning with
?b = 1 ? 10?3 and ?s = 1 ? 10?6. Over the pruned
charts, we ran 10 iterations of word-based ITG using
EM or VB. The charts were then pruned further by
applying the non-compositional constraint from the
Viterbi alignment links of that model. Finally we ran
10 iterations of phrase-based ITG over the residual
charts, using EM or VB, and extracted the Viterbi
alignments.
For translation, we used the standard phrasal de-
coding approach, based on a re-implementation of
the Pharaoh system (Koehn, 2004). The output of
the word alignment systems (GIZA++ or ITG) were
fed to a standard phrase extraction procedure that
extracted all phrases of length up to 7 and esti-
mated the conditional probabilities of source given
target and target given source using relative fre-
quencies. Thus our phrasal ITG learns only the
minimal non-compositional phrases; the standard
phrase-extraction algorithm learns larger combina-
tions of these minimal units. In addition the phrases
were annotated with lexical weights using the IBM
Model 1 tables. The decoder also used a trigram lan-
guage model trained on the target side of the training
data, as well as word count, phrase count, and distor-
tion penalty features. Minimum Error Rate training
(Och, 2003) over BLEU was used to optimize the
weights for each of these models over the develop-
ment test data.
We used the NIST 2002 evaluation datasets for
tuning and evaluation; the 10-reference develop-
ment set was used for minimum error rate training,
and the 4-reference test set was used for evaluation.
We trained several phrasal translation systems, vary-
ing only the word alignment (or phrasal alignment)
method.
Table 1 compares the four systems: the GIZA++
baseline, the ITG word-based model, the ITG multi-
word model using EM training, and the ITG multi-
word model using VB training. ITG-mwm-VB is
our best model. We see an improvement of nearly
Development Test
GIZA++ 37.46 28.24
ITG-word 35.47 26.55
ITG-mwm (VB) 39.21 29.02
ITG-mwm (EM) 39.15 28.47
Table 1: Translation results on Chinese-English, using
the subset of training data (141K sentence pairs) that have
length limit 35 on both sides. (No length limit in transla-
tion. )
2 points dev set and nearly 1 point of improvement
on the test set. We also observe the consistent supe-
riority of VB over EM. The gain is especially large
on the test data set, indicating VB is less prone to
overfitting.
8 Conclusion
We have presented an improved and more efficient
method of estimating phrase pairs directly. By both
changing the objective function to include a bias
toward sparser models and improving the pruning
techniques and efficiency, we achieve significant
gains on test data with practical speed. In addition,
these gains were shown without resorting to external
models, such as GIZA++. We have shown that VB
is both practical and effective for use in MT models.
However, our best system does not apply VB to a
single probability model, as we found an apprecia-
ble benefit from bootstrapping each model from sim-
pler models, much as the IBM word alignment mod-
els are usually trained in succession. We find that
VB alone is not sufficient to counteract the tendency
of EM to prefer analyses with smaller trees using
fewer rules and longer phrases. Both the tic-tac-toe
pruning and the non-compositional constraint ad-
dress this problem by reducing the space of possible
phrase pairs. On top of these hard constraints, the
sparse prior of VB helps make the model less prone
to overfitting to infrequent phrase pairs, and thus
improves the quality of the phrase pairs the model
learns.
Acknowledgments This work was done while the
first author was at Microsoft Research; thanks to Xi-
aodong He, Mark Johnson, and Kristina Toutanova.
The last author was supported by NSF IIS-0546554.
104
References
Matthew Beal. 2003. Variational Algorithms for Ap-
proximate Bayesian Inference. Ph.D. thesis, Gatsby
Computational Neuroscience Unit, University College
London.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathemat-
ics of statistical machine translation: Parameter esti-
mation. Computational Linguistics, 19(2):263?311,
June.
Colin Cherry and Dekang Lin. 2007. Inversion transduc-
tion grammar for joint phrasal translation modeling.
In Proceedings of SSST, NAACL-HLT 2007 / AMTA
Workshop on Syntax and Structure in Statistical Trans-
lation, pages 17?24, Rochester, New York, April. As-
sociation for Computational Linguistics.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
ACL, pages 263?270, Ann Arbor, Michigan, USA.
Sharon Goldwater and Tom Griffiths. 2007. A fully
bayesian approach to unsupervised part-of-speech tag-
ging. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages
744?751, Prague, Czech Republic, June. Association
for Computational Linguistics.
Mark Johnson. 2007. Why doesn?t EM find good
HMM POS-taggers? In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 296?305.
Philipp Koehn. 2004. Pharaoh: A beam search de-
coder for phrase-based statistical machine translation
models. In Proceedings of the 6th Conference of the
Association for Machine Translation in the Americas
(AMTA), pages 115?124, Washington, USA, Septem-
ber.
Kenichi Kurihara and Taisuke Sato. 2006. Variational
bayesian grammar induction for natural language. In
International Colloquium on Grammatical Inference,
pages 84?96, Tokyo, Japan.
Daniel Marcu and William Wong. 2002. A phrase-based,
joint probability model for statistical machine transla-
tion. In 2002 Conference on Empirical Methods in
Natural Language Processing (EMNLP).
Robert C. Moore. 2003. Learning translations of named-
entity phrases from parallel corpora. In Proceedings
of EACL, Budapest, Hungary.
Franz Och and Hermann Ney. 2003. A systematic com-
parison of various statistical alignment models. Com-
putational Linguistics, 29(1):19?51, March.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Computational Linguistics, 30(4):417?449, De-
cember.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of ACL,
pages 160?167, Sapporo, Japan.
Frank Soong and Eng Huang. 1991. A tree-trellis based
fast search for finding the n best sentence hypotheses
in continuous speech recognition. In Proceedings of
ICASSP 1991.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical trans-
lation. In Proceedings of COLING, pages 836?741,
Copenhagen, Denmark.
Stephan Vogel. 2005. PESA: Phrase pair extraction as
sentence splitting. In MT Summit X, Phuket, Thailand.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?403, Septem-
ber.
Hao Zhang and Daniel Gildea. 2005. Stochastic lexical-
ized inversion transduction grammar for alignment. In
Proceedings of ACL.
105
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 349?352,
Suntec, Singapore, 4 August 2009. c?2009 ACL and AFNLP
Improved Smoothing for N-gram Language Models
Based on Ordinary Counts
Robert C. Moore Chris Quirk
Microsoft Research
Redmond, WA 98052, USA
{bobmoore,chrisq}@microsoft.com
Abstract
Kneser-Ney (1995) smoothing and its vari-
ants are generally recognized as having
the best perplexity of any known method
for estimating N-gram language models.
Kneser-Ney smoothing, however, requires
nonstandard N-gram counts for the lower-
order models used to smooth the highest-
order model. For some applications, this
makes Kneser-Ney smoothing inappropri-
ate or inconvenient. In this paper, we in-
troduce a new smoothing method based on
ordinary counts that outperforms all of the
previous ordinary-count methods we have
tested, with the new method eliminating
most of the gap between Kneser-Ney and
those methods.
1 Introduction
Statistical language models are potentially useful
for any language technology task that produces
natural-language text as a final (or intermediate)
output. In particular, they are extensively used in
speech recognition and machine translation. De-
spite the criticism that they ignore the structure of
natural language, simple N-gram models, which
estimate the probability of each word in a text
string based on the N?1 preceding words, remain
the most widely used type of model.
The simplest possible N-gram model is the
maximum likelihood estimate (MLE), which takes
the probability of a word wn, given the preceding
context w1 . . . wn?1, to be the ratio of the num-
ber of occurrences in a training corpus of the N-
gram w1 . . . wn to the total number of occurrences
of any word in the same context:
p(wn|w1 . . . wn?1) =
C(w1 . . . wn)
?
w? C(w1 . . . wn?1w?)
One obvious problem with this method is that it
assigns a probability of zero to any N-gram that is
not observed in the training corpus; hence, numer-
ous smoothing methods have been invented that
reduce the probabilities assigned to some or all ob-
served N-grams, to provide a non-zero probability
for N-grams not observed in the training corpus.
The best methods for smoothing N-gram lan-
guage models all use a hierarchy of lower-order
models to smooth the highest-order model. Thus,
if w1w2w3w4w5 was not observed in the train-
ing corpus, p(w5|w1w2w3w4) is estimated based
on p(w5|w2w3w4), which is estimated based on
p(w5|w3w4) if w2w3w4w5 was not observed, etc.
In most smoothing methods, the lower-order
models, for all N > 1, are recursively estimated
in the same way as the highest-order model. How-
ever, the smoothing method of Kneser and Ney
(1995) and its variants are the most effective meth-
ods known (Chen and Goodman, 1998), and they
use a different way of computing N-gram counts
for all the lower-order models used for smooth-
ing. For these lower-order models, the actual cor-
pus counts C(w1 . . . wn) are replaced by
C ?(w1 . . . wn) =
?
?{w?|C(w?w1 . . . wn) > 0}
?
?
In other words, the count used for a lower-order
N-gram is the number of distinct word types that
precede it in the training corpus.
The fact that the lower-order models are es-
timated differently from the highest-order model
makes the use of Kneser-Ney (KN) smooth-
ing awkward in some situations. For example,
coarse-to-fine search using a sequence of lower-
order to higher-order language models has been
shown to be an efficient way of constraining high-
dimensional search spaces for speech recognition
(Murveit et al, 1993) and machine translation
(Petrov et al, 2008). The lower-order models used
in KN smoothing, however, are very poor esti-
mates of the probabilities for N-grams that have
been observed in the training corpus, so they are
349
p(wn|w1 . . . wn?1) =
?
?
?
?
?
?
?
?
?
?
?
?w1...wn?1
Cn(w1...wn)?Dn,Cn(w1...wn)
?
w? Cn(w1...wn?1w?)
+ ?w1...wn?1p(wn|w2 . . . wn?1) if Cn(w1 . . . wn) > 0
?w1...wn?1p(wn|w2 . . . wn?1) if Cn(w1 . . . wn) = 0
Figure 1: General language model smoothing schema
not suitable for use in coarse-to-fine search. Thus,
two versions of every language model below the
highest-order model would be needed to use KN
smoothing in this case.
Another case in which use of special KN counts
is problematic is the method presented by Nguyen
et al (2007) for building and applying language
models trained on very large corpora (up to 40 bil-
lion words in their experiments). The scalability
of their approach depends on a ?backsorted trie?,
but this data structure does not support efficient
computation of the special KN counts.
In this paper, we introduce a new smoothing
method for language models based on ordinary
counts. In our experiments, it outperformed all
of the previous ordinary-count methods we tested,
and it eliminated most of the gap between KN
smoothing and the other previous methods.
2 Overview of Previous Methods
All the language model smoothing methods we
will consider can be seen as instantiating the recur-
sive schema presented in Figure 1, for all n such
that N ? n ? 2,1 where N is the greatest N-gram
length used in the model.
In this schema, Cn denotes the counting method
used for N-grams of length n. For most smoothing
methods, Cn denotes actual training corpus counts
for all n. For KN smoothing and its variants, how-
ever, Cn denotes actual corpus counts only when
n is the greatest N-gram length used in the model,
and otherwise denotes the special KN C ? counts.
In this schema, each N-gram count is dis-
counted according to a D parameter that depends,
at most, on the N-gram length and the the N-gram
count itself. The values of the ?, ?, and ? parame-
ters depend on the context w1 . . . wn?1. For each
context, the values of ?, ?, and ? must be set to
produce a normalized conditional probability dis-
tribution. Additional constraints on the previous
1For n = 2, we take the expression p(wn|w2 . . . wn?1)
to denote a unigram probability estimate p(w2).
models we consider further reduce the degrees of
freedom so that ultimately the values of these para-
meters are completely fixed by the values selected
for the D parameters.
The previous smoothing methods we consider
can be classified as either ?pure backoff?, or ?pure
interpolation?. In pure backoff methods, all in-
stances of ? = 1 and all instances of ? = 0. The
pure backoff methods we consider are Katz back-
off and backoff absolute discounting, due to Ney
et al2 In Katz backoff, if C(w1 . . . wn) is greater
than a threshold (here set to 5, as recommended
by Katz) the corresponding D = 0; otherwise D
is set according to the Good-Turing method.3
In backoff absolute discounting, the D parame-
ters depends, at most, on n; there is either one dis-
count per N-gram length, or a single discount used
for all N-gram lengths. The values of D can be set
either by empirical optimization on held-out data,
or based on a theoretically optimal value derived
from a leaving-one-out analysis, which Ney et al
show to be approximated for each N-gram length
by N1/(N1 + 2N2), where Nr is the number of
distinct N-grams of that length occuring r times in
the training corpus.
In pure interpolation methods, for each context,
? and ? are constrained to be equal. The models
we consider that fall into this class are interpolated
absolute discounting, interpolated KN, and modi-
fied interpolated KN. In these three methods, all
instances of ? = 1.4 In interpolated absolute dis-
counting, the instances of D are set as in backoff
absolute discounting. The same is true for inter-
2For all previous smoothing methods other than KN, we
refer the reader only to the excellent comparative study of
smoothing methods by Chen and Goodman (1998). Refer-
ences to the original sources may be found there.
3Good-Turing discounting is usually expressed in terms
of a discount ratio, but this can be reformulated as Dr =
r ? drr, where Dr is the subtractive discount for an N-gram
occuring r times, and dr is the corresponding discount ratio.
4Jelinek-Mercer smoothing would also be a pure interpo-
lation instance of our language model schema, in which all
instances of D = 0 and, for each context, ?+ ? = 1.
350
polated KN, but the lower-order models are esti-
mated using the special KN counts.
In Chen and Goodman?s (1998) modified inter-
polated KN, instead of one D parameter for each
N-gram length, there are three: D1 for N-grams
whose count is 1, D2 for N-grams whose count is
2, and D3 for N-grams whose count is 3 or more.
The values of these parameters may be set either
by empirical optimization on held-out data, or by
a theoretically-derived formula analogous to the
Ney et al formula for the one-discount case:
Dr = r ? (r + 1)Y
Nr+1
Nr
,
for 1 ? r ? 3, where Y = N1/(N1 + 2N2), the
discount value derived by Ney et al
3 The New Method
Our new smoothing method is motivated by the
observation that unsmoothed MLE language mod-
els suffer from two somewhat independent sources
of error in estimating probabilities for the N-grams
observed in the training corpus. The problem that
has received the most attention is the fact that, on
the whole, the MLE probabilities for the observed
N-grams are overestimated, since they end up with
all the probability mass that should be assigned to
the unobserved N-grams. The discounting used in
Katz backoff is based on the Good-Turing estimate
of exactly this error.
Another source of error in MLE models, how-
ever, is quantization error, due to the fact that only
certain estimated probability values are possible
for a given context, depending on the number of
occurrences of the context in the training corpus.
No pure backoff model addresses this source of
error, since no matter how the discount parame-
ters are set, the number of possible probability val-
ues for a given context cannot be increased just
by discounting observed counts, as long as all N-
grams with the same count receive the same dis-
count. Interpolation models address quantization
error by interpolation with lower-order estimates,
which should have lower quantization error, due to
higher context counts. As we have noted, most ex-
isting interpolation models are constrained so that
the discount parameters fully determine the inter-
polation parameters. Thus the discount parameters
have to correct for both types of error.5
5Jelinek-Mercer smoothing is an exception to this gener-
alization, but since it has only interpolation parameters and
Our new model provides additional degrees of
freedom so the ? and ? interpolation parameters
can be set independently of the discount parame-
ters D, with the intention that the ? and ? para-
meters correct for quantization error, and the D
parameters correct for overestimation error. This
is accomplished by relaxing the link between the
? and ? parameters. We require that for each con-
text, ? ? 0, ? ? 0, and ? + ? = 1, and that
for every Dn,Cn(w1...wn) parameter, 0 ? D ?
Cn(w1 . . . wn). For each context, whatever values
we choose for these parameters within these con-
straints, we are guaranteed to have some probabil-
ity mass between 0 and 1 left over to be distributed
across the unobserved N-grams by a unique value
of ? that normalizes the conditional distribution.
Previous smoothing methods suggest several
approaches to setting the D parameters in our new
model. We try four such methods here:
1. The single theory-based discount for each N-
gram length proposed by Ney et al,
2. A single discount used for all N-gram
lengths, optimized on held-out data,
3. The three theory-based discounts for each N-
gram length proposed by Chen and Good-
man,
4. A novel set of three theory-based discounts
for each N-gram length, based on Good-
Turing discounting.
The fourth method is similar to the third, but
for the three D parameters per context, we use the
discounts for 1-counts, 2-counts, and 3-counts es-
timated by the Good-Turing method. This yields
the formula
Dr = r ? (r + 1)
Nr+1
Nr
,
which is identical to the Chen-Goodman formula,
except that the Y factor is omitted. Since Y is gen-
erally between 0 and 1, the resulting discounts will
be smaller than with the Chen-Goodman formula.
To set the ? and ? parameters, we assume that
there is a single unknown probability distribution
for the amount of quantization error in every N-
gram count. If so, the total quantization error for
a given context will tend to be proportional to the
no discount parameters, it forces the interpolation parameters
to do the same double duty that other models force the dis-
count parameters to do.
351
number of distinct counts for that context, in other
words, the number of distinct word types occur-
ring in that context. We then set ? and ? to replace
the proportion of the total probability mass for the
context represented by the estimated quantization
error with probability estimates derived from the
lower-order models:
?w1...wn?1 = ? |{w
?|Cn(w1...wn?1w?)>0}|
?
w? Cn(w1...wn?1w?)
?w1...wn?1 = 1? ?w1...wn?1
where ? is the estimated mean of the quantization
error introduced by each N-gram count.
We use a single value of ? for all contexts and
all N-gram lengths. As an a priori ?theory?-based
estimate, we assume that, since the distance be-
tween possible N-gram counts, after discounting,
is approximately 1.0, their mean quantization error
would be approximately 0.5. We also try setting ?
by optimization on held-out data.
4 Evaluation and Conclusions
We trained and measured the perplexity of 4-
gram language models using English data from
the WMT-06 Europarl corpus (Koehn and Monz,
2006). We took 1,003,349 sentences (27,493,499
words) for training, and 2000 sentences each for
testing and parameter optimization.
We built models based on six previous ap-
proaches: (1) Katz backoff, (2) interpolated ab-
solute discounting with Ney et al formula dis-
counts, backoff absolute discounting with (3) Ney
et al formula discounts and with (4) one empir-
ically optimized discount, (5) modified interpo-
lated KN with Chen-Goodman formula discounts,
and (6) interpolated KN with one empirically op-
timized discount. We built models based on four
ways of computing the D parameters of our new
model, with a fixed ? = 0.5: (7) Ney et al formula
discounts, (8) one empirically optimized discount,
(9) Chen-Goodman formula discounts, and (10)
Good-Turing formula discounts. We also built a
model (11) based on one empirically optimized
discount D = 0.55 and an empircially optimized
value of ? = 0.9. Table 1 shows that each of these
variants of our method had better perplexity than
every previous ordinary-count method tested.
Finally, we performed one more experiment, to
see if the best variant of our model (11) combined
with KN counts would outperform either variant
of interpolated KN. It did not, yielding a perplex-
ity of 53.9 after reoptimizing the two free parame-
Method PP
1 Katz backoff 59.8
2 interp-AD-fix 62.6
3 backoff-AD-fix 59.9
4 backoff-AD-opt 58.8
5 KN-mod-fix 52.8
6 KN-opt 53.0
7 new-AD-fix 56.3
8 new-AD-opt 55.6
9 new-CG-fix 57.4
10 new-GT-fix 56.1
11 new-AD-2-opt 54.9
Table 1: 4-gram perplexity results
ters of the model with the KN counts. However,
the best variant of our model eliminated 65% of
the difference in perplexity between the best pre-
vious ordinary-count method tested and the best
variant of KN smoothing tested, suggesting that it
may currently be the best approach when language
models based on ordinary counts are desired.
References
Chen, Stanley F., and Joshua Goodman. 1998.
An empirical study of smoothing techniques for
language modeling. Technical Report TR-10-
98, Harvard University.
Kneser, Reinhard, and Hermann Ney. 1995. Im-
proved backing-off for m-gram language mod-
eling. In Proceedings of ICASSP-95, vol. 1,
181?184.
Koehn, Philipp, and Christof Monz. 2006. Manual
and automatic evaluation of machine translation
between European languages. In Proceedings
of WMT-06, 102?121.
Murveit, Hy, John Butzberger, Vassilios Digalakis,
and Mitch Weintraub. 1993. Progressive search
algorithms for large-vocabulary speech recogni-
tion. In Proceedings of HLT-93, 87?90.
Nguyen, Patrick, Jianfeng Gao, and Milind Maha-
jan. 2007. MSRLM: a scalable language mod-
eling toolkit. Technical Report MSR-TR-2007-
144. Microsoft Research.
Petrov, Slav, Aria Haghighi, and Dan Klein. 2008.
Coarse-to-fine syntactic machine translation us-
ing language projections. In Proceedings of
ACL-08. 108?116.
352
English-Japanese Example-Based Machine Translation Using Abstract 
Linguistic Representations 
 
Chris Brockett, Takako Aikawa, Anthony Aue, Arul Menezes, Chris Quirk  
and Hisami Suzuki 
Natural Language Processing Group, Microsoft Research 
One Microsoft Way 
Redmond, WA 98052, USA 
{chrisbkt,takakoa,anthaue,arulm,chrisq,hisamis}@microsoft.com 
 
 
Abstract 
This presentation describes an example- 
based English-Japanese machine trans- 
lation system in which an abstract 
linguistic representation layer is used to 
extract and store bilingual translation 
knowledge, transfer patterns between 
languages, and generate output strings. 
Abstraction permits structural neutral-
izations that facilitate learning of trans-
lation examples across languages with 
radically different surface structure charac-
teristics, and allows MT development to 
proceed within a largely language- 
independent NLP architecture. Com-
parative evaluation indicates that after 
training in a domain the English-Japanese 
system is statistically indistinguishable 
from a non-customized commercially 
available MT system in the same domain. 
Introduction 
In the wake of the pioneering work of Nagao 
(1984), Brown et al (1990) and Sato and 
Nagao (1990), Machine Translation (MT) 
research has increasingly focused on the issue 
of how to acquire translation knowledge from 
aligned parallel texts. While much of this 
research effort has focused on acquisition of 
correspondences between individual lexical 
items or between unstructured strings of words, 
closer attention has begun to be paid to the 
learning of structured phrasal units: Yamamoto 
and Matsumoto (2000), for example, describe a 
method for automatically extracting correspon-
dences between dependency relations in 
Japanese and English. Similarly, Imamura 
(2001a, 2001b) seeks to match corresponding 
Japanese and English phrases containing 
information about hierarchical structures, 
including partially completed parses. 
 
Yamamoto and Matsumoto (2000) explicitly 
assume that dependency relations between 
words will generally be preserved across 
languages. However, when languages are as 
different as Japanese and English with respect 
to their syntactic and informational structures, 
grammatical or dependency relations may not 
always be preserved: the English sentence ?the 
network failed? has quite a different 
grammatical structure from its Japanese 
translation equivalent ??????????
???? ?a defect arose in the network.? One 
issue for example-based MT, then, is to capture 
systematic divergences through generic 
learning applicable to multiple language pairs. 
 
In this presentation we describe the MSR-MT 
English-Japanese system, an example-based 
MT system that learns structured phrase-sized 
translation units. Unlike the systems discussed 
in Yamamoto and Matsumoto (2000) and 
Imamura (2001a, 2001b), MSR-MT places the 
locus of translation knowledge acquisition at a 
greater level of abstraction than surface 
relations, pushing it into a semantically- 
motivated layer called LOGICAL FORM (LF) 
(Heidorn 2000; Campbell & Suzuki 2002a, 
2002b). Abstraction has the effect of 
neutralizing (or at least minimizing) differences 
in word order and syntactic structure, so that 
mappings between structural relations 
associated with lexical items can readily be 
acquired within a general MT architecture. 
 
In Section 1 below, we present an overview of 
the characteristics of the system, with special 
reference to English-Japanese MT. Section 2 
discusses a class of structures learned through 
phrase alignment, Section 3 presents the results 
of comparative evaluation, and Section 4 some 
factors that contributed to the evaluation results. 
Section 5 addresses directions for future work. 
 
  
1 The MSR-MT System 
The MSR-MT English-Japanese system is a 
hybrid example-based machine translation 
system that employs handcrafted broad- 
coverage augmented phrase structure grammars 
for parsing, and statistical and heuristic 
techniques to capture translation knowlege and 
for transfer between languages. The parsers are 
general purpose: the English parser, for 
example, forms the core of the grammar 
checkers used in Microsoft Word (Heidorn 
2000). The Japanese grammar utilizes much of 
the same codebase, but contains language- 
specific grammar rules and additional features 
owing to the need for word-breaking in 
Japanese (Suzuki et al 2000; Kacmarcik et al 
2000). These parsers are robust in that if the 
analysis grammar fails to find an appropriate 
parse, it outputs a best-guess ?fitted? parse. 
 
System development is not confined to 
English-Japanese: MSR-MT is part of a 
broader natural language processing project 
involving three Asian languages (Japanese, 
Chinese, and Korean) and four European 
languages (English, French, German, and 
Spanish). Development of the MSR-MT 
systems proceeds more or less simultaneously 
across these languages and in multiple 
directions, including Japanese-English. The 
Spanish-English version of MSR-MT has been 
described in Richardson et al 2001a, Richardson 
et al2001b, and the reader is referred to these 
papers for more information concerning 
algorithms employed during phrase alignment. 
A description of the French-Spanish MT 
system is found in Pinkham & Smets. 2002. 
 
1.1 Training Data 
MSR-MT requires that a large corpus of 
aligned sentences be available as examples for 
training. For English-Japanese MT, the system 
currently trains on a corpus of approximately 
596,000 pre-aligned sentence pairs. About 
274,000 of these are sentence pairs extracted 
from Microsoft technical documentation that 
had been professionally translated from 
English into Japanese. The remaining 322,000 
are sentence examples or sentence fragments 
extracted from electronic versions of student 
dictionaries.1  
1.2  Logical Form 
MSR-MT employs a post-parsing layer of 
semantic representation called LOGICAL FORM 
(LF) to handle core components of the 
translation process, namely acquisition and 
storage of translation knowledge, transfer 
between languages, and generation of target 
output. LF can be viewed as a representation of 
the various roles played by the content words 
after neutralizing word order and local 
morphosyntactic variation (Heidorn 2000; 
Campbell & Suzuki 2002a; 2002b). These can 
be seen in the Tsub (Typical Subject) and Tobj 
(Typical Object) relations in Fig. 1 in the 
sentence ?Mary eats pizza? and its Japanese 
counterpart. The graphs are simplified for 
expository purposes. 
Although our hypothesis is that equivalent 
sentences in two languages will tend to 
resemble each other at LF more than they do in 
the surface parse, we do not adopt a na?ve 
reductionism that would attempt to make LFs 
completely identical. In Fig. 2, for example, the 
LFs of the quantified nouns differ in that the 
Japanese LF preserves the classifier, yet are 
similar enough that learning the mapping 
between the two structures is straightforward. 
It will be noted that since the LF for each 
language stores words or morphemes of that 
language, this level of representation is not in 
any sense an interlingua. 
 
                                                   
1 Kodansha?s Basic English-Japanese Dictionary, 
1999; Kenkyusha?s New College Japanese-English 
Dictionary, 4th Edition, 1995 ; and Kenkyusha?s 
New College English-Japanese Dictionary, 6th 
Edition, 1994. 
 
 
Fig. 1  Canonical English and Japanese 
Logical Forms 
 
 
1.3  Mapping Logical Forms 
In the training phase, MSR-MT learns transfer 
mappings from the sentence-aligned bilingual 
corpus. First, the system deploys the 
general-purpose parsers to analyze the English 
and Japanese sentence pairs and generate LFs 
for each sentence. In the next step, an LF 
alignment algorithm is used to match source 
language and target language LFs at the 
sub-sentence level. 
 
The LF alignment algorithm first establishes 
tentative lexical correspondences between 
nodes in the source and target LFs on the basis 
of lexical matching over dictionary information 
and approximately 31,000 ?word associations,? 
that is, lexical mappings extracted from the 
training corpora using statistical techniques 
based on mutual information (Moore 2001). 
From these possible lexical correspondences, 
the algorithm uses a small grammar of 
(language-pair-independent) rules to align LF 
nodes on lexical and structural principles. The 
aligned LF pairs are then partitioned into 
smaller aligned LF segments, with individual 
node mappings captured in a relationship we 
call ?sublinking.? Finally, the aligned LF 
segments are filtered on the basis of frequency, 
and compiled into a database known as a 
Mindnet. (See Menezes & Richardson 2001 for a 
detailed description of this process.) 
 
The Mindnet is a general-purpose database of 
semantic information (Richardson et al 1998) 
that has been repurposed as the primary 
repository of translation information for MT 
applications. The process of building the 
Mindnet is entirely automated; there is no 
human vetting of candidate entries. At the end 
of a typical training session, 1,816,520 transfer 
patterns identified in the training corpus may 
yield 98,248 final entries in the Mindnet. Only 
the output of successful parses is considered 
for inclusion, and each mapping of LF 
segments must have been encountered twice in 
the corpus before it is incorporated into the 
Mindnet. 
 
In the Mindnet, LF segments from the source 
language are represented as linked to the 
corresponding LF segment from the target 
languages. These can be seen in Figs. 3 and 4, 
discussed below in Section 2. 
1.4  Transfer and Generation 
At translation time, the broad-coverage source 
language parser processes the English input 
sentence, and creates a source-language LF. 
This LF is then checked against the Mindnet 
entries. 2  The best matching structures are 
extracted and stitched together determinist-
ically into a new target-language ?transferred 
LF? that is then submitted to the Japanese 
system for generation of the output string. 
 
The generation module is language-specific 
and used for both monolingual generation and 
MT. In the context of MT, generation takes as 
input the transferred LF and converts it into a 
basic syntactic tree. A small set of heuristic 
rules preprocesses the transferred LF to 
?nativize? some structural differences, such as 
pro-drop phenomena in Japanese. A series of 
core generation rules then applies to the LF tree, 
transforming it into a Japanese sentence string. 
Generation rules operate on a single tree only, 
are application-independent and are developed 
in a monolingual environment (see Aikawa et 
al. 2001a, 2001b for further details.) 
Generation of inflectional morphology is also 
handled in this component. The generation 
component has no explicit knowledge of the 
source language. 
 
2 Acquisition of Complex Structural 
Mappings 
The generalization provided by LF makes it 
possible for MSR-MT to handle complex 
structural relations in cases where English and 
Japanese are systematically divergent. This is 
                                                   
2 MSR-MT resorts to lexical lookup only when a 
term is not found in the Mindnet. The handcrafted 
dictionary is slated for replacement by purely 
statistically generated data.  
 
 
Fig. 2  Cross-Linguistic Variation in Logical 
Form 
 
illustrated by the sample training pair in the 
lefthand column of Table 1. In Japanese, 
inanimate nouns tend to be avoided as subjects 
of transitive verbs; the word ?URL?, which is 
subject in the English sentence, thus 
corresponds to an oblique relation in the 
Japanese. (The Japanese sentence, although a 
natural and idiomatic translation of the English,  
is literally equivalent to ?one can access public 
folders with this URL.?)   
 
Nonetheless, mappings turn out to be learnable 
even where the information is structured so 
radically differently. Fig. 3 shows the Mindnet 
entry for ?provide,? which is result of training 
on sentence pairs like those in the lefthand 
column of Table 1. The system learns not only 
the mapping between the phrase ?provide 
access? and the potential form of ???? 
?access?, but also the crucial sublinking of the 
Tsub node of the English sentence and the node 
headed by ?  (underspecified for semantic 
role) in the Japanese. At translation time the 
system is able to generalize on the basis of the 
functional roles stored in the Mindnet; it can 
substitute lexical items to achieve a relatively 
natural translation of similar sentences such as 
shown in the right-hand side of Table 1.  
Differences of the kind seen in Fig 3 are 
endemic in our Japanese and English corpora. 
Fig. 4 shows part of the example Mindnet entry 
for the English word ?fail? referred to in the 
Introduction, which exhibits another mismatch 
in grammatical roles somewhat similar to that 
in observed in Fig. 3. Here again, the lexical 
matching and generic alignment heuristics have 
allowed the match to be captured into the 
Mindnet. Although the techniques employed 
may have been informed by analysis of 
language-specific data, they are in principle of 
general application. 
 
 
3 Evaluation 
In May 2002, we compared output of the 
MSR-MT English-Japanese system with a 
commercially available desktop MT system.3 
                                                   
3 Toshiba?s The Honyaku Office v2.0 desktop MT 
system was selected for this purpose. The Honyaku 
is a trademark of the Toshiba Corporation. Another 
desktop system was also considered for evaluation; 
however, comparative evaluation with that system 
indicated that the Toshiba system performed 
marginally, though not significantly, better on our 
technical documentation.  
 
Training Data Translation Output  
This URL provides access to public folders. 
 
This computer provides access to the internet. 
 
?? URL ?????? ????? 
????????? 
????????????????? 
????????? 
 
Table 1.  Sample Input and Output 
Fig. 3.  Part of the Mindnet Entry for ?provide? 
 
 
Fig. 4.  Part of the Mindnet Entry for ?fail? 
 
A total of 238 English-Japanese sentence pairs 
were randomly extracted from held-out 
software manual data of the same kinds used 
for training the system. 4  The Japanese 
sentences, which had been translated by human 
translators, were taken as reference sentences 
(and were assumed to be correct translations). 
The English sentences were then translated by 
the two MT systems into Japanese for blind 
evaluation performed by seven outside vendors 
unfamiliar with either system?s characteristics. 
 
No attempt was made to constrain or modify 
the English input sentences on the basis of 
length or other characteristics. Both systems 
provided a translation for each sentence.5  
 
For each of the Japanese reference sentences, 
evaluators were asked to select which 
translation was closer to the reference sentence. 
A value of +1 was assigned if the evaluator 
considered MSR-MT output sentence better 
and ?1 if they considered the comparison 
system better. If two translated sentences were 
considered equally good or bad in comparison 
                                                   
4  250 sentences were originally selected for 
evaluation; 12 were later discarded when it was 
discovered by evaluators that the Japanese reference 
sentences (not the input sentences) were defective 
owing to the presence of junk characters (mojibake) 
and other deficiencies.  
5 In MSR-MT, Mindnet coverage is sufficiently 
complete with respect to the domain that an 
untranslated sentence normally represents a 
complete failure to parse the input, typically owing 
to excessive length. 
to the reference, a value of 0 was assigned. On 
this metric, MSR-MT scored slightly worse 
than the comparison system rating of ?0.015. 
At a two-way confidence measure of +/?0.16, 
the difference between the systems is 
statistically insignificant. By contrast, an 
earlier evaluation conducted in October 2001 
yielded a score of ?0.34 vis-?-vis the 
comparison system. 
 
In addition, the evaluators were asked to rate 
the translation quality on an absolute scale of 1 
through 4, according to the following criteria: 
 
1. Unacceptable: Absolutely not comprehen- 
sible and/or little or no information trans- 
ferred accurately. 
2. Possibly Acceptable: Possibly compre- 
hensible (given enough context and/or 
time to work it out); some information 
transferred accurately. 
3. Acceptable: Not perfect, but definitely 
comprehensible, and with accurate transfer 
of all important information. 
4. Ideal: Not necessarily a perfect translation, 
but grammatically correct, and with all 
information accurately transferred. 
 
On this absolute scale, neither system 
performed exceptionally well: MSR-MT scored 
an average 2.25 as opposed to 2.32 for the 
comparison system. Again, the difference 
between the two is statistically insignificant. It 
should be added that the comparison presented 
here is not ideal, since MSR-MT was trained 
principally on technical manual sentences, 
 Evaluation 
Date 
Transfers 
per Sentence 
Nodes  
Per Transfer
 
 Oct. 2001 5.8 1.6  
 May 2002 6.7 2.0  
Table 2. Number of Transfers and Nodes Transferred per Sentence 
 
 Evaluation Date Word Class Total From 
Mindnet 
From 
Dictionary
Untranslated  
 Prepositions 410 17.1% 77.1% 5.9%  
 
Oct. 2001  
(250 sentences) Content Lemmas 2124 88.4% 7.8% 3.9%  
 Prepositions 842 61.9% 37.5% 0.6%  
 
May 2002 
(520 sentences) Content Lemmas 4429 95.9% 1.5% 2.6%  
Table 3.  Sources of Different Word Classes at Transfer 
 
while the comparison system was not 
specifically tuned to this corpus. Accordingly 
the results of the evaluation need to be 
interpreted narrowly, as demonstrating that:  
l  A viable example-based English-Japanese 
MT system can be developed that applies 
general-purpose alignment rules to semantic 
representations; and  
l  Given general-purpose grammars, a 
representation of what the sentence means, 
and suitable learning techniques, it is 
possible to achieve in a domain, results 
analogous with those of a mature 
commercial product, and within a relatively 
short time frame. 
4 Discussion 
It is illustrative to consider some of the factors 
that contributed to these results. Table 2 shows 
the number of transfers per sentence and the 
number of LF nodes per transfer in versions of 
the system evaluated in October 2001 and May 
2002. Not only is the MSR-MT finding more 
LF segments in the Mindnet, crucially the 
number of nodes transferred has also grown. 
An average of two connected nodes are now 
transferred with each LF segment, indicating 
that the system is increasingly learning its 
translation knowledge in terms of complex 
structures rather than simple lexical 
correspondences. 
 
It has been our experience that the greater 
MSR-MT?s reliance on the Mindnet, the better 
the quality of its output. Table 2 shows the 
sources of selected word classes in the two 
systems. Over time, reliance on the Mindnet 
has increased overall, while reliance on 
dictionary lookup has now diminished to the 
point where, in the case of content words, it 
should be possible to discard the handcrafted 
dictionary altogether and draw exclusively on 
the contextualized resources of the Mindnet 
and statistically-generated lexical data. Also 
striking in Table 2 is the gain shown in 
preposition handling: a majority of English 
prepositions are now being transferred only in 
the context of LF structures found in the 
Mindnet. 
 
The important observation underlying the gains 
shown in these tables is that they have 
primarily been obtained either as the result of 
LF improvements in English or Japanese (i.e., 
from better sentence analysis or LF 
construction), or as a result of generic 
improvements to the algorithms that map 
between LF segments (notably better 
coindexation and improved learning of 
mappings involving lexical attributes). In the 
latter case, although certain modifications may 
have been driven by phenomena observed 
between Japanese and English, the heuristics 
apply across all seven languages on which our 
group is currently working. Adaptation to the 
case of Japanese-English MT usually takes the 
form of loosening rather than tightening of 
constraints.  
 
 
5 Future Work 
Ultimately it is probably desirable that the 
system?s mean absolute score should approach 
3 (Acceptable) within the training domain: this 
is a high quality bar that is not attained by 
off-the-shelf systems. Much of the work will be 
of a general nature: improving the parses and 
LF structures of source and target languages 
will bring automatic benefits to both alignment 
of structured phrases and runtime translation. 
For example, efforts are currently underway to 
redesign LF to better represent scopal 
properties of quantifiers and negation 
(Campbell & Suzuki 2002a, 2002b). 
 
Work to improve the quality of alignment and 
transfer is ongoing within our group. In 
addition to improvement of alignment itself, 
we are also exploring techniques to ensure that 
the transferred LF is consistent with known 
LFs in the target language, with the eventual 
goal of obviating the need for heuristic rules 
used in preprocessing generation. Again, these 
improvements are likely to be system-wide and 
generic, and not specific to the 
English-Japanese case. 
 
 
Conclusions 
Use of abstract semantically-motivated 
linguistic representations (Logical Form) 
permits MSR-MT to align, store, and translate 
sentence patterns reflecting widely varying 
syntactic and information structures in 
Japanese and English, and to do so within the 
framework of a general-purpose NLP 
architecture applicable to both European 
languages and Asian languages. 
 
Our experience with English-Japanese example 
based MT suggests that the problem of MT 
among Asian languages may be recast as a 
problem of implementing a general represen- 
tation of structured meaning across languages 
that neutralizes differences where possible, and 
where this is not possible, readily permits 
researchers to identify general-purpose 
techniques of bridging the disparities that are 
viable across multiple languages. 
 
Acknowledgements 
We would like to thank Bill Dolan and Rich 
Campbell for their comments on a draft of this 
paper. Our appreciation also goes to the 
members of the Butler Hill Group for their 
assistance with conducting evaluations. 
References  
Aikawa, T., M. Melero, L. Schwartz, and A. Wu. 
2001a. Multilingual sentence generation. In 
Proceedings of 8th European Workshop on 
Natural Language Generation, Toulouse, France.  
Aikawa, T., M. Melero, L. Schwartz, and A. Wu. 
2001b. Sentence generation for multilingual 
machine translation. In Proceedings of the MT 
Summit VIII, Santiago de Compostela, Spain.  
Brown, P. F.,  J. Cocke, S. A. D. Pietra, V. J. D. 
Pietra, F. Jelinek, J. D. Lafferty, R. L. Mercer, 
and P. S. Roossin. 1990. A statistical approach to 
machine translation. Computational Linguistics, 
16(2): 79-85. 
Campbell, R. and H. Suzuki. 2002a. Language- 
neutral representation of syntactic structure. In 
Proceedings of the First International Workshop 
on Scalable Natural Language Understanding 
(SCANALU 2002), Heidelberg, Germany. 
Campbell, R. and H. Suzuki. 2002b. Language- 
Neutral Syntax: An Overview. Microsoft Research 
Techreport: MSR-TR-2002-76. 
Heidorn, G. 2000. Intelligent writing assistance. In 
R. Dale, H. Moisl and H. Somers (eds.), A 
Handbook of Natural Language Processing: 
Techniques and Applications for the Processing 
of Language as Text. Marcel Dekker, New York. 
pp. 181-207.  
Imamura, K. 2001a. Application of translation 
knowledge acquired by ierarchical phrase 
alignment. In Proceedings of TMI.  
Imamura, K. 2001b. Hierarchical phrase alignment 
harmonized with parsing. In Proceedings of 
NLPRS, Tokyo, Japan, pp 377-384.  
Kacmarcik, G., C. Brockett, and H. Suzuki. 2000. 
Robust segmentation of Japanese text into a 
lattice for parsing. In Proceedings of COLING 
2000, Saarbrueken, Germany, pp. 390-396. 
Menezes, A. and S. D. Richardson. 2001. A 
best-first alignment algorithm for automatic 
extraction of transfer mappings from bilingual 
corpora. In Proceedings of the Workshop on 
Data-driven Machine Translation at 39th Annual 
Meeting of the Association for Computational 
Linguistics, Toulouse, France, pp. 39-46. 
Moore, R. C. 2001. Towards a simple and accurate 
statistical approach to learning translation 
relationships among words," in Proceedings, 
Workshop on Data-driven Machine Translation, 
39th Annual Meeting and 10th Conference of the 
European Chapter, Association for 
Computational Linguistics, Toulouse, France, pp. 
79-86. 
Nagao, M. 1984. A framework of a mechanical 
translation between Japanese and English by 
analogy principle. In A. Elithorn. and R. Bannerji 
(eds.) Artificial and Human Intelligence.  Nato 
Publications. pp. 181-207.   
Pinkham, J., M. Corston-Oliver, M. Smets and M. 
Pettenaro. 2001. Rapid Assembly of a Large-scale 
French-English MT system. In Proceedings of the 
MT Summit VIII, Santiago de Compostela, Spain. 
Pinkham, J., and M. Smets. 2002. Machine 
translation without a bilingual dictionary. In 
Proceedings of the 9th International Conference 
on Theoretical and Methodological Issues in 
Machine Translation. Kyoto, Japan, pp. 146-156. 
Richardson, S. D., W. B. Dolan, A. Menezes, and M. 
Corston-Oliver. 2001. Overcoming the 
customization bottleneck using example-based 
MT. In Proceedings, Workshop on Data-driven 
Machine Translation, 39th Annual Meeting and 
10th Conference of the European Chapter, 
Association for Computational Linguistics. 
Toulouse, France, pp. 9-16. 
Richardson, S. D., W. B. Dolan, A. Menezes, and J. 
Pinkham. 2001. Achieving commercial-quality 
translation with example-based methods. In 
Proceedings of MT Summit VIII, Santiago De 
Compostela, Spain, pp. 293-298.  
Richardson, S. D., W. B. Dolan, and L. 
Vanderwende. 1998 MindNet: Acquiring and 
structuring semantic information from text, 
ACL-98. pp. 1098-1102. 
Sato, S. and Nagao M. 1990. Toward 
memory-based translation. In Proceedings of 
COLING 1990, Helsinki, Finland, pp. 247-252. 
Suzuki, H., C. Brockett, and G. Kacmarcik. 2000. 
Using a broad-coverage parser for word-breaking 
in Japanese. In Proceedings of COLING 2000, 
Saarbrueken, Germany, pp. 822-827.  
Yamamoto K., and Y Matsumoto. 2000. 
Acquisition of phrase-level bilingual 
correspondence using dependency structure. In 
Proceedings of COLING 2000, Saarbrueken, 
Germany, pp. 933-939.  
Monolingual Machine Translation for Paraphrase Generation 
Chris QUIRK,  Chris BROCKETT  and  William DOLAN 
Natural Language Processing Group 
Microsoft Research 
One Microsoft Way 
Redmond, WA  90852  USA 
{chrisq,chrisbkt,billdol}@microsoft.com 
 
 
Abstract 
We apply statistical machine translation 
(SMT) tools to generate novel paraphrases 
of input sentences in the same language. 
The system is trained on large volumes of 
sentence pairs automatically extracted from 
clustered news articles available on the 
World Wide Web. Alignment Error Rate 
(AER) is measured to gauge the quality of 
the resulting corpus. A monotone phrasal 
decoder generates contextual replacements. 
Human evaluation shows that this system 
outperforms baseline paraphrase generation 
techniques and, in a departure from previ-
ous work, offers better coverage and scal-
ability than the current best-of-breed 
paraphrasing approaches. 
1 Introduction 
The ability to categorize distinct word sequences 
as ?meaning the same thing? is vital to applications 
as diverse as search, summarization, dialog, and 
question answering. Recent research has treated 
paraphrase acquisition and generation as a machine 
learning problem (Barzilay & McKeown, 2001; 
Lin & Pantel, 2002; Shinyama et al 2002, Barzilay 
& Lee, 2003, Pang et al, 2003). We approach this 
problem as one of statistical machine translation 
(SMT), within the noisy channel model of Brown 
et al (1993). That is, we seek to identify the opti-
mal paraphrase T* of a sentence S by finding: 
 
( ){ }
{ })P()|P(maxarg
|Pmaxarg*
TTS
STT
T
T
=
=
 
 
T and S being sentences in the same language.  
We describe and evaluate an SMT-based para-
phrase generation system that utilizes a monotone 
phrasal decoder to generate meaning-preserving 
paraphrases across multiple domains. By adopting 
at the outset a paradigm geared toward generating 
sentences, this approach overcomes many prob-
lems encountered by task-specific approaches. In 
particular, we show that SMT techniques can be 
extended to paraphrase given sufficient monolin-
gual parallel data.1 We show that a huge corpus of 
comparable and alignable sentence pairs can be 
culled from ready-made topical/temporal clusters 
of news articles gathered on a daily basis from 
thousands of sources on the World Wide Web, 
thereby permitting the system to operate outside 
the narrow domains typical of existing systems. 
2 Related work 
Until recently, efforts in paraphrase were not 
strongly focused on generation and relied primarily 
on narrow data sources. One data source has been 
multiple translations of classic literary works (Bar-
zilay & McKeown 2001; Ibrahim 2002; Ibrahim et 
al. 2003). Pang et al (2003) obtain parallel mono-
lingual texts from a set of 100 multiply-translated 
news articles. While translation-based approaches 
to obtaining data do address the problem of how to 
identify two strings as meaning the same thing, 
they are limited in scalability owing to the diffi-
culty (and expense) of obtaining large quantities of 
multiply-translated source documents.  
Other researchers have sought to identify pat-
terns in large unannotated monolingual corpora. 
Lin & Pantel (2002) derive inference rules by pars-
ing text fragments and extracting semantically 
similar paths. Shinyama et al (2002) identify de-
pendency paths in two collections of newspaper 
articles. In each case, however, the information 
extracted is limited to a small set of patterns.  
Barzilay & Lee (2003) exploit the meta-
information implicit in dual collections of news-
                                                          
1
 Barzilay & McKeown (2001), for example, reject the 
idea owing to the noisy, comparable nature of their data. 
wire articles, but focus on learning sentence-level 
patterns that provide a basis for generation. Multi-
sequence alignment (MSA) is used to identify sen-
tences that share formal (and presumably semantic) 
properties. This yields a set of clusters, each char-
acterized by a word lattice that captures n-gram-
based structural similarities between sentences. 
Lattices are in turn mapped to templates that can 
be used to produce novel transforms of input sen-
tences. Their methodology provides striking results 
within a limited domain characterized by a high 
frequency of stereotypical sentence types. How-
ever, as we show below, the approach may be of 
limited generality, even within the training domain.   
3 Data collection 
Our training corpus, like those of Shinyama et 
al. and Barzilay & Lee, consists of different news 
stories reporting the same event. While previous 
work with comparable news corpora has been lim-
ited to just two news sources, we set out to harness 
the ongoing explosion in internet news coverage. 
Thousands of news sources worldwide are compet-
ing to cover the same stories, in real time. Despite 
different authorship, these stories cover the same 
events and therefore have significant content over-
lap, especially in reports of the basic facts. In other 
cases, news agencies introduce minor edits into a 
single original AP or Reuters story. We believe 
that our work constitutes the first to attempt to ex-
ploit these massively multiple data sources for 
paraphrase learning and generation.  
3.1 Gathering aligned sentence pairs 
We began by identifying sets of pre-clustered 
URLs that point to news articles on the Web, gath-
ered from publicly available sites such as 
http://news.yahoo.com/, http://news.google.com 
and http://uk.newsbot.msn.com. Their clustering 
algorithms appear to consider the full text of each 
news article, in addition to temporal cues, to pro-
duce sets of topically/temporally related articles. 
Story content is captured by downloading the 
HTML and isolating the textual content. A super-
vised HMM was trained to distinguish story con-
tent from surrounding advertisements, etc.2 
Over the course of about 8 months, we collected 
11,162 clusters, comprising 177,095 articles and 
averaging 15.8 articles per cluster. The quality of 
                                                          
2
 We hand-tagged 1,150 articles to indicate which por-
tions of the text were story content and which were ad-
vertisements, image captions, or other unwanted 
material.  We evaluated several classifiers on a 70/30 
test train split and found that an HMM trained on a 
handful of features was most effective in identifying 
content lines (95% F-measure). 
these clusters is generally good. Impressionisti-
cally, discrete events like sudden disasters, busi-
ness announcements, and deaths tend to yield 
tightly focused clusters, while ongoing stories like 
the SARS crisis tend to produce very large and 
unfocused clusters. 
To extract likely paraphrase sentence pairs from 
these clusters, we used edit distance (Levenshtein 
1966) over words, comparing all sentences pair-
wise within a cluster to find the minimal number of 
word insertions and deletions transforming the first 
sentence into the second. Each sentence was nor-
malized to lower case, and the pairs were filtered 
to reject:  
 
? Sentence pairs where the sentences were 
identical or differed only in punctuation;  
? Duplicate sentence pairs;  
? Sentence pairs with significantly different 
lengths (the shorter is less than two-thirds 
the length of the longer);  
? Sentence pairs where the Levenshtein dis-
tance was greater than 12.0.3  
 
A total of 139K non-identical sentence pairs were 
obtained. Mean Levenshtein distance was 5.17; 
mean sentence length was 18.6 words. 
3.2 Word alignment 
To this corpus we applied the word alignment 
algorithms available in Giza++ (Och & Ney, 
2000), a freely available implementation of IBM 
Models 1-5 (Brown, 1993) and the HMM align-
ment (Vogel et al 1996), along with various im-
provements and modifications motivated by 
experimentation by Och & Ney (2000). In order to 
capture the many-to-many alignments that identify 
correspondences between idioms and other phrasal 
chunks, we align in the forward direction and again 
in the backward direction, heuristically recombin-
ing each unidirectional word alignment into a sin-
gle bidirectional alignment (Och & Ney 2000). 
Figure 1 shows an example of a monolingual 
alignment produced by Giza++. Each line repre-
sents a uni-directional link; directionality is indi-
cated by a tick mark on the target side of the link. 
We held out a set of news clusters from our 
training data and extracted a set of 250 sentence 
pairs for blind evaluation. Randomly extracted on 
the basis of an edit distance of 5 ? n ? 20 (to allow 
a range of reasonably divergent candidate pairs 
while eliminating the most trivial substitutions), 
the gold-standard sentence pairs were checked by 
an independent human evaluator to ensure that 
                                                          
3
 Chosen on the basis of ablation experiments and opti-
mal AER (discussed in 3.2).  
they contained paraphrases before they were hand 
word-aligned. 
To evaluate the alignments, we adhered to the 
standards established in Melamed (2001) and Och 
& Ney (2000, 2003). Following Och & Ney?s 
methodology, two annotators each created an ini-
tial annotation for each dataset, subcategorizing 
alignments as either SURE (necessary) or POSSIBLE 
(allowed, but not required). Differences were high-
lighted and the annotators were asked to review 
their choices on these differences. Finally we com-
bined the two annotations into a single gold stan-
dard: if both annotators agreed that an alignment 
should be SURE, then the alignment was marked as 
SURE in the gold-standard; otherwise the alignment 
was marked as POSSIBLE. 
To compute Precision, Recall, and Alignment 
Error Rate (AER) for the twin datasets, we used 
exactly the formulae listed in Och & Ney (2003). 
Let A be the set of alignments in the comparison, S 
be the set of SURE alignments in the gold standard, 
and P be the union of the SURE and POSSIBLE 
alignments in the gold standard. Then we have:  
 
||
||precision
A
PA ?
=  ||
||
  recall
S
SA ?
=
 
 
||
||AER
SA
SAPA
+
?+?
=  
 
Measured in terms of AER4, final interrater agree-
ment between the two annotators on the 250 sen-
tences was 93.1%. 
                                                          
4
 The formula for AER given here and in Och  & Ney 
(2003) is intended to compare an automatic alignment 
against a gold standard alignment. However, when com-
paring one human against another, both comparison and 
reference distinguish between SURE and POSSIBLE links. 
Because the AER is asymmetric (though each direction 
Table 1 shows the results of evaluating align-
ment after trainng the Giza++ model. Although the 
overall AER of 11.58% is higher than the best bi-
lingual MT systems (Och & Ney, 2003), the train-
ing data is inherently noisy, having more in 
common with analogous corpora than conventional 
MT parallel corpora in that the paraphrases are not 
constrained by the source text structure. The iden-
tical word AER of 10.57% is unsurprising given 
that the domain is unrestricted and the alignment 
algorithm does not employ direct string matching 
to leverage word identity.5 The non-identical word 
AER of 20.88% may appear problematic in a sys-
tem that aims to generate paraphrases; as we shall 
see, however, this turns out not to be the case. Ab-
lation experiments, not described here, indicate 
that additional data will improve AER.  
3.3 Identifying phrasal replacements 
Recent work in SMT has shown that simple 
phrase-based MT systems can outperform more 
sophisticated word-based systems (e.g. Koehn et 
al. 2003). Therefore, we adopt a phrasal decoder 
patterned closely after that of Vogel et al (2003). 
We view the source and target sentences S and T 
as word sequences s1..sm and t1..tn. A word align-
ment A of S and T can be expressed as a function 
from each of the source and target tokens to a 
unique cept (Brown et al 1993); isomorphically, a 
cept represents an aligned subset of the source and 
target tokens. Then, for a given sentence pair and 
word alignment, we define a phrase pair as a sub-
set of the cepts in which both the source and target 
tokens are contiguous. 6  We gathered all phrase 
                                                                                           
differs by less than 5%), we have presented the average 
of the directional AERs. 
5
 However, following SMT practice of augmenting data 
with a bilingual lexicon, we did append an identity lexi-
con to the training data. 
6
 While this does preclude the usage of ?gapped? phrase 
pairs such as or ? either ? or, we found such map-
Training Data Type: L12 
Precision   87.46% 
Recall      89.52% 
AER         11.58% 
Identical word precision   89.36% 
Identical word recall      89.50% 
Identical word AER         10.57% 
Non-identical word preci-
sion   76.99% 
Non-identical word recall      90.22% 
Non-identical word AER     20.88% 
 
Table 1. AER on the Lev12 corpus 
 
Figure 1. An example Giza++ alignment 
pairs (limited to those containing no more than five 
cepts, for reasons of computational efficiency) oc-
curring in at least one aligned sentence somewhere 
in our training corpus into a single replacement 
database. This database of lexicalized phrase pairs, 
termed phrasal replacements, serves as the back-
bone of our channel model. 
As in (Vogel et al 2003), we assigned probabili-
ties to these phrasal replacements via IBM Model 
1. In more detail, we first gathered lexical transla-
tion probabilities of the form P(s | t) by running 
five iterations of Model 1 on the training corpus. 
This allows for computing the probability of a se-
quence of source words S given a sequence of tar-
get words T as the sum over all possible 
alignments of the Model 1 probabilities: 
 
( ) ( )
( )??
?
? ?
=
=
Tt Ss
A
tsP
TASPTSP
|
|,|
 
 
(Brown et al (1993) provides a more detailed deri-
vation of this identity.) Although simple, this ap-
proach has proven effective in SMT for several 
reasons. First and foremost, phrasal scoring by 
Model 1 avoids the sparsity problems associated 
with estimating each phrasal replacement probabil-
ity with MLE (Vogel et al 2003). Secondly, it ap-
pears to boost translation quality in more 
sophisticated translation systems by inducing lexi-
cal triggering (Och et al 2004). Collocations and 
other non-compositional phrases receive a higher 
probability as a whole than they would as inde-
pendent single word replacements. 
One further simplification was made. Given that 
our domain is restricted to the generation of mono-
lingual paraphrase, interesting output can be pro-
duced without tackling the difficult problem of 
inter-phrase reordering.7 Therefore, along the lines 
of Tillmann et al (1997), we rely on only mono-
tone phrasal alignments, although we do allow in-
tra-phrasal reordering. While this means certain 
common structural alternations (e.g., ac-
tive/passive) cannot be generated, we are still able 
to express a broad range of phenomena: 
 
                                                                                           
pings to be both unwieldy in practice and very often 
indicative of poor a word alignment. 
7
 Even in the realm of MT, such an assumption can pro-
duce competitive results (Vogel et al 2003).  In addi-
tion, we were hesitant to incur the exponential increase 
in running time associated with those movement models 
in the tradition of Brown el al (1993), especially since 
these offset models fail to capture important linguistic 
generalizations (e.g., phrasal coherence, headedness). 
? Synonymy: injured ? wounded 
? Phrasal replacements: Bush administration 
? White House 
? Intra-phrasal reorderings: margin of error 
? error margin 
 
Our channel model, then, is determined solely 
by the phrasal replacements involved. We first as-
sume a monotone decomposition of the sentence 
pair into phrase pairs (considering all phrasal de-
compositions equally likely), and the probability 
P(S | T) is then defined as the product of the each 
phrasal replacement probability. 
The target language model was a trigram model 
using interpolated Kneser-Ney smoothing (Kneser 
& Ney 1995), trained over all 1.4 million sentences 
(24 million words) in our news corpus. 
3.4 Generating paraphrases 
To generate paraphrases of a given input, a stan-
dard SMT decoding approach was used; this is de-
scribed in more detail below. Prior to decoding, 
however, the input sentence underwent preprocess-
ing: text was lowercased, tokenized, and a few 
classes of named-entities were identified using 
regular expressions. 
To begin the decoding process, we first con-
structed a lattice of all possible paraphrases of the 
source sentence based on our phrasal translation 
database. Figure 2 presents an example. The lattice 
was realized as a set of |S| + 1 vertices v0..v|S| and a 
set of edges between those vertices; each edge was 
labeled with a sequence of words and a real num-
ber. Thus a edge connecting vertex vi to vj labeled 
with the sequence of words w1..wk and the real 
number p indicates that the source words si+1 to sj 
can be replaced by words w1..wk with probability p. 
Our replacement database was stored as a trie with 
words as edges, hence populating the lattice takes 
worst case O(n2) time. Finally, since source and 
target languages are identical, we added an identity 
mapping for each source word si: an edge from vi-1 
to vi with label si and a uniform probability u. This 
allows for handling unseen words. A high u value 
permits more conservative paraphrases. 
We found the optimal path through the lattice as 
scored by the product of the replacement model 
and the trigram language model. This algorithm 
reduces easily to the Viterbi algorithm; such a dy-
namic programming approach guarantees an effi-
cient optimal search (worst case O(kn), where n is 
the maximal target length and k is the maximal 
number of replacements for any word). In addition, 
fast algorithms exist for computing the n-best lists 
over a lattice (Soong & Huang 1991). 
Finally the resultant paraphrases were cleaned 
up in a post-processing phase to ensure output was 
not trivially distinguishable from other systems 
during human evaluation. All generic named entity 
tokens were re-instantiated with their source val-
ues, and case was restored using a model like that 
used in Vita et al (2003). 
3.5 Alternate approaches 
Barzilay &  Lee (2003) have released a common 
dataset that provides a basis for comparing differ-
ent paraphrase generation systems. It consists of 59 
sentences regarding acts of violence in the Middle 
East. These are accompanied by paraphrases gen-
erated by their Multi-Sequence Alignment (MSA) 
system and a baseline employing WordNet (Fell-
baum 1998), along with human judgments for each 
output by 2-3 raters. 
The MSA WordNet baseline was created by se-
lecting a subset of the words in each test sen-
tence?proportional to the number of words 
replaced by MSA in the same sentence?and re-
placing each with an arbitrary word from its most 
frequent WordNet synset. 
Since our SMT approach depends quite heavily 
on a target language model, we presented an alter-
nate WordNet baseline using a target language 
model.8 In combination with the language model 
described in section 3.4, we used a very simple 
replacement model: each appropriately inflected 
member of the most frequent synset was proposed 
as a possible replacement with uniform probability. 
This was intended to isolate the contribution of the 
language model from the replacement model. 
Given that our alignments, while aggregated into 
phrases, are fundamentally word-aligned, one 
question that arises is whether the information we 
learn is different in character than that learned 
                                                          
8
 In contrast, Barzilay and Lee (2003) avoided using a 
language model for essentially the same reason: their 
MSA approach did not take advantage of such a re-
source. 
from much simpler techniques. To explore this 
hypothesis, we introduced an additional baseline 
that used statistical clustering to produce an auto-
mated, unsupervised synonym list, again with a 
trigram language model. We used standard bigram 
clustering techniques (Goodman 2002) to produce 
4,096 clusters of our 65,225 vocabulary items. 
4 Evaluation 
We have experimented with several methods for 
extracting a parallel sentence-aligned corpus from 
news clusters using word alignment error rate, or 
AER, (Och & Ney 2003) as an evaluation metric. 
A brief summary of these experiments is provided 
in Table 1. To evaluate the quality of generation, 
we followed the lead of Barzilay & Lee (2003). 
We started with the 59 sentences and correspond-
ing paraphrases from MSA and WordNet (desig-
nated as WN below). Since the size of this data set 
made it difficult to obtain statistically significant 
results, we also included 141 randomly selected 
sentences from held-out clusters. We then pro-
duced paraphrases with each of the following sys-
tems and compared them with MSA and WN: 
 
? WN+LM: WordNet with a trigram LM 
? CL: Statistical clusters with a trigram LM 
? PR: The top 5 sentence rewrites produced by 
Phrasal Replacement. 
 
For the sake of consistency, we did not use the 
judgments provided by Barzilay and Lee; instead 
we had two raters judge whether the output from 
each system was a paraphrase of the input sen-
tence. The raters were presented with an input sen-
tence and an output paraphrase from each system 
in random order to prevent bias toward any par-
ticular judgment. Since, on our first pass, we found 
inter-rater agreement to be somewhat low (84%), 
we asked the raters to make a second pass of judg-
ments on those where they disagreed; this signifi-
cantly improved agreement (96.9%). The results of 
this final evaluation are summarized in Table 2. 
 
Figure 2. A simplified generation lattice: 44 top ranked edges from a total 4,140 
5 Analysis 
Table 2 shows that PR can produce rewordings 
that are evaluated as plausible paraphrases more 
frequently than those generated by either baseline 
techniques or MSA. The WordNet baseline per-
forms quite poorly, even in combination with a 
trigram language model: the language model does 
not contribute significantly to resolving lexical 
selection. The performance of CL is likewise 
abysmal?again a language model does nothing to 
help. The poor performance of these synonym-
based techniques indicates that they have little 
value except as a baseline. 
The PR model generates plausible paraphrases 
for the overwhelming majority of test sentences, 
indicating that even the relatively high AER for 
non-identical words is not an obstacle to successful 
generation. Moreover, PR was able to generate a 
paraphrase for all 200 sentences (including the 59 
MSA examples). The correlation between accept-
ability and PR sentence rank validates both the 
ranking algorithm and the evaluation methodology.  
In Table 2, the PR model scores significantly 
better than MSA in terms of the percentage of 
paraphrase candidates accepted by raters. More-
over, PR generates at least five (and often hun-
dreds more) distinct paraphrases for each test 
sentence. Such perfect coverage on this dataset is 
perhaps fortuitous, but is nonetheless indicative of 
scalability. By contrast Barzilay & Lee (2003) re-
port being able to generate paraphrases for only 59 
out of 484 sentences in their training (test?) set, a 
total of 12%.  
One potential concern is that PR paraphrases 
usually involve simple substitutions of words and 
short phrases (a mean edit distance of 2.9 on the 
top ranked sentences), whereas MSA outputs more 
complex paraphrases (reflected in a mean edit dis-
tance of 25.8). This is reflected in Table 3, which 
provides a breakdown of four dimensions of inter-
est, as provided by one of our independent evalua-
tors. Some 47% of MSA paraphrases involve 
significant reordering, such as an active-passive 
alternation, whereas the monotone PR decoder 
precludes anything other than minor transpositions 
within phrasal replacements. 
Should these facts be interpreted to mean that 
MSA, with its more dramatic rewrites, is ulti-
mately more ambitious than PR? We believe that 
the opposite is true. A close look at MSA suggests 
that it is similar in spirit to example-based machine 
translation techniques that rely on pairing entire 
sentences in source and target languages, with the 
translation step limited to local adjustments of the 
target sentence (e.g. Sumita 2001). When an input 
sentence closely matches a template, results can be 
stunning. However, MSA achieves its richness of 
substitution at the cost of generality. Inspection 
reveals that 15 of the 59 MSA paraphrases, or 
25.4%, are based on a single high-frequency, do-
main-specific template (essentially a running tally 
of deaths in the Israeli-Palestinian conflict). Unless 
one is prepared to assume that similar templates 
can be found for most sentence types, scalability 
and domain extensibility appear beyond the reach 
of MSA. 
In addition, since MSA templates pair entire 
sentences, the technique can produce semantically 
different output when there is a mismatch in in-
formation content among template training sen-
tences. Consider the third and fourth rows of Table 
3, which indicate the extent of embellishment and 
lossiness found in MSA paraphrases and the top-
ranked PR paraphrases. Particularly noteworthy is 
the lossiness of MSA seen in row 4. Figure 3 illus-
trates a case where the MSA paraphrase yields a 
significant reduction in information, while PR is 
more conservative in its replacements.  
While the substitutions obtained by the PR 
model remain for the present relatively modest, 
they are not trivial. Changing a single content word 
is a legitimate form of paraphrase, and the ability 
to paraphrase across an arbitrarily large sentence 
set and arbitrary domains is a desideratum of para-
phrase research. We have demonstrated that the 
SMT-motivated PR method is capable of generat-
ing acceptable paraphrases for the overwhelming 
majority of sentences in a broad domain.  
Method B&L59 B&L59 + 141 
PR #1 54 / 59 = 91.5% 177 / 200 = 89.5% 
PR #2 53 / 59 = 89.8% 168 / 200 = 84.0% 
PR #3 46 / 59 = 78.0% 164 / 200 = 82.0% 
PR #4 49 / 59 = 83.1% 163 / 200 = 81.5% 
MSA 46 / 59 = 78.0% 46 /   59 = 78.0% 
PR #5 44 / 59 = 74.6% 155 / 200 = 77.5% 
WN 23 / 59 = 39.0% 25 /   59 = 37.9% 
WN+LM 30 / 59 = 50.9% 53 / 200 = 27.5% 
CL  14 / 59 = 23.7% 26 / 200 = 13.0% 
 
Table 2. Human acceptability judgments 
 
MSA PR#1 
Rearrangement 28 / 59 = 47% 0 / 100 =   0% 
Phrasal alternation 11 / 59 = 19% 3 / 100 =   3% 
Info added 19 / 59 = 32% 6 / 100 =   6% 
Info lost 43 / 59 = 73% 31 / 100 = 31% 
 
Table 3. Qualitative analysis of paraphrases 
6 Future work 
Much work obviously remains to be done. Our 
results remain constrained by data sparsity, despite 
the large initial training sets. One major agenda 
item therefore will be acquisition of larger (and 
more diverse) data sets. In addition to obtaining 
greater absolute quantities of data in the form of 
clustered articles, we also seek to extract aligned 
sentence pairs that instantiate a richer set of phe-
nomena. Relying on edit distance to identify likely 
paraphrases has the unfortunate result of excluding 
interesting sentence pairs that are similar in mean-
ing though different in form. For example: 
 
The Cassini spacecraft, which is en route to Saturn, 
is about to make a close pass of the ringed 
planet's mysterious moon Phoebe 
 
On its way to an extended mission at Saturn, the 
Cassini probe on Friday makes its closest ren-
dezvous with Saturn's dark moon Phoebe. 
 
We are currently experimenting with data extracted 
from the first two sentences in each article, which 
by journalistic convention tend to summarize con-
tent (Dolan et al 2004). While noisier than the edit 
distance data, initial results suggest that these can 
be a rich source of information about larger phrasal 
substitutions and syntactic reordering.  
Although we have not attempted to address the 
issue of paraphrase identification here, we are cur-
rently exploring machine learning techniques, 
based in part on features of document structure and 
other linguistic features that should allow us to 
bootstrap initial alignments to develop more data. 
This will we hope, eventually allow us to address 
such issues as paraphrase identification for IR.  
To exploit richer data sets, we will also seek to 
address the monotone limitation of our decoder 
that further limits the complexity of our paraphrase 
output. We will be experimenting with more so-
phisticated decoder models designed to handle re-
ordering and mappings to discontinuous elements. 
We also plan to pursue better (automated) metrics 
for paraphrase evaluation.  
7 Conclusions 
We presented a novel approach to the problem 
of generating sentence-level paraphrases in a broad 
semantic domain. We accomplished this by using 
methods from the field of SMT, which is oriented 
toward learning and generating exactly the sorts of 
alternations encountered in monolingual para-
phrase. We showed that this approach can be used 
to generate paraphrases that are preferred by hu-
mans to sentence-level paraphrases produced by 
other techniques. While the alternations our system 
produces are currently limited in character, the 
field of SMT offers a host of possible enhance-
ments?including reordering models?affording a 
natural path for future improvements.  
A second important contribution of this work is 
a method for building and tracking the quality of 
large, alignable monolingual corpora from struc-
tured news data on the Web. In the past, the lack of 
such a data source has hampered paraphrase re-
search; our approach removes this obstacle. 
Acknowledgements 
We are grateful to Mo Corston-Oliver, Jeff Ste-
venson, Amy Muia, and Orin Hargraves of the 
Butler Hill Group for their work in annotating the 
data used in the experiments. This paper has also 
benefited from discussions with Ken Church, Mark 
Johnson, and Steve Richardson. We greatly appre-
ciate the careful comments of three anonymous 
reviewers. We remain, however, solely responsible 
for this content. 
References 
R. Barzilay and K. R. McKeown. 2001. Extracting Para-
phrases from a parallel corpus. In Proceedings of the 
ACL/EACL. 
R. Barzilay and L. Lee. 2003. Learning to Paraphrase; 
an unsupervised approach using multiple-sequence 
alignment. In Proceedings of HLT/NAACL. 
P. Brown, S. A. Della Pietra, V. J. Della Pietra and R. L. 
Mercer. 1993. The Mathematics of Statistical Ma-
chine Translation. Computational Linguistics 19(2): 
263-311. 
W. Dolan, C. Quirk and C. Brockett. 2004. Unsuper-
vised Construction of Large Paraphrase Corpora: Ex-
ploiting Massively Parallel News Sources.  To appear 
in Proceedings of COLING-2004. 
C. Fellbaum, ed. 1998. WordNet: An Electronic Lexical 
Database. MIT Press, Cambridge, MA. 
J. Goodman. 2002. JCLUSTER. Software available at 
http://research.microsoft.com/~joshuago/ 
A. Ibrahim. 2002. Extracting Paraphrases from Aligned 
Corpora. Master of Engineering Thesis, MIT. 
A. Ibrahim, B. Katz, and J. Lin. 2003. Extracting Struc-
tural Paraphrases from Aligned Monolingual Cor-
pora. In Proceedings of the Second International 
Workshop on Paraphrasing (IWP 2003). Sapporo, 
Japan. 
R. Kneser and H. Ney. 1995. Improved backing-off for 
N-gram language modeling. In Proc. Int. Conf. on 
Acoustics, Speech and Signal Processing: 181-184. 
Detroit, MI. 
P. Koehn, F. Och, and D. Marcu. 2003. Statistical 
Phrase-Based Translation. In Proceedings of 
HLT/NAACL. 
V. Levenshtein. 1966. Binary codes capable of correct-
ing deletions, insertions, and reversals. Soviet 
Physice-Doklady 10: 707-710. 
D. Lin and P. Pantel. 2001. DIRT - Discovery of Infer-
ence Rules from Text. In Proceedings of ACM 
SIGKDD Conference on Knowledge Discovery and 
Data Mining: 323-328. 
I. D. Melamed. 2001. Empirical Methods for Exploiting 
Parallel Texts. The MIT Press. 
R. Mihalcea and T. Pedersen. 2003. An Evaluation Ex-
ercise for Word Alignment. In HLT/NAACL Work-
shop: Building and Using Parallel Texts: 1-10. 
F. Och and H. Ney. 2000. Improved Statistical Align-
ment Models. In Proceedings of the ACL: 440-447. 
Hong Kong, China. 
F. Och and H. Ney. 2003. A Systematic Comparison of 
Various Statistical Alignment Models. Computa-
tional Linguistics 29(1): 19-52. 
B. Pang, K. Knight, and D. Marcu. 2003. Syntax-based 
Alignment of Multiple Translations: Extracting Para-
phrases and Generating New Sentences. Proceedings 
of HLT/NAACL.  
Y. Shinyama, S. Sekine and K. Sudo. 2002. Automatic 
Paraphrase Acquisition from News Articles. In Pro-
ceedings of NAACL-HLT. 
F. K. Soong and E. F. Huang. 1991. A tree-trellis based 
fast search for finding the n-best sentence hypotheses 
in continuous speech recognition. In Proceedings of 
the IEEE International Conference on Acoustics, 
Speech and Signal Processing 1: 705-708. Toronto, 
Canada. 
E. Sumita. 2001. Example-based machine translation 
using DP-matching between work sequences. In Pro-
ceedings of the ACL 2001 Workshop on Data-Driven 
Methods in Machine Translation: 1?8. 
C. Tillmann, S. Vogel, H. Ney, and A. Zubaiga. 1997. A 
DP Based Search Using Monotone Alignments in 
Statistical Translation. In Proceedings of the ACL. 
L. Vita, A. Ittycheriah, S. Roukos, and N. Kambhatla. 
2003. tRuEcasing. In Proceedings of the ACL: 152-
159. Sapporo, Japan. 
S. Vogel, H. Ney and C. Tillmann. 1996. HMM-Based 
Word Alignment in Statistical Translation. In Pro-
ceedings of the ACL: 836-841. Copenhagen Den-
mark. 
S. Vogel, Y. Zhang, F. Huang, A. Venugopal, B. Zhao, 
A. Tribble, M. Eck, and A. Waibel. 2003. The CMU 
Statistical Machine Translation System. In Proceed-
ings of MT Summit IX, New Orleans, Louisiana, 
USA. 
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 62?69,
Sydney, July 2006. c?2006 Association for Computational Linguistics
The impact of parse quality on syntactically-informed statistical machine
translation
Chris Quirk and Simon Corston-Oliver
Microsoft Research
One Microsoft Way
Redmond, WA 98052 USA
{chrisq,simonco}@microsoft.com
Abstract
We investigate the impact of parse quality
on a syntactically-informed statistical ma-
chine translation system applied to techni-
cal text. We vary parse quality by vary-
ing the amount of data used to train the
parser. As the amount of data increases,
parse quality improves, leading to im-
provements in machine translation output
and results that significantly outperform a
state-of-the-art phrasal baseline.
1 Introduction
The current study is a response to a question
that proponents of syntactically-informed machine
translation frequently encounter: How sensitive is
a syntactically-informed machine translation sys-
tem to the quality of the input syntactic analysis?
It has been shown that phrasal machine translation
systems are not affected by the quality of the in-
put word alignments (Koehn et al, 2003). This
finding has generally been cast in favorable terms:
such systems are robust to poor quality word align-
ment. A less favorable interpretation of these re-
sults might be to conclude that phrasal statistical
machine translation (SMT) systems do not stand
to benefit from improvements in word alignment.
In a similar vein, one might ask whether con-
temporary syntactically-informed machine trans-
lation systems would benefit from improvements
in parse accuracy. One possibility is that cur-
rent syntactically-informed SMT systems are de-
riving only limited value from the syntactic anal-
yses, and would therefore not benefit from im-
proved analyses. Another possibility is that syn-
tactic analysis does indeed contain valuable infor-
mation that could be exploited by machine learn-
ing techniques, but that current parsers are not of
sufficient quality to be of use in SMT.
With these questions and concerns, let us be-
gin. Following some background discussion we
describe a set of experiments intended to elucidate
the impact of parse quality on SMT.
2 Background
We trained statistical machine translation systems
on technical text. In the following sections we
provide background on the data used for training,
the dependency parsing framework used to pro-
duce treelets, the treelet translation framework and
salient characteristics of the target languages.
2.1 Dependency parsing
Dependency analysis is an alternative to con-
stituency analysis (Tesnie`re, 1959; Melc?uk, 1988).
In a dependency analysis of syntax, words di-
rectly modify other words, with no intervening
non-lexical nodes. We use the terms child node
and parent node to denote the tokens in a depen-
dency relation. Each child has a single parent, with
the lexical root of the sentence dependent on a syn-
thetic ROOT node.
We use the parsing approach described in
(Corston-Oliver et al, 2006). The parser is trained
on dependencies extracted from the English Penn
Treebank version 3.0 (Marcus et al, 1993) by
using the head-percolation rules of (Yamada and
Matsumoto, 2003).
Given a sentence x, the goal of the parser is to
find the highest-scoring parse y? among all possible
parses y ? Y :
y? = argmax
y?Y
s(x,y) (1)
The score of a given parse y is the sum of the
62
scores of all its dependency links (i, j) ? y:
s(x,y) = ?
(i, j)?y
d(i, j) = ?
(i, j)?y
w ? f(i, j) (2)
where the link (i, j) indicates a parent-child de-
pendency between the token at position i and the
token at position j. The score d(i, j) of each de-
pendency link (i, j) is further decomposed as the
weighted sum of its features f(i, j).
The feature vector f(i, j) computed for each
possible parent-child dependency includes the
part-of-speech (POS), lexeme and stem of the par-
ent and child tokens, the POS of tokens adjacent
to the child and parent, and the POS of each to-
ken that intervenes between the parent and child.
Various combinations of these features are used,
for example a new feature is created that combines
the POS of the parent, lexeme of the parent, POS
of the child and lexeme of the child. Each feature
is also conjoined with the direction and distance
of the parent, e.g. does the child precede or follow
the parent, and how many tokens intervene?
To set the weight vector w, we train twenty
averaged perceptrons (Collins, 2002) on different
shuffles of data drawn from sections 02?21 of the
Penn Treebank. The averaged perceptrons are then
combined to form a Bayes Point Machine (Her-
brich et al, 2001; Harrington et al, 2003), result-
ing in a linear classifier that is competitive with
wide margin techniques.
To find the optimal parse given the weight vec-
tor w and feature vector f(i, j) we use the decoder
described in (Eisner, 1996).
2.2 Treelet translation
For syntactically-informed translation, we fol-
low the treelet translation approach described
in (Quirk et al, 2005). In this approach, trans-
lation is guided by treelet translation pairs. Here,
a treelet is a connected subgraph of a dependency
tree. A treelet translation pair consists of a source
treelet S, a target treelet T , and a word alignment
A ? S? T such that for all s ? S, there exists a
unique t ? T such that (s, t)? A, and if t is the root
of T , there is a unique s ? S such that (s, t) ? A.
Translation of a sentence begins by parsing
that sentence into a dependency representation.
This dependency graph is partitioned into treelets;
like (Koehn et al, 2003), we assume a uniform
probability distribution over all partitions. Each
source treelet is matched to a treelet translation
pair; together, the target language treelets in those
treelet translation pairs will form the target trans-
lation. Next the target language treelets are joined
to form a single tree: the parent of the root of each
treelet is dictated by the source. Let tr be the root
of the target language treelet, and sr be the source
node aligned to it. If sr is the root of the source
sentence, then tr is made the root of the target lan-
guage tree. Otherwise let sp be the parent of sr,
and tp be the target node aligned to sp: tr is at-
tached to tp. Finally the ordering of all the nodes
is determined, and the target tree is specified, and
the target sentence is produced by reading off the
labels of the nodes in order.
Translations are scored according to a log-linear
combination of feature functions, each scoring dif-
ferent aspects of the translation process. We use a
beam search decoder to find the best translation T ?
according to the log-linear combination of models:
T ? = argmax
T
{
?
f?F
? f f (S,T,A)
}
(3)
The models include inverted and direct channel
models estimated by relative frequency, lexical
weighting channel models following (Vogel et al,
2003), a trigram target language model using mod-
ified Kneser-Ney smoothing (Goodman, 2001),
an order model following (Quirk et al, 2005),
and word count and phrase count functions. The
weights for these models are determined using the
method described in (Och, 2003).
To estimate the models and extract the treelets,
we begin from a parallel corpus. First the cor-
pus is word-aligned using GIZA++ (Och and Ney,
2000), then the source sentence are parsed, and
finally dependencies are projected onto the target
side following the heuristics described in (Quirk et
al., 2005). This word aligned parallel dependency
tree corpus provides training material for an order
model and a target language tree-based language
model. We also extract treelet translation pairs
from this parallel corpus. To limit the combina-
torial explosion of treelets, we only gather treelets
that contain at most four words and at most two
gaps in the surface string. This limits the number
of mappings to be O(n3) in the worst case, where
n is the number of nodes in the dependency tree.
2.3 Language pairs
In the present paper we focus on English-to-
German and English-to-Japanese machine transla-
63
you can set this property using Visual Basic
Sie k?nnen diese Eigenschaft auch mit Visual Basic festlegen
Figure 1: Example German-English and Japanese-English sentence pairs, with word alignments.
tion. Both German and Japanese differ markedly
from English in ways that we believe illumi-
nate well the strengths of a syntactically-informed
SMT system. We provide a brief sketch of the lin-
guistic characteristics of German and Japanese rel-
evant to the present study.
2.3.1 German
Although English and German are closely re-
lated ? they both belong to the western branch of
the Germanic family of Indo-European languages
? the languages differ typologically in ways that
are especially problematic for current approaches
to statistical machine translation as we shall now
illustrate. We believe that these typological differ-
ences make English-to-German machine transla-
tion a fertile test bed for syntax-based SMT.
German has richer inflectional morphology than
English, with obligatory marking of case, num-
ber and lexical gender on nominal elements and
person, number, tense and mood on verbal ele-
ments. This morphological complexity, combined
with pervasive, productive noun compounding is
problematic for current approaches to word align-
ment (Corston-Oliver and Gamon, 2004).
Equally problematic for machine translation is
the issue of word order. The position of verbs is
strongly determined by clause type. For exam-
ple, in main clauses in declarative sentences, finite
verbs occur as the second constituent of the sen-
tence, but certain non-finite verb forms occur in fi-
nal position. In Figure 1, for example, the English
?can? aligns with German ?ko?nnen? in second po-
sition and ?set? aligns with German ?festlegen? in
final position.
Aside from verbs, German is usually charac-
terized as a ?free word-order? language: major
constituents of the sentence may occur in various
orders, so-called ?separable prefixes? may occur
bound to the verb or may detach and occur at a
considerable distance from the verb on which they
depend, and extraposition of various kinds of sub-
ordinate clause is common. In the case of extrapo-
sition, for example, more than one third of relative
clauses in human-translated German technical text
are extraposed. For comparable English text the
figure is considerably less than one percent (Ga-
mon et al, 2002).
2.3.2 Japanese
Word order in Japanese is rather different from
English. English has the canonical constituent or-
der subject-verb-object, whereas Japanese prefers
subject-object-verb order. Prepositional phrases
in English generally correspond to postpositional
phrases in Japanese. Japanese noun phrases are
strictly head-final whereas English noun phrases
allow postmodifiers such as prepositional phrases,
relative clauses and adjectives. Japanese has lit-
tle nominal morphology and does not obligatorily
mark number, gender or definiteness. Verbal mor-
phology in Japanese is complex with morphologi-
cal marking of tense, mood, and politeness. Top-
icalization and subjectless clauses are pervasive,
and problematic for current SMT approaches.
The Japanese sentence in Figure 1 illustrates
several of these typological differences. The
sentence-initial imperative verb ?move? in the En-
glish corresponds to a sentence-final verb in the
Japanese. The Japanese translation of the object
noun phrase ?the camera slider switch? precedes
the verb in Japanese. The English preposition ?to?
aligns to a postposition in Japanese.
3 Experiments
Our goal in the current paper is to measure the
impact of parse quality on syntactically-informed
statistical machine translation. One method for
producing parsers of varying quality might be to
train a parser and then to transform its output, e.g.
64
by replacing the parser?s selection of the parent for
certain tokens with different nodes.
Rather than randomly adding noise to the
parses, we decided to vary the quality in ways that
more closely mimic the situation that confronts us
as we develop machine translation systems. An-
notating data for POS requires considerably less
human time and expertise than annotating syntac-
tic relations. We therefore used an automatic POS
tagger (Toutanova et al, 2003) trained on the com-
plete training section of the Penn Treebank (sec-
tions 02?21). Annotating syntactic dependencies
is time consuming and requires considerable lin-
guistic expertise.1 We can well imagine annotat-
ing syntactic dependencies in order to develop a
machine translation system by annotating first a
small quantity of data, training a parser, training a
system that uses the parses produced by that parser
and assessing the quality of the machine transla-
tion output. Having assessed the quality of the out-
put, one might annotate additional data and train
systems until it appears that the quality of the ma-
chine translation output is no longer improving.
We therefore produced parsers of varying quality
by training on the first n sentences of sections 02?
21 of the Penn Treebank, where n ranged from 250
to 39,892 (the complete training section). At train-
ing time, the gold-standard POS tags were used.
For parser evaluation and for the machine transla-
tion experiments reported here, we used an auto-
matic POS tagger (Toutanova et al, 2003) trained
on sections 02?21 of the Penn Treebank.
We trained English-to-German and English-to-
Japanese treelet translation systems on approxi-
mately 500,000 manually aligned sentence pairs
drawn from technical computer documentation.
The sentence pairs consisted of the English source
sentence and a human-translation of that sentence.
Table 1 summarizes the characteristics of this data.
Note that German vocabulary and singleton counts
are slightly more than double the corresponding
English counts due to complex morphology and
pervasive compounding (see section 2.3.1).
3.1 Parser accuracy
To evaluate the accuracy of the parsers trained on
different samples of sentences we used the tradi-
1Various people have suggested to us that the linguistic
expertise required to annotate syntactic dependencies is less
than the expertise required to apply a formal theory of con-
stituency like the one that informs the Penn Treebank. We
tend to agree, but have not put this claim to the test.
75%
80%
85%
90%
95%
0 10,000 20,000 30,000 40,000
Sample size
D
ep
en
de
n
c
y 
ac
cu
ra
c
y.
PTB Section 23
Technical text
Figure 2: Unlabeled dependency accuracy of
parsers trained on different numbers of sentences.
The graph compares accuracy on the blind test sec-
tion of the Penn Treebank to accuracy on a set of
250 sentences drawn from technical text. Punctu-
ation tokens are excluded from the measurement
of dependency accuracy.
tional blind test section of the Penn Treebank (sec-
tion 23). As is well-known in the parsing commu-
nity, parse quality degrades when a parser trained
on the Wall Street Journal text in the Penn Tree-
bank is applied to a different genre or semantic do-
main. Since the technical materials that we were
training the translation system on differ from the
Wall Street Journal in lexicon and syntax, we an-
notated a set of 250 sentences of technical material
to use in evaluating the parser. Each of the authors
independently annotated the same set of 250 sen-
tences. The annotation took less than six hours for
each author to complete. Inter-annotator agree-
ment excluding punctuation was 91.8%. Differ-
ences in annotation were resolved by discussion,
and the resulting set of annotations was used to
evaluate the parsers.
Figure 2 shows the accuracy of parsers trained
on samples of various sizes, excluding punctua-
tion tokens from the evaluation, as is customary
in evaluating dependency parsers. When mea-
sured against section 23 of the Penn Treebank,
the section traditionally used for blind evaluation,
the parsers range in accuracy from 77.8% when
trained on 250 sentences to 90.8% when trained
on all of sections 02?21. As expected, parse accu-
racy degrades when measured on text that differs
greatly from the training text. A parser trained on
250 Penn Treebank sentences has a dependency
65
English German English Japanese
Training Sentences 515,318 500,000
Words 7,292,903 8,112,831 7,909,198 9,379,240
Vocabulary 59,473 134,829 66,731 68,048
Singletons 30,452 66,724 50,381 52,911
Test Sentences 2,000 2,000
Words 28,845 31,996 30,616 45,744
Table 1: Parallel data characteristics
accuracy of 76.6% on the technical text. A parser
trained on the complete Penn Treebank training
section has a dependency accuracy of 84.3% on
the technical text.
Since the parsers make extensive use of lexi-
cal features, it is not surprising that the perfor-
mance on the two corpora should be so similar
with only 250 training sentences; there were not
sufficient instances of each lexical item to train re-
liable weights or lexical features. As the amount
of training data increases, the parsers are able to
learn interesting facts about specific lexical items,
leading to improved accuracy on the Penn Tree-
bank. Many of the lexical items that occur in the
Penn Treebank, however, occur infrequently or not
at all in the technical materials so the lexical infor-
mation is of little benefit. This reflects the mis-
match of content. The Wall Street Journal articles
in the Penn Treebank concern such topics as world
affairs and the policies of the Reagan administra-
tion; these topics are absent in the technical mate-
rials. Conversely, the Wall Street Journal articles
contain no discussion of such topics as the intrica-
cies of SQL database queries.
3.2 Translation quality
Table 2 presents the impact of parse quality on a
treelet translation system, measured using BLEU
(Papineni et al, 2002). Since our main goal is to
investigate the impact of parser accuracy on trans-
lation quality, we have varied the parser training
data, but have held the MT training data, part-of-
speech-tagger, and all other factors constant. We
observe an upward trend in BLEU score as more
training data is made available to the parser; the
trend is even clearer in Japanese.2 As a baseline,
we include right-branching dependency trees, i.e.,
trees in which the parent of each word is its left
2This is particularly encouraging since various people
have remarked to us that syntax-based SMT systems may
be disadvantaged under n-gram scoring techniques such as
BLEU.
EG EJ
Phrasal decoder 31.7?1.2 32.9?0.9
Treelet decoder
Right-branching 31.4?1.3 28.0?0.7
250 sentences 32.8?1.4 34.1?0.9
2,500 sentences 33.0?1.4 34.6?1.0
25,000 sentences 33.7?1.5 35.7?0.9
39,892 sentences 33.6?1.5 36.0?1.0
Table 2: BLEU score vs. decoder and parser vari-
ants. Here sentences refer to the amount of parser
training data, not MT training data.
neighbor and the root of a sentence is the first
word. With this analysis, treelets are simply sub-
sequences of the sentence, and therefore are very
similar to the phrases of Phrasal SMT. In English-
to-German, this result produces results very com-
parable to a phrasal SMT system (Koehn et al,
2003) trained on the same data. For English-to-
Japanese, however, this baseline performs much
worse than a phrasal SMT system. Although
phrases and treelets should be nearly identical
under this scenario, the decoding constraints are
somewhat different: the treelet decoder assumes
phrasal cohesion during translation. This con-
straint may account for the drop in quality.
Since the confidence intervals for many pairs
overlap, we ran pairwise tests for each system to
determine which differences were significant at
the p < 0.05 level using the bootstrap method de-
scribed in (Zhang and Vogel, 2004); Table 3 sum-
marizes this comparison. Neither language pair
achieves a statistically significant improvement
from increasing the training data from 25,000
pairs to the full training set; this is not surprising
since the increase in parse accuracy is quite small
(90.2% to 90.8% on Wall Street Journal text).
To further understand what differences in de-
pendency analysis were affecting translation qual-
ity, we compared a treelet translation system that
66
Pharaoh Right-branching 250 2,500 25,000 39,892
Pharaoh ? > > > >
Right-branching > > > >
250 ? > >
2,500 > >
25,000 ?
(a) English-German
Pharaoh Right-branching 250 2,500 25,000 39,892
Pharaoh < ? > > >
Right-branching > > > >
250 > > >
2,500 > >
25,000 ?
(b) English-Japanese
Table 3: Pairwise statistical significance tests. > indicates that the system on the top is significantly better
than the system on the left; < indicates that the system on top is significantly worse than the system on
the left; ? indicates that difference between the two systems is not statistically significant.
32
33
34
35
36
37
100 1000 10000 100000
Parser training sentences
B
LE
U
 
sc
o
re
Japanese
German
Figure 3: BLEU score vs. number of sentences
used to train the dependency parser
used a parser trained on 250 Penn Treebank sen-
tences to a treelet translation system that used
a parser trained on 39,892 Treebank sentences.
From the test data, we selected 250 sentences
where these two parsers produced different anal-
yses. A native speaker of German categorized the
differences in machine translation output as either
improvements or regressions. We then examined
and categorized the differences in the dependency
analyses. Table 4 summarizes the results of this
comparison. Note that this table simply identifies
correlations between parse changes and translation
changes; it does not attempt to identify a causal
link. In the analysis, we borrow the term ?NP
[Noun Phrase] identification? from constituency
analysis to describe the identification of depen-
dency treelets spanning complete noun phrases.
There were 141 sentences for which the ma-
chine translated output improved, 71 sentences for
which the output regressed and 38 sentences for
which the output was identical. Improvements in
the attachment of prepositions, adverbs, gerunds
and dependent verbs were common amongst im-
proved translations, but rare amongst regressed
translations. Correct identification of the depen-
dent of a preposition3 was also much more com-
mon amongst improvements.
Certain changes, such as improved root identifi-
cation and final punctuation attachment, were very
common across the corpus. Therefore their com-
mon occurrence amongst regressions is not very
surprising. It was often the case that improve-
ments in root identification or final punctuation at-
tachment were offset by regressions elsewhere in
the same sentence.
Improvements in the parsers are cases where
the syntactic analysis more closely resembles the
analysis of dependency structure that results from
applying Yamada and Matsumoto?s head-finding
rules to the Penn Treebank. Figure 4 shows dif-
ferent parses produced by parsers trained on dif-
3In terms of constituency analysis, a prepositional phrase
should consist of a preposition governing a single noun
phrase
67
You can manipulate Microsoft Access objects from another application that also supports automation .ROOT
You can manipulate Microsoft Access objects from another application that also supports automation .ROOT
(a) Dependency analysis produced by parser trained on 250 Wall Street Journal sentences.
(b) Dependency analysis produced by parser trained on 39,892 Wall Street Journal sentences.Figure 4: Parses produced by parsers trained on different numbers of sentences.
ferent numbers of sentences. The parser trained
on 250 sentences incorrectly attaches the prepo-
sition ?from? as a dependent of the noun ?ob-
jects? whereas the parser trained on the complete
Penn Treebank training section correctly attaches
the preposition as a dependent of the verb ?ma-
nipulate?. These two parsers also yield different
analyses of the phrase ?Microsoft Access objects?.
In parse (a), ?objects? governs ?Office? and ?Of-
fice? in turn governs ?Microsoft?. This analy-
sis is linguistically well-motivated, and makes a
treelet spanning ?Microsoft Office? available to
the treelet translation system. In parse (b), the
parser has analyzed this phrase so that ?objects?
directly governs ?Microsoft? and ?Office?. The
analysis more closely reflects the flat branching
structure of the Penn Treebank but obscures the
affinity of ?Microsoft? and ?Office?.
An additional measure of parse utility for MT
is the amount of translation material that can be
extracted from a parallel corpus. We increased the
parser training data from 250 sentences to 39,986
sentences, but held the number of aligned sentence
pairs used train other modules constant. The count
of treelet translation pairs occurring at least twice
in the English-German parallel corpus grew from
1,895,007 to 2,010,451.
4 Conclusions
We return now to the questions and concerns
raised in the introduction. First, is a treelet SMT
system sensitive to parse quality? We have shown
that such a system is sensitive to the quality of
Error category Regress Improve
Attachment of prep 1% 22%
Root identification 13% 28%
Final punctuation 18% 30%
Coordination 6% 16%
Dependent verbs 14% 32%
Arguments of verb 6% 15%
NP identification 24% 33%
Dependent of prep 0% 7%
Other attachment 3% 22%
Table 4: Error analysis, showing percentage of
regressed and improved translations exhibiting a
parse improvement in each specified category
the input syntactic analyses. With the less accu-
rate parsers that result from training on extremely
small numbers of sentences, performance is com-
parable to state-of-the-art phrasal SMT systems.
As the amount of data used to train the parser in-
creases, both English-to-German and English-to-
Japanese treelet SMT improve, and produce re-
sults that are statistically significantly better than
the phrasal baseline.
In the introduction we mentioned the concern
that others have raised when we have presented
our research: syntax might contain valuable infor-
mation but current parsers might not be of suffi-
cient quality. It is certainly true that the accuracy
of the best parser used here falls well short of what
we might hope for. A parser that achieves 90.8%
dependency accuracy when trained on the Penn
Treebank Wall Street Journal corpus and evalu-
68
ated on comparable text degrades to 84.3% accu-
racy when evaluated on technical text. Despite the
degradation in parse accuracy caused by the dra-
matic differences between the Wall Street Journal
text and the technical articles, the treelet SMT sys-
tem was able to extract useful patterns. Research
on syntactically-informed SMT is not impeded by
the accuracy of contemporary parsers.
One significant finding is that as few as 250
sentences suffice to train a dependency parser for
use in the treelet SMT framework. To date our
research has focused on translation from English
to other languages. One concern in applying the
treelet SMT framework to translation from lan-
guages other than English has been the expense
of data annotation: would we require 40,000 sen-
tences annotated for syntactic dependencies, i.e.,
an amount comparable to the Penn Treebank, in
order to train a parser that was sufficiently accu-
rate to achieve the machine translation quality that
we have seen when translating from English? The
current study gives hope that source languages can
be added with relatively modest investments in
data annotation. As more data is annotated with
syntactic dependencies and more accurate parsers
are trained, we would hope to see similar improve-
ments in machine translation output.
We challenge others who are conducting re-
search on syntactically-informed SMT to verify
whether or to what extent their systems are sen-
sitive to parse quality.
References
M. Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of EMNLP.
Simon Corston-Oliver and Michael Gamon. 2004.
Normalizing German and English inflectional mor-
phology to improve statistical word alignment. In
R. E. Frederking and K. B. Taylor, editors, Machine
translation: From real users to research. Springer
Verlag.
Simon Corston-Oliver, Anthony Aue, Kevin Duh, and
Eric Ringger. 2006. Multilingual dependency pars-
ing using Bayes Point Machines. In Proceedings of
HLT/NAACL.
Jason M. Eisner. 1996. Three new probabilistic mod-
els for dependency parsing: An exploration. In Pro-
ceedings of COLING, pages 340?345.
Michael Gamon, Eric Ringger, Zhu Zhang, Robert
Moore, and Simon Corston-Oliver. 2002. Extrapo-
sition: A case study in German sentence realization.
In Proceedings of COLING, pages 301?307.
Joshua Goodman. 2001. A bit of progress in lan-
guage modeling, extended version. Technical Re-
port MSR-TR-2001-72, Microsoft Research.
Edward Harrington, Ralf Herbrich, Jyrki Kivinen,
John C. Platt, and Robert C. Williamson. 2003. On-
line bayes point machines. In Proc. 7th Pacific-Asia
Conference on Knowledge Discovery and Data Min-
ing, pages 241?252.
Ralf Herbrich, Thore Graepel, and Colin Campbell.
2001. Bayes Point Machines. Journal of Machine
Learning Research, pages 245?278.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of HLT/NAACL.
M. Marcus, B. Santorini, and M. Marcinkiewicz.
1993. Building a large annotated corpus of en-
glish: The Penn Treebank. Computational Linguis-
tics, 19(2):313?330.
Igor A. Melc?uk. 1988. Dependency Syntax: Theory
and Practice. State University of New York Press.
Franz Josef Och and Hermann Ney. 2000. Improved
statistical alignment models. In Proceedings of the
ACL, pages 440?447, Hongkong, China, October.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the ACL.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
ACL, pages 311?318, Philadelpha, Pennsylvania.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005.
Dependency treelet translation: Syntactically in-
formed phrasal SMT. In Proceedings of the ACL.
Lucien Tesnie`re. 1959. ?Ele?ments de syntaxe struc-
turale. Librairie C. Klincksieck.
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of HLT/EMNLP, pages 252?259.
Stephan Vogel, Ying Zhang, Fei Huang, Alicia Tribble,
Ashish Venugopal, Bing Zhao, and Alex Waibel.
2003. The CMU statistical machine translation sys-
tem. In Proceedings of the MT Summit.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statis-
tical dependency analysis with support vector ma-
chines. In Proceedings of IWPT, pages 195?206.
Ying Zhang and Stephan Vogel. 2004. Measuring con-
fidence intervals for mt evaluation metrics. In Pro-
ceedings of TMI.
69
Proceedings of the Workshop on Statistical Machine Translation, pages 158?161,
New York City, June 2006. c?2006 Association for Computational Linguistics
Microsoft Research Treelet Translation System:
NAACL 2006 Europarl Evaluation
Arul Menezes, Kristina Toutanova and Chris Quirk
Microsoft Research
One Microsoft Way
Redmond, WA 98052
{arulm,kristout,chrisq}@microsoft.com
Abstract
The  Microsoft  Research  translation  system  is  a
syntactically  informed  phrasal  SMT  system  that
uses  a  phrase  translation  model  based  on
dependency treelets and a global reordering model
based  on  the  source  dependency  tree.  These
models  are  combined  with  several  other
knowledge  sources  in  a  log-linear  manner.  The
weights of the individual components in the log-
linear model  are set  by an automatic  parameter-
tuning method.  We give a brief  overview of the
components  of  the  system  and  discuss  our
experience with the Europarl data translating from
English to Spanish.
1. Introduction
The  dependency  treelet  translation  system
developed at MSR is a statistical MT system that
takes  advantage  of  linguistic  tools,  namely  a
source language dependency parser,  as well as a
word alignment component. [1]
To  train  a  translation  system,  we  require  a
sentence-aligned parallel  corpus.  First  the source
side is parsed to obtain dependency trees. Next the
corpus  is  word-aligned,  and  the  source
dependencies  are  projected  onto  the  target
sentences  using  the  word  alignments.  From  the
aligned dependency corpus we extract  all  treelet
translation pairs,  and train an order model and a
bi-lexical dependency model.
To translate, we parse the input sentence, and
employ  a  decoder  to  find  a  combination  and
ordering of treelet translation pairs that cover the
source tree and are optimal according to a set of
models.  In  a  now-common generalization  of  the
classic  noisy-channel  framework,  we  use  a  log-
linear combination of models [2], as in below:
translation?S , F ,? ?=argmax
T {?f ?F ? f f ?S ,T ?}
Such an approach toward translation scoring has
proven  very  effective  in  practice,  as  it  allows  a
translation system to incorporate information from
a  variety  of  probabilistic  or  non-probabilistic
sources.  The weights  ? = {  ?f } are selected by
discriminatively training against held out data.
2. System Details
A brief word on notation: s and t represent source
and target lexical nodes; S and T represent source
and target trees; s and t represent source and target
treelets  (connected  subgraphs  of  the  dependency
tree).  The  expression  ?t? T refers  to  all  the
lexical items in the target language tree T and |T|
refers to the count of lexical items in  T. We use
subscripts to indicate selected words: Tn represents
the n
th
 lexical item in an in-order traversal of T.
2.1. Training
We  use  the  broad  coverage  dependency  parser
NLPWIN  [3]  to  obtain  source  language
dependency  trees,  and  we  use  GIZA++  [4]  to
produce  word  alignments.  The  GIZA++ training
regimen  and  parameters  are  tuned  to  optimize
BLEU [5] scores on held-out data. Using the word
alignments,  we  follow a  set  of  dependency  tree
projection  heuristics  [1]  to  construct  target
dependency  trees,  producing  a  word-aligned
parallel  dependency  tree  corpus.  Treelet
translation pairs are extracted by enumerating all
source treelets (to a maximum size) aligned to a
target treelet.
2.2. Decoding
We use a tree-based decoder, inspired by dynamic
programming. It searches for an approximation of
158
the n-best translations of each subtree of the input
dependency  tree.  Translation  candidates  are
composed from treelet  translation pairs  extracted
from the training corpus. This process is described
in more detail in [1].
2.3. Models
2.3.1. Channel models
We  employ  several  channel  models:  a  direct
maximum likelihood estimate of the probability of
target  given  source,  as  well  as  an  estimate  of
source given target and target given source using
the word-based IBM Model 1 [6]. For MLE, we
use  absolute  discounting  to  smooth  the
probabilities:
PMLE ? t?s ?= c ? s , t ???c ? s ,* ?
Here,  c represents  the  count  of  instances  of  the
treelet pair  ?s, t? in the training corpus, and  ? is
determined empirically.
For Model 1 probabilities we compute the sum
over all possible alignments of the treelet without
normalizing for length. The calculation of source
given  target  is  presented  below;  target  given
source is calculated symmetrically.
PM1? t?s ?=?t?t ?s?s P ? t?s ?
2.3.2. Bilingual n-gram channel models
Traditional  phrasal  SMT systems  are  beset  by a
number of theoretical problems, such as the ad hoc
estimation  of  phrasal  probability,  the  failure  to
model  the  partition  probability,  and  the  tenuous
connection  between  the  phrases  and  the
underlying  word-based  alignment  model.  In
string-based  SMT  systems,  these  problems  are
outweighed by the key role played by phrases in
capturing  ?local?  order.  In  the  absence  of  good
global  ordering  models,  this  has  led  to  an
inexorable  push  towards  longer  and  longer
phrases, resulting in serious practical problems of
scale, without, in the end, obviating the need for a
real global ordering story.
In [13] we discuss these issues in greater detail
and  also  present  our  approach  to  this  problem.
Briefly,  we  take  as  our  basic  unit  the  Minimal
Translation Unit (MTU) which we define as a set
of source and target word pairs such that there are
no word alignment links between distinct MTUs,
and  no  smaller  MTUs  can  be  extracted  without
violating the previous constraint.  In other words,
these are the minimal non-compositional phrases.
We then build models based on n-grams of MTUs
in  source  string,  target  string  and  source
dependency  tree  order.  These  bilingual  n-gram
models  in  combination  with  our  global  ordering
model allow us to use shorter phrases without any
loss  in  quality,  or  alternately  to  improve  quality
while keeping phrase size constant.
As an example,  consider the aligned sentence
pair in Figure 1. There are seven MTUs:
m1 = <we should / hemos>
m2 = <NULL / de>
m3 = <follow / cumplir>
m4 = <the / el>
m5 = <Rio / Rio>
m6 = <agenda / programa>
m7 = <NULL / de>
We can then predict the probability of each MTU
in the context of (a) the previous MTUs in source
order,  (b) the previous MTUs in target order,  or
(c) the ancestor MTUs in the tree. We consider all
of these traversal orders, each acting as a separate
feature function in the log linear combination. For
source and target traversal order we use a trigram
model, and a bigram model for tree order.
2.3.3. Target language models
We  use  both  a  surface  level  trigram  language
model  and a dependency-based bigram language
model  [7],  similar  to  the  bilexical  dependency
modes  used  in  some  English  Treebank  parsers
(e.g. [8]).
Psurf ?T ?=?i=1
?T?
Ptrisurf ?T i?T i?2 ,T i?1 ?
Pbilex ?T ?=?i=1
?T?
Pbidep ?T i?parent ?T i ??
Ptrisurf is a Kneser-Ney smoothed trigram language
model  trained  on  the  target  side  of  the  training
corpus,  and  Pbilex is  a  Kneser-Ney  smoothed
we??2 should??1 follow the??2 Rio??1 agenda?+1
hemos??1 de?+1 cumplir el??1 programa?+1 de??1 R?o?+1
Figure 1: Aligned dependency tree pair, annotated with head-
relative positions
159
bigram language model trained on target language
dependencies  extracted  from the aligned  parallel
dependency tree corpus.
2.3.4. Order model
The  order  model  assigns  a  probability  to  the
position  (pos)  of  each target  node relative  to its
head based on information in both the source and
target trees:
Porder ?order ?T ??S ,T ?=?t?T P ? pos ? t , parent ? t ???S ,T ?
Here, position is modeled in terms of closeness to
the head in the dependency tree. The closest pre-
modifier  of  a  given  head  has  position  -1;  the
closest  post-modifier  has  a  position  1.  Figure  1
shows an example dependency tree pair annotated
with head-relative positions.
We use a small set of features reflecting local
information in the dependency tree to model P(pos
(t,parent(t)) | S, T):
? Lexical items of t and parent(t), the parent of t
in the dependency tree.
? Lexical items of the source nodes aligned to t
and head(t).
? Part-of-speech  ("cat")  of  the  source  nodes
aligned to the head and modifier.
? Head-relative  position  of  the  source  node
aligned to the source modifier. 
These  features  along  with  the  target  feature  are
gathered  from  the  word-aligned  parallel
dependency  tree  corpus  and  used  to  train  a
statistical  model.  In  previous  versions  of  the
system,  we trained a decision tree model  [9].  In
the  current  version,  we  explored  log-linear
models. In addition to providing a different way of
combining  information  from  multiple  features,
log-linear models allow us to model the similarity
among different classes (target positions), which is
advantageous for our task.
 We  implemented  a  method  for  automatic
selection  of  features  and  feature  conjunctions  in
the log-linear model. The method greedily selects
feature  conjunction  templates  that  maximize  the
accuracy  on  a  development  set.  Our  feature
selection  study  showed  that  the  part-of-speech
labels of the source nodes aligned to the head and
the modifier and the head-relative position of the
source  node  corresponding  to  the  modifier  were
the  most  important  features.  It  was  useful  to
concatenate the part-of-speech of the source head
with  every  feature.  This  effectively  achieves
learning  of  separate  movement  models  for  each
source head category. Lexical information on the
pairs  of  head  and  dependent  in  the  source  and
target was also very useful.
To model the similarity among different target
classes  and  to  achieve  pooling  of  data  across
similar classes, we added multiple features of the
target position. These features let our model know,
for  example,  that  position  -5  looks  more  like
position  -6  than  like  position  3.  We  added  a
feature  ?positive?/?negative?  which  is  shared  by
all  positive/negative  positions.  We  also  added  a
feature looking at the displacement of a position in
the target from the corresponding position in the
source  and  features  which  group  the  target
positions  into  bins.  These  features  of  the  target
position are combined with features of the input.
This  model  was  trained  on  the  provided
parallel  corpus.  As  described  in  Section  2.1  we
parsed the source sentences,  and projected target
dependencies.  Each  head-modifier  pair  in  the
resulting target trees constituted a training instance
for the order model.
The  score  computed  by  the  log-linear  order
model is used as a single feature in the overall log-
linear  combination  of  models  (see  Section  1),
whose  parameters  were  optimized  using
MaxBLEU  [2].  This  order  model  replaced  the
decision tree-based model described in [1]. 
We compared  the  decision  tree  model  to  the
log-linear  model  on  predicting  the  position  of  a
modifier  using  reference  parallel  sentences,
independent of the full MT system. The decision
tree  achieved  per  decision  accuracy  of  69%
whereas  the  log-linear  model  achieved  per
decision accuracy of 79%.
1
 In the context of the
full  MT system,  however,  the  new order  model
provided  a  more  modest  improvement  in  the
BLEU score of 0.39%.
2.3.5. Other models
We include two pseudo-models that help balance
certain biases inherent in our other models.
? Treelet  count.  This  feature  is  a  count  of
treelets  used  to  construct  the  candidate.  It
acts as a bias toward translations that use a
smaller  number  of  treelets;  hence  toward
larger  sized  treelets  incorporating  more
context.
? Word count. We also include a count of the
words  in  the  target  sentence.  This  feature
1
 The per-decision accuracy numbers were obtained on
different (random) splits of training and test data.
160
helps  to  offset  the  bias  of  the  target
language model toward shorter sentences.
3. Discussion
We participated in  the English to  Spanish  track,
using  the  supplied  bilingual  data  only.  We used
only the target side of the bilingual corpus for the
target  language  model,  rather  than  the  larger
supplied  language  model.  We  did  find  that
increasing the target language order from 3 to 4
had a noticeable impact on translation quality. It is
likely that a larger target language corpus would
have an impact, but we did not explore this.
BLEU
Baseline treelet system 27.60
Add bilingual MTU models 28.42
Replace DT order model with log-linear model 28.81
Table 1: Results on development set
We found  that  the  addition of  bilingual  n-gram
based  models  had  a  substantial  impact  on
translation  quality.  Adding  these  models  raised
BLEU scores about 0.8%, but anecdotal evidence
suggests  that  human-evaluated  quality  rose  by
much more than the BLEU score difference would
suggest. In general, we felt that in this corpus, due
to the great diversity in translations for the same
source language words and phrases, and given just
one reference translation, BLEU score correlated
rather  poorly  with  human  judgments.  This  was
borne out in the human evaluation of the final test
results.  Humans  ranked  our  system  first  and
second,  in-domain  and  out-of-domain
respectively, even though it was in the middle of a
field of ten systems by BLEU score. Furthermore,
n-gram  channel  models  may  provide  greater
robustness. While our BLEU score dropped 3.61%
on out-of-domain data, the average BLEU score of
the other nine competing systems dropped 5.11%.
4. References
[1] Quirk,  C.,  Menezes,  A.,  and Cherry,  C.,  "Dependency
Tree Translation: Syntactically Informed Phrasal SMT",
Proceedings of ACL 2005, Ann Arbor, MI, USA, 2005.
[2] Och, F.  J.,  and Ney, H.,  "Discriminative  Training and
Maximum  Entropy  Models  for  Statistical  Machine
Translation",  Proceedings  of  ACL  2002,  Philadelphia,
PA, USA, 2002.
[3] Heidorn, G., ?Intelligent writing assistance?, in Dale et
al.  Handbook of Natural Language Processing, Marcel
Dekker, 2000.
[4] Och, F.  J.,  and Ney H.,  "A Systematic Comparison of
Various  Statistical  Alignment  Models",  Computational
Linguistics, 29(1):19-51, March 2003.
[5] Papineni,  K.,  Roukos,  S.,  Ward,  T.,  and  Zhu,  W.-J.,
"BLEU: a method for automatic evaluation of machine
translation",  Proceedings  of  ACL  2002,  Philadelphia,
PA, USA, 2002.
[6] Brown,  P.  F.,  Della Pietra,  S.,  Della Pietra,  V. J.,  and
Mercer, R. L., "The Mathematics of Statistical Machine
Translation:  Parameter  Estimation",  Computational
Linguistics 19(2): 263-311, 1994.
[7] Aue,  A.,  Menezes,  A.,  Moore,  R.,  Quirk,  C.,  and
Ringger,  E.,  "Statistical  Machine  Translation  Using
Labeled Semantic Dependency Graphs." Proceedings of
TMI 2004, Baltimore, MD, USA, 2004.
[8] Collins,  M.,  "Three  generative,  lexicalised  models  for
statistical  parsing",  Proceedings of ACL 1997,  Madrid,
Spain, 1997.
[9] Chickering,  D.M.,  "The  WinMine  Toolkit",  Microsoft
Research  Technical  Report  MSR-TR-2002-103,
Redmond, WA, USA, 2002.
[10] Och,  F.  J.,  Gildea,  D.,  Khudanpur,  S.,  Sarkar,  A.,
Yamada, K., Fraser, A., Kumar, S., Shen, L., Smith, D.,
Eng,  K.,  Jain,  V.,  Jin,  Z.,  and  Radev,  D.,  "A
Smorgasbord  of  Features  for  Statistical  Machine
Translation". Proceedings of HLT/NAACL 2004, Boston,
MA, USA, 2004.
[11] Bender,  O.,  Zens,  R.,  Matsuov,  E.  and  Ney,  H.,
"Alignment  Templates:  the  RWTH  SMT  System".
IWSLT Workshop at INTERSPEECH 2004, Jeju Island,
Korea, 2004.
[12] Och, F. J., "Minimum Error Rate Training for Statistical
Machine  Translation",  Proceedings  of  ACL  2003,
Sapporo, Japan, 2003.
[13] Quirk,  C  and  Menezes,  A,  ?Do  we  need  phrases?
Challenging  the  conventional  wisdom  in  Statistical
Machine  Translation?,  Proceedings  of  HLT/NAACL
2006, New York, NY, USA, 2006
161
Proceedings of the Second Workshop on Statistical Machine Translation, pages 1?8,
Prague, June 2007. c?2007 Association for Computational Linguistics
Using Dependency Order Templates to Improve Generality in 
Translation  
Arul Menezes and Chris Quirk
Microsoft Research 
One Microsoft Way, Redmond, WA 98052, USA 
{arulm, chrisq}@microsoft.com 
 
Abstract 
Today's statistical machine translation 
systems generalize poorly to new 
domains. Even small shifts can cause 
precipitous drops in translation quality. 
Phrasal systems rely heavily, for both 
reordering and contextual translation, on 
long phrases that simply fail to match out-
of-domain text. Hierarchical systems 
attempt to generalize these phrases but 
their learned rules are subject to severe 
constraints. Syntactic systems can learn 
lexicalized and unlexicalized rules, but the 
joint modeling of lexical choice and 
reordering can narrow the applicability of 
learned rules. The treelet approach models 
reordering separately from lexical choice, 
using a discriminatively trained order 
model, which allows  treelets to apply 
broadly, and has shown better 
generalization to new domains, but suffers 
a factorially large search space. We 
introduce a new reordering model based 
on dependency order templates, and show 
that it outperforms both phrasal and treelet 
systems on in-domain and out-of-domain 
text, while limiting the search space. 
1 Introduction 
Modern phrasal SMT systems such as (Koehn et 
al., 2003) derive much of their power from being 
able to memorize and use long phrases. Phrases 
allow for non-compositional translation, local 
reordering and contextual lexical choice. 
However the phrases are fully lexicalized, which 
means they generalize poorly to even slightly out-
of-domain text. In an open competition (Koehn & 
Monz, 2006) systems trained on parliamentary 
proceedings were tested on text from 'news 
commentary' web sites, a very slightly different 
domain. The 9 phrasal systems in the English to 
Spanish track suffered an absolute drop in BLEU 
score of between 4.4% and 6.34% (14% to 27% 
relative). The treelet system of Menezes et al 
(2006) fared somewhat better but still suffered an 
absolute drop of 3.61%.  
Clearly there is a need for approaches with 
greater powers of generalization. There are 
multiple facets to this issue, including handling of 
unknown words, new senses of known words etc. 
In this work, we will focus on the issue of 
reordering, i.e. can we learn how to transform the 
sentence structure of one language into the 
sentence structure of another, in a way that is not 
tied to a specific domain or sub-domains, or 
indeed, sequences of individual words.   
An early attempt at greater generality in a 
purely phrasal setting was the alignment template 
approach (Och & Ney 2004); newer approaches 
include formally syntactic (Chiang 2005), and 
linguistically syntactic approaches (Quirk et al 
2005), (Huang et al 2006). In the next section, we 
examine these representative approaches to the 
reordering problem. 
2 Related Work 
Our discussion of related work will be grounded 
in the following tiny English to Spanish example, 
where the training set includes:  
a very old book 
un libro m?s  antiguo 
a  book  very old1 
the old man 
el  hombre viejo 
the man    old 
it is very important 
es muy  importante 
is very important 
                                                           
1 English gloss of Spanish sentences in italics. 
1
and the test sentence and reference translation are 
a very old man 
un hombre muy  viejo 
a  man    very old 
Note that while the first training pair has the 
correct structure for the test sentence, most of the 
contextually correct lexical choices come from 
the other two pairs. 
2.1 Phrasal translation, Alignment templates 
The relevant phrases (i.e. those that match the test 
sentence) extracted from these training pairs are 
shown in Table 2.1. Only phrases up to size 3 are 
shown. The ones in italics are 'correct' in that they 
can lead to the reference translation. Note that 
none of the multi-word phrases lead to the 
reference, so the local reordering often captured 
in the phrasal model is no help at all in ordering 
this sentence. The system is unable to learn the 
correct structure from the first sentence because 
the words are wrong, and from the second 
sentence even though the phrase old man has the 
right words in the right order, it does not lead to 
the reference translation because the translation of 
very cannot be inserted in the right place.  
a un 
very m?s 
old antiguo 
very old m?s antiguo 
old viejo 
man hombre 
old man hombre viejo 
very muy 
Table 2.1: Relevant extracted phrases 
Looking at this as a sparse data issue we might 
suspect that generalization could solve the 
problem. The alignment template approach (Och 
& Ney, 2004) uses word classes rather than 
lexical items to model phrase translation. Yet this 
approach loses the advantage of context-sensitive 
lexical selection: the word translation model 
depends only on the word classes to subcategorize 
for translations, which leads to less accurate 
lexical choice in practice (Zens & Ney, 2004). 
2.2 Hierarchical translation 
Hierarchical systems (Chiang, 2005) induce a 
context-free grammar with one non-terminal 
directly from the parallel corpus, with the 
advantage of not requiring any additional 
knowledge source or tools, such as a treebank or a 
parser. However this can lead to an explosion of 
rules. In order to make the problem tractable and 
avoid spurious ambiguity, Chiang restricts the 
learned rules in several ways. The most 
problematic of these is that every rule must have 
at least one pair of aligned words, and that 
adjacent non-terminals are not permitted on the 
source side. In Table 2.2 we show the additional 
hierarchical phrases that would be learned from 
our training pairs under these restrictions. Again 
only those applicable to the test sentence are 
shown and the 'correct' rules, i.e. those that lead to 
the reference, are italicized. 
X1 old X1 antiguo 
very X1 m?s X1 
very old X1 X1 m?s antiguo 
X1 old X2 X2 X1 antiguo 
very X1 X2 X2 m?s X1 
X1 man hombre X1 
old X1 X1 viejo 
X1 old man X1 hombre viejo 
X1 very X1 muy 
very X2 muy X2 
X1 very X2 X1 muy X2 
Table 2.2: Additional hierarchical phrases 
Note that even though from the first pair, we learn 
several rules with the perfect reordering for the 
test sentence, they do not lead to the reference 
because they drag along the contextually incorrect 
lexical choices. From the second pair, we learn a 
rule (X1 old man) that has the right contextual 
word choice, but  does not lead to the reference, 
because the paucity of the grammar's single non-
terminal causes this rule to incorrectly imply that 
the translation of very be placed before hombre. 
2.3 Constituency tree transduction 
An alternate approach is to use linguistic 
information from a parser. Transduction rules 
between Spanish strings and English trees can be 
learned from a word-aligned parallel corpus with 
parse trees on one side (Graehl & Knight, 2004). 
Such rules can be used to translate from Spanish 
to English by searching for the best English 
language tree for a given Spanish language string 
(Marcu et al, 2006). Alternately English trees 
produced by a parser can be transduced to 
2
Spanish strings using the same rules (Huang et al, 
2006). Translation rules may reach beyond one 
level in the syntax tree; this extended domain of 
locality allows many phenomena including both 
lexicalized and unlexicalized rules. However 
reordering and translation are modeled jointly, 
which may exacerbate data sparsity. Furthermore 
it forces the system to pick between unlexicalized 
rules that capture reordering and lexicalized rules 
that model context-sensitive translation. 
For instance, the following rules can be 
extracted from the first sentence of the corpus: 
 
r1: un x1 x2 ? NP(DT(a) ADJP:x2 NN:x1) 
r2: x1 x2 ? ADJP(RB:x1 JJ:x2) 
  
Although together they capture the necessary 
reordering for our test sentence pair, they do not 
allow for context sensitive translations of the 
ambiguous terms very and old; each must be 
selected independently. Disappointingly, no 
single constituency tree transduction rule derived 
from this corpus translates old man as hombre 
viejo in a single step on the test sentence: the 
syntactic structures are slightly different, but the 
difference is sufficient to prevent matching. 2 
Again we note that phrases provide utility by 
capturing both reordering and context. While xRS 
                                                           
2 Marcu et al (2006) and Zollmann et al (2006) recognize 
this problem and attempt to alleviate it by grafting surface 
phrases into constituency trees by various methods. 
rules provide an elegant and powerful model of 
reordering, they come with a potential cost in 
context-sensitive translation.  
2.4 Dependency treelet translation 
We previously described (Quirk et al 2005) a 
linguistically syntax-based system that parses the 
source language, uses word-based alignments to 
project a target dependency tree, and extracts 
paired dependency tree fragments (treelets) 
instead of surface phrases.  In contrast to the xRS 
approach, ordering is very loosely coupled with 
translation via a separate discriminatively trained 
dependency tree-based order model. The switch 
to a dependency parse also changes the 
conditioning information available for translation: 
related lexical items are generally adjacent, rather 
than separated by a path of unlexicalized non-
terminals. In effect, by using a looser matching 
requirement, treelets retain the context-sensitive 
lexical choice of phrases: treelets must only be a 
connected subgraph of the input sentence to be 
applicable; some children may remain uncovered. 
Figure 2.2 shows source dependency parses 
and projected target dependencies for our training 
data; Figure 2.3 shows the treelet pairs that this 
system would extract that match the input 
a very old book
DT RB JJ NN
ADJP
NP
un libro m?s antiguo
the old man
DT JJ NN
NP
el hombre viejo
it is very important
PN VB RB JJ
ADJP
VP
S
es muy importante  
Figure 2.1:  Constituency parses 
 
Figure 2.2: Dependency trees for training pairs 
 
Figure 2.3: Relevant extracted treelets 
3
sentence (treelets of size 1 are not shown).  The 
second treelet supplies the order of viejo with 
respect to its head, and unlike the case with xRS 
rules, we can use this to make the correct 
contextual word choice. The difference is that 
because xRS rules provide both reordering and 
word choice, each rule must match all of the 
children at any given tree node. On the other 
hand, treelets are allowed to match more loosely. 
The translations of the unmatched children (un 
and muy in this case) are placed by exploring all 
possible orderings and scoring them with both 
order model and language model. Although this 
effectively decouples lexical selection from 
ordering, it comes at a huge cost in search space 
and translation quality may suffer due to search 
error. However, as mentioned in Section 1, this 
approach is able to generalize better to out-of-
domain data than phrasal approaches. Koehn and 
Monz (2006) also include a human evaluation, in 
which this system ranked noticeably higher than 
one might have predicted from its BLEU score.    
3 Dependency Order Templates 
The Dependency Order Templates approach 
leverages the power of the xR rule formalism, 
while avoiding the problems mentioned in Section 
2.3, by constructing the rules on the fly from two 
separately matched components: (a) Dependency 
treelet translation pairs described in Section 2.4 
that capture contextual lexical translations but are 
underspecified with respect to ordering, and (b) 
Order templates, which are unlexicalized rules 
(over dependency, rather than constituency trees) 
that capture reordering phenomena. 
Formally, an order template is an unlexicalized 
transduction rule mapping dependency trees 
containing only parts of speech to unlexicalized 
target language trees (see Figure 4.1b). 
Given an input sentence, we combine relevant 
treelet translation pairs and order templates to 
construct lexicalized transduction rules for that 
sentence, and then decode using standard 
transduction approaches. By keeping lexical and 
ordering information orthogonal until runtime, we 
can produce novel transduction rules not seen in 
the training corpus. This allows greater 
generalization capabilities than the constituency 
tree transduction approaches of Section 2.3. 
As compared to the treelet approach described 
in Section 2.4, the generalization capability is 
somewhat reduced. In the treelet system all 
reorderings are exhaustively evaluated, but the 
size of the search space necessitates tight pruning, 
leading to significant search error. By contrast, in 
the order template approach we consider only 
reorderings that are captured in some order 
template. The drastic reduction in search space 
leads to an overall improvement, not only in 
decoding speed, but also in translation quality due 
to reduced search error. 
3.1 Extracting order templates 
For each pair of parallel training sentences, we 
parse the source sentence, obtain a source 
dependency tree, and use GIZA++ word 
alignments to project a target dependency tree as 
described in Quirk et al (2005).  
Given this pair of aligned source and target 
dependency trees, we recursively extract one 
order template for each pair of aligned non-leaf 
source and target nodes. In the case of multi-word 
alignments, all contiguous 3  aligned nodes are 
added to the template. Next we recursively add 
child nodes as follows: For each node in the 
template, add all its children. For each such child, 
if it is aligned, stop recursing, if it is unaligned, 
recursively add its children.     
On each template node we remove the lexical 
items; we retain the part of speech on the source 
nodes (we do not use target linguistic features). 
We also keep node alignment information4. The 
resulting aligned source and target sub-graphs 
comprise the order template. Figure 4.1b lists the 
order templates extracted from the training pairs 
in Figure 2.1 that capture all the patterns 
necessary to correctly reorder the test sentence. 
4 Decoding 
Decoding is treated as a problem of syntax-
directed transduction. Input sentences are 
segmented into a token stream, annotated with 
part-of-speech information, and parsed into 
                                                           
3 If a multi-word alignment is not contiguous in either source 
or target dependency tree no order template is extracted. 
4 If a source or target node aligns to a tree node outside the 
template, the template breaks phrasal cohesion and is 
currently discarded. We intend to address these 'structural 
divergence' patterns in future work. 
4
unlabeled dependency trees. At each node in the 
input dependency tree we first find the set of 
matching treelet pairs: A pair matches if its source 
side corresponds to a connected subgraph of the 
input tree. Next we find matching order 
templates: order templates must also match a 
connected subgraph of the input tree, but in 
addition, for each input node, the template must 
match either all or none of its children 5 . 
Compatible combinations of treelets and order 
templates are merged to form xR rules. Finally, 
we search for the best transduction according to 
the constructed xR rules as scored by a log-linear 
combination of models (see Section 5). 
4.1 Compatibility 
A treelet and an order template are considered 
compatible if the following conditions are met: 
The treelet and the matching portions of the 
template must be structurally isomorphic. Every 
treelet node must match an order template node. 
Matching nodes must have the same part of 
speech. Unaligned treelet nodes must match an 
unaligned template node. Aligned treelet nodes 
must match aligned template nodes. Nodes that 
are aligned to each other in the treelet pair must 
match template nodes that are aligned to each 
other. 
4.2 Creating transduction rules 
Given a treelet, we can form a set of tree 
transduction rules as follows. We iterate over 
each source node n in the treelet pair; let s be the 
corresponding node in the input tree (identified 
during the matching). If, for all children of s there 
is a corresponding child of n, then this treelet 
specifies the placement of all children and no 
changes are necessary. Otherwise we pick a 
template that matched at s and is compatible with 
the treelet. The treelet and template are unified to 
produce an updated rule with variables on the 
source and target sides for each uncovered child 
of s. When all treelet nodes have been visited, we 
are left with a transduction rule that specifies the 
translation of all nodes in the treelet and contains 
variables that specify the placement of all 
                                                           
5 This is so the resulting rules fit within the xR formalism. At 
each node, a rule either fully specifies its ordering, or 
delegates the translation of the subtree to other rules.  
uncovered nodes. Due to the independence of 
ordering and lexical information, we may produce 
novel transduction rules not seen in the training 
corpus. Figure 4.1 shows this process as it applies 
to the test sentence in Section 2. 
If, at any node s, we cannot find a matching 
template compatible with the current treelet, we 
create an artificial source order template, which 
simply preserves the source language order in the 
target translation. We add a feature function that 
counts the number of such templates and train its 
weight during minimum error rate training. 
4.3 Transduction using xR rules 
In the absence of a language model or other 
contextually dependent features, finding the 
highest scoring derivation would be a simple 
dynamic program (Huang et al 2006) 6.However 
exact search using an ? -gram language model 
leads to split states for each ? -gram context. 
Instead we use an approximate beam search 
moving bottom-up in the tree, much like a CKY 
parser. Candidates in this search are derivations 
with respect to the transducer. 
Each transduction rule ?  has a vector of 
variables ???,? ??? . Each variable is associated 
with an input node ????. For each input node ?, 
we keep a beam of derivations ????. Derivations 
are represented as a pair ??, ??  where ?  is a 
transduction rule and ? ? ?? is a vector with one 
integer for each of the ?  variables in ? . The 
interpretation is that the complete candidate can 
be constructed by recursively substituting for each 
                                                           
6 Like Chiang (2005) we only search for the yield of the most 
likely derivation, rather than the most likely yield. 
Figure 4.1: Merging templates and treelets 
5
??? ?  ??? ????  the candidate constructed from 
the ?? th entry in the beam ?????????.  
Figure 4.2 describes the transduction process. 
Since we approach decoding as xR transduction, 
the process is identical to that of constituency-
based algorithms (e.g. Huang and Chiang, 2007). 
There are several free parameters to tune: 
? Beam size ? Maximum number of candidates 
per input node (in this paper we use 100) 
? Beam threshold ? maximum range of scores 
between top and bottom scoring candidate 
(we use a logprob difference of 30) 
? Maximum combinations considered ? To 
bound search time, we can stop after a 
specified number of elements are popped off 
the priority queue (we use 5000) 
5 Models 
We use all of the Treelet models we described in 
Quirk et al (2005) namely:  
? Treelet table with translation probabilities 
estimated using maximum likelihood, with 
absolute discounting.  
? Discriminative tree-based order model. 
? Forward and backward lexical weighting, 
using Model-1 translation probabilities. 
? Trigram language model using modified 
Kneser-Ney smoothing.  
? Word and phrase count feature functions. 
In addition, we introduce the following: 
? Order template table, with template 
probabilities estimated using maximum 
likelihood, with absolute discounting. 
? A feature function that counts the number of 
artificial source order templates (see below) 
used in a candidate. 
The models are combined in a log-linear 
framework, with weights trained using minimum 
error rate training to optimize the BLEU score. 
6 Experiments 
We evaluated the translation quality of the system 
using the BLEU metric (Papineni et al, 2002). 
We compared our system to Pharaoh, a leading 
phrasal SMT decoder (Koehn et al, 2003), and 
our treelet system. We report numbers for English 
to Spanish. 
6.1 Data 
We used the Europarl corpus provided by the 
NAACL 2006 Statistical Machine Translation 
workshop. The target language model was trained 
using only the target side of the parallel corpus. 
The larger monolingual corpus was not utilized.  
The corpus consists of European Parliament 
proceedings, 730,740 parallel sentence pairs of 
English-Spanish, amounting to about 15M words 
in each language. The test data consists of 2000 
sentences each of development (dev), 
development-test (devtest) and test data (test) 
from the same domain. There is also a separate set 
of 1064 test sentences (NC-test) gathered from 
"news commentary" web sites.  
6.2 Training 
We parsed the source (English) side of the corpus 
using NLPWIN, a broad-coverage rule-based 
parser able to produce syntactic analyses at 
varying levels of depth (Heidorn, 2002). For the 
purposes of these experiments we used a 
dependency tree output with part-of-speech tags 
and unstemmed, case-normalized surface words. 
For word alignment we used GIZA++, under a 
training regimen of five iterations of Model 1, 
five iterations of HMM, and five iterations of 
Model 4, in both directions. The forward and 
backward alignments were symmetrized using a 
tree-based heuristic combination. The word 
GetTranslationBeam(?) // memoized 
    prioq ? ? 
    beam ? ? 
    for ? ? ???? 
        Enqueue(prioq, ??, ??, EarlyScore(??, ??)) 
    while Size(prioq) ? 0 
        ??, ?? ? PopBest(prioq) 
        AddToBeam(beam, ??, ??, TrueScore(??, ??)) 
        for ? in 1. . |?| 
            Enqueue(prioq, ??, ? ? ???, 
                 EarlyScore(??, ? ? ???)) 
    return beam 
EarlyScore(??, ??) 
    ? ? RuleScore(?) 
    for ? in 1. . |?| 
        ? ? InputNode(GetVariable (?, ?)) 
        beam ? GetTranslationBeam(?) 
        ? ? ? ?TrueScore(GetNthEntry(beam, ??)) 
    return ? 
Figure 4.2: Beam tree transduction 
6
alignments and English dependency tree were 
used to project a target tree. From the aligned tree 
pairs we extracted a treelet table and an order 
template table.  
The comparison treelet system was identical 
except that no order template model was used. 
The comparison phrasal system was 
constructed using the same GIZA++ alignments 
and the heuristic combination described in (Och 
& Ney, 2003). Except for the order models 
(Pharaoh uses a penalty on the deviance from 
monotone), the same models were used. 
All systems used a treelet or phrase size of 7 
and a trigram language model. Model weights 
were trained separately for all 3 systems using 
minimum error rate training to maximize BLEU 
(Och, 2003) on the development set (dev). Some 
decoder pruning parameters were tuned on the 
development test (devtest). The test and NC-test 
data sets were not used until final tests. 
7 Results 
We present the results of our system comparisons 
in Table 7.1 and Figure 7.1 using three different 
test sets: The in-domain development test data 
(devtest), the in-domain blind test data (test) and 
the out-of-domain news commentary test data 
(NC-test). All differences (except phrasal vs. 
template on devtest), are statistically significant at 
the p>=0.99 level under the bootstrap resampling 
test. Note that while the systems are quite 
comparable on the in-domain data, on the out-of-
domain data the phrasal system's performance 
drops precipitously, whereas the performance of 
the treelet and order template systems drops much 
less, outperforming the phrasal system by 2.7% 
and 3.46% absolute BLEU. 
 devtest test NC-test
Phrasal 0.2910 0.2935 0.2354
Treelet 0.2819 0.2981 0.2624
Template 0.2896 0.3045 0.2700
Table 7.1: System Comparisons across domains 
Further insight may be had by comparing the 
recall 7  for different n-gram orders (Table 7.2). 
The phrasal system suffers a greater decline in the 
higher order n-grams than the treelet and template 
                                                           
7 n-gram precision cannot be directly compared across output 
from different systems due to different levels of 'brevity' 
systems, indicating that latter show improved 
generality in reordering. 
  1gm 2gm 3gm 4gm 
Test Phrasal 0.61 0.35 0.23 0.15 
 treelet 0.62 0.36 0.23 0.15 
 template 0.62 0.36 0.24 0.16 
NC-test phrasal 0.58 0.30 0.17 0.10 
 treelet 0.60 0.33 0.20 0.12 
 template 0.61 0.34 0.20 0.13 
Table 7.2: n-gram recall across domains 
7.1 Treelet vs. Template systems 
As described in Section 3.1, the order templates 
restrict the broad reordering space of the treelet 
system. Although in theory this might exclude 
reorderings necessary for some translations, Table 
7.3 shows that in practice, the drastic search space 
reduction allows the decoder to explore a wider 
beam and more rules, leading to reduced search 
error and increased translation speed. (The topK 
parameter is the number of phrases explored for 
each span, or rules/treelets for each input node.) 
 Devtest 
BLEU 
Sents. 
per sec 
Pharaoh, beam=100, topK=20 0.2910 0.94 
Treelet, beam=12, topK=5 0.2819 0.21 
Template, beam=100, topK=20 0.2896 0.56 
Table 7.3: Performance comparisons 
Besides the search space restriction, the other 
significant change in the template system is to 
include MLE template probabilities as an 
 
Figure 7.1: In-domain vs. Out-of-domain BLEU 
23
24
25
26
27
28
29
30
31
development in-domain out-of-domain
Phrasal Treelet Order Template
7
additional feature function. Given that the 
template system operates over rules where the 
ordering is fully specified, and that most tree 
transduction systems use MLE rule probabilities 
to model both lexical selection and reordering, 
one might ask if the treelet system's 
discriminatively trained order model is now 
redundant. In Table 7.4 we see that this is not the 
case.8 (Differences are significant at p>=0.99.) 
 devtest test NC-test 
MLE model only 0.2769 0.2922 0.2512 
Discriminative and 
MLE models 
0.2896 0.3045 0.2700 
Table 7.4: Templates and discriminative order model 
Finally we examine the role of frequency 
thresholds in gathering templates. In Table 7.5 it 
may be seen that discarding singletons reduces 
the table size by a factor of 5 and improves 
translation speed with negligible degradation in 
quality. 
 devtest 
BLEU 
Number of 
templates 
Sentences 
per sec. 
No threshold 0.2898 752,165 0.40 
Threshold=1 0.2896 137,584 0.56 
Table 7.5: Effect of template count cutoffs 
8 Conclusions and Future Work 
We introduced a new model of Dependency Order 
Templates that provides for separation of lexical 
choice and reordering knowledge, thus allowing 
for greater generality than the phrasal and xRS 
approaches, while drastically limiting the search 
space as compared to the treelet approach. We 
showed BLEU improvements over phrasal of over 
1% in-domain and nearly 3.5% out-of-domain. As 
compared to the treelet approach we showed an 
improvement of about 0.5%, but a speedup of 
nearly 3x, despite loosening pruning parameters.  
Extraposition and long distance movement still 
pose a serious challenge to syntax-based machine 
translation systems. Most of the today's search 
algorithms assume phrasal cohesion. Even if our 
search algorithms could accommodate such 
movement, we don't have appropriate models to 
                                                           
8 We speculate that other systems using transducers with 
MLE probabilities may also benefit from additional 
reordering models. 
account for such phenomena. Our system already 
extracts extraposition templates, which are a step 
in the right direction, but may prove too sparse 
and brittle to account for the range of phenomena.  
References 
Chiang, David. A hierarchical phrase-based model for 
statistical machine translation. ACL 2005.  
Galley, Michel, Mark Hopkins, Kevin Knight, and Daniel 
Marcu. What?s in a translation rule? HLT-NAACL 2004. 
Graehl, Jonathan and Kevin Knight. Training Tree 
Transducers. NAACL 2004. 
Heidorn, George. ?Intelligent writing assistance?. In Dale et 
al. Handbook of Natural Language Processing, Marcel 
Dekker. (2000) 
Huang, Liang, Kevin Knight, and Aravind Joshi. Statistical 
Syntax-Directed Translation with Extended Domain of 
Locality. AMTA 2006 
Huang, Liang and David Chiang. Faster Algorithms for 
Decoding with Integrated Language Models.  ACL 2007 
(to appear) 
Koehn, Philipp, Franz Josef Och, and Daniel Marcu. 
Statistical phrase based translation. NAACL 2003. 
Koehn, Philipp and Christof Monz. Manual and automatic 
evaluation of machine translation between european 
languages. Workshop on Machine Translation, NAACL 
2006. 
Marcu, Daniel, Wei Wang, Abdessamad Echihabi, and Kevin 
Knight. SPMT: Statistical Machine Translation with 
Syntactified Target Language Phrases. EMNLP-2006. 
Menezes, Arul, Kristina Toutanova and Chris Quirk. 
Microsoft Research Treelet translation system: NAACL 
2006 Europarl evaluation. Workshop on Machine 
Translation, NAACL 2006 
Och, Franz Josef and Hermann Ney. A systematic 
comparison of various statistical alignment models, 
Computational Linguistics, 29(1):19-51 (2003).  
Och, Franz Josef. Minimum error rate training in statistical 
machine translation. ACL 2003. 
Och, Franz Josef and Hermann Ney: The Alignment 
Template Approach to Statistical Machine Translation. 
Computational Linguistics 30 (4): 417-449 (2004) 
Papineni, Kishore, Salim Roukos, Todd Ward, and Wei-Jing 
Zhu. BLEU: a method for automatic evaluation of 
machine translation. ACL 2002. 
Quirk, Chris, Arul Menezes, and Colin Cherry. Dependency 
Tree Translation: Syntactically informed phrasal SMT. 
ACL 2005 
Zens, Richard and Hermann Ney. Improvements in phrase-
based statistical machine translation. HLT-NAACL 2004  
8
Proceedings of the Second Workshop on Statistical Machine Translation, pages 112?119,
Prague, June 2007. c?2007 Association for Computational Linguistics
An Iteratively-Trained Segmentation-Free Phrase Translation Model
for Statistical Machine Translation
Robert C. Moore Chris Quirk
Microsoft Research
Redmond, WA 98052, USA
{bobmoore,chrisq}@microsoft.com
Abstract
Attempts to estimate phrase translation
probablities for statistical machine transla-
tion using iteratively-trained models have
repeatedly failed to produce translations as
good as those obtained by estimating phrase
translation probablities from surface statis-
tics of bilingual word alignments as de-
scribed by Koehn, et al (2003). We pro-
pose a new iteratively-trained phrase trans-
lation model that produces translations of
quality equal to or better than those pro-
duced by Koehn, et al?s model. Moreover,
with the new model, translation quality de-
grades much more slowly as pruning is tigh-
tend to reduce translation time.
1 Introduction
Estimates of conditional phrase translation probabil-
ities provide a major source of translation knowl-
edge in phrase-based statistical machine translation
(SMT) systems. The most widely used method for
estimating these probabilities is that of Koehn, et
al. (2003), in which phrase pairs are extracted from
word-aligned bilingual sentence pairs, and their
translation probabilities estimated heuristically from
surface statistics of the extracted phrase pairs. We
will refer to this approach as ?the standard model?.
There have been several attempts to estimate
phrase translation probabilities directly, using gen-
erative models trained iteratively on a parallel cor-
pus using the Expectation Maximization (EM) algo-
rithm. The first of these models, that of Marcu and
Wong (2002), was found by Koehn, et al (2003),
to produce translations not quite as good as their
method. Recently, Birch et al (2006) tried the
Marcu and Wong model constrained by a word
alignment and also found that Koehn, et al?s model
worked better, with the advantage of the standard
model increasing as more features were added to the
overall translation model. DeNero et al (2006) tried
a different generative phrase translation model anal-
ogous to IBM word-translation Model 3 (Brown et
al., 1993), and again found that the standard model
outperformed their generative model.
DeNero et al (2006) attribute the inferiority of
their model and the Marcu and Wong model to a hid-
den segmentation variable, which enables the EM
algorithm to maximize the probability of the train-
ing data without really improving the quality of the
model. We propose an iteratively-trained phrase
translation model that does not require different seg-
mentations to compete against one another, and we
show that this produces translations of quality equal
to or better than those produced by the standard
model. We find, moreover, that with the new model,
translation quality degrades much more slowly as
pruning is tightend to reduce translation time.
Decoding efficiency is usually considered only in
the design and implementation of decoding algo-
rithms, or the choice of model structures to support
faster decoding algorithms. We are not aware of any
attention previously having been paid to the effect of
different methods of parameter estimation on trans-
lation efficiency for a given model structure.
The time required for decoding is of great im-
portance in the practical application of SMT tech-
112
nology. One of the criticisms of SMT often made
by adherents of rule-based machine translation is
that SMT is too slow for practical application. The
rapidly falling price of computer hardware has ame-
liorated this problem to a great extent, but the fact re-
mains that every factor of 2 improvement in transla-
tion efficiency means a factor of 2 decrease in hard-
ware cost for intensive applications of SMT, such
as a web-based translation service (?Translate this
page?). SMT surely needs all the help in can get in
this regard.
2 Previous Approaches
Koehn, et al?s (2003) method of estimating phrase-
translation probabilities is very simple. They start
with an automatically word-aligned corpus of bilin-
gual sentence pairs, in which certain words are
linked, indicating that they are translations of each
other, or that they are parts of phrases that are trans-
lations of each other. They extract every possi-
ble phrase pair (up to a given length limit) that (a)
contains at least one pair of linked words, and (b)
does not contain any words that have links to other
words not included in the phrase pair.1 In other
words, word alignment links cannot cross phrase
pair boundaries. Phrase translation probabilities are
estimated simply by marginalizing the counts of
phrase instances:
p(x|y) =
C(x, y)
?
x? C(x?, y)
This method is used to estimate the conditional
probabilities of both target phrases give source
phrases and source phrases given target phrases.
In contrast to the standard model, DeNero, et al
(2006) estimate phrase translation probabilities ac-
cording to the following generative model:
1. Begin with a source sentence a.
2. Stochastically segment a into some number of
phrases.
3. For each selected phrase in a, stochastically
choose a phrase position in the target sentence
b that is being generated.
1This method of phrase pair extraction was originally de-
scribed by Och et al (1999).
4. For each selected phrase in a and the corre-
sponding phrase position in b, stochastically
choose a target phrase.
5. Read off the target sentence b from the se-
quence of target phrases.
DeNero et al?s analysis of why their model per-
forms relatively poorly hinges on the fact that the
segmentation probabilities used in step 2 are, in
fact, not trained, but simply assumed to be uniform.
Given complete freedom to select whatever segmen-
tation maximizes the likelihood of any given sen-
tence pair, EM tends to favor segmentations that
yield source phrases with as few occurrences as pos-
sible, since more of the associated conditional prob-
ability mass can be concentrated on the target phrase
alignments that are possible in the sentence at hand.
Thus EM tends to maximize the probability of the
training data by concentrating probability mass on
the rarest source phrases it can construct to cover
the training data. The resulting probability estimates
thus have less generalizability to unseen data than
if probability mass were concentrated on more fre-
quently occurring source phrases.
3 A Segmentation-Free Model
To avoid the problem identified by DeNero et al,
we propose an iteratively-trained model that does
not assume a segmentation of the training data into
non-overlapping phrase pairs. We refer to our model
as ?iteratively-trained? rather than ?generative? be-
cause we have not proved any of the mathematical
properties usually associated with generative mod-
els; e.g., that the training procedure maximizes the
likelihood of the training data. We will motivate
the model, however, with a generative story as to
how phrase alignments are produced, given a pair of
source and target sentences. Our model extends to
phrase alignment the concept of a sentence pair gen-
erating a word alignment developed by Cherry and
Lin (2003).
Our model is defined in terms of two stochastic
processes, selection and alignment, as follows:
1. For each word-aligned sentence pair, we iden-
tify all the possible phrase pair instances ac-
cording to the criteria used by Koehn et al
113
2. Each source phrase instance that is included in
any of the possible phrase pair instances inde-
pendently selects one of the target phrase in-
stances that it forms a possible phrase pair in-
stance with.
3. Each target phrase instance that is included in
any of the possible phrase pair instances inde-
pendently selects one of the source phrase in-
stances that it forms a possible phrase pair in-
stance with.
4. A source phrase instance is aligned to a target
phrase instance, if and only if each selects the
other.
Given a set of selection probability distributions
and a word-aligned parallel corpus, we can eas-
ily compute the expected number of alignment in-
stances for a given phrase pair type. The probability
of a pair of phrase instances x and y being aligned is
simply ps(x|y) ? ps(y|x), where ps is the applica-
ble selection probability distribution. The expected
number of instances of alignment, E(x, y), for the
pair of phrases x and y, is just the sum of the align-
ment probabilities of all the possible instances of
that phrase pair type.
From the expected number of alignments and the
total number of occurrences of each source and tar-
get phrase type in the corpus (whether or not they
particpate in possible phrase pairs), we estimate the
conditional phrase translation probabilities as
pt(y|x) =
E(x, y)
C(x)
, pt(x|y) =
E(x, y)
C(y)
,
where E denotes expected counts, and C denotes
observed counts.
The use of the total observed counts of particu-
lar source and target phrases (instead of marginal-
ized expected joint counts) in estimating the condi-
tional phrase translation probabilities, together with
the multiplication of selection probabilities in com-
puting the alignment probability of particular phrase
pair instances, causes the conditional phrase transla-
tion probability distributions generally to sum to less
than 1.0. We interpret the missing probability mass
as the probability that a given word sequence does
not translate as any contiguous word sequence in the
other language.
We have seen how to derive phrase translation
probabilities from the selection probabilities, but
where do the latter come from? We answer this
question by adding the following constraint to the
model:
The probabilty of a phrase y selecting a
phrase x is proportional to the probability
of x translating as y, normalized over the
possible non-null choices for x presented
by the word-aligned sentence pair.
Symbolically, we can express this as
ps(x|y) =
pt(y|x)
?
x? pt(y|x?)
where ps denotes selection probability, pt denotes
translation probability, and x? ranges over the phrase
instances that could possibly align to y. We are, in
effect, inverting and renormalizing translation prob-
abilities to get selection probabilities. The reason
for the inversion may not be immediately apparent,
but it in fact simply generalizes the e-step formula
in the EM training for IBM Model 1 from words to
phrases.
This model immediately suggests (and, in fact,
was designed to suggest) the following EM-like
training procedure:
1. Initialize the translation probability distribu-
tions to be uniform. (It doesn?t matter at this
point whether the possibility of no translation
is included or not.)
2. E step: Compute the expected phrase alignment
counts according to the model, deriving the se-
lection probabilities from the current estimates
of the translation probabilities as described.
3. M step: Re-estimate the phrase translation
probabilities according to the expected phrase
alignment counts as described.
4. Repeat the E and M steps, until the desired de-
gree of convergence is obtained.
We view this training procedure as iteratively try-
ing to find a set of phrase translation probabilities
that satisfies all the constraints of the model, al-
though we have not proved that this training proce-
dure always converges. We also have not proved that
114
the procedure maximizes the likelihood of anything,
although we find empirically that each iteration de-
creases the conditional entropy of the phrase trans-
lation model. In any case, the training procedure
seems to work well in practice. It is also very simi-
lar to the joint training procedure for HMM word-
alignment models in both directions described by
Liang et al (2006), which was the original inspira-
tion for our training procedure.
4 Experimental Set-Up and Data
We evaluated our phrase translation model com-
pared to the standard model of Koehn et al in the
context of a fairly typical end-to-end phrase-based
SMT system. The overall translation model score
consists of a weighted sum of the following eight ag-
gregated feature values for each translation hypoth-
esis:
? the sum of the log probabilities of each source
phrase in the hypothesis given the correspond-
ing target phrase, computed either by our
model or the standard model,
? the sum of the log probabilities of each tar-
get phrase in the hypothesis given the corre-
sponding source phrase, computed either by
our model or the standard model,
? the sum of lexical scores for each source phrase
given the corresponding target phrase,
? the sum of lexical scores for each target phrase
given the corresponding source phrase,
? the log of the target language model probability
for the sequence of target phrases in the hypoth-
esis,
? the total number of words in the target phrases
in the hypothesis,
? the total number of source/target phrase pairs
composing the hypothesis,
? the distortion penalty as implemented in the
Pharaoh decoder (Koehn, 2003).
The lexical scores are computed as the (unnor-
malized) log probability of the Viterbi alignment for
a phrase pair under IBM word-translation Model 1
(Brown et al, 1993). The feature weights for the
overall translation models were trained using Och?s
(2003) minimum-error-rate training procedure. The
weights were optimized separately for our model
and for the standard phrase translation model. Our
decoder is a reimplementation in Perl of the algo-
rithm used by the Pharaoh decoder as described by
Koehn (2003).2
The data we used comes from an English-French
bilingual corpus of Canadian Hansards parliamen-
tary proceedings supplied for the bilingual word
alignment workshop held at HLT-NAACL 2003
(Mihalcea and Pedersen, 2003). Automatic sentence
alignment of this data was provided by Ulrich Ger-
mann. We used 500,000 sentences pairs from this
corpus for training both the phrase translation mod-
els and IBM Model 1 lexical scores. These 500,000
sentence pairs were word-aligned using a state-of-
the-art word-alignment method (Moore et al, 2006).
A separate set of 500 sentence pairs was used to train
the translation model weights, and two additional
held-out sets of 2000 sentence pairs each were used
as test data.
The two phrase translation models were trained
using the same set of possible phrase pairs extracted
from the word-aligned 500,000 sentence pair cor-
pus, finding all possible phrase pairs permitted by
the criteria followed by Koehn et al, up to a phrase
length of seven words. This produced approximately
69 million distinct phrase pair types. No pruning of
the set of possible phrase pairs was done during or
before training the phrase translation models. Our
phrase translation model and IBM Model 1 were
both trained for five iterations. The training pro-
cedure for our phrase translation model trains mod-
els in both directions simultaneously, but for IBM
Model 1, models were trained separately in each di-
rection. The models were then pruned to include
only phrase pairs that matched the source sides of
the small training and test sets.
5 Entropy Measurements
To verify that our iterative training procedure was
behaving as expected, after each training iteration
2Since Perl is a byte-code interpreted language, absolute de-
coding times will be slower than with the standard machine-
language-compiled implementation of Pharaoh, but relative
times between models should be comparable.
115
we measured the conditional entropy of the model
in predicting English phrases given French phrases,
according to the formula
H(E|F ) =
?
f
p(f)
?
e
pt(e|f) log2 pt(e|f),
where e and f range over the English and French
phrases that occur in the extracted phrase pairs, and
p(f) was estimated according to the relative fre-
quency of these French phrases in a 2000 sentence
sample of the French sentences from the 500,000
word-aligned sentence pairs. Over the five train-
ing iterations, we obtained a monotonically decreas-
ing sequence of entropy measurements in bits per
phrase: 1.329, 1.177, 1.146, 1.140, 1.136.
We also compared the conditional entropy of the
standard model to the final iteration of our model,
estimating p(f) using the first of our 2000 sentence
pair test sets. For this data, our model measured 1.38
bits per phrase, and the standard model measured
4.30 bits per phrase. DeNero et al obtained corre-
sponding measurements of 1.55 bits per phrase and
3.76 bits per phrase, for their model and the stan-
dard model, using a different data set and a slightly
different estimation method.
6 Translation Experiments
We wanted to look at the trade-off between decod-
ing time and translation quality for our new phrase
translation model compared to the standard model.
Since this trade-off is also affected by the settings of
various pruning parameters, we compared decoding
time and translation quality, as measured by BLEU
score (Papineni et al 2002), for the two models on
our first test set over a broad range of settings for the
decoder pruning parameters.
The Pharaoh decoding algorithm, has five pruning
parameters that affect decoding time:
? Distortion limit
? Translation table limit
? Translation table threshold
? Beam limit
? Beam threshold
The distortion limit is the maximum distance al-
lowed between two source phrases that produce ad-
jacent target phrases in the decoder output. The dis-
tortion limit can be viewed as a model parameter,
as well as a pruning paramter, because setting it to
an optimum value usually improves translation qual-
ity over leaving it unrestricted. We carried out ex-
periments with the distortion limit set to 1, which
seemed to produce the highest BLEU scores on our
data set with the standard model, and also set to 5,
which is perhaps a more typical value for phrase-
based SMT systems. Translation model weights
were trained separately for these two settings, be-
cause the greater the distortion limit, the higher the
distortion penalty weight needed for optimal trans-
lation quality.
The translation table limit and translation table
threshold are applied statically to the phrase trans-
lation table, which combines all components of the
overall translation model score that can be com-
puted for each phrase pair in isolation. This in-
cludes all information except the distortion penalty
score and the part of the language model score that
looks at n-grams that cross target phrase boundaries.
The translation table limit is the maximum number
of translations allowed in the table for any given
source phrase. The translation table threshold is
the maximum difference in combined translation ta-
ble score allowed between the highest scoring trans-
lation and lowest scoring translation for any given
source phrase. The beam limit and beam threshold
are defined similarly, but they apply dynamically to
the sets of competing partial hypotheses that cover
the same number of source words in the beam search
for the highest scoring translation.
For each of the two distortion limits we tried, we
carried out a systematic search for combinations of
settings of the other four pruning parameters that
gave the best trade-offs between decoding time and
BLEU score. Starting at a setting of 0.5 for the
threshold parameters3 and 5 for the limit parameters
we performed a hill-climbing search over step-wise
relaxations of all combinations of the four parame-
3We use difference in weighted linear scores directly for
our pruning thresholds, whereas the standard implementation of
Pharaoh expresses these as probability ratios. Hence the specific
values for these parameters are not comparable to published de-
scriptions of experiments using Pharaoh, although the effects of
pruning are exactly the same.
116
30.2
30.3
30.4
30.5
0.1 1 10 100
B
LE
U
[%
]
milliseconds per word
Figure 1: BLEU vs Decoding Time (DL = 1)
re-estimated phrase table
standard phrase table
re-estimated phrase table 
convex hull
standard phrase table convex 
hull
29.4
29.6
29.8
30
30.2
30.4
30.6
1 10 100 1000
B
LE
U
[%
]
milliseconds per word
Figure 2: BLEU vs Decoding Time (DL = 5)
re-estimated phrase table
standard phrase table
re-estimated phrase table 
convex hull
standard phrase table convex 
hull
ters, incrementing the threshold parameters by 0.5
and the limit parameters by 5 at each step. For each
resulting point that provided the best BLEU score yet
seen for the amount of decoding time used, we iter-
ated the search.
The resulting possible combinations of BLEU
score and decoding time for the two phrase trans-
lation models are displayed in Figure 1, for a distor-
tion limit of 1, and Figure 2, for a distortion limit
of 5. BLEU score is reported on a scale of 1?100
(BLEU[%]), and decoding time is measured in mil-
liseconds per word. Note that the decoding time axis
is presented on a log scale.
The points that represent pruning parameter set-
tings one might consider using in a practical system
are those on or near the upper convex hull of the
set of points for each model. These upper-convex-
hull points are highlighted in the figures. Points far
from these boundaries represent settings of one or
more of the parameters that are too restrictive to ob-
tain good translation quality, together with settings
of other parameters that are too permissive to obtain
good translation time.
Examining the results for a distortion limit of
1, we found that the BLEU score obtained with
the loosest pruning parameter settings (2.5 for both
threshold paramters, and 25 for both limit parame-
ters) were essentially identical for the two mod-
els: 30.42 BLEU[%]. As the pruning parameters
are tightened to reduce decoding time, however,
the new model performs much better. At a decod-
ing time almost 6 times faster than for the settings
that produced the highest BLEU score, the change
in score was only ?0.07 BLEU[%] with the new
model. To obtain a slightly worse4 BLEU score
(?0.08 BLEU[%]) using the standard model took
90% more decoding time.
It does appear, however, that the best BLEU score
for the standard model is slightly better than the best
BLEU score for the new model: 30.43 vs. 30.42.
It is in fact currious that there seem to be numer-
ous points where the standard model gets a slightly
4Points on the convex hulls with exactly comparable BLEU
scores do not often occur.
117
better BLEU score than it does with with the loos-
est pruning settings, which should have the lowest
search error.
We conjectured that this might be an artifact of
our test procedure. If a model is at all reasonable,
most search errors will reduce the ultimate objec-
tive function, in our case the BLEU score, but oc-
casionally a search error will increase the objective
function just by chance. The smaller the number of
search errors in a particular test, the greater the like-
lihood that, by chance, more search errors will in-
crease the objective function than decrease it. Since
we are sampling a fairly large number of combi-
nations of pruning parameter settings (179 for the
standard model with a distortion limit of 1), it is
possible that a small number of these have more
?good? search errors than ?bad? search errors sim-
ply by chance, and that this accounts for the small
number of points (13) at which the BLEU score ex-
ceeds that of the point which should have the fewest
search errors. This effect may be more pronounced
with the standard model than with the new model,
simply because there is more noise in the standard
model.
To test the hypothesis that the BLEU scores
greater than the score for the loosest pruning set-
tings simply represent noise in the data, we col-
lected all the pruning settings that produced BLEU
scores greater than or equal to the the one for the
loosest pruning settings, and evaluated the standard
model at those settings on our second held-out test
set. We then looked at the correlation between the
BLEU scores for these settings on the two test sets,
and found that it was very small and negative, with
r = ?0.099. The standard F-test for the significance
of a correlation yielded p = 0.74; in other words,
completely insignificant. This strongly suggests that
the apparent improvement in BLEU score for certain
tighter pruning settings is illusory.
As a sanity check, we tested the BLEU score cor-
relation between the two test sets for the points on
the upper convex hull of the plot for the standard
model, between the point with the fastest decod-
ing time and the point with the highest BLEU score.
That correlation was very high, with r = 0.94,
which was significant at the level p = 0.0004 ac-
cording to the F-test. Thus the BLEU score differ-
ences along most of the upper convex hull seem to
reflect reality, but not in the region where they equal
or exceed the score for the loosest pruning settings.
At a distortion limit of 5, there seems no question
that the new model performs better than the standard
model. The difference BLEU scores for the upper-
convex-hull points ranges from about 0.8 to 0.2
BLEU[%] for comparable decoding times. Again,
the advantage of the new model is greater at shorter
decoding times. Compared to the results with a dis-
tortion limit of 1, the standard model loses transla-
tion quality, with a change of about ?0.2 BLEU[%]
for the loosest pruning settings, while the new model
gains very slightly (+0.04 BLEU[%]).
7 Conclusions
This study seems to confirm DeNero et al?s diagno-
sis that the main reason for poor performance of pre-
vious iteratively-trained phrase translation models,
compared to Koehn et al?s model, is the effect of the
hidden segmentation variable in these models. We
have developed an iteratively-trained phrase transla-
tion model that is segmentation free, and shown that,
at a minimum, it eliminates the shortfall in BLEU
score compared to the standard model. With a larger
distortion limit, the new model produced transla-
tions with a noticably better BLEU score.
From a practical point of view, the main result
is probably that BLEU score degrades much more
slowly with our model than with the standard model,
when the decoding search is tuned for speed. For
some settings that appear reasonable, this difference
is close to a factor of 2, even if there is no differ-
ence in the translation quality obtainable when prun-
ing is loosened. For high-demand applications like
web page translation, roughly half of the investment
in translation servers could be saved while provid-
ing this level of translation quality with the same re-
sponse time.
Acknowledgement
The authors would like to thank Mark Johnson for
many valuable discussions of how to analyze and
present the results obtained in this study.
References
Alexandra Birch, Chris Callison-Burch, Miles Os-
borne, and Philipp Koehn. 2006. Constrain-
118
ing the Phrase-Based, Joint Probability Statistical
Translation Model. In Proceedings of the HLT-
NAACL 06 Workshop, Statistical Machine Trans-
lation, pp. 154?157, New York City, New York,
USA.
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. 1993. The
Mathematics of Statistical Machine Translation:
Parameter Estimation. Computational Linguis-
tics, 19(2):263?311.
Colin Cherry and Dekang Lin. 2003. A Probabil-
ity Model to Improve Word Alignment. In Pro-
ceedings of the 41st Annual Meeting of the ACL,
pp. 88?95, Sapporo, Japan.
John DeNero, Dan Gillick, James Zhang, and Dan
Klein. 2006. Why Generative Phrase Models
Underperform Surface Heuristics. In Proceed-
ings of the HLT-NAACL 06 Workshop, Statistical
Machine Translation, pp. 31?38, New York City,
New York, USA.
Philipp Koehn. 2003. Noun Phrase Translation.
PhD Dissertation, Computer Science, University
of Southern California, Los Angeles, California,
USA.
Philipp Koehn, Franz Joseph Och, and Daniel
Marcu. 2003. Statistical Phrase-Based Trans-
lation. In Proceedings of the Human Language
Technology Conference of the North American
Chapter of the Association for Computational
Linguistics, pp. 127?133, Edmonton, Alberta,
Canada.
Percy Liang, Ben Taskar, and Dan Klein. 2006.
Alignment by Agreement. In Proceedings of
the Human Language Technology Conference of
the North American Chapter of the Association
for Computational Linguistics, pp. 104?111, New
York City, New York, USA.
Daniel Marcu and William Wong. 2002. A Phrase-
Based, Joint Probability Model for Statistical Ma-
chine Translation. In Proceedings of the 2002
Conference on Empirical Methods in Natural
Language Processing, pp. 133?139, Philadelphia,
Pennsylvania, USA.
Rada Mihalcea and Ted Pedersen. 2003. An Evalu-
ation Exercise for Word Alignment. In Proceed-
ings of the HLT-NAACL 2003 Workshop, Building
and Using Parallel Texts: Data Driven Machine
Translation and Beyond, pp. 1?6, Edmonton, Al-
berta, Canada.
Robert C. Moore, Wen-tau Yih, and Andreas Bode.
2006. Improved Discriminative Bilingual Word
Alignment. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics
and 44th Annual Meeting of the Association for
Computational Linguistics, pp. 513-520, Sydney,
Australia.
Franz Joseph Och, Christoff Tillmann, and Hermann
Ney. 1999. Improved Alignment Models for Sta-
tistical Machine Translation. In Proceedings of
the 1999 Joint SIGDAT Conference on Empiri-
cal Methods in Natural Language Processing and
Very Large Corpora, pp. 20?28, College Park,
Maryland, USA.
Franz Joseph Och. 2003. Minimum Error Rate
Training in Statistical Machine Translation. In
Proceedings of the 41st Annual Meeting of the
ACL, pp. 160?167, Sapporo, Japan.
Kishore Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2002. BLEU: a Method for Auto-
matic Evaluation of Machine Translation. In Pro-
ceedings of the 40th Annual Meeting of the Asso-
ciation for Computational Linguistics, pp. 311?
318, Philadelphia, Pennsylvania, USA.
119
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 358?366,
Beijing, August 2010
A Large Scale Ranker-Based System  
for Search Query Spelling Correction 
 
Jianfeng Gao 
Microsoft Research, Redmond 
jfgao@microsoft.com 
Xiaolong Li 
Microsoft Corporation 
xiaolong.li@microsoft.com 
Daniel Micol 
Microsoft Corporation 
danielmi@microsoft.com 
Chris Quirk 
Microsoft Research, Redmond 
chrisq@microsoft.com 
Xu Sun 
University of Tokyo 
xusun@mist.i.u-tokyo.ac.jp 
 
 
Abstract 
This paper makes three significant extensions to a 
noisy channel speller designed for standard writ-
ten text to target the challenging domain of search 
queries. First, the noisy channel model is sub-
sumed by a more general ranker, which allows a 
variety of features to be easily incorporated. Se-
cond, a distributed infrastructure is proposed for 
training and applying Web scale n-gram language 
models. Third, a new phrase-based error model is 
presented. This model places a probability distri-
bution over transformations between multi-word 
phrases, and is estimated using large amounts of 
query-correction pairs derived from search logs. 
Experiments show that each of these extensions 
leads to significant improvements over the state-
of-the-art baseline methods. 
1 Introduction 
Search queries present a particular challenge for 
traditional spelling correction methods. New 
search queries emerge constantly. As a result, 
many queries contain valid search terms, such as 
proper nouns and names, which are not well es-
tablished in the language. Therefore, recent re-
search has focused on the use of Web corpora 
and search logs, rather than human-compiled lex-
icons, to infer knowledge about spellings and 
word usages in search queries (e.g., Whitelaw et 
al., 2009; Cucerzan and Brill, 2004).  
The spelling correction problem is typically 
formulated under the framework of the noisy 
channel model. Given an input query   
       , we want to find the best spelling correc-
tion           among all candidates: 
         
 
       (1) 
Applying Bayes' Rule, we have 
         
 
           (2) 
where the error model        models the trans-
formation probability from C to Q, and the lan-
guage model (LM)      models the likelihood 
that C is a correctly spelled query. 
This paper extends a noisy channel speller de-
signed for regular text to search queries in three 
ways: using a ranker (Section 3), using Web scale 
LMs (Section 4), and using phrase-based error 
models (Section 5). 
First of all, we propose a ranker-based speller 
that covers the noisy channel model as a special 
case. Given an input query, the system first gen-
erates a short list of candidate corrections using 
the noisy channel model. Then a feature vector is 
computed for each query and candidate correc-
tion pair. Finally, a ranker maps the feature vec-
tor to a real-valued score, indicating the likeli-
hood that this candidate is a desirable correction. 
We will demonstrate that ranking provides a flex-
ible modeling framework for incorporating a 
wide variety of features that would be difficult to 
model under the noisy channel framework. 
Second, we explore the use of Web scale LMs 
for query spelling correction. While traditional 
LM research focuses on how to make the model 
?smarter? via how to better estimate the probabil-
ity of unseen words (Chen and Goodman, 1999); 
and how to model the grammatical structure of 
language (e.g., Charniak, 2001), recent studies 
show that significant improvements can be 
achieved using ?stupid? n-gram models trained 
on very large corpora (e.g., Brants et al, 2007). 
We adopt the latter strategy in this study. We pre-
sent a distributed infrastructure to efficiently train 
and apply Web scale LMs. In addition, we ob-
serve that search queries are composed in a lan-
guage style different from that of regular text. We 
thus train multiple LMs using different texts as-
sociated with Web corpora and search queries. 
Third, we propose a phrase-based error model 
that captures the probability of transforming one 
358
multi-term phrase into another multi-term phrase. 
Compared to traditional error models that account 
for transformation probabilities between single 
characters or substrings (e.g., Kernighan et al, 
1990; Brill and Moore, 2000), the phrase-based 
error model is more effective in that it captures 
inter-term dependencies crucial for correcting 
real-word errors, prevalent in search queries. We 
also present a novel method of extracting large 
amounts of query-correction pairs from search 
logs. These pairs, implicitly judged by millions of 
users, are used for training the error models. 
Experiments show that each of the extensions 
leads to significant improvements over its base-
line methods that were state-of-the-art until this 
work, and that the combined method yields a sys-
tem which outperforms the noisy channel speller 
by a large margin: a 6.3% increase in accuracy on 
a human-labeled query set. 
2 Related Work 
Prior research on spelling correction for regular 
text can be grouped into two categories: correct-
ing non-word errors and real-word errors. The 
former focuses on the development of error mod-
els based on different edit distance functions (e.g., 
Kucich, 1992; Kernighan et al, 1990; Brill and 
Moore, 2000; Toutanova and Moore, 2002). Brill 
and Moore?s substring-based error model, con-
sidered to be state-of-the-art among these models, 
acts as the baseline against which we compare 
our models. On the other hand, real-word spelling 
correction tries to detect incorrect usages of a 
valid word based on its context, such as "peace" 
and "piece" in the context "a _ of cake". N-gram 
LMs and na?ve Bayes classifiers are commonly 
used models (e.g., Golding and Roth, 1996; 
Mangu and Brill, 1997; Church et al, 2007). 
While almost all of the spellers mentioned 
above are based on a pre-defined dictionary (ei-
ther a lexicon against which the edit distance is 
computed, or a set of real-word confusion pairs), 
recent research on query spelling correction fo-
cuses on exploiting noisy Web corpora and query 
logs to infer knowledge about spellings and word 
usag in queries (Cucerzan and Brill 2004; Ahmad 
and Kondrak, 2005; Li et al, 2006; Whitelaw et 
al., 2009).  Like those spellers designed for regu-
lar text, most of these query spelling systems are 
also based on the noisy channel framework. 
3 A Ranker-Based Speller 
The noisy channel model of Equation (2) does 
not have the flexibility to incorporate a wide va-
riety of features useful for spelling correction, 
e.g., whether a candidate appears as a Wikipedia 
document title. We thus generalize the speller to 
a ranker-based system. Let f be a feature vector 
of a query and candidate correction pair (Q, C). 
The ranker maps f to a real value y that indicates 
how likely C is a desired correction. For example, 
a linear ranker maps f to y with a weight vector w 
such as      , where w is optimized for accu-
racy on human-labeled       pairs. Since the 
logarithms of the LM and error model probabili-
ties can be included as features, the ranker covers 
the noisy channel model as a special case. 
For efficiency, our speller operates in two dis-
tinct stages: candidate generation and re-ranking. 
In candidate generation, an input query is first 
tokenized into a sequence of terms. For each term 
q, we consult a lexicon to identify a list of 
spelling suggestions c whose edit distance from q 
is lower than some threshold. Our lexicon con-
tains around 430,000 high frequency query uni-
gram and bigrams collected from 1 year of query 
logs. These suggestions are stored in a lattice.  
We then use a decoder to identify the 20-best 
candidates from the lattice according to Equation 
(2), where the LM is a backoff bigram model 
trained on 1 year of query logs, and the error 
model is approximated by weighted edit distance:  
                         (3) 
The decoder uses a standard two-pass algorithm. 
The first pass uses the Viterbi algorithm to find 
the best C according to the model of Equations 
(2) and (3).  The second pass uses the A-star al-
gorithm to find the 20-best corrections, using the 
Viterbi scores computed at each state in the first 
pass as heuristics. 
The core component in the second stage is a 
ranker, which re-ranks the 20-best candidate cor-
rections using a set of features extracted from 
     . If the top C after re-ranking is different 
from Q, C is proposed as the correction. We use 
96 features in this study. In addition to the two 
features derived from the noisy channel model, 
the rest of the features can be grouped into the 
following 5 categories. 
1. Surface-form similarity features, which 
check whether C and Q differ in certain patterns, 
359
e.g., whether C is transformed from Q by adding 
an apostrophe, or by adding a stop word at the 
beginning or end of Q. 
2. Phonetic-form similarity features, which 
check whether the edit distance between the met-
aphones (Philips, 1990) of a query term and its 
correction candidate is below some thresholds. 
3. Entity features, which check whether the 
original query is likely to be a proper noun based 
on an in-house named entity recognizer. 
4. Dictionary features, which check whether 
a query term or a candidate correction are in one 
or more human-compiled dictionaries, such as the 
extracted Wiki, MSDN, and ODP dictionaries. 
5. Frequency features, which check whether 
the frequency of a query term or a candidate cor-
rection is above certain thresholds in different 
datasets, such as query logs and Web documents. 
4 Web Scale Language Models 
An n-gram LM assigns a probability to a word 
string   
            according to  
    
   ? (  |  
   )
 
   
 ? (  |      
   )
 
   
 (4) 
where the approximation is based on a Markov 
assumption that each word depends only upon the 
immediately preceding n-1 words. In a speller, 
the log of n-gram LM probabilities of an original 
query and its candidate corrections are used as 
features in the ranker.  
While recent research reports the benefits of 
large LMs trained on Web corpora on a variety of 
applications (e.g. Zhang et al, 2006; Brants et al, 
2007), it is also clear that search queries are com-
posed in a language style different from that of 
the body or title of a Web document. Thus, in this 
study we developed a set of large LMs from dif-
ferent text streams of Web documents and query 
logs. Below, we first describe the n-gram LM 
collection used in this study, and then present a 
distributed n-gram LM platform based on which 
these LMs are built and served for the speller. 
4.1 Web Scale Language Models 
Table 1 summarizes the data sets and Web scale 
n-gram LMs used in this study. The collection is 
built from high quality English Web documents 
containing trillions of tokens, served by a popular 
commercial search engine. The collection con-
sists of several data sets built from different Web 
sources, including the different text fields from 
the Web documents (i.e., body, title, and anchor 
texts) and search query logs. The raw texts ex-
tracted from these different sources were pre- 
processed in the following manner: texts are to-
kenized based on white-space and upper case let-
ters are converted to lower case. Numbers are 
retained, and no stemming/inflection is per-
formed. The n-gram LMs are word-based backoff 
models, where the n-gram probabilities are esti-
mated using Maximum Likelihood Estimation 
with smoothing. Specifically, for a trigram mod-
el, the smoothed probability is computed as 
                (5) 
{
               (             )
           
                   
                              
 
where      is the count of the n-gram in the train-
ing corpus and   is a normalization factor.      
is a discount function for smoothing. We use 
modified absolute discounting (Gao et al, 2001), 
whose parameters can be efficiently estimated 
and performance converges to that of more elabo-
rate state-of-the-art techniques like Kneser-Ney 
smoothing in large data (Nguyen et al 2007).  
4.2 Distributed N-gram LM Platform 
The platform is developed on a distributed com-
puting system designed for storing and analyzing 
massive data sets, running on large clusters con-
sisting of hundreds of commodity servers con-
nected via high-bandwidth network.  
We use the SCOPE (Structured Computations 
Optimized for Parallel Execution) programming 
model (Chaiken et al, 2008) to train the Web 
scale n-gram LMs shown in Table 1. The SCOPE 
scripting language resembles SQL which many 
programmers are familiar with. It also supports 
Dataset Body Anchor Title Query 
Total tokens 1.3T 11.0B 257.2B 28.1B 
Unigrams 1.2B 60.3M 150M 251.5M 
Bigrams 11.7B 464.1M 1.1B 1.3B 
Trigrams 60.0B 1.4B 3.1B 3.1B 
4-grams 148.5B 2.3B 5.1B 4.6B 
Size on disk# 12.8TB 183GB 395GB 393GB 
# N-gram entries as well as other model parameters are 
stored. 
Table 1: Statistics of the Web n-gram LMs collection (count 
cutoff = 0 for all models). These models will be accessible at 
Microsoft (2010). 
360
C# expressions so that users can easily plug-in 
customized C# classes. SCOPE supports writing 
a program using a series of simple data transfor-
mations so that users can simply write a script to 
process data in a serial manner without wonder-
ing how to achieve parallelism while the SCOPE 
compiler and optimizer are responsible for trans-
lating the script into an efficient, parallel execu-
tion plan. We illustrate the usage of SCOPE for 
building LMs using the following example of 
counting 5-grams from the body text of English 
Web pages. The flowchart is shown in Figure 1.  
The program is written in SCOPE as a step-
by- step of computation, where a command takes 
the output of the previous command as its input. 
ParsedDoc=SELECT docId, TokenizedDoc 
FROM @?/shares/?/EN_Body.txt? 
USING DefaultTextExtractor; 
NGram=PROCESS ParsedDoc 
PRODUCE NGram, NGcount 
USING NGramCountProcessor(-stream       
TokenizedDoc -order 5 ?bufferSize 
20000000); 
NGramCount=REDUCE NGram 
ON NGram 
PRODUCE NGram, NGcount 
USING NGramCountReducer; 
 
OUTPUT TO @?Body-5-gram-count.txt?; 
The first SCOPE command is a SELECT 
statement that extracts parsed Wed body text. The 
second command uses a build-in Processor 
(NGramCountProcessor) to map the parsed doc-
uments into separate n-grams together with their 
counts. It generates a local hash at each node 
(i.e., a core in a multi-core server) to store the (n-
gram, count) pairs. The third command (RE-
DUCE) aggregates counts from different nodes 
according to the key (n-gram string). The final 
command (OUTPUT) writes out the resulting to a 
data file. 
The smoothing method can be implemented 
similarly by the customized smoothing Proces-
sor/Reducer. They can be imported from the ex-
isting C# codes (e.g., developed for building LMs 
in a single machine) with minor changes.  
It is straightforward to apply the built LMs for 
the ranker in the speller. The n-gram platform 
provides a DLL for n-gram batch lookup. In the 
server, an n-gram LM is stored in the form of 
multiple lists of key-value pairs, where the key is 
the hash of an n-gram string and the value is ei-
ther the n-gram probability or backoff parameter.  
5 Phrase-Based Error Models 
The goal of an error model is to transform a cor-
rectly spelled query C into a misspelled query Q. 
Rather than replacing single words in isolation, 
the phrase-based error model replaces sequences 
of words with sequences of words, thus incorpo-
rating contextual information. The training pro-
cedure closely follows Sun et al (2010). For in-
stance, we might learn that ?theme part? can be 
replaced by ?theme park? with relatively high 
probability, even though ?part? is not a mis-
spelled word. We use this generative story: first 
the correctly spelled query C is broken into K 
non-empty word sequences c1, ?, ck, then each is 
replaced with a new non-empty word sequence 
q1, ?, qk, finally these phrases are permuted and 
concatenated to form the misspelled Q. Here, c 
and q denote consecutive sequences of words. 
To formalize this generative process, let S de-
note the segmentation of C into K phrases c1?cK, 
and let T denote the K replacement phrases 
q1?qK ? we refer to these (ci, qi) pairs as bi-
phrases. Finally, let M denote a permutation of K 
elements representing the final reordering step. 
Figure 2 demonstrates the generative procedure. 
Next let us place a probability distribution 
over rewrite pairs. Let B(C, Q) denote the set of S, 
T, M triples that transform C into Q. Assuming a 
uniform probability over segmentations, the 
phrase-based probability can be defined as: 
Recursive 
Reducer
Node 1 Node 2 Node N?...
?...
Output
Web Pages
Parsing
Counting
Local 
Hash
Tokenize
Web Pages
Parsing
Counting
Local 
Hash
Tokenize
Web Pages
Parsing
Counting
Local 
Hash
Tokenize
 
Figure 1. Distributed 5-gram counting. 
C: ?disney theme park? correct query 
S: [?disney?, ?theme park?] segmentation 
T: [?disnee?, ?theme part?] translation 
M: (1 ? 2, 2? 1) permutation 
Q: ?theme part disnee? misspelled query 
Figure 2: Example demonstrating the generative procedure 
behind the phrase-based error model. 
361
       ?                    
            
 (6) 
As is common practice in SMT, we use the max-
imum approximation to the sum:  
          
            
                    (7) 
5.1 Forced Alignments 
Although we have defined a generative model for 
transforming queries, our goal is not to propose 
new queries, but rather to provide scores over 
existing Q and C pairs that will act as features for 
the ranker. Furthermore, the word-level align-
ments between Q and C can most often be identi-
fied with little ambiguity. Thus we restrict our 
attention to those phrase transformations con-
sistent with a good word-level alignment. 
Let J be the length of Q, L be the length of C, 
and A = a1?aJ  be a hidden variable representing 
the word alignment between them. Each ai takes 
on a value ranging from 1 to L indicating its cor-
responding word position in C, or 0 if the ith 
word in Q is unaligned. The cost of assigning k 
to ai is equal to the Levenshtein edit distance 
(Levenshtein, 1966) between the ith word in Q 
and the kth word in C, and the cost of assigning 0 
to ai is equal to the length of the i
th word in Q. 
The least cost alignment A* between Q and C is 
computed efficiently using the A-star algorithm. 
When scoring a given candidate pair, we fur-
ther restrict our attention to those S, T, M triples 
that are consistent with the word alignment, 
which we denote as B(C, Q, A*). Here, consisten-
cy requires that if two words are aligned in A*, 
then they must appear in the same bi-phrase (ci, 
qi). Once the word alignment is fixed, the final 
permutation is uniquely determined, so we can 
safely discard that factor. Thus we have: 
          
       
       
         (8) 
For the sole remaining factor P(T|C, S), we 
make the assumption that a segmented query T = 
q1? qK is generated from left to right by trans-
forming each phrase c1?cK independently: 
         ?         
 
   , (9) 
where          is a phrase transformation prob-
ability, the estimation of which will be described 
in Section 5.2.  
To find the maximum probability assignment 
efficiently, we use a dynamic programming ap-
proach, similar to the monotone decoding algo-
rithm described in Och (2002).  
5.2 Training the Error Model  
Given a set of (Q, C) pairs as training data, we 
follow a method commonly used in SMT (Och 
and Ney, 2004) to extract bi- phrases and esti-
mate their replacement probabilities. A detailed 
description is discussed in Sun et al (2010). 
We now describe how (Q, C) pairs are gener-
ated automatically from massive query reformu-
lation sessions of a commercial Web browser. 
A query reformulation session contains a list 
of URLs that record user behaviors that relate to 
the query reformulation functions, provided by a 
Web search engine. For example, most commer-
cial search engines offer the "did you mean" 
function, suggesting a possible alternate interpre-
tation or spelling of a user-issued query. Figure 3 
shows a sample of the query reformulation ses-
sions that record the "did you mean" sessions 
from three of the most popular search engines. 
These sessions encode the same user behavior: A 
user first queries for "harrypotter sheme part", 
Google: 
http://www.google.com/search? 
hl=en&source=hp& 
q=harrypotter+sheme+part&aq=f&oq=&aqi= 
http://www.google.com/search? 
hl=en&ei=rnNAS8-oKsWe_AaB2eHlCA& 
sa=X&oi=spell&resnum=0&ct= 
result&cd=1&ved=0CA4QBSgA& 
q=harry+potter+theme+park&spell=1 
Yahoo: 
http://search.yahoo.com/search; 
_ylt=A0geu6ywckBL_XIBSDtXNyoA? 
p=harrypotter+sheme+part& 
fr2=sb-top&fr=yfp-t-701&sao=1 
http://search.yahoo.com/search? 
ei=UTF-8&fr=yfp-t-701& 
p=harry+potter+theme+park 
&SpellState=n-2672070758_q-tsI55N6srhZa. 
qORA0MuawAAAA%40%40&fr2=sp-top 
Bing: 
http://www.bing.com/search? 
q=harrypotter+sheme+part&form=QBRE&qs=n 
http://www.bing.com/search? 
q=harry+potter+theme+park&FORM=SSRE 
Figure 3.  A sample of query reformulation sessions from 3 
popular search engines. These sessions show that a user first 
issues the query "harrypotter sheme part", and then clicks on 
the resulting spell suggestion "harry potter theme park". 
362
and then clicks on the resulting spelling sugges-
tion "harry potter theme park". We can "reverse-
engineer" the parameters from the URLs of these 
sessions, and deduce how each search engine en-
codes both a query and the fact that a user arrived 
at a URL by clicking on the spelling suggestion 
of the query ? an strong indication that the 
spelling suggestion is desired. In this study, from 
1 year of sessions, we extracted ~120 million 
pairs. We found the data set very clean because 
these spelling corrections are actually clicked, 
and thus judged implicitly, by many users. 
In addition to the "did you mean" functionali-
ty, recently some search engines have introduced 
two new spelling suggestion functions. One is the 
"auto-correction" function, where the search en-
gine is confident enough to automatically apply 
the spelling correction to the query and execute it 
to produce search results. The other is the "split 
pane" result page, where one half portion of the 
search results are produced using the original 
query, while the other half, usually visually sepa-
rate portion of results, are produced using the 
auto-corrected query. 
In neither of these functions does the user ever 
receive an opportunity to approve or disapprove 
of the correction. Since our extraction approach 
focuses on user-approved spelling suggestions, 
we ignore the query reformulation sessions re-
cording either of the two functions. Although by 
doing so we could miss some basic, obvious 
spelling corrections, our experiments show that 
the negative impact on error model training is 
negligible. One possible reason is that our base-
line system, which does not use any error model 
learned from the session data, is already able to 
correct these basic, obvious spelling mistakes. 
Thus, including these data for training is unlikely 
to bring any further improvement. 
We found that the error models trained using 
the data directly extracted from the query refor-
mulation sessions suffer from the problem of un-
derestimating the self-transformation probability 
of a query P(Q2=Q1|Q1), because we only includ-
ed in the training data the pairs where the query is 
different from the correction. To deal with this 
problem, we augmented the training data by in-
cluding correctly spelled queries, i.e., the pairs 
(Q1, Q2) where Q1 = Q2.  First, we extracted a set 
of queries from the sessions where no spell sug-
gestion is presented or clicked on. Second, we 
removed from the set those queries that were rec-
ognized as being auto-corrected by a search en-
gine. We do so by running a sanity check of the 
queries against our baseline noisy channel 
speller, which will be described in Section 6. If 
the system consider a query misspelled, we as-
sumed it an obvious misspelling, and removed it. 
The remaining queries were assumed to be cor-
rectly spelled and were added to the training data. 
6 Experiments 
We perform the evaluation using a manually an-
notated data set containing 24,172 queries sam-
pled from one year?s query logs from a commer-
cial search engine. The spelling of each query is 
manually corrected by four independent annota-
tors. The average length of queries in the data 
sets is 2.7 words. We divided the data set into 
non-overlapped training and test data sets. The 
training data contain 8,515       pairs, among 
which 1,743 queries are misspelled (i.e.    ). 
The test data contain 15,657       pairs, among 
which 2,960 queries are misspelled.  
The speller systems we developed in this 
study are evaluated using the following metrics. 
? Accuracy: The number of correct outputs 
generated by the system divided by the total 
number of queries in the test set. 
? Precision: The number of correct spelling 
corrections for misspelled queries generated 
by the system divided by the total number of 
corrections generated by the system. 
? Recall: The number of correct spelling cor-
rections for misspelled queries generated by 
the system divided by the total number of 
misspelled queries in the test set. 
We also perform a significance test, a t-test 
with a significance level of 0.05. 
In our experiments, all the speller systems are 
ranker-based. Unless otherwise stated, the ranker 
is a two-layer neural net with 5 hidden nodes. 
The free parameters of the neural net are trained 
to optimize accuracy on the training data using 
the back propagation algorithm (Burges et al, 
2005) .  
6.1 System Results 
Table 1 summarizes the main results of different 
spelling systems. Row 1 is the baseline speller 
where the noisy channel model of Equations (2) 
363
and (3) is used. The error model is based on the 
weighted edit distance function and the LM is a 
backoff bigram model trained on 1 year of query 
logs, with count cutoff 30. Row 2 is the speller 
using a linear ranker to incorporate all ranking 
features described in Section 3. The weights of 
the linear ranker are optimized using the Aver-
aged Perceptron algorithm (Freund and Schapire, 
1999). Row 3 is the speller where a nonlinear 
ranker (i.e., 2-layer neural net) is trained atop the 
features. Rows 4, 5 and 6 are systems that incor-
porate the additional features derived from the 
phrase-based error model (PBEM) described in 
Section 5 and the four Web scale LMs (WLMs) 
listed in Table 1. 
The results show that (1) the ranker is a very 
flexible modeling framework where a variety of 
fine-grained features can be easily incorporated, 
and a ranker-based speller outperforms signifi-
cantly (p < 0.01) the traditional system based on 
the noisy channel model (Row 2 vs. Row 1); (2) 
the speller accuracy can be further improved by 
using more sophisticated rankers and learning 
algorithms (Row 3 vs. Row 2); (3) both WLMs 
and PBEM bring significant improvements 
(Rows 4 and 5 vs. Row 3); and (4) interestingly, 
the gains from WLMs and PBEM are additive 
and the combined leads to a significantly better 
speller (Row 6 vs. Rows 4 and 5) than that of 
using either of them individually. 
In what follows, we investigate in detail how 
the WLMs and PBEM trained on massive Web 
content and search logs improve the accuracy of 
the speller system. We will compare our models 
with the state-of-the-art models proposed previ-
ously. From now on, the system listed in Row 3 
of Table 1 will be used as baseline. 
6.2 Language Models 
The quality of n-gram LMs depends on the order 
of the model, the size of the training data, and 
how well the training data match the test data. 
Figure 4 illustrates the perplexity results of the 
four LMs trained on different data sources tested 
on a random sample of 733,147 queries. The re-
sults show that (1) higher order LMs produce 
lower perplexities, especially when moving be-
yond unigram models; (2) as expected, the query 
LMs are most predictive for the test queries, 
though they are from independent query log 
snapshots; (3) although the body LMs are trained 
on much larger amounts of data than the title and 
anchor LMs, the former lead to much higher per-
plexity values, indicating that both title and an-
chor texts are quantitatively much more similar to 
queries than body texts. 
Table 2 summarizes the spelling results using 
different LMs. For comparison, we also built a 4-
gram LM using the Google 1T web 5-gram cor-
pus (Brants and Franz, 2006). This model is re-
ferred to as the G1T model, and is trained using 
the ?stupid backoff? smoothing method (Brants et 
al., 2007). Due to the high count cutoff applied 
by the Google corpus (i.e., n-grams must appear 
at least 40 times to be included in the corpus), we 
found the G1T model results to a higher OOV 
rate (i.e., 6.5%) on our test data than that of the 4 
Web scale LMs (i.e., less than 1%). 
The results in Table 2 are more or less con-
sistent with the perplexity results: the query LM 
is the best performer; there is no significant dif-
ference among the body, title and anchor LMs 
though the body LM is trained on a much larger 
amount of data; and all the 4 Web scale LMs out-
perform the G1T model substantially due to the 
significantly lower OOV rates. 
6.3 Error Models 
This section compares the phrase-based error 
model (PBEM) described in Section 5, with one 
of the state-of-the-art error models, proposed by 
Brill and Moore (2000), henceforth referred to as 
# System Accuracy Precision Recall 
1 Noisy channel 85.3 72.1 35.9 
2 Linear ranker 88.0 74.0 42.8 
3 Nonlinear ranker 89.0 74.1 49.6 
4 3 + PBEM 90.7 78.7 58.2 
5 3 + WLMs 90.4 75.1 58.7 
6 3 + PBEM + WLMs  91.6 79.1 63.9 
Table 1. Summary of spelling correction results. 
 
Figure 4. Perplexity results on test queries, using n-
gram LMs with different orders, derived from differ-
ent data sources. 
 
364
the B&M model. B&M is a substring error mod-
el. It estimates        as 
          
    
           
?        
   
   
  (10) 
where R is a partitioning of correction term c into 
adjacent substrings, and T is a partitioning of 
query term q, such that |T|=|R|. The partitions are 
thus in one-to-one alignment. To train the B&M 
model, we extracted 1 billion term-correction 
pairs       from the set of 120 million query-
correction pairs      , derived from the search 
logs as described in Section 5.2.  
Table 3 summarizes the comparison results. 
Rows 1 and 2 are our ranker-based baseline sys-
tems with and without the error model (EM) fea-
ture. The error model is based on weighted edit 
distance of Eq. (3), where the weights are learned 
on some manually annotated word-correction 
pairs (which is not used in this study). Rows 3 
and 4 are the B&M models using different maxi-
mum substring lengths, specified by L. L=1 re-
duces B&M to the weighted edit distance model 
in Row 2. Rows 5 and 6 are PBEMs with differ-
ent maximum phrase lengths. L=1 reduces PBEM 
to a word-based error model. The results show 
the benefits of capturing context information in 
error models. In particular, the significant im-
provements resulting from PBEM demonstrate 
that the dependencies between words are far 
more effective than that between characters 
(within a word) for spelling correction. This is 
largely due to the fact that there are many real-
word spelling errors in search queries. We also 
notice that PBEM is a more powerful model  than   
# # of word pairs Accuracy Precision Recall 
1 Baseline w/o EM 88.55 71.95 46.97 
2 1M 89.15 73.71 50.74 
3 10M 89.22 74.11 50.92 
4 100M 89.20 73.60 51.06 
5 1B 89.21 73.72 50.99 
Table 4. The performance of B&M error model (L=3) as a 
function of the size of training data (# of word pairs). 
# # of (Q, C) pairs Accuracy Precision Recall 
1 Baseline w/o EM 88.55 71.95 46.97 
2 5M 89.59 77.01 52.34 
3 15M 90.23 77.87 56.67 
4 45M 90.45 78.56 57.02 
5 120M 90.70 78.49 58.12 
Table 5. The performance of PBEM (L=3) as a function of 
the size of training data (# of (Q, C) pairs). 
B&M in that it can benefit more from increasing-
ly larger training data. As shown in Tables 4 and 
5, whilst the performance of B&M saturates 
quickly with the increase of training data, the per-
formance of PBEM does not appear to have 
peaked ? further improvements are likely given a 
larger data set. 
7 Conclusions and Future Work 
This paper explores the use of massive Web cor-
pora and search logs for improving a ranker- 
based search query speller. We show significant 
improvements over a noisy channel speller using 
fine-grained features, Web scale LMs, and a 
phrase-based error model that captures intern- 
word dependencies. There are several techniques 
we are exploring to make further improvements. 
First, since a query speller is developed for im-
proving the Web search results, it is natural to use 
features from search results in ranking, as studied 
in Chen et al (2007). The challenge is efficiency. 
Second, in addition to query reformulation ses-
sions, we are exploring other search logs from 
which we might extract more       pairs for er-
ror model training. One promising data source is 
clickthrough data (e.g., Agichtein et al 2006; 
Gao et al, 2009). For instance, we might try to 
learn a transformation from the title or anchor 
text of a document to the query that led to a click 
on that document. Finally, the phrase-based error 
model is inspired by phrase-based SMT systems. 
We are introducing more SMT techniques such 
as alignment and translation rule exaction. In a 
broad sense, spelling correction can be viewed as 
a monolingual MT problem where we translate 
bad English queries into good ones. 
# System Accuracy Precision Recall 
1 Baseline 89.0 74.1 49.6 
2 1+ query 4-gram 90.1 75.6 56.3 
3 1 + body 4-gram 89.9 75.7 54.4 
4 1 + title 4-gram 89.8 75.4 54.7 
5 1 + anchor 4-gram 89.9 75.1 55.6 
6 1 + G1T 4-gram 89.4 75.1 51.5 
Table 2. Spelling correction results using different LMs 
trained on different data sources. 
# System Accuracy Precision Recall 
1 Baseline w/o EM 88.6 72.0 47.0 
2 Baseline 89.0 74.1 49.6 
3 1 + B&M, L=1 89.0 73.3 50.1 
4 1 + B&M, L=3 89.2 73.7 51.0 
5 1 + PBEM, L=1 90.1 76.7 55.6 
6 1 + PBEM, L=3 90.7 78.5 58.1 
Table 3. Spelling correction results using different error 
models. 
365
Acknowledgments 
The authors would like to thank Andreas Bode, 
Mei Li, Chenyu Yan and Kuansan Wang for the 
very helpful discussions and collaboration. The 
work was done when Xu Sun was visiting Mi-
crosoft Research Redmond. 
References 
Agichtein, E., Brill, E. and Dumais, S. 2006. Improv-
ing web search ranking by incorporating user be-
havior information. In SIGIR, pp. 19-26. 
Ahmad, F., and Kondrak, G. 2005. Learning a spelling 
error model from search query logs. In HLT-
EMNLP, pp. 955-962. 
Brants, T., and Franz, A. 2006. Web 1T 5-gram corpus 
version 1.1. Technical report, Google Research. 
Brants, T., Popat, A. C., Xu, P., Och, F. J., and Dean, J. 
2007. Large language models in machine translation. 
In EMNLP-CoNLL, pp. 858 - 867. 
Brill, E., and Moore, R. C. 2000. An improved error 
model for noisy channel spelling correction. In ACL, 
pp. 286-293. 
Burges, C., Shaked, T., Renshaw, E., Lazier, A., 
Deeds, M., Hamilton, and Hullender, G. 2005. 
Learning to rank using gradient descent. In ICML, 
pp. 89-96.  
 Chaiken, R., Jenkins, B., Larson, P., Ramsey, B., 
Shakib, D., Weaver, S., and Zhou, J. 2008. SCOPE: 
easy and efficient parallel processing f massive data 
sets. In Proceedings of the VLDB Endowment, pp. 
1265-1276. 
Charniak, E. 2001. Immediate-head parsing for lan-
guage models. In ACL/EACL, pp. 124-131. 
Chen, S. F., and Goodman, J. 1999. An empirical 
study of smoothing techniques for language model-
ing. Computer Speech and Language, 13(10):359-
394. 
Chen, Q., Li, M., and Zhou, M. 2007. Improving que-
ry spelling correction using web search results. In 
EMNLP-CoNLL, pp. 181-189. 
Church, K., Hard, T., and Gao, J. 2007. Compressing 
trigram language models with Golomb coding. In 
EMNLP-CoNLL, pp. 199-207. 
Cucerzan, S., and Brill, E. 2004. Spelling correction as 
an iterative process that exploits the collective 
knowledge of web users. In EMNLP, pp. 293-300. 
Freund, Y. and Schapire, R. E. 1999. Large margin 
classification using the perceptron algorithm. In 
Machine Learning, 37(3): 277-296. 
Gao, J., Goodman, J., and Miao, J. 2001. The use of 
clustering techniques for language modeling -
application to Asian languages. Computational Lin-
guistics and Chinese Language Processing, 
6(1):27?60, 2001.  
Gao, J., Yuan, W., Li, X., Deng, K., and Nie, J-Y. 
2009. Smoothing clickthrough data for web search 
ranking. In SIGIR, pp. 355-362.  
Golding, A. R., and Roth, D. 1996. Applying winnow 
to context-sensitive spelling correction. In ICML, pp. 
182-190. 
Joachims, T. 2002. Optimizing search engines using 
clickthrough data. In SIGKDD, pp. 133-142. 
Kernighan, M. D., Church, K. W., and Gale, W. A. 
1990. A spelling correction program based on a 
noisy channel model. In COLING, pp. 205-210. 
Koehn, P., Och, F., and Marcu, D. 2003. Statistical 
phrase-based translation. In HLT/NAACL, pp. 127-
133. 
Kucich, K. 1992. Techniques for automatically 
correcting words in text. ACM Computing Surveys, 
24(4):377-439. 
Levenshtein, V. I. 1966. Binary codes capable of cor-
recting deletions, insertions and reversals. Soviet 
Physics Doklady, 10(8):707-710. 
Li, M., Zhu, M., Zhang, Y., and Zhou, M. 2006. Ex-
ploring distributional similarity based models for 
query spelling correction. In ACL, pp. 1025-1032. 
Mangu, L., and Brill, E. 1997. Automatic rule acquisi-
tion for spelling correction. In ICML, pp. 187-194. 
Microsoft Microsoft web n-gram services. 2010. 
http://research.microsoft.com/web-ngram 
Nguyen, P., Gao, J., and Mahajan, M. 2007. MSRLM: 
a scalable language modeling toolkit. Technical re-
port TR-2007-144, Microsoft Research. 
Och, F. 2002. Statistical machine translation: from 
single-word models to alignment templates. PhD 
thesis, RWTH Aachen. 
Och, F., and Ney, H. 2004. The alignment template 
approach to statistical machine translation. 
Computational Linguistics, 30(4): 417-449. 
Philips, L. 1990. Hanging on the metaphone. Comput-
er Language Magazine, 7(12):38-44. 
Sun, X., Gao, J., Micol, D., and Quirk, C. 2010. 
Learning phrase-based spelling error models from 
clickthrough data. In ACL.  
Toutanova, K., and Moore, R. 2002. Pronunciation 
modeling for improved spelling correction. In ACL, 
pp. 144-151.  
Whitelaw, C., Hutchinson, B., Chung, G. Y., and Ellis, 
G. 2009. Using the web for language independent 
spellchecking and autocorrection. In EMNLP, pp. 
890-899. 
Zhang, Y., Hildebrand, Al. S., and Vogel, S. 2006. 
Distributed language modeling for n-best list re-
ranking. In EMNLP, pp. 216-233. 
366
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 38?49,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Optimal Search for Minimum Error Rate Training
Michel Galley
Microsoft Research
Redmond, WA 98052, USA
mgalley@microsoft.com
Chris Quirk
Microsoft Research
Redmond, WA 98052, USA
chrisq@microsoft.com
Abstract
Minimum error rate training is a crucial compo-
nent to many state-of-the-art NLP applications,
such as machine translation and speech recog-
nition. However, common evaluation functions
such as BLEU or word error rate are generally
highly non-convex and thus prone to search
errors. In this paper, we present LP-MERT, an
exact search algorithm for minimum error rate
training that reaches the global optimum using
a series of reductions to linear programming.
Given a set of N -best lists produced from S
input sentences, this algorithm finds a linear
model that is globally optimal with respect to
this set. We find that this algorithm is poly-
nomial in N and in the size of the model, but
exponential in S. We present extensions of this
work that let us scale to reasonably large tuning
sets (e.g., one thousand sentences), by either
searching only promising regions of the param-
eter space, or by using a variant of LP-MERT
that relies on a beam-search approximation.
Experimental results show improvements over
the standard Och algorithm.
1 Introduction
Minimum error rate training (MERT)?also known
as direct loss minimization in machine learning?is a
crucial component in many complex natural language
applications such as speech recognition (Chou et al,
1993; Stolcke et al, 1997; Juang et al, 1997), statisti-
cal machine translation (Och, 2003; Smith and Eisner,
2006; Duh and Kirchhoff, 2008; Chiang et al, 2008),
dependency parsing (McDonald et al, 2005), summa-
rization (McDonald, 2006), and phonetic alignment
(McAllester et al, 2010). MERT directly optimizes
the evaluation metric under which systems are being
evaluated, yielding superior performance (Och, 2003)
when compared to a likelihood-based discriminative
method (Och and Ney, 2002). In complex text gener-
ation tasks like SMT, the ability to optimize BLEU
(Papineni et al, 2001), TER (Snover et al, 2006), and
other evaluation metrics is critical, since these met-
rics measure qualities (such as fluency and adequacy)
that often do not correlate well with task-agnostic
loss functions such as log-loss.
While competitive in practice, MERT faces several
challenges, the most significant of which is search.
The unsmoothed error count is a highly non-convex
objective function and therefore difficult to optimize
directly; prior work offers no algorithm with a good
approximation guarantee. While much of the ear-
lier work in MERT (Chou et al, 1993; Juang et al,
1997) relies on standard convex optimization tech-
niques applied to non-convex problems, the Och al-
gorithm (Och, 2003) represents a significant advance
for MERT since it applies a series of special line min-
imizations that happen to be exhaustive and efficient.
Since this algorithm remains inexact in the multidi-
mensional case, much of the recent work on MERT
has focused on extending Och?s algorithm to find
better search directions and starting points (Cer et al,
2008; Moore and Quirk, 2008), and on experiment-
ing with other derivative-free methods such as the
Nelder-Mead simplex algorithm (Nelder and Mead,
1965; Zens et al, 2007; Zhao and Chen, 2009).
In this paper, we present LP-MERT, an exact
search algorithm for N -best optimization that ex-
ploits general assumptions commonly made with
MERT, e.g., that the error metric is decomposable
by sentence.1 While there is no known optimal algo-
1Note that MERT makes two types of approximations. First,
the set of all possible outputs is represented only approximately,
by N -best lists, lattices, or hypergraphs. Second, error func-
tions on such representations are non-convex and previous work
only offers approximate techniques to optimize them. Our work
avoids the second approximation, while the first one is unavoid-
able when optimization and decoding occur in distinct steps.
38
rithm to optimize general non-convex functions, the
unsmoothed error surface has a special property that
enables exact search: the set of translations produced
by an SMT system for a given input is finite, so the
piecewise-constant error surface contains only a fi-
nite number of constant regions. As in Och (2003),
one could imagine exhaustively enumerating all con-
stant regions and finally return the best scoring one?
Och does this efficiently with each one-dimensional
search?but the idea doesn?t quite scale when search-
ing all dimensions at once. Instead, LP-MERT ex-
ploits algorithmic devices such as lazy enumeration,
divide-and-conquer, and linear programming to effi-
ciently discard partial solutions that cannot be max-
imized by any linear model. Our experiments with
thousands of searches show that LP-MERT is never
worse than the Och algorithm, which provides strong
evidence that our algorithm is indeed exact. In the
appendix, we formally prove that this search algo-
rithm is optimal. We show that this algorithm is
polynomial in N and in the size of the model, but
exponential in the number of tuning sentences. To
handle reasonably large tuning sets, we present two
modifications of LP-MERT that either search only
promising regions of the parameter space, or that rely
on a beam-search approximation. The latter modifica-
tion copes with tuning sets of one thousand sentences
or more, and outperforms the Och algorithm on a
WMT 2010 evaluation task.
This paper makes the following contributions. To
our knowledge, it is the first known exact search
algorithm for optimizing task loss on N -best lists in
general dimensions. We also present an approximate
version of LP-MERT that offers a natural means of
trading speed for accuracy, as we are guaranteed to
eventually find the global optimum as we gradually
increase beam size. This trade-off may be beneficial
in commercial settings and in large-scale evaluations
like the NIST evaluation, i.e., when one has a stable
system and is willing to let MERT run for days or
weeks to get the best possible accuracy. We think this
work would also be useful as we turn to more human
involvement in training (Zaidan and Callison-Burch,
2009), as MERT in this case is intrinsically slow.
2 Unidimensional MERT
Let fS1 = f1 . . . fS denote the S input sentences
of our tuning set. For each sentence fs, let Cs =
es,1 . . . es,N denote a set of N candidate translations.
For simplicity and without loss of generality, we
assume that N is constant for each index s. Each
input and output sentence pair (fs, es,n) is weighted
by a linear model that combines model parameters
w = w1 . . . wD ? RD with D feature functions
h1(f , e,?) . . . hD(f , e,?), where ? is the hidden
state associated with the derivation from f to e, such
as phrase segmentation and alignment. Furthermore,
let hs,n ? RD denote the feature vector representing
the translation pair (fs, es,n).
In MERT, the goal is to minimize an error count
E(r, e) by scoring translation hypotheses against a
set of reference translations rS1 = r1 . . . rS . As-
suming as in Och (2003) that error count is addi-
tively decomposable by sentence?i.e., E(rS1 , eS1 ) =?
sE(rs, es)?this results in the following optimiza-
tion problem:2
w? = argminw
{ S?
s=1
E(rs, e?(fs;w))
}
= argminw
{ S?
s=1
N?
n=1
E(rs, es,n)?(es,n, e?(fs;w))
}
(1)where
e?(fs;w) = argmax
n?{1...N}
{w?hs,n
}
The quality of this approximation is dependent on
how accurately the N -best lists represent the search
space of the system. Therefore, the hypothesis list is
iteratively grown: decoding with an initial parameter
vector seeds the N -best lists; next, parameter esti-
mation and N -best list gathering alternate until the
search space is deemed representative.
The crucial observation of Och (2003) is that the
error count along any line is a piecewise constant
function. Furthermore, this function for a single sen-
tence may be computed efficiently by first finding the
hypotheses that form the upper envelope of the model
score function, then gathering the error count for each
hypothesis along the range for which it is optimal. Er-
ror counts for the whole corpus are simply the sums
of these piecewise constant functions, leading to an
2A metric such as TER is decomposable by sentence. BLEU
is not, but its sufficient statistics are, and the literature offers
several sentence-level approximations of BLEU (Lin and Och,
2004; Liang et al, 2006).
39
efficient algorithm for finding the global optimum of
the error count along any single direction.
Such a hill-climbing algorithm in a non-convex
space has no optimality guarantee: without a perfect
direction finder, even a globally-exact line search may
never encounter the global optimum. Coordinate as-
cent is often effective, though conjugate direction set
finding algorithms, such as Powell?s method (Powell,
1964; Press et al, 2007), or even random directions
may produce better results (Cer et al, 2008). Ran-
dom restarts, based on either uniform sampling or a
random walk (Moore and Quirk, 2008), increase the
likelihood of finding a good solution. Since random
restarts and random walks lead to better solutions
and faster convergence, we incorporate them into our
baseline system, which we refer to as 1D-MERT.
3 Multidimensional MERT
Finding the global optimum of Eq. 1 is a difficult
task, so we proceed in steps and first analyze the
case where the tuning set contains only one sentence.
This gives insight on how to solve the general case.
With only one sentence, one of the two summations
in Eq. 1 vanishes and one can exhaustively enumer-
ate the N translations e1,n (or en for short) to find
the one that yields the minimal task loss. The only
difficulty with S = 1 is to know for each translation
en whether its feature vector h1,n (or hn for short)
can be maximized using any linear model. As we
can see in Fig. 1(a), some hypotheses can be maxi-
mized (e.g., h1, h2, and h4), while others (e.g., h3
and h5) cannot. In geometric terminology, the former
points are commonly called extreme points, and the
latter are interior points.3 The problem of exactly
optimizing a single N -best list is closely related to
the convex hull problem in computational geometry,
for which generic solvers such as the QuickHull al-
gorithm exist (Eddy, 1977; Bykat, 1978; Barber et
al., 1996). A first approach would be to construct the
convex hull conv(h1 . . .hN ) of the N -best list, then
identify the point on the hull with lowest loss (h1 in
Fig. 1) and finally compute an optimal weight vector
using hull points that share common facets with the
3Specifically, a point h is extreme with respect to a convex
set C (e.g., the convex hull shown in Fig. 1(a)) if it does not lie
in an open line segment joining any two points of C. In a minor
abuse of terminology, we sometimes simply state that a given
point h is extreme when the nature of C is clear from context.
w h1 h3: 0.41 
h1: 0.43 h4: 0.48 
h5: 0.46 h2: 0.51 
LM 
CM 
(a) (b) 
Figure 1: N -best list (h1 . . .hN ) with associated losses
(here, TER scores) for a single input sentence, whose
convex hull is displayed with dotted lines in (a). For effec-
tive visualization, our plots use only two features (D = 2).
While we can find a weight vector that maximizes h1 (e.g.,
the w in (b)), no linear model can possibly maximize any
of the points strictly inside the convex hull.
optimal feature vector (h2 and h4). Unfortunately,
this doesn?t quite scale even with a single N -best list,
since the best known convex hull algorithm runs in
O(N bD/2c+1) time (Barber et al, 1996).4
Algorithms presented in this paper assume that D
is unrestricted, therefore we cannot afford to build
any convex hull explicitly. Thus, we turn to linear
programming (LP), for which we know algorithms
(Karmarkar, 1984) that are polynomial in the number
of dimensions and linear in the number of points, i.e.,
O(NT ), where T = D3.5. To check if point hi is
extreme, we really only need to know whether we can
define a half-space containing all points h1 . . .hN ,
with hi lying on the hyperplane delimiting that half-
space, as shown in Fig. 1(b) for h1. Formally, a
vertex hi is optimal with respect to argmaxi{w?hi}
if and only if the following constraints hold:5
w?hi = y (2)
w?hj ? y, for each j 6= i (3)
w is orthogonal to the hyperplane defining the half-
space, and the intercept y defines its position. The
4A convex hull algorithm polynomial in D is very unlikely.
Indeed, the expected number of facets of high-dimensional con-
vex hulls grows dramatically, and?assuming a uniform distribu-
tion of points, D = 10, and a sufficiently large N?the expected
number of facets is approximately 106N (Buchta et al, 1985).
In the worst case, the maximum number of facets of a convex
hull is O(NbD/2c/bD/2c!) (Klee, 1966).
5A similar approach for checking whether a given point is
extreme is presented in http://www.ifor.math.ethz.
ch/?fukuda/polyfaq/node22.html, but our method
generates slightly smaller LPs.
40
above equations represent a linear program (LP),
which can be turned into canonical form
maximize c? w
subject to Aw ? b
by substituting y with w?hi in Eq. 3, by defining
A = {an,d}1?n?N ;1?d?D with an,d = hj,d ? hi,d
(where hj,d is the d-th element of hj), and by setting
b = (0, . . . , 0)? = 0. The vertex hi is extreme if
and only if the LP solver finds a non-zero vector w
satisfying the canonical system. To ensure that w is
zero only when hi is interior, we set c = hi ? h?,
where h? is a point known to be inside the hull (e.g.,
the centroid of the N -best list).6 In the remaining
of this section, we use this LP formulation in func-
tion LINOPTIMIZER(hi;h1 . . .hN ), which returns
the weight vector w? maximizing hi, or which returns
0 if hi is interior to conv(h1 . . .hN ). We also use
conv(hi;h1 . . .hN ) to denote whether hi is extreme
with respect to this hull.
Algorithm 1: LP-MERT (for S = 1).
input : sent.-level feature vectors H = {h1 . . .hN}
input : sent.-level task losses E1 . . . EN , where
En := E(r1, e1,n)
output :optimal weight vector w?
1 begin
. sort N -best list by increasing losses:
2 (i1 . . . iN )? INDEXSORT(E1 . . . EN )
3 for n? 1 to N do
. find w? maximizing in-th element:
4 w?? LINOPTIMIZER(hin ;H)
5 if w? 6= 0 then
6 return w?
7 return 0
An exact search algorithm for optimizing a single
N -best list is shown above. It lazily enumerates fea-
ture vectors in increasing order of task loss, keeping
only the extreme ones. Such a vertex hj is known to
be on the convex hull, and the returned vector w? max-
imizes it. In Fig. 1, it would first run LINOPTIMIZER
on h3, discard it since it is interior, and finally accept
the extreme point h1. Each execution of LINOPTI-
MIZER requires O(NT ) time with the interior point
6We assume that h1 . . .hN are not degenerate, i.e., that they
collectively span RD . Otherwise, all points are necessarily on
the hull, yet some of them may not be uniquely maximized.
0.001
0.01
0.1
1
10
100
1000
10000
100000
2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20
QuickHull
LP
Dimensions 
Sec
ond
s 
Figure 2: Running times to exactly optimize N -best lists
with an increasing number of dimensions. To determine
which feature vectors were on the hull, we use either linear
programming (Karmarkar, 1984) or one of the most effi-
cient convex hull computation tools (Barber et al, 1996).
method of (Karmarkar, 1984), and since the main
loop may run O(N) times in the worst case, time
complexity is O(N2T ). Finally, Fig. 2 empirically
demonstrates the effectiveness of a linear program-
ming approach, which in practice is seldom affected
by D.
3.1 Exact search: general case
We now extend LP-MERT to the general case, in
which we are optimizing multiple sentences at once.
This creates an intricate optimization problem, since
the inner summations over n = 1 . . . N in Eq. 1
can?t be optimized independently. For instance,
the optimal weight vector for sentence s = 1 may
be suboptimal with respect to sentence s = 2.
So we need some means to determine whether a
selection m = m(1) . . .m(S) ? M = [1, N ]S of
feature vectors h1,m(1) . . .hS,m(S) is extreme, that is,
whether we can find a weight vector that maximizes
each hs,m(s). Here is a reformulation of Eq. 1 that
makes this condition on extremity more explicit:
m? = argmin
conv(h[m];H)
m?M
{ S?
s=1
E(rs, es,m(n))
}
(4)
where
h[m] =
S?
s=1
hs,m(s)
H =
?
m??M
h[m?]
41
One na??ve approach to address this optimization
problem is to enumerate all possible combinations
among the S distinct N -best lists, determine for each
combination m whether h[m] is extreme, and return
the extreme combination with lowest total loss. It is
evident that this approach is optimal (since it follows
directly from Eq. 4), but it is prohibitively slow since
it processes O(NS) vertices to determine whether
they are extreme, which thus requires O(NST ) time
per LP optimization and O(N2ST ) time in total. We
now present several improvements to make this ap-
proach more practical.
3.1.1 Sparse hypothesis combination
In the na??ve approach presented above, each LP
computation to evaluate conv(h[m];H) requires
O(NST ) time since H contains NS vertices, but
we show here how to reduce it to O(NST ) time.
This improvement exploits the fact that we can elimi-
nate the majority of the NS points of H , since only
S(N ?1)+1 are really needed to determine whether
h[m] is extreme. This is best illustrated using an ex-
ample, as shown in Fig. 3. Both h1,1 and h2,1 in (a)
and (b) are extreme with respect to their own N -best
list, and we ask whether we can find a weight vector
that maximizes both h1,1 and h2,1. The algorith-
mic trick is to geometrically translate one of the two
N -best lists so that h1,1 = h?2,1, where h?2,1 is the
translation of h?2,1. Then we use linear programming
with the new set of 2N ? 1 points, as shown in (c), to
determine whether h1,1 is on the hull, in which case
the answer to the original question is yes. In the case
of the combination of h1,1 and h2,2, we see in (d) that
the combined set of points prevents the maximization
h1,1, since this point is clearly no longer on the hull.
Hence, the combination (h1,1,h2,2) cannot be maxi-
mized using any linear model. This trick generalizes
to S ? 2. In both (c) and (d), we used S(N ? 1) + 1
points instead of NS to determine whether a given
point is extreme. We show in the appendix that this
simplification does not sacrifice optimality.
3.1.2 Lazy enumeration, divide-and-conquer
Now that we can determine whether a given combi-
nation is extreme, we must next enumerate candidate
combinations to find the combination that has low-
est task loss among all of those that are extreme.
Since the number of feature vector combinations is
O(NS), exhaustive enumeration is not a reasonable
h1,1  h2,2  h2,1  
(a) (b) 
h1,1  h?2,2  
(c)  (d)  
h1,1 h?2,1  
Figure 3: Given two N -best lists, (a) and (b), we use
linear programming to determine which hypothesis com-
binations are extreme. For instance, the combination h1,1
and h2,1 is extreme (c), while h1,1 and h2,2 is not (d).
option. Instead, we use lazy enumeration to pro-
cess combinations in increasing order of task loss,
which ensures that the first extreme combination for
s = 1 . . . S that we encounter is the optimal one. An
S-ary lazy enumeration would not be particularly ef-
ficient, since the runtime is still O(NS) in the worst
case. LP-MERT instead uses divide-and-conquer
and binary lazy enumeration, which enables us to
discard early on combinations that are not extreme.
For instance, if we find that (h1,1,h2,2) is interior for
sentences s = 1, 2, the divide-and-conquer branch
for s = 1 . . . 4 never actually receives this bad com-
bination from its left child, thus avoiding the cost
of enumerating combinations that are known to be
interior, e.g., (h1,1,h2,2,h3,1,h4,1).
The LP-MERT algorithm for the general case is
shown as Algorithm 2. It basically only calls a re-
cursive divide-and-conquer function (GETNEXTBEST)
for sentence range 1 . . . S. The latter function uses bi-
nary lazy enumeration in a manner similar to (Huang
and Chiang, 2005), and relies on two global variables:
I and L. The first of these, I , is used to memoize the
results of calls to GETNEXTBEST; given a range of
sentences and a rank n, it stores the nth best combina-
tion for that range of sentences. The global variable
L stores hypotheses combination matrices, one ma-
trix for each range of sentences (s, t) as shown in
42
h11 h12 
h21 h22 h23 
69.1 69.2 69.3 69.2 69.4 h31 h32 h33 
h41 h42 
56.8 57.1 57.3 57.6 
h23 
57.9 
{h11, h23} 
{h31, h41} 
126.0 126.5 
126.1 
{h32, h41} 
{h12, h21} 
Combinations checked:  {h11, h23, h31, h41} {h12, h21, h31, h41} 
Combinations discarded:  {h11, h21, h31, h41} {h12, h22, h31, h41} {h12, h12, h31, h42} (and 7 others) h13 
h24 
69.9 70.0 
L[3,4] L[1,2] 
Figure 4: LP-MERT minimizes loss (TER) on four sen-
tences. O(N4) translation combinations are possible,
but the LP-MERT algorithm only tests two full combi-
nations. Without divide-and-conquer?i.e., using 4-ary
lazy enumeration?ten full combinations would have been
checked unnecessarily.
Algorithm 2: LP-MERT
input : feature vectors H = {hs,n}1?s?S;1?n?N
input : task losses E = {Es,n}1?s?S;1?n?N ,
where sent.-level costs Es,n := E(rs, es,n)
output :optimal weight vector w? and its loss L
1 begin
. sort N -best lists by increasing losses:
2 for s? 1 to S do
3 (is,1..is,N )? INDEXSORT(Es,1..Es,N )
. find best hypothesis combination for 1 . . . S:
4 (h?, H?, L)? GETNEXTBEST(H,E, 1, S)
5 w?? LINOPTIMIZER(h?;H?)
6 return (w?, L)
Fig. 4, to determine which combination to try next.
The function EXPANDFRONTIER returns the indices
of unvisited cells that are adjacent (right or down) to
visited cells and that might correspond to the next
best hypothesis. Once no more cells need to be added
to the frontier, LP-MERT identifies the lowest loss
combination on the frontier (BESTINFRONTIER), and
uses LP to determine whether it is extreme. To do so,
it first generates an LP using COMBINE, a function
that implements the method described in Fig. 3. If
the LP offers no solution, this combination is ignored.
LP-MERT iterates until it finds a cell entry whose
combination is extreme. Regarding ranges of length
one (s = t), lines 3-10 are similar to Algorithm 1 for
S = 1, but with one difference: GETNEXTBEST may
be called multiple times with the same argument s,
since the first output of GETNEXTBEST might not be
extreme when combined with other feature vectors.
Lines 3-10 of GETNEXTBEST handle this case effi-
ciently, since the algorithm resumes at the (n+1)-th
Function GetNextBest(H,E,s,t)
input : sentence range (s, t)
output :h?: current best extreme vertex
output :H?: constraint vertices
output :L: task loss of h?
. Losses of partial hypotheses:
1 L? L[s, t]
2 if s = t then
. n is the index where we left off last time:
3 n? NBROWS(L)
4 Hs ? {hs,1 . . .hs,N}
5 repeat
6 n? n+ 1
7 w?? LINOPTIMIZER(hs,in ;Hs)
8 L[n, 1]? Es,in
9 until w? 6= 0
10 return (hs,in , Hs,L[n, 1])
11 else
12 u? b(s+ t)/2c, v ? u+ 1
13 repeat
14 while HASINCOMPLETEFRONTIER(L) do
15 (m,n)? EXPANDFRONTIER(L)
16 x? NBROWS(L)
17 y ? NBCOLUMNS(L)
18 form? ? x+ 1 tom do
19 I[s, u,m?]? GETNEXTBEST(H,E, s, u)
20 for n? ? y + 1 to n do
21 I[v, t, n?]? GETNEXTBEST(H,E, v, t)
22 L[m,n]? LOSS(I[s, u,m])+LOSS(I[v, t, n])
23 (m,n)? BESTINFRONTIER(L)
24 (hm, Hm, Lm)? I[s, u,m]
25 (hn, Hn, Ln)? I[v, t, n]
26 (h?, H?)? COMBINE(hm, Hm,hn, Hn)
27 w?? LINOPTIMIZER(h?;H?)
28 until w? 6= 0
29 return (h?, H?,L[m,n])
element of the N -best list (where n is the position
where the previous execution left off).7 We can see
that a strength of this algorithm is that inconsistent
combinations are deleted as soon as possible, which
allows us to discard fruitless candidates en masse.
3.2 Approximate Search
We will see in Section 5 that our exact algorithm
is often too computationally expensive in practice
to be used with either a large number of sentences
or a large number of features. We now present two
7Each N -best list is augmented with a placeholder hypothesis
with loss +?. This ensures n never runs out of bounds at line 7.
43
Function Combine(h, H,h?, H ?)
input :H,H ?: constraint vertices
input :h,h?: extreme vertices, wrt. H and H ?
output :h?, H?: combination as in Sec. 3.1.1
1 for i? 1 to size(H) do
2 Hi ? Hi + h?
3 for i? 1 to size(H ?) do
4 H ?i ? H ?i + h
5 return (h+ h?, H ?H ?)
approaches to make LP-MERT more scalable, with
the downside that we may allow search errors.
In the first case, we make the assumption that we
have an initial weight vector w0 that is a reasonable
approximation of w?, where w0 may be obtained ei-
ther by using a fast MERT algorithm like 1D-MERT,
or by reusing the weight vector that is optimal with
respect to the previous iteration of MERT. The idea
then is to search only the set of weight vectors that
satisfy cos(w?,w0) ? t, where t is a threshold on
cosine similarity provided by the user. The larger the
t, the faster the search, but at the expense of more
search errors. This is implemented with two simple
changes in our algorithm. First, LINOPTIMIZER sets
the objective vector c = w0. Second, if the output
w? originally returned by LINOPTIMIZER does not
satisfy cos(w?,w0) ? t, then it returns 0. While this
modification of our algorithm may lead to search
errors, it nevertheless provides some theoretical guar-
antee: our algorithm finds the global optimum if it
lies within the region defined by cos(w?,w0) ? t.
The second method is a beam approximation of LP-
MERT, which normally deals with linear programs
that are increasingly large in the upper branches of
GETNEXTBEST?s recursive calls. The main idea is
to prune the output of COMBINE (line 26) by model
score with respect to wbest, where wbest is our cur-
rent best model on the entire tuning set. Note that
beam pruning can discard h? (the current best ex-
treme vertex), in which case LINOPTIMIZER returns
0. wbest is updated as follows: each time we pro-
duce a new non-zero w?, run wbest ? w? if w? has a
lower loss than wbest on the entire tuning set. The
idea of using a beam here is similar to using cosine
similarity (since wbest constrains the search towards
a promising region), but beam pruning also helps
reduce LP optimization time and thus enables us to
explore a wider space. Since wbest often improves
during search, it is useful to run multiple iterations of
LP-MERT until wbest doesn?t change. Two or three
iterations suffice in our experience. In our experi-
ments, we use a beam size of 1000.
4 Experimental Setup
Our experiments in this paper focus on only the ap-
plication of machine translation, though we believe
that the current approach is agnostic to the particular
system used to generate hypotheses. Both phrase-
based systems (e.g., Koehn et al (2007)) and syntax-
based systems (e.g., Li et al (2009), Quirk et al
(2005)) commonly use MERT to train free param-
eters. Our experiments use a syntax-directed trans-
lation approach (Quirk et al, 2005): it first applies
a dependency parser to the source language data at
both training and test time. Multi-word translation
mappings constrained to be connected subgraphs of
the source tree are extracted from the training data;
these provide most lexical translations. Partially lexi-
calized templates capturing reordering and function
word insertion and deletion are also extracted. At
runtime, these mappings and templates are used to
construct transduction rules to convert the source tree
into a target string. The best transduction is sought
using approximate search techniques (Chiang, 2007).
Each hypothesis is scored by a relatively standard
set of features. The mappings contain five features:
maximum-likelihood estimates of source given target
and vice versa, lexical weighting estimates of source
given target and vice versa, and a constant value that,
when summed across a whole hypothesis, indicates
the number of mappings used. For each template,
we include a maximum-likelihood estimate of the
target reordering given the source structure. The
system may fall back to templates that mimic the
source word order; the count of such templates is a
feature. Likewise we include a feature to count the
number of source words deleted by templates, and a
feature to count the number of target words inserted
by templates. The log probability of the target string
according to a language models is also a feature; we
add one such feature for each language model. We
include the number of target words as features to
balance hypothesis length.
For the present system, we use the training data of
WMT 2010 to construct and evaluate an English-to-
44
-1
0
1
2
3
4
5
6
7
0 100 200 300 400 500 600 700 800 900 1000
S=8
S=4
S=2
? 
BL
EU
[%
] 
Figure 5: Line graph of sorted differences in
BLEUn4r1[%] scores between LP-MERT and 1D-MERT
on 1000 tuning sets of size S = 2, 4, 8. The highest differ-
ences for S = 2, 4, 8 are respectively 23.3, 19.7, 13.1.
German translation system. This consists of approx-
imately 1.6 million parallel sentences, along with a
much larger monolingual set of monolingual data.
We train two language models, one on the target side
of the training data (primarily parliamentary data),
and the other on the provided monolingual data (pri-
marily news). The 2009 test set is used as develop-
ment data for MERT, and the 2010 one is used as test
data. The resulting system has 13 distinct features.
5 Results
The section evaluates both the exact and beam ver-
sion of LP-MERT. Unless mentioned otherwise, the
number of features isD = 13 and theN -best list size
is 100. Translation performance is measured with
a sentence-level version of BLEU-4 (Lin and Och,
2004), using one reference translation. To enable
legitimate comparisons, LP-MERT and 1D-MERT
are evaluated on the same combined N -best lists,
even though running multiple iterations of MERT
with either LP-MERT or 1D-MERT would normally
produce different combined N -best lists. We use
WMT09 as tuning set, and WMT10 as test set. Be-
fore turning to large tuning sets, we first evaluate
exact LP-MERT on data sizes that it can easily han-
dle. Fig. 5 offers a comparison with 1D-MERT, for
which we split the tuning set into 1,000 overlapping
subsets for S = 2, 4, 8 on a combined N -best after
five iterations of MERT with an average of 374 trans-
lation per sentence. The figure shows that LP-MERT
never underperforms 1D-MERT in any of the 3,000
experiments, and this almost certainly confirms that
length tested comb. total comb. order
8 639,960 1.33? 1020 O(N8)
4 134,454 2.31? 1010 O(2N4)
2 49,969 430,336 O(4N2)
1 1,059 2,624 O(8N)
Table 1: Number of tested combinations for the experi-
ments of Fig. 5. LP-MERT with S = 8 checks only 600K
full combinations on average, much less than the total
number of combinations (which is more than 1020).
1
10
100
1,000
10,000
2 3 4 5 6 7 8 9
se
co
nd
s 
1024
256
128
64
32
16
8
4
2
1dimension (D) 
Figure 6: Effect of the number of features (runtime on
1 CPU of a modern computer). Each curve represents a
different number of tuning sentences.
LP-MERT systematically finds the global optimum.
In the case S = 1, Powell rarely makes search er-
rors (about 15%), but the situation gets worse as S
increases. For S = 4, it makes search errors in 90%
of the cases, despite using 20 random starting points.
Some combination statistics for S up to 8 are
shown in Tab. 1. The table shows the speedup pro-
vided by LP-MERT is very substantial when com-
pared to exhaustive enumeration. Note that this is
using D = 13, and that pruning is much more ef-
fective with less features, a fact that is confirmed in
Fig. 6. D = 13 makes it hard to use a large tuning
set, but the situation improves with D = 2 . . . 5.
Fig. 7 displays execution times when LP-MERT
constrains the output w? to satisfy cos(w0, w?) ? t,
where t is on the x-axis of the figure. The figure
shows that we can scale to 1000 sentences when
(exactly) searching within the region defined by
cos(w0, w?) ? .84. All these running times would
improve using parallel computing, since divide-and-
conquer algorithms are generally easy to parallelize.
We also evaluate the beam version of LP-MERT,
which allows us to exploit tuning sets of reasonable
45
110
100
1,000
10,000
0.99 0.98 0.96 0.92 0.84 0.68 0.36 -0.28 -1
se
co
nd
s 
1024
512
256
128
64
32
16
8
4
2
1cosine 
Figure 7: Effect of a constraint on w (runtime on 1 CPU).
32 64 128 256 512 1024
1D-MERT 22.93 20.70 18.57 16.07 15.00 15.44
our work 25.25 22.28 19.86 17.05 15.56 15.67
+2.32 +1.59 +1.29 +0.98 +0.56 +0.23
Table 2: BLEUn4r1[%] scores for English-German on
WMT09 for tuning sets ranging from 32 to 1024 sentences.
size. Results are displayed in Table 2. The gains
are fairly substantial, with gains of 0.5 BLEU point
or more in all cases where S ? 512.8 Finally, we
perform an end-to-end MERT comparison, where
both our algorithm and 1D-MERT are iteratively used
to generate weights that in turn yield newN -best lists.
Tuning on 1024 sentences of WMT10, LP-MERT
converges after seven iterations, with a BLEU score
of 16.21%; 1D-MERT converges after nine iterations,
with a BLEU score of 15.97%. Test set performance
on the full WMT10 test set for LP-MERT and 1D-
MERT are respectively 17.08% and 16.91%.
6 Related Work
One-dimensional MERT has been very influential. It
is now used in a broad range of systems, and has been
improved in a number of ways. For instance, lattices
or hypergraphs may be used in place of N -best lists
to form a more comprehensive view of the search
space with fewer decoding runs (Macherey et al,
2008; Kumar et al, 2009; Chatterjee and Cancedda,
2010). This particular refinement is orthogonal to our
approach, though. We expect to extend LP-MERT
8One interesting observation is that the performance of 1D-
MERT degrades as S grows from 2 to 8 (Fig. 5), which contrasts
with the results shown in Tab. 2. This may have to do with the
fact that N -best lists with S = 2 have much fewer local maxima
than with S = 4, 8, in which case 20 restarts is generally enough.
to hypergraphs in future work. Exact search may be
challenging due to the computational complexity of
the search space (Leusch et al, 2008), but approxi-
mate search should be feasible.
Other research has explored alternate methods
of gradient-free optimization, such as the downhill-
simplex algorithm (Nelder and Mead, 1965; Zens
et al, 2007; Zhao and Chen, 2009). Although the
search space is different than that of Och?s algorithm,
it still relies on one-dimensional line searches to re-
flect, expand, or contract the simplex. Therefore, it
suffers the same problems of one-dimensional MERT:
feature sets with complex non-linear interactions are
difficult to optimize. LP-MERT improves on these
methods by searching over a larger subspace of pa-
rameter combinations, not just those on a single line.
We can also change the objective function in a
number of ways to make it more amenable to op-
timization, leveraging knowledge from elsewhere
in the machine learning community. Instance re-
weighting as in boosting may lead to better param-
eter inference (Duh and Kirchhoff, 2008). Smooth-
ing the objective function may allow differentiation
and standard ML learning techniques (Och and Ney,
2002). Smith and Eisner (2006) use a smoothed ob-
jective along with deterministic annealing in hopes
of finding good directions and climbing past locally
optimal points. Other papers use margin methods
such as MIRA (Watanabe et al, 2007; Chiang et al,
2008), updated somewhat to match the MT domain,
to perform incremental training of potentially large
numbers of features. However, in each of these cases
the objective function used for training no longer
matches the final evaluation metric.
7 Conclusions
Our primary contribution is the first known exact
search algorithm for direct loss minimization on N -
best lists in multiple dimensions. Additionally, we
present approximations that consistently outperform
standard one-dimensional MERT on a competitive
machine translation system. While Och?s method of
MERT is generally quite successful, there are cases
where it does quite poorly. A more global search
such as LP-MERT lowers the expected risk of such
poor solutions. This is especially important for cur-
rent machine translation systems that rely heavily on
MERT, but may also be valuable for other textual ap-
46
plications. Recent speech recognition systems have
also explored combinations of more acoustic and lan-
guage models, with discriminative training of 5-10
features rather than one million (Lo?o?f et al, 2010);
LP-MERT could be valuable here as well.
The one-dimensional algorithm of Och (2003)
has been subject to study and refinement for nearly
a decade, while this is the first study of multi-
dimensional approaches. We demonstrate the poten-
tial of multi-dimensional approaches, but we believe
there is much room for improvement in both scalabil-
ity and speed. Furthermore, a natural line of research
would be to extend LP-MERT to compact representa-
tions of the search space, such as hypergraphs.
There are a number of broader implications from
this research. For instance, LP-MERT can aid in the
evaluation of research on MERT. This approach sup-
plies a truly optimal vector as ground truth, albeit
under limited conditions such as a constrained direc-
tion set, a reduced number of features, or a smaller
set of sentences. Methods can be evaluated based on
not only improvements over prior approaches, but
also based on progress toward a global optimum.
Acknowledgements
We thank Xiaodong He, Kristina Toutanova, and
three anonymous reviewers for their valuable sug-
gestions.
Appendix A: Proof of optimality
In this appendix, we prove that LP-MERT (Algorithm 2)
is exact. As noted before, the na??ve approach of solving
Eq. 4 is to enumerate allO(NS) hypotheses combinations
inM, discard the ones that are not extreme, and return
the best scoring one. LP-MERT relies on algorithmic
improvements to speed up this approach, and we now show
that none of them affect the optimality of the solution.
Divide-and-conquer. Divide-and-conquer in Algo-
rithm 2 discards any partial hypothesis combination
h[m(j) . . .m(k)] if it is not extreme, even before consid-
ering any extension h[m(i) . . .m(j) . . .m(k) . . .m(l)].
This does not sacrifice optimality, since if conv(h;H)
is false, then conv(h;H ?G) is false for any set G.
Proof: Assume conv(h;H) is false, so h is interior to
H . By definition, any interior point h can be written as
a linear combination of other points: h =?i ?ihi, with
?i(hi ? H , hi 6= h, ?i ? 0) and ?i ?i = 1. This samecombination of points also demonstrates that h is interior
to H ?G, thus conv(h;H ?G) is false as well.
Sparse hypothesis combination. We show here
that the simplification of linear programs in Section 3.1.1
from size O(NS) to size O(NS) does not change the
value of conv(h;H). More specifically, this means that
linear optimization of the output of the COMBINE method
at lines 26-27 of function GETNEXTBEST does not
introduce any error. Let (g1 . . .gU ) and (h1 . . .hV ) be
two N -best lists to be combined, then:
conv
(
gu + hv;
U?
i=1
(gi + hv) ?
V?
j=1
(gu + hj)
)
= conv
(
gu + hv;
U?
i=1
V?
j=1
(gi + hj)
)
Proof: To prove this equality, it suffices to show that: (1)
if gu+hv is interior wrt. the first conv binary predicate
in the above equation, then it is interior wrt. the second
conv, and (2) if gu+hv is interior wrt. the second conv,
then it is interior wrt. the first conv. Claim (1) is evident,
since the set of points in the first conv is a subset of the
other set of points. Thus, we only need to prove (2). We
first geometrically translate all points by ?gu?hv . Since
gu+hv is interior wrt. the second conv, we can write:
0 =
U?
i=1
V?
j=1
?i,j(gi + hj ? gu ? hv)
=
U?
i=1
V?
j=1
?i,j(gi ? gu) +
U?
i=1
V?
j=1
?i,j(hj ? hv)
=
U?
i=1
(gi ? gu)
V?
j=1
?i,j +
V?
j=1
(hj ? hv)
U?
i=1
?i,j
=
U?
i=1
??i(gi ? gu) +
V?
j=1
??U+j(hj ? hv)
where {??i}1?i?U+V values are computed from
{?i,j}1?i?U,1?j?V as follows: ??i =
?
j ?i,j , i ? [1, U ]and ??U+j =
?
i ?i,j , j ? [1, V ]. Since the interiorpoint is 0, ??i values can be scaled so that they sum to 1
(necessary condition in the definition of interior points),
which proves that the following predicate is false:
conv
(
0;
U?
i=1
(gi ? gu) ?
V?
j=1
(hj ? hv)
)
which is equivalent to stating that the following is false:
conv
(
gu + hv;
U?
i=1
(gi + hv) ?
V?
j=1
(gu + hj)
)
47
References
C. Bradford Barber, David P. Dobkin, and Hannu Huhdan-
paa. 1996. The QuickHull algorithm for convex hulls.
ACM Trans. Math. Softw., 22:469?483.
C. Buchta, J. Muller, and R. F. Tichy. 1985. Stochastical
approximation of convex bodies. Math. Ann., 271:225?
235.
A. Bykat. 1978. Convex hull of a finite set of points in
two dimensions. Inf. Process. Lett., 7(6):296?298.
Daniel Cer, Dan Jurafsky, and Christopher D. Manning.
2008. Regularization and search for minimum error
rate training. In Proceedings of the Third Workshop on
Statistical Machine Translation, pages 26?34.
Samidh Chatterjee and Nicola Cancedda. 2010. Min-
imum error rate training by sampling the translation
lattice. In Proceedings of the 2010 Conference on Em-
pirical Methods in Natural Language Processing, pages
606?615. Association for Computational Linguistics.
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and structural
translation features. In EMNLP.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
W. Chou, C. H. Lee, and B. H. Juang. 1993. Minimum
error rate training based on N-best string models. In
Proc. IEEE Int?l Conf. Acoustics, Speech, and Signal
Processing (ICASSP ?93), pages 652?655, Vol. 2.
Kevin Duh and Katrin Kirchhoff. 2008. Beyond log-
linear models: boosted minimum error rate training for
programming N-best re-ranking. In Proceedings of the
46th Annual Meeting of the Association for Computa-
tional Linguistics on Human Language Technologies:
Short Papers, pages 37?40, Stroudsburg, PA, USA.
William F. Eddy. 1977. A new convex hull algorithm for
planar sets. ACM Trans. Math. Softw., 3:398?403.
Liang Huang and David Chiang. 2005. Better k-best pars-
ing. In Proceedings of the Ninth International Work-
shop on Parsing Technology, pages 53?64, Stroudsburg,
PA, USA.
Biing-Hwang Juang, Wu Hou, and Chin-Hui Lee. 1997.
Minimum classification error rate methods for speech
recognition. Speech and Audio Processing, IEEE Trans-
actions on, 5(3):257?265.
N. Karmarkar. 1984. A new polynomial-time algorithm
for linear programming. Combinatorica, 4:373?395.
Victor Klee. 1966. Convex polytopes and linear program-
ming. In Proceedings of the IBM Scientific Computing
Symposium on Combinatorial Problems.
Philipp Koehn, Hieu Hoang, Alexandra Birch Mayne,
Christopher Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proc. of ACL, Demonstration Session.
Shankar Kumar, Wolfgang Macherey, Chris Dyer, and
Franz Och. 2009. Efficient minimum error rate train-
ing and minimum Bayes-risk decoding for translation
hypergraphs and lattices. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference on Natural
Language Processing of the AFNLP: Volume 1, pages
163?171.
Gregor Leusch, Evgeny Matusov, and Hermann Ney.
2008. Complexity of finding the BLEU-optimal hy-
pothesis in a confusion network. In Proceedings of the
Conference on Empirical Methods in Natural Language
Processing, pages 839?847, Stroudsburg, PA, USA.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Ganitke-
vitch, Sanjeev Khudanpur, Lane Schwartz, Wren N. G.
Thornton, Jonathan Weese, and Omar F. Zaidan. 2009.
Joshua: an open source toolkit for parsing-based MT.
In Proc. of WMT.
P. Liang, A. Bouchard-Co?te?, D. Klein, and B. Taskar.
2006. An end-to-end discriminative approach to ma-
chine translation. In International Conference on Com-
putational Linguistics and Association for Computa-
tional Linguistics (COLING/ACL).
Chin-Yew Lin and Franz Josef Och. 2004. ORANGE:
a method for evaluating automatic evaluation metrics
for machine translation. In Proceedings of the 20th
international conference on Computational Linguistics,
Stroudsburg, PA, USA.
Jonas Lo?o?f, Ralf Schlu?ter, and Hermann Ney. 2010. Dis-
criminative adaptation for log-linear acoustic models.
In INTERSPEECH, pages 1648?1651.
Wolfgang Macherey, Franz Och, Ignacio Thayer, and
Jakob Uszkoreit. 2008. Lattice-based minimum error
rate training for statistical machine translation. In Pro-
ceedings of the 2008 Conference on Empirical Methods
in Natural Language Processing, pages 725?734.
David McAllester, Tamir Hazan, and Joseph Keshet. 2010.
Direct loss minimization for structured prediction. In
Advances in Neural Information Processing Systems
23, pages 1594?1602.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online large-margin training of dependency
parsers. In Proceedings of the 43rd Annual Meeting
on Association for Computational Linguistics, pages
91?98.
Ryan McDonald. 2006. Discriminative sentence compres-
sion with soft syntactic constraints. In Proceedings of
EACL, pages 297?304.
Robert C. Moore and Chris Quirk. 2008. Random restarts
in minimum error rate training for statistical machine
translation. In Proceedings of the 22nd International
48
Conference on Computational Linguistics - Volume 1,
pages 585?592.
J. A. Nelder and R. Mead. 1965. A simplex method for
function minimization. Computer Journal, 7:308?313.
Franz Josef Och and Hermann Ney. 2002. Discriminative
training and maximum entropy models for statistical
machine translation. In Proc. of the 40th Annual Meet-
ing of the Association for Computational Linguistics,
pages 295?302.
Franz Josef Och. 2003. Minimum error rate training for
statistical machine translation. In Proc. of ACL.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. BLEU: a method for automatic evalu-
ation of machine translation. In Proc. of ACL.
M.J.D. Powell. 1964. An efficient method for finding
the minimum of a function of several variables without
calculating derivatives. Comput. J., 7(2):155?162.
William H. Press, Saul A. Teukolsky, William T. Vetter-
ling, and Brian P. Flannery. 2007. Numerical Recipes:
The Art of Scientific Computing. Cambridge University
Press, 3rd edition.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005.
Dependency treelet translation: syntactically informed
phrasal SMT. In Proc. of ACL, pages 271?279.
David A. Smith and Jason Eisner. 2006. Minimum risk
annealing for training log-linear models. In Proceed-
ings of the COLING/ACL on Main conference poster
sessions, pages 787?794, Stroudsburg, PA, USA.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation. In
Proc. of AMTA, pages 223?231.
Andreas Stolcke, Yochai Knig, and Mitchel Weintraub.
1997. Explicit word error minimization in N-best list
rescoring. In In Proc. Eurospeech, pages 163?166.
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and Hideki
Isozaki. 2007. Online large-margin training for statisti-
cal machine translation. In EMNLP-CoNLL.
Omar F. Zaidan and Chris Callison-Burch. 2009. Feasibil-
ity of human-in-the-loop minimum error rate training.
In Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing: Volume 1 -
Volume 1, pages 52?61.
Richard Zens, Sasa Hasan, and Hermann Ney. 2007.
A systematic comparison of training criteria for sta-
tistical machine translation. In Proceedings of the
2007 Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL), pages 524?532,
Prague, Czech Republic.
Bing Zhao and Shengyuan Chen. 2009. A simplex Armijo
downhill algorithm for optimizing statistical machine
translation decoding parameters. In Proceedings of
Human Language Technologies: The 2009 Annual Con-
ference of the North American Chapter of the Associ-
ation for Computational Linguistics, Companion Vol-
ume: Short Papers, pages 21?24.
49
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1044?1054,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Joint Language and Translation Modeling with Recurrent Neural Networks
Michael Auli, Michel Galley, Chris Quirk, Geoffrey Zweig
Microsoft Research
Redmond, WA, USA
{michael.auli,mgalley,chrisq,gzweig}@microsoft.com
Abstract
We present a joint language and transla-
tion model based on a recurrent neural net-
work which predicts target words based on
an unbounded history of both source and tar-
get words. The weaker independence as-
sumptions of this model result in a vastly
larger search space compared to related feed-
forward-based language or translation models.
We tackle this issue with a new lattice rescor-
ing algorithm and demonstrate its effective-
ness empirically. Our joint model builds on a
well known recurrent neural network language
model (Mikolov, 2012) augmented by a layer
of additional inputs from the source language.
We show competitive accuracy compared to
the traditional channel model features. Our
best results improve the output of a system
trained on WMT 2012 French-English data by
up to 1.5 BLEU, and by 1.1 BLEU on average
across several test sets.
1 Introduction
Recently, several feed-forward neural network-
based language and translation models have
achieved impressive accuracy improvements on sta-
tistical machine translation tasks (Allauzen et al,
2011; Le et al, 2012b; Schwenk et al, 2012). In this
paper we focus on recurrent neural network archi-
tectures, which have recently advanced the state of
the art in language modeling (Mikolov et al, 2010;
Mikolov et al, 2011a; Mikolov, 2012), outperform-
ing multi-layer feed-forward based networks in both
perplexity and word error rate in speech recognition
(Arisoy et al, 2012; Sundermeyer et al, 2013). The
major attraction of recurrent architectures is their
potential to capture long-span dependencies since
predictions are based on an unbounded history of
previous words. This is in contrast to feed-forward
networks as well as conventional n-gram models,
both of which are limited to fixed-length contexts.
Building on the success of recurrent architectures,
we base our joint language and translation model
on an extension of the recurrent neural network lan-
guage model (Mikolov and Zweig, 2012) that intro-
duces a layer of additional inputs (?2).
Most previous work on neural networks for
speech recognition or machine translation used a
rescoring setup based on n-best lists (Arisoy et al,
2012; Mikolov, 2012) for evaluation, thereby side
stepping the algorithmic and engineering challenges
of direct decoder-integration.1 Instead, we exploit
lattices, which offer a much richer representation
of the decoder output, since they compactly encode
an exponential number of translation hypotheses in
polynomial space. In contrast, n-best lists are typi-
cally very redundant, representing only a few com-
binations of top scoring arcs in the lattice. A major
challenge in lattice rescoring with a recurrent neural
network model is the effect of the unbounded history
on search since the usual dynamic programming as-
sumptions which are exploited for efficiency do not
hold up anymore. We apply a novel algorithm to the
task of rescoring with an unbounded language model
and empirically demonstrate its effectiveness (?3).
The algorithm proves robust, leading to signif-
icant improvements with the recurrent neural net-
work language model over a competitive n-gram
baseline across several language pairs. We even ob-
serve consistent gains when pairing the model with a
large n-gram model trained on up to 575 times more
1One notable exception is Le et al (2012a) who rescore reorder-
ing lattices with a feed-forward network-based model.
1044
data, demonstrating that the model provides comple-
mentary information (?4).
Our joint modeling approach is based on adding a
continuous space representation of the foreign sen-
tence as an additional input to the recurrent neu-
ral network language model. With this extension,
the language model can measure the consistency
between the source and target words in a context-
sensitive way. The model effectively combines the
functionality of both the traditional channel and lan-
guage model features. We test the power of this
new model by using it as the only source of tradi-
tional channel information. Overall, we find that the
model achieves accuracy competitive with the older
channel model features and that it can improve over
the gains observed with the recurrent neural network
language model (?5).
2 Model Structure
We base our model on the recurrent neural network
language model of Mikolov et al (2010) which is
factored into an input layer, a hidden layer with re-
current connections, and an output layer (Figure 1).
The input layer encodes the target language word at
time t as a 1-of-N vector et, where |V | is the size
of the vocabulary, and the output layer yt represents
a probability distribution over target words; both of
size |V |. The hidden layer state ht encodes the his-
tory of all words observed in the sequence up to time
step t. This model is extended by an auxiliary input
layer ft which provides complementary information
to the input layer (Mikolov and Zweig, 2012). While
the auxiliary input layer can be used to feed in arbi-
trary additional information, we focus on encodings
of the foreign sentence (?5).
The state of the hidden layer is determined by the
input layer, the auxiliary input layer and the hidden
layer configuration of the previous time step ht?1.
The weights of the connections between the layers
are summarized in a number of matrices: U, F and
W, represent weights from the input layer to the hid-
den layer, from the auxiliary input layer to the hid-
den layer, and from the previous hidden layer to the
current hidden layer, respectively. Matrix V repre-
sents connections between the current hidden layer
and the output layer; G represents direct weights be-
tween the auxiliary input and output layers.
et
ht-1
ft
ht
yt
V
G
F
W
U
D
Figure 1: Structure of the recurrent neural network
model, including the auxiliary input layer ft.
The hidden and output layers are computed via a
series of matrix-vector products and non-linearities:
ht = s(Uet +Wht?1 + Ff t)
yt = g(Vht +Gf t)
where
s(z) =
1
1 + exp {?z}
, g(zm) =
exp {zm}
?
k exp {zk}
are sigmoid and softmax functions, respectively.
Additionally, the network is interpolated with a
maximum entropy model of sparse n-gram features
over input words (Mikolov et al, 2011a).2 The max-
imum entropy weights are added to the output acti-
vations before computing the softmax.
The model is optimized via a maximum likeli-
hood objective function using stochastic gradient
descent. Training is based on the back propaga-
tion through time algorithm, which unrolls the net-
work and then computes error gradients over mul-
tiple time steps (Rumelhart et al, 1986). Af-
ter training, the output layer represents posteriors
p(et+1|ett?n+1,ht, ft); the probabilities of words in
the output vocabulary given the n previous input
words ett?n+1, the hidden layer configuration ht as
well as the auxiliary input layer configuration ft.
2While these features depend on multiple input words, we de-
picted them for simplicity as a connection between the current
input word vector et and the output layer (D).
1045
Na??ve computation of the probability distribution
over the next word is very expensive for large vo-
cabularies. A well established efficiency trick uses
word-classing to create a more efficient two-step
process (Goodman, 2001; Emami and Jelinek, 2005;
Mikolov et al, 2011b) where each word is assigned
a unique class. To compute the probability of a
word, we first compute the probability of its class,
and then multiply it by the probability of the word
conditioned on the class:
p(et+1|e
t
t?n+1,ht, ft) =
p(ci|e
t
t?n+1,ht, ft)? p(et+1|ci, e
t
t?n+1,ht, ft)
This factorization reduces the complexity of com-
puting the output probabilities from O(|V |) to
O(|C| + maxi |ci|) where |C| is the number of
classes and |ci| is the number of words in class
ci. The best case complexity O(
?
|V |) requires the
number of classes and words to be evenly balanced,
i.e., each class contains exactly as many words as
there are classes.
3 Lattice Rescoring with an Unbounded
Language Model
We evaluate our joint language and translation
model in a lattice rescoring setup, allowing us to
search over a much larger space of translations than
would be possible with n-best lists. While very
space efficient, lattices also impose restrictions on
the context available to features, a particularly chal-
lenging setting for our model which depends on the
entire prefix of a translation. In the ensuing de-
scription we introduce a new algorithm to efficiently
tackle this issue.
Phrase-based decoders operate by maintaining a
set of states representing competing translations, ei-
ther partial or complete. Each state is scored by a
number of features including the n-gram language
model. The independence assumptions of the fea-
tures determine the amount of context each state
needs to maintain in order for it to be possible to
assign a score to it. For example, a trigram language
model is indifferent to any context other than the
two immediately preceding words. Assuming the
trigram model dominates the Markov assumptions
of all other features, which is typically the case, then
we have to maintain at least two words at each state,
also known as the n-gram context.
1: function RESCORELATTICE(k, V , E, s, T )
2: Q? TOPOLOGICALLY-SORT(V )
3: for all v in V do . Heaps of split-states
4: Hv ? MINHEAP()
5: end for
6: h0 ? ~0 . Initialize start-state
7: Hs.ADD(h0)
8: for all v in Q do . Examine outgoing arcs
9: for ?v, x? in E do
10: for h in Hv do . Extend LM states
11: h? ? SCORERNN(h, phrase(h))
12: parent(h?)? h . Backpointers
13: if Hx.size() ? k? . Beam width
14: Hx.MIN()<score(h?) then
15: Hx.REMOVEMIN()
16: if Hx.size()<k then
17: Hx.ADD(h?)
18: end for
19: end for
20: end for
21: I = MAXHEAP()
22: for all t in T do . Find best final split-state
23: I.MERGE(Ht)
24: end for
25: return I.MAX()
26: end function
Figure 2: Push-forward rescoring with a recurrent neu-
ral network language model given a beam-width for lan-
guage model split-states k, decoder states V , edges E, a
start state s and final states T .
However, a recurrent neural network language
model makes much weaker independence assump-
tions. In fact, the predictions of such a model depend
on all previous words in the sentence, which would
imply a potentially very large context. But storing
all words is an inefficient solution from a dynamic
programming point of view. Fortunately, we do not
need to maintain entire translations as context in the
states: the recurrent model compactly encodes the
entire history of previous words in the hidden layer
configuration hi. It is therefore sufficient to add hi
as context, instead of the entire translation. The lan-
guage model can then simply score any new words
1046
based on hi from the previous state when a new state
is created.
A much larger problem is that items, that were
previously equivalent from a dynamic programming
perspective, may now be different. Standard phrase-
based decoders (Koehn et al, 2007) recombine de-
coder states with the same context into a single
state because they are equivalent to the model fea-
tures; usually recombination retains only the high-
est scoring candidate.3 However, if the context is
large, then the amount of recombination will de-
crease significantly, leading to less variety in the de-
coder beam. This was confirmed in preliminary ex-
periments where we simulated context sizes of up to
100 words but found that accuracy dropped by be-
tween 0.5-1.0 BLEU.
Integrating a long-span language model na??vely
requires to keep context equivalent to the entire left
prefix of the translation, a setting which would per-
mit very little recombination. Instead of using ineffi-
cient long-span contexts, we propose to maintain the
usual n-gram context and to keep a fixed number of
hidden layer configurations k at each decoder state.
This leads to a new split-state dynamic program
which splits each decoder state into at most k new
items, each with a separate hidden layer configura-
tion representing an unbounded history (Figure 2).
This maintains diversity in the explored translation
hypothesis space and preserves high-scoring hidden
layer configurations.
What is the effect of this strategy? To answer
this question we measured translation accuracy for
various settings of k on our lattice rescoring setup
(see ?4 for details). In the same experiment, we
compare lattices to n-best lists in terms of accuracy,
model score and wall time impact.4 The results (Ta-
ble 1 and Figure 3) show that reranking accuracy on
lattices is not significantly better, however, rescor-
ing lattices with k = 1 is much faster than n-best
lists. Similar observations have been made in previ-
ous work on minimum error-rate training (Macherey
3Assuming a max-translation decision rule. In a minimum-risk
setting, we may assign the sum of the scores of all candidates
to the retained item.
4We measured running times on an HP z800 workstation
equipped with 24 GB main memory and two Xeon E5640
CPUs with four cores each, clocked at 2.66 GHz. All experi-
ments were run single-threaded.
BLEU oracle sec/sent
Baseline 28.25 - 0.173
100-best 28.90 37.22 0.470
1000-best 28.99 40.06 3.920
lattice (k = 1) 29.00 43.50 0.093
lattice (k = 10) 29.04 43.50 0.599
lattice (k = 100) 29.03 43.50 4.531
Table 1: Rescoring n-best lists and lattices with various
language model beam widths k. Accuracy is based on
the news2011 French-English task. Timing results are in
addition to the baseline.
Figure 3: BLEU vs. log probabilities of 1-best transla-
tions when rescoring n-best lists and lattices (cf. Table 1).
et al, 2008). The recurrent language model adds an
overhead of about 54% at k = 1 on top of the time
to produce the baseline 1-best output, a consider-
able but not necessarily prohibitive overhead. Larger
values of k return higher probability solutions, but
there is little impact on accuracy: the BLEU score
is nearly identical when retaining up to 100 histories
compared to keeping only the highest scoring.
While surprising at first, we believe that this ef-
fect is due to the high similarity of the translations
represented by the histories in the beam. Each his-
tory represents a different translation but all transla-
tion hypothesis share the same n-gram context, and,
more importantly, they are translations of the same
foreign words, since they have exactly the same cov-
erage vector. These commonalities are likely to re-
sult in similar recurrent histories, which in turn re-
duces the effect of aggressive pruning.
4 Language Model Experiments
Recurrent neural network language models have
previously only been used in n-best rescoring
1047
settings and on small-scale tasks with baseline
language models trained on only 17.5m words
(Mikolov, 2012). We extend this work by experi-
menting on lattices using strong baselines with n-
gram models trained on over one billion words and
by evaluating on a number of language pairs.
4.1 Experimental Setup
Baseline. We experiment with an in-house phrase-
based system similar to Moses (Koehn et al,
2003), scoring translations by a set of common fea-
tures including maximum likelihood estimates of
source given target mappings pMLE(e|f) and vice
versa pMLE(f |e), as well as lexical weighting es-
timates pLW (e|f) and pLW (f |e), word and phrase-
penalties, a linear distortion feature and a lexicalized
reordering feature. Log-linear weights are estimated
with minimum error rate training (Och, 2003).
Evaluation. We use training and test data
from the WMT 2012 campaign and report results
on French-English, German-English and English-
German. Translation models are estimated on 102m
words of parallel data for French-English, 91m
words for German-English and English-German; be-
tween 3.5-5m words are newswire, depending on the
language pair, and the remainder are parliamentary
proceedings. The baseline systems use two 5-gram
modified Kneser-Ney language models; the first is
estimated on the target-side of the parallel data,
while the second is based on a large newswire corpus
released as part of the WMT campaign. For French-
English and German-English we use a language
model based on 1.15bn words, and for English-
German we train a model on 327m words. We eval-
uate on the newswire test sets from 2010-2011 con-
taining between 2034-3003 sentences. Log-linear
weights are estimated on the 2009 data set compris-
ing 2525 sentences. We rescore the lattices produced
by the baseline systems with an aggressive but effec-
tive context beam of k = 1 that did not harm accu-
racy in preliminary experiments (?3).
Neural Network Language Model. The vocab-
ularies of the language models are comprised of
the words in the training set after removing single-
tons. We obtain word-classes using a version of
Brown-Clustering with an additional regularization
term to optimize the runtime of the language model
(Brown et al, 1992; Zweig and Makarychev, 2013).
Direct connections use maximum entropy features
over unigrams, bigrams and trigrams (Mikolov et al,
2011a). We use the standard settings for the model
with the default learning rate ? = 0.1 that decays
exponentially if the validation set entropy does not
increase after each epoch. Back propagation through
time computes error gradients over the past twenty
time steps. Training is stopped after 20 epochs or
when the validation entropy does not decrease over
two epochs. We experiment with varying training
data sizes and randomly draw the data from the same
corpora used for the baseline systems. Throughout,
we use a hidden layer size of 100 which provided a
good trade-off between time and accuracy in initial
experiments.
4.2 Results
Training times for neural networks can be a major
bottleneck. Recurrent architectures are particularly
hard to parallelize due to their inherent dependence
on the previous hidden layer configuration. One
straightforward way to influence training time is to
change the size of the training corpus.
Our results (Table 2, Table 3 and Table 4) show
that even small models trained on only two million
words significantly improve over the 1-best decoder
output (Baseline); this represents only 0.6 percent
of the data available to the n-gram model used by
the baseline. Models of this size can be trained in
only about 3.5 hours. A model trained on 50m words
took 63 hours to train. When paired with an n-gram
model trained on 25 times more data, accuracy im-
proved by up to 0.7 BLEU on French-English.
5 Joint Model Experiments
In the next set of experiments, we turn to the joint
language and translation model, an extension of the
recurrent neural network language model with ad-
ditional inputs for the foreign sentence. We first
introduce two continuous space representations of
the foreign sentence (?5.1). Using these represen-
tations we evaluate the accuracy of the joint model
in the lattice rescoring setup and compare against the
traditional translation channel model features (?5.2).
Next, we establish an upper bound on accuracy for
the joint model via an oracle experiment (?5.3). In-
spired by the results of the oracle experiment we
1048
dev news2010 news2011 newssyscomb2011 Avg(test)
Baseline 26.6 27.6 28.3 27.5 27.8
+RNNLM (2m) 27.5 28.1 28.6 28.1 28.3
+RNNLM (50m) 27.7 28.2 29.0 28.1 28.5
Table 2: French-English results when rescoring with the recurrent neural network language model; the baseline relies
on an n-gram model trained on 1.15bn words.
dev news2010 news2011 newssyscomb2011 Avg(test)
Baseline 21.2 20.7 19.2 20.6 20.0
+RNNLM (2m) 21.8 20.9 19.4 20.9 20.3
+RNNLM (50m) 22.1 21.1 19.7 21.0 20.5
Table 3: German-English results when rescoring with the recurrent neural network language model.
dev news2010 news2011 newssyscomb2011 Avg(test)
Baseline 15.2 15.6 14.3 15.7 15.1
+RNNLM (2m) 15.7 15.9 14.6 16.0 15.4
+RNNLM (50m) 15.8 15.9 14.7 16.1 15.5
Table 4: English-German results when rescoring with the recurrent neural network language model; the baseline relies
on an n-gram model trained on 327m words.
train a transform between the source words and the
reference representations. This leads to the best re-
sults improving 1.5 BLEU over the 1-best decoder
output and adding 0.2 BLEU on average to the gains
achieved by the recurrent language model (?5.4).
Setup. Conventional language models can be
trained on monolingual or bilingual data; however,
the joint model can only be trained on the latter.
In order to control for data size effects, we restrict
training of all models, including the baseline n-gram
model, to the target side of the parallel corpus, about
102m words for French-English. Furthermore we
train recurrent models only on the newswire portion
(about 3.5m words for training and 250k words for
validation) since initial experiments showed compa-
rable results to using the full parallel corpus, avail-
able to the baseline. This is reasonable since the test
data is newswire. Also, it allows for more rapid ex-
perimentation.
5.1 Foreign Sentence Representations
We represent foreign sentences either by latent se-
mantic analysis (LSA; Deerwester et al 1990) or by
word encodings produced as a by-product of train-
ing the recurrent neural network language model on
the source words.
LSA is widely used for representing words and
documents in low-dimensional vector space. The
method applies reduced singular value decomposi-
tion (SVD) to a matrix M of word counts; in our
setting, rows represent sentences and columns rep-
resent foreign words. SVD reduces the number
of columns while preserving similarity among the
rows, effectively mapping from a high-dimensional
representation of a sentence, as a set of words, to a
low-dimensional set of concepts. The output of SVD
is an approximation of M by three matrices: T con-
tains single word representations, R represents full
sentences, and S is a diagonal scaling matrix:
M ? TSRT
Given vocabulary V and n sentences, we construct
M as a matrix of size |V ?n|. The ij-th entry is the
number of times word i occurs in sentence j, also
known as the term frequency value; the entry is also
weighted by the inverse document frequency, the rel-
ative importance of word i among all sentences, ex-
pressed as the negative logarithm of the fraction of
sentences in which word i occurs.
As a second representation we use single word
1049
embeddings implicitly learned by the input layer
weights U of the recurrent neural network language
model (?2), denoted as RNN. Each word is repre-
sented by a vector of size |hi|, the number of neu-
rons in the hidden layer; in our experiments, we
consider concatenations of individual word vectors
to represent foreign word contexts. These encodings
have previously been found to capture syntactic and
semantic regularities (Mikolov et al, 2013) and are
readily available in our experimental framework via
training a recurrent neural network language model
on the source-side of the parallel corpus.
5.2 Results
We first experiment with the two previously intro-
duced representations of the source-side sentence.
Table 5 shows the results compared to the 1-best de-
coder output and an RNN language model (target-
only). We first try LSA encodings of the entire
foreign sentence as 80 or 240 dimensional vectors
(sent-lsa-dim80, sent-lsa-dim240). Next, we experi-
ment with single-word RNN representations of slid-
ing word-windows in the hope of representing rel-
evant context more precisely. Word-windows are
constructed relative to the source words aligned to
the current target word, and individual word vec-
tors are concatenated into a single vector. We
first try contexts which do not include the aligned
source words, in the hope of capturing information
not already modeled by the channel models, start-
ing with the next five words (ww-rnn-dim50.n5),
the five previous and the next five words (ww-rnn-
dim50.p5n5) as well as the previous three words
(ww-rnn-dim50.p3). Next, we experiment with
word-windows of up to five aligned source words
(ww-rnn-dim50.c5). Finally, we try contexts based
on LSA word vectors (ww-lsa-dim50.n5, ww-lsa-
dim50.p3).5
While all models improve over the baseline, none
significantly outperforms the recurrent neural net-
work language model in terms of BLEU. However,
the perplexity results suggest that the models uti-
lize the foreign representations since all joint mod-
els improve vastly over the target-only language
5We ignore the coverage vector when determining word-
windows which risks including already translated words.
Building word-windows based on the coverage vector requires
additional state in a rescoring setting meant to be light-weight.
?p(f |e)
?p(e|f) ?p(e|f)
Baseline without CM 24.0 22.5
+ target-only 24.5 22.6
+ sent-lsa-dim240 24.9 23.3
+ ww-rnn-dim50.n5 24.9 24.0
+ ww-rnn-dim50.p5n5 24.6 23.7
+ ww-rnn-dim50.p3 24.6 22.3
+ ww-rnn-dim50.c5 24.9 24.0
+ ww-lsa-dim50.n5 24.8 23.9
+ ww-lsa-dim50.p3 23.8 23.2
Table 6: Comparison of the joint model and the chan-
nel model features (CM) by removing channel features
corresponding to ?p(e|f) from the lattices, or both di-
rections ?p(e|f),?p(f |e) and replacing them by vari-
ous joint models. We re-tuned the log-linear weights for
different feature-sets. Accuracy is based on the average
BLEU over news2010, newssyscomb2010, news2011.
model. The lowest perplexity is achieved by the
context covering the aligned source words (ww-rnn-
dim50.c5) since the source words are a better pre-
dictor of the target words than outside context.
The experiments so far measured if the joint
model can improve in addition to the four channel
model features used by the baseline, that is, the max-
imum likelihood and lexical translation features in
both translation directions. The joint model clearly
overlaps with these features, but how well does
the recurrent model perform compared against the
channel model features? To answer this question,
we removed channel model features corresponding
to the same translation direction as the joint model,
specifically pMLE(e|f) and pLW (e|f), from the lat-
tices and measured the effect of adding the joint
models.
The results (Table 6, column ?p(e|f)) clearly
show that our joint models are competitive with the
channel model features by outperforming the orig-
inal baseline with all channel model features (24.7
BLEU) by 0.2 BLEU (ww-rnn-dim50.n5, ww-rnn-
dim50.c5). As a second experiment, we removed all
channel model features (column ?p(e|f), p(f |e)),
diminishing baseline accuracy to 22.5 BLEU. In this
setting, the best joint model is able to make up 1.5
of the 2.2 BLEU lost due to removal of the channel
1050
dev news2010 news2011 newssyscomb2010 Avg(test) PPL
Baseline 24.3 24.4 25.1 24.3 24.7 341
target-only 25.1 25.1 26.4 25.0 25.6 218
sent-lsa-dim80 25.2 25.2 26.3 25.1 25.6 147
sent-lsa-dim240 25.1 25.0 26.2 24.9 25.4 126
ww-rnn-dim50.n5 24.9 25.0 26.3 24.8 25.4 61
ww-rnn-dim50.p5n5 25.0 24.8 26.2 24.7 25.3 59
ww-rnn-dim50.p3 25.1 25.1 26.5 24.9 25.6 143
ww-rnn-dim50.c5 24.8 24.9 26.0 24.8 25.3 16
ww-lsa-dim50.n5 25.0 25.0 26.2 24.8 25.4 76
ww-lsa-dim50.p3 25.1 25.1 26.5 24.9 25.6 151
Table 5: Translation accuracy of the joint model with various encodings of the foreign sentence measured on the
French-English task. Perplexity (PPL) is based on news2011.
model features, while modeling only a single trans-
lation direction. This setup also shows the negligible
effect of the target-only language model in the ab-
sence of translation scores, whereas the joint models
are much more effective since they do model transla-
tion. Overall, the best joint models prove very com-
petitive to the traditional channel features.
5.3 Oracle Experiment
The previous section examined the effect of a set
of basic foreign sentence representations. Although
we find some benefit from these representations, the
differences are not large. One might naturally ask
whether there is greater potential upside from this
channel model. Therefore we turn to measuring the
upper bound on accuracy for the joint approach as a
whole.
Specifically, we would like to find a bound on ac-
curacy given an ideal representation of the source
sentence. To answer this question, we conducted an
experiment where the joint model has access to an
LSA representation of the reference translation.
Table 7 shows that the joint approach has an ora-
cle accuracy of up to 4.3 BLEU above the baseline.
This clearly confirms that the joint approach can ex-
ploit the additional information to improve BLEU,
given a good enough representation of the foreign
sentence. In terms of perplexity, we see an improve-
ment of up to 65% over the target-only model. It
should be noted that since LSA representations are
computed on reference words, perplexity no longer
has its standard meaning.
BLEU PPL
Baseline 25.2 341
target-only 26.4 218
oracle (sent-lsa-dim40) 27.7 124
oracle (sent-lsa-dim80) 28.5 103
oracle (sent-lsa-dim160) 29.0 86
oracle (sent-lsa-dim240) 29.5 76
Table 7: Oracle accuracy of the joint model when us-
ing an LSA encoding of the references, measured on the
news2011 French-English task.
5.4 Target Language Projections
Our experiments so far showed that joint models
based on direct representations of the source words
are very competitive to the traditional channel mod-
els (?5.2). However, these experiments have not
shown any improvements over the normal recurrent
neural network language model. The previous sec-
tion demonstrated that good representations can lead
to substantial gains (?5.3). In order to bridge the gap,
we propose to learn a separate transform from the
foreign words to an encoding of the reference target
words, thus making the source-side representations
look more like the target-side encodings used in the
oracle experiment.
Specifically, we learn a linear transform
d? : x? r mapping directly from a vector en-
coding of the foreign sentence x to an l-dimensional
LSA representation r of the reference sentence. At
test and training time we apply d? to the foreign
words and use the transformation instead of a direct
1051
dev news2010 news2011 newssyscomb2010 Avg(test) PPL
Baseline 24.3 24.4 25.1 24.3 24.7 341
target-only 25.1 25.1 26.4 25.0 25.6 218
proj-lsa-dim40 25.1 25.3 26.5 25.2 25.8 145
proj-lsa-dim80 25.1 25.3 26.6 25.2 25.8 134
Table 8: Translation accuracy of the joint model with a source-target transform, measured on the French-English task.
Perplexity (PPL) is based on news2011; differences to target-only are significant at the p < 0.001 level.
source-side representation.
The transform models all foreign words in the par-
allel corpus except singletons, which are collapsed
into a unique class, similar to the recurrent neural
network language model. We train the transform to
minimize the squared error with respect to the ref-
erence LSA vector using an SGD online learner:
?? = argmin
?
n?
i=1
(
ri ? d?(xi)
)2
(1)
We found a simple constant learning rate, tuned
on the validation data, to be as effective as sched-
ules based on constant decay, or reducing the learn-
ing rate when the validation error increased. Our
feature-set includes unigram and bigram word fea-
tures. The value of unigram features is simply the
unigram count in that sentence; bigram features re-
ceive a weight of the bigram count divided by two
to help prevent overfitting. Then the vector for each
sentence was divided by its L2 norm. Both weight-
ing and normalization led to substantial improve-
ments in test set error. More complex features such
as skip-bigrams, trigrams and character n-grams did
not yield any significant improvements. Even this
representation of sentences is composed of a large
number of instances, and so we resorted to feature
hashing by computing feature ids as the least signif-
icant 20 bits of each feature name. Our best trans-
form achieved a cosine similarity of 0.816 on the
training data, 0.757 on the validation data, and 0.749
on news2011.
The results (Table 8) show that the transform im-
proves over the recurrent neural network language
model on all test sets and by 0.2 BLEU on average.
We verified significance over the target-only model
using paired bootstrap resampling (Koehn, 2004)
over all test sets (7526 sentences) at the p < 0.001
level. Overall, we improve accuracy by up to 1.5
BLEU and by 1.1 BLEU on average across all test
sets over the decoder 1-best with our joint language
and translation model.
6 Related Work
Our approach of combining language and translation
modeling is very much in line with recent work on
n-gram-based translation models (Crego and Yvon,
2010), and more recently continuous space-based
translation models (Le et al, 2012a; Gao et al,
2013). The joint model presented in this paper dif-
fers in a number of key aspects: we use a recur-
rent architecture representing an unbounded history
of both source and target words, rather than a feed-
forward style network. Feed-forward networks and
n-gram models have a finite history which makes
predictions independent of anything but a small his-
tory of words. Furthermore, we only model the
target-side which is different to previous work mod-
eling both sides.
We introduced a new algorithm to tackle lattice
rescoring with an unbounded model. The auto-
matic speech recognition community has previously
addressed this issue by either approximating long-
span language models via simpler but more tractable
models (Deoras et al, 2011b), or by identifying con-
fusable subsets of the lattice from which n-best lists
are constructed and rescored (Deoras et al, 2011a).
We extend their work by directly mapping a recur-
rent neural network model onto the structure of the
lattice, rescoring all states instead of focusing only
on subsets.
7 Conclusion and Future Work
Joint language and translation modeling with recur-
rent neural networks leads to substantial gains over
the 1-best decoder output, raising accuracy by up
to 1.5 BLEU and by 1.1 BLEU on average across
1052
several test sets. The joint approach also improves
over the gains of the recurrent neural network lan-
guage model, adding 0.2 BLEU on average across
several test sets. Our models are competitive to the
traditional channel models, outperforming them in a
head-to-head comparison.
Furthermore, we tackled the issue of lattice
rescoring with an unbounded recurrent model by
means of a novel algorithm that keeps a beam of re-
current histories. Finally, we have shown that the
recurrent neural network language model can sig-
nificantly improve over n-gram baselines across a
range of language-pairs, even when the baselines
were trained on 575 times more data.
In future work we plan to directly learn represen-
tations of the source-side during training of the joint
model. Thus, the model itself can decide which en-
coding is best for the task. We also plan to change
the cross entropy objective to a BLEU-inspired ob-
jective in a discriminative training regime, which we
hope to be more effective. We would also like to ap-
ply recent advances in tackling the vanishing gradi-
ent problem (Pascanu et al, 2013) using a regular-
ization term to maintain the magnitude of the gradi-
ents during back propagation through time. Finally,
we would like to integrate the recurrent model di-
rectly into first-pass decoding, a straightforward ex-
tension of lattice rescoring using the algorithm we
developed.
Acknowledgments
We would like to thank Anthony Aue, Hany Has-
san Awadalla, Jon Clark, Li Deng, Sauleh Eetemadi,
Jianfeng Gao, Qin Gao, Xiaodong He, Will Lewis,
Arul Menezes, and Kristina Toutanova for helpful
discussions related to this work as well as for com-
ments on previous drafts. We would also like to
thank the anonymous reviewers for their comments.
References
Alexandre Allauzen, He?le`ne Bonneau-Maynard, Hai-Son
Le, Aure?lien Max, Guillaume Wisniewski, Franc?ois
Yvon, Gilles Adda, Josep Maria Crego, Adrien
Lardilleux, Thomas Lavergne, and Artem Sokolov.
2011. LIMSI @ WMT11. In Proc. of WMT, pages
309?315, Edinburgh, Scotland, July. Association for
Computational Linguistics.
Ebru Arisoy, Tara N. Sainath, Brian Kingsbury, and Bhu-
vana Ramabhadran. 2012. Deep Neural Network
Language Models. In NAACL-HLT Workshop on the
Future of Language Modeling for HLT, pages 20?28,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vin-
cent J. Della Pietra, and Jenifer C. Lai. 1992. Class-
based n-gram models of natural language. Computa-
tional Linguistics, 18(4):467?479, Dec.
Josep Crego and Franois Yvon. 2010. Factored bilingual
n-gram language models for statistical machine trans-
lation. Machine Translation, 24(2):159?175.
Scott Deerwester, Susan T. Dumais, George W. Furnas,
Thomas K. Landauer, and Richard Harshman. 1990.
Indexing by Latent Semantic Analysis. Journal of the
American Society for Information Science, 41(6):391?
407.
Anoop Deoras, Toma?s? Mikolov, and Kenneth Church.
2011a. A Fast Re-scoring Strategy to Capture Long-
Distance Dependencies. In Proc. of EMNLP, pages
1116?1127, Stroudsburg, PA, USA, July. Association
for Computational Linguistics.
Anoop Deoras, Toma?s? Mikolov, Stefan Kombrink,
M. Karafiat, and Sanjeev Khudanpur. 2011b. Varia-
tional Approximation of Long-Span Language Models
for LVCSR. In Proc. of ICASSP, pages 5532?5535.
Ahmad Emami and Frederick Jelinek. 2005. A Neural
Syntactic Language Model. Machine Learning, 60(1-
3):195?227, September.
Jianfeng Gao, Xiaodong He, Wen-tau Yih, and Li Deng.
2013. Learning Semantic Representations for the
Phrase Translation Model. Technical Report MSR-
TR-2013-88, Microsoft Research, September.
Joshua Goodman. 2001. Classes for Fast Maximum En-
tropy Training. In Proc. of ICASSP.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical Phrase-Based Translation. In Proc.
of HLT-NAACL, pages 127?133, Edmonton, Canada,
May.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open Source
Toolkit for Statistical Machine Translation. In Proc.
of ACL Demo and Poster Sessions, pages 177?180,
Prague, Czech Republic, Jun.
Philipp Koehn. 2004. Statistical Significance Tests for
Machine Translation Evaluation. In Proc. of EMNLP,
pages 388?395, Barcelona, Spain, Jul.
Hai-Son Le, Alexandre Allauzen, and Franc?ois Yvon.
2012a. Continuous Space Translation Models with
1053
Neural Networks. In Proc. of HLT-NAACL, pages 39?
48, Montre?al, Canada. Association for Computational
Linguistics.
Hai-Son Le, Thomas Lavergne, Alexandre Allauzen,
Marianna Apidianaki, Li Gong, Aure?lien Max, Artem
Sokolov, Guillaume Wisniewski, and Franc?ois Yvon.
2012b. LIMSI @ WMT12. In Proc. of WMT, pages
330?337, Montre?al, Canada, June. Association for
Computational Linguistics.
Wolfgang Macherey, Franz Josef Och, Ignacio Thayer,
and Jakob Uszkoreit. 2008. Lattice-based Minimum
Error Rate Training for Statistical Machine Transla-
tion. In Proc. of EMNLP, pages 725?734, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Toma?s? Mikolov and Geoffrey Zweig. 2012. Con-
text Dependent Recurrent Neural Network Language
Model. In Proc. of Spoken Language Technologies
(SLT), pages 234?239, Dec.
Toma?s? Mikolov, Karafia?t Martin, Luka?s? Burget, Jan Cer-
nocky?, and Sanjeev Khudanpur. 2010. Recurrent
Neural Network based Language Model. In Proc. of
INTERSPEECH, pages 1045?1048.
Toma?s? Mikolov, Anoop Deoras, Daniel Povey, Luka?s?
Burget, and Jan C?ernocky?. 2011a. Strategies for
Training Large Scale Neural Network Language Mod-
els. In Proc. of ASRU, pages 196?201.
Toma?s? Mikolov, Stefan Kombrink, Luka?s? Burget, Jan
Cernocky?, and Sanjeev Khudanpur. 2011b. Exten-
sions of Recurrent Neural Network Language Model.
In Proc. of ICASSP, pages 5528?5531.
Toma?s? Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013. Linguistic Regularities in Continuous Space-
Word Representations. In Proc. of NAACL, pages
746?751, Stroudsburg, PA, USA, June. Association
for Computational Linguistics.
Toma?s? Mikolov. 2012. Statistical Language Models
based on Neural Networks. Ph.D. thesis, Brno Uni-
versity of Technology.
Franz Josef Och. 2003. Minimum Error Rate Training
in Statistical Machine Translation. In Proc. of ACL,
pages 160?167, Sapporo, Japan, July.
Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio.
2013. On the difficulty of training Recurrent Neural
Networks. Proc. of ICML, abs/1211.5063.
David E. Rumelhart, Geoffrey E. Hinton, and Ronald J.
Williams. 1986. Learning Internal Representations
by Error Propagation. In Symposium on Parallel and
Distributed Processing.
Holger Schwenk, Anthony Rousseau, and Mohammed
Attik. 2012. Large, Pruned or Continuous Space Lan-
guage Models on a GPU for Statistical Machine Trans-
lation. In NAACL-HLT Workshop on the Future of
Language Modeling for HLT, pages 11?19. Associa-
tion for Computational Linguistics.
Martin Sundermeyer, Ilya Oparin, Jean-Luc Gauvain,
Ben Freiberg, Ralf Schlu?ter, and Hermann Ney. 2013.
Comparison of Feedforward and Recurrent Neural
Network Language Models. In IEEE International
Conference on Acoustics, Speech, and Signal Process-
ing, pages 8430?8434, Vancouver, Canada, May.
Geoff Zweig and Konstantin Makarychev. 2013. Speed
Regularization and Optimality in Word Classing. In
Proc. of ICASSP.
1054
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1077?1088,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Monolingual Marginal Matching for Translation Model Adaptation
Ann Irvine
Johns Hopkins University
anni@jhu.edu
Chris Quirk
Microsoft Research
chrisq@microsoft.com
Hal Daume? III
University of Maryland
me@hal3.name
Abstract
When using a machine translation (MT)
model trained on OLD-domain parallel data to
translate NEW-domain text, one major chal-
lenge is the large number of out-of-vocabulary
(OOV) and new-translation-sense words. We
present a method to identify new translations
of both known and unknown source language
words that uses NEW-domain comparable doc-
ument pairs. Starting with a joint distribution
of source-target word pairs derived from the
OLD-domain parallel corpus, our method re-
covers a new joint distribution that matches
the marginal distributions of the NEW-domain
comparable document pairs, while minimiz-
ing the divergence from the OLD-domain dis-
tribution. Adding learned translations to our
French-English MT model results in gains of
about 2 BLEU points over strong baselines.
1 Introduction
When a statistical machine translation (SMT) model
trained on OLD-domain (e.g. parliamentary proceed-
ings) parallel text is used to translate text in a NEW-
domain (e.g. medical or scientific), performance de-
grades drastically. One of the major causes is the
large number of NEW-domain words that are out-of-
vocabulary (OOV) with respect to the OLD-domain
text. Figure 1 shows the OOV rate for text in several
NEW-domains, with respect to OLD-domain parlia-
mentary proceedings. Even more challenging are
the difficult-to-detect new-translation-sense (NTS)
words: French words that are present in both the
OLD and NEW domains but that are translated dif-
ferently in each domain. For example, the French
Parliament Subtitles Medical Science
Perc
ent 
of W
ord 
Typ
es t
hat 
are 
OOV
0
10
20
30
40
50
60
3.95
27.03
41.42
50.93
Figure 1: Percent of test set word types by domain that
are OOV with respect to five million tokens of OLD-
domain French parliamentary proceedings data.
word enceinte is mostly translated in parliamentary
proceedings as place, house, or chamber; in medical
text, the translation is mostly pregnant; in scientific
text, enclosures.
One potential remedy is to collect parallel data in
the NEW-domain, from which we can train a new
SMT model. Smith et al (2010), for example, mine
parallel text from comparable corpora. Parallel sen-
tences are informative but also rare: in the data re-
leased by Smith et al (2010), only 21% of the for-
eign sentences have a near-parallel counterpart in the
English article.1 Furthermore, these sentences do
not capture all terms. In that same dataset, we find
that on average only 20% of foreign and 28% of En-
glish word types in a given article are represented in
the parallel sentence pairs.
In this work, we seek to learn a joint distribu-
1Only 12% of sentences from generally longer English arti-
cles have a near-parallel counterpart in the foreign language.
1077
tion of translation probabilities over all source and
target word pairs in the NEW-domain. We begin
with a maximum likelihood estimate of the joint
based on a word aligned OLD-domain corpus and
update this distribution using NEW-domain compa-
rable data. We define a model based on a single com-
parable corpus and then extend it to learn from doc-
ument aligned comparable corpora with any number
of comparable document pairs. This approach al-
lows us to identify translations for OOV words in the
OLD-domain (e.g. French cisaillement and perc?age,
which translate as shear and drilling, in the scien-
tific domain) as well as new translations for previ-
ously observed NTS words (e.g. enceinte translates
as enclosures, not place, in the scientific domain).
In our MT experiments, we use the learned NEW-
domain joint distribution to update our SMT model
with translations of OOV and low frequency words;
we leave the integration of new translations for NTS
words to future work.
Our approach crucially depends on finding com-
parable document pairs relevant to the NEW-domain.
Such pairs could be derived from a number of
sources, with document pairings inferred from
timestamps (e.g. news articles) or topics (inferred or
manually labeled). We use Wikipedia2 as a source
of comparable pairs. So-called ?interwiki links?
(which link Wikipedia articles written on the same
topic but in different languages) act as rough guid-
ance that pages may contain similar information.
Our approach does not exploit any Wikipedia struc-
ture beyond this signal, and thus is portable to alter-
nate sources of comparable articles, such as multi-
lingual news articles covering the same event.
Our model also relies on the assumption that each
comparable document pair describes generally the
same concepts, though the order and structure of
presentation may differ significantly. The efficacy
of this method likely depends on the degree of com-
parability of the data; exploring the correlation be-
tween comparability and MT performance is an in-
teresting question for future work.
2 Previous Work
In prior work (Irvine et al, 2013), we presented a
systematic analysis of errors that occur when shift-
2www.wikipedia.org
ing domains in machine translation. That work con-
cludes that errors resulting from unseen (OOV) and
new translation sense words cause the majority of
the degradation in translation performance that oc-
curs when an MT model trained on OLD-domain
data is used to translate data in a NEW-domain. Here,
we target OOV errors, though our marginal match-
ing method is also applicable to learning translations
for NTS words.
A plethora of prior work learns bilingual lex-
icons from monolingual and comparable corpora
with many signals including distributional, tempo-
ral, and topic similarity (Rapp, 1995; Fung and Yee,
1998; Rapp, 1999; Schafer and Yarowsky, 2002;
Schafer, 2006; Klementiev and Roth, 2006; Koehn
and Knight, 2002; Haghighi et al, 2008; Mimno
et al, 2009; Mausam et al, 2010; Prochasson
and Fung, 2011; Irvine and Callison-Burch, 2013).
However, this prior work stops short of using these
lexicons in translation. We augment a baseline MT
system with learned translations.
Our approach bears some similarity to Ravi and
Knight (2011), Dou and Knight (2012), and Nuhn
et al (2012); we learn a translation distribution de-
spite a lack of parallel data. However, we focus on
the domain adaptation setting. Parallel data in an
OLD-domain acts as a starting point (prior) for this
translation distribution. It is reasonable to assume an
initial bilingual dictionary can be obtained even in
low resource settings, for example by crowdsourc-
ing (Callison-Burch and Dredze, 2010) or pivoting
through related languages (Schafer and Yarowsky,
2002; Nakov and Ng, 2009).
Daume? III and Jagarlamudi (2011) mine trans-
lations for high frequency OOV words in NEW-
domain text in order to do domain adaptation. Al-
though that work shows significant MT improve-
ments, it is based primarily on distributional simi-
larity, thus making it difficult to learn translations for
low frequency source words with sparse word con-
text counts. Additionally, that work reports results
using artificially created monolingual corpora taken
from separate source and target halves of a NEW-
domain parallel corpus, which may have more lexi-
cal overlap with the corresponding test set than we
could expect from true monolingual corpora. Our
work mines NEW-domain-like document pairs from
Wikipedia. In this work, we show that, keeping
1078
data resources constant, our model drastically out-
performs this previous approach. Razmara et al
(2013) take a fundamentally different approach and
construct a graph using source language monolin-
gual text and identify translations for source lan-
guage OOV words by pivoting through paraphrases.
Della Pietra et al (1992) and Federico (1999) ex-
plore models for combining foreground and back-
ground distributions for the purpose of language
modeling, and their approaches are somewhat simi-
lar to ours. However, our focus is on translation.
3 Model
Our goal is to recover a probabilistic translation dic-
tionary in a NEW-domain, represented as a joint
probability distribution pnew(s, t) over source/target
word pairs. At our disposal, we have access to a joint
distribution pold(s, t) from the OLD-domain (com-
puted from word alignments), plus comparable doc-
ument pairs in the NEW-domain. From these com-
parable documents, we can extract raw word fre-
quencies on both the source and target side, repre-
sented as marginal distributions q(s) and q(t). The
key idea is to estimate this NEW-domain joint dis-
tribution to be as similar to the OLD-domain distri-
bution as possible, subject to the constraint that its
marginals match those of q.
To illustrate our goal, consider an example. Imag-
ine in the OLD-domain parallel data we find that ac-
corder translates as grant 10 times and as tune 1
time. In the NEW-domain comparable data, we find
that accorder occurs 5 times, but grant occurs only
once, and tune occurs 4 times. Clearly accorder no
longer translates as grant most of the time; perhaps
we should shift much of its mass onto the translation
tune instead. Figure 2 shows the intuition.
First, we present an objective function and set of
constraints over joint distributions to minimize the
divergence from the OLD-domain distribution while
matching both the source and target NEW-domain
marginal distributions. Next, we augment the objec-
tive with information about word string similarity,
which is particularly useful for the French-English
language pair. Optimizing this objective with a sin-
gle pair of source and target marginals can be per-
formed using an off-the-shelf solver. In practice,
though, we have a large set of document pairs, each
of which can induce a pair of marginals. Using
these per-document marginals provides additional
information to the learning function but would over-
whelm a common solver. Therefore, we present a se-
quential learning method for approximately match-
ing the large set of document pair marginal distribu-
tions. Finally, we describe how we identify compa-
rable document pairs relevant to the NEW-domain.
3.1 Marginal Matching Objective
Given word-aligned parallel data in the OLD-domain
and source and target comparable corpora in the
NEW-domain, we first estimate a joint distribution
pold(s, t) over word pairs (s, t) in the OLD-domain,
where s and t range over source and target lan-
guage words, respectively. For the OLD-domain
joint distribution, we use a simple maximum like-
lihood estimate based on non-null automatic word
alignments (using grow-diag-final GIZA++ align-
ments (Och and Ney, 2003)). Next, we find source
and target marginal distributions, q(s) and q(t), by
relative frequency estimates over the source and tar-
get comparable corpora. Our goal is to recover a
joint distribution pnew(s, t) for the new domain that
matches the marginals, q(s) and q(t), but is mini-
mally different from the original joint distribution,
pold(s, t).
We cast this as a linear programming problem:
pnew = arg min
p
?
?
?p? pold
?
?
?
1
(1)
subject to:
?
s,t
p(s, t) = 1, p(s, t) ? 0
?
s
p(s, t) = q(t),
?
t
p(s, t) = q(s)
In the objective function, the joint probability matri-
ces p and pold are interpreted as large vectors over
all word pairs (s, t). The first two constraints force
the result to be a well-formed distribution, and the
final two force the marginals to match.
Following prior work (Ravi and Knight, 2011),
we would like the matrix to remain as sparse as pos-
sible; that is, introduce the smallest number of new
translation pairs necessary. A regularization term
captures this goal:
?(p) =
?
s,t:
pold(s,t)=0
?r ? p(s, t) (2)
1079
house place pregnant dress
q
old
(s)
enceinte 0.30 0.40 0.10 0
0.80
habiller 0 0 0 0.20
0.20
q
old
(t)
0.30 0.40 0.10 0.20
(a) OLD-Domain Joint (b) NEW-Domain Marginals
house place pregnant dress girl
q(s)
enceinte
?
0.60
habiller
0.20
fille
0.20
q(t)
0.12 0.08 0.40 0.20 0.20
(c) Inferred NEW-Domain Joint
house place pregnant dress girl q
new
(s)
enceinte
0.12 0.08 0.40 0 0
0.60
habiller 0 0 0 0.20 0 0.20
fille 0 0 0 0 0.20 0.20
q
new
(t)
0.12 0.08 0.40 0.20 0.20
=
Matched 
Marginals
Figure 2: Starting with a joint distribution derived from OLD-domain data, we infer a NEW-domain joint distribution
based on the intuition that the new joint should match the marginals that we observe in NEW-domain comparable
corpora. In this example, a translation is learned for the previously OOV word fille, and pregnant becomes a preferred
translation for enceinte.
If the old domain joint probability pold(s, t) was
nonzero, there is no penalty. Otherwise, the penalty
is ?r times the new joint probability p(s, t). To dis-
courage the addition of translation pairs that are un-
necessary in the new domain, we use a value of ?r
greater than one. Thus, the benefit of a more sparse
matrix overwhelms the desire for preventing change.
Any value greater than one seems to suffice; we use
?r = 1.1 in our experiments.
Inspired by the preference for sparse matrices
captured by ?(p), we include another orthogonal
cue that words are translations of one another: their
string similarity. In prior work, string similarity was
a valuable signal for inducing translations, particu-
larly for closely related languages such as French
and English (Daume? III and Jagarlamudi, 2011). We
define a penalty function f(p) as follows: if the nor-
malized Levenshtein edit distance between swithout
accents and t is less than 0.2, no penalty is applied;
a penalty of 1 is applied otherwise. We chose the
0.2 threshold manually by inspecting results on our
development sets.
f(p) =
?
s,t
p(s, t) ?
{
0 if lev(t,strip(s))len(s)+len(t) < 0.2
1 otherwise
The objective function including this penalty is:
pnew = arg min
p
?
?
?p? pold
?
?
?
1
+ ?(p) + f(p)
In principle, additional penalties could be encoded
in a similar way.3 This objective can be optimized
by any standard LP solver; we use the Gurobi pack-
age (Gurobi Optimization Inc., 2013).
3.2 Document Pair Modification
The above formulation applies whenever we have
access to comparable corpora. However, often we
have access to comparable documents, such as those
given by Wikipedia inter-language links. We modify
our approach to take advantage of the document cor-
respondences within our comparable corpus. In par-
ticular, we would like to match the marginals for all
document pairs.4 By maintaining separate marginal
distributions, our algorithm is presented with more
3We experimented with penalties measuring document-pair
co-occurrence and monolingual frequency differences but did
not see gains on our development sets.
4This situation is not unique to our application; multiple
marginals are likely to exist in many cases.
1080
information. For example, imagine that one doc-
ument pair uses ?dog? and ?chien?, where another
document pair uses ?cat? and ?chat?, each with sim-
ilar frequency. If we sum these marginals to produce
a single marginal distribution, it is now difficult to
identify that ?dog? should correspond to ?chien? and
not ?chat.? Document pair alignments add informa-
tion at the cost of additional constraints.
An initial formulation of our problem with mul-
tiple comparable document pairs might require
the pnew marginals to match all of the document
marginals. In general, this constraint set is likely
to result in an infeasible problem. Instead, we take
an incremental, online solution, considering a sin-
gle comparable document pair at a time. For docu-
ment pair k, we solve the optimization problem in
Eq (1) to find the joint distribution minimally dif-
ferent from pk-1, while matching the marginals of
this pair only. This gives a new joint distribution,
tuned specifically for this pair. We then update our
current guess of the new domain joint toward this
document-pair-specific distribution, much like a step
in stochastic gradient ascent.
More formally, suppose that before processing the
kth document we have a guess at the NEW-domain
joint distribution, pnew1:k?1 (the subscript indicates that
it includes all document pairs up to and including
document k ? 1). We first solve Eq (1) solely on
the basis of this document pair, finding a joint dis-
tribution pnewk that matches the marginals of the kth
document pair only and is minimally different from
pnew1:k?1. Finally, we form a new estimate of the joint
distribution by moving pnew1:k?1 in the direction of
pnewk , via:
pnew1:k = p
new
1:k?1 + ?u
[
pnewk ? p
new
1:k?1
]
The learning rate ?u is set to 0.001.5
This incremental update of parameters is simi-
lar to the margin infused relaxed algorithm (MIRA)
(Crammer et al, 2006). Like MIRA and the percep-
tron, there is not an overall ?objective? function that
we are attempting to optimize (as one would in many
stochastic gradient steps). Instead, we?re aiming for
5We tuned ?u on semi-extrinsic results on the development
set. Note that although 0.001 seems small, the values we are
moving are joint probabilities, which are tiny and so small
learning rates make sense.
a solution that makes a small amount of progress
on each example, in such a way if it received that
example again, it would ?do better? (in this case:
have a closer match of marginals). Also like MIRA,
our learning rate is constant. We parallelize learning
with mini-batches for increased speed. Eight paral-
lel learners update an initial joint distribution based
on 100 document pairs (i.e. each learner makes 100
incremental updates), and then we merge results us-
ing an average over the 8 learned joint distributions.
3.3 Comparable Data Selection
It remains to select comparable document pairs. We
assume that we have enough monolingual NEW-
domain data in one language to rank comparable
document pairs (here, Wikipedia pages) according
to how NEW-domain-like they are. In particular, we
estimate the similarity to a source language (here,
French) corpus in the NEW domain. For our experi-
ments, we use the French side of a NEW-domain par-
allel corpus.6 We could have targeted our learning
even more by using our NEW-domain MT test sets.
Doing so would increase the chances that our source
language words of interest appear in the comparable
corpus. However, to avoid overfitting any particular
test set, we use the French side of the training data.
For each Wikipedia document pair, we com-
pute the percent of French phrases up to length
four that are observed in the French monolingual
NEW-domain corpus and rank document pairs by
the geometric mean of the four overlap measures.
More sophisticated ways to identify NEW-domain-
like Wikipedia pages (e.g. Moore and Lewis (2010))
may yield additional performance gains, but, quali-
tatively, the ranked Wikipedia pages seemed reason-
able to the authors.
4 Experimental setup
4.1 Data
We use French-English Hansard parliamentary pro-
ceedings7 as our OLD-domain parallel corpus. With
over 8 million parallel lines of text, it is one of the
largest freely available parallel corpora for any lan-
6We could have, analogously, used the target language (En-
glish) side of the parallel corpus and measure overlap with the
English Wikipedia documents, or even used both.
7http://www.parl.gc.ca
1081
guage pair. In order to simulate more typical data
settings, we sample every 32nd line, using the result-
ing parallel corpus of 253, 387 lines and 5, 051, 016
tokens to train our baseline model.
We test our model using three NEW-domain cor-
pora: (1) the EMEA medical corpus (Tiedemann,
2009), (2) a corpus of scientific abstracts (Carpuat
et al, 2013a), and (3) a corpus of translated movie
subtitles (Tiedemann, 2009). We use development
and test sets to tune and evaluate our MT mod-
els. We use the NEW-domain parallel training cor-
pora only for language modeling and for identifying
NEW-domain-like comparable documents.
4.2 Machine translation
We use the Moses MT framework (Koehn et al,
2007) to build a standard statistical phrase-based
MT model using our OLD-domain training data. Us-
ing Moses, we extract a phrase table with a phrase
limit of five words and estimate the standard set of
five feature functions (phrase and lexical translation
probabilities in each direction and a constant phrase
penalty feature). We also use a standard lexicalized
reordering model and two language models based on
the English side of the Hansard data and the given
NEW-domain training corpora. Features are com-
bined using a log-linear model optimized for BLEU,
using the n-best batch MIRA algorithm (Cherry and
Foster, 2012). We call this the ?simple baseline.? In
Section 5.2 we describe several other baseline ap-
proaches.
4.3 Experiments
For each domain, we use the marginal matching
method described in Section 3 to learn a new,
domain-adapted joint distribution, pnewk (s, t), over
all French and English words. We use the learned
joint to compute conditional probabilities, pnewk (t|s),
for each French word s and rank English translations
t accordingly. First, we evaluate the learned joint
directly using the distribution based on the word-
aligned NEW-domain development set as a gold
standard. Then, we perform end-to-end MT exper-
iments. We supplement phrase tables with transla-
tions for OOV and low frequency words (we ex-
periment with training data frequencies less than
101, 11, and 1) and include pnewk (t|s) and p
new
k (s|t)
as new translation features for those supplemental
translations. For these new phrase pairs, we use the
average lexicalized reordering values from the ex-
isting reordering tables. For phrase pairs extracted
bilingually, we use the bilingually estimated trans-
lation probabilities and uniform scores for the new
translation features. We experimented with using
pnewk (t|s) and p
new
k (s|t) to estimate additional lex-
ical translation probabilities for the bilingually ex-
tracted phrase pairs but did not observe any gains
(experimental details omitted due to space con-
straints). We re-run tuning in all experiments.
We also perform oracle experiments in which
we identify translations for French words in word-
aligned development and test sets and append these
translations to baseline phrase tables.
5 Results
5.1 Semi-extrinsic evaluation
Before doing end-to-end MT experiments, we eval-
uate our learned joint distribution, pnewk (s, t), by
comparing it to the joint distribution taken from a
word aligned NEW-domain parallel development set,
pgold(s, t). We call this evaluation semi-extrinsic
because it involves neither end-to-end MT (our ex-
trinsic task) nor an intrinsic evaluation based on our
training objective (L1 norm). We find it informa-
tive to evaluate the models using bilingual lexicon
induction metrics before integrating our output into
full MT. That is, we do not compare the full joint dis-
tributions, but, rather, for a given French word, how
our learned model ranks the word?s most probable
translation under the gold distribution. In particular,
because we are primarily concerned with learning
translations for previously unseen words, we eval-
uate over OOV French word types. In some cases,
the correct translation for OOV words is the identi-
cal string (e.g. na+, lycium). Because it is trivial
to produce these translations,8 we evaluate over the
subset of OOV development set French words for
which the correct translation is not the same string.
Figure 3 shows the mean reciprocal rank for the
learned distribution, pnewk (s, t), for each domains as
a function of the number of comparable document
pairs used in learning. In all domains, the compara-
ble document pairs are sorted according to their sim-
8And, indeed, by default our decoder copies OOV strings
into its output directly.
1082
0 10000 20000 30000 40000 50000
0.0
0.1
0.2
0.3
0.4
0.5
0.6
Number of Document Pairs
Mea
n Re
cipro
cal R
ank
0 10000 20000 30000 40000 50000
0.0
0.1
0.2
0.3
0.4
0.5
0.6
Number of Document Pairs
Mea
n Re
cipro
cal R
ank
0 10000 20000 30000 40000 50000
0.0
0.1
0.2
0.3
0.4
0.5
0.6
?
Number of Document Pairs
Mea
n Re
cipro
cal R
ank ?
Full modelEdit distance baselineCCA BaselineModel without ED penalty 0.38
M
e
a
n
 
R
e
c
i
p
r
o
c
a
l
 
R
a
n
k
0.22
0.14
0.11
0.08
0.26
0.39
0.07
0.05
0.03
(a) Science (b) EMEA (c) Subtitles
Figure 3: Semi-extrinsic bilingual lexicon induction results. Mean reciprocal rank is computed over all OOV devel-
opment set words for which identity is not the correct translation.
ilarity with the NEW-domain. Figure 3 also shows
the performance of baseline models and our learner
without the edit distance penalty. For each source
word s, the edit distance (ED) baseline ranks all En-
glish words t in our monolingual data by their edit
distance with s.9 The Canonical Correlation Analy-
sis (CCA) baseline uses the approach of Daume? III
and Jagarlamudi (2011) and the top 25, 000 ranked
document pairs as a comparable corpus. That model
performs poorly largely because of sparse word con-
text counts. Interestingly, for Science and EMEA,
the performance of our full model at 50, 000 doc-
ument pairs is higher than the sum of the edit dis-
tance baseline and the model without the edit dis-
tance penalty, indicating that our approach effec-
tively combines the marginal matching and edit dis-
tance signals.
The learning curves for the three domains vary
substantially. For Science, learning is gradual and
it appears that additional gains could be made by
iterating over even more document pairs. In con-
trast, the model learns quickly for the EMEA do-
main; performance is stable after 20, 000 document
pairs. Given these results and our experience with
the two domains, we hypothesize that the difference
is due to the fact that the Science data is much more
heterogenous than the EMEA data. The Science data
9In particular, for each domain and each OOV French word,
we ranked the set of all English words that appeared at least five
times in the set of 50,000 most NEW-domain like Wikipedia
pages. Using a frequency threshold of five helped eliminate
French words and improperly tokenized English words from the
set of candidates.
includes physics, chemistry, and biology abstracts,
among others. The drug labels that make up most of
the EMEA data are more homogeneous. In Section
6 we comment on the poor Subtitles performance,
which persists in our MT experiments.
We experimented with making multiple learning
passes over the document pairs and observed rela-
tively small gains from doing so. In all experiments,
learning from some number of additional new doc-
ument pairs resulted in higher semi-extrinsic per-
formance gains than passing over document pairs
which were already observed.
In the case of OOV words, it?s clear that learning
something about how to translate a previously un-
observed French word is beneficial. However, our
learning method also learns domain-specific new-
translation senses (NTS). Table 1 shows some exam-
ples of what the marginal matching method learns
for different types of source words (OOVs, low fre-
quency, and NTS).
5.2 MT evaluation
By default, the Moses decoder copies OOV words
directly into its translated output. In some cases,
this is correct (e.g. ensembles, blumeria, google).
In other cases, French words can be translated into
English correctly by simply stripping accent marks
off of the OOV word and then copying it to the out-
put (e.g. came?ra, e?le?ments, mole?cules). In the Sci-
ence and EMEA domains, we found that our base-
line BLEU scores improved from 21.91 to 22.20
and 23.67 to 24.45, respectively, when we changed
the default handling of OOVs to strip accents before
1083
French OLD top pold(t|s) NEW top pgold(t|s) MM-learned top pnew(t|s)
OOV words
cisaillement - shear strength shearing shear viscous newtonian
courbure - curvature bending curvatures curvature curved manifold
Low frequency words
line?aires linear linear nonlinear non-linear linear linearly nonlinear
re?cepteur receiver receptor receiver y1 receptor receiver receptors
New translation sense words
champ field jurisdiction scope field magnetic near-field field magnetic fields
marche working march work walk step walking march walk walking
Table 1: Hand-picked examples of Science-domain French words and their top English translations in the OLD-
domain, NEW-domain, and marginal matching distributions. The first two are OOVs. The next two only appeared four
and one time, respectively, in the training data and only aligned to a single English word. The last two are NTS French
words: words that appeared frequently in the training data but for which the word?s sense in the new domain shifts.
copying into the output. Interestingly, performance
on the Subtitles domain text did not change at all
with this baseline modification. This is likely due
to the fact that there are fewer technical OOVs (the
terms typically captured by this accent-stripping pat-
tern) in the subtitles domain.
Throughout our experiments, we found it criti-
cal to retain correct ?freebie? OOV translations. In
the results presented below, including the baselines,
we supplement phrase tables with a new candidate
translation but also include accent-stripped identity,
or ?freebie,? translations in the table for all OOV
words. We experimented with classifying French
words as freebies or needing a new translation, but
oracle experiments showed very little improvement
(about 0.2 BLEU improvement in the Science do-
main), so instead we simply include both types of
translations in the phrase tables.
In addition to the strip-accents baseline, we com-
pare results with four other baselines. First, we
drop OOVs from the output translations. Second,
like our semi-extrinsic baseline, we rank English
words by their edit distance away from each French
OOV word (ED baseline). Third, we rank En-
glish words by their document-pair co-occurrence
score with each French OOV word. That is, for
all words w, we compute D(w), the vector indicat-
ing the document pairs in which w occurs, over the
set of 50,000 document-pairs which are most NEW-
domain-like. For French and English words s and t,
ifD(s) andD(t) are dissimilar, it is less likely (s, t)
is a valid translation pair. We weight D(w) entries
with BM25 (Robertson et al, 1994). For all French
OOVs, we rank all English translations according to
the cosine similarity between the pair of D(w) vec-
tors. The fourth baseline uses the CCA model de-
scribed in Daume? III and Jagarlamudi (2011) to rank
English words according to their distributional sim-
ilarity with each French word. For the CCA base-
line comparison, we only learned translations using
25,000 Science-domain document pairs, rather than
the full 50,000 and for all domains. However, it?s
unlikely that learning over more data would over-
come the low performance observed so far. For the
final three baselines, we append French OOV words
and their highest ranked English translation to the
phrase table. Along with each new translation pair,
we include one new phrase table feature with the
relevant translation score (edit distance, document
similarity, or CCA distributional similarity). For all
baselines other than drop-OOVs, we also include
accent-stripped translation pairs with an additional
indicator feature.
Table 3 shows results appending the top ranked
English translation for each OOV French word using
each baseline method. None of the alternate base-
lines outperform the simplest baseline on the subti-
tles data. Using document pair co-occurrences is the
strongest baseline for the Science and EMEA do-
mains. This confirms our intuition that taking ad-
vantage of document pair alignments is worthwhile.
For Science and EMEA, supplementing a model
with OOV translations learned through our marginal
matching method drastically outperforms all base-
1084
OOVs translated correctly and incorrectly
Input les re?sistances au cisaillement par poinc?onnement ...
Ref the punching shear strengths...
Baseline the resistances in cisaillement by poinconnement ...
MM the resistances in shear reinforcement...
OOV translated incorrectly
Input pre?sentation d? un logiciel permettant de ge?rer les donne?es temporelles .
Ref presentation of software which makes it possible to manage temporal data .
Baseline introduction of a software to manage temporelles data .
MM introduction of a software to manage data plugged .
Low frequency French words
Input ...limite est lie?e a` la de?croissance tre`s rapide du couplage e?lectron-phonon avec la tempe?rature .
Ref ...limit is linked to the rapid decrease of the electron-phonon coupling with temperature .
Baseline ...limit is linked to the decline very rapid electron-phonon linkage with the temperature .
MM ...limit is linked to the linear very rapid electron-phonon coupling with the temperature .
Table 2: Example MT outputs for Science domain. The baseline strips accents (Table 3). In the first example, the
previously OOV word cisaillement is translated correctly by an MM-supplemented model. The OOV poinc?onnement
is translated as reinforcement instead of strengths, which is incorrect with respect to the reference but arguably not
bad. In the second example, temporelles is not translated correctly in the MM output. In the third example, the MM-
hypothesized correct translation of low frequency word couplage, coupling, is chosen instead of incorrect linkage. Also
in the third example, the low frequency word de?croissance is translated as the MM-hypothesized incorrect translation
linear. In the case of de?croissance, the baseline?s translation, decline, is much better than the MM translation linear.
lines. Using our model to translate OOV words
yields scores of 23.62 and 26.97 in the Science and
EMEA domains, or 1.19 and 1.94 BLEU points, re-
spectively, above the strongest baseline. We observe
additional gains by also supplementing the model
with translations for low frequency French words.
For example, when we use our approach to translate
source words in the Science domain which appear
ten or fewer times in our OLD-domain training data,
the BLEU score increases to 24.28.
We tried appending top-k translations, varying k.
However, we found that for the baselines as well as
our MM translations, using only the top-1 English
translations outperformed using more.
Table 3 also shows the result of supplementing a
baseline phrase table with oracle OOV translations.
Using the marginal matching learned OOV transla-
tions takes us 30% and 40% of the way from the
baseline to the oracle upper bound for Science and
EMEA, respectively.
We have focused on supplementing an SMT
model trained on a sample of the Hansard parallel
corpus in order to mimic typical data conditions, but
we have also performed experiments supplementing
Science EMEA Subs
Simple Baseline 21.91 23.67 13.18
Drop OOVs 20.22 18.95 11.86
Accent-Stripped 22.20 24.45 13.13
ED Baseline 22.10 24.35 12.95
Doc Sim Baseline. 22.43 25.03 13.02
CCA Baseline 21.41 - -
MM Freq<1 (OOV) 23.62 26.97 13.07
MM Freq<11 24.28 27.26 12.97
MM Freq<101 23.96 26.82 12.92
Oracle OOV 26.38 29.99 15.06
Table 3: BLEU results using: (1) baselines, (2) phrase
tables augmented with top-1 translations for French
words with indicated OLD training data frequencies, (3)
phrase tables augmented with OOV oracle translations.
a model trained on the full dataset.10 Beginning with
the larger model, we observe performance gains of
0.8 BLEU points for both the EMEA and the Sci-
ence domains over the strongest baselines, which are
based on document similarity, when we add OOV
10We still use the joint that was learned starting with the one
estimated over the sample; we may observe greater gains over
the full Hansard baseline with a stronger initial joint.
1085
translations. As expected, these gains are less than
what we observe when our baseline model is esti-
mated over less data, but they are still substantial.
In all experiments, we have assumed that we have
no NEW-domain parallel training data, which is the
case for the vast majority of language pairs and do-
mains. However, In the case that we do have some
NEW-domain parallel data, OOV rates will be some-
what lower, but our method is still applicable. For
example, we would need 2.3 million words of Sci-
ence (NEW-domain) parallel data to cover just 50%
of the OOVs in our Science test set, and 4.3 million
words to cover 70%.
6 Discussion
BLEU score performance gains are substantial for
the Science and EMEA domains, but we don?t ob-
serve gains on the subtitles text. We believe this dif-
ference relates to the difference between a corpus
domain and a corpus register. As Lee (2002) ex-
plains, a text?s domain is most related to its topic,
while a text?s register is related to its type and pur-
pose. For example, religious, scientific, and dia-
logue texts may be classified as separate registers,
while political and scientific expositions may have a
single register but different domains. Our science
and EMEA corpora are certainly different in do-
main from the OLD-domain parliamentary proceed-
ings, and our success in boosting MT performance
with our methods indicates that the Wikipedia com-
parable corpora that we mined match those domains
well. In contrast, the subtitles data differs from the
OLD-domain parliamentary proceedings in both do-
main and register. Although the Wikipedia data that
we mined may be closer in domain to the subtitles
data than the parliamentary proceedings,11 its regis-
ter is certainly not film dialogues.
Although the use of marginal matching is, to the
best of our knowledge, novel in MT, there are related
threads of research that might inspire future work.
The intuition that we should match marginal distri-
butions is similar to work using no example labels
but only label proportions to estimate labels, for ex-
ample in Quadrianto et al (2008). Unlike that work,
11In fact, we believe that it is. Wikipedia pages that ranked
very high in our subtitles-like list included, for example, the
movie The Other Side of Heaven and actor Frank Sutton.
our label set corresponds to entire vocabularies, and
we have multiple observed label proportions. Also,
while the marginal matching objective seems effec-
tive in practice, it is difficult to optimize. A number
of recently developed approximate inference meth-
ods use a decomposition that bears a strong resem-
blance to this objective function. Considering the
marginal distributions from each document pair to
be a separate subproblem, we could approach the
global objective of satisfying all subproblems as an
instance of dual decomposition (Sontag et al, 2010)
or ADMM (Gabay and Mercier, 1976; Glowinski
and Marrocco, 1975).
We experiment with French-English because tun-
ing and test sets are available in several domains for
that language pair. However, our techniques are di-
rectly applicable to other language pairs, including
those that are less related. We have observed that
many domain-specific terms, particularly in medi-
cal and science domains, are borrowed across lan-
guages, whether or not the languages are related.
Even for languages with different character sets,
one could do transliteration before measuring ortho-
graphical similarity.
Although we were able to identify translations for
some NTS words (Table 1), we did not make use of
them in our MT experiments. Recent work has iden-
tified NTS words in NEW-domain corpora (Carpuat
et al, 2013b), and in future work we plan to incorpo-
rate discovered translations for such words into MT.
7 Conclusions
We proposed a model for learning a joint distribu-
tion of source-target word pairs based on the idea
that its marginals should match those observed in
NEW-domain comparable corpora. Supplementing
a baseline phrase-based SMT model with learned
translations results in BLEU score gains of about
two points in the medical and science domains.
Acknowledgments
We gratefully acknowledge the support of the
2012 JHU Summer Workshop and NSF Grant No
1005411. We would like to thank the entire DAMT
team (http://hal3.name/damt/) and San-
jeev Khudanpur for their help and suggestions.
We also acknowledge partial support from DARPA
1086
CSSG Grant D11AP00279 and DARPA BOLT Con-
tract HR0011-12-C-0015 for Hal Daume? III and
support from the Johns Hopkins University Human
Language Technology Center of Excellence for Ann
Irvine. The views and conclusions contained in this
publication are those of the authors and should not
be interpreted as representing official policies or en-
dorsements of DARPA or the U.S. Government.
References
Chris Callison-Burch and Mark Dredze. 2010. Creating
speech and language data with Amazon?s Mechanical
Turk. In Proceedings of the NAACL Workshop on Cre-
ating Speech and Language Data with Amazon?s Me-
chanical Turk.
Marine Carpuat, Hal Daume? III, Alexander Fraser, Chris
Quirk, Fabienne Braune, Ann Clifton, Ann Irvine,
Jagadeesh Jagarlamudi, John Morgan, Majid Raz-
mara, Ales? Tamchyna, Katharine Henry, and Rachel
Rudinger. 2013a. Domain adaptation in machine
translation: Final report. In 2012 Johns Hopkins Sum-
mer Workshop Final Report.
Marine Carpuat, Hal Daume? III, Katharine Henry, Ann
Irvine, Jagadeesh Jagarlamudi, and Rachel Rudinger.
2013b. Sensespotting: Never let your parallel data tie
you to an old domain. In Proceedings of the Confer-
ence of the Association for Computational Linguistics
(ACL).
Colin Cherry and George Foster. 2012. Batch tun-
ing strategies for statistical machine translation. In
Proceedings of the Conference of the North American
Chapter of the Association for Computational Linguis-
tics (NAACL).
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yoram Singer. 2006. Online passive-
aggressive algorithms. J. Mach. Learn. Res., 7:551?
585, December.
Hal Daume? III and Jagadeesh Jagarlamudi. 2011. Do-
main adaptation for machine translation by mining un-
seen words. In Proceedings of the Conference of the
Association for Computational Linguistics (ACL).
S. Della Pietra, V. Della Pietra, R. L. Mercer, and
S. Roukos. 1992. Adaptive language modeling us-
ing minimum discriminant estimation. Proceedings
of the International Conference on Acoustics, Speech,
and Signal Processing (ICASSP).
Qing Dou and Kevin Knight. 2012. Large scale deci-
pherment for out-of-domain machine translation. In
Proceedings of the Conference on Empirical Methods
in Natural Language Processing (EMNLP).
Marcello Federico. 1999. Efficient language model
adaptation through mdi estimation. In Proceedings of
EUROSPEECH.
Pascale Fung and Lo Yuen Yee. 1998. An IR approach
for translating new words from nonparallel, compara-
ble texts. In Proceedings of the Conference of the As-
sociation for Computational Linguistics (ACL).
Daniel Gabay and Bertrand Mercier. 1976. A dual al-
gorithm for the solution of nonlinear variational prob-
lems via finite element approximation. Computers and
Mathematics with Applications, 2(1):17 ? 40.
Roland Glowinski and A. Marrocco. 1975. Sur
l?approximation, par e?le?ments finis d?ordre un, et la
re?solution, par pe?nalisation-dualite?, d?une classe de
proble`mes de dirichlet non line?aires. Rev. Franc. Au-
tomat. Inform. Rech. Operat., 140:41?76.
Gurobi Optimization Inc. 2013. Gurobi optimizer refer-
ence manual.
Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,
and Dan Klein. 2008. Learning bilingual lexicons
from monolingual corpora. In Proceedings of the Con-
ference of the Association for Computational Linguis-
tics (ACL).
Ann Irvine and Chris Callison-Burch. 2013. Supervised
bilingual lexicon induction with multiple monolingual
signals. In Proceedings of the Conference of the North
American Chapter of the Association for Computa-
tional Linguistics (NAACL).
Ann Irvine, John Morgan, Marine Carpuat, Hal Daume?
III, and Dragos Munteanu. 2013. Measuring machine
translation errors in new domains. Transactions of the
Association for Computational Linguistics (TACL).
Alexandre Klementiev and Dan Roth. 2006. Weakly
supervised named entity transliteration and discovery
from multilingual comparable corpora. In Proceed-
ings of the Conference of the Association for Compu-
tational Linguistics (ACL).
Philipp Koehn and Kevin Knight. 2002. Learning a
translation lexicon from monolingual corpora. In ACL
Workshop on Unsupervised Lexical Acquisition.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of the Conference of the Association for Compu-
tational Linguistics (ACL).
David Lee. 2002. Genres, registers, text types, domains
and styles: Clarifying the concepts and navigating a
path through the bnc jungle. Language and Comput-
ers, 42(1):247?292.
Mausam, Stephen Soderland, Oren Etzioni, Daniel S.
Weld, Kobi Reiter, Michael Skinner, Marcus Sammer,
1087
and Jeff Bilmes. 2010. Panlingual lexical transla-
tion via probabilistic inference. Artificial Intelligence,
174:619?637, June.
David Mimno, Hanna Wallach, Jason Naradowsky, David
Smith, and Andrew McCallum. 2009. Polylingual
topic models. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP).
Robert C. Moore and William Lewis. 2010. Intelligent
selection of language model training data. In Proceed-
ings of the Conference of the Association for Compu-
tational Linguistics (ACL).
Preslav Nakov and Hwee Tou Ng. 2009. Improved statis-
tical machine translation for resource-poor languages
using related resource-rich languages. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing (EMNLP).
Malte Nuhn, Arne Mauser, and Hermann Ney. 2012.
Deciphering foreign language by combining language
models and context vectors. In Proceedings of the
Conference of the Association for Computational Lin-
guistics (ACL).
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51, March.
Emmanuel Prochasson and Pascale Fung. 2011. Rare
word translation extraction from aligned comparable
documents. In Proceedings of the Conference of the
Association for Computational Linguistics (ACL).
Novi Quadrianto, Alex J. Smola, Tiberio S. Caetano, and
Quoc V. Le. 2008. Estimating labels from label pro-
portions. In Proceedings of the International Confer-
ence on Machine Learning (ICML).
Reinhard Rapp. 1995. Identifying word translations in
non-parallel texts. In Proceedings of the Conference of
the Association for Computational Linguistics (ACL).
Reinhard Rapp. 1999. Automatic identification of word
translations from unrelated English and German cor-
pora. In Proceedings of the Conference of the Associ-
ation for Computational Linguistics (ACL).
Sujith Ravi and Kevin Knight. 2011. Deciphering for-
eign language. In Proceedings of the Conference of
the Association for Computational Linguistics (ACL).
Majid Razmara, Maryam Siahbani, Gholamreza Haffari,
and Anoop Sarkar. 2013. Graph propagation for para-
phrasing out-of-vocabulary words in statistical ma-
chine translation. In Proceedings of the Conference of
the Association for Computational Linguistics (ACL).
S.E. Robertson, S. Walker, S. Jones, M.M. Hancock-
Beaulieu, and M. Gatford. 1994. Okapi at TREC-3.
In Proceedings of the Text REtrieval Conference.
Charles Schafer and David Yarowsky. 2002. Induc-
ing translation lexicons via diverse similarity measures
and bridge languages. In Proceedings of the Confer-
ence on Natural Language Learning (CoNLL).
Charles Schafer. 2006. Translation Discovery Using Di-
verse Similarity Measures. Ph.D. thesis, Johns Hop-
kins University.
Jason R. Smith, Chris Quirk, and Kristina Toutanova.
2010. Extracting parallel sentences from comparable
corpora using document level alignment. In Proceed-
ings of the Conference of the North American Chap-
ter of the Association for Computational Linguistics
(NAACL).
David Sontag, A. Globerson, and Tommi Jaakola, 2010.
Introduction to dual decomposition for inference,
chapter 1. MIT Press.
Jo?rg Tiedemann. 2009. News from OPUS - A collection
of multilingual parallel corpora with tools and inter-
faces. In N. Nicolov, K. Bontcheva, G. Angelova, and
R. Mitkov, editors, Recent Advances in Natural Lan-
guage Processing (RANLP).
1088
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1948?1959,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Regularized Minimum Error Rate Training
Michel Galley
Microsoft Research
mgalley@microsoft.com
Chris Quirk
Microsoft Research
chrisq@microsoft.com
Colin Cherry
National Research Council
colin.cherry@nrc-cnrc.gc.ca
Kristina Toutanova
Microsoft Research
kristout@microsoft.com
Abstract
Minimum Error Rate Training (MERT) re-
mains one of the preferred methods for tun-
ing linear parameters in machine translation
systems, yet it faces significant issues. First,
MERT is an unregularized learner and is there-
fore prone to overfitting. Second, it is com-
monly used on a noisy, non-convex loss func-
tion that becomes more difficult to optimize
as the number of parameters increases. To ad-
dress these issues, we study the addition of
a regularization term to the MERT objective
function. Since standard regularizers such as
`2 are inapplicable to MERT due to the scale
invariance of its objective function, we turn to
two regularizers?`0 and a modification of `2?
and present methods for efficiently integrating
them during search. To improve search in large
parameter spaces, we also present a new direc-
tion finding algorithm that uses the gradient of
expected BLEU to orient MERT?s exact line
searches. Experiments with up to 3600 features
show that these extensions of MERT yield re-
sults comparable to PRO, a learner often used
with large feature sets.
1 Introduction
Minimum Error Rate Training emerged a decade
ago (Och, 2003) as a superior training method for
small numbers of linear model parameters of machine
translation systems, improving over prior work using
maximum likelihood criteria (Och and Ney, 2002).
This technique quickly rose to prominence, becom-
ing standard in many research and commercial MT
systems. Variants operating over lattices (Macherey
et al, 2008) or hypergraphs (Kumar et al, 2009) were
subsequently developed, with the benefit of reducing
the approximation error from n-best lists.
The primary advantages of MERT are twofold. It
directly optimizes the evaluation metric under consid-
eration (e.g., BLEU) instead of some surrogate loss.
Secondly, it offers a globally optimal line search. Un-
fortunately, there are several potential difficulties in
scaling MERT to larger numbers of features, due
to its non-convex loss function and its lack of reg-
ularization. These challenges have prompted some
researchers to move away from MERT, in favor of lin-
early decomposable approximations of the evaluation
metric (Chiang et al, 2009; Hopkins and May, 2011;
Cherry and Foster, 2012), which correspond to easier
optimization problems and which naturally incorpo-
rate regularization. In particular, recent work (Chiang
et al, 2009) has shown that adding thousands or tens
of thousands of features can improve MT quality
when weights are optimized using a margin-based
approximation. On simulated datasets, Hopkins and
May (2011) found that conventional MERT strug-
gles to find reasonable parameter vectors, where a
smooth loss function based on Pairwise Ranking Op-
timization (PRO) performs much better; on real data,
this PRO method appears at least as good as MERT
on small feature sets, and also scales better as the
number of features increases.
In this paper, we seek to preserve the advantages
of MERT while addressing its shortcomings in terms
of regularization and search. The idea of adding a
regularization term to the MERT objective function
can be perplexing at first, because the most common
regularizers, such as `1 and `2, are not directly appli-
cable to MERT. Indeed, these regularizers are scale
sensitive, while the MERT objective function is not:
scaling the weight vector neither changes the predic-
tions of the linear model nor affects the error count.
Hence, MERT can hedge any regularization penalty
by maximally scaling down linear model weights.
The first contribution of this paper is to analyze var-
ious forms of regularization that are not susceptible
to this scaling problem. We analyze and experiment
with `0, a form of regularization that is scale insen-
sitive. We also present new parameterizations of `2
1948
regularization, where we apply `2 regularization to
scale-senstive linear transforms of the original linear
model. In addition, we introduce efficient methods
of incorporating regularization in Och (2003)?s exact
line searches. For all of these regularizers, our meth-
ods let us find the true optimum of the regularized
objective function along the line.
Finally, we address the issue of searching in a
high-dimensional space by using the gradient of ex-
pected BLEU (Smith and Eisner, 2006) to find better
search directions for our line searches. This direction
finder addresses one of the serious concerns raised
by Hopkins and May (2011): MERT widely failed
to reach the optimum of a synthetic linear objective
function. In replicating Hopkins and May?s experi-
ments, we confirm that existing search algorithms for
MERT?including coordinate ascent, Powell?s algo-
rithm (Powell, 1964), and random direction sets (Cer
et al, 2008)?perform poorly in this experimental
condition. However, when using our gradient-based
direction finder, MERT has no problem finding the
true optimum even in a 1000-dimensional space.
Our results suggest that the combination of a reg-
ularized objective function and a gradient-informed
line search algorithm enables MERT to scale well
with a large number of features. Experiments with
up to 3600 features show that these extensions of
MERT yield results comparable to PRO (Hopkins
and May, 2011), a parameter tuning method known
to be effective with large feature sets.
2 Unregularized MERT
Prior to introducing regularized MERT, we briefly
review standard unregularized MERT (Och, 2003).
We use fS1 = {f1 . . . fS} to denote the S input sen-
tences of a given tuning set. For each sentence fs, let
Cs = {es,1 . . . es,M} denote the list of M -best can-
didate translations. Each input and output sentence
pair (fs, es,m) is weighted using a linear model that
applies model parameters w = (w1 . . . wD) ? RD
to D feature functions h1(f , e,?) . . . hD(f , e,?),
where ? is the hidden state associated with the
derivation from f to e, such as phrase segmenta-
tion and alignment. Furthermore, let hs,m ? RD
denote the feature vector representing the translation
pair (fs, es,m).
In MERT, the goal is to minimize a loss function
E(r, e) that scores translation hypotheses against a
set of reference translations rS1 = {r1 . . . rS}. This
yields the following optimization problem:
w? = argmin
w
{ S?
s=1
E(rs, e?(fs;w))
}
=
argmin
w
{ S?
s=1
M?
m=1
E(rs, es,m)?(es,m, e?(fs;w))
}
(1)
where
e?(fs;w) = argmax
m?{1...M}
{
w?hs,m
}
(2)
While the error surface of Equation 1 is only an
approximation of the true error surface of the MT
decoder, the quality of this approximation depends
on the size of the hypothesis space represented by the
M -best list. Therefore, the hypothesis list is grown
iteratively: decoding with an initial parameter vector
seeds the M -best lists; next, parameter estimation
and M -best list gathering alternate until the cumula-
tive M -best list no longer grows, or until changes of
w between two decoding runs are deemed too small.
To increase the size of the hypothesis space, subse-
quent work (Macherey et al, 2008) instead operated
on lattices, but this paper focuses on M -best lists.
A crucial observation is that the unsmoothed error
count represented in Equation 1 is a piecewise con-
stant function. This enabled Och (2003) to devise a
line search algorithm guaranteed to find the optimum
point along the line. To extend the search from one
to multiple dimensions, MERT applies a sequence
of line optimizations along some fixed or variable
set of search directions {dt} until some convergence
criteria are met. Considering a given point wt and
a given direction dt at iteration t, finding the most
probable translation hypothesis in the set of candi-
dates translations Cs = {es,1 . . . es,M} corresponds
to solving the following optimization problem:
e?(fs; ?) = argmax
m?{1...M}
{
(wt + ? ? dt)
?hs,m
}
(3)
The function in this equation is piecewise linear (Pa-
pineni, 1999), which enables an efficient exhaustive
computation. Specifically, this function is optimized
by enumerating the up to M hypotheses that form
the upper envelope of the model score function. The
error count, then, is a piecewise constant function
1949
defined by the points ?fs1 < ? ? ? < ?
fs
M at which an in-
crease in ? causes a change of optimum in Equation 3.
Error counts for the whole corpus are simply the sums
of sentence-level piecewise constant functions aggre-
gated over all sentences of the corpus.1 The optimal ?
is finally computed by enumerating all piecewise con-
stant intervals of the corpus-level error function, and
by selecting the one that has the lowest error count
(or, correspondingly, highest BLEU score). Assum-
ing the optimum is found in the interval [?k?1, ?k],
we define ?opt = (?k?1 + ?k)/2 and change the pa-
rameters using the update wt+1 = wt + ?opt ? dt.
Finally, this method is turned into a global D-
dimensional search using algorithms that repeat-
edly use the aforementioned exact line search algo-
rithm. Och (2003) first advocated the use of Powell?s
method (Powell, 1964; Press et al, 2007). Pharaoh
(Koehn, 2004) and subsequently Moses (Koehn et al,
2007) instead use coordinate ascent, and more recent
work often uses random search directions (Cer et al,
2008; Macherey et al, 2008). In Section 4, we will
present a novel direction finder for maximum-BLEU
optimization, which uses the gradient of expected
BLEU to find directions where the BLEU score is
most likely to increase.
3 Regularization for MERT
Because MERT is prone to overfitting when a large
number of parameters must be optimized, we study
the addition of a regularization term to the objective
function. One conventional approach is to regularize
the objective function with a penalty based on the
Euclidean norm ||w||2 =
??
iw
2
i , also known as `2
regularization. In the case of MERT, this yields the
following objective function:2
w? = argmin
w
{ S?
s=1
E(rs, e?(fs;w)) +
||w||22
2?2
}
(4)
1This assumes that the sufficient statistics of the metric under
consideration are additively decomposable by sentence, which
is the case with most popular evaluation metrics such as BLEU
(Papineni et al, 2001).
2The `2 regularizer is often used in conjunction with log-
likelihood objectives. The regularization term of Equation 4
could similarly be added to the log of an objective?e.g.,
log(BLEU) instead of BLEU?but we found that the distinc-
tion doesn?t have much of an impact in practice.
-0.2
0
0.2
0.4
0.6
0.8
1
1.2
1.4
-0.4 -0.3 -0.2 -0.1 0 0.1 0.2 0.3 0.4
MERT
Max at 0.225
?
?
-0.2
0
0.2
0.4
0.6
0.8
1
1.2
1.4
-0.4 -0.3 -0.2 -0.1 0 0.1 0.2 0.3 0.4
MERT? `2
Max at -0.018
?
?
?`2
-0.2
0
0.2
0.4
0.6
0.8
1
1.2
1.4
-0.4 -0.3 -0.2 -0.1 0 0.1 0.2 0.3 0.4
?, the step size in the current direction
MERT? `0
Max at 0
?
?
`0
Figure 1: Example MERT values along one coordi-
nate, first unregularized. When regularized with `2, the
piecewise constant function becomes piecewise quadratic.
When using `0, the function remains piecewise constant
with a point discontinuity at 0.
where the regularization term 1/2?2 is a free param-
eter that controls the strength of the regularization
penalty. Similar regularizers have also been used
in conjunction with other norms, such as `1 and `0
norms. The `1 norm, defined as ||w||1 =
?
i |wi|,
applies a constant force toward zero, preferring vec-
tors with fewer non-zero components; `0, defined as
||w||0 = |{i | wi 6= 0}|, simply counts the number of
non-zero components of the weight vector, encoding
a preference for sparse vectors.
Geometrically, `2 is a parabola, `1 is the wedge-
shaped absolute value function, and `0 is an impulse
function with a spike at 0. The original formulation
(Equation 1) of MERT consists of a piecewise con-
stant representation of the loss, as a function of the
step size in a given direction. But with these three reg-
1950
ularization terms, the function respectively becomes
piecewise quadratic, piecewise linear, or piecewise
constant with a potential impulse jump for each dis-
tinct choice of regularizer. Figure 1 demonstrates this
effect graphically.
As discussed in (McAllester and Keshet, 2011),
the problem with optimizing Equation 4 directly is
that the output of the underlying linear classifier, and
therefore the error count, are not sensitive to the scale
of w. Moreover, `2 regularization (as well as `1 reg-
ularization) is scale sensitive, which means any op-
timizer of this function can drive the regularization
term down to zero by scaling down w. As special
treatments for `2, we evaluate three linear transforms
of the weight vector, where the vector w of the regu-
larization term ||w||22/2?
2 is replaced with either:
1. an affine transform: w? w0
2. a vector with only (D ? 1) free parameters, e.g.,
(1, w?2, ? ? ? , w
?
D)
3. an `1 renormalization: w/||w||1
In (1), regularization is biased towards w0, a weight
vector previously optimized using a competitive yet
much smaller feature set, such as core features of
a phrase-based (Koehn et al, 2007) or hierarchical
(Chiang, 2007) system. The requirement that this
feature set be small is to prevent overfitting. Other-
wise, any regularization toward an overfit parameter
vector w0 would defeat the purpose of introducing
a regularization term in the first place.3 In (2), the
transformation is motivated by the observation that
the D-parameter linear model of Equation 2 only
needs (D ? 1) degrees of freedom. Fixing one of
the components of w to any non-zero constant and
allowing the others to vary, the new linear model re-
tains the same modeling power, but the (D ? 1) free
parameters are no longer scale invariant, i.e., scaling
the (D ? 1)-dimensional vector now has an effect on
linear model predictions. In (3), the weight vector
is normalized as to have an `1-norm equal to 1. In
contrast, the `0 norm is scale insensitive, thus not
affected by this problem.
3.1 Exact line search with regularization
Optimizing with a regularized error surface requires
a change in the line search algorithm presented in
3(Gimpel and Smith, 2012, footnote 6) briefly mentions the
use of such a regularizer with its ramp loss objective function.
Section 2, but the other aspects of MERT remain the
same, and we can still use global search algorithms
such as coordinate ascent, Powell, and random di-
rections exactly the same way as with unregularized
MERT. Line search with a regularization term is still
as efficient as in (Och, 2003), and it is still guar-
anteed to find the optimum of the (now regularized)
objective function along the line. Considering again a
given point wt and a given direction dt at line search
iteration t, finding the optimum ?opt corresponds to
finding ? that minimizes:
S?
s=1
E(rs, e?(fs; ?)) +
||wt + ? ? dt||22
2?2
(5)
Since regularization does not affect the points at
which e?(fs; ?) changes its optimum, the points
?fs1 < ? ? ? < ?
fs
M of intersection in the upper enve-
lope remain the same, so the points of discontinuity
in the error surface remain the same. The difference
now is that the error count on each segment [?i?1, ?i]
is no longer constant. This means we need to adjust
the final step of line search, which consists of enu-
merating all [?i?1, ?i], and keeping the optimum of
Equation 5 for each segment. e?(fs; ?) remains con-
stant within the segment, so we only need to consider
the expression ||wt + ? ? dt||22 to select a segment
point. The optimum is either at the left edge, the right
edge, or in the middle if the vertex of the parabola
happens to lie within that segment.4 We compute
this optimum by finding the value ? for which the
derivative of the regularization term is zero. There is
an easy closed-form solution:
d
d?
[
||wt + ? ? dt||22
2?2
]
= 0
d
d?
[
?
i
(w2t,i + 2 ? ? ? wt,i ? dt,i + ?
2 ? d2t,i)
]
= 0
?
i
(2 ? wt,i ? dt,i + 2 ? ? ? d
2
t,i) = 0
? = ?
(?
i
wt,i ? dt,i
)/(?
i
d2t,i
)
= ?
wt?dt
dt?dt
This closed-form solution is computed in time pro-
portional to D, which doesn?t slow down the com-
4When the optimum is either at the left edge ?i?1 or right
edge ?i of a segment, we select a point at a small relative distance
within the segment (.999?i?1 + .001?i, in the former case) to
avoid ties in objective values.
1951
putation of Equation 5 for each segment (the con-
struction of each segment of the upper envelope is
proportional to D anyway).
We also use `0 regularization. While minimiza-
tion of the `0-norm is known to be NP-hard in gen-
eral (Hyder and Mahata, 2009), this optimization is
relatively trivial in the case of a line search. Indeed,
for a given segment, the value in Equation 5 is con-
stant everywhere except where we intersect any of
the coordinate hyperplanes, i.e., where one of the
coordinates is zero. Thus, our method consists of
evaluating Equation 5 at the intersection points be-
tween the line and coordinate hyperplanes, returning
the optimal point within the given segment. For any
segment that doesn?t cross any of these hyperplanes,
we evaluate the objective function at any point of the
segment (since the value is constant across the entire
segment).
4 Direction finding
4.1 A Gradient-based direction finder
Perhaps the greatest obstacle in scaling MERT
to many dimensions is finding good search direc-
tions. In problems of lower dimensions, iterating
through all the coordinates is computationally feasi-
ble, though not guaranteed to find a global maximum
even in the case of a perfect line search. As the
number of dimensions increases by orders of mag-
nitude, this coordinate direction approach becomes
less and less tractable, and the quality of the search
also suffers (Hopkins and May, 2011).
Optimization has traditionally relied on finding the
direction of steepest ascent: the gradient. Unfortu-
nately, the objective function optimized by MERT is
piecewise constant; while it may admit a subgradi-
ent, this direction is generally not very informative.
Instead we may consider a smoothed variation of the
original approximation. While some variants have
been considered (Och, 2003; Flanigan et al, 2013),
we use an expected BLEU approximation, assum-
ing hypotheses are drawn from a log-linear distri-
bution according to their parameter values (Smith
and Eisner, 2006). That is, we assume the proba-
bility of a translation candidate es,m is proportional
to (exp (w?hs,m))
?, where w are the parameters be-
ing optimized, hs,m is the vector of the features for
es,m, and ? is a scaling parameter. As ? approaches
infinity, the distribution places all its weight on the
highest scoring candidate.
The log of the BLEU score may be written as:
min
(
1?
R
C
, 0
)
+
1
N
N?
n=1
(logmn ? log cn)
where R is the sum of reference lengths across the
corpus, C is the sum of candidate lengths, mn is the
number of matched n-grams (potentially clipped),
and cn is the number of n-grams in all candidates.
Given a distribution over candidates, we can use
the expected value of the log of the BLEU score. This
is a smooth approximation to the BLEU score, which
asymptotically approaches the true BLEU score as
the scaling parameter ? approaches infinity. While
this expectation is difficult to compute exactly, we
can compute approximations thereof using Taylor se-
ries. Although prior work demonstrates that a second-
order Taylor approximation is feasible to compute
(Smith and Eisner, 2006), we find that a first-order
approximation is faster and very close to the second-
order approximation.5 The first order Taylor approxi-
mation is as follows:
min
(
1?
R
E[C]
, 0
)
+
1
N
N?
n=1
(logE[mn]? logE[cn])
where E is the expectation operator using the proba-
bility distribution P (h;w, ?).
First we note that the gradient ??wiP (h;w, ?) is
P (h;w, ?)
(
hi ?
?
h?
h?iP (h
?;w, ?)
)
Using the chain rule, the gradient of the first order
approximation to BLEU is as follows:
1
N
N?
n=1
( 1
E[mn]
?
h
mn(h)
?P (h;w, ?)
?wi
?
1
E[cn]
?
h
cn(h)
?P (h;w, ?)
?wi
)
+
{
0 if E[C] > R
R
E[C]2
?
h c1(h)
?P (h;w,?)
?wi
otherwise
5Experimentally, we compared our analytical gradient of
the first-order Taylor approximation with the finite-difference
gradients of the first- and second-order approximations, and we
found these three gradients to be very close in terms of cosine
similarity (> 0.99). We performed these measurements both at
arbitrary points and at points of convergence of MERT.
1952
In the case of `2-regularized MERT, the final gradi-
ent also includes the partial derivative of the regular-
ization penalty of Equation 4, which is wi/?2 for a
given component i of the gradient. We do not update
the gradient in the case of `0 regularization since the
`0-norm is not differentiable.
4.2 Search
Our search strategy consists of looking at the direc-
tions of steepest increase of expected BLEU, which
is similar to that of Smith and Eisner (2006), but with
the difference that we do so in the context of MERT.
We think this difference provides two benefits. First,
while the smooth approximation of BLEU reduces
the likelihood of remaining trapped in a local opti-
mum, we avoid approximation error by retaining the
original objective function. Second, the benefit of
exact line searches in MERT is that there is no need
to be concerned about step size, since step size in
MERT line searches is guaranteed to be optimal with
respect to the direction under consideration.
Finally, our gradient-based search algorithm oper-
ates as follows. Considering the current point wt, we
compute the gradient gt of the first order Taylor ap-
proximation at that point, using the current scaling pa-
rameter ?. (We initialize the search with ? = 0.01.)
We find the optimum along the line wt+? ?gt. When-
ever any given line search yields no improvement
larger than a small tolerance threshold, we multiply
? by two and perform a new line search. The increase
of this parameter ? corresponds to a cooling schedule
(Smith and Eisner, 2006), which progressively sharp-
ens the objective function to get a better estimate of
BLEU as the search converges to an optimum. We
repeatedly perform new line searches until ? exceeds
1000. The inability to improve the current optimum
with a sharp approximation (? > 1000) doesn?t mean
line searches would fail with smaller values, so we
find it helpful to repeat the above procedure until a
full pass of updates of ? from 0.01 to 1000 yields no
improvement.
4.3 Computational complexity
Computing the gradient increases the computational
cost of MERT, though not its asymptotic complexity.
The cost of a single exhaustive line search is
O (SM(D + logM + logS))
where S is the number of sentences, each with M
possible translations, andD is the number of features.
For each sentence, we first identify the model score
as a linear function of the step size, requiring two
dot products for an overall cost of O(SMD).6 Next
we construct the upper envelope for each sentence:
first the equations are sorted in increasing order of
slope, and then they are merged in linear time to form
an envelope, with an overall cost of O(SM logM).
A linear pass through the envelope converts these
into piecewise constant (or linear, or quadratic) repre-
sentations of the (regularized) loss function. Finally
the per-sentence envelopes are merged into a global
representation of the loss along that direction. Our
implementation successively merges adjacent pairs
of piecewise smooth loss function representations
until a single list remains. These logS passes lead to
a merging runtime of O(SM logS).
The time required to compute a gradient is pro-
portional to O(SMD). For each sentence, we first
gather the probability and its gradient, then use this to
compute expected n-gram counts and matches as well
as those gradients in time O(MD). A constant num-
ber of arithmetic operations suffice to compute the
final expected loss value and its gradient. Therefore,
computing the gradient does not increase the algo-
rithmic complexity when compared to conventional
approaches using coordinate ascent and random di-
rections. Likewise the runtime of a single iteration
is competitive with PRO, given that gradient finding
is generally the most expensive part of convex opti-
mization. Of course, it is difficult to compare overall
runtime of convex optimization with that of MERT,
as we know of no way to bound the number of gradi-
ent evaluations required for convergence with MERT.
Therefore, we resort to empirical comparison later in
the paper, and find that the two methods appear to
have comparable runtime.
6In the special case where the difference between the prior
direction and the current direction is sparse, we may update the
individual linear functions in time proportional to the number of
changed dimensions. Coordinate ascent in particular can update
the linear functions in time O(SM): to the intercept of the
equation for each translation, we may add the prior step size
multiplied by the feature value in the prior coordinate, and the
slope becomes the feature value in the new coordinate. However,
this optimization does not appear to be widely adopted, likely
because it does not lead to any speedup when random vectors,
conjugate directions, or other non-sparse directions are used.
1953
Language pair Train Tune Dev Test
G
B
M
Chinese-English 0.99M 1,797 1,000 1,082
(mt02+03) (mt05)
Finnish-English 2.20M 11,935 2,001 4,855
S
pa
rs
eH
R
M Chinese-English 3.51M 1,894 1,664 1,357
(mt05) (mt06) (mt08)
Arabic-English 1.49M 1,663 1,360 1,313
(mt06) (mt08) (mt09)
Table 1: Datasets for the two experimental conditions.
5 Experimental Design
Following Hopkins and May (2011), our experimen-
tal setup utilizes both real and synthetic data. The
motivation for using synthetic data is that it is a way
of gauging the quality of optimization methods, since
the data is constructed knowing the global optimum.
Hopkins and May also note that the use of an ob-
jective function that is linear in some gold weight
vector makes the search much simpler than in a real
translation setting, and they suggest that a learner
that performs poorly in such a simple scenario has
little hope of succeeding in a more complex one.
The setup of our synthetic data experiment is al-
most the same as that performed by Hopkins and
May (2011). We generate feature vectors of dimen-
sionality ranging from 10 to 1000. These features are
generated by drawing random numbers uniformly in
the interval [0, 500]. This synthetic dataset consists
of S=1000 source ?sentences?, and M=500 ?trans-
lation? hypotheses for each sentence. A pseudo
?BLEU? score is then computed for each hypothe-
sis, by computing the dot product between a prede-
fined gold weight vector w? and each feature vector
hs,m. By this linear construction, w? is guaranteed
to be a global optimum.7 The pseudo-BLEU score is
normalized for each M -best list, so that the transla-
tion with highest model score according to w? has
a BLEU score of 1, and so that the translation with
lowest model score for the sentence gets a BLEU of
zero. This normalization has no impact on search,
but makes results more interpretable.
For our translation experiments, we use multi-
stack phrase-based decoding (Koehn et al, 2007).
We report results for two feature sets: non-linear
features induced using Gradient Boosting Machines
(Toutanova and Ahn, 2013) and sparse lexicalized
7The objective function remains piecewise constant, and the
plateau containingw? maps to the optimal value of the function.
reordering features (Cherry, 2013). We exploit these
feature sets (GBM and SparseHRM, respectively) in
two distinct experimental conditions, which we de-
tail in the two next paragraphs. Both GBM and
SparseHRM augment baseline features similar to
Moses?: relative frequency and lexicalized phrase
translation scores for both translation directions; one
or two language model features, depending on the
language pair; distortion penalty; word and phrase
count; six lexicalized reordering features. For both
experimental conditions, phrase tables have maxi-
mum phrase length of 7 words on either side. In
reference to Table 1, we used the training set (Train)
for extracting phrase tables and language models; the
Tune set for optimization with MERT or PRO; the
Dev set for selecting hyperparameters of PRO and
regularized MERT; and the Test set for reporting fi-
nal results. In each experimental condition, we first
trained weights for the base feature sets, and then
decoded the Tune, Dev, and Test datasets, generating
500-best lists for each set. All results report rerank-
ing performance on these lists with different feature
sets and optimization methods, based on lower-cased
BLEU (Papineni et al, 2001).
The GBM feature set (Toutanova and Ahn, 2013)
consists of about 230 features automatically induced
using decision tree weak learners, which derive fea-
tures using various word-level, phrase-level, and mor-
phological attributes. For Chinese-English, the train-
ing corpus consists of approximately one million sen-
tence pairs from the FBIS and Hong Kong portions
of the LDC data for the NIST MT evaluation and the
Tune and Test sets are from NIST competitions. A
4-gram language model was trained on the Xinhua
portion of the English Gigaword corpus and on the
target side of the bitext. For Finnish-English we used
a dataset from a technical domain of software man-
uals. For this language pair we used two language
models: one very large model trained on billions of
words, and another language model trained from the
target side of the parallel training set.
The SparseHRM set (Cherry, 2013) contains 3600
sparse reordering features. For each phrase, the fea-
tures take the form of indicators describing its orienta-
tion in the derivation, and its lexical content in terms
of word clusters or frequent words. For both Chinese-
English and Arabic-English, systems are trained on
data from the NIST 2012 MT evaluation. 4-gram
1954
 0
 0.2
 0.4
 0.6
 0.8
 1
50 100 500 1000 20  200
BL
EU
number of features
expected BLEU gradient
random directionsPowell
coordinate ascent
 0
 0.2
 0.4
 0.6
 0.8
 1
50 100 500 1000 20  200
co
sin
e
number of features
expected BLEU gradient
random directionsPowell
coordinate ascent
Figure 2: Change in BLEU score and cosine similarity
to the gold weight vector w? as the number of features
increases, using the noisy synthetic experiments. The
gradient-based direction finding method is barely affected
by the noise. The increase of the number of dimensions en-
ables our direction finder to find a slightly better optimum,
which moved away from w? due to noise.
language models were trained on the target side of
the parallel training data for both Arabic and Chinese.
The Chinese systems development set is taken from
the NIST mt05 evaluation set, augmented with some
material reserved from our NIST training corpora in
order to better cover newsgroup and weblog domains.
6 Results
We conducted experiments with the synthetic data
scenario described in the previous section, as well
as with noise added to the data (Hopkins and May,
2011). The purpose of adding noise is to make the
optimization task more realistic. Specifically, af-
ter computing all pseudo-BLEU scores, we added
noise to each feature vector hs,m by drawing from
a zero-mean Gaussian with standard deviation 200.
Our results with both noiseless and noisy data yield
the same conclusion as Hopkins and May: standard
MERT struggles with many dimensions, and fails
to recover w?. However, our experiments with the
gradient direction finder of Section 4 are much more
positive. This direction finder not only recovers w?
 40
 50
 60
 70
 80
 90
 100
 1  10  100  1000
BL
EU
line search iteration
expected BLEU gradient(noisy) expected BLEU gradient
coordinate ascent(noisy) coordinate ascent
Figure 3: Comparison of rate of convergence between
coordinate ascent and our expected BLEU direction finder
(D = 500). Noisy refers to the noisy experimental setting.
(cosine > 0.999) even with 1000 dimensions, but its
effectiveness is also visible with noisy data, as seen
in Figure 2. The decrease of its cosine is relatively
small compared to other search algorithms, and this
decrease is not necessarily a sign of search errors
since the addition of noise causes the true optimum
to be different from w?. Finally, Figure 3 shows our
rate of convergence compared to coordinate ascent.
Our experimental results with the GBM feature
set data are shown in Table 2. Each table is di-
vided into three sections corresponding respectively
to MERT (Och, 2003) with Koehn-style coordinate
ascent (Koehn, 2004), PRO, and our optimizer featur-
ing both regularization and the gradient-based direc-
tion finder. All variants of MERT are initialized with
a single starting point, which is either uniform weight
or w0. Instead of providing MERT with additional
random starting points as in Moses, we use random
walks as in (Moore and Quirk, 2008) to attempt to
move out of local optima.8 Since PRO and our opti-
mizer have hyperparameters, we use a held-out set
(Dev) for adjusting them. For PRO, we adjust three
parameters: a regularization penalty for `2, the pa-
rameter ? in the add-? smoothed sentence-level ver-
sion of BLEU (Lin and Och, 2004), and a parameter
for scaling the corpus-level length of the references.
The latter scaling parameter is discussed in (He and
8In the case of the gradient-based direction finder, we also
use the following strategy whenever optimization converges to
a (possibly local) optimum. We run one round of coordinate
ascent, and continue with the gradient direction finder as soon as
the optimum improves. If the none of the coordinate directions
helped, we stop the search.
1955
Chinese-English Finnish-English
Method Starting pt. # feat. Tune Dev Test # feat. Tune Dev Test
MERT uniform 14 33.2 19.9 32.9 15 53.0 52.6 54.8
MERT uniform 224 33.0 19.2 32.1 232 53.2 51.7 53.8
MERT w0 224 34.1 20.1 33.0 232 53.9 52.5 54.7
PRO w0 224 33.4 20.1 33.3 232 53.3 52.9 55.3
`2 MERT (v1: ||w ?w0||) w0 224 33.2 20.3 33.5 232 53.2 52.7 55.2
`2 MERT (v2: D ? 1 dimensions) w0 224 33.0 20.4 33.2 232 52.9 52.6 55.0
`2 MERT (v3: `1-renormalized) w0 224 33.1 20.0 33.3 232 53.1 52.5 55.1
`0 MERT w0 224 33.4 20.3 33.2 232 53.2 52.6 55.1
Table 2: BLEU scores for GBM features. Model parameters were optimized on the Tune set. For PRO and regularized
MERT, we optimized with different hyperparameters (regularization weight, etc.), and retained for each experimental
condition the model that worked best on Dev. The table shows the performance of these retained models.
 51.2
 51.4
 51.6
 51.8
 52
 52.2
 52.4
 52.6
 1e-05  0.0001  0.001  0.01  0.1  1  10
BL
EU
regularization weight
expected BLEU gradient
coordinate ascent
Figure 4: BLEU score on the Finnish Dev set (GBM)
with different values for the 1/2?2 regularization weight.
To enable comparable results, the other hyperparameter
(length) is kept fixed.
Deng, 2012; Nakov et al, 2012) and addresses the
problem that systems tuned with PRO tend to pro-
duce sentences that are too short. On the other hand,
regularized MERT only requires one hyperparameter
to tune: a regularization penalty for `2 or `0. How-
ever, since PRO optimizes translation length on the
Dev dataset and MERT does so using the Tune set, a
comparison of the two systems would yield a discrep-
ancy in length that would be undesirable. Therefore,
we add another hyperparameter to regularized MERT
to tune length in the same manner using the Dev set.
Table 2 offers several findings. First, unregular-
ized MERT can achieve competitive results with a
small set of highly engineered features, but adding a
large set of more than 200 features causes MERT to
perform poorly, particularly on the test set. However,
unregularized MERT can recover much of this drop
of performance if it is given a good sparse initializer
w0. Regularized MERT (v1) provides an increase in
the order of 0.5 BLEU on the test set compared to
the best results with unregularized MERT. Regular-
ized MERT is competitive with PRO, even though the
number of features is relatively large. Using the same
GBM experimental setting, Figure 4 compares regu-
larized MERT using the gradient direction finder and
coordinate ascent. At the best regularization setting,
the two algorithms are comparable in terms of BLEU
(though coordinate ascent is slower due to its lack of
a good direction finder), but our method seems more
robust with suboptimal regularization parameters.
Our results with the SparseHRM feature set data
are shown in Table 3. As with the GBM feature set,
we find again that the version of `2 MERT regular-
ized towards ||w ?w0|| is competitive with PRO,
even though we train MERT with a large set of 3601
features.9 One remaining question is whether MERT
remains practical with large feature sets. As noted
in the complexity analysis of Section 4.3, MERT
has a dependence on the number of features that is
comparable to PRO, i.e., it is linear in both cases.
Practically, we find that optimization time is com-
parable between the two systems. In the case of
Chinese-English for the GBM feature set, one run of
the PRO optimizer took 26 minutes on average, while
regularized MERT with the gradient direction finder
took 37 minutes on average, taking into account the
time to compute w0. In the case of Chinese-English
for the SparseHRM feature set, average optimization
times for PRO and our method were 3.10 hours and
3.84 hours on average, respectively.
9We note that the experimental setup of (Cherry, 2013) inte-
grates the Sparse HRM features into the decoder, while we use
them in an M -best reranking scenario. The reranking setup of
this paper yields smaller improvements for both PRO and MERT
than those of (Cherry, 2013).
1956
Chinese-English Arabic-English
Method Starting pt. # feat. Tune Dev Test # feat. Tune Dev Test
MERT uniform 14 25.7 34.0 27.8 14 43.2 42.8 45.5
MERT uniform 3601 25.4 33.1 27.3 3601 45.7 42.3 44.9
MERT w0 3601 27.7 33.5 27.5 3601 46.0 42.4 45.2
PRO w0 3601 25.9 34.3 28.1 3601 44.6 43.4 46.1
`2 MERT (v1: ||w ?w0||) w0 3601 26.3 34.3 28.3 3601 45.2 43.2 46.0
`2 MERT (v2: D ? 1 dimensions) w0 3601 26.4 34.1 28.2 3601 45.0 43.4 45.9
`2 MERT (v3: `1-renormalized) w0 3601 26.1 34.0 27.9 3601 44.9 43.3 45.7
`0 MERT w0 3601 26.5 34.2 28.1 3601 45.4 43.1 46.0
Table 3: BLEU scores for SparseHRM features. Notes in Table 2 also apply here.
Finally, as shown in Table 2, we see that MERT ex-
periments that rely on a good initial starting point w0
generally perform better than when starting from
a uniform vector. While having to compute w0 in
the first place is a bit of a disadvantage compared
to standard MERT, the need for good initializer is
hardly surprising in the context of non-convex op-
timization. Other non-convex problems in machine
learning, such as deep neural networks (DNN) and
word alignment models, commonly require such ini-
tializers in order to obtain decent performance. In
the case of DNN, extensive research is devoted to the
problem of finding good initializers.10 In the case of
word alignment, it is common practice to initialize
search in non-convex optimization problems?such
as IBM Model 3 and 4 (Brown et al, 1993)?with
solutions of simpler models?such as IBM Model 1.
7 Related work
MERT and its extensions have been the target of ex-
tensive research (Och, 2003; Macherey et al, 2008;
Cer et al, 2008; Moore and Quirk, 2008; Kumar et
al., 2009; Galley and Quirk, 2011). More recent work
has focused on replacing MERT with a linearly de-
composable approximations of the evaluation metric
(Smith and Eisner, 2006; Liang et al, 2006; Watan-
abe et al, 2007; Chiang et al, 2008; Hopkins and
May, 2011; Rosti et al, 2011; Gimpel and Smith,
2012; Cherry and Foster, 2012), which generally
involve a surrogate loss function incorporating a reg-
ularization term such as the `2-norm. While we are
not aware of any previous work adding a penalty on
10For example, (Larochelle et al, 2009) presents a pre-trained
DNN that outperforms a shallow network, but the performance
of the DNN becomes much worse relative to the shallow network
once pre-training is turned off.
the weights in the context of MERT, (Cer et al, 2008)
achieves a related effect. Cer et al?s goal is to achieve
a more regular or smooth objective function, while
ours is to obtain a more regular set of parameters.
The two approaches may be complementary.
More recently, new research has explored direction
finding using a smooth surrogate loss function (Flani-
gan et al, 2013). Although this method is successful
in helping MERT find better directions, it also exac-
erbates the tendency of MERT to overfit.11 As an
indirect way of controlling overfitting on the tuning
set, their line searches are performed over directions
estimated over a separate dataset.
8 Conclusion
In this paper, we have shown that MERT can scale to
a much larger number of features than previously
thought, thanks to regularization and a direction
finder that directs the search towards the greatest
increase of expected BLEU score. While our best
results are comparable to PRO and not significantly
better, we think that this paper provides a deeper un-
derstanding of why standard MERT can fail when
handling an increasingly larger number of features.
Furthermore, this paper complements the analysis
by Hopkins and May (2011) of the differences be-
tween MERT and optimization with a surrogate loss
function.
Acknowledgments
We thank the anonymous reviewers for their helpful
comments and suggestions.
11Indeed, in their Table 3, a comparison between HILS and
HOLS suggests tuning set performance improves substantially,
while held out performance degrades.
1957
References
Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della
Pietra, and Robert L. Mercer. 1993. The mathematics
of statistical machine translation: parameter estimation.
Comput. Linguist., 19(2):263?311.
Daniel Cer, Dan Jurafsky, and Christopher D. Manning.
2008. Regularization and search for minimum error
rate training. In Proceedings of the Third Workshop on
Statistical Machine Translation, pages 26?34.
Colin Cherry and George Foster. 2012. Batch tuning
strategies for statistical machine translation. In Pro-
ceedings of the 2012 Conference of the North American
Chapter of the Association for Computational Linguis-
tics: Human Language Technologies, pages 427?436.
Colin Cherry. 2013. Improved reordering for phrase-
based translation using sparse features. In Proceedings
of the 2013 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies, pages 22?31.
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and structural
translation features. In Proceedings of the 2008 Con-
ference on Empirical Methods in Natural Language
Processing, pages 224?233.
David Chiang, Kevin Knight, and Wei Wang. 2009.
11,001 new features for statistical machine translation.
In Proceedings of Human Language Technologies: The
2009 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 218?226.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
Jeffrey Flanigan, Chris Dyer, and Jaime Carbonell. 2013.
Large-scale discriminative training for statistical ma-
chine translation using held-out line search. In Pro-
ceedings of the 2013 Conference of the North American
Chapter of the Association for Computational Linguis-
tics: Human Language Technologies, pages 248?258.
Michel Galley and Chris Quirk. 2011. Optimal search
for minimum error rate training. In Proceedings of
the 2011 Conference on Empirical Methods in Natural
Language Processing, pages 38?49.
Kevin Gimpel and Noah A. Smith. 2012. Structured ramp
loss minimization for machine translation. In Proceed-
ings of the 2012 Conference of the North American
Chapter of the Association for Computational Linguis-
tics: Human Language Technologies, pages 221?231.
Xiaodong He and Li Deng. 2012. Maximum expected
BLEU training of phrase and lexicon translation mod-
els. In Proceedings of the 50th Annual Meeting of
the Association for Computational Linguistics: Long
Papers - Volume 1, pages 292?301.
Mark Hopkins and Jonathan May. 2011. Tuning as rank-
ing. In Proceedings of the 2011 Conference on Empir-
ical Methods in Natural Language Processing, pages
1352?1362.
M. Hyder and K. Mahata. 2009. An approximate L0
norm minimization algorithm for compressed sens-
ing. In Acoustics, Speech and Signal Processing,
2009. ICASSP 2009. IEEE International Conference
on, pages 3365?3368.
Philipp Koehn, Hieu Hoang, Alexandra Birch Mayne,
Christopher Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proc. of ACL, Demonstration Session.
Philipp Koehn. 2004. Pharaoh: a beam search decoder
for phrase-based statistical machine translation models.
In Proc. of AMTA, pages 115?124.
Shankar Kumar, Wolfgang Macherey, Chris Dyer, and
Franz Och. 2009. Efficient minimum error rate train-
ing and minimum bayes-risk decoding for translation
hypergraphs and lattices. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference on Natural
Language Processing of the AFNLP, pages 163?171.
Hugo Larochelle, Yoshua Bengio, Je?ro?me Louradour, and
Pascal Lamblin. 2009. Exploring strategies for training
deep neural networks. J. Mach. Learn. Res., 10:1?40.
P. Liang, A. Bouchard-Co?te?, D. Klein, and B. Taskar.
2006. An end-to-end discriminative approach to ma-
chine translation. In International Conference on Com-
putational Linguistics and Association for Computa-
tional Linguistics (COLING/ACL).
Chin-Yew Lin and Franz Josef Och. 2004. ORANGE:
a method for evaluating automatic evaluation metrics
for machine translation. In Proceedings of the 20th
international conference on Computational Linguistics,
Stroudsburg, PA, USA.
Wolfgang Macherey, Franz Och, Ignacio Thayer, and
Jakob Uszkoreit. 2008. Lattice-based minimum error
rate training for statistical machine translation. In Pro-
ceedings of the 2008 Conference on Empirical Methods
in Natural Language Processing, pages 725?734.
David McAllester and Joseph Keshet. 2011. Generaliza-
tion bounds and consistency for latent structural probit
and ramp loss. In Advances in Neural Information
Processing Systems 24, pages 2205?2212.
Robert C. Moore and Chris Quirk. 2008. Random restarts
in minimum error rate training for statistical machine
translation. In Proceedings of the 22nd International
Conference on Computational Linguistics - Volume 1,
pages 585?592.
1958
Preslav Nakov, Francisco Guzman, and Stephan Vogel.
2012. Optimizing for sentence-level BLEU+1 yields
short translations. In Proceedings of COLING 2012,
pages 1979?1994.
Franz Josef Och and Hermann Ney. 2002. Discriminative
training and maximum entropy models for statistical
machine translation. In Proceedings of 40th Annual
Meeting of the Association for Computational Linguis-
tics, pages 295?302.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Annual Meeting of the Association for Computa-
tional Linguistics, pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. BLEU: a method for automatic evalu-
ation of machine translation. In Proc. of ACL.
Kishore Papineni. 1999. Discriminative training via linear
programming. In Proceedings IEEE International Con-
ference on Acoustics, Speech, and Signal Processing
(ICASSP), volume 2, pages 561?564, Vol. 2.
M.J.D. Powell. 1964. An efficient method for finding
the minimum of a function of several variables without
calculating derivatives. Comput. J., 7(2):155?162.
William H. Press, Saul A. Teukolsky, William T. Vetter-
ling, and Brian P. Flannery. 2007. Numerical Recipes:
The Art of Scientific Computing. Cambridge University
Press, 3rd edition.
Antti-Veikko Rosti, Bing Zhang, Spyros Matsoukas, and
Richard Schwartz. 2011. Expected BLEU training
for graphs: BBN system description for WMT11 sys-
tem combination task. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, pages
159?165.
David A. Smith and Jason Eisner. 2006. Minimum risk
annealing for training log-linear models. In Proceed-
ings of the COLING/ACL 2006 Main Conference Poster
Sessions, pages 787?794.
Kristina Toutanova and Byung-Gyu Ahn. 2013. Learn-
ing non-linear features for machine translation using
gradient boosting machines. In Proceedings of the 51st
Annual Meeting of the Association for Computational
Linguistics (Volume 2: Short Papers), pages 406?411.
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and Hideki
Isozaki. 2007. Online large-margin training for sta-
tistical machine translation. In Proceedings of the
2007 Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL), pages 764?773.
1959
Linguistic Structure Prediction
? 2012 Association for Computational Linguistics
Noah A. Smith
Carnegie Mellon University
Morgan & Claypool (Synthesis Lectures on Human Language Technologies, edited
by Graeme Hirst, volume 13), 2011, xx+248 pp; paperbound, ISBN 978-1-60845-405-1,
$60.00; ebook, ISBN 978-1-60845-406-8, $30.00 or by subscription
Reviewed by
Chris Quirk
Microsoft Research
Noah Smith?s ambitious newmonograph, Linguistic Structure Prediction, ?aims to bridge
the gap between natural language processing and machine learning.? Given that cur-
rent natural language processing (NLP) research makes heavy demands on machine-
learning techniques, and a sizeable fraction of modern machine learning (ML) research
focuses on structure prediction, this is clearly a timely and important topic. To address
the gaps and overlaps between these two large and well-developed fields in five brief
chapters is a difficult feat. The text, though not without its flaws, does an admirable job
of building this bridge.
An introductory first chapter surveys current research areas in statistical NLP,
cataloging and defining many common linguistic structure prediction tasks. Machine
learning students new to the area are likely to find this helpful albeit a bit terse; NLP
students will likely consider this section primarily a review. The subsequent chapters
change character abruptly, delving into mathematical details and heavy formalism.
Chapter 2 introduces the concept of decoding, presenting five distinct viewpoints
on the search for the highest scoring structure. The reader is quickly ushered through
graphical models, polytopes, grammars, hypergraphs, and weighted deduction sys-
tems, with descriptions based on an example in sequence tagging. The broad coverage,
multi-viewpoint discussion encourages the reader to make connections between many
distinct approaches, and provides solid formalism for reasoning about decoding prob-
lems. It is a comprehensive introduction to the most common and effective decoding
approaches, with one significant exception: the recent advances in dual decomposition
and Lagrangian relaxation methods. Timing is likely the culprit. This book was devel-
oped mainly from 2006 to 2009, whereas dual decomposition did not attain notoriety in
our community until a few years later (Rush et al 2010). Relaxation approaches, though
potentially a passing phase, have successfully broadened the reach of simpler decoding
techniques into more complicated domains such as structured event extraction. They
would have made a nice addition. Regardless, this second chapter equips the reader
with sufficient machinery to solve a number of structured prediction problems.
Chapter 3 applies the machinery described in the prior chapter to the problem
of supervised structure induction. Probabilistic generative and conditional models are
introduced in some detail, followed by a discussion of margin-based methods. Hidden
Markov models (HMMs) and probabilistic context-free grammars are introduced in
detail, followed by solid descriptions of maximum likelihood estimation and smooth-
ing. The section on conditional models is well written and crucial, because so many
commonly used tasks can be treated as sequence modeling using techniques such
as conditional random fields. Much of the subject matter introduced abstractly in
Chapter 2 is presented in this chapter using specific algorithms. For instance, sequence
Computational Linguistics Volume 38, Number 2
modeling is discussed broadly in Chapter 2; the specific algorithms for HMMs are fully
defined in Chapter 3. This coarse-to-fine introduction of material may challenge readers
who are accustomed to more practical descriptions of material. Were I to teach a course
based on this book, I would be tempted to present the third chapter before the second.
Chapter 4 focuses on semisupervised, unsupervised, and hidden variable learning.
With a good mix of theory and practical examples, Expectation-Maximization (EM) is
introduced and grounded in several problems, then generalized with log-linear models
and approximated with contrastive estimation. Hard EM is mentioned in the context
of several examples, though a more detailed description of this potentially important
technique (cf. Spitkovsky et al, 2010) would bridge thematerial of Chapters 2 and 4. The
chapter then describes Bayesian approaches to NLP, working from theory into specific
techniques and landing in models. Finally, a brief section is devoted to the related area
of hidden variable learning.
Chapter 5 begins by describing the partition function, as well as inference tech-
niques for the partition function and decoding methods. I found it strange that this
important section was postponed so late in the book; much of the material was forward-
referenced throughout Chapter 4. Regardless, the techniques are described in a unifying,
generic manner. The book concludes with a discussion of minimum Bayes risk decod-
ing, and a few other variants.
Four appendices are devoted to optimization, experimental techniques, maximum
entropy, and locally normalized conditional models. All of these sections provide some
useful background. The section on hypothesis testing in Appendix B would be espe-
cially useful to students new to the area. It can be difficult to pick the correct hypothesis
testing method in general, and this problem is exacerbated in structure prediction. This
material serves as a good guide for a researcher hoping to evaluate how effective these
methods are.
I have some concerns about the intended audience. Descriptions quickly descend
into heavy notation and require knowledge of a broad range of mathematical concepts,
from marginal polytopes to semirings. I suspect the average NLP graduate student
would find it difficult to approach much of the material without a series of courses
in probability, statistics, and machine learning. The book is also very theoretical: Few
concrete algorithms are provided. Instead, the concepts are introduced using only
mathematics and formalism. For readers already conversant in the mapping from
mathematical descriptions into concrete algorithms and implementations, this will not
be a significant barrier. From the other direction, the structures used in NLP (e.g.,
dependency trees) are relatively well motivated, though a machine-learning researcher
new to the areamight benefit from a fuller introduction to NLP. However, the text serves
as an effective guide for introducing the machine-learning community into the NLP
community, but I feel it would be challenging to use in the other direction.
This may be a personal bias, but I was surprised by the avoidance of machine-
translation?related techniques, despite obvious influences. Why resort to the term ?de-
coding? if not because of decipherment and translation? One of the most effective uses
of hidden variable learning is in word alignment; it seems like a personal example.
Of course, building an effective machine-translation system requires a huge amount
of engineering in addition to the underlying theory, but I felt some discussion of the
problem and effective techniques would be pertinent.
Despite my struggles with the book?s organization and a few important omissions,
I must admit that I want a copy for my bookshelf. The author covers a huge amount of
material in just under 200 pages, touching on some of the most important algorithmic
techniques and viewpoints in modern statistical NLP. At times the text reads like a
456
Book Reviews
survey, touching very briefly on a huge range of topics. Yet the survey is comprehensive
and enlightening, tying together a broad range of topics and viewpoints. Younger grad-
uate students may require serious effort to comprehend the full text, but the modern
NLP researcher looking to advance the state of the art in structured prediction must
truly understand the concepts presented here.
References
Rush, Alexander M., David Sontag,
Michael Collins, and Tommi Jaakkola.
2010. On dual decomposition and linear
programming relaxations for natural
language processing. In Proceedings of the
2010 Conference on Empirical Methods in
Natural Language Processing, pages 1?11.
Cambridge, MA.
Spitkovsky, Valentin I., Hiyan Alshawi,
Daniel Jurafsky, and Christopher
D. Manning. 2010. Viterbi training
improves unsupervised dependency
parsing. In Proceedings of the Fourteenth
Conference on Computational Natural
Language Learning, pages 9?17.
Uppsala.
This book review was edited by Pierre Isabelle.
Chris Quirk is a Researcher in the Natural Language Processing group at Microsoft Research. His
research interests include machine translation and paraphrase, particularly at the confluence of
large data and linguistic analysis tools. Quirk?s e-mail address is chrisq@microsoft.com.
457
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 403?411,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Extracting Parallel Sentences from Comparable Corpora using Document
Level Alignment
Jason R. Smith?
Center for Lang. and Speech Processing
Johns Hopkins University
Baltimore, MD 21218
jsmith@cs.jhu.edu
Chris Quirk and Kristina Toutanova
Microsoft Research
One Microsoft Way
Redmond, WA 98052
{chrisq,kristout}@microsoft.com
Abstract
The quality of a statistical machine transla-
tion (SMT) system is heavily dependent upon
the amount of parallel sentences used in train-
ing. In recent years, there have been several
approaches developed for obtaining parallel
sentences from non-parallel, or comparable
data, such as news articles published within
the same time period (Munteanu and Marcu,
2005), or web pages with a similar structure
(Resnik and Smith, 2003). One resource not
yet thoroughly explored is Wikipedia, an on-
line encyclopedia containing linked articles
in many languages. We advance the state
of the art in parallel sentence extraction by
modeling the document level alignment, mo-
tivated by the observation that parallel sen-
tence pairs are often found in close proximity.
We also include features which make use of
the additional annotation given by Wikipedia,
and features using an automatically induced
lexicon model. Results for both accuracy
in sentence extraction and downstream im-
provement in an SMT system are presented.
1 Introduction
For any statistical machine translation system, the
size of the parallel corpus used for training is a ma-
jor factor in its performance. For some language
pairs, such as Chinese-English and Arabic-English,
large amounts of parallel data are readily available,
but for most language pairs this is not the case. The
?This research was conducted during the author?s intern-
ship at Microsoft Research.
domain of the parallel corpus also strongly influ-
ences the quality of translations produced. Many
parallel corpora are taken from the news domain, or
from parliamentary proceedings. Translation qual-
ity suffers when a system is not trained on any data
from the domain it is tested on.
While parallel corpora may be scarce, compara-
ble, or semi-parallel corpora are readily available
in several domains and language pairs. These cor-
pora consist of a set of documents in two languages
containing similar information. (See Section 2.1
for a more detailed description of the types of non-
parallel corpora.) In most previous work on ex-
traction of parallel sentences from comparable cor-
pora, some coarse document-level similarity is used
to determine which document pairs contain paral-
lel sentences. For identifying similar web pages,
Resnik and Smith (2003) compare the HTML struc-
ture. Munteanu and Marcu (2005) use publication
date and vector-based similarity (after projecting
words through a bilingual dictionary) to identify
similar news articles.
Once promising document pairs are identified,
the next step is to extract parallel sentences. Usu-
ally, some seed parallel data is assumed to be avail-
able. This data is used to train a word align-
ment model, such as IBM Model 1 (Brown et al,
1993) or HMM-based word alignment (Vogel et al,
1996). Statistics from this word alignment model
are used to train a classifier which identifies bilin-
gual sentence pairs as parallel or not parallel. This
classifier is applied to all sentence pairs in docu-
ments which were found to be similar. Typically,
some pruning is done to reduce the number of sen-
403
tence pairs that need to be classified.
While these methods have been applied to news
corpora and web pages, very little attention has
been given to Wikipedia as a source of parallel sen-
tences. This is surprising, given that Wikipedia
contains annotated article alignments, and much
work has been done on extracting bilingual lexi-
cons on this dataset. Adafre and de Rijke (2006)
extracted similar sentences from Wikipedia article
pairs, but only evaluated precision on a small num-
ber of extracted sentences.
In this paper, we more thoroughly investigate
Wikipedia?s viability as a comparable corpus, and
describe novel methods for parallel sentence ex-
traction. Section 2 describes the multilingual re-
sources available in Wikipedia. Section 3 gives fur-
ther background on previous methods for parallel
sentence extraction on comparable corpora, and de-
scribes our approach, which finds a global sentence
alignment between two documents. In Section
4, we compare our approach with previous meth-
ods on datasets derived from Wikipedia for three
language pairs (Spanish-English, German-English,
and Bulgarian-English), and show improvements in
downstream SMT performance by adding the paral-
lel data we extracted.
2 Wikipedia as a Comparable Corpus
Wikipedia (Wikipedia, 2004) is an online collabo-
rative encyclopedia available in a wide variety of
languages. While the English Wikipedia is the
largest, with over 3 million articles, there are 24
language editions with at least 100,000 articles.
Articles on the same topic in different languages
are also connected via ?interwiki? links, which are
annotated by users. This is an extremely valuable
resource when extracting parallel sentences, as the
document alignment is already provided. Table
1 shows how many of these ?interwiki? links are
present between the English Wikipedia and the 16
largest non-English Wikipedias.
Wikipedia?s markup contains other useful indica-
tors for parallel sentence extraction. The many hy-
perlinks found in articles have previously been used
as a valuable source of information. (Adafre and
de Rijke, 2006) use matching hyperlinks to iden-
tify similar sentences. Two links match if the arti-
Figure 1: Captions for an image of a foil in English and
Spanish
cles they refer to are connected by an ?interwiki?
link. Also, images in Wikipedia are often stored
in a central source across different languages; this
allows identification of captions which may be par-
allel (see Figure 1). Finally, there are other minor
forms of markup which may be useful for finding
similar content across languages, such as lists and
section headings. In Section 3.3, we will explain
how features are derived from this markup.
2.1 Types of Non-Parallel Corpora
Fung and Cheung (2004) give a more fine-grained
description of the types of non-parallel corpora,
which we will briefly summarize. A noisy parallel
corpus has documents which contain many parallel
sentences in roughly the same order. Comparable
corpora contain topic aligned documents which are
not translations of each other. The corpora Fung
and Cheung (2004) examine are quasi-comparable:
they contain bilingual documents which are not
necessarily on the same topic.
Wikipedia is a special case, since the aligned
article pairs may range from being almost com-
pletely parallel (e.g., the Spanish and English en-
tries for ?Antiparticle?) to containing almost no par-
allel sentences (the Spanish and English entries for
?John Calvin?), despite being topic-aligned. It is
best characterized as a mix of noisy parallel and
comparable article pairs. Some Wikipedia authors
will translate articles from another language; others
404
French German Polish Italian Dutch Portuguese Spanish Japanese
496K 488K 384K 380K 357K 323K 311K 252K
Russian Swedish Finnish Chinese Norwegian Volapu?k Catalan Czech
232K 197K 146K 142K 141K 106K 103K 87K
Table 1: Number of aligned bilingual articles in Wikipedia by language (paired with English).
write the content themselves. Furthermore, even ar-
ticles created through translations may later diverge
due to independent edits in either language.
3 Models for Parallel Sentence Extraction
In this section, we will focus on methods for ex-
tracting parallel sentences from aligned, compara-
ble documents. The related problem of automatic
document alignment in news and web corpora has
been explored by a number of researchers, includ-
ing Resnik and Smith (2003), Munteanu and Marcu
(2005), Tillmann and Xu (2009), and Tillmann
(2009). Since our corpus already contains docu-
ment alignments, we sidestep this problem, and will
not discuss further details of this issue. That said,
we believe that our methods will be effective in cor-
pora without document alignments when combined
with one of the aforementioned algorithms.
3.1 Binary Classifiers and Rankers
Much of the previous work involves building a
binary classifier for sentence pairs to determine
whether or not they are parallel (Munteanu and
Marcu, 2005; Tillmann, 2009). The training data
usually comes from a standard parallel corpus.
There is a substantial class imbalance (O(n) pos-
itive examples, and O(n2) negative examples), and
various heuristics are used to mitigate this prob-
lem. Munteanu and Marcu (2005) filter out neg-
ative examples with high length difference or low
word overlap (based on a bilingual dictionary).
We propose an alternative approach: we learn
a ranking model, which, for each sentence in the
source document, selects either a sentence in the
target document that it is parallel to, or ?null?. This
formulation of the problem avoids the class imbal-
ance issue of the binary classifier.
In both the binary classifier approach and the
ranking approach, we use a Maximum Entropy
classifier, following Munteanu and Marcu (2005).
3.2 Sequence Models
In Wikipedia article pairs, it is common for par-
allel sentences to occur in clusters. A global sen-
tence alignment model is able to capture this phe-
nomenon. For both parallel and comparable cor-
pora, global sentence alignments have been used,
though the alignments were monotonic (Gale and
Church, 1991; Moore, 2002; Zhao and Vogel,
2002). Our model is a first order linear chain Condi-
tional Random Field (CRF) (Lafferty et al, 2001).
The set of source and target sentences are observed.
For each source sentence, we have a hidden vari-
able indicating the corresponding target sentence
to which it is aligned (or null). The model is simi-
lar to the discriminative CRF-based word alignment
model of (Blunsom and Cohn, 2006).
3.3 Features
Our features can be grouped into four categories.
Features derived from word alignments
We use a feature set inspired by (Munteanu and
Marcu, 2005), who defined features primarily based
on IBM Model 1 alignments (Brown et al, 1993).
We also use HMM word alignments (Vogel et al,
1996) in both directions (source to target and target
to source), and extract the following features based
on these four alignments:1
1. Log probability of the alignment
2. Number of aligned/unaligned words
3. Longest aligned/unaligned sequence of words
4. Number of words with fertility 1, 2, and 3+
We also define two more features which are in-
dependent of word alignment models. One is a
sentence length feature taken from (Moore, 2002),
1These are all derived from the one best alignment, and
normalized by sentence length.
405
which models the length ratio between the source
and target sentences with a Poisson distribution.
The other feature is the difference in relative doc-
ument position of the two sentences, capturing the
idea that the aligned articles have a similar topic
progression.
The above features are all defined on sentence
pairs, and are included in the binary classifier and
ranking model.
Distortion features
In the sequence model, we use additional dis-
tortion features, which only look at the difference
between the position of the previous and current
aligned sentences. One set of features bins these
distances; another looks at the absolute difference
between the expected position (one after the previ-
ous aligned sentence) and the actual position.
Features derived from Wikipedia markup
Three features are derived from Wikipedia?s
markup. The first is the number of matching links
in the sentence pair. The links are weighted by their
inverse frequency in the document, so a link that
appears often does not contribute much to this fea-
ture?s value. The image feature fires whenever two
sentences are captions of the same image, and the
list feature fires when two sentences are both items
in a list. These last two indicator features fire with
a negative value when the feature matches on one
sentence and not the other.
None of the above features fire on a null align-
ment, in either the ranker or CRF. There is also a
bias feature for these two models, which fires on all
non-null alignments.
Word-level induced lexicon features
A common problem with approaches for paral-
lel sentence classification, which rely heavily on
alignment models trained from unrelated corpora,
is low recall due to unknown words in the candi-
date sentence-pairs. One approach that begins to
address this problem is the use of self-training, as
in (Munteanu and Marcu, 2005). However, a self-
trained sentence pair extraction system is only able
to acquire new lexical items that occur in parallel
sentences. Within Wikipedia, many linked article
pairs do not contain any parallel sentences, yet con-
tain many words and phrases that are good transla-
tions of each other.
In this paper we explore an alternative approach
to lexicon acquisition for use in parallel sentence
extraction. We build a lexicon model using an ap-
proach similar to ones developed for unsupervised
lexicon induction from monolingual or compara-
ble corpora (Rapp, 1999; Koehn and Knight, 2002;
Haghighi et al, 2008). We briefly describe the lex-
icon model and its use in sentence-extraction.
The lexicon model is based on a probabilistic
modelP (wt|ws, T, S) wherewt is a word in the tar-
get language, ws is a word in the source language,
and T and S are linked articles in the target and
source languages, respectively.
We train this model similarly to the sentence-
extraction ranking model, with the difference that
we are aligning word pairs and not sentence pairs.
The model is trained from a small set of annotated
Wikipedia article pairs, where for some words in
the source language we have marked one or more
words as corresponding to the source word (in the
context of the article pair), or have indicated that the
source word does not have a corresponding transla-
tion in the target article. The word-level annotated
articles are disjoint from the sentence-aligned arti-
cles described in Section 4. The following features
are used in the lexicon model:
Translation probability. This is the translation
probability p(wt|ws) from the HMM word align-
ment model trained on the seed parallel data. We
also use the probability in the other direction, as
well as the log-probabilities in the two directions.
Position difference. This is the absolute value of
the difference in relative position of words ws and
wt in the articles S and T .
Orthographic similarity. This is a function of the
edit distance between source and target words. The
edit distance between words written in different al-
phabets is computed by first performing a determin-
istic phonetic translation of the words to a common
alphabet. The translation is inexact and this is a
promising area for improvement. A similar source
of information has been used to create seed lexicons
in (Koehn and Knight, 2002) and as part of the fea-
ture space in (Haghighi et al, 2008).
Context translation probability. This feature
looks at all words occurring next to word ws in the
406
article S and next to wt in the article T in a local
context window (we used one word to the left and
one word to the right), and computes several scor-
ing functions measuring the translation correspon-
dence between the contexts (using the IBM Model
1 trained from seed parallel data). This feature is
similar to distributional similarity measures used in
previous work, with the difference that it is limited
to contexts of words within a linked article pair.
Distributional similarity. This feature corre-
sponds more closely to context similarity measures
used in previous work on lexicon induction. For
each source headword ws, we collect a distribu-
tion over context positions o ? {?2,?1,+1,+2}
and context words vs in those positions based on a
count of times a context word occurred at that off-
set from a headword: P (o, vs|ws) ? weight(o) ?
C(ws, o, vs). Adjacent positions ?1 and +1 have
a weight of 2; other positions have a weight of 1.
Likewise we gather a distribution over target words
and contexts for each target headword P (o, vt|wt).
Using an IBM Model 1 word translation table
P (vt|vs) estimated on the seed parallel corpus,
we estimate a cross-lingual context distribution as
P (o, vt|ws) =
?
vs P (vt|vs) ? P (o, vs|ws). We de-
fine the similarity of a words ws and wt as one mi-
nus the Jensen-Shannon divergence of the distribu-
tions over positions and target words.2
Given this small set of feature functions, we
train the weights of a log-linear ranking model for
P (wt|ws, T, S), based on the word-level annotated
Wikipedia article pairs. After a model is trained,
we generate a new translation table Plex(t|s) which
is defined as Plex(t|s) ?
?
t?T,s?S P (t|s, T, S).
The summation is over occurrences of the source
and target word in linked Wikipedia articles. This
new translation table is used to define another
HMM word-alignment model (together with dis-
tortion probabilities trained from parallel data) for
use in the sentence extraction models. Two copies
of each feature using the HMM word alignment
model are generated: one using the seed data HMM
2We restrict our attention to words with ten or more occur-
rences, since rare words have poorly estimated distributions.
Also we discard the contribution from any context position and
word pair that relates to more than 1,000 distinct source or tar-
get words, since it explodes the computational overhead and
has little impact on the final similarity score.
model, and another using this new HMM model.
The training data for Bulgarian consisted of two
partially annotated Wikipedia article pairs. For
German and Spanish we used the feature weights
of the model trained on Bulgarian, because we did
not have word-level annotated Wikipedia articles.
4 Experiments
4.1 Data
We annotated twenty Wikipedia article pairs for
three language pairs: Spanish-English, Bulgarian-
English, and German-English. Each sentence
in the source language was annotated with pos-
sible parallel sentences in the target language
(the target language was English in all experi-
ments). The pairs were annotated with a quality
level: 1 if the sentences contained some parallel
fragments, 2 if the sentences were mostly paral-
lel with some missing words, and 3 if the sen-
tences appeared to be direct translations. In all
experiments, sentence pairs with quality 2 or 3
were taken as positive examples. The resulting
datasets are available at http://research.microsoft.com/en-
us/people/chrisq/wikidownload.aspx.
For our seed parallel data, we used the Europarl
corpus (Koehn, 2005) for Spanish and German and
the JRC-Aquis corpus for Bulgarian, plus the article
titles for parallel Wikipedia documents, and trans-
lations available from Wiktionary entries.3
4.2 Intrinsic Evaluation
Using 5-fold cross-validation on the 20 document
pairs for each language condition, we compared the
binary classifier, ranker, and CRF models for paral-
lel sentence extraction. To tune for precision/recall,
we used minimum Bayes risk decoding. We define
the loss L(?, ?) of picking target sentence ? when
the correct target sentence is ? as 0 if ? = ?, ?
if ? = NULL and ? 6= NULL, and 1 otherwise.
By modifying the null loss ?, the precision/recall
trade-off can be adjusted. For the CRF model, we
used posterior decoding to make the minimum risk
decision rule tractable. As a summary measure of
the performance of the models at different levels of
recall we use average precision as defined in (Ido
3Wiktionary is an online collaborative dictionary, similar to
Wikipedia.
407
Language Pair Binary Classifier Ranker CRF
Avg Prec R@90 R@80 Avg Prec R@90 R@80 Avg Prec R@90 R@80
English-Bulgarian 75.7 33.9 56.2 76.3 38.8 57.0 80.6 52.9 59.5
English-Spanish 90.4 81.3 87.6 93.4 81.0 84.5 94.7 87.6 90.2
English-German 61.8 9.4 27.5 66.4 25.7 42.4 78.9 52.2 54.7
Table 2: Average precision, recall at 90% precision, and recall at 80% precision for each model in all three language
pairs. In these experiments, the Wikipedia features and lexicon features are omitted.
Setting Ranker CRF
Avg Prec R@90 R@80 Avg Prec R@90 R@80
English-Bulgarian
One Direction 76.3 38.8 57.0 80.6 52.9 59.5
Intersected 78.2 47.9 60.3 79.9 38.8 57.0
Intersected +Wiki 80.8 39.7 68.6 82.1 53.7 62.8
Intersected +Wiki +Lex 89.3 64.4 79.3 90.9 72.0 81.8
English-Spanish
One Direction 93.4 81.0 84.5 94.7 87.6 90.2
Intersected 94.3 82.4 89.0 95.4 88.5 91.8
Intersected +Wiki 94.5 82.4 89.0 95.6 89.2 92.7
Intersected +Wiki +Lex 95.8 87.4 91.1 96.4 90.4 93.7
English-German
One Direction 66.4 25.7 42.4 78.9 52.2 54.7
Intersected 71.9 36.2 43.8 80.9 54.0 67.0
Intersected +Wiki 74.0 38.8 45.3 82.4 56.9 71.0
Intersected +Wiki +Lex 78.7 46.4 59.1 83.9 58.7 68.8
Table 3: Average precision, recall at 90% precision, and recall at 80% precision for the Ranker and CRF in all three
language pairs. ?+Wiki? indicates that Wikipedia features were used, and ?+Lex? means the lexicon features were
used.
et al, 2006). We also report recall at precision of
90 and 80 percent. Table 2 compares the different
models in all three language pairs.
In our next set of experiments, we looked at the
effects of the Wikipedia specific features. Since the
ranker and CRF are asymmetric models, we also
experimented with running the models in both di-
rections and combining their outputs by intersec-
tion. These results are shown in Table 3.
Identifying the agreement between two asym-
metric models is a commonly exploited trick else-
where in machine translation. It is mostly effec-
tive here as well, improving all cases except for
the Bulgarian-English CRF where the regression is
slight. More successful are the Wikipedia features,
which provide an auxiliary signal of potential par-
allelism.
The gains from adding the lexicon-based features
can be dramatic as in the case of Bulgarian (the
CRF model average precision increased by nearly
9 points). The lower gains on Spanish and German
may be due in part to the lack of language-specific
training data. These results are very promising and
motivate further exploration. We also note that this
is perhaps the first successful practical application
of an automatically induced word translation lexi-
con.
4.3 SMT Evaluation
We also present results in the context of a full ma-
chine translation system to evaluate the potential
utility of this data. A standard phrasal SMT sys-
tem (Koehn et al, 2003) serves as our testbed, us-
ing a conventional set of models: phrasal mod-
408
els of source given target and target given source;
lexical weighting models in both directions, lan-
guage model, word count, phrase count, distortion
penalty, and a lexicalized reordering model. Given
that the extracted Wikipedia data takes the standard
form of parallel sentences, it would be easy to ex-
ploit this same data in a number of systems.
For each language pair we explored two training
conditions. The ?Medium? data condition used eas-
ily downloadable corpora: Europarl for German-
English and Spanish-English, and JRC/Acquis for
Bulgarian-English. Additionally we included titles
of all linked Wikipedia articles as parallel sentences
in the medium data condition. The ?Large? data
condition includes all the medium data, and also in-
cludes using a broad range of available sources such
as data scraped from the web (Resnik and Smith,
2003), data from the United Nations, phrase books,
software documentation, and more.
In each condition, we explored the impact of in-
cluding additional parallel sentences automatically
extracted from Wikipedia in the system training
data. For German-English and Spanish-English,
we extracted data with the null loss adjusted to
achieve an estimated precision of 95 percent, and
for English-Bulgarian a precision of 90 percent. Ta-
ble 4 summarizes the characteristics of these data
sets. We were pleasantly surprised at the amount
of parallel sentences extracted from such a var-
ied comparable corpus. Apparently the average
Wikipedia article contains at least a handful of
parallel sentences, suggesting this is a very fertile
ground for training MT systems.
The extracted Wikipedia data is likely to make
the greatest impact on broad domain test sets ? in-
deed, initial experimentation showed little BLEU
gain on in-domain test sets such as Europarl, where
out-of-domain training data is unlikely to provide
appropriate phrasal translations. Therefore, we ex-
perimented with two broad domain test sets.
First, Bing Translator provided a sample of trans-
lation requests along with translations in German-
English and Spanish-English, which acted our stan-
dard development and test set. Unfortunately no
such tagged set was available in Bulgarian-English,
so we held out a portion of the large system?s train-
ing data to use for development and test. In each
language pair, the test set was split into a devel-
opment portion (?Dev A?) used for minimum error
rate training (Och, 2003) and a test set (?Test A?)
used for final evaluation.
Second, we created new test sets in each of
the three language pairs by sampling parallel sen-
tences from held out Wikipedia articles. To
ensure that this test data was clean, we man-
ually filtered the sentence pairs that were not
truly parallel and edited them as necessary to
improve adequacy. We called this ?Wikitest?.
This test set is available at http://research.microsoft.com/en-
us/people/chrisq/wikidownload.aspx. Characteristics of these
test sets are summarized in Table 5.
We evaluated the resulting systems using BLEU-
4 (Papineni et al, 2002); the results are pre-
sented in Table 6. First we note that the extracted
Wikipedia data are very helpful in medium data
conditions, significantly improving translation per-
formance in all conditions. Furthermore we found
that the extracted Wikipedia sentences substantially
improved translation quality on held-out Wikipedia
articles. In every case, training on medium data
plus Wikipedia extracts led to equal or better trans-
lation quality than the large system alone. Further-
more, adding the Wikipedia data to the large data
condition still made substantial improvements.
5 Conclusions
Our first substantial contribution is to demonstrate
that Wikipedia is a useful resource for mining par-
allel data. The sheer volume of extracted parallel
sentences within Wikipedia is a somewhat surpris-
ing result in the light of Wikipedia?s construction.
We are also releasing several valuable resources to
the community to facilitate further research: man-
ually aligned document pairs, and an edited test
set. Hopefully this will encourage research into
Wikipedia as a resource for machine translation.
Secondly, we improve on prior pairwise mod-
els by introducing a ranking approach for sentence
pair extraction. This ranking approach sidesteps the
problematic class imbalance issue, resulting in im-
proved average precision while retaining simplicity
and clarity in the models.
Also by modeling the sentence alignment of the
articles globally, we were able to show a substan-
tial improvement in task accuracy. Furthermore a
409
German English Spanish English Bulgarian English
sentences 924,416 924,416 957,884 957,884 413,514 413,514
Medium types 351,411 320,597 272,139 247,465 115,756 69,002
tokens 11,556,988 11,751,138 18,229,085 17,184,070 10,207,565 10,422,415
sentences 6,693,568 6,693,568 7,727,256 7,727,256 1,459,900 1,459,900
Large types 1,050,832 875,041 1,024,793 952,161 239,076 137,227
tokens 100,456,622 96,035,475 155,626,085 137,559,844 29,741,936 29,889,020
sentences 1,694,595 1,694,595 1,914,978 1,914,978 146,465 146,465
Wiki types 578,371 525,617 569,518 498,765 107,690 74,389
tokens 21,991,377 23,290,765 29,859,332 28,270,223 1,455,458 1,516,231
Table 4: Statistics of the training data size in all three language pairs.
German English Spanish English Bulgarian English
Dev A sentences 2,000 2,000 2,000 2,000 2,000 2,000
tokens 16,367 16,903 24,571 21,493 39,796 40,503
Test A sentences 5,000 5,000 5,000 5,000 2,473 2,473
tokens 42,766 43,929 68,036 60,380 52,370 52,343
Wikitest sentences 500 500 500 500 516 516
tokens 8,235 9,176 10,446 9,701 7,300 7,701
Table 5: Statistics of the test data sets.
Language pair Training data Dev A Test A Wikitest
Spanish-English Medium 32.6 30.5 33.0
Medium+Wiki 36.7 (+4.1) 33.8 (+3.3) 39.1 (+6.1)
Large 39.2 37.4 38.9
Large+Wiki 39.5 (+0.3) 37.3 (-0.1) 41.1 (+2.2)
German-English Medium 28.7 26.6 13.0
Medium+Wiki 31.5 (+2.8) 29.6 (+3.0) 18.2 (+5.2)
Large 35.0 33.7 17.1
Large+Wiki 34.8 (-0.2) 33.9 (+0.2) 20.2 (+3.1)
Bulgarian-English Medium 36.9 26.0 27.8
Medium+Wiki 37.9 (+1.0) 27.6 (+1.6) 37.9 (+10.1)
Large 51.7 49.6 36.0
Large+Wiki 51.7(+0.0) 49.4 (-0.2) 39.5(+3.5)
Table 6: BLEU scores under various training and test conditions. The first column is from minimum error rate training;
the next two columns are on held-out test sets. For training data conditions including extracted Wikipedia sentences,
parenthesized values indicate absolute BLEU difference against the corresponding system without Wikipedia extracts.
small sample of annotated articles is sufficient to
train these global level features, and the learned
classifiers appear very portable across languages. It
is difficult to say whether such improvement will
carry over to other comparable corpora with less
document structure and meta-data. We plan to ad-
dress this question in future work.
Finally, initial investigations have shown that
substantial gains can be achieved by using an in-
duced word-level lexicon in combination with sen-
tence extraction. This helps address modeling word
pairs that are out-of-vocabulary with respect to the
seed parallel lexicon, while avoiding some of the
issues in bootstrapping.
410
References
S. F Adafre and M. de Rijke. 2006. Finding similar
sentences across multiple languages in wikipedia. In
Proceedings of EACL, pages 62?69.
Phil Blunsom and Trevor Cohn. 2006. Discriminative
word alignment with conditional random fields. In
Proceedings of ACL.
P. F Brown, V. J Della Pietra, S. A Della Pietra, and
R. L Mercer. 1993. The mathematics of statistical
machine translation: Parameter estimation. Compu-
tational linguistics, 19(2):263?311.
P. Fung and P. Cheung. 2004. Multi-level bootstrap-
ping for extracting parallel sentences from a quasi-
comparable corpus. In Proceedings of the 20th in-
ternational conference on Computational Linguistics,
page 1051.
W. A Gale and K. W Church. 1991. Identifying word
correspondences in parallel texts. In Proceedings
of the workshop on Speech and Natural Language,
pages 152?157.
Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,
and Dan Klein. 2008. Learning bilingual lexicons
from monolingual corpora. In Proceedings of ACL,
pages 771?779.
Roy Bar-Haim Ido, Ido Dagan, Bill Dolan, Lisa Ferro,
Danilo Giampiccolo, Bernardo Magnini, and Idan
Szpektor. 2006. The second pascal recognising tex-
tual entailment challenge.
P. Koehn and K. Knight. 2002. Learning a translation
lexicon from monolingual corpora. In Proceedings of
the ACL Workshop on Unsupervised Lexical Acquisi-
tion.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of HLT/NAACL, pages 127?133, Edmonton,
Canada, May.
P. Koehn. 2005. Europarl: A parallel corpus for statisti-
cal machine translation. In MT summit, volume 5.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proceed-
ings of the 18th International Conference on Machine
Learning, pages 282?289.
R. C Moore. 2002. Fast and accurate sentence align-
ment of bilingual corpora. Lecture Notes in Computer
Science, 2499:135?144.
D. S Munteanu and D. Marcu. 2005. Improv-
ing machine translation performance by exploiting
non-parallel corpora. Computational Linguistics,
31(4):477?504.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
Annual Meeting of the Association for Computational
Linguistics, pages 160?167, Sapporo, Japan.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proceedings of the
Annual Meeting of the Association for Computational
Linguistics, pages 311?318, Philadelpha, Pennsylva-
nia, USA.
R. Rapp. 1999. Automatic identification of word trans-
lations from unrelated English and German corpora.
In Proceedings of ACL.
P. Resnik and N. A Smith. 2003. The web as a parallel
corpus. Computational Linguistics, 29(3):349?380.
C. Tillmann and J. Xu. 2009. A simple sentence-level
extraction algorithm for comparable data. In Pro-
ceedings of HLT/NAACL, pages 93?96.
C. Tillmann. 2009. A Beam-Search extraction algo-
rithm for comparable data. In Proceedings of ACL,
pages 225?228.
S. Vogel, H. Ney, and C. Tillmann. 1996. HMM-
based word alignment in statistical translation. In
Proceedings of the 16th conference on Computational
linguistics-Volume 2, pages 836?841.
Wikipedia. 2004. Wikipedia, the free encyclopedia.
[Online; accessed 20-November-2009].
B. Zhao and S. Vogel. 2002. Adaptive parallel sentences
mining from web bilingual news collection. In Pro-
ceedings of the 2002 IEEE International Conference
on Data Mining, page 745. IEEE Computer Society.
411
Proceedings of the NAACL-HLT 2012: Demonstration Session, pages 21?24,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
MSR SPLAT, a language analysis toolkit 
 
Chris Quirk, Pallavi Choudhury, Jianfeng 
Gao, Hisami Suzuki, Kristina Toutanova, 
Michael Gamon, Wen-tau Yih, Lucy 
Vanderwende 
Colin Cherry 
Microsoft Research National Research Council Canada 
Redmond, WA 98052 USA 1200 Montreal Road 
 Ottawa, Ontario K1A 0R6 
{chrisq, pallavic, jfgao, 
hisamis, kristout, 
mgamon,scottyih, 
lucyv@microsoft.com} 
colin.cherry@nrccnrc.gc.ca 
 
Abstract 
We describe MSR SPLAT, a toolkit for lan-
guage analysis that allows easy access to the 
linguistic analysis tools produced by the NLP 
group at Microsoft Research. The tools in-
clude both traditional linguistic analysis tools 
such as part-of-speech taggers, constituency 
and dependency parsers, and more recent de-
velopments such as sentiment detection and 
linguistically valid morphology. As we ex-
pand the tools we develop for our own re-
search, the set of tools available in MSR 
SPLAT will be extended. The toolkit is acces-
sible as a web service, which can be used 
from a broad set of programming languages. 
1 Introduction 
The availability of annotated data sets that have 
become community standards, such as the Penn 
TreeBank (Marcus et al, 1993) and PropBank 
(Palmer et al, 2005), has enabled many research 
institutions to build core natural language pro-
cessing components, including part-of-speech tag-
gers, chunkers, and parsers. There remain many 
differences in how these components are built, re-
sulting in slight but noticeable variation in the 
component output. In experimental settings, it has 
proved sometimes difficult to distinguish between 
improvements contributed by a specific component 
feature from improvements due to using a differ-
ently-trained linguistic component, such as tokeni-
zation. The community recognizes this difficulty, 
and shared task organizers are now providing ac-
companying parses and other analyses of the 
shared task data. For instance, the BioNLP shared 
task organizers have provided output from a num-
ber of parsers1, alleviating the need for participat-
ing systems to download and run unfamiliar tools. 
On the other hand, many community members 
provide downloads of NLP tools2 to increase ac-
cessibility and replicability of core components.  
Our toolkit is offered in this same spirit. We 
have created well-tested, efficient linguistic tools 
in the course of our research, using commonly 
available resources such as the PTB and PropBank. 
We also have created some tools that are less 
commonly available in the community, for exam-
ple linguistically valid base forms and semantic 
role analyzers. These components are on par with 
other state of the art systems. 
We hope that sharing these tools will enable 
some researchers to carry out their projects without 
having to re-create or download commonly used 
NLP components, or potentially allow researchers 
to compare our results with those of their own 
tools. The further advantage of designing MSR 
SPLAT as a web service is that we can share new 
components on an on-going basis. 
2 Parsing Functionality 
2.1 Constituency Parsing 
                                                          
1 See www-tsujii.is.s.u-tokyo.ac.jp/GENIA/SharedTask  for 
the description of other resources made available in addition to 
the shared task data. 
2 See, for example, http://nlp.stanford.edu/software; 
http://www.informatics.sussex.ac.uk/research/groups/nlp/rasp; 
http://incubator.apache.org/opennlp 
21
The syntactic parser in MSR SPLAT attempts to 
reconstruct a parse tree according the Penn Tree-
Bank specification (Marcus et al, 1993). This rep-
resentation captures the notion of labeled syntactic 
constituents using a parenthesized representation. 
For instance, the sentence ?Colorless green ideas 
sleep furiously.? could be assigned the following 
parse tree, written in the form of an S expression: 
(TOP (S 
   (NP (JJ Colorless) (JJ green) (NNS ideas)) 
   (VP (VB sleep) (ADVP (RB furiously))) 
   (. .))) 
For instance, this parse tree indicates that ?Color-
less green ideas? is a noun phrase (NP), and ?sleep 
furiously? is a verb phrase (VP). 
Using the Wall Street Journal portion of the 
Penn TreeBank, we estimate a coarse grammar 
over the given grammar symbols. Next, we per-
form a series of refinements to automatically learn 
fine-grained categories that better capture the im-
plicit correlations in the tree using the split-merge 
method of Petrov et al (2006). Each input symbol 
is split into two new symbols, both with a new 
unique symbol label, and the grammar is updated 
to include a copy of each original rule for each 
such refinement, with a small amount of random 
noise added to the probability of each production 
to break ties. We estimate new grammar parame-
ters using an accelerated form of the EM algorithm 
(Salakhutdinov and Roweis, 2003). Then the low-
est 50% of the split symbols (according to their 
estimated contribution to the likelihood of the data) 
are merged back into their original form and the 
parameters are again re-estimated using AEM. We 
found six split-merge iterations produced optimal 
accuracy on the standard development set. 
The best tree for a given input is selected ac-
cording to the max-rule approach (cf. Petrov et al 
2006). Coarse-to-fine parsing with pruning at each 
level helps increase speed; pruning thresholds are 
picked for each level to have minimal impact on 
development set accuracy. However, the initial 
coarse pass still has runtime cubic in the length of 
the sentence. Thus, we limit the search space of the 
coarse parse by closing selected chart cells before 
the parse begins (Roark and Hollingshead, 2008). 
We train a classifier to determine if constituents 
may start or end at each position in the sentence. 
For instance, constituents seldom end at the word 
?the? or begin at a comma. Closing a number of 
chart cells can substantially improve runtime with 
minimal impact on accuracy. 
2.2 Dependency Parsing 
The dependency parses produced by MSR SPLAT 
are unlabeled, directed arcs indicating the syntactic 
governor of each word. 
These dependency trees are computed from the 
output of the constituency parser. First, the head of 
each non-terminal is computed according to a set 
of rules (Collins, 1999). Then, the tree is flattened 
into maximal projections of heads. Finally, we in-
troduce an arc from a parent word p to a child 
word c if the non-terminal headed by p is a parent 
of the non-terminal headed by c. 
2.3 Semantic Role Labeling 
The Semantic Role Labeling component of MSR 
SPLAT labels the semantic roles of verbs accord-
ing to the PropBank specification (Palmer et al, 
2005). The semantic roles represent a level of 
broad-coverage shallow semantic analysis which 
goes beyond syntax, but does not handle phenome-
na like co-reference and quantification.  
For example, in the two sentences ?John broke 
the window? and ?The window broke?, the phrase 
the window will be marked with a THEME label. 
Note that the syntactic role of the phrase in the two 
sentences is different but the semantic role is the 
same. The actual labeling scheme makes use of 
numbered argument labels, like ARG0, ARG1, ?, 
ARG5 for core arguments, and labels like ARGM-
TMP,ARGM-LOC, etc. for adjunct-like argu-
ments. The meaning of the numbered arguments is 
verb-specific, with ARG0 typically representing an 
agent-like role, and ARG1 a patient-like role. 
This implementation of an SRL system follows 
the approach described in (Xue and Palmer, 04), 
and includes two log-linear models for argument 
identification and classification. A single syntax 
tree generated by the MSR SPLAT split-merge 
parser is used as input. Non-overlapping arguments 
are derived using the dynamic programming algo-
rithm by Toutanova et al (2008).  
3 Other Language Analysis Functionality 
3.1 Sentence Boundary / Tokenization 
22
This analyzer identifies sentence boundaries and 
breaks the input into tokens. Both are represented 
as offsets of character ranges. Each token has both 
a raw form from the string and a normalized form 
in the PTB specification, e.g., open and close pa-
rentheses are replaced by -LRB- and -RRB-, re-
spectively, to remove ambiguity with parentheses 
indicating syntactic structure. A finite state ma-
chine using simple rules and abbreviations detects 
sentence boundaries with high accuracy, and a set 
of regular expressions tokenize the input. 
3.2 Stemming / Lemmatization 
We provide three types of stemming: Porter stem-
ming, inflectional morphology and derivational 
morphology. 
3.2.1 Stems  
The stemmer analyzer indicates a stem form for 
each input token, using the standard Porter stem-
ming algorithm (Porter, 1980). These forms are 
known to be useful in applications such as cluster-
ing, as the algorithm assigns the same form ?dai? 
to ?daily? and ?day?, but as these forms are not 
citation forms of these words, presentation to end 
users is known to be problematic. 
3.2.2 Lemmas 
The lemma analyzer uses inflectional morphology 
to indicate the dictionary lookup form of the word. 
For example, the lemma of ?daily? will be ?daily?, 
while the lemma of ?children? will be ?child?. We 
have mined the lemma form of input tokens using 
a broad-coverage grammar NLPwin (Heidorn, 
2000) over very large corpora. 
3.2.3 Bases  
The base analyzer uses derivational morphology to 
indicate the dictionary lookup form of the word; as 
there can be more than one derivation for a given 
word, the base type returns a list of forms. For ex-
ample, the base form of ?daily? will be ?day?, 
while the base form of ?additional? will be ?addi-
tion? and ?add?. We have generated a static list of 
base forms of tokens using a broad-coverage 
grammar NLPwin (Heidorn, 2000) over very large 
corpora. If the token form has not been observed in 
those corpora, we will not return a base form. 
3.3 POS tagging 
We train a maximum entropy Markov Model on 
part-of-speech tags from the Penn TreeBank. This 
optimized implementation has very high accuracy 
(over 96% on the test set) and yet can tag tens of 
thousands of words per second. 
3.4 Chunking 
The chunker (Gao et al, 2001) is based on a Cas-
caded Markov Model, and is trained on the Penn 
TreeBank. With state-of-the-art chunking accuracy 
as evaluated on the benchmark dataset, the chunker 
is also robust and efficient, and has been used to 
process very large corpora of web documents. 
4 The Flexibility of a Web Service 
By making the MSR SPLAT toolkit available as a 
web service, we can provide access to new tools, 
e.g. sentiment analysis. We are in the process of 
building out the tools to provide language analysis 
for languages other than English. One step in this 
direction is a tool for transliterating between Eng-
lish and Katakana words. Following Cherry and 
Suzuki (2009), the toolkit currently outputs the 10-
best transliteration candidates with probabilities for 
both directions.  
Another included service is the Triples analyz-
er, which returns the head of the subject, the verb, 
and the head of the object, whenever such a triple 
is encountered. We found this functionality to be 
useful as we were exploring features for our sys-
tem submitted to the BioNLP shared task. 
5 Programmatic Access 
5.1 Web service reference 
We have designed a web service that accepts a 
batch of text and applies a series of analysis tools 
to that text, returning a bag of analyses. This main 
web service call, named ?Analyze?, requires four 
parameters: the language of the text (such as ?en? 
for English), the raw text to be analyzed, the set of 
analyzers to apply, and an access key to monitor 
and, if necessary, constrain usage. It returns a list 
of analyses, one from each requested analyzer, in a 
23
simple JSON (JavaScript Object Notation) format 
easy to parse in many programming languages. 
In addition, there is a web service call ?Lan-
guages? that enumerates the list of available lan-
guages, and ?Analyzers? to discover the set of 
analyzers available in a given language.  
5.2 Data Formats 
We use a relatively standard set of data representa-
tions for each component. Parse trees are returned 
as S expressions, part-of-speech tags are returned 
as lists, dependency trees are returned as lists of 
parent indices, and so on. The website contains an 
authoritative description of each analysis format. 
5.3 Speed 
Speed of analysis is heavily dependent on the 
component involved. Analyzers for sentence sepa-
ration, tokenization, and part-of-speech tagging 
process thousands of sentences per second; our 
fastest constituency parser handles tens of sentenc-
es per second. Where possible, the user is encour-
aged to send moderate sized requests (perhaps a 
paragraph at a time) to minimize the impact of 
network latency. 
6 Conclusion 
We hope that others will find the tools that we 
have made available as useful as we have. We en-
courage people to send us their feedback so that we 
can improve our tools and increase collaboration in 
the community. 
7 Script Outline 
The interactive UI (Figure 1) allows an arbitrary 
sentence to be entered and the desired levels of 
analysis to be selected as output. As there exist 
other such toolkits, the demonstration is primarily 
aimed at allowing participants to assess the quality, 
utility and speed of the MSR SPLAT tools. 
http://research.microsoft.com/en-us/projects/msrsplat/ 
References  
Colin Cherry and Hisami Suzuki. 2009. Discriminative sub-
string decoding for transliteration. In Proceedings of 
EMNLP. 
Michael Collins. 1999. Head-driven statistical models for 
natural language parsing. PhD Dissertation, University of 
Pennsylvania. 
Jianfeng Gao, Jian-Yun Nie, Jian Zhang, Endong Xun, Ming 
Zhou and Chang-Ning Huang. 2001. Improving query 
translation for CLIR using statistical Models. In Proceed-
ings of SIGIR. 
George Heidorn. 2000. Intelligent writing assistance. In R. 
Dale, H. Moisl and H. Somers (eds.), A Handbook of Natu-
ral Language Processing: Techniques and Applications for 
the Processing of Text. New York: Marcel Dekker. 
Mitchell Marcus, Beatrice Santorini, and Mary Ann 
Marcinkiewicz. 1993. Building a Large Annotated Corpus 
of English: The Penn Treebank. Computational Linguistics 
19(2): 313-330. 
Martha Palmer, Dan Gildea, Paul Kingsbury. 2005. The Prop-
osition Bank: An Annotated Corpus of Semantic Roles. 
Computational Linguistics, 31(1): 71-105 
Martin Porter. 1980. An algorithm for suffix stripping. Pro-
gram, 14(3): 130-137. 
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. 
2006. Learning Accurate, Compact, and Interpretable Tree 
Annotation. In Proceedings of ACL. 
Brian Roark and Kristy Hollingshead. 2008. Classifying chart 
cells for quadratic complexity context-free inference. In 
Proceedings of COLING. 
Ruslan Salakhutdinov and Sam Roweis. 2003. Adaptive Over-
relaxed Bound Optimization Methods. In Proceedings of 
ICML. 
Kristina Toutanova, Aria Haghighi, and Christopher D. Man-
ning. 2008. A global joint model for semantic role labeling, 
Computational Linguistics, 34(2): 161-191. 
Nianwen Xue and Martha Palmer. 2004. Calibrating Features 
for Semantic Role Labeling. In Proceedings of EMNLP. 
Munmun de Choudhury, Scott Counts, Michael Gamon. Not 
All Moods are Created Equal! Exploring Human Emotional 
States in Social Media. Accepted for presentation in 
ICWSM 2012 
Munmun de Choudhury, Scott Counts, Michael Gamon. Hap-
py, Nervous, Surprised? Classification of Human Affective 
States in Social Media. Accepted for presentation (short 
paper) in ICWSM 2012 
 
 
Figure 1. Screenshot of the MSR SPLAT interactive UI 
showing selected functionalities which can be toggled 
on and off. This is the interface that we propose to 
demo at NAACL. 
 
 
24
Proceedings of NAACL-HLT 2013, pages 12?21,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Beyond Left-to-Right: Multiple Decomposition Structures for SMT
Hui Zhang?
USC/ISI
Los Angeles, CA 90089
hzhang@isi.edu
Kristina Toutanova
Microsoft Research
Redmond, WA 98502
kristout@microsoft.com
Chris Quirk
Microsoft Research
Redmond, WA 98502
chrisq@microsoft.com
Jianfeng Gao
Microsoft Research
Redmond, WA 98502
jfgao@microsoft.com
Abstract
Standard phrase-based translation models do
not explicitly model context dependence be-
tween translation units. As a result, they rely
on large phrase pairs and target language mod-
els to recover contextual effects in translation.
In this work, we explore n-gram models over
Minimal Translation Units (MTUs) to explic-
itly capture contextual dependencies across
phrase boundaries in the channel model. As
there is no single best direction in which con-
textual information should flow, we explore
multiple decomposition structures as well as
dynamic bidirectional decomposition. The
resulting models are evaluated in an intrin-
sic task of lexical selection for MT as well
as a full MT system, through n-best rerank-
ing. These experiments demonstrate that ad-
ditional contextual modeling does indeed ben-
efit a phrase-based system and that the direc-
tion of conditioning is important. Integrating
multiple conditioning orders provides consis-
tent benefit, and the most important directions
differ by language pair.
1 Introduction
The translation procedure of a classical phrase-
based translation model (Koehn et al, 2003) first di-
vides the input sentence into a sequence of phrases,
translates each phrase, explores reorderings of these
translations, and then scores the resulting candi-
dates with a linear combination of models. Conven-
tional models include phrase-based channel models
that effectively model each phrase as a large uni-
gram, reordering models, and target language mod-
els. Of these models, only the target language model
?This research was conducted during the author?s internship
at Microsoft Research
(and, to some weak extent, the lexicalized reordering
model) captures some lexical dependencies that span
phrase boundaries, though it is not able to model in-
formation from the source side. Larger phrases cap-
ture more contextual dependencies within a phrase,
but individual phrases are still translated almost in-
dependently.
To address this limitation, several researchers
have proposed bilingual n-gram Markov models
(Marino et al, 2006) to capture contextual depen-
dencies between phrase pairs. Much of their work
is limited by the requirement ?that the source and
target side of a tuple of words are synchronized, i.e.
that they occur in the same order in their respective
languages? (Crego and Yvon, 2010).
For language pairs with significant typological di-
vergences, such as Chinese-English, it is quite dif-
ficult to extract a synchronized sequence of units;
in the limit, the smallest synchronized unit may be
the whole sentence. Other approaches explore incor-
poration into syntax-based MT systems or replacing
the phrasal translation system altogether.
We investigate the addition of MTUs to a phrasal
translation system to improve modeling of con-
text and to provide more robust estimation of long
phrases. However, in a phrase-based system there
is no single synchronized traversal order; instead,
we may consider the translation units in many pos-
sible orders: left-to-right or right-to-left according
to either the source or the target are natural choices.
Alternatively we consider translating a particularly
unambiguous unit in the middle of the sentence
and building outwards from there. We investigate
both consistent and dynamic decomposition orders
in several language pairs, looking at distinct orders
in isolation and combination.
12
2 Related work
Marino et al (2006) proposed a translation model
using a Markov model of bilingual n-grams, demon-
strating state-of-the-art performance compared to
conventional phrase-based models. Crego and
Yvon (2010) further explored factorized n-gram ap-
proaches, though both models considered rather
large n-grams; this paper focuses on small units with
asynchronous orders in source and target. Durrani
et al (2011) developed a joint model that captures
translation of contiguous and gapped units as well as
reordering. Two prior approaches explored similar
models in syntax based systems. MTUs have been
used in dependency translation models (Quirk and
Menezes, 2006) to augment syntax directed trans-
lation systems. Likewise in target language syntax
systems, one can consider Markov models over min-
imal rules, where the translation probability of each
rule is adjusted to include context information from
parent rules (Vaswani et al, 2011).
Most prior work tends to replace the existing
probabilities rather than augmenting them. We be-
lieve that Markov rules provide an additional sig-
nal but are not a replacement. Their distributions
should be more informative than the so-called ?lex-
ical weighting? models, and less sparse than rela-
tive frequency estimates, though potentially not as
effective for truly non-compositional units. There-
fore, we explore the inclusion of all such informa-
tion. Also, unlike prior work, we explore combina-
tions of multiple decomposition orders, as well as
dynamic decompositions. The most useful context
for translation differs by language pair, an important
finding when working with many language pairs.
We build upon a standard phrase-based approach
(Koehn et al, 2003). This acts as a proposal dis-
tribution for translations; the MTU Markov models
provide additional signal as to which translations are
correct.
3 MTU n-gram Markov models
We begin by defining Minimal Translation Units
(MTUs) and describing how to identify them in
word-aligned text. Next we define n-gram Markov
models over MTUs, which requires us to define
traversal orders over MTUs.
?Yu ??ZuoTian ??JuXing
held the meeting
??HuiTan
yesterdaynull
null
M1 M2 M3 M5M4
M1: Yu => null                               M2: ZuoTian => yesterdayM3:                                  JuXing => heldM4:                                       null => theM5:                                 HuiTan => meeting
?Yu ??ZuoTian ??JuXing
? ? ?
??HuiTan
??
null
Figure 1: Word alignment and minimum translation units.
3.1 Definition of an MTU
Informally, the notion of a minimal translation unit
is simple: it is a translation rule that cannot be
broken down any further without violating the con-
straints of the rules. We restrict ourselves to contigu-
ous MTUs. They are similar to small phrase pairs,
though unlike phrase pairs we allow MTUs to have
either an empty source or empty target side, thereby
allowing insertion and deletion phrases. Conven-
tional phrase pairs may be viewed as compositions
of these MTUs up to a given size limit.
Consider a word-aligned sentence pair consisting
of a sequence of source words s = s1 . . . sm, a se-
quence of target words t = t1 . . . tn, and a word align-
ment relation between the source and target words
? ? {1..m} ? {1..n}. A translation unit is a sequence
of source words si..s j and a sequence of target words
tk..tl (one of which may be empty) such that for all
aligned pairs i? ? k?, we have i ? i? ? j if and only
if k ? k? ? l. This definition, nearly identical to
that of a phrase pair (Koehn et al, 2003), relaxes the
constraint that one aligned word must be present.
A set of translation units is a partition of the sen-
tence pair if each source and target word is covered
exactly once. Minimal translation units is the par-
tition with the smallest average unit size, or, equiv-
alently, the largest number of units. For example,
Figure 1 shows a word-aligned sentence pair and its
corresponding set of MTUs. We extract these min-
imal translation units with an algorithm similar to
that of phrase extraction.
We train n-gram Markov models only over min-
13
imal rules for two reasons. First, the segmentation
of the sentence pair is not unique under composed
rules, which makes probability estimation compli-
cated. Second, some phrase pairs are very large,
which results in sparse data issues and compromises
the model quality. Therefore, training an n-gram
model over minimal translation units turns out to
be a simple and clean choice: the resulting segmen-
tation is unique, and the distribution is smooth. If
we want to capture more context, we can simply in-
crease the order of the Markov model.
Such Markov models address issues in large
phrase-based translation approaches. Where stan-
dard phrase-based models rely upon large unigrams
to capture contextual information, n-grams of mini-
mal translation units allow a robust contextual model
that is less constrained by segmentation.
3.2 MTU enumeration orders
When defining a joint probability distribution over
MTUs of an aligned sentence pair, it is necessary
to define a decomposition, or generation order for
the sentence pair. For a single sequence in lan-
guage modeling or synchronized sequences in chan-
nel modeling, the default enumeration order has
been left-to-right.
Different decomposition orders have been used
in part-of-speech tagging and named entity recog-
nition (Tsuruoka and Tsujii, 2005). Intuitively, in-
formation from the left or right could be more use-
ful for particular disambiguation choices. Our re-
search on different decomposition orders was moti-
vated by this work. When applying such ideas to
machine translation, there are additional challenges
and opportunities. The task exhibits much more am-
biguity ? the number of possible MTUs is in the
millions. An opportunity arises from the reordering
phenomenon in machine translation: while in POS
tagging the natural decomposition orders to study
are only left-to-right and right-to-left, in machine
translation we can further distinguish source and tar-
get sentence orders.
We first define the source left-to-right and the tar-
get left-to-right orders of the aligned sets of MTUs.
The definition is straightforward when there are no
inserted or deleted word. To place the nulls corre-
sponding to such word we use the following defi-
nition: the source position of the null for a target
inserted word is just after the position of the last
source word aligned to the closest preceding non-
null aligned target word. The target position for a
null corresponding to a source deleted MTU is de-
fined analogously. In Figure 1 we define the posi-
tion of M4 to be right after M3 (because ?the? is
after ?held? in left-to-right order on the target side).
The complete MTU sequence in source left-to-
right order is M1-M2-M3-M4-M5. The sequence
in target left-to-right order is M3-M4-M5-M1-M2.
This illustrates that decomposition structure may
differ significantly depending on which language is
used to define the enumeration order.
Once a sentence pair is represented as a sequence
of MTUs, we can define the probability of the
sentence pair using a conventional n-gram Markov
model (MM) over MTUs. For example, the 3-gram
MM probability of the sentence pair in Figure 1
under the source left-to-right order is as follows:
P(M1)?P(M2|M1)?P(M3|M1,M2)?P(M4|M2,M3)?
P(M5|M3,M4).
Different decomposition orders use different con-
text for disambiguation and it is not clear apriori
which would perform best. We compare all four
decomposition orders (source order left-to-right and
right-to-left, and target order left-to-right and right-
to-left). Although the independence assumptions of
left-to-right and right-to-left are the same, the result-
ing models may be different due to smoothing.
In addition to studying these four basic decompo-
sition orders, we report performance of two cyclic
orders: cyclic in source or target sentence order.
These models are inspired by the cyclic depen-
dency network model proposed for POS tagging
(Toutanova et al, 2003) and also used as a baseline
in previous work on dynamic decomposition orders
(Tsuruoka and Tsujii, 2005). 1
The probability according to the cyclic orders is
defined by conditioning each MTU on both its left
and right neighbor MTUs. For example, the prob-
ability of the sentence pair in Figure 1 under the
source cyclic order, using a 3-gram model is defined
as: P(M1|M2) ? P(M2|M1,M3) ? P(M3|M2,M4) ?
P(M4|M3,M5) ? P(M5|M4).
All n-gram Markov models over MTUs are esti-
1The correct application of such models requires sampling
to find the highest scoring sequence, but we apply the max prod-
uct approximation as done in previous work.
14
mated using Kneser-Ney smoothing. Each MTU is
treated as an atomic unit in the vocabulary of the
n-gram model. Counts of all n-grams are obtained
from the parallel MT training data, using different
MTU enumeration orders.
Note that if we use a target-order decomposition,
the model provides a distribution over target sen-
tences and the corresponding source sides of MTUs,
albeit unordered. Likewise source order based mod-
els provide distributions over source sentences and
unordered target sides of MTUs. We attempted to
introduce reordering models to predict an order over
the resulting MTU sequences using approaches sim-
ilar to reordering models for phrases. Although
these models produced gains in some language pairs
when used without translation MTU MMs, there
were no additional gains over a model using mul-
tiple translation MTU MMs.
4 Lexical selection
We perform an empirical evaluation of different
MTU decomposition orders on a simplified machine
translation task: lexical selection. In this task we
assume that the source sentence segmentation into
minimal translation units is given and that the or-
der of the corresponding target sides of the minimal
translation units is also given. The problem is to
predict the target sides of the MTUs, called target
MTUs for brevity (see Figure 2). The lexical selec-
tion task is thus similar to sequence tagging tasks
like part-of-speech tagging, though much more dif-
ficult: the predicted variables are sequences of target
language words with millions of possible outcomes.
?Yu ??ZuoTian ??JuXing
held the meet ng
??HuiTan
yesterdaynull
null
M1 M2 M3 M5M4
M1: Yu => null                               M2: ZuoTian => yesterdayM3:                                  JuXing => heldM4:                                       null => theM5:                                 HuiTan => meeting
?Yu ??ZuoTian ??JuXing
? ? ?
??HuiTan
??
null
Figure 2: Lexical selection.
We use this constrained MT setting to evaluate the
performance of models using different MTU decom-
position orders and models using combinations of
decomposition orders. The simplified setting allows
controlled experimentation while lessening the im-
pact of complicating factors in a full machine trans-
lation setting (search error, reordering limits, phrase
table pruning, interaction with other models).
To perform the tagging task, we use trigram MTU
models. The four basic decomposition orders for
MTU Markov models we use are left-to-right in tar-
get sentence order, right-to-left in target sentence or-
der, left-to-right in source sentence order, and right-
to-left in source sentence order. We also consider
cyclic orders in source and target.
Regardless of the decomposition order used, we
perform decoding using a beam search decoder, sim-
ilar to ones used in phrase-based machine transla-
tion. The decoder builds target hypotheses in left-
to-right target sentence order. At each step, it fills in
the translation of the next source MTU, in the con-
text of the already predicted MTUs to its left. The
top scoring complete hypotheses covering the first m
MTUs are maintained in a beam. When scoring with
a target left-to-right MTU Markov model (L2RT),
we can score each partial hypothesis exactly at each
step. When scoring using a R2LT model or a source
order model, we use lower-order approximations to
the trigram MTU Markov model scores as future
scores, since not all needed context is available for a
hypothesis at the time of construction. As additional
context becomes available, the exact score can be
computed. 2
4.1 Basic decomposition order combinations
We first introduce two methods of combining differ-
ent decomposition orders: product and system com-
bination.
The product method arises naturally in the ma-
chine translation setting, where probabilities from
different models are multiplied together and further
weighted to form the log-linear model for machine
translation (Och and Ney, 2002). We define a similar
scoring function using a set of MTU Markov models
MM1, ...,MMk for a hypothesis h as follows:
Score(h) = ?1logPMM1(h) + ... + ?klogPMMk (h)
2We apply hypothesis recombination, which can merge hy-
potheses that are indistinguishable with respect to future contin-
uations. This is similar to recombination in a standard-phrase
based decoder with the difference that it is not always the last
two target MTUs that define the context needed by future ex-
tensions.
15
The weights ? of different models are trained on a
development set using MER training to maximize
the BLEU score of the resulting model. Note that
this method of model combination was not consid-
ered in any of the previous works comparing differ-
ent decompositions.
The system combination method is motivated
by prior work in machine translation which com-
bined left-to-right and right-to-left machine trans-
lation systems (Finch and Sumita, 2009). Simi-
larly, we perform sentence-level system combina-
tion between systems using different MTU Markov
models to come up with most likely translations.
If we have k systems guessing hypotheses based
on MM1, . . . ,MMk respectively, we generate 1000-
best lists from each system, resulting in a pool of
up to 1000k possible distinct translations. Each of
the candidate hypotheses from MMi is scored with
its Markov model log-probability logPMMi(h). We
compute normalized probabilities for each system?s
n-best by exponentiating and normalizing: Pi(h) ?
PMMi(h). If a hypothesis h is not in system i?s n-
best list, we assume its probability is zero according
to that system. The final scoring function for each
hypothesis in the combined list of candidates is:
Score(h) = ?1P1(h) + ... + ?kPk(h)
The weights ? for the combination are tuned using
MERT as for the product model.
4.2 Dynamic decomposition orders
A more complex combination method chooses the
best possible decomposition order for each transla-
tion dynamically, using a set of constraints to de-
fine the possible decomposition orders, and a set of
features to score the candidate decompositions. We
term this method dynamic combination. The score
of each translation is defined as its score according
to the highest-scoring decomposition order for that
translation.
This method is very similar to the bidirectional
tagging approach of Tsuruoka and Tsujii (2005).
For this approach we only explored combinations of
target language orders (L2RT, CycT, and R2LT). If
source language orders were included, the complex-
ity of decoding would increase substantially.
Figure 3 shows two possible decompositions for
a short MTU sequence. The structures displayed are
1
 1
2
1  2| 1
3
2  3| 2,	 1
4
2  4| 3,	 2
 1  2| 1,	 3 1  3| 4  4
Figure 3: Different decompositions.
directed graphical models. They define the set of
parents (context) used to predict each target MTU.
The decomposition structures we consider are lim-
ited to acyclic graphs where each node can have one
of the following parent configurations: no parents
(C = 0 in the Figure), one left parent (C = 1L),
one right parent (C = 1R), one left and one right
parent (C = LR), two left parents (C = 2L), and
two right parents (C = 2R). If all nodes have two
left parents, we recover the left-to-right decomposi-
tion order, and if all nodes have two right parents,
the right-to-left decomposition order. A mixture of
parent configurations defines a mixed, dynamic de-
composition order. The decomposition order chosen
varies from translation to translation.
A directed graphical model defines the probability
of an assignment of MTUs to the variable nodes as a
product of local probabilities of MTUs given their
parents. Here we extend this definition to scores
of assignments by using a linear model with con-
figuration features and log-probability features. The
configuration features are indicators of which par-
ent configuration is active at a node and the settings
of these features for the decompositions in Figure
3 are shown as assignments to the C variables. The
log-probability feature values are obtained by query-
ing the appropriate n-gram model: L2RT, CycT, or
R2LT. For a node with one or two left parents, the
log-probability is computed according to the L2RT
model. For a node with one or two right parents, the
R2LT model is queried. The CycT model is used for
nodes with one left and one right parent.
To find the best translation of a sentence the
model now searches over hidden decomposition or-
16
ders in addition to assignments to target MTUs. The
final score of a translation and decomposition is a
linear combination of the two types of feature values
? model log-probabilities and configuration types.
There is one feature weight for each parent con-
figuration (six configuration weights) and one fea-
ture weight for each component model (three model
weights). The final score of the second decomposi-
tion and assignment in Figure 3 is:
Score(h)
= 2 ? wC0 + wCLR + wC1R
+ wL2RlogPLR(m1) + wCyclogPCyc(m2|m1,m3)
+ wR2LlogPRL(m3|m4) + wL2RlogPLR(m4)
There are two main differences between our ap-
proach and that of Tsuruoka and Tsujii (2005): we
perform beam search with hypothesis recombination
instead of exact decoding (due to the larger size of
the hypothesis set), and we use parameters to be
able to globally weight the probabilities from dif-
ferent models and to develop preferences for using
certain types of decompositions. For example, the
model can learn to prefer right-to-left decomposi-
tions for one language pair, and left-to-right decom-
positions for another. An additional difference from
prior work is the definition of the possible decompo-
sition orders that are searched over.
Compared to the structures allowed in (Tsuruoka
and Tsujii, 2005) for a trigram baseline model, our
allowed structures are a subset; in (Tsuruoka and
Tsujii, 2005) there are sixteen possible parent con-
figurations (up to two left and two right parents),
whereas we allow only six. We train and use only
three n-gram Markov models to assign probabilities:
L2RT, R2LT, and CycT, whereas the prior work used
sixteen models. One could potentially see additional
gains from considering a larger space of structures
but the training time and runtime memory require-
ments might become prohibitive for the machine
translation task.
Because of the maximization over decomposition
structures, the score of a translation is not a simple
linear function of the features, but rather a maximum
over linear functions. The score of a translation for
a fixed decomposition is a linear function of the fea-
tures, but the score of a translation is a maximum of
linear functions (over decompositions). Therefore,
if we define hypotheses as just containing transla-
tions, MERT training does not work directly for op-
timizing the weights of the dynamic combination
method. 3 We used a combination of approaches;
we did MERT training followed by local simplex-
method search starting from three starting points:
the MERT solution, a weight vector that strongly
prefers left-to-right decompositions, and a weight-
vector that strongly prefers right-to-left decomposi-
tions. In the Experiments section, we report results
for the weights that achieved the best development
set performance.
5 N-best reranking
To evaluate the impact of these models in a full MT
system, we investigate n-best reranking. We use a
phrase-based MT system to output 1000-best can-
didate translations. For each candidate translation,
we have access to the phrase pairs it used as well as
the alignments inside each phrase pair. Thus, each
source sentence and its candidate translation form a
word-aligned parallel sentence pair. We can extract
MTU sequences from this sentence pair and com-
pute its probability according to MTU Markov mod-
els. These MTU MM log-probabilities are appended
to the original MT features and used to rerank the
1000-best list. The weight vectors for systems using
the original features along with one or more MTU
Markov model log-probabilities are trained on the
development set using MERT.
6 Experiments
We report experimental results on the lexical selec-
tion task and the reranking task on three language
pairs. The datasets used for the different languages
are described in detail in Section 6.2.
6.1 Lexical selection experiments
The data used for the lexical selection experiments
consists of the training portion of the datasets used
for MT. These training sets are split into three sec-
tions: lex-train, for training MTU Markov models
and extracting possible translations for each source
3If we include the decompositions in the hypotheses we
could use MERT but then the n-best lists used for training might
not contain much variety in terms of translation options. This is
an interesting direction for future research.
17
Model Chs-En Deu-En En-Bgr
Dev Test Dev Test Dev Test
Baseline 06.45 06.30 11.60 10.98 15.09 14.40
Oracle 69.79 70.78 72.28 75.39 85.15 84.32
L2RT 24.02 25.09 28.69 28.70 49.86 46.45
R2LT 23.79 24.91 30.14 30.14* 49.22 46.58
CycT 18.59 20.33 25.91 26.83 41.30 38.85
L2RS 25.81 27.89* 25.52 25.10 45.69 43.98
R2LS 26.48 27.96* 26.03 26.30 47.36 43.91
CycS 21.62 23.38 22.68 23.58 39.11 36.44
Table 1: Lexical selection results for individual MTU
Markov models.
MTU, lex-dev for tuning combination weights for
systems using several MTU MMs, and lex-test, for
final evaluation results. The possible translations for
each source MTU are defined as the most frequent
100 translations seen in lex-train. The lex-dev sets
contain 200 sentence pairs each and the lex-test sets
contains 1000 sentence pairs each. These develop-
ment and test sets consist of equally spaced sen-
tences taken from the full MT training sets.
We start by reporting BLEU scores of the six in-
dividual MTU MMs on the three language pairs in
Table 1. The baseline predicts the most frequent tar-
get MTU for each source MTU (unigram MM not
using context). The oracle looks at the correct trans-
lation and always chooses the correct target MTU if
it is in the vocabulary of available MTUs.
We can see that there is a large difference between
the baseline and oracle performance, underscoring
the importance of modeling context for accurate pre-
diction. The best decomposition order varies from
language to language: right-to-left in source order is
best for Chinese-English, right-to-left in target order
is best for German-English and left-to-right or right-
to-left in target order are best in English-Bulgarian.
We computed statistical significance tests, testing
the difference between the L2RT model (the stan-
dard in prior work) and models achieving higher test
set performance. The models that are significantly
better at significance ? < 0.01 are marked with a
star in the table. We used a paired bootstrap test with
10,000 trials (Koehn, 2004).
Next we evaluate the methods for combining de-
composition orders introduced in Sections 4.1 and
4.2. The results are reported in Table 2. The up-
per part of the table focuses on combining different
Model Chs-En Deu-En En-Bgr
Dev Test Dev Test Dev Test
Baseline-1 24.04 25.09 30.14 30.14 49.86 46.45
TgtProduct 25.27 25.84* 30.47 30.49 51.04 47.27*
TgtSysComb 24.49 25.27 30.20 30.15 50.46 46.31
TgtDynamic 24.07 25.10 30.60 30.41 49.99 46.52
Baseline-2 26.48 27.96 30.14 30.14 49.86 46.45
AllProduct 28.68 29.59* 31.54 31.36* 51.50 48.10*
AllSyscomb 27.02 28.30 30.20 30.17 50.90 46.53
Table 2: Lexical selection results for combinations of
MTU Markov models.
target-order decompositions. The lower part of the
table looks at combining all six decomposition or-
ders. The baseline for the target order combinations,
Baseline-1, is the best single target MTU Markov
model from Table 1. Baseline-2 in the lower part
of the table is the best individual model out of all
six. We can see that the product models TgtProduct
(a product of the three target-order MTU MMs) and
AllProduct (a product of all six MTU MMs) are con-
sistently best. The dynamic decomposition models
TgtDynamic achieve slight but not significant gains
over the baseline. The combination models that are
statistically significantly better than corresponding
baselines (? < 0.01) are marked with a star.
Our takeaway from these experiments is that mul-
tiple decomposition orders are good, and thus taking
a product (which encourages agreement among the
models) is a good choice for this task. The dynamic
decomposition method shows some promise, but it
does not outperform the simpler product approach.
Perhaps a lager space of decompositions would
achieve better results, especially given a larger pa-
rameter set to trade off decompositions and better
tuning for those parameters.
6.2 Datasets and reranking settings
For Chinese-English, the training corpus consists
of 1 million sentence pairs from the FBIS and
HongKong portions of the LDC data for the NIST
MT evaluation. We used the union of the NIST
2002 and 2003 test sets as the development set and
the NIST 2005 test set as our test set. The baseline
phrasal system uses a 5-gram language model with
modified Kneser-Ney smoothing (Kenser and Ney,
1995), trained on the Xinhua portion of the English
Gigaword corpus (238M English words).
For German-English we used the dataset from
18
Language Training Dev Test
Chs-En 1 Mln NIST02+03 NIST05
Deu-En 751 K WMT06dev WMT06test
En-Bgr 4 Mln 1,497 2,498
Table 3: Data sets for different language pairs.
the WMT 2006 shared task on machine translation
(Koehn and Monz, 2006). The parallel training set
contains approximately 751K sentences. We also
used the English monolingual data of around 1 mil-
lion sentences for language model training. The de-
velopment set contains 2000 sentences. The final
test set (the in-domain test set for the shared task)
also contains 2000 sentences. Two Kneser-Ney lan-
guage models were used as separate features: a 4-
gram LM trained on the parallel portion of the data,
and a 5-gram LM trained on the monolingual corpus.
For English-Bulgarian we used a dataset con-
taining sentences from several data sources: JRC-
Acquis (Steinberger et al, 2006), TAUS4, and web-
scraped data. The development set consists of 1,497
sentences, the English side from WMT 2009 news
test data, and the Bulgarian side a human translation
thereof. The test set comes from the same mixture of
sources as the training set. For this system we used
a single four-gram target language model trained on
the target side of the parallel corpus.
All systems used phrase tables with a maximum
length of seven words on either side and lexicalized
reordering models. For the Chinese-English sys-
tem we used GIZA++ alignments, and for the other
two we used alignments by an HMM model aug-
mented with word-based distortion (He, 2007). The
alignments were symmetrized and then combined
with the heuristics ?grow-diag-final-and?. 5 We tune
parameters using MERT (Och, 2003) with random
restarts (Moore and Quirk, 2008) on the develop-
ment set. Case-insensitive BLEU-4 is our evaluation
metric (Papineni et al, 2002).
3-gram models 5-gram models
Model Dev Test Dev Test
Baseline 32.58 31.78 32.58 31.78
L2RT 33.05 32.78* 33.16 32.88*
R2LT 33.05 32.96* 33.16 32.81*
L2RS 32.90 33.00* 32.98 32.98*
R2LS 32.94 32.98* 33.09 32.96*
4 MMs 33.22 33.07* 33.37 33.00*
4 MMs phrs 32.58 31.78 32.58 31.78
Table 4: Reranking with 3-gram and 5-gram MTU trans-
lation models on Chinese-English. Starred results on the
test set indicate significantly better performance than the
baseline.
6.3 MT reranking experiments
We first report detailed experiments on Chinese-
English, and then verify our main conclusions on the
other language pairs. Table 4 looks at the impact of
individual 3-gram and 5-gram MTU Markov models
and their combination. Amongst the decomposition
orders tested (L2RT, R2LT, L2RS, and R2LS), each
of the individual MTU MMs was able to achieve
significant improvement over the baseline, around 1
BLEU point.6 The results achieved by the individ-
ual models differ, and the combination of four direc-
tions is better than the best individual direction, but
the difference is not statistically significant.
We ran an additional experiment to test whether
MTU MMs make effective use of context across
phrase boundaries, or whether they simply pro-
vide better smoothed estimates of phrasal transla-
tion probabilities. The last row of the table reports
the results achieved by a combination of MTU MMs
that do not use context across the phrasal bound-
aries. Since an MTU MM limited to look only inside
phrases can provide improved smoothing compared
to whole phrase relative frequency counts, it is con-
ceivable it could provide a large improvement. How-
ever, there is no improvement in practice for this lan-
guage pair; the additional improvements from MTU
MMs stem from modeling cross-phrase context.
4www.tausdata.org
5The combination heuristic was further refined to disallow
crossing one-to-many alignments, which would result in the ex-
traction of larger minimum translation units. We found that this
further refinement on the combination heuristic consistently im-
proved the BLEU scores by between 0.3 and 0.7.
6Here again we call a difference significant if the paired
bootstrap p-value is less than 0.01.
19
Table 5 shows the test set results of individ-
ual 3-gram MTU Markov models and the com-
bination of 3-gram and 5-gram models on the
English-Bulgarian and German-English datasets.
For English-Bulgarian all individual 3-gram Markov
models achieve significant improvements of close to
one point; their combination is better than the best
individual model (but not significantly). The indi-
vidual 5-gram models and their combination bring
much larger improvement, for a total increase of
2.82 points over the baseline. We believe the 5-
gram models were more effective in this setting be-
cause the larger training set alowed for successful
training of models of larger capacity. Also the in-
creased context size helps to resolve ambiguity in
the forms of morphologically-rich Bulgarian words.
For German-English we see a similar pattern, with
the combination of models outperforming the in-
dividual ones, and the 5-gram models being better
than the 3-gram. Here the individual 3-gram models
are better than the baseline at significance level 0.02
and their combination is better than the baseline at
our earlier defined threshold of 0.01. The within-
phrase MTU MMs (results shown in the last two
rows) improve upon the baseline slightly, but here
again the improvements mostly stem from the use of
context across phrase boundaries. Our final results
on German-English are better than the best result of
27.30 from the shared task (Koehn and Monz, 2006).
Thanks to the reviewers for referring us to re-
cent work by (Clark et al, 2011) that pointed out
problems with significance tests for machine trans-
lation, where the randomness and local optima in the
MERT weight tuning method lead to a large vari-
ance in development and test set performance across
different runs of optimization (using a different ran-
dom seed or starting point). (Clark et al, 2011) pro-
posed a stratified approximate randomization statis-
tical significance test, which controls for optimizer
instability. Using this test, for the English-Bulgarian
system, we confirmed that the combination of four
3-gram MMs and the combination of 5-gram MMs
is better than the baseline (p = .0001 for both, using
five runs of parameter tuning). We have not run the
test for the other language pairs.
Model En-Bgr Deu-En
Baseline 45.75 27.92
L2RT 3-gram 47.07* 28.15
R2LT 3-gram 47.06* 28.19
L2RS 3-gram 46.44* 28.15
R2LS 3-gram 47.04* 28.18
4 3-gram 47.17* 28.37*
4 5-gram 48.57* 28.47*
4 3-gram phrs 46.08 27.92
4 5-gram phrs 46.17* 27.93
Table 5: English-Bulgarian and German-English test set
results: reranking with MTU translation models.
7 Conclusions
We introduced models of Minimal Translation Units
for phrasal systems, and showed that they make a
substantial and statistically significant improvement
on three distinct language-pairs. Additionally we
studied the importance of decomposition order when
defining the probability of MTU sequences. In a
simplified lexical selection task, we saw that there
were large differences in performance among the
different decompositions, with the best decomposi-
tions differing by language. We investigated multi-
ple methods to combine decompositions and found
that a simple product approach was most effective.
Results in the lexical selection task were consistent
with those obtained in a full MT system, although
the differences among decompositions were smaller.
In future work, perhaps we would see larger gains
by including additional decomposition orders (e.g.,
top-down in a dependency tree), and taking this idea
deeper into the machine translation model, down to
the word-alignment and language-modeling levels.
We were surprised to find n-best reranking so ef-
fective. We are incorporating the models into first
pass decoding, in hopes of even greater gains.
References
Jonathan H. Clark, Chris Dyer, Alon Lavie, and Noah A.
Smith. 2011. Better hypothesis testing for statistical
machine translation: Controlling for optimizer insta-
bility. In Proc. ACL-11.
JM Crego and F Yvon. 2010. Factored bilingual n-
gram language models for statistical machine transla-
tion. Machine Translation, Special Issue: Pushing the
frontiers of SMT, 24(2):159?175.
20
Nadir Durrani, Helmut Schmid, and Alexander Fraser.
2011. A joint sequence translation model with inte-
grated reordering. In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguis-
tics: Human Language Technologies, pages 1045?
1054, Portland, Oregon, USA, June. Association for
Computational Linguistics.
Andrew Finch and Eiichiro Sumita. 2009. Bidirectional
phrase-based machine translation. In In proceedings
of EMNLP.
Xiaodong He. 2007. Using word-dependent transition
models in hmm based word alignment for statistical
machine translation. In WMT workshop.
Reinhard Kenser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In Proc.
ICASSP 1995, pages 181?184.
Philipp Koehn and Christof Monz. 2006. Manual and au-
tomatic evaluation of machine translation between eu-
ropean languages. In Proceedings on the Workshop on
Statistical Machine Translation, pages 102?121, June.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
HLT-NAACL 2003, pages 127?133.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In In Proceedings of
EMNLP.
JB Marino, RE Banchs, JM Crego, A de Gispert, P Lam-
bert, JA Fonollosa, and MR Costa-Jussa. 2006. N-
gram-based machine translation. Computational Lin-
guistics, 32(4):527?549.
Robert C. Moore and Chris Quirk. 2008. Random
restarts in minimum error training for statistical ma-
chine translation. In Proc. Coling-08.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for statis-
tical machine translation. In In Proceedings of ACL,
pages 295?302.
Franz Joseph Och. 2003. Minimum error training in sta-
tistical machine translation. In Proc. ACL-03.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proc. 40th Annual
Meeting of the ACL, pages 311?318.
Chris Quirk and Arul Menezes. 2006. Do we need
phrases? challenging the conventional wisdom in sta-
tistical machine translation. In Proceedings of the Hu-
man Language Technology Conference of the NAACL,
Main Conference, pages 9?16, New York City, USA,
June. Association for Computational Linguistics.
Ralf Steinberger, Bruno Pouliquen, Anna Widiger,
Camelia Ignat, Toma Erjavec, Dan Tufis, and Dniel
Varga. 2006. The JRC-Acquis: A multilingual
aligned parallel corpus with 20+ languages. In LREC,
Genoa, Italy.
Kristina Toutanova, Dan Klein, Christopher D. Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In In Pro-
ceedings of HLT-NAACL.
Yoshimasa Tsuruoka and Jun?ichi Tsujii. 2005. Bidi-
rectional inference with the easiest-first strategy
for tagging sequence data. In In proceedings of
HLT/EMNLP.
Ashish Vaswani, Haitao Mi, Liang Huang, and David
Chiang. 2011. Rule markov models for fast tree-to-
string translation. In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguis-
tics: Human Language Technologies, pages 856?864,
Portland, Oregon, USA, June. Association for Compu-
tational Linguistics.
21
Tutorials, NAACL-HLT 2013, pages 16?18,
Atlanta, Georgia, June 9 2013. c?2013 Association for Computational Linguistics
Morphological, Syntactical and Semantic
Knowledge in Statistical Machine Translation
Marta R. Costa-jussa`?, Chris Quirk?
?Institute for Infocomm Research
?Microsoft Research
martaruizcostajussa@gmail.com
chrisq@microsoft.com
1 Overview
This tutorial focuses on how morphology, syntax and semantics may be introduced
into a standard phrase-based statistical machine translation system with techniques
such as machine learning, parsing and word sense disambiguation, among others.
Regarding the phrase-based system, we will describe only the key theory be-
hind it. The main challenges of this approach are that the output contains unknown
words, wrong word orders and non-adequate translated words. To solve these chal-
lenges, recent research enhances the standard system using morphology, syntax
and semantics.
Morphologically-rich languages have many different surface forms, even though
the stem of a word may be the same. This leads to rapid vocabulary growth, as var-
ious prefixes and suffixes can combine with stems in a large number of possible
combinations. Language model probability estimation is less robust because many
more word forms occur rarely in the data. This morphologically-induced sparsity
can be reduced by incorporating morphological information into the SMT system.
We will describe the three most common solutions to face morphology: preprocess-
ing the data so that the input language more closely resembles the output language;
using additional language models that introduce morphological information; and
post-processing the output to add proper inflections.
Syntax differences between the source and target language may lead to signif-
icant differences in the relative word order of translated words. Standard phrase-
based SMT systems surmount reordering/syntactic challenges by learning from
data. Most approaches model reordering inside translation units and using sta-
tistical methodologies, which limits the performance in language pairs with dif-
ferent grammatical structures. We will briefly introduce some recent advances in
16
SMT that use modeling approaches based on principles more powerful flat phrases
and better suited to the hierarchical structures of language: SMT decoding with
stochastic synchronous context free grammars and syntax-driven translation mod-
els.
Finally, semantics are not directly included in the SMT core algorithm, which
means that challenges such as polysemy or synonymy are either learned directly
from data or they are incorrectly translated. We will focus on recent attempts to
introduce semantics into statistical-based systems by using source context infor-
mation.
The course material will be suitable both for attendees with limited knowledge
of the field, and for researchers already familiar with SMT who wish to learn about
modern tendencies in hybrid SMT. The mathematical content of the course include
probability and simple machine learning, so reasonable knowledge of statistics and
mathematics is required. There will be a small amount of linguistics and ideas from
natural language processing.
2 Outline
1. Statistical Machine Translation
? Introduction to Machine Translation approaches
? Phrase-based systems
2. Morphology in SMT
? Types of languages in terms of morphology
? Enriching source language
? Inflection generation
? Class-based language models
3. Syntax in SMT
4. Semantics in SMT
? Sense disambiguation
? Context-dependent translations
17
3 Speaker Bios
Marta R. Costa-jussa`1, Institute for Infocomm Research (I2R), is a Telecommu-
nication?s Engineer by the Universitat Polite`cnica de Catalunya (UPC, Barcelona)
and she received her PhD from the UPC in 2008. Her research experience is
mainly in Automatic Speech Recognition, Machine Translation and Information
Retrieval. She has worked at LIMSI-CNRS (Paris), Barcelona Media Innovation
Center (Barcelona) and the Universidade de Sao Paulo (Sa?o Paulo). Since Decem-
ber 2012 she is working at Institute for Infocomm Research (Singapore) imple-
menting the IMTraP project (?Integration of Machine Translation Paradigms?) on
Hybrid Machine Translation, funded by the European Marie Curie International
Outgoing European Fellowship program. She is currently organizing the ACL
Workshop HyTRA 2013 and she will be teaching a summer school course on hy-
brid machine translation at ESSLLI 2013.
Chris Quirk2, Microsoft Research. After studying Computer Science and
Mathematics at Carnegie Mellon University, Chris joined Microsoft in 2000 to
work on the Intentional Programming project, an extensible compiler and develop-
ment framework. He moved to the Natural Language Processing group in 2001,
where his research has mostly focused on statistical machine translation powering
Microsoft Translator, especially on several generations of a syntax directed transla-
tion system that powers over half of the translation systems. He is also interested in
semantic parsing, paraphrase methods, and very practical problems such as spelling
correction and transliteration.
1http://www.costa-jussa.com
2http://research.microsoft.com/en-us/people/chrisq/
18
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 266?274,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
  Learning Phrase-Based Spelling Error Models  
from Clickthrough Data 
 
 
 
Xu Sun? 
Dept. of Mathematical Informatics 
University of Tokyo, Tokyo, Japan 
xusun@mist.i.u-tokyo.ac.jp
Jianfeng Gao 
Microsoft Research 
Redmond, WA, USA 
jfgao@microsoft.com 
 
Daniel Micol 
Microsoft Corporation 
Munich, Germany 
danielmi@microsoft.com 
Chris Quirk 
Microsoft Research 
Redmond, WA, USA 
chrisq@microsoft.com 
 
                                                     
? The work was done when Xu Sun was visiting Microsoft Research Redmond. 
Abstract 
This paper explores the use of clickthrough data 
for query spelling correction. First, large amounts 
of query-correction pairs are derived by analyzing 
users' query reformulation behavior encoded in 
the clickthrough data. Then, a phrase-based error 
model that accounts for the transformation 
probability between multi-term phrases is trained 
and integrated into a query speller system. Expe-
riments are carried out on a human-labeled data 
set. Results show that the system using the 
phrase-based error model outperforms signifi-
cantly its baseline systems. 
1 Introduction 
Search queries present a particular challenge for 
traditional spelling correction methods for three 
main reasons (Ahmad and Kondrak, 2004).  First, 
spelling errors are more common in search queries 
than in regular written text: roughly 10-15% of 
queries contain misspelled terms (Cucerzan and 
Brill, 2004). Second, most search queries consist 
of a few key words rather than grammatical sen-
tences, making a grammar-based approach inap-
propriate. Most importantly, many queries con-
tain search terms, such as proper nouns and names, 
which are not well established in the language. 
For example, Chen et al (2007) reported that 
16.5% of valid search terms do not occur in their 
200K-entry spelling lexicon. 
Therefore, recent research has focused on the 
use of Web corpora and query logs, rather than 
human-compiled lexicons, to infer knowledge 
about misspellings and word usage in search 
queries (e.g., Whitelaw et al, 2009). Another 
important data source that would be useful for this 
purpose is clickthrough data. Although it is 
well-known that clickthrough data contain rich 
information about users' search behavior, e.g., 
how a user (re-) formulates a query in order to 
find the relevant document, there has been little 
research on exploiting the data for the develop-
ment of a query speller system. 
In this paper we present a novel method of 
extracting large amounts of query-correction pairs 
from the clickthrough data.  These pairs, impli-
citly judged by millions of users, are used to train 
a set of spelling error models. Among these 
models, the most effective one is a phrase-based 
error model that captures the probability of 
transforming one multi-term phrase into another 
multi-term phrase. Comparing to traditional error 
models that account for transformation probabili-
ties between single characters (Kernighan et al, 
1990) or sub-word strings (Brill and Moore, 
2000), the phrase-based model is more powerful 
in that it captures some contextual information by 
retaining inter-term dependencies. We show that 
this information is crucial to detect the correction 
of a query term, because unlike in regular written 
text, any query word can be a valid search term 
and in many cases the only way for a speller 
system to make the judgment is to explore its 
usage according to the contextual information. 
We conduct a set of experiments on a large 
data set, consisting of human-labeled 
266
query-correction pairs. Results show that the error 
models learned from clickthrough data lead to 
significant improvements on the task of query 
spelling correction. In particular, the speller sys-
tem incorporating a phrase-based error model 
significantly outperforms its baseline systems. 
To the best of our knowledge, this is the first 
extensive study of learning phase-based error 
models from clickthrough data for query spelling 
correction. The rest of the paper is structured as 
follows. Section 2 reviews related work. Section 3 
presents the way query-correction pairs are ex-
tracted from the clickthrough data. Section 4 
presents the baseline speller system used in this 
study. Section 5 describes in detail the phrase- 
based error model. Section 6 presents the expe-
riments. Section 7 concludes the paper. 
2 Related Work 
Spelling correction for regular written text is a 
long standing research topic. Previous researches 
can be roughly grouped into two categories: 
correcting non-word errors and real-word errors. 
In non-word error spelling correction, any 
word that is not found in a pre-compiled lexicon is 
considered to be misspelled.  Then, a list of lexical 
words that are similar to the misspelled word are 
proposed as candidate spelling corrections. Most 
traditional systems use a manually tuned similar-
ity function (e.g., edit distance function) to rank 
the candidates, as reviewed by Kukich (1992). 
During the last two decades, statistical error 
models learned on training data (i.e., 
query-correction pairs) have become increasingly 
popular, and have proven more effective (Ker-
nighan et al, 1990; Brill and Moore, 2000; Tou-
tanova and Moore, 2002; Okazaki et al, 2008).  
Real-word spelling correction is also referred 
to as context sensitive spelling correction (CSSC). 
It tries to detect incorrect usages of a valid word 
based on its context, such as "peace" and "piece" 
in the context "a _ of cake". A common strategy in 
CSSC is as follows. First, a pre-defined confusion 
set is used to generate candidate corrections, then 
a  scoring model, such as a trigram language 
model or na?ve Bayes classifier, is used to rank the 
candidates according to their context (e.g., 
Golding and Roth, 1996; Mangu and Brill, 1997; 
Church et al, 2007). 
When designed to handle regular written text, 
both CSSC and non-word error speller systems 
rely on a pre-defined vocabulary (i.e., either a 
lexicon or a confusion set). However, in query 
spelling correction, it is impossible to compile 
such a vocabulary, and the boundary between the 
non-word and real-word errors is quite vague. 
Therefore, recent research on query spelling 
correction has focused on exploiting noisy Web 
data and query logs to infer knowledge about 
misspellings and word usage in search queries. 
Cucerzan and Brill (2004) discuss in detail the 
challenges of query spelling correction, and 
suggest the use of query logs. Ahmad and Kon-
drak (2005) propose a method of estimating an 
error model from query logs using the EM algo-
rithm. Li et al (2006) extend the error model by 
capturing word-level similarities learned from 
query logs. Chen et al (2007) suggest using web 
search results to improve spelling correction. 
Whitelaw et al (2009) present a query speller 
system in which both the error model and the 
language model are trained using Web data. 
Compared to Web corpora and query logs, 
clickthrough data contain much richer informa-
tion about users? search behavior.  Although there 
has been a lot of research on using clickthrough 
data to improve Web document retrieval (e.g., 
Joachims, 2002; Agichtein et al, 2006; Gao et al, 
2009), the data have not been fully explored for 
query spelling correction. This study tries to learn 
error models from clickthrough data. To our 
knowledge, this is the first such attempt using 
clickthrough data. 
Most of the speller systems reviewed above are 
based on the framework of the source channel 
model. Typically, a language model (source 
model) is used to capture contextual information, 
while an error model (channel model) is consi-
dered to be context free in that it does not take into 
account any contextual information in modeling 
word transformation probabilities. In this study 
we argue that it is beneficial to capture contextual 
information in the error model. To this end, in-
spired by the phrase-based statistical machine 
translation (SMT) systems (Koehn et al, 2003; 
Och and Ney, 2004), we propose a phrase-based 
error model where we assume that query spelling 
correction is performed at the phrase level. 
In what follows, before presenting the phrase- 
based error model, we will first describe the 
clickthrough data and the query speller system we 
used in this study. 
3 Clickthrough Data and Spelling Cor-
rection 
This section describes the way the 
query-correction pairs are extracted from click-
267
through data. Two types of clickthrough data are 
explored in our experiment. 
The clickthrough data of the first type has been 
widely used in previous research and proved to be 
useful for Web search (Joachims, 2002; Agichtein 
et al, 2006; Gao et al, 2009) and query refor-
mulation (Wang and Zhai, 2008; Suzuki et al, 
2009). We start with this same data with the hope 
of achieving similar improvements in our task. 
The data consist of a set of query sessions that 
were extracted from one year of log files from a 
commercial Web search engine. A query session 
contains a query issued by a user and a ranked list 
of links (i.e., URLs) returned to that same user 
along with records of which URLs were clicked. 
Following Suzuki et al (2009), we extract 
query-correction pairs as follows. First, we extract 
pairs of queries Q1 and Q2 such that (1) they are 
issued by the same user; (2) Q2 was issued within 
3 minutes of Q1; and (3) Q2 contained at least one 
clicked URL in the result page while Q1 did not 
result in any clicks.  We then scored each query 
pair (Q1, Q2) using the edit distance between Q1 
and Q2, and retained those with an edit distance 
score lower than a pre-set threshold as query 
correction pairs. 
Unfortunately, we found in our experiments 
that the pairs extracted using the method are too 
noisy for reliable error model training, even with a 
very tight threshold, and we did not see any sig-
nificant improvement. Therefore, in Section 6 we 
will not report results using this dataset. 
The clickthrough data of the second type con-
sists of a set of query reformulation sessions 
extracted from 3 months of log files from a 
commercial Web browser.  A query reformulation 
session contains a list of URLs that record user 
behaviors that relate to the query reformulation 
functions, provided by a Web search engine. For 
example, almost all commercial search engines 
offer the "did you mean" function, suggesting a 
possible alternate interpretation or spelling of a 
user-issued query. Figure 1 shows a sample of the 
query reformulation sessions that record the "did 
you mean" sessions from three of the most pop-
ular search engines. These sessions encode the 
same user behavior: A user first queries for 
"harrypotter sheme park", and then clicks on the 
resulting spelling suggestion "harry potter theme 
park". In our experiments, we "reverse-engineer" 
the parameters from the URLs of these sessions, 
and deduce how each search engine encodes both 
a query and the fact that a user arrived at a URL 
by clicking on the spelling suggestion of the query 
? an important indication that the spelling sug-
gestion is desired. From these three months of 
query reformulation sessions, we extracted about 
3 million query-correction pairs. Compared to the 
pairs extracted from the clickthrough data of the 
first type (query sessions), this data set is much 
cleaner because all these spelling corrections are 
actually clicked, and thus judged implicitly, by 
many users. 
In addition to the "did you mean" function, 
recently some search engines have introduced two 
new spelling suggestion functions. One is the 
"auto-correction" function, where the search 
engine is confident enough to automatically apply 
the spelling correction to the query and execute it 
to produce search results for the user.  The other is 
the "split pane" result page, where one half por-
tion of the search results are produced using the 
original query, while the other half, usually vi-
sually separate portion of results are produced 
using the auto-corrected query. 
In neither of these functions does the user ever 
receive an opportunity to approve or disapprove 
of the correction. Since our extraction approach 
focuses on user-approved spelling suggestions, 
Google: 
http://www.google.com/search? 
hl=en&source=hp& 
q=harrypotter+sheme+park&aq=f&oq=&aqi= 
http://www.google.com/search? 
hl=en&ei=rnNAS8-oKsWe_AaB2eHlCA& 
sa=X&oi=spell&resnum=0&ct= 
result&cd=1&ved=0CA4QBSgA& 
q=harry+potter+theme+park&spell=1 
Yahoo: 
http://search.yahoo.com/search; 
_ylt=A0geu6ywckBL_XIBSDtXNyoA? 
p=harrypotter+sheme+park& 
fr2=sb-top&fr=yfp-t-701&sao=1 
http://search.yahoo.com/search? 
ei=UTF-8&fr=yfp-t-701& 
p=harry+potter+theme+park 
&SpellState=n-2672070758_q-tsI55N6srhZa. 
qORA0MuawAAAA%40%40&fr2=sp-top 
Bing: 
http://www.bing.com/search? 
q=harrypotter+sheme+park&form=QBRE&qs=n 
http://www.bing.com/search? 
q=harry+potter+theme+park&FORM=SSRE 
Figure 1.  A sample of query reformulation sessions 
from three popular search engines. These sessions 
show that a user first issues the query "harrypotter 
sheme park", and then clicks on the resulting spell 
suggestion "harry potter theme park". 
268
we ignore the query reformulation sessions re-
cording either of the two functions. Although by 
doing so we could miss some basic, obvious 
spelling corrections, our experiments show that 
the negative impact on error model training is 
negligible. One possible reason is that our base-
line system, which does not use any error model 
learned from the clickthrough data, is already able 
to correct these basic, obvious spelling mistakes. 
Thus, including these data for training is unlikely 
to bring any further improvement. 
We found that the error models trained using 
the data directly extracted from the query refor-
mulation sessions suffer from the problem of 
underestimating the self-transformation probabil-
ity of a query P(Q2=Q1|Q1), because we only 
included in the training data the pairs where the 
query is different from the correction. To deal 
with this problem, we augmented the training data 
by including correctly spelled queries, i.e., the 
pairs (Q1, Q2) where Q1 = Q2.  First, we extracted a 
set of queries from the sessions where no spell 
suggestion is presented or clicked on. Second, we 
removed from the set those queries that were 
recognized as being auto-corrected by a search 
engine. We do so by running a sanity check of the 
queries against our baseline spelling correction 
system, which will be described in Section 6. If 
the system thinks an input query is misspelled, we 
assumed it was an obvious misspelling, and re-
moved it. The remaining queries were assumed to 
be correctly spelled and were added to the training 
data. 
4 The Baseline Speller System 
The spelling correction problem is typically 
formulated under the framework of the source 
channel model. Given an input query ? ?
??. . . ??, we want to find the best spelling correc-
tion ? ? ??. . . ??  among all candidate spelling 
corrections: 
?? ? argmax
?
???|?? (1) 
Applying Bayes' Rule and dropping the constant 
denominator, we have 
?? ? argmax
?
???|?????? (2) 
where the error model ???|?? models the trans-
formation probability from C to Q, and the lan-
guage model ????  models how likely C is a 
correctly spelled query. 
The speller system used in our experiments is 
based on a ranking model (or ranker), which can 
be viewed as a generalization of the source 
channel model. The system consists of two 
components: (1) a candidate generator, and (2) a 
ranker. 
In candidate generation, an input query is first 
tokenized into a sequence of terms. Then we scan 
the query from left to right, and each query term q 
is looked up in lexicon to generate a list of spel-
ling suggestions c whose edit distance from q is 
lower than a preset threshold. The lexicon we 
used contains around 430,000 entries; these are 
high frequency query terms collected from one 
year of search query logs. The lexicon is stored 
using a trie-based data structure that allows effi-
cient search for all terms within a maximum edit 
distance. 
The set of all the generated spelling sugges-
tions is stored using a lattice data structure, which 
is a compact representation of exponentially many 
possible candidate spelling corrections. We then 
use a decoder to identify the top twenty candi-
dates from the lattice according to the source 
channel model of Equation (2).  The language 
model (the second factor) is a backoff bigram 
model trained on the tokenized form of one year 
of query logs, using maximum likelihood estima-
tion with absolute discounting smoothing.  The 
error model (the first factor) is approximated by 
the edit distance function as 
?log???|?? ? EditDist??, ?? (3) 
The decoder uses a standard two-pass algorithm 
to generate 20-best candidates. The first pass uses 
the Viterbi algorithm to find the best C according 
to the model of Equations (2) and (3).  In the 
second pass, the A-Star algorithm is used to find 
the 20-best corrections, using the Viterbi scores 
computed at each state in the first pass as heuris-
tics. Notice that we always include the input query 
Q in the 20-best candidate list. 
The core of the second component of the 
speller system is a ranker, which re-ranks the 
20-best candidate spelling corrections. If the top 
C after re-ranking is different than the original 
query Q, the system returns C as the correction.   
Let f be a feature vector extracted from a query 
and candidate spelling correction pair (Q, C). The 
ranker maps f to a real value y that indicates how 
likely C is a desired correction of Q.  For example, 
a linear ranker simply maps f to y with a learned 
weight vector w such as ? ? ? ? ?, where w is 
optimized w.r.t. accuracy on a set of hu-
269
man-labeled (Q, C) pairs. The features in f are 
arbitrary functions that map (Q, C) to a real value. 
Since we define the logarithm of the probabilities 
of the language model and the error model (i.e., 
the edit distance function) as features, the ranker 
can be viewed as a more general framework, 
subsuming the source channel model as a special 
case. In our experiments we used 96 features and a 
non-linear model, implemented as a two-layer 
neural net, though the details of the ranker and the 
features are beyond the scope of this paper. 
5 A Phrase-Based Error Model 
The goal of the phrase-based error model is to 
transform a correctly spelled query C into a 
misspelled query Q. Rather than replacing single 
words in isolation, this model replaces sequences 
of words with sequences of words, thus incorpo-
rating contextual information. For instance, we 
might learn that ?theme part? can be replaced by 
?theme park? with relatively high probability, 
even though ?part? is not a misspelled word. We 
assume the following generative story: first the 
correctly spelled query C is broken into K 
non-empty word sequences c1, ?, ck, then each is 
replaced with a new non-empty word sequence q1, 
?, qk, and finally these phrases are permuted and 
concatenated to form the misspelled Q. Here, c 
and q denote consecutive sequences of words. 
To formalize this generative process, let S 
denote the segmentation of C into K phrases c1?cK, 
and let T denote the K replacement phrases 
q1?qK ? we refer to these (ci, qi) pairs as 
bi-phrases. Finally, let M denote a permutation of 
K elements representing the final reordering step. 
Figure 2 demonstrates the generative procedure. 
Next let us place a probability distribution over 
rewrite pairs. Let B(C, Q) denote the set of S, T, M 
triples that transform C into Q. If we assume a 
uniform probability over segmentations, then the 
phrase-based probability can be defined as: 
???|?? ? ? ???|?, ?? ? ???|?, ?, ??
??,?,???
???,??
 (4) 
As is common practice in SMT, we use the 
maximum approximation to the sum:  
???|?? ? max
??,?,???
???,??
???|?, ?? ? ???|?, ?, ?? (5) 
5.1 Forced Alignments 
Although we have defined a generative model for 
transforming queries, our goal is not to propose 
new queries, but rather to provide scores over 
existing Q and C pairs which act as features for 
the ranker. Furthermore, the word-level align-
ments between Q and C can most often be iden-
tified with little ambiguity. Thus we restrict our 
attention to those phrase transformations consis-
tent with a good word-level alignment. 
Let J be the length of Q, L be the length of C, 
and A = a1, ?, aJ be a hidden variable 
representing the word alignment. Each ai takes on 
a value ranging from 1 to L indicating its corres-
ponding word position in C, or 0 if the ith word in 
Q is unaligned. The cost of assigning k to ai is 
equal to the Levenshtein edit distance (Levensh-
tein, 1966) between the ith word in Q and the kth 
word in C, and the cost of assigning 0 to ai is equal 
to the length of the ith word in Q. We can deter-
mine the least cost alignment A* between Q and C 
efficiently using the A-star algorithm. 
When scoring a given candidate pair, we fur-
ther restrict our attention to those S, T, M triples 
that are consistent with the word alignment, which 
we denote as B(C, Q, A*). Here, consistency re-
quires that if two words are aligned in A*, then 
they must appear in the same bi-phrase (ci, qi). 
Once the word alignment is fixed, the final per-
mutation is uniquely determined, so we can safely 
discard that factor. Thus we have: 
???|?? ? max
??,?,???
???,?,???
???|?, ?? (6) 
For the sole remaining factor P(T|C, S), we 
make the assumption that a segmented query T = 
q1? qK is generated from left to right by trans-
forming each phrase c1?cK independently: 
C: ?disney theme park? correct query 
S: [?disney?, ?theme park?] segmentation 
T: [?disnee?, ?theme part?] translation 
M: (1 ? 2, 2? 1) permutation 
Q: ?theme part disnee? misspelled query 
Figure 2: Example demonstrating the generative 
procedure behind the phrase-based error model. 
 
270
???|?, ?? ? ? ????|???
?
??? , (7) 
where ????|???  is a phrase transformation 
probability, the estimation of which will be de-
scribed in Section 5.2.  
To find the maximum probability assignment 
efficiently, we can use a dynamic programming 
approach, somewhat similar to the monotone 
decoding algorithm described in Och (2002). 
Here, though, both the input and the output word 
sequences are specified as the input to the algo-
rithm, as is the word alignment. We define the 
quantity ?? to be the probability of the most likely 
sequence of bi-phrases that produce the first j 
terms of Q and are consistent with the word 
alignment and C. It can be calculated using the 
following algorithm: 
1. Initialization:  
 ?? ? 1 (8) 
2. Induction:  
 ?? ? max
????,???
????
???
??
?????????? (9) 
3. Total:   
 ???|?? ? ?? (10) 
The pseudo-code of the above algorithm is 
shown in Figure 3. After generating Q from left to 
right according to Equations (8) to (10), we record 
at each possible bi-phrase boundary its maximum 
probability, and we obtain the total probability at 
the end-position of Q. Then, by back-tracking the 
most probable bi-phrase boundaries, we obtain B*.  
The algorithm takes a complexity of O(KL2), 
where K is the total number of word alignments in 
A* which does not contain empty words, and L is 
the maximum length of a bi-phrase, which is a 
hyper-parameter of the algorithm. Notice that 
when we set L=1, the phrase-based error model is 
reduced to a word-based error model which as-
sumes that words are transformed independently 
from C to Q, without taking into account any 
contextual information. 
 
5.2 Model Estimation 
We follow a method commonly used in SMT 
(Koehn et al, 2003) to extract bi-phrases and 
estimate their replacement probabilities.  From 
each query-correction pair with its word align-
ment (Q, C, A*), all bi-phrases consistent with the 
word alignment are identified. Consistency here 
implies two things. First, there must be at least 
one aligned word pair in the bi-phrase. Second, 
there must not be any word alignments from 
words inside the bi-phrase to words outside the 
bi-phrase. That is, we do not extract a phrase pair 
if there is an alignment from within the phrase 
pair to outside the phrase pair. The toy example 
shown in Figure 4 illustrates the bilingual phrases 
we can generate by this process. 
 After gathering all such bi-phrases from the 
full training data, we can estimate conditional 
relative frequency estimates without smoothing. 
For example, the phrase transformation probabil-
ity ???|?? in Equation (7) can be estimated ap-
proximately as 
Input: biPhraseLattice ?PL? with length = K & height 
= L;  
Initialization: biPhrase.maxProb = 0; 
for (x = 0; x <= K ? 1; x++) 
      for (y = 1; y <= L; y++) 
            for (yPre = 1; yPre <= L; yPre++) 
            { 
                  xPre = x ? y;  
                  biPhrasePre = PL.get(xPre, yPre); 
                  biPhrase = PL.get(x, y); 
                  if (!biPhrasePre || !biPhrase) 
                         continue; 
                  probIncrs = PL.getProbIncrease(biPhrasePre,  
                                                                      biPhrase); 
                  maxProbPre = biPhrasePre.maxProb;  
                  totalProb = probIncrs + maxProbPre; 
                  if  (totalProb > biPhrase.maxProb)  
                  { 
                        biPhrase.maxProb = totalProb;  
                        biPhrase.yPre = yPre; 
                   } 
             } 
Result: record at each bi-phrase boundary its maxi-
mum probability (biPhrase.maxProb) and optimal 
back-tracking biPhrases (biPhrase.yPre). 
 
Figure 3: The dynamic programming algorithm for 
Viterbi bi-phrase segmentation. 
 A B C D E F  a A 
a #       adc ABCD 
d    #    d D 
c   #     dc CD 
f      #  dcf CDEF 
        c C 
        f F 
 
Figure 4: Toy example of (left) a word alignment 
between two strings "adcf" and "ABCDEF"; and (right) 
the bi-phrases containing up to four words that are 
consistent with the word alignment. 
 
 
271
???|?? ?
???, ??
? ???, ?????
 (11) 
where ???, ?? is the number of times that c is 
aligned to q in training data. These estimates are 
useful for contextual lexical selection with suffi-
cient training data, but can be subject to data 
sparsity issues. 
An alternate translation probability estimate 
not subject to data sparsity issues is the so-called 
lexical weight estimate (Koehn et al, 2003). 
Assume we have a word translation distribution 
???|??  (defined over individual words, not 
phrases), and a word alignment A between q and c; 
here, the word alignment contains (i, j) pairs, 
where  ? ? 1. . |?| and ? ? 0. . |?|, with 0 indicat-
ing an inserted word.  Then we can use the fol-
lowing estimate: 
????|?, ?? ??
1
|??|??, ?? ? ??|
? ????| ???
???,????
|?|
???
 (12) 
We assume that for every position in q, there is 
either a single alignment to 0, or multiple align-
ments to non-zero positions in c. In effect, this 
computes a product of per-word translation scores; 
the per-word scores are averages of all the trans-
lations for the alignment links of that word. We 
estimate the word translation probabilities using 
counts from the word aligned corpus: ???|?? ?
???,??
? ???,?????
. Here ???, ?? is the number of times that 
the words (not phrases as in Equation 11) c and q 
are aligned in the training data. These word based 
scores of bi-phrases, though not as effective in 
contextual selection, are more robust to noise and 
sparsity. 
Throughout this section, we have approached 
this model in a noisy channel approach, finding 
probabilities of the misspelled query given the 
corrected query. However, the method can be run 
in both directions, and in practice SMT systems 
benefit from also including the direct probability 
of the corrected query given this misspelled query 
(Och, 2002). 
5.3 Phrase-Based Error Model Features 
To use the phrase-based error model for spelling 
correction, we derive five features and integrate 
them into the ranker-based query speller system, 
described in Section 4. These features are as 
follows. 
? Two phrase transformation features: 
These are the phrase transformation scores 
based on relative frequency estimates in two 
directions. In the correction-to-query direc-
tion, we define the feature as  ?????, ?, ?? ?
log ???|?? , where ???|??  is computed by 
Equations (8) to (10), and ??????? is the rel-
ative frequency estimate of Equation (11).   
? Two lexical weight features: These are the 
phrase transformation scores based on the 
lexical weighting models in two directions. 
For example, in the correction-to-query di-
rection, we define the feature 
as ?????, ?, ?? ? log ???|??, where ???|?? 
is computed by Equations (8) to (10), and the 
phrase transformation probability is the 
computed as lexical weight according to Eq-
uation (12). 
? Unaligned word penalty feature: the feature 
is defined as the ratio between the number of 
unaligned query words and the total number 
of query words. 
6 Experiments 
We evaluate the spelling error models on a large 
scale real world data set containing 24,172 queries 
sampled from one year?s worth of query logs from 
a commercial search engine. The spelling of each 
query is judged and corrected by four annotators.  
We divided the data set into training and test 
data sets. The two data sets do not overlap. The 
training data contains 8,515 query-correction 
pairs, among which 1,743 queries are misspelled 
(i.e., in these pairs, the corrections are different 
from the queries). The test data contains 15,657 
query-correction pairs, among which 2,960 que-
ries are misspelled. The average length of queries 
in the training and test data is 2.7 words.  
The speller systems we developed in this study 
are evaluated using the following three metrics. 
? Accuracy: The number of correct outputs 
generated by the system divided by the total 
number of queries in the test set. 
? Precision: The number of correct spelling 
corrections for misspelled queries generated 
by the system divided by the total number of 
corrections generated by the system. 
? Recall: The number of correct spelling cor-
rections for misspelled queries generated by 
the system divided by the total number of 
misspelled queries in the test set. 
We also perform a significance test, i.e., a t-test 
with a significance level of 0.05. A significant 
difference should be read as significant at the 95% 
level. 
272
In our experiments, all the speller systems are 
ranker-based. In most cases, other than the base-
line system (a linear neural net), the ranker is a 
two-layer neural net with 5 hidden nodes. The free 
parameters of the neural net are trained to optim-
ize accuracy on the training data using the back 
propagation algorithm, running for 500 iterations 
with a very small learning rate (0.1) to avoid 
overfitting. We did not adjust the neural net 
structure (e.g., the number of hidden nodes) or 
any training parameters for different speller sys-
tems. Neither did we try to seek the best tradeoff 
between precision and recall. Since all the sys-
tems are optimized for accuracy, we use accuracy 
as the primary metric for comparison. 
Table 1 summarizes the main spelling correc-
tion results.  Row 1 is the baseline speller system 
where the source-channel model of Equations (2) 
and (3) is used. In our implementation, we use a 
linear ranker with only two features, derived 
respectively from the language model and the 
error model models. The error model is based on 
the edit distance function. Row 2 is the rank-
er-based spelling system that uses all 96 ranking 
features, as described in Section 4. Note that the 
system uses the features derived from two error 
models.  One is the edit distance model used for 
candidate generation. The other is a phonetic 
model that measures the edit distance between the 
metaphones (Philips, 1990) of a query word and 
its aligned correction word. Row 3 is the same 
system as Row 2, with an additional set of features 
derived from a word-based error model. This 
model is a special case of the phrase-based error 
model described in Section 5 with the maximum 
phrase length set to one.  Row 4 is the system that 
uses the additional 5 features derived from the 
phrase-based error models with a maximum 
bi-phrase length of 3. 
In phrase based error model, L is the maxi-
mum length of a bi-phrase (Figure 3). This value 
is important for the spelling performance. We 
perform experiments to study the impact of L; 
the results are displayed in Table 2. Moreover, 
since we proposed to use clickthrough data for 
spelling correction, it is interesting to study the 
impact on spelling performance from the size of 
clickthrough data used for training. We varied 
the size of clickthrough data and the experi-
mental results are presented in Table 3. 
The results show first and foremost that the 
ranker-based system significantly outperforms 
the spelling system based solely on the 
source-channel model, largely due to the richer 
set of features used (Row 1 vs. Row 2).  Second, 
the error model learned from clickthrough data 
leads to significant improvements (Rows 3 and 4 
vs. Row 2).  The phrase-based error model, due to 
its capability of capturing contextual information, 
outperforms the word-based model with a small 
but statistically significant margin (Row 4 vs. 
Row 3), though using phrases longer (L > 3) does 
not lead to further significant improvement (Rows 
6 and 7 vs. Rows 8 and 9). Finally, using more 
clickthrough data leads to significant improve-
ment (Row 13 vs. Rows 10 to 12). The benefit 
does not appear to have peaked ? further im-
provements are likely given a larger data set. 
7 Conclusions 
Unlike conventional textual documents, most 
search queries consist of a sequence of key words, 
many of which are valid search terms but are not 
stored in any compiled lexicon. This presents a 
challenge to any speller system that is based on a 
dictionary.  
This paper extends the recent research on using 
Web data and query logs for query spelling cor-
rection in two aspects. First, we show that a large 
amount of training data (i.e. query-correction 
pairs) can be extracted from clickthrough data, 
focusing on query reformulation sessions. The 
resulting data are very clean and effective for 
error model training. Second, we argue that it is 
critical to capture contextual information for 
query spelling correction. To this end, we propose 
# System Accuracy Precision Recall 
1 Source-channel 0.8526 0.7213 0.3586 
2 Ranker-based 0.8904 0.7414 0.4964 
3 Word model 0.8994 0.7709 0.5413 
4 Phrase model (L=3) 0.9043 0.7814 0.5732 
Table 1. Summary of spelling correction results. 
# System Accuracy Precision Recall 
5 Phrase model (L=1) 0.8994 0.7709 0.5413 
6 Phrase model (L=2) 0.9014 0.7795 0.5605 
7 Phrase model (L=3) 0.9043 0.7814 0.5732 
8 Phrase model (L=5) 0.9035 0.7834 0.5698 
9 Phrase model (L=8) 0.9033 0.7821 0.5713 
Table 2. Variations of spelling performance as a func-
tion of phrase length. 
 
# System Accuracy Precision Recall 
10 L=3; 0 month data 0.8904 0.7414 0.4964 
11 L=3; 0.5 month data 0.8959 0.7701 0.5234 
12 L=3; 1.5 month data 0.9023 0.7787 0.5667 
13 L=3; 3 month data 0.9043 0.7814 0.5732 
Table 3. Variations of spelling performance as a func-
tion of the size of clickthrough data used for training. 
 
 
273
a new phrase-based error model, which leads to 
significant improvement in our spelling correc-
tion experiments.  
There is additional potentially useful informa-
tion that can be exploited in this type of model. 
For example, in future work we plan to investigate 
the combination of the clickthrough data collected 
from a Web browser with the noisy but large 
query sessions collected from a commercial 
search engine. 
Acknowledgments 
The authors would like to thank Andreas Bode, 
Mei Li, Chenyu Yan and Galen Andrew for the 
very helpful discussions and collaboration. 
References 
Agichtein, E., Brill, E. and Dumais, S. 2006. Im-
proving web search ranking by incorporating user 
behavior information. In SIGIR, pp. 19-26. 
Ahmad, F., and Kondrak, G. 2005. Learning a 
spelling error model from search query logs. In 
HLT-EMNLP, pp 955-962. 
Brill, E., and Moore, R. C. 2000. An improved error 
model for noisy channel spelling correction. In 
ACL, pp. 286-293. 
Chen, Q., Li, M., and Zhou, M. 2007. Improving 
query spelling correction using web search results. 
In EMNLP-CoNLL, pp. 181-189. 
Church, K., Hard, T., and Gao, J. 2007. Compress-
ing trigram language models with Golomb cod-
ing. In EMNLP-CoNLL, pp. 199-207. 
Cucerzan, S., and Brill, E. 2004. Spelling correction 
as an iterative process that exploits the collective 
knowledge of web users. In EMNLP, pp. 293-300. 
Gao, J., Yuan, W., Li, X., Deng, K., and Nie, J-Y. 
2009. Smoothing clickthrough data for web 
search ranking. In SIGIR.  
Golding, A. R., and Roth, D. 1996. Applying win-
now to context-sensitive spelling correction. In 
ICML, pp. 182-190. 
Joachims, T. 2002. Optimizing search engines using 
clickthrough data. In SIGKDD, pp. 133-142. 
Kernighan, M. D., Church, K. W., and Gale, W. A. 
1990. A spelling correction program based on a 
noisy channel model. In COLING, pp. 205-210. 
Koehn, P., Och, F., and Marcu, D. 2003. Statistical 
phrase-based translation. In HLT/NAACL, pp. 
127-133. 
Kukich, K. 1992. Techniques for automatically 
correcting words in text. ACM Computing Sur-
veys. 24(4): 377-439. 
Levenshtein, V. I. 1966. Binary codes capable of 
correcting deletions, insertions and reversals. So-
viet Physics Doklady, 10(8):707-710. 
Li, M., Zhu, M., Zhang, Y., and Zhou, M. 2006. 
Exploring distributional similarity based models 
for query spelling correction. In ACL, pp. 
1025-1032. 
Mangu, L., and Brill, E. 1997. Automatic rule ac-
quisition for spelling correction. In ICML, pp. 
187-194. 
Och, F. 2002. Statistical machine translation: from 
single-word models to alignment templates. PhD 
thesis, RWTH Aachen. 
Och, F., and Ney, H. 2004. The alignment template 
approach to statistical machine translation. 
Computational Linguistics, 30(4): 417-449. 
Okazaki, N., Tsuruoka, Y., Ananiadou, S., and 
Tsujii, J. 2008. A discriminative candidate gene-
rator for string transformations. In EMNLP, pp. 
447-456. 
Philips, L. 1990. Hanging on the metaphone. 
Computer Language Magazine, 7(12):38-44. 
Suzuki, H., Li, X., and Gao, J. 2009. Discovery of 
term variation in Japanese web search queries. In 
EMNLP. 
Toutanova, K., and Moore, R. 2002. Pronunciation 
modeling for improved spelling correction. In 
ACL, pp. 144-151. 
Wang, X., and Zhai, C. 2008. Mining term associa-
tion patterns from search logs for effective query 
reformulation. In CIKM, pp. 479-488. 
Whitelaw, C., Hutchinson, B., Chung, G. Y., and 
Ellis, G. 2009. Using the web for language inde-
pendent spellchecking and autocorrection. In 
EMNLP, pp. 890-899.  
274
Proceedings of the ACL 2010 Conference Short Papers, pages 200?204,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Top-Down K-Best A? Parsing
Adam Pauls and Dan Klein
Computer Science Division
University of California at Berkeley
{adpauls,klein}@cs.berkeley.edu
Chris Quirk
Microsoft Research
Redmond, WA, 98052
chrisq@microsoft.com
Abstract
We propose a top-down algorithm for ex-
tracting k-best lists from a parser. Our
algorithm, TKA? is a variant of the k-
best A? (KA?) algorithm of Pauls and
Klein (2009). In contrast to KA?, which
performs an inside and outside pass be-
fore performing k-best extraction bottom
up, TKA? performs only the inside pass
before extracting k-best lists top down.
TKA? maintains the same optimality and
efficiency guarantees of KA?, but is sim-
pler to both specify and implement.
1 Introduction
Many situations call for a parser to return a k-
best list of parses instead of a single best hypothe-
sis.1 Currently, there are two efficient approaches
known in the literature. The k-best algorithm of
Jime?nez and Marzal (2000) and Huang and Chi-
ang (2005), referred to hereafter as LAZY, oper-
ates by first performing an exhaustive Viterbi in-
side pass and then lazily extracting k-best lists in
top-down manner. The k-best A? algorithm of
Pauls and Klein (2009), hereafter KA?, computes
Viterbi inside and outside scores before extracting
k-best lists bottom up.
Because these additional passes are only partial,
KA? can be significantly faster than LAZY, espe-
cially when a heuristic is used (Pauls and Klein,
2009). In this paper, we propose TKA?, a top-
down variant of KA? that, like LAZY, performs
only an inside pass before extracting k-best lists
top-down, but maintains the same optimality and
efficiency guarantees as KA?. This algorithm can
be seen as a generalization of the lattice k-best al-
gorithm of Soong and Huang (1991) to parsing.
Because TKA? eliminates the outside pass from
KA?, TKA? is simpler both in implementation and
specification.
1See Huang and Chiang (2005) for a review.
2 Review
Because our algorithm is very similar to KA?,
which is in turn an extension of the (1-best) A?
parsing algorithm of Klein and Manning (2003),
we first introduce notation and review those two
algorithms before presenting our new algorithm.
2.1 Notation
Assume we have a PCFG2 G and an input sen-
tence s0 . . . sn?1 of length n. The grammar G has
a set of symbols denoted by capital letters, includ-
ing a distinguished goal (root) symbol G. With-
out loss of generality, we assume Chomsky nor-
mal form: each non-terminal rule r in G has the
form r = A ? B C with weight wr. Edges
are labeled spans e = (A, i, j). Inside deriva-
tions of an edge (A, i, j) are trees with root non-
terminalA, spanning si . . . sj?1. The weight (neg-
ative log-probability) of the best (minimum) inside
derivation for an edge e is called the Viterbi in-
side score ?(e), and the weight of the best deriva-
tion of G ? s0 . . . si?1 A sj . . . sn?1 is called
the Viterbi outside score ?(e). The goal of a k-
best parsing algorithm is to compute the k best
(minimum weight) inside derivations of the edge
(G, 0, n).
We formulate the algorithms in this paper
in terms of prioritized weighted deduction rules
(Shieber et al, 1995; Nederhof, 2003). A prior-
itized weighted deduction rule has the form
?1 : w1, . . . , ?n : wn
p(w1,...,wn)????????? ?0 : g(w1, . . . , wn)
where ?1, . . . , ?n are the antecedent items of the
deduction rule and ?0 is the conclusion item. A
deduction rule states that, given the antecedents
?1, . . . , ?n with weights w1, . . . , wn, the conclu-
sion ?0 can be formed with weight g(w1, . . . , wn)
and priority p(w1, . . . , wn).
2While we present the algorithm specialized to parsing
with a PCFG, this algorithm generalizes to a wide range of
200
VP
s
2
s
3
s
4
s
0
s
2
... s
5
s
n-1
...
VP
VBZ NP
DT NN
s
2
s
3
s
4
VP
G
(a) (b)
(c)
VP
VP NP
s
1
s
2
s
n-1
(d) G
s
0
NN
NP
Figure 1: Representations of the different types of items
used in parsing. (a) An inside edge item I(VP, 2, 5). (b)
An outside edge item O(VP, 2, 5). (c) An inside deriva-
tion item: D(TVP, 2, 5). (d) An outside derivation item:
Q(TGVP, 1, 2, {(NP, 2, n)}. The edges in boldface are fron-
tier edges.
These deduction rules are ?executed? within
a generic agenda-driven algorithm, which con-
structs items in a prioritized fashion. The algo-
rithm maintains an agenda (a priority queue of
items), as well as a chart of items already pro-
cessed. The fundamental operation of the algo-
rithm is to pop the highest priority item ? from the
agenda, put it into the chart with its current weight,
and apply deduction rules to form any items which
can be built by combining ? with items already
in the chart. When the resulting items are either
new or have a weight smaller than an item?s best
score so far, they are put on the agenda with pri-
ority given by p(?). Because all antecedents must
be constructed before a deduction rule is executed,
we sometimes refer to particular conclusion item
as ?waiting? on another item before it can be built.
2.2 A?
A? parsing (Klein and Manning, 2003) is an al-
gorithm for computing the 1-best parse of a sen-
tence. A? operates on items called inside edge
items I(A, i, j), which represent the many pos-
sible inside derivations of an edge (A, i, j). In-
side edge items are constructed according to the
IN deduction rule of Table 1. This deduction rule
constructs inside edge items in a bottom-up fash-
ion, combining items representing smaller edges
I(B, i, k) and I(C, k, j) with a grammar rule r =
A ? B C to form a larger item I(A, i, j). The
weight of a newly constructed item is given by the
sum of the weights of the antecedent items and
the grammar rule r, and its priority is given by
hypergraph search problems as shown in Klein and Manning
(2001).
VP
NP
s
1
s
2
s
3
G
s
0
NN
NP
s
4
s
5
VP
VP
NP
s
1
s
2
s
3
G
s
0
NN
NP
s
4
s
5
VP
VP NN
(a)
(b)
Figure 2: (a) An outside derivation item before expansion at
the edge (VP, 1, 4). (b) A possible expansion of the item in
(a) using the rule VP? VP NN. Frontier edges are marked in
boldface.
its weight plus a heuristic h(A, i, j). For consis-
tent and admissible heuristics h(?), this deduction
rule guarantees that when an inside edge item is
removed from the agenda, its current weight is its
true Viterbi inside score.
The heuristic h controls the speed of the algo-
rithm. It can be shown that an edge e satisfying
?(e) + h(A, i, j) > ?(G, 0, n) will never be re-
moved from the agenda, allowing some edges to
be safely pruned during parsing. The more closely
h(e) approximates the Viterbi outside cost ?(e),
the more items are pruned.
2.3 KA?
The use of inside edge items in A? exploits the op-
timal substructure property of derivations ? since
a best derivation of a larger edge is always com-
posed of best derivations of smaller edges, it is
only necessary to compute the best way of build-
ing a particular inside edge item. When finding
k-best lists, this is no longer possible, since we are
interested in suboptimal derivations.
Thus, KA?, the k-best extension of A?, must
search not in the space of inside edge items,
but rather in the space of inside derivation items
D(TA, i, j), which represent specific derivations
of the edge (A, i, j) using tree TA. However, the
number of inside derivation items is exponential
in the length of the input sentence, and even with
a very accurate heuristic, running A? directly in
this space is not feasible.
Fortunately, Pauls and Klein (2009) show that
with a perfect heuristic, that is, h(e) = ?(e) ?e,
A? search on inside derivation items will only
remove items from the agenda that participate
in the true k-best lists (up to ties). In order
to compute this perfect heuristic, KA? makes
use of outside edge items O(A, i, j) which rep-
resent the many possible derivations of G ?
201
IN??: I(B, i, l) : w1 I(C, l, j) : w2
w1+w2+wr+h(A,i,j)??????????????? I(A, i, j) : w1 + w2 + wr
IN-D?: O(A, i, j) : w1 D(TB , i, l) : w2 D(TC , l, j) : w3
w2+w3+wr+w1??????????? D(TA, i, j) : w2 + w3 + wr
OUT-L?: O(A, i, j) : w1 I(B, i, l) : w2 I(C, l, j) : w3
w1+w3+wr+w2??????????? O(B, i, l) : w1 + w3 + wr
OUT-R?: O(A, i, j) : w1 I(B, i, l) : w2 I(C, l, j) : w3
w1+w2+wr+w3??????????? O(C, l, j) : w1 + w2 + wr
OUT-D?: Q(TGA , i, j,F) : w1 I(B, i, l) : w2 I(C, l, j) : w3
w1+wr+w2+w3+?(F)???????????????? Q(TGB , i, l,FC) : w1 + wr
Table 1: The deduction rules used in this paper. Here, r is the rule A ? B C. A superscript * indicates that the rule is used
in TKA?, and a superscript ? indicates that the rule is used in KA?. In IN-D, the tree TA is rooted at (A, i, j) and has children
TB and TC . In OUT-D, the tree TGB is the tree T
G
A extended at (A, i, j) with rule r, FC is the list F with (C, l, j) prepended,
and ?(F) is
P
e?F ?(e). Whenever the left child I(B, i, l) of an application of OUT-D represents a terminal, the next edge is
removed from F and is used as the new point of expansion.
s1 . . . si A sj+1 . . . sn (see Figure 1(b)).
Outside items are built using the OUT-L and
OUT-R deduction rules shown in Table 1. OUT-
L and OUT-R combine, in a top-down fashion, an
outside edge over a larger span and inside edge
over a smaller span to form a new outside edge
over a smaller span. Because these rules make ref-
erence to inside edge items I(A, i, j), these items
must also be built using the IN deduction rules
from 1-best A?. Outside edge items must thus wait
until the necessary inside edge items have been
built. The outside pass is initialized with the item
O(G, 0, n) when the inside edge item I(G, 0, n) is
popped from the agenda.
Once we have started populating outside scores
using the outside deductions, we can initiate a
search on inside derivation items.3 These items
are built bottom-up using the IN-D deduction rule.
The crucial element of this rule is that derivation
items for a particular edge wait until the exact out-
side score of that edge has been computed. The al-
gorithm terminates when k derivation items rooted
at (G, 0, n) have been popped from the agenda.
3 TKA?
KA? efficiently explores the space of inside
derivation items because it waits for the exact
Viterbi outside cost before building each deriva-
tion item. However, these outside costs and asso-
ciated deduction items are only auxiliary quanti-
ties used to guide the exploration of inside deriva-
tions: they allow KA? to prioritize currently con-
structed inside derivation items (i.e., constructed
derivations of the goal) by their optimal comple-
tion costs. Outside costs are thus only necessary
because we construct partial derivations bottom-
up; if we constructed partial derivations in a top-
down fashion, all we would need to compute opti-
3We stress that the order of computation is entirely speci-
fied by the deduction rules ? we only speak about e.g. ?initi-
ating a search? as an appeal to intuition.
mal completion costs are Viterbi inside scores, and
we could forget the outside pass.
TKA? does exactly that. Inside edge items are
constructed in the same way as KA?, but once the
inside edge item I(G, 0, n) has been discovered,
TKA? begins building partial derivations from the
goal outwards. We replace the inside derivation
items of KA? with outside derivation items, which
represent trees rooted at the goal and expanding
downwards. These items bottom out in a list of
edges called the frontier edges. See Figure 1(d)
for a graphical representation. When a frontier
edge represents a single word in the input, i.e. is
of the form (si, i, i+ 1), we say that edge is com-
plete. An outside derivation can be expanded by
applying a rule to one of its incomplete frontier
edges; see Figure 2. In the same way that inside
derivation items wait on exact outside scores be-
fore being built, outside derivation items wait on
the inside edge items of all frontier edges before
they can be constructed.
Although building derivations top-down obvi-
ates the need for a 1-best outside pass, it raises a
new issue. When building derivations bottom-up,
the only way to expand a particular partial inside
derivation is to combine it with another partial in-
side derivation to build a bigger tree. In contrast,
an outside derivation item can be expanded any-
where along its frontier. Naively building deriva-
tions top-down would lead to a prohibitively large
number of expansion choices.
We solve this issue by always expanding the
left-most incomplete frontier edge of an outside
derivation item. We show the deduction rule
OUT-D which performs this deduction in Fig-
ure 1(d). We denote an outside derivation item as
Q(TGA , i, j,F), where T
G
A is a tree rooted at the
goal with left-most incomplete edge (A, i, j), and
F is the list of incomplete frontier edges exclud-
ing (A, i, j), ordered from left to right. Whenever
the application of this rule ?completes? the left-
202
most edge, the next edge is removed from F and
is used as the new point of expansion. Once all
frontier edges are complete, the item represents a
correctly scored derivation of the goal, explored in
a pre-order traversal.
3.1 Correctness
It should be clear that expanding the left-most in-
complete frontier edge first eventually explores the
same set of derivations as expanding all frontier
edges simultaneously. The only worry in fixing
this canonical order is that we will somehow ex-
plore the Q items in an incorrect order, possibly
building some complete derivation Q?C before a
more optimal complete derivation QC . However,
note that all items Q along the left-most construc-
tion ofQC have priority equal to or better than any
less optimal complete derivation Q?C . Therefore,
when Q?C is enqueued, it will have lower priority
than all Q; Q?C will therefore not be dequeued un-
til all Q ? and hence QC ? have been built.
Furthermore, it can be shown that the top-down
expansion strategy maintains the same efficiency
and optimality guarantees as KA? for all item
types: for consistent heuristics h, the first k en-
tirely complete outside derivation items are the
true k-best derivations (modulo ties), and that only
derivation items which participate in those k-best
derivations will be removed from the queue (up to
ties).
3.2 Implementation Details
Building derivations bottom-up is convenient from
an indexing point of view: since larger derivations
are built from smaller ones, it is not necessary to
construct the larger derivation from scratch. In-
stead, one can simply construct a new tree whose
children point to the old trees, saving both mem-
ory and CPU time.
In order keep the same efficiency when build-
ing trees top-down, a slightly different data struc-
ture is necessary. We represent top-down deriva-
tions as a lazy list of expansions. The top node
TGG is an empty list, and whenever we expand an
outside derivation item Q(TGA , i, j,F) with a rule
r = A ? B C and split point l, the resulting
derivation TGB is a new list item with (r, l) as the
head data, and TGA as its tail. The tree can be re-
constructed later by recursively reconstructing the
parent, and adding the edges (B, i, l) and (C, l, j)
as children of (A, i, j).
3.3 Advantages
Although our algorithm eliminates the 1-best out-
side pass of KA?, in practice, even for k = 104,
the 1-best inside pass remains the overwhelming
bottleneck (Pauls and Klein, 2009), and our modi-
fications leave that pass unchanged.
However, we argue that our implementation is
simpler to specify and implement. In terms of de-
duction rules, our algorithm eliminates the 2 out-
side deduction rules and replaces the IN-D rule
with the OUT-D rule, bringing the total number
of rules from four to two.
The ease of specification translates directly into
ease of implementation. In particular, if high-
quality heuristics are not available, it is often more
efficient to implement the 1-best inside pass as
an exhaustive dynamic program, as in Huang and
Chiang (2005). In this case, one would only need
to implement a single, agenda-based k-best extrac-
tion phase, instead of the 2 needed for KA?.
3.4 Performance
The contribution of this paper is theoretical, not
empirical. We have argued that TKA? is simpler
than TKA?, but we do not expect it to do any more
or less work than KA?, modulo grammar specific
optimizations. Therefore, we simply verify, like
KA?, that the additional work of extracting k-best
lists with TKA? is negligible compared to the time
spent building 1-best inside edges.
We examined the time spent building 100-best
lists for the same experimental setup as Pauls and
Klein (2009).4 On 100 sentences, our implemen-
tation of TKA? constructed 3.46 billion items, of
which about 2% were outside derivation items.
Our implementation of KA? constructed 3.41 bil-
lion edges, of which about 0.1% were outside edge
items or inside derivation items. In other words,
the cost of k-best extraction is dwarfed by the
the 1-best inside edge computation in both cases.
The reason for the slight performance advantage
of KA? is that our implementation of KA? uses
lazy optimizations discussed in Pauls and Klein
(2009), and while such optimizations could easily
be incorporated in TKA?, we have not yet done so
in our implementation.
4This setup used 3- and 6-round state-split grammars from
Petrov et al (2006), the former used to compute a heuristic
for the latter, tested on sentences of length up to 25.
203
4 Conclusion
We have presented TKA?, a simplification to the
KA? algorithm. Our algorithm collapses the 1-
best outside and bottom-up derivation passes of
KA? into a single, top-down pass without sacri-
ficing efficiency or optimality. This reduces the
number of non base-case deduction rules, making
TKA? easier both to specify and implement.
Acknowledgements
This project is funded in part by the NSF under
grant 0643742 and an NSERC Postgraduate Fel-
lowship.
References
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proceedings of the International Work-
shop on Parsing Technologies (IWPT), pages 53?64.
V??ctor M. Jime?nez and Andre?s Marzal. 2000. Com-
putation of the n best parse trees for weighted and
stochastic context-free grammars. In Proceedings
of the Joint IAPR International Workshops on Ad-
vances in Pattern Recognition, pages 183?192, Lon-
don, UK. Springer-Verlag.
Dan Klein and Christopher D. Manning. 2001. Pars-
ing and hypergraphs. In Proceedings of the Interna-
tional Workshop on Parsing Technologies (IWPT),
pages 123?134.
Dan Klein and Christopher D. Manning. 2003. A*
parsing: Fast exact Viterbi parse selection. In
Proceedings of the Human Language Technology
Conference and the North American Association
for Computational Linguistics (HLT-NAACL), pages
119?126.
Mark-Jan Nederhof. 2003. Weighted deductive pars-
ing and Knuth?s algorithm. Computationl Linguis-
tics, 29(1):135?143.
Adam Pauls and Dan Klein. 2009. K-best A* parsing.
In Proccedings of the Association for Computational
Linguistics (ACL).
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and in-
terpretable tree annotation. In Proccedings of the
Association for Computational Linguistics (ACL).
Stuart M. Shieber, Yves Schabes, and Fernando C. N.
Pereira. 1995. Principles and implementation of
deductive parsing. Journal of Logic Programming,
24:3?36.
Frank K. Soong and Eng-Fong Huang. 1991. A tree-
trellis based fast search for finding the n best sen-
tence hypotheses in continuous speech recognition.
In Proceedings of the Workshop on Speech and Nat-
ural Language.
204
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1308?1317,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Gappy Phrasal Alignment by Agreement
Mohit Bansal?
UC Berkeley, CS Division
mbansal@cs.berkeley.edu
Chris Quirk
Microsoft Research
chrisq@microsoft.com
Robert C. Moore
Google Research
robert.carter.moore@gmail.com
Abstract
We propose a principled and efficient phrase-
to-phrase alignment model, useful in machine
translation as well as other related natural lan-
guage processing problems. In a hidden semi-
Markov model, word-to-phrase and phrase-
to-word translations are modeled directly by
the system. Agreement between two direc-
tional models encourages the selection of par-
simonious phrasal alignments, avoiding the
overfitting commonly encountered in unsu-
pervised training with multi-word units. Ex-
panding the state space to include ?gappy
phrases? (such as French ne ? pas) makes the
alignment space more symmetric; thus, it al-
lows agreement between discontinuous align-
ments. The resulting system shows substantial
improvements in both alignment quality and
translation quality over word-based Hidden
Markov Models, while maintaining asymptot-
ically equivalent runtime.
1 Introduction
Word alignment is an important part of statisti-
cal machine translation (MT) pipelines. Phrase
tables containing pairs of source and target lan-
guage phrases are extracted from word alignments,
forming the core of phrase-based statistical ma-
chine translation systems (Koehn et al, 2003).
Most syntactic machine translation systems extract
synchronous context-free grammars (SCFGs) from
aligned syntactic fragments (Galley et al, 2004;
Zollmann et al, 2006), which in turn are de-
rived from bilingual word alignments and syntactic
?Author was a summer intern at Microsoft Research during
this project.
French
English
voudrais voyager par chemin de fer
would like traveling by railroad
ne pas
not
Figure 1: French-English pair with complex word alignment.
parses. Alignment is also used in various other NLP
problems such as entailment, paraphrasing, question
answering, summarization and spelling correction.
A limitation to word-based alignment is undesir-
able. As seen in the French-English example in Fig-
ure 1, many sentence pairs are naturally aligned with
multi-word units in both languages (chemin de fer;
would ? like, where ? indicates a gap). Much work
has addressed this problem: generative models for
direct phrasal alignment (Marcu and Wong, 2002),
heuristic word-alignment combinations (Koehn et
al., 2003; Och and Ney, 2003), models with pseudo-
word collocations (Lambert and Banchs, 2006; Ma
et al, 2007; Duan et al, 2010), synchronous gram-
mar based approaches (Wu, 1997), etc. Most have a
large state-space, using constraints and approxima-
tions for efficient inference.
We present a new phrasal alignment model based
on the hidden Markov framework (Vogel et al,
1996). Our approach is semi-Markov: each state can
generate multiple observations, representing word-
to-phrase alignments. We also augment the state
space to include contiguous sequences. This cor-
responds to phrase-to-word and phrase-to-phrase
alignments. We generalize alignment by agreement
(Liang et al, 2006) to this space, and find that agree-
ment discourages EM from overfitting. Finally, we
make the alignment space more symmetric by in-
cluding gappy (or non-contiguous) phrases. This al-
lows agreement to reinforce non-contiguous align-
1308
f1
f2
f3
e1 e2 e3 f1 f2 f3
e1
e2
e3
Observations? 
?
?
S
ta
tes?
 
HMM(E|F) HMM(F|E)
Figure 2: The model of E given F can represent the phrasal
alignment {e1, e2} ? {f1}. However, the model of F given
E cannot: the probability mass is distributed between {e1} ?
{f1} and {e2} ? {f1}. Agreement of the forward and back-
ward HMM alignments tends to place less mass on phrasal links
and greater mass on word-to-word links.
ments, such English not to French ne ? pas. Prun-
ing the set of allowed phrases preserves the time
complexity of the word-to-word HMM alignment
model.
1.1 Related Work
Our first major influence is that of conditional
phrase-based models. An early approach by Deng
and Byrne (2005) changed the parameterization of
the traditional word-based HMM model, modeling
subsequent words from the same state using a bi-
gram model. However, this model changes only the
parameterization and not the set of possible align-
ments. More closely related are the approaches
of Daume? III and Marcu (2004) and DeNero et
al. (2006), which allow phrase-to-phrase alignments
between the source and target domain. As DeN-
ero warns, though, an unconstrained model may
overfit using unusual segmentations. Interestingly,
the phrase-based hidden semi-Markov model of
Andre?s-Ferrer and Juan (2009) does not seem to
encounter these problems. We suspect two main
causes: first, the model interpolates with Model 1
(Brown et al, 1994), which may help prevent over-
fitting, and second, the model is monotonic, which
screens out many possible alignments. Monotonic-
ity is generally undesirable, though: almost all par-
allel sentences exhibit some reordering phenomena,
even when languages are syntactically very similar.
The second major inspiration is alignment by
agreement by Liang et al (2006). Here, soft inter-
section between the forward (F?E) and backward
(E?F) alignments during parameter estimation pro-
duces better word-to-word correspondences. This
unsupervised approach produced alignments with
incredibly low error rates on French-English, though
only moderate gains in end-to-end machine transla-
tion results. Likely this is because the symmetric
portion of the HMM space contains only single word
to single word links. As shown in Figure 2, in order
to retain the phrasal link f1 ? e1, e2 after agree-
ment, we need the reverse phrasal link e1, e2 v f1
in the backward direction. However, this is not pos-
sible in a word-based HMM where each observa-
tion must be generated by a single state. Agreement
tends to encourage 1-to-1 alignments with very high
precision and but lower recall. As each word align-
ment acts as a constraint on phrase extraction, the
phrase-pairs obtained from those alignments have
high recall and low precision.
2 Gappy Phrasal Alignment
Our goal is to unify phrasal alignment and align-
ment by agreement. We use a phrasal hidden semi-
Markov alignment model, but without the mono-
tonicity requirement of Andre?s-Ferrer and Juan
(2009). Since phrases may be used in both the state
and observation space of both sentences, agreement
during EM training no longer penalizes phrasal links
such as those in Figure 2. Moreover, the benefits of
agreement are preserved: meaningful phrasal links
that are likely in both directions of alignment will be
reinforced, while phrasal links likely in only one di-
rection will be discouraged. This avoids segmenta-
tion problems encountered by DeNero et al (2006).
Non-contiguous sequences of words present an
additional challenge. Even a semi-Markov model
with phrases can represent the alignment between
English not and French ne ? pas in one direction
only. To make the model more symmetric, we ex-
tend the state space to include gappy phrases as
well.1 The set of alignments in each model becomes
symmetric, though the two directions model gappy
phrases differently. Consider not and ne ? pas:
when predicting French given English, the align-
ment corresponds to generating multiple distinct ob-
1We only allow a single gap with one word on each end.
This is sufficient for the vast majority of the gapped phenomena
that we have seen in our training data.
1309
voudrais
voyager
par
chemin
de
fer
wo
ul
d
lik
e
tra
ve
lin
g
by ra
ilr
oa
d
C
would
like
traveling
by
railroad
vo
ud
ra
is
vo
ya
ge
r
pa
r
ch
em
in
de fe
r
no
t
pas
ne
not
ne pa
s
Observations? 
S
ta
tes?
 
Observations? 
S
ta
tes?
 
Figure 3: Example English-given-French and French-given-English alignments of the same sentence pair using the Hidden Semi-
Markov Model (HSMM) for gapped-phrase-to-phrase alignment. It allows the state side phrases (denoted by vertical blocks),
observation side phrases (denoted by horizontal blocks), and state-side gaps (denoted by discontinuous blocks in the same column
connected by a hollow vertical ?bridge?). Note both directions can capture the desired alignment for this sentence pair.
servations from the same state; in the other direction,
the word not is generated by a single gappy phrase
ne ? pas. Computing posteriors for agreement is
somewhat complicated, so we resort to an approx-
imation described later. Exact inference retains a
low-order polynomial runtime; we use pruning to in-
crease speed.
2.1 Hidden Markov Alignment Models
Our model can be seen as an extension of the stan-
dard word-based Hidden Markov Model (HMM)
used in alignment (Vogel et al, 1996). To
ground the discussion, we first review the struc-
ture of that model. This generative model has
the form p(O|S) =
?
A p(A,O|S), where S =
(s1, . . . , sI) ? ?? is a sequence of words from a
vocabulary ?; O = (o1, . . . , oJ) ? ?? is a sequence
from vocabulary ?; and A = (a1, . . . , aJ) is the
alignment between the two sequences. Since some
words are systematically inserted during translation,
the target (state) word sequence is augmented with
a special NULL word. To retain the position of the
last aligned word, the state space contains I copies
of the NULL word, one for each position (Och and
Ney, 2003). The alignment uses positive positions
for words and negative positions for NULL states, so
aj ? {1..I} ? {?1..? I}, and si = NULL if i < 0.
It uses the following generative procedure. First
the length of the observation sequence is selected
based on pl(J |I). Then for each observation posi-
tion, the state is selected based on the prior state: a
null state with probability p0, or a non-null state at
position aj with probability (1 ? p0) ? pj(aj |aj?1)
where pj is a jump distribution. Finally the observa-
tion word oj at that position is generated with prob-
ability pt(oj |saj ), where pt is an emission distribu-
tion:
p(A,O|S) = pl(J |I)
J?
j=1
pj(aj |aj?1)pt(oj |saj )
pj(a|a
?) =
{
(1? p0) ? pd(a? |a?|) a > 0
p0 ? ?(|a|, |a?|) a < 0
We pick p0 using grid search on the development
set, pl is uniform, and the pj and pt are optimized by
EM.2
2.2 Gappy Semi-Markov Models
The HMM alignment model identifies a word-
to-word correspondence between the observation
2Note that jump distances beyond -10 or 10 share a single
parameter to prevent sparsity.
1310
words and the state words. We make two changes
to expand this model. First, we allow contiguous
phrases on the observation side, which makes the
model semi-Markov: at each time stamp, the model
may emit more than one observation word. Next, we
also allow contiguous and gappy phrases on the state
side, leading to an alignment model that can retain
phrasal links after agreement (see Section 4).
The S and O random variables are unchanged.
Since a single state may generate multiple observa-
tion words, we add a new variable K representing
the number of states. K should be less than J , the
number of observations. The alignment variable is
augmented to allow contiguous and non-contiguous
ranges of words. We allow only a single gap, but of
unlimited length. The null state is still present, and
is again represented by negative numbers.
A =(a1, . . . , aK) ? A(I)
A(I) ={(i1, i2, g)|0 < i1 ? i2 ? I,
g ? {GAP, CONTIG}}?
{(?i,?i, CONTIG) | 0 < i ? I}
We add one more random variable to capture the to-
tal number of observations generated by each state.
L ? {(l0, l1, . . . , lK) | 0 = l0 < ? ? ? < lK = J}
The generative model takes the following form:
p(A,L,O|S) =pl(J |I)pf (K|J)
K?
k=1
pj(ak|ak?1)?
pt(lk, o
lk
lk?1+1
|S[ak], lk?1)
First, the length of the observation sequence (J)
is selected, based on the number of words in the
state-side sentence (I). Since it does not affect the
alignment, pl is modeled as a uniform distribution.
Next, we pick the total number of states to use (K),
which must be less than the number of observations
(J). Short state sequences receive an exponential
penalty: pf (K|J) ? ?(J?K) if 0 ? K ? J , or 0
otherwise. A harsh penalty (small positive value of
?) may prevent the systematic overuse of phrases.3
3We found that this penalty was crucial to prevent overfitting
in independent training. Joint training with agreement made it
basically unnecessary.
Next we decide the assignment of each state.
We retain the first-order Markov assumption: the
selection of each state is conditioned only on the
prior state. The transition distribution is identical
to the word-based HMM for single word states. For
phrasal and gappy states, we jump into the first word
of that state, and out of the last word of that state,
and then pay a cost according to how many words
are covered within that state. If a = (i1, i2, g), then
the beginning word of a is F (a) = i1, the end-
ing word is L(a) = i2, and the length N(a) is 2
for gapped states, 0 for null states, and last(a) ?
first(a) + 1 for all others. The transition probabil-
ity is:
pj(a|a
?) =
?
??
??
p0 ? ?(|F (a)|, |L(a?)|) if F (a) < 0
(1? p0)pd(F (a)? |L(a?)|)?
pn(N(a)) otherwise
where pn(c) ? ?c is an exponential distribution. As
in the word HMM case, we use a mixture parameter
p0 to determine the likelihood of landing in a NULL
state. The position of that NULL state remembers the
last position of the prior state. For non-null words,
we pick the first word of the state according to the
distance from the last word of the prior state. Finally,
we pick a length for that final state according to an
exponential distribution: values of ? less than one
will penalize the use of phrasal states.
For each set of state words, we maintain an emis-
sion distribution over observation word sequences.
Let S[a] be the set of state words referred to by
the alignment variable a. For example, the English
given French alignment of Figure 3 includes the fol-
lowing state word sets:
S[(2, 2, CONTIG)] = voudrais
S[(1, 3, GAP)] = ne ? pas
S[(6, 8, CONTIG)] = chemin de fer
For the emission distribution we keep a multinomial
over observation phrases for each set of state words:
p(l, oll? |S[a], l
?) ? c(oll? |S[a])
In contrast to the approach of Deng and Byrne
(2005), this encourages greater consistency across
instances, and more closely resembles the com-
monly used phrasal translation models.
1311
We note in passing that pf (K|J) may be moved
inside the product: pf (K|J) ? ?(J?K) =
?K
k=1 ?
(lk?lk?1?1). The following form derived us-
ing the above rearrangement is helpful during EM.
p(A,L,O|S) ?
K?
k=1
pj(ak|ak?1)?
pt(lk, o
lk
lk?1+1
|S[ak], lk?1)?
?(lk?lk?1?1)
where lk ? lk?1 ? 1 is the length of the observation
phrase emitted by state S[ak].
2.3 Minimality
At alignment time we focus on finding the minimal
phrase pairs, under the assumption that composed
phrase pairs can be extracted in terms of these min-
imal pairs. We are rather strict about this, allowing
only 1 ? k and k ? 1 phrasal alignment edges
(or links). This should not cause undue stress, since
edges of the form 2 ? 3 (say e1e2 ? f1f2f3) can
generally be decomposed into 1 ? 1 ? 1 ? 2 (i.e.,
e1 ? f1 ? e2 ? f2f3), etc. However, the model
does not require this to be true: we will describe re-
estimation for unconstrained general models, but use
the limited form for word alignment.
3 Parameter Estimation
We use Expectation-Maximization (EM) to estimate
parameters. The forward-backward algorithm effi-
ciently computes posteriors of transitions and emis-
sions in the word-based HMM. In a standard HMM,
emission always advances the observation position
by one, and the next transition is unaffected by
the emission. Neither of these assumptions hold
in our model: multiple observations may be emit-
ted at a time, and a state may cover multiple state-
side words, which affects the outgoing transition. A
modified dynamic program computes posteriors for
this generalized model.
The following formulation of the forward-
backward algorithm for word-to-word alignment is
a good starting point. ?[x, 0, y] indicates the total
mass of paths that have just transitioned into state y
at observation x but have not yet emitted; ?[x, 1, y]
represents the mass after emission but before subse-
quent transition. ? is defined similarly. (We omit
NULL states for brevity; the extension is straightfor-
ward.)
?[0, 0, y] = pj(y|INIT)
?[x, 1, y] = ?[x, 0, y] ? pt(ox|sy)
?[x, 0, y] =
?
y?
?[x? 1, 1, y?] ? pj(y|y
?)
?[n, 1, y] = 1
?[x, 0, y] = pt(ox|sy) ? ?[x, 1, y]
?[x, 1, y] =
?
y?
pj(y
?|y) ? ?[x+ 1, 0, y?]
Not only is it easy to compute posteriors of both
emissions (?[x, 0, y]pt(ox|sy)?[x, 1, y]) and transi-
tions (?[x, 1, y]pj(y?|y)?[x+ 1, 0, y?]) with this for-
mulation, it also simplifies the generalization to
complex emissions. We update the emission forward
probabilities to include a search over the possible
starting points in the state and observation space:
?[0, 0, y] =pj(y|INIT)
?[x, 1, y] =
?
x?<x,y??y
?[x?, 0, y?] ? EMIT(x? : x, y? : y)
?[x, 0, y] =
?
y?
?[x? 1, 1, y?] ? pj(y|y
?)
?[n, 1, y] =1
?[x?, 0, y?] =
?
x?<x,y??y
EMIT(x? : x, y? : y) ? ?[x, 1, y]
?[x, 1, y] =
?
y?
pj(y
?|y) ? ?[x+ 1, 0, y?]
Phrasal and gapped emissions are pooled into EMIT:
EMIT(w : x, y : z) =pt(o
x
w|s
z
y) ? ?
z?y+1 ? ?x?w+1+
pt(o
x
w|sy ? sz) ? ?
2 ? ?x?w+1
The transition posterior is the same as above. The
emission is very similar: the posterior probability
that oxw is aligned to s
z
y is proportional to ?[w, 0, y] ?
pt(oxw|s
z
y) ??
z?y+1 ??x?w+1 ??[x, 1, z]. For a gapped
phrase, the posterior is proportional to ?[w, 0, y] ?
pt(oxw|sy ? sz) ? ?
2 ? ?x?w+1 ? ?[x, 1, z].
Given an inference procedure for computing pos-
teriors, unsupervised training with EM follows im-
mediately. We use a simple maximum-likelihood
update of the parameters using expected counts
based on the posterior distribution.
1312
4 Alignment by Agreement
Following Liang et al (2006), we quantify agree-
ment between two models as the probability that the
alignments produced by the two models agree on the
alignment z of a sentence pair x = (S,O):
?
z
p1(z|x; ?1)p2(z|x; ?2)
To couple the two models, the (log) probability of
agreement is added to the standard log-likelihood
objective:
max
?1,?2
?
x
[
log p1(x; ?1) + log p2(x; ?2)+
log
?
z
p1(z|x; ?1)p2(z|x; ?2)
]
We use the heuristic estimator from Liang et al
(2006), letting q be a product of marginals:
E : q(z; x) :=
?
z?z
p1(z|x; ?1)p2(z|x; ?2)
where each pk(z|x; ?k) is the posterior marginal of
some edge z according to each model. Such a
heuristic E step computes the marginals for each
model separately, then multiplies the marginals cor-
responding to the same edge. This product of
marginals acts as the approximation to the posterior
used in the M step for each model. The intuition is
that if the two models disagree on a certain edge z,
then the marginal product is small, hence that edge
is dis-preferred in each model.
Contiguous phrase agreement. It is simple to
extend agreement to alignments in the absence of
gaps. Multi-word (phrasal) links are assigned some
posterior probability in both models, as shown in the
example in Figure 3, and we multiply the posteriors
of these phrasal links just as in the single word case.4
?F?E(fi, ej) := ?E?F (ej , fi)
:= [?F?E(fi, ej)? ?E?F (ej , fi)]
4Phrasal correspondences can be represented in multiple
ways: multiple adjacent words could be generated from the
same state either using one semi-Markov emission, or using
multiple single word emissions followed by self-jumps. Only
the first case is reinforced through agreement, so the latter is
implicitly discouraged. We explored an option to forbid same-
state transitions, but found it made little difference in practice.
Gappy phrase agreement. When we introduce
gappy phrasal states, agreement becomes more chal-
lenging. In the forward direction F?E, if we have a
gappy state aligned to an observation, say fi ? fj ?
ek, then its corresponding edge in the backward di-
rection E?F would be ek v fi ? fj . How-
ever, this is represented by two distinct and unre-
lated emissions. Although it is possible the compute
the posterior probability of two non-adjacent emis-
sions, this requires running a separate dynamic pro-
gram for each such combination to sum the mass be-
tween these emissions. For the sake of efficiency
we resort to an approximate computation of pos-
terior marginals using the two word-to-word edges
ek v fi and ek v fj .
The forward posterior ?F?E for edge fi ? fj ?
ek is multiplied with the min of the backward pos-
teriors of the edges ek v fi and ek v fj .
?F?E(fi ? fj , ek) := ?F?E(fi ? fj , ek)?
min
{
?E?F (ek, fi), ?E?F (ek, fj)
}
Note that this min is an upper bound on the desired
posterior of edge ek v fi ? fj , since every path
that passes through ek v fi and ek v fj must pass
through ek v fi, therefore the posterior of ek v
fi ? fj is less than that of ek v fi, and likewise less
than that of ek v fj .
The backward posteriors of the edges ek v fi and
ek v fj are also mixed with the forward posteriors
of the edges to which they correspond.
?E?F (ek, fi) := ?E?F (ek, fi)?
[
?F?E(fi, ek)+
?
h<i<j
{
?F?E(fh ? fi, ek) + ?F?E(fi ? fj , ek)
}
]
5 Pruned Lists of ?Allowed? Phrases
To identify contiguous and gapped phrases that are
more likely to lead to good alignments, we use word-
to-word HMM alignments from the full training data
in both directions (F?E and E?F). We collect ob-
servation phrases of length 2 toK aligned to a single
state, i.e. oji ? s, to add to a list of allowed phrases.
For gappy phrases, we find all non-consecutive ob-
servation pairs oi and oj such that: (a) both are
1313
aligned to the same state sk, (b) state sk is aligned to
only these two observations, and (c) at least one ob-
servation between oi and oj is aligned to a non-null
state other than sk. These observation phrases are
collected from F?E and E?F models to build con-
tiguous and gappy phrase lists for both languages.
Next, we order the phrases in each contiguous list
using the discounted probability:
p?(o
j
i ? s|o
j
i ) =
max(0, count(oji ? s)? ?)
count(oji )
where count(oji ? s) is the count of occurrence of
the observation-phrase oji , all aligned to some sin-
gle state s, and count(oji ) is the count of occur-
rence of the observation phrase oji , not all necessar-
ily aligned to a single state. Similarly, we rank the
gappy phrases using the discounted probability:
p?(oi ? oj ? s|oi ? oj) =
max(0, count(oi ? oj ? s)? ?)
count(oi ? oj)
where count(oi ? oj ? s) is the count of occur-
rence of the observations oi and oj aligned to a sin-
gle state s with the conditions mentioned above, and
count(oi ? oj) is the count of general occurrence of
the observations oi and oj in order. We find that 200
gappy phrases and 1000 contiguous phrases works
well, based on tuning with a development set.
6 Complexity Analysis
Let m be the length of the state sentence S and n
be the length of the observation sentence O. In IBM
Model 1 (Brown et al, 1994), with only a translation
model, we can infer posteriors or max alignments
in O(mn). HMM-based word-to-word alignment
model (Vogel et al, 1996) adds a distortion model,
increasing the complexity to O(m2n).
Introducing phrases (contiguous) on the observa-
tion side, we get a HSMM (Hidden Semi-Markov
Model). If we allow phrases of length no greater
than K, then the number of observation types
rises from n to Kn for an overall complexity of
O(m2Kn). Introducing state phrases (contiguous)
with length ? K grows the number of state types
from m to Km. Complexity further increases to
O((Km)2Kn) = O(K3m2n).
Finally, when we introduce gappy state phrases of
the type si ? sj , the number of such phrases is
O(m2), since we may choose a start and end point
independently. Thus, the total complexity rises to
O((Km + m2)2Kn) = O(Km4n). Although this
is less than the O(n6) complexity of exact ITG (In-
version Transduction Grammar) model (Wu, 1997),
a quintic algorithm is often quite slow.
The pruned lists of allowed phrases limit this
complexity. The model is allowed to use observa-
tion (contiguous) and state (contiguous and gappy)
phrases only from these lists. The number of
phrases that match any given sentence pair from
these pruned lists is very small (? 2 to 5). If the
number of phrases in the lists that match the obser-
vation and state side of a given sentence pair are
small constants, the complexity remains O(m2n),
equal to that of word-based models.
7 Results
We evaluate our models based on both word align-
ment and end-to-end translation with two language
pairs: English-French and English-German. For
French-English, we use the Hansards NAACL 2003
shared-task dataset, which contains nearly 1.1 mil-
lion training sentence pairs. We also evaluated
on German-English Europarl data from WMT2010,
with nearly 1.6 million training sentence pairs. The
model from Liang et al (2006) is our word-based
baseline.
7.1 Training Regimen
Our training regimen begins with both the forward
(F?E) and backward (E?F) iterations of Model 1
run independently (i.e. without agreement). Next,
we train several iterations of the forward and back-
ward word-to-word HMMs, again with independent
training. We do not use agreement during word
alignment since it tends to produce sparse 1-1 align-
ments, which in turn leads to low phrase emission
probabilities in the gappy model.
Initializing the emission probabilities of the semi-
Markov model is somewhat complicated, since the
word-based models do not assign any mass to
the phrasal or gapped configurations. Therefore
we use a heuristic method. We first retrieve the
Viterbi alignments of the forward and backward
1314
word-to-word HMM aligners. For phrasal corre-
spondences, we combine these forward and back-
ward Viterbi alignments using a common heuris-
tic (Union, Intersection, Refined, or Grow-Diag-
Final), and extract tight phrase-pairs (no unaligned
words on the boundary) from this alignment set.
We found that Grow-Diag-Final was most effective
in our experiments. The counts gathered from this
phrase extraction are used to initialize phrasal trans-
lation probabilities. For gappy states in a forward
(F?E) model, we use alignments from the back-
ward (E?F) model. If a state sk is aligned to two
non-consecutive observations oi and oj such that sk
is not aligned to any other observation, and at least
one observation between oi and oj is aligned to a
non-null state other than sk, then we reverse this
link to get oi ? oj ? sk and use it as a gapped-
state-phrase instance for adding fractional counts.
Given these approximate fractional counts, we per-
form a standard MLE M-step to initialize the emis-
sion probability distributions. The distortion proba-
bilities from the word-based model are used without
changes.
7.2 Alignment Results (F1)
The validation and test sentences have been hand-
aligned (see Och and Ney (2003)) and are marked
with both sure and possible alignments. For French-
English, following Liang et al (2006), we lowercase
all words, and use the validation set plus the first
100 test sentences as our development set and the
remaining 347 test-sentences as our test-set for fi-
nal F1 evaluation.5 In German-English, we have a
development set of 102 sentences, and a test set of
258 sentences, also annotated with a set of sure and
possible alignments. Given a predicted alignmentA,
precision and recall are computed using sure align-
ments S and possible alignments P (where S ? P )
as in Och and Ney (2003):
Precision =
|A ? P |
|A|
? 100%
Recall =
|A ? S|
|S|
? 100%
5We report F1 rather than AER because AER appears not to
correlate well with translation quality.(Fraser and Marcu, 2007)
Language pair Word-to-word Gappy
French-English 34.0 34.5
German-English 19.3 19.8
Table 2: BLEU results on German-English and French-English.
AER =
(
1?
|A ? S|+ |A ? P |
|A|+ |S|
)
? 100%
F1 =
2? Precision?Recall
Precision+Recall
? 100%
Many free parameters were tuned to optimize
alignment F1 on the development set, including the
number of iterations of each Model 1, HMM, and
Gappy; the NULL weight p0, the number of con-
tiguous and gappy phrases to include, and the max-
imum phrase length. Five iterations of all models,
p0 = 0.3, using the top 1000 contiguous phrases
and the top 200 gappy phrases, maximum phrase
length of 5, and penalties ? = ? = 1 produced
competitive results. Note that by setting ? and ? to
one, we have effectively removed the penalty alto-
gether without affecting our results. In Table 1 we
see a consistent improvement with the addition of
contiguous phrases, and some additional gains with
gappy phrases.
7.3 Translation Results (BLEU)
We assembled a phrase-based system from the align-
ments (using only contiguous phrases consistent
with the potentially gappy alignment), with 4 chan-
nel models, word and phrase count features, dis-
tortion penalty, lexicalized reordering model, and a
5-gram language model, weighted by MERT. The
same free parameters from above were tuned to opti-
mize development set BLEU using grid search. The
improvements in Table 2 are encouraging, especially
as a syntax-based or non-contiguous phrasal system
(Galley and Manning, 2010) may benefit more from
gappy phrases.
8 Conclusions and Future Work
We have described an algorithm for efficient unsu-
pervised alignment of phrases. Relatively straight-
forward extensions to the base HMM allow for ef-
ficient inference, and agreement between the two
1315
Data Decoding method Word-to-word +Contig phrases +Gappy phrases
FE 10K Viterbi 89.7 90.6 90.3
FE 10K Posterior ? 0.1 90.1 90.4 90.7
FE 100K Viterbi 93.0 93.6 93.8
FE 100K Posterior ? 0.1 93.1 93.7 93.8
FE All Viterbi 94.1 94.3 94.3
FE All Posterior ? 0.1 94.2 94.4 94.5
GE 10K Viterbi 76.2 79.6 79.7
GE 10K Posterior ? 0.1 76.7 79.3 79.3
GE 100K Viterbi 81.0 83.0 83.2
GE 100K Posterior ? 0.1 80.7 83.1 83.4
GE All Viterbi 83.0 85.2 85.6
GE All Posterior ? 0.1 83.7 85.3 85.7
Table 1: F1 scores of automatic word alignments, evaluated on the test set of the hand-aligned sentence pairs.
models prevents EM from overfitting, even in the ab-
sence of harsh penalties. We also allow gappy (non-
contiguous) phrases on the state side, which makes
agreement more successful but agreement needs ap-
proximation of posterior marginals. Using pruned
lists of good phrases, we maintain complexity equal
to the baseline word-to-word model.
There are several steps forward from this point.
Limiting the gap length also prevents combinato-
rial explosion; we hope to explore this in future
work. Clearly a translation system that uses discon-
tinuous mappings at runtime (Chiang, 2007; Gal-
ley and Manning, 2010) may make better use of
discontinuous alignments. This model can also be
applied at the morpheme or character level, allow-
ing joint inference of segmentation and alignment.
Furthermore the state space could be expanded and
enhanced to include more possibilities: states with
multiple gaps might be useful for alignment in lan-
guages with template morphology, such as Arabic or
Hebrew. More exploration in the model space could
be useful ? a better distortion model might place a
stronger distribution on the likely starting and end-
ing points of phrases.
Acknowledgments
We would like to thank the anonymous reviewers for
their helpful suggestions. This project is funded by
Microsoft Research.
References
Jesu?s Andre?s-Ferrer and Alfons Juan. 2009. A phrase-
based hidden semi-Markov approach to machine trans-
lation. In Proceedings of EAMT.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1994. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, 19:263?311.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics.
Hal Daume? III and Daniel Marcu. 2004. A phrase-based
HMM approach to document/abstract alignment. In
Proceedings of EMNLP.
John DeNero, Dan Gillick, James Zhang, and Dan Klein.
2006. Why generative phrase models underperform
surface heuristics. In Proceedings of ACL.
Yonggang Deng and William Byrne. 2005. HMM word
and phrase alignment for statistical machine transla-
tion. In Proceedings of HLT-EMNLP.
Xiangyu Duan, Min Zhang, and Haizhou Li. 2010.
Pseudo-word for phrase-based machine translation. In
Proceedings of ACL.
Alexander Fraser and Daniel Marcu. 2007. Measuring
word alignment quality for statistical machine transla-
tion. Computational Linguistics, 33(3):293?303.
Michel Galley and Christopher D. Manning. 2010. Ac-
curate non-hierarchical phrase-based translation. In
HLT/NAACL.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In Pro-
ceedings of HLT-NAACL.
Philipp Koehn, Franz Och, and Daniel Marcu. 2003. Sta-
tistical Phrase-Based Translation. In Proceedings of
HLT-NAACL.
Patrik Lambert and Rafael Banchs. 2006. Grouping
multi-word expressions according to part-of-speech in
1316
statistical machine translation. In Proc. of the EACL
Workshop on Multi-Word-Expressions in a Multilin-
gual Context.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of HLT-NAACL.
Yanjun Ma, Nicolas Stroppa, and Andy Way. 2007.
Boostrapping word alignment via word packing. In
Proceedings of ACL.
Daniel Marcu and Daniel Wong. 2002. A phrase-based,
joint probability model for statistical machine transla-
tion. In Proceedings of EMNLP.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29:19?51.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical trans-
lation. In Proceedings of COLING.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23:377?404.
Andreas Zollmann, Ashish Venugopal, and Stephan Vo-
gel. 2006. Syntax augmented machine translation via
chart parsing. In Processings of the Statistical Ma-
chine Translation Workshop at NAACL.
1317
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1669?1679,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Lightly Supervised Learning of Procedural Dialog Systems
Svitlana Volkova
CLSP
Johns Hopkins University
Baltimore, MD
svitlana@jhu.edu
Pallavi Choudhury, Chris Quirk, Bill Dolan
NLP Group
Microsoft Research
Redmond, WA
pallavic,chrisq,
billdol@microsoft.com
Luke Zettlemoyer
Computer Science and Engineering
University of Washington
Seattle, WA
lsz@cs.washington.edu
Abstract
Procedural dialog systems can help users
achieve a wide range of goals. However,
such systems are challenging to build,
currently requiring manual engineering of
substantial domain-specific task knowl-
edge and dialog management strategies. In
this paper, we demonstrate that it is pos-
sible to learn procedural dialog systems
given only light supervision, of the type
that can be provided by non-experts. We
consider domains where the required task
knowledge exists in textual form (e.g., in-
structional web pages) and where system
builders have access to statements of user
intent (e.g., search query logs or dialog
interactions). To learn from such tex-
tual resources, we describe a novel ap-
proach that first automatically extracts task
knowledge from instructions, then learns a
dialog manager over this task knowledge
to provide assistance. Evaluation in a Mi-
crosoft Office domain shows that the indi-
vidual components are highly accurate and
can be integrated into a dialog system that
provides effective help to users.
1 Introduction
Procedural dialog systems aim to assist users
with a wide range of goals. For example, they
can guide visitors through a museum (Traum et
al., 2012; Aggarwal et al, 2012), teach students
physics (Steinhauser et al, 2011; Dzikovska et
al., 2011), or enable interaction with a health care
U: ?I want to add page numbers and a title?
S: ?Top or Bottom of the page??
U: ?Top?
S: ?Please select page design from the tem-
plates? (*System shows drop down menu*)
U: *User selects from menu*
S: ?Enter header or footer content?
U: ?C.V.?
S: ?Task completed.?
Figure 1: An example dialog interaction between
a system (S) and user (U) that can be automatically
achieved by learning from instructional web page
and query click logs.
system (Morbini et al, 2012; Rizzo et al, 2011).
However, such systems are challenging to build,
currently requiring expensive, expert engineering
of significant domain-specific task knowledge and
dialog management strategies.
In this paper, we present a new approach for
learning procedural dialog systems from task-
oriented textual resources in combination with
light, non-expert supervision. Specifically, we as-
sume access to task knowledge in textual form
(e.g., instructional web pages) and examples of
user intent statements (e.g., search query logs or
dialog interactions). Such instructional resources
are available in many domains, ranging from
recipes that describe how to cook meals to soft-
ware help web pages that describe how to achieve
goals by interacting with a user interface.1
1ehow.com,wikianswers.com
1669
There are two key challenges: we must (1)
learn to convert the textual knowledge into a us-
able form and (2) learn a dialog manager that pro-
vides robust assistance given such knowledge. For
example, Figure 1 shows the type of task assis-
tance that we are targeting in the Microsoft Office
setting, where the system should learn from web
pages and search query logs. Our central contribu-
tion is to show that such systems can be built with-
out the help of knowledge engineers or domain ex-
perts. We present new approaches for both of our
core problems. First, we introduce a method for
learning to map instructions to tree representations
of the procedures they describe. Nodes in the tree
represent points of interaction with the questions
the system can ask the user, while edges represent
user responses. Next, we present an approach that
uses example user intent statements to simulate di-
alog interactions, and learns how to best map user
utterances to nodes in these induced dialog trees.
When combined, these approaches produce a com-
plete dialog system that can engage in conversa-
tions by automatically moving between the nodes
of a large collection of induced dialog trees.
Experiments in the Windows Office help do-
main demonstrate that it is possible to build an
effective end-to-end dialog system. We evaluate
the dialog tree construction and dialog manage-
ment components in isolation, demonstrating high
accuracy (in the 80-90% range). We also conduct
a small-scale user study which demonstrates that
users can interact productively with the system,
successfully completing over 80% of their tasks.
Even when the system does fail, it often does so in
a graceful way, for example by asking redundant
questions but still reaching the goal within a few
additional turns.
2 Overview of Approach
Our task-oriented dialog system understands user
utterances by mapping them to nodes in dialog
trees generated from instructional text. Figure 2
shows an example of a set of instructions and the
corresponding dialog tree. This section describes
the problems that we must solve to enable such in-
teractions, and outlines our approach for each.
Knowledge Acquisition We extract task knowl-
edge from instructional text (e.g., Figure 2, left)
that describes (1) actions to be performed, such
as clicking a button, and (2) places where input
is needed from the user, for example to enter the
contents of the footer or header they are trying to
create. We aim to convert this text into a form that
will enable a dialog system to automatically assist
with the described task. To this end, we construct
dialog trees (e.g., Figure 2, right) with nodes to
represent entire documents (labeled as topics t),
nodes to represent user goals or intents (g), and
system action nodes (a) that enable execution of
specific commands. Finally, each node has an as-
sociated system action as, which can prompt user
input (e.g., with the question ?Top or bottom of
the page??) and one or more user actions au that
represent possible responses. All nodes connect
to form a tree structure that follows the workflow
described in the document. Section 3 presents a
scalable approach for inducing dialog trees.
Dialog Management To understand user intent
and provide task assistance, we need a dialog man-
agement approach that specifies what the system
should do and say. We adopt a simple approach
that at all times maintains an index into a node in
a dialog tree. Each system utterance is then simply
the action as for that node. However, the key chal-
lenge comes in interpreting user utterances. After
each user statement, we must automatically up-
date our node index. At any point, the user can
state a general goal (e.g., ?I want to add page num-
bers?), refine their goal (e.g., ?in a footer?), or both
(e.g.,?I want to add page numbers in the footer?).
Users can also change their goals in the process of
completing the tasks.
We develop a simple classification approach
that is robust to these different types of user behav-
ior. Specifically, we learn classifiers that, given the
dialog interaction history, predict how to pick the
next tree node from the space of all nodes in the di-
alog trees that define the task knowledge. We iso-
late two specific cases, classifying initial user ut-
terances (Section 4) and classifying all subsequent
utterances (Section 5). This approach allows us to
isolate the difference in language for the two cases,
and bias the second case to prefer tree nodes near
the current one. The resulting approach allows for
significant flexibility in traversing the dialog trees.
Data and Evaluation We collected a large set of
such naturally-occurring web search queries that
resulted in a user click on a URL in the Microsoft
Office help domain.2 We found that queries longer
that 4-5 words often resembled natural language
utterances that could be used for dialog interac-
2http://office.microsoft.com
1670
Figure 2: An example instructional text paired with a section of the corresponding dialog tree.
tions, for example how do you add borders, how
can I add a footer, how to insert continuous page
numbers, and where is the header and footer.
We also collected instructional texts from the
web pages that describe how to solve 76 of the
most pressing user goals, as indicated by query
click log statistics. On average 1,000 user queries
were associated with each goal. To some extent
clickthroughs can be treated as a proxy for user
frustration; popular search targets probably repre-
sent user pain points.
3 Building Dialog Trees from
Instructions
Our first problem is to convert sets of instructions
for user goals to dialog trees, as shown in Figure
2. These goals are broadly grouped into topics
(instructional pages). In addition, we manually
associate each node in a dialog tree with a train-
ing set of 10 queries. For the 76 goals (246 in-
structions) in our data, this annotation effort took
a single annotator a total of 41 hours. Scaling this
approach to the entire Office help domain would
require a focused annotation effort. Crucially,
though, this annotation work can be carried out by
non-specialists, and could even be crowdsourced
(Bernstein et al, 2010).
Problem Definition As input, we are given in-
structional text (p1 . . . pn), comprised of topics
(t1 . . . tn) describing:
(1) high-level user intents (e.g., t1 ? ?add and for-
mat page numbers?)
(2) goals (g1, . . . , gk) that represent more spe-
cific user intents (e.g., g1 ? ?add header or
footer content to a preformatted page number
design?, g2 ? ?place the page number in the
side margin of the page?).
Given instructional text p1 . . . pn and queries
q1 . . . qm per topic ti, our goals are as follows:
Figure 3: Relationships between user queries and
OHP with goals, instructions and dialog trees.
- for every instructional page pi extract a topic
ti and a set of goals g1 . . . gk;
- for every goal gj for a topic ti, extract a set of
instructions i1 . . . il;
- from topics, goals and instructions, construct
dialog trees f1 . . . fn (one dialog tree per
topic). Classify instructions to user interac-
tion types thereby identifying system action
nodes a1s . . . als. Transitions between these
nodes are the user actions a1u . . . alu.
Figure 2 (left) presents an example of a topic
extracted from the help page, and a set of goals
and instructions annotated with user action types.
In the next few sections of the paper, we out-
line an overall system component design demon-
strating how queries and topics are mapped to the
dialog trees in Figure 3. The figure shows many-
to-one relations between queries and topics, one-
to-many relations between topics and goals, goals
and instructions, and one-to-one relations between
topics and dialog trees.
User Action Classification We aim to classify
instructional text (i1 . . . il) for every goal gj in the
decision tree into four categories: binary, selec-
tion, input or none.
Given a single instruction i with category au,
we use a log-linear model to represent the distri-
1671
bution over the space of possible user actions. Un-
der this representation, the user action distribution
is defined as:
p(au|i, ?) =
e???(au,i)?
a?u e
???(au,i) , (1)
where ?(au, i) ? Rn is an n-dimensional fea-
ture representation and ~? is a parameter vector we
aim to learn. Features are indicator functions of
properties of the instructions and a particular class.
For smoothing we use a zero mean, unit variance
Gaussian prior (0, 1) that penalizes ~? for drifting
too far from the mean, along with the following
optimization function:
log p(Au, ?|I) = log p(Au|I, ?)? log p(?) =
=
?
au,i?(Au,I)
p(au|i, ?)?
?
i
(? ? ?i)2
2?2i
+ k
(2)
We use L-BFGS (Nocedal and Wright, 2000) as
an optimizer.
Experimental Setup As described in Section 2,
our dataset consists of 76 goals grouped into 30
topics (average 2-3 goals per topic) for a total of
246 instructions (average 3 instructions per goal).
We manually label all instructions with user ac-
tion au categories. The distribution over cate-
gories is binary=14, input=23, selection=80 and
none=129. The data is skewed towards the cat-
egories none and selection. Many instruction do
not require any user input and can be done auto-
matically, e.g., ?On the Insert tab, in the Header
and Footer group, click Page Number?. The ex-
ample instructions with corresponding user action
labels are shown in Figure 2 (left) . Finally, we di-
vide the 246 instructions into 2 sets: 80% training
and 20% test, 199 and 47 instructions respectively.
Results We apply the user action type classifi-
cation model described in the Eq.1 and Eq.2 to
classify instructions from the test set into 4 cate-
gories. In Table 1 we report classification results
for 2 baselines: a majority class and heuristic-
based approach, and 2 models with different fea-
ture types: ngrams and ngrams + stems. For a
heuristic baseline, we use simple lexical clues to
classify instructions (e.g., X or Y for binary, select
Y for selection and type X, insert Y for input). Ta-
ble 1 summarizes the results of mapping instruc-
tional text to user actions.
Features # Features Accuracy
Baseline 1: Majority ? 0.53
Baseline 2: Heuristic ? 0.64
Ngrams 10,556 0.89
Ngrams + Stems 12,196 0.89
Table 1: Instruction classification results.
Building the Dialog Trees Based on the classi-
fied user action types, we identify system actions
a1s . . . als which correspond to 3 types of user ac-
tions a1s . . . als (excluding none type) for every goal
in a topic ti. This involved associating all words
from an instruction il with a system action als. Fi-
nally, for every topic we automatically construct a
dialog tree as shown in Figure 2 (right). The dia-
log tree includes a topic t1 with goals g1 . . . g4, and
actions (user actions au and system actions as).
Definition 1. A dialog tree encodes a user-system
dialog flow about a topic ti represented as a di-
rected unweighted graph fi = (V,E) where top-
ics, goals and actions are nodes of correspond-
ing types {t1 . . . tn}, {g1 . . . gk}, {a1 . . . al} ? V .
There is a hierarchical dependency between topic,
goal and action nodes. User interactions are
represented by edges ti ? {g1 . . . gk}, a1u =
(gj , a1) . . . alu = (ak?1, ak) ? E.
For example, in the dialog tree in Figure 2 there
is a relation t1 ? g4 between the topic t1 ?add
and format page numbers? and the goal g4 ?in-
clude page of page X of Y with the page number?.
Moreover, in the dialog tree, the topic level node
has one index i ? [1..n], where n is the number
of topics. Every goal node includes information
about its parent (topic) node and has double index
i.j, where j ? [1..k]. Finally, action nodes include
information about their parent (goal) and grand-
parent (topic) nodes and have triple index i.j.z,
where z ? [1..l].
4 Understanding Initial Queries
This section presents a model for classifying ini-
tial user queries to nodes in a dialog tree, which
allows for a variety of different types of queries.
They can be under-specified, including informa-
tion about a topic only (e.g., ?add or delete page
numbers?); partially specified, including informa-
tion about a goal (e.g., ?insert page number?); or
over-specified, including information about an ac-
tion ( e.g., ?page numbering at bottom page?.)
1672
Figure 4: Mapping initial user queries to the nodes
on different depth in a dialog tree.
Problem Definition Given an initial query, the
dialog system initializes to a state s0, searches for
the deepest relevant node given a query, and maps
the query to a node on a topic ti, goal gj or action
ak level in the dialog tree fi, as shown in Figure 4.
More formally, as input, we are given automati-
cally constructed dialog trees f1 . . . fn for instruc-
tional text (help pages) annotated with topic, goal
and action nodes and associated with system ac-
tions as shown in Figure 2 (right). From the query
logs, we associate queries with each node type:
topic qt, goal qg and action qa. This is shown in
Figure 2 and 4. We join these dialog trees repre-
senting different topics into a dialog network by
introducing a global root. Within the network,
we aim to find (1) an initial dialog state s0 that
maximizes the probability of state given a query
p(s0|q, ?); and (2) the deepest relevant node v ? V
on topic ti, goal gj or action ak depth in the tree.
Initial Dialog State Model We aim to predict
the best node in a dialog tree ti, gj , al ? V based
on a user query q. A query-to-node mapping is en-
coded as an initial dialog state s0 represented by a
binary vector over all nodes in the dialog network:
s0 = [t1, g1.1, g1.2, g1.2.1 . . . , tn, gn.1, gn.1.1].
We employ a log-linear model and try to maxi-
mize initial dialog state distribution over the space
of all nodes in a dialog network:
p(s0|q, ?) =
e
?
i ?i?i(s0,q)
?
s?0 e
?
i ?i?i(s?0,q)
, (3)
Optimization follows Eq. 2.
We experimented with a variety of features.
Lexical features included query ngrams (up to 3-
grams) associated with every node in a dialog tree
with removed stopwords and stemming query un-
igrams. We also used network structural features:
Accuracy
Features Topic Goal Action
Random 0.10 0.04 0.04
TFIDF 1Best 0.81 0.21 0.45
Lexical (L) 0.92 0.66 0.63
L + 10TFIDF 0.94 0.66 0.64
L + 10TFIDF + PO 0.94 0.65 0.65
L + 10TFIDF + QO 0.95 0.72 0.69
All above + QHistO 0.96 0.73 0.71
Table 2: Initial dialog state classification results
where L stands for lexical features, 10TFIDF - 10
best tf-idf scores, PO - prompt overlap, QO - query
overlap, and QHistO - query history overlap.
tf-idf scores, query ngram overlap with the topic
and goal descriptions, as well as system action
prompts, and query ngram overlap with a history
including queries from parent nodes.
Experimental Setup For each dialog tree,
nodes corresponding to single instructions were
hand-annotated with a small set of user queries,
as described in Section 3. Approximately 60% of
all action nodes have no associated queries3 For
the 76 goals, the resulting dataset consists of 972
node-query pairs, 80% training and 20% test.
Results The initial dialog state classification
model of finding a single node given an initial
query is described in Eq. 3.
We chose two simple baselines: (1) randomly
select a node in a dialog network and (2) use a tf-
idf 1-best model.4 Stemming, stopword removal
and including top 10 tf-idf results as features led
to a 19% increase in accuracy on an action node
level over baseline (2). Adding the following fea-
tures led to an overall 26% improvement: query
overlap with a system prompt (PO), query overlap
with other node queries (QO), and query overlap
with its parent queries (QHistO) .
We present more detailed results for topic, goal
and action nodes in Table 2. For nodes deeper in
the network, the task of mapping a user query to an
action becomes more challenging. Note, however,
that the action node accuracy numbers actually un-
3There are multiple possible reasons for this: the soft-
ware user interface may already make it clear how to accom-
plish this intent, the user may not understand that the software
makes this fine-grained option available to them, or their ex-
perience with search engines may lead them to state their in-
tent in a more coarse-grained way.
4We use cosine similarity to rank all nodes in a dialog
network and select the node with the highest rank.
1673
derstate the utility of the resulting dialog system.
The reason is that even incorrect node assignments
can lead to useful system performance. As long
as a misclassification results being assigned to a
too-high node within the correct dialog tree, the
user will experience a graceful failure: they may
be forced to answer some redundant questions, but
they will still be able to accomplish the task.
5 Understanding Query Refinements
We also developed a classifier model for mapping
followup queries to the nodes in a dialog network,
while maintaining a dialog state that summarizes
the history of the current interaction.
Problem Definition Similar to the problem def-
inition in Section 4, we are given a network of di-
alog trees f1 . . . fn and a query q?, but in addition
we are given the previous dialog state s, which
contains the previous user utterance q and the last
system action as. We aim to find a new dialog
state s? that pairs a node from the dialog tree with
updated history information, thereby undergoing a
dialog state update.
We learn a linear classifier that models
p(s?|q?, q, as, ?), the dialog state update distribu-
tion, where we constrain the new state s? to contain
the new utterance q? we are interpreting. This dis-
tribution models 3 transition types: append, over-
ride and reset.
Definition 2. An append action defines a dialog
state update when transitioning from a node to its
children at any depth in the same dialog tree e.g.,
ti ? gi.j (from a topic to a goal node), gi.j ?
ai.j.z (from a goal to an action node) etc.
Definition 3. An override action defines a dialog
state update when transitioning from a goal to its
sibling node. It could also be from an action node5
to another in its parent sibling node in the same di-
alog tree e.g., gi.j?1 ? gi.j (from one goal to an-
other goal in the same topic tree), ai.j.z ? ai.?j.z
(from an action node to another action node in a
different goal in the same dialog tree) etc.
Definition 4. A reset action defines a dialog state
update when transitioning from a node in a current
dialog tree to any other node at any depth in a
dialog tree other than the current dialog tree e.g.,
ti ? t?i, (from one topic node to another topic
5A transition from ai.j.z must be to a different goal or an
action node in a different goal but in the same dialog tree.
(a) Updates from topic node ti
(b) Updates from goal node gj
(c) Updates from action node al
Figure 5: Information state updates: append, reset
and override updates based on Definition 2, 3 and
4, respectively, from topic, goal and action nodes.
node) ti ? g?i.j (from a topic node to a goal node
in a different topic subtree), etc.
The append action should be selected when the
user?s intent is to clarify a previous query (e.g.,
?insert page numbers? ? ?page numbers in the
footer?). An override action is appropriate when
the user?s intent is to change a goal within the
same topic (e.g., ?insert page number? ?change
page number?). Finally, a reset action should be
used when the user?s intent is to restart the dialog
(e.g., ?insert page x of y? ? ?set default font?).
We present more examples for append, override
and reset dialog state update actions in Table 3.
1674
Previous Utterance, q User Utterance, q? Transition Update Action, a
inserting page numbers qt1 add a background ti ? t?i 2, reset-T, reset
how to number pages qt2 insert numbers on pages in margin ti ? si.j 1.4, append-G, append
page numbers qt3 set a page number in a footer ti ? ai.j.z 1.2.1, append-A, append
page number a document qt4 insert a comment ti ? g?i.j 21.1, reset-G, reset
page number qt5 add a comment ?redo? ti ? a?i.j.z 21.2.1, reset-A, reset
page x of y qg1 add a border gi.j ? t?i 6, reset-T, resetformat page x of x qg2 enter text and page numbers gi.j ? gi.?j 1.1, override-G, overrideenter page x of y qg3 page x of y in footer gi.j ? ai.j.z 1.3.1, append-A, appendinserting page x of y qg4 setting a default font gi.j ? g?i.j 6.1, reset-G, resetshowing page x of x qg5 set default font and style gi.j ? a?i.j.z 6.4.1, reset-A, resetpage numbers bottom qa1 make a degree symbol ai.j.z ? t?i 13, reset-T, reset
numbering at bottom page qa2 insert page numbers ai.j.z ? gi.?j 1.1, override-G, override
insert footer page numbers qa3 page number design ai.j.z?1 ? ai.j.z 1.2.2, append-A, append
headers page number qa4 comments in document ai.j.z ? g?i.j 21.1, reset-G, reset
page number in a footer qa5 changing initials in a comment ai.j.z ? a?i.j.z 21.2.1, reset-A, reset
Table 3: Example q and q? queries for append, override and reset dialog state updates.
Figure 5 illustrates examples of append, over-
ride and reset dialog state updates. All transitions
presented in Figure 5 are aligned with the example
q and q? queries in Table 3.
Dialog State Update Model We use a log-linear
model to maximize a dialog state distribution over
the space of all nodes in a dialog network:
p(s?|q?, q, as?) =
e
?
i ?i?i(s?,q?,as,q)
?
s?? e
?
i ?i?i(s??,q?,as,q)
, (4)
Optimization is done as described in Section 3.
Experimental Setup Ideally, dialog systems
should be evaluated relative to large volumes of
real user interaction data. Our query log data,
however, does not include dialog turns, and so we
turn to simulated user behavior to test our system.
Our approach, inspired by recent work (Schatz-
mann et al, 2006; Scheffler and Young, 2002;
Georgila et al, 2005), involves simulating dialog
turns as follows. To define a state s we sam-
ple a query q from a set of queries per node v
and get a corresponding system action as for this
node; to define a state s?, we sample a new query
q? from another node v? ? V, v 6= v? which
is sampled using a prior probability biased to-
wards append: p(append)=0.7, p(override)=0.2,
p(reset)=0.1. This prior distribution defines a dia-
log strategy where the user primarily continues the
current goal and rarely resets.
We simulate 1100 previous state and new query
pairs for training and 440 pairs for testing. The
features were lexical, including word ngrams,
stems with no stopwords; we also tested network
structure, such as:
- old q and new q? query overlap (QO);
- q? overlap with a system prompt as (PO);
- q? ngram overlap with all queries from the old
state s (SQO);
- q? ngram overlap with all queries from the
new state s? (S?QO);
- q? ngram overlap with all queries from the
new state parents (S?ParQO).
Results Table 4 reports results for dialog state
updates for topic, goal and action nodes. We also
report performance for two types of dialog updates
such as: append (App.) and override (Over.).
We found that the combination of lexical and
query overlap with the previous and new state
queries yielded the best accuracies: 0.95, 0.84 and
0.83 for topic, goal and action node level, respec-
tively. As in Section 4, the accuracy on the topic
level node was highest. Perhaps surprisingly, the
reset action was perfectly predicted (accuracy is
100% for all feature combinations, not included
in figure). The accuracies for append and override
actions are also high (append 95%, override 90%).
Features Topic Goal Action App. Over.
L 0.92 0.76 0.78 0.90 0.89
L+Q 0.93 0.80 0.80 0.92 0.83
L+P 0.93 0.80 0.79 0.91 0.85
L+Q+P 0.94 0.80 0.80 0.93 0.85
L+SQ 0.94 0.82 0.81 0.93 0.85
L+S?Q 0.93 0.80 0.80 0.91 0.90
L+S?+ParQ 0.94 0.80 0.80 0.91 0.86
L+Q+S?Q 0.94 0.81 0.81 0.91 0.88
L+SQ+S?Q 0.95 0.84 0.83 0.94 0.88
Table 4: Dialog state updates classification ac-
curacies where L stands for lexical features, Q -
query overlap, P - prompt overlap, SQ - previous
state query overlap, S?Q - new state query overlap,
S?ParQ - new state parent query overlap.
1675
6 The Complete Dialog System
Following the overall setup described in Section 2,
we integrate the learned models into a complete
dialog system. To evaluate the quality of the in-
teractions with this system, we performed a small
scale user study, as described here.
Experimental Setup We randomly chose 8
goals, across topics, from the 76 used to train our
models. For each goal, six users were shown a Mi-
crosoft Word document before and after the goal
was executed. Users were allowed to interact with
the system in an unconstrained manner providing a
total of 48 interactions. If at any point they thought
the system response was irrelevant to their intent,
they could either rephrase or start over.
User Study We objectively measured task com-
pletion and the quality of individual system utter-
ances. We also asked the users to rate the system
(on a scale of 1-5, with 5 being best) on various
measures, including whether the system asked rel-
evant questions. Table 5 summarizes the results.
In general, users were able to complete the tasks
and were reasonably satisfied.
Discussion Figure 6 shows four example di-
alogs. User utterances ranged from single key-
words to rich, sentence-like goal descriptions. For
the shorter inputs, the system was generally able
to identify a small cohort of high-probability in-
tents. For longer inputs, especially with explicitly
specified parameter settings, the system was usu-
ally able to elicit whatever additional information
was required to complete the task. The majority of
errors fell into two categories when the system:
I. missed key information in the input and
asked redundant questions ? Figure 6 (C);
II. failed to correctly identify the intent, and
asked an irrelevant question ? Figure 6 (D).
Table 5 shows the frequency of each error type.
Objective Measures Observed Rates
User achieved task 83%
System asked irrelevant questions 19%
System asked redundant questions 18%
User Satisfaction Score Std.Dev.
Overall satisfaction 3.42 0.49
Questions were relevant 4.17 0.41
Ambiguity was managed 4.00 0.89
Table 5: User study results.
U: ?I want a page number and title at the bot-
tom of the page?
S: ?Choose a page number design from the
gallery of designs?
U: *User selects design*
S: ?To insert header or footer content before the
page number, type the content, and then press
TAB to position the content?
U: ?Professional Resume?
S: ?Task Completed?
(A)
U: ?Alphabetize bullets?
S: ?Do you want to sort a list alphabetically?
U: ?Sort a list?
S: ?Select the text in a bulleted or numbered
list?
U: *User selection*
S: ?Ascending or Descending??
U: ?Ascending?
S: ?Task Completed?
(B)
U: ?I want the background color to be yellow?
S: ?Choose a color for the document?
U: ?Yellow ?
S: ?Task completed?
(C)
U: ?I want to change the font face and size?
S: ?Choose the default font?
U: ?No, reduce the size of the text?
S: *System Restarts*
(D)
Figure 6: Four example dialogs from the user
study, including cases that (A and B) complete
successfully, (C) have a redundant question, and
(D) fail to recognize the user intent.
7 Related work
To the best of our knowledge, this paper presents
the first effort to induce full procedural dialog sys-
tems from instructional text and query click logs.
Grounded Language Learning There has been
significant interest in grounded language learn-
ing. Perhaps the most closely related work
learns to understand instructions and automati-
1676
cally complete the tasks they describe (Branavan
et al, 2009; Vogel and Jurafsky, 2010; Kush-
man et al, 2009; Branavan et al, 2010; Artzi and
Zettlemoyer, 2013). However, these approaches
did not model user interaction. There are also
many related approaches for other grounded lan-
guage problems, including understanding game
strategy guides (Branavan et al, 2011), model-
ing users goals in a Windows domain (Horvitz
et al, 1998), learning from conversational inter-
action (Artzi and Zettlemoyer, 2011), learning
to sportscast (Chen and Mooney, 2011), learning
from event streams (Liang et al, 2009), and learn-
ing paraphrases from crowdsourced captions of
video snippets (Chen and Dolan, 2011).
Dialog Generation from Text Similarly to Pi-
wek?s work (2007; 2010; 2011), we study extract-
ing dialog knowledge from documents (mono-
logues or instructions). However, Piwek?s ap-
proach generates static dialogs, for example to
generate animations of virtual characters having a
conversation. There is no model of dialog man-
agement or user interaction, and the approach does
not use any machine learning. In contrast, to the
best of our knowledge, we are the first to demon-
strate it is possible to learn complete, interactive
dialog systems using instructional texts (and non-
expert annotation).
Learning from Web Query Logs Web query
logs have been extensively studied. For example,
they are widely used to represent user intents in
spoken language dialogs (Tu?r et al, 2011; Celiky-
ilmaz et al, 2011; Celikyilmaz and Hakkani-Tur,
2012). Web query logs are also used in many other
NLP tasks, including entity linking (Pantel et al,
2012) and training product and job intent classi-
fiers (Li et al, 2008).
Dialog Modeling and User Simulation Many
existing dialog systems learn dialog strategies
from user interactions (Young, 2010; Rieser and
Lemon, 2008). Moreover, dialog data is often lim-
ited and, therefore, user simulation is commonly
used (Scheffler and Young, 2002; Schatzmann et
al., 2006; Georgila et al, 2005).
Our overall approach is also related to many
other dialog management approaches, including
those that construct dialog graphs from dialog data
via clustering (Lee et al, 2009), learn information
state updates using discriminative classification
models (Hakkani-Tur et al, 2012; Mairesse et al,
2009), optimize dialog strategy using reinforce-
ment learning (RL) (Scheffler and Young, 2002;
Rieser and Lemon, 2008), or combine RL with
information state update rules (Heeman, 2007).
However, our approach is unique in the use of in-
ducing task and domain knowledge with light su-
pervision to assist the user with many goals.
8 Conclusions and Future Work
This paper presented a novel approach for au-
tomatically constructing procedural dialog sys-
tems with light supervision, given only textual re-
sources such as instructional text and search query
click logs. Evaluations demonstrated highly accu-
rate performance, on automatic benchmarks and
through a user study.
Although we showed it is possible to build com-
plete systems, more work will be required to scale
the approach to new domains, scale the complex-
ity of the dialog manager, and explore the range of
possible textual knowledge sources that could be
incorporated. We are particularly interested in sce-
narios that would enable end users to author new
goals by writing procedural instructions in natural
language.
Acknowledgments
The authors would like to thank Jason Williams
and the anonymous reviewers for their helpful
comments and suggestions.
References
Priti Aggarwal, Ron Artstein, Jillian Gerten, An-
thanasios Katsamanis, Shrikanth Narayanan, Angela
Nazarian, and David R. Traum. 2012. The twins
corpus of museum visitor questions. In Proceedings
of LREC.
Yoav Artzi and Luke Zettlemoyer. 2011. Learning
to recover meaning from unannotated conversational
interactions. In NIPS Workshop In Learning Seman-
tics.
Yoav Artzi and Luke Zettlemoyer. 2013. Weakly su-
pervised learning of semantic parsers for mapping
instructions to actions. Transactions of the Associa-
tion for Computational Linguistics, 1(1):49?62.
Michael S. Bernstein, Greg Little, Robert C. Miller,
Bjo?rn Hartmann, Mark S. Ackerman, David R.
Karger, David Crowell, and Katrina Panovich.
2010. Soylent: a word processor with a crowd in-
side. In Proceedings of ACM Symposium on User
Interface Software and Technology.
1677
S. R. K. Branavan, Harr Chen, Luke S. Zettlemoyer,
and Regina Barzilay. 2009. Reinforcement learning
for mapping instructions to actions. In Proceedings
of ACL.
S. R. K. Branavan, Luke S. Zettlemoyer, and Regina
Barzilay. 2010. Reading between the lines: learn-
ing to map high-level instructions to commands. In
Proceedings of ACL.
S. R. K. Branavan, David Silver, and Regina Barzi-
lay. 2011. Learning to win by reading manuals in
a monte-carlo framework. In Proceedings of ACL.
Asli Celikyilmaz and Dilek Hakkani-Tur. 2012. A
joint model for discovery of aspects in utterances.
In Proceedings of ACL.
Asli Celikyilmaz, Dilek Hakkani-Tu?r, and Gokhan Tu?r.
2011. Mining search query logs for spoken language
understanding. In Proceedings of ICML.
David L. Chen and William B. Dolan. 2011. Collect-
ing highly parallel data for paraphrase evaluation. In
Proceedings of ACL.
David L. Chen and Raymond J. Mooney. 2011. Learn-
ing to interpret natural language navigation instruc-
tions from observations. In Proceedings of AAAI.
Myroslava Dzikovska, Amy Isard, Peter Bell, Jo-
hanna D. Moore, Natalie B. Steinhauser, Gwen-
dolyn E. Campbell, Leanne S. Taylor, Simon Caine,
and Charlie Scott. 2011. Adaptive intelligent tuto-
rial dialogue in the beetle ii system. In Proceedings
of AIED.
Kallirroi Georgila, James Henderson, and Oliver
Lemon. 2005. Learning user simulations for infor-
mation state update dialogue systems. In Proceed-
ings of Eurospeech.
Dilek Hakkani-Tur, Gokhan Tur, Larry Heck, Ashley
Fidler, and Asli Celikyilmaz. 2012. A discrimi-
native classification-based approach to information
state updates for a multi-domain dialog system. In
Proceedings of Interspeech.
Peter Heeman. 2007. Combining Reinforcement
Learning with Information-State Update Rules. In
Proceedings of ACL.
Eric Horvitz, Jack Breese, David Heckerman, David
Hovel, and Koos Rommelse. 1998. The Lumiere
project: Bayesian user modeling for inferring the
goals and needs of software users. In Proceedings
of Uncertainty in Artificial Intelligence.
Nate Kushman, Micah Brodsky, S. R. K. Branavan,
Dina Katabi, Regina Barzilay, and Martin Rinard.
2009. WikiDo. In ACM HotNets.
Cheongjae Lee, Sangkeun Jung, Kyungduk Kim, and
Gary Geunbae Lee. 2009. Automatic agenda graph
construction from human-human dialogs using clus-
tering method. In Proceedings of NAACL.
Xiao Li, Ye-Yi Wang, and Alex Acero. 2008. Learn-
ing query intent from regularized click graphs. In
Proceedings of SIGIR.
Percy Liang, Michael I. Jordan, and Dan Klein. 2009.
Learning semantic correspondences with less super-
vision. In Proceedings of ACL-IJCNLP.
F. Mairesse, M. Gasic, F. Jurcicek, S. Keizer, B. Thom-
son, K. Yu, and S. Young. 2009. Spoken lan-
guage understanding from unaligned data using dis-
criminative classification models. In Proceedings of
Acoustics, Speech and Signal Processing.
Fabrizio Morbini, Eric Forbell, David DeVault, Kenji
Sagae, David R. Traum, and Albert A. Rizzo. 2012.
A mixed-initiative conversational dialogue system
for healthcare. In Proceedings of SIGDIAL.
Jorge Nocedal and Stephen J. Wright. 2000. Numeri-
cal Optimization. Springer.
Patric Pantel, Thomas Lin, and Michael Gamon. 2012.
Mining entity types from query logs via user intent.
In Proceedings of ACL.
Paul Piwek and Svetlana Stoyanchev. 2010. Generat-
ing expository dialogue from monologue: Motiva-
tion, corpus and preliminary rules. In Proceedings
of NAACL.
Paul Piwek and Svetlana Stoyanchev. 2011. Data-
oriented monologue-to-dialogue generation. In Pro-
ceedings of ACL, pages 242?247.
Paul Piwek, Hugo Hernault, Helmut Prendinger, and
Mitsuru Ishizuka. 2007. T2d: Generating dialogues
between virtual agents automatically from text. In
Proceedings of Intelligent Virtual Agents.
Verena Rieser and Oliver Lemon. 2008. Learning ef-
fective multimodal dialogue strategies from wizard-
of-oz data: Bootstrapping and evaluation. In Pro-
ceedings of ACL.
A. Rizzo, Kenji Sagae, E. Forbell, J. Kim, B. Lange,
J. Buckwalter, J. Williams, T. Parsons, P. Kenny,
David R. Traum, J. Difede, and B. Rothbaum. 2011.
Simcoach: An intelligent virtual human system for
providing healthcare information and support. In
Proceedings of ITSEC.
Jost Schatzmann, Karl Weilhammer, Matt Stuttle, and
Steve Young. 2006. A survey of statistical user sim-
ulation techniques for reinforcement-learning of dia-
logue management strategies. Knowledge Engineer-
ing Review, 21(2).
Konrad Scheffler and Steve Young. 2002. Automatic
learning of dialogue strategy using dialogue simula-
tion and reinforcement learning. In Proceedings of
Human Language Technology Research.
Natalie B. Steinhauser, Gwendolyn E. Campbell,
Leanne S. Taylor, Simon Caine, Charlie Scott, My-
roslava Dzikovska, and Johanna D. Moore. 2011.
1678
Talk like an electrician: Student dialogue mimick-
ing behavior in an intelligent tutoring system. In
Proceedings of AIED.
David R. Traum, Priti Aggarwal, Ron Artstein, Susan
Foutz, Jillian Gerten, Athanasios Katsamanis, Anton
Leuski, Dan Noren, and William R. Swartout. 2012.
Ada and grace: Direct interaction with museum vis-
itors. In Proceedings of Intelligent Virtual Agents.
Go?khan Tu?r, Dilek Z. Hakkani-Tu?r, Dustin Hillard, and
Asli C?elikyilmaz. 2011. Towards unsupervised spo-
ken language understanding: Exploiting query click
logs for slot filling. In Proceedings of Interspeech.
Adam Vogel and Dan Jurafsky. 2010. Learning to fol-
low navigational directions. In Proceedings of ACL.
Steve Young. 2010. Cognitive user interfaces. In IEEE
Signal Processing Magazine.
1679
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 7?11,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Exact Maximum Inference for the Fertility Hidden Markov Model
Chris Quirk
Microsoft Research
One Microsoft Way
Redmond, WA 98052, USA
chrisq@microsoft.com
Abstract
The notion of fertility in word alignment
(the number of words emitted by a sin-
gle state) is useful but difficult to model.
Initial attempts at modeling fertility used
heuristic search methods. Recent ap-
proaches instead use more principled ap-
proximate inference techniques such as
Gibbs sampling for parameter estimation.
Yet in practice we also need the single best
alignment, which is difficult to find us-
ing Gibbs. Building on recent advances in
dual decomposition, this paper introduces
an exact algorithm for finding the sin-
gle best alignment with a fertility HMM.
Finding the best alignment appears impor-
tant, as this model leads to a substantial
improvement in alignment quality.
1 Introduction
Word-based translation models intended to model
the translation process have found new uses iden-
tifying word correspondences in sentence pairs.
These word alignments are a crucial training com-
ponent in most machine translation systems. Fur-
thermore, they are useful in other NLP applica-
tions, such as entailment identification.
The simplest models may use lexical infor-
mation alone. The seminal Model 1 (Brown
et al, 1993) has proved very powerful, per-
forming nearly as well as more complicated
models in some phrasal systems (Koehn et al,
2003). With minor improvements to initializa-
tion (Moore, 2004) (which may be important
(Toutanova and Galley, 2011)), it can be quite
competitive. Subsequent IBM models include
more detailed information about context. Models
2 and 3 incorporate a positional model based on
the absolute position of the word; Models 4 and
5 use a relative position model instead (an English
word tends to align to a French word that is nearby
the French word aligned to the previous English
word). Models 3, 4, and 5 all incorporate a no-
tion of ?fertility?: the number of French words that
align to any English word.
Although these latter models covered a broad
range of phenomena, estimation techniques and
MAP inference were challenging. The au-
thors originally recommended heuristic proce-
dures based on local search for both. Such meth-
ods work reasonably well, but can be computation-
ally inefficient and have few guarantees. Thus,
many researchers have switched to the HMM
model (Vogel et al, 1996) and variants with more
parameters (He, 2007). This captures the posi-
tional information in the IBM models in a frame-
work that admits exact parameter estimation infer-
ence, though the objective function is not concave:
local maxima are a concern.
Modeling fertility is challenging in the HMM
framework as it violates the Markov assump-
tion. Where the HMM jump model considers only
the prior state, fertility requires looking across
the whole state space. Therefore, the standard
forward-backward and Viterbi algorithms do not
apply. Recent work (Zhao and Gildea, 2010) de-
scribed an extension to the HMM with a fertility
model, using MCMC techniques for parameter es-
timation. However, they do not have a efficient
means of MAP inference, which is necessary in
many applications such as machine translation.
This paper introduces a method for exact MAP
inference with the fertility HMM using dual de-
composition. The resulting model leads to sub-
stantial improvements in alignment quality.
7
2 HMM alignment
Let us briefly review the HMM translation model
as a starting point. We are given a sequence of
English words e = e1, . . . , eI . This model pro-
duces distributions over French word sequences
f = f1, . . . , fJ and word alignment vectors a =
a1, . . . , aJ , where aj ? [0..J ] indicates the En-
glish word generating the jth French word, 0 rep-
resenting a special NULL state to handle systemat-
ically unaligned words.
Pr(f ,a|e) = p(J |I)
J?
j=1
p(aj |aj?1) p
(
fj
??eaj
)
The generative story begins by predicting the num-
ber of words in the French sentence (hence the
number of elements in the alignment vector). Then
for each French word position, first the alignment
variable (English word index used to generate the
current French word) is selected based on only the
prior alignment variable. Next the French word is
predicted based on its aligned English word.
Following prior work (Zhao and Gildea, 2010),
we augment the standard HMM with a fertility dis-
tribution.
Pr(f ,a|e) =p(J |I)
I?
i=1
p(?i|ei)
J?
j=1
p(aj |aj?1) p
(
fj
??eaj
)
(1)
where ?i =?Jj=1 ?(i, aj) indicates the number of
times that state j is visited. This deficient model
wastes some probability mass on inconsistent con-
figurations where the number of times that a state
i is visited does not match its fertility ?i. Follow-
ing in the footsteps of older, richer, and wiser col-
leagues (Brown et al, 1993),we forge ahead un-
concerned by this complication.
2.1 Parameter estimation
Of greater concern is the exponential complex-
ity of inference in this model. For the standard
HMM, there is a dynamic programming algorithm
to compute the posterior probability over word
alignments Pr(a|e, f). These are the sufficient
statistics gathered in the E step of EM.
The structure of the fertility model violates the
Markov assumptions used in this dynamic pro-
gramming method. However, we may empirically
estimate the posterior distribution using Markov
chain Monte Carlo methods such as Gibbs sam-
pling (Zhao and Gildea, 2010). In this case,
we make some initial estimate of the a vector,
potentially randomly. We then repeatedly re-
sample each element of that vector conditioned
on all other positions according to the distribu-
tion Pr(aj |a?j , e, f). Given a complete assign-
ment of the alignment for all words except the cur-
rent, computing the complete probability includ-
ing transition, emission, and jump, is straightfor-
ward. This estimate comes with a computational
cost: we must cycle through all positions of the
vector repeatedly to gather a good estimate. In
practice, a small number of samples will suffice.
2.2 MAP inference with dual decomposition
Dual decomposition, also known as Lagrangian
relaxation, is a method for solving complex
combinatorial optimization problems (Rush and
Collins, 2012). These complex problems are sepa-
rated into distinct components with tractable MAP
inference procedures. The subproblems are re-
peatedly solved with some communication over
consistency until a consistent and globally optimal
solution is found.
Here we are interested in the problem of find-
ing the most likely alignment of a sentence pair
e, f . Thus, we need to solve the combinatorial op-
timization problem argmaxa Pr(f ,a|e). Let us
rewrite the objective function as follows:
h(a) =
I?
i=1
?
?log p(?i|ei) +
?
j,aj=i
log p(fj |ei)
2
?
?
+
J?
j=1
(
log p(aj |aj?1) +
log p
(
fj
??eaj
)
2
)
Because f is fixed, the p(J |I) term is constant and
may be omitted. Note how we?ve split the opti-
mization into two portions. The first captures fer-
tility as well as some component of the translation
distribution, and the second captures the jump dis-
tribution and the remainder of the translation dis-
tribution.
Our dual decomposition method follows this
segmentation. Define ya as ya(i, j) = 1 if aj = i,
and 0 otherwise. Let z ? {0, 1}I?J be a binary
8
u(0)(i, j) := 0 ?i ? 1..I, j ? 1..J
for k = 1 to K
a(k) := argmaxa
(
f(a) +?i,j u(k?1)(i, j)ya(i, j)
)
z(k) := argmaxz
(
g(z)??i,j u(k?1)(i, j)z(i, j)
)
if ya = z
return a(k)
end if
u(k)(i, j) := u(k)(i, j) + ?k
(
ya(k)(i, j)? z(k)(i, j)
)
end for
return a(K)
Figure 1: The dual decomposition algorithm for
the fertility HMM, where ?k is the step size at the
kth iteration for 1 ? k ? K, and K is the max
number of iterations.
matrix. Define the functions f and g as
f(a) =
J?
j=1
(
log p(aj |aj?1) +
1
2 log p
(
fj
??eaj
))
g(z) =
I?
i=1
(
log p(? (zi)|ei) +
J?
j=1
z(i, j)
2 log p(fj |ei)
)
Then we want to find
argmax
a,z
f(a) + g(z)
subject to the constraints ya(i, j) = z(i, j)?i, j.
Note how this recovers the original objective func-
tion when matching variables are found.
We use the dual decomposition algorithm
from Rush and Collins (2012), reproduced
here in Figure 1. Note how the langrangian
adds one additional term word, scaled by a
value indicating whether that word is aligned
in the current position. Because it is only
added for those words that are aligned, we
can merge this with the log p(fj
??eaj
) terms
in both f and g. Therefore, we can solve
argmaxa
(
f(a) +
?
i,j u(k?1)(i, j)ya(i, j)
)
us-
ing the standard Viterbi algorithm.
The g function, on the other hand, does not have
a commonly used decomposition structure. Luck-
ily we can factor this maximization into pieces that
allow for efficient computation. Note that g sums
over arbitrary binary matrices. Unlike the HMM,
where each French word must have exactly one
English generator, this maximization allows each
z(i, j) := 0 ?(i, j) ? [1..I]? [1..J ]
v := 0
for i = 1 to I
for j = 1 to J
x(j) := (log p(fj |ei) , j)
end for
sort x in descending order by first component
max := log p(? = 0|ei) , arg := 0, sum := 0
for f = 1 to J
sum := sum+ x[f, 1]
if sum+ log p(? = f |ei) > max
max := sum+ log p(? = f |ei)
arg := f
end if
end for
v := v +max
for f = 1 to arg
z(i, x[f, 2]) := 1
end for
end for
return z, v
Figure 2: Algorithm for finding the arg max and
max of g, the fertility-related component of the
dual decomposition objective.
French word to have zero or many generators. Be-
cause assignments that are in accordance between
this model and the HMM will meet the HMM?s
constraints, the overall dual decomposition algo-
rithm will return valid assignments, even though
individual selections for this model may fail to
meet the requirements.
As the scoring function g can be decomposed
into a sum of scores for each row?i gi (i.e., there
are no interactions between distinct rows of the
matrix) we can maximize each row independently:
max
z
I?
i=1
gi(zi) =
I?
i=1
max
z
gi(zi)
Within each row, we seek the best of all 2J pos-
sible configurations. These configurations may
be grouped into equivalence classes based on the
number of non-zero entries. In each class, the
max assignment is the one using words with the
highest log probabilities; the total score of this as-
signment is the sum those log probabilities and
the log probability of that fertility. Sorting the
scores of each cell in the row in descending or-
der by log probability allows for linear time com-
putation of the max for each row. The algorithm
described in Figure 2 finds this maximal assign-
ment in O(IJ log J) time, generally faster than
the O(I2J) time used by Viterbi.
We note in passing that this maximizer is pick-
ing from an unconstrained set of binary matri-
9
ces. Since each English word may generate as
many French words as it likes, regardless of all
other words in the sentence, the underlying ma-
trix have many more or many fewer non-zero en-
tries than there are French words. A straightfor-
ward extension to the algorithm of Figure 2 returns
only z matrices with exactly J nonzero entries.
Rather than maximizing each row totally indepen-
dently, we keep track of the best configurations
for each number of words generated in each row,
and then pick the best combination that sums to J :
another straightforward exercise in dynamic pro-
gramming. This refinement does not change the
correctness of the dual decomposition algorithm;
rather it speeds the convergence.
3 Fertility distribution parameters
Original IBM models used a categorical distribu-
tion of fertility, one such distribution for each En-
glish word. This gives EM a great amount of free-
dom in parameter estimation, with no smoothing
or parameter tying of even rare words. Prior work
addressed this by using the single parameter Pois-
son distribution, forcing infrequent words to share
a global parameter estimated from the fertility of
all words in the corpus (Zhao and Gildea, 2010).
We explore instead a feature-rich approach to
address this issue. Prior work has explored
feature-rich approaches to modeling the transla-
tion distribution (Berg-Kirkpatrick et al, 2010);
we use the same technique, but only for the fertil-
ity model. The fertility distribution is modeled as
a log-linear distribution of F , a binary feature set:
p(?|e) ? exp (? ? F (e, ?)). We include a simple
set of features:
? A binary indicator for each fertility ?. This
feature is present for all words, acting as
smoothing.
? A binary indicator for each word id and fer-
tility, if the word occurs more than 10 times.
? A binary indicator for each word length (in
letters) and fertility.
? A binary indicator for each four letter word
prefix and fertility.
Together these produce a distribution that can
learn a reasonable distribution not only for com-
mon words, but also for rare words. Including
word length information aids in for languages with
compounding: long words in one language may
correspond to multiple words in the other.
Algorithm AER (G?E) AER (E?G)
HMM 24.0 21.8
FHMM Viterbi 19.7 19.6
FHMM Dual-dec 18.0 17.4
Table 1: Experimental results over the 120 evalu-
ation sentences. Alignment error rates in both di-
rections are provided here.
4 Evaluation
We explore the impact of this improved MAP in-
ference procedure on a task in German-English
word alignment. For training data we use the news
commentary data from the WMT 2012 translation
task.1 120 of the training sentences were manually
annotated with word alignments.
The results in Table 1 compare several differ-
ent algorithms on this same data. The first line is
a baseline HMM using exact posterior computa-
tion and inference with the standard dynamic pro-
gramming algorithms. The next line shows the fer-
tility HMM with approximate posterior computa-
tion from Gibbs sampling but with final alignment
selected by the Viterbi algorithm. Clearly fertil-
ity modeling is improving alignment quality. The
prior work compared Viterbi with a form of local
search (sampling repeatedly and keeping the max),
finding little difference between the two (Zhao and
Gildea, 2010). Here, however, the difference be-
tween a dual decomposition and Viterbi is signifi-
cant: their results were likely due to search error.
5 Conclusions and future work
We have introduced a dual decomposition ap-
proach to alignment inference that substantially
reduces alignment error. Unfortunately the algo-
rithm is rather slow to converge: after 40 iterations
of the dual decomposition, still only 55 percent
of the test sentences have converged. We are ex-
ploring improvements to the simple sub-gradient
method applied here in hopes of finding faster con-
vergence, fast enough to make this algorithm prac-
tical. Alternate parameter estimation techniques
appear promising given the improvements of dual
decomposition over sampling. Once the perfor-
mance issues of this algorithm are improved, ex-
ploring hard EM or some variant thereof might
lead to more substantial improvements.
1www.statmt.org/wmt12/translation-task.html
10
References
Taylor Berg-Kirkpatrick, Alexandre Bouchard-Co?te?,
John DeNero, and Dan Klein. 2010. Painless un-
supervised learning with features. In Human Lan-
guage Technologies: The 2010 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, pages 582?590, Los
Angeles, California, June. Association for Compu-
tational Linguistics.
Peter F. Brown, Vincent J. Della Pietra, Stephen
A. Della Pietra, and Robert L. Mercer. 1993.
The mathematics of statistical machine translation:
parameter estimation. Computational Linguistics,
19(2):263?311.
Xiaodong He. 2007. Using word-dependent transition
models in HMM-based word alignment for statisti-
cal machine translation. In Proceedings of the Sec-
ond Workshop on Statistical Machine Translation,
pages 80?87, Prague, Czech Republic, June. Asso-
ciation for Computational Linguistics.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceedings
of the 2003 Human Language Technology Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics.
Robert C. Moore. 2004. Improving ibm word align-
ment model 1. In Proceedings of the 42nd Meet-
ing of the Association for Computational Linguistics
(ACL?04), Main Volume, pages 518?525, Barcelona,
Spain, July.
Alexander M Rush and Michael Collins. 2012. A tuto-
rial on dual decomposition and lagrangian relaxation
for inference in natural language processing. Jour-
nal of Artificial Intelligence Research, 45:305?362.
Kristina Toutanova and Michel Galley. 2011. Why
initialization matters for ibm model 1: Multiple op-
tima and non-strict convexity. In Proceedings of the
49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, pages 461?466, Portland, Oregon, USA, June.
Association for Computational Linguistics.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical
translation. In COLING.
Shaojun Zhao and Daniel Gildea. 2010. A fast fertil-
ity hidden markov model for word alignment using
MCMC. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Process-
ing, pages 596?605, Cambridge, MA, October. As-
sociation for Computational Linguistics.
11
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 222?227,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Semantic Neighborhoods as Hypergraphs
Chris Quirk and Pallavi Choudhury
Microsoft Research
One Microsoft Way
Redmond, WA 98052, USA
{chrisq,pallavic}@microsoft.com
Abstract
Ambiguity preserving representations
such as lattices are very useful in a num-
ber of NLP tasks, including paraphrase
generation, paraphrase recognition, and
machine translation evaluation. Lattices
compactly represent lexical variation, but
word order variation leads to a combina-
torial explosion of states. We advocate
hypergraphs as compact representations
for sets of utterances describing the same
event or object. We present a method
to construct hypergraphs from sets of
utterances, and evaluate this method on
a simple recognition task. Given a set of
utterances that describe a single object or
event, we construct such a hypergraph,
and demonstrate that it can recognize
novel descriptions of the same event with
high accuracy.
1 Introduction
Humans can construct a broad range of descrip-
tions for almost any object or event. In this paper,
we will refer to such objects or events as ground-
ings, in the sense of grounded semantics. Exam-
ples of groundings include pictures (Rashtchian et
al., 2010), videos (Chen and Dolan, 2011), transla-
tions of a sentence from another language (Dreyer
and Marcu, 2012), or even paraphrases of the same
sentence (Barzilay and Lee, 2003).
One crucial problem is recognizing whether
novel utterances are relevant descriptions of those
groundings. In the case of machine translation,
this is the evaluation problem; for images and
videos, this is recognition and retrieval. Generat-
ing descriptions of events is also often an interest-
ing task: we might like to find a novel paraphrase
for a given sentence, or generate a description of a
grounding that meets certain criteria (e.g., brevity,
use of a restricted vocabulary).
Much prior work has used lattices to compactly
represent a range of lexical choices (Pang et al,
2003). However, lattices cannot compactly repre-
sent alternate word orders, a common occurrence
in linguistic descriptions. Consider the following
excerpts from a video description corpus (Chen
and Dolan, 2011):
? A man is sliding a cat on the floor.
? A boy is cleaning the floor with the cat.
? A cat is being pushed across the floor by a
man.
Ideally we would like to recognize that the fol-
lowing utterance is also a valid description of that
event: A cat is being pushed across the floor by a
boy. That is difficult with lattice representations.
Consider the following context free grammar:
S ? X0 X1
| X2 X3
X0 ? a man | a boy
X1 ? is sliding X2 on X4
| is cleaning X4 with X2
X2 ? a cat | the cat
X3 ? is being pushed across X4 by X0
X4 ? the floor
This grammar compactly captures many lexical
and syntactic variants of the input set. Note how
the labels act as a kind of multiple-sequence-
alignment allowing reordering: spans of tokens
covered by the same label are, in a sense, aligned.
This hypergraph or grammar represents a seman-
tic neighborhood: a set of utterances that describe
the same entity in a semantic space.
Semantic neighborhoods are defined in terms of
a grounding. Two utterances are neighbors with
respect to some grounding (semantic event) if they
are both descriptions of that grounding. Para-
phrases, in contrast, may be defined over all pos-
sible groundings. That is, two words or phrases
222
are considered paraphrases if there exists some
grounding that they both describe. The para-
phrase relation is more permissive than the seman-
tic neighbor relation in that regard. We believe that
it is much easier to define and evaluate semantic
neighbors. Human annotators may have difficulty
separating paraphrases from unrelated or merely
related utterances, and this line may not be con-
sistent between judges. Annotating whether an ut-
terance clearly describes a grounding is a much
easier task.
This paper describes a simple method for con-
structing hypergraph-shaped Semantic Neighbor-
hoods from sets of expressions describing the
same grounding. The method is evaluated in
a paraphrase recognition task, inspired by a
CAPTCHA task (Von Ahn et al, 2003).
2 Inducing neighborhoods
Constructing a hypergraph to capture a set of utter-
ances is a variant of grammar induction. Given a
sample of positive examples, we infer a compact
and accurate description of the underlying lan-
guage. Conventional grammar induction attempts
to define the set of grammatical sentences in the
language. Here, we search for a grammar over the
fluent and adequate descriptions of a particular in-
put. Many of the same techniques still apply.
Rather than starting from scratch, we bootstrap
from an existing English parser. We begin by pars-
ing the set of input utterances. This parsed set of
utterances acts as a sort of treebank. Reading off a
grammar from this treebank produces a grammar
that can generate not only the seed sentences, but
also a broad range of nearby sentences. In the case
above with cat, man, and boy, we would be able
to generate cases legitimate variants where man
was replaced by boy as well as undesired variants
where man is replaced by cat or floor. This initial
grammar captures a large neighborhood of nearby
utterances including many such undesirable ones.
Therefore, we refine the grammar.
Refinements have been in common use in syn-
tactic parsing for years now. Inspired by the re-
sult that manual annotations of Treebank cate-
gories can substantially increase parser accuracy
(Klein and Manning, 2003), several approaches
have been introduced to automatically induce la-
tent symbols on existing trees. We use the split-
merge method commonly used in syntactic pars-
ing (Petrov et al, 2006). In its original setting,
the refinements captured details beyond that of the
original Penn Treebank symbols. Here, we cap-
ture both syntactic and semantic regularities in the
descriptions of a given grounding.
As we perform more rounds of refinement, the
grammar becomes tightly constrained to the orig-
inal sentences. Indeed, if we iterated to a fixed
point, the resulting grammar would parse only the
original sentences. This is a common dilemma in
paraphrase learning: the safest meaning preserv-
ing rewrite is to change nothing. We optimize the
number of split-merge rounds for task-accuracy;
two or three rounds works well in practice. Fig-
ure 1 illustrates the process.
2.1 Split-merge induction
We begin with a set of utterances that describe
a specific grounding. They are parsed with a
conventional Penn Treebank parser (Quirk et al,
2012) to produce a type of treebank. Unlike con-
ventional treebanks which are annotated by human
experts, the trees here are automatically created
and thus are more likely to contain errors. This
treebank is the input to the split-merge process.
Split: Given an input treebank, we propose re-
finements of the symbols in hopes of increasing
the likelihood of the data. For each original sym-
bol in the grammar such as NP, we consider two la-
tent refinements: NP0 and NP1. Each binary rule
then produces 8 possible variants, since the par-
ent, left child, and right child now have two possi-
ble refinements. The parameters of this grammar
are then optimized using EM. Although we do not
know the correct set of latent annotations, we can
search for the parameters that optimize the likeli-
hood of the given treebank. We initialize the pa-
rameters of this refined grammar with the counts
from the original grammar along with a small ran-
dom number. This randomness prevents EM from
starting on a saddle point by breaking symmetries;
Petrov et al describe this in more detail.
Merge: After EM has run to completion, we
have a new grammar with twice as many symbols
and eight times as many rules. Many of these sym-
bols may not be necessary, however. For instance,
nouns may require substantial refinement to dis-
tinguish a number of different actors and objects,
where determiners might not require much refine-
ment at all. Therefore, we discard the splits that
led to the least increase in likelihood, and then
reestimate the grammar once again.
223
(a) Input:
? the man plays the piano
? the guy plays the keyboard
(b) Parses:
? (S (NP (DT the) (NN man))
(VP (VBZ plays)
(NP (DT the) (NN piano)))
? (S (NP (DT the) (NN guy))
(VP (VBZ plays)
(NP (DT the) (NN keyboard)))
(c) Parses with latent annotations:
? (S (NP0 (DT the) (NN0 man))
(VP (VBZ plays)
(NP1 (DT the) (NN1 piano)))
? (S (NP0 (DT the) (NN0 guy))
(VP (VBZ plays)
(NP1 (DT the) (NN1 keyboard)))
(d) Refined grammar:
S ? NP0 VP
NP0 ? DT NN0
NP1 ? DT NN1
NP ? VBZ NP1
DT ? the
NN0 ? man | guy
NN1 ? piano | keyboard
VBZ ? plays
Figure 1: Example of hypergraph induction. First
a conventional Treebank parser converts input ut-
terances (a) into parse trees (b). A grammar could
be directly read from this small treebank, but it
would conflate all phrases of the same type. In-
stead we induce latent refinements of this small
treebank (c). The resulting grammar (d) can match
and generate novel variants of these inputs, such
as the man plays the keyboard and the buy plays
the piano. While this simplified example sug-
gests a single hard assignment of latent annota-
tions to symbols, in practice we maintain a dis-
tribution over these latent annotations and extract
a weighted grammar.
Iteration: We run this process in series. First
the original grammar is split, then some of the
least useful splits are discarded. This refined
grammar is then split again, with the least useful
splits discarded once again. We repeat for a num-
ber of iterations based on task accuracy.
Final grammar estimation: The EM proce-
dure used during split and merge assigns fractional
counts c(? ? ? ) to each refined symbol Xi and each
production Xi ? Yj Zk. We estimate the final
grammar using these fractional counts.
P (Xi ? Yj Zk) =
c(Xi, Yj , Zk)
c(Xi)
In Petrov et al, these latent refinements are later
discarded as the goal is to find the best parse with
the original coarse symbols. Here, we retain the
latent refinements during parsing, since they dis-
tinguish semantically related utterances from un-
related utterances. Note in Figure 1 how NN0
and NN1 refer to different objects; were we to ig-
nore that distinction, the parser would recognize
semantically different utterances such as the piano
plays the piano.
2.2 Pruning and smoothing
For both speed and accuracy, we may also prune
the resulting rules. Pruning low probability rules
increases the speed of parsing, and tends to in-
crease the precision of the matching operation at
the cost of recall. Here we only use an absolute
threshold; we vary this threshold and inspect the
impact on task accuracy. Once the fully refined
grammar has been trained, we only retain those
rules with a probability above some threshold. By
varying this threshold t we can adjust precision
and recall: as the low probability rules are re-
moved from the grammar, precision tends to in-
crease and recall tends to decrease.
Another critical issue, especially in these small
grammars, is smoothing. When parsing with a
grammar obtained from only 20 to 50 sentences,
we are very likely to encounter words that have
never been seen before. We may reasonably re-
ject such sentences under the assumption that they
are describing words not present in the training
corpus. However, this may be overly restrictive:
we might see additional adjectives, for instance.
In this work, we perform a very simple form of
smoothing. If the fractional count of a word given
a pre-terminal symbol falls below a threshold k,
then we consider that instance rare and reserve a
fraction of its probability mass for unseen words.
This accounts for lexical variation of the ground-
ing, especially in the least consistently used words.
Substantial speedups could be attained by us-
ing finite state approximations of this grammar:
matching complexity drops to cubic to linear in
the length of the input. A broad range of approxi-
mations are available (Nederhof, 2000). Since the
small grammars in our evaluation below seldom
exhibit self-embedding (latent state identification
224
tends to remove recursion), these approximations
would often be tight.
3 Experimental evaluation
We explore a task in description recognition.
Given a large set of videos and a number of de-
scriptions for each video (Chen and Dolan, 2011),
we build a system that can recognize fluent and
accurate descriptions of videos. Such a recognizer
has a number of uses. One example currently in
evaluation is a novel CAPTCHAs: to differentiate
a human from a bot, a video is presented, and the
response must be a reasonably accurate and fluent
description of this video.
We split the above data into training and test.
From the training sets, we build a set of recogniz-
ers. Then we present these recognizers with a se-
ries of inputs, some of which are from the held out
set of correct descriptions of this video, and some
of which are from descriptions of other videos.
Based on discussions with authors of CAPTCHA
systems, a ratio of actual users to spammers of 2:1
seemed reasonable, so we selected one negative
example for every two positives. This simulates
the accuracy of the system when presented with a
simple bot that supplies random, well-formed text
as CAPTCHA answers.1
As a baseline, we compare against a simple tf-
idf approach. In this baseline we first pool all
the training descriptions of the video into a sin-
gle virtual document. We gather term frequen-
cies and inverse document frequencies across the
whole corpus. An incoming utterance to be classi-
fied is scored by computing the dot product of its
counted terms with each document; it is assigned
to the document with the highest dot product (co-
sine similarity).
Table 2 demonstrates that a baseline tf-idf ap-
proach is a reasonable starting point. An oracle
selection from among the top three is the best per-
formance ? clearly this is a reasonable approach.
That said, grammar based approach shows im-
provements over the baseline tf-idf, especially in
recall. Recall is crucial in a CAPTCHA style task:
if we fail to recognize utterances provided by hu-
mans, we risk frustration or abandonment of the
service protected by the CAPTCHA. The relative
importance of false positives versus false negatives
1A bot might perform object recognition on the videos and
supply a stream of object names. We might simulate this by
classifying utterances consisting of appropriate object words
but without appropriate syntax or function words.
Total videos 2,029
Training descriptions 22,198
types 5,497
tokens 159,963
Testing descriptions 15,934
types 4,075
tokens 114,399
Table 1: Characteristics of the evaluation data.
The descriptions from the video description cor-
pus are randomly partitioned into training and test.
(a)
Algorithm S k Prec Rec F-0
tf-idf 99.9 46.6 63.6
tf-idf (top 3 oracle) 99.9 65.3 79.0
grammar 2 1 86.6 51.5 64.6
2 4 80.2 62.6 70.3
2 16 74.2 74.2 74.2
2 32 73.5 76.4 74.9
3 1 91.1 43.9 59.2
3 4 83.7 54.4 65.9
3 16 77.3 65.7 71.1
3 32 76.4 68.1 72.0
4 1 94.1 39.7 55.8
4 4 85.5 51.1 64.0
4 16 79.1 61.5 69.2
4 32 78.2 63.9 70.3
(b)
t S Prec Rec F-0
? 4.5? 10?5 2 74.8 73.9 74.4
? 4.5? 10?5 3 79.6 60.9 69.0
? 4.5? 10?5 4 82.5 53.2 64.7
? 3.1? 10?7 2 74.2 75.0 74.6
? 3.1? 10?7 3 78.1 64.6 70.7
? 3.1? 10?7 4 80.7 58.8 68.1
> 0 2 73.4 76.4 74.9
> 0 3 76.4 68.1 72.0
> 0 4 78.2 63.9 70.3
Table 2: Experimental results. (a) Comparison of
tf-idf baseline against grammar based approach,
varying several free parameters. An oracle checks
if the correct video is in the top three. For the
grammar variants, the number of splits S and the
smoothing threshold k are varied. (b) Variations
on the rule pruning threshold t and number of
split-merge rounds S. > 0 indicates that all rules
are retained. Here the smoothing threshold k is
fixed at 32.
225
(a) Input descriptions:
? A cat pops a bunch of little balloons that are on the groung.
? A dog attacks a bunch of balloons.
? A dog is biting balloons and popping them.
? A dog is playing balloons.
? A dog is playing with balloons.
? A dog is playing with balls.
? A dog is popping balloons with its teeth.
? A dog is popping balloons.
? A dog is popping balloons.
? A dog plays with a bunch of balloons.
? A small dog is attacking balloons.
? The dog enjoyed popping balloons.
? The dog popped the balloons.
(b) Top ranked yields from the resulting grammar:
+0.085 A dog is popping balloons.
+0.062 A dog is playing with balloons.
+0.038 A dog is playing balloons.
0.038 A dog is attacking balloons.
+0.023 A dog plays with a bunch of balloons.
+0.023 A dog attacks a bunch of balloons.
0.023 A dog pops a bunch of balloons.
0.023 A dog popped a bunch of balloons.
0.023 A dog enjoyed a bunch of balloons.
0.018 The dog is popping balloons.
0.015 A dog is biting balloons.
0.015 A dog is playing with them.
0.015 A dog is playing with its teeth.
Figure 2: Example yields from a small grammar. The descriptions in (a) were parsed as-is (including the
typographical error ?groung?), and a refined grammar was trained with 4 splits. The top k yields from
this grammar along with the probability of that derivation are listed in (b). A ?+? symbol indicates that
the yield was in the training set. No smoothing or pruning was performed on this grammar.
may vary depending on the underlying resource.
Adjusting the free parameters of this method al-
lows us to achieve different thresholds. We can
see that rule pruning does not have a large impact
on overall results, though it does allow yet another
means of tradiing off precision vs. recall.
4 Conclusions
We have presented a method for automatically
constructing compact representations of linguis-
tic variation. Although the initial evaluation only
explored a simple recognition task, we feel the
underlying approach is relevant to many linguis-
tic tasks including machine translation evalua-
tion, and natural language command and con-
trol systems. The induction procedure is rather
simple but effective, and addresses some of the
reordering limitations associated with prior ap-
proaches.(Barzilay and Lee, 2003) In effect, we
are performing a multiple sequence alignment that
allows reordering operations. The refined symbols
of the grammar act as a correspondence between
related inputs.
The quality of the input parser is crucial. This
method only considers one possible parse of the
input. A straightforward extension would be to
consider an n-best list or packed forest of input
parses, which would allow the method to move
past errors in the first input process. Perhaps also
this reliance on symbols from the original Tree-
bank is not ideal. We could merge away some or
all of the original distinctions, or explore different
parameterizations of the grammar that allow more
flexibility in parsing.
The handling of unseen words is very simple.
We are investigating means of including addi-
tional paraphrase resources into the training to in-
crease the effective lexical knowledge of the sys-
tem. It is inefficient to learn each grammar inde-
pendently. By sharing parameters across different
groundings, we should be able to identify Seman-
tic Neighborhoods with fewer training instances.
Acknowledgments
We would like to thank William Dolan and the
anonymous reviewers for their valuable feedback.
References
Regina Barzilay and Lillian Lee. 2003. Learn-
ing to paraphrase: An unsupervised approach us-
ing multiple-sequence alignment. In Proceedings of
NAACL-HLT.
David Chen and William Dolan. 2011. Collecting
highly parallel data for paraphrase evaluation. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 190?200, Portland, Ore-
gon, USA, June. Association for Computational Lin-
guistics.
Markus Dreyer and Daniel Marcu. 2012. Hyter:
Meaning-equivalent semantics for translation eval-
uation. In Proceedings of the 2012 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 162?171, Montre?al, Canada, June.
Association for Computational Linguistics.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting of the Association for Com-
putational Linguistics, pages 423?430, Sapporo,
226
Japan, July. Association for Computational Linguis-
tics.
Mark-Jan Nederhof. 2000. Practical experiments with
regular approximation of context-free languages.
Computational Linguistics, 26(1):17?44, March.
Bo Pang, Kevin Knight, and Daniel Marcu. 2003.
Syntax-based alignment of multiple translations:
Extracting paraphrases and generating new sen-
tences.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and
interpretable tree annotation. In Proceedings of
the 21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 433?440,
Sydney, Australia, July. Association for Computa-
tional Linguistics.
Chris Quirk, Pallavi Choudhury, Jianfeng Gao, Hisami
Suzuki, Kristina Toutanova, Michael Gamon, Wen-
tau Yih, Colin Cherry, and Lucy Vanderwende.
2012. Msr splat, a language analysis toolkit. In
Proceedings of the Demonstration Session at the
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, pages 21?24, Montre?al,
Canada, June. Association for Computational Lin-
guistics.
Cyrus Rashtchian, Peter Young, Micah Hodosh, and
Julia Hockenmaier. 2010. Collecting image annota-
tions using amazon?s mechanical turk. In Proceed-
ings of the NAACL HLT 2010 Workshop on Creating
Speech and Language Data with Amazon?s Mechan-
ical Turk, pages 139?147, Los Angeles, June. Asso-
ciation for Computational Linguistics.
Luis Von Ahn, Manuel Blum, Nicholas J. Hopper, and
John Langford. 2003. Captcha: Using hard ai prob-
lems for security. In Eli Biham, editor, Advances in
Cryptology ? EUROCRYPT 2003, volume 2656 of
Lecture Notes in Computer Science, pages 294?311.
Springer Berlin Heidelberg.
227
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 676?686,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Graph-based Semi-Supervised Learning of Translation Models from
Monolingual Data
Avneesh Saluja
?
Carnegie Mellon University
Pittsburgh, PA 15213, USA
avneesh@cs.cmu.edu
Hany Hassan, Kristina Toutanova, Chris Quirk
Microsoft Research
Redmond, WA 98502, USA
hanyh,kristout,chrisq@microsoft.com
Abstract
Statistical phrase-based translation learns
translation rules from bilingual corpora,
and has traditionally only used monolin-
gual evidence to construct features that
rescore existing translation candidates. In
this work, we present a semi-supervised
graph-based approach for generating new
translation rules that leverages bilingual
and monolingual data. The proposed tech-
nique first constructs phrase graphs using
both source and target language mono-
lingual corpora. Next, graph propaga-
tion identifies translations of phrases that
were not observed in the bilingual cor-
pus, assuming that similar phrases have
similar translations. We report results
on a large Arabic-English system and a
medium-sized Urdu-English system. Our
proposed approach significantly improves
the performance of competitive phrase-
based systems, leading to consistent im-
provements between 1 and 4 BLEU points
on standard evaluation sets.
1 Introduction
Statistical approaches to machine translation
(SMT) use sentence-aligned, parallel corpora to
learn translation rules along with their probabil-
ities. With large amounts of data, phrase-based
translation systems (Koehn et al, 2003; Chiang,
2007) achieve state-of-the-art results in many ty-
pologically diverse language pairs (Bojar et al,
2013). However, the limiting factor in the suc-
cess of these techniques is parallel data availabil-
ity. Even in resource-rich languages, learning re-
liable translations of multiword phrases is a chal-
lenge, and an adequate phrasal inventory is crucial
?
This work was done while the first author was interning
at Microsoft Research
for effective translation. This problem is exacer-
bated in the many language pairs for which par-
allel resources are either limited or nonexistent.
While parallel data is generally scarce, monolin-
gual resources exist in abundance and are being
created at accelerating rates. Can we use monolin-
gual data to augment the phrasal translations ac-
quired from parallel data?
The challenge of learning translations from
monolingual data is of long standing interest,
and has been approached in several ways (Rapp,
1995; Callison-Burch et al, 2006; Haghighi et
al., 2008; Ravi and Knight, 2011). Our work in-
troduces a new take on the problem using graph-
based semi-supervised learning to acquire trans-
lation rules and probabilities by leveraging both
monolingual and parallel data resources. On the
source side, labeled phrases (those with known
translations) are extracted from bilingual corpora,
and unlabeled phrases are extracted from mono-
lingual corpora; together they are embedded as
nodes in a graph, with the monolingual data de-
termining edge strengths between nodes (?2.2).
Unlike previous work (Irvine and Callison-Burch,
2013a; Razmara et al, 2013), we use higher order
n-grams instead of restricting to unigrams, since
our approach goes beyond OOV mitigation and
can enrich the entire translation model by using
evidence from monolingual text. This enhance-
ment alone results in an improvement of almost
1.4 BLEU points. On the target side, phrases ini-
tially consisting of translations from the parallel
data are selectively expanded with generated can-
didates (?2.1), and are embedded in a target graph.
We then limit the set of translation options for
each unlabeled source phrase (?2.3), and using
a structured graph propagation algorithm, where
translation information is propagated from la-
beled to unlabeled phrases proportional to both
source and target phrase similarities, we esti-
mate probability distributions over translations for
676
Source! Target!
el gato!
los gatos!
un gato! cat!
the cat! the cats!
a cat!
Target! Prob.!
the cat! 0.7!
cat! 0.15!
?! ?!
felino!
canino! el perro!
Target! Prob.!
canine! 0.6!
dog! 0.3!
?! ?!
Target! Prob.!
the cats! 0.8!
cats! 0.1!
?! ?!
Target! Prob.!
the dog! 0.9!
dog! 0.05!
?! ?!
canine!
dog!
the dog!
catlike!
Figure 1: Example source and target graphs used in our approach. Labeled phrases on the source side are black (with their
corresponding translations on the target side also black); unlabeled and generated (?2.1) phrases on the source and target sides
respectively are white. Labeled phrases also have conditional probability distributions defined over target phrases, which are
extracted from the parallel corpora.
the unlabeled source phrases (?2.4). The addi-
tional phrases are incorporated in the SMT sys-
tem through a secondary phrase table (?2.5). We
evaluated the proposed approach on both Arabic-
English and Urdu-English under a range of sce-
narios (?3), varying the amount and type of mono-
lingual corpora used, and obtained improvements
between 1 and 4 BLEU points, even when using
very large language models.
2 Generation & Propagation
Our goal is to obtain translation distributions for
source phrases that are not present in the phrase
table extracted from the parallel corpus. Both par-
allel and monolingual corpora are used to obtain
these probability distributions over target phrases.
We assume that sufficient parallel resources ex-
ist to learn a basic translation model using stan-
dard techniques, and also assume the availability
of larger monolingual corpora in both the source
and target languages. Although our technique ap-
plies to phrases of any length, in this work we con-
centrate on unigram and bigram phrases, which
provides substantial computational cost savings.
Monolingual data is used to construct separate
similarity graphs over phrases (word sequences),
as illustrated in Fig. 1. The source similarity graph
consists of phrase nodes representing sequences of
words in the source language. If a source phrase
is found in the baseline phrase table it is called a
labeled phrase: its conditional empirical probabil-
ity distribution over target phrases (estimated from
the parallel data) is used as the label, and is sub-
sequently never changed. Otherwise it is called an
unlabeled phrase, and our algorithm finds labels
(translations) for these unlabeled phrases, with the
help of the graph-based representation. The la-
bel space is thus the phrasal translation inventory,
and like the source side it can also be represented
in terms of a graph, initially consisting of target
phrase nodes from the parallel corpus.
For the unlabeled phrases, the set of possible
target translations could be extremely large (e.g.,
all target language n-grams). Therefore, we first
generate and fix a list of possible target transla-
tions for each unlabeled source phrase. We then
propagate by deriving a probability distribution
over these target phrases using graph propagation
techniques. Next, we will describe the generation,
graph construction and propagation steps.
2.1 Generation
The objective of the generation step is to popu-
late the target graph with additional target phrases
for all unlabeled source phrases, yielding the full
set of possible translations for the phrase. Prior to
generation, one phrase node for each target phrase
occurring in the baseline phrase table is added to
the target graph (black nodes in Fig. 1?s target
graph). We only consider target phrases whose
source phrase is a bigram, but it is worth noting
that the target phrases are of variable length.
The generation component is based on the ob-
servation that for structured label spaces, such as
translation candidates for source phrases in SMT,
even similar phrases have slightly different labels
(target translations). The exponential dependence
677
of the sizes of these spaces on the length of in-
stances is to blame. Thus, the target phrase inven-
tory from the parallel corpus may be inadequate
for unlabeled instances. We therefore need to en-
rich the target or label space for unknown phrases.
A na??ve way to achieve this goal would be to ex-
tract all n-grams, from n = 1 to a maximum n-
gram order, from the monolingual data, but this
strategy would lead to a combinatorial explosion
in the number of target phrases.
Instead, by intelligently expanding the target
space using linguistic information such as mor-
phology (Toutanova et al, 2008; Chahuneau et al,
2013), or relying on the baseline system to gener-
ate candidates similar to self-training (McClosky
et al, 2006), we can tractably propose novel trans-
lation candidates (white nodes in Fig. 1?s target
graph) whose probabilities are then estimated dur-
ing propagation. We refer to these additional can-
didates as ?generated? candidates.
To generate new translation candidates using
the baseline system, we decode each unlabeled
source bigram to generate its m-best translations.
This set of candidate phrases is filtered to include
only n-grams occurring in the target monolingual
corpus, and helps to prune passed-through OOV
words and invalid translations. To generate new
translation candidates using morphological infor-
mation, we morphologically segment words into
prefixes, stem, and suffixes using linguistic re-
sources. We assume that a morphological ana-
lyzer which provides context-independent analysis
of word types exists, and implements the functions
STEM(f ) and STEM(e) for source and target word
types. Based on these functions, source and target
sequences of words can be mapped to sequences
of stems. The morphological generation step adds
to the target graph all target word sequences from
the monolingual data that map to the same stem
sequence as one of the target phrases occurring in
the baseline phrase table. In other words, this step
adds phrases that are morphological variants of ex-
isting phrases, differing only in their affixes.
2.2 Graph Construction
At this stage, there exists a list of source bigram
phrases, both labeled and unlabeled, as well as a
list of target language phrases of variable length,
originating from both the phrase table and the gen-
eration step. To determine pairwise phrase similar-
ities in order to embed these nodes in their graphs,
we utilize the monolingual corpora on both the
source and target sides to extract distributional
features based on the context surrounding each
phrase. For a phrase, we look at the pwords before
and the p words after the phrase, explicitly distin-
guishing between the two sides, but not distance
(i.e., bag of words on each side). Co-occurrence
counts for each feature (context word) are accu-
mulated over the monolingual corpus, and these
counts are converted to pointwise mutual infor-
mation (PMI) values, as is standard practice when
computing distributional similarities. Cosine sim-
ilarity between two phrases? PMI vectors is used
for similarity, and we take only the k most simi-
lar phrases for each phrase, to create a k-nearest
neighbor similarity matrix for both source and tar-
get language phrases. These graphs are distinct,
in that propagation happens within the two graphs
but not between them.
While accumulating co-occurrence counts for
each phrase, we also maintain an inverted index
data structure, which is a mapping from features
(context words) to phrases that co-occur with that
feature within a window of p.
1
The inverted index
structure reduces the graph construction cost from
?(n
2
), by only computing similarities for a sub-
set of all possible pairs of phrases, namely other
phrases that have at least one feature in common.
2.3 Candidate Translation List Construction
As mentioned previously, we construct and fix
a set of translation candidates, i.e., the label set
for each unlabeled source phrase. The probabil-
ity distribution over these translations is estimated
through graph propagation, and the probabilities
of items outside the list are assumed to be zero.
We obtain these candidates from two sources:
2
1. The union of each unlabeled phrase?s la-
beled neighbors? labels, which represents the
set of target phrases that occur as transla-
tions of source phrases that are similar to
the unlabeled source phrase. For un gato in
Fig. 1, this source would yield the cat and
cat, among others, as candidates.
2. The generated candidates for the unlabeled
phrase ? the ones from the baseline system?s
1
The q most frequent words in the monolingual corpus
were removed as keys from this mapping, as these high en-
tropy features do not provide much information.
2
We also obtained the k-nearest neighbors of the transla-
tion candidates generated through these methods by utilizing
the target graph, but this had minimal impact.
678
decoder output, or from a morphological gen-
erator (e.g., a cat and catlike in Fig. 1).
The morphologically-generated candidates for a
given source unlabeled phrase are initially de-
fined as the target word sequences in the mono-
lingual data that have the same stem sequence
as one of the baseline?s target translations for a
source phrase which has the same stem sequence
as the unlabeled source phrase. These candidates
are scored using stem-level translation probabili-
ties, morpheme-level lexical weighting probabili-
ties, and a language model, and only the top 30
candidates are included.
After obtaining candidates from these two pos-
sible sources, the list is sorted by forward lexical
score, using the lexical models of the baseline sys-
tem. The top r candidates are then chosen for each
phrase?s translation candidate list.
In Figure 2 we provide example outputs of
our system for a handful of unlabeled source
phrases, and explicitly note the source of the trans-
lation candidate (?G? for generated, ?N? for labeled
neighbor?s label).
2.4 Graph Propagation
A graph propagation algorithm transfers label in-
formation from labeled nodes to unlabeled nodes
by following the graph?s structure. In some appli-
cations, a label may consist of class membership
information, e.g., each node can belong to one of
a certain number of classes. In our problem, the
?label? for each node is actually a probability dis-
tribution over a set of translation candidates (target
phrases). For a given node f , let e refer to a can-
didate in the label set for node f ; then in graph
propagation, the probability of candidate e given
source phrase f in iteration t + 1 is:
P
t+1
(e|f) =
X
j2N (f)
T
s
(j|f)P
t
(e|j) (1)
where the setN (f) contains the (labeled and unla-
beled) neighbors of node f , and T
s
(j|f) is a term
that captures how similar nodes f and j are. This
quantity is also known as the propagation proba-
bility, and its exact form will depend on the type
of graph propagation algorithm used. For our pur-
poses, node f is a source phrasal node, the set
N (f) refers to other source phrases that are neigh-
bors of f (restricted to the k-nearest neighbors as
in ?2.2), and the aim is to estimate P (e|f), the
probability of target phrase e being a phrasal trans-
lation of source phrase f .
A classic propagation algorithm that has been
suitably modified for use in bilingual lexicon in-
duction (Tamura et al, 2012; Razmara et al, 2013)
is the label propagation (LP) algorithm of Zhu et
al. (2003). In this case, T
s
(f, j) is chosen to be:
T
s
(j|f) =
w
s
f,j
P
j
0
2N (f)
w
s
f,j
0
(2)
where w
s
f,j
is the cosine similarity (as computed
in ?2.2) between phrase f and phrase j on side s
(the source side).
As evident in Eq. 2, LP only takes into account
source language similarity of phrases. To see this
observation more clearly, let us reformulate Eq. 1
more generally as:
P
t+1
(e|f) =
X
j2N (f)
T
s
(j|f)
X
e
0
2H(j)
T
t
(e
0
|e)P
t
(e
0
|j) (3)
where H(j) is the translation candidate set for
source phrase j, and T
t
(e
0
|e) is the propagation
probability between nodes or phrases e and e
0
on the target side. We have simply replaced
P
t
(e|j) with
P
e
0
2H(j)
T
t
(e
0
|e)P
t
(e
0
|j), defining it
in terms of j?s translation candidate list.
Note that in the original LP formulation the tar-
get side information is disregarded, i.e., T
t
(e
0
|e) =
1 if and only if e = e
0
and 0 otherwise. As a
result, LP is suboptimal for our needs, since it is
unable to appropriately handle generated transla-
tion candidates for the unlabeled phrases. These
translation candidates are usually not present as
translations for the labeled phrases (or for the la-
beled phrases that neighbor the unlabeled one in
question). When propagating information from
the labeled phrases, such candidates will obtain
no probability mass since e 6= e
0
. Thus, due to
the setup of the problem, LP naturally biases away
from translation candidates produced during the
generation step (?2.1).
2.4.1 Structured Label Propagation
The label set we are considering has a similarity
structure encoded by the target graph. How can
we exploit this structure in graph propagation on
the source graph? In Liu et al (2012), the authors
generalize label propagation to structured label
propagation (SLP) in an effort to work more el-
egantly with structured labels. In particular, the
definition of target similarity is similar to that of
source similarity:
T
t
(e
0
|e) =
w
t
e,e
0
P
e
00
2H(j)
w
t
e,e
00
(4)
679
Therefore, the final update equation in SLP is:
P
t+1
(e|f) =
X
j2N (f)
T
s
(j|f)
X
e
0
2H(j)
T
t
(e
0
|e)P
t
(e
0
|j) (5)
With this formulation, even if e 6= e
0
, the simi-
larity T
t
(e
0
|e) as determined by the target phrase
graph will dictate propagation probability. We re-
normalize the probability distributions after each
propagation step to sum to one over the fixed list
of translation candidates, and run the SLP algo-
rithm to convergence.
3
2.5 Phrase-based SMT Expansion
After graph propagation, each unlabeled phrase
is labeled with a categorical distribution over
the set of translation candidates defined in ?2.3.
In order to utilize these newly acquired phrase
pairs, we need to compute their relevant features.
The phrase pairs have four log-probability fea-
tures with two likelihood features and two lexical
weighting features. In addition, we use a sophis-
ticated lexicalized hierarchical reordering model
(HRM) (Galley and Manning, 2008) with five fea-
tures for each phrase pair.
We utilize the graph propagation-estimated for-
ward phrasal probabilities P(e|f) as the forward
likelihood probabilities for the acquired phrases;
to obtain the backward phrasal probability for a
given phrase pair, we make use of Bayes? Theo-
rem:
P(f |e) =
P(e|f)P(f)
P(e)
where the marginal probabilities of source and tar-
get phrases e and f are obtained from the counts
extracted from the monolingual data. The baseline
system?s lexical models are used for the forward
and backward lexical scores. The HRM probabil-
ities for the new phrase pairs are estimated from
the baseline system by backing-off to the average
values for phrases with similar length.
3 Evaluation
We performed an extensive evaluation to exam-
ine various aspects of the approach along with
overall system performance. Two language pairs
were used: Arabic-English and Urdu-English. The
Arabic-English evaluation was used to validate the
decisions made during the development of our
3
Empirically within a few iterations and a wall-clock time
of less than 10 minutes in total.
method and also to highlight properties of the
technique. With it, in ?3.2 we first analyzed the
impact of utilizing phrases instead of words and
SLP instead of LP; the latter experiment under-
scores the importance of generated candidates. We
also look at how adding morphological knowledge
to the generation process can further enrich per-
formance. In ?3.3, we then examined the effect of
using a very large 5-gram language model train-
ing on 7.5 billion English tokens to understand the
nature of the improvements in ?3.2. The Urdu to
English evaluation in ?3.4 focuses on how noisy
parallel data and completely monolingual (i.e., not
even comparable) text can be used for a realistic
low-resource language pair, and is evaluated with
the larger language model only. We also exam-
ine how our approach can learn from noisy parallel
data compared to the traditional SMT system.
Baseline phrasal systems are used both for com-
parison and for generating translation candidates
for unlabeled phrases as described in ?2.1. The
baseline is a state-of-the-art phrase-based system;
we perform word alignment using a lexicalized
hidden Markov model, and then the phrase ta-
ble is extracted using the grow-diag-final
heuristic (Koehn et al, 2003). The 13 baseline
features (2 lexical, 2 phrasal, 5 HRM, and 1 lan-
guage model, word penalty, phrase length feature
and distortion penalty feature) were tuned using
MERT (Och, 2003), which is also used to tune
the 4 feature weights introduced by the secondary
phrase table (2 lexical and 2 phrasal, other fea-
tures being shared between the two tables). For
all systems, we use a distortion limit of 4. We use
case-insensitive BLEU (Papineni et al, 2002) to
evaluate translation quality.
3.1 Datasets
Bilingual corpus statistics for both language pairs
are presented in Table 2. For Arabic-English, our
training corpus consisted of 685k sentence pairs
from standard LDC corpora
4
. The NIST MT06
and MT08 Arabic-English evaluation sets (com-
bining the newswire and weblog domains for both
sets), with four references each, were used as
tuning and testing sets respectively. For Urdu-
English, the training corpus was provided by the
LDC for the NIST Urdu-English MT evaluation,
and most of the data was automatically acquired
from the web, making it quite noisy. After fil-
tering, there are approximately 65k parallel sen-
4
LDC2007T08 and LDC2008T09
680
Parameter Description Value
m m-best candidate list size when bootstrapping candidates in generation stage. 100
p Window size on each side when extracting features for phrases. 2
q Filter the q most frequent words when storing the inverted index data structure for graph construction.
Both source and target sides share the same value.
25
k Number of neighbors stored for each phrase for both source and target graphs. This parameter controls
the sparsity of the graph.
500
r Maximum size of translation candidate list for unlabeled phrases. 20
Table 1: Parameters, explanation of their function, and value chosen.
tences; these were supplemented by an additional
100k dictionary entries. Tuning and test data con-
sisted of the MT08 and MT09 evaluation corpora,
once again a mixture of news and web text.
Corpus Sentences Words (Src)
Ar-En Train 685,502 17,055,168
Ar-En Tune (MT06) 1,664 33,739
Ar-En Test (MT08) 1,360 42,472
Ur-En Train 165,159 1,169,367
Ur-En Tune (MT08) 1,864 39,925
Ur-En Test (MT09) 1,792 39,922
Table 2: Bilingual corpus statistics for the Arabic-English
and Urdu-English datasets used.
Table 3 contains statistics for the monolingual
corpora used in our experiments. From these cor-
pora, we extracted all sentences that contained at
least one source or target phrase match to com-
pute features for graph construction. For the Ara-
bic to English experiments, the monolingual cor-
pora are taken from the AFP Arabic and English
Gigaword corpora and are of a similar date range
to each other (1994-2010), rendering them compa-
rable but not sentence-aligned or parallel.
Corpus Sentences Words
Ar Comparable 10.2m 290m
En I Comparable 29.8m 900m
Ur Noisy Parallel 470k 5m
En II Noisy Parallel 470k 4.7m
Ur Non-Comparable 7m 119m
En II Non-Comparable 17m 510m
Table 3: Monolingual corpus statistics for the Arabic-English
and Urdu-English evaluations. The monolingual corpora can
be sub-divided into comparable, noisy parallel, and non-
comparable components. En I refers to the English side of
the Arabic-English corpora, and En II to the English side of
the Urdu-English corpora.
For the Urdu-English experiments, completely
non-comparable monolingual text was used for
graph construction; we obtained the Urdu side
through a web-crawler, and a subset of the AFP
Gigaword English corpus was used for English. In
addition, we obtained a corpus from the ELRA
5
,
which contains a mix of parallel and monolingual
data; based on timestamps, we extracted a compa-
rable English corpus for the ELRA Urdu monolin-
gual data to form a roughly 470k-sentence ?noisy
parallel? set. We used this set in two ways: ei-
ther to augment the parallel data presented in Table
2, or to augment the non-comparable monolingual
data in Table 3 for graph construction.
For the parameters introduced throughout the
text, we present in Table 1 a reminder of their in-
terpretation as well as the values used in this work.
3.2 Experimental Variations
In our first set of experiments, we looked at the im-
pact of choosing bigrams over unigrams as our ba-
sic unit of representation, along with performance
of LP (Eq. 2) compared to SLP (Eq. 4). Re-
call that LP only takes into account source sim-
ilarity; since the vast majority of generated can-
didates do not occur as labeled neighbors? labels,
restricting propagation to the source graph dras-
tically reduces the usage of generated candidates
as labels, but does not completely eliminate it. In
these experiments, we utilize a reasonably-sized
4-gram language model trained on 900m English
tokens, i.e., the English monolingual corpus.
Table 4 presents the results of these variations;
overall, by taking into account generated candi-
dates appropriately and using bigrams (?SLP 2-
gram?), we obtained a 1.13 BLEU gain on the
test set. Using unigrams (?SLP 1-gram?) actu-
ally does worse than the baseline, indicating the
importance of focusing on translations for sparser
bigrams. While LP (?LP 2-gram?) does reason-
ably well, its underperformance compared to SLP
underlines the importance of enriching the trans-
lation space with generated candidates and han-
dling these candidates appropriately.
6
In ?SLP-
5
ELRA-W0038
6
It is relatively straightforward to combine both unigrams
and bigrams in one source graph, but for experimental clarity
we did not mix these phrase lengths.
681
HalfMono?, we use only half of the monolingual
comparable corpora, and still obtain an improve-
ment of 0.56 BLEU points, indicating that adding
more monolingual data is likely to improve the
system further. Interestingly, biasing away from
generated candidates using all the monolingual
data (?LP 2-gram?) performs similarly to using
half the monolingual corpora and handling gener-
ated candidates properly (?SLP-HalfMono?).
BLEU
Setup Tune Test
Baseline 39.33 38.09
SLP 1-gram 39.47 37.85
LP 2-gram 40.75 38.68
SLP 2-gram 41.00 39.22
SLP-HalfMono 2-gram 40.82 38.65
SLP+Morph 2-gram 41.02 39.35
Table 4: Results for the Arabic-English evaluation. The LP
vs. SLP comparison highlights the importance of target side
enrichment via translation candidate generation, 1-gram vs.
2-gram comparisons highlight the importance of emphasiz-
ing phrases, utilizing half the monolingual data shows sensi-
tivity to monolingual corpus size, and adding morphological
information results in additional improvement.
Additional morphologically generated candi-
dates were added in this experiment as detailed in
?2.3. We used a simple hand-built Arabic morpho-
logical analyzer that segments word types based
on regular expressions, and an English lexicon-
based morphological analyzer. The morphological
candidates add a small amount of improvement,
primarily by targeting genuine OOVs.
3.3 Large Language Model Effect
In this set of experiments, we examined if the
improvements in ?3.2 can be explained primar-
ily through the extraction of language model char-
acteristics during the semi-supervised learning
phase, or through orthogonal pieces of evidence.
Would the improvement be less substantial had we
used a very large language model?
To answer this question we trained a 5-gram
language model on 570M sentences (7.6B tokens),
with data from various sources including the Gi-
gaword corpus
7
, WMT and European Parliamen-
tary Proceedings
8
, and web-crawled data from
Wikipedia and the web. Only m-best generated
candidates from the baseline were considered dur-
ing generation, along with labeled neighbors? la-
bels.
7
LDC2011T07
8
http://www.statmt.org/wmt13/
BLEU
Setup Tune Test
Baseline+LargeLM 41.48 39.86
SLP+LargeLM 42.82 41.29
Table 5: Results with the large language model scenario. The
gains are even better than with the smaller language model.
Table 5 presents the results of using this lan-
guage model. We obtained a robust, 1.43-BLEU
point gain, indicating that the addition of the
newly induced phrases provided genuine transla-
tion improvements that cannot be compensated by
the language model effect. Further examination of
the differences between the two systems yielded
that most of the improvements are due to better
bigrams and trigrams, as indicated by the break-
down of the BLEU score precision per n-gram,
and primarily leverages higher quality generated
candidates from the baseline system. We analyze
the output of these systems further in the output
analysis section below (?3.5).
3.4 Urdu-English
In order to evaluate the robustness of these results
beyond one language pair, we looked at Urdu-
English, a low resource pair likely to benefit from
this approach. In this set of experiments, we used
the large language model in ?3.3, and only used
baseline-generated candidates. We experimented
with two extreme setups that differed in the data
assumed parallel, from which we built our base-
line system, and the data treated as monolingual,
from which we built our source and target graphs.
In the first setup, we use the noisy parallel
data for graph construction and augment the non-
comparable corpora with it:
? parallel: ?Ur-En Train?
? Urdu monolingual: ?Ur Noisy Parallel?+?Ur
Non-Comparable?
? English monolingual: ?En II Noisy Paral-
lel?+?En II Non-Comparable?
The results from this setup are presented as ?Base-
line? and ?SLP+Noisy? in Table 6. In the second
setup, we train a baseline system using the data in
Table 2, augmented with the noisy parallel text:
? parallel: ?Ur-En Train?+?Ur Noisy Paral-
lel?+?En II Noisy Parallel?
? Urdu monolingual: ?Ur Non-Comparable?
? English monolingual: ?En II Non-
Comparable?
682
!Ex Source Reference Baseline System 1 (Ar) !???#$#"! %$??" ! sending reinforcements strong reinforcements sending reinforcements (N) 2 (Ar)  !???$??'!+!! with extinction OOV with extinction (N) 3 (Ar) !???#?? ??? ! thwarts address  thwarted (N) 4 (Ar) !?? ???# ! was quoted as saying attributed to was quoted as saying (G) 5 (Ar) ????"! ??! $#??& ! abdalmahmood said he said abdul mahmood  mahmood said (G) 6 (Ar)  ?#"! ????? it deems OOV it deems (G) 7 (Ur) !?"! ?$ ! I am hopeful this hope I am hopeful (N) 8 (Ur) ??! $???$ ! to defend him to defend to defend himself (G) 9 (Ur) !??? ???? ! while speaking In the  in conversation (N) 
Figure 2: Nine example outputs of our system vs. the baseline highlighting the properties of our approach. Each example is
labeled (Ar) for Arabic source or (Ur) for Urdu source, and system candidates are labeled with (N) if the candidate unlabeled
phrase?s labeled neighbor?s label, or (G) if the candidate was generated.
The results from this setup are presented as ?Base-
line+Noisy? and ?SLP? in Table 6. The two setups
allow us to examine how effectively our method
can learn from the noisy parallel data by treating it
as monolingual (i.e., for graph construction), com-
pared to treating this data as parallel, and also ex-
amines the realistic scenario of using completely
non-comparable monolingual text for graph con-
struction as in the second setup.
BLEU
Setup Tune Test
Baseline 21.87 21.17
SLP+Noisy 26.42 25.38
Baseline+Noisy 27.59 27.24
SLP 28.53 28.43
Table 6: Results for the Urdu-English evaluation evaluated
with BLEU. All experiments were conducted with the larger
language model, and generation only considered the m-best
candidates from the baseline system.
In the first setup, we get a huge improvement of
4.2 BLEU points (?SLP+Noisy?) when using the
monolingual data and the noisy parallel data for
graph construction. Our method obtained much
of the gains achieved by the supervised baseline
approach that utilizes the noisy parallel data in
conjunction with the NIST-provided parallel data
(?Baseline+Noisy?), but with fewer assumptions
on the nature of the corpora (monolingual vs.
parallel). Furthermore, despite completely un-
aligned, non-comparable monolingual text on the
Urdu and English sides, and a very large language
model, we can still achieve gains in excess of
1.2 BLEU points (?SLP?) in a difficult evaluation
scenario, which shows that the technique adds a
genuine translation improvement over and above
na??ve memorization of n-gram sequences.
3.5 Analysis of Output
Figure 2 looks at some of the sample hypotheses
produced by our system and the baseline, along
with reference translations. The outputs produced
by our system are additionally annotated with the
origin of the candidate, i.e., labeled neighbor?s la-
bel (N) or generated (G).
The Arabic-English examples are numbered 1
to 5. The first example shows a source bigram un-
known to the baseline system, resulting in a sub-
optimal translation, while our system proposes the
correct translation of ?sending reinforcements?.
The second example shows a word that was an
OOV for the baseline system, while our system
got a perfect translation. The third and fourth ex-
amples represent bigram phrases with much bet-
ter translations compared to backing off to the
lexical translations as in the baseline. The fifth
Arabic-English example demonstrates the pitfalls
of over-reliance on the distributional hypothesis:
the source bigram corresponding to the name ?abd
almahmood? is distributional similar to another
named entity ?mahmood? and the English equiva-
lent is offered as a translation. The distributional
hypothesis can sometimes be misleading. The
sixth example shows how morphological informa-
tion can propose novel candidates: an OOV word
is broken down to its stem via the analyzer and
candidates are generated based on the stem.
The Urdu-English examples are numbered 7
to 9. In example 7, the bigram ?par umeed?
(corresponding to ?hopeful?) is never seen in the
baseline system, which has only seen ?umeed?
(?hope?). By leveraging the monolingual corpus
to understand the context of this unlabeled bigram,
we can utilize the graph structure to propose a syn-
tactically correct form, also resulting in a more flu-
ent and correct sentence as determined by the lan-
guage model. Examples 8 & 9 show cases where
the baseline deletes words or translates them into
more common words e.g., ?conversation? to ?the?,
while our system proposes reasonable candidates.
683
4 Related Work
The idea presented in this paper is similar in spirit
to bilingual lexicon induction (BLI), where a seed
lexicon in two different languages is expanded
with the help of monolingual corpora, primarily by
extracting distributional similarities from the data
using word context. This line of work, initiated
by Rapp (1995) and continued by others (Fung
and Yee, 1998; Koehn and Knight, 2002) (inter
alia) is limited from a downstream perspective, as
translations for only a small number of words are
induced and oftentimes for common or frequently
occurring ones only. Recent improvements to BLI
(Tamura et al, 2012; Irvine and Callison-Burch,
2013b) have contained a graph-based flavor by
presenting label propagation-based approaches us-
ing a seed lexicon, but evaluation is once again
done on top-1 or top-3 accuracy, and the focus is
on unigrams.
Razmara et al (2013) and Irvine and Callison-
Burch (2013a) conduct a more extensive evalua-
tion of their graph-based BLI techniques, where
the emphasis and end-to-end BLEU evaluations
concentrated on OOVs, i.e., unigrams, and not on
enriching the entire translation model. As with
previous BLI work, these approaches only take
into account source-side similarity of words; only
moderate gains (and in the latter work, on a sub-
set of language pairs evaluated) are obtained. Ad-
ditionally, because of our structured propagation
algorithm, our approach is better at handling mul-
tiple translation candidates and does not need to
restrict itself to the top translation.
Klementiev et al (2012) propose a method that
utilizes a pre-existing phrase table and a small
bilingual lexicon, and performs BLI using mono-
lingual corpora. The operational scope of their ap-
proach is limited in that they assume a scenario
where unknown phrase pairs are provided (thereby
sidestepping the issue of translation candidate
generation for completely unknown phrases), and
what remains is the estimation of phrasal proba-
bilities. In our case, we obtain the phrase pairs
from the graph structure (and therefore indirectly
from the monolingual data) and a separate gener-
ation step, which plays an important role in good
performance of the method. Similarly, Zhang and
Zong (2013) present a series of heuristics that are
applicable in a fairly narrow setting.
The notion of translation consensus, wherein
similar sentences on the source side are encour-
aged to have similar target language translations,
has also been explored via a graph-based approach
(Alexandrescu and Kirchhoff, 2009). Liu et al
(2012) extend this method by proposing a novel
structured label propagation algorithm to deal with
the generalization of propagating sets of labels
instead of single labels, and also integrated in-
formation from the graph into the decoder. In
fact, we utilize this algorithm in our propagation
step (?2.4). However, the former work operates
only at the level of sentences, and while the latter
does extend the framework to sub-spans of sen-
tences, they do not discover new translation pairs
or phrasal probabilities for new pairs at all, but
instead re-estimate phrasal probabilities using the
graph structure and add this score as an additional
feature during decoding.
The goal of leveraging non-parallel data in ma-
chine translation has been explored from several
different angles. Paraphrases extracted by ?pivot-
ing? via a third language (Callison-Burch et al,
2006) can be derived solely from monolingual
corpora using distributional similarity (Marton et
al., 2009). Snover et al (2008) use cross-lingual
information retrieval techniques to find potential
sentence-level translation candidates among com-
parable corpora. In this case, the goal is to
try and construct a corpus as close to parallel
as possible from comparable corpora, and is a
fairly different take on the problem we are look-
ing at. Decipherment-based approaches (Ravi and
Knight, 2011; Dou and Knight, 2012) have gen-
erally taken a monolingual view to the problem
and combine phrase tables through the log-linear
model during feature weight training.
5 Conclusion
In this work, we presented an approach that
can expand a translation model extracted from a
sentence-aligned, bilingual corpus using a large
amount of unstructured, monolingual data in both
source and target languages, which leads to im-
provements of 1.4 and 1.2 BLEU points over
strong baselines on evaluation sets, and in some
scenarios gains in excess of 4 BLEU points. In
the future, we plan to estimate the graph structure
through other learned, distributed representations.
Acknowledgments
The authors would like to thank Chris Dyer, Arul
Menezes, and the anonymous reviewers for their
helpful comments and suggestions.
684
References
Andrei Alexandrescu and Katrin Kirchhoff. 2009.
Graph-based learning for statistical machine trans-
lation. In Proceedings of Human Language Tech-
nologies: The 2009 Annual Conference of the North
American Chapter of the Association for Compu-
tational Linguistics, NAACL-HLT ?09, pages 119?
127. Association for Computational Linguistics,
June.
Ond?rej Bojar, Christian Buck, Chris Callison-Burch,
Christian Federmann, Barry Haddow, Philipp
Koehn, Christof Monz, Matt Post, Radu Soricut, and
Lucia Specia. 2013. Findings of the 2013 Work-
shop on Statistical Machine Translation. In Pro-
ceedings of the Eighth Workshop on Statistical Ma-
chine Translation, pages 1?44, Sofia, Bulgaria, Au-
gust. Association for Computational Linguistics.
Chris Callison-Burch, Philipp Koehn, and Miles Os-
borne. 2006. Improved statistical machine trans-
lation using paraphrases. In Proceedings of the
Human Language Technology Conference of the
NAACL, Main Conference, pages 17?24, New York
City, USA, June. Association for Computational
Linguistics.
Victor Chahuneau, Eva Schlinger, Noah A. Smith, and
Chris Dyer. 2013. Translating into morphologically
rich languages with synthetic phrases. In Proc. of
EMNLP.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228,
June.
Qing Dou and Kevin Knight. 2012. Large scale deci-
pherment for out-of-domain machine translation. In
Proceedings of the 2012 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning, pages
266?275. Association for Computational Linguis-
tics, July.
Pascale Fung and Lo Yuen Yee. 1998. An ir approach
for translating new words from nonparallel, compa-
rable texts. In Proceedings of the 36th Annual Meet-
ing of the Association for Computational Linguis-
tics and 17th International Conference on Computa-
tional Linguistics - Volume 1, ACL ?98, pages 414?
420, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Michel Galley and Christopher D. Manning. 2008. A
simple and effective hierarchical phrase reordering
model. EMNLP ?08, pages 848?856, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,
and Dan Klein. 2008. Learning bilingual lexicons
from monolingual corpora. In Proceedings of ACL-
08: HLT, pages 771?779, Columbus, Ohio, June.
Association for Computational Linguistics.
Ann Irvine and Chris Callison-Burch. 2013a. Com-
bining bilingual and comparable corpora for low re-
source machine translation. In Proceedings of the
Eighth Workshop on Statistical Machine Transla-
tion, pages 262?270, Sofia, Bulgaria, August. As-
sociation for Computational Linguistics.
Ann Irvine and Chris Callison-Burch. 2013b. Su-
pervised bilingual lexicon induction with multiple
monolingual signals. In Proceedings of the 2013
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, pages 518?523, Atlanta,
Georgia, June. Association for Computational Lin-
guistics.
Alexandre Klementiev, Ann Irvine, Chris Callison-
Burch, and David Yarowsky. 2012. Toward sta-
tistical machine translation without parallel corpora.
In Proceedings of the 13th Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics, pages 130?140, Avignon, France, April.
Association for Computational Linguistics.
Philipp Koehn and Kevin Knight. 2002. Learning a
translation lexicon from monolingual corpora. In
In Proceedings of ACL Workshop on Unsupervised
Lexical Acquisition, pages 9?16.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the 2003 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics on Human Language Technology - Vol-
ume 1, NAACL ?03, pages 48?54, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Shujie Liu, Chi-Ho Li, Mu Li, and Ming Zhou. 2012.
Learning translation consensus with structured la-
bel propagation. In Proceedings of the 50th Annual
Meeting of the Association for Computational Lin-
guistics: Long Papers - Volume 1, ACL ?12, pages
302?310, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Yuval Marton, Chris Callison-Burch, and Philip
Resnik. 2009. Improved statistical machine trans-
lation using monolingually-derived paraphrases. In
Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing, EMNLP
?09, pages 381?390, Singapore, August. Association
for Computational Linguistics.
David McClosky, Eugene Charniak, and Mark John-
son. 2006. Effective self-training for parsing. In
Proceedings of the Human Language Technology
Conference of the NAACL, Main Conference, pages
152?159, New York City, USA, June. Association
for Computational Linguistics.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics - Volume 1, ACL ?03, pages 160?
167, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
685
Kishore Papineni, Salim Roukos, Todd Ward, and Wei
jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. pages 311?318.
Reinhard Rapp. 1995. Identifying word translations in
non-parallel texts. In Proceedings of the 33rd An-
nual Meeting of the Association for Computational
Linguistics, ACL ?95.
Sujith Ravi and Kevin Knight. 2011. Deciphering for-
eign language. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies, pages 12?
21, Portland, Oregon, USA, June. Association for
Computational Linguistics.
Majid Razmara, Maryam Siahbani, Gholamreza Haf-
fari, and Anoop Sarkar. 2013. Graph propagation
for paraphrasing out-of-vocabulary words in statis-
tical machine translation. In Proceedings of the
51st of the Association for Computational Linguis-
tics, ACL-51, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Matthew Snover, Bonnie Dorr, and Richard Schwartz.
2008. Language and translation model adaptation
using comparable corpora. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP ?08, pages 857?866,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Akihiro Tamura, Taro Watanabe, and Eiichiro Sumita.
2012. Bilingual lexicon extraction from compara-
ble corpora using label propagation. In Proceedings
of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, EMNLP-CoNLL ?12,
pages 24?36.
Kristina Toutanova, Hisami Suzuki, and Achim Ruopp.
2008. Applying morphology generation models to
machine translation. In Proceedings of ACL-08:
HLT, pages 514?522, Columbus, Ohio, June. Asso-
ciation for Computational Linguistics.
Jiajun Zhang and Chengqing Zong. 2013. Learning
a phrase-based translation model from monolingual
data with application to domain adaptation. In Pro-
ceedings of the 51st Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long
Papers), pages 1425?1434, Sofia, Bulgaria, August.
Association for Computational Linguistics.
Xiaojin Zhu, Zoubin Ghahramani, and John D. Laf-
ferty. 2003. Semi-supervised learning using gaus-
sian fields and harmonic functions. In Proceedings
of the Twentieth International Conference on Ma-
chine Learning, ICML ?03, pages 912?919.
686
Proceedings of BioNLP Shared Task 2011 Workshop, pages 155?163,
Portland, Oregon, USA, 24 June, 2011. c?2011 Association for Computational Linguistics
MSR-NLP Entry in BioNLP Shared Task 2011 
 
 
Chris Quirk, Pallavi Choudhury, Michael Gamon, and Lucy Vanderwende 
Microsoft Research 
One Microsoft Way 
Redmond, WA 98052 USA 
{chrisq,pallavic,mgamon,lucyv}@microsoft.com 
 
 
Abstract 
We describe the system from the Natural 
Language Processing group at Microsoft 
Research for the BioNLP 2011 Shared 
Task. The task focuses on event extraction, 
identifying structured and potentially 
nested events from unannotated text. Our 
approach follows a pipeline, first 
decorating text with syntactic information, 
then identifying the trigger words of 
complex events, and finally identifying the 
arguments of those events. The resulting 
system depends heavily on lexical and 
syntactic features. Therefore, we explored 
methods of maintaining ambiguities and 
improving the syntactic representations, 
making the lexical information less brittle 
through clustering, and of exploring novel 
feature combinations and feature reduction. 
The system ranked 4th in the GENIA task 
with an F-measure of 51.5%, and 3rd in the 
EPI task with an F-measure of 64.9%. 
1 Introduction 
We describe a system for extracting complex 
events and their arguments as applied to the 
BioNLP-2011 shared task.  Our goal is to explore 
general methods for fine-grained information 
extraction, to which the data in this shared task is 
very well suited.  We developed our system using 
only the data provided for the GENIA task, but 
then submitted output for two of the tasks, GENIA 
and EPI, training models on each dataset 
separately, with the goal of exploring how general 
the overall system design is with respect to text 
domain and event types. We used no external 
knowledge resources except a text corpus used to 
train cluster features. We further describe several 
system variations that we explored but which did 
not contribute to the final system submitted. We 
note that the MSR-NLP system consistently is 
among those with the highest recall, but needs 
additional work to improve precision. 
2 System Description 
Our event extraction system is a pipelined 
approach, closely following the structure used by 
the best performing system in 2009 (Bj?rne et al, 
2009). Given an input sentence along with 
tokenization information and a set of parses, we 
first attempt to identify the words that trigger 
complex events using a multiclass classifier. Next 
we identify edges between triggers and proteins, or 
between triggers and other triggers. Finally, given 
a graph of proteins and triggers, we use a rule-
based post-processing component to produce 
events in the format of the shared task. 
2.1 Preprocessing and Linguistic Analysis 
We began with the articles as provided, with an 
included tokenization of the input and 
identification of the proteins in the input. However, 
we did modify the token text and the part-of-
speech tags of the annotated proteins in the input to 
be PROT after tagging and parsing, as we found 
that it led to better trigger detection. 
The next major step in preprocessing was to 
produce labeled dependency parses for the input. 
Note that the dependencies may not form a tree: 
there may be cycles and some words may not be 
connected. During feature construction, this 
parsing graph was used to find paths between 
155
words in the sentence. Since proteins may consist 
of multiple words, for paths we picked a single 
representative word for each protein to act as its 
starting point and ending point. Generally this was 
the token inside the protein that is closest to the 
root of the dependency parse. In the case of ties, 
we picked the rightmost such node. 
2.1.1 McClosky-Charniak-Stanford parses 
The organizers provide parses from a version of 
the McClosky-Charniak parser, MCCC (McClosky 
and Charniak, 2008), which is a two-stage 
parser/reranker trained on the GENIA corpus. In 
addition, we used an improved set of parsing 
models that leverage unsupervised data, MCCC-I 
(McClosky, 2010). In both cases, the Stanford 
Parser was used to convert constituency trees in the 
Penn Treebank format into labeled dependency 
parses: we used the collapsed dependency format. 
2.1.2 Dependency posteriors 
Effectively maintaining and leveraging the 
ambiguity present in the underlying parser has 
improved task accuracy in some downstream tasks 
(e.g., Mi et al 2008). McClosky-Charniak parses 
in two passes: the first pass is a generative model 
that produces a set of n-best candidates, and the 
second pass is a discriminative reranker that uses a 
rich set of features including non-local 
information. We renormalized the outputs from 
this log-linear discriminative model to get a 
posterior distribution over the 50-best parses. This 
set of parses preserved some of the syntactic 
ambiguity present in the sentence. 
The Stanford parser deterministically converts 
phrase-structure trees into labeled dependency 
graphs (de Marneffe et al, 2006). We converted 
each constituency tree into a dependency graph 
separately and retained the probability computed 
above on each graph. 
One possibility was to run feature extraction on 
each of these 50 parses, and weight the resulting 
features in some manner. However, this caused a 
significant increase in feature count. Instead, we 
gathered a posterior distribution over dependency 
edges: the posterior probability of a labeled 
dependency edge was estimated by the sum of the 
probability of all parses containing that edge. 
Gathering all such edges produced a single labeled 
graph that retained much of the ambiguity of the 
input sentence. Figure 1 demonstrates this process 
on a simple example. We applied a threshold of 0.5 
and retained all edges above that threshold, 
although there are many alternative ways to exploit 
this structure.  
 
Figure 1: Example sentence from the GENIA corpus. (a) Two of the top 50 constituency parses from the MCCC-I 
parser; the first had a total probability mass of 0.43 and the second 0.25 after renormalization. Nodes that differ 
between parses are shaded and outlined. (b) The dependency posteriors (labels omitted due to space) after 
conversion of 50-best parses. Solid lines indicate edges with posterior > 0.95; edges with posterior < 0.05 were 
omitted. Most of the ambiguity is in the attachment of ?elicited?. 
156
As above, the resulting graph is likely no longer 
a connected tree, though it now may also be cyclic 
and rather strange in structure. Most of the 
dependency features were built on shortest paths 
between words. We used the algorithm in Cormen 
et al (2002, pp.595) to find shortest paths in a 
cyclic graph with non-negative edge weights. The 
shortest path algorithm used in feature finding was 
supplied uniform positive edge weights. We could 
also weight edges by the negative log probability 
to find the shortest, most likely path. 
2.1.3 ENJU 
We also experimented with the ENJU parses 
(Miyao and Tsujii, 2008) provided by the shared 
task organizers. The distribution contained the 
output of the ENJU parser in a format consistent 
with the Stanford Typed Dependency 
representation . 
2.1.4 Multiple parsers 
We know that even the best modern parsers are 
prone to errors. Including features from multiple 
parsers helps mitigate these errors. When different 
parsers agree, they can reinforce certain 
classification decisions. The features that were 
extracted from a dependency parse have names 
that include an identifier for the parser that 
produced them. In this way, the machine learning 
algorithm can assign different weights to features 
from different parsers. For finding heads of multi-
word entities, we preferred the ENJU parser if 
present in that experimental condition, then fell 
back to MCCC parses, and finally MCCC-I. 
2.1.5 Dependency conversion rules 
We computed our set of dependency features (see 
2.2.1) from the collapsed, propagated Stanford 
Typed Dependency representation (see 
http://nlp.stanford.edu/software/dependencies_man
ual.pdf and de Marneffe et al, 2006), made 
available by the organizers.  We chose this form of 
representation since we are primarily interested in 
computing features that hold between content 
words.  Consider, for example, the noun phrase 
?phosphorylation of TRAF2?. A dependency 
representation would specify head-modifier 
relations for the tuples (phosphorylation, of) and 
(of, TRAF2). Instead of head-modifier, a typed 
dependency representation specifies PREP and 
PPOBJ as the two grammatical relations: 
PREP(phosphorylation-1, of-2) and PPOBJ(of-2, 
TRAF2-3). A collapsed representation has a single 
triplet specifying the relation between the content 
words directly, PREP_OF(phosphorylation-1, 
TRAF2-3); we considered this representation to be 
the most informative.   
We experimented with a representation that 
further normalized over syntactic variation.  The 
system submitted for the GENIA subtask does not 
use these conversion rules, while the system 
submitted for the EPI subtask does use these rules.  
See Table 2 for further details. While for some 
applications it may be useful to distinguish 
whether a given relation was expressed in the 
active or passive voice, or in a main or a relative 
clause, we believe that for this application it is 
beneficial to normalize over these types of 
syntactic variation.  Accordingly, we had a set of 
simple renaming conversion rules, followed by a 
rule for expansion; this list was our first effort and 
could likely be improved.  We modeled this 
normalized level of representation on the logical 
form, described in Jensen (1993), though we were 
unable to explore NP-or VP-anaphora 
 
Renaming conversion rules: 
1. ABBREV -> APPOS 
2. NSUBJPASS -> DOBJ 
3. AGENT -> NSUBJ 
4. XSUBJ -> NSUBJ 
5. PARTMOD(head, modifier where last 3 
characters are "ing") -> NSUBJ(modifier, head) 
6. PARTMOD(head, modifier where last 3 
characters are "ed") -> DOBJ(modifier, head) 
Expansion: 
1. For APPOS, find all edges that point to the head 
(gene-20) and duplicate those edges, but 
replacing the modifier with the modifier of the 
APPOS relation (kinase-26).  
 
Thus, in the 2nd sentence in PMC-1310901-01-
introduction, ?... leading to expression of a bcr-abl 
fusion gene, an aberrant activated tyrosine kinase, 
....?, there are two existing grammatical relations: 
 
PREP_OF(expression-15, gene-20) 
APPOS(gene-20, kinase-26) 
 
to which this rule adds: 
 
PREP_OF(expression-15, kinase-26) 
157
2.2 Trigger Detection 
We treated trigger detection as a multi-class 
classification problem: each token should be 
annotated with its trigger type or with NONE if it 
was not a trigger. When using the feature set 
detailed below, we found that an SVM 
(Tsochantaridis et al, 2004) outperformed a 
maximum entropy model by a fair margin, though 
the SVM was sensitive to its free parameters. A 
large value of C, the penalty incurred during 
training for misclassifying a data point, was 
necessary to achieve good results. 
2.2.1 Features for Trigger Detection 
Our initial feature set for trigger detection was 
strongly influenced by features that were 
successful in Bj?rne et al, (2009).  
Token Features. We included stems of single 
tokens from the Porter stemmer (Porter, 1980), 
character bigrams and trigrams, a binary indicator 
feature if the token has upper case letters, another 
indicator for the presence of punctuation, and a 
final indicator for the presence of a number. We 
gathered these features for both the current token 
as well as the three immediate neighbors on both 
the left and right hand sides. 
We constructed a gazetteer of possible trigger 
lemmas in the following manner. First we used a 
rule-based morphological analyzer (Heidorn, 2000) 
to identify the lemma of all words in the training, 
development, and test corpora. Next, for each word 
in the training and development sets, we mapped it 
to its lemma. We then computed the number of 
times that each lemma occurred as a trigger for 
each type of event (and none). Lemmas that acted 
as a trigger more than 50% of the time were added 
to the gazetteer. 
During feature extraction for a given token, we 
found the lemma of the token, and then look up 
that lemma in the gazetteer. If found, we included 
a binary feature to indicate its trigger type. 
Frequency Features. We included as features 
the number of entities in the sentence, a bag of 
words from the current sentence, and a bag of 
entities in the current sentence. 
Dependency Features. We used primarily a set 
of dependency chain features that were helpful in 
the past (Bj?rne et al, 2009); these features walk 
the Stanford Typed Dependency edges up to a 
distance of 3. 
We also found it helpful to have features about 
the path to the nearest protein, regardless of 
distance. In cases of multiple shortest paths, we 
took only one, exploring the dependency tree 
generally in left to right order. For each potential 
trigger, we looked at the dependency edge labels 
leading to that nearest protein. In addition we had a 
feature including both the dependency edge labels 
and the token text (lowercased) along that path. 
Finally, we had a feature indicating whether some 
token along that path was also in the trigger 
gazetteer. The formulation of this set of features is 
still not optimal especially for the ?binding? events 
as the training data will include paths to more than 
one protein argument.  Nevertheless, in Table 3, 
 
Key Relation Value Key Relation Value 
quantities child(left, NNS?JJ) measurable measurable child-1(left, NNS?JJ) quantities 
found child(after, VBN?NNS) hours hours child-1(after, VBN?NNS) found 
found child(after, VBN?NN) ingestion ingestion child-1(after, VBN?NN) found 
 
Figure 2: A sample PubMed sentence along with its dependency parse, and some key/relation/value triples 
extracted from that parse for computation of distributional similarity. Keys with a similar distribution of values 
under the same relation are likely semantically related. Inverse relations are indicated with a superscript -1. 
Prepositions are handled specially: we add edges labeled with the preposition from its parent to each child 
(indicated by dotted edges). 
158
we can see that this set of features contributed to 
improved precision. 
Cluster Features. Lexical and stem features 
were crucial for accuracy, but were unfortunately 
sparse and did not generalize well. To mitigate 
this, we incorporated word cluster features. In 
addition to the lexical item and the stem, we added 
another feature indicating the cluster to which each 
word belongs. To train clusters, we downloaded all 
the PubMed abstracts (http://pubmed.gov), parsed 
them with a simple dependency parser (a 
reimplementation of McDonald, 2006 trained on 
the GENIA corpus), and extracted dependency 
relations to use in clustering: words that occur in 
similar contexts should fall into the same cluster. 
An example sentence and the relations that were 
extracted for distributional similarity computation 
are presented in Figure 2. We ran a distributional 
similarity clustering algorithm (Pantel et al, 2009) 
to group words into clusters. 
Tfidf features. This set of features was intended 
to capture the salience of a term in the medical and 
?general? domain, with the aim of being able to 
distinguish domain-specific terms from more 
ambiguous terms. We calculated the tf.idf score for 
each term in the set of all PubMed abstracts and 
did the same for each term in Wikipedia. For each 
token in the input data, we then produced three 
features: (i) the tf.idf value of the token in PubMed 
abstracts, (ii) the tf.idf value of the token in 
Wikipedia, and (iii) the delta between the two 
values. Feature values were rounded to the closest 
integer. We found, however, that adding these 
features did not improve results. 
2.2.2 Feature combination and reduction 
We experimented with feature reduction and 
feature combination within the set of features 
described here. For feature reduction we tried a 
number of simple approaches that typically work 
well in text classification. The latter is similar to 
the task at hand, in that there is a very large but 
sparse feature set. We tried two feature reduction 
methods: a simple count cutoff, and selection of 
the top n features in terms of log likelihood ratio 
(Dunning, 1993) with the target values. For a count 
cutoff, we used cutoffs from 3 to 10, but we failed 
to observe any consistent gains. Only low cutoffs 
(3 and occasionally 5) would ever produce any 
small improvements on the development set. Using 
log likelihood ratio (as determined on the training 
set), we reduced the total number of features to 
between 10,000 and 75,000. None of these 
experiments improved results, however. One 
potential reason for this negative result may be that 
there were a lot of features in our set that capture 
the same phenomenon in different ways, i.e. which 
correlate highly. By retaining a subset of the 
original feature set using a count cutoff or log 
likelihood ratio we did not reduce this feature 
overlap in any way. Alternative feature reduction 
methods such as Principal Component Analysis, on 
the other hand, would target the feature overlap 
directly. For reasons of time we did not experiment 
with other feature reduction techniques but we 
believe that there may well be a gain still to be had. 
For our feature combination experiments the 
idea was to find highly predictive Boolean 
combinations of features. For example, while the 
features a and b may be weak indicators for a 
particular trigger, the cases where both a and b are 
present may be a much stronger indicator. A linear 
classifier such as the one we used in our 
experiments by definition is not able to take such 
Boolean combinations into account. Some 
classifiers such as SVMs with non-linear kernels 
do consider Boolean feature combinations, but we 
found the training times on our data prohibitive 
when using these kernels. As an alternative, we 
decided to pre-identify feature combinations that 
are predictive and then add those combination 
features to our feature inventory. In order to pre-
identify feature combinations, we trained decision 
tree classifiers on the training set, and treated each 
path from the root to a leaf through the decision 
tree classifier as a feature combination. We also 
experimented with adding all partial paths through 
the tree (as long as they started from the root) in 
addition to adding all full paths. Finally, we tried 
to increase the diversity of our combination 
features by using a ?bagging? approach, where we 
trained a multitude of decision trees on random 
subsets of the data. Again, unfortunately, we did 
not find any consistent improvements. Two 
observations that held relatively consistently across 
our experiments with combination features and 
different feature sets were: (i) only adding full 
paths as combination features sometimes helped, 
while adding partial paths did not, and (ii) bagging 
hardly ever led to improvements. 
159
2.3 Edge Detection 
This phase of the pipeline was again modeled as 
multi-class classification. There could be an edge 
originating from any trigger word and ending in 
any trigger word or protein. Looking at the set of 
all such edges, we trained a classifier to predict the 
label of this edge, or NONE if the edge was not 
present. Here we found that a maximum entropy 
classifier performed somewhat better than an SVM, 
so we used an in-house implementation of a 
maximum entropy trainer to produce the models. 
2.3.1 Features for Edge Detection 
As with trigger detection, our initial feature set for 
edge detection was strongly influenced by features 
that were successful in Bj?rne et al (2009). 
Additionally, we included the same dependency 
path features to the nearest protein that we used for 
trigger detection, described in 2.2.1. Further, for a 
prospective edge between two entities, where the 
entities are either a trigger and a protein, or a 
trigger and a second trigger, we added a feature 
that indicates (i) if the second entity is in the path 
to the nearest protein, (ii) if the head of the second 
entity is in the path to the nearest protein, (iii) the 
type of the second entity.   
2.4 Post-processing 
Given the set of edges, we used a simple 
deterministic procedure to produce a set of events. 
This step is not substantially different from that 
used in prior systems (Bj?rne et al, 2009). 
2.4.1 Balancing Precision and Recall 
As in Bj?rne et al (2009), we found that the trigger 
detector had quite low recall. Presumably this is 
due to the severe class imbalance in the training 
data: less than 5% of the input tokens are triggers. 
Thus, our classifier had a tendency to overpredict 
NONE. We tuned a single free parameter ? ? ?? 
(the ?recall booster?) to scale back the score 
associated with the NONE class before selecting 
the optimal class. The value was tuned for whole-
system F-measure; optimal values tended to fall in 
the range 0.6 to 0.8, indicating that only a small 
shift toward recall led to the best results. 
  Development Set  Test Set 
Event Class Count Recall Precision F1 Count Recall Precision F1 
Gene_expression 749 76.37 81.46 78.83 1002 73.95 73.22 73.58 
Transcription 158 49.37 73.58 59.09 174 41.95 65.18 51.05 
Protein_catabolism 23 69.57 80.00 74.42 15 46.67 87.50 60.87 
Phosphorylation 111 73.87 84.54 78.85 185 87.57 81.41 84.37 
Localization 67 74.63 75.76 75.19 191 51.31 79.03 62.22 
=[SVT-TOTAL]= 1108 72.02 80.51 76.03 1567 68.99 74.03 71.54 
Binding 373 47.99 50.85 49.38 491 42.36 40.47 41.39 
=[EVT-TOTAL]= 1481 65.97 72.73 69.18 2058 62.63 65.46 64.02 
Regulation 292 32.53 47.05 38.62 385 24.42 42.92 31.13 
Positive_Regulation 999 38.74 51.67 44.28 1443 37.98 44.92 41.16 
Negative_Regulation 471 35.88 54.87 43.39 571 41.51 42.70 42.10 
=[REG-TOTAL]= 1762 36.95 51.79 43.13 2399 36.64 44.08 40.02 
ALL-Total 3243 50.20 62.60 55.72 4457 48.64 54.71 51.50 
Table 1: Approximate span matching/approximate recursive matching on development and test data 
sets for GENIA Shared Task -1 with our system. 
Trigger 
Detection 
Features 
Trigger 
Loss Recall Prec. F1 
B 2.14 48.44 64.08 55.18 
B + TI 2.14 48.17 62.49 54.40 
B + TI + C 2.14 50.32 60.90 55.11 
B + TI + C + PI 2.03 50.20 62.60 55.72 
B + TI + C + PI 
+D 
2.02 49.21 62.75 55.16 
Table 2: Recall/Precision/F1 on the GENIA 
development set using MCCC-I + Enju parse; 
adding different features for Trigger Detection. 
B = Base set Features, TI = Trigger inflect 
forms, 
160
3 Results 
Of the five evaluation tracks in the shared task, we 
participated in two: the GENIA core task, and the 
EPI (Epigenetics and Post-translational 
modifications) task. The systems used in each track 
were substantially similar; differences are called 
out below. Rather than building a system 
customized for a single trigger and event set, our 
goal was to build a more generalizable framework 
for event detection. 
3.1 GENIA Task 
Using F-measure performance on the development 
set as our objective function, we trained the final 
system for the GENIA task with all the features 
described in section 2, but without the conversion 
rules and without either feature combination or 
reduction. Furthermore, we trained the cluster 
features using the full set of PubMed documents 
(as of  January 2011). The results of our final 
submission are summarized in Table 1. Overall, we 
saw a substantial degradation in F-measure when 
moving from the development set to the test set, 
though this was in line with past experience from 
our and other systems.  
We compared the results for different parsers in 
Table 3. MCCC-I is not better in isolation but does 
produce higher F-measures in combination with 
other parsers. Although posteriors were not 
particularly helpful on the development set, we ran 
Parser 
SVT-Total Binding REG-Total All-Total 
Recall Prec. F1 Recall Prec. F1 Recall Prec. F1 Recall Prec. F1 
MCCC 70.94 82.72 76.38 45.04 55.26 49.63 34.39 51.88 41.37 48.10 64.39 55.07 
MCCC-I 68.59 82.59 74.94 42.63 58.67 49.38 32.58 52.76 40.28 46.06 65.50 54.07 
Enju 71.66 82.18 76.56 40.75 51.01 45.31 32.24 49.39 39.01 46.69 62.70 53.52 
MCCC-I + 
Posteriors 
70.49 78.87 74.44 47.72 51.59 49.58 35.64 50.40 41.76 48.94 61.47 54.49 
MCCC + 
Enju 
71.84 82.04 76.60 44.77 53.02 48.55 34.96 53.15 42.18 48.69 64.59 55.52 
MCCC-I + 
Enju 
72.02 80.51 76.03 47.99 50.85 49.38 36.95 51.79 43.13 50.20 62.60 55.72 
Table 3: Comparison of Recall/Precision/F1 on the GENIA Task-1 development set using various 
combinations of parsers: Enju, MCCC (Mc-Closky Charniak), and MCCC-I (Mc-Closky Charniak 
Improved self-trained biomedical parsing model) with Stanford collapsed dependencies were used for 
evaluation. Results on Simple, Binding and Regulation and all events are shown. 
 
  Development Set  Test Set 
Event Class Count Recall Precision F1 Count Recall Precision F1 
Hydroxylation 31 25.81 61.54 36.36 69 30.43 84.00 44.68 
Dehydroxylation 0 100.00 100.00 100.00 0 100.00 100.00 100.00 
Phosphorylation 32 71.88 85.19 77.97 65 72.31 85.45 78.33 
Dephosphorylation 1 0.00 0.00 0.00 4 0.00 0.00 0.00 
Ubiquitination 76 63.16 75.00 68.57 180 67.78 81.88 74.16 
Deubiquitination 8 0.00 0.00 0.00 10 0.00 0.00 0.00 
DNA_methylation 132 72.73 72.18 72.45 182 71.43 73.86 72.63 
DNA_demethylation 9 0.00 0.00 0.00 6 0.00 0.00 0.00 
Glycosylation 70 61.43 67.19 64.18 169 39.05 69.47 50.00 
Deglycosylation 7 0.00 0.00 0.00 12 0.00 0.00 0.00 
Acetylation 65 89.23 75.32 81.69 159 87.42 85.28 86.34 
Deacetylation 19 68.42 92.86 78.79 24 62.50 93.75 75.00 
Methylation 65 64.62 75.00 69.42 193 62.18 73.62 67.42 
Demethylation 7 0.00 0.00 0.00 10 0.00 0.00 0.00 
Catalysis 60 3.33 15.38 5.48 111 4.50 33.33 7.94 
====[TOTAL]==== 582 57.22 72.23 63.85 1194 55.70 77.60 64.85 
Table 4: Approximate span matching/approximate recursive matching on development and test data 
sets for EPI CORE Task with our system 
161
a system consisting of MCCC-I with posteriors 
(MCCC-I + Posteriors) on the test set after the 
final results were submitted, and found that it was 
competitive with our submitted system (MCCC-I + 
ENJU). We believe that ambiguity preservation 
has merit, and hope to explore more of this area in 
the future. Diversity is important: although the 
ENJU parser alone was not the best, combining it 
with other parsers led to consistently strong results.  
Table 2 explores feature ablation: TI appears to 
degrade performance, but clusters regain that loss. 
Protein depth information was helpful, but 
dependency rule conversion was not.  Therefore 
the B+TI+C+PI combination was our final 
submission on GENIA.  
3.2 EPI Task 
We trained the final system for the Epigenetics 
task with all the features described in section 2. 
Further, we produced the clusters for the 
Epigenetics task using only the set of GENIA 
documents provided in the shared task. 
In contrast to GENIA, we found that the 
dependency rule conversions had a positive impact 
on development set performance. Therefore, we 
included them in the final system. Otherwise the 
system was identical to the GENIA task system.  
4 Discussion 
After two rounds of the BioNLP shared task, in 
2009 and 2011, we wonder whether it might be 
possible to establish an upper-bound on recall and 
precision. There is considerable diversity among 
the participating systems, so it would be interesting 
to consider whether there are some annotations in 
the development set that cannot be predicted by 
any of the participating systems1. If this is the case, 
then those triggers and edges would present an 
interesting topic for discussion. This might result 
either in a modification of the annotation protocols, 
or an opportunity for all systems to learn more. 
After a certain amount of feature engineering, 
we found it difficult to achieve further 
improvements in F1. Perhaps we need a significant 
shift in architecture, such as a shift to joint 
inference (Poon and Vanderwende, 2010). Our 
system may be limited by the pipeline architecture. 
                                                          
1 Our system output for the 2011development set can be 
downloaded from http://research.microsoft.com/bionlp/ 
MWEs (multi-word entities) are a challenge. 
Better multi-word triggers accuracy may improve 
system performance. Multi-word proteins often led 
to incorrect part-of-speech tags and parse trees. 
Cursory inspection of the Epigenetics task 
shows that some domain-specific knowledge 
would have been beneficial. Our system had 
significant difficulties with the rare inverse event 
types, e.g. ?demethylation? (e.g., there are 319 
examples for ?methylation? in the combined 
training/development set, but only 12 examples for 
?demethylation?). Each trigger type was treated 
independently, thus we did not share information 
between an event and its related inverse event type. 
Furthermore, our system also failed to identify 
edges for these rare events. One approach would 
be to share parameters between types that differ 
only in a prefix, e.g., ?de?. In general, some 
knowledge about the hierarchy of events may let 
the learner generalize among related events. 
5 Conclusion and Future Work 
We have described a system designed for fine-
grained information extraction, which we show to 
be general enough to achieve good performance 
across different sets of event types and domains.  
The only domain-specific characteristic is the pre-
annotation of proteins as a special class of entities. 
We formulated some features based on this 
knowledge, for instance the path to the nearest 
protein.  This would likely have analogues in other 
domains, given that there is often a special class of 
target items for any Information Extraction task. 
As the various systems participating in the 
shared task mature, it will be viable to apply the 
automatic annotations in an end-user setting.  
Given a more specific application, we may have 
clearer criteria for balancing the trade-off between 
recall and precision.  We expect that fully-
automated systems coupled with reasoning 
components will need very high precision, while 
semi-automated systems, designed for information 
visualization or for assistance in curating 
knowledge bases, could benefit from high recall.  
We believe that the data provided for the shared 
tasks will support system development in either 
direction. As mentioned in our discussion, though, 
we find that improving recall continues to be a 
major challenge. We seek to better understand the 
data annotations provided. 
162
Our immediate plans to improve our system 
include semi-supervised learning and system 
combination.  We will also continue to explore 
new levels of linguistic representation to 
understand where they might provide further 
benefit.  Finally, we plan to explore models of joint 
inference to overcome the limitations of pipelining 
and deterministic post-processing. 
Acknowledgments 
We thank the shared task organizers for providing 
this interesting task and many resources, the Turku 
BioNLP group for generously providing their 
system and intermediate data output, and Patrick 
Pantel and the MSR NLP group for their help and 
support. 
References  
Jari Bj?rne, Juho Heimonen, Filip Ginter, Antti Airola, 
Tapio Pahikkala and Tapio Salakoski. 2009. 
Extracting Complex Biological Events with Rich 
Graph-Based Feature Sets. In Proceedings of  the 
Workshop on BioNLP: Shared Task. 
Thomas Cormen, Charles Leiserson, and Ronald Rivest. 
2002. Introduction to Algorithms. MIT Press. 
Ted Dunning. 1993. Accurate methods for the statistics 
of surprise and coincidence. Computational 
Linguistics, 19(1), pp. 61-74. 
George E. Heidorn, 2000. Intelligent Writing 
Assistance. In Handbook of Natural Language 
Processing, ed. Robert Dale, Hermann Moisl, and 
Harold Somers.  Marcel Dekker Publishers. 
Karen Jensen. 1993. PEGASUS: Deriving Argument 
Structures after Syntax. In Natural Language 
Processing: the PLNLP approach, ed. Jensen, K., 
Heidorn, G.E., and Richardson, S.D. Kluwer 
Academic Publishers. 
Marie-Catherine de Marneffe, Bill MacCartney and 
Christopher D. Manning. 2006. Generating Typed 
Dependency Parses from Phrase Structure Parses. In 
LREC 2006. 
Yusuke Miyao and Jun?ichi Tsujii. 2008. Feature forest 
models for probabilistic HPSG parsing. 
Computational Linguistics 34(1): 35-80. 
David McClosky and Eugene Charniak. 2008.  Self-
Training for Biomedical Parsing. In Proceedings of 
the Association for Computational Linguistics 2008. 
David McClosky. 2010. Any Domain Parsing: 
Automatic Domain Adaptation for Natural Language 
Parsing. Ph.D. thesis, Department of Computer 
Science, Brown University.  
Ryan McDonald. 2006. Discriminative training and 
spanning tree algorithms for dependency parsing. Ph. 
D. Thesis. University of Pennsylvania. 
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based Translation.  In Proceedings of ACL 2008, 
Columbus, OH. 
Patrick Pantel, Eric Crestan, Arkady Borkovsky, Ana-
Maria Popescu and Vishnu Vyas. 2009. Web-Scale 
Distributional Similarity and Entity Set Expansion. In 
Proceedings of EMNLP 2009.  
Hoifung Poon and Lucy Vanderwende. 2010. Joint 
inference for knowledge extraction from biomedical 
literature. In Proceedings of NAACL-HLT 2010. 
Martin.F. Porter, 1980, An algorithm for suffix 
stripping, Program, 14(3):130?137. 
Ioannis Tsochantaridis, Thomas Hofmann, Thorsten 
Joachims, and Yasemin Alton. 2004. Support vector 
machine learning for interdependent and structured 
output spaces. In ICML 2004. 
163
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 200?209,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
On Hierarchical Re-ordering and Permutation Parsing
for Phrase-based Decoding
Colin Cherry
National Research Council
colin.cherry@nrc-cnrc.gc.ca
Robert C. Moore
Google
bobmoore@google.com
Chris Quirk
Microsoft Research
chrisq@microsoft.com
Abstract
The addition of a deterministic permutation
parser can provide valuable hierarchical in-
formation to a phrase-based statistical ma-
chine translation (PBSMT) system. Permuta-
tion parsers have been used to implement hier-
archical re-ordering models (Galley and Man-
ning, 2008) and to enforce inversion trans-
duction grammar (ITG) constraints (Feng et
al., 2010). We present a number of theoret-
ical results regarding the use of permutation
parsers in PBSMT. In particular, we show that
an existing ITG constraint (Zens et al, 2004)
does not prevent all non-ITG permutations,
and we demonstrate that the hierarchical re-
ordering model can produce analyses during
decoding that are inconsistent with analyses
made during training. Experimentally, we ver-
ify the utility of hierarchical re-ordering, and
compare several theoretically-motivated vari-
ants in terms of both translation quality and
the syntactic complexity of their output.
1 Introduction
Despite the emergence of a number of syntax-based
techniques, phrase-based statistical machine transla-
tion remains a competitive and very efficient trans-
lation paradigm (Galley and Manning, 2010). How-
ever, it lacks the syntactically-informed movement
models and constraints that are provided implicitly
by working with synchronous grammars. There-
fore, re-ordering must be modeled and constrained
explicitly. Movement can be modeled with a dis-
tortion penalty or lexicalized re-ordering probabili-
ties (Koehn et al, 2003; Koehn et al, 2007), while
decoding can be constrained by distortion limits or
by mimicking the restrictions of inversion transduc-
tion grammars (Wu, 1997; Zens et al, 2004).
Recently, we have begun to see deterministic per-
mutation parsers incorporated into phrase-based de-
coders. These efficient parsers analyze the sequence
of phrases used to produce the target, and assem-
ble them into a hierarchical translation history that
can be used to inform re-ordering decisions. Thus
far, they have been used to enable a hierarchical
re-ordering model, or HRM (Galley and Manning,
2008), as well as an ITG constraint (Feng et al,
2010). We discuss each of these techniques in turn,
and then explore the implications of ITG violations
on hierarchical re-ordering.
We present one experimental and four theoreti-
cal contributions. Examining the HRM alone, we
present an improved algorithm for extracting HRM
statistics, reducing the complexity of Galley and
Manning?s solution from O(n4) to O(n2). Examin-
ing ITG constraints alone, we demonstrate that the
three-stack constraint of Feng et al can be reduced
to one augmented stack, and we show that another
phrase-based ITG constraint (Zens et al, 2004) ac-
tually allows some ITG violations to pass. Finally,
we show that in the presence of ITG violations, the
original HRM can fail to produce orientations that
are consistent with the orientations collected during
training. We propose three HRM variants to address
this situation, including an approximate HRM that
requires no permutation parser, and compare them
experimentally. The variants perform similarly to
the original in terms of BLEU score, but differently
in terms of how they permute the source sentence.
200
We begin by establishing some notation. We view
the phrase-based translation process as producing a
sequence of source/target blocks in their target or-
der. For the purposes of this paper, we disregard
the lexical content of these blocks, treating blocks
spanning the same source segment as equivalent.
The block [si, ti] indicates that the source segment
wsi+1, . . . , wti was translated as a unit to produce
the ith target phrase. We index between words;
therefore, a block?s length in tokens is t ? s, and
for a sentence of length n, 0 ? s ? t ? n. Empty
blocks have s = t, and are used only in special cases.
Two blocks [si?1, ti?1] and [si, ti] are adjacent iff
ti?1 = si or ti = si?1. Note that we concern our-
selves only with adjacency in the source. Adjacency
in the target is assumed, as the blocks are in target
order. Figure 1 shows an example block sequence,
where adjacency corresponds to cases where block
corners touch. In the shift-reduce permutation parser
we describe below, the parsing state is encoded as a
stack of these same blocks.
2 Hierarchical Re-ordering
Hierarchical re-ordering models (HRMs) for phrase-
based SMT are an extension of lexicalized re-
ordering models (LRMs), so we begin by briefly
reviewing the LRM (Tillmann, 2004; Koehn et al,
2007). The goal of an LRM is to characterize how
a phrase-pair tends to be placed with respect to the
block that immediately precedes it. Both the LRM
and the HRM track orientations traveling through
the target from left-to-right as well as right-to-left.
For the sake of brevity and clarity, we discuss only
the left-to-right direction except when stated oth-
erwise. Re-ordering is typically categorized into
three orientations, which are determined by exam-
ining two sequential blocks [si?1, ti?1] and [si, ti]:
? Monotone Adjacent (M): ti?1 = si
? Swap Adjacent (S): ti = si?1
? Disjoint (D): otherwise
Figure 1 shows a simple example, where the first
two blocks are placed in monotone orientation, fol-
lowed by a disjoint ?red?, a swapped ?dog? and a
disjoint period. The probability of an orientation
Oi ? {M,S,D} is determined by a conditional
distribution: Pr(Oi|source phrasei, target phrasei).
Em
ily	 ?
	 ?aim
e	 ?	 ?s
on
	 ?	 ?gr
os	 ?
	 ?ch
ien
	 ?	 ?ro
uge
	 ?	 ?	 ?	 ?.
	 ?
[0,	 ?2]	 ?
Emily	 ?	 ?loves	 ?	 ?
[2,	 ?4]	 ?
her	 ?	 ?big	 ?	 ?
[5,6]	 ?
red	 ?	 ?
[4,5]	 ?
dog	 ?
[6,7]	 ?
.	 ?
Figure 1: A French-to-English translation with 5 blocks.
To build this model, orientation counts can be ex-
tracted from aligned parallel text using a simple
heuristic (Koehn et al, 2007).
The HRM (Galley and Manning, 2008) maintains
similar re-ordering statistics, but determines orienta-
tion differently. It is designed to address the LRM?s
dependence on the previous block [si?1, ti?1]. Con-
sider the period [6,7] in Figure 1. If a different seg-
mentation of the source had preceded it, such as one
that translates ?chien rouge? as a single [4,6] block,
the period would have been in monotone orienta-
tion. Galley and Manning (2008) introduce a de-
terministic shift-reduce parser into decoding, so that
the decoder always has access to the largest possible
previous block, given the current translation history.
The parser has two operations: shift places a newly
translated block on the top of the stack. If the top
two blocks are adjacent, then a reduce is immedi-
ately performed, replacing them with a single block
spanning both. Table 1 shows the parser states cor-
responding to our running example. Whether ?chien
rouge? is translated using [5,6],[4,5] or [4,6] alone,
the shift-reduce parser provides a consolidated pre-
vious block of [0,6] at the top of the stack (shown
with dotted lines). Therefore, [6,7] is placed in
monotone orientation in both cases.
The parser can be easily integrated into a phrase-
based decoder?s translation state, so each partial hy-
pothesis carries its own shift-reduce stack. Time and
memory costs for copying and storing stacks can
be kept small by sharing tails across decoder states.
The stack subsumes the coverage vector in that it
contains strictly more information: every covered
201
Op Stack
S [0,2]
S [0,2],[2,4]
R [0,4]
S [0,4],[5,6]
S [0,4],[5,6],[4,5]
R [0,4],[4,6]
R [0,6]
S [0,6],[6,7]
R [0,7]
Table 1: Shift-reduce states corresponding to Figure 1.
word will be present in one of the stack?s blocks.
However, it can be useful to maintain both.
The top item of a parser?s stack can be approxi-
mated using only the coverage vector. The approx-
imate top is the largest block of covered words that
contains the last translated block. This approxima-
tion will always be as large or larger than the true top
of the stack, and it will often match the true top ex-
actly. For example, in Figure 1, after we have trans-
lated [2,4], we can see that the coverage vector con-
tains all of [0,4], making the approximate top [0,4],
which is also the true top. In fact, this approxima-
tion is correct at every time step shown in Figure 1.
Keep this approximation in mind, as we return to it
in Sections 3.2 and 4.3.
We do not use a shift-reduce parser that consumes
source words from right-to-left;1 therefore, we ap-
ply the above approximation to handle the right-to-
left HRM. Before doing so, we re-interpret the de-
coder state to simulate a right-to-left decoder. The
last block becomes [si, ti] and the next block be-
comes [si?1, ti?1], and the coverage vector is in-
verted so that covered words become uncovered and
vice versa. Taken all together, the approximate test
for right-to-left adjacency checks that any gap be-
tween [si?1, ti?1] and [si, ti] is uncovered in the
original coverage vector.2 Figure 2 illustrates how a
monotone right-to-left orientation can be (correctly)
determined for [2, 4] after placing [5, 6] in Figure 1.
Statistics for the HRM can be extracted from
word-aligned training data. Galley and Manning
(2008) propose an algorithm that begins by run-
1This would require a second, right-to-left decoding pass.
2Galley and Manning (2008) present an under-specified ap-
proximation that is consistent with what we present here.
Prev	 ?
2	 ? 4	 ? 5	 ? 7	 ?6	 ?0	 ?
Next	 ?
Coverage	 ?/	 ?Approx	 ?Top	 ?
Next	 ?
2	 ? 4	 ? 5	 ? 7	 ?6	 ?0	 ?
Prev	 ?
Cov	 ?/	 ?Approx	 ?Top	 ?
Le?-??to-??Right	 ?(Disjoint	 ?[5,6])	 ?
Implied	 ?Right-??to-??Le?	 ?(Monotone	 ?[2,4])	 ?
Figure 2: Illustration of the coverage-vector stack ap-
proximation, as applied to right-to-left HRM orientation.
Phrase	 ?Sou
rce	 ?
Target	 ?
??M	 ?
??S	 ? ??M	 ?
??S	 ?
Figure 3: Relevant corners in HRM extraction. ? indi-
cates left-to-right orientation, and? right-to-left.
ning standard phrase extraction (Och and Ney, 2004)
without a phrase-length limit, noting the corners of
each phrase found. Next, the left-to-right and right-
to-left orientation for each phrase of interest (those
within the phrase-length limit) can be determined by
checking to see if any corners noted in the previous
step are adjacent, as shown in Figure 3.
2.1 Efficient Extraction of HRM statistics
The time complexity of phrase extraction is bounded
by the number of phrases to be extracted, which is
determined by the sparsity of the input word align-
ment. Without a limit on phrase length, a sentence
pair with nwords in each language can have as many
as O(n4) phrase-pairs.3 Because it relies on unre-
stricted phrase extraction, the corner collection step
for determining HRM orientation is also O(n4).
By leveraging the fact that the first step col-
lects corners, not phrase-pairs, we can show that
HRM extraction can actually be done inO(n2) time,
through a process we call corner propagation. In-
stead of running unrestricted phrase-extraction, cor-
ner propagation begins by extracting all minimal
3Consider a word-alignment with only one link in the center
of the grid.
202
So
urc
e	 ?
Target	 ?
??M	 ?
??S	 ? ??M	 ?
??S	 ?
??S	 ???M	 ?
??M	 ???S	 ?
Figure 4: Corner Propagation: Each of the four passes
propagates two types of corners along a single dimension.
phrase-pairs; that is, those that do not include un-
aligned words at their boundaries. The complex-
ity of this step is O(n2), as the number of mini-
mal phrases is bounded by the minimum of the num-
ber of monolingual phrases in either language. We
note corners for each minimal pair, as in the orig-
inal HRM extractor. We then carry out four non-
nested propagation steps to handle unaligned words,
traversing the source (target) in forward and reverse
order, with each unaligned row (column) copying
corners from the previous row (column). Each pass
takes O(n2) time, for a total complexity of O(n2).
This process is analogous to the growing step in
phrase extraction, but computational complexity is
minimized because each corner is considered inde-
pendently. Pseudo-code is provided in Algorithm 1,
and the propagation step is diagrammed in Fig-
ure 4. In our implementation, corner propagation is
roughly two-times faster than running unrestricted
phrase-extraction to collect corners.
Note that the trickiest corners to catch are those
that are diagonally separated from their minimal
block (they result from unaligned growth in both
the source and target). These cases are handled cor-
rectly because each corner type is touched by two
propagators, one for the source and one for the tar-
get (see Figure 4). For example, the top-right-corner
array Aq is populated by both propagate-right and
propagate-up. Thus, one propagator can copy a cor-
ner along one dimension, while the next propagator
copies the copies along the other dimension, moving
the original corner diagonally.
Algorithm 1 Corner Propagation
Initialize target-source indexed binary arrays
Aq[m][n], Ay[m][n], Ap[m][n] and Ax[m][n] to
record corners found in minimal phrase-pairs.
{Propagate Right}
for i from 2 to m s.t. target [i] is unaligned do
for j from 1 to n do
Aq[i][j] = True if Aq[i? 1][j] is True
Ay[i][j] = True if Ay[i? 1][j] is True
{Propagate Up}
for j from 2 to n s.t. source[j] is unaligned do
for i from 1 to m do
Ap[i][j] = True if Ap[i][j ? 1] is True
Aq[i][j] = True if Aq[i][j ? 1] is True
{Propagate Left and Down are similar}
return Aq, Ay, Ap and Ax
3 ITG-Constrained Decoding
Phrase-based decoding places no implicit limits on
re-ordering; all n! permutations are theoretically
possible. This is undesirable, as it leads to in-
tractability (Knight, 1999). Therefore, re-ordering is
limited explicitly, typically using a distortion limit.
One particularly well-studied re-ordering constraint
is the ITG constraint, which limits source permu-
tations to those achievable by a binary bracketing
synchronous context-free grammar (Wu, 1997). ITG
constraints are known to stop permutations that gen-
eralize 3142 and 2413,4 and can drastically limit the
re-ordering space for long strings (Zens and Ney,
2003). There are two methods to incorporate ITG
constraints into a phrase-based decoder, one using
the coverage vector (Zens et al, 2004), and the
other using a shift-reduce parser (Feng et al, 2010).
We begin with the latter, returning to the coverage-
vector constraint later in this section.
Feng et al (2010) describe an ITG constraint that
is implemented using the same permutation parser
used in the HRM. To understand their method, it is
important to note that the set of ITG-compliant per-
mutations is exactly the same as those that can be
reduced to a single-item stack using the shift-reduce
permutation parser (Zhang and Gildea, 2007). In
fact, this manner of parsing was introduced to SMT
42413 is shorthand notation that denotes the block sequence
[1,2],[3,4],[0,1],[2,3] as diagrammed in Figure 5a.
203
Sou
rce	 ?
Target	 ?
0[1,2]4	 ?
[2,3]	 ?
[0,1]	 ?
2[3,4]4	 ?
0[1,2]5	 ?
2[2,3]4	 ?
[0,1]	 ?
[3,4]	 ?
Sou
rce	 ?
2[4,5]5	 ?
Target	 ?(a)	 ? (b)	 ?
Figure 5: Two non-ITG permutations. Violations of po-
tential adjacency are indicated with dotted spans. Bounds
for the one-stack constraint are shown as subscripts.
in order to binarize synchronous grammar produc-
tions (Zhang et al, 2006). Therefore, enforcing
an ITG constraint in the presence of a shift-reduce
parser amounts to ensuring that every shifted item
can eventually be reduced. To discuss this con-
straint, we introduce a notion of potential adjacency,
where two blocks are potentially adjacent if any
words separating them have not yet been covered.
Formally, blocks [s, t] and [s?, t?] are potentially ad-
jacent iff one of the following conditions holds:
? they are adjacent (t? = s or t = s?)
? t? < s and [t?, s] is uncovered
? t < s? and [t, s?] is uncovered
Recall that a reduction occurs when the top two
items of the stack are adjacent. To ensure that re-
ductions remain possible, we only shift items onto
the stack that are potentially adjacent to the cur-
rent top. Figure 5 diagrams two non-ITG permu-
tations and highlights where potential adjacency is
violated. Note that no reductions occur in either
of these examples; therefore, each block [si, ti] is
also the top of the stack at time i. Potential ad-
jacency can be confirmed with some overhead us-
ing the stack and coverage vector together, but Feng
et al (2010) present an elegant three-stack solution
that provides potentially adjacent regions in constant
time, without a coverage vector. We improve upon
their method later this section. From this point on,
we abbreviate potential adjacency as PA.
We briefly sketch a proof that maintaining po-
tential adjacency maintains reducibility, by showing
that non-PA shifts produce irreducible stacks, and
that PA shifts are reducible. It is easy to see that ev-
ery non-PA shift leads to an irreducible stack. Let
[s?, t?] be an item to be shifted onto the stack, and
[s, t] be the current top. Assume that t? < s and the
two items are not PA (the case where t < s? is simi-
lar). Because they are not PA, there is some index k
in [t?, s] that has been previously covered. Since it is
covered, k exists somewhere in the stack, buried be-
neath [s, t]. Because k cannot be re-used, no series
of additional shift and reduce operations can extend
[s?, t?] so that it becomes adjacent to [s, t]. Therefore,
[s, t] will never participate in a reduction, and pars-
ing will close with at least two items on the stack.
Similarly, one can easily show that every PA shift is
reducible, because the uncovered space [t?, s] can be
filled by extending the new top toward the previous
top using strictly adjacent shifts.
3.1 A One-stack ITG Constraint
As mentioned earlier, Feng et al (2010) provide a
method to track potential adjacency that does not re-
quire a coverage vector. Instead, they maintain three
stacks, the original stack and two others to track po-
tentially adjacent regions to the left and right respec-
tively. These regions become available to the de-
coder only when the top of the original stack is ad-
jacent to one of the adjacency stacks.
We show that the same goal can be achieved with
even less book-keeping by augmenting the items on
the original stack to track the regions of potential
adjacency around them. The intuition behind this
technique is that on a shift, the new top inherits all
of the constraints on the old top, and the old top be-
comes a constraint itself. Each stack item now has
four fields, the original block [s, t], plus a left and
right adjacency bound, denoted together as `[s, t]r,
where ` and r are indices for the maximal span con-
taining [s, t] that is uncovered except for [s, t]. If the
top of the stack is `[s, t]r, then shifted items must fall
inside one of the two PA regions, [`, s] or [t, r]. The
region shifted into determines new item?s bounds.
The stack is initialized with a special 0[0, 0]n item,
and we then shift unannotated blocks onto the stack.
As we shift [s?, t?] onto the stack, rules derive bounds
`? and r? for the new top based on the old top `[s, t]r:
? Shift-left (t? ? s): `? = `, r? = s
? Shift-right (t ? s?): `? = t, r? = r
204
[2,4]	 ?
[5,7]	 ?
0	 ? 9	 ?
Shi?	 ?[5,7]	 ?
4	 ? 9	 ?
[2,4]	 ? 9	 ?0	 ?
[2,7]	 ?
[4,7]	 ?
0	 ? 9	 ?
Reduce	 ?
4	 ? 9	 ?
[2,4]	 ? 9	 ?0	 ?
(a)	 ? (b)	 ?
Figure 6: Two examples of boundaries for the one-stack solution for potential adjacency. Stacks are built from bottom
to top, blocks indicate [s,t] blocks, while tails are left and right adjacency boundaries.
Meanwhile, when reducing a stack with `? [s
?, t?]r?
at the top and `[s, t]r below it, the new top simply
copies ` and r. The merged item is larger than [s, t],
but it is PA to the same regions. Figure 6 diagrams
a shift-right and a reduce, while Figure 5 annotates
bounds for blocks during its ITG violations.
3.2 The Coverage-Vector ITG Constraint is
Incomplete
The stack-based solution for ITG constraints is el-
egant, but there is also a proposed constraint that
uses only the coverage vector (Zens et al, 2004).
This constraint can be stated with one simple rule:
if the previously translated block is [si?1, ti?1] and
the next block to be translated is [si, ti], one must
be able to travel along the coverage vector from
[si?1, ti?1] to [si, ti] without transitioning from an
uncovered word to a covered word. Feng et al
(2010) compare the two ITG constraints, and show
that they perform similarly, but not identically. They
attribute the discrepancy to differences in when the
constraints are applied, which is strange, as the two
constraints need not be timed differently.
Let us examine the coverage-vector constraint
more carefully, assuming that ti < si?1 (the case
where ti?1 < si is similar). The constraint consists
of two phases: first, starting from si?1, we travel to
the left toward ti, consuming covered words until we
reach the first uncovered word. We then enter into
the second phase, and the path must remain uncov-
ered until we reach ti. The first step over covered
positions corresponds to finding the left boundary
of the largest covered block containing [si?1, ti?1],
which is an approximation to the top of the stack
(Section 2). The second step over uncovered posi-
tions corresponds to determining whether [si, ti] is
PA to the approximate top. That is, the coverage-
vector ITG constraint checks for potential adjacency
using the same top-of-stack approximation as the
right-to-left HRM.
This implicit approximation implies that there
may well be cases where the coverage-vector con-
straint makes the wrong decision. Indeed this is
the case, which we prove by example. Consider
the irreducible sequence 25314, illustrated in Fig-
ure 5b. This non-ITG permutation is allowed by
the coverage-vector approximation, but not by the
stack-based constraint. Both constraints allow the
placement of the first three blocks [1, 2], [4, 5] and
[2, 3]. After adding [0, 1], the stack-based solution
detects a PA-violation. Meanwhile, the vector-based
solution checks the path from 2 to 1 for a transition
from uncovered to covered. This short path touches
only covered words. Similarly, as we add [3, 4], the
path from 1 to 3 is also completely covered. The
entire permutation is accepted without complaint.
The proof provided by Zens et al (2004) misses
this case, as it accounts for phrasal generalizations
of the 2413 ITG-forbidden substructure, but it does
not account for generalizations where the substruc-
ture is interrupted by a discontiguous item, such as
in 25{3}14, where 2413 is revealed not by merging
items but by deleting 3.
4 Inconsistencies in HRM parsing
We have shown that the HRM and the ITG con-
straints for phrase-based decoding use the same de-
terministic shift-reduce parser. The entirety of the
ITG discussion was devoted to preventing the parser
from reaching an irreducible state. However, up
until now, work on the HRM has not addressed
the question of irreducibility (Galley and Manning,
2008; Nguyen et al, 2009).
Irreducible derivations do occur during HRM de-
coding, and when they do, they can create inconsis-
tencies with respect to HRM extraction from word-
205
?  
?  
??
  ?
?  
?  
??
  ?
?  
??
 	 ??	 ?
[4,6]	 ?How	 ?can	 ?
[0,1]	 ?you	 ?
[6,7]	 ?achieve	 ?
[1,2]	 ?the	 ?
[3,4]	 ?economic	 ?and	 ?
[2,3]	 ?tourism	 ?
[7,9]	 ?benefits	 ??	 ?
Figure 7: An example irreducible derivation, drawn from
our Chinese-to-English decoder?s k-best output.
Last translated block 2-red *-red approx
How can [4, 6] [4,6] [4,6] [4,6]
you [0, 1] [0,1] [0,1] [0,1]
achieve [6, 7] [6,7] [6,7] [4,7]
the [1, 2] [1,2] [1,2] [0,2]
economic and [3, 4] [3,4] [3,4] [3,7]
tourism [2, 3] [1,4] [0,7] [0,7]
benefits? [7, 9] [7,9] [0,9] [0,9]
Table 2: Top of stack at each time step in Figure 7, under
2-reduction (as in the original HRM), *-reduction, and
the coverage-vector approximation.
aligned training data. In Figure 7, we show an ir-
reducible block sequence, extracted from a Chinese-
English decoder. The parser can perform a few small
reductions, creating a [1,4] block indicated with a
dashed box, but translation closes with 5 items on
the stack. One can see that [7,9] is assigned a dis-
joint orientation by the HRM. However, if the same
translation and alignment were seen during train-
ing, the unrestricted phrase extractor would find a
phrase at [0,7], indicated with a dotted box, and [7,9]
would be assigned monotone orientation. This in-
consistency penalizes this derivation, as ?benefits ??
is forced into an unlikely disjoint orientation. One
potential implication is that the decoder will tend
to avoid irreducible states, as those states will tend
to force unlikely orientations, resulting in a hidden,
soft ITG-constraint. Indeed, our decoder does not
select this hypothesis, but instead a (worse) transla-
tion that is fully reducible. The impact of these in-
consistencies on translation quality can only be de-
termined empirically. However, to do so, we require
alternatives that address these inconsistencies. We
describe three such variants below.
4.1 ITG-constrained decoding
Perhaps the most obvious way to address irreducible
states is to activate ITG constraints whenever decod-
ing with an HRM. Irreducible derivations will disap-
pear from the decoder, along with the corresponding
inconsistencies in orientation. Since both techniques
require the same parser, there is very little overhead.
However, we will have also limited our decoder?s re-
ordering capabilities.
4.2 Unrestricted shift-reduce parsing
The deterministic shift-reduce parser used through-
out this paper is actually a special case of a general
class of permutation parsers, much in the same way
that a binary ITG is a special case of synchronous
context-free grammar. Zhang and Gildea (2007) de-
scribe a family of k-reducing permutation parsers,
which can reduce the top k items of the stack in-
stead of the top 2. For k ? 2 we can generalize the
adjacency requirement for reduction to a permuta-
tion requirement. Let {[si, ti]|i=1. . . k} be the top k
items of a stack; they are a permutation iff:
max
i
(ti)?min
i
(si) =
?
i
[ti ? si]
That is, every number between the max and min is
present somewhere in the set. Since two adjacent
items always fulfill this property, we know the orig-
inal parser is 2-reducing. k-reducing parsers reduce
by moving progressively deeper in the stack, looking
for the smallest 2 ? i ? k that satisfies the permu-
tation property (see Algorithm 2). As in the original
parser, a k-reduction is performed every time the top
of the stack changes; that is, after each shift and each
successful reduction.
If we set k = ?, the parser will find the small-
est possible reduction without restriction; we refer
to this as a *-reducing parser. This parser will never
reach an irreducible state. In the worst case, it re-
duces the entire permutation as a single n-reduction
after the last shift. This means it will exactly mimic
unrestricted phrase-extraction when predicting ori-
entations, eliminating inconsistencies without re-
stricting our re-ordering space. The disadvantage is
206
Algorithm 2 k-reduce a stack
input stack {[si, ti]|i = 1 . . . l}; i = 1 is the top
input max reduction size k, k ? 2
set s? = s1; t? = t1; size = t1 ? s1
for i from 2 to min(k, l) do
set s? = min(s?, si); t? = max(t?, ti)
set size = size + (ti ? si)
if t? ? s? == size then
pop {[sj , tj ]|j = 1 . . . i} from the stack
push [s?, t?] onto the stack;
return true // successful reduction
return false // failed to reduce
that reduction is no longer a constant-time operation,
but is insteadO(n) in the worst case (consider Algo-
rithm 2 with k =? and l = n items on the stack).5
As a result, we will carefully track the impact of this
parser on decoding speed.
4.3 Coverage vector approximation
One final option is to adopt the top-of-stack approxi-
mation for left-to-right orientations, in addition to its
current use for right-to-left orientations, eliminating
the need for any permutation parser. The next block
[si, ti] is adjacent to the approximate top of the stack
only if any space between [si, ti] and the previous
block [si?1, ti?1] is covered. But before committing
fully to this approximation, we should better under-
stand it. Thus far, we have implied that this approx-
imation can fail to predict correct orientations, but
we have not specified when these failures occur. We
now show that incorrect orientations can only occur
while producing a non-ITG permutation.
Let [si?1, ti?1] be the last translated block, and
[si, ti] be the next block. Recall that the approxima-
tion determines the top of the stack using the largest
block of covered words that contains [si?1, ti?1].
The approximate top always contains the true top,
because they both contain [si?1, ti?1] and the ap-
proximate top is the largest block that does so.
Therefore, the approximation errs on the side of ad-
jacency, meaning it can only make mistakes when
5Zhang and Gildea (2007) provide an efficient algorithm for
*-reduction that uses additional book-keeping so that the num-
ber of permutation checks as one traverses the entire sequence
is linear in aggregate; however, we implement the simpler, less
efficient version here to simplify decoder integration.
Prev	 ? Next	 ?
si-??1	 ? ti-??1	 ? si	 ? ti	 ?t?	 ?
True	 ?top	 ?
Approximate	 ?top	 ?
Breaks	 ?
PA	 ?
Figure 8: Indices for when the coverage approximation
predicts a false M.
assigning an M or S orientation; if it assigns a D, it
is always correct. Let us consider the false M case
(the false S case is similar). If we assign a false M,
then ti?1 < si and si is adjacent to the approximate
top; therefore, all positions between ti?1 and si are
covered. However, since the M is false, the true top
of the stack must end at some t? : ti?1 ? t? < si.
Since we know that every position between t? and si
is covered, [si, ti] cannot be PA to the true top of the
stack, and we must be in the midst of making a non-
ITG permutation. See Figure 8 for an illustration of
the various indices involved. As it turns out, both the
approximation and the 2-reducing parser assign in-
correct orientations only in the presence of ITG vio-
lations. However, the approximation may be prefer-
able, as it requires only a coverage vector.
4.4 Qualitative comparison
Each solution manages its stack differently, and we
illustrate the differences in terms of the top of the
stack at time i in Table 2. The *-reducing parser is
the gold standard, so we highlight deviations from
its decisions in bold. As one can see, the original 2-
reducing parser does fine before and during an ITG
violation, but can create false disjoint orientations
after the violation is complete, as the top of its stack
becomes too small due to missing reductions. Con-
versely, the coverage-vector approximation makes
errors inside the violation: the approximate top be-
comes too large, potentially creating false monotone
or swap orientations. Once the violation is complete,
it recovers nicely.
5 Experiments
We compare the LRM, the HRM and the three HRM
variants suggested in Section 4 on a Chinese-to-
English translation task. We measure the impact on
translation quality in terms of BLEU score (Papineni
et al, 2002), as well as the impact on permutation
207
BLEU NIST 08 Complexity Counts Speed
Method nist04 nist06 nist08 > 2 4 5 6 7 ? 8 sec/sent
LRM 38.00 33.79 27.12 241 146 40 32 12 11 3.187
HRM 2-red 38.53 34.20 27.57 176 113 31 20 8 4 3.353
HRM apprx 38.58 34.09 27.60 280 198 41 26 13 2 3.231
HRM *-red 38.39 34.22 27.41 328 189 71 34 20 14 3.585
HRM itg 38.70 34.26 27.33 0 0 0 0 0 0 3.274
Table 3: Chinese-to-English translation results, comparing the LRM and 4 HRM variants: the original 2-reducing
parser, the coverage vector approximation, the *-reducing parser, and an ITG-constrained decoder.
complexity, as measured by the largest k required to
k-reduce the translations.
5.1 Data
The system was trained on data from the NIST 2009
Chinese MT evaluation, consisting of more than
10M sentence pairs. The training corpora were split
into two phrase tables, one for Hong Kong and UN
data, and one for all other data. The dev set was
taken from the NIST 05 evaluation set, augmented
with some material reserved from other NIST cor-
pora; it consists of 1.5K sentence pairs. The NIST
04, 06, and 08 evaluation sets were used for testing.
5.2 System
We use a phrase-based translation system similar to
Moses (Koehn et al, 2007). In addition to our 8
translation model features (4 for each phrase table),
we have a distortion penalty incorporating the min-
imum possible completion cost described by Moore
and Quirk (2007), a length penalty, a 5-gram lan-
guage model trained on the NIST09 Gigaword cor-
pus, and a 4-gram language model trained on the tar-
get half of the parallel corpus. The LRM and HRM
are represented with six features, with separate
weights for M, S and D in both directions (Koehn et
al., 2007). We employ a gap constraint as our only
distortion limit (Chang and Collins, 2011). This re-
stricts the maximum distance between the start of a
phrase and the earliest uncovered word, and is set to
7 words. Parameters are tuned using a batch-lattice
version of hope-fear MIRA (Chiang et al, 2008;
Cherry and Foster, 2012). We re-tune parameters
for each variant.
5.3 Results
Our results are summarized in Table 3. Speed and
complexity are measured on the NIST08 test set,
which has 1357 sentences. We measure permutation
complexity by parsing the one-best derivations from
each system with an external *-reducing parser, and
noting the largest k-reduction for each derivation.
Therefore, the>2 column counts the number of non-
ITG derivations produced by each system.
Regarding quality, we have verified the effective-
ness of the HRM: each HRM variant outperforms
the LRM, with the 2-reducing HRM doing so by 0.4
BLEU points on average. Unlike Feng et al (2010),
we see no consistent benefit from adding hard ITG
constraints, perhaps because we are building on an
HRM-enabled system. In fact, all HRM variants
perform more or less the same, with no clear win-
ner emerging. Interestingly, the approximate HRM
is included in this pack, which implies that groups
wishing to augment their phrase-based decoder with
an HRM need not incorporate a shift-reduce parser.
Regarding complexity, the 2-reducing HRM pro-
duces about half as many non-ITG derivations as the
*-reducing system, confirming our hypothesis that
a 2-reducing HRM acts as a sort of soft ITG con-
straint. Both the approximate and *-reducing de-
coders produce more violating derivations than the
LRM. This is likely due to their encouragement of
more movement overall. The largest reduction we
observed was k = 11.
Our speed tests show that all of the systems trans-
late at roughly the same speed, with the LRM being
fastest and the *-reducing HRM being slowest. The
*-reducing system is less than 7% slower than the 2-
reducing system, alleviating our concerns regarding
the cost of *-reduction.
208
6 Discussion
We have presented a number of theoretical contribu-
tions on the topic of phrase-based decoding with an
on-board permutation parser. In particular, we have
shown that the coverage-vector ITG constraint is ac-
tually incomplete, and that the original HRM can
produce inconsistent orientations in the presence of
ITG violations. We have presented three HRM vari-
ants that address these inconsistencies, and we have
compared them in terms of both translation quality
and permutation complexity. Though our results in-
dicate that a permutation parser is actually unneces-
sary to reap the benefits of hierarchical re-ordering,
we are excited about the prospects of further ex-
ploring the information provided by these on-board
parsers. In particular, we are interested in using fea-
tures borrowed from transition-based parsing while
decoding.
References
Yin-Wen Chang and Michael Collins. 2011. Exact de-
coding of phrase-based translation models through la-
grangian relaxation. In EMNLP, pages 26?37, Edin-
burgh, Scotland, UK., July.
Colin Cherry and George Foster. 2012. Batch tuning
strategies for statistical machine translation. In HLT-
NAACL, Montreal, Canada, June.
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In EMNLP, pages 224?233.
Yang Feng, Haitao Mi, Yang Liu, and Qun Liu. 2010. An
efficient shift-reduce decoding algorithm for phrase-
based machine translation. In COLING, pages 285?
293, Beijing, China, August.
Michel Galley and Christopher D. Manning. 2008. A
simple and effective hierarchical phrase reordering
model. In EMNLP, pages 848?856, Honolulu, Hawaii,
October.
Michel Galley and Christopher D. Manning. 2010. Ac-
curate non-hierarchical phrase-based translation. In
HLT-NAACL, pages 966?974, Los Angeles, Califor-
nia, June.
Kevin Knight. 1999. Squibs and discussions: Decod-
ing complexity in word-replacement translation mod-
els. Computational Linguistics, 25(4):607?615, De-
cember.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In HLT-NAACL,
pages 127?133.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In ACL,
pages 177?180, Prague, Czech Republic, June.
Robert C. Moore and Chris Quirk. 2007. Faster beam-
search decoding for phrasal statistical machine trans-
lation. In MT Summit XI, September.
Vinh Van Nguyen, Akira Shimazu, Minh Le Nguyen,
and Thai Phuong Nguyen. 2009. Improving a lexi-
calized hierarchical reordering model using maximum
entropy. In MT Summit XII, Ottawa, Canada, August.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Computational Linguistics, 30(4), December.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In ACL, pages 311?318.
Christoph Tillmann. 2004. A unigram orientation model
for statistical machine translation. In HLT-NAACL,
pages 101?104, Boston, USA, May.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?403.
Richard Zens and Hermann Ney. 2003. A comparative
study on reordering constraints in statistical machine
translation. In ACL, pages 144?151.
Richard Zens, Hermann Ney, Taro Watanabe, and Ei-
ichiro Sumita. 2004. Reordering constraints for
phrase-based statistical machine translation. In COL-
ING, pages 205?211, Geneva, Switzerland, August.
Hao Zhang and Daniel Gildea. 2007. Factorization
of synchronous context-free grammars in linear time.
In Proceedings of SSST, NAACL-HLT 2007 / AMTA
Workshop on Syntax and Structure in Statistical Trans-
lation, pages 25?32, Rochester, New York, April.
Hao Zhang, Liang Huang, Daniel Gildea, and Kevin
Knight. 2006. Synchronous binarization for machine
translation. In HLT-NAACL, pages 256?263, New
York City, USA, June.
209
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 460?467,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Leave-One-Out Phrase Model Training for Large-Scale Deployment
Joern Wuebker
Human Language Technology
and Pattern Recognition Group
RWTH Aachen University, Germany
wuebker@cs.rwth-aachen.de
Mei-Yuh Hwang, Chris Quirk
Microsoft Corporation
Redmond, WA, USA
{mehwang,chrisq}@microsoft.com
Abstract
Training the phrase table by force-aligning
(FA) the training data with the reference trans-
lation has been shown to improve the phrasal
translation quality while significantly reduc-
ing the phrase table size on medium sized
tasks. We apply this procedure to several
large-scale tasks, with the primary goal of re-
ducing model sizes without sacrificing transla-
tion quality. To deal with the noise in the auto-
matically crawled parallel training data, we in-
troduce on-demand word deletions, insertions,
and backoffs to achieve over 99% successful
alignment rate. We also add heuristics to avoid
any increase in OOV rates. We are able to re-
duce already heavily pruned baseline phrase
tables by more than 50% with little to no
degradation in quality and occasionally slight
improvement, without any increase in OOVs.
We further introduce two global scaling fac-
tors for re-estimation of the phrase table via
posterior phrase alignment probabilities and
a modified absolute discounting method that
can be applied to fractional counts.
Index Terms: phrasal machine translation, phrase
training, phrase table pruning
1 Introduction
Extracting phrases from large amounts of noisy
word-aligned training data for statistical machine
translation (SMT) generally has the disadvantage of
producing many unnecessary phrases (Johnson et
al., 2007). These can include poor quality phrases,
composite phrases that are concatenations of shorter
ones, or phrases that are assigned very low proba-
bilities, so that they have no realistic chance when
competing against higher scoring phrase pairs. The
goal of this work is two-fold: (i) investigating forced
alignment training as a phrase table pruning method
for large-scale commercial SMT systems and (ii)
proposing several extensions to the training proce-
dure to deal with practical issues and stimulate fur-
ther research.
Generative phrase translation models have the in-
herent problem of over-fitting to the training data
(Koehn et al, 2003; DeNero et al, 2006). (Wue-
bker et al, 2010) introduce a leave-one-out proce-
dure which is shown to counteract over-fitting ef-
fects. The authors report significant improvements
on the German-English Europarl data with the ad-
ditional benefit of a severely reduced phrase table
size. This paper investigates its impact on a num-
ber of commercial large-scale systems and presents
several extensions.
The first extension is to deal with the highly noisy
training data, which is automatically crawled and
sentence aligned. The noise and the baseline prun-
ing of the phrase table lead to low success rates
when aligning the source sentence with the target
sentence. We introduce on-demand word deletions,
insertions, and backoff phrases to increase the suc-
cess rate so that we can cover essentially the en-
tire training data. Secondly, phrase table pruning
makes out-of-vocabulary (OOV) issues even more
pronounced. To avoid an increased OOV rate, we
retrieve single-word translations from the baseline
phrase table. Lastly, we propose two global scaling
460
factors to allow fine-tuning of the phrase counts in
an attempt to re-estimate the translation probabili-
ties and a modification of absolute discounting that
can be applied to fractional counts.
Our main contribution is applying forced-
alignment on the training data to prune the phrase
table. The rationale behind this is that by decoding
the training data, we can identify the phrases that are
actually used by the decoder. Further, we present
preliminary experiments on re-estimating the chan-
nel models in the phrase table based on counts ex-
tracted from the force-aligned data.
This work is organized as follows. We discuss re-
lated work in Section 2, describe our decoder and
training procedure in Section 3 and the experiments
in Section 4. A conclusion and discussion of future
work is given in Section 5.
2 Related Work
Force-aligning bilingual data has been explored as
a means of model training in previous work. Liang
et al (2006) use it for their bold updating strategy
to update discriminative feature weights. Utilizing
force-aligned data to train a unigram phrase segmen-
tation model is proposed by Shen et al (2008). Wue-
bker et al (2010) apply forced alignment to train the
phrase table in an EM-like fashion. They report a
significant reduction in phrase table size.
In this work we apply forced alignment training
as a pure phrase table pruning technique. Johnson
et al (2007) successfully investigate a number of
pruning methods for the phrase inventory based on
significance testing. While their approach is more
straightforward and less elaborate, we argue that our
method is directly tailored to the decoding process
and works on top of an already heavily pruned base-
line phrase table.
We further experiment with applying the (scaled)
phrase alignment posteriors to train the phrase ta-
ble. A similar idea has been addressed in previous
work, e.g. (Venugopal et al, 2003; de Gispert et al,
2010), where word alignment posterior probabilities
are leveraged for grammar extraction.
Finally, a number of papers describe extending
real phrase training to the hierarchical machine
translation paradigm (Blunsom et al, 2008; Cme-
jrek et al, 2009; Mylonakis and Sima?an, 2010).
3 Phrase Training
3.1 Decoder
Our translation decoder is similar to the open-source
toolkit Moses (Koehn et al, 2007). It models trans-
lation as a log-linear combination of two phrasal
and two lexical channel models, an n-gram language
model (LM), phrase, word and distortion penalties
and a lexicalized reordering model. The decoding
can be summarized as finding the best scoring target
sentence T ? given a source sentence S:
T ? = argmax
T
?
i
?i log gi(S,T ) (1)
where each gi represents one feature (the channel
models, n-gram, phrase count, etc.). The model
weights ?i are usually discriminatively learned on a
development data set via minimum error rate train-
ing (MERT) (Och, 2003).
Constraining the decoder to a fixed target sentence
is straightforward. Each partial hypothesis is com-
pared to the reference and discarded if it does not
match. The language model feature can be dropped
since all hypotheses lead to the same target sentence.
The training data is divided into subsets for parallel
alignment. A bilingual phrase matching is applied to
the phrase table to extract only the subset of entries
that are pertinent to each subset of training data, for
memory efficiency. For forced alignment training,
we set the distortion limit ? to be larger than in reg-
ular translation decoding. As unlimited distortion
leads to very long training times, we compromise on
the following heuristic. The distortion limit is set
to be the maximum of 10, twice that of the baseline
setting, and 1.5 times the maximum phrase length:
? = max{10,
2? (baseline distortion),
1.5? (max phrase length)} (2)
To avoid over-fitting, we employ the same leave-
one-out procedure as (Wuebker et al, 2010) for
training. Here, it is applied on top of the Good-
Turing (GT) smoothed phrase table (Foster et al,
461
2006). Our phrase table stores the channel proba-
bilites and marginal counts for each phrase pair, but
not the discounts applied. Therefore, for each sen-
tence, if the phrase pair (s, t) has a joint count c(s, t)
computed from the entire training data, and occurs
c1(s, t) times in the current sentence, the leave-one-
out probability p?(t|s) for the current sentence will
be:
p?(t|s) =
c?(s, t)?d
c?(s)
=
c(s, t)? c1(s, t)?d
c(s)? c1(s)
=
p(t|s)c(s)? c1(s, t)
c(s)? c1(s)
(3)
since p(t|s)c(s) = c(s, t)?d, where d is the GT dis-
count value. In the case where c(s, t) = c1(s, t) (i.e.
(s, t) occurs exclusively in one sentence pair), we
use a very low probability as the floor value. We
apply leave-one-out discounting to the forward and
backward translation models only, not to the lexical
channel models.
Our baseline phrase extraction applies some
heuristic-based pruning strategies. For example,
it prunes offensive translations and many-words to
many-words singletons (i.e. a joint count of 1 and
both source phrase and target phrase contain mul-
tiple words)?. Finally the forward and backward
translation probabilities are smoothed with Good-
Turing discounting.
3.2 Weak Lambda Training with High
Distortion
Our leave-one-out training flowchart can be illus-
trated in Figure 1. To force-align the training data
with good quality, we need a set of trained lambda
weights, as shown in Equation 1. We can use the
lambda weights learned from the baseline system for
that purpose. However, ideally we want the lambda
values to be learned under a similar configuration as
the forced alignment. Therefore, for this purpose we
run MERT with the larger distortion limit given in
Equation 2.
?The pruned entries are nevertheless used in computing joint
counts and marginal counts.
 Parallel training data 
with word-level alignments 
Phrase extraction with 
heuristic pruning 
Weak lambda training 
Phrase table 
Leave-one-out 
forced alignment 
? 1 = {?} 
Normal lambda training 
Intersection + 
OOV Recovery 
Selected phrases 
Selected phrases+ 
Large ? 
2-grams 
5-grams 
Small ? 
? 2 = {?} 
 {uniform ?} 
 {baseline ?} 
Figure 1: Flowchart of forced-alignment phrase training.
Additionally, since forced alignment does not use
the language model, we propose to use a weaker lan-
guage model for training the lambdas (?1) to be used
in the forced alignment decoding.
Using a weaker language model also speeds up the
lambda training process, especially when we are us-
ing a distortion limit ? at least twice as high as in
the baseline system. In our experiments, the base-
line system uses an English 5-gram language model
trained on a large amount of monolingual data. The
lambda values used for forced alignment are learned
using the bigram LM trained on the target side of the
462
parallel corpus for each system.
We compared a number of systems using differ-
ent degrees of weak models and found out the im-
pact on the final system was minimal. However, us-
ing a small bigram LM with large distortion yielded
a stable performance in terms of BLEU, and was
25% faster than using a large 5-gram with the base-
line distortion. Because of the speed improvement
and its stability, this paper adopts the weak bigram
lambda training.
3.3 On-demand Word Insertions and Deletions
For many training sentences the translation decoder
is not able to find a phrasal alignment. We identified
the following main reasons for failed alignments:
? Incorrect sentence alignment or sentence seg-
mentation by the data crawler,
? OOVs due to initial pruning in the phrase ex-
traction phase,
? Faulty word alignments,
? Strongly reordered sentence structure. That is,
the distortion limit during forced alignment is
too restrictive.
For some of these cases, discarding the sentence
pairs can be seen as implicit data cleaning. For
others, there do exist valid sub-sentences that are
aligned properly. We would like to be able to lever-
age those sub-sentences, effectively allowing us to
do partial sentence removal. Therefore, we in-
troduce on-demand word insertions and deletions.
Whenever a partial hypothesis can not be expanded
to the next target word t j, with the given phrase ta-
ble, we allow the decoder to artificially introduce a
phrase pair (null, t j) to insert the target word into
the hypothesis without consuming any source word.
These artificial phrase pairs are introduced with a
high penalty and are ignored when creating the out-
put phrase table. We can also introduce backoff
phrase pairs (si, t j) for all source words si that are
not covered so far, also with a fixed penalty.
After we reach the end of the target sentence, if
there are any uncovered source words si, we arti-
ficially add the deletion phrase pairs (si,null) with
a high penalty. Introducing on-demand word inser-
tions and deletions increases the data coverage to
at least 99% of the training sentences on all tasks
we have worked on. Due to the success of inser-
tion/deletion phrases, we have not conducted exper-
iments using backoff phrases within the scope of this
work, but leave this to future work.
3.4 Phrase Training as Pruning
This work concentrates on practical issues with large
and noisy training data. Our main goal is to ap-
ply phrase training to reduce phrase table size with-
out sacrificing quality. We do this by dumping n-
best alignments of the training data, where n ranges
from 100-200. We prune the baseline phrase table to
only contain phrases that appear in any of the n-best
phrase alignments, leaving the channel probabilities
unchanged. That is, the model scores are still esti-
mated from the original counts. We can control the
size of the final phrase table by adjusting the size
of the n-best list. Based on the amount of memory
we can afford, we can thus keep the most important
entries in the phrase table.
3.5 OOV retrieval
When performing phrase table pruning as de-
scribed in Section 3.4, OOV rates tend to increase.
This effect is even more pronounced when dele-
tion/insertion phrases are not used, due to the low
alignment success rate. For commercial applica-
tions, untranslated words are a major concern for
end users, although it rarely has any impact on BLEU
scores. Therefore, for the final phrase table after
forced alignment training, we check the translations
for single words in the baseline phrase table. If any
single word has no translation in the new table, we
recover the top x translations from the baseline table.
In practice, we set x = 3.
3.6 Fractional Counts and Model
Re-estimation
As mentioned in Section 3.4, for each training sen-
tence pair we produce the n-best phrasal alignments.
If we interpret the model score of an alignment as
its log likelihood, we can weight the count for each
phrase by its posterior probability. However, as the
463
log-linear model weights are trained in a discrim-
inative fashion, they do not directly correspond to
probabilities. In order to leverage the model scores,
we introduce two scaling factors ? and ? that al-
low us to shape the count distribution according to
our needs. For one sentence pair, the count for the
phrase pair (s, t) is defined as
c(s, t)=
?
?
?
?
?
n
?
i=1
c(s, t|hi) ?
exp(? ??(hi))
n
?
j=1
exp(? ??(h j))
?
?
?
?
?
?
, (4)
where hi is the i-th hypothesis of the n-best list,
?(hi) the log-linear model score of the alignment
hypothesis hi and c(s, t|hi) the count of (s, t) within
hi. If ? = 0, all alignments within the n-best list
are weighted equally. Setting ? = 0 means that all
phrases that are used anywhere in the n-best list re-
ceive a count of 1.
Absolute discounting is a popular smoothing
method for relative frequencies (Foster et al, 2006).
Its application, however, is somewhat difficult, if
counts are not required to be integer numbers and
can in fact reach arbitrarily small values. We pro-
pose a minor modification, where the discount pa-
rameter d is added to the denominator, rather than
subtracting it from the numerator. The discounted
relative frequency for a phrase pair (s, t) is computed
as
p(s|t) =
c(s, t)
d+?
s?
c(s?, t)
(5)
3.7 Round-Two Lambda Training
After the phrase table is pruned with forced align-
ment (either re-estimating the channel probabilities
or not), we recommend a few more iterations of
lambda training to ensure our lambda values are ro-
bust with respect to the new phrase table. In our
experiments, we start from the baseline lambdas and
train at most 5 more iterations using the baseline dis-
tortion and the 5-gram English language model. The
settings have to be consistent with the final decod-
ing; therefore we are not using weak lambda training
here.
system parallel corpus Dev Test1 WMT
(sent. pairs)
it-en 13.0M 2000 5000 3027
pt-en 16.9M 2448 5000 1000
nl-en 15.0M 499 4996 1000
et-en 3.5M 1317 1500 995
Table 1: Data sizes of the four systems Italian, Por-
tuguese, Dutch and Estonian to English. All numbers
refer to sentence pairs.
Empirically we found the final lambdas (?2) made
a very small improvement over the baseline lamb-
das. However, we decided to keep this second round
of lambda training to guarantee its stability across
all language pairs.
4 Experiments
In this section, we describe our experiments on
large-scale training data. First, we prune the orig-
inal phrase table without re-estimation of the mod-
els. We conducted experiments on many language
pairs. But due to the limited space here, we chose to
present two high traffic systems and the two worst
systems so that readers can set the correct expecta-
tion with the worst-case scenario. The four systems
are: Italian (it), Portuguese (pt), Dutch (nl) and Es-
tonian (et), all translating to English (en).
4.1 Corpora
The amount of data for the four systems is shown in
Table 1. There are two test sets: Test1 and WMT.
Test1 is our internal data set, containing web page
translations among others. WMT is sampled from
the English side of the benchmark test sets of the
Workshop on Statistical Machine Translation?. The
sampled English sentences are then manually trans-
lated into other languages, as the input to test X-to-
English translation. WMT tends to contain news-
like and longer sentences. The development set (for
learning lambdas) is from our internal data set. We
make sure that there is no overlap among the devel-
opment set, test sets, and the training set.
?www.statmt.org/wmt09
464
baseline FA w/ del. FA w/o del.
it-en
suc.rate ? 99.5% 61.2%
Test1 42.27 42.05 42.31
WMT 30.16 30.19 30.19
pt-en
suc.rate ? 99.5% 66.9%
Test1 47.55 47.47 47.24
WMT 40.74 41.36 41.01
nl-en
suc.rate ? 99.6% 79.9%
Test1 32.39 31.87 31.18
WMT 43.37 43.06 43.38
et-en
suc.rate ? 99.1% 73.1%
Test1 46.14 46.35 45.77
WMT 20.08 19.60 19.83
Table 2: BLEU scores of forced-alignment-based phrase-
table pruning using weak lambda training. n-best size is
100 except for nl-en, where it is 160. We contrast forced
alignment with and without on-demand insertion/deletion
phrases. With the on-demand artificial phrases, FA suc-
cess rate is over 99%.
4.2 Insertion/Deletion Phrases
Unless explicitly stated, all experiments here used
the weak bigram LMs to obtain the lambdas used for
forced alignment, and on-demand insertion/deletion
phrases are applied. For the size of n-best, we use
n = 100. The only exception is the nl-en language
pair, for which we set n = 160 because its phrase
distortion setting is higher than the others and for its
higher number of morphological variations. Table 2
shows the BLEU performance of the four systems, in
the baseline setting and in the forced-alignment set-
ting with insertion/deletion phrases and without in-
sertion/deletion phrases. Whether partial sentences
should be kept or not (via insertion/deletion phrases)
depends on the quality of the training data. One
would have to run both settings to decide which is
better for each system. In all cases, there is little
or no degradation in quality after the table is suffi-
ciently pruned.
Table 3 shows that our main goal of reducing the
phrase table size is achieved. On all four language
pairs, we are able to prune over 50% of the phrase
PT size reduction
w/o del. w/ del.
it-en 65.4% 54.0%
pt-en 68.5% 61.3%
nl-en 64.1% 56.9%
et-en 63.6% 58.5%
Table 3: % Phrase table size reduction compared with the
baseline phrase table
table. Without on-demand insertions/deletions, the
size reduction is even stronger. Notice the size re-
duction here is relative to the already heavily pruned
baseline phrase table.
With such a successful size cut, we expected a
significant increase in decoding speed in the final
system. In practice we experienced 3% to 12% of
speedup across all the systems we tested. Both our
baseline and the reduced systems use a tight beam
width of 20 hypotheses per stack. We assume that
with a wider beam, the speed improvement would
be more pronounced.
We also did human evaluation on all 8 system out-
puts (four language pairs, with two test sets per lan-
guage pair) and all came back positive (more im-
provements than regressions), even on those that had
minor BLEU degradation. We conclude that the size
cut in the phrase table is indeed harmless, and there-
fore we declare our initial goal of phrase table prun-
ing without sacrificing quality is achieved.
In (Wuebker et al, 2010) it was observed, that
phrase training reduces the average phrase length.
The longer phrases, which are unlikely to gener-
alize, are dropped. We can confirm this obersva-
tion for the it-en and pt-en language pairs in Ta-
ble 4. However, for nl-en and et-en the aver-
age source phrase length is not significantly af-
fected by phrase training, especially with the inser-
tion/deletion phrases. When these artificial phrases
are added during forced alignment, they tend to en-
courage long target phrases as uncovered single tar-
get words can be consumed by the insertion phrases.
However, these insertion phrases are not dumped
into the final phrase table and hence cannot help
in reducing the average phrase length of the final
phrase table.
465
avg. src phrase length
baseline w/o del. w/ del.
it-en 3.1 2.4 2.4
pt-en 3.7 3.0 3.0
nl-en 3.1 3.0 3.0
et-en 2.9 2.8 3.0
Table 4: Comparison of average source phrase length in
the phrase table.
nl-en Test1 WMT PT size reduction
baseline 32.29 43.37 ?
n=100 31.45 42.90 66.0%
n=160 31.87 43.06 64.1%
et-en Test1 WMT PT size reduction
baseline 46.14 20.08 ?
n=100 46.35 19.60 63.6%
n=200 46.34 19.88 58.4%
Table 5: BLEU scores of different n-best sizes for the
highly inflected Dutch system and the noisy Estonian sys-
tem.
Table 5 illustrates how the n-best size affects
BLEU scores and model sizes for the nl-en and et-
en systems.
4.3 Phrase Model Re-estimation
This section conducts a preliminary evaluation of
the techniques introduced in Section 3.6. For fast
turnaround, these experiments were conducted on
approximately 1/3 of the Italian-English training
data. Training is performed with and without inser-
tion/deletion phrases and both with (FaTrain) and
without (FaPrune) re-training of the forward and
backward phrase translation probabilities. Table 6
shows the BLEU scores with different settings of the
global scaling factor ? and the inverse discount d.
The second global scaling factor is fixed to ? = 0.
The preliminary results seem to be invariant of the
settings. We conclude that using forced alignment
posteriors as a feature training method seems to be
less effective than using competing hypotheses from
free decoding as in (He and Deng, 2012).
BLEU
ins/del ? d Test1 WMT
baseline - - - 40.6 28.9
FaPrune no - - 40.7 29.1
FaTrain no 0 0 40.4 28.9
0.5 0 40.2 28.9
FaPrune yes - - 40.6 28.9
FaTrain yes 0 0 40.1 28.6
0.5 0 40.5 29.1
0.5 0.2 40.5 29.0
0.5 0.4 40.5 29.0
Table 6: Phrase pruning (FaPrune) vs. further model
re-estimation after pruning (FaTrain) on 1/3 it-en train-
ing data, both with and without on-demand inser-
tions/deletions.
5 Conclusion and Outlook
We applied forced alignment on parallel training
data with leave-one-out on four large-scale commer-
cial systems. In this way, we were able to reduce the
size of our already heavily pruned phrase tables by
at least 54%, with almost no loss in translation qual-
ity, and with a small improvement in speed perfor-
mance. We show that for language pairs with strong
reordering, the n-best list size needs to be increased
to account for the larger search space.
We introduced several extensions to the training
procedure. On-demand word insertions and dele-
tions can increase the data coverage to nearly 100%.
We plan to extend our work to use backoff transla-
tions (the target word that can not be extended given
the input phrase table will be aligned to any uncov-
ered single source word) to provide more alignment
varieties, and hence hopefully to be able to keep
more good phrase pairs. To avoid higher OOV rates
after pruning, we retrieved single-word translations
from the baseline phrase table.
We would like to emphasize that this leave-one-
out pruning technique is not restricted to phrasal
translators, even though all experiments presented
in this paper are on phrasal translators. It is possible
to extend the principle of forced alignment guided
pruning to hierarchical decoders, treelet decoders, or
syntax-based decoders, to prune redundant or use-
less phrase mappings or translation rules.
466
Re-estimating phrase translation probabilities us-
ing forced alignment posterior scores did not yield
any noticable BLEU improvement so far. Instead, we
propose to apply discriminative training similar to
(He and Deng, 2012) after forced-alignment-based
pruning as future work.
References
[Blunsom et al2008] Phil Blunsom, Trevor Cohn, and
Miles Osborne. 2008. A discriminative latent vari-
able model for statistical machine translation. In Pro-
ceedings of the 46th Annual Conference of the Associa-
tion for Computational Linguistics: Human Language
Technologies (ACL-08:HLT), pages 200?208, Colum-
bus, Ohio, June. Association for Computational Lin-
guistics.
[Cmejrek et al2009] Martin Cmejrek, Bowen Zhou, and
Bing Xiang. 2009. Enriching SCFG Rules Directly
From Efficient Bilingual Chart Parsing. In Proc. of the
International Workshop on Spoken Language Transla-
tion, pages 136?143, Tokyo, Japan.
[de Gispert et al2010] Adria? de Gispert, Juan Pino, and
William Byrne. 2010. Hierarchical Phrase-based
Translation Grammars Extracted from Alignment Pos-
terior Probabilities. In Proceedings of the 2010 Con-
ference on Empirical Methods in Natural Language
Processing, pages 545?554, MIT, Massachusetts,
U.S.A., October.
[DeNero et al2006] John DeNero, Dan Gillick, James
Zhang, and Dan Klein. 2006. Why Generative Phrase
Models Underperform Surface Heuristics. In Proceed-
ings of the Workshop on Statistical Machine Transla-
tion, pages 31?38, New York City, June.
[Foster et al2006] George Foster, Roland Kuhn, and
Howard Johnson. 2006. Phrasetable Smoothing for
Statistical Machine Translation. In Proc. of the Conf.
on Empirical Methods for Natural Language Process-
ing (EMNLP), pages 53?61, Sydney, Australia, July.
[He and Deng2012] Xiaodong He and Li Deng. 2012.
Maximum Expected BLEU Training of Phrase and
Lexicon Translation Models. In Proceedings of the
50th Annual Meeting of the Association for Computa-
tional Linguistics (ACL), page to appear, Jeju, Republic
of Korea, Jul.
[Johnson et al2007] J Howard Johnson, Joel Martin,
George Foster, and Roland Kuhn. 2007. Improv-
ing Translation Quality by Discarding Most of the
Phrasetable. In Proceedings of 2007 Joint Conference
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
pages 967?975, Prague, June.
[Koehn et al2003] P. Koehn, F. J. Och, and D. Marcu.
2003. Statistical Phrase-Based Translation. In Pro-
ceedings of the 2003 Meeting of the North American
chapter of the Association for Computational Linguis-
tics (NAACL-03), pages 127?133, Edmonton, Alberta.
[Koehn et al2007] Philipp Koehn, Hieu Hoang, Alexan-
dra Birch, Chris Callison-Burch, Marcello Federico,
Nicola Bertoldi, Brooke Cowan, Wade Shen, Chris-
tine Moran, Richard Zens, Chris Dyer, Ondr?ej Bo-
jar, Alexandra Constantine, and Evan Herbst. 2007.
Moses: Open Source Toolkit for Statistical Machine
Translation. In Annual Meeting of the Association for
Computational Linguistics (ACL), demonstration ses-
sion, pages 177?180, Prague, Czech Republic, June.
[Liang et al2006] Percy Liang, Alexandre Buchard-Co?te?,
Dan Klein, and Ben Taskar. 2006. An End-to-End
Discriminative Approach to Machine Translation. In
Proceedings of the 21st International Conference on
Computational Linguistics and the 44th annual meet-
ing of the Association for Computational Linguistics,
pages 761?768, Sydney, Australia.
[Mylonakis and Sima?an2010] Markos Mylonakis and
Khalil Sima?an. 2010. Learning Probabilistic Syn-
chronous CFGs for Phrase-based Translation. In Pro-
ceedings of the Fourteenth Conference on Computa-
tional Natural Language Learning, pages 117?, Up-
psala,Sweden, July.
[Och2003] Franz Josef Och. 2003. Minimum Error Rate
Training in Statistical Machine Translation. In Proc. of
the 41th Annual Meeting of the Association for Com-
putational Linguistics (ACL), pages 160?167, Sapporo,
Japan, July.
[Shen et al2008] Wade Shen, Brian Delaney, Tim Ander-
son, and Ray Slyh. 2008. The MIT-LL/AFRL IWSLT-
2008 MT System. In Proceedings of IWSLT 2008,
pages 69?76, Hawaii, U.S.A., October.
[Venugopal et al2003] Ashish Venugopal, Stephan Vo-
gel, and Alex Waibel. 2003. Effective Phrase Transla-
tion Extraction from Alignment Models. In Proceed-
ings of the 41st Annual Meeting on Association for
Computational Linguistics, pages 319?326, Sapporo,
Japan, July.
[Wuebker et al2010] Joern Wuebker, Arne Mauser, and
Hermann Ney. 2010. Training phrase translation mod-
els with leaving-one-out. In Proceedings of the 48th
Annual Meeting of the Assoc. for Computational Lin-
guistics, pages 475?484, Uppsala, Sweden, July.
467
Proceedings of the Second Workshop on Hybrid Approaches to Translation, pages 51?66,
Sofia, Bulgaria, August 8, 2013. c?2013 Association for Computational Linguistics
Controlled Ascent: Imbuing Statistical MT with Linguistic Knowledge
William D. Lewis and Chris Quirk
Microsoft Research
One Microsoft Way
Redmond, WA 98052
{wilewis,chrisq}@microsoft.com
Abstract
We explore the intersection of rule-based and sta-
tistical approaches in machine translation, with a
particular focus on past and current work here at
Microsoft Research. Until about ten years ago,
the only machine translation systems worth using
were rule-based and linguistically-informed. Along
came statistical approaches, which use large cor-
pora to directly guide translations toward expres-
sions people would actually say. Rather than mak-
ing local decisions when writing and conditioning
rules, goodness of translation was modeled numer-
ically and free parameters were selected to opti-
mize that goodness. This led to huge improvements
in translation quality as more and more data was
consumed. By necessity, the pendulum is swing-
ing towards the inclusion of linguistic features in
MT systems. We describe some of our statistical
and non-statistical attempts to incorporate linguis-
tic insights into machine translation systems, show-
ing what is currently working well, and what isn?t.
We also look at trade-offs in using linguistic knowl-
edge (?rules?) in pre- or post-processing by lan-
guage pair, with a particular eye on the return on
investment as training data increases in size.
1 Introduction
Machine translation has undergone several
paradigm shifts since its original conception.
Early work considered the problem as cryptog-
raphy, imagining that a word replacement cipher
could find the word correspondences between two
languages. Clearly Weaver was decades ahead of
his time in terms of both computational power
and availability of data: only now is this approach
gaining some traction (Knight, 2013)1 At the time,
however, this direction did not appear promising,
and work turned toward rule-based approaches.
Effective translation needs to handle a broad
range of phenomena. Word substitution ciphers
may address lexical selection, but there are many
additional complexities: morphological normal-
ization in the source language, morphological in-
flection in the target language, word order differ-
ences, and sentence structure differences, to name
1For the original 1949 Translation memorandum by
Weaver see (Weaver, 1955).
a few. Many of these could be captured, at least
to a first degree of approximation, by rule-based
approaches. A single rule might capture the fact
that English word order is predominantly SVO
and Japanese word order is predominantly SOV.
While many exceptions exist, such rules handle
many of the largest differences between languages
rather effectively. Therefore, rule-based systems
that did a reasonable job of addressing morpho-
logical and syntactic differences between source
and target dominated the marketplace for decades.
With the broader usage of computers, greater
amounts of electronic data became available to
systems. Example-based machine translation
systems, which learn corpus-specific translations
based on data, began to show substantial improve-
ments in the core problem of lexical selection.
This task was always quite difficult for rule-based
approaches: finding the correct translation in con-
text requires a large amount of knowledge. In
practice, nearby words are effective disambigua-
tors once a large amount of data has been captured.
Phrasal statistical machine translation systems
formalized many of the intuitions in example-
based machine translation approaches, replacing
heuristic selection functions with robust statistical
estimators. Effective search techniques developed
originally for speech recognition were strong start-
ing influences in the complicated realm of MT de-
coding. Finally, large quantities of parallel data
and even larger quantities of monolingual data al-
lowed such phrasal methods to shine even in broad
domain translation.
Translations were still far from perfect, though.
Phrasal systems capture local context and local re-
ordering well, but struggle with global reordering.
Over the past decade, statistical machine transla-
tion has begun to be influenced by linguistic infor-
mation once again. Syntactic models have shown
some of the most compelling gains. Many sys-
tems leverage the syntactic structure of either the
51
source or the target sentences to make better deci-
sions about reordering and lexical selection.
Our machine translation group has been an ac-
tive participant in many of these latest develop-
ments. The first MSR MT system used deep lin-
guistic features, often with great positive effect.
Inspired by the successes and failures of this sys-
tem, we invested heavily in syntax-based SMT.
However, our current statistical systems are still
linguistically impoverished in comparison.
This paper attempts to document important
lessons learned, highlight current best practices,
and identify promising future directions for im-
proving machine translation. A brief review of
our earlier generation of machine translation tech-
nology sets the stage; this older system remains
relevant given renewed interest in semantics (e.g.,
http://amr.isi.edu/). Next we describe some of
our statistical and non-statistical attempts to in-
corporate linguistic insights into machine transla-
tion systems, showing what is currently working
well, and what is not. We also look at trade-offs
in using linguistic knowledge (?rules?) in pre- or
post-processing by language pair, with a particu-
lar eye on the return on investment as training data
increases in size. Systems built on different ar-
chitectures, particularly those incorporating some
linguistic information, may have different learn-
ing curves on data. The advent of social media
and big data presents new challenges; we review
some effective research in this area. We conclude
by exploring promising directions for improving
translation quality, especially focusing on areas
that stand to benefit from linguistic information.
2 Logical Form Translation
Machine translation research at Microsoft Re-
search began in 1999. Analysis components had
been developed to parse surface sentences into
deep logical forms: predicate-argument structures
that normalized away many morphological and
syntactic differences. This deep representation
was originally intended for information mining
and question answering, allowing facts to rein-
force one another, and simplifying question and
answer matching. These same normalizations
helped make information more consistent across
languages: machine translation was a clear poten-
tial application. Consider the deep representations
of the sentence pairs in Figure 1: many of the sur-
face differences, such as word order and morpho-
Figure 1: Example logical forms for three distinct
inputs, demonstrating how differences in syntactic
structure may be normalized away. In each case,
the logical form is a graph of nodes such as ?be?
and ?difficult?, and relations such as ?Tobj? (typ-
ical object) and ?Tsub? (typical subject). In addi-
tion, nodes are marked with binary features called
bits, prefixed with a + symbol in the notation, that
capture unstructured pieces of information such as
tense and number.
logical inflection, are normalized away, potentially
easing the translation process.
Substantial differences remained, however.
Many words and phrases have non-compositional
contextually-influenced translations. Commercial
systems of the time relied on complex, hand-
curated dictionaries to make this mapping. Yet
example-based and statistical systems had already
begun to show promise, especially in the case of
domain-specific translations. Microsoft in par-
ticular had large internal demand for ?technical?
translations. With increasing language coverage
and continuing updates to product documentation
and support articles came increasing translation
costs. Producing translations tailored to this do-
main would have been an expensive task for a
rule-based system; a corpus-based approach was
pursed.
This was truly a hybrid system. Source and tar-
get language surface sentences were parsed into
deep logical forms using rule-based analyzers.2
2These parsers were developed with a strong focus on cor-
pora, though. George Heidorn, Karen Jensen, and the NLP
research group developed a toolchain for quickly parsing a
large bank of test sentences and comparing against the last
best result. The improvements and regressions resulting from
a change to the grammar could be manually evaluated, and
the changes refined until the end result. The end result was a
52
Figure 2: The process of learning translation in-
formation from parallel data in the LF system.
Likewise a rule-based target language generation
component could find a surface realization of a
deep logical form. However, the mapping from
source language logical form fragments to target
language logical form fragments was learned from
parallel data.
2.1 Details of the LF-based system
Training started with a parallel corpus. First, the
source and target language sentences were parsed.
Then the logical forms of the source and target
were aligned (Menezes and Richardson, 2001).
These aligned logical forms were partitioned into
minimal non-compositional units, each consisting
of some non-empty subset of the source and tar-
get language nodes and relations. Much like in
example-based or phrasal systems, both minimal
and composed versions of these units were then
stored as possible translations. A schematic of the
this data flow is presented in Figure 2.
At runtime, an input sentence was first parsed
into a logical form. Units whose source sides
matched the logical form were gathered. A heuris-
tic search found a set of fragments that: (a) cov-
ered every input node at least once, and (b) were
consistent in their translation selections. If some
node or relation was not uncovered, it was copied
from source to target. The resulting target lan-
guage logical form was then fed into a genera-
tion component, which produced the final string.
A schematic diagram is presented in Figure 3.
This overview sweeps many fine details un-
der the rug. Many morphological and syntactic
distinctions were represented as binary features
(?bits?) in the LF; mapping bits was difficult. The
data driven but not statistical approach to parser development.
Figure 3: The process of translating a new sen-
tence in the LF system.
logical form was a graph rather than a tree ? in
?John ate and drank?, John is the DSUB (deep sub-
ject) of both eat and drink ? which led to com-
plications in transferring structure. Many such
complications were often handled through rules;
these rules grew more complex over time. Corpus-
based approaches efficiently learned many non-
compositional and domain specific issues.
2.2 Results and lessons learned
The system was quite successful at the time. MSR
used human evaluation heavily, performing both
absolute and relative quality evaluations. In the
absolute case, human judges gave each transla-
tion a score between 1 (terrible translation) and
4 (perfect). For relative evaluations, judges were
presented with two translations in randomized or-
der, and were asked whether they preferred system
A, system B, or neither. In its training domain,
the LF-based system was able to show substantial
improvements over rule-based systems that domi-
nated the market at the time.
Much of these gains were due to domain- and
context-sensitivity of the system. Consider the
Spanish verb ?activar?. A fair gloss into En-
glish is ?activate?, but the most appropriate trans-
lation in context varies (?signal?, ?flag?, etc.). The
example-based approach was able to capture those
contexts very effectively, leading to automatic do-
main customization given only translation mem-
ories. This was a huge improvement over rule-
based systems of the time.
During this same era, however, statistical ap-
proaches (Och and Ney, 2004) were showing great
promise. Therefore, we ran a comparison be-
tween the LF-based system and a statistical system
53
(a) Effecitve LF translation. Note how the LF system is able to translate ?se lleveban a cabo? even though that particular
surface form was not present in the training data.
SRC: La tabla muestra adema?s do?nde se llevaban a cabo esas tareas en Windows NT versio?n 4.0.
REF: The table also shows where these tasks were performed in Windows NT version 4.0.
LF: The table shows where, in addition, those tasks were conducted on Windows NT version 4.0.
STAT: The table also shows where llevaban to Windows NT version 4.0.
(b) Parsing errors may degrade translation quality; the parser interprted ?/? as coordination.
SRC: La sintaxis del operador / tiene las siguientes partes:
REF: The / operator syntax has these parts:
LF: The operator syntax it has the parts:
STAT: The / operator syntax has these parts:
(c) Graph-like structures for situations such as coordination are difficult to transfer (see the parenthesized group in particular);
selecting the correct form at generation time is difficult in the absence of a target language model.
SRC: Debe ser una consulta de seleccio?n (no una consulta de tabla de referencias cruzadas ni una consulta de accio?n).
REF: Must be a select query (not a crosstab query or action query).
LF: You must not be a select query neither not a query in table in cross-references nor not an action query.
STAT: Must be a select query (not a crosstab query or an action query).
Figure 4: Example source Spanish sentences, English reference translations of those sentences, transla-
tions from the LF system, and translations from a statistical translation system without linguistic features.
without linguistic information. Both systems were
trained and tuned on the same data, and translated
the same unseen test set. The linguistic system
had the additional knowledge sources at its dis-
posal: morphological, lexical, syntactic, and se-
mantic information. Regardless, the systems per-
formed nearly equally well on average. Each had
distinct strengths and weaknesses, though.
Often the success or failure of the LF-system
was tied to the accuracy of its deep analysis. When
these representations were accurate, they could
lead to effective generalizations and better trans-
lations of rare phenomena. Since surface words
were lemmatized and syntactic differences nor-
malized, unseen surface forms could still be trans-
lated as long as their lemma was known (see Fig-
ure 4(a)). Yet mistakes in identifying the correct
logical form could lead to major translation errors,
as in Figure 4(b).
Likewise the lack of statistics in the com-
ponents could cause problems. Statistical ap-
proaches found great benefits from the target lan-
guage model. Using a rule-based generation com-
ponent made it difficult to leverage a target lan-
guage model. Often, even if a particular transla-
tion was presented tens, hundreds, or thousands
of times in the data, the LF-based system could
not produce it because the rule-based generation
component would not propose the common sur-
face form, as in Figure 4(c).
We drew several lessons from this system when
developing our next generation of machine trans-
lation systems. It was clear to us that syntactic rep-
resentations can help translation, especially in re-
ordering and lexical selection: appropriate repre-
sentations allows better generalization. However,
over-generalization can lead to translation error, as
can parsing errors.
3 The Next Generation MSR MT
Systems
Research in machine translation at Microsoft has
been strongly influenced by this prior experience
with the LF system. First we must notice that
there is a huge space of possible translations. Con-
sider human reference translations: unless tied to
a specific domain or area, they seldom agree com-
pletely on lexical selection and word order. If our
system is to produce reasonable output, it should
consider a broad range of translation options, pre-
ferring outputs most similar to language used by
humans. Why do we say ?order of magnitude?
rather than ?magnitude order?, or ?master of cer-
emonies? rather than ?ceremonies master?? Many
choices in language are fundamentally arbitrary,
but we need to conform to those arbitrary deci-
sions if we are to produce fluent and understand-
able output. Second, while there is leverage to be
gained from deep features, seldom do we have a
component that identifies these features with per-
54
fect accuracy. In practice it seems that the error
rate increases as the depth of component analy-
sis increases. Finally, we need a representation
of ?good translations? that is understandable by a
computer. When forced to choose between two
translations, the system needs to make a choice:
an ordering.
Therefore, our data-driven systems crucially
rely on several components. First, we must effi-
ciently search a broad range of translations. Sec-
ond, we must rank according to both our linguistic
intuitions and the patterns that emerge from data.
We use a number of different systems based
on the availability of linguistic resources. So-
called phrasal statistic machine translation sys-
tems, which model translations using no more than
sequences of contiguous words, perform surpris-
ingly well and require nothing but tokenization in
both languages. In language pairs for which we
have a source language parser, a parse of the in-
put sentence is used to guide reordering and help
select relevant non-contiguous units; this is the
treelet system (Quirk and Menezes, 2006). Re-
gardless of which system we use, however, tar-
get language models score the fluency of the out-
put, and have a huge positive impact on translation
quality.
We are interested in means of incorporating lin-
guistic intuition deeper into such a system. As in
the case of the treelet system, this may define the
broad structure of the system. However, there are
also more accessible ways of influencing existing
systems. For instance, linguists may author fea-
tures that identify promising or problematic trans-
lations. We describe one such attempt in the fol-
lowing system.
3.1 Like and DontLike
Even in our linguistically-informed treelet sys-
tem (Quirk and Menezes, 2006), which uses syn-
tax in its translation system, many of the individ-
ual mappings are clearly bad, at least to a human.
When working with linguistic experts, one gut re-
sponse is to write rules that inspect the transla-
tion mappings and discard those translation map-
pings that appear dangerous. Perhaps they seem
to delete a verb, perhaps they use a speculative re-
ordering rule ? something makes them look bad to
a linguist. However, even if we are successful in
removing a poor translation choice, the remaining
possibilities may be even worse ? or perhaps no
translation whatsoever remains.
Instead, we can soften this notion. Imagine that
a linguist is able to say that this mapping is not
preferred because of some property. Likewise, a
skilled linguist might be able to identify mappings
that look particularly promising, and prefer those
mappings to others; see Figure 5 for an example.
This begs the question: how much should we
weight such influence? Our answer is a corpus
driven one. Each of these linguistic preferences
should be noted, and the weight of these prefer-
ences should be tuned with all others to optimize
the goodness of translation. Already our statisti-
cal system has a number of signals that attempt to
gauge translation quality: the translation models
attempt to capture fidelity of translation; language
models focus on fluency; etc. We use techniques
such as MERT (Och, 2003) and PRO (Hopkins
and May, 2011) to tune the relative weight of these
signals. Why not tune indicators from linguists in
the same manner?
When our linguists mark a mapping as +Like or
+DontLike, we track that throughout the search.
Each final translation incorporates a count of Like
mappings and a count of DontLike mappings, just
as it accumulates a language model score, trans-
lation model scores, word penalties, and so on.
These weights are tuned to optimize some approx-
imate evaluation metric. In Figure 6, the weight
of Like and DontLike is shown for a number of
systems, demonstrating how optimization may be
used to tune the effect of hand-written rules. Re-
moving these features degrades the performance
of an MT system by at least 0.5 BLEU points,
though the degradations are often even more visi-
ble to humans.
This mechanism has been used to capture a
number of effects in translation commonly missed
by statistical methods. It is crucial yet challenging
to maintain negation during translation, especially
in language pairs where negation is expressed dif-
ferently: some languages use a free morpheme
(Chinese tends to have a separate word), others
use a bound morpheme (English may use pre-
fixes), others require two separated morphemes
(French has negation agreement); getting any of
these wrong can lead to poor translations. Rules
that look at potentially distant words can help
screen away negation errors. Likewise rules can
help ensure that meaning is preserved, by prevent-
ing main verbs mapping to punctuation, or screen-
55
// don?t allow verb to be lost
if (forany(NodeList(rMapping),[Cat=="Verb" & ?Aux(SynNode(InputNode))])) {
list {segrec} bad_target=sublist(keeplist,
[forall(NodeList,[pure_punk(Lemma) | coord_conjunction(foreign_language,Lemma)])]);
if (bad_target) {
segrec rec;
foreach (rec; bad_target) {
+DontLike(rec);
}
}
}
Figure 5: An example rule for marking mappings as ?DontLike?. In this case, the rule searches for
source verbs that are not auxiliaries and that are translated into lemmas or punctuation. Such translations
are marked as DontLike.
Figure 6: A plot of the weights +Like map-
ping count and +DontLike mapping count weights
across language pairs. Generally Like is assigned
a positive weight (sometimes quite positive), and
DontLike is assigned a negative weight. In our
system, weights are L1 normalized (the sum of the
absolute values of the weights is equal to one), so
feature weights greater than 0.1 are very influen-
tial.
ing out mappings that seem unlikely, especially
when those mappings involve unusual tokens.
These two features are a rather coarse means of
introducing linguistic feedback. As our parame-
ter estimation techniques scale to larger features
more effectively, we are considering using finer-
grained feedback from linguists to say not only
that they like or don?t like a particular mapping,
but why. The relative impact of each type of feed-
back can be weighted: perhaps it is critical to pre-
serve verbs, but not so important to handle defi-
niteness. Given recent successes in scaling param-
eter estimation to larger and larger values, this area
shows great promise.
3.2 Linguistic component accuracy
Another crucial issue is the quality of the linguistic
components. We would certainly hope that better
quality of linguistic analysis should lead to bet-
ter quality translations. Indeed, in certain circum-
stances it appears that this correlation holds.
In the case of the treelet system, we hope to de-
rive benefit from linguistic features via a depen-
dency tree. To investigate the impact of the parse
quality, we can degrade a Treebank-trained parser
by limiting the amount of training data made avail-
able. As this decreases, the parser quality should
degrade. If we hold all other information in the
MT system fixed (parallel and monolingual train-
ing data, training regimen, etc.), then all differ-
ences should be due to the changes in parse qual-
ity. Table 1 presents the results of an experiment
of this form (Quirk and Corston-Oliver, 2006). As
the amount of training data increase, we see a sub-
stantial increase in parse quality.
Another way to mitigate parser error is to main-
tain syntactic ambiguity through the translation
process. For syntax directed translation systems,
this can be achieved by translating forests rather
than single trees, ideally including the score of
56
English- English-
System German Japanese
Phrasal 31.7 32.9
Right branching 31.4 28.0
250 instances 32.8 34.1
2,500 instances 33.0 34.6
25,000 instances 33.7 35.7
39,082 instances 33.8 36.0
Table 1: Comparison of BLEU scores as linguistic
information is varied. A phrasal system provides
a baseline free of linguistic information. Next we
consider a treelet system with a very weak base-
line: a right branching tree is always proposed.
This baseline is much worse than a simple phrasal
system. The final four rows evaluate the impact
of a parser trained on increasing amounts of sen-
tences from the English Penn Treebank. Even with
a tiny amount of training data, the system gets
some benefit from syntactic information, and the
returns appear to increase with more training data.
parse as part of the translation derivation. In un-
published results, we found that this made a sub-
stantial improvement in translation quality; the
effect was corroborated in other syntax directed
translation systems (Mi et al, 2008). Alterna-
tively, allowing a neighborhood of trees similar
to some predicted tree can handle ambiguity even
when the original parser does not maintain a for-
est. This also allows translation to handle phenom-
ena that are systematically mis-parsed, as well as
cases where the parser specification is not ideal
for the translation task. Recent work in this area
has show substantial improvements (Zhang et al,
2011).
4 Evaluation
4.1 Fact or Fiction: BLEU is Biased Against
Rule-Based or Linguistically-Informed
Systems?
It has generally been accepted as common wis-
dom that BLEU favors statistical MT systems and
disfavors those that are linguistically informed or
rule-based. Surprisingly, the literature on the topic
is rather sparse, with some notable exceptions
(Riezler and Maxwell, 2005; Farru?s et al, 2012;
Carpuat and Simard, 2012). We too have made
this assumption, and had a few years ago coined
the term treelet penalty to indicate the degree by
which BLEU favored our phrasal systems over our
treelet systems. We had noted on a few occa-
sions that treelet systems had lower BLEU scores
than our phrasal systems over the same data (the
?penalty?), but when compared against one an-
other in human evaluation, there was little dif-
ference, or often, treelet was favored. A notable
case was on German-English, where we noted a
three-point difference in BLEU between equiva-
lent treelet and phrasal systems (favoring phrasal),
and a ship/no-ship decision was dependent on the
resulting human eval. The general consensus of
the team was that the phrasal system was markedly
better, based on the BLEU result, and treelet sys-
tem should be pulled. However, after a human eval
was conducted, we discovered that the treelet sys-
tem was significantly better than the phrasal. From
that point forward, we talked about the treelet
penalty for German being three points, a ?fact?
that has lived in the lore of our team ever since.
What was really missing, however, was sys-
tematic experimental evidence showing the differ-
ences between treelet and phrasal systems. We
talked about the treelet penalty as a given, but
there was slow rumble of counter evidence sug-
gesting that maybe the assumptions behind the
?penalty? were actually unfounded, or minimally,
misinformed.
One piece of evidence was from experiments
done by Xiaodong He and an intern that showed an
interaction in quality differences between treelet
and phrasal gated by the length of the sentence.
Xiaodong was able to show that phrasal systems
tended to do better on longer sentences and treelet
on shorter: for Spanish-English, he showed a dif-
ference in BLEU of 1.29 on ?short? content on a
general domain test set, and 1.77 for short content
on newswire content (the NIST08 test set). The
BLEU difference diminished as the length of the
content increased, until there was very little dif-
ference (less than 1/2 point) for longer content.3
An interaction between decoder type and sentence
length means that there might also be an interac-
3These results were not published, but were provided to
the authors in a personal conversation with Xiaodong. In a
related paper (He et al, 2008), He and colleagues showed
significant improvements in BLEU on a system combination
system, but no diffs in human eval. Upon analysis, the re-
searchers were able to show that the biggest benefit to BLEU
was in short content, but the same preference was not exhib-
ited on the same content by the human evaluators. In other
words, the improvements observed in the short content that
BLEU favored had little impact on the overall impressions of
the human evaluators.
57
tion between decoder type and test set, especially
if particular test sets contain a lot of long-ish sen-
tences, e.g., WMT and Europarl). To the contrary,
most IT text, which is quite common in Microsoft-
specific localization content, tends to be shorter.
The other was based on general impressions
between treelet and phrasal systems. Because
treelet systems are informed by dependency parses
built over the source sentences (a parse can help
constrain a search space of possible translations,
and prune undesirable mappings e.g., constrain to
nominal types when the source is a noun), and,
as noted earlier, because the parses allow linguists
to pre- or post-process content based on observa-
tions in the parse, we have tended to see more
?fluent? output in treelet than phrasal. However,
as the sizes of data have grown steadily over the
years, the quality of translations in our phrasal sys-
tems have grown proportionally with the increase
in data. The question arose: is there also an in-
teraction between the size of our training data and
decoder type? In effect, does the quality of phrasal
systems catch-up to the quality of treelet systems
when trained over very large sets of data?
4.2 Treelet Penalty Experiments
We ran a set of experiments to measure the dif-
ferences between treelet and phrasal systems over
varying sizes of data, in order to measure the size
of the treelet penalty and its interaction with train-
ing data size. Our assumption was that a such
a penalty existed, and that the penalty decreased
as training data size increased, perhaps converg-
ing on zero for very large systems. Likewise,
we wanted to test the interaction between decoder
type and sentence length.
We chose two languages to run these exper-
iments on, Spanish and German, which we ran
in both directions, that is, English-to-target (EX)
and target-to-English (XE). We chose Spanish and
German for several reasons, first among them be-
ing that we have high-quality parsers for both lan-
guages, as we do for English. Further, we have
done significant development work on pre- and
post-processing for both languages over the past
several years. Both of these facts combined meant
that the treelet systems stood a real chance of be-
ing strong contenders in the experiments against
the equivalent phrasal systems. Further, although
the languages are typologically close neighbors
of English, the word order differences and high
distortion rates from English to or from German
might favor a parser-based approach.
We had four baseline systems that were built
over very large sets of data. For Spanish  En-
glish, the baseline systems were trained on over
22M sentence pairs; for German  English, the
baseline systems were trained on over 36M sen-
tence pairs.4 We then created five samples of the
baseline data for each language pair, consisting of
100K, 500K, 1M, 2M, and 5M sentence pairs (the
same samples were used for both EX and XE for
the respective pairs). We then trained both treelet
and phrasal systems in both directions (EX and
XE) over each sample of data. Language mod-
els were trained on all systems over the target-side
data.
For dev data, we used development data from
the 2010 WMT competition (Callison-Burch et al,
2010), and we used MERT (Och, 2003) to tune
each system. We tested each system against three
different test sets: two were from the WMT com-
petitions of 2009 and 2010, and the other was
one locally constructed from 5000 sentences of
content translated by users of our production ser-
vice (http://bing.com/translator), which we subse-
quently had manually translated into the target lan-
guages. The former two test sets are somewhat
news focused; the latter is a random sample of
miscellaneous translations, and is more generally
focused.
The results of the experiments are shown in Ta-
bles 2 and 3, with the relevant graphs in Fig-
ures 9 - 10. The reader will note that in all cases?
Spanish and German, EX and XE?the treelet sys-
tems scored higher than the related phrasal sys-
tems. This result surprised us, since we thought
that treelet systems would score less than phrasal
systems, especially at lower data sizes. That said,
in the Spanish systems, there is a clear conver-
gence as data sizes increased: on the WMT09
test set for English-Spanish, for instance, the diff
starts at 1.46 BLEU (treelet minus phrasal) for
the 100K sentence system, with a steady conver-
gence to near zero (0.12) for the full-data baseline.
The other test sets show the same steady conver-
gence, although they do not approach zero quite
as closely. (One might ask whether they would
converge to zero with more training data.) The
4A sizable portion of the data for each were scraped from
the Web, but there were other sources used as well, such as
Europarl, data from TAUS, MS internal localization data, UN
content, WMT news content, etc.
58
other direction is even more dramatic: on all test
sets the diffs converge on negative values, indi-
cating that phrasal systems surpass the quality of
the associated treelet systems at the largest data
points. This is a nice result since it shows, at least
in the case of Spanish, that there is an interac-
tion between decoder type and the amount of data:
treelet clearly does better at lower data amounts,
but phrasal catches up with, and can even pass, the
quality of equivalent treelet given sufficient data.
With larger data, phrasal may, in fact, be favored
over treelet.
The German systems do not tell quite as nice a
story. While it is still true that treelet has higher
BLEU scores than phrasal throughout, and that
systems trained using both decoders improve in
quality as more data is added (and the trajectory
is similar), there is no observable convergence as
data size increases. For German, then, we can only
say that more data helps either decoder, but we
cannot say that phrasal benefits from larger data
more than treelet. Why the difference between
Spanish and German? We suspect there may be an
interaction with the parsers, in that two separate
teams developed them. Thus, it could be the fact
that the strength of the respective parsers affected
how ?linguistically informed? particular systems
are. There could also be an interaction with the
number of word types vs. tokens in the German
data?given German?s rampant compounding?
which increases data sparsity, dampening effects
until much larger amounts of data are used. We
are still in the process of running additional ex-
periments to see if there are observable effects in
German with much larger data sizes, or at least,
to determine why German does not show the same
effects as Spanish.
Figure 7: English-Spanish BLEU graph across dif-
ferent data sizes, Treelet vs. Phrasal.
Since human evaluation is the gold standard we
Figure 8: Spanish-English BLEU graph across dif-
ferent data sizes, Treelet vs. Phrasal.
Figure 9: English-German BLEU graph across
different data sizes, Treelet vs. Phrasal.
Figure 10: German-English BLEU graph across
different data sizes, Treelet vs. Phrasal.
59
EX Treelet Phrasal Diff - T-P
Req Log WMT 2009 WMT 2010 Req Log WMT 2009 WMT 2010 Req Log WMT 2009 WMT 2010
100K 26.49 21.52 23.69 23.10 20.06 21.19 3.39 1.46 2.50
500K 28.61 22.85 25.20 25.64 21.47 22.86 2.97 1.38 2.34
1M 30.52 24.82 27.74 28.36 24.17 26.28 2.16 0.65 1.46
2M 31.61 25.59 28.54 29.48 24.76 26.91 2.13 0.83 1.63
5M 32.86 26.37 30.14 30.89 25.84 28.56 1.97 0.53 1.58
22M 33.80 27.01 30.61 32.55 26.89 30.12 1.25 0.12 0.49
XE
100K 27.72 21.76 23.21 26.18 20.80 21.78 1.54 0.96 1.43
500K 29.89 22.86 24.89 28.16 22.15 23.44 1.73 0.71 1.45
1M 32.18 24.76 27.14 31.32 24.32 26.02 0.86 0.44 1.12
2M 33.31 25.44 28.09 32.77 25.26 27.38 0.54 0.18 0.71
5M 34.47 26.17 29.10 34.18 26.10 28.74 0.29 0.07 0.36
22M 35.88 27.16 30.20 36.21 27.26 30.48 -0.33 -0.10 -0.28
Table 2: BLEU Score results for the Spanish Treelet Penalty experiments
EX Treelet Phrasal Diff (T-P)
Req Log WMT 2009 WMT 2010 Req Log WMT 2009 WMT 2010 Req Log WMT 2009 WMT 2010
100K 18.98 11.13 12.19 18.22 10.81 11.53 0.76 0.32 0.66
500K 22.13 13.18 14.33 21.09 12.74 13.68 1.04 0.44 0.65
1M 23.23 13.98 15.12 21.89 13.51 14.27 1.34 0.47 0.85
2M 23.72 14.77 15.87 23.11 14.04 15.03 0.61 0.73 0.84
5M 24.82 15.31 16.58 24.35 15.00 16.01 0.47 0.31 0.57
36M 26.72 16.72 18.20 25.83 16.33 17.18 0.89 0.39 1.02
XE
100K 27.42 15.91 16.37 26.75 15.83 16.28 0.67 0.08 0.09
500K 30.98 18.25 19.16 29.80 18.11 19.09 1.18 0.14 0.07
1M 32.30 19.16 20.40 31.26 19.06 20.18 1.04 0.10 0.22
2M 33.40 19.95 21.48 32.25 19.65 21.06 1.15 0.30 0.42
5M 34.86 21.14 22.55 33.91 20.67 22.13 0.95 0.47 0.42
36M 37.31 22.72 24.97 36.08 21.99 23.85 1.23 0.73 1.12
Table 3: BLEU Score results for the German Treelet Penalty experiments
60
seek to achieve with our quality measures, and
since BLEU is only weakly correlated with hu-
man eval (Coughlin, 2003), we ran human evals
against both the English-Spanish and English-
German output. Performing human evaluation
gives us two additional perspectives on the data:
(1) do humans perceive a qualitative difference be-
tween treelet and phrasal, as we see with BLEU,
and (2), if the difference is perceptible, what is its
magnitude relative to BLEU. If the magnitude of
the difference is much larger than that of BLEU,
and especially does not show convergence in the
Spanish cases, then we still have a strong case
for the Treelet Penalty. In fact, if human evalu-
ators perceive a difference Spanish cases on the
full data systems, the case where we show con-
vergence, then the resulting differences could be
described as the penalty value.
Unfortunately, our human evaluation data on
the Treelet Penalty effect was inconclusive. Our
evaluations show a strong correlation between
BLEU and human evaluation, something that is at-
tested to in the literature (e.g., , the first paper on
BLEU (Papineni et al, 2002), and a deeper explo-
ration in (Coughlin, 2003)). However, the effect
we were looking for ? that is, a difference between
human evaluations across decoders ? was not evi-
dent. In fact, the human evaluations followed the
differences we saw in BLEU between the two de-
coders very closely. Figure 11 shows data points
for each data size for each decoder, plotting BLEU
against human evaluation. When we fit a regres-
sion line against the data points for each decoder,
we see complete overlap.5
Figure 11: Scatterplot showing Treelet vs Phrasal
systems across different data sizes, plotting BLEU
(Y) against Human Eval scores (X)
5Clearly, the sample is very small, so the regression line
should be taken with a grain of salt. We would need a lot
more data to be able to draw any strong conclusions.
In summary, we show a strong effect of treelet
systems performing better than phrasal systems
trained on the same data. That difference, how-
ever, generally diminishes as data sizes increase,
and in the case of Spanish (both directions), there
is a convergence in very large data sizes. These
results are not completely surprising, but still are a
nice systematic confirmation that linguistically in-
formed systems really do better in lower-data en-
vironments. Without enough data, statistical sys-
tems cannot learn the generalizations that might
otherwise be provided by a parse, or codified in
rules. What we failed to show, at least with Span-
ish and German, is a confirmation of the existence
of the Treelet Penalty. Given the small number of
samples, a larger study which includes many more
language pairs and data sizes, may once and for all
confirm the Penalty. Thus far, human evaluations
do not show qualitative differences between the
two decoders?at least, not divergent from BLEU.
4.3 Interaction Between Decoder Type and
Sentence Length
When comparing the differences between de-
coders, another area to pay special attention to is
systematic differences in behavior as input content
is varied. For example, we may expect a phrasal
decoder to do better on noisier, less grammatical
data than a parser-informed decoder, since in the
latter case the parser may fail to parse; the failure
could ripple through subsequent processes, and
thus lessen the quality of the output. Likewise, a
parser-informed decoder may do better on content
that is short and easy to parse. If we were to do a
coarse-grained separation of data into length buck-
ets, making the very gross assumption that short
equals easy-to-parse and long not, then we may
see some qualitative differences between the de-
coders across these buckets.
To see length-based effects across decoder
types, we designed a set of experiments on Ger-
man and Spanish in both directions, where we sep-
arated the WMT 2010 test data into length-based
word-count buckets: 0-10, 10-20, 20-30, 30-40,
and 40+ words. We then calculated the BLEU
scores on each of these buckets, the results for
which are shown in Figures 12.
Treelet does better than phrasal in almost all
conditions (except one). That is not surprising,
given the results we observed in Section 4.2. What
is interesting is to see how much stronger treelet
61
Figure 12: Treelet-Phrasal BLEU differences by
bucket across language pair
performs on short content than phrasal: treelet
does the best on the shortest content, with quality
dropping off anywhere between 10-30 words.
One conclusion that can be drawn from these
data is that treelet performs best on short con-
tent precisely because the parser can easily parse
the content, and the parse is effective in inform-
ing subsequent processes. The most sustained
benefit is observable in English-German, with a
bump up at 10-20, and a slow tapering off there-
after. Processing the structural divergence be-
tween the two languages, especially when it comes
to word order, may benefit more from a parse. In
other words, the parser can help inform alignment
where there are long-distance distortion effects; a
phrasal system?s view is too local to catch them.
However, at longer sentence sizes, the absence
of good parses lessen the treelet advantage. In
fact, in English-German (and in Spanish-English)
at 40+, there is no observable benefit of treelet
over phrasal.6
5 The Data Gap
All Statistical Machine Translation work relies on
data, and the manipulation of the data as a pre-
process can often have significant effects down-
stream. ?Data munging?, as we like to call it, is
every team?s ?secret sauce?, something that can
often lead to multi-point differences in BLEU.
For most teams, the heuristics that are applied are
fairly ad hoc, and highly dependent on the kind of
data being consumed. Since data sources are of-
ten quite noisy, e.g., the Web, noise reduction is a
key component of many of the heuristics. Here is
6The bump up at 40+ on English-Spanish and German-
English is inexplicable, but may be attributable to the diffi-
culty that either decoder has in processing such long content.
There is also likely an interaction with statistical noise cause
by such small sample sizes.
a list of common heuristics applied to data. Some
of these are drawn from our own pre-processing,
some are mentioned explicitly in other literature,
in particular, (Denkowski et al, 2012).
? Remove lines containing escape characters,
invalid Unicode, and other non-linguistic
noise.
? Remove content that where the ratio of cer-
tain content passes some threshold, e.g., al-
phabetic/numeric ratio, script ratio (percent-
age of characters in wrong form passes some
threshold, triggering removal).
? Normalize space, hyphens, quotes, etc. to
standard forms.
? Normalize Unicode characters to canonical
forms, e.g., Form C, Form KC.
? In parallel data, measure the degree of ratio
of length imbalance (e.g., character or word
count) between source and target, as a test for
misalignments. Remove sentence pairs that
pass some threshold.
? Remove content where character count for
any token, or token count across a sentence,
exceeds some threshold (the assumption be-
ing that really long content is of little benefit
due to complications it causes in downstream
processing).
The point of data cleaning heuristics is to in-
crease the value of training data. Each data point
that is noisy increases the chance of learning
something that could be distracting or harmful.
Likewise, each data point that is cleaned reduces
the level of data sparsity (e.g., through normaliza-
tions or substitutions) and improves the chances
that the models will be more robust. Although
it has been shown that increasing the amount of
training data for SMT improves results (Brants et
al., 2007), not all data is beneficial, and clean data
is best of all.
Crucially, most data munging is done through
heuristics, or rules, although thresholds or con-
straints can be tuned by data. A more sophis-
ticated example of data cleaning is described in
(Denkowski et al, 2012) where the authors used
machine learning methods for measuring quality
estimation to select the ?best? portions of a cor-
pus. So, rather than training their SMT on an en-
tire corpus, they trained an estimator that selected
62
the best portions, and used only those. In their en-
try in the 2012 WMT competition, they used only
60% of the English-French Gigaword corpus7 and
came in first in the shared translation task for the
pair.
Another important aspect of data as it relates to
SMT is task-dependence: what domain or genre
of data will an SMT engine be applied to? For
instance, will an SMT engine be used to trans-
late IT content, news content, subtitles, or Eu-
roparl proceedings? If the engine itself is trained
on data that is dissimilar to the desired goal, then
results may be less than satisfying. This is a com-
mon problem in the field, and a cottage industry
has been built around customization and domain-
adaptation, e.g., (Moore and Lewis, 2010; Axelrod
et al, 2011; Wang et al, 2012). In general, the so-
lution is to adapt an SMT engine to the desired
domain using a set of seed data in that domain.
A more difficult problem is when there is very
little parallel data in the desired domain, which is
a problem we will look at in the next section.
5.1 Preprocessing Data to Make it Match
A little over a year ago, Facebook activated a
translation feature in their service, which directly
called Bing Translator. This feature has allowed
users to translate pages or posts not in their native
language with a See Translation option. An exam-
ple is shown in Figure 13.
The real problem with translating ?FB-speak?,
or content from virtually any kind of social media,
is the paucity of parallel data in the domain. This
flies in the face of the usual way problems are tack-
led in SMT, that is, locate (lots of) relevant parallel
data, and then train up a decoder. Outside of a few
slang dictionaries, there is almost no FB-like par-
allel content available.
Given the relatively formal nature of the text
that most of our engines are trained on, the mis-
match between FB content and our translation en-
gines often led to very poor translations. Yet,
given the absence of in-domain parallel data, it
was not possible for us to train-up FB-specific
SMT engines. We realized that our only option
was to somehow manipulate the input to make it
look more like the content we trained our engines
on. Effectively, if we treated ?FB-speak? as a di-
alect of the source language, we could use distri-
7The English-French Gigaword corpus is described in
(Callison-Burch et al, 2009)
Regex Output
frnd[sz] friends
plz+ please
yess* yes
be?c[uo][sz] because
nuff enough
wo?u?lda would have
srr+y sorry
Table 5: Some example regexes to ?fix? FaceBook
content
butional queues of dialect-specific content to find
the counterparts in the majority dialect.
Table 4 gives some examples of FB content on
the left, and the more formal representation of the
same on the right. The reader will note some sys-
tematic characteristics of the FB content as com-
pared to the formal content (see also (Hassan and
Menezes, 2013)). Given the absence of parallel
training data, we could ?correct? the FB content
to make it look more like English, and then trans-
late the ?corrected? English through our engines.
Our first inclination was to examine the logs of
the most frequent words being translated by FB
users and use string substitutions or regexes (regu-
lar expressions) to effect repairs. We arrived very
quickly at a large set of simple repairs like those
shown in Table 5. We were able to achieve greater
than 97% precision using a large table of substitu-
tions for the most common translations (against a
held-out set of FB content). However, there were
two problems with the approach: (1) recall was
relatively low, at 52.03%, and (2) the solution was
not easily scalable to additional languages and sce-
narios.
To address these two deficiencies, we sought a
more data-driven approach. But we had to be cre-
ative since our standard ?hammer? of parallel data
did not exist. Our intuition was that there were
distributional regularities in the FB content that
could help discover a mapping for a given target
word, e.g., the distribution of plzzz in the FB con-
tent would allow us to discover that it distributes
similarly to please in our non-FB content. Hany
Hassan developed a TextCorrector tool that is, as
he put it (Hassan and Menezes, 2013), ?based on
constructing a lattice from possible normalization
candidates and finding the best normalization se-
quence according to an n-gram language model
using a Viterbi decoder?, where he developed an
63
Figure 13: Two Facebook posts: the first translated, the second showing the See Translation option
FB Speak English Translation Comment
goooood morniiing good morning Extended characters for emphasis or dramatic effect
wuz up bro What?s up brother ?Phonetic? spelling to reflect local dialect or usage
cm to c my luv Come to see my love Remove vowels in common words, sound-alike sequences
4get, 2morrow forget, tomorrow Sound-alike number substitution
r u 4 real? Are you for real? Sound-alike letter and number substitutions
LMS Like my status Single ?word? abbreviations for
IDK I don?t know multi-word expressions
ROFL Rolling on the floor laughing
Table 4: FB Speak with English references
?unsupervised approach to learn the normalization
candidates from unlabeled text data.? He then used
a Random Walk strategy to walk a contextual sim-
ilarity graph. The two principal benefits of this
approach is that it did not require parallel train-
ing data?two large monolingual corpora are re-
quired, one for the ?noisy? data (i.e., FB content)
and one for the clean data (i.e., our large supply
of language model training data)?nor did it re-
quire labeled data (i.e., , the algorithm is unsu-
pervised). After several iterations over very large
corpora (tens of millions of sentences) he arrived
at a solution that had comparable precision to the
regex method but had much higher recall. The best
iteration achieved 96.51% precision (the regex ap-
proach achieve 97.07% precision) and 72.38% re-
call (regex: 52.03%).8 Crucially, as the size of
the data increases, the TextCorrector continues to
show improvement.
The end result was a much better User Expe-
rience for FB users. Rather than badly mangled
translations, or worse, no translations at all, users
get translations generated by our standard, very
large statistical engines (for English source, no-
tably, our treelet engines). An example English
source string is shown in Table 6, with transla-
8For a complete description of TextCorrector, please
see (Hassan and Menezes, 2013).
tions shown for both the corrected and uncorrected
source.
6 Conclusions and Future Directions
A crucial lesson from the work on the FB correc-
tions described in Section 5.1 is its analog to Ma-
chine Learning as a whole: rule-based approaches
often achieve very high precision, but often at the
sacrifice of recall. The same is true in Machine
Translation: rule-based MT is often more accurate
when it was accurate, resulting in more precise and
grammatical translations. However, it tends to be
somewhat brittle and does not do as well on cases
not explicitly coded for. SMT, on the other hand,
tends to be more malleable and adaptable, but of-
ten less precise. Tapping rule-based approaches
in a statistical framework can really give us the
best of both worlds, giving us higher precision and
higher recall.
Finding an appropriate mix is difficult, though.
As in the case of parsing, we can see how errors
can substantially degrade translation quality, espe-
cially if we only consider the single best analysis.
By making our analysis components as robust as
possible, quantifying our degree of certainty with
scoring mechanisms, and preserving ambiguity of
the analysis, we can achieve a better return on in-
64
Language Unrepaired Repaired
Original English i?l do cuz ma parnts r ma lyf I?ll do because my parents are my life
To Italian i ? l fare cuz ma parnts r ma lyf lo far perch i miei genitori sono la mia vita
To German i ? l tun Cuz Ma Parnts R Ma lyf Ich werde tun, weil meine Eltern mein Leben sind
To Spanish traer hacer cuz ma parnts r ma lyf voy a hacer porque mis padres son mi vida
Table 6: One English FB sentence with and without normalizations, translated to various languages
vestment. Making this linguistic information be
included softly as features is a powerful way of
surfacing linguistic generalizations to the system
while not forcing its hand.
Some of the greatest successes in mixing lin-
guistic and statistical methods have been in syn-
tax. There is much ground to cover still. Mor-
phology is integrated weakly into current SMT
systems, mostly as broad features (Jeong et al,
2010) though sometimes with more sophistica-
tion (Chahuneau et al, 2013). Better integration of
morphological features could have great effect, es-
pecially in agglutinative languages such as Finnish
and Turkish.
Deeper models of semantics present a rich chal-
lenge to the field. As we proceed into deeper mod-
els, picking the correct representation is a signifi-
cant issue. Humans can generally agree on words,
mostly on morphology, and somewhat on syntax.
But semantics touches on issues of meaning repre-
sentation: how should we best represent semantic
information? Should we attempt to faithfully rep-
resent all the information in the source language,
or gather only a simple model that suffices to dis-
ambiguate information? Others are focusing on
lexical semantics using continuous space repre-
sentations (Mikolov et al, 2013), a softer means
of representing meaning.
Regardless of the details, one point is very clear:
future work in MT will require dealing with data.
Systems, whether statistical or rule-based, will
need to work with and learn from the increas-
ing volumes of information available to comput-
ers. Effective hybrid systems will be no exception
? tempering the keen insights of experts with the
noisy wisdom of big data from the crowd holds
great promise.
References
Amittai Axelrod, Xiaodong He, and Jianfeng Gao.
2011. Domain adaptation via pseudo in-domain data
selection. In Proceedings of EMNLP, pages 355?
362.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.
Och, and Jeffrey Dean. 2007. Large language
models in machine translation. In Proceedings
of the 2007 Joint Conference on Empirical Meth-
ods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 858?867, Prague, Czech Republic,
June. Association for Computational Linguistics.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the 2009
Workshop on Statistical Machine Translation. In
Proceedings of the Fourth Workshop on Statistical
Machine Translation, pages 1?28, Athens, Greece,
March. Association for Computational Linguistics.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Kay Peterson, Mark Przybocki, and Omar Zaidan.
2010. Findings of the 2010 joint workshop on sta-
tistical machine translation and metrics for machine
translation. In Proceedings of the Joint Fifth Work-
shop on Statistical Machine Translation and Metric-
sMATR, pages 17?53, Uppsala, Sweden, July. Asso-
ciation for Computational Linguistics.
Marine Carpuat and Michel Simard. 2012. The trouble
with smt consistency. In Proceedings of the Seventh
Workshop on Statistical Machine Translation, pages
442?449, Montre?al, Canada, June. Association for
Computational Linguistics.
Victor Chahuneau, Noah A. Smith, and Chris Dyer.
2013. Knowledge-rich morphological priors for
bayesian language models. In Proceedings of the
2013 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 1206?1215, At-
lanta, Georgia, June. Association for Computational
Linguistics.
Deborah A. Coughlin. 2003. Correlating automated
and human assessments of machine translation qual-
ity. In Proceedings of MT Summit IX, New Or-
leans, Louisiana, USA, September. The Association
for Machine Translation in the Americas (AMTA).
Michael Denkowski, Greg Hanneman, and Alon Lavie.
2012. The CMU-Avenue French-English Transla-
tion System. In Proceedings of the NAACL 2012
Workshop on Statistical Machine Translation.
Mireia Farru?s, Marta R. Costa-jussa?, and Maja
Popovic. 2012. Study and correlation analysis of
linguistic, perceptual and automatic machine trans-
lation evaluations. Journal of the American Society
for Information Science and Technology, 63(1):174?
184, January.
65
Hany Hassan and Arul Menezes. 2013. Social text nor-
malization using contextual graph random walks. In
Proceedings of the 51st Annual Meeting of the As-
sociation for Computational Linguistics, Sofia, Bul-
garia, August. Association for Computational Lin-
guistics.
Xiaodong He, Mei Yang, Jianfeng Gao, Patrick
Nguyen, and Robert Moore. 2008. Indirect-hmm-
based hypothesis alignment for combining outputs
from machine translation systems. In Proceedings
of EMNLP.
Mark Hopkins and Jonathan May. 2011. Tuning as
ranking. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1352?1362, Edinburgh, Scotland, UK.,
July. Association for Computational Linguistics.
Minwoo Jeong, Kristina Toutanova, Hisami Suzuki,
and Chris Quirk. 2010. A discriminative lexicon
model for complex morphology. In The Ninth Con-
ference of the Association for Machine Translation
in the Americas (AMTA-2010).
Kevin Knight. 2013. Tutorial on decipherment. In
ACL 2013, Sofia, Bulgaria, August.
Arul Menezes and Stephen D. Richardson. 2001. A
best-first alignment algorithm for automatic extrac-
tion of transfer mappings from bilingual corpora. In
Association for Computational Linguistics.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proceedings of ACL-08: HLT,
pages 192?199, Columbus, Ohio, June. Association
for Computational Linguistics.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013. Linguistic regularities in continuous space
word representations. In Proceedings of the 2013
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, pages 746?751, Atlanta,
Georgia, June. Association for Computational Lin-
guistics.
Robert C. Moore and William D. Lewis. 2010. Intel-
ligent Selection of Language Model Training Data.
In Proceedings of the ACL 2010 Conference Short
Papers, Uppsala, Sweden, July.
Franz Josef Och and Hermann Ney. 2004. The
Alignment Template Approach to Statistical Ma-
chine Translation. Computational Linguisitics,
30(4):417?449, September.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 41st ACL, Sapporo, Japan.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of the 40th ACL, Philadelphia, PA.
Chris Quirk and Simon Corston-Oliver. 2006. The im-
pact of parse quality on syntactically-informed sta-
tistical machine translation. In Proceedings of the
2006 Conference on Empirical Methods in Natural
Language Processing, pages 62?69, Sydney, Aus-
tralia, July. Association for Computational Linguis-
tics.
Chris Quirk and Arul Menezes. 2006. Dependency
Treelet Translation: The convergence of statistical
and example-based Machine Translation? Machine
Translation, 20:43?65.
Stefan Riezler and John T. Maxwell. 2005. On some
pitfalls in automatic evaluation and significance test-
ing for MT. In Proceedings of the ACL Workshop
on Intrinsic and Extrinsic Evaluation Measures for
Machine Translation and/or Summarization, pages
57?64, Ann Arbor, Michigan, June. Association for
Computational Linguistics.
Wei Wang, Klaus Macherey, Wolfgang Macherey,
Franz Och, and Peng Xu. 2012. Improved do-
main adaptation for statistical machine translation.
In Proceedings of AMTA.
Warren Weaver. 1955. Translation. In William N.
Locke and A. Donald Booth, editors, Machine
Translation of Languages, pages 15?23. MIT Press,
Massachussets.
Hao Zhang, Licheng Fang, Peng Xu, and Xiaoyun Wu.
2011. Binarized forest to string translation. In Pro-
ceedings of the 49th Annual Meeting of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies, pages 835?845, Portland, Ore-
gon, USA, June. Association for Computational Lin-
guistics.
66
