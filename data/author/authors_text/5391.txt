Proceedings of the 2nd Workshop on Building Educational Applications Using NLP,
pages 69?76, Ann Arbor, June 2005. c?Association for Computational Linguistics, 2005
Evaluating State-of-the-Art Treebank-style Parsers for
Coh-Metrix and Other Learning Technology Environments
Christian F. Hempelmann, Vasile Rus, Arthur C. Graesser, and Danielle S. McNamara
Institute for Intelligent Systems
Departments of Computer Science and Psychology
The University of Memphis
Memphis, TN 38120, USA
{chmplmnn, vrus, a-graesser, dsmcnamr}@memphis.edu
Abstract
This paper evaluates a series of freely
available, state-of-the-art parsers on a
standard benchmark as well as with
respect to a set of data relevant for
measuring text cohesion. We outline
advantages and disadvantages of exist-
ing technologies and make recommen-
dations. Our performance report uses
traditional measures based on a gold
standard as well as novel dimensions
for parsing evaluation. To our knowl-
edge this is the first attempt to eval-
uate parsers accross genres and grade
levels for the implementation in learn-
ing technology.
1 Introduction
The task of syntactic parsing is valuable to
most natural language understanding applica-
tions, e.g., anaphora resolution, machine trans-
lation, or question answering. Syntactic parsing
in its most general definition may be viewed as
discovering the underlying syntactic structure of
a sentence. The specificities include the types
of elements and relations that are retrieved by
the parsing process and the way in which they
are represented. For example, Treebank-style
parsers retrieve a bracketed form that encodes
a hierarchical organization (tree) of smaller el-
ements (called phrases), while Grammatical-
Relations(GR)-style parsers explicitly output re-
lations together with elements involved in the
relation (subj(John,walk)).
The present paper presents an evaluation of
parsers for the Coh-Metrix project (Graesser et
al., 2004) at the Institute for Intelligent Systems
of the University of Memphis. Coh-Metrix is a
text-processing tool that provides new methods
of automatically assessing text cohesion, read-
ability, and difficulty. In its present form, v1.1,
few cohesion measures are based on syntactic
information, but its next incarnation, v2.0, will
depend more heavily on hierarchical syntactic
information. We are developing these measures.
Thus, our current goal is to provide the most
reliable parser output available for them, while
still being able to process larger texts in real
time. The usual trade-off between accuracy and
speed has to be taken into account.
In the first part of the evaluation, we adopt
a constituent-based approach for evaluation, as
the output parses are all derived in one way or
another from the same data and generate simi-
lar, bracketed output. The major goal is to con-
sistently evaluate the freely available state-of-
the-art parsers on a standard data set and across
genre on corpora typical for learning technology
environments. We report parsers? competitive-
ness along an array of dimensions including per-
formance, robustness, tagging facility, stability,
and length of input they can handle.
Next, we briefly address particular types of
misparses and mistags in their relation to mea-
sures planned for Coh-Metrix 2.0 and assumed
to be typical for learning technology applica-
tions. Coh-Metrix 2.0 measures that centrally
rely on good parses include:
causal and intentional cohesion, for which the
main verb and its subject must be identified;
69
anaphora resolution, for which the syntactic re-
lations of pronoun and referent must be identi-
fied;
temporal cohesion, for which the main verb and
its tense/aspect must be identified.
These measures require complex algorithms
operating on the cleanest possible sentence
parse, as a faulty parse will lead to a cascad-
ing error effect.
1.1 Parser Types
While the purpose of this work is not to propose
a taxonomy of all available parsers, we consider
it necessary to offer a brief overview of the var-
ious parser dimensions. Parsers can be classi-
fied according to their general approach (hand-
built-grammar-based versus statistical), the way
rules in parses are built (selective vs. genera-
tive), the parsing algorithm they use (LR, chart
parser, etc.), type of grammar (unification-based
grammars, context-free grammars, lexicalized
context-free grammars, etc.), the representation
of the output (bracketed, list of relations, etc.),
and the type of output itself (phrases vs gram-
matical relations). Of particular interest to our
work are Treebank-style parsers, i.e., parsers
producing an output conforming to the Penn
Treebank (PTB) annotation guidelines. The
PTB project defined a tag set and bracketed
form to represent syntactic trees that became a
standard for parsers developed/trained on PTB.
It also produced a treebank, a collection of hand-
annotated texts with syntactic information.
Given the large number of dimensions along
which parsers can be distinguished, an evalua-
tion framework that would provide both parser-
specific (to understand the strength of differ-
ent technologies) and parser-independent (to be
able to compare different parsers) performance
figures is desirable and commonly used in the
literature.
1.2 General Parser Evaluation Methods
Evaluation methods can be broadly divided
into non-corpus- and corpus-based methods
with the latter subdivided into unannotated
and annotated corpus-based methods (Carroll
et al, 1999). The non-corpus method sim-
ply lists linguistic constructions covered by the
parser/grammar. It is well-suited for hand-
built grammars because during the construction
phase the covered cases can be recorded. How-
ever, it has problems with capturing complex-
ities occuring from the interaction of covered
cases.
The most widely used corpus-based eval-
uation methods are: (1) the constituent-
based (phrase structure) method, and (2) the
dependency/GR-based method. The former has
its roots in the Grammar Evaluation Interest
Group (GEIG) scheme (Grishman et al, 1992)
developed to compare parsers with different un-
derlying grammatical formalisms. It promoted
the use of phrase-structure bracketed informa-
tion and defined Precision, Recall, and Cross-
ing Brackets measures. The GEIG measures
were extended later to constituent information
(bracketing information plus label) and have
since become the standard for reporting auto-
mated syntactic parsing performance. Among
the advantages of constituent-based evaluation
are generality (less parser specificity) and fine
grain size of the measures. On the other hand,
the measures of the method are weaker than ex-
act sentence measures (full identity), and it is
not clear if they properly measure how well a
parser identifies the true structure of a sentence.
Many phrase boundary mismatches spawn from
differences between parsers/grammars and cor-
pus annotation schemes (Lin, 1995). Usually,
treebanks are constructed with respect to infor-
mal guidelines. Annotators often interpret them
differently leading to a large number of different
structural configurations.
There are two major approaches to evaluate
parsers using the constituent-based method. On
the one hand, there is the expert-only approach
in which an expert looks at the output of a
parser, counts errors, and reports different mea-
sures. We use a variant of this approach for
the directed parser evaluation (see next section).
Using a gold standard, on the other hand, is a
method that can be automated to a higher de-
gree. It replaces the counting part of the former
method with a software system that compares
the output of the parser to the gold standard,
70
highly accurate data, manually parsed ? or au-
tomatically parsed and manually corrected ? by
human experts. The latter approach is more
useful for scaling up evaluations to large collec-
tions of data while the expert-only approach is
more flexible, allowing for evaluation of parsers
from new perspectives and with a view to spe-
cial applications, e.g., in learning technology en-
vironments.
In the first part of this work we use the gold
standard approach for parser evaluation. The
evaluation is done from two different points of
view. First, we offer a uniform evaluation for the
parsers on section 23 from the Wall Street Jour-
nal (WSJ) section of PTB, the community norm
for reporting parser performance. The goal of
this first evaluation is to offer a good estimation
of the parsers when evaluated in identical en-
vironments (same configuration parameters for
the evaluator software). We also observe the fol-
lowing features which are extremely important
for using the parsers in large-scale text process-
ing and to embed them as components in larger
systems.
Self-tagging: whether or not the parser does tag-
ging itself. It is advantageous to take in raw text
since it eliminates the need for extra modules.
Performance: if the performance is in the mid
and upper 80th percentiles.
Long sentences: the ability of the parser to han-
dle sentences longer than 40 words.
Robustness: relates to the property of a parser
to handle any type of input sentence and return
a reasonable output for it and not an empty line
or some other useless output.
Second, we evaluate the parsers on narrative
and expository texts to study their performance
across the two genres. This second evaluation
step will provide additional important results
for learning technology projects. We use evalb
(http://nlp.cs.nyu.edu/evalb/) to evaluate the
bracketing performance of the output of a parser
against a gold standard. The software evaluator
reports numerous measures of which we only re-
port the two most important: labelled precision
(LR), labelled recall (LR) which are discussed in
more detail below.
1.3 Directed Parser Evaluation Method
For the third step of this evaluation we looked
for specific problems that will affect Coh-Metrix
2.0, and presumably learning technology appli-
cations in general, with a view to amending
them by postprocessing the parser output. The
following four classes of problems in a sentence?s
parse were distinguished:
None: The parse is generally correct, unambigu-
ous, poses no problem for Coh-Metrix 2.0.
One: There was one minor problem, e.g., a mis-
labeled terminal or a wrong scope of an adver-
bial or prepositional phrase (wrong attachment
site) that did not affect the overall parse of the
sentence, which is therefore still usable for Coh-
Metrix 2.0 measures.
Two: There were two or three problems of the
type one, or a problem with the tree structure
that affected the overall parse of the sentence,
but not in a fatal manner, e.g., a wrong phrase
boundary, or a mislabelled higher constituent.
Three: There were two or more problems of the
type two, or two or more of the type one as
well as one or more of the type two, or another
fundamental problem that made the parse of the
sentence completely useless, unintelligible, e.g.,
an omitted sentence or a sentence split into two,
because a sentence boundary was misidentified.
2 Evaluated Parsers
2.1 Apple Pie
Apple Pie (AP) (Sekine and Grishman, 1995)
extracts a grammar from PTB v.2 in which S
and NP are the only true non-terminals (the
others are included into the right-hand side of
S and NP rules). The rules extracted from the
PTB have S or NP on the left-hand side and a
flat structure on the right-hand side, for instance
S ? NP VBX JJ. Each such rule has the most
common structure in the PTB associated with
it, and if the parser uses the rule it will gener-
ate its corresponding structure. The parser is
a chart parser and factors grammar rules with
common prefixes to reduce the number of active
nodes. Although the underlying model of the
parser is simple, it can?t handle sentences over
40 words due to the large variety of linguistic
71
constructs in the PTB.
2.2 Charniak?s Parser
Charniak presents a parser (CP) based on prob-
abilities gathered from the WSJ part of the PTB
(Charniak, 1997). It extracts the grammar and
probabilities and with a standard context-free
chart-parsing mechanism generates a set of pos-
sible parses for each sentence retaining the one
with the highest probability (probabilities are
not computed for all possible parses). The prob-
abilities of an entire tree are computed bottom-
up. In (Charniak, 2000), he proposes a gen-
erative model based on a Markov-grammar. It
uses a standard bottom-up, best-first probabilis-
tic parser to first generate possible parses before
ranking them with a probabilistic model.
2.3 Collins?s (Bikel?s) Parser
Collins?s statistical parser (CBP; (Collins,
1997)), improved by Bikel (Bikel, 2004), is based
on the probabilities between head-words in parse
trees. It explicitly represents the parse proba-
bilities in terms of basic syntactic relationships
of these lexical heads. Collins defines a map-
ping from parse trees to sets of dependencies,
on which he defines his statistical model. A
set of rules defines a head-child for each node
in the tree. The lexical head of the head-
child of each node becomes the lexical head of
the parent node. Associated with each node is
a set of dependencies derived in the following
way. For each non-head child, a dependency is
added to the set where the dependency is identi-
fied by a triplet consisting of the non-head-child
non-terminal, the parent non-terminal, and the
head-child non-terminal. The parser is a CYK-
style dynamic programming chart parser.
2.4 Stanford Parser
The Stanford Parser (SP) is an unlexical-
ized parser that rivals state-of-the-art lexical-
ized ones (Klein and Manning, 2003). It
uses a context-free grammar with state splits.
The parsing algorithm is simpler, the grammar
smaller and fewer parameters are needed for the
estimation. It uses a CKY chart parser which
exhaustively generates all possible parses for a
sentence before it selects the highest probabil-
ity tree. Here we used the default lexicalized
version.
3 Experiments and Results
3.1 Text Corpus
We performed experiments on three data sets.
First, we chose the norm for large scale parser
evaluation, the 2416 sentences of WSJ section
23. Since parsers have different parameters that
can be tuned leading to (slightly) different re-
sults we first report performance values on the
standard data set and then use same parameter
settings on the second data set for more reliable
comparison.
The second experiment is on a set of three nar-
rative and four expository texts. The gold stan-
dard for this second data set was built manually
by the authors starting from CP?s as well as SP?s
output on those texts. The four texts used ini-
tially are two expository and two narrative texts
of reasonable length for detailed evaluation:
The Effects of Heat (SRA Real Science Grade 2
Elementary Science): expository; 52 sentences,
392 words: 7.53 words/sentence;
The Needs of Plants (McGraw-Hill Science):
expository; 46 sentences, 458 words: 9.96
words/sentence;
Orlando (Addison Wesley Phonics Take-Home
Reader Grade 2): narrative; 65 sentences, 446
words: 6.86 words/sentence;
Moving (McGraw-Hill Reading - TerraNova Test
Preparation and Practice - Teachers Edition
Grade 3): narrative, 33 sentences, 433 words:
13.12 words/sentence.
An additional set of three texts was cho-
sen from the Touchstone Applied Science As-
sociates, Inc., (TASA) corpus with an average
sentence length of 13.06 (overall TASA average)
or higher.
Barron17: expository; DRP=75.14 (college
grade); 13 sentences, 288 words: 22.15
words/sentence;
Betty03: narrative; DRP=56.92 (5th grade); 14
sentences, 255 words: 18.21 words/sentence;
Olga91: expository; DRP=74.22 (college grade);
12 sentences, 311 words: 25.92 words/sentence.
72
We also tested all four parsers for speed on a
corpus of four texts chosen randomly from the
Metametrix corpus of school text books, across
high and low grade levels and across narrative
and science texts (see Section 3.2.2).
G4: 4th grade narrative text, 1,500 sentences,
18,835 words: 12.56 words/sentence;
G6: 6th grade science text, 1,500 sentences,
18,237 words: 12.16 words/sentence;
G11: 11th grade narrative text, 1,558 sentences,
18,583 words: 11.93 words/sentence;
G12: 12th grade science text, 1,520 sentences,
25,098 words: 16.51 words/sentence.
3.2 General Parser Evaluation Results
3.2.1 Accuracy
The parameters file we used for evalb was
the standard one that comes with the package.
Some parsers are not robust, meaning that for
some input they do not output anything, leading
to empty lines that are not handled by the evalu-
ator. Those parses had to be ?aligned? with the
gold standard files so that empty lines are elim-
inated from the output file together with their
peers in the corresponding gold standard files.
In Table 1 we report the performance values
on Section 23 of WSJ. Table 2 shows the results
for our own corpus. The table gives the average
values of two test runs, one against the SP-based
gold standard, the other against the CP-based
gold standard, to counterbalance the bias of the
standards. Note that CP and SP possibly still
score high because of this bias. However, CBP
is clearly a contender despite the bias, while AP
is not.1 The reported metrics are Labelled Pre-
cision (LP) and Labelled Recall (LR). Let us de-
note by a the number of correct phrases in the
output from a parser for a sentence, by b the
number of incorrect phrases in the output and
by c the number of phrases in the gold standard
for the same sentence. LP is defined as a/(a+b)
and LR is defined as a/c. A summary of the
other dimensions of the evaluation is offered in
Table 3. A stability dimension is not reported
1AP?s performance is reported for sentences < 40
words in length, 2,250 out of 2,416. SP is also not ro-
bust enough and the performance reported is only on
2,094 out of 2,416 sentences in section 23 of WSJ.
because we were not able to find a bullet-proof
parser so far, but we must recognize that some
parsers are significantly more stable than oth-
ers, namely CP and CBP. In terms of resources
needed, the parsers are comparable, except for
AP which uses less memory and processing time.
The LP/LR of AP is significantly lower, partly
due to its outputting partial trees for longer sen-
tences. Overall, CP offers the best performance.
Note in Table 1 that CP?s tagging accuracy is
worst among the three top parsers but still de-
livers best overall parsing results. This means
that its parsing-only performance is slighstly
better than the numbers in the table indicate.
The numbers actually represent the tagging and
parsing accuracy of the tested parsing systems.
Nevertheless, this is what we would most likely
want to know since one would prefer to input
raw text as opposed to tagged text. If more
finely grained comparisons of only the parsing
aspects of the parsers are required, perfect tags
extracted from PTB must be provided to mea-
sure performance.
Table 4 shows average measures for each of
the parsers on the PTB and seven expository
and narrative texts in the second column and
for expository and narrative in the fourth col-
umn. The third and fifth columns contain stan-
dard deviations for the previous columns, re-
spectively. Here too, CP shows the best result.
3.2.2 Speed
All parsers ran on the same Linux Debian ma-
chine: P4 at 3.4GHz with 1.0GB of RAM.2 AP?s
and SP?s high speeds can be explained to a large
degree by their skipping longer sentences, the
very ones that lead to the longer times for the
other two candidates. Taking this into account,
SP is clearly the fastest, but the large range of
processing times need to be heeded.
3.3 Directed Parser Evaluation Results
This section reports the results of expert rating
of texts for specific problems (see Section 1.3).
The best results are produced by CP with an av-
erage of 88.69% output useable for Coh-Metrix
2.0 (Table 6). CP also produces good output
2Some of the parsers also run under Windows.
73
Table 1: Accuracy of Parsers.
Parser Performance(LP/LR/Tagging - %)
WSJ 23 Expository Narrative
Applie Pie 43.71/44.29/90.26 41.63/42.70 42.84/43.84
Charniak?s 84.35/88.28/92.58 91.91/93.94 93.74/96.18
Collins/Bikel?s 84.97/87.30/93.24 82.08/85.35 67.75/85.19
Stanford 84.41/87.00/95.05 75.38/85.12 62.65/87.56
Table 2: Performance of parsers on the narrative and expository text (average against CP-based
and SP-based gold standard).
File Performance (LR/LP - %)
AP CP CBP SP
Heat 48.25/47.59 91.96/93.77 92.47/94.14 92.44/91.85
Plants 41.85/45.89 85.34/88.02 78.24/88.45 81.00/85.62
Orlando 45.82/49.03 85.83/91.88 65.87/93.97 57.75/90.72
Moving 37.77/41.45 88.93/92.74 53.94/91.68 76.56/84.97
Barron17 43.22/42.95 89.74/91.32 80.49/89.32 87.22/86.31
Betty03 46.53/44.67 90.77/90.74 87.95/85.21 74.53/80.91
Olga91 32.29/32.69 77.65/80.04 61.61/75.43 61.65/70.60
Table 3: Evaluation of Parsers with Respect to the Criteria Listed at the Top of Each Column.
Parser Self-tagging Performance Long-sentences Robustness
AP Yes No No No
CP Yes Yes Yes Yes
CBP Yes Yes Yes Yes
SP Yes Yes No No
Table 4: Average Performance of Parsers.
Parser Ave. (LR/LP - %) S.D. (%) Ave. on S.D. on
Exp+Nar (LR/LP - %) Exp+Nar (%)
AP 42.73/43.61 1.04/0.82 42.24/43.46 5.59/5.41
CP 90.00/92.80 4.98/4.07 87.17/89.79 4.85/4.66
CBP 78.27/85.95 9.22/1.17 74.36/88.31 14.24/6.51
SP 74.14/86.56 10.93/1.28 75.88/84.42 12.66/7.11
74
Table 5: Parser Speed in Seconds.
G4 G6 G11 G12
#sent 619 3336 4976 2215
AP 144 89 144 242
CP 647 499 784 1406
CBP 485 1947 1418 1126
SP 449 391 724 651
Ave. 431 732 768 856
most consistently at a standard deviation over
the seven texts of 8.86%. The other three candi-
dates are clearly trailing behing, namely by be-
tween 5% (SP) and 11% (AP). The distribution
of severe problems is comparable for all parsers.
Table 6: Average Performance of Parsers over
all Texts (Directed Evaluation).
Ave. (%) S.D. (%)
AP 77.31 15.00
CP 88.69 8.86
CBP 79.82 18.94
SP 83.43 11.42
As expected, longer sentences are more prob-
lematic for all parsers, as can be seen in Ta-
ble 7. No significant trends in performance dif-
ferences with respect to genre difference, narra-
tive (Orlando, Moving, Betty03) vs. expository
texts (Heat, Plants, Barron17, Olga91), were de-
tected (cf. also speed results in Table 5). But
we assume that the difference in average sen-
tence length obscures any genre differences in
our small sample.
The most common non-fatal problems (type
one) involved the well-documented adjunct at-
tachment site issue, in particular for preposi-
tional phrases ((Abney et al, 1999), (Brill and
Resnik, 1994), (Collins and Brooks, 1995)) as
well as adjectival phrases (Table 8)3. Similar
misattachment issues for adjuncts are encoun-
tered with adverbial phrases, but they were rare
3PP = wrong attachment site for a prepositional
phrase; ADV = wrong attachment site for an adverbial
phrase; cNP = misparsed complex noun phrase; &X =
wrong coordination
Table 7: Correlation of Average Performance
per Text for all Parsers and Average Sentence
Length (Directed Evaluation).
Text perf. (%) length (#words)
Heat 92.31 7.54
Plants 90.76 9.96
Orlando 93.46 6.86
Moving 90.91 13.12
Barron17 76.92 22.15
Betty03 71.43 18.21
Olga91 60.42 25.92
in our corpus.
Another common problem are deverbal nouns
and denominal verbs, as well as -ing/VBG
forms. They share surface forms leading to am-
biguous part of speech assignments. For many
Coh-Metrix 2.0 measures, most obviously tem-
poral cohesion, it is necessary to be able to dis-
tinguish gerunds from gerundives and deverbal
adjectives and deverbal nouns.
Table 8: Specific Problems by Parser.
PP ADV cNP &X
AP 13 10 8 9
CP 15 1 2 7
CBP 10 0 0 13
SP 22 6 3 4
Sum 60 17 13 33
Problems with NP misidentification are par-
ticularly detrimental in view of the impor-
tant role of NPs in Coh-Metrix 2.0 mea-
sures. This pertains in particular to the mistag-
ging/misparsing of complex NPs and the coor-
dination of NPs. Parses with fatal problems
are expected to produce useless results for algo-
rithms operating with them. Wrong coordina-
tion is another notorious problem of parsers (cf.
(Cremers, 1993), (Grootveld, 1994)). In our cor-
pus we found 33 instances of miscoordination,
of which 23 involved NPs. Postprocessing ap-
proaches that address these issues are currently
under investigation.
75
4 Conclusion
The paper presented the evaluation of freely
available, Treebank-style, parsers. We offered
a uniform evaluation for four parsers: Apple
Pie, Charniak?s, Collins/Bikel?s, and the Stan-
ford parser. A novelty of this work is the evalua-
tion of the parsers along new dimensions such as
stability and robustness and across genre, in par-
ticular narrative and expository. For the latter
part we developed a gold standard for narrative
and expository texts from the TASA corpus. No
significant effect, not already captured by vari-
ation in sentence length, could be found here.
Another novelty is the evaluation of the parsers
with respect to particular error types that are
anticipated to be problematic for a given use of
the resulting parses. The reader is invited to
have a closer look at the figures our tables pro-
vide. We lack the space in the present paper to
discuss them in more detail. Overall, Charniak?s
parser emerged as the most succesful candidate
of a parser to be integrated where learning tech-
nology requires syntactic information from real
text in real time.
ACKNOWLEDGEMENTS
This research was funded by Institute for Educa-
tions Science Grant IES R3056020018-02. Any
opinions, findings, and conclusions or recom-
mendations expressed in this article are those
of the authors and do not necessarily reflect the
views of the IES. We are grateful to Philip M.
McCarthy for his assistance in preparing some
of our data.
References
S. Abney, R. E. Schapire, and Y. Singer. 1999.
Boosting applied to tagging and pp attachment.
Proceedings of the 1999 Joint SIGDAT Confer-
ence on Empirical Methods in Natural Language
Processing and Very Large Corpora, pages 38?45.
D. M. Bikel. 2004. Intricacies of collins? parsing
model. Computational Linguistics, 30-4:479?511.
E. Brill and P. Resnik. 1994. A rule-based approach
to prepositional phrase attachment disambigua-
tion. In Proceedings of the 15th International Con-
ference on Computational Linguistics.
J. Carroll, E. Briscoe, and A. Sanfilippo, 1999.
Parser evaluation: current practice, pages 140?
150. EC DG-XIII LRE EAGLES Document EAG-
II-EWG-PR.1.
E. Charniak. 1997. Statistical parsing with a
context-free grammar and word statistics. Pro-
ceedings of the Fourteenth National Conference
on Artificial Intelligence, AAAI Press/MIT Press,
Menlo Park.
E. Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of the North-American
Chapter of Association for Computational Lin-
guistics, Seattle, Washington.
M. Collins and J. Brooks. 1995. Prepositional phrase
attachment through a backed-off model. In Pro-
ceedings of the Third Workshop on Very Large
Corpora, Cambridge.
M. Collins. 1997. Three generative, lexicalised mod-
els for statistical parsing. In Proceedings of the
35th Annual Meeting of the Association for Com-
putational Linguistic, Madrid, Spain.
C. Cremers. 1993. On Parsing Coordination Cate-
gorially. Ph.D. thesis, Leiden University.
A. C. Graesser, D.S. McNamara, M. M. Louwerse,
and Z. Cai. 2004. Coh-metrix: Analysis of text on
cohesion and language. Behavior Research Meth-
ods, Instruments, and Computers, 36-2:193?202.
R. Grishman, C. MacLeod, and J. . Sterling. 1992.
Evaluating parsing strategies using standardized
parse files. In Proceedings of the Third Conference
on Applied Natural Language Processing, pages
156?161.
M. Grootveld. 1994. Parsing Coordination Genera-
tively. Ph.D. thesis, Leiden University.
D. Klein and C. Manning. 2003. Accurate unlexi-
calized parsing. In Proceedings of the 41st Annual
Meeting of the Association for Computational Lin-
guistic, Sapporo, Japan.
D. Lin. 1995. A dependency-based method for eval-
uating broad-coverage parsers. Proceedings of In-
ternational Joint Conference on Artificial Intelli-
gence, pages 1420?1427.
A. Ratnaparkhi, J. Renyar, and S. Roukos. 1994. A
maximum entropy model for prepositional phrase
attachment. In Proceedings of the ARPA Work-
shop on Human Language Technology.
S. Sekine and R. Grishman. 1995. A corpus-
based probabilistic grammar with only two non-
terminals. Proceedings of the International Work-
shop on Parsing Technologies, pages 216?223.
76
Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 242?250,
Atlanta, Georgia, June 13 2013. c?2013 Association for Computational Linguistics
Native Language Identification: A Key N-gram Category Approach   Kristopher Kyle, Scott Crossley Jianmin Dai, Danielle S. McNamara Georgia State University Arizona State University 34 Peachtree Ave, Ste 1200 PO Box 872111 Atlanta, GA 30303 Tempe, AZ 85287 kkyle3@student.gsu.edu, scrossley@gsu.edu Jianmin.Dai@asu.edu,  dsmcnamara1@gmail.com        Abstract 
This study explores the efficacy of an ap-proach to native language identification that utilizes grammatical, rhetorical, semantic, syntactic, and cohesive function categories comprised of key n-grams. The study found that a model based on these categories of key n-grams was able to successfully predict the L1 of essays written in English by L2 learners from 11 different L1 backgrounds with an ac-curacy of 59%. Preliminary findings concern-ing instances of crosslinguistic influence are discussed, along with evidence of language similarities based on patterns of language misclassification. 
1. Introduction Native language identification (NLI) is generally an automated task that can be used in authorship profiling (Wong & Dras, 2009) and in assisting automatic writing evaluation systems provide fo-cused feedback (e.g., Rozovskaya & Roth, 2011). NLI is achieved by identifying patterns of lan-guage use that are common to a group of users of a particular second language (L2; e.g., English) that share a native language (L1). Useful to the discus-sion of these patterns is the concept of crosslin-guistic influence (CLI), which references ?the consequences - both direct and indirect - that being a speaker of a particular native language (L1) has on the person?s use of a later learned language (Jarvis, 2012, p.1). Beyond its theoretical applica-
tions, CLI can also be used to inform L2 classroom pedagogy (Granger, 2009; Laufer & Girsai, 2008). NLI studies, then, are informed by and can inform CLI, and have diverse applications. The current study seeks to add to the discus-sions of NLI and CLI by testing the efficacy of a new approach ? the use of grammatical, rhetorical, semantic, syntactic, and cohesive function catego-ries of key n-grams.  2. Background In this section we outline two approaches to CLI, provide a selected review of relevant literature, and address gaps in the current body of NLI research. 2.1 Approaches to CLI  Jarvis (2000, 2010, 2012) has outlined two ap-proaches to the investigation of CLI: a compari-son-based and a detection-based approach. The comparison-based approach is generally con-structed based on specific observed difference be-tween language systems (e.g., article usage in English as compared to article usage in Korean). Whether or not these L1 differences affect L2 pro-duction is then analyzed by examining example texts (e.g., inappropriate use of articles by native speakers of Korean writing in English as an L2). The detection-based argument, on the other hand, is built with the opposite trajectory. Instead of be-ginning with hypotheses based on differences in language systems, researchers begin by identifying patterns of language use (e.g., inappropriate article use) that occur regularly by members of an L1 that 
242
use a particular L2 (intragroup homogeneity) but do not occur regularly by other L1 users of the same L2 (intergroup heterogeneity). These patterns of use are then verified through statistical and ma-chine learning techniques that use these patterns to predict the L1 group membership of L2 texts (i.e., NLI).  Recent advances in corpus development and natural language processing allow for larger num-bers of texts to be searched using a greater number of linguistic features. These features can then be used to create an NLI predictor model. A success-ful model not only fulfills the NLI task, but pro-vides further evidence that the observed patterns of language use can be attributable to CLI. While Type I errors are certainly a potential issue in this argument, Jarvis (2012) explains that false posi-tives can be mitigated by balancing or controlling for potentially confounding variables (e.g., profi-ciency levels and essay prompts) during the con-struction of the target corpus. 2.2 Selected literature review A limited but growing number of studies have in-vestigated CLI using the detection-based approach, many of which are included in a volume edited by Jarvis and Crossley (2012). Researchers have ex-plored the topic of CLI in the areas of lexical style (Jarvis et al, 2012a), lexical n-grams (Jarvis & Paquot, 2012), character n-grams (Tsur & Rappo-prot, 2007), using variables related to cohesion, lexical sophistication, syntactic complexity and conceptual knowledge (Crossley & McNamara, 2012), error patterns (Bestgen, et al, 2012; Wong & Dras, 2009), and a combination of these ap-proaches (Jarvis et al, 2012b; Koppel et al, 2005; Mayfield Tomokiyo & Jones, 2001, Wong & Dras, 2009).  Such studies have demonstrated relatively strong success rates for classifying an L2 writing sample based on the L1 of the writer. For instance, Jarvis and Paquot (2012), using 1-4-grams as pre-dictor variables on a subset of argumentative es-says included in the International Corpus of Learner English (ICLE) (Granger et al, 2009) achieved a 53.6% classification accuracy for 12 groups of L1s. Crossley and McNamara (2012) used features related to cohesion, lexical sophisti-cation, syntactic complexity, and conceptual knowledge taken from the computational tool Coh-
Metrix (Graesser et al, 2004) to classify essays written in English by Czech, Finnish, German, and Spanish participants and achieved an L1 classifica-tion accuracy of 65-67.6%. Using error types, Bestgen et al (2012), on 223 ICLE essays written by French, German, and Spanish L1 participants, achieved a classification accuracy of 65%. In a follow-up study, Jarvis et al (2012b) explored the relative efficacy of these three CLI methods (n-grams, Coh-Metrix indices, and error types) using the corpus found in Bestgen et al (2012). When all three approaches were used in the classification task, the accuracy increased to 79%.  2.3 Weakness of extant research in CLI Although the studies discussed so far have pro-duced statistical models that can predict the L1 group of a text written in L2 English with accura-cies well above chance, the degree to which these studies have demonstrated instances of CLI may be questionable as they draw on the ICLE corpus, which is arguably imbalanced (Jarvis et al, 2012a, and Mayfield Tomokiyo, & Jones, 2001 being the exceptions). While ICLE was designed with an attempt to control for a number of variables, the proficiency levels vary across language groups (as suggested by Koppel et al, 2005, and empirically confirmed by Bestgen et al, 2012) and though the argumentative texts are limited to a particular set of prompts within the corpus, these prompts are not equally distributed across language groups, raising the question of the degree to which the ob-served differences in texts were due to CLI, profi-ciency level, or essay prompt.  In addition, many of the linguistic features previously investigated did not lend themselves to providing strong links between observed differ-ences and CLI (e.g., the word concreteness and word frequency variables investigated in Crossley & McNamara, 2012). A potentially promising method that has not been applied to detection-based CLI studies that may address these limita-tions is the use of rhetorical, syntactic, grammati-cal and cohesive categories comprised of key n-grams. Such features have recently been investi-gated by Crossley, Defore, Kyle, Dai, and McNa-mara (submitted for publication), in which they explored their usefulness for assessing the efficacy of an automatic writing evaluation (AWE) system. In this study, Crossley et al separated a corpus of 
243
essays into introduction, body, and conclusion paragraphs, and then further separated these into high and low proficiency categories based on over-all essay score. They then identified n-grams that occurred significantly more often (positive keyness values) in paragraphs of a certain type (e.g., intro-duction) from high scoring essays than the same type of paragraphs from low-scoring essays. Addi-tionally, they identified n-grams that occurred sig-nificantly less often (negative keyness values) in high-scoring paragraphs of a certain type than low-scoring paragraphs of the same type. Positively and negatively key n-grams for each paragraph type were then separated into categories based on their rhetorical, syntactic, grammatical, and cohesive features. These categories were then successfully used as variables in a multiple regression to create a model that accounted for between 24%-33% of the variance in essay scores. This study demon-strates the efficacy of using grammatical, rhetori-cal, syntactic, and cohesive function categories of key n-grams to identify instances of linguistic variation that successfully predict essay quality. These findings hold promise for the use of similar methods to contribute to the study of CLI by iden-tifying linguistic variation across different L1 groups writing in the same L2. 2.4 Goals of the current study The current study, while drawing on previous re-search (notably Jarvis & Paquot, 2012 and Crossley et al, submitted for publication), contrib-utes to the detection-based CLI discussion by: a) examining a prompt and proficiency-controlled corpus and, b) using n-gram indices related to grammatical, rhetorical, semantic, syntactic, and cohesive functions to assess difference in L2 es-says based on the L1 of the writers. This study is guided by the following research questions:  1. Can a model consisting of functional categorical n-grams predict the native language of an L2 writer of English?  2. Does the resulting model inform theories of CLI? 
3. Method In this section, we describe the corpus used for our training and test set, the methods used for key n-gram identification, and the grouping of these n-grams into grammatical, rhetorical, semantic, syn-tactic, and cohesive categories. 3.1 Corpus For this project we used an 11,000 essay subset of the 12,100 essay TOEFL11 corpus (Blanchard, Tetreault, Higgins, Cahill, & Chodorow, 2013). The TOEFL11 corpus is comprised of independent task essays written during administrations of the Test of English as a Foreign Language (TOEFL) between 2006-2007 (Blanchard et al, 2013). The corpus is balanced across 11 native language (L1) groups, includes responses to eight different inde-pendent-task prompts, and includes essays written by low, medium, and high proficiency writers. The languages represented include Arabic, Chinese, French, German, Hindi, Italian, Japanese, Korean, Spanish, Telugu, and Turkish. Following the pro-cedures of the NLI shared task (Tetreault, Blanch-ard, & Cahill, 2013), 1,100 of the original 11,000 essays were set aside as the test set, leaving a train-ing corpus of 9,900 essays. 3.2 Identifying key n-grams In this study, we considered n-grams from 1-10 words in length. N-grams were considered to be key if they occur in a corpus significantly more or less frequently than in a reference corpus. We identified key n-grams using the KeyWords func-tion of Wordsmith Tools 6 (Scott, 2013) and the default log likelihood method of identifying key n-grams (McEnery & Hardie, 2012). To ensure that the keyness of a particular n-gram was representa-tive of use across a particular L1 group and not due to prolific use by a small number of individuals, we set the minimum threshold for inclusion at a range of 10 percent (n-grams had to occur in at least 10 percent of the texts written by a particular L1 group). Using these parameters, we conducted keyness tests for each language group. To create the key n-gram list for the Arabic group, for exam-ple, we compared the frequency of n-grams in the Arabic group to the frequency of n-grams in all of the other language groups combined. This process  
244
was completed for each language group until a key n-gram list existed for each. Because one of the goals of our study was to generalize instances of CLI to essays written on prompts other than those included in the TOEFL11 Corpus, it was important to remove all prompt-based words from our key n-gram lists. Removing all words occurring in the prompts from the n-grams list would remove a number of high fre-quency words that may not be prompt-based (e.g., the, to), so prompt-based words were operationally defined as content words and their lemmas in-cluded in the prompt that had a Kucera and Francis (1967) written frequency value of 715 or less. N-grams were removed from potential predictor sets if they contained any of these prompt-based words. The remaining key n-grams for each language group were then sorted by absolute keyness in each group and filtered for redundancy. For example, prior to this stage, the Chinese key n-gram list in-cluded both more and have more. Because more had a higher absolute keyness value than have more, have more was removed from the Chinese key n-gram list.  Table 1 provides a summary of the length of key n-grams identified in each stage of the selec-tion process. Although n-grams from 1-10 words in length were initially considered, no n-grams longer than 5-grams were identified as being key. Addi-tionally, all 5-grams, such as the key Chinese n-gram ?group led by a tour?, and the Telugu n-gram ?agree with the statement that? contained prompt-based words and were removed from further con-sideration. After the final n-gram refining step, the longest n-gram was a single 4-gram, the Turkish n-gram ?on the other hand?. 
3.3 Grouping of key n-grams into indices The last stage in our variable selection process was to group the key n-grams in each language group into categories. First, two indices for each lan-guage group were created. The first included all n-grams with positive keyness values that remained after the filtering process described above. The second included all of the n-grams with negative keyness values after filtering. Next, positive and negative n-grams were sorted into grammatical, rhetorical, semantic, syntactic, and cohesive func-tion categories by two trained linguists with expe-rience in the area of second language writing. The purpose of sorting n-grams in this manner was to identify patterns of relative over/underuse by each language group. See Table 2 for a list of all of the indices created during this process. 3.4 Evaluation of model In CLI studies and other studies that attempt to predict the group membership of a text, discrimi-nant function analysis (DFA) is often used (Jarvis & Paquot, 2012; Crossley & McNamara, 2012). Although other methods can be used, such as sup-port vector machine decision trees (e.g., Koppel et al, 2005) or Na?ve Bayes (e.g., Mayfield To-mokiyo & Jones, 2001), DFA has the advantage of being the most transparent of these with regard to interpreting results (Jarvis, 2012). DFA was there-fore chosen as the method of analysis for this study, using L1 as the dependent variable and n-gram indices as independent variables. The first step in the analysis was to check the independent variables for multicollinearity using a Pearson correlation matrix. Any two variables above a threshold of p>.899 were flagged for fur-ther analysis. A MANOVA was then conducted using the languages from one proficiency group as independent variables and the predictor indices/n-grams as dependent variables. The effect sizes pro-duced by the MANOVA were used to select which variables flagged in the correlation matrix would be retained, and which would be eliminated. Within each highly correlated pair, the variable with the largest effect size was kept. Finally, a DFA was conducted on the training set. The pre-dictor model sets identified in the DFA were then  
N-gram Length Original No Prompt Words After Final Sort 5 5 0 0 4 19 3 1 3 110 54 8 2 699 512 147 1 1100 877 770 Total 1933 1446 926     Table 1: Length of key n-grams. 
245
   
  L1 Index Coverage  Variable Category - + Total Examples ALL  11 11 22 see below Adjectives Syntactic 0 1 1 little, kind, real Adverbs Syntactic 0 2 4 always, easily just, still Articles Cohesion 8 8 16 a, an, the Auxilliary Verbs Syntactic 2 0 2 has, have, will Certainty Semantic 0 1 1 necessary, sure, true Cognition Semantic 0 1 1 experience, thought Comparatives Rhetorical 0 1 1 easier, much more Conjunctions Cohesion 6 5 11 and, because, or Connectives Cohesion 1 2 3 and to, and that, also Determiners Cohesion 1 0 1 that, this Evaluation Semantic 0 1 1 good, fun, like to Examples Semantic 0 1 1 particular, etc Explanation Semantic 0 4 4 explain, in order to, that is Go Semantic 0 1 1 are going, go, going to Irrealis Grammatical 0 1 1 what, will Modality Rhetorical 9 9 18 we can, could, can be Negation Syntactic 3 8 11 but not, no Nouns Syntactic 3 7 10 country, person, places Options Rhetorical 0 1 1 consider, different, instead People Semantic 1 4 5 people, society, friends Place Semantic 0 1 1 city, place, places Possession Semantic 1 1 2 his, having, your Possibility Rhetorical 0 3 3 probably, maybe, possible Pre-infinitive Syntactic 0 1 1 how to, time to, way to Prepositions Grammatical 10 9 19 from, about, with a Problems Semantic 1 1 2 problem, problems Pronouns Cohesion 10 11 21 he, his, your Quantity Semantic 11 11 22 every, more than, some Questions Syntactic 7 6 13 where, who, why, question Science/ Tech-nology Semantic 0 2 2 computer, internet Signifying Rhetorical 0 1 1 see, mean Specificity Rhetorical 0 3 3 certain, especially, special Stance Rhetorical 2 6 8 feel that, in my, opinion Temporality Semantic 6 7 13 during, more and more, often  To Be Syntactic 6 8 14 are, been, it is Transitions Cohesion 4 9 13 but, however, therefore Vagueness Semantic 0 1 1 general, someone, something Verbs Syntactic 5 8 13 choose, make, play Work/Study Semantic 2 7 9 money, study, parents Total  110 167 277        Table 2: Negative and positive key n-gram variables. 
246
used on the essays in the test set to determine whether the model sets could generalize to a new population.  
4. Results The training set DFA predicted L1 group member-ship of TOEFL independent essays with an accu-racy of 60% using 184 indices (df= 100, n= 9900, ?2= 32997.259, p< .001), which is significantly higher than the baseline chance of 9%. The re-ported Kappa = .560, indicates a moderate rela-tionship between actual and predicted L1.  The predictive accuracy of the model was veri-fied on the test set, in which L1 group membership was predicted with an accuracy of 59% (df= 100, n= 1100, ?2= 3550.791, p< .001). The reported Kappa = .549, indicates a moderate agreement be-tween the actual and predicted L1. Table 3 in-cludes the test set confusion matrix.  5. Discussion  The results of this study suggest the usefulness of key n-grams grouped into categories based on their grammatical, rhetorical, semantic, syntactic, and cohesive features for NLI. The results demonstrate that such indices can correctly classify 59% of es-says written in English as belonging to 1 of 11 L1 populations. In addition, with regard to n-gram length, we found that although n-grams 1-10 words in length were initially considered, no n-grams longer than 
5-grams were identified as key, and the longest n-gram that remained after removing prompt-based and redundancy was a single 4-gram. This suggests that 4-grams (or possibly even 3-grams) may be a useful threshold for future investigations. 5.1 Preliminary CLI findings As Jarvis (2012) notes, CLI studies that use the detection-based argument to CLI are exploratory in nature, while studies that use the comparison-based argument are confirmatory in nature. The present study is, thus, exploratory in nature, and without substantial further investigation, we cannot defini-tively posit whether observed differences and simi-larities in English use can be attributed to the influence of the L1 itself or to cultural or educa-tional norms. Nonetheless, a few preliminary observations are worthy of discussion. First, we identified a number of patterns of language use that may be attributable to CLI. Although a full discussion of these is beyond the scope of this paper, Table 4 includes examples of potential CLI features in ref-erence to the German writers represented in the corpus. The table demonstrates the particular n-grams that German writers are likely to use more or less often than writers of the other 10 languages. German writers, for example, are more likely to use the phrasal modals able to, have to, has to, and singular modals might and would more often than writers of the other language groups, but are less likely to use the modals can and may. These find-ings are preliminary, and further research that links 
  ARA
 CHI FRE GER HIN ITA JPN KOR SPA TEL TUR Precision Recall F-measure ARA 66 0 5 3 1 3 2 4 8 1 7 53.2% 66.0% 58.9% CHI 3 63 5 3 2 0 6 9 0 3 6 57.8% 63.0% 60.3% FRE 3 4 64 7 3 6 2 1 6 0 4 64.6% 64.0% 64.3% GER 2 5 5 64 3 5 2 4 6 0 4 62.7% 64.0% 63.4% HIN 4 5 0 7 54 1 0 1 6 17 5 56.8% 54.0% 55.4% ITA 4 1 9 10 1 64 2 1 6 0 2 68.8% 64.0% 66.3% JPN 6 7 1 1 0 1 64 9 2 1 8 61.5% 64.0% 62.7% KOR 5 9 2 1 2 0 19 56 2 0 4 57.7% 56.0% 56.9% SPA 14 6 6 3 4 9 2 3 43 2 8 47.3% 43.0% 45.0% TEL 5 3 0 1 22 1 1 1 4 60 2 70.6% 60.0% 64.9% TUR 12 6 2 2 3 3 4 8 8 1 51 50.5% 51.0% 50.7%                Table 3: Test set confusion matrix. 
247
these English n-grams with patterns of use in Ger-man is needed. Additionally, our findings provide some evi-dence for close relationships between languages. For example, when checking for multicollinearity, 
we found that the All Negative Japanese and All Negative Korean categories were very strongly correlated (r =.946, p< .001). Upon further exami-nation, 8 of the 19 n-grams (42%) in the All Nega-tive Japanese category occurred in the corresponding Korean category. The overlapping n-grams were the n-grams all, any, but, different, or, person, this, and your, which may indicate 
similarities between these language systems in that speakers from both language avoid the use of these words. Patterns of essay categorization also provide preliminary insights into language similarities. Based on the test set confusion matrix (see Table 3), a few conflicting patterns emerged. Among the Indo-European languages represented, the Ro-mance (French, Italian, and Spanish) and Germanic (German) languages were regularly miscategorized as one another. Italian essays, for example, were predicted to be French, German, and Spanish 9%, 10%, and 6% of the time, respectively, but were predicted to be other languages only 0%-4% of the time. This seems to confirm generally accepted language taxonomies, though Spanish was pre-dicted to be Arabic (14%) and Turkish (8%) more often than Italian (6%) or French (6%) (as com-pared to 3% for German, and no more than 4% for other languages).  While similarities between language families seem to support extant language taxonomies (see Blanchard et al, 2013) and lend credence to claims of CLI, other observations may cast doubt on these. Hindi (an Indo-Iranian member of the Indo-European family) essays were predicted to be Telugu (Dravidian) essays 17% of the time, and Telugu essays were predicted to be Hindi essays 22% of the time. This may indicate instances of cultural proximity or educational similarities as opposed to linguistic transfer (and/or borrowing) because these languages are both spoken within India. Further investigations of these issues are clearly needed. 5.2 Limitations While we have confidence in our findings, there are limitations to the analysis that need to be dis-cussed. The TOEFL11 corpus was designed to be comparable across languages. While it largely ac-complishes this goal, it is not well balanced across proficiency levels (which may reflect the relative proficiency levels of TOEFL test-takers). Although medium and high proficiency levels are well (though not equally) represented, the low profi-ciency group represents only 11% of the number of texts and an estimated 7.2% of total words (based on mean lengths of essays from each proficiency level given in Blanchard et al, 2013). The medium proficiency group represented 54.4% of the texts 
Variable Positive Negative Adverbs just, only, there, nec-essary  Compara-tives easier, much more  Conjunc-tions or, but, as well  Modals able to, have to, has to, might, would can, may Nouns development, job, topic, something person, place Preposi-tions at, on about, by 
Pronouns everybody, this, you, your 
she, its, I his, us, he, we, they, our Quantity (and ex-ample) another, amount of, both, less, lot, whole any, many, some, such Specific certain, especially, special  Stance in my, of course, opinion, point  
Tempo-rality often, still 
day, now, second, then, time to, second To Be be able, it is, to be was Transi-tions furthermore, one hand, other hand  Verbs look, to get, work go, going, study    Table 4: German predictor variables. 
248
and an estimated 52.8% of words in the corpus, and the high proficiency group comprised the re-maining 34.7% of the texts and an estimated 40% of the words. This indicates that caution should be used when generalizing any CLI findings from this study to low proficiency language users. Further-more, any CLI findings will be biased towards me-dium proficiency language users. Another limitation that may have affected the accuracy of the model was the way in which poten-tial predictor variables were refined. For each lan-guage, the absolute keyness values were used when refining the lists of potential n-gram predictors (as discussed in Section 3.2). After the data had been processed, we discovered that this process re-moved some n-grams that should have remained. In a very few instances redundant n-grams (e.g., have; have more) had a positive keyness value for one n-gram (have) and a negative keyness value for the other (have more). Because all n-grams were later grouped into categories based on posi-tive and negative keyness values, both have and have more should have been retained (as they would not have occurred in any of the same cate-gories). In future studies, positive and negative n-grams will be kept separate during the elimination of redundant n-grams. Another limitation that was discovered after the data analysis was that a data input error caused All Negative Chinese n-gram category to be com-bined with two n-grams included in the Positive Chinese School and Home category. A similar er-ror retained two positive German adverb categories (with one overlapping n-gram, just). The models described in this study retained these variables, as they were not highly correlated with each other or any other variable (based on the r > .899 thresh-old), so any CLI findings based solely on these variables should be considered with caution. 5.3 Future research Although it is clear that categorical n-grams can be used as successful NLI predictor variables, it is unclear whether this approach is more or less ef-fective than the use of raw counts of frequent words or n-grams (e.g., Jarvis et al, 2012a; Jarvis & Paquot, 2012). Future research should explore the relative effectiveness of these methods using the TOEFL11 corpus to determine whether the 
time involved to create key n-gram lists and then sort those lists into categories is warranted. Finally, another remaining question is whether the key n-grams identified in this study are due to linguistic factors or, alternatively, other influences such as culture and educational materials. Acknowledgements We thank ETS for compiling and providing the TOEFL11 corpus, and we also thank the organizers of the NLI Shared Task 2013.  References  Bestgen, Y., Granger, S., & Thewissen, J. (2012). Error patterns and automatic l1 identification. In S. Jarvis and S. A. Crossley (Eds.), Approaching Language Transfer through Text Classification: Explorations in the Detection-Based Approach. (pp. 127-153). Bris-tol, UK: Multilingual Matters. Blanchard, D., Tetreault, J., Higgins, D., Cahill, A., & Chodorow, M. (2013). TOEFL11: A Corpus of Non-Native English. Princeton, NJ: Educational Testing Service. Crossley, S. A., & McNamara, D. S. (2012). Detecting the first language of second language writers using automated indices of cohesion, lexical sophistication, syntactic complexity, and conceptual knowledge. In S. Jarvis and S. A. Crossley (Eds.), Approaching Language Transfer through Text Classification: Ex-plorations in the Detection-Based Approach. (pp. 106-126). Bristol, UK: Multilingual Matters. Crossley, S. A., Defore, C., Kyle, K., Dai, J., & McNa-mara, D. S. (under review). Paragraph specific n-gram approaches to automatically assessing essay quality. Sixth International Conference on Educa-tional Data Mining, Memphis, TN. Graesser, A. C., McNamara, D. S., Louwerse, M. M., & Cai, Z. (2004). Coh-metrix: Analysis of text on cohe-sion and language. Behavior Research Methods, In-struments, & Computers, 36(2), 193-202. Granger, S., Dagneaux, E.., Meunier, F., Paquot, M. (Eds.) (2009). International corpus of learner eng-lish. version 2. Belgium: Presses universitaires de Louvain.  Granger, S. (2009) The contribution of learner corpora to second language acquisition and foreign language teaching: A critical evaluation. In: Aijmer, K., Cor-pora and Language Teaching, Benjamins: Amster-dam and Philadelphia, 2009, p. 13-32.  Jarvis, S. (2000). Methodological rigor in the study of transfer: Identifying L1 influence in the interlan-guage lexicon. Language Learning, 50, 245-309. Jarvis, S. (2010). Comparison-based and detection-based approaches to transfer research. In L. Roberts, 
249
M. Howard, M. ? Laoire, & D. Singleton (Eds.), EUROSLA Yearbook 10 (pp. 169-192). Amsterdam: Benjamins. Jarvis, S. (2012). The detection-based approach: An overview. In S. Jarvis & S.A. Crossley (Eds.), Ap-proaching language transfer through text classifica-tion: Explorations in the detection-based approach (pp. 1-33). Bristol, UK: Multilingual Matters. Jarvis, S., & Crossley, S. A. (2012). Approaching lan-guage transfer through text classification: Explora-tions in the detection-based approach. Bristol, UK: Multilingual Matters. Jarvis, S., Bestgen, Y., Crossley, S. A., Granger, S., Paquot, M., Thewissen, J., & McNamara, D. S. (2012). The comparative and combined contributions of n-grams, Coh-Metrix indices, and error types in the L1 classification of learner texts. In S. Jarvis & S.A. Crossley (Eds.), Approaching language transfer through text classification: Explorations in the detec-tion-based approach (pp. 154-177). Bristol, UK: Multilingual Matters.  Jarvis, S., & Paquot, M. (2012). Exploring the role of n-grams in L1 identification. In S. Jarvis & S.A. Crossley (Eds.), Approaching language transfer through text classification: Explorations in the detec-tion-based approach (pp. 71-105). Bristol, UK: Mul-tilingual Matters. Jarvis, S., Casta?eda-Jim?nez, G., & Nielsen, R. (2012). Detecting L2 writers? L1s on the basis of their lexical styles. In S. Jarvis & S.A. Crossley (Eds.), Approach-ing language transfer through text classification: Ex-plorations in the detection-based approach (pp. 34-70). Bristol, UK: Multilingual Matters. Koppel, M., Schler, J. & Zigdon, K. (2005). Determin-ing an author?s native language by mining for text er-rors. In Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining (pp. 624-628). Chicago: Association for Computing Machinery. Kucera, H. and Francis, W. N. (1967). Computational Analysis of Present-Day American English Provi-dence, RI: Brown University Press. Laufer, B., & Girsai, N. (2008). Form-focused instruc-tion in second language vocabulary learning: A case for contrastive analysis and translation. Applied Lin-guistics, 29(4), 694-716.  Mayfield Tomokiyo, L. & Jones, R. (2001). You?re not from ?round here, are you? Na?ve Bayes detection of non-native utterance text. In Proceedings of the Sec-ond Meeting of the North American Chapter of the Association for Computational Linguistics (NAACL ?01), unpaginated electronic document. Cambridge, MA: The Association for Computational Linguistics. McEnery, T., & Hardie, A. (2012). Corpus linguistics: method, theory and practice. Cambridge, New York: Cambridge University Press, 2012. 
Rozovskaya, A. & Roth, D. (2011). Algorithm selection and model adaptation for ESL correction tasks. In Proceedings of the 49th Annual Meeting of the Asso-ciation for Computational Linguistics (ACL ?11). Scott, M., (2013). WordSmith Tools. Liverpool: Lexical Analysis Software. Tetreault, J., Blanchard, D., & Cahill, A. (2013). Sum-mary report on the first shared task on native lan-guage identification. In Proceedings of the Eighth Workshop on Building Educational Applications Us-ing NLP, unpaginated electronic document. Atlanta, GA: Association for Computational Linguistics. Tsur, O. & Rappoport, A. (2007). Using classifier fea-tures for studying the effect of native language on the choice of written second language words. In Pro-ceedings of the Workshop on Cognitive Aspects of Computational Language Acquisition. (pp. 9-16). Cambridge, MA: The Association for Computational Linguistics. Wong, S.-M.J. & Dras, M. (2009). Contrastive analysis and native language identification. In Proceedings of the Australasion Language Technology Association (pp. 53-61). Cambridge, MA: The Association for Computational Linguistics.  
250
