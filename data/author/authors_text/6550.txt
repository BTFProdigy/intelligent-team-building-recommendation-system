Proceedings of the 7th Workshop on Statistical Machine Translation, pages 71?75,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Class error rates for evaluation of machine translation output
Maja Popovic?
German Research Center for Artificial Intelligence (DFKI)
Language Technology (LT), Berlin, Germany
maja.popovic@dfki.de
Abstract
We investigate the use of error classification
results for automatic evaluation of machine
translation output. Five basic error classes are
taken into account: morphological errors, syn-
tactic (reordering) errors, missing words, ex-
tra words and lexical errors. In addition, lin-
ear combinations of these categories are in-
vestigated. Correlations between the class er-
ror rates and human judgments are calculated
on the data of the third, fourth, fifth and sixth
shared tasks of the Statistical Machine Trans-
lation Workshop. Machine translation outputs
in five different European languages are used:
English, Spanish, French, German and Czech.
The results show that the following combina-
tions are the most promising: the sum of all
class error rates, the weighted sum optimised
for translation into English and the weighted
sum optimised for translation from English.
1 Introduction
Recent investigations have shown that it is possi-
ble to carry out a reliable automatic error analysis
of a given translation output in order to get more
information about actual errors and details about
particular strengthnesses and weaknesses of a sys-
teml (Popovic? and Ney, 2011). The obtained results
correlate very well with the human error classifica-
tion results. The question we try to answer is: how
the class error rates correlate with the human eval-
uation (ranking) results? As a first step, we inves-
tigate the correlations of five basic class error rates
with human rankings. In the next step, linear com-
binations (sums) of basic class error rates are inves-
tigated.
Spearman?s rank correlation coefficients on the
document (system) level between all the metrics and
the human ranking are computed on the English,
French, Spanish, German and Czech texts gener-
ated by various translation systems in the frame-
work of the third (Callison-Burch et al, 2008),
fourth (Callison-Burch et al, 2009), fifth (Callison-
Burch et al, 2010) and sixth (Callison-Burch et al,
2011) shared translation tasks.
2 Class error rates
In this work, the method proposed in (Popovic?
and Ney, 2011) is used, i.e. classification of
the translation errors into five basic categories
based on the Word Error Rate (WER) (Levenshtein,
1966) together with the recall- and precision-based
Position-independent Error Rates called Reference
PER (RPER) and Hypothesis PER (HPER).
As a result of an error classification, two values
are usually of interest: raw error counts for each er-
ror class, and error rates for each class, i.e. raw error
counts normalised over the total number of running
words. Which of the values is preferred depends of
the exact task. For example, if only a distribution
of error types within a translation output is of in-
terest, the raw error counts are sufficient. On the
other hand, if we want to compare different transla-
tion outputs, normalised values i.e. error rates are
more suitable. Therefore they are appropriate candi-
dates to be used for the evaluation task.
In this work, we explore the error rates calculated
on the word level as well as on the block level, where
71
a group of consecutive words labelled with the same
error category is called a block. The normalisation
in both cases is carried out over the total number of
running words. Therefore the block level error rate
for a particular error class is always less or equal
than the corresponding word level error rate.
2.1 Basic class error rates
The following five basic class error rates are ex-
plored:
INFER (inflectional error rate):
Number of words translated into correct base
form but into incorrect full form, normalised
over the hypothesis length.
RER (reordering error rate):
Number of incorrectly positioned words nor-
malised over the hypothesis length.
MISER (missing word error rate):
Number of words which should appear in the
translation hypothesis but do not, normalised
over the reference length.
EXTER (extra word error rate):
Number of words which appear in the transla-
tion hypothesis but should not, normalised over
the hypothesis length.
LEXER (lexical error rate):
Number of words translated into an incorrect
lexical choice in the target language (false dis-
ambiguation, unknown/untranslated word, in-
correct terminology, etc.) normalised over the
hypothesis length.
Table 1 presents an example of word and block
level class error rates. Each erroneous word is la-
belled with the corresponding error category, and the
blocks are marked within the parentheses { and }.
The error rates on the block level are marked with a
letter ?b? at the beginning. It should be noted that
the used method at its current stage does not enable
assigning multiple error tags to one word.
2.2 Combined error rates (sums)
The following linear combinations (sums) of the ba-
sic class error rates are investigated:
reference:
The famous journalist Gustav Chalupa ,
born in C?eske? Bude?jovice ,
also confirms this .
hypothesis containing 14 running words:
The also confirms the famous
Austrian journalist Gustav Chalupa ,
from Budweis Lamborghini .
hypothesis labelled with error classes:
The {alsoorder confirmsorder}
{theextra} {famousorder} {Austrianextra}
{journalistorder Gustavorder Chalupaorder} ,
{fromlex Budweislex Lamborghinilex} .
class error rates:
word order:
RER = 6/14 = 42.8%
bRER = 3/14 = 21.4%
extra words:
EXTER = 2/14 = 14.3%
bEXTER = 2/14 = 14.3%
lexical errors:
LEXER = 3/14 = 21.4%
bLEXER = 1/14 = 7.1%
Table 1: Example of word and block level class error
rates: the word groups within the parentheses { and } are
considered as blocks; all error rates are normalised over
the hypothesis length, i.e. 14 running words.
W?ER (sum of word level error rates)1 :
Sum of all basic class error rates on the word
level;
B?ER (sum of block level error rates):
Sum of all basic class error rates on the block
level;
WB?ER (sum of word and block level error rates):
Arithmetic mean of W?ER and B?ER.
1This error rate has already been introduced in (Popovic? and
Ney, 2011) and called ?ER; however, for the sake of clarity, in
this work we will call it W?ER, i.e. word level ?ER.
72
XEN?ER (X?English sum of error rates):
Linear interpolation of word level and block
level class error rates optimised for translation
into English;
ENX?ER (English?X sum of error rates):
Linear interpolation of word level and block
level class error rates optimised for translation
from English.
For the example sentence shown in Table 1,
W?ER = 84.7%, B?ER = 46.2% and WB?ER =
65.4%. XEN?ER and ENX?ER are weighted sums
which will be explained in the next section.
The prerequisite for the use of the described met-
rics is availability of an appropriate morphological
analyser for the target language which provides base
forms of the words.
3 Experiments on WMT 2008, 2009, 2010
and 2011 test data
3.1 Experimental set-up
The class error rates described in Section 2 were
produced for outputs of translations from Spanish,
French, German and Czech into English and vice
versa using Hjerson (Popovic?, 2011), an open-
source tool for automatic error classification. Span-
ish, French, German and English base forms were
produced using the TreeTagger2, and the Czech base
forms using Morc?e (Spoustova? et al, 2007). In this
way, all references and hypotheses were provided
with the base forms of the words.
For each error rate, the system level Spearman
correlation coefficients ? with human ranking were
calculated for each document. In total, 40 correla-
tion coefficients were obtained for each error rate ?
twelve English outputs from the WMT 2011, 2010
and 2009 task and eight from the WMT 2008 task,
together with twenty outputs in other four target lan-
guages. For further analysis, the obtained corre-
lation results were summarised into the following
three values:
? mean
average correlation coefficient;
2http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/
? rank>
percentage of documents where the particular
error rate has better correlation than the other
error rates;
? rank?
percentage of documents where the particular
error rate has better or equal correlation than
the other error rates.
3.2 Comparison of basic class error rates
Our first experiment was to compare correlations for
the basic set of class error rates in order to investi-
gate a general behaviour of each class error rate and
to see if some of the error categories are particularly
(in)convenient for the evaluation task. Since certain
differences between English and non-English trans-
lation outputs are observed for some error classes,
the values described in the previous section were
also calculated separately.
Table 2 presents the results of this experiment.
The mean values over all documents, over the En-
glish documents and over the non-English docu-
ments are shown.
According to the overall mean values, the most
promising error categories are lexical and reorder-
ing errors. However, the mean values for English
outputs are significantly different than those for non-
English outputs: the best error classes for English
are in deed lexical and reordering errors, however
for the non-English outputs the inflectional errors
and missing words have higher correlations. On the
other hand, for the English outputs missing words
have even negative correlations, whereas correla-
tions for inflectional errors are relatively low. The
extra word class seems to be the least convenient in
general, especially for non-English outputs.
Therefore, the rank? values were calculated only
separately for English and non-English outputs, and
the previous observations were confirmed: for the
English outputs lexical and reordering errors are the
most relevant, whereas for the non-English outputs
all classes except extra words are almost equally im-
portant.
Apart from this, it can be noticed that the group-
ing of words into blocks significantly improves cor-
relation for reordering errors. The reason for this
is ambiguity of tagging words as reordering errors.
73
error mean rank?
rate overall x?en en?x x?en en?x
INFER 0.398 0.190 0.595 46.2 71.7
RER 0.360 0.344 0.373 53.8 51.1
MISER 0.173 -0.101 0.434 26.3 54.4
EXTER 0.032 0.212 -0.195 42.7 12.2
LEXER 0.508 0.669 0.355 86.0 58.3
bINFER 0.423 0.211 0.624 47.9 75.6
bRER 0.508 0.594 0.426 78.3 60.0
bMISER 0.169 -0.121 0.446 21.1 53.9
bEXTER -0.031 0.186 -0.238 36.8 10.0
bLEXER 0.515 0.634 0.402 79.5 62.8
Table 2: mean and rank? values for each basic word level
and block level error rate over all documents, over En-
glish documents and over non-English documents.
For example, if the translation reference is ?a very
good translation?, and the obtained hypothesis is ?a
translation very good? , one possibility is to mark
the word ?translation? as reordering error, another
possibility is to mark the words ?very good? as re-
ordering errors, and it is also possible to mark all the
words as reordering errors. In such cases, the group-
ing of consecutive word level errors into blocks is
beneficial.
3.3 Comparison of error rate sums
A first step towards combining the basic class error
rates was investigation of simple sums, i.e. W?ER,
B?ER as well as WB?ER as arithmetic mean of pre-
vious two. The overall average correlation coeffi-
cients of the sums were shown to be higher than
those of the basic class error rates. Further exper-
iments have been carried out taking into account the
results described in the previous section. Firstly, ex-
tra word class was removed from all sums, however
no improvement of correlation coefficients was ob-
served. Then the sums containing only the most
promising error categories separately for English
and non-English output were investigated, but this
also resulted in no improvements. Finally, we in-
troduced weights for each translation direction ac-
cording to the rank? value for each of the basic
class error rates (see Table 2), and this approach
was promising. In this way, the specialised sums
XEN?ER and ENX?ER were introduced.
In Table 3 the results for all five error rate sums
are presented. mean, rank> and rank? values are
presented over all translation outputs, over English
outputs and over non-English outputs. As already
mentioned, the overall correlation coefficients of the
sums are higher than those of the basic class error
rates. This could be expected, since summing class
error rates is oriented towards the overall quality of
the translation output whereas the class error rates
are giving more information about details.
According to the overall values, the best error rate
is combination of all word and block level class er-
ror rates, i.e. WB?ER followed by the block sum
B?ER, whereas the W?ER and the specialised sums
XEN?ER and ENX?ER have lower correlations.
For the translation into English, this error rate is also
very promising, followed by the specialised sum
XEN?ER. On the other hand, for the translation
from English, the most promising error rates are the
block sum B?ER and the corresponding specialised
sum ENX?ER. Following these observations, we
decided to submit WB?ER scores for all transla-
tion outputs together with XEN?ER and ENX?ER
scores, each one for the corresponding translation
direction. In addition, we submitted B?ER scores
since this error rate also showed rather good results,
especially for the translation out of English.
4 Conclusions
The presented results show that the error classifica-
tion results can be used for evaluation and ranking
of machine translation outputs. The most promis-
ing way to do it is to sum all word level and block
level error rates, i.e. to produce the WB?ER error
rate. This error rate has eventually been submitted
to the WMT 2012 evaluation task. In addition, the
next best metrics have been submitted, i.e. the block
level sum B?ER for all translation directions, and
the specialised sums XEN?ER and ENX?ER each
for the corresponding translation outputs.
The experiments described in this work are still at
early stage: promising directions for future work are
better optimisation of weights3, further investigation
of each language pair and also of each non-English
3First steps have already been made in this direction using
an SVM classifier, and the resulting evaluation metric has also
been submitted to the WMT 2012.
74
error rate mean rank? rank>
overall x?en en?x overall x?en en?x overall x?en en?x
W?ER 0.616 0.694 0.541 55.1 50.0 61.2 39.1 48.6 36.2
B?ER 0.629 0.666 0.594 60.3 55.2 68.8 46.1 39.5 52.5
WB?ER 0.639 0.696 0.585 68.0 67.1 63.7 48.7 52.6 45.0
XEN?ER 0.587 0.692 0.487 51.9 63.2 41.2 37.8 52.6 23.7
ENX?ER 0.599 0.595 0.602 50.6 38.1 62.5 39.1 32.9 45.0
Table 3: mean, rank? and rank> values for error rate sums compared over all documents, over English documents
and over non-English documents.
target language separately, filtering error categories
by POS classes, etc.
Acknowledgments
This work has partly been developed within the
TARAXU? project4 financed by TSB Technologies-
tiftung Berlin ? Zukunftsfonds Berlin, co-financed
by the European Union ? European fund for regional
development. Special thanks to Mark Fishel and
Ondr?ej Bojar.
References
Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,
Christof Monz, and Josh Schroeder. 2008. Further
Meta-Evaluation of Machine Translation. In Proceed-
ings of the 3rd ACL 08 Workshop on Statistical Ma-
chine Translation (WMT 2008), pages 70?106, Colum-
bus, Ohio, June.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the 2009
Workshop on Statistical Machine Translation. In Pro-
ceedings of the Fourth Workshop on Statistical Ma-
chine Translation (WMT 2009), pages 1?28, Athens,
Greece, March.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Kay Peterson, Mark Przybocki, and Omar Zaidan.
2010. Findings of the 2010 Joint Workshop on Sta-
tistical Machine Translation and Metrics for Machine
Translation. In Proceedings of the Joint Fifth Work-
shop on Statistical Machine Translation and Metrics-
MATR (WMT 2010), pages 17?53, Uppsala, Sweden,
July.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Omar Zaidan. 2011. Findings of the 2011 Work-
shop on Statistical Machine Translation. In Proceed-
ings of the Sixth Workshop on Statistical Machine
4http://taraxu.dfki.de/
Translation (WMT 2011), pages 22?64, Edinburgh,
Scotland, July.
Vladimir Iosifovich Levenshtein. 1966. Binary Codes
Capable of Correcting Deletions, Insertions and Re-
versals. Soviet Physics Doklady, 10(8):707?710,
February.
Maja Popovic? and Hermann Ney. 2011. Towards Au-
tomatic Error Analysis of Machine Translation Out-
put. Computational Linguistics, 37(4):657?688, De-
cember.
Maja Popovic?. 2011. Hjerson: An Open Source Tool
for Automatic Error Classification of Machine Trans-
lation Output. The Prague Bulletin of Mathematical
Linguistics, (96):59?68, October.
Drahom??ra Spoustova?, Jan Hajic?, Jan Votrubec, Pavel Kr-
bec, and Pavel Kve?ton?. 2007. The best of two worlds:
Cooperation of statistical and rule-based taggers for
czech. In Proceedings of the Workshop on Balto-
Slavonic Natural Language Processing, ACL 2007,
pages 67?74, Praha.
75
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 133?137,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Morpheme- and POS-based IBM1 scores and language model scores for
translation quality estimation
Maja Popovic?
German Research Center for Artificial Intelligence (DFKI)
Language Technology (LT), Berlin, Germany
maja.popovic@dfki.de
Abstract
We present a method we used for the quality
estimation shared task of WMT 2012 involving
IBM1 and language model scores calculated
on morphemes and POS tags. The IBM1 scores
calculated on morphemes and POS-4grams of
the source sentence and obtained translation
output are shown to be competitive with the
classic evaluation metrics for ranking of trans-
lation systems. Since these scores do not re-
quire any reference translations, they can be
used as features for the quality estimation task
presenting a connection between the source
language and the obtained target language. In
addition, target language model scores of mor-
phemes and POS tags are investigated as esti-
mates for the obtained target language quality.
1 Introduction
Automatic quality estimation is a topic of increas-
ing interest in machine translation. Different from
evaluation task, quality estimation does not rely on
any reference translations ? it relies only on infor-
mation about the input source text, obtained target
language text, and translation process. Being a new
topic, it still does not have well established base-
lines, datasets or standard evaluation metrics. The
usual approach is to use a set of features which are
used to train a classifier in order to assign a predic-
tion score to each sentence.
In this work, we propose a set of features based
on the morphological and syntactic properties of in-
volved languages thus abstracting away from word
surface particularities (such as vocabulary and do-
main). This approach is shown to be very useful for
evaluation task (Popovic?, 2011; Popovic? et al, 2011;
Callison-Burch et al, 2011). The features investi-
gated in this work are based on the language model
(LM) scores and on the IBM1 lexicon scores (Brown
et al, 1993).
The inclusion of IBM1 scores in translation sys-
tems has shown experimentally to improve transla-
tion quality (Och et al, 2003). They also have been
used for confidence estimation for machine transla-
tion (Blatz et al, 2003). The IBM1 scores calcu-
lated on morphemes and POS-4grams are shown to
be competitive with the classic evaluation metrics
based on comparison with given reference transla-
tions (Popovic? et al, 2011; Callison-Burch et al,
2011). To the best of our knowledge, these scores
have not yet been used for translation quality esti-
mation. The LM scores of words and POS tags are
used for quality estimation in previous work (Spe-
cia et al, 2009), and in our work we investigate the
scores calculated on morphemes and POS tags.
At this point, only preliminary experiments have
been carried out in order to determine if the pro-
posed features are promising at all. We did not use
any classifier, we used the obtained scores to rank
the sentences of a given translation output from the
best to the worst. The Spearman?s rank correlation
coefficients between our ranking and the ranking ob-
tained using human scores are then computed on the
provided manually annotated data sets.
2 Morpheme- and POS-based features
A number of features for quality estimation have
been already investigated in previous work (Specia
et al, 2009). In this paper, we investigate two sets of
133
features which do not depend on any aspect of trans-
lation process but only on the morphological and
syntactic structures of the involved languages: the
IBM1 scores and the LM scores calculated on mor-
phemes and POS tags. The IBM1 scores describe
the correspondences between the structures of the
source and the target language, and the LM scores
describe the structure of the target language. In ad-
dition to the input source text and translated target
language hypothesis, a parallel bilingual corpus for
the desired language pair and a monolingual corpus
for the desired target language are required in or-
der to learn IBM1 and LM probabilities. Appropriate
POS taggers and tools for splitting words into mor-
phemes are necessary for each of the languages. The
POS tags cannot be only basic but must have all de-
tails (e.g. verb tenses, cases, number, gender, etc.).
2.1 IBM1 scores
The IBM1 model is a bag-of-word translation model
which gives the sum of all possible alignment proba-
bilities between the words in the source sentence and
the words in the target sentence. Brown et al (1993)
defined the IBM1 probability score for a translation
pair fJ1 and e
I
1 in the following way:
P (fJ1 |e
I
1) =
1
(I + 1)J
J?
j=1
I?
i=0
p(fj |ei) (1)
where fJ1 is the source language sentence of length
J and eI1 is the target language sentence of length I .
As it is a conditional probability distribution, we
investigated both directions as quality scores. In or-
der to avoid frequent confusions about what is the
source and what the target language, we defined our
scores in the following way:
? source-to-hypothesis (sh) IBM1 score:
IBM1sh =
1
(H + 1)S
S?
j=1
H?
i=0
p(sj |hi) (2)
? hypothesis-to-source (hs) IBM1 score:
IBM1hs =
1
(S + 1)H
H?
i=1
S?
j=0
p(hi|sj) (3)
where sj are the units of the original source lan-
guage sentence, S is the length of this sentence, hi
are the units of the target language hypothesis, and
H is the length of this hypothesis.
The units investigated in this work are morphemes
and POS-4grams, thus we have the following four
IBM1 scores:
? MIBM1sh and MIBM1hs:
IBM1 scores of word morphemes in each direc-
tion;
? P4IBM1sh and P4IBM1hs:
IBM1 scores of POS 4grams in each direction.
2.2 Language model scores
The n-gram language model score is defined as:
P (eI1) =
I?
i=1
p(ei|ei...ei?n) (4)
where ei is the current target language word and
ei...ei?n is the history, i.e. the preceeding n words.
In this paper, the two following language model
scores are explored:
? MLM6:
morpheme-6gram language model score;
? PLM6:
POS-6gram language model score.
3 Experimental set-up
The IBM1 probabilities necessary for the IBM1
scores are learnt using the WMT 2010 News
Commentary Spanish-English, French-English and
German-English parallel texts. The language mod-
els are trained on the corresponding target parts of
this corpus using the SRI language model tool (Stol-
cke, 2002). The POS tags for all languages were pro-
duced using the TreeTagger1, and the morphemes
are obtained using the Morfessor tool (Creutz and
Lagus, 2005). The tool is corpus-based and
language-independent: it takes a text as input and
produces a segmentation of the word forms observed
in the text. The obtained results are not strictly
1http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/
134
linguistic, however they often resemble a linguistic
morpheme segmentation. Once a morpheme seg-
mentation has been learnt from some text, it can
be used for segmenting new texts. In our experi-
ments, the splitting are learnt from the training cor-
pus used for the IBM1 lexicon probabilities. The
obtained segmentation is then used for splitting the
corresponding source texts and hypotheses. Detailed
corpus statistics are shown in Table 1.
Using the obtained probabilities, the scores de-
scribed in Section 2 are calculated for the pro-
vided annotated data: the English-Spanish data from
WMT 2008 consisting of four translation outputs
produced by four different systems (Specia et al,
2010), the French-English and English-Spanish data
from WMT 2010 (Specia, 2011), as well as for an
additional WMT 2011 German-English and English-
German annotated data. The human quality scores
for the first two data sets range from 1 to 4, and for
the third data set from 1 to 3. The interpretation of
human scores is:
1. requires complete retranslation (bad)
2. post-editing quicker than retranslation (edit?);
this class was omitted for the third data set
3. little post-editing needed (edit+)
4. fit for purpose (good)
As a first step, the arithmetic means and standard
deviations are calculated for each feature and each
class in order to see if the features are at all possible
candidates for quality estimation, i.e. if the values
for different classes are distinct.
After that, the main test is carried out: for each
of the features, the Spearman correlation coefficient
? with the human ranking are calculated for each
document. In total, 9 correlation coefficients are ob-
tained for each score ? four Spanish outputs from the
WMT 2008 task, one Spanish and one English output
from the WMT 2010 as well as one English and two
German outputs from the WMT 2011 task.
The obtained correlation results were then sum-
marised into the following two values:
? mean
a correlation coefficient averaged over all trans-
lation outputs;
? rank>
percentage of translation outputs where the par-
ticular feature has better correlation than the
other investigated features.
4 Results
4.1 Arithmetic means
The preliminary experiments consisted of compar-
ing arithmetic means of scores for each feature and
each class. The idea is: if the values are distinct
enough, the feature is a potential candidate for qual-
ity estimation. In addition, standard deviations were
calculated in order to estimate the overlapping.
For most translation outputs, all of our features
have distinct arithmetic means for different classes
and decent standard deviations, indicating that they
are promising for further investigation. On all WMT
2011 outputs annotated with three classes, the dis-
tinction is rather clear, as well as for the majority of
the four class outputs.
However, on some of the four class translation
outputs, the values of the bad translation class were
unexpected in the following two ways:
? the bad class overlaps with the edit? class;
? the bad class overlaps with the edit+ class.
The first overlapping problem occured on two trans-
lation outputs of the 2011 set, and the second one on
the both outputs of the 2010 set.
Examples for the PLM6 and P4IBM1sh features
are shown in Table 2. First two rows present three
class and four class outputs with separated arith-
metic means, the first problem is shown in the third
row, and the second (and more serious) problem is
presented in the last row.
These overlaps have not been investigated further
in the framework of this work, however this should
be studied deeply (especially the second problem) in
order to better understand the underlying phenom-
ena and improve the features.
4.2 Spearman correlation coefficients
As mentioned in the previous section, Spearman
rank correlation coefficients are calculated for each
translation output and for each feature, and sum-
marised into two values described in Section 3, i.e.
135
Spanish English French English German English
sentences 97122 83967 100222
running words 2661344 2338495 2395141 2042085 2475359 2398780
vocabulary:
words 69620 53527 56295 50082 107278 54270
morphemes 14178 13449 12004 12485 22211 13499
POS tags 69 44 33 44 54 44
POS-4grams 135166 121182 62177 114555 114314 123550
Table 1: Statistics of the corpora for training IBM1 lexicon models and language models.
feature output / class ok edit+ edit? bad
PLM6 de-en 13.5 / 7.3 23.7 / 13.6 33.0 / 19.7
es-en4 10.9 / 5.0 20.7 / 8.7 34.6 / 16.4 49.0 / 23.7
es-en3 18.5 / 11.0 30.2 / 15.6 38.4 / 17.4 37.9 / 18.9
fr-en 15.2 / 8.8 26.2 / 13.7 34.5 / 18.4 21.7 / 11.3
P4IBM1sh de-en 50.5 / 38.4 109.7 / 75.6 161.8 / 108.3
es-en4 37.9 / 25.0 88.7 / 48.7 165.8 / 89.0 241.5 / 127.4
es-en3 77.0 / 56.7 139.8 / 82.5 186.4 / 94.6 185.2 / 102.0
fr-en 53.5 / 44.3 110.0 / 69.3 151.8 / 90.9 90.8 / 59.0
Table 2: Arithmetic means with standard deviations of PLM6 and P4IBM1sh scores for four translation outputs: first
two rows present decently separated classes, third row illustrates the overlap problem concerning the bad and the edit?
class, the last row illustrates the overlap problem concerning the bad and the edit+ class.
mean and rank>. The results are shown in Table 3.
In can be seen that the best individual features are
POS IBM1 scores followed by POS LM score.
The next step was to investigate combinations of
the individual features. First, we calculated arith-
metic mean of POS based features only, since they
are more promising than the morpheme based ones,
however we did not yield any improvements over
the individual mean values. As a next step, we in-
troduced weights to the features according to their
mean correlations, i.e. we did not omit the mor-
pheme features but put more weight on the POS
based ones. Nevertheless, this also did not result
in an improvement. Furthermore, we tried a sim-
ple arithmetic mean of all features, and this resulted
in a better Spearman correlation coefficients.
Following all these observations, we decided to
submit the arithmetic mean of all features to the
WMT 2012 quality estimation task. Our submission
consisted only of sentence ranking without scores,
since we did not convert our scores to the inter-
val [1,5]. Therefore we did not get any MAE or
RMSE results, only DeltaAvg and Spearman corre-
lation coefficients which were both 0.46. The high-
est scores in the shared task were 0.63, the lowest
about 0.15, and for the ?baseline? system which uses
a set of well established features with an SVM clas-
sifier about 0.55.
5 Conclusions and outlook
The results presented in this article show that the
IBM1 and the LM scores calculated on POS tags and
morphemes have the potential to be used for the
estimation of translation quality. These results are
very preliminary, offering many directions for future
work. The most important points are to use a classi-
fier, as well as to combine the proposed features with
already established features. Furthermore, the bad
class overlapping problem described in Section 4.1
should be further investigated and understood.
Acknowledgments
This work has been partly developed within the
TARAXU? project financed by TSB Technologies-
136
mean rank>
0.449 P4IBM1sh 70.4 P4IBM1sh
0.445 P4IBM1hs 68.5 P4IBM1hs
0.444 PLM6 61.1 PLM6
0.430 MLM6 27.7 MLM6
0.426 MIBM1sh 20.3 MIBM1sh
0.420 MIBM1hs 9.2 MIBM1hs
0.450 arithmetic mean 83.3 arithmetic mean
Table 3: Features sorted by average correlation (column 1) and rank> value (column 2). The most promising score
is the arithmetic mean of all individual features. The most promising individual features are POS-4gram IBM1 scores
followed by POS-6gram language model score.
tiftung Berlin ? Zukunftsfonds Berlin, co-financed
by the European Union ? European fund for regional
development.
References
John Blatz, Erin Fitzgerald, George Foster, Simona Gan-
drabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis,
and Nicola Ueffing. 2003. Confidence estimation for
machine translation. Final report, JHU/CLSP Summer
Workshop.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathemat-
ics of statistical machine translation: Parameter esti-
mation. Computational Linguistics, 19(2):263?311,
June.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Omar Zaidan. 2011. Findings of the 2011 Work-
shop on Statistical Machine Translation. In Proceed-
ings of the Sixth Workshop on Statistical Machine
Translation (WMT 2011), pages 22?64, Edinburgh,
Scotland, July.
Mathias Creutz and Krista Lagus. 2005. Unsupervised
morpheme segmentation and morphology induction
from text corpora using morfessor 1.0. Technical Re-
port Report A81, Computer and Information Science,
Helsinki University of Technology, Helsinki, Finland,
March.
Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur,
Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar
Kumar, Libin Shen, David Smith, Katherine Eng,
Viren Jain, Zhen Jin, and Dragomir Radev. 2003. Syn-
tax for statistical machine translation. Technical re-
port, Johns Hopkins University 2003 Summer Work-
shop on Language Engineering, Center for Language
and Speech Processing, Baltimore, MD, USA, August.
Maja Popovic?, David Vilar Torres, Eleftherios Avramidis,
and Aljoscha Burchardt. 2011. Evaluation without
references: IBM1 scores as evaluation metrics. In Pro-
ceedings of the Sixth Workshop on Statistical Machine
Translation (WMT 2011), pages 99?103, Edinburgh,
Scotland, July.
Maja Popovic?. 2011. Morphemes and POS tags for
n-gram based evaluation metrics. In Proceedings of
the Sixth Workshop on Statistical Machine Translation
(WMT 2011), pages 104?107, Edinburgh, Scotland,
July.
Lucia Specia, Marco Turchi, Zhuoran Wang, John
Shawe-Taylor, and Craig Saunders. 2009. Improv-
ing the confidence of machine translation quality es-
timates. In Machine Translation Summit XII, Ottawa,
Canada.
Lucia Specia, Nicola Cancedda, and Marc Dymetman.
2010. A Dataset for Assessing Machine Translation
Evaluation Metrics. In Proceedings of the Seventh
conference on International Language Resources and
Evaluation (LREC?2010), pages 3375?3378, Valletta,
Malta, May.
Lucia Specia. 2011. Exploiting Objective Annotations
for Measuring Translation Post-editing Effort. In Pro-
ceedings of the 15th Annual Conference of the Euro-
pean Association for Machine Translation (EAMT 11),
pages 73?80, Leuven, Belgium, May.
Drahom??ra ?Johanka? Spoustova?, Jan Hajic?, Jan Raab,
and Miroslav Spousta. 2009. Semi-Supervised Train-
ing for the Averaged Perceptron POS Tagger. In
Proceedings of the 12th Conference of the European
Chapter of the ACL (EACL 2009), pages 763?771,
Athens, Greece, March.
Andreas Stolcke. 2002. SRILM ? an extensible language
modeling toolkit. In Proceedings of the 7th Inter-
national Conference on Spoken Language Processing
(ICSLP 02), volume 2, pages 901?904, Denver, CO,
September.
137
