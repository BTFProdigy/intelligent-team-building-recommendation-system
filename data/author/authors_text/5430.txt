A Representation for Complex and Evolving Data Dependencies 
in Generation 
C Me l l i sh  $, R Evans  t, L Cah i l l  t, C Doran  t, D Pa iva  t, M Reape $, D Scot t  t, N T ipper  t 
t Information Technology Research Institute, University of Brighton, Lewes Rd, Brighton, UK 
SDivision of Informatics, University of Edinburgh, 80 South Bridge, Edinburgh, UK 
rags@itri, brighton, ac. uk 
http :/www. itri. brighton, ac. uk/proj ect s/rags 
Abst rac t  
This paper introduces an approach to represent- 
ing the kinds of information that components 
in a natural language generation (NLG) sys- 
tem will need to communicate to one another. 
This information may be partial, may involve 
more than one level of analysis and may need 
to include information about the history of a 
derivation. We present a general representation 
scheme capable of handling these cases. In ad- 
dition, we make a proposal for organising inter- 
module communication i an NLG system by 
having a central server for this information. We 
have validated the approach by a reanalysis of 
an existing NLG system and through a full im- 
plementation of a runnable specification. 
1 In t roduct ion  
One of the distinctive properties of natural an- 
guage generation when compared with other 
language ngineering applications i that it has 
to take seriously the full range of linguistic rep- 
resentation, from concepts to morphology, or 
even phonetics. Any processing system is only 
as sophisticated as its input allows, so while a 
natural language understanding system might 
be judged primarily by its syntactic prowess, 
even if its attention to semantics, pragmatics 
and underlying conceptual analysis is minimal, 
a generation system is only as good as its deep- 
est linguistic representations. Moreover, any at- 
tempt to abstract away from individual gener- 
ation systems to a more generic architectural 
specification faces an even greater challenge: 
not only are complex linguistic representations 
required, able to support the dynamic evolu- 
tionary development of data during the gener- 
* Now at the MITRE Corporation, Bedford, MA, USA, 
cdoran@mitre, org. 
ation process, but they must do so in a generic 
and flexible fashion. 
This paper describes a representation devel- 
oped to meet these requirements. It offers a 
formally well-defined eclarative representation 
language, which provides a framework for ex- 
pressing the complex and dynamic data require- 
ments of NLG systems. The approach supports 
different levels of representation, mixed repre- 
sentations that cut across levels, partial and 
shared structures and 'canned' representations, 
as well as dynamic relationships between data 
at different stages in processing. We are using 
the approach to develop a high level data model 
for NLG systems as part of a generic generation 
architecture called RAGS 1. 
The framework has been implemented in the 
form of a database server for modular genera- 
tion systems. As proof of concept of the frame- 
work, we have reimplemented an existing NLG 
system. The system we chose was the Caption 
Generation System (CGS) (Mittal et al, 1995; 
Mittal et al, 1998). The reimplementation in- 
volved defining the interfaces to the modules of 
CGS in terms of the RAGS representations and 
then implementing modules that had the requi- 
site input and output representations. 
Generation systems, especially end-to-end, 
applied generation systems, have, unsurpris- 
ingly, many things in common. Reiter (1994) 
proposed an analysis of such systems in terms 
of a simple three stage pipeline. More recently, 
the RAGS project attempted to repeat he anal- 
1This work is supported by ESPRC grants 
GR/L77041 (Edinburgh) and GR/L77102 (Brighton), 
RAGS: Reference Architecture for Generation Systems. 
We would also like to acknowledge the contribution of 
Jo Calder to the ideas and formalisation described in 
this paper. In particular, parts of this paper are based 
on (Calder et al, 1999). 
119 
ysis (Cahill et al, 1999a), but found that while 
most systems did implement a pipeline, they 
did not implement the same pipeline - different 
functionalities occurred in different places and 
different orders in different systems. In order 
to accommodate his result, we sought to de- 
velop an architecture that is more general than 
a simple pipeline, and thus supports the range 
of pipelines observed, as well as other more com- 
plex control regimes (see (Cahill et al, 1999a; 
Cahill et al, 1999b)). In this paper, we argue 
that supporting such an architecture requires 
careful consideration of the way data represen- 
tations interact and develop. Any formal frame- 
work for expressing the architecture must take 
account of this. 
2 The  representat iona l  requ i rements  
o f  generat ion  sys tems 
We noted in the introduction that generation 
systems have to deal with a range of linguis- 
tic information. It is natural, especially in the 
context of a generic architecture proposal, to 
model this breadth in terms of discrete layers 
of representation: (1999a) introduce layers such 
as conceptual, semantic, rhetorical, syntactic 
and document structure, but the precise demar- 
cation is not as important here as the princi- 
ple. The different kinds of information are typi- 
cally represented differently, and built up sepa- 
rately. However the layers are far from indepen- 
dent: objects at one layer are directly related to 
those at others, forming chains of dependency 
from conceptual through rhetorical and seman- 
tic structure to final syntactic and document re- 
alisation. This means that data resources, such 
as grammars and lexicons, and processing mod- 
ules in the system, are often defined in terms of 
mixed  data: structures that include informa- 
tion in more than one representation layer. So 
the ability to represent such mixed structures 
in a single formal framework is an important 
property of a generic data proposal. 
In addition, it is largely standard in gener- 
ation as elsewhere in language applications, to 
make extensive use of par t ia l  representations, 
often using a type system to capture grades of 
underspecification. An immediate corollary of 
providing support for partial structures is the 
notion that they may become further specified 
over time, that data structures evolve. If the 
framework seeks to avoid over-commitment to 
particular processing strategies it needs to pro- 
vide a way of representing such evolution ex- 
plicitly if required, rather than relying on de- 
structive modification of a structure. Related 
to this, it should provide explicit support for 
representing a l te rnat ive  specifications at any 
point. Finally, to fully support efficient pro- 
cessing across the range of applications, from 
the simple to the most complex, the represen- 
tation must allow for compact sharing of infor- 
mation in tang led  structures (two structures 
which share components). 
In addition to these direct requirements of the 
generation task itself, additional requirements 
arise from more general methodological consid- 
erations: we desire a representation that is for- 
mally well  def ined,  allows for theoretical rea-  
son ing about the data and performance of sys- 
tems, and supports control regimes from simple 
deterministic pipelines to complex parallel ar- 
chitectures. 
3 The  Representat ion  Scheme 
In this section, we present our proposal for a 
general representation scheme capable of cover- 
ing the above requirements. Our formulation is 
layered: the foundation is a simple, flexible, rig- 
orously defined graph representation formalism, 
on top of which we introduce notions of com- 
plex types and larger data structures and rela- 
tionships between them. This much is sufficient 
to capture the requirements just discussed. We 
suppose a yet higher level of specification could 
capture a more constraining data model but 
make no specific proposals about this here, how- 
ever the following sections use examples that do 
conform to such a higher level data model. 
The lowest level of the representation scheme 
is: 
? re lat iona l :  the basic data entity is x -~ y, 
an ar row representing a relation from ob- 
ject x to object y; 
? typed:  objects and arrows have an asso- 
ciated type system, so it is possible to de- 
fine classes and subclasses of objects and 
arrows. 
At the most fundamental level, this is more or 
less the whole definition. There is no commit- 
ment to what object or arrow types there are or 
120 
how they relate to each other. So a representa- 
tion allowed by the scheme consists of: 
? a set of objects, organised into types; 
? a set of binary relations, organised into 
types; 
? a set of arrows, each indicating that a rela- 
tion holds between one object and another 
object. 
Sets,  sequences  and  funct ions  
For the next level, we introduce more struc- 
ture in the type system to support sets, se- 
quences and functions. Objects are always 
atomic (though they can be of type set, se- 
quence or function) - it is not possible to make 
an object which actually is a set of two other 
objects (as you might with data structures in a 
computer program). To create a set, we intro- 
duce a set type for the object, and a set mem- 
bership arrow type (el), that links the set's el- 
ements to the set. Similarly, for a sequence, we 
introduce a sequence type and sequence mem- 
ber arrow types (1-el, 2-el, 3-el, . . .  ), and for a 
function, we have a complex type which spec- 
ifies the types of the arrows that make up the 
domain and the range of the function. 
SemRep 
~ fun(Role.SemRep) 
7 V show SemRep SemRep 
Figure 1: The partial semantic representation 
of "The second chart shows the number of days 
on the market" 
As an example, consider Figure 1, which 
shows a semantic representation (SemRep) from 
the CGS reimplementation. Here, the tree 
nodes correspond to objects, each labelled with 
its type. The root node is of type SemRep, and 
although it is not an explicit sequence type, we 
can see that it is a triple, as it has three sequence 
member arrows (with types 1-el, 2-el and 3-el). 
Its first arrow's target is an object of type DR 
(Discourse Referent). Its second represents a set 
of SemPred (Semantic Predicate) objects, and in 
this case there's just one, of type show. Its third 
element is a (partial) function, from Role arrow 
types (agent and affected are both subtypes of 
Role) to SemReps. (In this case, the SemReps 
have not yet been fully specified.) 
Local  and  non- loca l  a r rows  
The second extension to the basic representa- 
tion scheme is to distinguish two different ab- 
stract kinds of arrows - local and non-local. 
Fundamentally we are representing just a homo- 
geneous network of objects and relationships. In 
the example above we saw a network of arrows 
that we might want to view as a single data 
structure, and other major data types might 
similarly appear as networks. Additionally, we 
want to be able to express relationships between 
these larger 'structures' - between structures 
of the same type (alternative solutions, or re- 
vised versions) or of different ypes (semantic 
and syntactic for example). To capture these 
distinctions among arrows, we classify our ar- 
row types as local or non-local (we could do 
this in the type system itself, or leave it as an 
informal distinction). Local arrows are used to 
build up networks that we think of as single 
data structures. Non-local arrows express rela- 
tionships between such data structures. 
All the arrow types we saw above were local. 
Examples of non-local arrows might include: 
real ises These arro~vs link something more ab- 
stract to something less abstract hat re- 
alises it. Chains of realises arrows might 
lead from the original conceptual input to 
the generator through rhetorical, seman- 
tic and syntactic structures to the actual 
words that express the input. 
revises These arrows link a structure to an- 
other one of the same type, which is con- 
sidered to be a 'better' solution - perhaps 
because it is more instantiated. It is impor- 
tant to note that parts of larger structures 
can be revised without revising the entire 
structure. 
coreference These arrows link structures 
which are somehow "parallel" and which 
perhaps hare some substructure, i.e., tan- 
gled structures. For instance, document 
representations may be linked to rhetorical 
representations, either as whole isomorphic 
structures or at the level of individual con- 
stituents. 
121 
Notice that the representation scheme does 
not enforce any kind of well-formedness with 
respect o local and non-local arrows. In fact, 
although it is natural to think of a 'structure' as 
being a maximal network of local arrows with 
a single root object, there's no reason why this 
should be so - networks with multiple roots rep- 
resent tangled structures (structures that share 
content), networks that include non-local links 
might be mixed representations, containing in- 
formation of more than one sort. Such tech- 
niques might be useful for improving generator 
efficiency, or representing canned text or tem- 
plates, cf. (Calder et al, 1999). 
Par t ia l  and  Opaque s t ruc tures  
Partial structures are essential when a module 
needs to produce a skeleton of a representa- 
tion that it does not have the competence to 
completely fill out. For instance, lexical choice 
brings with it certain syntactic commitments, 
but in most NLG systems lexical choice occurs 
some time before a grammar is consulted to 
flesh out syntactic structure in detail. 
Figure 2: A partial structure 
By simply leaving out local arrows, we can 
represent a range of partial structures. Con- 
sider Fig. 2, where the triangles represent local 
structure, representing a sentence object and its 
component verb phrase. There is a link to a sub- 
ject noun phrase object, but none of the local 
arrows of the actual noun phrase are present. In 
subsequent processing this local structure might 
be filled in. This is possible as long as the noun 
phrase object has been declared to be of the 
right type. 
An opaque structure is one which has an in- 
complete derivational history - for example part 
of a syntactic structure without any correspond- 
ing semantic structure. Three possible reasons 
for having such structures are (a) to allow struc- 
ture to be introduced that the generator is not 
capable of producing directly, (b) to prevent he 
generator from interfering with the structure 
thus built (for example, by trying to modify an 
idiom in an inappropriate way), or (c) to im- 
prove generator efficiency by hiding detail that 
may lead to wasteful processing. An opaque 
structure is represented simply by the failure 
to include a rea l i ses  arrow to that structure. 
Such structures provide the basis for a gener- 
alised approach to "canning". 
4 Imp lementat ion  
There are many ways that modules in an 
NLG system could communicate information 
using the representation scheme just outlined. 
Here we describe a particularly general model 
of inter-module communication, based around 
modules communicating with a single cen- 
tralised repository of data called the whiteboard 
(Calder et al, 1999). A whiteboard is a cumu- 
lative typed relational blackboard: 
? t yped  and  re lat iona l :  because it is based 
on using the above representation scheme; 
? a b lackboard :  a control architec- 
ture and data store shared between 
processing modules; typically, modules 
add/change/remove objects in the data 
store, examine its contents, and/or ask to 
be notified of changes; 
? cumulat ive :  unlike standard blackboards, 
once data is added, it can't be changed or 
removed. So a structure is built incremen- 
tally by making successive copies of it (or of 
constituents of it) linked by rev ises  links 
(although actually, there's no constraint on 
the order in which they are built). 
A whiteboard allows modules to add ar- 
rows (typically forming networks through ar- 
rows sharing source or target objects), to in- 
spect the set of arrows looking for particular 
configurations of types, or to be informed when 
a particular type of arrow (or group of arrows) 
is added. 
The whiteboard is an active database server. 
This means that it runs as an independent pro- 
cess that other modules connect o by appropri- 
ate means. There are essentially three kinds of 
interaction that a module might have with the 
whiteboard server: 
? pub l i sh  - add an arrow or arrows to the 
whiteboard; 
122 
? query  - look for an arrow or arrows in the 
whiteboard; 
? wa i t  - register interest in an arrow or ar- 
rows appearing in the whiteboard. 
In both query and wait ,  arrows are specified 
by type, and with a hierarchical type system on 
objects and relations, this amounts to a pattern 
that matches arrows of subtypes as well. The 
wait  function allows the whiteboard to take the 
initiative in processing - if a module wai ts  on a 
query then the whiteboard waits until the query 
is satisfied, and then tells the module about it. 
So the module does not have to continuously 
scan the whiteboard for work to do, but can 
let the whiteboard tell it as soon as anything 
interesting happens. 
Typically a module will start up and regis- 
ter interest in the kind of arrow that represents 
the module's input data. It will then wait for 
the whiteboard to notify it of instances of that 
data (produced by other modules), and when- 
ever anything turns up, it processes it, adding 
its own results to the whiteboard. All the mod- 
ules do this asynchronously, and processing con- 
tinues until no module has any more work to 
do. This may sound like a recipe for confusion, 
but more standard pipelined behaviour is not 
much different. In fact, pipelining is exactly a 
data-based constraint - the second module in a 
pipeline does not start until the first one pro- 
duces its output. 
However, to be a strict pipeline, the first mod- 
ule must produce all of its output before the sec- 
ond one starts. This can be achieved simply by 
making the first module produce all its output 
at once, but sometimes that is not ideal - for ex- 
ample if the module is recursive and wishes to 
react to its own output. Alternative strategies 
include the use of markers in the whiteboard, 
so that modules can tell each other that they've 
finished processing (by adding a marker), or 
extending the whiteboard architecture itself so 
that modules can tell the whiteboard that they 
have finished processing, and other modules can 
wait for that to occur. 
5 Reconst ruct ion  o f  the  Capt ion  
Generat ion  System 
In order to prove this representation scheme 
in practice, we have implemented the white- 
board in Sicstus Prolog and used it to support 
data communications between modules in a re- 
construction of the Caption Generation System 
(Mittal et al, 1995). CGS is a system developed 
at the University of Pittsburgh, which takes in- 
put from the SAGE graphics presentation sys- 
tem (Roth et al, 1994) and generates captions 
for the graphics SAGE produces. We selected it 
for this effort because it appeared to be a fairly 
simple pipelined system, with modules perform- 
ing clearly defined linguistic tasks. As such, we 
thought it would be a good test case for our 
whiteboard specification. 
Although the CGS is organised as a pipeline, 
shown in Figure 3, the representations commu- 
nicated between the modules do not correspond 
to complete, separate instances of RAGS data- 
type representations. Instead, the representa- 
tions at the various levels accumulate along the 
pipeline or are revised in a way that does not 
correspond exactly to module boundaries. Fig- 
ure 3 gives a simple picture of how the different 
levels of representation build up. The labels for 
the RAGS representations refer to the following: 
? I = conceptual; 
? II -- semantic; 
? I I I  = rhetorical; 
? IV = document; 
? V = syntactic. 
For instance, some semantic (II) information is 
produced by the Text Planning module, and 
more work is done on this by Aggregation, but 
the semantic level of representation is not com- 
plete and final until the Referring Expression 
module has run. Also, for instance, at the 
point where the Ordering module has run, there 
are partially finished versions of three different 
types of representation. It is clear from this that 
the interfaces between the modules are more 
complex than could be accounted for by just re- 
ferring to the individual evels of representation 
of RAGS. The ability to express combinations of 
structures and partial structures was fundamen- 
tal to the reimplementation of CGS. We high- 
light below a few of the interesting places where 
these features were used. 
123 
AbsSemRep 
I-el ~ ~  .................................... SemRep 
--(~------~_set{KBPredl ~ fun(Role,set(KBId)) I-el ~3-e l  
. . . .  /X  . . . . . . . .  
el agent affected . . . .  DR fun(Role,set(SemRep)) ~i/  ~ ..... ~ el?set(SemPredi~t A ~ . ? 
nresent set(KSld) 0 . . . . . .  v ? ~--"- ................. / agen, /  \a\] Jec,ea 
el / \ el . . . . .  " . . . . . . . . . .  ~ ?J / "k~ present S~mRep SemRep 
chart1 chart2 
Figure 4: Combined Abstract Semantic Representation a d Concrete Semantic Representation for 
the output: "These two charts present information about house sales from data-set ts-1740" 
CG$ aroh i ta ,~ lu 'e  RAGS representat/on$ 
II I l l  IV ~' SAGE 
- -  . . . . . . . . . .  
tuning II 
- . . . . . . . . . .  
I1 I11 iV  
--' . . . . . . . . . .  
I\[ I11 IV 
. . . . . . . . . .  I ;11@ 
11 III I v  v 
. . . . . . . . .  
II I11 IV V 
. . . . . . . . .  III1  
II 111 IV V 
l - -  . . . . . . . . . .  I I I I I  
FUF 
Figure 3: A RAGS view of the CGS system 
5.1 Referr ing Express ion Generat ion  
In many NLG systems, (nominal) referring ex- 
pression generation is an operation that is in- 
voked at a relatively late stage, after the struc- 
ture of individual sentences i  fairly well speci- 
fied (at least semantically). However, referring 
expression generation eeds to go right back to 
the original world model/knowledge base to se- 
lect appropriate semantic ontent o realise a 
particular conceptual item as an NP (whereas 
all other content has been determined much ear- 
lier). In fact, there seems to be no place to 
put referring expression generation i a pipeline 
without there being some resulting awkward- 
ness. 
In RAGS, pointers to conceptual items can 
be included inside the first, "abstract", level of 
semantic representation (AbsSemRep), which is 
intended to correspond to an initial bundling of 
conceptual material under semantic predicates. 
On the other hand, the final, "concrete", level 
of semantic representation (SemRep) is more 
like a fully-fledged logical form and it is no 
longer appropriate for conceptual material to 
be included there. In the CGS reimplementa- 
tion, it is necessary for the Aggregation mod- 
ule to reason about the final high-level semantic 
representation f sentences, which means that 
this module must have access to "concrete" se- 
mantic representations. The Referring Expres- 
sion generation module does not run until later, 
which means that these representations cannot 
be complete. 
Our way around this was to ensure that the 
initial computation of concrete semantics from 
abstract semantics (done as part of Aggrega- 
tion here) left a record of the relationship by 
including realises arrows between correspond- 
ing structures. That computation could not be 
completed whenever it reached conceptual ma- 
terial - at that point it left a "hole" (an ob- 
ject with no further specification) in the con- 
crete semantic representation li ked back to the 
conceptual material. When referring expression 
was later invoked, by following the arrows in the 
124 
resulting mixed structure, it could tell exactly 
which conceptual entity needed to be referred 
to and where in the semantic structure the re- 
sulting semantic expression should be placed. 
Figure 4 shows the resulting arrangement for 
one example CGS sentence. The dashed lines 
indicate realises, i.e. non-local, arrows. 
5.2 Handling Centering Information 
The CGS Centering module reasons about the 
entities that will be referred to in each sentence 
and produces a representation which records the 
forward and backward-looking centers (Grosz et 
al., 1995). This representation is later used by 
the Referring Expression generation module in 
making pronominalisation decisions. This in- 
formation could potentially also be used in the 
Realisation module. 
Since Centering is not directly producing re- 
ferring expressions, its results have to sit around 
until they can actually be used. This posed 
a possible problem for us, because the RAGS 
framework does not provide a specific level of 
representation for Centering information and 
therefore seems on first sight unable to account 
for this information being communicated be- 
tween modules. The solution to the problem 
came when we realised that Centering informa- 
tion is in fact a kind of abstract syntactic in- 
formation. Although one might not expect ab- 
stract syntactic structure to be determined until 
the Realisation module (or perhaps lightly ear- 
lier), the CGS system starts this computation i
the Centering module. 
Thus in the reimplementation, the Centering 
module computes (very partial) abstract syn- 
tactic representations for the entities that will 
eventually be realised as NPs. These represen- 
tations basically just indicate the relevant Cen- 
tering statuses using syntactic features. Figure 
5 shows an example of the semantics for a typi- 
cal output sentence and the two partial abstract 
syntactic representations computed by the Cen- 
tering module for what will be the two NPs in 
that sentence 2. As before, dashed lines indicate 
realises arrows. Of course, given the discussion 
of the last section, the semantic representation 
objects that are the source of these arrows are in 
fact themselves linked back to conceptual enti- 
ties by being the destination of realises arrows 
2FVM = Feature Value Matrix. 
from them. 
When the Referring Expression generation 
module runs, it can recover the Centering infor- 
mation by inspecting the partial syntactic rep- 
resentations for the phrases it is supposed to 
generate. These partial representations are then 
further instantiated by, e.g., Lexical Choice at 
later stages of the pipeline. 
6 Conc lus ion  
The representation scheme we have proposed 
here is designed specifically to support he re- 
quirements of the current state-of-the-art NLG 
systems, and our pilot implementation demon- 
strates the practical applicability of the pro- 
posal. Tangled, partial and mixed structures 
are of obvious utility to any system with a flex- 
ible control strategy and we have shown here 
how the proposed representation scheme sup- 
ports them. By recording the derivational his- 
tory of computations, it also supports decisions 
which partly depend on earlier stages of the 
generation process (e.g., possibly, lexical choice) 
and revision-based architectures which typically 
make use of such information. We have shown 
how the representation scheme might be the ba- 
sis for an inter-module communication model, 
the whiteboard, which supports a wide range of 
processing strategies that require the represen- 
tation of complex and evolving data dependem 
cies. The fact that the whiteboard is cumula- 
tive, or monotonic in a logical sense, means that 
the whiteboard also supports reasoning about 
the behaviour of NLG systems implemented in 
terms of it. This is something that we would 
like to exploit directly in the future. 
The reimplementation f the CGS system 
in the RAGS framework was a challenge to 
the framework because it was a system that 
had already been developed completely inde- 
pendently. Even though we did not always un- 
derstand the detailed motivation for the struc- 
ture of CGS being as it was, within a short time 
we reconstructed a working system with mod- 
ules that corresponded closely to the original 
CGS modules. The representation scheme we 
have proposed here was a key ingredient in giv- 
ing us the flexibility to achieve the particular 
processing scheme used by CGS whilst remain- 
ing faithful to the (relatively simple) RAGS 
data model. 
125 
SemRep 
fun(Role,setlSemRep)) 
sl S " ' .  
t t ~ .  
2 AbsSynRep "~ AbsSynRep _(:5 ~ ,  
, , / \ \ 
ckward-looking-cemer ckward.looking-cenler 
+ + 
Figure 5: Arrangement of centering information for the output sentence above 
The representation scheme is useful in situa- 
tions where modules need to be defined and im- 
plemented to work with other modules, possibly 
developed by different people. In such cases, the 
representation scheme we propose permits pre- 
cise definition of the interfaces of the modules, 
even where they are not restricted to a single 
'level' of representation. Even though the con- 
trol structure of CGS is quite simple, we found 
that the use of a centralised whiteboard was use- 
ful in helping us to agree on interfaces and on 
the exact contribution that each module should 
be making. Ultimately, it is hoped that the use 
of a scheme of this type will permit much more 
widespread 'plug-and-play' among members of 
the NLG community. 
Re ferences  
Lynne Cahill, Christy Doran, Roger Evans, Chris 
Mellish, Daniel Paiva, Mike Reape, Donia Scott, 
and Neil Tipper. 1999a. In Search of a Reference 
Architecture for NLG Systems. In Proceedings of 
the 7th European Workshop on Natural Language 
Generation, pages 77-85, Toulouse. 
Lynne Cahill, Christy Doran, Roger Evans, Chris 
Mellish, Daniel Paiva, Mike Reape, Donia Scott, 
and Neil Tipper. 1999b. Towards a Reference 
Architecture for Natural Language Genera- 
tion Systems. Technical Report ITRI-99-14, 
Information Technology Research Institute 
(ITRI), University of Brighton. Available at 
http://www, i t r i  .brighton. ac. uk/proj ects/rags.  
Jo Calder, Roger Evans, Chris Mellish, and Mike 
Reape. 1999. "Free choice" and templates: how 
to get both at the same time. In "May I speak 
freely?" Between templates and free choice in nat- 
ural language generation, number D-99-01, pages 
19-24. Saarbriicken. 
B.J. Grosz, A.K. Joshi, and S. Weinstein. 1995. 
Centering: a framework for modelling the local co- 
herence of discourse. Computational Linguistics, 
21 (2):203-226. 
V. O. Mittal, S. Roth, J. D. Moore, J. Mattis, and 
G. Carenini. 1995. Generating explanatory cap- 
tions for information graphics. In Proceedings of 
the 15th International Joint Conference on Ar- 
tificial Intelligence (IJCAI'95), pages 1276-1283, 
Montreal, Canada, August. 
V. O. Mittal, J. D. Moore, G. Carenini, and S. Roth. 
1998. Describing complex charts in natural lan- 
guage: A caption generation system. Computa- 
tional Linguistics, 24(3):431-468. 
Ehud Reiter. 1994. Has a consensus NL generation 
architecture appeared and is it psycholinguisti- 
cally plausible? In Proceedings of the Seventh In- 
ternational Workshop on Natural Language Gen- 
eration, pages 163-170, Kennebunkport, Maine. 
Steven F. Roth, John Kolojejchick, Joe Mattis, and 
Jade Goldstein. 1994. Interactive graphic design 
using automatic presentation knowledge. In Pro- 
ceedings of CHI'9~: Human Factors in Computing 
Systems, Boston, MA. 
126 
 	

 
	
	An Empirical Analysis of Constructing Non.restrictive NP 
Modifiers to Express Semantic Relations 
Hua Cheng and  Chr i s  Me l l i sh  
D iv i s ion  of In fo rmat ics ,  Un ivers i ty  of  Ed inburgh  
80 South  Br idge,  Ed inburgh  EH1 1HN,  UK  
huac, chr ism @ dai. ed. ac. uk 
Abst rac t  
It is not a rare phenomenon for human written text 
to use non-restrictive NP modifiers to express es- 
sential pieces of information or support the situa- 
tion presented in the main proposition containing 
the NP, for example, "Private Eye, which couldn't 
afford the libel payment, had been threatened with 
closure." (from Wall Street Journal) Yet no previ- 
ous research in NLG investigates this in detail. This 
paper describes corpus analysis and a psycholinguis- 
tic experiment regarding the acceptability of using 
non-restrictive NP modifiers to express emantic re- 
lations that might normally be signalled by 'because' 
and 'then'. The experiment tests several relevant 
factors and enables us to accept or reject a number 
of hypotheses. The results are incorporated into an 
NLG system based on a Genetic Algorithm. 
1 In t roduct ion  
To produce natural language text, an NLG system 
must be able to choose among possible paraphrases 
one that satisfies the highest number of constraints 
in a certain context. Paraphrases can use various 
constructions, for example, using nominalisation i - 
stead of a clause for event representation. We are 
particularly interested in the use of non-restrictive 
(NR) modifiers within a referring expression to ex- 
press certain semantic relationQ other than object- 
attribute elaboration (in the sense defined in (Mann 
and Thompson, 1987)), for instance, causal rela- 
tions, which are normally expressed by separate 
clauses connected by cue phrases (Knott, 1996) such 
as 'because '. 
"A non-restrictive component gives additional in- 
formation to a head that has already been viewed 
as unique or as a member of a class that has been 
independently identified,-mud therefoee is not' essml; 
tial for the identification of the head" (Quirk et al, 
1985). This definition can be extended to account 
for modifiers of not only definite referring expres- 
sions, but also definite and indefinite NPs of var- 
ious types. In this paper, an NR modifier refers 
to any NP modifying component that is not essen- 
tial for identifying the object denoted by the head, 
including all modifiers of an NP that does not in- 
tend to identify (e.g. indefinite referring expressions 
and predicative phrases) (Kronfeld, 1990). Our dis- 
cussion focuses on definite referring expressions in- 
cluding proper-names because of the dominance of 
such examples in our corpus. However, we would 
expect no difficulty in applying our observation to 
other types of NPs. 
The semantic roles of NR modifiers, in particular 
NR clauses, are mentioned in many grammar and 
linguistics books. Quirk et al (1985) point out that 
an NR clause in a referring expression is usually neu- 
tral in its semantic role (i.e. it provides descriptive 
information about its head), but sometimes it can 
contribute to the semantics of the main clause in 
a variety of ways. They summarise three types of 
semantic relations that can be expressed by an NR 
clause (examples are given in Figure 1): 
? Causal, where the situation in the main clause 
is caused by that in the NR clause, e.g. (la). 
? Temporal, where the two clauses form a time 
sequence, e.g. (lb). 
? Circumstantial, where the NR clause sets a tem- 
poral or spatial framework for interpreting the 
main clause, e.g. (lc). 
Halliday (1985) mentions that a subordinate 
clause can elaborate a part of its primary clause 
through restating, clarifying, refining or adding a de- 
scriptive attribute or comment (see (2) of Figure 1). 
Halliday's notion of elaboration is much more gen- 
eral than that in other coherence theories like RST 
(Maim "andThompson; t987), and  :the rdat ion ex- 
pressed in (2) would not be treated as elaboration 
in most NLG systems. 
Similar phenomena were observed from the MUSE 
corpus 2, a corpus of museum exhibit labels, which 
l kVe are concerned with semant ic  ( informational) relations 
in this paper? Argumentat ive {intentional) relations are be- 
yond the scope of this paper. 
2This corpus is collected and annotated for the GNOME 
project (Poesio, 2000), which aims at developing eneral al- 
gor i thms for generat ing nominal expressions. 
108 
(1) a. 
b. 
C. 
He sent ahead the se,yeant, who was the most  exper ienced scout in the company.  
In 1960 he came to ,London4 .:wh.are. :he :haa~lived ? ever  ~in~ze. 
The boy, who had his satchel  trail ing behind him, ran past. 
(2) Inflation, which was necessary  fo r  the system,  became also "lethal. 
(3) In spite of his French name, Martin Carlin was born in Germany and emigrated to Paris to become 
an ebeniste. 
Figure 1: -Examples for NR modifiers .contributing. to the semantics of the main clauses 
? _ . . . . . . .  . ? ~ ' . ,  . . . . . .  _- - 
describe museum objects on display. For example, 
in (3) of Figure 1, the modifier French is not for 
identifying the name, but for establishing a conces- 
sion relation between the main proposition and the 
subordinate phrase to increase the reader's positive 
regard for where Martin Carlin was born. 
For the convenience of discussion, we define some 
terminology to be used throughout the paper: 
An NR construction/sentence : a sentence that has 
a main clause and a subordinate NR modifier 
attached to one of its NPs (e.g. (4b) of Fig- 
ure 2). 
A hypotactic onstruction/sentence : a sentence 
that has a main clause and a dependent clause, 
connected by a cue phrase. This is a common 
way of expressing semantic relations such as 
causality (e.g. (4a) of Figure 2). In this syn- 
tactic category, we single out a subclass of sen- 
tences according to one possible semantic on- 
nection between the two clauses. It is defined 
below. 
An elaboration realisation : a type of hypotactic 
construction where one clause elaborates the se- 
mantics of the other. We take cue phrases "as 
for" or "what is more" to signal elaboration re- 
lations 3. 
Previous research in NLG mainly focuses on us- 
ing NR constructions to realise elaboration relations 
but not other semantic relations (e.g. (Scott and 
de Souza, 1990) and (Hovy, 1993)). The NR modi- 
tier usually adds a descriptive attribute to the object 
denoted by the head. 
The linguistic research suggests for an NLG sys- 
tem the possibility to express certain semantic rela- 
cue phrases in most cases, and therefore could avoid 
using cues too heavily., This could be a better re- 
alisation under certain circumstances. Secondly, an 
NR construction enables a wider range of relations 
(especially those that are preferred to be expressed 
implicitly) to be selected for text structuring because 
the corresponding syntactic option is available. 
To understand how to enable an NLG system to 
generate such modifiers, we are faced with two ques- 
tions, which are not answered by linguistic research: 
1. Can this type of modifier be identified by human 
subjects, i.e. can humans tell the difference be- 
tween different NP modifier uses? 
2. Under what circumstances can an NR construc- 
tion be used in substitution of a hypotactic on- 
struction without changing the meaning dra- 
matically and how close are the meanings con- 
veyed by the two representations? 
An NLG system must come up with some solu- 
tions, simple or complex, to these two questions in 
order to choose among paraphrases. In this paper, 
we use cue phrases ms a signal of semantic relations 
rather than try to identify the relations directly. 
We describe systematically controlled experiments 
aimed at finding out the factors related to the gen- 
eration of this type of modifier in referring expres- 
sions. The result is intended to be reliable enough 
to be used by NLG systems in generating descriptive 
text. 
2 Corpus  annotat ion  
To answer the first question, we annotated the 
MUSE corpus, from which we have observed three 
types of modifier uses in an NP: 
tions through NR constructions, which is important . . . .  Firstly,. pro~i.ding .properties ?o .uniquely identify 
in two aspects. Firstly, an  NR construction gives--.--the objects or concepts denoted bythe-NP .Wi th -  
a more concise alternative realisation for a relation, 
where the relation is expressed implicitly rather than 
explicitly and usually more subtly. It does not need 
3\Ve acknowledge that these cue phrases are controversial 
in their semantic interpretations, but not using cue phrases 
would be even more ambiguous. Besides, our experiment does 
not heavily depend on these cue phrases. 
out these modifiers, the NP can denote more than 
one object/concept or sets of objects/concepts and 
is ambiguous in its interpretation, e.g. those in (6a). 
Such modifiers usually appear in phrases headed by 
the definite article 'the', which according to Loebner 
(1987) has the same meaning in all its uses, includ- 
ing in generic references and predicatives. Modifiers 
109 
(4) a. 
b. 
(5) a. 
b. 
Private Eye had been threatened with closure because it couldn't afford the libel payment. 
Private ~Ege;-',which. couldn~t~.a~o.rd.thevlibel. :paymen.t,.,: had:~been~threa~ned'with" closure. 
But P&G contends the new Cheer is a unique formula that also offers an ingredient hat prevents 
colors from fading. And retailers are expected to embrace the product, because it will take up less 
shelf space. 
And retailers are expected to embrace the product, which will take up less shelf  space. 
Figure 2: Examples for inferrability 
in other types of genericreferences, .g. indefini:tes; 
also belong here. 
This type subsumes the modifiers normally con- 
sidered by the referring expression generation mod- 
ule of an NLG system for uniquely identifying the 
referents (e.g. (Dale, 1992)). 
Secondly, having no effect in constraining a unique 
or unambiguous concept out of the NP which is ei- 
ther already unique or not required to have a unique 
interpretation, but being important o the situation 
presented in the main proposition containing the NP. 
This type includes the modifiers described in the 
previous section and many modifiers in indefinite 
predicatives, e.g. that in (6b). 
Thirdly, providing additional details about the 
referents of the NP, which functions the same way 
as the NP without these modifiers, e.g. those in 
(6c). The effect of such modifiers is usually local 
to the heads they describe rather than to the main 
propositions as a whole, which is the main difference 
between this and the second type of modifier. 
This type subsumes the modifiers normally gen- 
erated by an aggregation module, in particular one 
using embedding (e.g. (Shaw and McKeown, 1997), 
(Cheng, 1998)). 
(6) a. the decoration on this cabinet; the best 
looking food I ever saw 
b. This is a mighty  empty  country. 
c. the wide gilt bronze straps on the cof- 
fer fronts and sides; He lived in a five- 
room apartment in the Faubourg Saint-  
Anto ine .  
To find out whether the above distinctions make 
sense to human subjects, we designed an annotation 
scheme for modifiers in NPs, describing which ele- 
ments of an NP should be marked as a modifier and 
how to mark the features for a modifier. Apart from 
other features, each modifier should be anno/atecl 
with a pragmatic function feature (PRAGM), which 
specifies why a modifier is used it: an NP. The pos- 
sible values for this feature are unique, int and attr, 
corresponding to the three types of modifier uses de- 
scribed above (we will use the value names to refer 
to the different types of modifier in the rest of this 
paper). X.XlL was used as the markup language. 
We' had -two trained annotators mark the  NP  
modifiers in the MUSE corpus according to their 
understanding of the scheme. The agreement be- 
tween them on the PRAGM feature by means of the 
Kappa statistic (Caxletta, 1996) is .734, which means 
that the distinctions we are trying to make can be 
identified by human subjects to some extent. The 
main ambiguity exists between int and attr modi- 
fiers. There seems to be a gradual difference between 
them and where to draw the line is a bit arbitrary. 
In the MUSE corpus annotated so far, 19% of 1078 
modifiers in all types of NPs axe identified as int. So 
this is not a trivial phenomenon. 
3 An  exper iment  
We reduced the size of the problem of when to use 
an NR construction by focusing on two relations: a 
causal relation signalled by 'because' and a temporal 
relation signalled by 'then'. The reason for choosing 
these relations is that the possibilities of expressing 
them through NR constructions have already been 
shown by linguists. The two cue phrases are typical 
for the corresponding relations and can often substi- 
tute other cue phrases for the same relations. In the 
rest of this paper, we will still use the term causal 
or temporal relation, but what we actually mean is 
the specific relation signalled by 'because' or 'then'. 
3.1 Independent  var iab les  and  hypotheses  
From the generation point of view, our question is: 
given two facts and the semantic relation between 
them, what extra input do we need for making real- 
isation decisions? 
We collected examples of 'because' sentences from 
the MUSE corpus, and Wall .Street Journal source 
data, and transfered them to NR sentences by hand. 
Comparing the two constructions, we found some 
~, .An~eresting..vaxiation.:. _Eor.example,:compaxing the 
sentences in Figure 2, we found intuitively that the 
meanings of (4a) and (4b) are much closer than those 
of (5a) and (5b). In other words, (4b) can be used 
in substitution of (4a), whereas (5b) cannot, so easily 
41n (Carletta, 1996), a value of K between .8 and I in- 
dicates good agreement; a value between .6 and .8 indicates 
some agreement. 
110 
I ndependent  Var iables I\] Levels 
.Relation ...causal , temporal 
Inferrability strong weak 
Position initial final 
Order hypotactic vs. NR NR vs. hypotactic 
Subordination I nuc subordinate sat subordinate 
Cued/NoCue I use cue not use cue 
Table 1: Independent variables and their values 
substitute (5a). A simiiar pa~ttern can be foun(i in a 
number of other collected sentences. 
We claim that it is the degree ofinferrability of the 
relation between the semantics expressed through 
the two clauses that makes the difference. We define 
the inferrability of a causal/temporal relation as: 
Given two separate \]acts, the likeli- 
hood of human subjects inferring from their 
world knowledge that a causal/temporal 
connection between the \]acts might plausi- 
bly exist. 
In examples (4) and (5), the fact that Private Eye 
cannot afford the libel payment is very likely to di- 
rectly cause the closure threaten, whereas a prod- 
uct occupying less space is not usually a cause of 
it being accepted by retailers according to common 
sense. Therefore, the two realisations in (4) can be 
used in substitution of one another whereas those in 
(5) cannot. 
In\]errability is dynamic and user dependent. 
Given two facts, people with different background 
knowledge can infer the relation between them with 
different ease. If a relation is easily recognisable 
according to general world knowledge, we say that 
the inferrability of the relation is globally strong, 
in which case a hypotactic and an NR construction 
can express the relation almost equally well (if not 
considering rhetorical effect). Context can also con- 
tribute to the inferrability of a relation. A relation 
not easily recognisable from world knowledge may 
be identified by a reader with ease as the discourse 
proceeds. In this case, we say that the inferrabil- 
ity of the relation is locally strong, where the two 
constructions can express the relation equally well 
only in a certain context. In this paper, we mainly 
consider the global aspect of a relation and we will 
describe how we decided the value of inferrability in 
the next section. 
In Table 1, we summarise the factors (indepen- 
dent variables) that might play a role in the close- 
ness judgement between the semantics of a hypotac- 
tie construction and an NR construction. The levels 
are possible values of these factors. Besides Rela- 
tion and In\]errability. Position gives the location of 
the NP that contains the NR modifier. It can be the 
first (initial) or the last (final) phrase in a sentenceS; 
Order gives the order of presentation; a hypotactic 
sentence to be compared with an NR sentence or vice 
versa, which is used to balance the influence of cue 
phrases on human judgement; Subordination speci- 
fies whether the nucleus or the satellite is realised 
as an NR clause6; and Cued/NoCue means using a 
cue phrase in the NR clause or not, which is only 
applicable to the temporal relation, for example, 
(7) The health-care services announced the spinoff 
plan last January, which was then revised 
in May. 
Based on our observation of human written sen- 
tences, we have the following hypotheses: 
Hypothes is  ! For both causal and temporal rela- 
tions, the inferrability of the relation between the se- 
mantics of two \]acts contributes ignificantly to the 
semantic similarities between a hypotactic onstruc- 
tion and an NR construction. 
In other words, if the in\]errability of the relation 
between the two facts is strong, the semantic rela- 
tion can be expressed similarly through an NR con- 
struction, otherwise, the similarity is significantly re- 
duced. 
Hypothes is  2 For the causal relation, the satellite 
subordination bears significantly higher similarity m 
meaning to the hypotactic onstruction than the nu- 
cleus subordination does. 
For example, (4b) would be preferred to "Private 
Eye, which had been threatened with closure, couldn't 
afford the libel payment." 
Hypothes is  3 For the temporal relation, both the 
position of subordination and the use of an appro- 
priate cue phrase in the NR clause make a signifi- 
cant difference to the semantic similarities between 
? a hypotactic and an NR construction. - 
This hypothesis prefers Example (7) to the reali- 
sation that does not have 'then'. 
5|n our implementation, we restrict ourselves to sentences 
with two NPs. 
aWe assume that in the causal relation, the clause bearing 
'because'is always the satellite. Since the temporal relation 
is a multinuclear relation, this factor does not apply. 
111 
Dependent  Variables 
Naturalness Similarity. 
exactly the same _~ 
very similar 
more similar than di~erent 
N/A 
natural 
fairly natural 
more different han similar so-so 
very different fairly unnatural 
totally different unnatural 
Table 2: Dependent variables and their values 
3.2 The  des ign of  the  exper iment  
To assess a semantic similarity, which is thought to 
be influenced by the independent variables, we use 
human subjects to judge the following two depen- 
dent variables: 
Naturalness : how fluent a sentence is on its own. 
Similarity : how similar the meanings of two sen- 
tences are without considering their natural- 
ness. 
The scales of the variables are selected such that 
all values on the scale have natural verbal descrip- 
tions that could be grasped easily by our subjects 
(see Table 2). Similar rating methods have been 
described in (Jordan et al, 1993) to compare the 
output of a machine translation system with that of 
expert humans. 
Since we want to measure different groups of 
similarity judgement based on different in\]errabil- 
ity, order or position levels, a between-groups de- 
sign (Hatch and Lazaraton, 1991) seems to be most 
appropriate. The design we used is illustrated in 
Table 3, where all possible combinations of the in- 
dependent variables are listed. In the table, para- 
phrases gives the types of alternative sentences each 
original sentence has. They should be scored by hu- 
man subjects for their similarities to the original sen- 
tences and their naturalness. 
We used a method similar to random selection 
to create a stratified random sample. The sample 
should contain 12 hypotactic sentences and 12 NR 
sentences: two for each combination of the causal re- 
lation and one for each combination of the temporal 
relation. These numbers were used to obtain as big 
a sample as possible which could still be judged by 
human subjects in a relatively short period of time 
(say less than 30 minutes). 
Using cue phrases as- the indicators o f ' the  se ..... 
mantic relations between clauses, we collected all 
the sentences containing 'because' or 'then' from the 
Wall Street Journal source data. and went through 
each of them to pick out those that actually signal 
the desired relations and can potentially have NR- 
realisations, i.e. where there is a coreference r lation 
between the two NPs in the two clauses. Sentences 
containing NR clauses signalled by ', which' or ', 
who ':~were~=coUected similarly<,<From: these~:seritcnces, 
we randomly selected one by category. If it realised 
an unused factor combination, it was kept in the 
sample. This process was repeated until we collected 
the right number of test items which instantiated all 
combinations of properties in Table 3. 
We asked two subjects to mark the 24 selected 
items with regard to their inferrability on a five- 
point scale: 5 for very likely, 4 for quite likely, 3 
for possibly, 2 for .even less possibly and 1 for un- 
known.-We~took values of 4 and 5 as Strong ahd"the  
others as weak. The subjects and an author agreed 
on 19 items, and the author's version was used for 
the experiment. 
For the test items, we manually produced the cor- 
responding paraphrases, which were then put into a 
questionnaire for human assessment of the two de- 
pendent variables for each paraphrase. 
3.3 Resu l ts  
We had ten native English speakers evaluating tile 
similarity and naturalness on the sample. 
3.3.1 Simi lar i ty  
Since the similarity data is ordinal data and departs 
significantly from a theoretical normal distribution 
according to One-Sample Komogorov-Smirnov Test, 
we chose Mann Whitney U, which is a test for com- 
paring two groups on the basis of their ranks above 
and below the median. The result is summarised in
Table 4, with statistically significant items in bold- 
face (taking the conventional .05 p level). The Z 
scores tell how many standard deviations above or 
below the mean an observation might be. Means 
gives the means of the similarity scores with respect 
to the values of the independent variables in Table 1. 
For the causal relation, there is a significant dif- 
ference between the means of similarities of the two 
groups of different inferrabilities (P<.0005). So we 
have high confidence to accept part of Hypothesis 1. 
i.e. the strong inferrability of the causal relation be- 
tween the semantics of two facts makes the semantic 
similarities between a hypotactic onstruction and 
an NR construction significantly higher than the 
weak case does. In the strong case, tile mean of 
similarity is 4.59, wilich is ,close to very similar. 
We treated order as a factor to be balanced and 
did not expect it to have a significant effect, but 
it does (P=.008). An NR paraphrase shows much 
higher similarity to its corresponding hypotactic sen-  .... 
tence (with a mean of 4.46) than the other way 
round (with a mean of 3.83), but the difference be- 
comes smaller for the strong inferrability case. This 
could be because the causal relations expressed in 
NR sentences generally sound weaker than those in 
hypotactic sentences and the cue phrase has a big 
influence on the perceptibility of a relation. 
112 
Independent  Variables I 
Relation \[ Order  I inferrabflity I.Position 
causal 
temporal 
strong initial 
hypotactic vs. final 
NR sentence weak initial 
final 
strong initial 
NR sentence final 
vs. hypotactic weak initial 
final 
strong initial 
~ypot, actic vs . . . . . . .  5finAl 
NR sentence weak initial 
final 
strong initial 
NR sentence final 
vs. hypotactic weak initial 
final 
Paraphrases 
nuc & sat subordination 
NR sentence 
nuc & sat subordination 
NR sentence 
causal & 
elaboration hypotactic 
? cued & not 
cued NR sentence 
temporal & 
elaboration hypotactic 
Table 3: A between-groups 
Relat ion DependVar  \[ Factors 
causal 
(160 cases) 
temporal 
(80 cases) 
Similarity 
Similarity 
(cued) 
design 
Means  Z 2-tai led P 
Inferrability 4.59/3.70 -4.1015 <.0005 
Order 4.46/3.83 -2.6400 .0083 
Position 4.11/4.18 -.2136 .8308 
Inferrability 4.88/5.00 -.1022 .9086 
Order 5.08/4.80 -1.1756 .2398 
Position 4.80/5.08 -2.0649 I .0389 
Table 4: The output of Mann 
For the temporal relation, position is the only sig- 
nificant factor (P=.0389). So part of Hypothesis 3 is 
confirmed, that is, the final position subordination 
makes an NR paraphrase significantly more similar 
to the corresponding hypotactic onstruction than 
the initial position does. 
We do not have enough evidence to accept the 
claim that the inferrability of the temporal relation 
contributes ignificantly to the similarity judgement 
(as in Hypothesis 1). However, when we calculated 
the similarity mean for the alternative sentences us- 
ing cue phrases, strong or weak in inferrability, we 
got 4.94 (very similar). Comparing this with that of 
the strong causal case using the Mann Whitney U 
test, we get a significance l vel of 0.0294. This means 
that we have strong confidence to believe that the 
similarity mean for the temporal relation if using a 
cue phrase is significantly . higher. -than, that for the 
strong causal relation. Therefore, the temporal re- 
lation can always be realised by an NR construction 
as long as an appropriate cue phrase is used in the 
NR clause. 
The assumption of normality is also not met by 
the subset of the data related to Hypothesis 2 and 3 
(i.e. the similarity scores for nucleus/satellite subor- 
Whitney U on the similarity data 
dination paraphrases and cued/nocue paraphrases). 
We used the Wilcoxon Matched-Pairs Signed-Ranks 
Test because we were comparing pairs of para- 
phrases. The result is given in Table 5. We accept 
the hypothesis that the similarity means of nucleus 
and satellite subordination are significantly different 
in the initial position (Hypothesis 2). This confirms 
the linguistic observation that information of greater 
importance should be presented in a main position 
rather than a subordinate position. We can also ac- 
cept the hypothesis that for the temporal relation, 
using cue phrases in NR clauses can significantly im- 
prove the similarity score of the NR construction 
(Hypothesis 3). 
3.3.2 Natura lness  
~,?e -used the Mann Whitney U test on naturalness 
with regards to order, inferrability and position, and 
found no significant connection. Figure 3 shows the 
distribution of naturalness assessment of the para- 
phrases for the causal and temporal relation respec- 
tively. The majority of the NR constructions are 
natural or fairly natural, which suggests that they 
could be good alternative realisations. 
113 
causal 
temporal 
D.. 
Paired Var iab les~ Means \] Z value \] 2-tail Sig \] 
 ua7evaa -3.o2 .oo3 
Relation \[ 
Table 5: The output of the Wilcoxon Matched-Pairs Signed-Ranks Test 
 because  to NR clause 
~NR clause to because 
60 
50- 
?) 
O 
0.. 
60- 
50- 
 thee  to NR clause 
~NR clause to then 
Figure 3: The naturalness of the causal paraphrases (left) and the temporal paraphrases (right) 
3.3.3 Summary  
We briefly summarise the heuristics drawn from the 
experiment for expressing the causal and temporal 
relations with an NR construction. This is an ac- 
ceptable realisation in the following circumstances: 
e the causal relation holds between two facts and 
the inferrability of the relation is strong, in 
which case satellite subordination should be 
used; or 
? the temporal relation holds between two facts, 
in which case a final position subordination and 
an appropriate cue phrase, like 'then', should be 
used in the NR clause. 
We also found that an NR construction can ex- 
press the causal/temporal relation and the object- 
attribute elaboration relation at the same time, ir- 
respective of the inferrability of the relation. Gen- 
erally speaking, a semantic relation expressed by an 
NR construction sounds weaker than a hypotactic 
realisation with a cue phrase. Therefore, if a rela- 
tion is to be emphasised, NR constructions should 
not be used. 
4 Imp lement ing  the  resu l t s  in  a 
OA-based  text  p lanner  
int-modifiers have a mixed character, i.e. like attr- 
modifiers they are not essential for identifying the 
referents, but like unique-modifiers they are not op- 
tional. Because of their role in supporting the se- 
mantics of the main propositions, the selection of 
int-modifiers hould be a part of the text planning 
process, where a text structure is constructed to ful- 
fill the overall goals for producing the text. How- 
ever, compared with unique-modifiers, int-modifiers 
are less essential for an NP and they can only be 
added if there are available syntactic slots. 
Since embedding deals with attr-modifiers at both 
a content selection and an abstract realisation level, 
it could coordinate the addition of int-modifiers. 
Therefore, the text planner could consult the embed- 
ding module as to whether a property can be realised 
as an NP modifier, under the constraints from the 
NP type and the unique-modifiers that are already 
there. In other words, the text planner chooses facts 
to satisfy certain goals and the embedding process 
decides if the facts can be realised as NP modifiers 
in an abstract sense. 
We need a generation architecture that allows a 
certain degree of interaction between text planning, 
referring expression generation and embedding. So 
we chose the Genetic Algorithm based text planner 
described in (Mellish et el., 1998). Their task is, 
given a set of "facts and-relations between facts, 'to 
produce a legal RST tree using all the facts and some 
relations. Tile text planning is basically a two step 
process. Firstly sequences of facts are generated by 
applying GA operators, and secondly the rhetorical 
structure trees built from these sequences are evalu- 
ated and the good sequences are kept for producing 
better offspring. 
114 
We extended the text planner by adding a GA op- 
erator called embedding mutation, .which ~andomly 
selects two items mentioning a common entity from 
a sequence and assumes an embedding on them. Em- 
beddings are evaluated together with the other prop- 
erties an RST tree has. In this way, embedding is 
performed uring text planning. The ultimate score 
of a tree is the sum of positive and negative scores 
for all the good and bad properties it bears. Since 
good embeddings are scored higher, they are kept in 
the sequences for producing,better offspring and. are 
very likely to be included in the final output. 
We incorporated the results from the experiment 
into the GA planner by using them as preferences 
for evaluating RST trees. We treated inferrability 
as an input to the system. If a good embedding can 
be formed from two facts connected by an RST re- 
lation (i.e. either of the two cases in Section 3.3.3 
is satisfied and the required syntactic slot is free), 
the embedding is scored higher than the hypotactic 
realisation. However, this emphasis on embedding 
might not be appropriate. In a real application en- 
vironment, other communicative intentions hould 
be incorporated to balance the scoring for differ- 
ent realisations. And generally, inferrability has to 
be implemented based on limited domain-dependent 
knowledge and user configuration. 
5 Conc lus ion  and  fu ture  work  
This paper investigates the use of NR modifiers in 
referring expressions to express certain semantic re- 
lations. This is a commonly used strategy by human 
authors, which has not been explored by an NLG 
system before. Our experiment shows that when the 
conditions for inferrability etc. are satisfied, certain 
relations can be expressed through an NR construc- 
tion as well as a normally used hypotactic onstruc- 
tion with little difference in semantics. This facili- 
tates for an NLG system a way of expressing these 
semantic relations more concisely and subtly which 
could not be achieved by other means. 
Our experiment is restricted in many ways. One 
possible xtension is to use more cue phrases to cover 
a wider range of cases for each semantic relation. In 
reality, the application domain should decide which 
relations need to be tested. 
Re ferences  
Jean Carletta. 1996. Assessing agreement on classi- 
fication tasks: the kappa statistic,. Computational 
Linguistics, 22(2):249-254. 
Hua Cheng. 1998. Embedding new information into 
referring expressions. In Proceedings of COLING- 
A CL '98, pages 1478-1480, Montreal, Canada. 
Robert Dale. 1992. Generating Referring Expres- 
sions: Constructing Descriptions in a Domain of 
Objects and Processes. The MIT Press. 
M.A.K. Halliday. 1985. An Introduction to Func- 
tianal- Grammar. Edward..,&rnold (.PUblishers) 
Ltd., London, UK. 
Evelyn Hatch and Anne Lazaraton. 1991. The Re- 
search Manual: Design and Statistics for Applied 
Linguistics. Newbury House Publishers. 
Eduard Hovy. 1993. Automated iscourse genera- 
tion using discourse structure relations. Artificial 
Intelligence 63, Special Issue on Natural Language 
Processing, 1. 
: ~Pamela: Jordan,:~:~Bonnie: Dorr, _and John Benoit. 
..... 1993: A first-pass approach for evaluating ma- 
chine translation systems. Machine Translation, 
8(1-2):49-58. 
Alistair Knott. 1996. A Data-Driven Methodol- 
ogy for Motivating a Set of..Coherence Relations. 
Ph.D. thesis, Department ofArtificial Intelligence, 
University of Edinburgh, Edinburgh. 
Amichai Kronfeld. 1990. Reference and Compu- 
tation. Studies in Natural Language Processing. 
Cambridge University Press. 
Sebastian Loebner. 1987. Definites. Journal of Se- 
mantics, 4:279-306. 
William Mann and Sandra Thompson. 1987. 
Rhetorical structure theory: A theory of text or- 
ganization. Technical Report ISI/RR-87-190, In- 
formation Sciences Institute, University of South- 
ern California. 
Chris Mellish, Alistair Knott, Jon Oberlander, 
and Mick O'Donnell. 1998. Experiments using 
stochastic search for text planning. In Proceed- 
ings of the 9th International Workshop on Natural 
Language Generation, Ontario, Canada. 
Massimo Poesio. 2000. Annotating a corpus to de- 
velop and evaluate discourse ntity realization al- 
gorithms: Issues and preliminary results. In Pro- 
ceedings of LREC, Athens, May. 
Randolph Quirk, Sidney Greenbaum, Geoffrey 
Leech, and Jan Svartvik. 1985. A Grammar of 
Contemporary English. Longman Group Ltd. 
Donia Scott and Clarisse Sieckenius de Souza. 1990. 
Getting the message across in rst-based text gen- 
eration. In R. Dale, C. Mellish, and M. Zock, edi- 
tors, Current Research in Natural Language Gen- 
eration, pages 47-73. Academic Press. 
James Shaw and Kathleen McKeown. 1997. An ar- 
chitecture for aggregation i  text generation. In 
Proceedings of the Fifteenth International Joint 
Conference on Artificial Intelligence, Poster Ses- 
sion, Japan. 
115 
Optimising text quality in generation from relational databases 
Michael  O 'Donne l l t  (micko@dai .ed .ac .uk) ,  
A l i s ta i r  Knott:~ (a l i k@hermes .o tago .ac .nz ) ,  
Jon  Ober lander ,  ( jon@cogsc i .ed .ac .uk) ,  
Chr i s  Me l l i sh t (chr i sm@dai .ed .ac .uk)  
, D iv is ion  of  In fo rmat ics ,  Un ivers i ty  of  Ed inburgh .  
. . . . .  ~.:D.eparl~me~t.nf: Compulzer?c ience~ ~Otago Univers ity:  
Abst rac t  
This paper outlines a text generation system suited 
to a large class of information sources, relational 
databases. We focus on one aspect of the problem: 
the additional information which needs to be spe- 
cified to produce reasonable text quality when gen- 
erating from relational databases. We outline how 
databases need to be prepared, and then describe 
various types of domain semantics which can be used 
to improve text qualify. 
1 In t roduct ion  
As the problems of how we generate text are gradu- 
ally solved, a new problem is gaining prominence 
- where do we obtain the information which feeds 
the generation. Many domain models for existing 
generation systems are hand-crafted for the specific 
system. Other systems take advantage of existing 
information sources. 
A good information source for text generation 
resides in the vast number of relational databases 
which are in use around tile world. These resources 
have usually been provided for some reason other 
than text generation, such as inventory manage- 
ment, accounting, etc. However, given that the in- 
formation is on hand, it can be of value to conuect 
these databases to text generation facilities. 
The benefits include natural anguage access to in- 
formation which is usually accessed in tabular form, 
which can be difficult to interpret. Natural Lan- 
guage descriptions are easier to read, can be tailored 
to user types, and can be expressed in different lan- 
guages if properly represented. 
This paper outlines the domain specification lan- 
guage for the ILEX text g~neration system, (for 
Intelligent Labelling Explorer). 1
ILEX is a tool for ?dynamic browsing of database- 
defined information: it allows a user to browse 
through the information in a database using hyper- 
1Earlier ILEX papers have been based on Ilex 2.0, which 
was relatively domain-dependent.  This  paper is based around 
version 3.0 of ILEX, a re-draft to make the system domain- 
independent, and domain acquisition far easier. The ILEX 
project was supported by EPSRC grant GR/K53321.  
text. ILEX generates descriptions of database ob- 
jects on the fly, taking into account he user's con- 
text of browsing. Figure 1 shows the ILEX web in- 
terface, as applied to a museum domain, in this case 
the Twentieth Century Jewellery exhibition at the 
the National Museum of Scotland. 2 The links to 
related database objects are also automatically gen- 
erated. ILEX has been applied to other domains, in- 
cluding personnel (Nowson, 1999), and a sales cata- 
logue for computer systems and peripherals (Ander- 
son and Bradshaw, 1998). 
One of the advantages of using NLG for database 
browsing is that the system can keep track of what 
has already been said about objects, and not repeat 
that information on later pages. Appropriate refer- 
ring expressions can also be selected on the basis 
of the discourse history. The object descriptions can 
be tailored to the informational interests of the user. 
See Knott et al (1997) and Mellish et al (1998) for 
more information on these aspects of ILEX. 
In section 2, we consider some systems related to 
the ILEX system. Section 3 describes the form of 
relational database that ILEX accepts as input. Sec- 
tion 4 outlines what additional information - domain 
semantics - needs to be provided for coherent ext 
production from the database, while section 5 de- 
scribes additional information which can be provided 
to improve the quality of the text produced. 
2 Re la ted  Work  
It should be clear that the task we are discussing is 
very distinct from the task of response generation in 
a natural language interface to a database (e.g., see 
Androutsopoulos et al (1995)). ' In such systtems, 
the role of text planning is quite simple or absent, 
usually dealing with single sentences, or in the most 
? ? complex systems;~ a:single:sentence ,answer ~with an 
additional clause or two of supporting information. 
ILEX is not a query response generation system, 
it is an object description system. It composes a full 
text, at whatever size, with the goal of making that 
text a coherent discourse. 
2The authors thank the museum for making their database 
available: 
133 
Sflver.A~nd Ename . :  
!- S.~.v~ t !~s ,  w i~ blu~-~e~.~i/e.1 . ! 
' ) . :~v{ .  ,EX :~- - : :  . . . . . . . . . . . . . .  )' . . . . . . .  " t ' ': i:: ' :  :!:i 
? lessie-I~-X~g.,l~. Place of,. ; 
? This Jewel !s apel'l.d~mat-neckla~ ililitwaS . . 
I madebZ,aSa~h de:a,S~caUed-Jesae M , 
l<: gin,g:ilt~bnedlhe f~mrRemStn:tht~:case.,::_: , '  
:: ? / lowers reseri~A a~ai~t  i t - I t  is tn ~e Arts :~ud,  
. Crafts:style and was made t1~ :lgfl~ It has an . . . .  
elaborate aesign; specifically It h~ floral mows.  
: :;::anlllustrat~too, In fact., shg did qttite, a' l~ of,-: : 
" differei~tl~rpes of creative Wark;/cwdleiTls ? : i:. 
:. ; :  :i'; ~:t~n Arts. amt Craft#Style . ),.::.:):i:.: 2,} :,i.: 'i:' :" 
:'::' ; :-'? ;~.~,~t,~I,t~,,/~l.a~.~_~': ~" ; : "  : : ; " : " ,  -'; 7.; ...:. 
."  ' .:,L'~n Ai'ts aiid Crafts:s~lgne~iil~e -: :.::'..': " i 
J 
.; ... (; . 
Figure 1: Browsing Object Descriptions 
In this regard, ILEX should be more fruit- 
fully compared with text generation systems such 
as GOSSIP (Carcagno and Iordanskaja, 1993), 
PEBA (Milosavljevie, 1997; Milosavljevic, 1999), or 
POWER (Dale et al, 1998), systems which build an 
extended text fl'om an underlying database. 
ILEX 3.0 has been developed to be domain in- 
dependent, to handle relational databases from any 
domain, as long as the information is provided in the 
required format. The first two of the systems above 
are single domain systems. T:he third, POWER,  is 
an extension of PEBA to handle a new domain. It 
is not clear however whether the resulting system is 
.. itself domain-dependent or not. 
This last system is perhaps the best comparison 
for the ILEX system, since it also generates de- 
scriptions of museum objects from an underlying 
database. In that paper, the main focus is on the 
problem of extracting out usable information from 
badly structured databases (as often provided by 
museulns), and on generating texts using only only 
this information (plus some linguistic knowledge). 
The present paper differs from this approach by as- 
suming that information is already available in a nor- 
malised relational database. We observe, as do Dale 
et al (1998), that texts generated from this inform- 
ation alone are quite poor in quality. We go one 
step further by examining what additional informa- 
tion can be provided to improve the quality of the 
text to a reasonable l vel. 
The ILEX system has been implemented to be 
flexible in regards to the available domain inform- 
ation. With a bare minimum, the system provides 
poor quality texts, but as the domain developer ex- 
.tends-the domain semantics, the quahty of.texts im- 
proves, up to a point where users sometimes nfistake 
ILEX-generated texts for human-authored texts. 
3 The Structure of a Relational 
Database 
Databases vary widely in form, so we have assumed 
a fairly" standard relational database format. 
134 
3.1 Entity Files 
:.The database consists of .a number:.:of ~ntity files, 
each file providing the records for a different entity 
type. Each record (row) in the entity file defines a 
unique entity. The columns define attributes of the 
entities. In a museum domain, we might have an 
entity file for museum artifacts, another for people 
involved with the artifacts (designers, owners, etc.), 
another for locations, etc. See figure 2 for a sample 
entity file for the Jewellery domain. Given the wide 
.range of database formats..a~vailable, !LEX ~sumes 
a tab-delimited format for database files. 
ILEX imposes two requirements on the entity files 
it uses: 
1. Single field key: while relational databases of- 
ten use multiple attributes to form a unique key 
(e.g., name and birthdate), ILEX requires that 
each entity have a unique identifier in a single 
attribute. This identifier must be under a field 
labelled ID. 
2. Typing of entities: ILEX depends trongly on a 
type system. We require that each entity record 
provides a type for the entity in a field labelled 
Class. 
Some other attribute labels are reserved by the 
system, allowing ILEX to deal intelligently with 
them, including Name, Short-Name and Gender. 
3.2 L ink Fi les 
In some cases, an entity will have multiple fillers of 
an attribute, for instance, a jewellery piece may be 
made of any number of materials. Entity files, with 
fixed record structure, cannot handle such eases. 
The standard approach in relational databases i to 
provide a link file for each case where multiple fillers 
are possible. A link file consists of two columns only, 
one identifying the entity, the other identifying the 
filler (the name of the attribute is provided in the 
first line of the file, see figure 3). 
We are aware that the above specification repres- 
ents an impoverished view of relational databases. 
Many relational databases provide far more than 
simple entity and link files. However, by no means 
all relational databases provide more than this, so 
we have adopted the lowest common denominator. 
Most relational databases can be exported in a form 
which meets our requirements. 
3.3 Terminology 
In the following discussion, we will use the following 
terminology: 
* Predicate: each column of an entity file defines 
a predicate. Class, Designer and Date are thus 
predicates introduced in figure 2. Each link file 
also defines a predicate. 
? Record: each row of an entity table provides the 
attributes o f  a: single.,entity.: The row is termed 
a record in database terminology. 
? Fact: each entry in a record defines what we 
call a fact about that entity, a A fact consists o f  
three parts: its predicate name, and two argu- 
ments, being the entity of the record, and the 
filler of the slot. 
? ARC1: the first argument of a fact, the entity 
the  fact is about. 
. ARC2: the second argument of a fact, the filler 
of the attribute for the entity. 
4 Spec i fy ing  the  Semant ics  o f  the  
Database  
A database itself says nothing about the nature of 
the contents of each field in the database. It might 
be a name, a date, a price, etc. Similarly for the 
field label: the field label names a relation between 
the entity represented by the record and the entity 
represented by the filler. However, without further 
specification, we do not know what this relationship 
entails, apart from the label itself, e.g., 'Designer'. 
Before we can begin to process a database intelli- 
gently, we need to define the 'semantics' of the data- 
base. This section will outline how this is done in the 
ILEX case. There has been some work on automatic 
acquisition of database semantics, uch as in the con- 
struction of taxonomies of domain entity types (see 
Dale et al (1998) for instance). However, it is diffi- 
cult to perform this process reliably and in a domain- 
independent manner, so we have not attempted to 
in this case. The specification of domain semantics 
is still a manual process which has to be undertaken 
to link a database to the text generator. 
To use a database for generation, additional in- 
formation of several kinds needs to be provided: 
1. Taxonomic organisation: supplying of types for 
each database ntity, and organisation of these 
types into taxonomies; 
2. Taxonomic lexification: specif~'ing how each do- 
main type is lexified; 
3. Data type off attribute fillers: telling the system 
to expect the filler of a record slot to be an 
entity-id, a string, a date, etc. 
4. Domain type specification:specifying What do- 
main type the slot filler can be assumed to be. 
Each of these aspects of domain specification will 
be briefly described below. 
3Excepting the first column, which provides the entity-id 
for tile record. 
135 
 Class brooch -necklace necklace Designer KingO1 "KingO1 ChanelO1 Style J___190~ A-rt-Deco : ~_~_~ Art-Noveux London Paris 
L_ 
Sponsor 
Liberty01 
Figure 2: A Sample from an Entity file 
\ [ ~ .  Material 
Figure 3: A Sample from a Link file 
(def-basic-type 
:domain jewellery-domain 
:head jewellery 
:mn-link 3D-PHYS-0BJECT) 
(def-taxonomy 
:type jewellery 
:subtypes (neck-jewellery wrist-jewellery 
pin-jewellery pendant buckle 
earring earring-pair finger-ring 
ringset watch button dress-clip 
hat-pin)) 
Figure 4: Defining Taxonomic Knowledge 
4.1 Taxonomic  Organ isat ion  
ILEX requires that the entities of the domain are or- 
ganised under a domain taxonomy. The user defines 
a basic type (e.g., jewellery), and then defines the 
sub-types of the basic-type, and perhaps further sub- 
classification. Figure 4 shows the lisp forms defining 
a basic type in the jewellery domain, and the sub- 
classification of this type. The basic type is also 
mapped onto a type (or set of types) in the concept 
ontology used for sentence generation, a version of 
Penman's Upper Model (Bateman, 1990). This al- 
lows the sentence generator to reason about the ob- 
jects it expresses. 
Taxonomic organisation is important for several 
reasons, including among others: 
1. Expressing Entities: each type can be related to 
lexical i tems'to use,to-express that  type (e.g., 
linking the type brooch to a the lexical item for 
"brooch". If no lexical item is defined for a type, 
a lexical item associated with some super-type 
can be used instead. Other aspects of the ex- 
pression of entities may depend on the concep- 
tual type, for instance pronominalisation, deixis 
(e.g., mass or count entities), etc. 
2. Supporting Inferences and Generalisations: 
ILEX allows the user to assert generalisations 
about types, e.g., that Arts and Crafts jewellery 
tends to be made using enamel (see section 5.4). 
The type hierarchy is used to check whether a 
particular generalisation is appropriate for any 
given instance. 
The earlier version of ILEX, Ilex2.0, allowed the 
full representational power of the Systemic formal- 
ism for representing domain taxonomies, including 
cross-classification, and multiple inheritance (both 
disjunctive and conjunctive). However, our exper- 
iences with non-linguists trying to define domain 
models showed us that the more scope for expres- 
sion, the more direction was needed. We thus sim- 
plified the formalism, by requiring taxonomies to be 
simple, with no cross-classification r multiple inher- 
itance. We felt that the minor loss of expressivity 
was well balanced by the gain in simplicity for do- 
main developers. 
4.2 Type Lexi f icat ion 
To express each database ntity, it is essential to be 
able to map from its defined type, to a noun to use 
in a referring expression, e.g., this brooch. 
Ilex comes with a basic lexicon already provided. 
covering the commonly occurring words. Each entry 
defines the svntactic and morphological information 
required for sentence generation. For these items, 
the domain developer needs to provide a simpl e map- 
ping from domain type to lexical item, for instance, 
the following lisp form specifies that the domain type 
location should be lexified by the lexical item whose 
id is location=noun: 
(lexify location location-noun) 
For those lexical items not already defined, the do- 
main developer needs to provide in addition lexical 
item definitions for the nouns expressing the types 
in their domain. A typical entry has the form shown 
in figure 5. 
136 
(def-lexical-item 
:name professor-noun 
:spelling "professor" 
:grammatical-features (common-noun count-noun) 
) 
Figure 5: A Sample Lexical item Specification 
. . . .  (defobject-structurejewellery- " ..... 
:class :generic-type 
:subclass :generic-type 
:designer :entity-id 
:style :entity-id 
:material :generic-type 
:date :date 
:place :string 
:dimension :dimension) 
Figure 6: Specifying Field Semantics 
(def-predicateClass 
:expression (:verb be-verb) 
) 
Figure 8: Simple Fact Expression 
4.3 Data Type of Slot Fillers 
Each field in a database record contains a string of 
characters. It is not clear whether this string is an 
identifier for another domain entity, a string (e.g., 
someone's urname), a date, a number, a type in 
the type hierarchy, etc. 
ILEX requires, for each entity file, a statement as 
to how the field fillers should be interpreted. See 
figure 6 for an example. 
Some special filler types have been provided to 
facilitate the import of structured ata types. This 
includes both :date and :dimension in the current 
example. Special code has been written to convert 
the fillers of these slots into ILEX objects. Other 
special filler types are being added as needed. 
4.4 Domain  Type  o f  Slot Fi l lers 
The def-predicate form allows the domain developer 
to state what type the fillers of a particular field 
should be. This not only allows for type checking, 
but also allows the type of an entity to be inferred 
if not otherwise provided. For instance, by assert- 
ing that fillers of the Place field should of type city, 
the system can infer that "London" is a city even if 
London itself has no database record. See figure 7. 
(def-predicate Place 
:argl jewellery 
:arg2 city 
) 
Figure 7: Speci~'ing Predicate Fillers 
4.5 Summary  
..... '.:~With:just chisvmuch-semantics~specified,. ILEX e-an 
generate very poor texts, but texts which convey 
the content of the database records. In the next 
section, we will outline the extensions to the domain 
semantics which are needed to improve the quality 
of the text produced by ILEX. 
5 Extending Domain Semantics for 
Improved Text Quality 
So far we have discussed only the simplest level of 
domain semantics, which allows a fairly direct ex- 
pression of domain information. ILEX allows the 
domain developer to provide additional domain se- 
mantics to improve the quality of the text. 
5.1 Expression of Facts 
Unless told otherwise, ILEX will express each fact in 
a simple regular form, such as The designer of this 
brooch is Jessie M. King, using a template form4: 
The <predicate> of <entity-expression> 
is <filler-expression>. 
However, a text consisting solely of clauses of this 
form is unnatural, and depends on the predicate la- 
bel being appropriate to the task (labels like given-by 
will produce nonsense sentences). 
To produce better text, ILEX can be told how 
to express facts. The domain developer can provide 
an optional slot to the &f-predicate form as shown 
in figure 8. The expression specification first of all 
defines which verb to use in the expression. By de- 
fault, the ARG1 element is mapped onto the Sub- 
ject, and the ARG2 onto the Object. Default val- 
ues are assumed for tense, modality, polarity, voice. 
finiteness, quantification, etc., unless otherwise spe- 
cified. So, using the above expression specification, 
the Class fact of a jewel would be expressed by a 
clause like: This item is a brooch. 
To .produce less .standard expressions, we need to 
modify some of the defaults. A more complex ex- 
pression specification is shown in figure 9, which 
would result in the expression such as: For further 
information, see Liberty Style Guide No. 326: 
4ILEX3.0  borrowed this use of a default  express ion tem- 
p late  from the POWER system (Dale et al, 1998). In previ-  
ous vers ions of ILEX,  all facts were expressed by full NLG as 
exp la ined below. 
137 
(def-predicate Bib-Note 
:argl jewellery 
:expression ( 
:adjunctl "for further information" 
:mood imperative 
:verb see-verb 
:voice active) 
Figure 9: More Complex Fact Expression 
The expression form is used to construct a par- 
tial syntactic specification, which is then completed 
using the sentence generation module of the WAG 
sentence generator (O'Donnell, 1996). 
With the level of domain semantics pecified so 
far, ILEX is able to produce texts such as the two be- 
low, which provides an initial page describing data- 
base entity BUNDY01, and then a subsequent page 
when more information was requested (this from the 
Personnel domain (Nowson, 1999)): 
o Page  1: Alan Bundy is located in room F1, 
which is in South Bridge. He lectures a course 
called Advanced Automated Reasoning and is in 
the Institute for Representation and Reasoning. 
He is the Head of Division and is a professor. 
* Page  2: As already mentioned, Alan Bundy lec- 
tures Advanced Automated Reasoning. AAR is 
lectured to MSc and AI4. 
This expression specification form has been de- 
signed to limit the linguistic skills needed for domain 
developers working with the system. Given that the 
domain developers may be museum staff, not com- 
putational linguists, this is necessary. The notation 
however allows for a wide range of linguistic expres- 
sions if the full range of parameters are used. 
5.2 User  Adapt ion  
To enable the system to adapt its content to the 
type of user, the domain developers can associate 
information with each predicate indicating the sys- 
tem's view of the predicate's interest, importance, 
etc., to the user. This information is added to the 
d@predicate form, as shown in figure 10. 
The user annotations allowed by ILEX include: 
1. Interest: how interesting does the system judge 
the information to be to the user; 
2. Importance: how important is it to the system 
that the user reads the information; 
3. Assimilation: to what degree does the system 
judge the user to already know the infornlation: 
.<def~predicate Designer 
. o .  
:importance ((expert lO)(default 6)(child 5)) 
:interest ((expert lO)(default 6)(child 4)) 
:assimilation ((expert O)(default O)(child 0)) 
:assim-rate ((expert l)(default l)(child 0.5)) 
) 
Figure 10: Specifying User Parameters 
4. Assimilation Rate: How quickly does the sys- 
tem believe the user will absorb the information 
when presented (is one presentation enough?). 
This information influences what content will be 
expressed to a particular user, and in what or- 
der (more relevant on earlier pages). Information 
already assimilated will not be delivered, except 
when relevant for other purposes (e.g., when refer- 
ring to the entity). If no annotations are provided, 
no user customisation will occur. 
The values in ILEX's user models have been set 
intuitively by the implementers. While ideally these 
values would be derived through user studies, our 
purpose was purely to test the adaptive mechanism, 
and demonstrate that it works. We .leave the devel- 
opment of real user models for later work. 
ILEX has opted out of using adaptive user model- 
ling, whereby the user model attributes are adapted 
as a result of observed user choices in the web inter- 
face. We leave this for future research. 
5.3 Compar i sons  
When describing an object, it seems sometimes use- 
ful to compare it to similar articles already seen. 
With small addition to the domain specification, 
ILEX can compare items (an extension by Maria Mi- 
losavljevic), as demonstrated in the following text: 
This item is also a brooch. Like the previ- 
ous item, it was designed by King. How- 
ever, it differs from the previous item in 
that it is made of gold and enamel, while 
the previous brooch was made of silver and 
enamel. 
For ILEX to properly compare two entities, it 
needs to Mmw how the various.attributes of the en- 
tity can be compared (nominal, ordinal, scalar, etc.). 
Again, information can be added to the d@predicate 
for each predicate to define its scale of comparabil- 
ity. See Milosavljevic (1997) and (1999) for more de- 
tail. Figure 11 shows the additions for the Designer 
predicate. Comparisons introduce several RST re- 
lations to the text structure, including rst-contrast, 
rst-similarity and rst-whereas. 
138 
(def-predicate Designer 
:variation (string i) 
:scale nominal 
) 
Figure lh Specifying Predicate Comparability 
(def-defeasible-rule 
? :qv ($jewel jewellery) ....... 
:lhs (some ($X (style $jewel $X)) 
(arts-and-crafts SX))) 
:rhs (some ($X (made-of Sjewel SX)) 
(enamel SX))) 
Figure 12: Specifying Generalisations 
5 . 4  G e n e r a l i s a t i o n s  
We found it useful to allow facts about general types 
of entities to be asserted, for instance, that Arts and 
Crafts jewellery tend to be made of enamel. These 
generalisations can then be used to improve the qual- 
ity of text, producing object descriptions as in the 
following: 
This brooch is in the Arts and Crafts style. 
Arts and Crafts jewels tend to be made of 
enamel. However, this one is not. 
These generalisations are defined using defeasible 
implication - similar to the usual implication, but 
working in terms of few, many, or most rather than 
all or none. They are entered in a form derived 
from first order predicate calculus, for instance, see 
figure 12 which specifies that most Arts and Crafts 
jewellery uses enamel. 
ILEX find each instance which matches the gen- 
eral type (in this case, instances of type jewellery 
which have Arts and Crafts in the Style role). If 
the fact about the generic object has a correspond- 
ing fact on the instantial object, an exemplification 
relation is asserted between the facts. Otherwise, 
a ?concession relation is asserted. See Knott et al 
(1997) for more details on this procedure. 
6 Summary  
While observing people trying to convert an earlier 
ILEX system to a new domain, we noted the diffi- 
culty they had. To avoid these problems, we under- 
took to re-implement the domain specification as- 
pects of ILEX to simplify the task. 
Towards this end, we have followed a number of 
steps. Firstly, we reconstructed ILEX to be domain 
- Taxonomies 
- Lexification of Types 
- Filler Domain Type Information 
- Filler Data Type Information 
OBLIGATORY 
- Predicate Expression 
- Comparison Information 
- Generalisations 
- User Annotations 
OPTIONAL 
Figure 13: Obligatory and Optional Steps in Domain 
Specification 
independent, with all domain information defined in 
declarative resource files. This means that domain 
developers do not have to deal with code. 
Secondly, we built into ILEX the ability to import 
entity definitions directly from a relational database 
(although with some restrictions as to its form). 
A database by itself does not provide enough in- 
formation to produce text. Domain semantics is re- 
quired. We have provided a system of incremental 
specification of this semantics which allows a domain 
developer to hook up adynamic hypertext interface 
to a relational database quickly, although producing 
poor quality text. Minimally, the system requires 
a domain taxonomy, information on lexification of 
types, and specification of the data type of each re- 
cord field. 
Additional effort can then improve the quality of 
text up to a quite reasonable l vel. The additional 
information can include: specification of predicate 
expression, and specifications supporting comparis- 
ons, user adaption, and generalisations. 
Figure 13 summarises the obligatory and optional 
steps in domain specification in ILEX. 
Simplifying the domain specification task is a ne- 
cessity as text generation systems move outside of 
research labs and into the real world, where the 
domain developer may not be a computational lin- 
guist, but a museum curator, personnel officer or 
wine salesman. ~ have tried to take a step towards 
making their task easier. 
Re ferences  
Gail Anderson and Tim Bradshaw. 1998. ILEX: 
The intelligent labelling explorer: Experience of 
Building a Demonstrator for the Workstation Do- 
main. Internal Report, Artificial Intelligence Ap- 
plications tnstitute,University of Edinburgh. 
I. Androutsopoulos, G.D. Ritchie, and P. Thanisch. 
1995. Natural language interfaces to databases - 
an introduction. Natural Language Engineering, 1
(1):29-81. 
John Bateman. 1990. Upper modeling: organiz- 
ing knowledge for natural language processing. 
In Proceedings of the Fifth International Work- 
139 
shop on Natural Language Generation, Pitts- 
burgh, June. 
Denis Carcagno and Lidija Iordanskaja. 1993. Con- 
tent determination a d text structuring: two in- 
terrelated processes. In Helmut Horocek and Mi- 
chael Zock, editors, New Concepts in Natural Lan- 
guage Generation, Communication i Artificial 
Intelligence Series, pages 10 - 26. Pinter: London. 
Robert Dale, Stephen J Green, Maria Milosavljevic, 
CEcile Paris, Cornelia Verspoor, and Sandra Wil- 
liams. 1998. The realities of generating natural 
language from databases. In "Proceedings of the 
11th Australian Joint Conference on Artificial In- 
telligence, Brisbane, Australia, 13-17 July. 
Alistair Knott, Michael O'Donnell, Jon Oberlander, 
and Chris Mellish. 1997. Defeasible rules in con- 
tent selection and text structuring. In Proceedings 
of the 6th European Workshop on Natural Lan- 
guage Generation, Gerhard-Mercator University, 
Duisburg, Germany, March 24 - 26. 
Chris Mellish, Mick O'Donnell, Jon Oberlander, and 
Alistair Knott. 1998. An architecture for oppor- 
tunistic text generation. In Proceedings of the 
Ninth International Workshop on Natural Lan- 
guage Generation, Niagara-on-the-Lake, Ontario, 
Canada. 
Maria Milosavljevic. 1997. Augmenting the user's 
knowledge via comparison. In Proceedings of the 
6th International Conference on User Modelling, 
pages 119-130, Sardinia, 2-5 June. 
Maria Milosavljevic. 1999. Maximising the Co- 
herence of Descriptions via Comparison. Ph.D. 
thesis, Macquarie University, Sydney, Australia. 
Scott Nowson. 1999. Acquiring ILEX for a Per- 
sonnel Domain. Honours Thesis, Artificial Intel- 
ligence, University of Edinburgh. 
Michael O'Donnell. 1996. Input specification i the 
wag sentence generation system. In Proceedings of 
the 8th International Workshop on Natural Lan- 
guage Generation, Herstmonceux Castle, UK, 13- 
15 June. 
140 - ' 
Capturing the Interaction between Aggregation and Text 
Planning in Two Generation Systems 
Hua Cheng and Chris Mel l i sh  
Division of Informatics, University of Edinburgh 
80 South Bridge, Ed inburgh EH1 1HN, UK 
huac, chrism@dai, ed. ac. uk 
Abst ract  
In natural language generation, different gener- 
ation tasks often interact with each other in a 
complex way. We think that how to resolve the 
complex interactions inside and between tasks 
is more important to the generation of a co- 
herent text than how to model each individual 
factor. This paper focuses on the interaction be- 
tween aggregation and text planning, and tries 
to explore what preferences exist among the fea- 
tures considered by the two tasks. The prefer- 
ences are implemented in two generation sys- 
tems, namely ILEX-TS and a text planner us- 
ing a Genetic Algorithm. The evaluation em- 
phasises the second implementation and shows 
that capturing these preferences properly can 
lead to coherent ext. 
1 D iscourse  coherence  and  
aggregat ion  
hi NLG, theories based on domain-independent 
rhetorical relations, in particular, Rhetorical 
Structure Theory (Mann and Thompson, 1987), 
are often used in text planning, whose task 
is to select the relevant information to be ex- 
pressed and organise it into a hierarchical struc- 
ture which captures certain discourse prefer- 
ences such as preferences for the use of rhetori- 
cal relations. 
In the theory of discourse structure developed 
by Grosz and Sidner (1986), each discourse seg- 
ment exhibits two types of coherence: local co- 
herence among utterances inside the segment, 
and global coherence between this segment and 
other discourse segments. Discourse segments 
are connected by either a dominaTzce relation or 
a satisfaction-precedence relation. 
There has been an effort to synthesise tile 
two accounts of discourse structure. X loser and 
Moore (1996) argue that the two theories have 
considerable common ground, which lies in the 
correspondence between the notion of domi- 
nance and nuclearity. It is possible to map 
between Grosz and Sidner's linguistic structure 
and RST text structure, and relation-based co- 
herence and global coherence capture similar 
discourse properties. 
Oberlander et al (1999) propose a dis- 
tinction between two types of discourse coher- 
ence: proposition-based coherence, which ex- 
ists between text spans connected by RST re- 
lations except for object-attribute laboration, 
and entity-based coherence, which exists be- 
tween spans of text in virtue of shared entities. 
entity-based coherence captures the coherence 
among adjacent propositions, which resembles 
local coherence in Grosz and Sidner's theory. 
To generate a coherent ext, the text planning 
process must try to achieve both local (entity- 
based) and global (relation-based) coherence. 
Since the task of aggregation is to combine sinl- 
ple representations together to form a complex 
one, which in the mean time leads to a shorter 
text as a whole, aggregation could affect the or- 
dering of text plans and the length of the whole 
text.. Therefore, it is closely related to tile task 
of maintaining both types of coherence. Here 
we treat embedding as a type of aggregation. 
There is no consensus as to where aggregation 
should happen or how it is related to other gen- 
eration processes (Wilkinson, 1995; Reape and 
Mellish, 1999). In many NLG systems, aggre- 
gation is a post planning process whose prefer- 
ences are only partially taken into account by 
the text planner. 
1.1 Aggregat ion  and local coherence  
In a structured text plan produced by the text 
planner, local coherence is normally maintained 
through the ordering of the selected facts, where 
186 
certain types of center transition (e.g. cen- 
ter continuation) :are preferred :over:others (eig,. -; 
center shifting) (Centering Theory (Grosz et al, 
1995)). Aggregation may affect text planning 
by taking away facts from a sequence featuring 
preferred center movements for embedding or 
subordination. As a result, the preferred cen- 
ter transitions in the original sequences could 
be cut off. For example, comparing the first 
two descriptions of.a necklace in Figure 1, 2 is 
less coherent than 1 because of the shifting from 
the description of the necklace to that of the de- 
signer, which is a side effect of embedding. 
Since the centers of sentences are normally 
NPs and embedding adds non-restrictive com- 
ponents into an NP, it could affect the way a Cb 
is realised (e.g. preventing it from being a pro- 
noun). As pointed out in (Grosz et al, 1995), 
different realisations (e.g. pronoun vs. definite 
description) are not equivalent with respect o 
their effect on coherence. Therefore, embedding 
could influence local coherence by forcing a dif- 
ferent realisation from that preferred by Center- 
ing Theory. There is an obvious need to balance 
the consideration for local coherence and stylis- 
tic preferences. 
1.2 Aggregat ion  and  global coherence  
Different types of aggregation eed to be com- 
patible among themselves, in particular, embed- 
ding and semantic parataxis and hypotaxis. Us- 
ing the abstraction of RST, semantic parataxis 
concerns facts related by explicit multi-nuclear 
semantic relations (e.g. sequence and contrast) 
or by implicit connections like parallel common 
parts. If two facts have at least two identi- 
cal parallel components, we say that a conjunct 
or disjunct relation exists between them, and 
these relations are multi-nuclear relations. Se- 
mantic hypotaxis concerns facts connected by 
nucleus-satellite r lations (e.g. cause). Seman- 
tic parataxis and hypotaxis feature in relation- 
based coherence and they depend on the text 
planner to put the related facts next to each 
other in order to perform a combination. 
(Cheng, 1998) describes interactions that 
need to be taken into account in aggrega- 
tion. Firstly, complex embedded components 
like non-restrictive clauses may interrupt tile 
semantic onnection or syntactic similarity be- 
tween a set of clauses. Secondly, the possibilities 
of other types of aggregation should be consid- 
ered for both the main fact and the fact to be 
-embedded . uring .:embedding .decision. maki ng... 
And thirdly, performing parataxis inside a hy- 
potaxis could convey wrong information. 
We argue that the effect of aggregation is not 
limited to the particular NP or sentence where 
aggregation happens, but to the coherence of 
the text as a whole. The complex interactions 
demand the features of aggregation to be eval- 
uated .together with other coherence~ features 
and aggregation to be planned as a part of text 
structuring. This requires better coordination 
between aggregation and other generation tasks 
as well as among different ypes of aggregation 
than is present in current NLG systems. 
In this paper, we describe how to capture the 
above interactions as preferences among related 
features, and the implementation f the prefer- 
ences in two very different generation architec- 
tures to produce descriptions of museum objects 
on display. 
2 P re ferences  among coherence  
features  
We claim that it is the relative preferences 
among features rather than the absolute magni- 
tude of each individual one that play the crucial 
role in the production of a coherent text. In this 
section we discuss the preferences among fea- 
tures related to text planning, based on which 
those for embedding can be introduced. 
2.1 P re ferences  for global coherence  
A semantic relation other than conjunct or dis- 
junet is preferred to be used whenever possible 
because it usually conveys interesting informa- 
tion about domain objects and leads to a coher- 
ent text span. If a conjunct relation shares a fact 
with a semantic relation, the conjunct should 
be suppressed. For example, in 3 of Figure 1. 
apart from other relations, there is an amplifica- 
tion relation signalled by indeed and a conjunct 
between the last two propositions. Compared 
with 3, 4 is less preferred because it misses tile 
amplification and the center transition from the 
necklace to an Arts and Crafts style jewel is not 
so smooth, whereas 3 expresses the amplifica- 
tion explicitly and the conjunct implicitly. 
However, a semantic relation can only be used 
if the knowledge assumed to be shared by the 
hearer is introduced in the previous discourse 
(Mellish et al. 1998a). \Ve assume the strategy 
187 
1. This necklace is in the Arts and Crafts style. Arts and Crafts style jewels usually have an elaborate 
design. They tend to have floral motifs. For instance, this necklace has floral motifs. It was designed 
by Jessie King. King was Scottish. She once lived in London. 
2. This necklace, which was designed by Jessie King, is in the Arts and Crafts style. Arts and 
Crafts style jewels usually have an elaborate design. They tend to have floral motifs. For instance, 
this necklace has floral motifs. King was Scottish. She once lived in London. 
3. The necklace is in the Arts and Crafts style. It is set with jewels in that it features cabuchon 
stones. Indeed, an Arts and Crafts style jewel usually uses cabuchon stones. It usually uses oval 
stones. 
4. The necklace is in the Arts and Crafts style. It is set. with jewels in that it features cabuchon 
stones. An Arts and Crafts style jewel usually uses cabuchon stones and oval stones. 
Figure 1: Aggregation examples 
of (Mellish et al, 1998a) which uses a joint re- 
lation to connect every two text spans that do 
not have a semantic relation other than object- 
attribute elaboration and conjunct/disjunct in 
between. Although joint is not preferred when 
other relations are present, it is better than 
missing presuppositions or embedding a con- 
junct relation inside a semantic relation. There- 
fore, we have the following heuristics, where 
"A>B" means that A is preferred over B. 
Heur i s t i c  1 Preferences among features for 
global coherence: 
a semantic relation > Conjunct/Disjunct > 
Joint > presuppositions not met 
Joint > Conjunct inside a semantic relation 
2.2 Pre ferences  for local coherence  
One way to achieve local coherence is to con- 
trol center transitions among utterances. In 
Centering Theory, Rule 2 specifies preferences 
among center movement in a locally coherent 
discourse segment: sequences of continuation 
are preferred over sequences of retaining; which 
are then preferred over sequences of shifting. 
Brennan et el. (1987) also describe typical 
discourse topic movements in terms of center 
transitions between pairs of utterances. They 
argue that the order of coherence among the 
transitions is continuing > retaining > smooth 
shifting > abrupt shifting. Instead of claiming 
that these are the best models, we use them 
simply as an example of linguistic models being 
used for evaluating features of text planning. 
A type of center transition that appears fre- 
quently in descriptive text is that the descrit)- 
tion starts with an object, but shifts to associ- 
ated objects or perspectives of that object. This 
is a type of abrupt shifting, but it is appropriate 
as long as the objects are highly associated to 
the original object (Schank, 1977). This phe- 
nomenon is handled in the system of (Grosz, 
1977), where subparts of an object are included 
into a focus space as the implicit foci when the 
object itself is to be included. 
We call this center movement an associate 
shifting, where the center moves from a trig- 
ger entity to a closely associated entity. Our 
informal observation from museum descriptions 
shows that associate shifting is preferred by hu- 
man writers to all other types of center move- 
ments except for continuation. There are two 
types of associate shifting: where the trigger 
is in the previous utterance or two entities in 
two adjacent utterances have the same trigger. 
There is no preference between them. 
Heuristic 2 summarises the above preferences. 
We admit that these are strict heuristics and 
that human texts are sometimes more flexible. 
Heur i s t i c  2 Preferences among center transi- 
tions: 
Continuation > Associate shifting > RetaiTI- 
ing > Smooth shifting > Abrupt shifting 
2.3 Pre ferences  for both  types  of 
coherence 
Two propositions can be connected in differ- 
ent ways, e.g. through a semmxtic relation or a 
smooth center transition only. Since a semantic 
relation is always preferred, we have the follow- 
ing heuristic: 
Heur i s t i c  3 Preferences among semantic rela- 
tions and center transitions: 
a semantic relation > Joint ? Continuation 
188 
2.4 P re ferences  for embedd ing  Good embedding > Normal embedding > 
We distinguish between.a.-good,.rwrmal,and-bad Jo int  > Bad embedding . . . . .  =:--..~ .:-- ~ .--:.: ........ 
embedding based on the features it bears. We do Continuation + Smooth shifting + Joint > 
not claim that the set of features is complete. 
In a different context, more criteria might have 
to be considered. 
A good embedding is one satisfying all the fol- 
lowing conditions: 
1. The referring part is an indefinite, a demon- 
strative or a bridging description (as de- 
fined in (Poesio et al, 1997)). 
2. The embedded part can be realised as an 
adjective or a prepositional phrase (Scott 
and de Souza, 1990). 
3. In the resulting text, the embedded part 
does not lie between text spans connected 
by semantic parataxis and hypotaxis. 
4. There is an available syntactic slot to hold 
the embedded part. 
A good embedding is highly preferred and 
should be performed whenever possible. A nor- 
mal embedding is one satisfying condition 1, 3 
and 4 and the embedded part is a relative clause 
which provides additional information about 
the referent. Bad embeddings are all those left, 
for example, if there is no available syntactic 
slot for the embedded part. 
Since semantic parataxis has a higher priority 
than embedding (Cheng, 1998), a good embed- 
ding should be less preferred than using a con- 
junct relation, but it should be preferred over a 
center continuation for it to happen. 
To decide the interaction between an embed- 
ding and a center transition, we use the first two 
examples in Figure 1 again. The only difference 
between I and 2 is the position of the sentence 
"This necklace was de.signed by Jessie King", 
which can be represented in terms of features of 
local coherence and embedding as follows: 
the last three sentences in 1: Joint + Contin- 
uation + Joint + Smooth shifting 
the last two sentences plus embedding in 2: 
Joint + Abrupt shifting + Normal embedding 
1 is preferred over 2 because the center inoves 
more smoothly in 1. The heuristics derived from 
the above discussions are summarised below: 
Heur i s t i c  4 Preferences among features for 
embedding and center transition: 
Abrupt shifting + Normal embedding 
Good embedding > Continuation + Joint 
Conjunct > Good embedding 
The '+' symbol can be interpreted in different 
ways, depending on how the features are used 
in an NLG system. In a traditional system, it 
means the coexistence of two features. In a sys- 
tem using numbers for planning, it can have the 
same meaning as the arithmetic symbol. 
3 Captur ing  the  pre ferences  in I LEX  
The architecture of text planning has a great 
effect on aggregation possibilities. In object de- 
scriptive text generation, there lacks a central 
overriding communicative goal which could be 
decomposed in a structured way into subgoals. 
The main goal is to provide interesting infor- 
mation about the target object. There are gen- 
erally only a small number of relations, mainly 
object-attribute elaboration and joint. For such a 
genre, a domain-dependent bottom-up lanner 
(Marcu, 1997) or opportunistic planner (Mel- 
lish et al, 1998b) suits better than a domain- 
independent top-down planner. In these archi- 
tectures, aggregation is important o text plan- 
ning because it changes the order in which infor- 
mation is expressed. The first implementation 
we will describe is based on ILEX (Oberlander 
et al, 1998). 
ILEX is an adaptive hypertext generation 
system, providing natural anguage descriptions 
for museum objects. The bottom-up text plan- 
ning is fulfilled in two steps: a content selection 
procedure, where a set of fact nodes with high 
relevance is selected from the Content Potential 
(following a search algorithm), and a content 
structuring procedure, where selected facts are 
reorganised to form entity-chains (based on the 
theory of entity-based coherence), which repre- 
sent a coherent ext arrangement. 
To make it possible for the ILEX planner to 
take into account aggregation, we use a revised 
version of Meteer's Text Structure (Meteer, 
1992; Panaget, 1997) as the intermediate l vel of 
representation between text planning and sen- 
tence rcalisation to provkte abstract syntactic 
constraints to the planning. We call this sys- 
tem ILEX-TS (ILEX based on Text Structure). 
189 
In ILEX-TS, abstract referring expression de- 
termination and.aggxegation are performed ur - . . :  
ing text structuring. For each fact whose Text 
Structure is being built, if an NP in the fact can 
take modifiers, the embedding process will find 
a list of elaboration facts to the referent and 
make embedding decisions based on the con- 
straints imposed by the NP form. The decisions 
include what to embed and what syntactic form 
the embedded part should use. 
Heuristic 1, 2 and 3 are followed naturally ~ 
by the ILEX text planner, which calculates the 
best RS tree and puts facts connected by the 
imaginary conjunct relation next to each other. 
It tries to feature center continuations as often 
as possible. When it needs to shift topic, it uses 
a smooth shifting. 
ILEX-TS has a set of embedding rules, where 
those rules featuring good embedding are al- 
ways used first, then a rule featuring a normal 
embedding. Bad embedding is not allowed at 
all. To coordinate different types of aggrega- 
tion, the algorithm checks parataxis and hy- 
potaxis possibilities for each nucleus fact and 
the fact to be embedded before it applies an 
embedding rule. These realise most of Heuris- 
tic 4 (except for the second set). However, be- 
cause the various factors are optimised in order 
(with no backtracking), there is no guarantee 
that the best overall text will be found. In addi- 
tion, complex interactions between aggregation 
and center transition cannot be easily captured. 
4 Text  p lann ing  us ing  a GA 
Although most heuristics can be followed in 
ILEX-TS, some interactions are missing, for ex- 
ample, 9 of Figure 1 will probably be generated. 
For better coordination, we adopt the text plan- 
ner based on a Genetic Algorithm (GA) as de- 
scribed in (Mellish et al, 1998a). The task is. 
given a set of facts and a set of relations between 
facts, to produce a legal rhetoricalstrncture tree 
using all the facts and some relations. 
A fact is represented in terms of a subject, 
a verb and a complement (as well as a unique 
identifier). A relation is represented in terms of 
the relation name, the two facts that are con- 
nected t) 3" the relation and a list of precondition 
facts which need to have been mentioned before 
the relation can be used i. 
1As this is an experimental system, the ability of the 
A genetic algorithm is suitable for such a 
problem.because,:the..numher-.of.-possihle-com- 
binations is huge and the search space is not 
perfectly smooth and unimodal (there can be 
many good combinations). Also the generation 
task does not require a global optimum to be 
found. What we need is a combination that is 
coherent enough for people to understand. 
(Mellish et al, 1998a) summarises the genetic 
algorithm roughly as follows: 
1. Enumerate a set of random initial se- 
quences by loosely following sequences of 
facts where consecutive facts mention the 
same entity. 
2. Evaluate sequences by evaluating the 
rhetorical structure trees they give rise to. 
3. Perform mutation and crossover on the se- 
quences. 
4. Stop after a given number of iterations, and 
return the tree for the "best" sequence. 
The advantage of this approach is that it pro- 
vides a mechanism to integrate planning factors 
in the evaluation function and search for the 
best combinations of them. So it is an excellent 
framework for experimenting with the interac- 
tion between aggregation and text planning. 
In the algorithm, the RS trees are right- 
branching and are almost deterministically built 
from sequences of facts. Given two sequences, 
crossover inserts a random segment from one 
sequence in a random position in the other to 
produce two new sequences. Mutation selects 
a random segment of a sequence and moves it 
into a random position in the same sequence. 
To explore the whole space of aggregation. 
we decide not to perform aggregation on struc- 
tured facts or on adjacent facts in a linear se- 
quence because they might restrict the possibil- 
ities and even miss out good candidates. In- 
stead, we define a third operator called embed- 
ding mutation. Suppose we have a sequence 
\[U1,U2,...,Ui,...,U.\], where we call each element 
of the sequence a unit, which can be either a fact 
or a list of facts or units with no depth limit. 
For a list, we call its very first fact the main fact, 
system is limited in all aspects. It does not have a real 
realisation component,  so the parts we are less interested 
in are realised by canned phrases for readability. 
190 
Features /Factors  
Semant ic  re la t ions  
a jo int  
a conjunct or dis junct  
a relat ion other  than  jo int ,  con junct  or  d is junct  
a con junct  ,inside o ther  semant ic  re la t ions  
a precondi t ion  not  sat isf ied 
Focus  moves  
a cont inu ing  
an assoc iate  sh i f t ing 
a smooth shifting 
resuming  a previous focus 
Embedd ing  
a good embedding 
a normal embedd ing  
a bad embedding 
Others  
topic not mentioned in the first sentence 
Va lues  ( ra ters )  
1 \] 2 .. 
-20 -46 
10 11 
21 69 
-50 -63 
-30 -61 
20 7 
16 1 
14 -3 
6 -43 
6 3 
3 0 
-30 -64 
-10 -12 
Table 1: Two different raters satisfying the same constraints 
/ x I % / \ 
i i  ~ - - ~ t  ! x x i I x 
I x 
I I ~ x i I 
/ I  x l 
_ m  
, -h-- . . . . .  X . . . . . .  g . . . .  -i~ . . . . . . .  T . . . . . . .  i; . . . . . . .  ~ .......... ,o 
Figure 2: Scores for four museum descriptive texts 
into which the remaining facts in the list are to 
be embedded. The embedding mutat ion ran- 
domly selects a unit Ui from the sequence and 
an entity in its main fact. It then collects all 
the units mentioning this entity and randomly 
chooses one Uk. The list containing these two 
units \[Ui,Uk\] represents a random embedding 
and will be treated as a single unit in later op- 
erations. It takes the. position of Ui to produce 
a new sequence \[U~,U2,...,\[Ui,Uk\],...,U,\] and all 
repetit ions outside \[Ui,U~:\] are removed. This 
sequence is then evaluated and ordered in the 
populat ion. 
The probabil it ies of apl)lying the three opera- 
tots are: 65% for crossow'r. 30% for embedding 
mutat ion and 5% for normal umtation. This  is 
because the first two are more likely to produce 
sequences bearing desired properties by e i ther  
combining the good bits of two sequences or 
performing a reasonable amount of embedding,  
whereas normal mutat ion is entirely random 2. 
5 Jus t i fy ing  the  GA eva luat ion  
funct ion  
The  linguistic theories discussed in Section 2 
only give evidence in qualitative terms. For a 
GA-based planner to work, we have to come up 
with actual numbers that can be used to evalu- 
2The values for crossover and mutation rate used m 
our algorithm are fairly standard. 
191 
The smal l  por tab le  throne f rom the t ime o f  the  Qianlong Emperor  1736-95 is mad e . . . .  
of ~acquer~d`wo~d~.wit~T.de~rati~n-in~-g~d`~and.red~It-was-use~-in-the-private.apartrn~r~`~ ..... "......... 
of the Imperial Palaces. The cover f rom the  re ign o f  J iaquing,  1796-1820 is woven in yellow 
silk, wh ich  is the  imper ia l  colour o f  the Q ing  Dynasty ,1644-1911.  It would have covered 
the throne when not in use. 
The design on the seat is a imper ia l  f ive c lawed dragon in a circular medal l ion.  On the 
inside of the arm pieces are small shelves. Precious possessions can be placed in small shelves 
and can be studied as an aid to contemplation. 
Figure 3: A generated text scored the h ighest ,  w i th  the  embedded parts  highlighted 
Score2 Score3 Score4 Score5 Score6 
Score1 .9567 .9337 .9631 .9419 .9515 
Score2 .9435 .8819 .9280 .9185 
Score3 .8650 .8462 .9574 
Score4 .9503 .8940 
Score5 .8486 
Table 2: Correlations between six raters 
ate an RS tree. Mellish et al (1998a) present 
some scores for evaluating the basic features of a 
tree, but they make it clear that the scores are 
there for descriptive purpose, not for making 
any serious claim about the best way of evalu- 
ating RS trees. 
The methodology we adopted was that we 
took the existing evaluation function and ex- 
tended it to take into account features for local 
coherence, embedding and semantic paratmxis. 
This resulted in rater 1 in Table 1, which sat- 
isfied all the heuristics mentioned in Section 2. 
We manually broke down four human written 
museum descriptions into individual facts and 
relations and reconstructed sequences of facts 
with the same orderings and aggregations a  in 
the original texts. We then used our evaluation 
flmction to score the RS trees built from these 
sequences. In the mean time. we ran the GA 
algorithm for 5000 iterations on the facts and 
relations for 10 times. The results are shown in 
Figure 2, where the four line styles correspond 
to the four texts. The jagged lines represent-the 
scores of the machine generated texts and the 
straight lines represent the scores for the corre- 
sponding human texts. 
All human texts were scored among the high- 
est and machine generated texts can get scores 
very close to human ones sometimes. Since the 
human texts were written and revised bv mu- 
seum experts, they can be treated as "'nearly 
best  texts". The figure shows that the evalu- 
ation function based on our heuristics can find 
good and correct combinations. The reason for 
a relatively bad text being generated sometimes 
might be that really bad sequences were pro- 
duced at the beginning. This could be improved 
by using certain heuristics to get better initial 
sequences. Also when the number of facts be- 
comes larger, more iterations are needed to get 
readable texts. Figure 3 gives a text generated 
using rater 1. 
To justify our claim that it is the preferences 
among generation factors that decide the coher- 
ence of a text, we fed the preferences into a con- 
straint based program. If a feature can take a 
range of values, the program randomly selects 
a number in that range. A number of raters 
compatible with the constraints were generated 
and one of them is given in Table 1 as rater 2. 
We then generated all possible combinations, in- 
cluding embedding, of seven facts from a human 
text and used six randomly produced raters to 
score each of them. 
The .qualities .of the generated texts are nor- 
real distributed according to all raters. The 
raters distinguish between good and bad texts 
and they classify the majority of texts as of 
moderate quality and only very small percent- 
ages as very good or very bad texts. The be- 
haviours of the raters are very similar as the 
histograms are of roughly the same shape. 
192 
To see to what extent he six raters agree with 
each other, we calculated the  Pearson correla- 
tion coefficient between them, which is shown 
in Table 2. We can claim from the table that 
for this data, the scores from the six raters cor- 
relate, and we have a fairly good chance to be- 
lieve that the six raters, randomly produced in 
a sense, agree with each other on evaluating the 
text and they measure basically the same thing. 
Daniel Marcu. 1997. From local to global coherence: 
. A_ bottom=up.approach' to. text:planning.. 'In Pro- 
ceedings of the Fourteenth National Conference on 
Artificial Intelligence, pages 629-635, Providence, 
Rhode Island. 
Chris Mellish, Alistair Knott, Jon Oberlander, 
and Mick O'Donnell. 1998a. Experiments using 
stochastic search for text planning. In Proceed- 
ings of the 9th International Workshop on Natural 
Language Generation, Ontario, Canada. 
6 Conc lus ions  and  fu ture  work  
. . . . .  Chris:MeUish,: Mick O'_Donnell,:.Jon Oberlander, and 
Alistair Knott. 1998b. An architecture for op- 
This paper describes an experiment with the 
preferences among features concerning aggrega- 
tion and text planning, in particular, we present 
an mechanism for how relevant features can be 
scored to contribute together to the planning of 
a coherent text. The statistical results partially 
justify our claim that it is the preferences among 
generation features that decide the coherence of 
a text. 
Our experiment could be extended in many 
ways, for example, validating the evaluation 
function through empirical analysis of human 
assessments of the generated texts, and us- 
ing more texts to test the correlation between 
raters. The architecture based on the Genetic 
Algorithm can also be used for testing interac- 
tions between or within other text generation 
modules. 
Re ferences  
Susan Brennan, Marilyn Walker Friedman, and Carl 
Pollard. 1987. A centering apporach to pronouns. 
In Proceedings of the 25th Annual Meeting of the 
Association for Computational Linguistics, pages 
155-162, Stanford, CA. 
Hua Cheng. 1998. Embedding new information i to 
referring expressions. In Proceedings ofCOLING- 
A CL '98, pages 1478-1480, Montreal, Canada. 
Barbara Grosz and Candace Sidner. 1986. Atten- 
tions, intentions and the structure of discourse. 
Computational Linguistics, 12:175-204. 
Barbara Grosz, Aravind Joshi, and Scott Weinstein. 
1995. Centering: A framework for modelling the 
local coherence of discourse. Computational Lin- 
guistics, 21(2):203-226. 
Barbara Grosz. 1977. T-he representation a d use of 
focus in dialogue understanding. Technical report 
151, SRI International. 
William Mann and Sandra Thompson. 198.7. 
Rhetorical structure theory: A theory of text or- 
ganization. Technical Report ISI/RR-87-190, In- 
formation Sciences Institute, University of South- 
ern California. 
portunistic text generation. In Proceedings of the 
9th International Workshop on Natural Language 
Generation, Ontario, Canada. 
Marie Meteer. 1992. Expressibility and The Prob- 
lem of Efficient Text Planning. Communication 
in Artificial Intelligence. Pinter Publishers Lim- 
ited, London. 
Megan Moser and Johanna Moore. 1996. Toward a 
synthesis of two accounts of discourse structure. 
Computational Linguistics, 22(3):409-419. 
Jon Oberlander, Mick O'Donnell, Ali Knott, and 
Chris Mellish. 1998. Conversation i  the mu- 
seum: Experiments in dynamic hypermedia with 
the intelligent labelling explorer. New Review of 
Hypermedia nd Multimedia, 4:11-32. 
Jon Oberlander, Alistair Knott, Mick O'Donnell, 
and Chris Mellish. 1999. Beyond elaboration: 
Generating descriptive texts containing it-clefts. 
In T Sanders, J Schilperoord, and W Spooren, 
editors, Text Representation: Linguistic and Psy- 
cholinguistic Aspects. Benjamins, Amsterdam. 
Franck Panaget. 1997. Micro-planning: A uni- 
fied representation f lexical and grammatical re- 
sources. In Proceeding of the 6th European Work- 
shop on Natural Language Generation, pages 97- 
106. 
Massimo Poesio, Renata Vieira, and Simone Teufel. 
1997. Resolving bridging references in unre- 
stricted text. Research paper hcrc-rp87, Centre 
for Cognitive Science, University of Edinburgh. 
Michael Reape and Chris Mellish. 1999. Just what 
is aggregation anyway? In Proceedings of the 7th 
European Workshop on Natural Language Gener- 
ation, pages 20-29, Toulouse, France. 
Roger Schank. 1977. Rules and topics in conversa- 
tion. Cognitive Science, 1(1):421-441. 
Donia Scott and Clarisse Sieckenius de Souza. 1990. 
Getting the-message across in rst-based text gen- 
eration. In R. Dale, C. Mellish, and M. Zock, edi- 
tors, Current Research in Natural Language Gen- 
eration, pages 47-73. Academic Press. 
John Wilkinson. 1995. Aggregation i Natural Lan- 
guage Generation: Another Look. Technical re- 
port, Computer Science Department, University 
of \Vnterloo. 
193 
Demonstrat ion of ILEX 3.0 
Michae l  O 'Donne l l t  (micko@dai .ed.ac .uk) ,  
A l is ta i r  Knott:~ (a l ik@hermes.otago.ac .nz) ,  
Jon  Ober lander t  ( jon@cogsci .ed.ac.uk) ,  
Chr is  Mel l isht (chr ism@dai .ed.ac .uk)  
t Divis ion o f  Informat ics ,  Un ivers i ty  of  Ed inburgh .  
:~ Depar tment  of  Computer  Science ~ Otago  University.  
Abst rac t  
We will demonstrate the ILEX system, a system 
which dynamically generates descriptions of data- 
base objects for the web, adapting the description to 
the discourse context and user type. Among other 
improvements in version 3, the system now gener- 
ates from relational databases, and this demonstra- 
tion will focus on this ability. We will also show how 
incremental extensions to the domain semantics im- 
prove the quality of the text produced. 
1 In t roduct ion  
ILEX is a tool for dynamic browsing of database- 
defined information: it allows a user to browse 
through the information in a database using hyper- 
text. ILEX generates descriptions of a database ob- 
ject on the fly, taking into account he user's con- 
text of browsing. For more information on ILEX, 
see Knott et al (1997) and Mellish et al (1998). 
The demonstration will consist of generating a
series of texts, in each case adding in additional com- 
ponents of the domain semantics. This short paper 
should be read in conjunction with the full paper 
elsewhere in this volume. 
2 Generat ing  f rom Bare  Data  
We start initially with a relational database, as 
defined by a set of tab-delimited database files, plus 
some minimal semantics. As discussed in the paper, 
we use assume a relational database to consist of two 
types of files: 
1. Entity Files: each of which provides data for 
a particular entity type. Each row (or record) 
defines the attributes of a different entity. See 
figure 1. 
2. Link Files: where a particular attribute may 
have multiple fillers, we use link files to define 
the entity-entity relations. See figure 2. 
To generate from these files, the dolnain-editor 
needs to provide two additional resources: 
1. Data-type specification for each entity-file, a 
specification of what data-type the values in the 
~ Material 
silver 
enamel 
gold 
Figure 2: A Sample from a Link file 
. 
3. 
column are, e.g., string, entity-id, domain type, 
etc. 
Domain Taxonomy: detailing the taxonomic or- 
ganisation of the various classes of the entities. 
Mapping Domain taxonomy onto Upper Model: 
ILEX uses an Upper Model (a domain- 
independent semantic taxonomy, see Bateman 
(1990)), which supports the grammatical ex- 
pression of entities, e.g., selection of pronoun, 
differentiation between mass and count entities, 
between things and qualities, etc. We require 
that the basic types in the domain taxonomy 
are mapped onto the upper model, to allow the 
entities to be grammaticalised and lexicalised 
appropriately. 
With just this semantics, we can generate texts, 
although impoverished texts, such as: 
The class of J-997 is necklace. It's de- 
signer is Jessie M. King. It's date is 1905. 
Several tricks are needed to generate without a 
specified omain semantics: 
Use of standard clause templates: lacking any 
knowledge of how different attributes are to be 
expressed, the system-can only generate ach 
attribute using a standard template structures, 
such as the X of Y is Z or It's X is Z. The 
attribute names, e.g., Designer, Style, etc. can 
be assumed to work as the lexical head of the 
Subject. This ploy sometimes goes wrong, but 
in general works. (this approach borrowed from 
Dale et al (1998)). 
257 
ID Class 
J-997 brooch 
J~998: :neddace 
J-999 i necklace 
etc. I 
Designer Date Style Place Sponsor 
King01 11905 Art-Deco London Liberty01 
King01 '19116 - Art-Deco "London 
Chanel01 1910 Art-Noveau Paris 
Figure 1: A Sample from an Entity file 
* Referring to Entities: there are a number of 
strategies open for referring to entities. If the 
Name attribute.is.supplied-(a:defined- attribute 
within the ILEX system), then the system can 
use this for referring. Lacking a name, it is pos- 
sible for the system to form nominal references 
using the Class attr ibute of the entity (all en- 
tities in ILEX databases are required to have 
this attribute provided). We could thus gener- 
ate indefinite references such as a brooch as first 
mentions, and on subsequent mentions, gener- 
ate forms such as the brooch or the brooch whose 
designer is Jessie M. King. Without specifica- 
tion of which entities should be considered part 
of the general knowledge of the reader, we must 
assume all entities are initially unknown. 
* Fact Annotations: ILEX was designed to work 
with various extra information known about 
facts, such as the assumed level of interest to the 
current reader model, the importance of the fact 
to the system's educational agenda, and the as- 
sumed assimilation of the information (how well 
does the system believe the reader to already 
understand it). See the main paper for more 
details. 
Lacking this information, the system assumes 
an average value for interest and importance, 
and a 0 value for assimilation (totally un- 
known). 
With only default values, the system cannot 
customise the text to the particular user. It may 
provide information already well known by the 
user, and thus risking boring them. Also, there 
can be no selection of information to ensure that 
the more interesting and important information 
is provided on earlier pages (the reader may not 
bother to look at later pages). 
Other information (defeasible rules), which allows 
us to organise the material into complex rhetorical 
structure, is also missing. 
So, these tricks allow us to generate simple texts, 
consisting of a list of template-formatted clauses. 
3 Add ing  Express ion  in fo rmat ion  
In the next step, we will add in information about 
how the various attributes hould be expressed. This 
includes three main resources: 
1. Syntactic expression of attributes: for each at- 
tribute, we provide a specification of how the 
......... ~. ~.~ribu:te~should~be~-expressed. syntactically. 
2. Lexicalisation of domain types: by providing 
a lexicon, which maps domain types to lexical 
items, we avoid problems of using the domain 
type itself as the spelling. The lexical inform- 
ation allows correct generation of inflectional 
forms (e.g., of the plural for nouns, comparative 
or superlative forms for adjectives). 
3. Restrictive modifiers for referring expressions: 
In choosing restrictive modifiers for forming re- 
ferring expressions, ome facts work better than 
others. For instance, the brooch designed by 
King is more likely to refer adequately than the 
brooch which was 3 inches long. ILEX allows 
the user to state the preferential order for choos- 
ing restrictive modifiers. 
The addition of these resources will result in im- 
proved expression within the clauses, but not af- 
fect the text structure itself, which are still a list 
of clauses in random order. 
4 Add ing  User  Annotat ions  
In the next step, we add in the user model, which 
provides, for each attribute type, predicted user in- 
terest, importance for the system, and expected user 
assimilation. 
Using these values, ILEX can start to organise 
the text, placing important/interesting i formation 
on earlier pages, and avoiding information already 
known by tile user. 
5 Add ing  Defeas ib le  Ru les ,  S tor ies  
As a final step, we add in various resources which 
improve the texture of the text. 
o Defeasible Rules: ILEX allows the assertion 
of generalisations like most Art Deco jewels 
use enamel. These rules allow the genera- 
tion of complex rhetorical structures which in- 
dude Generalisation, Exemplification and Con- 
cession. The use of these relations improves tim 
quality of the text generated. 
* Stories: much of the information obtainable 
about tile domain is in natural language. Of- 
ten, the information is specific to a particular 
258 
entity, and as such, it would be a waste of time 
to reduce the in.formation i to ILEX's Pred-Arg 
knowledge structure, just to regenerate he text. 
Because of this, ILEX allows the association 
of canned text with a database ntity (e.g., J- 
999), or type of entity (e.g., jewels designed for 
Liberty). The text can then be included in the 
text when the entity or type of entity is men- 
tioned. 
The intermixing of generated and canned text 
improves the qual i ty of generated texts by 
providing more variety of structures, and al- 
lowing anecdotes, which would be difficult to 
model in terms of the knowledge representation 
system. 
6 Conc lus ion  
By showing incremental addition of domain spe- 
cification within the ILEX system, we have demon- 
strated that it is a system which can function with 
varying degrees of information. This allows domain 
developers to rapidly prototype a working system, 
after which they can concentrate on improving the 
quality of text in the directions they favour. 
Re ferences  
John Bateman. 1990. Upper modeling: organiz- 
ing knowledge for natural language processing. 
In Proceedings of the Fifth International Work- 
shop on Natural Language Generation, Pitts- 
burgh, June. 
Robert Dale, Stephen J Green, Maria Milosavljevic, 
CEcile Paris, Cornelia Verspoor, and Sandra Wil- 
liams. 1998. The realities of generating natural 
language from databases. In Proceedings of the 
11th Australian Joint Conference on Artificial In- 
telligence, Brisbane, Australia, 13-17 July. 
Alistair Knott, Michael O'Donnell, Jon Oberlander, 
and Chris Mellish. 1997. Defeasible rules in con- 
tent selection and text structuring. In Proceedings 
of the 6th European Workshop on Natural Lan- 
9uage Generation, Gerhard-Mercator University, 
Duisburg, Germany, March 24 - 26. 
Chris Mellish, Mick O'Donnell, Jon Oberlander, and 
Alistair Knott. 1998. An architecture for oppor- 
tunistic text generation. In Proceedings of the 
Ninth International Workshop on Natural Lan- 
guage Generation, Niagara-on-the-Lake, Ontario, 
Canada. 
259 
Evaluating Centering for Information
Ordering Using Corpora
Nikiforos Karamanis?
University of Cambridge
Chris Mellish??
University of Aberdeen
Massimo Poesio?
University of Essex
Jon Oberlander?
University of Edinburgh
In this article we discuss several metrics of coherence defined using centering theory and
investigate the usefulness of such metrics for information ordering in automatic text generation.
We estimate empirically which is the most promising metric and how useful this metric is using
a general methodology applied on several corpora. Our main result is that the simplest metric
(which relies exclusively on NOCB transitions) sets a robust baseline that cannot be outperformed
by other metrics which make use of additional centering-based features. This baseline can be used
for the development of both text-to-text and concept-to-text generation systems.
1. Introduction
Information ordering (Barzilay and Lee 2004), that is, deciding in which sequence to
present a set of preselected information-bearing items, has received much attention in
recent work in automatic text generation. This is because text generation systems need
to organize the content in a way that makes the output text coherent, that is, easy to read
and understand. The easiest way to exemplify coherence is by arbitrarily reordering the
sentences of a comprehensible text. This process very often gives rise to documents that
do not make sense although the information content is the same before and after the
reordering (Hovy 1988; Marcu 1997; Reiter and Dale 2000).
Entity coherence, which is based on the way the referents of noun phrases (NPs)
relate subsequent clauses in the text, is an important aspect of textual organization.
Since the early 1980s, when it was first introduced, centering theory has been an
influential framework for modelling entity coherence. Seminal papers on centering such
as Brennan, Friedman [Walker], and Pollard (1987, page 160) and Grosz, Joshi, and
Weinstein (1995, page 215) suggest that centering may provide solutions for information
ordering.
Indeed, following the pioneering work of McKeown (1985), recent work on text
generation exploits constraints on entity coherence to organize information (Mellish
et al 1998; Kibble and Power 2000, 2004; O?Donnell et al 2001; Cheng 2002; Lapata
? Computer Laboratory, William Gates Building, Cambridge CB3 0FD, UK.
Nikiforos.Karamanis@cl.cam.ac.uk.
?? Department of Computing Science, King?s College, Aberdeen AB24 3UE, UK.
? Department of Computer Science, Wivenhoe Park, Colchester CO4 3SQ, UK.
? School of Informatics, 2 Buccleuch Place, Edinburgh EH8 9LW, UK.
Submission received: 15 May 2006; revised submission received: 15 December 2007; accepted for publication:
7 January 2008.
? 2008 Association for Computational Linguistics
Computational Linguistics Volume 35, Number 1
2003; Barzilay and Lee 2004; Barzilay and Lapata 2005, among others). Although these
approaches often make use of heuristics related to centering, the features of entity
coherence they employ are usually defined informally. Additionally, centering-related
features are combined with other coherence-inducing factors in ways that are based
mainly on intuition, leaving many equally plausible options unexplored.
Thus, the answers to the following questions remain unclear: (i) How appropriate
is centering for information ordering in text generation? (ii) Which aspects of centering are
most useful for this purpose? These are the issues we investigate in this paper, which
presents the first systematic evaluation of centering for information ordering. To do this,
we define centering-based metrics of coherence which are compatible with several extant
information ordering approaches. An important insight of our work is that centering
can give rise to many such metrics of coherence. Hence, a general methodology
for identifying which of these metrics represent the most promising candidates for
information ordering is required.
We adopt a corpus-based approach to compare the metrics empirically and
demonstrate the portability and generality of our evaluation methods by experimenting
with several corpora. Our main result is that the simplest metric (which relies
exclusively on NOCB transitions) sets a baseline that cannot be outperformed by
other metrics that make use of additional centering-related features. Thus, we provide
substantial insight into the role of centering as an information ordering constraint and
offer researchers working on text generation a simple, yet robust, baseline to use against
their own information ordering approaches during system development.
The article is structured as follows: In Section 2 we discuss our information ordering
approach in relation to other work on text generation. After a brief introduction
to centering in Section 3, Section 4 demonstrates how we derived centering data
structures from existing corpora. Section 5 discusses how centering can be used to
define various metrics of coherence suitable for information ordering. Then, Section 6
outlines a corpus-based methodology for choosing among these metrics. Section 7
reports on the results of our experiments and Section 8 discusses their implications.
We conclude the paper with directions for future work and a summary of our main
contributions.1
2. Information Ordering
Information ordering has been investigated by substantial recent work in text-to-
text generation (Barzilay, Elhadad, and McKeown 2002; Lapata 2003; Barzilay and
Lee 2004; Barzilay and Lapata 2005; Bollegala, Okazaki, and Ishizuka 2006; Ji and
Pulman 2006; Siddharthan 2006; Soricut and Marcu 2006; Madnani et al 2007,
among others) as well as concept-to-text generation (particularly Kan and McKeown
[2002] and Dimitromanolaki and Androutsopoulos 2003).2 We added to this work
by presenting approaches to information ordering based on a genetic algorithm
(Karamanis and Manurung 2002) and linear programming (Althaus, Karamanis, and
Koller 2004) which can be applied to both concept-to-text and text-to-text generation.
These approaches use a metric of coherence defined using features derived from
1 Earlier versions of this work were presented in Karamanis et al (2004) and Karamanis (2006).
2 Concept-to-text generation is concerned with the automatic generation of text from some underlying
non-linguistic representation. By contrast, the input to text-to-text generation applications is text.
30
Karamanis et al Centering for Information Ordering
centering and will serve as the premises of our investigation of centering in this
article.
Metrics of coherence are used in other work on text generation, too (Mellish et al
1998; Kibble and Power 2000, 2004; Cheng 2002). With the exception of Kibble and
Power?s work, the features of entity coherence used in these metrics are informally
defined using heuristics related to centering. Additionally, the metrics are further
specified by combining these features with other coherence-inducing factors such
as rhetorical relations (Mann and Thompson 1987). However, as acknowledged in
most of this work, these are preliminary computational investigations of the complex
interactions between different types of coherence which leave many other equally
plausible combinations unexplored.
Clearly, one would like to know what centering can achieve on its own before
devising more complicated metrics. To address this question, we define metrics which
are purely centering-based, placing any attempt to specify a more elaborate model of
coherence beyond the scope of this article. This strategy is similar to most work on
centering for text interpretation in which additional constraints on coherence are not
taken into account (the papers in Walker, Joshi, and Prince [1998] are characteristic
examples). This simplification makes it possible to assess for the first time how useful
the employed centering features are for information ordering.
Work on text generation which is solely based on rhetorical relations (Hovy 1988;
Marcu 1997, among others) typically masks entity coherence under the ELABORATION
relation. However, ELABORATION has been characterized as ?the weakest of all
rhetorical relations? (Scott and de Souza 1990, page 60). Knott et al (2001) identified
several theoretical problems all related to ELABORATION and suggested that this relation
be replaced by a theory of entity coherence for text generation. Our work builds on this
suggestion by investigating how appropriate centering is as a theory of entity coherence
for information ordering.
McKeown (1985, pages 60?75) also deployed features of entity coherence to
organize information for text generation. McKeown?s ?constraints on immediate focus?
(which are based on the model of entity coherence that was introduced by Sidner
[1979] and precedes centering) are embedded within the schema-driven approach to
generation which is rather domain-specific (Reiter and Dale 2000). By contrast, our
metrics are general and portable across domains and can be applied within information
ordering approaches which are applicable to both concept-to-text and text-to-text
generation.
3. Centering Overview
This section provides an overview of centering, focusing on the aspects which are most
closely related to our work. Poesio et al (2004) and Walker, Joshi, and Prince (1998)
discuss centering and its relation to other theories of coherence in more detail.
According to Grosz, Joshi, and Weinstein (1995), each utterance Un is assigned a
ranked list of forward looking centers (i.e., discourse entities) denoted as CF(Un). The
members of CF(Un) must be realized by the NPs in Un (Brennan, Friedman [Walker],
and Pollard 1987). The first member of CF(Un) is called the preferred center
CP(Un).
The backward looking center CB(Un) links Un to the previous utterance Un?1.
CB(Un) is defined as the most highly ranked member of CF(Un?1) which also belongs
to CF(Un). CF lists prior to CF(Un?1) are not taken into account for the computation
31
Computational Linguistics Volume 35, Number 1
Table 1
Centering transitions are defined according to whether the backward looking center, CB, is
the same in two subsequent utterances, Un?1 and Un, and whether the CB of the current
utterance, CB(Un), is the same as its preferred center, CP(Un). These identity checks are also
known as the principles of COHERENCE and SALIENCE, the violations of which are denoted
with an asterisk.
COHERENCE: COHERENCE?:
CB(Un)=CB(Un?1) CB(Un) =CB(Un?1)
or CB(Un?1) undef.
SALIENCE: CB(Un)=CP(Un) CONTINUE SMOOTH-SHIFT
SALIENCE?: CB(Un) =CP(Un) RETAIN ROUGH-SHIFT
of CB(Un). The original formulations of centering by Brennan, Friedman [Walker], and
Pollard (1987) and Grosz, Joshi, and Weinstein (1995) lay emphasis on the uniqueness
and the locality of the CB and will serve as the foundations of our work.
The CB and the CP are combined to define transitions across pairs of
adjacent utterances (Table 1). This definition of transitions is based on Brennan,
Friedman [Walker], and Pollard (1987) and has been popular with subsequent work.
There exist several variations, however, the most important of which comes from Grosz,
Joshi, and Weinstein (1995), who define only one SHIFT transition.3
Centering makes two major claims about textual coherence, the first of which
is known as Rule 2. Rule 2 states that CONTINUE is preferred to RETAIN, which
is preferred to SMOOTH-SHIFT, which is preferred to ROUGH-SHIFT. Although the
Rule was introduced within an algorithm for anaphora resolution, Brennan, Friedman
[Walker], and Pollard (1987, page 160) consider it to be relevant to text generation
too. Grosz, Joshi, and Weinstein (1995, page 215) also take Rule 2 to suggest that
text generation systems should attempt to avoid unfavorable transitions such as
SHIFTs.
The second claim, which is implied by the definition of the CB (Poesio et al 2004),
is that CF(Un) should contain at least one member of CF(Un?1). This became known
as the principle of CONTINUITY (Karamanis and Manurung 2002). Although Grosz,
Joshi, and Weinstein and Brennan, Friedman [Walker], and Pollard do not discuss
the effect of violating CONTINUITY, Kibble and Power (2000, Figure 1) define the
additional transition NOCB to account for this case. Different types of NOCB transitions
are introduced by Passoneau (1998) and Poesio et al (2004), among others. Other
researchers, however, consider the NOCB transition to be a type of ROUGH-SHIFT
(Miltsakaki and Kukich 2004).
Kibble (2001) and Beaver (2004) introduced the principles of COHERENCE and
SALIENCE, which correspond to the identity checks used to define the transitions
(see Table 1). To improve the way centering resolves pronominal anaphora, Strube
and Hahn (1999) introduced a fourth principle called CHEAPNESS and defined it as
CB(Un)=CP(Un?1). They also redefined Rule 2 to favor transition pairs which satisfy
3 ?CB(Un?1) undef.? in Table 1 stands for the cases where Un?1 does not have a CB. Instead of classifying
the transition of Un as a CONTINUE or a RETAIN in such cases, the additional transition ESTABLISHMENT
is sometimes used (Kameyama 1998; Poesio et al 2004).
32
Karamanis et al Centering for Information Ordering
CHEAPNESS over those which violate it. This means that CHEAPNESS is given priority
over every other centering principle in Strube and Hahn?s model.
In addition to the variability caused by the numerous definitions of transitions and
the introduction of the various principles, parameters such as ?utterance,? ?ranking,?
and ?realization? can also be specified in several ways giving rise to different
instantiations of centering (Poesio et al 2004). The following section discusses how these
parameters were defined in the corpora we deploy.
4. Experimental Data
We made use of the data of Dimitromanolaki and Androutsopoulos (2003), the GNOME
corpus (Poesio et al 2004), and the two corpora that Barzilay and Lapata (2005)
experimented with. In this section, we discuss how the centering representations we
utilize were derived from each corpus.
4.1 The MPIRO-CF Corpus
Dimitromanolaki and Androutsopoulos (2003, henceforth D&A) derived facts from the
database of the MPIRO concept-to-text generation system (Isard et al 2003), realized
them as sentences, and organized them in sets. Each set consisted of six facts which
were ordered by a domain expert. The orderings produced by this expert were shown
to be very close to those produced by two other archeologists (Karamanis and Mellish
2005b).
Our first corpus, MPIRO-CF, consists of 122 orderings that were made available
to us by D&A. We computed a CF list for each fact in each ordering by applying the
instantiation of centering introduced by Kibble and Power (2000, 2004) for concept-to-
text generation. That is, we took each database fact to correspond to an ?utterance?
and specified the ?realization? parameter using the arguments of each fact as the
members of the corresponding CF list. Table 2 shows the CF lists, the CBs, the
centering transitions, and the violations of CHEAPNESS for the following example from
MPIRO-CF:
(1) (a) This exhibit is an amphora.
(b) This exhibit was decorated by the Painter of Kleofrades.
(c) The Painter of Kleofrades used to decorate big vases.
(d) This exhibit depicts a warrior performing splachnoscopy before leaving for the
battle.
(e) This exhibit is currently displayed in the Martin von Wagner Museum.
(f) The Martin von Wagner Museum is in Germany.
MPIRO facts consist of two arguments, the first of which was specified as the CP
following the definition of ?CF ranking? in O?Donnell et al (2001).4 Notice that the
second argument can often be an entity such as en914 that is realized by a canned phrase
of significant syntactic complexity (a warrior performing splachnoscopy before leaving for
the battle). Moreover, the deployed definition of ?realization? is similar to what Grosz,
4 This is the main difference between our approach and that of Kibble and Power, who allow for more than
one potential CP in their CF lists.
33
Computational Linguistics Volume 35, Number 1
Table 2
The CF list, the CB, NOCB, or centering transition (see Table 1) and violations of CHEAPNESS
(denoted with an asterisk) for each fact in Example (1) from the MPIRO-CF corpus.
Fact CF list: next referent} CB Transition CHEAPNESS
{CP, CBn=CPn?1
(1a) {ex1, amphora} n.a. n.a. n.a.
(1b) {ex1, paint-of-kleofr} ex1 CONTINUE ?
(1c) {paint-of-kleofr, en404} paint-of-kleofr SMOOTH-SHIFT ?
(1d) {ex1, en914} ? NOCB n.a.
(1e) {ex1, wagner-mus} ex1 CONTINUE ?
(1f) {wagner-mus, germany} wagner-mus SMOOTH-SHIFT ?
Joshi, and Weinstein (1995) call ?direct realization,? which ignores potential bridging
relations (Clark 1977) between the members of two subsequent CF lists. These relations
are typically not taken into account for information ordering and were not considered
in any of the deployed corpora.
4.2 The GNOME-LAB Corpus
We also made use of the GNOME corpus (Poesio et al 2004), which contains object
descriptions (museum labels) reliably annotated with features relevant to centering.
The motivation for this study was to examine whether the phenomena observed in
MPIRO-CF (which is arguably somewhat artificial) also manifest in texts from the
same genre written by humans without the constraints imposed by a text generation
system.
Based on the definition of museum labels in Cheng (2002, page 65), we identified
20 such texts in GNOME, which were published in a book and a museum Web site (and
were thus taken to be coherent). The following example is a characteristic text from this
subcorpus (referred to here as GNOME-LAB):
(2) (a) Item 144 is a torc.
(b) Its present arrangement, twisted into three rings, may be a modern alteration;
(c) it should probably be a single ring, worn around the neck.
(d) The terminals are in the form of goats? heads.
The GNOME corpus provides us with reliable annotation of discourse units (i.e.,
clauses and sentences) that can be used for the computation of ?utterance? and of
NPs which introduce entities to the CF list. Each feature was marked up by at
least two annotators and agreement was checked using the ? statistic on part of the
corpus.
In order to avoid deviating too much from the MPIRO application domain, we
computed the CF lists from the units that seemed to correspond more closely to MPIRO
facts. So instead of using sentence for the definition of ?utterance,? we followed most
work on centering for English and computed CF lists from GNOME?s finite units.5 The
5 This definition includes titles which do not always have finite verbs, but excludes finite relative clauses,
the second element of coordinated VPs and clause complements which are often taken as not having their
own CF lists in the centering literature.
34
Karamanis et al Centering for Information Ordering
Table 3
First two members of the CF list, the CB, NOCB, or centering transition (see Table 1) and
violations of CHEAPNESS (denoted with an asterisk) for each finite unit in Example (2) from the
GNOME-LAB corpus.
Unit CF list: next referent} CB Transition CHEAPNESS
{CP, CBn=CPn?1
(2a) {de374, de375} n.a. n.a. n.a.
(2b) {de376, de374, ... } de374 RETAIN ?
(2c) {de374, de379, ... } de374 CONTINUE ?
(2d) {de380, de381, ... } ? NOCB n.a.
text spans with the indexes (a) to (d) in Example (2) are examples of such units. Units
such as (2a) are as simple as the MPIRO-generated sentence (1a), whereas others appear
to be of similar syntactic complexity to (1d). On the other hand, the second sentence in
Example (2) consists of two finite units, namely (b) and (c), and appears to correspond
to higher degrees of aggregation than is typically seen in an MPIRO fact. The texts in
GNOME-LAB consist of 8.35 finite units on average.
Table 3 shows the first two members of the CF list, the CB, the transitions, and the
violations of CHEAPNESS for Example (2). Note that the same entity (i.e., de374) is used
to denote the referent of the NP Item 144 in (2a) and its in (2b), which is annotated as
coreferring with Item 144. All annotated NPs introduce referents to the CF list (which
often contains more entities than in MPIRO), but only direct realization is used for the
computation of the list. This means that, similarly to the MPIRO domain, bridging
relations between, for example, it in (2c) and the terminals in (2d), are not taken into
account.
The members of the CF list were ranked by combining grammatical function with
linear order, which is a robust way of estimating ?CF ranking? in English (Poesio et al
2004). In this instantiation, the CP corresponds to the referent of the first NP within the
unit that is annotated as a subject or as the post-copular NP in a there-clause.
4.3 The NEWS and ACCS Corpora
Barzilay and Lapata (2005) presented a probabilistic approach for information ordering
which is particularly suitable for text-to-text generation and is based on a new
representation called the entity grid. A collection of 200 articles from the North American
News Corpus (NEWS) and 200 narratives of accidents from the National Transportation
Safety Board database (ACCS) was used for training and evaluation. Example (3)
presents a characteristic text from the NEWS corpus:
(3) (a) [The Justice Department]S is conducting [an anti-trust trial]O against [Microsoft
Corp.]X with [evidence]X that [the company]S is increasingly attempting to crush
[competitors]O.
(b) [Microsoft]O is accused of trying to forcefully buy into [markets]X where [its
own products]S are not competitive enough to unseat [established brands]O.
(c) [The case]S revolves around [evidence]O of [Microsoft]S aggressively pressuring
[Netscape]O into merging [browser software]O.
(d) [Microsoft]S claims [its tactics]S are commonplace and good economically.
35
Computational Linguistics Volume 35, Number 1
Table 4
Fragment of the entity grid for Example (3). The grammatical function of the referents in each
sentence is reported using S, O, and X (for subject, object, and other). The symbol ??? is used for
referents which do not occur in the sentence.
Referents
Sentences department trial microsoft evidence ... products brands ...
(3a) S O S X ... ? ? ...
(3b) ? ? O ? ... S O ...
(3c) ? ? S O ... ? ? ...
(3d) ? ? S ? ... ? ? ...
(3e) ? ? ? ? ... ? ? ...
(3f) ? X S ? ... ? ? ...
(e) [The government]S may file [a civil suit]O ruling that [conspiracy]S to curb
[competition]O through [collusion]X is [a violation]O of [the Sherman Act]X.
(f) [Microsoft]S continues to show [increased earnings]O despite [the trial]X.
Barzilay and Lapata automatically annotated their corpora for the grammatical function
of the NPs in each sentence (denoted in the example by the subscripts S, O, and
X for subject, object, and other, respectively) as well as their coreferential relations
(which do not include bridging references). More specifically, they used a parser
(Collins 1997) to determine the constituent structure of the sentences from which the
grammatical function for each NP was derived.6 Coreferential NPs such as Microsoft
Corp. and the company in (3a) were identified using the system of Ng and Cardie
(2002).
The entity grid is a two-dimensional array that captures the distribution of NP
referents across sentences in the text using the aforementioned symbols for their
grammatical role and the symbol ??? for a referent that does not occur in a sentence.
Table 4 illustrates a fragment of the grid for the sentences in Example (3).7
Barzilay and Lapata use the grid to compute models of coherence that are
considerably more elaborate than centering. To derive an appropriate instantiation of
centering for our investigation, we compute a CF list for each grid row using the
referents with the symbols S, O, and X. These referents are ranked according to their
grammatical function and their position in the text. This definition of ?CF ranking? is
similar to the one we use in GNOME-LAB. For instance, department is ranked higher
than microsoft in CF(3a) because the Justice Department is mentioned before Microsoft
Corp. in the text. The derived sequence of CF lists is used to compute the additional
centering data structures shown in Table 5.
The average number of sentences per text is 10.4 in NEWS and 11.5 in ACCS.
As we explain in the next section, our centering-based metrics of coherence can be
6 They also used a small set of patterns to recognize passive verbs and annotate arguments involved in
passive constructions with their underlying grammatical function. This is why Microsoft is marked with
the role O in sentence (3b).
7 If a referent such as microsoft is attested by several NPs in the same sentence, for example, Microsoft
Corp. and the company in (3a), the role with the highest priority (in this case S) is used to represent it.
36
Karamanis et al Centering for Information Ordering
Table 5
First two members of the CF list, the CB, NOCB, or centering transitions (see Table 1) and
violations of CHEAPNESS (denoted with an asterisk) for Example (3) from the NEWS corpus.
Sentence CF list: next referent} CB Transition CHEAPNESS
{CP, CBn=CPn?1
(3a) {department, microsoft, ...} n.a. n.a. n.a.
(3b) {products, microsoft, ...} microsoft RETAIN ?
(3c) {microsoft, case, ...} microsoft CONTINUE ?
(3d) {microsoft, tactics} microsoft CONTINUE ?
(3e) {government, conspiracy, ...} ? NOCB n.a.
(3f) {microsoft, earnings, ... } ? NOCB n.a.
deployed directly on unseen texts, so we treated all texts in NEWS and ACCS as test
data.8
5. Computing Centering-Based Metrics of Coherence
Following our previous work (Karamanis and Manurung 2002; Althaus, Karamanis,
and Koller 2004), the input to information ordering is an unordered set of information-
bearing items represented as CF lists. A set of candidate orderings is produced by
creating different permutations of these lists. A metric of coherence uses features from
centering to compute a score for each candidate ordering and select the highest scoring
ordering as the output.9
A wide range of metrics of coherence can be defined in centering?s terms, simply
on the basis of the work we reviewed in Section 3. To exemplify this, let us first assume
that the ordering in Example (3), which is analyzed as a sequence of CF lists in Table 5,
is a candidate ordering. Table 6 summarizes the NOCBs, the violations of COHERENCE,
SALIENCE, and CHEAPNESS, and the centering transitions for this ordering.10
The candidate ordering contains two NOCBs in sentences (3e) and (3f). Its score
according to M.NOCB, the metric used by Karamanis and Manurung (2002) and
Althaus, Karamanis, and Koller (2004), is 2. Another ordering with fewer NOCBs (should
such an ordering exist) will be preferred over this candidate as the selected output of
information ordering if M.NOCB is used to guide this process. M.NOCB relies only on
CONTINUITY. Because satisfying this principle is a prerequisite for the computation of
every other centering feature, M.NOCB is the simplest possible centering-based metric
and will be used as the baseline in our experiments.
According to Strube and Hahn (1999) the principle of CHEAPNESS is the most
important centering feature for anaphora resolution. We are interested in assessing how
suitable M.CHEAP, a metric which utilizes CHEAPNESS, is for information ordering.
CHEAPNESS is violated twice according to Table 6 so the score of the candidate ordering
8 By contrast, Barzilay and Lapata used 100 texts in each domain to train their models and reserved the
other 100 for testing them.
9 If the best coherence score is assigned to several candidate orderings, then the information ordering
algorithm will choose randomly between them.
10 Principles and transitions will be collectively referred to as ?features? from now on.
37
Computational Linguistics Volume 35, Number 1
Table 6
Violations of CONTINUITY (NOCB), COHERENCE, SALIENCE, and CHEAPNESS and centering
transitions for Example (3), based on the analysis in Table 5. The table reports the sentences
marked with each centering feature: That is, sentences (3e) and (3f) are classified as NOCBs, and
so on.
CONTINUITY? COHERENCE? SALIENCE? CHEAPNESS?
NOCB: CBn = CBn?1: CBn = CPn: CBn = CPn?1:
(3e), (3f) ? (3b) (3b), (3c)
CONTINUE: RETAIN: SMOOTH-SHIFT: ROUGH-SHIFT:
(3c), (3d) (3b) ? ?
according to M.CHEAP is 2.11 If another candidate ordering with fewer violations of
CHEAPNESS exists, it will be chosen as a preferred output according to M.CHEAP.
M.BFP employs the transition preferences of Rule 2 as specified by Brennan,
Friedman [Walker], and Pollard (1987). The first score to be computed by M.BFP is
the sum of CONTINUE transitions, which is 2 for the candidate ordering according to
Table 6. If this ordering is found to score higher than every other candidate ordering for
the number of CONTINUEs, it is selected as the output. If another ordering is found to
have the same number of CONTINUEs, the sum of RETAINs is examined, and so forth for
the other two types of centering transitions.12
M.KP, the metric deployed by Kibble and Power (2000) in their text generation
system, sums up the NOCBs as well as the violations of CHEAPNESS, COHERENCE,
and SALIENCE, preferring the ordering with the lowest total cost. In addition to
the violations of CONTINUITY and CHEAPNESS, the candidate ordering also violates
SALIENCE once, so its score according to M.KP is 5. An alternative ordering with a
lower score (if any) will be preferred by this metric. Although Kibble and Power (2004)
introduced a weighted version of M.KP, the exact weighting of centering?s principles
remains an open question, as argued by Kibble (2001). This is why we decided to
experiment with M.KP instead of its weighted variant.
In the remainder of the paper, we take forward the four metrics motivated in this
section as the most appropriate starting point for experimentation. We would like to
emphasize, however, that these are not the only possible options. Indeed, similarly to
the various ways in which centering?s parameters can be specified, there exist many
other ways of using centering to define metrics of entity coherence for information
ordering. These possibilities arise from the numerous other definitions of centering?s
transitions and the various ways in which transitions and principles can be combined.
These are explored in more detail in Karamanis (2003, Chapter 3), which also provides
a formal definition of the metrics discussed previously.
6. Evaluation Methodology
Because using naturally occurring discourse in psycholinguistic studies to investigate
coherence effects is almost infeasible, computational corpus-based experiments are
11 In order to estimate the effect of CHEAPNESS only, NOCBs are not counted as violations of CHEAPNESS.
12 Following Brennan, Friedman [Walker], and Pollard (1987), NOCBs are not taken into account for the
definition of transitions in M.BFP.
38
Karamanis et al Centering for Information Ordering
often the most viable alternative (Poesio et al 2004; Barzilay and Lee 2004). Corpus-
based evaluation can be usefully employed during system development and may
be later supplemented by less extended evaluation based on human judgments as
suggested by Lapata (2006).
The corpus-based methodology of Karamanis (2003) served as our experimental
framework. This methodology is based on the premise that the original sentence order
(OSO, Barzilay and Lee 2004) observed in a corpus text is more coherent than any other
ordering. If a metric takes an alternative ordering to be more coherent than the OSO, it
has to be penalized.
Karamanis (2003) introduced a performance measure called the classification error
rate which is computed according to the formula: Better(M,OSO)+Equal(M,OSO)/2.
Better(M,OSO) stands for the percentage of orderings that score better than the OSO
according to a metric M, and Equal(M,OSO) is the percentage of orderings that score
equal to the OSO.13 This measure provides an indication of how likely a metric is to lead
to an ordering different from the OSO. When comparing several metrics with each other,
the one with the lowest classification error rate is the most appropriate for ordering
the sentences that the OSO consists of. In other words, the smaller the classification
error rate, the better a metric is expected to perform for information ordering. The
average classification error rate is used to summarize the performance of each metric in
a corpus.
To compute the classification error rate we permute the CF lists of the OSO and
classify each alternative ordering as scoring better, equal, or worse than the OSO
according to M. When the number of CF lists in the OSO is fairly small, it is feasible
to search through all possible orderings. For OSOs consisting of more than 10 CF
lists, the classification error rate for the entire population of orderings can be reliably
estimated using a random sample of one million permutations (Karamanis 2003,
Chapter 5).
7. Results
Table 7 shows the average performance of each metric in the corpora employed in our
experiments. The smallest?that is, best?score in each corpus is printed in boldface.
The table indicates that the baseline M.NOCB performs best in three out of four corpora.
The experimental results of the pairwise comparisons of M.NOCB with each of
M.CHEAP, M.KP, and M.BFP in each corpus are reported in Table 8. The exact number
of texts for which the classification error rate of M.NOCB is lower than its competitor for
each comparison is reported in the columns headed by ?lower.? For instance, M.NOCB
has a lower classification error rate than M.CHEAP for 110 (out of 122) texts from
MPIRO-CF. M.CHEAP achieves a lower classification error rate for just 12 texts, and
there do not exist any ties, that is, cases in which the classification error rate of the two
metrics is the same.
The p value returned by the two-tailed Sign Test for the difference in the number
of texts in each corpus, rounded to the third decimal place, is also reported.14 With
13 Weighting Equal(M,OSO) by 0.5 is based on the assumption that, similarly to tossing a coin, the OSO will
on average do better than half of the orderings that score the same as it does when other coherence
constraints are considered.
14 The Sign Test was chosen over its parametric alternatives to test significance because it does not carry
specific assumptions about population distributions and variance.
39
Computational Linguistics Volume 35, Number 1
Table 7
Average classification error rate for the centering-based metrics in each corpus.
Corpus
Metric MPIRO-CF GNOME-LAB NEWS ACCS Mean
M.NOCB 20.42 19.95 30.90 15.51 21.70
M.BFP 19.91 33.01 37.90 21.20 28.01
M.KP 53.15 58.22 57.70 55.60 56.12
M.CHEAP 81.04 57.23 64.60 76.29 69.79
No. of texts 122 20 200 200
Table 8
Comparing M.NOCB with M.CHEAP, M.KP, and M.BFP in each corpus.
MPIRO-CF GNOME-LAB
M.NOCB M.NOCB
lower greater ties p lower greater ties p
M.CHEAP 110 12 0 <0.001 18 2 0 <0.001
M.KP 103 16 3 <0.001 16 2 2 0.002
M.BFP 42 31 49 0.242 12 3 5 0.036
No. of texts 122 20
NEWS ACCS
M.NOCB M.NOCB
lower greater ties p lower greater ties p
M.CHEAP 155 44 1 <0.001 183 17 0 <0.001
M.KP 131 68 1 <0.001 167 33 0 <0.001
M.BFP 121 71 8 <0.001 100 100 0 1.000
No. of texts 200 200
respect to the exemplified comparison of M.NOCB against M.CHEAP in MPIRO-CF,
the p value is lower than 0.001 after rounding. This in turn means that M.NOCB
returns a better classification error rate for significantly more texts in MPIRO-CF
than M.CHEAP. In other words, M.NOCB outperforms M.CHEAP significantly in this
corpus.
Notably, M.NOCB performs significantly better than its competitor in 10 out of
12 cases.15 In the remaining two comparisons, the difference in performance between
M.NOCB and M.BFP is not significant (p > 0.05). However, this does not constitute
evidence against M.NOCB, the simplest of the investigated metrics. In fact, because
M.BFP fails to outperform the baseline, the latter may be considered as the most
promising solution for information ordering in these cases too by applying Occam?s
razor. Thus, M.NOCB is shown to be the best performing metric across all four
corpora.
15 This result is significant too according to the two-tailed Sign Test (p < 0.05).
40
Karamanis et al Centering for Information Ordering
8. Discussion
Our experiments show that M.NOCB is the most suitable metric for information
ordering among the metrics we experimented with. Despite the differences between our
corpora (in genre, average length, syntactic complexity, number of referents in the CF
list, etc.), M.NOCB proves robust across all four of them. It is also the most appropriate
metric to use in both application areas we relate our corpora to, namely concept-to-text
(MPIRO-CF and GNOME-LAB) as well as text-to-text (NEWS and ACCS) generation.
These results indicate that when purely centering-based metrics are used, simply
avoiding NOCBs is more relevant to information ordering than the combinations of
additional centering features that the other metrics make use of.
In this section, we compare our work with other recent evaluation studies,
including the corpus-based investigation of centering by Poesio et al (2004); discuss
the implications of our findings for text generation; and summarize our contributions.
8.1 Recent Evaluation Studies in Information Ordering
There has been significant recent work on the corpus-based evaluation for information
ordering. In this section, we discuss the methodological differences between our work
and the studies which are most closely related to it.
Barzilay and Lee (2004) introduce a stochastic model for information ordering
which computes the probability of generating the OSO and every alternative ordering.
Then, all orderings are ranked according to this probability and the rank given to the
OSO is retrieved. Several evaluation measures are discussed, the most important of
which is the average OSO rank, that is, the average rank of the OSOs in their corpora.
This measure does not take into account that the OSOs differ in length. However, this
information is necessary to estimate reliably the performance of an information ordering
approach, as we discuss in Karamanis and Mellish (2005a) in more detail.
Barzilay and Lapata (2005) overcome this difficulty by introducing a performance
measure called ranking accuracywhich expresses the percentage of alternative orderings
that are ranked lower than the OSO. In Karamanis?s (2003) terms, ranking accuracy
equals 100% ? Better(M,OSO), assuming that no equally ranking orderings exist.16
Barzilay and Lapata (2005) compare the OSO with just 20 alternative orderings,
often sampled out of several millions. On the other hand, Barzilay and Lee (2004)
enumerate exhaustively each possible ordering, which might become impractical as the
search space grows factorially. We overcame these problems by using a large random
sample for the texts which consist of more than 10 sentences as suggested in Karamanis
(2003, Chapter 5). Equally important is the emphasis we placed on the use of statistical
tests, which were not deployed by either Barzilay and Lee or Barzilay and Lapata.
Lapata (2003) presented a methodology for automatically evaluating generated
orderings on the basis of their distance from observed sentence orderings in a corpus.
A measure of rank correlation (called Kendall?s ?), which was subsequently shown to
correlate reliably with human ratings and reading times (Lapata 2006), was used to
estimate the distance between orderings.
16 Neither Barzilay and Lapata (2005) nor Barzilay and Lee (2004) appear to consider the possibility that two
orderings may be equally ranked.
41
Computational Linguistics Volume 35, Number 1
Whereas ? estimates how close the predictions of a metric are to several original
orderings, we measure how likely a metric is to lead to an ordering different than the
OSO. Taking into account more than one OSO for information ordering is the main
strength of Lapata?s method, but to do this one needs to ask several humans to order the
same set of sentences (Madnani et al 2007). Karamanis and Mellish (2005b) conducted
an experiment in the MPIRO domain using Lapata?s methodology which supplements
the work reported in this article. However, such an approach is less practical for much
larger collections of texts such as NEWS and ACCS. This is presumably the reason why
Barzilay and Lapata (2005) use ranking accuracy instead of ? in their evaluation.
8.2 Previous Corpus-Based Evaluations of Centering
Our work investigates how the coherence score of the OSO compares to the scores
of alternative orderings of the sentences that the OSO consists of. As Kibble (2001,
page 582) noticed, this question is crucial from an information ordering viewpoint, but
was not taken into account by any previous corpus-based study of centering. Grosz,
Joshi, and Weinstein (1995, page 215) also suggested that Rule 2 should be tested by
examining ?alternative multi-utterance sequences that differentially realize the same
content.? We are the first to have pursued this research objective in the evaluation of
centering for information ordering.
Poesio et al (2004) observed that there remained a large number of NOCBs under
every instantiation of centering they tested and concluded that centering is inadequate
as a coherence model.17 However, the frequency of NOCBs does not necessarily provide
adequate indication of how appropriate NOCBs (and centering in general) are for
information ordering. Although over 50% of the transitions in GNOME-LAB are NOCBs,
the average classification error rate of approximately 20% for M.NOCB suggests that the
OSO tends to be in greater agreement with the preference to avoid NOCBs than 80% of
the alternative orderings. Thus, it appears that the observed ordering in the corpus does
optimize with respect to the number of potential NOCBs to a great extent.
8.3 A Simple and Robust Baseline for Text Generation
How likely is M.NOCB to come up with the attested ordering in the corpus (the OSO)
if it is actually used to guide an algorithm that orders the CF lists in our corpora?
The average classification error rates (Table 7) estimate exactly this variable. The
performance of M.NOCB varies across the corpora from about 15.5% (ACCS) to 30.9%
(NEWS). We attribute this variation to the aforesaid differences between the corpora.
Notice, however, that these differences affect all metrics in a similar way, not allowing
for another metric to significantly outperform M.NOCB.
Noticeably, even in ACCS, for which M.NOCB achieves its best performance,
approximately one out of six alternative orderings on average are taken to be more
coherent than the OSO. Given the average number of sentences per text in this corpus
17 We viewed the definition of the centering instantiation as being related to the application domain, as we
explained in Section 4. This is why, unlike Poesio et al, we did not experiment with different
instantiations of centering on the same data.
42
Karamanis et al Centering for Information Ordering
(11.5), this means that several millions of alternative orderings are often taken to be
more coherent than the gold standard.
Barzilay and Lapata (2005) report an average ranking accuracy of 87.3% for their
best sentence ordering method in ACCS. This corresponds to an average classification
error rate of 12.7% (assuming that there are no equally scoring orderings in their
evaluation; see Section 8.1). This is equal to an improvement of just 2.8% over
the performance of our baseline metric (15.5%) using a coherence model which is
substantially more elaborate than centering. However, it is in NEWS (for which
M.NOCB returns its worst performance of 30.9%) that this model shows its real strength,
approximating an average classification error rate of 9.6%, which corresponds to an
improvement of 21.3% over our baseline. We believe that the experiments reported in
this article put the studies of our colleagues in better perspective by providing a reliable
baseline to compare their metrics against.
8.4 Moving Beyond Centering-Based Metrics
Following McKeown (1985), Kibble and Power argue in favor of an integrated approach
for concept-to-text generation in which the same centering features are used at different
stages in the generation pipeline. However, our study suggests that features such as
CHEAPNESS and the centering transitions are not particularly relevant to information
ordering. The poor performance of these features can be explained by the fact that they
were originally introduced to account for pronoun resolution rather than information
ordering. CONTINUITY, on the other hand, captures a fundamental intuition about entity
coherence which constitutes part of several other discourse theories.18
CONTINUITY, however, captures just one aspect of coherence. This explains the
relatively high classification error rates for M.NOCB, which needs to be supplemented
with other coherence-inducing factors in order to be used in practice. This verifies the
premises of researchers such as Kibble and Power who a priori use features derived
from centering in combination with other factors in the definition of their metrics. Our
work should be quite helpful for that effort too, suggesting that M.NOCB is a better
starting point for defining such metrics than M.CHEAP or M.KP.
9. Conclusion
In conclusion, our analysis sheds more light on two previously unaddressed questions
in the corpus-based evaluation of centering: (i) which aspects of centering are most
relevant to information ordering and (ii) to what extent centering on its own can be
useful for this purpose. We have shown that the metric which relies exclusively on NOCB
transitions (M.NOCB) sets a baseline that cannot be outperformed by other coherence
metrics which make use of additional centering features. Although this metric does not
perform well enough to be used on its own, it constitutes a simple, yet robust, baseline
against which more elaborate information ordering approaches can be tested during
system development in both text-to-text and concept-to-text generation.
This work can be extended in numerous ways. For instance, given the abundance
of possible centering-based metrics one may investigate whether a different metric can
18 We thank one anonymous reviewer for suggesting this explanation of our results.
43
Computational Linguistics Volume 35, Number 1
outperform M.NOCB in any corpus or application domain. M.NOCB can also serve as
the starting point for the definition of more informed metrics which will incorporate
additional coherence-inducing factors. Finally, given that we used the instantiation
of centering which seemed to correspond more closely to the targeted application
domains, the extent to which computing the CF list in a different way may affect the
performance of the metrics is another question to explore in future work.
Acknowledgments
Many thanks to Aggeliki Dimitromanolaki,
Mirella Lapata, and Regina Barzilay for their
data; to David Schlangen, Ruli Manurung,
James Soutter, and Le An Ha for
programming solutions; and to Ruth Seal
and two anonymous reviewers for their
comments. Nikiforos Karamanis received
support from the Greek State Scholarships
Foundation (IKY) as a PhD student in
Edinburgh as well as the Rapid Item
Generation project and the BBSRC-funded
FlySlip grant (No 38688) as a postdoc in
Wolverhampton and Cambridge,
respectively.
References
Althaus, Ernst, Nikiforos Karamanis, and
Alexander Koller. 2004. Computing locally
coherent discourses. In Proceedings of ACL
2004, pages 399?406, Barcelona.
Barzilay, Regina, Noemie Elhadad, and
Kathleen McKeown. 2002. Inferring
strategies for sentence ordering in
multidocument news summarization.
Journal of Artificial Intelligence Research,
17:35?55.
Barzilay, Regina and Mirella Lapata. 2005.
Modeling local coherence: An entity-based
approach. In Proceedings of ACL 2005,
pages 141?148, Ann Arbor, MI.
Barzilay, Regina and Lillian Lee. 2004.
Catching the drift: Probabilistic content
models with applications to generation
and summarization. In Proceedings of
HLT-NAACL 2004, pages 113?120,
Boston, MA.
Beaver, David. 2004. The optimization of
discourse anaphora. Linguistics and
Philosophy, 27(1):3?56.
Bollegala, Danushka, Naoaki Okazaki, and
Mitsuru Ishizuka. 2006. A bottom-up
approach to sentence ordering for
multi-document summarization. In
Proceedings of ACL-COLING 2006,
pages 385?392, Sydney.
Brennan, Susan E., Marilyn A.
Friedman [Walker], and Carl J. Pollard.
1987. A centering approach to pronouns.
In Proceedings of ACL 1987, pages 155?162,
Stanford, CA.
Cheng, Hua. 2002. Modelling Aggregation
Motivated Interactions in Descriptive Text
Generation. Ph.D. thesis, Division of
Informatics, University of Edinburgh.
Clark, Herbert. H. 1977. Bridging. In P. N.
Johnson-Laird and P. C. Wason, editors,
Thinking: Readings in Cognitive Science.
Cambridge University Press, Cambridge,
pages 9?27.
Collins, Michael. 1997. Three generative,
lexicalised models for statistical parsing.
In Proceedings of ACL-EACL 1997,
pages 16?23, Madrid.
Dimitromanolaki, Aggeliki and Ion
Androutsopoulos. 2003. Learning to order
facts for discourse planning in natural
language generation. In Proceedings of
ENLG 2003, pages 23?30, Budapest.
Grosz, Barbara J., Aravind K. Joshi, and Scott
Weinstein. 1995. Centering: A framework
for modeling the local coherence of
discourse. Computational Linguistics,
21(2):203?225.
Hovy, Eduard. 1988. Planning coherent
multisentential text. In Proceedings of ACL
1988, pages 163?169, Buffalo, NY.
Isard, Amy, Jon Oberlander, Ion
Androutsopoulos, and Colin Matheson.
2003. Speaking the users? languages. IEEE
Intelligent Systems Magazine, 18(1):40?45.
Ji, Paul and Stephen Pulman. 2006. Sentence
ordering with manifold-based
classification in multi-document
summarization. In Proceedings of EMNLP
2006, pages 526?533, Sydney.
Kameyama, Megumi. 1998. Intrasentential
centering: A case study. In Walker, Joshi,
and Prince 1998, pages 89?122.
Kan, Min-Yen and Kathleen McKeown. 2002.
Corpus-trained text generation for
summarization. In Proceedings of INLG
2002, pages 1?8, Harriman, NY.
Karamanis, N. 2006. Evaluating centering for
information ordering in two new domains.
In Proceedings of NAACL 2006, Companion
Volume, pages 65?68, New York.
Karamanis, N., M. Poesio, C. Mellish, and
J. Oberlander. 2004. Evaluating
centering-based metrics of coherence using
44
Karamanis et al Centering for Information Ordering
a reliably annotated corpus. In Proceedings
of ACL 2004, pages 391?398, Barcelona.
Karamanis, Nikiforos. 2003. Entity Coherence
for Descriptive Text Structuring. Ph.D.
thesis, Division of Informatics, University
of Edinburgh.
Karamanis, Nikiforos and Hisar Maruli
Manurung. 2002. Stochastic text
structuring using the principle of
continuity. In Proceedings of INLG 2002,
pages 81?88, Harriman, NY.
Karamanis, Nikiforos and Chris Mellish.
2005a. A review of recent corpus-based
methods for evaluating information
ordering in text production. In Proceedings
of Corpus Linguistics 2005 Workshop on
Using Corpora for NLG, pages 13?18,
Birmingham.
Karamanis, Nikiforos and Chris Mellish.
2005b. Using a corpus of sentence
orderings defined by many experts to
evaluate metrics of coherence for text
structuring. In Proceedings of ENLG 2005,
pages 174?179, Aberdeen.
Kibble, Rodger. 2001. A reformulation of rule
2 of centering theory. Computational
Linguistics, 27(4):579?587.
Kibble, Rodger and Richard Power. 2000.
An integrated framework for text
planning and pronominalisation. In
Proceedings of INLG 2000, pages 77?84,
Mitzpe Ramon.
Kibble, Rodger and Richard Power. 2004.
Optimizing referential coherence in text
generation. Computational Linguistics,
30(4):401?416.
Knott, Alistair, Jon Oberlander, Mick
O?Donnell, and Chris Mellish. 2001.
Beyond elaboration: The interaction of
relations and focus in coherent text. In
T. Sanders, J. Schilperoord, and
W. Spooren, editors, Text Representation:
Linguistic and Psycholinguistic Aspects.
John Benjamins, Amsterdam, chapter 7,
pages 181?196.
Lapata, Mirella. 2003. Probabilistic text
structuring: Experiments with sentence
ordering. In Proceedings of ACL 2003,
pages 545?552, Sapporo.
Lapata, Mirella. 2006. Automatic evaluation
of information ordering: Kendall?s tau.
Computational Linguistics, 32(4):1?14.
Madnani, Nitin, Rebecca Passonneau,
Necip Fazil Ayan, John Conroy, Bonnie
Dorr, Judith Klavans, Dianne O?Leary, and
Judith Schlesinger. 2007. Measuring
variability in sentence ordering for news
summarization. In Proceedings of ENLG
2007, pages 81?88, Schloss Dagstuhl.
Mann, William C. and Sandra A. Thompson.
1987. Rhetorical structure theory: A theory
of text organisation. Technical Report
RR-87-190, University of Southern
California / Information Sciences Institute.
Marcu, Daniel. 1997. The Rhetorical Parsing,
Summarization and Generation of Natural
Language Texts. Ph.D. thesis, University of
Toronto.
McKeown, Kathleen. 1985. Text Generation:
Using Discourse Strategies and Focus
Constraints to Generate Natural Language
Text. Studies in Natural Language
Processing. Cambridge University Press,
Cambridge.
Mellish, Chris, Alistair Knott, Jon
Oberlander, and Mick O?Donnell. 1998.
Experiments using stochastic search for
text planning. In Proceedings of INLG 1998,
pages 98?107, Niagara-on-the-Lake.
Miltsakaki, Eleni and Karen Kukich. 2004.
Evaluation of text coherence for electronic
essay scoring systems. Natural Language
Engineering, 10(1):25?55.
Ng, Vincent and Claire Cardie. 2002.
Improving machine learning approaches
to coreference resolution. In Proceedings of
ACL 2002, pages 104?111, Philadelphia,
PA.
O?Donnell, Mick, Chris Mellish, Jon
Oberlander, and Alistair Knott. 2001. ILEX:
An architecture for a dynamic hypertext
generation system. Natural Language
Engineering, 7(3):225?250.
Passoneau, Rebecca J. 1998. Interaction
of discourse structure with explicitness
of discourse anaphoric phrases. In
Walker, Joshi, and Prince 1998,
pages 327?358.
Poesio, Massimo, Rosemary Stevenson,
Barbara Di Eugenio, and Janet Hitzeman.
2004. Centering: a parametric theory and
its instantiations. Technical Report
CSM-369, Department of Computer
Science, University of Essex. Extended
version of the paper that appeared in
Computational Linguistics 30(3):309?363,
2004.
Reiter, Ehud and Robert Dale. 2000.
Building Natural Language Generation
Systems. Cambridge University Press,
Cambridge.
Scott, Donia and Clarisse Sieckenius
de Souza. 1990. Getting the message across
in RST-based text generation. In Robert
Dale, Chris Mellish, and Michael Zock,
editors, Current Research in Natural
Language Generation. Academic Press, San
Diego, CA, pages 47?74.
45
Computational Linguistics Volume 35, Number 1
Siddharthan, Advaith. 2006. Syntactic
simplification and text cohesion.
Research on Language and Computation,
4(1):77?109.
Sidner, Candace L. 1979. Towards a
Computational Theory of Definite Anaphora
Comprehension in English. Ph.D. thesis, AI
Laboratory/MIT, Cambridge, MA. Also
available as Technical Report No.
AI-TR-537.
Soricut, Radu and Daniel Marcu. 2006.
Discourse generation using utility-trained
coherence models. In Proceedings of
ACL-COLING 2006 Poster Session,
pages 803?810, Sydney.
Strube, Michael and Udo Hahn. 1999.
Functional centering: Grounding
referential coherence in information
structure. Computational Linguistics,
25(3):309?344.
Walker, Marilyn A., Aravind K. Joshi, and
Ellen F. Prince, editors. 1998. Centering
Theory in Discourse. Clarendon Press,
Oxford.
46
Evaluating Centering-based metrics of coherence for text
structuring using a reliably annotated corpus
Nikiforos Karamanis,? Massimo Poesio,? Chris Mellish,? and Jon Oberlander?
?School of Informatics, University of Edinburgh, UK, {nikiforo,jon}@ed.ac.uk
?Dept. of Computer Science, University of Essex, UK, poesio at essex dot ac dot uk
?Dept. of Computing Science, University of Aberdeen, UK, cmellish@csd.abdn.ac.uk
Abstract
We use a reliably annotated corpus to compare
metrics of coherence based on Centering The-
ory with respect to their potential usefulness for
text structuring in natural language generation.
Previous corpus-based evaluations of the coher-
ence of text according to Centering did not com-
pare the coherence of the chosen text structure
with that of the possible alternatives. A corpus-
based methodology is presented which distin-
guishes between Centering-based metrics taking
these alternatives into account, and represents
therefore a more appropriate way to evaluate
Centering from a text structuring perspective.
1 Motivation
Our research area is descriptive text generation
(O?Donnell et al, 2001; Isard et al, 2003), i.e.
the generation of descriptions of objects, typi-
cally museum artefacts, depicted in a picture.
Text (1), from the gnome corpus (Poesio et al,
2004), is an example of short human-authored
text from this genre:
(1) (a) 144 is a torc. (b) Its present arrangement,
twisted into three rings, may be a modern al-
teration; (c) it should probably be a single ring,
worn around the neck. (d) The terminals are
in the form of goats? heads.
According to Centering Theory (Grosz et al,
1995; Walker et al, 1998a), an important fac-
tor for the felicity of (1) is its entity coherence:
the way centers (discourse entities), such as
the referent of the NPs ?144? in clause (a) and
?its? in clause (b), are introduced and discussed
in subsequent clauses. It is often claimed in
current work on in natural language generation
that the constraints on felicitous text proposed
by the theory are useful to guide text struc-
turing, in combination with other factors (see
(Karamanis, 2003) for an overview). However,
how successful Centering?s constraints are on
their own in generating a felicitous text struc-
ture is an open question, already raised by the
seminal papers of the theory (Brennan et al,
1987; Grosz et al, 1995). In this work, we ex-
plored this question by developing an approach
to text structuring purely based on Centering,
in which the role of other factors is deliberately
ignored.
In accordance with recent work in the emerg-
ing field of text-to-text generation (Barzilay et
al., 2002; Lapata, 2003), we assume that the in-
put to text structuring is a set of clauses. The
output of text structuring is merely an order-
ing of these clauses, rather than the tree-like
structure of database facts often used in tradi-
tional deep generation (Reiter and Dale, 2000).
Our approach is further characterized by two
key insights. The first distinguishing feature is
that we assume a search-based approach to text
structuring (Mellish et al, 1998; Kibble and
Power, 2000; Karamanis and Manurung, 2002)
in which many candidate orderings of clauses
are evaluated according to scores assigned by
a given metric, and the best-scoring ordering
among the candidate solutions is chosen. The
second novel aspect is that our approach is
based on the position that the most straight-
forward way of using Centering for text struc-
turing is by defining a Centering-based metric
of coherence Karamanis (2003). Together, these
two assumptions lead to a view of text planning
in which the constraints of Centering act not
as filters, but as ranking factors, and the text
planner may be forced to choose a sub-optimal
solution.
However, Karamanis (2003) pointed out that
many metrics of coherence can be derived from
the claims of Centering, all of which could be
used for the type of text structuring assumed in
this paper. Hence, a general methodology for
identifying which of these metrics represent the
most promising candidates for text structuring
is required, so that at least some of them can
be compared empirically. This is the second re-
search question that this paper addresses, build-
ing upon previous work on corpus-based evalu-
ations of Centering, and particularly the meth-
ods used by Poesio et al (2004). We use the
gnome corpus (Poesio et al, 2004) as the do-
main of our experiments because it is reliably
annotated with features relevant to Centering
and contains the genre that we are mainly in-
terested in.
To sum up, in this paper we try to iden-
tify the most promising Centering-based metric
for text structuring, and to evaluate how useful
this metric is for that purpose, using corpus-
based methods instead of generally more expen-
sive psycholinguistic techniques. The paper is
structured as follows. After discussing how the
gnome corpus has been used in previous work
to evaluate the coherence of a text according to
Centering we discuss why such evaluations are
not sufficient for text structuring. We continue
by showing how Centering can be used to define
different metrics of coherence which might be
useful to drive a text planner. We then outline
a corpus-based methodology to choose among
these metrics, estimating how well they are ex-
pected to do when used by a text planner. We
conclude by discussing our experiments in which
this methodology is applied using a subset of the
gnome corpus.
2 Evaluating the coherence of a
corpus text according to Centering
In this section we briefly introduce Centering,
as well as the methodology developed in Poesio
et al (2004) to evaluate the coherence of a text
according to Centering.
2.1 Computing CF lists, CPs and CBs
According to Grosz et al (1995), each ?utter-
ance? in a discourse is assigned a list of for-
ward looking centers (CF list) each of which is
?realised? by at least one NP in the utterance.
The members of the CF list are ?ranked? in or-
der of prominence, the first element being the
preferred center CP.
In this paper, we used what we considered to
be the most common definitions of the central
notions of Centering (its ?parameters?). Poe-
sio et al (2004) point out that there are many
definitions of parameters such as ?utterance?,
?ranking? or ?realisation?, and that the setting
of these parameters greatly affects the predic-
tions of the theory;1 however, they found viola-
tions of the Centering constraints with any way
of setting the parameters (for instance, at least
25% of utterances have no CB under any such
setting), so that the questions addressed by our
work arise for all other settings as well.
Following most mainstream work on Center-
ing for English, we assume that an ?utterance?
corresponds to what is annotated as a finite unit
in the gnome corpus.2 The spans of text with
the indexes (a) to (d) in example (1) are exam-
ples. This definition of utterance is not optimal
from the point of view of minimizing Centering
violations (Poesio et al, 2004), but in this way
most utterances are the realization of a single
proposition; i.e., the impact of aggregation is
greatly reduced. Similarly, we use grammatical
function (gf) combined with linear order within
the unit (what Poesio et al (2004) call gfthere-
lin) for CF ranking. In this configuration, the
CP is the referent of the first NP within the unit
that is annotated as a subject for its gf.3
Example (2) shows the relevant annotation
features of unit u210 which corresponds to
utterance (a) in example (1). According to
gftherelin, the CP of (a) is the referent of ne410
?144?.
(2) <unit finite=?finite-yes? id=?u210?>
<ne id="ne410" gf="subj">144</ne>
is
<ne id="ne411" gf="predicate">
a torc</ne> </unit>.
The ranking of the CFs other than the
CP is defined according to the following pref-
erence on their gf (Brennan et al, 1987):
obj>iobj>other. CFs with the same gf are
ranked according to the linear order of the cor-
responding NPs in the utterance. The second
column of Table 1 shows how the utterances in
example (1) are automatically translated by the
scripts developed by Poesio et al (2004) into a
1For example, one could equate ?utterance? with sen-
tence (Strube and Hahn, 1999; Miltsakaki, 2002), use
indirect realisation for the computation of the CF list
(Grosz et al, 1995), rank the CFs according to their
information status (Strube and Hahn, 1999), etc.
2Our definition includes titles which are not always
finite units, but excludes finite relative clauses, the sec-
ond element of coordinated VPs and clause complements
which are often taken as not having their own CF lists
in the literature.
3Or as a post-copular subject in a there-clause.
CF list: cheapness
U {CP, other CFs} CB Transition CBn=CPn?1
(a) {de374, de375} n.a. n.a. n.a.
(b) {de376, de374, de377} de374 retain +
(c) {de374, de379} de374 continue ?
(d) {de380, de381, de382} - nocb +
Table 1: CP, CFs other than CP, CB, nocb or standard (see Table 2) transition and violations of
cheapness (denoted with an asterisk) for each utterance (U) in example (1)
coherence: coherence?:
CBn=CBn?1 CBn 6=CBn?1
or nocb in CFn?1
salience: CBn=CPn continue smooth-shift
salience?: CBn 6=CPn retain rough-shift
Table 2: coherence, salience and the table of standard transitions
sequence of CF lists, each decomposed into the
CP and the CFs other than the CP, according
to the chosen setting of the Centering param-
eters. Note that the CP of (a) is the center
de374 and that the same center is used as the
referent of the other NPs which are annotated
as coreferring with ne410.
Given two subsequent utterances Un?1 and
Un, with CF lists CFn?1 and CFn respectively,
the backward looking center of Un, CBn, is de-
fined as the highest ranked element of CFn?1
which also appears in CFn (Centering?s Con-
straint 3). For instance, the CB of (b) is de374.
The third column of Table 1 shows the CB for
each utterance in (1).4
2.2 Computing transitions
As the fourth column of Table 1 shows, each
utterance, with the exception of (a), is also
marked with a transition from the previous one.
When CFn and CFn?1 do not have any cen-
ters in common, we compute the nocb transi-
tion (Kibble and Power, 2000) (Poesio et als
null transition) for Un (e.g., utterance (d) in
Table 1).5
4In accordance with Centering, no CB is computed
for (a), the first utterance in the sequence.
5In this study we do not take indirect realisation into
account, i.e., we ignore the bridging reference (anno-
tated in the corpus) between the referent of ?it? de374
in (c) and the referent of ?the terminals? de380 in (d),
by virtue of which de374 might be thought as being a
member of the CF list of (d). Poesio et al (2004) showed
that hypothesizing indirect realization eliminates many
violations of entity continuity, the part of Constraint
1 that rules out nocb transitions. However, in this work
we are treating CF lists as an abstract representation
Following again the terminology in Kibble
and Power (2000), we call the requirement that
CBn be the same as CBn?1 the principle of co-
herence and the requirement that CBn be the
same as CPn the principle of salience. Each
of these principles can be satisfied or violated
while their various combinations give rise to the
standard transitions of Centering shown in Ta-
ble 2; Poesio et als scripts compute these vio-
lations.6 We also make note of the preference
between these transitions, known as Centering?s
Rule 2 (Brennan et al, 1987): continue is pre-
ferred to retain, which is preferred to smooth-
shift, which is preferred to rough-shift.
Finally, the scripts determine whether CBn
is the same as CPn?1, known as the principle
of cheapness (Strube and Hahn, 1999). The
last column of Table 1 shows the violations of
cheapness (denoted with an asterisk) in (1).7
2.3 Evaluating the coherence of a text
and text structuring
The statistics about transitions computed as
just discussed can be used to determine the de-
gree to which a text conforms with, or violates,
Centering?s principles. Poesio et al (2004)
found that nocbs account for more than 50%
of the atomic facts the algorithm has to structure, i.e.,
we are assuming that CFs are arguments of such facts;
including indirectly realized entities in CF lists would
violate this assumption.
6If the second utterance in a sequence U2 has a CB,
then it is taken to be either a continue or a retain,
although U1 is not classified as a nocb.
7As for the other two principles, no violation of
cheapness is computed for (a) or when Un is marked as
a nocb.
of the transitions in the gnome corpus in con-
figurations such as the one used in this pa-
per. More generally, a significant percentage of
nocbs (at least 20%) and other ?dispreferred?
transitions was found with all parameter config-
urations tested by Poesio et al (2004) and in-
deed by all previous corpus-based evaluations of
Centering such as Passoneau (1998), Di Eugenio
(1998), Strube and Hahn (1999) among others.
These results led Poesio et al (2004) to the
conclusion that the entity coherence as formal-
ized in Centering should be supplemented with
an account of other coherence inducing factors
to explain what makes texts coherent.
These studies, however, do not investigate
the question that is most important from the
text structuring perspective adopted in this pa-
per: whether there would be alternative ways of
structuring the text that would result in fewer
violations of Centering?s constraints (Kibble,
2001). Consider the nocb utterance (d) in (1).
Simply observing that this transition is ?dispre-
ferred? ignores the fact that every other ordering
of utterances (b) to (d) would result in more
nocbs than those found in (1). Even a text-
structuring algorithm functioning solely on the
basis of the Centering constraints might there-
fore still choose the particular order in (1). In
other words, a metric of text coherence purely
based on Centering principles?trying to mini-
mize the number of nocbs?may be sufficient to
explain why this order of clauses was chosen,
at least in this particular genre, without need
to involve more complex explanations. In the
rest of the paper, we consider several such met-
rics, and use the texts in the gnome corpus to
choose among them. We return to the issue of
coherence (i.e., whether additional coherence-
inducing factors need to be stipulated in addi-
tion to those assumed in Centering) in the Dis-
cussion.
3 Centering-based metrics of
coherence
As said previously, we assume a text structuring
system taking as input a set of utterances rep-
resented in terms of their CF lists. The system
orders these utterances by applying a bias in
favour of the best scoring ordering among the
candidate solutions for the preferred output.8
In this section, we discuss how the Centering
8Additional assumptions for choosing between the or-
derings that are assigned the best score are presented in
the next section.
concepts just described can be used to define
metrics of coherence which might be useful for
text structuring.
The simplest way to define a metric of coher-
ence using notions from Centering is to classify
each ordering of propositions according to the
number of nocbs it contains, and pick the or-
dering with the fewest nocbs. We call this met-
ric M.NOCB, following (Karamanis and Manu-
rung, 2002). Because of its simplicity, M.NOCB
serves as the baseline metric in our experiments.
We consider three more metrics. M.CHEAP
is biased in favour of the ordering with the
fewest violations of cheapness. M.KP sums
up the nocbs and the violations of cheapness,
coherence and salience, preferring the or-
dering with the lowest total cost (Kibble and
Power, 2000). Finally, M.BFP employs the
preferences between standard transitions as ex-
pressed by Rule 2. More specifically, M.BFP
selects the ordering with the highest number
of continues. If there exist several orderings
which have the most continues, the one which
has the most retains is favoured. The number
of smooth-shifts is used only to distinguish
between the orderings that score best for con-
tinues as well as for retains, etc.
In the next section, we present a general
methodology to compare these metrics, using
the actual ordering of clauses in real texts of
a corpus to identify the metric whose behav-
ior mimics more closely the way these actual
orderings were chosen. This methodology was
implemented in a program called the System for
Evaluating Entity Coherence (seec).
4 Exploring the space of possible
orderings
In section 2, we discussed how an ordering of
utterances in a text like (1) can be translated
into a sequence of CF lists, which is the repre-
sentation that the Centering-based metrics op-
erate on. We use the term Basis for Comparison
(BfC) to indicate this sequence of CF lists. In
this section, we discuss how the BfC is used in
our search-oriented evaluation methodology to
calculate a performance measure for each metric
and compare them with each other. In the next
section, we will see how our corpus was used
to identify the most promising Centering-based
metric for a text classifier.
4.1 Computing the classification rate
The performance measure we employ is called
the classification rate of a metric M on a cer-
tain BfC B. The classification rate estimates
the ability of M to produce B as the output of
text structuring according to a specific genera-
tion scenario.
The first step of seec is to search through
the space of possible orderings defined by the
permutations of the CF lists that B consists of,
and to divide the explored search space into sets
of orderings that score better, equal, or worse
than B according to M.
Then, the classification rate is defined accord-
ing to the following generation scenario. We
assume that an ordering has higher chances of
being selected as the output of text structuring
the better it scores for M. This is turn means
that the fewer the members of the set of better
scoring orderings, the better the chances of B
to be the chosen output.
Moreover, we assume that additional factors
play a role in the selection of one of the order-
ings that score the same for M. On average, B
is expected to sit in the middle of the set of
equally scoring orderings with respect to these
additional factors. Hence, half of the orderings
with the same score will have better chances
than B to be selected by M.
The classification rate ? of a metric M on
B expresses the expected percentage of order-
ings with a higher probability of being gener-
ated than B according to the scores assigned
by M and the additional biases assumed by the
generation scenario as follows:
(3) Classification rate:
?(M,B) = Better(M) + Equal(M)2
Better(M) stands for the percentage of order-
ings that score better than B according to M,
whilst Equal(M) is the percentage of order-
ings that score equal to B according to M. If
?(Mx, B) is the classification rate of Mx on B,
and ?(My, B) is the classification rate of My on
B, My is a more suitable candidate than Mx
for generating B if ?(My, B) is smaller than
?(Mx, B).
4.2 Generalising across many BfCs
In order for the experimental results to be re-
liable and generalisable, Mx and My should be
compared on more than one BfC from a corpus
C. In our standard analysis, the BfCs B1, ..., Bm
from C are treated as the random factor in a
repeated measures design since each BfC con-
tributes a score for each metric. Then, the clas-
sification rates for Mx and My on the BfCs are
compared with each other and significance is
tested using the Sign Test. After calculating the
number of BfCs that return a lower classifica-
tion rate for Mx than for My and vice versa, the
Sign Test reports whether the difference in the
number of BfCs is significant, that is, whether
there are significantly more BfCs with a lower
classification rate for Mx than the BfCs with a
lower classification rate for My (or vice versa).9
Finally, we summarise the performance of M
on m BfCs from C in terms of the average clas-
sification rate Y :
(4) Average classification rate:
Y (M,C) = ?(M,B1)+...+?(M,Bm)m
5 Using the gnome corpus for a
search-based comparison of
metrics
We will now discuss how the methodology
discussed above was used to compare the
Centering-based metrics discussed in Section
3, using the original ordering of texts in the
gnome corpus to compute the average classi-
fication rate of each metric.
The gnome corpus contains texts from differ-
ent genres, not all of which are of interest to us.
In order to restrict the scope of the experiment
to the text-type most relevant to our study, we
selected 20 ?museum labels?, i.e., short texts
that describe a concrete artefact, which served
as the input to seec together with the metrics
in section 3.10
5.1 Permutation and search strategy
In specifying the performance of the metrics we
made use of a simple permutation heuristic ex-
ploiting a piece of domain-specific communica-
tion knowledge (Kittredge et al, 1991). Like
Dimitromanolaki and Androutsopoulos (2003),
we noticed that utterances like (a) in exam-
ple (1), should always appear at the beginning
of a felicitous museum label. Hence, we re-
stricted the orderings considered by the seec
9The Sign Test was chosen over its parametric al-
ternatives to test significance because it does not carry
specific assumptions about population distributions and
variance. It is also more appropriate for small samples
like the one used in this study.
10Note that example (1) is characteristic of the genre,
not the length, of the texts in our subcorpus. The num-
ber of CF lists that the BfCs consist of ranges from 4 to
16 (average cardinality: 8.35 CF lists).
Pair M.NOCB p Winner
lower greater ties
M.NOCB vs M.CHEAP 18 2 0 0.000 M.NOCB
M.NOCB vs M.KP 16 2 2 0.001 M.NOCB
M.NOCB vs M.BFP 12 3 5 0.018 M.NOCB
N 20
Table 3: Comparing M.NOCB with M.CHEAP, M.KP and M.BFP in gnome
to those in which the first CF list of B, CF1,
appears in first position.11
For very short texts like (1), which give rise to
a small BfC, the search space of possible order-
ings can be enumerated exhaustively. However,
when B consists of many more CF lists, it is im-
practical to explore the search space in this way.
Elsewhere we show that even in these cases it
is possible to estimate ?(M,B) reliably for the
whole population of orderings using a large ran-
dom sample. In the experiments reported here,
we had to resort to random sampling only once,
for a BfC with 16 CF lists.
5.2 Comparing M.NOCB with other
metrics
The experimental results of the comparisons of
the metrics from section 3, computed using the
methodology in section 4, are reported in Ta-
ble 3.
In this table, the baseline metric M.NOCB is
compared with each of M.CHEAP, M.KP and
M.BFP. The first column of the Table identifies
the comparison in question, e.g. M.NOCB ver-
sus M.CHEAP. The exact number of BfCs for
which the classification rate of M.NOCB is lower
than its competitor for each comparison is re-
ported in the next column of the Table. For ex-
ample, M.NOCB has a lower classification rate
than M.CHEAP for 18 (out of 20) BfCs from
the gnome corpus. M.CHEAP only achieves a
lower classification rate for 2 BfCs, and there
are no ties, i.e. cases where the classification
rate of the two metrics is the same. The p value
returned by the Sign Test for the difference in
the number of BfCs, rounded to the third deci-
mal place, is reported in the fifth column of the
Table. The last column of the Table 3 shows
M.NOCB as the ?winner? of the comparison
with M.CHEAP since it has a lower classifica-
11Thus, we assume that when the set of CF lists serves
as the input to text structuring, CF1 will be identified
as the initial CF list of the ordering to be generated
using annotation features such as the unit type which
distinguishes (a) from the other utterances in (1).
tion rate than its competitor for significantly
more BfCs in the corpus.12
Overall, the Table shows that M.NOCB does
significantly better than the other three metrics
which employ additional Centering concepts.
This result means that there exist proportion-
ally fewer orderings with a higher probability of
being selected than the BfC when M.NOCB is
used to guide the hypothetical text structuring
algorithm instead of the other metrics.
Hence, M.NOCB is the most suitable among
the investigated metrics for structuring the CF
lists in gnome. This in turn indicates that sim-
ply avoiding nocb transitions is more relevant
to text structuring than the combinations of the
other Centering notions that the more compli-
cated metrics make use of. (However, these no-
tions might still be appropriate for other tasks,
such as anaphora resolution.)
6 Discussion: the performance of
M.NOCB
We already saw that Poesio et al (2004) found
that the majority of the recorded transitions in
the configuration of Centering used in this study
are nocbs. However, we also explained in sec-
tion 2.3 that what really matters when trying
to determine whether a text might have been
generated only paying attention to Centering
constraints is the extent to which it would be
possible to ?improve? upon the ordering chosen
in that text, given the information that the text
structuring algorithm had to convey. The av-
erage classification rate of M.NOCB is an esti-
12No winner is reported for a comparison when the p
value returned by the Sign Test is not significant (ns),
i.e. greater than 0.05. Note also that despite conduct-
ing more than one pairwise comparison simultaneously
we refrain from further adjusting the overall threshold
of significance (e.g. according to the Bonferroni method,
typically used for multiple planned comparisons that em-
ploy parametric statistics) since it is assumed that choos-
ing a conservative statistic such as the Sign Test already
provides substantial protection against the possibility of
a type I error.
Pair M.NOCB p Winner
lower greater ties
M.NOCB vs M.CHEAP 110 12 0 0.000 M.NOCB
M.NOCB vs M.KP 103 16 3 0.000 M.NOCB
M.NOCB vs M.BFP 41 31 49 0.121 ns
N 122
Table 4: Comparing M.NOCB with M.CHEAP, M.KP and M.BFP using the novel methodology
in MPIRO
mate of exactly this variable, indicating whether
M.NOCB is likely to arrive at the BfC during
text structuring.
The average classification rate Y for
M.NOCB on the subcorpus of gnome studied
here, for the parameter configuration of Cen-
tering we have assumed, is 19.95%. This means
that on average the BfC is close to the top 20%
of alternative orderings when these orderings
are ranked according to their probability of
being selected as the output of the algorithm.
On the one hand, this result shows that al-
though the ordering of CF lists in the BfC
might not completely minimise the number of
observed nocb transitions, the BfC tends to
be in greater agreement with the preference to
avoid nocbs than most of the alternative or-
derings. In this sense, it appears that the BfC
optimises with respect to the number of poten-
tial nocbs to a certain extent. On the other
hand, this result indicates that there are quite
a few orderings which would appear more likely
to be selected than the BfC.
We believe this finding can be interpreted in
two ways. One possibility is that M.NOCB
needs to be supplemented by other features in
order to explain why the original text was struc-
tured this way. This is the conclusion arrived at
by Poesio et al (2004) and those text structur-
ing practitioners who use notions derived from
Centering in combination with other coherence
constraints in the definitions of their metrics.
There is also a second possibility, however: we
might want to reconsider the assumption that
human text planners are trying to ensure that
each utterance in a text is locally coherent.
They might do all of their planning just on the
basis of Centering constraints, at least in this
genre ?perhaps because of resource limitations?
and simply accept a certain degree of incoher-
ence. Further research on this issue will require
psycholinguistic methods; our analysis never-
theless sheds more light on two previously un-
addressed questions in the corpus-based evalu-
ation of Centering ? a) which of the Centering
notions are most relevant to the text structur-
ing task, and b) to which extent Centering on
its own can be useful for this purpose.
7 Further results
In related work, we applied the methodology
discussed here to a larger set of existing data
(122 BfCs) derived from the MPIRO system
and ordered by a domain expert (Dimitro-
manolaki and Androutsopoulos, 2003). As Ta-
ble 4 shows, the results from MPIRO verify the
ones reported here, especially with respect to
M.KP and M.CHEAP which are overwhelm-
ingly beaten by the baseline in the new do-
main as well. Also note that since M.BFP fails
to overtake M.NOCB in MPIRO, the baseline
can be considered the most promising solution
among the ones investigated in both domains
by applying Occam?s logical principle.
We also tried to account for some additional
constraints on coherence, namely local rhetor-
ical relations, based on some of the assump-
tions in Knott et al (2001), and what Kara-
manis (2003) calls the ?PageFocus? which cor-
responds to the main entity described in a text,
in our example de374. These results, reported
in (Karamanis, 2003), indicate that these con-
straints conflict with Centering as formulated in
this paper, by increasing - instead of reducing
- the classification rate of the metrics. Hence,
it remains unclear to us how to improve upon
M.NOCB.
In our future work, we would like to experi-
ment with more metrics. Moreover, although we
consider the parameter configuration of Center-
ing used here a plausible choice, we intend to ap-
ply our methodology to study different instan-
tiations of the Centering parameters, e.g. by
investigating whether ?indirect realisation? re-
duces the classification rate for M.NOCB com-
pared to ?direct realisation?, etc.
Acknowledgements
Special thanks to James Soutter for writing the
program which translates the output produced by
gnome?s scripts into a format appropriate for seec.
The first author was able to engage in this research
thanks to a scholarship from the Greek State Schol-
arships Foundation (IKY).
References
Regina Barzilay, Noemie Elhadad, and Kath-
leen McKeown. 2002. Inferring strategies
for sentence ordering in multidocument news
summarization. Journal of Artificial Intelli-
gence Research, 17:35?55.
Susan E. Brennan, Marilyn A. Fried-
man [Walker], and Carl J. Pollard. 1987. A
centering approach to pronouns. In Proceed-
ings of ACL 1987, pages 155?162, Stanford,
California.
Barbara Di Eugenio. 1998. Centering in Italian.
In Walker et al (Walker et al, 1998b), pages
115?137.
Aggeliki Dimitromanolaki and Ion Androut-
sopoulos. 2003. Learning to order facts for
discourse planning in natural language gen-
eration. In Proceedings of the 9th European
Workshop on Natural Language Generation,
Budapest, Hungary.
Barbara J. Grosz, Aravind K. Joshi, and Scott
Weinstein. 1995. Centering: A framework
for modeling the local coherence of discourse.
Computational Linguistics, 21(2):203?225.
Amy Isard, Jon Oberlander, Ion Androutsopou-
los, and Colin Matheson. 2003. Speaking the
users? languages. IEEE Intelligent Systems
Magazine, 18(1):40?45.
Nikiforos Karamanis and Hisar Maruli Manu-
rung. 2002. Stochastic text structuring us-
ing the principle of continuity. In Proceedings
of INLG 2002, pages 81?88, Harriman, NY,
USA, July.
Nikiforos Karamanis. 2003. Entity Coherence
for Descriptive Text Structuring. Ph.D. the-
sis, Division of Informatics, University of Ed-
inburgh.
Rodger Kibble and Richard Power. 2000. An
integrated framework for text planning and
pronominalisation. In Proceedings of INLG
2000, pages 77?84, Israel.
Rodger Kibble. 2001. A reformulation of Rule
2 of Centering Theory. Computational Lin-
guistics, 27(4):579?587.
Richard Kittredge, Tanya Korelsky, and Owen
Rambow. 1991. On the need for domain com-
munication knowledge. Computational Intel-
ligence, 7:305?314.
Alistair Knott, Jon Oberlander, Mick
O?Donnell, and Chris Mellish. 2001. Beyond
elaboration: The interaction of relations
and focus in coherent text. In T. Sanders,
J. Schilperoord, and W. Spooren, edi-
tors, Text Representation: Linguistic and
Psycholinguistic Aspects, chapter 7, pages
181?196. John Benjamins.
Mirella Lapata. 2003. Probabilistic text struc-
turing: Experiments with sentence ordering.
In Proceedings of ACL 2003, Saporo, Japan,
July.
Chris Mellish, Alistair Knott, Jon Oberlander,
and Mick O?Donnell. 1998. Experiments us-
ing stochastic search for text planning. In
Proceedings of the 9th International Work-
shop on NLG, pages 98?107, Niagara-on-the-
Lake, Ontario, Canada.
Eleni Miltsakaki. 2002. Towards an aposyn-
thesis of topic continuity and intrasenten-
tial anaphora. Computational Linguistics,
28(3):319?355.
Mick O?Donnell, Chris Mellish, Jon Oberlan-
der, and Alistair Knott. 2001. ILEX: An ar-
chitecture for a dynamic hypertext genera-
tion system. Natural Language Engineering,
7(3):225?250.
Rebecca J. Passoneau. 1998. Interaction of dis-
course structure with explicitness of discourse
anaphoric phrases. In Walker et al (Walker
et al, 1998b), pages 327?358.
Massimo Poesio, Rosemary Stevenson, Barbara
Di Eugenio, and Janet Hitzeman. 2004. Cen-
tering: a parametric theory and its instantia-
tions. Computational Linguistics, 30(3).
Ehud Reiter and Robert Dale. 2000. Building
Natural Language Generation Systems. Cam-
bridge.
Michael Strube and Udo Hahn. 1999. Func-
tional centering: Grounding referential coher-
ence in information structure. Computational
Linguistics, 25(3):309?344.
Marilyn A. Walker, Aravind K. Joshi, and
Ellen F. Prince. 1998a. Centering in nat-
urally occuring discourse: An overview. In
Walker et al (Walker et al, 1998b), pages
1?30.
Marilyn A. Walker, Aravind K. Joshi, and
Ellen F. Prince, editors. 1998b. Centering
Theory in Discourse. Clarendon Press, Ox-
ford.
Natural Language Directed Inference
in the
Presentation of Ontologies
Chris Mellish and Xiantang Sun
Department of Computing Science
University of Aberdeen
Aberdeen AB24 3UE, UK
{cmellish,xsun}@csd.abdn.ac.uk
Abstract
It is hard to come up with a general formalisation of
the problem of content determination in natural lan-
guage generation because of the degree of domain-
dependence that is involved. This paper presents
a novel way of looking at a class of content deter-
mination problems in terms of a non-standard kind
of inference, which we call natural language di-
rected inference. This is illustrated through exam-
ples from a system under development to present
parts of ontologies in natural language. Natural lan-
guage directed inference represents an interesting
challenge to research in automated reasoning and
natural language processing.
1 Introduction: Content Determination in
NLG
Content determination in natural language generation (NLG)
is the task of determining relevant material to be included in a
natural language text. Usually this is taken to involve in addi-
tion some planning of the overall structure of the text, as this
will affect whether particular combinations of content will be
able to be coherently realised. Because the details of con-
tent determination depend on characteristics of the applica-
tion domain and in general every NLG system has a different
domain or goals, there has been little success in coming up
with general models of the structure of this process. In par-
ticular, reference architectures for NLG have relatively little
to say about it [Mellish et al, 2004].
It is useful to distinguish between two broad classes of
content determination problems. ?Top-down? problems have
specific goals in terms, for instance, of convincing or persuad-
ing the reader about something. These have typically been
addressed through the framework of planning (e.g. [Moore,
1994]), where content is sought to fill in requirements of plan-
ning operators. Here the requirement to build a successful ar-
gument of some kind drives the process. On the other hand,
?bottom-up? problems require the production of a more gen-
eral expository or descriptive text that puts together informa-
tion to satisfy more diffuse goals. For these problems text
coherence is more imporant than which particular arguments
or points are made. For instance, the ILEX project aimed to
emulate a museum curator telling a good story to link together
a sequence of selected exhibits. It was argued that a more op-
portunistic approach to content determination was needed for
this sort of application [Mellish et al, 1998].
In this paper, we concentrate primarily on the ?bottom-up?
type of content determination problem. But what makes con-
tent determination hard in either case is largely the fact that
two different ?worlds? are involved ? the domain model and
the linguistic world. Content determination is selecting mate-
rial from the (not necessarily very linguistic) domain model,
e.g. facts, rules and numbers, in the hope that it will permit a
coherent realisation as a text. In between the domain model
? and the set of possible texts Text sits a possibly non-trivial
mapping ? (?realisation?):
? : {?|? is content selected from ?} ? 2Text
The problem is that since ? may be complex, it will be hard
to judge which content will yield the most successful text.
Meteer [1992] pointed out that this ?generation gap? in the
worst case will mean that content is formulated which is not
expressible in language at all. This is also related to the
?problem of logical form equivalence? [Shieber, 1993] which
arises because from a domain point of view two logically
equivalent formulae are interchangeable and so it is a mat-
ter of chance which of the many logically equivalent formu-
lae is given to a realiser. ? must therefore be able to choose
between realisations corresponding to all formulae logically
equivalent to its input.
The problems raised by Meteer and Shieber do not, how-
ever, always arise in practice. In many applications, the pos-
sible forms of ? are restricted enough and close enough to a
linguistic representation that one can be sure that there will
always be at least one value for ?. Also ? doesn?t have to
map onto all possible texts ? one can artificially limit the ex-
tent to which realisation diverges from what is suggested by
the surface form of the input. All NLG systems adopt these
sorts of simplications.
In the next two sections, we show how realisation and con-
tent determination initially worked in our project to present
ontologies in natural language. In section 4 we then consider
limitations of this approach to content determination, which
gives rise to the novel idea of treating content determination
as a kind of inference, natural language directed inference.
We then outline our initial steps to implement this process
and how it relates to existing work in automated reasoning
and natural language generation.
2 Realisation from Ontology Axioms
Our current research addresses the problem of presenting
parts of OWL DL [McGuinness and van Harmelen, 2004]
ontologies in natural language. This will extend existing
approaches to generating from simpler DLs (e.g. [Wagner
et al, 1999]) by taking into account the fact that in a lan-
guage like OWL DL a concept is described more by a set
of constraints than by a frame-like definition. For instance,
the bottom of Figure 1 shows a set of axioms relevant to
the concept TemporalRegion in an example ontology. Be-
cause there may be a number of axioms providing differ-
ent facts about a concept, the information cannot in gen-
eral be presented in a single sentence but requires an ex-
tended text with multiple sentences, the overall structure hav-
ing to be planned so as to be coherent as a discourse. Our
work is also different from other work which generates text
about individuals described using ontologies [Wilcock, 2003;
Bontcheva and Wilks, 2004], in that it presents the ontology
class axioms themselves.
In this section, we give an example of how ? complicates
the reasoning about appropriate content, by showing that al-
though the ? that we are developing is relatively simple, it
nevertheless complicates decisions about the complexity of
what can be presented in a sentence.
Given an axiom to be expressed as a sentence (we discuss
in section 3 how such axioms are selected), our realisation
approach uses rules with simple recursive structural patterns
and assembles text with grammatically-annotated templates1.
The idea is that we will collect rules for special case expres-
sions that can be realised relatively elegantly in language as
well as having generic rules that ensure that every possible
structure can be handled. Optimal English will arise from de-
tecting the part of speech of any class and role names which
are English words (as well as cases such as multiple word
names and roles such as ?hasX?, ?Xof? where X is a noun),
and we have been able to obtain this information with rea-
sonable quality automatically using WordNet2. Unless such
conventions are used in the ontology definition or the reader
is familiar with some of the ontology terms, it will not be pos-
sible to convey any useful information to them without extra
1Note that our initial approach is to see how much can be
achieved with no restrictions on the ontology (as long as it is ex-
pressed in legal OWL DL) and only generic linguistic resources
(such as WordNet [Miller, 1995]). This is partly because there is
a need to present parts of current ontologies, which often come with
no consistent commenting or linguistic annotations, and partly so
that we can then make informed recommendations about what kinds
of extra annotations would be valuable in the ontologies of the fu-
ture. Also, note that the term ?realisation? will be taken here to in-
clude elements of ?microplanning? which, for instance, introduces
appropriate pronominalisation.
2We cannot guarantee that the ontology writer will use such
mnemonic names (if not, then generation will have to use the less op-
timised templates), but we should exploit these cases when they arise
(and our investigations have shown that they are extremely common
in human-written ontologies).
   
   
  
  









  
  
  



  
  
  
Region AbstractRegion TimeInterval
A2 A51
A10
A63 A45
TemporalRegion Perdurant
Concession
A10: TemporalRegion v Region
A2: AbstractRegion u TemporalRegion =?
A63: T imeInterval v TemporalRegion
A45: Perdurant v ?HappenAt.T imeInterval
A51: AbstractRegion v Region
Figure 1: Graph of axioms
domain-specific resources.
A given axiom may match multiple rules and therefore
have multiple possible realisations. For instance, the axiom:
Student ? Person u ?Supervisor.Academic
would be mapped to ?A student is a person with at least
one academic supervisor?, which exploits knowledge of the
lexical categories of the names used, but another possibility
would be something like ?Something in class student is some-
thing in class person with at least one value for the role super-
visor which is something in class academic? (this might have
been the only possibility if the class names had been arbitrary
identifiers such as ?Class1? and ?Class2?.).
Where a logical formula has multiple realisations, a mea-
sure of linguistic complexity of the results can be used to se-
lect a preferred one. Currently we measure linguistic com-
plexity as the number of words in the English string gen-
erated. Better measures will take into account the shape of
the parse tree. Notice that linguistic complexity does not di-
rectly mirror the complexity of the formula, but depends on ?
and whatever linguistic resources underlie it. Although more
complex formulae tend to yield more complex linguistic out-
put, linguistic complexity is also affected by:
? The extent to which special-case shorter rules match
some of its subexpressions
? The extent to which class and role names can be inter-
preted as English words of relevant classes
? Whether a recursive linguistic structure uses left, right
or centre embedding [Miller and Isard, 1964]
The linguistic complexity of a formula is obtained by taking
the linguistic complexity of the realisation that is least com-
plex. Again, although there is a correlation with the complex-
ity of the formula, the relevant complexity for deciding, for
instance, whether a formula can be presented in a single sen-
tence, is a linguistic one which needs to take ? into account.
What is a Temporal Region?
[One kind of Temporal Region is a Time Interval.]
[A Perdurant can happen at a Time Interval.]
[nothing is both a Temporal Region and an Abstract Region.]but[An Abstract Region is also a kind of Region]
[A Temporal Region is a kind of Region.] 
Figure 2: Example text with coherence relations shown
3 Selecting Material
The designer of an ontology has chosen one of many pos-
sible logically equivalent ways to axiomatise their domain,
and this is important information. Therefore our initial ap-
proach worked from the axioms themselves without manipu-
lating them in any way.
We basically followed the same procedure for content de-
termination as in the ILEX system [O?Donnell et al, 2001].
Thus the axioms can be seen as forming a graph, where each
axiom is connected to the concepts it mentions (and where
there may also be other links for relations between axioms)
? see Figure 1. In this graph, routes between axioms cor-
respond to different possible transitions in a coherent text ?
a text proceeds from one sentence to another by exploiting
shared entities or by virtue of a rhetorical relation between
the sentences3.
A possible hand-generated text from the above axioms,
showing the coherence relations which hold by virtue of
shared entities or a rhetorical relation (the latter shown in
dashes) is shown in Figure 2.
Assuming for the moment that a user has asked the ques-
tion What is X?, where X is some class used in the ontology,
selecting the axioms to express in the answer involves a best-
first search for axioms, starting at the entity X. Each axiom is
evaluated according to:
? how close it is (in terms of edges of the graph) to the
concept X, and
? how intrinsically interesting, important and understand-
able it is.
? how few times is has already been presented
Following ILEX, these three measures are multiplied together
and, for a text of length n, the n facts with the highest mea-
sures are selected for inclusion. The first component of the
measure ensures that the retrieved axioms are relevant to the
question to be answered. In terms of this, the best axioms
to use are ones directly involving the class X. On the other
3The ILEX model makes use of the idea that there may be rhetor-
ical relations, such as concession or background, between facts
which could potentially be expressed in a text. It is however not
immediately clear how they arise in our context. It seems that it
may be plausible to say, for instance, ?Although students are people
and lecturers are people, lecturers and students are disjoint?, but the
general principles for this need to be worked out.
hand, axioms that are only indirectly involved with X can be
selected if they score well according to the second compo-
nent (or if there are not enough closer axioms). The fact that
there is a path between X and each chosen axiom ensures that
there is a way of linking the two in a coherent text, by pro-
gressively moving the focus entity of the text to new entities
in the axioms already expressed or through expressing rhetor-
ical relations.
The second component of the evaluation score for axioms
can be used to make the system sensitive to the user, for in-
stance by preferring axioms that involve concepts known to
the user or axioms that have not previously been told to them.
We have not yet exploited this feature. The third component
penalises axioms that have already been presented.
4 Natural Language Directed Inference
The content determination approach just described, which se-
lects from among the provided axioms, suffers from a number
of deficiencies:
Over-complex sentences: The axioms may not package the
available information appropriately for natural language
sentences. On the one hand, an axiom may be too
complex to express in a single sentence (as determined
by applying ? and measuring the linguistic complex-
ity). In this case, it might be appropriate to present a
?weaker? (axiom). For instance, instead of expressing
X ? Y t Z t . . . one might express Y v X (if it men-
tions the entities needed for coherence with the rest of
the text).
Repetitive sentences: On the other hand, the axioms may
give rise to sentences that are short and repetitive. Thus,
rather than using three sentences to express:
Student v Person
Student v UnEmployed
Student v ?Supervisor.Academic
one could combine them all into a formula realised as
?a student is an unemployed person with at least one
academic supervisor?. In NLG, the process of build-
ing such complex sentences is known as ?aggregation?
[Shaw, 1995]. This kind of aggregation could be imple-
mented by combining the axioms together before reali-
sation is performed, but success can only be measured
by looking at the linguistic complexity of the result.
Inappropriate focus: An axiom may be expressed in a way
that, when realised, places inappropriate emphasis on
entities. For instance, an axiom X v Y could be re-
alised by ?An X is a kind of Y?, whereas the equivalent
Y w X could be realised by ?Y?s include X?s?. The
latter would be much better than the former at a point in
a text that is discussing the properties of Y. The above
example of ?weakening? also has the effect of changing
the likely subject of the sentence produced. Sometimes
the text will be better if one can switch around the mate-
rial in an axiom to emphasise different material.
Misleading partial information: It may be better to present
some of the consequences of an axiom, given the rest
of the theory, rather than the axiom itself. For instance,
instead of presenting
Student v ?supervisor.Academic
in an ontology which also has the axiom
functional(supervisor), it would be more infor-
mative to present the consequence
Student v = 1 supervisor.Academic
Indeed, with number restrictions a reader can draw false
implicatures (in the sense of [Grice, 1975]) if only par-
tial information is presented. In this case, a scalar im-
plicature [Levinson, 1983] is involved. A reader, on be-
ing told that ?a student has at least one academic su-
pervisor?, will naturally assume that they could have
more than one, or that they could have other supervi-
sors belonging to other classes. Similarly, on being told
?a supervisor of a student is always an academic?, one
will assume that there can be more than one supervi-
sor (otherwise the text would have said ?the supervisor
. . .?). Some of the principles at work here may be simi-
lar to those encountered in cooperative question answer-
ing [Gaasterland et al, 1992].
The only way to overcome these limitations is to enable con-
tent determination to select material in more ways than just
choosing an axiom. It must always choose to express some-
thing that is true, given the logical theory, and content deter-
mination will therefore be a form of inference. In general, in
fact, we could consider using any logical consequence of the
axioms. However, not all logical consequences are equally
good. The formulae that are presented should:
1. Soundness: follow from the original logical theory (set of
axioms)
2. Relevance: contribute information relevant to the goal of
the text. For instance, if the goal is to answer the ques-
tion ?what is concept X?? then the formulae should be
about X or other concepts which shed light on X.
3. Conservatism: be not very different from the original ax-
ioms (and so capture some of the intent behind those
axioms)
4. Complexity: have appropriate linguistic complexity (sec-
tion 2)
5. Coherence: satisfy linguistic coherence constraints (i.e.
be linked to other selected material by the kinds of re-
lations discussed in section 3).
6. Novelty: not have already been expressed (and not be tau-
tologies). There is no point in weakening axioms to the
point that nothing new is expressed, or in presenting the
same material many times.
7. Fullness: be complete, to the extent that they don?t sup-
port false implicatures
8. User-orientation: be in accord with user model prefer-
ences (as in section 3)
We call the kind of inference required to find such logical
consequences natural language directed inference (NLDI). It
is a kind of forwards inference with very specific goals, which
arise from its use for natural language generation.
Although we have motivated NLDI through our own par-
ticular content determination problem, this may be a useful
way to view content determination in general, as long as the
starting point can be viewed as some kind of logical theory,
?, there is an available realisation relation ? and an evaluation
function eval for linguistic outputs, which takes into account
the above desiderata. In this case, content determination can
be viewed as the problem of determining
argmax(? such that ?|=?) max{eval(t)|t ? ?(?)}
The process of enumerating promising consequences of ?
for this optimisation is certainly a form of logical inference.
But its goal is unlike standard goals of automated reasoning
and is shaped by the idiosyncracies of the requirements for
natural language output. There is an interesting parallel here
with the work of [Sripada et al, 2003]. Sripada et alfound
that, for generating natural language summaries of time se-
ries data, standard data analysis algorithms such as segmenta-
tion had to be modified. They characterised the extra require-
ments that forced these modifications in terms of the Gricean
maxims of cooperative communication [Grice, 1975]. Our
8 desiderata above could also be thought of as cases of the
Gricean maxims.
5 Techniques for NLDI
Unfortunately, standard refutation-based approaches to in-
ference rely on having a precisely specified inference goal,
whose negation is incompatible with the axioms. For DLs,
the standard tableaux methods [Horrocks, 1998] have simi-
lar properties. NLDI does not have an inference goal that
can be expressed in structural terms, so even approaches to
?matching? cannot straightforwardly be used to derive lin-
guistically appropriate results. NLDI is more akin to other
?non-standard? types of inference, perhaps to approximation
[Brandt et al, 2002], though again the target logical language
is without a simple formal characterisation. Perhaps the clos-
est approach we are aware of is meta-level control of infer-
ence, where factors outside of the logic (e.g. other kinds of
descriptions of the shapes of logical formulae) are used to
guide inference [Bundy and Welham, 1981].
One advantage of NLDI is that it does not have to be a com-
plete inference procedure, though in general the more logical
consequences of the original axioms it can find, the more pos-
sible texts will be considered and the higher the quality of the
one chosen.
Realisation 
Evaluation
Inference
Ontology Axioms
Text
Feedback score Final result
Possible sequence of formulae
Figure 3: Overgeneration Architecture
The approach to NLDI we are currently working on is in-
spired by the idea of ?overgeneration? approaches to NLG, as
used, for instance, by those using statistical models [Langk-
ilde and Knight, 1998] and instance-based search [Varges and
Mellish, 2001]. In this approach, instead of attempting to in-
telligently order the relevant choices to come up with an op-
timal text, an NLG system consciously enumerates a large
number of possible texts (in a cheap way) and then chooses
between them using a linguistically-aware evaluation func-
tion of some kind (the eval of NLDI). Our approach differs
from these others, however, in that, whereas the other sys-
tems implement overgeneration of surface forms, we consider
overgeneration of possible content.
Figure 3 shows the architecture of our system under de-
velopment. The simple inference system implements a beam
search among possible sets of content for generating texts,
where each state in the search space is a sequence of formu-
lae. In logical terms, each sequence represents a conjunction
that follows from the input axioms. The resulting text for any
such sequence (i.e. the result of applying ?) will be the re-
sult of realising the elements of the sequence, in order, as the
sentences of the text.
At each point in the search, the current state can give rise
to new states in two possible ways:
1. One of the original axioms is added to the end of the
sequence.
2. The final formula of the sequence is replaced by a for-
mula inferred from it (given the whole axiom set) by one
inference step
The inference steps represent simple ways of modifying a for-
mula to something close to it which follows from the com-
plete set of axioms and which may yield a more appropriate
realisation. We have currently implemented a small number
of relevant steps, including steps for aggregation, disaggrega-
tion and elimination of disjunctions.
Whenever a new state in the search space is generated, it is
sent to the realisation component (which implements ?) and
from there through an evaluation function (which implements
eval). The evaluation function takes into account the average
deviation of the sentence lengths (in words) from an ?ideal?
sentence length and some other heuristics (see below). This
is used as feedback to drive the search of the inference com-
ponent in a best-first manner. The search terminates when
the best scoring state is one element longer that the desired
number of sentences for the text, at which point its sequence
of formulae, apart from the last one, is returned. The ex-
ploration to a length longer than the desired one ensures that
other states shorter than or equal to the desired length have a
chance to be explored.
Our approach makes initial attempts to address the desider-
ata of NLDI by constraining the search in the following ways:
1. Soundness: All new formulae are derived by sound rules
of inference from the existing axioms and so are true.
2. Relevance: Only axioms which might affect the interpre-
tation of the class asked about are ever considered (the
rest are discarded at the start of the process). For the
purposes of this, we use the conservative relevant-only
translation of [Tsarkov et al, 2004] to discard axioms
that cannot be relevant to the question.
3. Conservatism: Inferred formulae are based on individual
axioms, and shorter inferences are enumerated before
longer ones.
4. Complexity: The complexity of the best realisations is
used to order the search candidates. Candidates which
are inappropriate for realisation do not match the reali-
sation rules and so are not considered.
5. Coherence: When a new axiom is added to a sequence, it
is constrained in its realisation to have a subject which
is a class mentioned in the previous element of the se-
quence. The subject of the first element of the sequence
must be the class which is the subject of the original
question. Also the evaluation function has a preference
for the first sentence with a given class as subject to be
an ?is a? type sentence.
6. Novelty: In order to prevent information being presented
more than once, only one logical consequence of any
given axiom is ever included in a sequence. This is im-
plemented via a simple way of tracking the axioms that
have contributed to each formula. This makes the as-
sumption that the original axioms are logically indepen-
dent.
7. Fullness: Formulae are closed with respect to cardinality
information before being added to the lists.
8. User-orientation: We don?t currently take this into ac-
count, but intend to reward formulae that contain class
and role names already familiar to the user (e.g. used
in answers to previous questions, or appearing earlier in
the answer to the current question).
All of these are relatively crude measures, which nevertheless
give some appropriate direction to the process.
This system has been implemented and tested informally
on examples from three different ontologies. For example, in
creating a 3-sentence text to answer ?What is an Electrode??
using a fuel cell ontology with 133 axioms, the relevance filter
first of all reduces the set of axioms to 31, which include:
(1) Electrode v Actuality
(2) Electrode v ?contains.Catalyst
(3) Electrode v (?contains.Support u
? 1 contains.>)
(4) domain(contains,
FuelCell t MEA t Electrode t Catalyst)
as well as other axioms such as Catalyst v
?contains.ActiveMetal. If these 4 axioms were se-
lected unchanged and realised in this order (which by chance
happens to be quite a good order), then the following text
would result:
An Electrode is a kind of Actuality. An Elec-
trode always contains something which is a Cat-
alyst. An Electrode always contains something
which is a Support and always contains at most 1
thing. Only something which is a FuelCell, a MEA,
an Electrode or a Catalyst contains something.
Instead of this, our simple implementation of NLDI proceeds
as follows. The initial states are those axioms which when
realised will have Electrode in subject position, with a pref-
erence for those that will be realised as ?an Electrode is a ...?.
Thus the state consisting of the one element sequence:
Electrode v Actuality
will be a favourite. This state can be developed in several
ways. For instance, another axiom could be aggregated with
this one (to give a sentence of the form ?an Electrode is an
actuality which ...?). Another possibility is for this axiom to
be accepted in this form and for another axiom to be added
to the end of the sequence. This second possibility generates
the following state, among others:
Electrode v Actuality
Electrode v = 1 contains.Catalyst
(notice how more precise cardinality information has been at-
tached to axiom (2)). This state can be further developed by
adding a further axiom, or by applying an inference rule to
the last added formula. In this case, aggregation with axiom
(3) is possible, yielding:
Electrode v Actuality
Electrode v = 1 contains.(CatalystuSupport)
This state is further developed by adding new axioms to the
end, and so on. The final sequence of formulae selected is:
Electrode v Actuality
Electrode v = 1 contains.(CatalystuSupport)
domain(contains,
FuelCell t MEA t Electrode t Catalyst)
This is realised by the following short text:
An Electrode is a kind of Actuality. An Elec-
trode contains exactly one thing, which must be a
Catalyst and a Support. Only something which is
a FuelCell, a MEA, an Electrode or a Catalyst con-
tains something.
(This realisation relies on part-of-speech information which
can be obtained automatically from WordNet, apart from the
term ?MEA?).
6 Discussion
Although NLG lacks a general account of content determi-
nation, one area of content determination that has been well
formalised is the problem of generating referring expressions.
Here the task is to find a distinguishing description of an en-
tity that it is true of the entity but not of any of the ?distrac-
tors? in some current context. Recent work has formalised
NLG algorithms for referring expression generation in terms
of algorithms for finding an appropriate subgraph of a graph
representing the domain knowledge [Krahmer et al, 2003].
Given that the graphs involved are very similar to Concep-
tual Graphs [Sowa, 1984] and that the projection relation be-
tween Conceptual Graphs (an extended notion of subgraph) is
a kind of inference, it follows that these referring expression
algorithms can also be viewed as performing inference. As
work considers an increasing range of referring expressions
(e.g. using relations, logical connectives, plurals and even
quantifiers), the complexity of the inference required is forc-
ing researchers increasingly to depart from the original graph
matching approach. We believe that it may well prove pro-
ductive to view this as a case of NLDI, especially as (in spite
of the assumptions of most current work) logical complexity
and linguistic complexity are not always the same.
There are many issues to be addressed in the development
of a convincing approach to NLDI. For instance, it is neces-
sary to determine what kinds of inference steps are relevant
to the optimisation of linguistic properties. In our system, we
would certainly like to introduce unfolding operations to steer
the system towards using concepts that the reader is famil-
iar with. In addition, ideas from linear logic [Girard, 1987]
may be relevant to avoiding duplication in the information
conveyed. Finally, there are real questions about the ideal ar-
chitecture of an NLDI system. If the eval function or ? is
expensive, then it may be necessary to interleave the evalua-
tion and the inference steps more than we have done, to the
extent that inference is directly aimed at achieving linguistic
effects.
Acknowledgments
This work is supported by EPSRC research grant GR/S62932.
Many thanks to Ian Horrocks, Alan Rector and members of
the Aberdeen NLG group for useful discussions.
References
[Bontcheva and Wilks, 2004] K. Bontcheva and Y. Wilks.
Automatic report generation from ontologies: the mi-
akt approach. In Ninth International Conference on Ap-
plications of Natural Language to Information Systems
(NLDB?2004), Manchester, UK, August 2004.
[Brandt et al, 2002] S. Brandt, R. Ku?sters, and A.-Y.
Turhan. Approximation and difference in description log-
ics. In D. Fensel, D. McGuinness, and M.-A. Williams,
editors, Procs of KR-02. Morgan Kaufmann Publishers,
2002.
[Bundy and Welham, 1981] Alan Bundy and Bob Welham.
Using meta-level inference for selective application of
multiple rewrite rule sets in algebraic manipulation. Ar-
tificial Intelligence, 16(2):111?224, May 1981.
[Gaasterland et al, 1992] T Gaasterland, P Godfrey, and
J Minker. An overview of cooperative answering. Intel-
ligent Information Systems, 1(2):123?157, 1992.
[Girard, 1987] J.-Y. Girard. Linear logic. Theoretical Com-
puter Science, 50:1?102, 1987.
[Grice, 1975] H. P. Grice. Logic and conversation. In P. Cole
and J. Morgan, editors, Syntax and Semantics: Vol 3,
Speech Acts. Academic Press, 1975.
[Horrocks, 1998] Ian Horrocks. Using an expressive descrip-
tion logic: Fact or fiction? In Proceedings of the 6th In-
ternational Conference on Principles of Knowledge Rep-
resentation and Reasoning (KR?98), pages 636?647, 1998.
[Krahmer et al, 2003] E. Krahmer, S. van Erk, and A. Ver-
leg. Graph-based generation of referring expressions.
Computational Linguistics, 29(1):53?72, 2003.
[Langkilde and Knight, 1998] I. Langkilde and K. Knight.
Generation that exploits corpus-based statistical knowl-
edge. In Proc. of the Conference of the Association for
Computational Linguistics (COLING/ACL), 1998.
[Levinson, 1983] S. C. Levinson. Pragmatics. Cambridge
University Press, 1983.
[McGuinness and van Harmelen, 2004] D. L.
McGuinness and F. van Harmelen.
Owl web ontology language overview.
http://www.w3.org/TR/owl-features/,
2004.
[Mellish et al, 1998] C. Mellish, M. O?Donnell, J. Ober-
lander, and A. Knott. An architecture for opportunistic
text generation. In Proceedings of the ninth international
workshop on natural language generation, pages 28?37.
Association for Computational Linguistics, 1998.
[Mellish et al, 2004] C. Mellish, M. Reape, D. Scott,
L. Cahill, R. Evans, and D. Paiva. A reference architecture
for generation systems. Natural Language Engineering,
10(3/4):227?260, 2004.
[Meteer, 1992] Marie Meteer. Expressibility and the Prob-
lem of Efficient Text Planning. Pinter publishers, London,
1992.
[Miller and Isard, 1964] G. Miller and S. Isard. Free recall
of self embedded english sentences. Information and Con-
trol, 7:292?303, 1964.
[Miller, 1995] G. Miller. Wordnet: A lexical database for
english. CACM, 38(11):39?41, 1995.
[Moore, 1994] Johanna Moore. Participating in Explanatory
Dialogues. MIT Press, 1994.
[O?Donnell et al, 2001] M. O?Donnell, A. Knott, C. Mel-
lish, and J. Oberlander. ILEX: The architecture of a dy-
namic hypertext generation system. Natural Language En-
gineering, 7:225?250, 2001.
[Shaw, 1995] James Shaw. Conciseness through aggregation
in text generation. In Procs of the 33rd Annual Meeting of
the Association for Computational Linguistics, MIT, 1995.
[Shieber, 1993] Stuart Shieber. The problem of logical-form
equivalence. Computational Linguistics, 19(1):179?190,
March 1993.
[Sowa, 1984] J. F. Sowa. Conceptual Structures - Informa-
tion Processing in Mind and Machine. Addison-Wesley,
1984.
[Sripada et al, 2003] S. Sripada, E. Reiter, J. Hunter, and
J. Yu. Generating english summaries of time series data
using the gricean maxims. In Proceedings of the Ninth
ACM SIGMOD International Conference on Knowledge
Discovery and Data Mining (KDD-2003), pages 187?196,
2003.
[Tsarkov et al, 2004] Dmitry Tsarkov, Alexandre Riazanov,
Sean Bechhofer, and Ian Horrocks. Using vampire to rea-
son with owl. In Sheila A. McIlraith, Dimitris Plexousakis,
and Frank van Harmelen, editors, Procs of the 2004 In-
ternational Semantic Web Conference (ISWC 2004), pages
471?485. Springer LNCS 3298, 2004.
[Varges and Mellish, 2001] S. Varges and C. Mellish.
Instance-based natural language generation. In Procs of
NAACL-01. Carnegie Mellon University, 2001.
[Wagner et al, 1999] J. Wagner, J. Rogers, R. Baud, and J-
R. Scherrer. Natural language generation of surgical pro-
cedures. Medical Informatics, 53:175?192, 1999.
[Wilcock, 2003] G. Wilcock. Talking owls: Towards an on-
tology verbalizer. In Human Language Technology for the
Semantic Web and Web Services, ISWC-2003, pages 109?
112, Sanibel Island, Florida, 2003.
Using a Corpus of Sentence Orderings Defined by Many Experts
to Evaluate Metrics of Coherence for Text Structuring
Nikiforos Karamanis
Computational Linguistics Research Group
University of Wolverhampton, UK
N.Karamanis@wlv.ac.uk
Chris Mellish
Department of Computing Science
University of Aberdeen, UK
cmellish@csd.abdn.ac.uk
Abstract
This paper addresses two previously unresolved is-
sues in the automatic evaluation of Text Structuring
(TS) in Natural Language Generation (NLG). First,
we describe how to verify the generality of an exist-
ing collection of sentence orderings defined by one
domain expert using data provided by additional
experts. Second, a general evaluation methodol-
ogy is outlined which investigates the previously
unaddressed possibility that there may exist many
optimal solutions for TS in the employed domain.
This methodology is implemented in a set of ex-
periments which identify the most promising can-
didate for TS among several metrics of coherence
previously suggested in the literature.1
1 Introduction
Research in NLG focused on problems related to TS from
very early on, [McKeown, 1985] being a classic example.
Nowadays, TS continues to be an extremely fruitful field of
diverse active research. In this paper, we assume the so-
called search-based approach to TS [Karamanis et al, 2004]
which employs a metric of coherence to select a text struc-
ture among various alternatives. The TS module is hypothe-
sised to simply order a preselected set of information-bearing
items such as sentences [Barzilay et al, 2002; Lapata, 2003;
Barzilay and Lee, 2004] or database facts [Dimitromanolaki
and Androutsopoulos, 2003; Karamanis et al, 2004].
Empirical work on the evaluation of TS has become in-
creasingly automatic and corpus-based. As pointed out by
[Karamanis, 2003; Barzilay and Lee, 2004] inter alia, using
corpora for automatic evaluation is motivated by the fact that
employing human informants in extended psycholinguistic
experiments is often simply unfeasible. By contrast, large-
scale automatic corpus-based experimentation takes place
much more easily.
[Lapata, 2003] was the first to present an experimental set-
ting which employs the distance between two orderings to es-
timate automatically how close a sentence ordering produced
1Chapter 9 of [Karamanis, 2003] reports the study in more detail.
by her probabilistic TS model stands in comparison to order-
ings provided by several human judges.
[Dimitromanolaki and Androutsopoulos, 2003] derived
sets of facts from the database of MPIRO, an NLG system
that generates short descriptions of museum artefacts [Isard
et al, 2003]. Each set consists of 6 facts each of which cor-
responds to a sentence as shown in Figure 1. The facts in
each set were manually assigned an order to reflect what a
domain expert, i.e. an archaeologist trained in museum la-
belling, considered to be the most natural ordering of the
corresponding sentences. Patterns of ordering facts were au-
tomatically learned from the corpus created by the expert.
Then, a classification-based TS approach was implemented
and evaluated in comparison to the expert?s orderings.
Database fact Sentence
subclass(ex1, amph) ? This exhibit is an amphora.
painted-by(ex1, p-Kleo) ? This exhibit was decorated by
the Painter of Kleofrades.
painter-story(p-Kleo, en4049) ? The Painter of Kleofrades
used to decorate big vases.
exhibit-depicts(ex1, en914) ? This exhibit depicts a warrior performing
splachnoscopy before leaving for the battle.
current-location(ex1, wag-mus) ? This exhibit is currently displayed
in the Martin von Wagner Museum.
museum-country(wag-mus, ger) ? The Martin von Wagner Museum
is in Germany.
Figure 1: MPIRO database facts corresponding to sentences
A subset of the corpus created by the expert in the previous
study (to whom we will henceforth refer as E0) is employed
by [Karamanis et al, 2004] who attempt to distinguish be-
tween many metrics of coherence with respect to their use-
fulness for TS in the same domain. Each human ordering of
facts in the corpus is scored by each of these metrics which
are then penalised proportionally to the amount of alternative
orderings of the same material that are found to score equally
to or better than the human ordering. The few metrics which
manage to outperform two simple baselines in their overall
performance across the corpus emerge as the most suitable
candidates for TS in the investigated domain. This method-
ology is very similar to the way [Barzilay and Lee, 2004]
evaluate their probabilistic TS model in comparison to the
approach of [Lapata, 2003].
Because the data used in the studies of [Dimitromanolaki
and Androutsopoulos, 2003] and [Karamanis et al, 2004]
are based on the insights of just one expert, an obvious un-
resolved question is whether they reflect general strategies
for ordering facts in the domain of interest. This paper ad-
dresses this issue by enhancing the dataset used in the two
studies with orderings provided by three additional experts.
These orderings are then compared with the orders of E0 us-
ing the methodology of [Lapata, 2003]. Since E0 is found
to share a lot of common ground with two of her colleagues
in the ordering task, her reliability is verified, while a fourth
?stand-alone? expert who uses strategies not shared by any
other expert is identified as well.
As in [Lapata, 2003], the same dependent variable which
allows us to estimate how different the orders of E0 are from
the orders of her colleagues is used to evaluate some of the
metrics which perform best in [Karamanis et al, 2004]. As
explained in the next section, in this way we investigate the
previously unaddressed possibility that there may exist many
optimal solutions for TS in our domain. The results of this
additional evaluation experiment are presented and emphasis
is laid on their relation with the previous findings.
Overall, this paper addresses two general issues: a) how to
verify the generality of a dataset defined by one expert using
sentence orderings provided by other experts and b) how to
employ these data for the automatic evaluation of a TS ap-
proach. Given that the methodology discussed in this paper
does not rely on the employed metrics of coherence or the as-
sumed TS approach, our work can be of interest to any NLG
researcher facing these questions.
The next section discusses how the methodology imple-
mented in this study complements the methods of [Karamanis
et al, 2004]. After briefly introducing the employed metrics
of coherence, we describe the data collected for our exper-
iments. Then, we present the employed dependent variable
and formulate our predictions. In the results section, we state
which of these predictions were verified. The paper is con-
cluded with a discussion of the main findings.
2 An additional evaluation test
As [Barzilay et al, 2002] report, different humans often order
sentences in distinct ways. Thus, there might exist more than
one equally good solution for TS, a view shared by almost
all TS researchers, but which has not been accounted for in
the evaluation methodologies of [Karamanis et al, 2004] and
[Barzilay and Lee, 2004].2
Collecting sentence orderings defined by many experts in
our domain enables us to investigate the possibility that there
might exist many good solutions for TS. Then, the measure
of [Lapata, 2003], which estimates how close two orderings
stand, can be employed not only to verify the reliability of E0
but also to compare the orderings preferred by the assumed
TS approach with the orderings of the experts.
However, this evaluation methodology has its limitations
as well. Being engaged in other obligations, the experts nor-
mally have just a limited amount of time to devote to the
2A more detailed discussion of existing corpus-based methods
for evaluating TS appears in [Karamanis and Mellish, 2005].
NLG researcher. Similarly to standard psycholinguistic ex-
periments, consulting these informants is difficult to extend
to a larger corpus like the one used e.g. by [Karamanis et al,
2004] (122 sets of facts).
In this paper, we reach a reasonable compromise by show-
ing how the methodology of [Lapata, 2003] supplements the
evaluation efforts of [Karamanis et al, 2004] using a similar
(yet by necessity smaller) dataset. Clearly, a metric of coher-
ence that has already done well in the previous study, gains
extra bonus by passing this additional test.
3 Metrics of coherence
[Karamanis, 2003] discusses how a few basic notions of co-
herence captured by Centering Theory (CT) can be used to
define a large range of metrics which might be useful for TS
in our domain of interest.3 The metrics employed in the ex-
periments of [Karamanis et al, 2004] include:
M.NOCB which penalises NOCBs, i.e. pairs of adjacent
facts without any arguments in common [Karamanis and
Manurung, 2002]. Because of its simplicity M.NOCB
serves as the first baseline in the experiments of [Kara-
manis et al, 2004].
PF.NOCB, a second baseline, which enhances M.NOCB
with a global constraint on coherence that [Karamanis,
2003] calls the PageFocus (PF).
PF.BFP which is based on PF as well as the original for-
mulation of CT in [Brennan et al, 1987].
PF.KP which makes use of PF as well as the recent re-
formulation of CT in [Kibble and Power, 2000].
[Karamanis et al, 2004] report that PF.NOCB outper-
formed M.NOCB but was overtaken by PF.BFP and PF.KP.
The two metrics beating PF.NOCB were not found to differ
significantly from each other.
This study employs PF.BFP and PF.KP, i.e. two of the best
performing metrics of the experiments in [Karamanis et al,
2004], as well as M.NOCB and PF.NOCB, the two previously
used baselines. An additional random baseline is also defined
following [Lapata, 2003].
4 Data collection
16 sets of facts were randomly selected from the corpus of
[Dimitromanolaki and Androutsopoulos, 2003].4 The sen-
tences that each fact corresponds to and the order defined by
E0 was made available to us as well. We will subsequently
refer to an unordered set of facts (or sentences that the facts
correspond to) as a Testitem.
4.1 Generating the BestOrders for each metric
Following [Karamanis et al, 2004], we envisage a TS ap-
proach in which a metric of coherence M assigns a score to
3Since discussing the metrics in detail is well beyond the scope
of this paper, the reader is referred to Chapter 3 of [Karamanis, 2003]
for more information on this issue.
4These are distinct from, yet very similar to, the sets of facts used
in [Karamanis et al, 2004].
each possible ordering of the input set of facts and selects the
best scoring ordering as the output. When many orderings
score best, M chooses randomly between them. Crucially, our
hypothetical TS component only considers orderings starting
with the subclass fact (e.g. subclass(ex1, amph)
in Figure 1) following the suggestion of [Dimitromanolaki
and Androutsopoulos, 2003]. This gives rise to 5! = 120
orderings to be scored by M for each Testitem.
For the purposes of this experiment, a simple algorithm
was implemented that first produces the 120 possible order-
ings of facts in a Testitem and subsequently ranks them ac-
cording to the scores given by M. The algorithm outputs the
set of BestOrders for the Testitem, i.e. the orderings which
score best according to M. This procedure was repeated for
each metric and all Testitems employed in the experiment.
4.2 Random baseline
Following [Lapata, 2003], a random baseline (RB) was im-
plemented as the lower bound of the analysis. The random
baseline consists of 10 randomly selected orderings for each
Testitem. The orderings are selected irrespective of their
scores for the various metrics.
4.3 Consulting domain experts
Three archaeologists (E1, E2, E3), one male and two females,
between 28 and 45 years of age, all trained in cataloguing
and museum labelling, were recruited from the Department
of Classics at the University of Edinburgh.
Each expert was consulted by the first author in a separate
interview. First, she was presented with a set of six sentences,
each of which corresponded to a database fact and was printed
on a different filecard, as well as with written instructions de-
scribing the ordering task.5 The instructions mention that the
sentences come from a computer program that generates de-
scriptions of artefacts in a virtual museum. The first sentence
for each set was given by the experimenter.6 Then, the expert
was asked to order the remaining five sentences in a coherent
text.
When ordering the sentences, the expert was instructed to
consider which ones should be together and which should
come before another in the text without using hints other than
the sentences themselves. She could revise her ordering at
any time by moving the sentences around. When she was sat-
isfied with the ordering she produced, she was asked to write
next to each sentence its position, and give them to the ex-
perimenter in order to perform the same task with the next
randomly selected set of sentences. The expert was encour-
aged to comment on the difficulty of the task, the strategies
she followed, etc.
5 Dependent variable
Given an unordered set of sentences and two possible order-
ings, a number of measures can be employed to calculate the
5The instructions are given in Appendix D of [Karamanis, 2003]
and are adapted from the ones used in [Barzilay et al, 2002].
6This is the sentence corresponding to the subclass fact.
distance between them. Based on the argumentation in [How-
ell, 2002], [Lapata, 2003] selects Kendall?s ? as the most ap-
propriate measure and this was what we used for our analysis
as well. Kendall?s ? is based on the number of inversions
between the two orderings and is calculated as follows:
(1) ? = 1? 2IPN = 1?
2I
N(N?1)/2
PN stands for the number of pairs of sentences and N is the
number of sentences to be ordered.7 I stands for the number
of inversions, that is, the number of adjacent transpositions
necessary to bring one ordering to another. Kendall?s ? ranges
from ?1 (inverse ranks) to 1 (identical ranks). The higher the
? value, the smaller the distance between the two orderings.
Following [Lapata, 2003], the Tukey test is employed to in-
vestigate significant differences between average ? scores.8
First, the average distance between (the orderings of)9 two
experts e.g. E0 and E1, denoted as T (E0E1), is calculated as
the mean ? value between the ordering of E0 and the order-
ing of E1 taken across all 16 Testitems. Then, we compute
T (EXPEXP ) which expresses the overall average distance
between all expert pairs and serves as the upper bound for the
evaluation of the metrics. Since a total of E experts gives rise
to PE = E(E?1)2 expert pairs, T (EXPEXP ), is computedby summing up the average distances between all expert pairs
and dividing the sum by PE .
While [Lapata, 2003] always appears to single out a unique
best scoring ordering, we often have to deal with many best
scoring orderings. To account for this, we first compute the
average distance between e.g. the ordering of an expert E0
and the BestOrders of a metric M for a given Testitem. In
this way, M is rewarded for a BestOrder that is close to the
expert?s ordering, but penalised for every BestOrder that is
not. Then, the average T (E0M ) between the expert E0 and
the metric M is calculated as their mean distance across all
16 Testitems. Finally, yet most importantly, T (EXPM ) is the
average distance between all experts and M. It is calculated by
summing up the average distances between each expert and M
and dividing the sum by the number of experts. As the next
section explains in more detail, T (EXPM ) is compared with
the upper bound of the evaluation T (EXPEXP ) to estimate
the performance of M in our experiments.
RB is evaluated in a similar way as M using the 10 ran-
domly selected orderings instead of the BestOrders for each
Testitem. T (EXPRB) is the average distance between all ex-
perts and RB and is used as the lower bound of the evaluation.
7In our data, N is always equal to 6.
8Provided that an omnibus ANOVA is significant, the Tukey test
can be used to specify which of the conditions c1, ..., cn measured
by the dependent variable differ significantly. It uses the set of means
m1, ...,mn (corresponding to conditions c1, ..., cn) and the mean
square error of the scores that contribute to these means to calculate
a critical difference between any two means. An observed differ-
ence between any two means is significant if it exceeds the critical
difference.
9Throughout the paper we often refer to e.g. ?the distance be-
tween the orderings of the experts? with the phrase ?the distance
between the experts? for the sake of brevity.
E0E1: ** ** **
0.692 E0E2: ** ** **
0.717 E1E2: ** ** **
0.758 E0E3:
CD at 0.01: 0.338 0.258 E1E3:
CD at 0.05: 0.282 0.300 E2E3:
F(5,75)=14.931, p<0.000 0.192
Table 1: Comparison of distances between the expert pairs
6 Predictions
Despite any potential differences between the experts, one ex-
pects them to share some common ground in the way they or-
der sentences. In this sense, a particularly welcome result for
our purposes is to show that the average distances between
E0 and most of her colleagues are short and not significantly
different from the distances between the other expert pairs,
which in turn indicates that she is not a ?stand-alone? expert.
Moreover, we expect the average distance between the ex-
pert pairs to be significantly smaller than the average distance
between the experts and RB. This is again based on the as-
sumption that even though the experts might not follow com-
pletely identical strategies, they do not operate with absolute
diversity either. Hence, we predict that T (EXPEXP ) will be
significantly greater than T (EXPRB).
Due to the small number of Testitems employed in this
study, it is likely that the metrics do not differ significantly
from each other with respect to their average distance from
the experts. Rather than comparing the metrics directly with
each other (as [Karamanis et al, 2004] do), this study com-
pares them indirectly by examining their behaviour with re-
spect to the upper and the lower bound. For instance, al-
though T (EXPPF.KP ) and T (EXPPF.BFP ) might not be
significantly different from each other, one score could be sig-
nificantly different from T (EXPEXP ) (upper bound) and/or
T (EXPRB) (lower bound) while the other is not.
We identify the best metrics in this study as the ones whose
average distance from the experts (i) is significantly greater
from the lower bound and (ii) does not differ significantly
from the upper bound.10
7 Results
7.1 Distances between the expert pairs
On the first step in our analysis, we computed the T score
for each expert pair, namely T (E0E1), T (E0E2), T (E0E3),
T (E1E2), T (E1E3) and T (E2E3). Then we performed all
15 pairwise comparisons between them using the Tukey test,
the results of which are summarised in Table 1.11
The cells in the Table report the level of significance re-
turned by the Tukey test when the difference between two
10Criterion (ii) can only be applied provided that the average dis-
tance between the experts and at least one metric Mx is found to
be significantly lower than T (EXPEXP ). Then, if the average dis-
tance between the experts and another metric My does not differ
significantly from T (EXPEXP ), My performs better than Mx.
11The Table also reports the result of the omnibus ANOVA, which
is significant: F(5,75)=14.931, p<0.000.
E0E1: ** ** **
0.692 E0E2: ** ** **
0.717 E1E2: ** ** **
0.758 E0RB :
CD at 0.01: 0.242 0.323 E1RB :
CD at 0.05: 0.202 0.347 E2RB :
F(5,75)=18.762, p<0.000 0.352
E0E3:
0.258 E1E3:
0.300 E2E3:
CD at 0.01: 0.219 0.192 E3RB :
CD at 0.05: 0.177 0.302
F(3,45)=1.223, p=0.312
Table 2: Comparison of distances between the experts (E0,
E1, E2, E3) and the random baseline (RB)
distances exceeds the critical difference (CD). Significance
beyond the 0.05 threshold is reported with one asterisk (*),
while significance beyond the 0.01 threshold is reported with
two asterisks (**). A cell remains empty when the difference
between two distances does not exceed the critical difference.
For example, the value of T (E0E1) is 0.692 and the value of
T (E0E3) is 0.258. Since their difference exceeds the CD at
the 0.01 threshold, it is reported to be significant beyond that
level by the Tukey test, as shown in the top cell of the third
column in Table 1.
As the Table shows, the T scores for the distance between
E0 and E1 or E2, i.e. T (E0E1) and T (E0E2), as well as the
T for the distance between E1 and E2, i.e. T (E1E2), are quite
high which indicates that on average the orderings of the three
experts are quite close to each other. Moreover, these T scores
are not significantly different from each other which suggests
that E0, E1 and E2 share quite a lot of common ground in
the ordering task. Hence, E0 is found to give rise to similar
orderings to the ones of E1 and E2.
However, when any of the previous distances is compared
with a distance that involves the orderings of E3 the differ-
ence is significant, as shown by the cells containing two as-
terisks in Table 1. In other words, although the orderings of
E1 and E2 seem to deviate from each other and the orderings
of E0 to more or less the same extent, the orderings of E3
stand much further away from all of them. Hence, there ex-
ists a ?stand-alone? expert among the ones consulted in our
studies, yet this is not E0 but E3.
This finding can be easily explained by the fact that by con-
trast to the other three experts, E3 followed a very schematic
way for ordering sentences. Because the orderings of E3
manifest rather peculiar strategies, at least compared to the or-
derings of E0, E1 and E2, the upper bound of the analysis, i.e.
the average distance between the expert pairs T (EXPEXP ),
is computed without taking into account these orderings:
(2) T (EXPEXP ) = 0.722 = T (E0E1)+T (E0E2)+T (E1E2)3
7.2 Distances between the experts and RB
As the upper part of Table 2 shows, the T score between any
two experts other than E3 is significantly greater than their
distance from RB beyond the 0.01 threshold. Only the dis-
tances between E3 and another expert, shown in the lower
section of Table 2, are not significantly different from the dis-
tance between E3 and RB.
Although this result does not mean that the orders of E3
are similar to the orders of RB,12 it shows that E3 is roughly
as far away from e.g. E0 as she is from RB. By contrast,
E0 stands significantly closer to E1 than to RB, and the same
holds for the other distances in the upper part of the Table.
In accordance with the discussion in the previous section, the
lower bound, i.e. the overall average distance between the
experts (excluding E3) and RB T (EXPRB), is computed as
shown in (3):
(3) T (EXPRB) = 0.341 = T (E0RB)+T (E1RB)+T (E2RB)3
7.3 Distances between the experts and each metric
So far, E3 was identified as an ?stand-alone? expert standing
further away from the other three experts than they stand from
each other. We also identified the distance between E3 and
each expert as similar to her distance from RB.
Similarly, E3 was found to stand further away from the
metrics compared to their distance from the other three ex-
perts.13 This result, gives rise to the set of formulas in (4) for
calculating the overall average distance between the experts
(excluding E3) and each metric.
(4) (4.1): T (EXPPF.BFP ) = 0.629 =
T (E0PF.BFP )+T (E1PF.BFP )+T (E2PF.BFP )
3
(4.2): T (EXPPF.KP ) = 0.571 =
T (E0PF.KP )+T (E1PF.KP )+T (E2PF.KP )
3
(4.3): T (EXPPF.NOCB) = 0.606 =
T (E0PF.NOCB)+T (E1PF.NOCB)+T (E2PF.NOCB)
3
(4.4): T (EXPM.NOCB) = 0.487 =
T (E0M.NOCB)+T (E1M.NOCB)+T (E2M.NOCB)
3
In the next section, we present the concluding analysis for
this study which compares the overall distances in formu-
las (2), (3) and (4) with each other. As we have already
mentioned, T (EXPEXP ) serves as the upper bound of the
analysis whereas T (EXPRB) is the lower bound. The aim
is to specify which scores in (4) are significantly greater than
T (EXPRB), but not significantly lower than T (EXPEXP ).
7.4 Concluding analysis
The results of the comparisons of the scores in (2), (3) and (4)
are shown in Table 3. As the top cell in the last column of
the Table shows, the T score between the experts and RB,
T (EXPRB), is significantly lower than the average distance
between the expert pairs, T (EXPEXP ) at the 0.01 level.
12This could have been argued, if the value of T (E3RB) had been
much closer to 1.
13Due to space restrictions, we cannot report the scores for these
comparisons here. The reader is referred to Table 9.4 on page 175
of Chapter 9 in [Karamanis, 2003].
This result verifies one of our main predictions showing that
the orderings of the experts (modulo E3) stand much closer
to each other compared to their distance from randomly as-
sembled orderings.
As expected, most of the scores that involve the met-
rics are not significantly different from each other, ex-
cept for T (EXPPF.BFP ) which is significantly greater than
T (EXPM.NOCB) at the 0.05 level. Yet, what we are mainly
interested in is how the distance between the experts and each
metric compares with T (EXPEXP ) and T (EXPRB). This
is shown in the first row and the last column of Table 3.
Crucially, T (EXPRB) is significantly lower than
T (EXPPF.BFP ) as well as T (EXPPF.NOCB) and
T (EXPPF.KP ) at the 0.01 level. Notably, even the dis-
tance of the experts from M.NOCB, T (EXPM.NOCB), is
significantly greater than T (EXPRB), albeit at the 0.05
level. These results show that the distance from the experts is
significantly reduced when using the best scoring orderings
of any metric, even M.NOCB, instead of the orderings of
RB. Hence, all metrics score significantly better than RB in
this experiment.
However, simply using M.NOCB to output the best
scoring orders is not enough to yield a distance from
the experts which is comparable to T (EXPEXP ). Al-
though the PF constraint appears to help towards this di-
rection, T (EXPPF.KP ) remains significantly lower than
T (EXPEXP ), whereas T (EXPPF.NOCB) falls only 0.009
points short of CD at the 0.05 threshold. Hence, PF.BFP
is the most robust metric, as the difference between
T (EXPPF.BFP ) and T (EXPEXP ) is clearly not signifi-
cant.
Finally, the difference between T (EXPPF.NOCB) and
T (EXPM.NOCB) is only 0.006 points away from the CD.
This result shows that the distance from the experts is reduced
to a great extent when the best scoring orderings are com-
puted according to PF.NOCB instead of simply M.NOCB.
Hence, this experiment provides additional evidence in favour
of enhancing M.NOCB with the PF constraint of coherence,
as suggested in [Karamanis, 2003].
8 Discussion
A question not addressed by previous studies making use of
a certain collection of orderings of facts is whether the strate-
gies reflected there are specific to E0, the expert who created
the dataset. In this paper, we address this question by enhanc-
ing E0?s dataset with orderings provided by three additional
experts. Then, the distance between E0 and her colleagues
is computed and compared to the distance between the other
expert pairs. The results indicate that E0 shares a lot of com-
mon ground with two of her colleagues in the ordering task
deviating from them as much as they deviate from each other,
while the orderings of a fourth ?stand-alone? expert are found
to manifest rather individualistic strategies.
The same variable used to investigate the distance between
the experts is employed to automatically evaluate the best
scoring orderings of some of the best performing metrics in
[Karamanis et al, 2004]. Despite its limitations due to the
necessarily restricted size of the employed dataset, this eval-
EXPEXP : ** ** **
0.722 EXPPF.BFP : * **
0.629 EXPPF.NOCB : **
0.606 EXPPF.KP : **
CD at 0.01: 0.150 0.571 EXPM.NOCB : *
CD at 0.05: 0.125 0.487 EXPRB :
F(5,75)=19.111, p<0.000 0.341
Table 3: Results of the concluding analysis comparing the distance between the expert pairs (EXPEXP ) with the distance
between the experts and each metric (PF.BFP, PF.NOCB, PF.KP, M.NOCB) and the random baseline (RB)
uation task allows us to explore the previously unaddressed
possibility that there exist many good solutions for TS in the
employed domain.
Out of a much larger set of possibilities, 10 metrics were
evaluated in [Karamanis et al, 2004], only a handful of which
were found to overtake two simple baselines. The additional
test in this study carries on the elimination process by point-
ing out PF.BFP as the single most promising metric to be used
for TS in the explored domain, since this is the metric that
manages to clearly survive both tests.
Equally crucially, our analysis shows that all employed
metrics are superior to a random baseline. Additional evi-
dence in favour of the PF constraint on coherence introduced
in [Karamanis, 2003] is provided as well. The general evalu-
ation methodology as well as the specific results of this study
will be useful for any subsequent attempt to automatically
evaluate a TS approach using a corpus of sentence orderings
defined by many experts.
As [Reiter and Sripada, 2002] suggest, the best way to treat
the results of a corpus-based study is as hypotheses which
eventually need to be integrated with other types of evalua-
tion. Although we followed the ongoing argumentation that
using perceptual experiments to choose between many possi-
ble metrics is unfeasible, our efforts have resulted into a sin-
gle preferred candidate which is much easier to evaluate with
the help of psycholinguistic techniques (instead of having to
deal with a large number of metrics from very early on). This
is indeed our main direction for future work in this domain.
Acknowledgments
We are grateful to Aggeliki Dimitromanolaki for entrusting
us with her data and for helpful clarifications on their use; to
Mirella Lapata for providing us with the scripts for the com-
putation of ? together with her extensive and prompt advice;
to Katerina Kolotourou for her invaluable assistance in re-
cruiting the experts; and to the experts for their participation.
This work took place while the first author was studying at
the University of Edinburgh, supported by the Greek State
Scholarship Foundation (IKY).
References
[Barzilay and Lee, 2004] Regina Barzilay and Lillian Lee. Catch-
ing the drift: Probabilistic content models with applications to
generation and summarization. In Proceedings of HLT-NAACL
2004, pages 113?120, 2004.
[Barzilay et al, 2002] Regina Barzilay, Noemie Elhadad, and
Kathleen McKeown. Inferring strategies for sentence ordering
in multidocument news summarization. Journal of Artificial In-
telligence Research, 17:35?55, 2002.
[Brennan et al, 1987] Susan E. Brennan, Marilyn A. Fried-
man [Walker], and Carl J. Pollard. A centering approach to pro-
nouns. In Proceedings of ACL 1987, pages 155?162, Stanford,
California, 1987.
[Dimitromanolaki and Androutsopoulos, 2003] Aggeliki Dimitro-
manolaki and Ion Androutsopoulos. Learning to order facts for
discourse planning in natural language generation. In Proceed-
ings of the 9th European Workshop on Natural Language Gener-
ation, Budapest, Hungary, 2003.
[Howell, 2002] David C. Howell. Statistical Methods for Psychol-
ogy. Duxbury, Pacific Grove, CA, 5th edition, 2002.
[Isard et al, 2003] Amy Isard, Jon Oberlander, Ion Androutsopou-
los, and Colin Matheson. Speaking the users? languages. IEEE
Intelligent Systems Magazine, 18(1):40?45, 2003.
[Karamanis and Manurung, 2002] Nikiforos Karamanis and
Hisar Maruli Manurung. Stochastic text structuring using the
principle of continuity. In Proceedings of INLG 2002, pages
81?88, Harriman, NY, USA, July 2002.
[Karamanis and Mellish, 2005] Nikiforos Karamanis and Chris
Mellish. A review of recent corpus-based methods for evaluat-
ing text structuring in NLG. 2005. Submitted to Using Corpora
for NLG workshop.
[Karamanis et al, 2004] Nikiforos Karamanis, Chris Mellish, Jon
Oberlander, and Massimo Poesio. A corpus-based methodology
for evaluating metrics of coherence for text structuring. In Pro-
ceedings of INLG04, pages 90?99, Brockenhurst, UK, 2004.
[Karamanis, 2003] Nikiforos Karamanis. Entity Coherence for De-
scriptive Text Structuring. PhD thesis, Division of Informatics,
University of Edinburgh, 2003.
[Kibble and Power, 2000] Rodger Kibble and Richard Power. An
integrated framework for text planning and pronominalisation. In
Proceedings of INLG 2000, pages 77?84, Israel, 2000.
[Lapata, 2003] Mirella Lapata. Probabilistic text structuring: Ex-
periments with sentence ordering. In Proceedings of ACL 2003,
pages 545?552, Saporo, Japan, July 2003.
[McKeown, 1985] Kathleen McKeown. Text Generation: Using
Discourse Strategies and Focus Constraints to Generate Natural
Language Text. Studies in Natural Language Processing. Cam-
bridge University Press, 1985.
[Reiter and Sripada, 2002] Ehud Reiter and Somayajulu Sripada.
Should corpora texts be gold standards for NLG? In Proceedings
of INLG 2002, pages 97?104, Harriman, NY, USA, July 2002.
Proceedings of the 12th European Workshop on Natural Language Generation, pages 146?153,
Athens, Greece, 30 ? 31 March 2009. c?2009 Association for Computational Linguistics
Towards Empirical Evaluation of Affective Tactical NLG
Ielka van der Sluis
Trinity College Dublin
Dublin
ielka.vandersluis@cs.tcd.ie
Chris Mellish
University of Aberdeen
Aberdeen
c.mellish@abdn.ac.uk
Abstract
One major aim of research in affective nat-
ural language generation is to be able to
use language intelligently to induce effects
on the emotions of the reader/ hearer. Al-
though varying the content of generated
language (?strategic? choices) might be
expected to change the effect on emotions,
it is not obvious that varying the form of
the language (?tactical? choices) can do
this. Indeed, previous experiments have
been unable to show emotional effects of
tactical variations. Building on what has
been discovered in previous experiments,
we present a new experiment which does
demonstrate such effects. This represents
an important step towards the empirical
evaluation of affective NLG systems.
1 Introduction
This paper is about developing techniques for the
empirical evaluation of affective natural language
generation (NLG). Affective NLG has been de-
fined as ?NLG that relates to, arises from or de-
liberately influences emotions or other non-strictly
rational aspects of the Hearer? (De Rosis and
Grasso, 2000). It currently covers two main
strands of work, the portrayal of non-rational as-
pects in an artificial speaker/writer (e.g. the work
of Mairesse and Walker (2008) on projecting per-
sonality) and the use of NLG in ways sensitive to
the non-rational aspects of the hearer/reader and
calculated to achieve effects on these aspects (e.g.
the work of De Rosis et al (1999) on generat-
ing instructions in an emotionally charged situa-
tion and that of Moore et al (2004) on producing
appropriate tutorial feedback). Although there has
been success in evaluating work of the first kind,
it remains more problematic to evaluate whether
work of the second type directly affects emotion or
mood, or whether it influences task performance
for other reasons.
Since the work of Thompson (1977), NLG tasks
have been considered to divide mainly into those
involving strategy (?deciding what to say?) and
tactics (?deciding how to say it?). It seems clear
that one can affect a reader?s emotion differently
by making different strategic decisions about con-
tent (e.g. telling someone that they have passed
an exam will make them happier than telling them
that they have failed), but it is less clear that tac-
tical alternations (e.g. involving ordering of ma-
terial, choice of words or syntactic constructions)
can have these kinds of effects. Unfortunately,
the exact dividing line between strategy and tac-
tics remains a matter of debate. For the purpose
of this paper, we take ?strategic? to cover matters
of basic propositional content (the basic informa-
tion to be communicated) and ?tactical? to include
most linguistic issues, including matters of em-
phasis and focus, inasmuch as they can be influ-
enced by linguistic formulation. It is important to
know whether tactical choices can influence emo-
tions because to a large extent NLG research con-
centrates on tactical issues (partly because strate-
gic NLG remains a rather domain-specific activ-
ity).
Some light on the effects of tactical variations
in text is shed by work in Psychology, where there
has been a great deal of work on the effects of the
?framing? of a text (Moxey and Sanford, 2000;
Teigen and Brun, 2003). Some of this has been
industrially funded, as there are considerable ap-
plications, for instance, in advertising. The alter-
native texts considered differ in ways that NLG re-
searchers would call tactical. For instance, a piece
of meat could be described as ?75% lean? or ?25%
fat?, and arguably these are alternative truthful de-
scriptions of the same situation. However, evalu-
ation of this work has been primarily in terms of
whether it affects people?s choices or evaluations
146
of options available (Levin et al, 1998), or other
aspects of task performance (O?Hara and Stern-
berg, 2001; Brown and Pinel, 2003; Cadinu et al,
2005). As far as we know it is unknown whether
emotions can be affected in this way. There is
therefore an open question about whether it is pos-
sible to detect the non-rational effects of differ-
ent tactical decisions on readers. We believe that
achieving this is important for the further scientific
development of affective NLG.
In the rest of this paper, we discuss previous
(unsuccessful) attempts to measure emotional ef-
fects of tactical decisions in texts (section 2), the
particular linguistic choices we have focussed on,
including a text validation experiment (section 3)
and our choice of a method for measuring emo-
tions (section 4). In section 5 we then present a
new study which for the first time demonstrates
significant differences in emotions evoked in read-
ers associated with tactical textual variations. We
then briefly reflect on this result in a concluding
section.
2 Background for the Present Study
In (van der Sluis and Mellish, 2008) we de-
scribed several experiments investigating differ-
ent methods of measuring the effects of texts on
emotions to demonstrate that tactical differences
would lead to differences in effects. Our method
was to present participants with texts about cancer-
causing chemicals in foods or unexpected health-
giving properties of drinking water and to attempt
to measure the emotions invoked by different vari-
ations of these texts. However, we were unable
to show statistically significant results of tactical
variations. We mentioned the following possible
explanations for this:
? We used methods where participants reported on their
own emotions. However, it could be that (in this con-
text) participants were unwilling or unable to report ac-
curately.
? The self-reporting methods used were perhaps not fine
grained enough to register the differences between the
effects of similar texts.
? The texts themselves were perhaps too subtly different
or not long enough to induce strong emotions.
? The participants were perhaps not involved enough in
the task to get strong emotions.
We believe that of these, the final reason is the
most compelling. The self-reporting methods used
had been validated and used in multiple previous
studies in Psychology, and so there was no rea-
son to suggest that they would fundamentally fail
in this new context. The granularity of the mea-
surement methods can be improved relatively sim-
ply (see section 4 below). But it is very believ-
able that the participants would fail to be really
concerned by the texts in the experiments reported
since the source was unclear, the message a gen-
eral one not addressed to them individually and the
topic (healthy and unhealthy food) one that occurs
often enough in newspapers to fail to overcome
natural boredom.
The main innovation of the experiment we de-
scribe below was in our method of seeking the
emotional involvement of the participants. The
texts that the participants read took the form of
?feedback? on a (fake) IQ test that they undertook
as part of the experiment. We selected university
students as the participants, as they would likely
be concerned about their intelligence, especially
as compared to their peers. The texts appeared to
be written individually for the participants and so
sought to engage them directly.
3 Linguistic Choice and Framing
As in (van der Sluis and Mellish, 2008), the study
we present here sought to evoke positive emotions
to differing extents in a reader by tactical manip-
ulations to ?slant? the tasks positively to varying
degrees. This section describes the text variations
used and their validation.
3.1 Tactical Methods
The two texts produced for this experiment were
written by hand, but used the following methods
to give a more ?positive slant? to a text. These are
all methods that could be implemented straight-
forwardly in an NLG system1. In the follow-
ing, the word ?positive polarity? is used to refer
to propositions giving good news to the reader or
attributes which give good news to the reader if
they have high values (such as the reader?s intel-
ligence). Similarly ?negative polarity? refers to
items that represent bad news, e.g. failing a test.
For ethical reasons, negative polarity items did not
arise in this experiment.
A. Sentence emphasis - include explicit emphasis in sen-
tences expressing positive polarity propositions (e.g.
exclamation marks and phrases such as ?on top of
this?).
1Though the choice about when to apply them might not
be so straightforward.
147
B. Choice of vague evaluative adjectives - when evaluat-
ing positive polarity attributes, choose vague evaluative
adjectives that are more positive over ones that are less
positive (e.g. ?excellent?, rather than ?ok?).
C. Choice of vague adverbs - provide explicit emphasis to
positive polarity propositions by including vague ad-
verbs expressing great extent (e.g. ?significantly?,
rather than ?to some extent? or no adverb).
D. Choice of verbs - for a positive polarity proposition,
choose a verb that emphasises the great extent of the
proposition (e.g. ?outperformed?, rather than ?did bet-
ter than?).
E. Choice of realisation of rhetorical relations - when re-
alising a concession/contrast relation between a pos-
itive polarity proposition and one that is negative or
neutral, word it so that the positive polarity proposi-
tion is in the nucleus (more emphasised) position (e.g.
say ?although you did badly on X, you did well on Y?
instead of ?although you did well on Y, you did badly
on X?).
The idea is that an NLG system would employ
methods of this kind in order to ?slant? a mes-
sage positively, rather that to present a message
in a more neutral way. This might be done, for
instance, to induce positive emotions in a reader
who needs encouragement.
We claim that these choices can be viewed as
tactical, i.e. that they are ?allowable? alterna-
tive realisations of the same underlying content.
For instance, we believe a teacher could use such
methods in giving feedback to a student need-
ing encouragement without fear of prosecution for
misrepresenting the same truth that would be ex-
pressed without the use of these methods.
Whenever one words a proposition in differ-
ent ways, it can be claimed that a (perhaps sub-
tle) change of meaning is involved. However, in
these cases we claim that it is the writer?s atti-
tudes that are being manipulated (and reflected in
the text). We can therefore choose between these
alternatives by varying the writer, not the under-
lying message. Our view is supported by a num-
ber of current accounts of the semantics of vague
adjectives (though this is not an area without con-
troversy). Many accounts of vagueness appeal to
the idea that there is a norm which an adjective
like ?tall? implicitly refers to, and some of these
argue both that the norm itself can be contextually
determined and also that the amount by which the
norm has to be exceeded has to be ?significant?
to a degree which is ?relativized to some agent?
(Kennedy, 2007). For instance, with the phrase
?John is tall?
?the property [...] attributed to John is not
an intrinsic property, but rather a relational one.
Moreover, it is not a property the possession of
which depends only on the difference between
Johns height and some norm, but also on whether
that difference is a significant one. I take it that
whether or not a difference is a significant differ-
ence does not depend only on its magnitude, but
also on what our interests are? (Graff, 2000)
It is compatible with these accounts that differ-
ent agents, with different interests and notions of
what is noteworthy, can use vague adjectives in
different ways2.
Another reason for considering these meth-
ods as tactical is that in an NLG system, they
would likely be implemented somewhere late in
the ?pipeline?.
Probably the best way to check that we are using
tactical alternations (according to our definition) is
via some kind of text validation experiment with
human participants. Section 3.3 below describes
such an experiment, which provides strong sup-
port for this position.
3.2 Test Texts
For the experiment, we produced two feedback
texts describing the same set of intelligence test
results, one relatively neutral and one ?positively
slanted? using the above methods. In the ex-
periment, they were given to participants in two
groups, named ?0? and ?+? respectively. Each text
consisted of 7 sentences, with a direct correspon-
dance between the sentences of the two texts. Fig-
ure 1 presents the variations used in the feedback
used in the experiment for group + (i.e. positively
slanted) and group 0 (i.e. neutrally slanted). Note
that the actual numbers are the same in both texts.
3.3 Text validation
A text validation study was conducted in which
15 colleagues participated. The participants were
asked to comment on 12 sentence pairs, the 7
shown in Figure 1 and 5 additional filler pairs. The
following analysis reports on our findings on the 7
sentence pairs shown in Figure 1 only.
In order that we could test our intuitions about
the tactical nature of the linguistic alternations
(discussed in section 3.1 above), the participants
were presented with a scenario where there were
two different teachers, Mary Jones and Gordon
2Though there are certainly some limits on the situations
where a word like ?tall? can be truthfully used to describe a
height
148
+1: Your Baumgartner score of 7.38 is excellent!
01: Your Baumgartner score of 7.38 is ok.
+2: You did distinctively better than the average score ob-
tained by other people in your age group.
02: You did somewhat better than the average score ob-
tained by other people in your age group.
+3: Especially your scores on Imagination/Creativity and
on Clarity of Thought were great and considerably
higher than average.
03: Your scores on Imagination/Creativity and on Clarity
of Thought were good and a little higher than average.
+4: A factor analyses of your Baumgartner score results in
an overall excellent performance.
04: A factor analyses of your Baumgartner score results in
an overall reasonable performance.
+5: Although, compared to your peers, you have only
slightly higher Spatial Intelligence (7.5 vs 7.0) and Vi-
sual Intelligence (7.2 vs 6.8) scores, your Clarity of
Thought Score is very much better (7.2 vs 6.3).
05: Compared to your peers, you have a somewhat better
Clarity of Thought Score (7.2 vs 6.3), but you have only
slightly higher Spatial Intelligence (7.5 vs 7.0) and Vi-
sual Intelligence (7.2 vs 6.8) scores.
+6: On top of this you also outperformed most people
in your age group with your exceptional scores for
Imagination and Creativity (7.9 vs 7.2) and Logical-
Mathematical Intelligence (7.1 vs. 6.5).
06: You did better than most people in your age group with
your scores for Imagination and Creativity (7.9 vs 7.2)
and Logical-Mathematical Intelligence (7.1 vs. 6.5).
+7: There is a lot of variation in your age group, but your
score is significantly higher than average.
07: Your score is higher than average, but there is a lot of
variation in your age group.
Figure 1: Linguistic variation used in the IQ test feedback
Smith, both completely honest but with very dif-
ferent ideas about teaching (Mary believing that
any pupil can succeed, given encouragement,
but Gordon believing that most pupils are lazy
and have overinflated ideas about their abilities).
Given a positively slanted sentence (e.g. +7) from
Mary and a corresponding more neutrally slanted
one (e.g. 07) from Gordon, addressed to one or
more pupils, participants were asked to indicate:
1. ?Is it possible that Mary and Gordon might actually be
(honestly) giving different feedback to the same pupil
on the same task??
2. ?If the two pieces of feedback were given to the same
pupil (for the same task) and the pupil?s parents found
out, do you think they would have grounds to make a
complaint that one of the teachers is lying??
The hypothesis was that (for the 7 pairs of
sentences from Figure 1) in general participants
would answer ?yes? to question 1 and ?no? to
question 2. Indeed, for 6 pairs at least 14 out of the
15 participants answered as we had predicted. For
the other pair (+4/04), 12 out of 15 agreed with
both predictions. We see this as very strong evi-
dence for our position (the participants gave dif-
ferent answers for the filler pairs, and so were not
just producing these answers blindly).
No alterations were made to the two feedback
texts on the basis of the text validation results.
4 Measuring Emotions
There are two broad ways of measuring the emo-
tions of human subjects ? physiological methods
and self-reporting. Physiological methods unfor-
tunately tend to have the problems of complex
setup and calibration, which mean that it is hard to
transport them between tasks or individuals. In ad-
dition, although emotional states are undoubtedly
connected to physiological variables, it is not al-
ways clear what is being measured by these meth-
ods (cf. (Lazarus et al, 1980); (Cacioppo et al,
2000) ).
Because of these problems, we have opted to in-
vestigate self-reporting methods, as validated and
used widely in psychological experiments. Three
well-established methods that are used frequently
in the field of psychology are the Russel Affect
Grid (Russell et al, 1989), the Self Assessment
Manikin (SAM) (Lang, 1980) and the Positive and
Negative Affect Scale (PANAS) (Watson et al,
1988). In our previous study (van der Sluis and
Mellish, 2008), we had problems with participants
understanding how to use the Russel Affect Grid
and SAM and so now we opted to use a version of
the PANAS test.
The PANAS test is a scale using affect terms
that describe positive and negative feelings and
emotions. Participants in the experiment read the
terms and indicate to what extent they experi-
ence(d) the emotions indicated by each of them
using a five point scale ranging from (1) very
slightly/not at all, (2) a little, (3) moderately, (4)
quite a bit to (5) extremely. A total score for
positive affect is calculated by simply adding the
scores for the positive terms, and similarly for neg-
ative affect.
As before, we used a simplified version of the
PANAS scale in order not to overburden the partic-
ipants with questions and to avoid bored answer-
ing. In this test, which has been fully validated
(Mackinnon et al, 1999), participants have to rate
only 10 instead of 20 terms: 5 for positive af-
149
fect (i.e. alert, determined, enthusiastic, excited,
inspired) and 5 for negative affect (i.e. afraid,
scared, nervous, upset, distressed).
Our use of the simplified PANAS in this study
differed from our previous study, however, by hav-
ing participants respond to the PANAS questions
using a slider, rather than a five point scale. This
means that only two terms were put at the extreme
ends of the slider (i.e. ?very slightly/not at all? and
?extremely? were presented but not ?a little?, ?mod-
erately? or ?quite a bit?). The change to use a slider
was because van der Sluis and Mellish (2008) ob-
served partipants only using a small part of the
possible scale for answers, and within this the five
point scale might have lost useful information.
Although our particular experiment focussed on
positive affect, we included the negative affect
terms partly so that we could detect outliers in
our participant set ? people who were perhaps ex-
tremely nervous about the test or sensitive about
their IQ. In fact, we did not find any such outliers.
5 Experiment to Measure Emotional
Effects of Positive Feedback
5.1 Set Up of the Study
As stated above, the texts that we presented to
our participants were portrayed as giving feedback
on an IQ test that the participants had just taken.
The IQ test was set up as a web experiment in
which participants could linearly traverse through
the various phases of the test. An outline of the
set up is given in Figure 2. In the general intro-
duction to the experiment, participants were told
that the experiment was ?an assessment of a new
kind of intelligence test which combines a number
of well-established methods that are used as indi-
cators of human brain power?. To make it more
difficult for the participant to keep track of how
well/poorly she performed over the course of the
test, it also said that the test consisted of open and
multiple choice questions that had different weight
factors in the calculation of the overall score and
that would assess various aspects of their intelli-
gence. Subsequently, the participant was asked
to tick a consent form to participate in the study.
Then a questionnaire followed in which the par-
ticipant was asked about her age, gender and the
quality of her English. She was also asked if she
had any experience with IQ tests and how she ex-
pected to score on this one. These questions were
interleaved with an emotion assessment test (re-
duced PANAS) in which the participant was asked
?how do you feel right now??.
After filling out the questionnaire, the partici-
pant could start the ?IQ test? whenever she was
ready. The ?IQ test? consisted of 30 questions
which she had to answer one at a time. The par-
ticipant could not skip a question and also had
to indicate for each of the questions how confi-
dent she was about her answer. The questions
that were used for the test were carefully collected
from the internet and included items from various
tests and games. Different types of questions were
used: questions about logical truths, mathematical
questions that required some calculations, ques-
tions about words and letter sequences, questions
including pictures and questions about the partic-
ipant?s personality. They were ordered randomly
(but with the same order for each participant).
When the participant had finished the test, she
was asked to wait patiently while the system cal-
culated the test scores. When enough calculation
time had passed the participant was presented with
the test feedback (one of the two texts, regardless
of their actual performance). This feedback first
explained the test and its type of scoring:
The Baumgartner test which you have just un-
dertaken tests various kinds of intelligence, for
instance, your visual intelligence, your logical-
mathematical intelligence and your spatial in-
telligence. These various aspects of your in-
telligence contribute to an overall Baumgartner
Score. The Baumgartner Score rates your intel-
ligence on a 10-point scale with 10 as the high-
est possible score. Note that your Baumgartner
Score can change over time dependent on expe-
rience and practice. Below your test score is pre-
sented in comparison with the average score in
your age group.
The introduction to the test was followed by ei-
ther the positively (+1..+7, Figure 1) or the rela-
tively neutrally (01..07, Figure 1) phrased test re-
sults. After the participant had processed the feed-
back, she was asked to fill out one more question-
naire to assess her emotions (i.e. ?How do you
feel right now knowing your scores on the test?).
This time the simplified PANAS test was inter-
leaved with questions about the participant?s re-
sults, (e.g. were they as expected and how did she
value them), the test (e.g. was it difficult, doable
or easy?) and space for comments on the test and
the experiment. Finally the participant was de-
briefed about the experiment and about the goal
of the study.
150
1. General introduction to the experiment;
2. Consent form;
3. Questionnaire on participant?s background and famil-
iarity with IQ-test interleaved with a PANAS test to as-
sess the participant?s current emotional state;
4. Message: ?Please press the next button at the bottom
of this page whenever you are ready to start the intelli-
gence test?;
5. IQ test questions;
6. Message: Please be patient while your answers are be-
ing processed and your test score is computed. After
the result page, you will be asked another set of ques-
tions about the test, your performance and the way you
feel about it. This information is very important for
this study, so please answer the questions as honestly
as possible.?;
7. Feedback + or 0;
8. Questionnaire: PANAS test to assess how the partic-
ipants felt after reading the test feedback interleaved
with questions about the test, their expectations and
space for comments;
9. Debriefing which informed participants about the
study?s purpose and stated that the IQ test was not real
and that their test results did not contain any truth.
Figure 2: Phases in the experiment set up
5.2 Pilot Experiment
A pilot of the experiment was carried out by ask-
ing a number of people to try the experiment via
the web interface. The main outcomes of this
study, in which 11 colleagues participated, was
that the experiment was too long. Accordingly, the
questionnaires before and after the IQ test (phase
3 and 8 in Figure 2) were shortened. Also the IQ
test itself was shortened from 40 to 30 questions.
5.3 Main Experiment: participants and
experimental setting
30 participants, all female university students,
took the IQ test. All participants except two were
in age band 18-24. The exceptions were in age
band 25-29 (group +) and 30-34 (group 0). The
participants were randomly distributed over group
+ and group 0 and (for ethical reasons) did the test
one by one in a one-person experiment room while
the experimenter was waiting outside the room.
As soon as the participant indicated that she had
finished the task (i.e. stepped out of the exper-
iment room), she was debriefed about the study
by the experimenter and was paid with a voucher
worth 5 pounds.
5.4 Hypotheses
Since the message of the feedback texts was rel-
atively positive and there is no necessary correla-
0-group +-group
Negative PANAS terms Before 1.60(.76) 1.58(.68)
Negative PANAS terms After 1.57(.68) 1.31(.45)
Positive PANAS terms Before 3.25(.78) 3.32(.55)
Positive PANAS terms After 3.13(.58) 3.75(.55)
Table 1: Means and Standard deviations (between brack-
ets) for the negative and positive PANAS terms as indicated
before and after the IQ test undertaken by participants that
received neutral and participants that received positive feed-
back on their performance.
tion between positive and negative PANAS scores
(Watson and Clark, 1999), we expected the main
effects of the texts to be on the average evaluation
of the positive PANAS terms. In order to cater for
the fact that individuals might differ in their initial
positive PANAS scores, we decided to look at the
difference of the scores (score after minus score
before). Therefore the hypothesis for this study
was that participants who received the positively
phrased feedback would show a larger change in
their positive emotions than the participants who
received the neutrally phrased feedback.
5.5 Results
Table 1 indicates that on average after they had re-
ceived their test results, participants in the +-group
were more positively tuned than participants in the
0-group. Participants in the +-group also rated the
positive emotion terms higher than they had done
before they undertook the IQ test. No such results
were found for the 0-group. In contrast, compared
to their responses before the IQ test, participants in
the 0-group rated the positive terms slightly lower
after they had processed their neutrally phrased
feedback. With respect to the negative PANAS
terms, participants in the +-group report slightly
less negative emotions after they read their test
scores, but none of the differences found in the
negative PANAS scores were significant.
A 2 (feedback type) * 2 (before/after) * 2 (pos-
itive/negative mean) repeated measures ANOVA
was carried out on the average PANAS scores.
This showed no main effect of feedback type
(+ vs 0) and no main effect of before/after on
average PANAS scores. However, there was a
highly significant interaction between feedback
type and before/after, which indicates that the
change in PANAS mean before and after the text
was strongly dependent on feedback type3 (F(1,
28) = 10.246, p < .003). We interpret this to mean
that the (after minus before) value is significantly
3An ANOVA test on the positive means only produces a
similar result.
151
0-group +-group
Alert Before 3.96(.80) 3.17(.99)
Alert After 3.45(.76) 3.65(.75)
Determined Before 3.49(1.02) 3.60(.50)
Determined After 3.50(1.13) 3.74(.61)
Enthusiastic Before 3.52(1.05 3.49(.72)
Enthusiastic After 2.97(.81) 3.84(.66)
Excited Before 2.74(.97) 3.28(.61)
Excited After 2.64(.75) 3.69(.83)
Inspired Before 2.56(1.21) 3.06(.77)
Inspired After 3.06(1.05) 3.81(.78)
Table 2: Means and Standard deviations (between brack-
ets) for the positive PANAS terms as indicated after the IQ
test undertaken by participants that received positive and par-
ticipants that received neutral feedback on their performance.
0-group +-group
ER
not disclosed 1 0
not so good 0 1
ok 9 4
well 4 10
extremely well 1 0
Table 3: Participant responses when questioned about the
results they expected (ER) .
greater for the +-group. A two-tailed, two sam-
ple t-test verifies this (t = 3.2, p < 0.004). We did
some post-hoc investigation in an attempt to un-
derstand the main result more fully. When look-
ing at the positive PANAS scores in more detail
(see Table 2), it turns out that only three of the
five positive PANAS terms included in the simpli-
fied PANAS test render promising results. Inter-
actions were found for the terms ?alert? (F(1, 28)
= 10.291, p < .003) and ?enthusiastic? (F(1, 28)
= 5.651, p < .025). No interactions were found
for the terms ?determined? and ?inspired?. For ?in-
spired? however, we found a main effect of feed-
back type : (F(1, 28) = 8.755, p < .006), which in-
dicates that participants in the +-group could have
been more inspired because of their test scores
than participants in the 0-group. Not all of these
results would be significant if Bonferroni correc-
tions were made.
5.6 The Role of Expectations
It is possible that this result could have been
caused by other (systematic but unanticipated) dif-
ferences between the two groups. In particular,
perhaps the result could be caused by a differ-
ence in how well the two groups of participants
expected to perform. As it happens, participants
were asked: ?How do you expect to score on an
intelligence test?? before they did the test. The
answers to this question are summarised in Ta-
ble 3. This data suggests that participants in the
+-group initially had higher expectations. It is
difficult to get a consensus from the psychologi-
cal literature about how this might have affected
the results. On the one hand, some studies have
shown that positive expectations can have an ac-
celerating effect on a person?s actual positive emo-
tional experience (Wilson et al, 2003; Wilson and
Klaaren, 1992). Such results might suggest an al-
ternative explanation of the fact that the +-group
showed a greater change in positive emotions. On
the other hand, it might be argued that subjects
with lower expectations would be more surprised
(since both texts presented good results) and so
their emotions would have been influenced more
significantly. That is, if a subject already expects
to do well then one would not expect that find-
ing that they actually did well would cause much
of a change in their emotions. This would predict
that it should be the 0-group that shows the great-
est emotion change. Overall, it is hard to know
whether the data about expectations should affect
our confidence in the experiment result, though it
would be worthwhile controlling for initial expec-
tations in further experiments of this kind.
6 Discussion and Future Directions
6.1 Discussion
Compared with the previous study of van der Sluis
and Mellish (2008), we expected participants to
indicate stronger emotional effects, because the
text participants were asked to read was about their
own capabilities instead of about something in the
world around them which they could think would
not affect them. Indeed, this seems to have been
the case. In van der Sluis and Mellish (2008), all
responses used the lower half of the scale, whereas
with the slider our participants indicated values up
to both extremes of the range available. Unfortu-
nately, the fact that one set of values is discrete and
the other continuous means that it is hard to carry
out a simple statistical comparison.
6.2 Future Work
In the study described in the paper, a number of
different techniques (e.g. emphasis, vague adjec-
tives and adverbs) were used to phrase the various
propositions in the feedback. In future work we
aim to identify the relative importance of the indi-
vidual techniques.
152
6.3 Conclusion
The fact that we have been able to show a signifi-
cant difference in the emotions induced by the two
texts is very encouraging. It suggests that there
is a possible methodology for directly evaluating
affective NLG and that the tactical concerns with
which much of NLG research is occupied are rel-
evant to affective NLG. A similar methodology
could perhaps now be used to determine the ef-
fectiveness of specific NLG methods and mecha-
nisms in terms of inducing emotions. Although we
have now shown that NLG tactical decisions can
affect emotions, it remains to be seen what kind of
changes in strategy, learning, motivation, etc., can
be induced by positive affect and thus how these
framing decisions would best be made by an NLG
system.
Acknowledgments
This work was supported by the EPSRC platfrom grant ?Af-
fecting people with natural language? (EP/E011764/1) and
also in part by Science Foundation Ireland under a CSET
grant (NGL/CSET). We would like to thank the people who
contributed to this study, most notably Judith Masthoff, Al-
bert Gatt and Kees van Deemter and Nikiforos Karamanis.
References
R. Brown and E. Pinel. 2003. Stigma on my mind: Indi-
vidual differences in the experience of stereotype threat.
Journal of Experimental Social Psychology, 39:626?633.
J. Cacioppo, G. Bernston, J. Larson, K. Poehlmann, and
T. Ito. 2000. The psychophysiology of emotion. In
M. Lewis and J. Haviland-Jones, editors, Handbook of
Emotions, pages 173?191. New York: Guilford Press.
M. Cadinu, A. Maass, A. Rosabianca, and J. Kiesner. 2005.
Why do women underperform under stereotype threat?
Psychological Science, 16(7):572?578.
D. Graff. 2000. Shifting sands: An interest-relative theory of
vagueness. Philosophical Topics, 20:45?81.
C. Kennedy. 2007. Vagueness and grammar: the semantics
of relative and absolute gradable adjectives. Linguistics
and Philosophy, 30:1?45.
P. Lang. 1980. Behavioral treatment and bio-behavioral
assessment: Computer applications. In J. Sidowske,
J. Johnson, and T. Williams, editors, Technology in Mental
Health Care Delivery Systems, pages 119?137. Norwood,
NJ: Ablex.
R. Lazarus, A. Kanner, and S. Folkman. 1980. Emotions: A
cognitive-phenomenological analysis. In R. Plutchik and
H. Kellerman, editors, Emotion, theory, research, and ex-
perience. New York: Academic Press.
I. Levin, S. Schneider, and G. Gaeth. 1998. All frames
are not created equal: A typology and critical analysis of
framing effects. Organizational behaviour and human de-
cision processes, 76(2):149?188.
A. Mackinnon, A. Jorm, H. Christensen, A. Korten, P. Ja-
comb, and B. Rodgers. 1999. A short form of the positive
and negative affect schedule: evaluation of factorial valid-
ity and invariance across demographic variables in a com-
munity sample. Personality and Individual Differences,
27(3):405?416.
F. Mairesse and M. Walker. 2008. Trainable generation of
big-five personality styles through data-driven parameter
estimation. In Proc. of the 46th Annual Meeting of the
ACL.
J. Moore, K. Porayska-Pomsta, S. Varges, and C. Zinn. 2004.
Generating tutorial feedback with affect. In Proceedings
of the 7th International Florida Artificial Intelligence Re-
search Symposium Conference (FLAIRS).
L. Moxey and A. Sanford. 2000. Communicating quantities:
A review of psycholinguistic evidence of how expressions
determine perspectives. Applied Cognitive Psychology,
14(3):237?255.
L. O?Hara and R. Sternberg. 2001. It doesn?t hurt to ask:
Effects of instructions to be creative, practical, or ana-
lytical on essay-writing performance and their interaction
with students? thinking styles. Creativity Research Jour-
nal, 13(2):197?210.
F. De Rosis and F. Grasso. 2000. Affective natural lan-
guage generation. In A. Paiva, editor, Affective Interac-
tions. Springer LNAI 1814.
F. De Rosis, F. Grasso, and D. Berry. 1999. Refining in-
structional text generation after evaluation. Artificial In-
telligence in Medicine, 17(1):1?36.
J. Russell, A. Weiss, and G. Mendelsohn. 1989. Affect grid:
A single-item scale of pleasure and arousal. Journal of
Personality and Social Psychology, 57:493?502.
K. Teigen and W. Brun. 2003. Verbal probabilities: A ques-
tion of frame. Journal of Behavioral Decision Making,
16:53?72.
H. Thompson. 1977. Strategy and tactics: A model for lan-
guage production. In Proceedings of the Chicago Linguis-
tics Society, Chicago.
I. van der Sluis and C. Mellish. 2008. Using tactical NLG to
induce affective states: Empirical investigations. In Pro-
ceedings of the fifth international natural language gener-
ation conference, pages 68?76.
D. Watson and L. Clark, 1999. Manual for the Positive and
Negative Affect Schedule - Expanded Form. The Univer-
sity of Iowa.
D. Watson, L. Clark, and A. Tellegen. 1988. Development
and validation of brief measures of positive and negative
affect: The PANAS scales. Journal of Personality and
Social Psychology, 54(1063-1070).
T. Wilson and K. Klaaren. 1992. The role of affective expec-
tations in affective experience. In M. Clark, editor, Review
of Personality and Social Psychology, volume 14: Emo-
tion and Social Behaviour, pages 1?31. Newbury Park,
CA: Sage.
T. Wilson, D Gilbert, and D. Centerbar. 2003. Making sense:
The causes of emotional evanescence. In I. Brocas and
J. Carrillo, editors, The Psychology of Economic Deci-
sions, volume 1: Rationality and Well Being, pages 209?
233. New York: Oxford University Press.
153
Reinterpretation of an existing?NLG system in a Generic Generation 
Architecture 
L. Cahill, C. Doran~ R. Evans, C. Meilish, D. Paiva,:M. Reape, D. Scott,, N. Tipper 
.Universities of Brighton and Edinburgh. 
Email rags@itri, brighton, ac. uk 
Abstract 
The RAGS project aims to define a reference ar- 
chitecture for Natural Language Generation (NLG) 
systems. Currently the major part of this archi- 
tecture consists of a set of datatype definitions for 
specifying the input and output formats for mod- 
ules within NLG systems. In this paper we describe 
our efforts to reinterpret an existing NLG system in 
terms of these definitions. The system chosen was 
the Caption Generation System. 
2. Which aspects of the RAGS repertoire would 
: . . . .  . . . .  - .... -,.= ., ~,~,aemaltybe'requireti~ftrr~strch~a-~reinterpretation; 
which would be unnecessary and which addi- 
tions to the RAGS repertoire would be moti- 
vated. 
1 Introduction 
The RAGS project ~ aims to define a reference ar- 
chitecture for natural anguage generation systems. 
Currently the major part of this architecture consists 
of a set of datatype definitions for specifying the 
input and output formats for modules within NLG 
systems. The intention is that such representations 
can be used to assist in reusability of components 
of NLG systems. System components that adhere 
to these representations, or use a format hat can be 
translated into such representations relatively eas- 
ily, can then, in principle, be substituted into other 
systems. Also, individual components could be de- 
veloped without the need for a complete system if 
datasets, based on the representations, were made 
available. 
In this paper we describe an attempt to reinterpret 
an existing NLG system in terms of the RAGS data 
definitions. The point of this exercise was to lem-n: 
1. Whether these data structures were sufficient 
to describe the input and output functionality 
of an existing, independently developed, ap- 
3. Whether studying the system would generate 
good ideas about possible reusable generation 
modules that could be developed. 
In this exercise it was important o choose a sys- 
tem that had been developed by people outside the 
RAGS project. Equally, it was important o have 
sufficient clear information about the system in the 
available literature, and/or by means of personal 
contact with the developers. The system chosen was 
the Caption Generation System (Mittal et al, 1995; 
Mittal et al, 1998) 3. This system was chosen be- 
cause, as well as fulfilling the criteria above, it ap- 
peared to be a relatively simple pipeline, thus avoid- 
ing complex control issues, with individual modules 
performing the varied linguistic tasks that the RAGS 
data structures had been designed to handle. 
The reinterpretation exercise took the form of 
coming up with an account of how the interfaces 
to the CGS modules corresponded to the RAGS 
model and reimplementing a working version of 
each module (apart from Text Planning and Realisa- 
tion) which was tested to ensure that, given appro- 
priate input, its output was correct (i.e. conforming 
to the global account) on key examples. Naturally, 
given the scope of this exercise, we had to gloss over 
some interesting implementational issues. The aim 
was not to produce a complete system or a system 
as good as CGS, but merely to demonstrate hat the 
broad functionality of the system could be repro- 
plied 2 NLG system. 
? Now at the MITRE Corporation, Bedford, MA, USA, 
cdoran.?mitre, org. 
tThis work was supported by ESPRC grants GR/L77041 
(Edinburgh) and GR/L77102 (Brighton), RAGS: Reference Ar- 
chitecture for Generation Systems. 
-'See (Paiva, 1998) for a definition of applied in this specific 
context. 
" . -ducedwithin:the RAGS .structures. 
In this paper we first describe the RAGS data 
structures. We then describe the CGS system 
3In addition to these published sources, we were greatly 
helped by the developers of the system who gave us the ben- 
efit of their own expertise as well as access to the original code 
of the system and a technical report hat included implementa- 
tional details such as system traces. 
69 
followed by our reinterpretation of the system in Abstract Rhetorical Abstract Rhetorical Repre- 
RAGS terms. Finally we discuss,, the :implications:. :. -._..sentations ,are--tree-structures with,rhetorical .rela- 
for RAGS of this exercise, tions at the internal nodes and Abstract Rhetorical 
2 The RAGS datatypes  
The RAGS project initially set out to develop a ref- 
erence architecture based on the three-stage pipeline 
suggested by Reiter (Reiter, 1994). However, a 
trees or Abstract Semantic Representations at the 
leaves. 
Rhetorical Abstract Rhetorical Representations 
are viewed as descriptions of sets of possible 
Rhetorical Representations. Each one may be trans- 
detailed analysis of existing applied NLG systems formed into some subset of the possible Rhetori- 
(Cahill and Reape~_~ l:998}:suggested~,that~ttch.an~ ar -~: ~<.eaLReprese, ntations by,,means ~ofa,set..o_f~.petmitted 
chitecture was not specific enough and not closely transformations, e.g. reversing the order of nucleus 
enough adhered to by the majority of the systems 
surveyed for this to be used as the basis of the archi- 
tecture. 
The abstract functionality of a generation system 
can be specified without specific reference to pro- 
cessing. The RAGS approach to this is to develop a 
data model, that is, to define the functional modules 
entirely in terms of the datatypes they manipulate 
and the operations they can perform on them. On 
top of such a model, more specific process models 
can be created in terms of constraints on the order 
and level of instantiation of different ypes of data in 
the data model. A 'rational reconstnaction' of some 
pipeline model might then be produced, but other 
process models would also be possible. 
The RAGS levels of representation are as fol- 
lows4: 
Conceptual The conceptual level of representa- 
tion is defined only indirectly through an API via 
which a knowledge base (providing the content 
from which generation takes place) can be viewed 
as if it were defined in a simple KL-ONE (Brach- 
man and Schmolze, 1985) like system. 
Abstract Semantic Abstract semantic representa- 
tions are the first level at which semantic predicates 
are associated with arguments. At this level, seman- 
tic predicates and roles are those used in the API to 
query the knowledge base and arguments are knowl- 
edge base entities. 
Semantic (Concrete) semantic representations 
provide a complete notation for "logical forms" 
where there is no longer any reference to ,the knowl- 
edge base. The representations are based on sys- 
tems such as SPL (Kasper, 1989) and DRT (Kamp 
and Reyle, 1993). 
4More details can be found in (Cahill et 
al., 1999) and at the RAGS project web site: 
ht tp  : / /www.  i t r i  . b r ighton ,  ac. uk/rags.  
and satellite or changing the rhetorical relation to 
one within a permitted set. 
Abstract Document Document structure defines 
the linear ordering of the constituents of the Rhetor- 
ical Representation with a POSITION feature, as 
well as two other features, TEXT-LEVEL, which 
takes values such as paragraph or sentence; and 
LAYOUT, which takes values such as wrapped-text 
and vertical list. It takes the form of a tree, usu- 
ally, but not necessarily, isomorphic to the Rhetor- 
ical Representation a d linked to it, but with these 
three features at the nodes instead of rhetorical rela- 
tions. 
Abstract Syntactic Abstract Syntactic Represen- 
tations capture high-level aspects of syntactic struc- 
ture in terms of notions such as lexical head, speci- 
fiers, modifiers and complements. This level of rep- 
resentation is compatible with approaches such as 
LFG f-structure, HPSG and Meteer's Text Structure. 
3 Partial and Mixed Representations 
For all of the RAGS levels partial representations 
are possible. Without this, it is not possible for a 
module to pass any result to another until that re- 
sult is completely determined, and this would im- 
pose an unwanted bias towards simple pipeline ar- 
chitectures into the model. There are many cases 
in NLG where a representation is built collabora- 
tively by several modules. For instance, many sys- 
tems have a referring expression generation module 
whose task is to complete a semantic representation 
which lacks those structures which will be realised 
as NPs. Such a functionality cannot be described 
unless partially complete semantic representations 
can be communicated. 
In addition, mixed representations are possible, 
where (possibly partial) representations at several 
levels are combined with explicit links between the 
elements. Many NLG modules have to be sensi- 
70 
tive to a number of levels at once (consider, for 
.......... instance, -aggregatiomxeferring,expmssion.,genera- 
tion and lexicalisation, all of which need to take 
into account rhetorical, semantic and syntactic on- 
straints). The input to most reusable realisation sys- 
tems is also best viewed as a mixture of semantic 
and abstract syntactic information. 
The extra flexibility of having partial and mixed 
representations turned out to be vital in the recon- 
struction of the CGS system. (Mellish et al, 2000). 
4 The CGS system 
The Caption Generation System (CGS) generates 
explanatory captions of graphical presentations (2- 
D charts and graphs). Its architecture is a pipeline 
with several modules, shown in the left hand part of 
Figure 1. An example of a diagram and its accom- 
panying text are given in Figure 2. The propositions 
are numbered for ease of reference throughout the 
paper. 
The input to CGS is a picture representation 
(graphical elements and its mapping from the data 
set) generated by SAGE plus its complexity metric. 
The text planning module (Moore and Paris (1993)) 
plans an explanation i  terms of high level discourse 
goals. The output of the planner is a partially or- 
dered plan with speech-acts as leaves. 
The ordering module receives as input the dis- 
course plan with links specifying the ordering re- 
lations between sub-trees and specifies an order for 
them based on heuristics uch as that the description 
should be done from left to right in the visual space. 
The aggregation module "only conjoins pairs of 
contiguous propositions about the same grapheme 
type 5 in the same space" (Mittai et al, 1999) and 
inserts cue phrases compatible with the propositions 
e o ( .=., "whereas" for contrastive ones). The internal 
order of the sentence constituents i determined by 
the centering module using an extension of the cen- 
tering theory of Grosz and colleagues (Grosz et al, 
1995). 
The referring expression module uses Date and 
Reiter's (Dale and Reiter, 1995) algorithm to con- 
struct the set of attributes that can uniquely identify 
a referent. There are'two, situations where the text 
planning module helps specifically in the generation 
of referring expressions: (1) when the complexity 
for expressing a graphic demands an example and 
5"Graphemes are the basic building blocks for constructing 
pictures. Marks, text, lines and bars are some of the different 
grapheme classes available in SAGE." (IVlittal et al, 1999). 
CGS architecture 
SAGE 
RAGS representations 
I I I  I I I  IV  V 
I II HI  IV  V 
? I I I  I I I  IV  " V . 
I I I  In  IV  v 
l -  .......... I /11  
1 It  I I I  IV V 
l -  .......... 
I 11 11I IV V 
.......... III1  
I I I  HI  IV  V 
; "  .......... I I I I I  
FUF 
Figure 1: A RAGS view of the CGS system. The 
labels for the RAGS representations refer to the fol- 
lowing: I = conceptual; II = semantic; III = rhetori- 
cal; IV = document; V = syntactic. 
it signals this both to SAGE (for highlighting the 
corresponding grapheme) and to the rest of the text 
generation modules; and (2) when in a specific sit- 
uation the referring algorithm would need several 
interactions for detecting that an entity is unique in 
? a certain visual space and.the planning could detect 
it in the construction of the description of this space. 
When this occurs, the text planner "circumvents he 
problem for the:.referring ,expression :module at the 
planning stage itself, processing the speech-acts ap- 
propriately to avoid this situation completely". 
After lexicalisation, which adds lexeme and ma- 
jor category information, the resulting functional 
descriptions are passed to the FUF/SURGE realiser 
that generates texts like the caption of Figure 2. 
71 
\ [ \ ]  
O 
te l  
O 
IZl 
ZS:3 
I21 ,:7-. ,,S . . . .  ; . . . .  .' ? 
O ~ ~Ipc~ q~L~ 
\] 
I 
\] 
=::::::;=a___.,____.__,_______~ 
. ,  , : ,  ; .  . ,  
Figure 2: (1) These two charts present information about house sales from data-set ts-1740. (2) In the two 
charts, the y-axis indicates the houses. (7) In the first chart, the left edge of the bar shows the house's elling 
price whereas (8) the right edge shows the asking price. (3) The horizontal position of the mark shows the 
agency estimate. (4) The color shows the neighbourhood and (5) shape shows the listing agency. (6) Size 
shows the number of rooms. (9) The second chart shows the number of days on the market. 
5 Reinterpretat ion f  CGS in RAGS 
Our reinterpretation f the CGS system defines the 
interfaces between the modules of CGS in terms 
of the RAGS data structures discussed above. In 
this section we discuss the input and output inter- 
faces for each CGS module in turn as well as any 
problems we encountered in mapping the structures 
into RAGS structures. Figure 1 shows the incre- 
mental build-up of the RAGS data levels across 
the pipeline. Here we have collapsed the Abstract 
Rhetorical and Rhetorical and the Abstract Seman- 
tic and Semantic. It is-interesting to note that the 
build up of levels of representation does not tend to 
correspond exactly with module boundaries. 
One of the major issues we faced in' our reinter- 
pretation was where to produce representations (or
partial representations) whose emergence was not 
defined clearly in the descriptions of CGS. For in- 
stance, many decisions about document structure 
are made only implicitly by the system. In most 
cases we have opted to produce all types of repre- 
sentations at the earliest point where they can con- 
ceivably have any content. This means, for instance, 
that our reimplementation assumes an (unimple- 
mented) text planner which produces an Abstract 
Rhetorical Representation with Abstract Semantic 
leaves and an Abstract Document Representation. 
Text Planner The input to the Longbow text plan- 
ner discussed in section 4 above is a representation 
of a picture in SAGE format (which has been an- 
notated to indicate the types of complexity of each 
grapheme) together with a goal, which can typi- 
cally be interpreted as "describe". It outputs an es- 
sentially fiat sequence of plan operators, each of 
which corresponds in the output? text .to .a.speech 
act. In our reinterpretation, we have assumed that 
this fiat structure needs to be translated into an Ab- 
stract Rhetorical Representation with (at least) min- 
imal structure. Such a structure is implicit in the 
plan steps, and our interpretation f the rhetorical 
structure for the example text corresponds closely to 
that of the post-processing trace produced by CGS. 
72 
I .AYOI  FII" * 'upped tel l  
" IU  ,I.EVIZL. p J t l~aph 
~ f ~ I O N :  2 
POSlllON I 
I.AYOtr'I+: -~pped tell 
TEX"T.L~ VEL 
(1) 
POSITION: I POSITION: 2 
LAYOUT: *T~,pl~n.l teat 
"IEXT-LEVEL: + 
(2) 
Po$ : I POSITION: 1 
LAYOUT: -mtpFcd te~t 
. TE.ICr-t.EVEL~ ?
0OSFI-K~N. 2 PosmoN: i 
POSIllON: I PosrnoN: I POsmoN. ~ FoSmON: 4 POSt'nON I PosrnoN: 2 
LAYOUT: ~pp~d lesl LAYOU'T. ~ppe,.f ~xt LAYO\[rF. ~apped lesl LAYOUT: ~+r~pS~d I?xt LAYOUT. ~'?~l~,Od ~est LAYOUT: ~Tappe~ text 
TEXT,LEVEL  7 "II~XT,LEVEI.: ~ "II~XT-LEVEL ? "I I~XT-LEVEL: ? TEXT-LEVEL  "+ TIE~XT-L.EVI:I.: ? 
(3) (4) (5) (6) (7) (8) 
Figure 3: Initial Document Structure 
. . .Z., 
However, we are still not entirely sure 
exactly CGS creates this structure, so 
posed it at the very beginning, onto the 
text planner. 
Already at this stage it is necessary 
about where 
we have im- 
output of the 
to make use 
of mixed RAGS representations. As well as this 
Abstract Rhetorical Representation, the text planner 
has to produce an Abstract Document Representa- 
tion, linked to the Abstract Rhetorical Representa- 
tion. This is already partially ordered - although the 
exact value of POSITION features cannot be speci- 
fied at this stage, the document tree is constructed 
so that propositions are already grouped together. 
In addition, we make explicit certain default infor- 
mation that the CGS leaves implicit at this stage, 
namely, that the LAYOUT feature is always wrapped 
text and that the TEXT-LEVEL feature of the top 
node is always paragraph. 
Ordering The ordering module takes the Abstract 
Document Representation a d the Abstract Rhetor- 
ical Representation as input and outputs an Abstract 
Document Representation with the POSITION fea- 
ture 's  value filled,for all :the nodes, .That is, it fixes. ? 
the linear order of the final output of the speech acts. 
In our example, the ordering is changed so that steps 
7 and 8 are promoted to appear before 3, 4, 5 and 6. 
The resulting structure is shown in figure 36 . 
6In this and the.following diagrams, objects are represented 
by circles with (labelled) arrows indicating the relations be-- 
Aggregation Although aggregation might seem 
like a self-contained process within NLG, in prac- 
tice it can make changes at a number of levels of 
representation a d indeed it may be the last opera- 
tion that has an effect on several levels. The aggre- 
gation module in our reinterpretation thus has the fi- 
nal responsibility to convert an Abstract Rhetorical 
Representation with Abstract Semantic Represen- 
tation leaves into a Rhetorical Representation with 
Semantic Representation leaves. The new Rhetori- 
cal Representation may be different from before as 
a result of speech acts being aggregated but whether 
different or not, it can now be considered final as 
it will no longer be changed by the system. The 
resulting Semantic Representations are no longer 
Abstract because further structure may have been 
determined for arguments to predicates. On the 
other hand, referring expressions have not yet been 
generated and so the (Concrete) Semantic Repre- 
sentations cannot be complete. The reconstruc- 
,.tion createspartia.i Semantic Representations with 
"holes" where the referring expressions (Semantic 
Representations) will be inserted. These "holes" are 
linked back to the knowledge base entities tfiat they 
correspond to. 
Because Aggregation affects text levels, it also af- 
fects the Abstract Document Representation, which 
has its TEXT-LEVEL feature's values all filled at this 
tween them. Dashed arrows indicate links between different 
levels of representation. 
73 
SemRep 
fun(Role,SemRep) 
DR preS  , . mRep 
? AbsSynRep ~ AbsSynRe~ 
/ " \  Y^.Z(" 
FVM (~ ~ ,un(Funs,~gS~c) (,~ ~M (~) lun(Funs.ArgSpec) 
0 ~ 0 0 
? + 
Adjs 
. . - ; . .  
Figure 4: Syntactic representations constructed by Centering 
point. It may also need to change the structure 
of the Abstract Document Representation, for in- 
stance, adding in a node for a sentence above two, 
now aggregated, clause nodes. 
Centering Because Centering comes before Re- 
ferring Expression generation and Realisation, all it 
can do is establish constraints that must be heeded 
by the later modules. At one stage, it seemed as if 
this required communicating a kind of information 
that was not covered by the RAGS datatypes. How- 
ever, the fact that an NP corresponds (or not) to a 
center of some kind can be regarded as a kind of 
abstract syntactic information. The reconstruction 
therefore has the centering module building a partial 
(unconnected) Abstract Syntactic representation for 
each Semantic Representation that will be realised 
as an NP, inserting a feature that specifies whether 
it constitutes a forward- or backward-facing cen- 
ter, approximately following Grosz et al(Grosz et 
al., 1995). This information is used to determine 
whether active or passive voice will be used. An 
example of such a partial Abstract Syntactic Repre- 
sentation is given in Figure 4. 
Referring Expression In our reconstruction of 
the CGS system, we have deviated from reproduc- 
ing the exact functionality for the referring expres- 
sion module and part of the lexical choice module. 
In the CGS system, the referring expression module 
computes association lists which can be used by the 
lexical choice module to construct referring expres- 
sions suitable for realisation. In our reconstruction, 
however, the referring expression module directly 
computes the Semantic Representations of referring 
expressions. 
We believe that this is a good example of a 
case where developing a system with the RAGS 
data structures in mind simplifies the task. There 
are undoubtedly many different ways in which the 
same results could be achieved, and there are many 
(linguistic, engineering etc.) reasons for choosing 
one rather than another. Our particular choice is 
driven by the desire for conceptual simplicity, rather 
than any strictly linguistic or computational motiva- 
tions. We considered for each module which RAGS 
level(s) it contributed to and then implemented it to 
manipulate that (or those) level(s). In this case, that 
meant a much more conceptually simple module 
which just adds information to the Semantic Rep- 
resentations. 
Lexical Choice In CGS, this module performs a 
range of tasks, including what we might call the 
later.stages of_referring expression generation and 
lexical choice, before converting the plan leaves 
into FDs (Functional Descriptions), which serve as 
the input to the FUF/SURGE module. In the re- 
construction, on the other hand, referring expres- 
sions have already been computed and the Rhetor- 
ical Representation, with its now complete Seman- 
tic Representations, needs to be "lexicalised" and 
74 
' ,t ~1  
" .  set 
Figure 5: Combined Semantic and Abstract Syntactic Representation 
translated into FUF/SURGE format. Lexicalisa- 
tion in our terms involves adding the lexeme and 
major category information to the Abstract Syntac- 
tic Representations for the semantic predicates in 
each Semantic Representation. The FUF/SURGE 
input format was regarded as a combination of Se- 
mantic and Abstract Syntactic information, and this 
can easily be produced from the RAGS representa- 
tions. The combined Semantic and Abstract Syn- 
tactic Representations for the plan step "These two 
charts present information about house sales from 
data set ts-1740" is shown in Figure 5. The boxes 
indicate suppressed subgraphs of the lexemes cor- 
responding to the word in the boxes and triangles 
indicate suppressed subgraphs of the two adjuncts. 
6 Conclusions 
The reconstruction of CGS has taken the form of 
working out in detail the RAGS representations 
passed between modules at each stage for a set 
of key examples and reimplementing the modules 
(apart from the Planner and Realiser) in a way that 
correctly reproduces these representations. The ac- 
tual implementation used an incrementally growing 
data store for the RAGS representations which the 
modules accessed in turn, though the passing of data 
could also have been achieved in other ways. 
The fact that the reconstruction has been success- 
ful indicates that the RAGS architecture is broadly 
adequate to redescribe this NLG system: 
? No changes to the existing levels of represen- 
tation were needed, though it was necessary to 
make extensive use of partial and mixed repre- 
sentations. 
o No new levels of representation needed to be 
introduced to capture the inter-module com- 
munication of the system. 
o All of the levels of representation_apart from 
the Conceptual level were used significantly in
the reconstruction. 
In some ways, i t  is unfortunate that none of the 
inter-module interfaces of CGS turned out to use a 
single level of RAGS representation. Given the mo- 
tivation for partial and mixed representations above, 
however, this did not really come as a surprise. It 
may well be that any really useful reusable modules 
for NLG will have to have this complexity. 
75 
In spite of the successful testing of the RAGS data 
model, somedifficulties were encountered: 
* It was difficult to determine the exact nature 
of the representations produced by the Planner, 
though in the end we were able to develop a 
system to automatically translate these into a 
format we could deal with. 
o Although the theoretical model o f  CGS has a 
simple modular structure, in practice the mod- 
ules are very tightly inte-gr~ifed and making-the " 
exact interfaces explicit was not always easy. 
? Referring expression generation requires fur- 
ther access to the "knowledge base" holding 
information about he graphic to be produced. 
This knowledge was only available via interac- 
tions with SAGE, and so it was not possible to 
determine whether the RAGS view of Concep- 
tual Representations was applicable. Our own 
implementation f referring expression gener- 
ation had to work around this problem in a non- 
portable way. 
? It became clear that there are many housekeep- 
ing tasks that an NLG system must perform 
following Lexical Choice in order for the final 
Semantic and Abstract Syntactic Representa- 
tions to be appropriate for direct input to a re- 
alisation system such as FUF. 
o The fact that the system was driving 
FUF/SURGE seems to have had a signif- 
icant effect on the internal representations 
used by CGS. The reconstruction echoed this 
and as a result may not be as general as could 
be desired. 
? Even though CGS only performs imple types 
of Aggregation, it is clear that this is a critical 
module for determining the final form of sev- 
eral levels of representation. 
The division of CGS into modules is different from 
that used in any NLG systems we have previously 
worked on and so has been a useful stimulus to think 
about ways in which reusable modules can be de- 
signed. We envisage reusmgat  least,the reimple- 
mentation of the Centering module in our further 
work. 
References 
R. Brachman and J. Schmolze. 1985. An overview of the KL- 
ONE knowledge representation system. Cognitive Science, 
9:171-216. 
Lynne Cahill and Mike Reape. 1998. Component asks 
in applied NLG .systems . . . .  Technical Report ITR!- 
99-05, ITRI, University of Brighton. obtainable at 
http:/lwww.itri.brighton.ac.uk/projects/rags/. 
Lynne Cahill, Christy Doran, Roger Evans, Chris Mellish, 
Daniel Paiva, Mike Reape, Donia Scott, and Neil Tipper. 
1999. In Search of a Reference Architecture for NLG Sys- 
tems. In Proceedings of the 7th European Workshop on Nat- 
ural Language Generation, pages 77-85, Toulouse. 
Robert Dale and Ehud Reiter. 1995. Computational interpre- 
tations of the Gricean maxims in the generation ofreferring 
expressions. Cognitive Science, 18:233-263. 
B J .  Grosz, A/K.J6shil-and S.Weinstein. 1995~ Centering: a 
framework for modelling the local coherence of discourse. 
Computational Linguistics, 21 (2):203-226. 
H. Kamp and U. Reyle. 1993. From discourse to logic: Intro- 
duction to model theoretic semantics of natural language, 
formal logic and discourse representation theory. Kluwer, 
Dordrecht; London. 
R. T. Kasper. 1989. A flexible interface for linking applica- 
tions to penman's sentence generator. In Proceedings of the 
DARPA Speech and Natural Language Workshop, Philadel- 
phia. 
C. Mellish, R. Evans, L. Cahill, C. Doran, D. Paiva, M. Reape, 
D. Scott, and N. Tipper. 2000. A representation forcomplex 
and evolving data dependencies in generation. In Proceed- 
ings of the Applied Natural Language Processing (ANLP- 
NAACL2000) Conference, Seattle. 
V. O. Mittal, S. Roth, J. D. Moore, J. Mattis, and G. Carenini. 
1995. Generating explanatory captions for information 
graphics. In Proceedings of the 15th International Joint 
Conference on Artificial Intelligence (IJCAI'95), pages 
1276-1283, Montreal, Canada, August. 
V. O. Mittal, J. D. Moore, G. Carenini, and S. Roth. 1998. 
Describing complex charts in natural anguage: A caption 
generation system. Computational Linguistics, 24(3):431- 
468. 
Daniel Paiva. 1998. A survey of applied natural lan- 
guage generation systems. Technical Report ITRI- 
98-03, Information Technology Research Insti- 
tute (ITRI), University of Brighton. Available at 
http://www.itri.brighton.ac.uk/techreports. 
Ehud Reiter. 1994. Has a consensus NL generation architec- 
ture appeared and is it psycholinguistically p ausible? In 
Proceedings of the Seventh International Workshop on Nat- 
ural Language Generation, pages 163-170, Kennebunkport, 
Maine. 
Acknowledgements 
We would like to thank the numerous people who have 
helped us in this work. The developers of CGS, especially 
Giuseppe Carenini and Vibhu Mittal; the RAGS consultants 
and other colleagues at Brighton and Edinburgh, who have con- 
tributed greatly to our development ofthe representations; and 
finally to the anonymous reviewers of this paper. 
76 
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 169?174,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Tag2Blog: Narrative Generation from Satellite Tag Data
Kapila Ponnamperuma Advaith Siddharthan Cheng Zeng Chris Mellish
Department of Computing Science
University of Aberdeen
{k.ponnamperuma, advaith, c.zeng, c.mellish}@abdn.ac.uk
Rene? van der Wal
Aberdeen Centre for Environmental Sustainability (ACES)
University of Aberdeen
r.vanderwal@abdn.ac.uk
Abstract
The aim of the Tag2Blog system is to
bring satellite tagged wild animals ?to
life? through narratives that place their
movements in an ecological context. Our
motivation is to use such automatically
generated texts to enhance public engage-
ment with a specific species reintroduction
programme, although the protocols devel-
oped here can be applied to any animal or
other movement study that involves signal
data from tags. We are working with one
of the largest nature conservation chari-
ties in Europe in this regard, focusing on
a single species, the red kite. We de-
scribe a system that interprets a sequence
of locational fixes obtained from a satellite
tagged individual, and constructs a story
around its use of the landscape.
1 Introduction
We present a system, Tag2Blog, that uses Natu-
ral Language Generation (NLG) in bringing up-to-
date information about wild animals in their nat-
ural environment to nature enthusiasts. We fo-
cus on the reintroduction of the red kite to the
UK. The red kite, a member of the raptor fam-
ily, has been persecuted to near extinction in the
UK. Since 1989, efforts have been underway to
reintroduce the species across the UK with mixed
success. Where less successful, illegal activities of
humans are partly responsible (Smart et al, 2010).
We are working with the RSPB1, one of the
largest nature conservation charities in Europe,
around a reintroduction site where the species
struggles to get re-established. We propose to use
NLG for public engagement around a small num-
ber of satellite tagged individuals. The nature con-
servation goal is to create a positive perception of
1http://www.rspb.org.uk
the species through informative blogs based on the
movements of individual birds. The NLG goal is
the generation of these blogs; specifically, to put
individual locations of a bird into an ecological
context. This paper describes the design and im-
plementation of the system. We are also carrying
out concurrent ecological research on red kites that
will further inform the NLG component.
2 Related work
There is increasing realisation of the potential of
digital approaches, including the use of websites
and social media, to increase public engagement
with nature conservation issues. For instance, in
the UK, the Open Air Laboratories (OPAL) net-
work2 is a large initiative led by Imperial Col-
lege, which aims to create and inspire a new gen-
eration of nature-lovers by getting people to ex-
plore their local environment (Silvertown, 2009).
Such initiatives are typically labour and time in-
tensive, and require continual effort to maintain in-
terest through the creation of new content. To date,
initiatives such as OPAL have largely focused on
biological recording as a public engagement tool,
thereby using - for example - standard social net-
working sites to prompt the collection of species
distributional data (Stafford et al, 2010), or web
interfaces that use NLG to provide feedback to cit-
izen scientists (Blake et al, 2012).
We propose something altogether different: the
use of sensor data as a starting point for public en-
gagement through the delivery of self-updating au-
tomatically generated blogs. This application pro-
vides fresh challenges for the field of NLG, where
typically systems are designed to offer decision
support in the workplace (Goldberg et al, 1994;
Portet et al, 2009). Decision support requires ac-
curacy and clarity first and foremost. We, on the
other hand, aim to generate texts that are suffi-
2http://www.opalexplorenature.org
169
Figure 1: Screenshot of the Tag2Blog system.
ciently fluent and engaging for the general public
to be attracted and informed by.
This does not mean that there is no precedent
to our work. There are a handful of NLG sys-
tems that offer ?info-tainment?, such as Dial Your
Disc (Van Deemter and Odijk, 1997) and Ilex
(O?Donnell et al, 2001). Systems that gener-
ate sports commentary are particularly relevant, as
they contextualise objects spatially and temporally
and track the movement of objects as part of the
game analysis (Andre? et al, 2000). Rhodes et al
(2010) further explore dramatic narrative genera-
tion, to bring emotional content into the texts.
We subscribe to the same goals, adding to these
the requirement that texts should be easy to read.
For instance, ecological concepts (such as site fi-
delity) could be communicated by explicitly defin-
ing them. However, we would prefer these to be
inferred from more engaging narratives, such as
that in Fig. 1, which is a screenshot showing sam-
ple text generated by our system.
3 System architecture
The aim of the Tag2Blog system is to bring satel-
lite tagged individuals of a species (e.g., the red
kite) ?to life? by constructing narratives describ-
ing their movements. In this regard, we need to
interpret a sequence of locational fixes obtained
from a tagged bird, and construct a story around
its use of the landscape. To facilitate ecologi-
cal interpretations, it is important to first supple-
ment the locational data with other spatially rel-
evant data; for example, landscape features and
Figure 2: Architecture of the Tag2Blog system
weather. The Tag2Blog system therefore consists
of two modules: Data Acquisition and Contextual-
isation (DAC), described in ?3.1 and Natural Lan-
guage Generation (NLG), described in ?3.2.
3.1 Data acquisition and contextualisation
This module is composed of a spatial database
and a set of services for updating and access-
ing data. We start with the information obtained
from the satellite tags on the birds, which provide
time-stamped locational information. This is aug-
mented with data of associated habitat types, ter-
rain features, place names and weather conditions.
Our database thus stores rich information about
the locations visited, acquired from a variety of
sources summarised below:
Habitats: Land cover maps3 are used to as-
sociate different habitat types (e.g., coniferous
woodland, moorland, improved grassland, etc.) to
3http://www.ceh.ac.uk
170
locational fixes.
Terrain features: Ordnance Survey Vector Map
data4 are used to identify features (e.g., lochs,
rivers, roads, etc.) in the vicinity of the fixes.
Names: Ordnance Survey Gazetteer data is used
to obtain place and feature names.
Weather: The closest weather station to the fix
is queried for historical weather data from the time
of the fix, using an external web service.
The following services were implemented to
update and enrich red kite location fixes:
Data update service: The satellite tags on the
red kites have been programmed to transmit up to
5 GPS fixes per day, usually every two hours be-
tween 8am and 6pm5. The satellite data provider
sends a daily email, using which we update the
spatial database with red kite locations automat-
ically. We also provide the conservation charity
with a user interface, to allow them to censor eco-
logically sensitive locations (such as nesting sites),
as and when required.
Data analysis service: Location data of each
individual bird is periodically clustered (i.e.,
weekly) to identify their temporary home ranges.
These clusters are spatially represented as ellipses
and are stored in the database so that new fixes can
be compared against known locational patterns.
Weather web service client: Weather data rele-
vant to the time and location of each red kite loca-
tional fix is obtained on demand from a met office
web service by providing the date, time, and the
closest weather station.
Data access service: Each satellite fix is asso-
ciated with a Java object (GeoLocation), which
encapsulates the enriched data (habitats, place
names, features, weather, etc.) for that loca-
tion. Apart from individual locations, overall
fight parameters such as distance from geographic
features, displacement from or presence within
known home ranges, are also computed and en-
capsulated into a Java object. These objects are
generated on demand and passed onto the NLG
module, described next.
3.2 Natural language generation module
The Tag2Blog system follows the NLG architec-
ture proposed by Reiter and Dale (2000) and is
4http://www.ordnancesurvey.co.uk
5The satellite tags are solar powered, and only have power
to provide a single fix per day in the winter months.
composed of three components: a document plan-
ner (?3.2.2), a microplanner (?3.2.3) and a surface
realiser (?3.2.4). The document planner utilises a
domain model (?3.2.1) to populate and order mes-
sage definitions, which are in turn passed on to the
microplanner for creating sentence specifications.
The surface realiser then generates text from these
sentence.specifications.
3.2.1 Domain model and data analysis
The enriched data, as described above, be used
as such to generate narratives of journeys. How-
ever in order to make these narratives insightful,
an ecological interpretation is needed, and kite
behaviours must also be included in the domain
model. Siddharthan et al (2012) has identified
key behaviours that can be fruitfully communi-
cated through such narratives. We broadly cate-
gorise these behaviours into:
? Site fidelity and exploratory behaviour
? Feeding and roosting behaviour
? Social behaviour (associations with other red kites)
A domain model was developed to infer likely
kite behaviours from the enriched data. To build
the domain model, we used explicit and implicit
knowledge elicitation methods, such as data anal-
ysis and interviews, annotations of NLG produced
blogs by ecologists, and analysis of hand-written
blogs by ecologists from source data.
Site fidelity and exploratory behaviour: His-
torical location data is used to identify clusters
(temporary home ranges) for each bird using the
ADEHABITATHR6 package (Calenge, 2006). In
order to describe the overall movement pattern
during the period, spatial data analysis is carried
out and parameters, such as total distance trav-
elled, displacement from clusters, percentage of
fixes within each cluster, are calculated. These
parameters are then used to identify the overall
movement pattern. Fig. 3 shows three such pat-
terns: Stationary, Short circular trip and Long dis-
tance movement.
Feeding and roosting behaviours: After con-
ducting structured interviews with ecologists and
analysing blogs written by ecologists, a set of
rules were created to identify different feeding
and roosting behaviours. Likely foraging patterns
were defined on the basis of habitat type, season,
6http://cran.rstudio.com/web/packages/adehabitatHR
171
(a) (b) (c)
Figure 3: Movement patterns demonstrated in different weeks by different birds: (a) Stationary, staying
within the temporary home range, (b) Short circular trip, moving out and returning to the temporary
home range, and (c) Long distance movement, ending in a different location. The orange areas represent
clusters of locations of the red kite from historical data that model temporary home ranges for the bird.
time of the day and weather conditions. The fol-
lowing extract from a blog written by an ecolo-
gist shows how experts can infer a behaviour from
data. Note that it is acceptable in our application
for such behaviours to be speculative, as long as
they have a basis in kite ecology, and are plausible
given the data.
?Early that evening she was seen in farmland
near Torness. Here, the rain must have brought
up earthworms to the surface snacks well worth
catching!?
From this text, we inferred the following rule:
Rule: Feeding on Earthworms
IF it previously rained AND habitat is farm land,
THEN it is likely that the red kite is feeding on
earthworms.
We have expressed a range of such behaviours as
JBoss7 rules.
Social behaviours: Red kites being social birds,
there are many social interactions that could be in-
ferred from the type of data we brought together.
Associations between red kites are typically in-
ferred by analysing relative locations of differ-
ent red kites. However, there is one specific be-
haviour, communal roosting, where a large group
of red kites sleeps together in woodland during
the winter months, for which we make use of our
7http://www.jboss.org/drools
knowledge of known communal roost locations;
i.e., local knowledge provided by ecologists.
3.2.2 Document planner
The document planner carries out content determi-
nation and document structuring.
Content determination: There are several
types of message definitions, implemented as
Java classes, that correspond to different narrative
descriptions (flying, feeding, etc.). The message
generator infers possible behaviours (feeding,
roosting, exploring, etc.) using the domain model
and then selects one or more based on content
determination rules. For example, the message
generator might infer possible behaviours such
as feeding and exploring from the analyses
described above in ?3.2.1. However, the content
determination rules would prioritise exploring
behaviours over feeding (due to their rarity) and
hence generate a EXPLORINGMESSAGE, which
contains the information required to generate a
description of the exploration journey. Similarly,
corresponding messages would be generated for
other flying, feeding, and social behaviours.
Document structuring: Our weekly blogs con-
tain an introductory paragraph, which captures the
overall movement pattern for the week, followed
by a more detailed paragraph, which describes in-
teresting behaviours during that week. Each para-
172
graph is internally represented as a schema, which
also orders the messages into a document plan.
3.2.3 Microplanner
The document plan generated at the previous stage
is passed on to the microplanner for creating text
specifications. This includes phrase specifications
and their aggregation into sentences. Clauses are
combined using discourse cues to express different
discourse relations, such as concession, compari-
son and explanation.
3.2.4 Surface realiser
The role of the surface realiser is to convert the
text specification received from the microplanner
into text that the user can read and understand.
This includes linguistic realisation (converting the
sentence specifications into sentences) and struc-
tural realisation (structuring the sentences inside
the document). Both the linguistic and structural
realisations are performed by using functionali-
ties provided by the SIMPLENLG realiser library
(Gatt and Reiter, 2009).
4 Utility of blogs in this domain
Until recently, our partner charity was publishing
hand-written blogs based on the journeys of these
satellite tagged red kites. They have had to close
down the site due to resource constraints: Such
blogs are difficult, monotonous and time consum-
ing to produce by hand. Tag2Blog will allow the
charity to restart this form of public engagement.
We have earlier studied the use of ecological
blogs based on satellite tag data (Siddharthan et
al., 2012). Using hand-written blogs in a toy do-
main, we found that readers were willing to an-
thromorphise the bird, and generally formed a pos-
itive perception of it. Additionally, users were able
to recall ecological insights communicated in the
blog, demonstrating that such blogs are informa-
tive as well.
In this paper, we restrict ourselves to reporting
a very preliminary evaluation of the quality of the
computer generated blogs. We compared three
blogs produced from the same data (the move-
ments of one individual red kite during one week):
a) A computer generated blog of a journey, produced
without using any domain knowledge as described in
?3.2.1, and merely describing spatial movements of the
red kite over time.
b) A computer generated blog of a journey with ecological
insights, as described in ?3.2.1. This is the production
version used in Fig. 1.
c) Version (a), which has been post-edited by an ecologist
to introduce ecological insights into the narrative. The
ecologist was give access to a table containing habitat,
terrain and weather information for each satellite fix.
Tab. 1 shows samples from the three versions.
All three versions were shown to five human
judges, without indication of provenance. They
were asked to rate each blog on a scale of 1 (low)
to 5 (high) for how readable, informative, engag-
ing and ecologically sound they considered the
texts. They were also asked to rate the relevance of
each blog to different age groups (primary school
children, secondary school children and adults).
We used as judges, a social scientist specialised
in human?nature interactions, a public engage-
ment officer at our University who interacts with
local schools on a regular basis, a secondary
school English teacher, and two school students,
aged 14 and 16. Our goal was to obtain a diversity
of opinion to inform system design.
Tab. 2 shows the ratings of our five evalua-
tors for different aspects of each blog. The av-
erages show that in most aspects, version (b) is
rated higher than version (a) and, rather expect-
edly, the human edited/annotated version (c) is
rated the highest. But, note that the two school
students rated the automatically generated blogs
highly, and that both felt that version (b) was the
best suited for secondary schools. The public en-
gagement officer rated (b) as less readable, and
less relevant to schools. She specifically high-
lighted the use of terminology without introduc-
tion (e.g., ?roost? and ?foraging?) as an issue.
Our focus will now be on improving the lan-
guage, to address some of the readability and en-
gagingness concerns.
5 Conclusions and Future Work
We have presented an NLG system that can gen-
erate ecologically informative and engaging narra-
tives of animal (red kite) movements. Our initial
evaluations have shown encouraging results and
further evaluations are now planned. The system
can be accessed through http://redkite.abdn.ac.uk/blog/.
Acknowledgements
This research is supported by an award made
by the RCUK Digital Economy programme
to the dot.rural Digital Economy Hub (ref.
EP/G066051/1). We also thank our partner organ-
isation, the RSPB (with special thanks to Stuart
Benn), who manage the reintroduction project.
173
Text First four sentences from each blog
(a) This week, Millie did not travel far, but was actively exploring a small area. During this week, Millie has been
observed on various habitats. However, except Thursday she chose to spend the night at the same woodland near
Torness. No doubt Millie was not alone this week as kites Moray and Beauly were also observed often in the
vicinity.
(b) This week, Millie did not travel far, but was actively exploring a small area mainly within her home range. During
this week, Millie?s foraging patterns have been varied. However, except Thursday she chose to roost in the same
woodland near Torness. No doubt Millie had a quite social week as kites Moray and Beauly were also observed
often in the vicinity.
(c) This week Millie did not travel far but was actively exploring a small area north-east of Loch Ness. Friday morning
Millie left the woodland where she spend the night to fly to Loch Ruthven amid heavy rain. The poor visibility
may have driven her to fly low when searching for food along the water sides. Early that evening she was seen in
farmland near Torness.
Table 1: Excerpts of texts in each experimental condition
Sociologist Pub. Eng. Teacher 16yo 14yo Average
Blog a b c a b c a b c a b c a b c a b c
Readability 3 3 5 4 3 5 3 2 4 3 4 4 3 4 4 3.2 3.2 4.4
Informativeness 3 4 5 5 5 5 2 1 2 3 4 5 3 3 4 3.2 3.4 4.4
Engagingness 2 4 5 3 3 4 2 1 3 3 4 5 2 4 4 2.4 3.2 4.2
Ecological soundness 4 3 3 4 4 4 5 5 5 3 4 4 3 4 3 3.8 4.0 3.8
Relevance to:
Primary Schools 3 4 5 3 2 4 4 4 4 4 4 3 3 2 3 3.4 3.2 3.8
Secondary Schools 3 4 5 4 3 4 2 2 2 4 5 3 3 4 3 3.2 3.6 3.4
Adults 3 4 5 4 4 4 3 1 3 3 4 5 3 4 4 3.2 3.4 4.2
Table 2: Evaluation of Blogs by Experts
References
E. Andre?, K. Binsted, K. Tanaka-Ishii, S. Luke, G.
Herzog, and T. Rist. 2000. Three robocup simu-
lation league commentator systems. AI Magazine,
21(1):57.
S. Blake, A. Siddharthan, H. Nguyen, N. Sharma, A.
Robinson, E. O Mahony, B. Darvill, C. Mellish, and
R. van der Wal. 2012. Natural language genera-
tion for nature conservation: Automating feedback
to help volunteers identify bumblebee species. In
Proceedings of the 24th International Conference on
Computational Linguistics (COLING 2012), pages
311?324.
C. Calenge. 2006. The package adehabitat for the r
software: tool for the analysis of space and habitat
use by animals. Ecological Modelling, 197:1035.
A. Gatt and E. Reiter. 2009. SimpleNLG: A realisation
engine for practical applications. In Proceedings of
the 112th European Workshop on Natural Language
Generation (ENLG), pages 90?93.
E. Goldberg, N. Driedger, and R.I. Kittredge.
1994. Using natural-language processing to produce
weather forecasts. IEEE Expert, 9(2):45?53.
M. O?Donnell, C. Mellish, J. Oberlander, and A. Knott.
2001. Ilex: an architecture for a dynamic hypertext
generation system. Natural Language Engineering,
7(3):225?250.
F. Portet, E. Reiter, A. Gatt, J. Hunter, S. Sripada,
Y. Freer, and C. Sykes. 2009. Automatic gener-
ation of textual summaries from neonatal intensive
care data. Artificial Intelligence, 173(7-8):789?816.
E. Reiter and R. Dale. 2000. Building Natural Lan-
guage Generation Systems. Cambridge University
Press.
M. Rhodes, S. Coupland, and T. Cruickshank. 2010.
Enhancing real-time sports commentary generation
with dramatic narrative devices. In Proceedings of
the Third Joint Conference on Interactive Digital
Storytelling, ICIDS 2010, Lecture Notes in Com-
puter Science, pages 111?116. Springer.
A. Siddharthan, M. Green, K. van Deemter, C. Mel-
lish, and R. van der Wal. 2012. Blogging birds:
Generating narratives about reintroduced species to
promote public engagement. In Proceedings of the
Seventh International Natural Language Generation
Conference (INLG), pages 120?124.
J. Silvertown. 2009. A new dawn for citizen science.
Trends in Ecology & Evolution, 24(9):467?471.
J. Smart, A. Amar, I.M.W. Sim, B. Etheridge, D.
Cameron, G. Christie, and J.D. Wilson. 2010.
Illegal killing slows population recovery of a re-
introduced raptor of high conservation concern?the
red kite milvus milvus. Biological Conservation,
143(5):1278?1286.
R. Stafford, A.G. Hart, L. Collins, C.L. Kirkhope, R.L.
Williams, S.G. Rees, J.R. Lloyd, and A.E. Goode-
nough. 2010. Eu-Social Science: The Role of Inter-
net Social Networks in the Collection of Bee Biodi-
versity Data. PloS one, 5(12):e14381.
K. van Deemter and J. Odijk. 1997. Context model-
ing and the generation of spoken discourse. Speech
Communication, 21(1-2):101?121.
174
Using Tactical NLG to Induce Affective States: Empirical Investigations
Ielka van der Sluis
Computing Science,
University of Aberdeen
i.v.d.sluis@abdn.ac.uk
Chris Mellish
Computing Science
University of Aberdeen
c.mellish@abdn.ac.uk
Abstract
This paper reports on attempts at Aberdeen1
to measure the effects on readers? emotions of
positively and negatively ?slanted? texts with
the same basic message. The ?slanting? meth-
ods could be implemented in an (NLG) sys-
tem. We discuss a number of possible reasons
why the studies were unable to show clear, sta-
tistically significant differences between the
effects of the different texts.
1 Introduction: Affective NLG
?Affective NLG? has been defined as ?NLG that re-
lates to, arises from or deliberately influences emo-
tions or other non-strictly rational aspects of the
Hearer? (De Rosis and Grasso, 2000). Although this
term could cover a range of types of NLG, in prac-
tice, a lot of work on affective NLG emphasises the
depiction of emotional states/personalities (Oberlan-
der and Gill, 2004), rather than the induction of emo-
tional effects on readers. However, there are many
applications where the intention is, for instance, to
motivate or discourage, as well as to inform.
How can NLG influence the emotions of its read-
ers? It is apparent that strategical decisions (?what
to say?) can make a difference on how a reader re-
sponds emotionally to a text. If you tell someone
good news, they will be happier than if you tell them
bad news. On the other hand, much of NLG is con-
cerned with tactical decisions (?how to say it?), and
the affective relevance of these is less clear. Can tac-
tical NLG choices be used to achieve goals in terms
1Ielka van der Sluis is now at the Department of Computer
Science, Trinity College, Dublin
of the reader?s emotions? In the area of affective
computing, there has been some work on assess-
ing the effects of interfaces on the emotions of their
users, e.g. on their frustration levels (Prendinger et
al., 2006) or their feelings of support/trust (Lee et
al., 2007). In NLG there has been some work on
task-based evaluation cf. STOP (Reiter et al, 2003)
and SKILLSUM (Williams and Reiter, forthcoming).
However, to our knowledge, there has not yet been
any demonstration of tactical decisions making a
difference on a reader?s emotions.
The paper is organised as follows: Section 2 intro-
duces the tactical choices we are studying, our test
texts and a text validation study. Section 3 discusses
a pilot study that was conducted to try out poten-
tial psychological measurement methods. Section 4
presents a full study to measure the affect of text in-
voked in readers. The paper closes with a discussion
of the findings and their possible implications.
2 Tactical Choices
We decided that a safe way to start would be
to choose primitive positive versus negative emo-
tions (such as sadness, joy, disappointment, sur-
prise, anger), as opposed to more complex emo-
tions related to trust, persuasion, advice, reassur-
ance. Therefore we focus here on alternatives that
give a text a positive or negative ?slant?. These could
be applied by an NLG system whose message has
?positive? and ?negative? aspects, where ?positive?
information conjures up scenarios that are pleasant
and acceptable to the reader, makes them feel happy
and cooperative etc. and ?negative? information
conjures up unpleasant or threatening situations and
68
so makes them feel more unhappy, confused etc. For
instance, (DeRosis et al, 1999) discuss generating
instructions on how to take medication which have
to both address positive aspects (?this will make you
feel better if you do the following?) and also negative
ones (?this may produce side-effects, which I have to
tell you about by law?). An NLG system in such a
domain could make itself popular by only mention-
ing the positive information, but then it could leave
itself open to later criticism (or litigation) if by do-
ing so it clearly misrepresented the true situation.
Although it may be inappropriate grossly to misrep-
resent the provided message, there are more subtle
(tactical) ways to ?colour? or ?slant? the presenta-
tion of the message in order to emphasise either the
positive or the negative aspects.
We assume that the message to be conveyed is
a simple set of propositions, each classified in an
application-dependent way as having positive or
negative polarity according to whether the reader is
likely to welcome it or be unhappy about it in the
context of the current message.2 In general, this
classification could, for instance, be derived from
the information that a planning system has about
which propositions support which goals (e.g. to stay
healthy one needs to eat healthy food). We also as-
sume that a possible phrasing for a proposition has
a magnitude, which indicates the degree of impact it
has. This is independent of the polarity. We will not
need to actually measure magnitudes, but when we
make claims that one wording of a proposition has
a smaller magnitude than another we indicate this
with <. For instance, we would claim that usually:
?a few rats died? < ?many rats died?
Thus we claim that ?a few rats died? has less im-
pact than ?many rats died?, whether or not rats dy-
ing is considered a good thing (i.e. whether the po-
larity is positive or negative). In general, an NLG
system can manipulate the magnitude of wordings
of the propositions it expresses, to indicate its own
(subjective) view of their importance. In order to
slant a text positively, it can express positive polarity
propositions in ways that have high magnitudes and
negative polarity propositions in ways that have low
2Note that this sense of ?polarity? is not the same as the one
used to describe ?negative polarity items? in Linguistics
magnitudes. The opposite applies for negative slant-
ing. Thus, for instance, in an application where it
is bad for rats to die, expressing a given proposition
by ?a few rats died? would be giving more of a pos-
itive slant, whereas saying ?many rats died? would
be slanting it more negatively.
Whenever one words a proposition in different
ways, it can be claimed that a (perhaps subtle)
change of meaning is involved. In an example like
this, therefore, perhaps the content of the message
changes between the two wordings and so this is in
fact a strategic alternation. In this work, we take the
view that it is legal to make changes that relate to the
writer?s attitude to the material of the text. The dif-
ference between ?a few rats? and ?many rats? is (in
our view) that the number of rats is either less than
or more than the writer would have expected. We
can therefore choose between these alternatives by
varying the writer, not the underlying message. An-
other reason for considering this choice as tactical
is that in an NLG system, it would likely be imple-
mented somewhere late in the ?pipeline?. Our claim
that pairs such as this can appropriately describe the
same event is also supported by our text validation
experiments described below.
2.1 Test Texts
We started by composing by hand two messages
containing mainly negative and positive polarity
propositions respectively. The negative message
tells the reader that a cancer-causing colouring sub-
stance is found in some foods available in the su-
permarkets. The positive message tells the reader
that foods that contain Scottish water contain a min-
eral which helps to fight cancer. The first paragraph
of both texts states that there is a substance found
in consumer products that has an effect on people?s
health and it addresses the way in which this fact
is handled by the relevant authorities. The second
paragraph of the text elaborates on the products that
contain the substance and the third paragraph ex-
plains in what way the substance can affect people?s
health.
To study the effects of different wordings, for
each text a positive and a negative version was pro-
duced by slanting propositions in either a positive
or a negative way. This resulted in four texts in to-
tal, two texts with a negative message one positively
69
and one negatively phrased (NP and NN), and two
texts with a positive message one positively and one
negatively verbalised (PP and PN). To maximise the
impact aimed for, various slanting techniques were
used by hand as often as possible without loss of be-
lievability (this was assessed by the intuition of the
researchers). The positive and negative texts were
slanted in parallel as far as possible, that is in both
texts similar sentences were adapted so that they em-
phasised the positive or the negative aspects of the
message. The linguistic variation used in the texts
was algorithmically reproducible and the techniques
are illustrated below. A number of these were sug-
gested by work on ?framing? in Psychology (Moxey
and Sanford, 2000; Teigen and Brun, 2003). Indeed,
that work also suggests further variations that could
be manipulated, for instance, the choice between us-
ing numerical and non-numerical values for express-
ing quantities.
SLANTING EXAMPLES FOR THE NEGATIVE MESSAGE
Here it is assumed that recalls of products, risks
of danger etc. involve negative polarity proposi-
tions. Therefore negative slanting will amongst
other things choose high magnitude realisations for
these.
Techniques involving adjectives and adverbs:
- ?A recall? < ?A large-scale recall? of infected
merchandise was triggered
Techniques involving quantification:
- Sausages, tomato sauce and lentil soup are
?some? < ?only some? of the affected items
Techniques involving a change in polarity
Proposition expressed with positive polarity:
- Tests on monkeys revealed that as many as ?40
percent? of the animals infected with this sub-
stance ?did not develop any tumors?
Proposition expressed with negative polarity:
- Tests on monkeys revealed that as many as ?60
percent? of the animals infected with this sub-
stance ?developed tumors?.
Techniques manipulating rhetorical prominence
Positive slant:
- ?So your health is at risk, but every possible
thing is being done to tackle this problem?
Negative slant:
- ?So although every possible thing is being
done to tackle this problem, your health is at
risk?
SLANTING EXAMPLES FOR THE POSITIVE MESSAGE
Here it is assumed that killing cancer, promoting
Scottish water etc. involve positive polarity proposi-
tions. Therefore positive slanting will amongst other
things choose high magnitude realisations for these.
Techniques involving adjectives and adverbs:
- Neolite is a ?detoxifier? < ?powerful detoxi-
fier? preventing cancer cells
Techniques involving quantification:
- ?Cancer-killing Neolite? < ?Substantial
amounts of cancer-killing Neolite? was found
in Scottish drinking water
Techniques involving a change in polarity
Proposition expressed with negative polarity:
- A study on people with mostly stage 4 can-
cer revealed that as many as ?40 percent? of
the patients that were given Neolite ?still had
cancer? at the end of the study.
Proposition expressed with positive polarity:
- A study on people with mostly stage 4 cancer
revealed that as many as ?60 percent? of the
patients that were given Neolite ?were cancer
free? at the end of the study.
Techniques manipulating rhetorical prominence
Negative slant:
- ?Neolite is certainly advantageous for your
health, but it is not a guaranteed cure for, or
defence against cancer?
Positive slant:
- ?So Although Neolite is not a guaranteed cure
for, or defence against cancer, it is certainly
advantageous for your health?
2.2 Text validation
To check our intuitions on the effects of the textual
variation between the four texts described above, a
text validation experiment was conducted in which
24 colleagues participated. The participants were
randomly assigned to one of two groups (i.e. P and
N), group P was asked to validate 23 sentence pairs
from the positive message (PN versus PP) and group
N was asked to validate 17 sentence pairs from the
negative message (NN versus NP). Each pair con-
sisted of two sentences intended to differ in their
magnitude but to be possible realisations of the same
underlying content (as in the examples in the last
section). Both the N and the P group sentence pairs
included four filler pairs. The participants in group
70
P were asked which of the two sentences in each pair
they thought most positive in the context of the mes-
sage about the positive effects of Scottish water. The
participants in group N were asked which of the two
sentences in each pair they found most alarming in
the context of the message about the contamination
of food available for consumption. All participants
were asked to indicate if they thought the sentences
in each pair could be used to report on the same
event (i.e. represented purely tactical variations).
Results in the N group indicated that in 89.75%
of the cases participants agreed with our intuitions
about which one of the two sentences was most
alarming. On average, per sentence pair 1.08 of the
12 participants judged the sentences differently than
what we expected. In 7 of the 13 sentence pairs (17
- 4 fillers) participants unanimously agreed with our
intuitions. In the other sentence pairs 1 to, maxi-
mally, 4 participants did not share our point of view.
In the two cases in which four participants did not
agree with or were unsure about the difference we
expected, we adapted our texts. One of these cases
was the pair:
?just 359? infected products have been
withdrawn < ?as many as 359? infected
products have been withdrawn ?already?
We thought that the latter of the two would be
more alarming (and correspond to negative slanting)
because it is a bad thing if products have to be
withdrawn (negative polarity). However, some
participants felt that products being withdrawn
was a good thing (positive polarity), because it
meant that something was being done to tackle the
problem, in which case the latter would be imposing
a positive slant. As a consequence of the validation
results, it was decided to ?neutralise? this sentence
in both the NP and NN versions of the text to ?359
infected products have been withdrawn?. Overall,
in 78.85% of the cases the participants thought that
both sentences in a pair could report on the same
event.
Results in the P group were similar. In 82.46% of
the cases participants agreed with our intuitions
about which one of the two sentences was most
positive. In two cases, minor changes were made to
make the texts clearer. Overall, in 86.84 % of the
cases the participants thought that both sentences in
a pair could report on the same event.
3 Pilot Study: Testing Psychological
Methods to Measure Emotions
3.1 Psychological Methods
The next step was to determine plausible methods
to measure the emotional effect of a text. There are
two broad ways of measuring the emotions of human
subjects ? physiological methods and self-reporting.
Because of the technical complications and the con-
flicting results to be found in the literature, we opted
to ignore physiological measurement methods and
to investigate self-reporting. To measure these emo-
tions we decided do a pilot study to try out three
well-established methods that are used frequently
in the field of psychology, the Russel Affect Grid
(Russell et al, 1989), the Positive and Negative Af-
fect Scale (PANAS) (Watson et al, 1988), and the
Self Assessment Manikin (SAM) (Lang, 1980). The
PANAS test is a scale consisting of 20 words and
phrases (10 for positive affect and 10 for negative
affect) that describe feelings and emotions. Partic-
ipants read the terms and indicate to what extent
they experience(d) the emotions indicated by each
of them using a five point scale ranging from (1)
very slightly/not at all, (2) a little, (3) moderately,
(4) quite a bit to (5) extremely. A total score for pos-
itive affect is calculated by simply adding the scores
for the positive terms, and similarly for negative af-
fect. The Russel Affect Grid and the SAM test both
assess valence and arousal on a nine-point scale.
3.2 Method: Subjects, Stimuli and Setting
Our pilot study aimed to test a general experiment
set up, and to help us find the most promising of
the above methods to measure emotions evoked by
text. 24 colleagues and students (other than the ones
involved in the text validation experiments) partic-
ipated as subjects in this pilot study in which they
were asked to fill in a few forms about how they
felt after reading a particular text. All, except three,
were native or fluent speakers of English and none
was familiar with the purposes of the study. The
subjects were divided in two groups of 12 subjects
each, and were asked to fill in some questionnaires
and to read a text about a general topic with a partic-
71
ular consequence for the addressee. For this exper-
iment, just the negative message texts illustrated in
the previous section were used (i.e. ?some of your
food contains a substance that causes cancer?). One
group of subjects, the NP-group, was given this neg-
ative message verbalised in a neutral way giving the
impression that although there was a problem every
possible thing was being done to tackle it. The other
group, the NN-group, was given the same negative
message presented in a negative way implying that
although many things were being done to tackle the
problem, there still was a problem. We expected that
after the subjects had read the text, the emotions of
the subjects in the NN-group would be more neg-
ative than the emotions of the subjects in the NP-
group. We also expected the subjects in the NN-
group to be more strongly affected than the subjects
in the NP-group.
For ethical reasons, both in this experiment and
the following one, the main experimental procedure
was followed by a debriefing session in which the
subjects were informed that they had been deceived
by the texts presented and during which it was possi-
ble to provide support for subjects if their emotional
reactions had been especially strong.
3.3 Results and Discussion
Overall, t-test results failed to find significant differ-
ences between the the NN-group and the NP-group
for any of the emotion measurement methods used.
The Russel test, which was taken before the partic-
ipants read the test text3, indicated that the partici-
pants in the NP group might be feeling slightly more
positive and less aroused than the participants in the
NN group. The results for the PANAS test, taken af-
ter the participants read the test text, show that the
NP group might be feeling a little bit more positive
that the NN group about the content of the text they
just read. The Sam test, which the participants were
also asked to fill out with respect to their feelings af-
ter reading the test text, indicates that the NP group
might be feeling less positive and more aroused than
the NN group.
How to interpret the outcomes of the pilot study?
There are several factors that could have caused the
3Ideally we would have presented all tests both before and
after the text was read, but we believed that this would overload
the subjects and lead to distorted results.
lack of significant results. One reason could be that
the differences between the NP and NN texts were
not large enough. Yet another reason could be that
the people that took part in the study were not really
involved in the topic of the text or the consequences
of the message. When looking at the three emotion
measurement methods used, some participants did
indicate that the SAM and Russel tests were difficult
to interpret. Also some participants showed signs
of boredom or disinterest while rating the PANAS
terms, which were all printed on one A4 page; some
just marked all the terms as ?slightly/not at all? by
circling them all in one go instead of looking at the
terms separately. Also, some participants indicated
that they found it difficult to distinguish particular
terms. For example the PANAS test includes both
?scared? and ?afraid?. As a consequence, there were
several things that could be improved and adjusted
before going ahead with a full scale experiment in
which all four texts were tested.
4 Full Study: Measuring Emotional
Effects of Text
This section presents a full scale experiment con-
ducted to assess the emotional effect invoked in
readers of a text. The experimental set up attempts
to take into account the results found in the pilot
study presented in the previous section. However,
there were obviously a number of things that could
be improved after this study, and so many things
were changed without any direct evidence that
they would improve the experiment. Below the
method, data processing and results are presented
and discussed.
4.1 Method: subjects, stimuli and
experimental setting
Based on the pilot results, the setup of this study
was adapted in a number of ways. For instance,
we decided to increase the likelihood of finding
measurable emotional effects of text by targeting
a group of subjects other than our sceptical col-
leagues. Because it has been shown that young
women are highly interested in health issues and es-
pecially health risks (Finucane et al, 2000), we de-
cided on young female students as our participants.
72
In total 60 female students took part in the experi-
ment and were paid a small fee for their efforts. The
average age of the participants was about 20.57 (std.
2.41) years old. The participants were evenly and
randomly distributed over the four texts (i.e. NN,
NP, PN, PP) tested in this study, that is 15 partici-
pants per group. The texts were tailored to the sub-
ject group, by for example mentioning food products
that are typically consumed by students as examples
in the texts and by specifically mentioning young fe-
males as targets of the consequences of the message.
On a more general level, the texts were adapted to a
Scottish audience by, for instance, mentioning Scot-
tish products and a Scottish newspaper as the source
of the article. Although the results of the pilot study
did not indicate that the texts were not believable,
we thought that the presentation of the texts could
be improved by making them look more like news-
paper articles, with a date and a source indication.
To enhance the experimental setting, the emo-
tion measurement methods were better tailored to
the task. The SAM test as well as the Russel Grid
were removed from the experiment set up, because
they caused confusion for the participants in the pi-
lot study. Another reason for removing these tests
was to reduce the number of questions to be an-
swered by the participants and to avoid bored an-
swering. For the latter reason, also a previously used
reduced version of the PANAS test (Mackinnon et
al., 1999) was used, in which the number of emo-
tion terms that participants had to rate for themselves
was decreased from 20 to 10. This PANAS set, con-
sisting of five positive (i.e. alert, determined, en-
thusiastic, excited, inspired) and five negative terms
(i.e. afraid, scared, nervous, upset, distressed), was
used both before and after participants read the test
text. Before the participants read the test text, they
were asked to indicate how they felt at that point in
time using the PANAS terms. After the participants
read the test text, they were asked to rate the affect
terms with respect to their feelings about the text.
Note that this is different from asking them about
their current feelings, because we wanted to empha-
sise that we wanted to know about their emotions re-
lated to the content of the text they just read and not
about their feelings in general. We expected that the
reduced PANAS test would produce reliable results
because of its previous successful use. Whereas in
the pilot study each test was handled individually,
the PANAS terms were now interleaved with other
questions about recall and opinions to further avoid
boredom.
4.2 Hypotheses
In this full study four texts were tested on four differ-
ent groups of subjects. Two groups read the positive
message (PP-group and PN-group) two groups read
the negative message (NN-group and NP-group). Of
the two groups that read the positive message, we
expected the positive emotions of the participants
that read the positive version of this message (PP-
group) to be stronger than the positive emotions of
the participants that read the neutral/negative version
of this message (PN-group). Of the two groups that
read the negative message, we expected the partici-
pants that read the negative version of this message
(NN-group) to be more negative than the partici-
pants that read the positive version of the message
(NP-group).
4.3 Results
Overall, participants in this study were highly inter-
ested in the experiment and in the text they were
asked to read. Participants that read the positive
message, about the benefits of Scottish water, ap-
peared very enthusiastic and expressed disappoint-
ment when they read the debriefing from which they
learned that the story contained no truth. Simi-
larly, participants that read the negative message ex-
pressed anger and fear in their comments on the
experiment and showed relief when the debriefing
told them that the story on food poisoning was com-
pletely made up for the purposes of the experiment.
Only a few participants that read a version of the
negative message commented that they had got used
to the fact that there was often something wrong
with food and were therefore less scared. Table
1 shows some descriptives that underline these im-
pressions. For instance, on a 5-point scale the par-
ticipants rated the texts they read more than mod-
erately interesting (average of po-i = 3.74). They
also found the text informative (average of inform
= 3.82) and noted that it contained new information
(average of new = 4.05). These are surprisingly pos-
itive figures when we consider that the participants
indicated only an average interest in food (average of
73
PN PP NN NP
pr-i 2.47(1.13) 3.07(1.03) 3.00(.85) 3.00(1.25)
inf 3.87(.83) 3.80(.94) 3.67(1.05) 3.93(.70)
pos 3.93(.96) 4.27(1.03) 1.67(.98) 1.67(.97)
neg 1.53(.64) 1.27(5.94) 4.07(1.22) 3.53(1.19)
new 4.13(1.18) 4.53(.64) 3.87(1.30) 3.67(1.59)
po-i 3.67(.82) 3.80(.78) 3.67(.72) 3.80(1.01)
Table 1: Means and Standard deviations (between brack-
ets) for the PN, PP, NP and NN texts for various vari-
ables: pr-i interest in food before reading the text, the
inf ormativeness of the message, the positive or negative
polarity of the message, new information and the po-i
post interest in the message. All measured on a 5-point
Scale: 1 = not at all, . . ., 5 = extremely.
pr-i = 2.89) before they read the test text. The partic-
ipants that read the negative messages (NN and NP)
recognised that the message was negative (cf. pos
and neg in Table 1). Moreover, the NN-group rated
the text more negatively than the NP-group (4.07 vs
3.53). The participants that read the positive mes-
sage found that they had read a positive message.
The PP-group rated their text slightly more positive
than the PN-group rated theirs.
The bar chart presented in Figure 1 illustrates the
results of the PANAS questionnaire after reading the
texts. In terms of the differences in message content
(P* vs N*), there is a difference between the ratings
of the negative terms, which is as expected. How-
ever, there is no significant difference for the posi-
tive terms, which were rated fairly similarly for all
groups. Also, contrary to what was expected, the rat-
ing of the negative PANAS terms by both N* groups
is lower than their rating of the positive terms. The
hoped-for results for the positive/negative slanting
are also not forthcoming - t-tests show no signifi-
cant differences between the PN-group and the PP-
group and no significant differences between the
NN-group and the NP-group. All mean ratings stay
far below 3, the ?moderate? average of the scale.
When looking at these results in more detail, it ap-
pears that, of the positive PANAS terms, only ?ex-
cited? and ?inspired? had a higher mean for the posi-
tively worded message when comparing the positive
and the negative version of the positive message (PP
and PN). When comparing the positive and the neg-
ative version of the negative message (NP vs NN),
as expected, the NN-group has lower means for all 5
positive terms than the NP group.
From this study various conclusions can be
Figure 1: Positive and negative PANAS means after the
Participants read the test text.
drawn. First of all, from the fact that only the lower
half of the 5-point PANAS scale was used it can be
concluded that the participants in this study seem
to have difficulties with reporting on their emotions.
This was the case both before and after the test text
was read. Furthermore, participants seem to have a
preference for reporting their positive emotions and
focus less on their negative emotions. This can be in-
ferred from the fact that the negative PANAS terms
of the PP-group and the PN-group were lower than
the means of the negative PANAS terms of the NN-
group and the NP-group, but all groups had about
the same means for the positive PANAS terms. The
inference that self-reporting of emotions is trouble-
some is also indicated by the fact that the partici-
pants of this full study seemed highly interested and
involved in the experiment and in what they read in
the experiment texts. The participants generally be-
lieved the story they read and they expressed dis-
appointment or relief when they were told the truth
after the experiment. In addition, the descriptives
in Table 1 show that participants generally correctly
identified the text they read as either positive or neg-
ative. Note that in this respect the more fine-grained
differences between the PP-group and the PN-group
as well as the differences between the NN-group and
the NP-group also confirm our expectations.
74
5 Conclusion and Discussion
This paper presented our efforts to measure differ-
ences in emotional effects invoked in readers. These
efforts were based on our assumption that the word-
ing used to present a particular proposition matters
in how the message is received. Participants? judge-
ments of the negative or positive nature of a text (in
both the text validation and in the full study) are in
accord with our predictions. In terms of reflective
analysis of the text, therefore, participants behave
as we expected. Although we strongly emphasised
that we were interested in emotions with respect to
the test text, our attempts to measure the emotional
effects invoked in readers caused by tactical text dif-
ferences did, however, not produce any significant
results.
There are several reasons that may have played
a role in this. It may be that the emotion measur-
ing methods we tried are not fine-grained enough
to measure the emotions that were invoked by the
texts. As mentioned above, participants only used
part of the PANAS scale and seemed to be reluc-
tant to record their emotions (especially negative
ones). Other ways of recording levels of emotional
response that are more fine-grained than a 5-point
scale, such as magnitude estimation (Bard et al,
1996), might be called for here. Carrying out exper-
iments with even more participants might reveal pat-
terns that are obscured by noise in the current study,
but this would be expensive.
Alternatively, it could be that the differences be-
tween the versions of the messages are just too sub-
tle and/or that there is not enough text for these sub-
tle differences to produce measurable effects. In-
deed, we are not aware of PANAS being used to as-
sess purely textual effects before. Perhaps it is nec-
essary to immerse participants more fully in slanted
text in order to really affect them differently. Or
perhaps more extreme versions of slanting could be
found. Perhaps indeed the main way in which NLG
can achieve effects on emotions is through appro-
priate content determination (strategy), rather than
through lexical or presentation differences (tactics).
Another reason could still be a lack of involve-
ment of the participants of the study. Although the
participants of the full study indicated their enthu-
siasm for the study as well as their interest in the
topic and the message, they may have felt that the
news did not affect them too much, because they
considered themselves as responsible people when
it comes to health and food issues. We are design-
ing a follow up experiment in which, to increase the
reader?s involvement, a feedback task is used, where
participants play a game or answer some questions
after which they receive feedback on their perfor-
mance. The study will aim to measure the emotional
effects of slanting this feedback text in a positive or
a negative way. As in such a feedback situation the
test text is directly related to the participants? own
performance, we expect an increased involvement
and stronger emotions.
As argued above, the results of our study seem
to indicate that self-reporting of emotions is diffi-
cult. This could be because participants do not like
to show their emotions, because the emotions in-
voked by what they read were just not very strong
or because they do not have good conscious access
to their emotions. Although self-reporting is widely
used in Psychology, it could be that participants are
not (entirely) reporting their true emotions, and that
maybe this matters more when effects are likely to
be subtle. In all of these situations, the solution
could be to use additional measuring methods (e.g.
physiological methods), and to check if the results of
such methods can strengthen the results of the ques-
tionnaires. Another option is to use an objective ob-
server during the experiment (e.g. videotaping the
participants and observing the duration of smiles or
frowns) to judge whether the subject is affected.
Yet another possibility would be only to measure
emotional effects via performance on a task that is
known to be facilitated by particular emotions. For
instance, one could use the methods of (Carenini and
Moore, 2000) to measure persuasiveness of different
textual realisations that may induce emotions.
Acknowledgments
This work was supported by the EPSRC
grant ?Affecting people with natural language?
(EP/E011764/1) and also in part by Science Foun-
dation Ireland under a CSET grant (NGL/CSET).
We would like to thank the people who contributed
to this study, most notably Louise Phillips, Emiel
Krahmer, Linda Moxey, Graeme Ritchie, Judith
Masthoff, Albert Gatt and Kees van Deemter.
75
References
E. G. Bard, D. Robertson, and A. Sorace. 1996. Magni-
tude estimation of linguistic acceptability. Language,
72(1):32?68.
G. Carenini and J. D. Moore. 2000. An empirical study
of the influence of argument conciseness on argument
effectiveness. In Proceedings of the 38th annual meet-
ing of the Association for Computational Linguistics.
F. DeRosis, F. Grasso, and D. Berry. 1999. Refining
instructional text generation after evaluation. Artificial
Intelligence in Medicine, 17(1):1?36.
M. Finucane, P. Slovic, C. Mertz, J. Flynn, and T. Sat-
terfield. 2000. Gender, race, and perceived risk: the
?white male? effect. Health, Risk & Society, 2(2):159
? 172.
P. Lang, 1980. Technology in Mental Health Care De-
livery Systems, chapter Behavioral Treatment and Bio-
behavioral Assessment: Computer Applications, page
119 137. Norwood, NJ: Ablex.
J.-E. Lee, C. Nass, S. Brave, Y. Morishima, H. Nakajima,
and R. Yamada. 2007. The case for caring co-learners:
The effects of a computer-mediated co-learner agent
on trust and learning. Journal of Communication.
A. Mackinnon, A. Jorm, H. Christensen, A. Korten, P. Ja-
comb, and B. Rodgers. 1999. A short form of the pos-
itive and negative affect schedule: evaluation of fac-
torial validity and invariance across demographic vari-
ables in a community sample. Personality and Indi-
vidual Differences, 27(3):405?416.
L. Moxey and A. Sanford. 2000. Communicating quan-
tities: A review of psycholinguistic evidence of how
expressions determine perspectives. Applied Cogni-
tive Psychology, 14(3):237?255.
J. Oberlander and A. Gill. 2004. Individual differences
and implicit language: Personality, parts-of-speech
and pervasiveness. In Proceedings of the 26th Annual
Conference of the Cognitive Science Society.
Helmut Prendinger, Christian Becker, and Mitsuru
Ishizuka. 2006. A study in users? physiological re-
sponse to an empathic interface agent. International
Journal of Humanoid Robotics, 3(3):371?391.
E. Reiter, R. Robertson, and L. Osman. 2003. Lessons
from a failure: Generating tailored smoking cessation
letters. Artificial Intelligence, 144:41?58.
F. De Rosis and F Grasso. 2000. Affective natural lan-
guage generation. In A. Paiva, editor, Affective Inter-
actions. Springer LNAI 1814.
J. Russell, A. Weiss, and G. Mendelsohn. 1989. Af-
fect grid: A single-item scale of pleasure and arousal.
Journal of Personality and Social Psychology, 57:493?
502.
K. Teigen and W. Brun. 2003. Verbal probabilities: A
question of frame. Journal of Behavioral Decision
Making, 16:53?72.
D. Watson, L. Clark, and A. Tellegen. 1988. Develop-
ment and validation of brief measures of positive and
negative affect: The PANAS scales. Journal of Per-
sonality and Social Psychology, 54(1063-1070).
S. Williams and E. Reiter. forthcoming. Generating basic
skills reports for lowskilled readers. Journal of Natu-
ral Language Engineering.
76
Evaluating an Ontology-Driven WYSIWYM Interface
Feikje Hielkema Chris Mellish
Computing Science
School of Natural & Computing Sciences
University of Aberdeen
Aberdeen, AB24 3FX, UK
{f.hielkema, c.mellish, p.edwards}@abdn.ac.uk
Peter Edwards
Abstract
This paper describes an evaluation study of
an ontology-driven WYSIWYM interface for
metadata creation. Although the results are
encouraging, they are not as positive as those
of a similar tool developed for the medical
domain. We believe this may be due, not to
the WYSIWYM interface, but to the complex-
ity of the underlying ontologies and the fact
that subjects were unfamiliar with them. We
discuss the ways in which ontology develop-
ment might be influenced by issues stemming
from using an NLG approach for user access
to data, and the effect these factors have on
general usability.
1 Introduction
In the PolicyGrid1 project we are investigating how
best to support social science researchers through
the use of Semantic Grid (De Roure et al, 2005)
technologies. The Semantic Grid is often described
as an ?extension of the current Grid in which infor-
mation and services are given well-defined mean-
ing, better enabling computers and people to work
in cooperation?. Semantic Grids thus not only share
data and compute resources, but also share and pro-
cess metadata and knowledge, e.g. through the use
of RDF2 (Resource Description Framework, a meta-
data model for making statements about resources)
1Funded under the UK Economic and Social Research
Council e-Social Science programme; grant reference RES-
149-25-1027 (http://www.policygrid.org)
2http://www.w3.org/RDF/
or OWL3 (knowledge representation language for
authoring ontologies).
Numerous e-science applications rely on meta-
data descriptions of resources. But how does meta-
data come into existence? Ideally the user should
create it. However, metadata creation is a complex
task, and few users know how to create them in RDF.
To enable our users to describe their resources, we
need to provide a tool that facilitates creation, query-
ing and browsing of metadata by users with no prior
experience of such technologies.
Existing tools that provide access to RDF meta-
data are often graphical, e.g. (Handschuh et al,
2001; Catarci et al, 2004). However, we believe
that, for social scientists, natural language is the
best medium to use, as the way they conduct their
research and the structure of their documents and
data indicate that they are more oriented towards text
than graphics. Natural language approaches include
GINO (Bernstein and Kaufmann, 2006), an ontol-
ogy editor with an approach reminiscent of Natu-
ral Language Menus (Tennant et al, 1983), and us-
ing Controlled languages such as PENG-D (Schwit-
ter and Tilbrook, 2004). Such natural language ap-
proaches tend to restrict expressivity to ensure that
every entry can be parsed, limiting the language and
often making it stilted, so that there is a small learn-
ing curve before the user knows which structures are
allowed. In order to maintain full expressivity and
to shorten the learning curve, we have elected to use
WYSIWYM (What You See Is What You Meant)
(Power et al, 1998). This is a natural language gen-
eration approach where the system generates a feed-
3http://www.w3.org/TR/owl-features/
138
back text for the user that is based on a semantic rep-
resentation. This representation is edited directly by
the user by manipulating the feedback text. WYSI-
WYM has been used by a number of other projects,
such as MILE (Piwek et al, 2000) and CLEF (Hal-
lett, 2006). As evaluation results in both of these
projects were very positive (Piwek, 2002; Hallett et
al., 2007), we felt that WYSIWYM would be a suit-
able approach to use in our work.
We have developed a metadata elicitation tool that
enables users to create metadata in the shape of on-
tology instance data; the tool is driven by the on-
tologies that define those instances. We are currently
implementing a WYSIWYM tool for querying, that
uses the same interface as the metadata creation tool.
We also aim to develop a tool for presenting the
results of the query, and for browsing the descrip-
tions in the database. These three tools will be inte-
grated into one consistent interface, so that users can
switch effortlessly between querying, browsing and
editing ontology instance data. This aim is similar
to the support that the graphical tool SHAKEN pro-
vides for ontology editing and browsing (Thome?re?
et al, 2002). We want to ensure that these tools are
generic, so that if the ontologies change over time
or are replaced, the tools will still function. That
means that all domain specific information (as much
as is possible) should be contained in the ontologies.
In this paper we explore the ways in which Natu-
ral Language Generation issues influence ontology
building and vice versa.
This paper is structured as follows: section 2 de-
scribes the tool for metadata creation that we have
implemented; section 3 discusses issues in ontol-
ogy development and Natural Language Generation;
and section 4 presents an evaluation study of the
metadata creation tool. In section 5 the results of
this study are discussed and compared to those of
the CLEF project; we argue that different domains
and ontologies affect the usability and complexity
of metadata access interfaces.
2 The Metadata Creation Tool
We have developed a WYSIWYM tool that enables
users to upload resources (e.g. acadamic papers,
statistical datasets, interview transcripts) and create
metadata descriptions for them, even if these users
Figure 1: The Metadata Creation Tool.
are unfamiliar with ontologies. First, the user selects
the type of resource he is depositing (e.g. a Tran-
script). The tool then generates a brief feedback text
that presents the information specified by the user.
The feedback text contains anchors, phrases in red
boldface and blue italics that signal where new in-
formation can be added. When the user clicks on an
anchor, a menu pops up listing the kinds of infor-
mation that can be added here (see Figure 1). After
selecting a menu item, the user is prompted to enter
an appropriate value; this may be a date, a free-text
string, or another object that may or may not be in
the text already. The feedback text is regenerated
whenever the user has added some information.
The tool is driven by one or more ontologies.
Their class hierarchies are presented when users are
selecting a resource type, or creating a new object
as range for a property. The anchors correspond to
individuals in the ontology; the menu items to the
properties of those individuals. The feedback text is
divided into paragraphs which correspond to the in-
dividuals; each property of an individual is realised
as (part of) a sentence in its paragraph. Each prop-
erty in the ontology is associated with a linguistic
specification, a Dependency Tree (Mel?cuk, 1988)
that corresponds to a sentence. The specification
has slots where the source and target of the prop-
erty should be inserted, and is sufficiently detailed
to support processes such as aggregation, through
which the feedback text is made more fluent. For
a more extensive description of the metadata cre-
139
ation tool and its implementation, see Hielkema et
al. (2007b).
In August 2007 we ran a pilot evaluation study
(Hielkema et al, 2007a) on this tool. This study was
heuristic in nature, with subjects discussing the in-
terface with the experimenter while performing set
tasks. It highlighted a number of aspects which
we felt it was necessary to improve before embark-
ing on the formal evaluation. Apart from there be-
ing standard usability considerations such as a need
for better undo and help functions, it became evi-
dent that the underlying ontology was neither ex-
tensive enough nor sufficiently well-structured: sub-
jects struggled to find the options they needed, and
were often not satisfied with the options? names or
their location in the sub-menus. We therefore de-
cided that, as well as improving the basic usability
of the interface, we needed to redevelop the ontol-
ogy that was driving the interface. Users, we felt,
would find it easier to navigate the menus when
this ontology matched their mental model of the do-
main. Throughout the development of this new on-
tology, user requirements and feedback were gath-
ered through a number of focus group sessions. The
next section describes the ways in which this ontol-
ogy development was affected by the demands of the
metadata interface.
3 Ontologies in NLG
Portability has always been a major issue in NLG.
Language generation involves the use of much infor-
mation that is domain-specific, and cannot be gener-
alised without a cost in the expressivity of the result-
ing text. If we want to create an application that is
domain-independent, we have to find a way to store
all domain-specific information in a structure that is
easily extended or replaced.
We have decided to use an ontology, a com-
mon structure whose use has become widespread in
knowledge representation. Ideally, we would like to
create a generator that can be applied to any domain,
provided there is an appropriate domain-specific on-
tology. But what information should such an ontol-
ogy contain? How should it be structured? In this
section we explore issues that occur when devel-
oping or adapting ontologies for use in the WYSI-
WYM tool; we believe that this can at least in part
be generalised to NLG. The ontologies we have used
so far were developed at the same time as the WYSI-
WYM tool, so that both tool and ontology influenced
each other?s development. We are currently adapting
an ontology from another e-science project for use
in our WYSIWYM interface, to further investigate
such issues (see section 5).
There are a number of existing tools that generate
language from ontologies, using various approaches.
Wilcock (2003) describes an ontology verbaliser us-
ing XML-based generation. As Wilcock states, his
approach is domain-specific, and therefore probably
incompatible with more general ontologies (and pre-
sumably with ontologies from a different domain).
MIAKT (Bontcheva and Wills, 2004) is a sys-
tem that generates textual medical reports from an
RDF description. It uses a medical domain ontol-
ogy and an NLG lexicon that contains lexicalisa-
tions for the concepts and instances in the ontol-
ogy. In order to verbalise properties, MIAKT?s sur-
face realiser needs lexical specifications for them.
Four basic property types are distinguished whose
sub-properties can mostly be realised automatically
through the grammar rules in the realiser. This tech-
nique increases the portability of the system, but
does affect the variability and expressivity of the
generator.
We do not aim to generate from any ontology in
a domain, but to generate texts with high expressiv-
ity and clarity from ontologies that are designed in
an ?NLG-aware? way. We are investigating what re-
quirements an ontology has to meet in order to be
usable for our application, so that for any domain
an ontology can be built or adapted which we can
use to produce a usable NL-interface. As many on-
tology developers are not linguists, ideally we want
to support the adaptation to ?NLG-aware? ontologies
without requiring linguistic expertise, for instance
through a supporting software tool. Ontologies are
primarily built to model domain-specific knowledge,
making domain assumptions explicit, and to facili-
tate reasoning with this knowledge. These aims may
sometimes conflict with the requirements of NLG
applications, but they do frequently coincide (e.g.
the need for clear, unambiguous resource names.
140
3.1 Domain Ontologies for WYSIWYM
What information does our WYSIWYM application
need its ontologies to provide? First of all, the parts
of it that will be shown to the user need to be eas-
ily mapped to natural language. The purpose of
the tool is to support creation of ontology instance
data by users unfamiliar with ontologies, so the parts
they see should be comprehensible to novices. The
names of properties are used to populate the pop-
up menus, while the class names are shown in the
class hierarchy. These names are mapped to natu-
ral language by replacing capitals and underscores
with whitespace, and if necessary adding a deter-
miner. Therefore, they need to correspond to phrases
in natural language in order to be understood by the
user, with individual words separated by capitals or
underscores. If there is no intuitive NL-phrase to
represent a class, it probably does not correspond to
a concept in the domain either and might confuse
the user, so it should be removed from the hierarchy.
Classes whose instances are best presented by some
distinctive name (e.g. Person or Paper) should have
a name or title property whose value can be used
(e.g. ?John?). For other classes (e.g. Interview), the
class name can be used (e.g. ?some interview?).
We need a linguistic specification for each prop-
erty, sufficiently detailed to support aggregation and
pronominalisation, but also to produce more than
one surface form: a query is presented differently
than a description, even if it contains the same infor-
mation (compare the texts in Figure 1 and 2). The
linguistic specification should be sufficiently rich
to support the generation of these different surface
forms. For this purpose we are using Dependency
Trees, whose richness in both syntactic and semantic
information provides ample support for such trans-
formations. These trees can be associated with the
domain ontology4. This specification also contains
the header of the submenu in which the property
should appear.
Some peculiarities in natural language are
domain-independent. For instance, an address is
presented in a very specific way and cannot be re-
alised in the standard manner without sacrificing
4For an example of how this is done, see
http://www.csd.abdn.ac.uk/research/policygrid/ontologies/
Lexicon/Lexicon.owl
Figure 2: The Query Tool.
clarity (e.g. ?The address? street is Union Street.
Its place is Aberdeen?). Such ?utility? classes are
used across domains. In PolicyGrid we have cre-
ated a utility ontology that contains classes such as
?Person?, ?Address? and ?Date? 5. Instances of these
classes are generated to a special surface form. In or-
der to get the best realisation from the WYSIWYM
tool, domain ontologies should use the classes from
this utility ontology. As the properties of the utility
classes are already furnished with linguistic specifi-
cations, they are already NLG-aware. Another way
to hasten the process is to use, where possible, prop-
erties from this ontology instead of those from the
domain ontology.
3.2 WYSIWYM for Ontologies
What should the WYSIWYM application do in or-
der to provide access to ontologies? For metadata
creation it is essential that users can only produce
?correct? metadata, which does not violate the con-
straints in the ontology. The feedback text should
be presented coherently, while the Text Planner only
uses information that is either domain independent
or present in the ontology. Perhaps most impor-
tantly, the application should support easy creation
of the linguistic information that the ontology must
contain, as we cannot expect ontology developers to
have the linguistic expertise to create Dependency
Trees. We are devising a way for users to cre-
ate a specification by manipulating the surface form
of a ?template? specification. We currently have
12 templates which represent commonly used sen-
5http://www.policygrid.org/utility.owl
141
tences to present ontology properties in text. The
user can fine-tune the surface form by adding adjec-
tives, changing morphological information and the
root of individual words; actions for which only a
basic linguistic knowledge is needed. This approach
is outlined in more detail in (Hielkema et al, 2007b).
The main challenge with this approach is that the
specification is used to generate two surface forms;
it remains to be seen whether a specification that is
fine-tuned through one surface form will accommo-
date the accurate generation of another.
The Penman Upper Model (Bateman, 1990) sup-
ports the specification of linguistic information
through a different approach. The Upper Model is a
domain-independent ontology that supports sophis-
ticated NLP. To make a domain ontology available
for NLP, its resources have to be placed in the hier-
archy of the Upper Model; their place there deter-
mines their surface realisation. This task appears to
require considerable linguistic expertise, but like the
creation of our Dependency Trees could probably be
made easier for non-linguists through some special-
purpose interface.
4 Usability Evaluation
The best evaluation of our tool would be to let users
deposit their resources in real-life contexts, but our
tool is not ready for a full deployment. Another way
would be to compare its usability to another meta-
data creation tool in an experiment where users com-
pleted the same tasks with both tools. Unfortunately,
most metadata tools focus on providing support for
ontology editing (e.g. Prote?ge?6 or GINO (Bern-
stein and Kaufmann, 2006)), or query formulation
(e.g. SEWASIE (Catarci et al, 2004)). A number
of tools for metadata creation use formal (RDF) or
controlled languages, which are difficult to use for
those wholly unfamiliar with formal logic. Other
tools were developed for one specific purpose, e.g.
CREAM (Handschuh et al, 2001) which was de-
veloped for the annotation of web pages, and could
not easily be adapted to our purposes. We were not
aware of any tool that we could adapt to the e-social
science ontologies and thus use in an experiment.
Alternatively, we could have compared our inter-
face to direct authoring of RDF; but in an environ-
6http://protege.stanford.edu/
ment where most users have no experience of on-
tologies or metadata this seemed spurious. Instead,
we adopted an approach similar to that used in the
CLEF project (Hallett et al, 2007). They evaluated
their WYSIWYM system (which enabled users to
create SQL queries for a database in a medical do-
main) by measuring the performance of fifteen sub-
jects on four consecutive tasks, after a brief intro-
duction. These subjects were all knowledgeable in
the domain, and all but two knew the representation
language of the repository and how the data con-
tained in it was structured. These subjects achieved
perfect results from the second task onwards, and
became faster with each task, especially after the
first. We also expected users to become faster and
more accurate with each completed task, and indeed
hoped for perfect scores on their last task.
Subjects Sixteen researchers and PhD students
from various social science-related disciplines par-
ticipated in the experiment. None of them had prior
experience of the metadata elicitation interface, and
only two of the subjects had any previous experi-
ence of using ontologies. The ontology driving the
system models the description of social science re-
sources and was based on requirements gathering
sessions, in which a few subjects had participated.
None of the subjects knew its precise structure.
Methodology After providing some information
about their background, subjects viewed a video in-
troduction7 of six minutes. This video showed the
construction of a simple resource description, high-
lighting the main functionalities of the interface,
while a voice-over explained what was happening
on the screen.
Subjects were then handed four short resource de-
scriptions expressed as paragraphs of English (see
?Materials?) and asked to reproduce these descrip-
tions as closely as possible using the tool. To avoid
making the choice of the correct options too obvi-
ous, we tried to avoid phrases that corresponded lit-
erally to those in the menus. Each subject received
the descriptions in a different order, in case there
were differences in the complexity of the tasks. Sub-
jects were allowed as much time as they needed to
7This video can be viewed at http://www.csd.abdn.ac.uk/ re-
search/policygrid/demos/WysiwymIntroduction1.mov
142
Task order Completion time Operations Total errors Avoidable errors
? ? ? ? ? ? ? ?
First 512.81 366.132 48.38 24.527 3.31 1.922 1.56 .727
Second 329.50 95.716 37.75 12.228 2.69 2.442 1.38 .957
Third 260.06 90.542 35.13 9.749 2.75 2.720 1.63 1.310
Fourth 309.81 106.049 39.38 10.844 2.00 1.966 1.44 1.504
Table 1: Mean completion times, operations and errors per completed task.
complete each task.
For each task, the tool recorded the completion
time, the produced description, the number of op-
erations used to produce it, and the frequency with
which various operation types were used, such as
?undo? or the ?help? functions. After the subjects
had completed all four tasks, they were asked to rate
the usability (very difficult - difficult - OK - easy -
very easy) and usefulness (useless - not much use
- adequate - useful - very useful) of the tool on a
five-point Likert scale, and to note any feedback they
might have. The entire experiment took on average
50 min. per subject.
Materials We used four resource descriptions,
one of which was:
You are depositing the transcript of an
interview that was held by Dr. Rivers
in 1907, at Eddystone. The interview
mainly discussed ?male-female relation-
ships?, ?burial practices? and ?the social
impact of the interdiction on head hunt-
ing?. Access to this transcript should re-
main private.
Figure 1 shows the corresponding description that
could be produced with the tool. The separation of
the transcript from the interview is an example of the
clear distinctions necessary for knowledge represen-
tation. In natural language, this distinction would
not necessarily be made, and indeed this step was
missed by a number of users.
To ensure that tasks did not repeat identical sub-
tasks, we tried to use different parts of the ontol-
ogy in each task. Every task described a differ-
ent resource type (conference paper, transcript, aca-
demic paper, report), which corresponded to a dif-
ferent class in the ontology. We were also careful to
choose varying menu items (corresponding to prop-
erties in the ontology), although some repetition was
unavoidable (e.g. specifying names). In fact, a real-
life use of the tool would involve rather more task
repetition (specifying titles, authors and dates would
be necessary for practically any resource) than the
artificial descriptions in this study.
Results To analyse the accuracy of the produced
descriptions, we divided each description task into 8
to 10 subtasks. For the task shown in the previous
paragraph, these subtasks were:
? Specify that you are depositing a ?Transcript?
? Specify that access is private
? Specify that it is a transcript of an ?Interview?
(creating an ?interview? object)
? Specify the three main topics
? Add an interviewer (creating a ?Person?
object)
? Call this person ?Dr. Rivers?
? Specify the location of the interview
? Specify the date of the interview
As some subtasks are more complicated than oth-
ers and take longer, we did not try to give each task
exactly the same number of subtasks, but instead en-
sured that all tasks needed the same number of op-
erations (e.g. menu item selections, button clicks,
etc.) in order to be completed. Each subtask that was
missing or completed differently than in the descrip-
tion shown in ?Materials? was counted as one er-
ror. Erroneous ways to complete subtasks included
choosing a different menu item and adding informa-
tion to the wrong object. For instance, a number of
subjects, instead of specifying an interviewer for the
interview, added a creator for the transcript; this was
143
counted as one erroneously completed subtask, and
therefore one error.
The list of subtasks above shows that some sub-
tasks depend on the successful completion of other
tasks; for instance, you cannot add an interviewer
unless you have created an ?interview? object. We
therefore analysed two error counts: the total num-
ber of errors, and the ?avoidable? errors. The ?avoid-
able? errors were the total number of errors minus
those subtasks that depended on another subtask that
was missing or had been completed incorrectly.
We analysed the mean completion times, number
of operations used and the two error counts of the
tasks that were completed first, second, third and
last, using a repeated measures ANOVA (see Table 1
for the means and standard deviations). Mean com-
pletion times went down significantly (Huynh-Feldt
p-value < 0.01). Tukey?s HSD post-hoc (applied to
a univariate ANOVA, with task order as the indepen-
dent variable) test shows that both the third (p-value
< 0.01) and the fourth (p-value 0.030) were com-
pleted significantly faster than the first task. How-
ever, no significant differences were found for the
number of operations (Huynh-Feldt p-value 0.062),
the total number of errors (Huynh-Feldt p-value
.322) or the number of avoidable errors (Huynh-
Feldt p-value .931).
Subject feedback on the tool was positive: it was
perceived as useful (? 3.94; 1=?useless?, 5=?very
useful?), and OK or easy to use (? 2.69; 1=?very
easy?, 5=?very difficult?). Five subjects expressed a
preference for a form-based interface, and five oth-
ers for a NL-interface such as the one tested. In
feedback, subjects indicated a desire for more form-
based elements in the interface, to speed up the
creation of the standard description elements (e.g.
name/title, author), and complained that the envi-
ronment was initially unfamiliar, with some menu
items overlapping. This unfamiliarity meant that
items that were necessary to complete the descrip-
tion were often overlooked; subjects often solved
this by choosing the closest approximation they
could find, e.g. ?creator? instead of ?interviewer?.
5 Discussion and Future Work
Although users quickly gained speed using the tool,
and were positive in their feedback, the evaluation
results are not nearly as positive as those found for
CLEF (see section 4). The mean number of errors
decreased, but this effect was not significant and
only five out of sixteen subjects received a perfect
score on the last task (four other subjects performed
some earlier task(s) perfectly). Evidently there is
a difference in usability of both tools - but what
causes it? No doubt the difference can partly be as-
cribed to differences in the implementation of the
interface. However, the most common feedback we
received from the subjects was that they were over-
whelmed by the large number of options available to
them. Each class in the social science ontology has
on average 30 properties, which means a descrip-
tion with three objects provides 90 options. In con-
trast, the number of available options in the CLEF
system was deliberately kept small (max. three) for
?non-terminal anchors?. Especially in the first task,
users had trouble finding the option they wanted,
and although it became easier in the later tasks as
they familiarised themselves with the system, the
results indicate that it remained a problem. This
was likely aggravated by our deliberate avoidance
of subtask-repetition; more standard descriptions,
which always involve titles and authors, might have
produced a greater learning effect. CLEF was de-
veloped for a medical domain, which is well defined
and understood by the experimental subjects. The
social science domain encompasses many different
theories and concepts, not just about what subjects
are investigated, but also about how the research
should be conducted. PolicyGrid has tried to de-
velop an ontology that the different disciplines in
social science could be satisfied with. As a result,
it is quite large and complex, and most users will
only recognise parts of it. Thus the number of avail-
able options in the tool driven by this ontology is
large, and users have to explore the ontology and
learn to navigate it where their domain knowledge
does not suffice. This flattens the learning curve and
decreases the usability of the tool.
Half the users preferred a form-based interface
over an NLG interface. Although forms are an eas-
ily understood mechanism which are just as familiar
to users as natural language, we have three reasons
for preferring the WYSIWYM approach. First, the
large number of options in the ontology means that
a form would reach truly daunting proportions. Sec-
144
ond, we want our resource descriptions to be con-
nected through shared people, projects, institutions,
etc; using the expressivity that RDF offers us. This
would be more difficult to achieve in an interface
where the user completes a form by providing each
property with a free-text description. Thirdly, forms
can be confusing for the user as well; the brief de-
scriptions provided for each element are frequently
ambiguous and therefore misunderstood. An NLG-
interface, which provides feedback by presenting the
property in a complete sentence, should help to clar-
ify the meaning of the property name for the user.
As we discussed earlier, there are many con-
straints on the development of domain ontologies
that can be accessed through NLG, and the evalua-
tion indicates that the structure of the ontology is es-
sential for the tool?s usability. Still, the evaluation is
sufficiently positive that we believe the WYSIWYM
approach suitable for providing access to ontologies,
especially for users who are unfamiliar with ontolo-
gies or their graphical representations. Navigation
could be made easier by providing users with an
overview of the underlying ontology, possibly pre-
sented as an index of objects, and the information
that can be specified about each object. An online
manual with some worked examples and screenshots
might also help users get started on the more obvi-
ous parts of a description. We are currently attempt-
ing to adapt an ontology developed in another UK e-
science project8 for use in theWYSIWYMmetadata
elication tool. Instead of assuming the depositing of
a resource, this ontology was developed to capture
user-elicited metadata for video annotation. Part of
this metadata is captured automatically, part of it is
elicited from the user. We hope that the adaptation
of an ontology that was originally developed for a
different purpose for use in an NLG application will
highlight other issues involved in the use of ontolo-
gies in NLG.
One way in which subjects did tasks erroneously
was by using the ?hasComment? property when they
could not find the option they wanted. This is not
precisely wrong: the metadata it produces is correct
and any human readers will understand the descrip-
tion. But it is not the best description for query-
ing purposes. We think some subjects may have
8http://www.ncess.ac.uk/research/digital records/
had trouble grasping the exact purpose of the pro-
duced descriptions. We hope that users who have
used the query tool to find (the descriptions of) re-
sources, will have a better understanding of what an
effective metadata description is.
We intend to run more evaluation experiments, to
assess the usability but also the usefulness of the
combined toolset. Rather than asking subjects to
copy descriptions or queries, we may ask them to
find a particular resource, or to try to deposit and de-
scribe one of their own papers. If possible, it would
also be interesting to see how they perform the same
tasks using a different interface for metadata ac-
cess, e.g. a graphical interface such as SHAKEN
(Thome?re? et al, 2002).
6 Conclusion
We have presented a WYSIWYM interface for the
creation of RDF metadata, which will be extended
by the addition of querying and browsing tools.
This tool is driven by an ontology that contains all
domain-specific information needed to present it in
natural language. We have highlighted a number of
issues in ontology development for access through
NLG. We have evaluated the tool?s usability through
an experiment with potential users. The results were
encouraging, but indicate that the structure and fa-
miliarity of the underlying ontology strongly influ-
ence the usability of the interface.
Acknowledgments
Many thanks to Catalina Hallett and Richard Power
from the CLEF project, for their help in comparing
the two tools and their different evaluation results.
References
J.A. Bateman. 1990. Upper Modelling: A General Or-
ganisation of Knowledge for Natural Language Pro-
cessing. In Proceedings of the International Language
Generation Workshop, Pittsburgh, USA.
A. Bernstein and E. Kaufmann. 2006. GINO - A Guided
Input Natural Language Ontology Editor. In Inter-
national Semantic Web Conference 2006, pages 144?
157.
K. Bontcheva and Y. Wills. 2004. Automatic Re-
port Generation from Ontologies: the MIAKT ap-
proach. In Nineth International Conference on Appli-
145
cations of Natural Language to Information Systems
(NLDB?2004), Manchester, UK.
T. Catarci, P. Dongilli, T. Di Mascio, E. Franconi, G. San-
tucci, and S. Tessaris. 2004. An Ontology-based Vi-
sual Tool for Query Formulation Support. In Proceed-
ings of the Sixteenth European Conference on Artifi-
cial Intelligence (ECAI 2004).
D. De Roure, N.R. Jennings, and N.R. Shadbolt. 2005.
The Semantic Grid: Past, Present and Future. In Pro-
ceedings of the IEEE 93(3), pages 669?681.
C. Hallett, D. Scott, and R. Power. 2007. Composing
Questions through Conceptual Authoring. Computa-
tional Linguistics, 33(1):105?133.
C. Hallett. 2006. Generic Querying of Relational
Databases using Natural Language Generation Tech-
niques. In Proceedings of the Fourth International
Natural Language Generation Conference, pages 88?
95, Nottingham, UK.
S. Handschuh, S. Staab, and A. Maedche. 2001.
CREAM: creating relational metadata with a
component-based, ontology-driven annotation
framework. In K-CAP ?01: Proceedings of the 1st
international conference on Knowledge capture, pages
76?83, New York, NY, USA. ACM Press.
F. Hielkema, P. Edwards, C. Mellish, and J. Farrington.
2007a. A Flexible Interface to Community-Driven
Metadata. In Proceedings of the Third International
Conference on eSocial Science.
F. Hielkema, C. Mellish, and P. Edwards. 2007b. Using
WYSIWYM to Create an Open-ended Interface for the
Semantic Grid. In S. Busemann, editor, Proceedings
of the 11th European Workshop on Natural Language
Generation.
I.A. Mel?cuk. 1988. Dependency Syntax: Theory and
Practice. State University of New York.
P. Piwek, R. Evans, L. Cahil, and N. Tipper. 2000. Natu-
ral Language Generation in the MILE System. In Pro-
ceedings of IMPACTS in NLG workshop, pages 33?42,
Schloss Dagstuhl, Germany.
P. Piwek. 2002. Requirements Definition, Validation,
Verification and Evaluation of the CLIME Interface
and Language Processing Technology. Technical Re-
port ITRI-02-03, ITRI, University of Brighton.
R. Power, D. Scott, and R. Evans. 1998. What You See Is
What YouMeant: Direct Knowledge Editing with Nat-
ural Language Feedback. In Proceedings of the Thir-
teenth European Conference on Artificial Intelligence,
Brighton, UK.
R. Schwitter and M. Tilbrook. 2004. Controlled Natu-
ral Language meets the Semantic Web. In Proceed-
ings of the Australasian Language Technology Work-
shop 2004.
H.R. Tennant, K.M. Ross, R.M. Saenz, C.W.Thompson,
and J.R. Miller. 1983. Menu-based Natural Language
Understanding. In Proceedings of the Twenty-first An-
nual Meetings on Association for Computational Lin-
guistics, pages 151?158, Cambridge, Massachusetts.
J. Thome?re?, K. Barker, V. Chaudhri, P. Clark, M. Erik-
sen, S. Mishra, B. Porter, and A. Rodriguez. 2002.
A Web-based Ontology Browsing and Editing System.
In Eighteenth National Conference on Artificial Intelli-
gence, pages 927?934, Menlo Park, CA, USA. Amer-
ican Association for Artificial Intelligence.
G. Wilcock. 2003. Talking OWLs: Towards an Ontol-
ogy Verbalizer. In Human Language Technology for
the Semantic Web and Web Services (ISWC?03), pages
109?112, Sanibel Island, Florida.
146
Towards a Programmable Instrumented Generator 
 
Chris Mellish 
Computing Science 
University of Aberdeen 
AB24 3UE, UK 
c.mellish@abdn.ac.uk 
 
 
 
 
 
 
Abstract 
In this paper, we propose a general way of con-
structing an NLG system that permits the system-
atic exploration of the effects of particular system 
choices on output quality. We call a system devel-
oped according to this model a Programmable In-
strumented Generator (PIG). Although a PIG could 
be designed and implemented from scratch, it is 
likely that researchers would also want to create 
PIGs based on existing systems. We therefore pro-
pose an approach to ?instrumenting? an NLG sys-
tem so as to make it PIG-like. To experiment with 
the idea, we have produced code to support the 
?instrumenting? of any NLG system written in 
Java. We report on initial experiments with ?in-
strumenting? two existing systems and attempting 
to ?tune? them to produce text satisfying complex 
stylistic constraints. 
1 Introduction 
Existing NLG systems are often fairly impenetra-
ble pieces of code. It is hard to see what an NLG 
system is doing and usually impossible to drive it 
in any way other than what was originally envis-
aged. This is particularly unfortunate if the system 
is supposed to produce text satisfying complex sty-
listic requirements. Even when an NLG system 
actually performs very well, it is hard to see why 
this is or how particular generator decisions pro-
duce the overall effects. We propose a way of 
building systems that will permit more systematic 
exploration of decisions and their consequences, as 
well as better exploitation of machine learning to 
make these decisions better. We call a system built 
in this way a Programmable Instrumented Genera-
tor (PIG). As an initial exploration of the PIG idea, 
we have developed a general way of partially in-
strumenting any NLG system written in Java and 
have carried out two short experiments with exist-
ing NLG systems. 
2 Controlling an NLG System: Examples 
NLG systems are frequently required to produce 
output that conforms to particular stylistic guide-
lines. Often conformance can only be tested at the 
end of the NLG pipeline, when a whole number of 
complex strategic and tactical decisions have been 
made, resulting in a complete text. A number of 
recent pieces of work have begun to address the 
question of how to tune systems in order to make 
the decisions that lead to the most stylistically pre-
ferred outputs. 
 
Paiva and Evans (2005) (henceforth PE) investi-
gate controlling generator decisions for achieving 
stylistic goals, e.g. choices between: 
 
The patient takes the two gram dose of the pa-
tient?s medicine twice a day. 
 
and 
 
The dose of the patient?s medicine is taken 
twice a day. It is two grams. 
 
In this case, a stylistic goal of the system is ex-
pressed as goal values for features SSi,  where each 
SSi  expresses something that can be measured in 
the output text, e.g. counting the number of pro-
nouns or passives. The system learns to control the 
number of times specific binary generator deci-
Figure 1: Example PERSONAGE rule 
sions are made (GDj), where these decisions in-
volve things like whether to split the input into 2 
sentences or whether to generate an N PP clause. A 
process of offline training is first used to establish 
correspondences between counts of generator deci-
sions and the values of the stylistic features. This 
works by running the system with multiple outputs 
(making decisions in many possible ways) and 
keeping track of both the counts of the decisions 
and also the values of the stylistic features 
achieved. From this data the system then learns 
correlations between these: 
 
 
 
 
To actually generate a text given stylistic goals SSi, 
the system then uses an online control regime. At 
each choice point, it considers making GDj versus 
not GDj. For each of these two, it estimates all the 
SSi that will be obtained for the complete text, us-
ing the learned equations. It prefers the choice that 
minimises the sum of absolute differences between 
these and the goal SSi, but is prepared to backtrack 
if necessary (best-first search). 
 
Mairesse and Walker (2008) (henceforth MW) use 
a different method for tuning their NLG system 
(?PERSONAGE?), whose objective is to produce 
texts in the styles of writers with different person-
ality types. In this case, the system performance 
depends on 67 parameters, e.g. REPETITIONS 
(whether to repeat existing propositions), PERIOD 
(leave two sentences connected just with ?.?, rather 
than any other connective) and NEGATION (ne-
gate a verb and replace it by its antonym). For 
MW, offline training involves having the program 
generate a set of outputs with random values for all 
the parameters. Human judges estimate values for 
the ?big five? personality traits (e.g. extroversion, 
neuroticism) for each output. Machine learning is 
then used to generate rules to predict how the pa-
rameter values depend on the big five numbers. 
For instance, Figure 1 shows the rule predicting the 
STUTTERING parameter. 
 
Once these rules are learned, online control to pro-
duce text according to a given personality (speci-
fied by numerical values for the big five traits) 
uses the learned models to set the parameters, 
which then determine NLG system behaviour. 
Human judges indeed recognise these personalities 
in the texts. 
3 Towards a PIG 
Looking at the previous two examples, one can 
detect some common features which could be used 
in other situations: 
? An NLG system able to generate random (or 
all possible) outputs 
? Outputs which can be evaluated (by human or 
machine) 
? The logging of key NLG parameters/choices 
? Learning of connections between parameters 
and output quality 
 
j
j
j
est
ii GDxxSSSS .0 ?+=?
This then being used to drive the system to achieve 
specific goals more efficiently than before. 
 
PE and MW both constructed special NLG systems 
for their work. One reason for this was that both 
wanted to ensure that the underlying NLG system 
allowed the kinds of stylistic variation that would 
be relevant for their applications. But also, in order 
to be able to track the choices made by a generator, 
Paiva and Evans had to implement a new system 
that kept an explicit record of choices made. This 
new system also had to be able to organise the 
search through choices according to a best-first 
search (it was possibly the first NLG system to be 
driven in this way). The only possibility for them 
was to implement a new special purpose generator 
for their domain with the desired control character-
istics.  
 
NLG systems are not usually immediately suitable 
for tuning of this kind because they make choices 
that are not exposed for external inspection. Also 
the way in which choices are made and the overall 
search strategy is usually hardwired in a way that 
prevents easy changing. It seems plausible that the 
approaches of PE and MW would work to some 
extent for any NLG system that can tell you about 
its choices/ parameter settings, and for any stylistic 
goal whose success can be measured in the text. 
Morever, these two are not the only ways one 
might train/guide an NLG system from such in-
formation (for instance, Hovy?s (1990) notion of 
?monitoring? would be an alternative way of using 
learned rules to drive the choices of an NLG sys-
tem). It would be revealing if one could easily 
compare different control regimes in a single ap-
plication (e.g. monitoring for PE?s task or best-first 
search for MW?s), but this is currently difficult 
because the different systems already have particu-
lar control built in. 
 
This discussion motivates the idea of developing a 
general methodology for the development of NLG 
systems that permits the systematic exploration of 
learning and control possibilities. We call a system 
built in such a way a Programmable Instrumented 
Generator (PIG).1  A PIG would be an NLG sys-
                                                          
1
 If one had a sufficiently expressive PIG then perhaps one 
could train it for any testable stylistic goals ? a kind of ?uni-
versal? NLG system? 
tem that implements standard NLG algorithms and 
competences but which is organised in a way that 
permits inspection and reuse. It would be instru-
mented, in that one would be able to track the 
choices made in generating a text or texts, in order 
to tune the performance. It would also be pro-
grammable in that it would be possible to drive the 
system in different ways according to a learned (or 
otherwise determined) ?policy?, e.g. to: 
 
? Generate all solutions (overgeneration) 
? Generate solutions with some choices 
fixed/constrained 
? Generate solutions with user control of some 
decisions 
? Generate solutions using an in-built choice 
mechanism 
? Generate solutions according to some global 
search strategy (e.g. monitoring, best-first 
search) 
4 Using a PIG 
A general way of using a PIG is shown in Figure 2. 
A PIG interacts with a (conceptually) separate 
processing component, which we call the ?oracle?. 
This applies a policy to make choices for the gen-
erator and receives evaluations of generated texts. 
It logs the choices made and (using machine learn-
ing) can use this information to influence the pol-
icy.  
 
There are two main modes in which the PIG can be 
run, though mixtures are also possible. In (offline) 
training mode, the system is run on multiple inputs 
and uses random or exhaustive search to sample 
the space of generatable texts. The choices made 
Figure 2: Using a PIG 
are logged, as is the quality of the outputs gener-
ated. In (online) execution mode, the PIG is run as 
a normal generator, running on a single input and 
making choices according to a learned policy. 
 
To support this, the PIG itself needs minimally to 
support provide external access to the following 
function: 
 
generate(input:InputSpec) returns text:String 
 
which produces a text, from a given input specifi-
cation. On the other hand, the Oracle needs to pro-
vide external access to at least the following (used 
by the PIG): 
 
choice(question:String, suggestion:int,  
       possibilities:ListOfString, state:String) 
returns decision:int  or RESTART 
 
outcome(state:String, value:Float) (no return value)  
 
where question represents a choice to be made 
(with possible answers possibilities), suggestion 
is the index of a suggested choice and decision is 
the index of the choice made. state is a representa-
tion of generator state, in some standard format 
(e.g. ARFF (Hall et al2009)) and outcome (giving 
the final state and the text quality) is called as the 
last action of generating a text. RESTART is a 
special value that by convention causes the system 
to return to a state where it can be asked to gener-
ate another text. 
 
To support the above, the PIG needs to maintain 
some representation of program state. Also the ora-
cle needs to implement a training/testing algorithm 
that involves providing the PIG with example in-
puts, restarting the PIG on the current or a new 
example, implementing a policy, logging results 
and possibly interacting with a user. 
 
The above model of how to use a PIG is partly mo-
tivated by existing approaches to monitoring and 
testing complex electronic equipment. Testing is 
often carried out by attaching ?automatic test 
equipment? to the unit under test. This automatic 
test equipment is akin to our ?oracle? in that it 
drives the unit through special test sequences and 
automatically records what is going on. 
 
 
5 The PIG panel 
There is a practical question of how best to build 
PIGs and what resources there might be to support 
this. Given their concern with explicit representa-
tion of choices, NLG models based on Systemic 
Grammar (Bateman 1997) might well be promising 
as a general framework here. But in reality, NLG 
systems are built using many different theoretical 
approaches, and most decisions are hard-coded in a 
conventional programming language. In order to 
investigate the PIG concept further, therefore, we 
have developed a general way of ?instrumenting? 
in a limited way any NLG system written in Java 
(giving rise to a PIGlet). We have also imple-
mented a general enough oracle for some initial 
experiments to be made with a couple of PIGlets. 
This experimental work is in line with the API 
given above but implemented in a way specific to 
the Java language.  
 
In order to instrument the client generator, one has 
to identify places where interesting choices are 
made. This is obviously best done by someone 
with knowledge of the system. There are a number 
of ways to do this, but the simplest basically re-
places a construct of the form: 
 
if (<condition>) <action> 
 
by 
 
if (Oracle.condRec(<name>,<condition>)) <action> 
 
where <name> is a string naming this particular 
choice. This allows the oracle to intervene when 
the choice is made, but possibly taking into ac-
count the suggested answer (<condition>). 
 
The implemented oracle (the ?PIG panel?) sup-
ports a kind of ?single stepping? of the generator 
(between successive choices), manual control of 
choices and restarting. It has built in policies which 
include random generation, following the choices 
suggested by the PIGlet, systematic generation of 
all possibilities (depth-first) and SARSA, a kind of 
reinforcement learning (Sutton and Barto 1998). It 
provides simple statistics about the evaluations of 
the texts generated using the current policy and a 
user interface (Figure 3). 
 For the oracle to be able to control the PIGlet, it 
needs to be provided with a ?connector? which 
represents it through a standard API (specifying 
how to generate a text, how to evaluate a text, what 
examples can be used, etc.). This also includes a 
specification of how to derive the ?state? informa-
tion about the generator which is logged for ma-
chine learning process. State information can 
include the number of times particular choices are 
made (as in PE), the most recent choices made and 
other generator-specific parameters which are 
communicated to the oracle (as in MW). 
 
Finally the PIGlet and oracle are linked via a ?har-
ness? which specifies the basic mode of operation 
(essentially training vs execution). 
 
In the following sections, we describe two tentative 
experiments which produced PIGlets from existing 
NLG systems and investigated the use of the PIG 
panel to support training of the system. It is impor-
tant to note that for these systems the instrument-
ing was done by someone (the author) with limited 
knowledge of the underlying NLG system and with 
a notion of text quality different from that used by 
the original system. Also, in both cases the limited 
availability of example data meant that testing had 
to be performed on the training data (and so any 
positive results may be partly due to overfitting). 
 
6 Experiment 1: Matching human texts 
For this experiment, we took an NLG system that 
produces pollen forecasts and was written by Ross 
Turner (Turner et al2006). Turner collected 68 
examples of pollen prediction data for Scotland 
(each consisting of 6 small integers and a charac-
terisation of the previous trend) with human-
written forecasts, which we took as both our train-
ing and test data. We evaluated text quality by 
similarity to the human text, as measured by the 
Meteor metric (Lavie and Denkowski 2009). Note 
that the human forecasters had access to more 
background knowledge than the system, and so this 
is not a task that the system would be expected to 
do particularly well on. 
 
The notion of program ?state? that the oracle 
logged took the form of the 6 input values, together 
with the values of 7 choices made by the system 
(relating to the inclusion of trend information, 
thresholds for the words ?high? and ?low?, 
whether to segment the data and whether to include 
hay fever information).  
 
The system was trained by generating about 10000 
random texts (making random decisions for ran-
domly selected examples). For each, the numerical 
outcome (Meteor score) and state information was 
recorded. The half of the resulting data with high-
est outcomes was extracted and used to predict 
rules for the 7 choices, given the 6 input parame-
ters (we used Weka (Hall et al2009) with the JRip 
algorithm). The resulting rules were transcribed 
into a specific ?policy? (Java class) for the oracle. 
 
Applied to the 68 examples, trying random genera-
tion for 3 times on each, the system obtained an 
average Meteor score of 0.265. Following the 
original system?s suggestions produced an average 
score of 0.279. Following the learned policy, the 
system also obtained an average of 0.279. The dif-
ference between the learned behaviour and random 
generation is significant (p =0.002) according to a t 
test. 
7 Experiment 2: Text length control 
A challenging stylistic requirement for NLG is that 
of producing a text satisfying precise length re-
quirements (Reiter 2000). For this experiment, we 
took the EleonPlus NLG system developed by 
Hien Nguyen. This combines the existing Eleon 
user interface for domain authoring (Bilidas et al
2007) with a new NLG system that incorporates 
the SimpleNLG realiser (Gatt and Reiter 2009). 
Figure 3: PIG Panel interface 
The system was used for a simple domain of texts 
about university buildings. The data used was the 
authored information about 7 university buildings 
and associated objects. We evaluated texts using a 
simple (character) length criterion, where the ideal 
text was 250 characters, with a steeply increasing 
penalty for texts longer than this and a slowly in-
creasing penalty for texts that are shorter. 
 
The notion of ?state? that was logged took account 
of the depth of the traversal of the domain data, the 
maximum number of facts per sentence and an ag-
gregation decision. 
 
Following the previous successful demonstration 
of reinforcement learning for NLG decisions (Rie-
ser and Lemon 2006), we decided to use the 
SARSA approach (though without function ap-
proximation) for the training. This involves re-
warding individual states for their (direct or 
indirect) influence on outcome quality as the sys-
tem actually performs. The policy is a mixture of 
random exploration and the choosing of the cur-
rently most promising states, according to the 
value of a numerical parameter ?. 
 
Running the system on the 7 examples with 3 ran-
dom generations for each produced an average text 
quality of -2514. We tried a SARSA training re-
gime with 3000 random examples at ?=0.1, fol-
lowed by 2000 random examples at ?=0.001. 
Following this, we looked at performance on the 7 
examples with ?=0. The average text quality was -
149. This was exactly the same quality as that 
achieved by following the original NLG system?s 
policy. Even though there is a large difference in 
average quality between random generation and 
the learned policy, this is, however, not statistically 
significant (p = 0.12) because of the small number 
of examples and large variation between text quali-
ties. 
8 Conclusions and Further Work 
Each of our initial experiments was carried out by 
a single person in less than a week of work, (which 
included some concurrent development of the PIG 
panel software and some initial exploration of the 
underlying NLG system). This shows that it is rela-
tively quick (even with limited knowledge of the 
original NLG system) for someone to instrument 
an existing NLG system and to begin to investigate 
ways of optimizing its performance (perhaps with 
different goals than it was originally built for). 
This result is probably more important than the 
particular results achieved (though it is promising 
that some are statistically significant).  
 
Further work on the general software could focus 
on the issue of the visualization of choices. Here it 
might be interesting to impose a Systemic network 
description on the interdependencies between 
choices, even when the underlying system is built 
with quite a different methodology. 
 
More important, however, is to develop a better 
understanding of what sorts of behaviour in an 
NLG system can be exposed to machine learning 
to optimize the satisfaction of what kinds of stylis-
tic goals. Also we need to develop methodologies 
for systematically exploring the possibilities, in 
terms of the characterization of NLG system state 
and the types of learning that are attempted. It is to 
be hoped that software of the kind we have devel-
oped here will help to make these tasks easier. 
 
Finally, this paper has described the development 
and use of PIGs mainly from the point of view of 
making the best of NLG systems rather like what 
we already have. The separation of logic and con-
trol supported by the PIG architecture could 
change the way we think about NLG systems in 
the first place. For instance, a PIG could easily be 
made to overgenerate (in the manner, for instance, 
of HALOGEN (Langkilde-Geary 2003)), in the 
confidence that an oracle could later be devised 
that appropriately weeded out non-productive 
paths. 
Acknowledgments 
This work was supported by EPSRC grant 
EP/E011764/1. The ideas here have benefited par-
ticularly from discussions with Graeme Ritchie and 
Roger Evans. We also acknowledge the helpful 
comments of two anonymous reviewers. 
References  
John Bateman. 1997. Enabling technology for multilin-
gual natural language generation: the KPML devel-
opment environment. Natural Language Engineering 
3(1):15-55. 
Dimitris Bilidas,   MariaTheologou and Vangelis 
Karkaletsis. 2007.  Enriching OWL Ontologies with 
Linguistic and User-Related Annotations: The 
ELEON System. Proceedings of the IEEE Interna-
tional Conference on Tools with Artificial Intelli-
gence (ICTAI), Patra, Greece. 
 
Albert Gatt and Ehud Reiter. 2009. SimpleNLG: A re-
alisation engine for practical applications. Proceed-
ings of ENLG-2009. 
 
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard 
Pfahringer, Peter Reutemann and Ian H. Witten. 
2009. The WEKA Data Mining Software: An Up-
date; SIGKDD Explorations, Volume 11, Issue 1. 
Eduard H. Hovy. 1990. Pragmatics and Natural Lan-
guage Generation. Artificial Intelligence 43(2), pp. 
153?198. 
Alon Lavie and Michael Denkowski. 2009. The 
METEOR Metric for Automatic Evaluation of Ma-
chine Translation. Machine Translation, published 
online 1st November 2009. 
Irene Langkilde-Geary. 2003. A foundation for general-
purpose natural language generation: sentence reali-
zation using probabilistic  models of language. PhD 
thesis,  University of Southern California, Los Ange-
les, USA. 
Fran?ois Mairesse and Marilyn Walker. 2008. Trainable 
Generation of Big-Five Personality Styles through 
Data-driven Parameter Estimation. In Proceedings of 
the 46th Annual Meeting of the Association for Com-
putational Linguistics (ACL), Columbus.  
 
Daniel Paiva and Roger Evans. 2005. Empirically-based   
    control of natural language generation. Proceedings  
    of the 43rd Annual Meeting of the ACL, pages 58-65. 
 
Ehud Reiter. 2000. Pipelines and Size Constraints. Com-
putational Linguistics. 26:251-259. 
 
Verena Rieser and Oliver Lemon. 2006. Using Machine 
Learning to Explore Human Multimodal Clarification 
Strategies. Procs of  ACL 2006. 
Richard S. Sutton and Andrew G. Barto. 1998. Rein-
forcement Learning: An Introduction. MIT Press, 
Cambridge, MA. 
 
Ross Turner, Yaji Sripada, Ehud Reiter and Ian Davy. 
2006. Generating Spatio-Temporal Descriptions in 
Pollen Forecasts. Proceedings of EACL06 Compan-
ion Volume. 
 
 
Using Semantic Web Technology to Support NLG
Case study: OWL finds RAGS
Chris Mellish
Computing Science
University of Aberdeen, Aberdeen AB24 3UE, UK
c.mellish@abdn.ac.uk
Abstract
The semantic web is a general vision
for supporting knowledge-based process-
ing across the WWW and its successors.
As such, semantic web technology has po-
tential to support the exchange and pro-
cessing of complex NLG data. This pa-
per discusses one particular approach to
data sharing and exchange that was de-
veloped for NLG ? the RAGS framework.
This was developed independently of the
semantic web. RAGS was relatively com-
plex and involved a number of idiosyn-
cratic features. However, we present a ra-
tional reconstruction of RAGS in terms of
semantic web concepts, which yields a rel-
atively simple approach that can exploit
semantic web technology directly. Given
that RAGS was motivated by the concerns
of the NLG community, it is perhaps re-
markable that its aspirations seem to fit so
well with semantic web technology.
1 Introduction
The semantic web is a vision of a future world
wide web where content, rather than being primar-
ily in the form of unanalysed natural language, is
machine accessible (Antoniou and van Harmelen,
2004). This could bring a number of advantages
compared to the present web, in terms, for instance
of the precision of web search mechanisms and the
extent to which web resources can be brought to-
gether automatically for solving complex process-
ing problems.
From the point of view of NLG, the seman-
tic web offers a vision of a situation where re-
sources can be formally described and composed,
and where it is possible to live with the variety of
different approaches and views of the world which
characterise the users of the web. Given the het-
erogeneous nature of NLG, it seems worth con-
sidering whether there might be some useful ideas
here for NLG.
The foundation of the semantic web is the idea
of replacing formatting-oriented languages such as
HTML by varieties of XML which can capture the
structure of content explicitly. Markup of linguis-
tic resources (text corpora, transcribed dialogues,
etc.) via XML is now standard in NLP, but very
often each use of XML is unique and hard to rec-
oncile with any other use. The semantic web goes
beyond this in proposing a more abstract basic lan-
guage and allowing explicit representation of what
things in it mean. For the semantic web, RDF
(Klyne and Carroll, 2003), which is built on top of
XML, represents a common language for express-
ing content as a ?semantic network? of triples, and
ontology languages, such as OWL (McGuinness
and van Harmelen, 2004), allow the expression
of constraints and principles which partially con-
strain possible interpretations of the symbols used
in the RDF. These ontologies are statements that
themselves can be inspected and modified. They
can provide the basis for different people to ex-
press their assumptions, agreements and disagree-
ments, and to synthesise complex data from mul-
tiple sources.
2 RAGS
RAGS (?Reference Architecture for Generation
Systems?) was an attempt to exploit previous ideas
about common features between NLG systems
in order to propose a reference architecture that
would help researchers to share, modularise and
evaluate NLG systems and their components. In
practice, the project found that there was less
agreement than expected among NLG researchers
on the modules of an NLG system or the order of
their running. On the other hand, there was rea-
sonable agreement (at an abstract level) about the
kinds of data that an NLG system needs to repre-
Figure 1: RAGS
sent, in passing from some original non-linguistic
input to a fully-formed linguistic description as its
output. Figure 1 summarises RAGS and how it
was intended to be used. The following descrip-
tion simplifies/ rationalises in a number of ways;
more information about RAGS can be found in
(Mellish et al, 2006) and (Cahill et al, 2001).
RAGS provides abstract type definitions for 6
different types of data representations: concep-
tual, rhetorical, document, semantic, syntactic and
?quote?. As an example, here are the definitions
associated with document representations (which
describe the parts of a document and aspects of
their logical formatting).
DocRep = DocAttr ?DocRepSeq
DocRepSeq = DocRep?
DocAttr = (DocFeat? DocAtom)
DocFeat,DocAtom ? Primitives
These type definitions express the types in terms
of set theory (using constructions such as union,
Cartesian product, subset and function), where
the ?primitive? types correspond to basic sets
that have to be defined in a theory-specific way.
Thus a document representation (DocRep) has
two components, a DocAttr and a DocRepSeq.
A DocRepSeq is a sequence of zero or more
DocReps, which represent the document structure
of the parts of the document. A DocAttr is a
function from DocFeats to DocAtoms. The for-
mer can be thought of as a set of names of ?fea-
tures? for parts of documents (e.g. text level, in-
dentation) and the latter as a set of values for these
(e.g. ?Clause?, 3). However the sets DocFeat
and DocAtom are left unspecified in the RAGS
formalisation. The idea is that researchers will not
necessarily agree how to instantiate these primi-
tives. Clusters of researchers may agree on stan-
dard possibilities for these sets and this will help
them to share data (but even researchers not able
to agree on the primitive sets will be able to under-
stand one anothers? data to some extent). When
two NLG modules need to exchange data, they
need to refer to an agreed instantiation of the prim-
itive types in order to share fully.
Although it gives some examples, RAGS does
not specify any particular formats in which data
should be represented in different programming
languages and used by NLG modules ? poten-
tially, arbitrary ?native formats? could be used,
as long as they can be viewed as ?implementa-
tions? of the abstract type definitions. Further
conditions are, however, imposed by requiring a
correspondance between native formats and repre-
sentations in a ?reference implementation? called
the Objects and Arrows (OA) model. This pro-
vides answers to further questions, such as what
partially-specified data representations are possi-
ble, where re-entrancy can occur and how data
representations of different types can be mixed.
The OA model represents data as directed graphs,
whose nodes represent typed pieces of data and
whose edges represent relations. The possible le-
gal states of an OA representation are formally de-
fined, in a way that resembles the way that infor-
mation states in a unification grammar can be char-
acterised (Shieber, 1986). Each node in the graph
is labelled with a type, e.g. DocRep, DocAtom.
Each node is assumed to have a unique identifier
and for primitive types a node can also have a sub-
type, a theory-dependent elaboration that applies
to this particular data object (e.g. a DocAtom
could have the subtype 3). Some edges in the
graph indicate ?local arrows?, which describe the
parts of complex datastructures. For instance,
edges labelled el indicate elements of unordered
structures, and arrows labelled el-1, el-2 etc. indi-
cate components of ordered structures. Edges can
also represent ?non-local arrows? which describe
relationships between representations at different
levels. Non-local arrows allow data representa-
tions at different levels to be mixed into a single
graph.
Representations in the Objects and Arrows
model can be mapped to an XML interchange rep-
resentation. The correspondance between native
formats and the OA model can then be used to map
between native data representations and XML (in
both directions). Modules can communicate via
agreed native formats or, if this is undesirable, via
the XML representation.
3 Some Problems with RAGS
Some of the problems with RAGS, which have im-
peded its uptake, include:
? Complexity and lack of tools ? RAGS was a
proposal with a unique shape and takes some
time to understand fully. It ploughs its own
distinctive furrow. Because it was developed
in a project with limited resources, there are
limited tools provided for, for instance, dis-
playing RAGS structures, supporting differ-
ent programming languages and styles and
automatic consistency checking. This means
that engaging with RAGS involves initially
a significant amount of low-level program-
ming, with benefits only to be seen at some
time in the future.
? Idiosyncratic use of XML ? RAGS had to ad-
dress the problem of expressing a graph in a
serialised form, where there can be multiple,
but different, serialisations of the same graph.
It did this in its own way, which means that it
is hard to exploit general tools which address
this problem in other areas.
? Inclarity about how to ?buy-in? to limited de-
grees - there is no defined mechanism for di-
viding generally agreed from non-agreed el-
ements of a RAGS representation or for ex-
pressing or referring to an ?agreed instantia-
tion?.
4 Recasting RAGS data in terms of RDF
The first step in recasting RAGS in semantic web
terms is to exploit the fact that it is the OA model
(rather than the abstract type definitions) that is the
basis of data communication, since this model ex-
presses more concrete requirements on the form of
the data. Therefore initially we concentrate on the
OA model and its XML serialisation.
RDF is a data model that fits OA graphs very
well. It provides a way of creating ?seman-
tic networks? with sets of object-attribute-value
triples. Objects and attributes are ?resources?,
which are associated with Universal Resource
Identifiers (URIs), and values are either resources
or basic data items (?literals?, e.g. strings or in-
tegers). Resources have types, indicated by the
RDF type attribute. The idea of an RDF resource
maps nicely to a RAGS object, and the idea of an
RDF attribute maps nicely to a RAGS arrow.
URIs provide a natural way to allow reen-
trancy to be represented and at the same time per-
mit unambiguous references to external objects
in the way that RAGS intended should be pos-
sible. The XML namespace mechanism allows
complex IDs to be abbreviated by names of the
form Prefix:N, where Prefix is an abbrevi-
ation for the place where the name N is defined
and N is the basic name (sometimes the prefix can
be inferred from context and can be missed out).
Thus, for instance, if the prefix rags is defined
to stand for the start of a URI identifying RAGS
then rags:DocRep identifies the type DocRep
defined by RAGS, as distinct from any other defi-
nition anyone might have.
It follows from the preceding discussion that in-
stances of the RAGS abstract types can be mapped
naturally to RDF resources with the abstract type
as the value for the RDF attribute type. Arrows
can be mapped into RDF attributes, and so it re-
ally only remains to have a convention for the rep-
resentation of ?subtype? information in RAGS. In
this paper, we will assume that instances of primi-
tive types can have a value for the attribute sub.
RDF can be serialised in XML in a number of
ways (which in fact are closely related to the pos-
sible XML serialisations of RAGS).
To summarise, using RDF rather than RAGS
XML introduces negligable extra complexity but
has a number of advantages:
? Because it is a standard use of XML, it means
that generic software tools can be used with
it. Existing tools, for instance, support read-
ing and writing RDF from different program-
ming languages, visualising RDF structures
(see Figure 4) and consistency checking.
? Because it comes with a universal way of
naming concepts, it means that it is possi-
ble for different RAGS resources to be un-
ambiguous and reference one another.
5 Formalising the RAGS types using
ontologies
RDF gives us a more standard way to interpret the
OA model and to serialise OA instance informa-
tion in XML. However, on its own it does not en-
force data representations to be consistent with the
intent of the abstract type definitions. For instance,
it does not prevent an element of a DocRepSeq
being something other than a DocRep.
For RAGS, an XML DTD provided constraints
on what could appear in the XML serialisation, but
DTDs are not very expressive and the RAGS DTD
had to be quite loose in order to allow partial rep-
resentations. The modern way to define the terms
that appear in a use of RDF, and what constraints
there are on their use, is to define an ontology us-
ing a language like RDFS (Brickley and Guha,
2003) or OWL (McGuinness and van Harmelen,
2004). An ontology can be thought of as a set of
logical axioms that limits possible interpretations
of the terms. This could be used to show, for in-
stance, that a given set of instance data is inconsis-
tent with an ontology, or that further logical conse-
quences follow from it. There are various versions
of the web ontology language OWL. In this paper,
we use OWL DL, which is based on a description
logic, and we will use standard description logic
notation in this paper.
Description logics allow one to make statements
about the terms (names of concepts and roles) used
in some knowledge representation. In our case,
a concept corresponds to a RAGS type (imple-
mented by an RDF resource linked to from in-
dividuals by a type attribute) and a role cor-
responds to a RAGS arrow (implemented by an
RDF attribute). Complex names of concepts can
be built from simple names using particular con-
structors. For instance, if ? and ? are two con-
cept names (simple concept names or more com-
plex expressions made from them) and ? is a role
name, then, the following are also concept names:
?unionsq ? - names the concept of everything
which is ? or ?
?u ? - names the concept of everything
which is ? and ?
??.? - names the concept of everything
which has a value for ? which is an
instance of concept ?
??.? - names the concept of everything
which only has values for ? which
are instances of concept ?
=n ? - names the concept of everything
with exactly n different values of ?
Constructors can be nested, so that, for instance,
C1 u ?r.C2 is a possible concept name, assuming
that C1 and C2 are simple concept names and r is
a role name.
For an ontology, one then writes logical axioms
stating relationships between simple or complex
concept names, e.g.
?1 v ?2 - states that ?1 names a more
specific concept than ?2
?1 ? ?2 - states that ?1 names the same
concept as ?2
disjoint({?1, . . . ?n}) - states that
?1, . . . ?n are disjoint concepts (no
pair can have a common instance).
?1 vr ?2 - states that ?1 names a sub-
property of ?2
domain(?, ?) - states that ? can only
apply to things satisfying concept ?
range(?, ?) - states that values of ?
must satisfy concept ?
functional(?) - states that ? is a func-
tional role
Figure 2: Using Multiple Ontologies in RAGS
For more information on the formal basis of de-
scription logics and their relevance for ontologies,
see (Horrocks, 2005).
For RAGS, a number of advantages follow from
adopting DLs as the basis for formalising data rep-
resentations:
Modularity. A given set of instance data may re-
late to more than one ontology which ex-
presses constraints on it. One ontology is
said to import another if it inherits the con-
straints that the other provides. The standard
(monotonic) logic approach applies, in that
one can choose to describe the world in terms
of any consistent set of axioms. Ontologies
package up sets of axioms into bundles that
one might decide to include or not include in
one?s own model of the world. Ontologies for
different purposes can be built by different
people but used together in an eclectic way.
This formalises the idea of ?variable buy-in?
in RAGS.
Openness. Also corresponding to the usual ap-
proach with logic, the semantics of OWL
makes no closed world assumption. Thus a
statement cannot be inconsistent purely by
failing to specify something. This means that
it is only necessary to describe the proper-
ties of complete datastructures in an ontol-
ogy. Partial descriptions of data will be not
be inconsistent by virtue of their partiality.
Only having to describe complete datastruc-
tures makes the specification job much sim-
pler. In a similar way, the semantics of OWL
makes no unique names assumption. Thus
individuals with different names are not nec-
essarily distinct. This means that it is gen-
erally possible to make a given description
more specific by specifying the identity of
two individuals (unless inconsistency arises
through, for instance, the individuals having
incompatible types). This is another require-
ment if one wishes the power to add further
information to partial representations.
Software tools. As with RDF, use of OWL DL
opens up the possibility of exploiting generic
tools developed elsewhere, for instance rea-
soners and facilities to translate RAGS con-
cepts into programming language structures.
6 The RAGS Ontologies
It is convenient to modularise what RAGS requires
as well-formedness constraints as a set of ontolo-
gies. This allows us to formalise what it means to
?buy-in? to one or more parts of RAGS. It simply
means importing one or more of the RAGS ontolo-
gies (in addition to one?s own) and making use of
some of the terms defined in them. We now outline
one possible version of the core RAGS ontologies.
Figure 2 shows the way that the RAGS ontolo-
gies are intended to be used. A dataset in general
makes use of concepts defined in the core RAGS
ontologies (the ?upper ontology? and the ?NLG
Figure 3: The RAGS ?NLG ontology?
ontology?)1 and also theory-dependent elabora-
tions defined in separate ontologies (which may
correspond one-to-one to the different levels, as
shown, but need not do so necessarily). These
elaborations are not (initially) provided by RAGS
but may arise from arbitrary research subcom-
munities. Logically, the dataset is simple de-
scribed/constrained by the union of the axioms
coming from the ontologies it makes use of. In
general, different datasets will make consistent
references to the concepts in the core RAGS
ontologies, but they may make use of different
theory-dependent elaborations.
The basis of RAGS is a very neutral theory
about datatypes (and how they can be encoded in
XML). This is in fact independent of the fact that
RAGS is intended for NLG - at this level, RAGS
could be used to describe data in other domains, or
NLG-oriented data that is not covered by RAGS. It
therefore makes sense to think of this as a separa-
ble part of the theory, the ?upper ontology?. At the
top level, datastructures (instances of Object)
belong to one of the concepts Ordered, Set
and Primitive. Ordered structures are divided
1These are both available in full from
http://www.abdn.ac.uk/?csc248/ontologies/
up in terms of the number of components (con-
cepts Arity-1, Arity-2 etc) and whether they
are Tuples or Sequences. For convenience,
union types such as Arity-atleast-2 are
also defined.
The RAGS NLG ontology (see Figure 3 for
an overview) contains the main substance of the
RAGS type definitions. As the figure shows,
it introduces a number of new concepts as sub-
concepts of the upper ontology concepts. For
instance, DocRepSeq, RhetRepSeq, Adj
and Scoping are introduced as subconcepts
of SpecificSequence (these concepts cor-
respond to types used in RAGS at the doc-
ument, rhetorical, syntactic and semantic lev-
els). Not shown in the diagram is the type of
roles, Functional that includes all arguments
of RAGS functional objects2. The set of type
definitions describing a level of representation in
RAGS translates quite directly into a set of axioms
in this ontology. For instance, the following is the
encoding of the type definitions for document rep-
2Whereas in RAGS a functional type (e.g. DocAttr)
is represented as an unordered set of (ordered) pairs of the
form <function argument,function value>, here we can sim-
ply implement the function arguments as RDF attributes and
omit the functional types.
Figure 4: Visualisation of example Document Representation
resentations. First of all, it is necessary to specify
that aDocRep is a tuple with arity 1 (theDocAttr
is not needed), and that its component must have a
specific type:
DocRep v Tuple uArity-1
DocRep v (?el-1.DocRepSeq)
The next few axioms do a similar job for
DocRepSeq, a kind of sequence:
DocRepSeq v SpecificSequence
DocRepSeq v (?n-el.DocRep)
Finally, a high level role DocFeat is introduced,
whose subroles will correspond to particular docu-
ment features like Indentation. The domain and
range of such roles are constrained via constraints
on DocFeat:
DocFeat vr Functional
domain(DocFeat,DocRep)
range(DocFeat,DocAtom)
7 Other Ontologies and RAGS
As stated above, in general one produces special-
isations of the RAGS framework by creating new
ontologies that:
? Introduce specialisations of the RAGS prim-
itive concepts (and perhaps new roles that in-
stances of these can have).
? Introduce subroles of the RAGS functional
roles.
? Add new axioms that specialise existing
RAGS requirements, involving the core con-
cepts and roles and/or the newly introduced
ones.
An example of this might be an example ontol-
ogy that instantiates a simple theory of document
structure, following (Power et al, 2003). Given
the notion of document structure introduced in
section 2 and formalised in section 6, it is really
only necessary to specify the ?features? of pieces
of document structure (DocFeat) and their ?val-
ues? (DocAtom). The former are modelled as
roles and the latter in terms of concepts. First we
introduce the basic types of values:
DocAtom ? (Position unionsq Indentation unionsq
Level unionsq Connective)
disjoint({(Position, Indentation, Level,
Connective})
Positions in the text could be modelled by objects
whose sub values are positive integers (there is a
standard (RDFS) datatype for these). The follow-
ing axioms capture this and the characteristics of
the role hasPosition:
Position v (?sub.xsd : positiveInteger)
hasPosition vr DocFeat
range(hasPosition, Position)
functional(hasPosition)
For text levels, on the other hand, there is a fixed
set of possible values. These are introduced as dis-
joint concepts. In addition, the role hasLevel is
introduced:
Level ? (ChapterunionsqParagraphunionsqSectionunionsq
Text-Clause unionsq Text-Phrase unionsq
Text-Sentence)
disjoint({Chapter, Paragraph, Section,
Text-Clause, Text-Phrase, Text-Sentence})
hasLevel vr DocFeat
range(hasLevel, Level)
functional(hasLevel)
Figure 4 shows an example DocRep (labelled
?d12?) described by this ontology, as visualised
by the RDF-Gravity tool developed by Salzburg
Research. It consists of aDocRepSeq (?d6?) with
two DocRep components (?d0? and ?d13?). The
indentations of ?d12? and ?d0? are not known, but
they are constrained to be the same.
It is easy to think of examples of other (ex-
isting or potential) ontologies that could provide
theories of the RAGS primitive types. For in-
stance, WordNet (Miller, 1995) or the Generalised
Upper Model (Bateman et al, 1995) could be
used to bring in a theory of semantic predicates
(SemPred). An ontology of rhetorical relations
(RhetRel) could be built based on RST, and so
on.
Ontologies can use the expressive power of
OWL to make relatively complex statements.
For instance, the following could be used in an
RST ontology to capture the concept of nucleus-
satellite relations and the constraint that a rhetori-
cal representation with such a relation (as its first
component) has exactly two subspans (recorded in
the second component):
NS v RhetRel
(RhetRep u ?el-1.NS) v (?el-2.Arity-2)
8 Relation to Other Work
Reworking RAGS to use semantic web technology
relates to two main strands of previous work: work
on XML-based markup of linguistic resources and
work on linguistic ontologies.
The trouble with applying existing annotation
methods (e.g. the Text Encoding Initiative) to
NLG is that they presuppose the existence of a
linear text to start with, whereas in NLG one is
forced to represent more abstract structures before
coming up with the actual text. A recent proposal
from Linguistics for a linguistic ontology for the
semantic web (Farrar and Langendoen, 2003) is
again based around making annotations to exist-
ing text. Research is only just beginning to es-
cape from a ?time-based? mode of annotation, for
instance by using ?stand-off? annotations to indi-
cate layout (Bateman et al, 2002). In addition,
most annotation schemes are partial (only describe
certain aspects of the text) and non-structured (as-
sign simple labels to portions of text). For NLG,
one needs a way of representing all the informa-
tion that is needed for generating a text, and this
usually has complex internal structure.
Linguistic ontologies are ontologies developed
to describe linguistic concepts. Although ontolo-
gies are used in a number of NLP projects (e.g.
(Estival et al, 2004)), the ontologies used are usu-
ally ontologies of the application domain rather
than the linguistic structures of natural languages.
The development of ontologies to describe aspects
of natural languages is comparatively rare. The
WordNet ontologies are a widely used resource
describing the repertoire of word senses of nat-
ural languages, but these concentrate on individ-
ual words rather than larger linguistic structures.
More relevant to NLG is work on various versions
of the Generalised Upper Model (Bateman et al,
1995), which outlines aspects of meaning relevant
to making NLG decisions. This has been used to
help translate domain knowledge in a number of
NLG systems (Aguado et al, 1998).
In summary, existing approaches to using on-
tologies or XML for natural language related pur-
poses are not adequate to describe the datastruc-
tures needed for NLG. Semantic web technology
applied to specifications with the complexity of
those generated by RAGS might, however, be able
to fill this gap.
9 The Semantic Web for NLG tasks
In the above, we have made a case for the use of
semantic web technology to aid inter-operability
and sharing of resources for NLG. This was jus-
tified largely by the fact that the most significant
NLG ?standardisation? effort so far, RAGS, can
be straightfowardly recast in semantic web terms,
bringing distinct advantages. Even if RAGS itself
is not taken forward in its current form, this sug-
gests that further developments of the idea could
bear fruit in semantic web terms.
The semantic web is certainly not a panacea for
all the problems of NLG, and indeed there are as-
pects of the technology that are still at an early
stage of development. For instance, the problems
of matching/ reconciling alternative ontologies are
many and complex. Some researchers even dis-
pute the viability of the general approach. On the
other hand, the semantic web community is con-
cerned with a number of problems that are also
very relevant to NLG. Fundamentally, the seman-
tic web is about sharing and exploiting distributed
computational resources in an open community
where many different goals, viewpoints and the-
ories are represented. This is something that NLG
also seeks to do in a number of ways. The seman-
tic web movement has considerable momentum.
There are more of them than us. Let?s see what we
can get from it.
Acknowledgments
This work was supported by EPSRC grant
EP/E011764/1.
References
G. Aguado, A. Ba no?n, John A. Bateman, S. Bernardos,
M. Ferna?ndez, A. Go?mez-Pe?rez, E. Nieto, A. Olalla,
R. Plaza, and A. Sa?nchez. 1998. Ontogeneration:
Reusing domain and linguistic ontologies for Span-
ish text generation. In Proceedings of the ECAI?98
Workshop on Applications of Ontologies and Prob-
lem Solving Methods, pages 1?10, Brighton, UK.
Grigoris Antoniou and Frank van Harmelen. 2004. A
Semantic Web Primer. MIT Press.
John A. Bateman, Renate Henschel, and Fabio Rinaldi.
1995. Generalized Upper Model 2.0: documenta-
tion. Technical report, GMD/Institut fu?r Integrierte
Publikations- und Informationssysteme, Darmstadt,
Germany.
John Bateman, Renate Henschel, and Judy Delin.
2002. A brief introduction to the GeM annotation
scheme for complex document layout. In Proceed-
ings of NLP-XML 2002, Taipei.
D. Brickley and R. V. Guha. 2003. Rdf vocabulary
description language 1.0: Rdf schema. Technical
Report http://www.w3.org/TR/rdf-schema, World
Wide Web Consortium.
Lynne Cahill, Roger Evans, Chris Mellish, Daniel
Paiva, Mike Reape, and Donia Scott. 2001.
The RAGS Reference Manual . Available at
http://www.itri.brighton.ac.uk/projects/rags.
Dominique Estival, Chris Nowak, and Andrew
Zschorn. 2004. Towards ontology-based natural
language processing. In Proceedings of NLP-XML
2004, Barcelona.
Scott Farrar and Terry Langendoen. 2003. A linguistic
ontology for the semantic web. Glot International,
7(3):1?4.
Ian Horrocks. 2005. Description logics in ontology
applications. In B. Beckert, editor, Proc. of the 9th
Int. Conf. on Automated Reasoning with Analytic
Tableaux and Related Methods (TABLEAUX 2005),
pages 2?13. Springer Verlag LNCS 3702.
Baden Hughes and Steven Bird. 2003. Grid-enabling
natural language engineering by stealth. In Pro-
ceedings of the HLT-NAACL 2003 Workshop on The
Software Engineering and Architecture of Language
Technology Systems.
G. Klyne and J. Carroll. 2003. Resource descrip-
tion framework (rdf): Concepts and abstract syn-
tax. Technical Report http://www.w3.org/TR/rdf-
concepts, World Wide Web Consortium.
D. L. McGuinness and F. van Harmelen.
2004. Owl web ontology language overview.
http://www.w3.org/TR/owl-features/.
Chris Mellish, Donia Scott, Lynne Cahill, Daniel Paiva,
Roger Evans, and Mike Reape. 2006. A reference
architecture for generation systems. Natural lan-
guage engineering, 1:1?34.
G. Miller. 1995. Wordnet: A lexical database for en-
glish. CACM, 38(11):39?41.
Richard Power, Donia Scott, and Nadjet Bouayad-
Agha. 2003. Document structure. Computational
Linguistics, 29:211?260.
Stuart M. Shieber. 1986. An introduction to
unification-based approaches to grammar. CSLI.
INLG 2012 Proceedings of the 7th International Natural Language Generation Conference, pages 17?21,
Utica, May 2012. c?2012 Association for Computational Linguistics
MinkApp: Generating Spatio-temporal Summaries for Nature Conservation
Volunteers
Nava Tintarev, Yolanda Melero, Somayajulu Sripada,
Elizabeth Tait, Rene Van Der Wal, Chris Mellish
University of Aberdeen
{n.tintarev, y.melero, yaji.sripada,
elizbeth.tait, r.vanderwal, c.mellish@abdn.ac.uk}@abdn.ac.uk
Abstract
We describe preliminary work on generat-
ing contextualized text for nature conservation
volunteers. This Natural Language Genera-
tion (NLG) differs from other ways of describ-
ing spatio-temporal data, in that it deals with
abstractions on data across large geographi-
cal spaces (total projected area 20,600 km2),
as well as temporal trends across longer time
frames (ranging from one week up to a year).
We identify challenges at all stages of the clas-
sical NLG pipeline.
1 Introduction
We describe preliminary work on summarizing
spatio-temporal data, with the aim to generate con-
textualized feedback for wildlife management vol-
unteers. The MinkApp project assesses the use
of NLG to assist volunteers working on the Scot-
tish Mink Initiative (SMI). This participatory initia-
tive aims to safeguard riverine species of economic
importance (e.g., salmon and trout) and species of
nature conservation interest including water voles,
ground nesting birds and other species that are ac-
tively preyed upon by an invasive non-native species
- the American mink (Bryce et al, 2011).
2 Background
Our test ground is one of the world?s largest
community-based invasive species management
programs, which uses volunteers to detect, and sub-
sequently remove, American mink from an area of
Scotland set to grow from 10,000 km2 in 2010 to
20,600 km2 by the end of 2013 (Bryce et al, 2011).
Such a geographical expansion means that an in-
creasing share of the monitoring and control work is
undertaken by volunteers supported by a fixed num-
ber of staff. An important contribution of volunteers
is to help collect data over a large spatial scale.
Involving members of the public in projects such
as this can play a crucial role in collecting observa-
tional data (Silvertown, 2009). High profile exam-
ples of data-gathering programmes, labelled as cit-
izen science, include Galaxy Zoo and Springwatch
(Raddick et al, Published online 2010; Underwood
et al, 2008). However, in such long-term and wide
ranging initiatives, maintaining volunteer engage-
ment can be challenging and volunteers must get
feedback on their contributions to remain motivated
to participate (Silvertown, 2009). NLG may serve
the function of supplying this feedback.
3 Related work
We are particularly interested in summarizing raw
geographical and temporal data whose semantics
need to be computed at run time ? so called spatio-
temporal NLG. Such extended techniques are stud-
ied in data-to-text NLG (Molina and Stent, 2010;
Portet et al, 2009; Reiter et al, 2005; Turner et
al., 2008; Thomas et al, Published online 2010).
Generating text from spatio-temporal data involves
not just finding data abstractions, but also determin-
ing appropriate descriptors for them (Turner et al,
2008). Turner et. al (2008) present a case study in
weather forecast generation where selection of spa-
tial descriptors is partly based on domain specific
(weather related) links between spatial descriptors
17
and weather phenomena. In the current project we
see an opportunity to investigate such domain spe-
cific constraints in the selection of descriptors over
larger temporal and spatial scales.
4 Current Status
Over 600 volunteers currently notify volunteer man-
agers of their ongoing mink recording efforts. Our
work is informed by in-depth discussions and inter-
views with the volunteer managers, as well as 58
(ground level) volunteers? responses to a question-
naire about their volunteering experience. The set of
volunteers involves different people, such as conser-
vation professionals, rangers, landowners and farm-
ers with the degree of volunteer involvement varying
among them. Most volunteers check for sightings:
footprints on a floating platform with a clay-based
tracking plate (raft hereafter) readily used by mink,
or visual sightings on land or water. Others set and
check traps, and (much fewer volunteers) dispatch
trapped mink.1 In terms of feedback, volunteers cur-
rently receive regional quarterly newsletters, but tai-
lored and contextualized feedback is limited to spo-
radic personal communication, mostly via email.2
4.1 Why NLG in this context?
Where the initiative has been successful, mink sight-
ings are sparse. Such a lack of sightings can be de-
motivating for volunteers and leads to a situation in
which negative records are seldom recorded (Beirne,
2011). As one volunteer stated: ?Nothing much hap-
pens on my raft so my enthusiasm wanes.? Also,
73% of the volunteers who completed the ques-
tionnaire said they checked their raft at the recom-
mended frequency of every two weeks. Similarly,
72% said that they got in touch with their manager
rarely or only every couple of months ? when they
needed more clay or saw footprints. NLG based
feedback could motivate volunteers by informing
them about the value of negative records. If they
were to stop because of a lack of interest, mink are
likely to reinvade the area.
1Traps are only placed once a sighting has occurred. Once
placed, by law a trap must be checked daily.
2In this project, we are using a corpus based on newsletters
from the North Scotland Mink Project and the Cairngorms Wa-
ter Vole Conversation Project.
In addition, volunteers who work alone can be
isolated and lack natural mechanisms for informa-
tion exchange with peers. We postulate that giving
the volunteers contextualized feedback for an area
gives them a better feeling for their contribution to
the project and a better sense of how the initiative is
going overall. A need for this has already been felt
by volunteers: ?Knowing even more about progress
in the catchment would be good - and knowing in de-
tail about water vole returning and latest mink sight-
ings. It would be helpful to learn about other neigh-
boring volunteers captures sightings in ?real time?.?
5 Approach
In this section we describe the generation of text in
terms of a classic NLG pipeline, (Reiter and Dale,
2000), while addressing the additional tasks of in-
terpreting the input data (from volunteers) to mean-
ingful messages that achieve the desired communi-
cation goals: providing information to, as well as
motivating volunteers. The NLG system which will
generate these texts is actively under development.
5.1 Gold standard
Our nearest comparison is a corpus of domain spe-
cific conservation newsletters containing text such
as the one below. These newsletters give us an idea
of the type of structure and lexical choice applied
when addressing volunteers, using both temporal
and spatial summaries. However, these texts are not
contextualized, or adapted to a particular volunteer.
?With an ever expanding project area, we
are progressing exceptionally well achiev-
ing and maintaining areas free of breed-
ing mink through-out the North of Scot-
land. Currently, the upper Spey, upper
Dee and Ythan appear to be free of breed-
ing mink, with only a few transients pass-
ing through...?
We would like to improve on these existing texts
and aim to generate texts that are tailored and con-
sider the context of the volunteer. The text below is
developed from a template supplied from a volunteer
manager in the process of corpus collection. In the
following sections we describe the steps and chal-
lenges involved in the process of generating such a
text.
18
?Thank you for your helpful contribution!
You may have not seen any signs this time,
but in the last week two people in the Spey
catchment have seen footprints on their
rafts. This means there might be a female
with a litter in your neighborhood ? please
be on the lookout in the coming weeks!
Capturing her could mean removing up to
6 mink at once!?
5.2 Example input
The data we receive from volunteers includes pos-
itive and negative records from raft checks (every
14 days), visual sightings, and mink captures. Each
record contains a geographical reference (x and y co-
ordinate) and a timestamp. In addition, for trapped
mink we may know the sex (male, female, or un-
known) and age (juvenile, adult, or unknown).
5.3 Data analysis and interpretation
Spatial trends. The current version of the system
can reason over geographical information, defin-
ing various notions of neighborhood.3 For a given
point the following attributes can be used to describe
its neighborhood: geographical region (catchment
and subcatchment), Euclidean distance from another
point, and relative cardinal direction to another point
(north, south, east, west). The system reasons about
sightings and captures using facts such as:
? This point (on land or water) is in the Dee
catchment.
? Three neighbors have seen footprints (within a
given time window).
? One neighbor has caught a mink (within a given
time window).
? The nearest mink footprint is 15 km north east
of this point.
The definition of neighborhood will differ accord-
ing to domain specific factors. Euclidean distance
appears to be the most likely candidate for use, be-
cause sightings may belong to different geographic
3The reasoning is performed using the opensource GIS
Java library Geotools, http://geotools.org, retrieved
Jan 2012
regions (catchments) but be very close to each other.
More importantly, the definition of neighborhood is
likely to depend on the geographic region (e.g. ar-
eas differ in terms of mink population density with
mountainous regions less likely to be utilized than
coastal regions).
Temporal trends. Aside from geographic trends,
the system will also be used to portray temporal
trends. These look at the change in sightings be-
tween two time intervals, identifying it as a falling,
rising or steady trend in mink numbers. We are
primarily observing trends between different years,
but also taking into consideration the ecology of the
mink including their behavior in different seasons
and for quantification. For example, we need to be
able to decide if an increase from 0 to 5 mink sight-
ings in an area during breeding is worth mentioning
? most likely it is, as this a common size for a litter.
Another example is the definition of a ?cleared? area
- Example 1 below describes a stable zero trend over
a longer period of time.
...Currently, the upper Spey, upper Dee and Ythan
appear to be free of breeding mink...
(1)
5.4 Document planning
Content determination While useful on its own,
the text that could be generated from the data analy-
sis and interpretation described above is much more
useful when domain specific rules are applied. Ex-
ample 2 describes a significant year-on-year increase
for a given definition of neighborhood, during breed-
ing season.
IF ( (month >= 6 AND month <9)
AND sightingsLastYear(area) == 0
AND sightingsThisYear >= 5 )
THEN feedback +=
?It looks like the area has been reinvaded.
We should get ready to trap them to keep this
area mink free.?
(2)
Example rule 2 is applied in the breeding season (ca
June-Aug.). It will be given a score which signi-
fies its relative importance compared to other de-
rived content to allow prioritization. For example,
19
if there are both female and male captures in a re-
gion, it would be more important to speak about the
female capture. This is because the capture of breed-
ing mink has a much larger positive impact on the
success of the initiative.4 This importance should
be reflected in texts such as: ...Capturing her could
mean removing up to 6 mink at once!...
Document structuring Since our goal is to moti-
vate as well as inform, the structure of the text will
be affected. If we consider the example text in Sec-
tion 5.1, we can roughly divide it into three summary
types:
? Personal - ?Thank you for your helpful contri-
bution! You may have not seen any signs this
time.?
? Neighbor - ?In the last week two people in the
Spey catchment have seen small footprints on
their rafts.?
? Biology - ?There might be a female with a litter
in your neighborhood ... Capturing her could
mean removing up to 6 mink at once!?
If, in contrast to the previous example, a volun-
teer would capture a mink, then the neighborhood
summary can be used to emphasize the importance
of rare captures.
?IF currentMonth == August AND
capture == true AND nCapturesInSummer == 0?
(3)
The feedback for rule 3 might read something
like: ?Well done! So far, this was the only mink cap-
tured during the breeding season in the Spey catch-
ment!?
5.5 Microplanning
Microplanning will need to consider the aggrega-
tion of spatio-temporal data that happens on a deeper
level e.g., for a given catchment and year. This ag-
gregation is likely to result in a surface aggregation
as well deeper data aggregation, such as the catch-
ments in Example 1. In terms of lexical choice, the
system will have to use domain appropriate vocabu-
lary. The latter example refers to ?breeding mink?,
4Established adult females with litters.
which informs the reader that their capture has a
large impact on population control. Another exam-
ple of lexical choice may be ?quieter autumn? to de-
note a decrease in mink for an area.
The best way to communicate neighborhood to
volunteers is still an open question. The texts in
our corpus describe neighborhoods in terms of geo-
graphic regions (catchments and subcatchments, e.g.
Spey). However, Euclidean distance may be more
informative, in particular close to catchment bound-
aries.
6 Challenges
There are several key challenges when generating
motivating text for nature conservation volunteers,
using spatio-temporal NLG.
One challenge is to tailor feedback texts to in-
dividuals according to their motivations and infor-
mation needs. In line with previous research in
affective NLG (de Rosis and Grasso, 2000; Belz,
2003; Sluis and Mellish, 2010; Tintarev and Mas-
thoff, 2012; Mahamood and Reiter, 2011), we con-
tinue to study the factors which are likely to have
an effect on volunteer motivation. So far we have
worked together with volunteer managers. We col-
lected a corpus of texts, written by the managers,
that are tailored to motivate different volunteer per-
sonas, and conducted interviews and a focus group
with them. While we found that the mink managers
tailored texts to different personas, interviews indi-
cated that the biggest factor to tailor for was the def-
inition of neighborhood. Some volunteers are inter-
ested in a local update, while others are interested in
a larger overview.
A second, related challenge, regards correctly
defining the reasoning over spatio-temporal facts
e.g., quantifying the magnitude of significant
changes (increases and decreases in sightings and
captures) for different seasons, regions, and the time
frames over which they occur. We believe this will
lead to generating text referring to more compound
abstractions such as mink free areas, or re-invasion.
A final challenge brought out by the interviews
is to supply varied feedback that helps volunteers to
continue to learn about mink and their habitat. This
is a challenge for both content determination and mi-
croplanning.
20
References
Christopher Beirne. 2011. Novel use of mark-recapture
framework to study volunteer retention probabilities
within an invasive non-native species management
project reveals vocational and temporal trends. Mas-
ter?s thesis, University of Aberdeen.
Anja Belz. 2003. And now with feeling: Developments
in emotional language generation. Technical Report
ITRI-03-21, Information Technology Research Insti-
tute, University of Brighton.
Rosalind Bryce, Matthew K. Oliver, Llinos Davies, He-
len Gray, Jamie Urquhart, and Xavier Lambin. 2011.
Turning back the tide of american mink invasion at an
unprecedented scale through community participation
and adaptive management. Biological Conservation,
144:575?583.
Fiorella de Rosis and Floriana Grasso, 2000. Affective
Interactions, volume 1814 of Lecture Notes in Artifi-
cial Intelligence, chapter Affective Natural Language
Generation. Springer-Verlag.
Saad Mahamood and Ehud Reiter. 2011. Generating af-
fective natural language for parents of neonatal infants.
In ENLG.
Martin Molina and Amanda Stent. 2010. A knowledge-
based method for generating summaries of spatial
movement in geographic areas. International Journal
on Artificial Intelligence Tools, 19(3):393?415.
Francois Portet, Ehud Reiter, Albert Gatt, Jim Hunter,
Somayajulu Sripada, Yvonne Freer, and Cindy Sykes.
2009. Automatic generation of textual summaries
from neonatal intensive care data. Artificial Intelli-
gence, 173:789?816.
M. Jordan Raddick, Georgia Bracey, Pamela L. Gay,
Chris J. Lintott, Phil Murray, Kevin Schawinski,
Alexander S. Szalay, and Jan Vandenberg. Published
online 2010. Galaxy zoo: Exploring the motivations
of citizen science volunteers. Astronomy Education
Review, 9(1), 010103, doi:10.3847/AER2009036.
Ehud Reiter and Robert Dale. 2000. Building natural
language generation systems. Cambridge University
Press.
Ehud Reiter, Somayajulu Sripada, Jim Hunter, Jin Yu,
and Ian Davy. 2005. Choosing words in computer-
generated weather forecasts. Artificial Intelligence,
167:137?169.
Jonathan Silvertown. 2009. A new dawn for citizen sci-
ence. Trends in Ecology & Evolution, 24:467?471.
Ielka Van der Sluis and Chris Mellish, 2010. Empiri-
cal Methods in Natural Language Generation, volume
5980 of Lecture Notes in Computer Science, chap-
ter Towards Empirical Evaluation of Affective Tactical
NLG. Springer, Berlin / Heidelberg.
Kavita E. Thomas, Somayajulu Sripada, and Matthijs L.
Noordzij. Published online 2010. Atlas.txt: Ex-
ploring linguistic grounding techniques for commu-
nicating spatial information to blind users. Journal
of Universal Access in the Information Society, DOI
10.1007/s10209-010-0217-5.
Nava Tintarev and Judith Masthoff. 2012. Evaluating
the effectiveness of explanations for recommender sys-
tems: Methodological issues and empirical studies on
the impact of personalization. User Modeling and
User-Adapted Interaction, (to appear).
Ross Turner, Somayajulu Sripada, Ehud Reiter, and Ian
Davy. 2008. Using spatial reference frames to gener-
ate grounded textual summaries of georeferenced data.
In INLG.
Joshua Underwood, Hilary Smith, Rosemary Luckin, and
Geraldine Fitzpatrick. 2008. E-science in the class-
room towards viability. Computers & Education,
50:535?546.
21
INLG 2012 Proceedings of the 7th International Natural Language Generation Conference, pages 120?124,
Utica, May 2012. c?2012 Association for Computational Linguistics
Blogging birds: Generating narratives about reintroduced species to
promote public engagement
Advaith Siddharthan, Matthew Green, Kees van Deemter, Chris Mellish & Rene? van der Wal
{advaith, mjgreen, k.vdeemter, c.mellish, r.vanderwal}@abdn.ac.uk
University of Aberdeen
Abstract
This paper proposes the use of NLG to en-
hance public engagement during the course of
species reintroductions. We examine whether
ecological insights can be effectively commu-
nicated through blogs about satellite-tagged
individuals, and whether such blogs can help
create a positive perception of the species in
readers? minds, a requirement for successful
reintroduction. We then discuss the impli-
cations for NLG systems that generate blogs
from satellite-tag data.
1 Introduction
Conservation of wildlife is an objective to which
considerable effort is devoted by governments and
NGOs across the world. A variety of web-based
approaches can help make the natural world more
accessible to the public, which in turn may trans-
late into greater public support for nature conserva-
tion initiatives. The present paper explores the role
of Natural Language Generation (NLG) in bringing
up-to-date information about wild animals in their
natural environment to nature enthusiasts.
We focus on the reintroduction of the red kite
to the UK. This member of the raptor family was
once widespread in the UK, but prolonged and in-
tense persecution led to its near extinction. Since
1989, efforts have been ongoing to reintroduce this
species in various locations across the country. We
are working together with one of the largest na-
ture conservation charities in Europe to use NLG
for public engagement around a small number of
satellite-tagged reintroduced red kites.
The public engagement activities surrounding this
reintroduction initiative have two subtly different
objectives: (i) to communicate ecological insights to
increase awareness about the species, and (ii) to cre-
ate a positive image of the reintroduced species to
harness public support for the reintroduction. Cur-
rently, data from these satellite tags are being used
by the charity to manually create blogs such as:
...Ruby (Carrbridge) had an interesting flight
down to Loch Duntelchaig via Dochfour on
the 6th March before flying back to the
Drumsmittal area, spending the 10th March in
the Loch Ussie area (possibly also attracted by
the feeding potential there!) and then back to
Drumsmittal for the 13th...
Such blogs are used by schools which have
adopted individual kites, and pupils can read these
texts alongside a map plotting the GPS locations of
?their? kite. As can already be seen from the above,
there is currently little ecological information about
the species in these blogs. Because of the perceived
importance of education to the success of reintro-
ductions, there is a clear desire to include more eco-
logical insights. Yet, time and resource limitations
have prevented the charity from doing so; they per-
ceive the writing of such blogs already as very time
consuming, and indeed, rather mundane.
In this paper, we explore the use of blogs based on
satellite tag data for communicating ecological in-
sights and creating a positive image of a species. We
consider both aspects, deemed essential for a suc-
cessful species reintroduction, and focus on how the
blogs can be made more informative than those cur-
rently being written by the charity.
2 Related work
Data-to-text systems (e.g., Goldberg et al (1994);
Theune et al (2001); Portet et al (2009)) have typ-
120
(a) (b)
Figure 1: Plot of (a) distance from nest as a function of time, and (b) clusters of visited locations.
ically been used to generate summaries of technical
data for professionals, such as engineers, nurses and
oil rig workers. There is some work on the use of
data-to-text for lay audiences; e.g., generating nar-
ratives from sensor data for automotive (Reddington
et al, 2011) and environmental (Molina et al, 2011)
applications, generating personal narratives to help
children with complex communication needs (Black
et al, 2010), and summarising neonatal intensive
care data for parents (Mahamood et al, 2008).
Our application differs from the above-mentioned
data-to-text applications, in that we aim to gener-
ate inspiring as well as informative texts. It bears
some resemblance to NLG systems that offer ?info-
tainment?, such as Dial Your Disc (Van Deemter and
Odijk, 1997) and Ilex (O?Donnell et al, 2001). In
fact, Dial Your Disc, which generates spoken mono-
logues about classical music, focused emphatically
on generating engaging texts, and achieved linguis-
tic variation through the use of recursive, syntacti-
cally structured templates (see also, Theune et al
(2001)). We intend to extend a data-to-text system
in similar ways, using ecological insights to make
narratives engaging for non-experts.
3 Overall Goals
Our overall aim is to bring satellite tagged animals
(in this case study, red kites) ?to life? by construct-
ing narratives around their patterns of movement.
We require individual locations of a bird to be ex-
plained in the context of its wider spatial use, and
the ecological interpretations thereof. This paper has
the following goals:
1. To illustrate how satellite tag data can be analysed to
identify behavioural patterns for use in generating
blogs (content selection);
2. To test whether blogs written by an ecologist based
on such data analysis can be used to educate as well
as create a positive perception of the species;
3. To investigate the challenges for NLG in automat-
ing the generation of such blogs.
4 Data analysis for identifying behaviours
From an NLG perspective, our interest in automat-
ing the generation of blogs from satellite tag data
is in making these narratives more interesting, by
using the data to illustrate key aspects of red kite
behaviour. To illustrate how we can relate the data
to behaviours, we provide two graphical views of
GPS fixes from a tagged red kite. Fig. 1(a) shows
how far a focal kite is located from its nest over the
course of a year. We propose to use such data to con-
struct narratives around ecological insights regard-
ing the exploratory behaviours of red kites during
their first year after fledgling. Fig. 1(b) shows the
same GPS data, but now spatially, thereby plotting
latitude against longitude of all fixes without regard
to time. This portrayal highlights the kite?s favoured
locations (indicated in different colours based on a
MATLAB cluster analysis which automatically esti-
mates the parameters of a Gaussian mixture model,
121
even when clusters overlap substantially), as well as
its broad range.
These plots illustrate two key aspects of kite be-
haviour: exploration and site-fidelity (the presence
of favoured locations that the kite tends to return to).
In addition, we are interested in communicating var-
ious feeding behaviours as well as that, unlike many
other birds of prey, red kites are social birds, often
found in groups. Feeding and social behaviours can-
not be directly identified from the data. However,
they can often be inferred; for instance, a red kite
spending its time by the side of a main road is likely
to be looking to scavenge on road kill.
5 Study on engaging readers using blogs
We now report a study that explores whether such
ecological insights can be effectively communicated
through blogs constructed around an individual of
the species, and whether such blogs can help create a
positive perception of the species in a reader?s mind.
This study was based on a text manually con-
structed by an ecologist based on five weeks of
data such as in Fig 1 from a red kite named ?Red
Baroness?. For this study, the data was mapped onto
a simplified world with seven features: a lake, a
shoreline, fields, a road, a moor, a forest and a river.
A sample of the text is shown in Figure 2 for illus-
tration.
Week 2: How different the pattern of movements
of Red Baroness was this week! On Monday, she
shot off past Bleak Moor, on her longest journey
so far north-east of the lake. She appeared not to
find much of interest there, and on the next day
she was observed combing the edges of Green
Park, possibly in search of a group of birds rest-
ing in the top half of the trees. The bird was
clearly restless however, as on Thursday she was
observed following River Rapid, downstream for
further than she had been last month, finally stop-
ping when she reached Blue Lake again.
Figure 2: Sample material showing week 2 from the five
week blog
5.1 Experimental Design
80 participants were shown the material: a five week
blog on the movements of the focal red kite, named
Red Baroness, alongside a picture of a red kite and a
schematic map marking the seven features of inter-
est. Participants were students at the University of
Aberdeen. The experiment was conducted in a lab in
a supervised setting. After reading and returning the
blog, each participant was asked to (a) summarise
the blog they had just read in 5 lines, (b) state what
they found most interesting, and (c) state what they
did not like about the blog. These textual responses
were manually coded for whether the four behaviour
types (site fidelity, exploration, feeding and social)
were identified by each participant.
To gauge the participants? perceptions of the kite,
we used two methods. First, we asked the participant
to answer four questions that tested various aspects
of their willingness to engage with red kite conser-
vation:
Q1 Would you be willing to contribute money to a char-
ity that tries to protect kites?
Q2 The use of rat poison also leads to the death of kites
that feed on the bodies of these rats. Would you be
willing to sign a campaign against rat poison?
Q3 Should governments allocate more money than they
do currently to protect kites from extinction?
Q4 Write your email if you wish to be sent more blogs.
Further to this, participants were asked to assess
the red kite?s personality. We follow (Gosling et al,
2003), who use the 44 question Big Five Inventory
(BFI) (John et al, 1991; John et al, 2008) to as-
sess the personality of dogs. We are interested in
whether readers did assign personalities to the red
kite in the blog and, if so, what these personality
profiles looked like.
5.2 Results
We now analyse the extent to which our participants
were informed about red kite ecology as well as how
willing they were to engage with conservation ef-
forts and how they perceived the species.
5.2.1 Informativeness
More than half the participants identified feed-
ing behaviour (61%) and social (54%) behaviour.
The other two ecological concepts were not men-
tioned explicitly in the blog that participants read,
but needed to be inferred. Around a quarter of par-
ticipants managed to infer the notion of site fidelity
122
(23%), the most difficult of the concepts, and 41%
inferred exploratory behaviour.
5.2.2 Engagement
39% provided their email address to receive fur-
ther blogs (the only real commitment), and an equal
number expressed willingness to contribute money
for red kite conservation efforts. 85% expressed
willingness to sign a campaign against rat poisoning,
and 61% wanted increased government spending for
red kite conservation.
We detected a correlation between re-
call/inference of behaviours and willingness to
engage (plotting total number of behaviours re-
called/inferred by each participant against the total
number of engagement questions answered affirma-
tively, rpearson = 0.31; p < 0.005; n = 80). One
interpretation of this result is that greater insights
into the life of this bird has positively influenced
the reader?s perceptions of it. Further qualitative
studies are needed to substantiate this, but we view
this result as evidence in favour of incorporating
ecological insights into the blogs.
5.2.3 Perception
Table 1 shows the big five personality traits as-
signed to Red Baroness by participants. The BFI is
constructed such that being non-committal about the
44 trait questions would result in scores of 3. The
ability of readers to assign human personality traits
(significantly different from 3.0) to the red kite indi-
cates a willingness to anthromorphise the bird. The
last column shows the average personality of 21 year
old humans (from Srivastava et al (2003)), which is
the same age group as our participants. The values
for extroversion, agreeableness and conscientious-
ness are very similar, and the kite has lower neu-
roticism and openness.
6 Implications for NLG
The above study indicates that it is possible to use
narratives based on satellite tag data to communi-
cate ecological insights as well as create a positive
perception of the species in the readers? minds. To
generate texts that are fluent and engaging enough
that readers will be both informed and entertained
by them poses challenges that are sharply differ-
ent from the ones facing most data-to-text systems,
Trait Red Kite Conf. Int. 21 yo
Extroversion 3.28 3.07?3.48 3.25
Agreeableness 3.64 3.47?3.80 3.64
Conscientiousness 3.48 3.26?3.69 3.45
Neuroticism 2.60 2.41?2.80 3.32
Openness 3.29 3.11?3.47 3.92
Table 1: Big five personality traits of Red Baroness with
99.9% confidence intervals, compared to average 21 year
olds (6076 people) (Srivastava et al, 2003)
whose primary purpose is to offer decision support.
Our goals are more similar to those of Dial Your
Disc (Van Deemter and Odijk, 1997), with the added
requirement that texts should be easy to read. For
instance, ecological concepts (such as site fidelity)
could be communicated by explicitly defining them.
However, we would prefer these to be inferred from
more engaging narratives.
The blogs currently created by the charity (cf.
Section 1) are, stripped down to their essence, a se-
quence of locations. We propose to interlay these
sequences of locations with descriptions of red kite
behaviours, broadly categorised as fidelity, explo-
ration, feeding or social. Algorithm 1 outlines the
planning process. We have developed an initial pro-
totype that implements this for our simplified world.
Using template based generation, we can automati-
cally generate blogs such as the following for arbi-
trary sequences of locations in our simplified world:
This week Red Baroness continued to feel like
stretching her wings. On Monday she was
seen in the fields by the lake, calling out to
other kites. On Tuesday and Wednesday she
stayed along the road, looking for roadkill on
the country lanes. On Thursday she returned
to the fields by the lake ? clearly there was
plenty to eat there.
To scale this up to the real world, work is in
progress to augment our data analysis component by
using a variety of GIS data to map geo-coordinates
to habitat, terrain and demographic features from
which we can identify relevant kite behaviours.
Our remaining challenges are to (a) compile a
large list of red kite behaviours, (b) use paraphras-
ing approaches to create variety in descriptions of
behaviour and (c) develop means to interweave more
123
1. Identify place names of interest to the user among
the many GIS locations frequented by the red kite
2. For each place of interest (ordered by time):
(a) describe place in terms of relevant geographi-
cal features
(b) describe one or two behaviours (feeding or so-
cial) associated with any of these features
(c) make a reference to any exploratory behaviour
or site fidelity if identified from previous se-
quence.
Algorithm 1: Generate a blog about a red kite
complex behaviours, such as mating, into the narra-
tives. There is ongoing interdisciplinary work into
each of the above. Variation is likely to be critical to
the endeavour as these blogs are aimed at engaging
the reader, not just at presenting information. This
can be achieved both by expanding the range of be-
haviours we describe, and the range of ways we can
realise these through language.
7 Conclusions
This paper reports a study that informs the appli-
cation of NLG technologies to conservation efforts
centred around public engagement. We report on
findings which indicate that it is possible to use nar-
ratives loosely based on satellite tag data to com-
municate ecological insights as well as to create a
positive perception of the species in readers? minds.
This informs an approach to automating the creation
of blogs from satellite-tagged red kites by interleav-
ing sequences of locations with descriptions of be-
haviour. A proof of concept system has been devel-
oped for a simplified world, and is in the process of
being scaled up to the real world, using GIS data.
Acknowledgments
This research is supported by an award made by
the RCUK Digital Economy programme to the
dot.rural Digital Economy Hub; award reference:
EP/G066051/1.
References
R. Black, J. Reddington, E. Reiter, N. Tintarev, and
A. Waller. 2010. Using nlg and sensors to support
personal narrative for children with complex commu-
nication needs. In Proceedings of the NAACL HLT
2010 Workshop on Speech and Language Processing
for Assistive Technologies, pages 1?9. Association for
Computational Linguistics.
E. Goldberg, N. Driedger, and R.I. Kittredge. 1994. Us-
ing natural-language processing to produce weather
forecasts. IEEE Expert, 9(2):45?53.
S.D. Gosling, V.S.Y. Kwan, and O.P. John. 2003. A
dog?s got personality: a cross-species comparative
approach to personality judgments in dogs and hu-
mans. Journal of Personality and Social Psychology,
85(6):1161.
O.P. John, E.M. Donahue, and R.L. Kentle. 1991. The
big five inventoryversions 4a and 54. Berkeley: Uni-
versity of California, Berkeley, Institute of Personality
and Social Research.
O.P. John, L.P. Naumann, and C.J. Soto. 2008. Paradigm
shift to the integrative big five trait taxonomy. Hand-
book of personality: Theory and research, pages 114?
158.
S. Mahamood, E. Reiter, and C. Mellish. 2008. Neona-
tal intensive care information for parents an affec-
tive approach. In Computer-Based Medical Systems,
2008. CBMS?08. 21st IEEE International Symposium
on, pages 461?463. IEEE.
M. Molina, A. Stent, and E. Parodi. 2011. Generating
automated news to explain the meaning of sensor data.
Advances in Intelligent Data Analysis X, pages 282?
293.
M. O?Donnell, C. Mellish, J. Oberlander, and A. Knott.
2001. Ilex: an architecture for a dynamic hypertext
generation system. Natural Language Engineering,
7(3):225?250.
F. Portet, E. Reiter, A. Gatt, J. Hunter, S. Sripada,
Y. Freer, and C. Sykes. 2009. Automatic generation of
textual summaries from neonatal intensive care data.
Artificial Intelligence, 173(7-8):789?816.
J. Reddington, E. Reiter, N. Tintarev, R. Black, and
A. Waller. 2011. ?Hands Busy, Eyes Busy?: Generat-
ing Stories from Sensor Data for Automotive applica-
tions. In Proceedings of IUI Workshop on Multimodal
Interfaces for Automotive Applications.
S. Srivastava, O.P. John, S.D. Gosling, and J. Potter.
2003. Development of personality in early and middle
adulthood: Set like plaster or persistent change?. Jour-
nal of Personality and Social Psychology, 84(5):1041.
M. Theune, E. Klabbers, J.R. de Pijper, E. Krahmer, and
J. Odijk. 2001. From data to speech: a general ap-
proach. Natural Language Engineering, 7(01):47?86.
K. Van Deemter and J. Odijk. 1997. Context modeling
and the generation of spoken discourse. Speech Com-
munication, 21(1-2):101?121.
124
INLG 2012 Proceedings of the 7th International Natural Language Generation Conference, pages 146?149,
Utica, May 2012. c?2012 Association for Computational Linguistics
Content Selection From Semantic Web Data
Nadjet Bouayad-Agha1
Gerard Casamayor1
Leo Wanner1,2
1DTIC, University Pompeu Fabra
2Institucio? Catalana de Recerca i Estudis Avanc?ats
Barcelona, Spain
firstname.lastname@upf.edu
Chris Mellish
Computing Science
University of Aberdeen
Aberdeen AB24 3UE, UK
c.mellish@abdn.ac.uk
Abstract
So far, there has been little success in Natural
Language Generation in coming up with gen-
eral models of the content selection process.
Nonetheless, there has been some work on
content selection that employ Machine learn-
ing or heuristic search. On the other side, there
is a clear tendency in NLG towards the use of
resources encoded in standard Semantic Web
representation formats. For these reasons, we
believe that time has come to propose an initial
challenge on content selection from Semantic
Web data. In this paper, we briefly outline the
idea and plan for the execution of this task.
1 Motivation
So far, there has been little success in Natural Lan-
guage Generation in coming up with general mod-
els of the content selection process. Most of the
researchers in the field agree that this lack of suc-
cess is because the knowledge and context (commu-
nicative goals, user profile, discourse history, query,
etc) needed for this task depend on the application
domain. This often led in the past to template-
or graph-based combined content selection and dis-
course structuring approaches operating on idiosyn-
cratically encoded small sets of input data. Fur-
thermore, in many NLG-applications, target texts
and sometimes even empirical data are not avail-
able, which makes it difficult to employ empirical
approaches to knowledge elicitation. Nonetheless,
during the last decade, there has been a steady flow
of new work on content selection that employed Ma-
chine learning (Barzilay and Lapata, 2005; Duboue
and McKeown, 2003; Jordan and Walker, 2005;
Kelly et al, 2009), heuristic search (O?Donnell et
al., 2001; Demir et al, 2010; Mellish and Pan,
2008), or a combination thereof (Bouayad-Agha et
al., 2011). All of these strategies can deal with large
volumes of data.
On the other side, there is a clear tendency in NLG
towards the use of resources encoded in terms of
standard Semantic Web representation formats such
as OWL and RDF, e.g., (Wilcock and Jokinen, 2003;
Bontcheva and Wilks, 2004; Mellish and Pan, 2008;
Power and Third, 2010; Bouayad-Agha et al, 2011;
Dannells et al, 2012), to name but a few. However,
although most of these works make a good attempt
at realisation, the problem of content determination
from Semantic Web data is relatively untouched.
For these reasons, we believe that the time has
come to bring together researchers working on (or
interested in working on) content selection to par-
ticipate in a challenge for this task using standard
freely available web data as input. The availability
of open modular multi-domain multi-billion triple
data and of open ontological resources (Bizer et al,
2009) presented in a standard knowledge represen-
tation formalism make semantic web data a natural
choice for such a challenge.
As will be presented below, this initial challenge
presents a relatively simple content selection task
with no user model and a straightforward commu-
nicative goal so that people are encouraged to take
part and motivated to stay on for later challenges, in
which the task will be successively enhanced from
gained experience.
A content determination challenge would be a
chance to (i) directly compare the performance of
146
different types of content selection strategies; (ii)
contribute towards developing a standard ?off-the-
shelf? content selection module; and (iii) contribute
towards a standard interface between text planning
and linguistic generation.
To get the widest reception possible, the challenge
will be open to any approach, be it template-, rule-
or heuristic-based, or empirical. Furthermore, it will
be advertised in the Semantic Web Community to
get contributors from other horizons, see, e.g., (Dai
et al, 2010).
In what follows, we briefly outline the idea and
plan for the execution of the challenge. In Section 2,
we outline a description of the task. In Section 3,
the data and domain that will be used are presented.
Section 4 describes how this data is to be prepared
for the task, and Section 5 how it will be released to
the participants. In Section 6, we sketch the eval-
uation including the preparation of the evaluation
dataset. Section 7 gives a proposed schedule for
each of the tasks involved in organizing the chal-
lenge. Finally, in Section 8, we provide short bi-
ographies of the members of the organization team,
focusing on their experience in the proposed task.
2 Task Description
The core of the task to be addressed can be formu-
lated as follows:
Build a system which, given a set of RDF
triples containing facts about a celebrity
and a target text (for instance, a wikipedia-
style article about that person), selects
those triples that are reflected in the target
text.
The participants are also free to consider the se-
mantics defined by the data sources in their ap-
proach, rely on additional resources like ontologies
from other sources, or disregard the semantics com-
pletely.
The implemented system should output its results
in a predefined standard format that can be used for
automatic evaluation.
It could be that the RDF data does not contain ev-
erything that would ideally be included in such an
article, but that is ignored here. The task consists in
selecting content that is communicated in the target
text.
3 The data
The domain will be constituted by short biographies
of famous people. This is an interesting domain for
the challenge because Semantic Web data and corre-
sponding texts for this domain are available in large
quantities (e.g., DBPedia or Freebase for the data
and many other sources for biography texts, among
them Wikipedia).
The data will consist, for each famous person, of
a pair of RDF-triple set and associated text(s). For
each pair, the RDF data will include both informa-
tion communicated and excluded from the text. The
text may convey information not present in the RDF-
triples, but this will be kept to a minimum, always
subject to using naturally-occurring texts. All pairs
should contain enough RDF-triples and text to make
the pair interesting for the content selection task.
When choosing data for the challenge, we will
prefer semantic contents classified under consistent
ontologies over plain Linked Data with no explicit
semantics. The semantics of the RDF data (vocab-
ularies, ontologies) will be provided, preferably en-
coded in Semantic Web standards (e.g., in RDFS or
OWL).
4 Data Preparation
The task of data preparation consists in 1) data gath-
ering and preparation, which is to be carried out by
the organizers, and 2) working dataset selection and
annotation, which is to be carried out by both the
organizers and participants.
4.1 Data gathering and preparation
This preparatory stage consists in choosing the
repository sources, downloading the relevant on-
tologies (to the extent those will be provided), and
downloading and pairing the data and associated
texts (= the paired corpus).
4.2 Working Dataset selection and annotation
The participants will be asked to participate in a pre-
liminary task consisting in marking which triples are
included in the text given a subset of the paired cor-
pus (the size of the subset still has to be decided).
This task could be supported by some automatic
anchoring techniques such as used in (Duboue and
McKeown, 2003; Barzilay and Lapata, 2005). The
147
objectives of the task are threefold: (1) to provide all
participants with a common set of ?correct answers?
to be exploited in their approach, (2) to familiarize
the participants with the nature of the contents, their
semantics and the texts, and (3) to provide the task
with a ceiling for the evaluation, i.e. inter-annotator
agreement.
Annotation guidelines will be needed to ensure
that all participants follow the same procedure when
annotating texts. For this purpose, an early docu-
ment will be produced detailing the procedure to-
gether with examples and descriptions of relevant
problems such as ambiguities in the annotation. The
guidelines will be improved in multiple stages of an-
notation and revision with the goal of maximizing
inter-annotator agreement.
5 Data release
The participants in the challenge will be given ac-
cess to the set of all correct answers and a large
portion of the non-marked paired corpus, as well as
their semantics (i.e., ontologies and the like). The
remaining unseen, non-marked set will be kept for
evaluation.
6 Evaluation
The evaluation consists of 1) a preparatory stage for
selecting and annotating the evaluation dataset, and
2) an evaluation stage.
6.1 Evaluation dataset selection and annotation
Once all participants have submitted their exe-
cutable to solve the task, the evaluation set will be
processed. If timing is tight, however, this could be
done whilst the participants are still working on the
task or extra effort (for instance, from the organiz-
ers) could be brought in. A subset of the data is
randomly selected and annotated with the selected
triples by the participants. This two-stage approach
to triple selection annotation is proposed in order to
avoid any bias on the evaluation data.
6.2 Evaluation
Each executable is run against the test corpus and the
selected triples evaluated against the gold triple se-
lection set. Since this is formally a relatively simple
task of selecting a subset of a given set, we will use
for evaluation standard precision, recall and F mea-
sures. In addition, other appropriate metrics will be
explored?for instance, certain metrics for extrac-
tive summarisation (which is to some extent a simi-
lar task).
The organizers will explore whether it will be fea-
sible to select and annotate some test examples from
a different corpus and have the systems evaluated on
these as a separate task.
7 Schedule
Table 1 presents the different tasks, protagonists and
the schedule involved in the organization of the chal-
lenge. The challenge proper will take place between
November 2012 and May/June 2013.
8 Organizers
Nadjet Bouayad-Agha has been a lecturer and re-
searcher at DTIC, UPF, since 2002. She obtained
her PhD on Text Planning in 2001 from the Univer-
sity of Brighton and has been working ever since her
postgraduate studies at the University of Paris VII in
NLG, more specifically on Text Planning. In recent
years her focus has been on how to exploit semantic
web representations and technologies for Text Plan-
ning in general and content selection in particular.
Gerard Casamayor is a PhD student at DTIC,
UPF, working on text planning from general-
purpose semantic data. His main interests are ma-
chine learning and interactive, collaborative text
planning. As part of his thesis, he is developing a
text planning approach that can be trained directly
by domain experts, minimizing the need of encoding
or annotating prior knowledge about how to solve
the task.
Chris Mellish has been a professor at the Univer-
sity of Aberdeen since 2003, when he moved from a
similar position at the University of Edinburgh. He
has been doing research in NLG since 1984 and or-
ganised the second European NLG workshop. His
work on content selection includes the opportunis-
tic planning approach used by the ILEX system and
a rule-based approach to content selection from se-
mantic web data presented in ENLG 2011.
Leo Wanner has been ICREA Research Profes-
sor at DTIC, UPF, since 2005. Before, he was
148
What? Who? When?
Data gathering and preparation Organizers Summer 2012
Working dataset selection and annotation Organizers and Participants Sept/Oct 2012
Data Release Organizers November 2012
Evaluation dataset selection and annotation Organizers and Participants May 2013
Evaluation Organizers June 2013
Publication@INLG Organizers August 2013
Table 1: Content Selection Challenge Organization Schedule
affiliated as Assistant Professor with the Univer-
sity of Stuttgart. Wanner is involved in research
on multilingual text generation since the late 80ies.
Among his research foci are user-oriented con-
tent selection and the interface between language-
independent ontology-based and linguistic represen-
tations in text generation.
References
Regina Barzilay and Mirella Lapata. 2005. Collec-
tive Content Selection for Concept-to-Text Genera-
tion. Proceedings of the Joint Human Language Tech-
nology and Empirical Methods in Natural Language
Processing Conferences (HLT/EMNLP-2005) Van-
couver, Canada.
Christian Bizer, Tom Heath and Tim Berners-Lee 2009.
Linked Data - The Story So Far. International Journal
on Semantic Web and Information Systems 5(3) 1?22
Kalina Bontcheva and Yorick Wilks 2004. Automatic
Report Generation from Ontologies: the MIAKT ap-
proach Nineth International Conference on Appli-
cations of Natural Language to Information Systems
(NLDB?2004) 324?335.
Nadjet Bouayad-Agha, Gerard Casamayor and Leo Wan-
ner. 2011. Content selection from an ontology-based
knowledge base for the generation of football sum-
maries Proceedings of the 13th European Workshop
on Natural Language Generation (ENLG?2011) 72?
81 Nancy, France.
Yintang Dai, Shiyong Zhang, Jidong Chen, Tianyuan
Chen and Wei Zhang. 2010 Semantic Network Lan-
guage Generation based on a Semantic Networks Seri-
alization Grammar World Wide Web 13:307341
Dana Danne?lls, Mariana Damova, Ramona Enache and
Milen Chechev. 2012 Multilingual Online Genera-
tion from Semantic Web Ontologies Proceedings of
the 21st International Conference on World Wide Web
(WWW?12) 239?242
Seniz Demir, Sandra Carberry and Kathleen F. McCoy.
2010. A Discourse-Aware Graph-Based Content-
Selection Framework. Proceedings of the Interna-
tional Language Generation Conference. Sweden.
Pablo A. Duboue and Kathleen R. McKeown. 2003. Sta-
tistical Acquisition of Content Selection Rules for Nat-
ural Language Generation. Proceedings of the 2003
conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP). Sapporo, Japan.
Dimitrios Galanis and Ion Androutsopoulos. 2007. Gen-
erating Multilingual Personalized Descriptions from
OWL Ontologies on the Semantic Web: the Natu-
ralOWL System. Proceedings of the Eleventh Eu-
ropean Workshop on Natural Language Generation
(ENLG07)
Pamela W. Jordan and Marilyn A. Walker 2005 Learning
content selection rules for generating object descrip-
tions in dialogue Journal of Artificial Intelligence Re-
search 24, 157?194.
Colin Kelly, Ann Copestake, and Nikiforos Karamanis.
2009 Investigating content selection for language gen-
eration using machine learning. Proceedings of the
12th European Workshop on Natural Language Gen-
eration.. 130?137.
Chris Mellish and Jeff Z. Pan. 2008 Language Di-
rected Inference from Ontologies. Artificial Intelli-
gence. 172(10):1285-1315.
Mick ODonnell, Chris Mellish, Jon Oberlander, and Alis-
tair Knott. 2001. ILEX: an architecture for a dynamic
hypertext generation system. Natural Language Engi-
neering. 7(3):225?250.
Richard Power and Allan Third 2010 Expressing OWL
axioms by English sentences: dubious in theory, fea-
sible in practice Proceedings of the 23rd Interna-
tional Conference on Computational Linguistics (CI-
CLING?01). 1006?1013.
Graham Wilcok and Kristiina Jokinen 2003 Generat-
ing Responses and Explanations from RDF/XML and
DAML+OIL. IJCAI03 Workshop on Knowledge and
Reasoning in Practical Dialogue Systems. 58?63.
149
Proceedings of the 14th European Workshop on Natural Language Generation, pages 98?102,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
Overview of the First Content Selection Challenge from Open Semantic
Web Data
Nadjet Bouayad-Agha1
Gerard Casamayor1
Leo Wanner1,2
1DTIC, University Pompeu Fabra
2Institucio? Catalana de Recerca i Estudis Avanc?ats
Barcelona, Spain
firstname.lastname@upf.edu
Chris Mellish
Computing Science
University of Aberdeen
Aberdeen AB24 3UE, UK
c.mellish@abdn.ac.uk
Abstract
In this overview paper we present the out-
come of the first content selection chal-
lenge from open semantic web data, fo-
cusing mainly on the preparatory stages
for defining the task and annotating the
data. The task to perform was described
in the challenge?s call as follows: given a
set of RDF triples containing facts about
a celebrity, select those triples that are re-
flected in the target text (i.e., a short bi-
ography about that celebrity). From the
initial nine expressions of interest, finally
two participants submitted their systems
for evaluation.
1 Introduction
In (Bouayad-Agha et al, 2012), we presented the
NLG challenge of content selection from seman-
tic web data. The task to perform was described
as follows: given a set of RDF triples contain-
ing facts about a celebrity, select those triples that
are reflected in the target text (i.e., a short biogra-
phy about that celebrity). The task first required
a data preparation stage that involved the follow-
ing two subtasks: 1) data gathering and prepara-
tion, that is, deciding which data and texts to use,
then downloading and pairing them, and 2) work-
ing dataset selection and annotation, that is, defin-
ing the criteria/guidelines for determining when a
triple is marked as selected in the target text, and
producing a corpus of triples annotated for selec-
tion.
There were initially nine interested participants
(including the two organizing parties). Five of
which participated in the (voluntary) triple anno-
tation rounds.1 In the end, only two participants
submitted their systems:
1We would like to thank Angelos Georgaras and Stasinos
Konstantopoulos from NCSR (Greece) for their participation
in the annotation rounds.
UA: Roman Kutlak, Chris Mellish and Kees van
Deemter. Department of Computing Science
, University of Aberdeen, Scotland (UK).
UIC: Hareen Venigalla and Barbara Di iEugenio.
Department of Computer Science, University
of Illinois at Chicago (USA).
Before the presentation of the baseline evalua-
tion of the submitted systems and the discussion
of the results (Section 4), we outline the two data
preparation subtasks (Sections 2 and 3). In Sec-
tion 5, we then sketch some conclusions with re-
gard to the achievements and future of the con-
tent selection task challenge. More details about
the data, annotation and resources described in this
overview, as well as links for downloading the data
and other materials (e.g., evaluation results, code,
etc.) are available on the challenge?s website.2
2 Data gathering and preparation
We chose Freebase as our triple datastore.3,4 We
obtained the triple set for each person in the Turtle
format (ttl) by grepping the official Freebase RDF
dump released on the 30th of December 2012 for
all triples whose subject is the person?s URI; cer-
tain meta-data and irrelevant triples (i.e., triples
with specific namespaces such as ?base? or ?com-
mon?) have been filtered out.
Each triple set is paired with the person?s sum-
mary biography typically available in Wikipedia,
which consists of the first paragraph(s) preceding
the page?s table of contents5
Our final corpus consists of 60000+ pairs, all of
which follow two restrictions that are supposed to
2http://www.taln.upf.edu/cschallenge2013/
3http://www.freebase.com
4For a comparison between Freebase and DBPedia, see
http://wiki.freebase.com/wiki/DBPedia.
5For example, the first four paragraphs in the follow-
ing page constitute the summary biography of that person:
http://en.wikipedia.org/wiki/George Clooney.
98
maximize the chances of having interesting pairs
with sufficient original and selected input triples
for the challenge. Firstly, the number of unique
predicates in the input ttl must be greater than
10. The number 10 is estimated based on the
fact that a person?s nationality, date and place of
birth, profession, type and gender are almost al-
ways available and selected, such that we need a
somewhat large set to select content from in or-
der to make the task minimally challenging. Sec-
ondly, the Wikipedia-extracted summary biogra-
phy must contain more than 5 anchors and at least
20% of the available anchors, where an anchor is
a URI in the text (i.e., external href attribute value
in the html) pointing to another Wikipedia article
which is directly related to that person. Given that
most Freebase topics have a corresponding DBPe-
dia entity with a Wikipedia article, anchors found
in the introductory text are an indicator of potential
relevant facts available in Freebase and are com-
municated in the text. In other words, the anchor
threshold restriction is useful to discard pairs with
very few triples to annotate. We found this crite-
rion more reliable than the absolute length of the
text which is not necessarily proportional with the
number of triples available for that person.
3 Working Dataset selection and
annotation
The manual annotation task consisted in emulat-
ing the content selection task of a Natural Lan-
guage Generation system, by marking in the triple
dataset associated with a person the triples predi-
cated in the summary biography of that person ac-
cording to a set of guidelines. We performed two
rounds of annotations. In the first round, partic-
ipants were asked to select content for the same
three celebrities. The objectives of this annota-
tion, in which five individuals belonging to four
distinct institutions participated, were 1) for par-
ticipants to get acquainted with the content selec-
tion task envisaged, the domain and guidelines,
2) to validate the guidelines, and 3) to formally
evaluate the complexity of the task by calculat-
ing inter-annotator agreement. For the latter we
used free-marginal multi-rater Kappa, as it seemed
suited for the annotation task (i.e. independent rat-
ings, discrete categories, multiple raters, annota-
tors are not restricted in how they distribute cat-
egories across cases) (Justus, 2005). We obtained
an average Kappa of 0.92 across the three pairs for
the 5 annotators and 2 categories (selected, not se-
lected), which indicates a high level of agreement
and therefore validates our annotation guidelines.
Our objective for the second round of annota-
tions was to obtain a dataset for participants to
work with. In the end, we gathered 344 pairs from
5 individuals of 5 distinct institutions. It should be
noted that although both rounds of annotations fol-
low the anchor restriction presented in Section 2,
the idea to set a minimum number of predicates
for the larger corpus of 60000+ pairs came forth
after analysing the results of the second round and
noting the data sparsity in some pairs. In what fol-
lows, we detail how the triples were presented to
human annotators and what were the annotation
criteria set forth in the guidelines.
3.1 Data presentation
A machine-readable triple consists of a subject
which is a Freebase machine id (mid), a predicate
and an object which can either be a Freebase mid
or a literal, as shown in the following two triples:
ns:m.0dvld
ns:people.person.spouse_s
ns:m.02kknf3 .
ns:m.0dvld
ns:people.person.date_of_birth
"1975-10-05"??xsd:datetime .
Triples were transformed into a human-readable
form. In particular, each mid in object position
(e.g., 02kknf3) was automatically mapped onto
an abbreviated description of the Freebase topic
it refers to. Thus, the triples above have been
mapped onto a tabular form consisting of (1) pred-
icate, (2) object description, (3) object id, and (4)
object types (for literals):
(1) /people/person/spouse_s
(2) "1998-11-22 - Jim Threapleton -
2001-12-13 - Marriage -
Freebase Data Team - Marriage"
(3) /m/02kknf3
(1) /people/person/date_of_birth
(2) value
(3) "1975-10-05"
(4) "datetime"
For each triple thus presented, annotators were
asked to mark 1) whether it was selected, 2) in
which sentence(s) of the text did it appear, and
3) which triples, if any, are its coreferents. Two
triples are coreferent if their overlap in meaning is
such that either of them can be selected to repre-
sent the content communicated by the same text
99
fragment and as such should not count as two sep-
arate triples in the evaluation. Thus, the same text
might say He is probably best known for his stint
with heavy metal band Godsmack and He has also
toured and recorded with a number of other bands
including Detroit based metal band Halloween
?The Heavy Metal Horror Show? . . . , thus refer-
ring in two different sentences to near-equivalent
triples /music/artist/genre ??Heavy
metal" and /music/artist/genre
??Hard rock".
3.2 Annotation criteria
Annotators were asked to first read the text care-
fully, trying to identify propositional units (i.e.,
potential triples) and then to associate each iden-
tified propositional unit with zero, one or more
(coreferent) triples according to the following
rules:
Rule 1. One cannot annotate facts that are not
predicated and cannot be inferred from predicates
in the text. In other words, all facts must be
grounded in the text. For example, in the sentence
He starred in Annie Hall, the following is pred-
icated: W.H.has profession actor and
W.H. acted in film Annie Hall. The
former fact can be inferred from the latter. How-
ever, the following is not predicated: (1) Person
has name W.H., (2) W.H. is Male, and (3)
W.H. is Person.
Rule 2. In general, one can annotate more
generic facts if they can be inferred from more
specific propositions in the text, but one cannot
annotate specific facts just because a more gen-
eral proposition is found in the text. In the exam-
ple He was a navigator, we can mark the triples
Person has profession Sailor as well
as Person has profession Navigator
(we would also mark them as coreferent). How-
ever, given the sentence He was a sailor, we can-
not mark the triple Person has profession
Navigator, unless we can infer it from the text
or world knowledge.
Rule 3. One can annotate specific facts from a
text where the predicate is too vague or general if
the facts can be inferred from the textual context,
from the available data, or using world knowledge.
This rule subsumes four sub-cases:
Rule 3.1. The predicate in the proposition is too
vague or general and can be associated with mul-
tiple, more specific triples. In this case, do not
select any triple. In the example Film A was a
great commercial success, we have several triples
associating the celebrity with Film A, as direc-
tor, actor, writer, producer and composer and none
of them with a predicate resembling ?commercial
success?. In this case there are no triples that can
be associated with the text.
Rule 3.2. The predicate in the proposition is
too vague or general, but according to the data
there is just one specific triple it can be associated
with. In this case, select that triple. In the ex-
ample Paris released Confessions of an Heiress,
the term released could be associated with au-
thored, wrote or published. However, there is only
one triple associating that subject with that object,
which matches one of the interpretations (i.e., au-
thoring) of the predicate. Therefore that triple can
be selected.
Rule 3.3. The predicate in the proposition is
too vague or general, but one or more specific
triples can be inferred using world knowl-
edge. In this case, select all. The sentence
He is also a jazz clarinetist who performs
regularly at small venues in Manhattan, can
be associated with the available triples W.H.
profession Clarinetist and W.H.
music/group member/instruments played
Clarinet, even though for this latter triple
the person being in a group is not mentioned
explicitly. However, this can be inferred from
basic world knowledge.
Rule 3.4. The predicate in the proposition is
too vague or general, but one or more specific
triples can be inferred using the textual context.
In this case, select all. In the example By the
mid-1960s Allen was writing and directing films
. . . Allen often stars in his own films . . . Some of
the best-known of his over 40 films are Annie Hall
(1977) . . . , the relations of the person with the
film Annie Hall are that of writer, director and
actor, as supported by the previous text. There-
fore we would annotate facts stating that the per-
son wrote, directed and starred in Annie Hall.
However, we wouldn?t annotate composer or pro-
ducer triples if they existed.
Rule 4. A proposition can be associated
with multiple facts with identical or over-
lapping meanings. In the example, Woody
Allen is a musician, we have the triples
W.H occupation musician and W.H
profession musician, which have near
100
identical meanings. Therefore, we mark both
triples and indicate that they co-refer. The
sentence Woody Allen won prize as best director
for film Manhattan, on the other hand, can be
associated with non-coreferring triples W.H won
prize and W.H. directed Manhattan.
Rule 5. If the text makes reference to a set of
facts but it does not enumerate them explicitly, and
there is no reason to believe it makes reference to
any of them in particular, then do not annotate in-
dividual facts. Thus, sentence Clint Eastwood has
seven children does not warrant marking each of
the seven children triples as selected, given that
they are not enumerated explicitly.
Rule 6. If the text makes a clear and unam-
biguous reference to a fact, do not annotate any
other facts, even though they can be inferred from
it. In other words, as explained in Rule 1, all an-
notated triples must be grounded in the text. In
the sentence For his work in the films Unforgiven
(1992) and Million Dollar Baby (2004), Eastwood
won Academy Awards for Best Director and Pro-
ducer of the Best Picture, we can infer from world
knowledge that the celebrity was nominated prior
to winning the award in those categories. How-
ever, the text makes a clear reference only to the
fact that he won the award and there is no reason
to believe that it is also predicating the fact that the
celebrity was nominated.
4 Baseline evaluation
Briefly speaking, the UA system uses a general
heuristic based on the cognitive notion of com-
munal common ground regarding each celebrity,
which is approximated by scoring each lexicalized
triple (or property) associated with a celebrity ac-
cording to the number of hits of the Google search
API. Only the top-ranked triples are selected (Kut-
lak et al 2013). The UIC system uses a small
set of rules for the conditional inclusion of pred-
icates that was derived offline from the statistical
analysis of the co-occurrence between predicates
that are about the same topic or that share some
shared arguments; only the best performing rules
tested against a subset of the development set are
included (Venigalla and Di Eugenio, 2013).
For the baseline evaluation, we used the devel-
opment set obtained in the second round annota-
tion (see Section 3). However, we only consider
pairs obtained during the second round annotation
that 1) follow both restrictions presented in Sec-
Baseline UIC UA
Precision 49 64 47
Recall 67 50 39
F1 51 51 42
Table 1: Baseline evaluation results (%)
tion 2, and 2) have no coreferring triples. This
last restriction was added to minimize errors be-
cause we observed that annotators were not al-
ways consistent in their annotation of triple coref-
erence.6 We therefore considered 188 annotations
from the 344 annotations of the development set.
Of these, we used 40 randomly selected annota-
tions for evaluating the systems and 144 for es-
timating a baseline that only considers the top 5
predicates (i.e., the predicates most often selected)
and the type-predicate.7.
The evaluation results of the three systems
(baseline, UIC and UA) are presented in Table 1.
The figures in the table were obtained by compar-
ing the triples selected and rejected by each system
against the manual annotation. The performance
of the baseline is quite high. The UA system based
on a general heuristic scores lower than the base-
line, whilst the UIC system has a better precision
than the baseline, albeit a lower recall. This might
be due, as the UA authors observe in their sum-
mary (Venigalla and Di Eugenio, 2013), to ?the
large number of predicates that are present only
in a few files . . . [which] makes it harder to de-
cide whether we have to include these predicates
or not.?
5 Conclusions
We have given an overview of the first content se-
lection challenge from open semantic web data,
focusing on the rather extensive and challenging
technological and methodological work involved
in defining the task and preparing the data. Unfor-
tunately, despite agile participation in these early
6Type-predicate triples were filtered out of the annotated
files in the development set whilst they were included in the
large corpus made available to the candidates. Therefore,
we added type-predicate triples in the development set a
posteriori for this evaluation. These type-predicate triples
might be coreferring with other triples, say ns:m.08rd51
ns:type.object.type ns:film.actor and
ns:m.08rd5 people/person/profession
"Actor" /m/02hrh1q . Nonetheless, this was not
taken into account in the evaluation.
7The top 5 predicates were (in descending order of fre-
quency): music track, film actor, profession, date of birth and
nationality
101
preparatory stages, the number of submitted sys-
tems was limited. Both of the presented systems
were data-intensive in that they usedeither a pool
of textual knowledge or the corpus of triple data
provided by the challenge in order to select the
most relevant data.
Unlike several previous challenges that involve
more traditional NLG tasks (e.g., surface realiza-
tion, referring expression generation), content se-
lection from large input semantic data is a rela-
tively new research endeavour in the NLG com-
munity that coincides with the rising interest in
statistical approaches to NLG and dates back, to
the best of our knowledge, to (Duboue and McK-
eown, 2003). Furthermore, although we had ini-
tially planned to produce a training set for the
task, the cost of manual annotation turned out
to be prohibitive and the resulting corpus was
only fit for development and baseline evaluation.
Despite these setbacks, we believe that open se-
mantic web data is a promising test-bed and ap-
plication field for NLG-oriented content selec-
tion (Bouayad-Agha et al, 2013) and trust that
this first challenge has prepared the ground for
follow up challenges with a larger participation.
We would also like to encourage researchers from
NLG and Semantic Web research fields to exploit
the framework and materials developed during the
course of this challenge to advance research in
content selection.
References
Nadjet Bouayad-Agha, Gerard Casamayor, and Leo
Wanner. 2013. Natural Language Generation in the
Context of the Semantic Web. Submitted to the Se-
mantic Web Journal.
Nadjet Bouayad-Agha, Gerard Casamayor, Chris Mel-
lish, and Leo Wanner. 2012. Content Selection from
Semantic Web Data. INLG ?12 Proceedings of the
Seventh International Natural Language Generation
Conference. Pages 146-149.
Pablo A. Duboue and Kathleen R. McKeown. 2003.
Statistical Acquisition of Content Selection Rules
for Natural Language Generation Proceedings of
the Conference on Empirical Methods for Natural
Language Processing (EMNLP). Pages 121?128.
Randolph, Justus J. 2005. Free-marginal multirater
kappa (multirater Kfree): An alternative to fleiss
fixed-marginal multirater kappa. Presented as the
Joensuu University Learning and Instruction Sym-
posium.
Roman Kutlak, Chris Mellish and Kees van Deemter
2013. Content Selection Challenge University of
Aberdeen entry Proceedings of the 14th European
Natural Language Generation (ENLG) Workshop.
Hareen Venigalla and Barbara Di Eugenio. 2013. UIC-
CSC: The Content Selection Challenge Entry from
the University of Illinois at Chicago Proceedings
of the 14th European Natural Language Generation
(ENLG) Workshop.
102
Proceedings of the 14th European Workshop on Natural Language Generation, pages 152?156,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
MIME - NLG in Pre-Hospital Care
Anne H. Schneider Alasdair Mort Chris Mellish Ehud Reiter Phil Wilson
University of Aberdeen
{a.schneider, a.mort, c.mellish, e.reiter, p.wilson}@abdn.ac.uk
Pierre-Luc Vaudry
Universite? de Montre?al
vaudrypl@iro.umontreal.ca
Abstract
The cross-disciplinary MIME project aims
to develop a mobile medical monitoring
system that improves handover transac-
tions in rural pre-hospital scenarios be-
tween the first person on scene and am-
bulance clinicians. NLG is used to pro-
duce a textual handover report at any time,
summarising data from novel medical sen-
sors, as well as observations and actions
recorded by the carer. We describe the
MIME project with a focus on the NLG
algorithm and an initial evaluation of the
generated reports.
1 Introduction
Applications of Natural Language Generation
(NLG) in the medical domain have been manifold.
A new area where NLG could contribute to the im-
provement of services and to patient safety is pre-
hospital care: care delivered to a patient before ar-
rival at hospital. There are many challenges in de-
livering pre-hospital care, making it different from
care taking place in the controlled circumstances
of emergency departments or hospital wards.
Some Ambulance Services have developed in-
novative models to care for patients whilst an am-
bulance is en-route. Community First Responder
(CFR) schemes recruit volunteers from local com-
munities and give them the necessary training and
equipment to deal with a limited range of medical
emergencies. The premise is that even those with
basic first-aid skills can save a life. It is their task
to attend the casualty while waiting for the am-
bulance and to record their observations and ac-
tions on a paper patient report form (PRF). They
may also assess the patient?s physiological mea-
surements (e.g. heart rate). In practice, due to
time constraints, a verbal handover is performed
and the PRF is filled in later. Physiological mea-
surements may be written in ink on the back of a
protective glove, and are rarely passed on in any
systematic way.
The MIME (Managing Information in Medical
Emergencies)1 project is developing technology to
support CFRs in the UK when they respond to pa-
tients. The project aims to enable CFRs to capture
a greater volume of physiological patient data, giv-
ing them a better awareness of a patient?s medical
status so they can deliver more effective care.
There are two parts to our work: the use of novel
lightweight wireless medical sensors that are sim-
ple and quick to apply, and the use of novel soft-
ware that takes these inherently complex sensor
data, along with some other information inputted
by the user (e.g. patient demographics or actions
performed) on a tablet computer, and present it
very simply. We are working with two sensors that
provide measurements of the patient?s respiratory
rate, heart rate and blood oxygen saturation. Our
software can use NLG to produce a textual han-
dover report at any time. This can be passed to an
arriving paramedic to give a quick summary of the
situation and can accompany the patient to inform
later stages of care. We anticipate that our sys-
tem will also provide some basic decision support
based upon the patients clinical condition.
2 Related Work
Many situations arise in the medical domain where
vast amounts of data are produced and their correct
interpretation is crucial to the lives of patients. In-
terpreting these data is usually a demanding and
complex task. Medical data are therefore often
presented graphically or preferably in textual sum-
maries (Law et al, 2005) making NLG important
for various applications in the medical domain.
A number of systems address the problem of
presenting medical information to patients in a
form that they will understand. Examples are
1www.dotrural.ac.uk/mime
152
STOP (Reiter et al, 2003), PILLS (Bouayad-Agha
et al, 2002), MIGRANE (Buchanan et al, 1992),
and Healthdoc (Hirst et al, 1997). Other systems,
such as TOPAZ (Kahn et al, 1991) and Suregen
(Hu?ske-Kraus, 2003), aim to summarise informa-
tion in order to support medical decision-making.
In the case of MIME, the challenge is to sum-
marise large amounts of sensor data, in the context
of carer observations and actions, in a coherent
way that supports quick decision making by the
reader. The problem of describing the data relates
to previous work on summarising time series data
(e.g. (Yu et al, 2007)). In many ways, though, our
problem is most similar to that of Babytalk BT-
Nurse system (Hunter et al, 2012), which gener-
ates shift handover reports for nurses in a neona-
tal intensive care unit. The nature of the recipi-
ent is, however, different. Whereas BabyTalk ad-
dresses clinical staff in a controlled environment,
MIME is aimed at people with little training who
may have to deal with emergency situations very
quickly. Further, while BT-Nurse works with an
existing clinical record system, which does not al-
ways record all actions and observations which
ideally would be included in a report, in MIME
users enter exactly the information which MIME
needs. This simplifies the NLG task, at the cost of
adding a new task (interface construction).
3 The MIME project
In the first stage of MIME, we have developed
a desktop application to prototype the generation
of handover reports. We used simulated scenar-
ios, where a panel of medical experts determined
the sequence of events and predicted the stream of
data from the simulated sensors.
The generated reports must provide a quick
overview of the situation but at the same time be
sufficiently comprehensive, while the format must
enhance the readability. A general structure for
the handover reports was determined in a user-
centred development process together with ambu-
lance clinicians. After the demographic descrip-
tion of the casualty and incident details (entered by
the responder whenever they have an opportunity),
two sections of generated text follow: the initial
assessment section and the treatments and findings
section. The initial assessment contains informa-
tion on the patient gathered by the CFRs just after
the sensors are applied and also any observations
made during the first minute after the application
of the sensors. The treatment and findings section
is a report on the observations and actions of the
CFRs while they waited for the ambulance to ar-
rive. This includes a paragraph that sums up the
condition of the patient at the time of handover.
Using sensors to capture physiological data
continuously introduces the problem that irrele-
vant information needs to be suppressed in order
not to overload the ambulance clinicians and hin-
der interpretation. The NLG algorithm that gen-
erates short as well as comprehensive handover re-
ports accomplishes text planning in the two stages
of document planning and micro-planning (Re-
iter and Dale, 2000). Document planning is re-
sponsible for the selection of the information that
will be mentioned in the generated report. Events
that will be mentioned in the text are selected
and structured into a list of trees (similar to trees
in Rhetorical Structure Theory (Scott and Siecke-
nius de Souza, 1990)). In the micro-planning step
the structure of the document plan is linearised and
sentences are compiled using coordination and ag-
gregation.
Whereas some parts of the handover document
(e.g. patient demographics) are relatively stylised,
the main technically demanding part of the NLG
involves the description of the ?treatment and find-
ings?, which describes the events that happen
whilst the patient is being cared for and relevant
parts of the sensor data (see Figure 1). For this
section of the report, the document planning al-
gorithm is based on that of (Portet et al, 2007),
which identifies a number of key events and cre-
ates a paragraph for each key event. Events that
are explicitly linked to the key event or events that
happen at the same time are added to the relevant
paragraph. This is based on the earlier work of
(Hallett et al, 2006).
4 Evaluation
In an initial evaluation we sought to assess how
our reports would be received in comparison with
the current situation ? either short verbal reports
or paper report forms (PRFs)? and also in com-
parison with what might be regarded as a ?gold
standard? report produced by an expert.
Materials: Two videos were produced indepen-
dently of the NLG team, based on two scenarios
of medical incidents typical of a CFRs caseload.
These scenarios, a farm injury and chest pain, in-
cluded a short description of the incident, similar
153
At 02:12, after RR remained fairly
constant around 30 bpm for 4 minutes,
high flow oxygen was applied, she took
her inhaler and RR decreased to 27
bpm. However, subsequently RR once
more remained fairly constant around
30 bpm for 8 minutes.
At 02:15 she was feeling faint.
At 02:15 the casualty was moved.
At 02:17 the casualty was once more
moved.
Figure 1: Part of the ?Treatment and Findings? for an
asthma scenario.
to the initial information a CFR would receive, a
time line of events that happened before the ambu-
lance arrived as well as simulated sensor data from
the patient. The videos showed an actor in the
role of CFR and another as patient, with the sce-
nario time displayed in one corner. When the CFR
performed readings of the physiological measures
they were shown as subtitles.
The videos were presented to two CFRs and a
paramedic, who were asked to imagine themselves
in the situation of the CFR in the video, and to
produce a handover report. Each video was only
played once in order to produce more realistic re-
sults. We asked one CFR to construct a written
?verbal? handover for the first scenario and to fill
out a PRF for the other scenario, and the other
CFR to do the ?verbal? handover for the second
scenario and to fill out the PRF for the first. To
anonymise the PRF it was transcribed into a digi-
tal version. The paramedic received a blank sheet
of paper and was requested to produce a handover
report that he would like to receive from a CFR
when arriving at the scene. Based on the scenarios
we also generated two reports with the MIME sys-
tem. This process resulted in four reports for each
of the two scenarios, one transcribed verbal han-
dover and a PRF from a CFR, a written handover
report from a paramedic and the generated report.
Hypotheses: Our hypothesis was that the gen-
erated reports would improve on the current prac-
tice of verbal handovers and PRFs, and that
paramedics would perceive them to be more suit-
able, hence rank them higher than the CFRs? ver-
bal or PRF reports. The paramedic handover re-
port might be regarded as a gold standard pro-
duced by an expert and we were interested in how
the generated reports fared in comparison. Fur-
ther, we hoped to gain information on how to im-
prove our generated reports.
Participants: We approached paramedics in
the Scottish Ambulance Service to participate in
our study. Nine paramedics responded (eight male
and one female; age range 32?56 years with 10?24
years? service).
Procedure: Participants received an invitation
email with a link to a brief online survey and the
eight reports as attachments. After an introduction
and consent form they were forwarded to one of
the two scenario descriptions and asked to rank the
respective four reports. After that the participant
was asked to rate the accuracy, understandability
and usefulness of the generated report for this sce-
nario on a 5-point Likert scale ranging from very
good to very bad and to indicate what they liked
or disliked about it in a free text box. This process
was repeated for the second scenario.
4.1 Results
Ranking: An overview of the rankings can be
found in Table 1. Apart from the rankings of par-
ticipant 7 and 8, no large differences in how the
reports were ranked could be observed between
the two scenarios. We performed a Friedman
test (Friedman, 1937) (farm injury scenario: chi-
squared=4.3, df=3, p=0.23; chest pain scenario:
chi-squared=12.44, df=3, p=0.006): some reports
were ranked consistently higher or lower than oth-
ers. The verbal CFR report was ranked worst in all
but five cases. There is a high disparity in the rank-
ings for the PRF, which was ranked first on eight
occasions and in the other ten instances in third
or fourth place. The generated report was ranked
in first place only once, but eleven times in sec-
ond place and in third place the other six times. In
general the paramedic report, which was regarded
as the ?gold standard?, was ranked better than the
generated report, but in five cases the generated
report was ranked better.
Rating: An overview of the ratings for the gen-
erated reports can be found in Table 2. The rat-
ings for both scenarios were good on average, with
a majority of ratings lying between very good to
moderate. Only one rating (the accuracy of the
generated report for the farm injury scenario) was
bad; none was very bad. The ratings for the gen-
erated report of the chest pain scenario were on
average better than those for the farm injury sce-
nario. Accuracy had better ratings than usefulness
and understandability in both scenarios.
154
Participant: 1 2 3 4 5 6 7 8 9 med min max
farm injury scenario
Paramedic 2 2 3 1 1 3 3 2 1 2 1 3
Generated 3 3 2 2 2 2 2 3 2 2 2 3
CFR PRF 1 1 1 3 4 1 4 4 3 3 1 4
CFR verbal 4 4 4 4 3 4 1 1 4 4 1 4
chest pain scenario
Paramedic 2 2 3 1 1 2 2 1 1 2 1 3
Generated 3 3 2 2 2 3 1 2 2 2 1 3
CFR PRF 1 1 1 3 4 1 4 3 3 3 1 4
CFR verbal 4 4 4 4 3 4 3 4 4 4 3 4
Table 1: Overview of the ranking results (most preferred
(1) to least preferred (4)), median (med), maximum (max)
and minimum (min) values for the patient report form (CFR
PRF), paramedic report (Paramedic), generated report (gen-
erated) and verbal report (verbal CFR).
Participant: 1 2 3 4 5 6 7 8 9 med min max
farm injury scenario
accuracy 1 2 1 4 2 2 1 1 1 1 1 4
useful. 3 3 2 2 1 2 2 1 1 2 1 3
unders. 2 3 2 2 1 3 3 1 1 2 1 3
chest pain scenario
accuracy 2 2 1 1 1 3 1 2 1 1 1 3
useful. 2 3 2 1 1 2 1 1 1 1 2 3
unders. 2 3 2 1 1 3 2 1 1 2 1 3
Table 2: Overview of the rating results, median (med), max-
imum (max) and minimum (min) values for accuracy, useful-
ness (useful.) and understandability (unders.) of the gener-
ated reports, on a Likert scale (very good (1) to very bad (5)).
4.2 Discussion
We hypothesised that the generated reports would
fare better than the verbal handovers and the PRFs.
Results confirm a preference for the generated re-
ports over the verbal handover. The paramedic
reports, which were regarded as our ?gold stan-
dard? were ranked higher than the generated re-
ports. Interestingly, in almost half the cases there
was a clear preference for the PRF and in the other
cases the PRF ranked badly. This may have been
affected by the familiarity of this medium and per-
haps by the background assumption that this is
how handover reports ?should? be presented.
We regard this as a tentative confirmation that
the generated texts compete favourably with the
status quo. In a real world scenario the paramedics
often get a verbal handover instead of the PRF and
it should be noted that the PRF was printed and not
handwritten. Furthermore, although the CFRs and
paramedics only saw the scenario video once they
were under no time pressure to submit the reports.
Hence the quality of all the human reports in our
experiment is likely to be better than normal.
Although each individual generally provided
consistent responses across the two scenarios,
there were variations between individuals. These
different preferences may be merely stylistic
choices or they may reflect in task performance.
Preferences are not necessarily an indication of
usefulness for a task (cf. (Law et al, 2005)).
In general the accuracy, understandability and
usefulness of the generated reports received good
ratings. Although participation was low, the qual-
itative data we gathered were valuable, every par-
ticipant offered comments in the free text box on
what they liked or disliked about the generated re-
port. In general there seemed to be an impres-
sion that some sections were longer than neces-
sary. One participant observed that reporting on
observations a long time later is only useful if
things have changed significantly. The structure
and organisation of the report received some posi-
tive comments. For example one participant stated
that he liked ?the separate sections for informa-
tion? and another commented that the report was
?logically laid out?, that it was ?easy to obtain
information? from the report and that it ?clearly
states intervention and outcome of intervention?.
5 Conclusion and Future Work
Despite the fact that the experiment reported here
involved a small number of participants, which
implies that its results need to be interpreted with
some caution, the generated reports produced by
the MIME system appear to improve on the cur-
rent practice of verbal handover. We aim to col-
lect more responses and repeat the evaluation that
has been presented. Our next step in evaluating the
report generator will be to carry out a task based
evaluation to see whether the preference ratings
we have gathered can be reflected in performance
measures.
We are now moving into the second stage of
MIME and have started developing a new proto-
type, a mobile device that gets signals from two
lightweight sensors. Here we will collect data
from real emergency ambulance callouts by hav-
ing a researcher join ambulance crews for their
normal activity, which will be used to modify the
NLG system (e.g. in order to allow for more reli-
able handling of noise).
6 Acknowledgments
This work is supported by the RCUK dot.rural
Digital Economy Research Hub, University of Ab-
erdeen (Grant reference: EP/G066051/1)
155
References
N. Bouayad-Agha, R. Power, D. Scott, and A. Belz.
2002. PILLS: Multilingual generation of medical
information documents with overlapping content. In
Proceedings of LREC 2002, pages 2111?2114.
B. Buchanan, J. Moore, D. Forsythe, G. Banks, and
S. Ohlsson. 1992. Involving patients in health care:
explanation in the clinical setting. In Proceedings of
the Annual Symposium on Computer Application in
Medical Care, pages 510?514, January.
M. Friedman. 1937. The Use of Ranks to Avoid the
Assumption of Normality Implicit in the Analysis
of Variance. Journal of the American Statistical As-
sociation, 32(200):675?701.
C. Hallett, R. Power, and D. Scott. 2006. Summari-
sation and visualisation of e-Health data repositories
Conference Item Repositories. In UK E-Science All-
Hands Meeting, pages 18?21.
G. Hirst, C. DiMarco, E. Hovy, and K. Parsons. 1997.
Authoring and Generating Health-Education Docu-
ments That Are Tailored to the Needs of the Individ-
ual Patient. In Anthony Jameson, Ce?cile Paris, and
Carlo Tasso, editors, User Modeling: Proceedings
of the Sixth International Conference, UM97, pages
107?118. Springer Wien New York.
J. Hunter, Y. Freer, A. Gatt, E. Reiter, S. Sripada, and
C Sykes. 2012. Automatic generation of natural
language nursing shift summaries in neonatal in-
tensive care: BT-Nurse. Artificial Intelligence in
Medicine, 56:157?172.
D. Hu?ske-Kraus. 2003. Suregen-2: A Shell System for
the Generation of Clinical Documents. In Proceed-
ings of the 10th Conference of the European Chap-
ter of the Association for Computational Linguistics
(EACL-2003), pages 215?218.
M. Kahn, L. Fagan, and L. Sheiner. 1991. Combining
physiologic models and symbolic methods to inter-
pret time-varying patient data. Methods of informa-
tion in medicine, 30(3):167?78, August.
A. Law, Y. Freer, J. Hunter, R. Logie, N. McIntosh,
and J. Quinn. 2005. A comparison of graphical and
textual presentations of time series data to support
medical decision making in the neonatal intensive
care unit. Journal of clinical monitoring and com-
puting, 19(3):183?94, June.
F. Portet, E. Reiter, J. Hunter, and S. Sripada. 2007.
Automatic generation of textual summaries from
neonatal intensive care data. In In Proccedings
of the 11th Conference on Artificial Intelligence in
Medicine (AIME 07). LNCS, pages 227?236.
E. Reiter and R. Dale. 2000. Building Natural Lan-
guage Generation Systems. Studies in Natural Lan-
guage Processing. Cambridge University Press.
E. Reiter, R. Robertson, and L. Osman. 2003. Lessons
from a failure: Generating tailored smoking cessa-
tion letters. Artificial Intelligence, 144(1-2):41?58,
March.
D. Scott and C. Sieckenius de Souza. 1990. Get-
ting the message across in rst-based text genera-
tion. In R. Dale, C. Mellish, and M. Zock, editors,
Current Research in Natural Language Generation.
Academic Press.
J. Yu, E. Reiter, J. Hunter, and C. Mellish. 2007.
Choosing the content of textual summaries of large
time-series data sets. Natural Language Engineer-
ing, 13(1):25?49.
156
Proceedings of the 14th European Workshop on Natural Language Generation, pages 198?199,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
MIME- NLG Support for Complex and Unstable Pre-hospital
Emergencies
Anne H. Schneider Alasdair Mort Chris Mellish Ehud Reiter Phil Wilson
University of Aberdeen
{a.schneider, a.mort, c.mellish, e.reiter, p.wilson}@abdn.ac.uk
Pierre-Luc Vaudry
Universite? de Montre?al
vaudrypl@iro.umontreal.ca
Abstract
We present the first prototype of a han-
dover report generator developed for the
MIME (Managing Information in Medi-
cal Emergencies) project. NLG applica-
tions in the medical domain have been var-
ied but most are deployed in clinical situa-
tions. We develop a mobile device for pre-
hospital care which receives streamed sen-
sor data and user input, and converts these
into a handover report for paramedics.
1 Introduction
Natural Language Generation underlies many ap-
plications in the medical domain but most are em-
ployed under relatively predictable clinical situa-
tions. The MIME project employs a mobile de-
vice with novel lightweight sensors to improve
pre-hospital care service delivery. The term pre-
hospital care denotes the treatment delivered to
a patient before they arrive at hospital. Usu-
ally this entails paramedics and ambulance teams,
but it can also include a wide range of volun-
tary and professional care groups. Care for ru-
ral pre-hospital patients can sometimes be car-
ried out by volunteers from local communities:
Community First Responders (CFR). Their task is
to assess patients, perform potentially life-saving
first aid procedures and record medical observa-
tions whilst the ambulance clinicians are en-route.
These data are then handed over to the receiv-
ing ambulance team upon arrival. Because of
their time-critical nature, handover reports are of-
ten verbal and hence maybe incomplete or misun-
derstood.
MIME was inspired by the Babytalk BT-Nurse
system (Hunter et al, 2012), which generates shift
handover reports for nurses in a neonatal intensive
care unit. While BT-Nurse works with an exist-
ing clinical record system, which does not always
At 02:12, after RR remained fairly
constant around 30 bpm for 4 minutes,
high flow oxygen was applied, she took
her inhaler and RR decreased to 27
bpm. However, subsequently RR once
more remained fairly constant around
30 bpm for 8 minutes.
At 02:15 she was feeling faint.
At 02:15 the casualty was moved.
At 02:17 the casualty was once more
moved.
Figure 1: Part of the ?Treatment and Findings? for
an asthma scenario.
record all actions and observations which ideally
would be included in a report, in MIME the elec-
tronic record and user interface for acquiring ex-
actly the desired information are effectively de-
signed. This simplifies the NLG task, at the cost
of adding a new task (interface construction).
2 The MIME project
Pre-hospital care is especially challenging because
the environment in which it is delivered is inher-
ently unpredictable. The clinical condition of a
patient may have improved or deteriorated since
the original call for help. The unpredictability of
the environment at the scene of the call and the
minimal level of clinical training of the CFRs con-
tributes to the challenges presented to developers
of a mobile device for this situation. In particular,
the continuous capture of physiological data intro-
duces the problem that irrelevant material needs
to be suppressed in order not to overload the am-
bulance clinicians and hinder interpretation. The
generated reports must provide a quick overview
of the situation but at the same time be compre-
hensive. It is also vital that the format must en-
hance the readability, and the user-interface be
simple and intuitive in order to avoid what has
198
Figure 2: First hardware prototype of the MIME
project (GETAC Z710 tablet and Pulse Oxymeter
sensor).
been termed ?creeping featurism? (His and Potts,
2000), whereby option saturation hinders task per-
formance.
In a user centred development process we estab-
lished a structure for the handover reports. After
the demographic description of the casualty (i.e.
age and gender) and incident details that were re-
layed to the CFR by the ambulance control centre
two elements of generated text follow, the initial
assessment section and the treatment and findings
section. The initial assessment contains informa-
tion on the casualty that is gathered by the CFRs
before the sensors are applied including baseline
observation during the first minute after the ap-
plication of the sensors. The treatment and find-
ings section (Figure 1) is a report of the observa-
tions and actions of the CFRs while they attended
the casualty and waited for the ambulance to ar-
rive. This includes a paragraph that sums up the
condition of the patient at the time of handover.
There are three types of events included in the re-
port: discrete events (action and observation) and
continuous events (trends in sensor readings). Ac-
tions (e.g. applying oxygen) and observations (e.g.
the patient feels faint) have to be entered by the
CFR through an interface. Continuous events are
derived from the medical sensors: currently res-
piratory rate, blood oxygen saturation, and heart
rate are recorded. Since some events, especially
those that deviate from the norm are more impor-
tant than others (Hallett et al, 2006), in the docu-
ment planning stage we employ an algorithm that
decides which events are mentioned in the report
and in which order. This process is loosely based
on similar decision processes reported in (Hallett
et al, 2006) and (Portet et al, 2007).
3 Summary and Conclusion
We have developed a first prototype of the system
which uses simulated data to produce handover re-
ports. This runs on standard desktop PCs. For our
second prototype, which is currently being devel-
oped, we port the NLG algorithm onto a GETAC
Z710 tablet1 which has been chosen for it?s robust-
ness, capacitative touch screen, and long battery
life (Figure 2). Our research also includes the es-
tablishment of a connection between the tablet and
sensors, the recording of the incoming data stream
and the development of an interface for the tablet,
which can be used by the CFR to enter observa-
tions and actions taken or any other useful infor-
mation.
At the ENLG workshop we will present our first
hardware prototype alongside the desktop com-
puter version, highlighting the challenges that the
project faces in developing a handover report gen-
erator for pre-hospital care.
4 Acknowledgments
This work is supported by the RCUK dot.rural
Digital Economy Research Hub, University of Ab-
erdeen (Grant reference: EP/G066051/1)
References
C. Hallett, R. Power, and D. Scott. 2006. Summarisa-
tion and visualisation of e-Health data repositories.
In UK E-Science All-Hands Meeting, pages 18?21,
Nottingham, UK.
I. His and C. Potts. 2000. Studying the Evolution and
Enhancement of Software Features. In Proceedings
of the International Conference on Software Mainte-
nance, ICSM ?00, pages 143?151, Washington, DC,
USA.
J. Hunter, Y. Freer, A. Gatt, E. Reiter, S. Sripada, and
C Sykes. 2012. Automatic generation of natural
language nursing shift summaries in neonatal in-
tensive care: BT-Nurse. Artificial Intelligence in
Medicine, 56:157?172.
F. Portet, E. Reiter, J. Hunter, and S. Sripada. 2007.
Automatic generation of textual summaries from
neonatal intensive care data. In Proccedings of
the 11th Conference on Artificial Intelligence in
Medicine (AIME 07). LNCS, pages 227?236.
1http://en.getac.com/products/Z710/
Z710_overview.html
199
Proceedings of the 14th European Workshop on Natural Language Generation, pages 208?209,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
Content Selection Challenge - University of Aberdeen Entry
Roman Kutlak
Chris Mellish
Kees van Deemter
Department of Computing Science
University of Aberdeen
Aberdeen AB24 3UE, UK
r.kutlak, c.mellish, k.vdeemter @abdn.ac.uk
1 Introduction
Bouayad-Agha et al (2012) issued a content de-
termination challenge in which researchers were
asked to create systems that can automatically
select content suitable for a first paragraph in a
Wikipedia article from an RDF knowledge base
of information about people. This article is a de-
scription of the system built at the University of
Aberdeen.
Our working assumption is that the target text
should contain information that is commonly
known about the target person. The Wikipedia?s
manual of style mentions that ?The lead [section]
serves as an introduction to the article and a sum-
mary of its most important aspects1.? What is most
important about a person is likely to be often men-
tioned in biographies and hence it is more likely to
be commonly known.
Our system was motivated by the notion of
common ground, especially the way it was ac-
counted for by (Clark and Marshall, 1981). Clark
and Marshall (1981) introduce two categories of
common ground: personal common ground shared
by a small group of individuals and communal
common ground shared by a community of peo-
ple. We are most interested in the concept of com-
munal common ground, which arises from the ex-
posure to the same information within a commu-
nity. For example, if there is a statue in front of
your work place, you expect your colleagues to
also know about this statue and so the information
that there is a statue in front of you workplace be-
comes a part of the community knowledge (where
the community are people who work at the same
place).
Our hypothesis is that if we take a corpus of
documents produced by some large community
(e.g., English speakers), we should be able to ap-
1http://en.wikipedia.org/wiki/
Wikipedia:Manual_of_Style/Lead_section
proximate the community?s knowledge of certain
facts by counting how frequently they are men-
tioned in the corpus. For example, if a corpus con-
tains 1000 articles about Sir Isaac Newton and 999
of the examined documents mention the property
of him being a physicist and only 50 documents
mention that he held the position as the warden
of the Royal Mint in 1696 we should expect more
people to know that he was a physicist.
We implemented the heuristic for approximat-
ing communal common ground and tested it in
an experiment with human participants to measure
whether there is a correlation between the heuris-
tic?s predictions and actual knowledge of people
(Kutlak et al, 2012). In our implementation, we
used the Internet as a corpus of documents and we
used the Google search engine for counting the
number of documents containing the properties.
Although the number of hits is only an estimate
of the actual number of documents containing a
particular term, the heuristic achieved a Spearman
correlation of 0.639 with p < 0.001 between the
knowledge of people and the numbers of hits re-
turned by Google.
Although there are some issues with the use of a
proprietary search engine such as Google (for ex-
ample, the search engine can perform stemming;
see Kilgarriff (2007) for a discussion) search en-
gines have been successfully used previously (Tur-
ney, 2001; Goudbeek and Krahmer, 2012).
2 Algorithm
The submitted system employs the heuristic out-
lined in in the previous section. The input is a col-
lection of files containing information about peo-
ple and a collection of human readable strings for
each of the files. The data were taken from Free-
base - a community created repository of informa-
tion about people, places and other things. Each
file is a small knowledge base containing a set of
RDF triples describing the entity.
208
The data is encoded in machine-readable form
(e.g., the fact that Newton was an astronomer
is encoded as ns:m.03s9v ns:type.object.type
ns:astronomy.astronomer .) so in order to find
collocations in a human written text, each RDF
triple has to be ?lexicalised.? This is done by map-
ping the RDF values to human produced strings
provided by Freebase. After substituting the lexi-
calisations and removing some unnecessary infor-
mation the algorithm adds the name of the target,
which results in text such as Isaac Newton type
Astronomer.
The algorithm reads one file at a time and cre-
ates a human readable string for each of the prop-
erties in the file. In the second step, the system re-
moves disambiguations (text in brackets) and fil-
ters out properties that have the same string rep-
resentation (duplicates). Additionally, properties
with certain attributes are filtered out to reduce the
number of queries2.
In the third step, the system uses Google cus-
tom search API (a programming interface to the
search engine) to estimate the score of each prop-
erty. Properties that contain the name of the entity
are penalised. This is done to reduce the impor-
tance of properties such as the target?s parents or
relatives. For example, if the algorithm was rank-
ing properties of Sir Isaac Newton and a property
contained the string Newton, the score assigned to
that property was multiplied by 0.75. The prop-
erties were then ordered by the number of corre-
sponding hits in descending order.
In the last step the algorithm selects the top
ranked properties. The number of properties to
select was calculated by the following equation
5 ? log(|properties|). This equation was chosen
by intuition so that a larger proportion of proper-
ties was selected for entities with a small number
of properties than for entities with a large number
of properties. The set of properties in the above
equation is the set obtained after the filtering.
To prevent the system from selecting too many
properties with the same attribute and to intro-
duce variation, the system selected only five prop-
erties with the same attribute (e.g., five films, five
books).
2For example, the knowledge base describing Anton??n
Dvor?a?k contains 5670 properties of which 5154 have the at-
tribute music.artist.track.
3 Concluding Remarks
The implemented system uses a simple document-
based collocation heuristic to decide what prop-
erties to select. This makes it prone to favour-
ing properties that contain common words or the
name of the described entity. The advantage is
that the system is relatively simple and versatile.
The ?common ground? heuristic could be com-
bined with another heuristic that assigns negative
score to properties that contain common words or
a heuristic that estimates how interesting the prop-
erty is.
Finally, we do not expect the system to perform
better than machine learning based approaches
such as that of Duboue and McKeown (2003) but
it will certainly be interesting to see how far one
can get with a simple heuristic.
References
Nadjet Bouayad-Agha, Gerard Casamayor, Leo Wan-
ner, and Chris Mellish. 2012. Content selection
from semantic web data. In Proceedings of INLG
2012, pages 146?149, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.
Herbert H. Clark and Catherine Marshall. 1981. Def-
inite reference and mutual knowledge. In A. K.
Joshi, B. L. Webber, and I. A. Sag, editors, El-
ements of discourse understanding, pages 10?63.
Cambridge University Press, New York.
Pablo A. Duboue and Kathleen R. McKeown. 2003.
Statistical acquisition of content selection rules for
natural language generation. In Proceedings of
the 2003 EMNLP, pages 121?128, Morristown, NJ,
USA. Association for Computational Linguistics.
Martijn Goudbeek and Emiel Krahmer. 2012. Align-
ment in interactive reference production: Con-
tent planning, modifier ordering, and referential
overspecification. Topics in Cognitive Science,
4(2):269?289.
Adam Kilgarriff. 2007. Googleology is bad science.
Comput. Linguist., 33:147?151, March.
Roman Kutlak, Kees van Deemter, and Chris Mellish.
2012. Corpus-based metrics for assessing commu-
nal common ground. Proceedings of the 34th An-
nual Meeting of the Cognitive Science Society.
P. Turney. 2001. Mining the web for synonyms: PMI-
IR versus LSA on TOEFL. In Proceedings of the
twelfth european conference on machine learning
(ecml-2001).
209
Proceedings of the 8th International Natural Language Generation Conference, pages 113?117,
Philadelphia, Pennsylvania, 19-21 June 2014. c?2014 Association for Computational Linguistics
Determining Content for Unknown Users: Lessons from 
           the MinkApp Case Study 
Gemma Webster, Somayajulu G. Sripada, Chris Mellish, Yolanda Melero, Koen Arts, 
Xavier Lambin, Rene Van Der Wal  
University of Aberdeen 
{gwebster, yaji.sripada, c.mellish, y.melero, k.arts, x.lambin, r.vanderwal}@abdn.ac.uk  
 
Abstract 
If an NLG system needs to be put in 
place as soon as possible it is not always 
possible to know in advance who the us-
ers of a system are or what kind of in-
formation will interest them. This paper 
describes the development of a system 
and contextualized text for unknown us-
ers. We describe the development, design 
and initial findings with a system for un-
known users that allows the users to de-
sign their own contextualised text. 
1 Introduction 
Requirements of an NLG system are derived 
commonly by analysing a gold standard corpus. 
Other knowledge acquisition (KA) techniques 
such as interviewing experts and end-users are 
also frequently employed. However, when these 
KA studies result in only a partial specification 
of the system requirements or complications 
make carrying out a detailed user study in the 
time available difficult, an initial system for un-
known users may need to be developed. The ini-
tial system needs to fulfil the known require-
ments making a number of assumptions to fill the 
gaps in the requirements. In this paper, we con-
centrate on the content determination problem 
for such a system. 
 
We encountered this particular problem when 
producing an initial NLG system to give feed-
back to volunteers submitting information about 
signs of American Mink, an invasive species in 
Scotland. Our response can be viewed, on one 
hand, as that of exposing an early prototype for 
evaluation in real use. On the other hand, it can 
be viewed as an approach to allowing users to 
?design their own contextualised text?. We ex-
pected that this approach would have a number 
of advantages. In the paper, we draw our conclu-
sions about how this worked out in our example 
application. 
2 Background - MinkApp 
The Scottish Mink Initiative (SMI) project aims 
to protect native wildlife by removing breeding 
American Mink (an invasive species) from the 
North of Scotland. SMI in the form discussed 
here was launched in May 2011 and ran until 
August 2013, after which it continued but on a 
much smaller funding base. SMI?s success and 
future rely on an ongoing network of volunteers 
from across Scotland to monitor the American 
mink population. During the period from 2011 to 
2013, these volunteers were coordinated by 4 and 
later 3 full-time Mink Control officers (MCOs) 
who had 2.5 year fixed term contracts, had no 
communal offices and were geographically lo-
cated across Scotland.  
At present volunteers are provided with rafts to 
monitor American mink. Rafts are simple devic-
es that float on water and are monitored by vol-
unteers who regularly check a clay pad for mink 
footprints. In the past, volunteers in turn reported 
signs or lack of signs to their corresponding 
MCO. Now volunteers can do the same through 
the MinkApp website, introduced in 2012, 
though some choose to continue to use the previ-
ous reporting method. The data should ideally be 
entered roughly every 10 days; it concerns either 
positive or negative records from raft checks, or 
visual sightings of mink and actual mink cap-
tures. The records contain geographical infor-
mation and a timestamp. MinkApp checks 
whether this data is complete and then informs 
the respective mink officer for that volunteer?s 
area and enters the data into the database.  
 
Volunteers used to receive a quarterly newsletter 
that had some regional specific content but was 
not volunteer specific. They could receive spo-
radic contact from their mink control officer in 
the form of a phone call or email. MinkApp al-
lowed an infrastructure to be developed to pro-
vide volunteers with specific and immediate 
113
feedback upon submission of their observations 
by means of contextualised feedback text. 
 
SMI?s funding base was severely reduced in Au-
gust 2013 and MinkApp has proven central to its 
endurance. Volunteer activities of the SMI are 
now supported by staff from 10 local rivers and 
fisheries trusts (as one of their many activities). 
This limited amount of staff time available could 
make the development of automatic personalised 
feedback generation vital to allow volunteers to 
have tailored information on the progress of the 
project and to keep volunteers engaged. 
3 The Problem - SMI Volunteers: The 
Unknown Users 
The nearest to a gold standard for what infor-
mation to offer was the corpus of newsletters 
containing information on the project as a whole. 
However, we learned that these newsletters were 
often not read and we have no way of judging 
their level of success. These newsletters, along 
with emails and discussions conducted with SMI 
employees on their interactions with volunteers, 
however, gave us ideas about potential content 
that could be selected and indication of potential 
lexical structure and word use when addressing 
volunteers.  
Although some SMI volunteers monitor mink as 
part of their job (e.g. gamekeepers), they could in 
fact be anyone with a desire to contribute to na-
ture conservation. Volunteers are located in very 
disparate geographical locations across Scotland, 
with no set gender or age range and so volun-
teers? motivations, computer skills and profes-
sions are mostly unknown. Because of the range 
of types of people who could in principle be vol-
unteers, they can be expected to be very varied. 
It is extremely difficult to contact all volunteers 
as each SMI catchment is managed and orga-
nized in different ways and volunteers are con-
tacted using different media e.g. mail, email, tel-
ephone, face-to-face. SMI is also careful to avoid 
attempting to contact volunteers too often, con-
scious that they are providing their services for 
free and should not be bothered unnecessarily.  
There is also some uncertainty about which vol-
unteers are active, as records are often partial or 
out of date. It is known anecdotally from MCOs 
that many volunteers are unwilling to use any 
kind of computer system and so it is unclear 
what kind of people will be reached through 
MinkApp. Finally, most observations of mink 
signs that arise are ?null records?, i.e. records of 
observing no mink prints on rafts. It is not known 
which volunteers will be sufficiently motivated 
to submit ?null records? and which will remain 
apparently inactive because they have nothing 
positive to report. 
So, even though there was a need for automati-
cally generated feedback now, there was a real 
question of who the readers would be and how to 
select the content to include in the feedback. 
4 Related Work 
A standard approach to establish user require-
ments for NLG is to assemble a corpus of hu-
man-authored texts and their associated inputs 
(Reiter & Dale, 2000). This can be the basis of 
deriving rules by hand, or one can attempt to rep-
licate content selection rules from the corpus by 
machine learning (Duboue & McKeown, 2003; 
Konstas & Lapata, 2012). To produce a useful 
corpus, however, one has to know one?s users or 
have reliable expert authors. 
 
As first pointed out by Levine et al. (1991), an 
NLG system that produces hypertext, rather than 
straight text, can avoid some content selection 
decisions, as the user makes some of these deci-
sions by selecting links to follow. A similar ad-
vantage applies to other adaptive hypertext sys-
tems (Brusilovsky, 2001).  Another general pos-
sibility is to allow users to design aspects of the 
texts they receive. For instance, ICONOCLAST 
(Power, Scott, & Bouayad-Agha, 2003) allows 
users to make choices about text style. However, 
relatively little is known about how such ap-
proaches work ?in the wild?. 
 
Various previous work has attempted to build 
models of users through observing interactions 
with an interface (Fischer, 2001). Alternatively, 
it is possible to explicitly ask questions to the 
user about their interests (Tintarev & Masthoff, 
2008), though this requires the users to have the 
time and motivation to take part in an initial ac-
tivity with no direct reward. 
 
Our approach can be seen to have similarities 
with hypertext generation, in that we are offering 
alternative texts to users, and non-invasive ap-
proaches to user modelling. 
114
5 Approach to Content Selection 
To overcome the ?unknown? user and ?unknown? 
feedback problem it was decided to implement a 
relatively quick exploratory tool that could be 
used to help understand user requirements, pro-
vide initial evaluation of feedback content and 
build an understanding of user interests. To 
achieve these aims we developed a tool that al-
lows users to generate their own text, selecting 
content from a larger set of possibilities. The in-
formation on the type of feedback generated by 
the user would allow us to investigate user stere-
otypes, their detection and the automatic adapta-
tion of content based on their interests 
(Zancanaro, Kuflik, Boger, Goren-Bar, & 
Goldwasser, 2007). 
5.1 Exploratory Tool - The Feedback Form 
The feedback form (Figure 1) is displayed to us-
ers of the MinkApp system once they have sub-
mitted a raft check. The form allows the user to 
select which raft they wish to have their feedback 
generated on from a list of the rafts they manage. 
The users have four types of information they 
can select to have feedback generated on: Signs 
(information on signs of mink reported through 
raft checks), Captures (information on mink cap-
tures), My Rafts (information on their personal 
raft checks and submission record) and Mink 
Ecology (information on mink behaviour and 
seasonality).  
Two of the four options, Signs and Captures, 
allow the user to select to what geographic scale 
they would like their feedback based on: the 
whole of the SMI project area, their river or their 
catchment ? the geographical region that they 
report to e.g. Aberdeenshire, Tayside etc.  
 
Once the user has made their selection the per-
sonalised feedback based on their choices is gen-
erated and displayed along with an option to rank 
how interesting they found this feedback or any 
comments they wish to make. The user can gen-
erate multiple texts in one session. All data from 
each click of an option, the generated text and 
user comments on the text are recorded.  
5.2 Generation of the paragraphs 
The structure of the text is separated out into 
self-contained paragraphs to allow analysis of 
what volunteers regularly view. For each type, 
the structure of the generated paragraph is de-
termined by a simple schema: 
Signs:  
Neighbourhood (based on user selection) ? In the 
Don catchment there have been 6 signs of mink 
reported over the past 12 months which is higher 
than the previous 12 months 
Additional Information / Motivation ? Mink are 
coming into your area to replace captured mink. 
This shows your area has good ecology for mink 
and it is important to keep monitoring. 
Personal ? There have been no signs of mink (in 
the form of either footprints or scat) in the past 
30 days. No signs of mink recently does not mean 
they are gone - remain vigilant. 
  
Captures: 
Neighbourhood (based on user selection) ? In the 
Spey catchment we have trapped 5 mink over the 
past 12 months which is lower than the previous 
12 months. 
Additional Information / Motivation ? Infor-
mation available on this year's captures: An 
adult female mink was captured on: 2014-02-19. 
 
My Rafts: 
Personal ?You have been very active over the 
past 60 days with 7 'no mink signs' reported and 
2 signs of mink (in the form of either footprints 
or scat) reported, the last of which was logged 
on 14 Sep 2013 23:00:00 GMT. 
Additional Information / Motivation ? Please 
keep checking your raft as this evidence means 
there are mink in your area. 
 
Mink Ecology: 
Temporal - We are in the normal mink breeding 
season!  
Motivation ? During the breeding season female 
mink will defend an area covering approximately 
1.5 miles.  
Additional Information - Female mink are small 
enough to fit into water vole burrows which they 
explore in search of prey.Did you know there can 
be brown, black, purple, white and silver mink 
which reflects the colours bred for fur? 
115
 
To produce the actual content to fill the slots of 
the schemas, the system was designed to reason 
over geographical location to allow examination 
of the various notions of neighbourhood 
(Tintarev et al 2012). The system also looks at 
temporal trends when developing text based on 
the number of record submissions for a given 
time. The system initially looks at record sub-
missions in the past week then opens out to a 
month, season and finally activity between the 
same seasons on different years. This use of 
temporal trends ensures volunteers are supplied 
with the most relevant (recent) mink activity in-
formation first in busy periods such as the breed-
ing season but ensures ?cleared? areas with little 
mink activity are still provided with informative 
feedback.  
6 Evaluation of the Feedback Approach 
We were initially apprehensive about how much 
usage the feedback system would get. MinkApp 
was launched through the SMI newsletters, but 
we knew that volunteers were not always receiv-
ing or reading these. Also it turned out that the 
initial estimate of active volunteers was over-
inflated. Indeed, initially the usage of MinkApp 
in general was much lower than was expected. 
So we worked hard to promote the system, for 
instance asking the fisheries trusts to actively ask 
any volunteers they had contact with if they had 
heard of MinkApp and to try to use it. As a re-
sult, we did manage to increase the system usage 
to a level where some initial conclusions can be 
drawn. 
MinkApp and specifically the feedback form use 
were monitored for 50 days (7 weeks). During 
this time 308 raft checks were submitted by vol-
unteers for 98 different rafts by 44 unique users. 
The feedback system was used by volunteers to 
generate 113 different texts about 36 different 
rafts. 32 out of the 44 (72.7%) of all MinkApp 
users requested generated feedback at least once.  
 
In 47% of the feedback form use sessions multi-
ple texts were generated and there are some par-
ticularly interesting use patterns: 
? ?Regular explorer?: One user accessed 
MinkApp seven times and generated 
feedback text on every use: 1 text, 3 
texts, 5 texts, 5 texts, 4 texts, 2 texts and 
1 text 
? ?Periodic explorer?: One user accessed 
MinkApp six times and generated at 
least one feedback text on every second 
use 
? ?Try once only?: The user who accessed 
MinkApp the most with eleven different 
sessions only generated feedback text on 
their first use of MinkApp.  
These different patterns of use require further 
investigation as the number of users using 
MinkApp increases. The patterns can be affected 
by idiosyncratic factors. For instance, one volun-
teer informed the project coordinator that they 
continually selected Captures within their area as 
they had caught a mink and their capture had not 
yet been added to the system - the volunteer was 
using the feedback form to monitor how long it 
took for mink capture data to appear in 
MinkApp.  
 
Of the four types of information available to vol-
unteers Signs was the most viewed although 
Captures was what SMI staff had felt volunteers 
would be most interested in. Signs had 56.6% of 
the overall use and catchment was the most 
widely selected option for geographic area for 
both Signs and Captures. However there was no 
clearly predominant second choice for infor-
mation option with Captures and My Rafts hav-
ing only 2.7% of a difference within their use. 
Mink Ecology was the least used category, partly 
to do with the lack of clarity in the name ?Mink 
Ecology?. Signs on a local geographical scale 
were the most common selection for volunteers 
but the actual use was not clear enough to sup-
port a fixed text type or removing other options. 
7 Conclusions 
The results of this initial study did support the 
value of feedback to volunteers (more directly 
than we would have been able to determine in 
advance) with 73% of volunteers choosing to 
generate feedback. The feedback enabled us to 
offer contextualized information to volunteers 
quickly, without initial extensive user studies, 
which was very important for supporting the 
continuation of SMI. 
The fact that the volunteer population was rela-
tively unknown meant that there were some un-
pleasant surprises in terms of uptake and interest. 
It was necessary to make special efforts to en-
courage participation to get larger numbers. 
116
When our system gets used over longer periods 
we might observe more meaningful patterns of 
behaviour. 
The patterns of interest we observed were noisy 
and were influenced by many contextual factors 
meaning there was little potential yet for statisti-
cal analysis or machine learning.  
8 Future Work 
In-depth analysis is required as more volunteers 
use MinkApp and the feedback form to fully un-
derstand patterns of behaviour. Additionally 
qualitative studies such as interviews with volun-
teers could help explain use and preferences. 
These studies could help us improve the feed-
back system and text to better suit the user?s 
needs. In the meantime, we have a working sys-
tem that offers choices to users to ?generate their 
own text? even though we had hoped to be able 
to tailor to individual volunteer preferences 
sooner. 
9 Acknowledgments 
We would like to thank SMI for their on-going 
commitment to this research. This work is sup-
ported by the Rural Digital Economy Research 
Hub (EPSRC EP/G066051/1). 
Reference 
Arts, K., Webster, G. ., Sharma, N. ., Melero, Y. ., 
Mellish, C., Lambin, X., & Van der Wal, R. 
(2013). Capturing mink and data. Interacting with a 
small and dispersed environmental initiative over 
the introduction of digital innovation Uploader. 
Case study for the online platform ?Framework for 
Responsible Research and Innovation in ICT.? Re-
trieved from http://responsible-
innovation.org.uk/torrii/resource-detail/1059 
Beirne, C., & Lambin, X. (2013). Understanding the 
Determinants of Volunteer Retention Through 
Capture-Recapture Analysis: Answering Social 
Science Questions Using a Wildlife Ecology 
Toolkit. Conservation Letters, 6(6), 391?401. 
doi:10.1111/conl.12023 
Brusilovsky, P. (2001). Adaptive Hypermedia. User 
Modeling and User-Adapted Interaction, 11(1-2), 
87?110. doi:10.1023/A:1011143116306 
Bryce, R., Oliver, M. K., Davies, L., Gray, H., Ur-
quhart, J., & Lambin, X. (2011). Turning back the 
tide of American mink invasion at an unprecedent-
ed scale through community participation and 
adaptive management. Biological conservation, 
144(1), 575?583. Retrieved from 
http://cat.inist.fr/?aModele=afficheN&cpsidt=2377
9637 
Duboue, P. A., & McKeown, K. R. (2003). Statistical 
acquisition of content selection rules for natural 
language generation. In Proceedings of the 2003 
conference on Empirical methods in natural lan-
guage processing - (Vol. 10, pp. 121?128). Morris-
town, NJ, USA: Association for Computational 
Linguistics. doi:10.3115/1119355.1119371 
Fischer, G. (2001). User Modeling in Human?
Computer Interaction. User Modeling and User-
Adapted Interaction, 11(1-2), 65?86. 
doi:10.1023/A:1011145532042 
Konstas, I., & Lapata, M. (2012). Concept-to-text 
generation via discriminative reranking, 369?378. 
Retrieved from 
http://dl.acm.org/citation.cfm?id=2390524.239057
6 
Levine, J., Cawsey, A., Mellish, C., Poynter, L., 
Reiter, E., Tyson, P., & Walker, J. (1991). IDAS: 
Combining hypertext and natural language genera-
tion. In Procs of the Third European Workshop on 
NLG (pp. 55?62). Innsbruck, Austria. 
Power, R., Scott, D., & Bouayad-Agha, N. (2003). 
Generating texts with style, 444?452. Retrieved 
from 
http://dl.acm.org/citation.cfm?id=1791562.179161
9 
Reiter, E., & Dale, R. (2000). Building Applied Natu-
ral Language Generation Systems. clt.mq.edu.au 
(Vol. 33.). Cambridge: Cambridge university press. 
Retrieved from 
http://clt.mq.edu.au/~rdale/publications/papers/199
7/jnle97.pdf 
Tintarev, N., & Masthoff, J. (2008). Adaptive Hyper-
media and Adaptive Web-Based Systems. (W. 
Nejdl, J. Kay, P. Pu, & E. Herder, Eds.) (Vol. 
5149, pp. 204?213). Berlin, Heidelberg: Springer 
Berlin Heidelberg. doi:10.1007/978-3-540-70987-9 
Tintarev, N., Melero, Y., Sripada, S., Tait, E., Van 
Der Wal, R., & Mellish, C. (2012). MinkApp: gen-
erating spatio-temporal summaries for nature con-
servation volunteers, 17?21. Retrieved from 
http://dl.acm.org/citation.cfm?id=2392712.239272
0 
Zancanaro, M., Kuflik, T., Boger, Z., Goren-Bar, D., 
& Goldwasser, D. (2007). Analyzing museum vis-
itors? behavior patterns. In C. Conati, K. McCoy, 
& G. Paliouras (Eds.), 11th International Confer-
ence on User Modeling (Vol. 4511, pp. 238?246). 
Berlin, Heidelberg: Springer Berlin Heidelberg. 
doi:10.1007/978-3-540-73078-1 
117
