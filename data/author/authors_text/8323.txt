Proceedings of the COLING/ACL 2006 Interactive Presentation Sessions, pages 17?20,
Sydney, July 2006. c?2006 Association for Computational Linguistics
An intelligent search engine and GUI-based efficient MEDLINE search
tool based on deep syntactic parsing
Tomoko Ohta
Yoshimasa Tsuruoka??
Jumpei Takeuchi
Jin-Dong Kim
Yusuke Miyao
Akane Yakushiji?
Kazuhiro Yoshida
Yuka Tateisi?
Department of Computer Science, University of Tokyo
Hongo 7-3-1, Bunkyo-ku, Tokyo 113-0033 JAPAN
{okap, yusuke, ninomi, tsuruoka, akane, kmasuda, tj jug,
kyoshida, harasan, jdkim, yucca, tsujii}@is.s.u-tokyo.ac.jp
Takashi Ninomiya?
Katsuya Masuda
Tadayoshi Hara
Jun?ichi Tsujii
Abstract
We present a practical HPSG parser for
English, an intelligent search engine to re-
trieve MEDLINE abstracts that represent
biomedical events and an efficient MED-
LINE search tool helping users to find in-
formation about biomedical entities such
as genes, proteins, and the interactions be-
tween them.
1 Introduction
Recently, biomedical researchers have been fac-
ing the vast repository of research papers, e.g.
MEDLINE. These researchers are eager to search
biomedical correlations such as protein-protein or
gene-disease associations. The use of natural lan-
guage processing technology is expected to re-
duce their burden, and various attempts of infor-
mation extraction using NLP has been being made
(Blaschke and Valencia, 2002; Hao et al, 2005;
Chun et al, 2006). However, the framework of
traditional information retrieval (IR) has difficulty
with the accurate retrieval of such relational con-
cepts. This is because relational concepts are
essentially determined by semantic relations of
words, and keyword-based IR techniques are in-
sufficient to describe such relations precisely.
This paper proposes a practical HPSG parser
for English, Enju, an intelligent search engine for
the accurate retrieval of relational concepts from
?Current Affiliation:
?School of Informatics, University of Manchester
?Knowledge Research Center, Fujitsu Laboratories LTD.
?Faculty of Informatics, Kogakuin University
?Information Technology Center, University of Tokyo
F-Score
GENIA treebank Penn Treebank
HPSG-PTB 85.10% 87.16%
HPSG-GENIA 86.87% 86.81%
Table 1: Performance for Penn Treebank and the
GENIA corpus
MEDLINE, MEDIE, and a GUI-based efficient
MEDLINE search tool, Info-PubMed.
2 Enju: An English HPSG Parser
We developed an English HPSG parser, Enju 1
(Miyao and Tsujii, 2005; Hara et al, 2005; Ni-
nomiya et al, 2005). Table 1 shows the perfor-
mance. The F-score in the table was accuracy
of the predicate-argument relations output by the
parser. A predicate-argument relation is defined
as a tuple ??,wh, a, wa?, where ? is the predi-
cate type (e.g., adjective, intransitive verb), wh
is the head word of the predicate, a is the argu-
ment label (MOD, ARG1, ..., ARG4), and wa is
the head word of the argument. Precision/recall
is the ratio of tuples correctly identified by the
parser. The lexicon of the grammar was extracted
from Sections 02-21 of Penn Treebank (39,832
sentences). In the table, ?HPSG-PTB? means that
the statistical model was trained on Penn Tree-
bank. ?HPSG-GENIA? means that the statistical
model was trained on both Penn Treebank and GE-
NIA treebank as described in (Hara et al, 2005).
The GENIA treebank (Tateisi et al, 2005) consists
of 500 abstracts (4,446 sentences) extracted from
MEDLINE.
Figure 1 shows a part of the parse tree and fea-
1http://www-tsujii.is.s.u-tokyo.ac.jp/enju/
17
Figure 1: Snapshot of Enju
ture structure for the sentence ?NASA officials
vowed to land Discovery early Tuesday at one
of three locations after weather conditions forced
them to scrub Monday?s scheduled return.?
3 MEDIE: a search engine for
MEDLINE
Figure 2 shows the top page of the MEDIE. ME-
DIE is an intelligent search engine for the accu-
rate retrieval of relational concepts from MED-
LINE 2 (Miyao et al, 2006). Prior to retrieval, all
sentences are annotated with predicate argument
structures and ontological identifiers by applying
Enju and a term recognizer.
3.1 Automatically Annotated Corpus
First, we applied a POS analyzer and then Enju.
The POS analyzer and HPSG parser are trained
by using the GENIA corpus (Tsuruoka et al,
2005; Hara et al, 2005), which comprises around
2,000 MEDLINE abstracts annotated with POS
and Penn Treebank style syntactic parse trees
(Tateisi et al, 2005). The HPSG parser generates
parse trees in a stand-off format that can be con-
verted to XML by combining it with the original
text.
We also annotated technical terms of genes and
diseases in our developed corpus. Technical terms
are annotated simply by exact matching of dictio-
2http://www-tsujii.is.s.u-tokyo.ac.jp/medie/
nary entries and the terms separated by space, tab,
period, comma, hat, colon, semi-colon, brackets,
square brackets and slash in MEDLINE.
The entire dictionary was generated by apply-
ing the automatic generation method of name vari-
ations (Tsuruoka and Tsujii, 2004) to the GENA
dictionary for the gene names (Koike and Takagi,
2004) and the UMLS (Unified Medical Language
System) meta-thesaurus for the disease names
(Lindberg et al, 1993). It was generated by ap-
plying the name-variation generation method, and
we obtained 4,467,855 entries of a gene and dis-
ease dictionary.
3.2 Functions of MEDIE
MEDIE provides three types of search, seman-
tic search, keyword search, GCL search. GCL
search provides us the most fundamental and pow-
erful functions in which users can specify the
boolean relations, linear order relation and struc-
tural relations with variables. Trained users can
enjoy all functions in MEDIE by the GCL search,
but it is not easy for general users to write ap-
propriate queries for the parsed corpus. The se-
mantic search enables us to specify an event verb
with its subject and object easily. MEDIE auto-
matically generates the GCL query from the se-
mantic query, and runs the GCL search. Figure 3
shows the output of semantic search for the query
?What disease does dystrophin cause??. This ex-
ample will give us the most intuitive understand-
ings of the proximal and structural retrieval with a
richly annotated parsed corpus. MEDIE retrieves
sentences which include event verbs of ?cause?
and noun ?dystrophin? such that ?dystrophin? is the
subject of the event verbs. The event verb and its
subject and object are highlighted with designated
colors. As seen in the figure, small sentences in
relative clauses, passive forms or coordination are
retrieved. As the objects of the event verbs are
highlighted, we can easily see what disease dys-
trophin caused. As the target corpus is already
annotated with diseases entities, MEDIE can ef-
ficiently retrieve the disease expressions.
4 Info-PubMed: a GUI-based
MEDLINE search tool
Info-PubMed is a MEDLINE search tool with
GUI, helping users to find information about
biomedical entities such as genes, proteins, and
18
Figure 2: Snapshot of MEDIE: top page?
Figure 3: Snapshot of MEDIE: ?What disease does
dystrophin cause??
the interactions between them 3.
Info-PubMed provides information from MED-
LINE on protein-protein interactions. Given the
name of a gene or protein, it shows a list of the
names of other genes/proteins which co-occur in
sentences from MEDLINE, along with the fre-
quency of co-occurrence.
Co-occurrence of two proteins/genes in the
same sentence does not always imply that they in-
teract. For more accurate extraction of sentences
that indicate interactions, it is necessary to iden-
tify relations between the two substances. We
adopted PASs derived by Enju and constructed ex-
traction patterns on specific verbs and their argu-
ments based on the derived PASs (Yakusiji, 2006).
Figure 4: Snapshot of Info-PubMed (1)
Figure 5: Snapshot of Info-PubMed (2)
Figure 6: Snapshot of Info-PubMed (3)
4.1 Functions of Info-PubMed
In the ?Gene Searcher? window, enter the name
of a gene or protein that you are interested in.
For example, if you are interested in Raf1, type
?raf1? in the ?Gene Searcher? (Figure 4). You
will see a list of genes whose description in our
dictionary contains ?raf1? (Figure 5). Then, drag
3http://www-tsujii.is.s.u-tokyo.ac.jp/info-pubmed/
19
one of the GeneBoxes from the ?Gene Searcher?
to the ?Interaction Viewer.? You will see a list
of genes/proteins which co-occur in the same
sentences, along with co-occurrence frequency.
The GeneBox in the leftmost column is the one
you have moved to ?Interaction Viewer.? The
GeneBoxes in the second column correspond to
gene/proteins which co-occur in the same sen-
tences, followed by the boxes in the third column,
InteractionBoxes.
Drag an InteractionBox to ?ContentViewer? to
see the content of the box (Figure 6). An In-
teractionBox is a set of SentenceBoxes. A Sen-
tenceBox corresponds to a sentence in MEDLINE
in which the two gene/proteins co-occur. A Sen-
tenceBox indicates whether the co-occurrence in
the sentence is direct evidence of interaction or
not. If it is judged as direct evidence of interac-
tion, it is indicated as Interaction. Otherwise, it is
indicated as Co-occurrence.
5 Conclusion
We presented an English HPSG parser, Enju, a
search engine for relational concepts from MED-
LINE, MEDIE, and a GUI-based MEDLINE
search tool, Info-PubMed.
MEDIE and Info-PubMed demonstrate how the
results of deep parsing can be used for intelligent
text mining and semantic information retrieval in
the biomedical domain.
6 Acknowledgment
This work was partially supported by Grant-in-Aid
for Scientific Research on Priority Areas ?Sys-
tems Genomics? (MEXT, Japan) and Solution-
Oriented Research for Science and Technology
(JST, Japan).
References
C. Blaschke and A. Valencia. 2002. The frame-based
module of the SUISEKI information extraction sys-
tem. IEEE Intelligent Systems, 17(2):14?20.
Y. Hao, X. Zhu, M. Huang, and M. Li. 2005. Dis-
covering patterns to extract protein-protein interac-
tions from the literature: Part II. Bioinformatics,
21(15):3294?3300.
H.-W. Chun, Y. Tsuruoka, J.-D. Kim, R. Shiba, N. Na-
gata, T. Hishiki, and J. Tsujii. 2006. Extraction
of gene-disease relations from MedLine using do-
main dictionaries and machine learning. In Proc.
PSB 2006, pages 4?15.
Carl Pollard and Ivan A. Sag. 1994. Head-Driven
Phrase Structure Grammar. University of Chicago
Press.
Yusuke Miyao and Jun?ichi Tsujii. 2005. Probabilis-
tic disambiguation models for wide-coverage HPSG
parsing. In Proc. of ACL?05, pages 83?90.
Tadayoshi Hara, Yusuke Miyao, and Jun?ichi Tsu-
jii. 2005. Adapting a probabilistic disambiguation
model of an HPSG parser to a new domain. In Proc.
of IJCNLP 2005.
Takashi Ninomiya, Yoshimasa Tsuruoka, Yusuke
Miyao, and Jun?ichi Tsujii. 2005. Efficacy of beam
thresholding, unification filtering and hybrid parsing
in probabilistic HPSG parsing. In Proc. of IWPT
2005, pages 103?114.
Yuka Tateisi, Akane Yakushiji, Tomoko Ohta, and
Jun?ichi Tsujii. 2005. Syntax Annotation for the
GENIA corpus. In Proc. of the IJCNLP 2005, Com-
panion volume, pp. 222?227.
Yusuke Miyao, Tomoko Ohta, Katsuya Masuda, Yoshi-
masa Tsuruoka, Kazuhiro Yoshida, Takashi Ni-
nomiya and Jun?ichi Tsujii. 2006. Semantic Re-
trieval for the Accurate Identification of Relational
Concepts in Massive Textbases. In Proc. of ACL ?06,
to appear.
Yoshimasa Tsuruoka, Yuka Tateisi, Jin-Dong Kim,
Tomoko Ohta, John McNaught, Sophia Ananiadou,
and Jun?ichi Tsujii. 2005. Part-of-speech tagger for
biomedical text. In Proc. of the 10th Panhellenic
Conference on Informatics.
Y. Tsuruoka and J. Tsujii. 2004. Improving the per-
formance of dictionary-based approaches in protein
name recognition. Journal of Biomedical Informat-
ics, 37(6):461?470.
Asako Koike and Toshihisa Takagi. 2004.
Gene/protein/family name recognition in biomed-
ical literature. In Proc. of HLT-NAACL 2004
Workshop: Biolink 2004, pages 9?16.
D.A. Lindberg, B.L. Humphreys, and A.T. McCray.
1993. The unified medical language system. Meth-
ods in Inf. Med., 32(4):281?291.
Akane Yakushiji. 2006. Relation Information Extrac-
tion Using Deep Syntactic Analysis. Ph.D. Thesis,
University of Tokyo.
20
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 155?163,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Extremely Lexicalized Models for Accurate and Fast HPSG Parsing
Takashi Ninomiya
Information Technology Center
University of Tokyo
Takuya Matsuzaki
Department of Computer Science
University of Tokyo
Yoshimasa Tsuruoka
School of Informatics
University of Manchester
Yusuke Miyao
Department of Computer Science
University of Tokyo
Jun?ichi Tsujii
Department of Computer Science, University of Tokyo
School of Informatics, University of Manchester
SORST, Japan Science and Technology Agency
Hongo 7-3-1, Bunkyo-ku, Tokyo, 113-0033, Japan
{ninomi, matuzaki, tsuruoka, yusuke, tsujii}@is.s.u-tokyo.ac.jp
Abstract
This paper describes an extremely lexi-
calized probabilistic model for fast and
accurate HPSG parsing. In this model,
the probabilities of parse trees are de-
fined with only the probabilities of select-
ing lexical entries. The proposed model
is very simple, and experiments revealed
that the implemented parser runs around
four times faster than the previous model
and that the proposed model has a high
accuracy comparable to that of the previ-
ous model for probabilistic HPSG, which
is defined over phrase structures. We
also developed a hybrid of our probabilis-
tic model and the conventional phrase-
structure-based model. The hybrid model
is not only significantly faster but also sig-
nificantly more accurate by two points of
precision and recall compared to the pre-
vious model.
1 Introduction
For the last decade, accurate and wide-coverage
parsing for real-world text has been intensively
and extensively pursued. In most of state-of-the-
art parsers, probabilistic events are defined over
phrase structures because phrase structures are
supposed to dominate syntactic configurations of
sentences. For example, probabilities were de-
fined over grammar rules in probabilistic CFG
(Collins, 1999; Klein and Manning, 2003; Char-
niak and Johnson, 2005) or over complex phrase
structures of head-driven phrase structure gram-
mar (HPSG) or combinatory categorial grammar
(CCG) (Clark and Curran, 2004b; Malouf and van
Noord, 2004; Miyao and Tsujii, 2005). Although
these studies vary in the design of the probabilistic
models, the fundamental conception of probabilis-
tic modeling is intended to capture characteristics
of phrase structures or grammar rules. Although
lexical information, such as head words, is known
to significantly improve the parsing accuracy, it
was also used to augment information on phrase
structures.
Another interesting approach to this problem
was using supertagging (Clark and Curran, 2004b;
Clark and Curran, 2004a; Wang and Harper, 2004;
Nasr and Rambow, 2004), which was originally
developed for lexicalized tree adjoining grammars
(LTAG) (Bangalore and Joshi, 1999). Supertag-
ging is a process where words in an input sen-
tence are tagged with ?supertags,? which are lex-
ical entries in lexicalized grammars, e.g., elemen-
tary trees in LTAG, lexical categories in CCG,
and lexical entries in HPSG. Supertagging was,
in the first place, a technique to reduce the cost
of parsing with lexicalized grammars; ambiguity
in assigning lexical entries to words is reduced
by the light-weight process of supertagging be-
fore the heavy process of parsing. Bangalore and
Joshi (1999) claimed that if words can be assigned
correct supertags, syntactic parsing is almost triv-
ial. What this means is that if supertags are cor-
rectly assigned, syntactic structures are almost de-
155
termined because supertags include rich syntac-
tic information such as subcategorization frames.
Nasr and Rambow (2004) showed that the accu-
racy of LTAG parsing reached about 97%, assum-
ing that the correct supertags were given. The
concept of supertagging is simple and interesting,
and the effects of this were recently demonstrated
in the case of a CCG parser (Clark and Curran,
2004a) with the result of a drastic improvement in
the parsing speed. Wang and Harper (2004) also
demonstrated the effects of supertagging with a
statistical constraint dependency grammar (CDG)
parser. They achieved accuracy as high as the
state-of-the-art parsers. However, a supertagger it-
self was used as an external tagger that enumerates
candidates of lexical entries or filters out unlikely
lexical entries just to help parsing, and the best
parse trees were selected mainly according to the
probabilistic model for phrase structures or depen-
dencies with/without the probabilistic model for
supertagging.
We investigate an extreme case of HPSG pars-
ing in which the probabilistic model is defined
with only the probabilities of lexical entry selec-
tion; i.e., the model is never sensitive to charac-
teristics of phrase structures. The model is simply
defined as the product of the supertagging proba-
bilities, which are provided by the discriminative
method with machine learning features of word
trigrams and part-of-speech (POS) 5-grams as de-
fined in the CCG supertagging (Clark and Curran,
2004a). The model is implemented in an HPSG
parser instead of the phrase-structure-based prob-
abilistic model; i.e., the parser returns the parse
tree assigned the highest probability of supertag-
ging among the parse trees licensed by an HPSG.
Though the model uses only the probabilities of
lexical entry selection, the experiments revealed
that it was as accurate as the previous phrase-
structure-based model. Interestingly, this means
that accurate parsing is possible using rather sim-
ple mechanisms.
We also tested a hybrid model of the su-
pertagging and the previous phrase-structure-
based probabilistic model. In the hybrid model,
the probabilities of the previous model are mul-
tiplied by the supertagging probabilities instead
of a preliminary probabilistic model, which is in-
troduced to help the process of estimation by fil-
tering unlikely lexical entries (Miyao and Tsujii,
2005). In the previous model, the preliminary
probabilistic model is defined as the probability
of unigram supertagging. So, the hybrid model
can be regarded as an extension of supertagging
from unigram to n-gram. The hybrid model can
also be regarded as a variant of the statistical CDG
parser (Wang, 2003; Wang and Harper, 2004), in
which the parse tree probabilities are defined as
the product of the supertagging probabilities and
the dependency probabilities. In the experiments,
we observed that the hybrid model significantly
improved the parsing speed, by around three to
four times speed-ups, and accuracy, by around two
points in both precision and recall, over the pre-
vious model. This implies that finer probabilistic
model of lexical entry selection can improve the
phrase-structure-based model.
2 HPSG and probabilistic models
HPSG (Pollard and Sag, 1994) is a syntactic the-
ory based on lexicalized grammar formalism. In
HPSG, a small number of schemata describe gen-
eral construction rules, and a large number of
lexical entries express word-specific characteris-
tics. The structures of sentences are explained us-
ing combinations of schemata and lexical entries.
Both schemata and lexical entries are represented
by typed feature structures, and constraints repre-
sented by feature structures are checked with uni-
fication.
An example of HPSG parsing of the sentence
?Spring has come? is shown in Figure 1. First,
each of the lexical entries for ?has? and ?come?
is unified with a daughter feature structure of the
Head-Complement Schema. Unification provides
the phrasal sign of the mother. The sign of the
larger constituent is obtained by repeatedly apply-
ing schemata to lexical/phrasal signs. Finally, the
parse result is output as a phrasal sign that domi-
nates the sentence.
Given a set W of words and a set F of feature
structures, an HPSG is formulated as a tuple, G =
?L,R?, where
L = {l = ?w,F ?|w ? W, F ? F} is a set of
lexical entries, and
R is a set of schemata; i.e., r ? R is a partial
function: F ? F ? F .
Given a sentence, an HPSG computes a set of
phrasal signs, i.e., feature structures, as a result of
parsing. Note that HPSG is one of the lexicalized
grammar formalisms, in which lexical entries de-
termine the dominant syntactic structures.
156
Spring
HEAD  nounSUBJ  < >COMPS  < > 2HEAD  verbSUBJ  <    >COMPS  <    >1
has
HEAD  verbSUBJ  <    >COMPS  < >1
come
2
head-comp
HEAD  verb
SUBJ  < >
COMPS  < >
HEAD  nounSUBJ  < >COMPS  < >1
=?
Spring
HEAD  nounSUBJ  < >COMPS  < > 2HEAD  verbSUBJ  <    >COMPS  <    >1
has
HEAD  verbSUBJ  <    >COMPS  < >1
come
2
HEAD  verbSUBJ  <    >COMPS  < >1
HEAD  verbSUBJ  < >COMPS  < >
1
subject-head
head-comp
Figure 1: HPSG parsing.
Previous studies (Abney, 1997; Johnson et al,
1999; Riezler et al, 2000; Malouf and van Noord,
2004; Kaplan et al, 2004; Miyao and Tsujii, 2005)
defined a probabilistic model of unification-based
grammars including HPSG as a log-linear model
or maximum entropy model (Berger et al, 1996).
The probability that a parse result T is assigned to
a given sentence w = ?w1, . . . , wn? is
phpsg(T |w) = 1Zw exp
(?
u
?ufu(T )
)
Zw =
?
T ?
exp
(?
u
?ufu(T ?)
)
,
where ?u is a model parameter, fu is a feature
function that represents a characteristic of parse
tree T , and Zw is the sum over the set of all pos-
sible parse trees for the sentence. Intuitively, the
probability is defined as the normalized product
of the weights exp(?u) when a characteristic cor-
responding to fu appears in parse result T . The
model parameters, ?u, are estimated using numer-
ical optimization methods (Malouf, 2002) to max-
imize the log-likelihood of the training data.
However, the above model cannot be easily es-
timated because the estimation requires the com-
putation of p(T |w) for all parse candidates as-
signed to sentence w. Because the number of
parse candidates is exponentially related to the
length of the sentence, the estimation is intractable
for long sentences. To make the model estimation
tractable, Geman and Johnson (Geman and John-
son, 2002) and Miyao and Tsujii (Miyao and Tsu-
jii, 2002) proposed a dynamic programming algo-
rithm for estimating p(T |w). Miyao and Tsujii
HEAD  verbSUBJ  <>COMPS <>
HEAD  nounSUBJ  <>COMPS <>
HEAD  verbSUBJ  <   >COMPS <>
HEAD  verbSUBJ  <   >COMPS <   >
HEAD  verbSUBJ  <   >COMPS <>
subject-head
head-comp
Spring/NN has/VBZ come/VBN
1
1 11 22
froot= <S, has, VBZ,                  >HEAD  verbSUBJ  <NP>COMPS <VP>
fbinary=
head-comp, 1, 0,
1, VP, has, VBZ,                    ,
1, VP, come, VBN,
HEAD  verbSUBJ  <NP>COMPS <VP>
HEAD  verbSUBJ  <NP>COMPS <>
flex= <spring, NN,                    > HEAD  nounSUBJ  <>COMPS <>
Figure 2: Example of features.
(2005) also introduced a preliminary probabilistic
model p0(T |w) whose estimation does not require
the parsing of a treebank. This model is intro-
duced as a reference distribution of the probabilis-
tic HPSG model; i.e., the computation of parse
trees given low probabilities by the model is omit-
ted in the estimation stage. We have
(Previous probabilistic HPSG)
phpsg?(T |w) = p0(T |w) 1Zw exp
(?
u
?ufu(T )
)
Zw =
?
T ?
p0(T ?|w) exp
(?
u
?ufu(T ?)
)
p0(T |w) =
n?
i=1
p(li|wi),
where li is a lexical entry assigned to word wi in T
and p(li|wi) is the probability of selecting lexical
entry li for wi.
In the experiments, we compared our model
with the probabilistic HPSG model of Miyao and
Tsujii (2005). The features used in their model are
combinations of the feature templates listed in Ta-
ble 1. The feature templates fbinary and funary
are defined for constituents at binary and unary
branches, froot is a feature template set for the
root nodes of parse trees, and flex is a feature tem-
plate set for calculating the preliminary probabilis-
tic model. An example of features applied to the
parse tree for the sentence ?Spring has come? is
shown in Figure 2.
157
fbinary =
? r, d, c,
spl, syl, hwl, hpl, hll,
spr, syr, hwr, hpr, hlr
?
funary = ?r, sy, hw, hp, hl?
froot = ?sy, hw, hp, hl?
flex = ?wi, pi, li?
combinations of feature templates for fbinary
?r, d, c, hw, hp, hl?, ?r, d, c, hw, hp?, ?r, d, c, hw, hl?,
?r, d, c, sy, hw?, ?r, c, sp, hw, hp, hl?, ?r, c, sp, hw, hp?,
?r, c, sp, hw, hl?, ?r, c, sp, sy, hw?, ?r, d, c, hp, hl?,
?r, d, c, hp?, ?r, d, c, hl?, ?r, d, c, sy?, ?r, c, sp, hp, hl?,
?r, c, sp, hp?, ?r, c, sp, hl?, ?r, c, sp, sy?
combinations of feature templates for funary
?r, hw, hp, hl?, ?r, hw, hp?, ?r, hw, hl?, ?r, sy, hw?,
?r, hp, hl?, ?r, hp?, ?r, hl?, ?r, sy?
combinations of feature templates for froot
?hw, hp, hl?, ?hw, hp?, ?hw, hl?,
?sy, hw?, ?hp, hl?, ?hp?, ?hl?, ?sy?
combinations of feature templates for flex
?wi, pi, li?, ?pi, li?
r name of the applied schema
d distance between the head words of the daughters
c whether a comma exists between daughters
and/or inside daughter phrases
sp number of words dominated by the phrase
sy symbol of the phrasal category
hw surface form of the head word
hp part-of-speech of the head word
hl lexical entry assigned to the head word
wi i-th word
pi part-of-speech for wi
li lexical entry for wi
Table 1: Features.
3 Extremely lexicalized probabilistic
models
In the experiments, we tested parsing with the pre-
vious model for the probabilistic HPSG explained
in Section 2 and other three types of probabilis-
tic models defined with the probabilities of lexi-
cal entry selection. The first one is the simplest
probabilistic model, which is defined with only
the probabilities of lexical entry selection. It is
defined simply as the product of the probabilities
of selecting all lexical entries in the sentence; i.e.,
the model does not use the probabilities of phrase
structures like the previous models.
Given a set of lexical entries, L, a sentence,
w = ?w1, . . . , wn?, and the probabilistic model
of lexical entry selection, p(li ? L|w, i), the first
model is formally defined as follows:
(Model 1)
pmodel1(T |w) =
n?
i=1
p(li|w, i),
where li is a lexical entry assigned to word wi
in T and p(li|w, i) is the probability of selecting
lexical entry li for wi.
The second model is defined as the product of
the probabilities of selecting all lexical entries in
the sentence and the root node probability of the
parse tree. That is, the second model is also de-
fined without the probabilities on phrase struc-
tures:
(Model 2)
pmodel2(T |w) =
1
Zmodel2 pmodel1(T |w) exp
?
??
?
u
(fu?froot)
?ufu(T )
?
??
Zmodel2 =
?
T ?
pmodel1(T ?|w) exp
?
??
?
u
(fu?froot)
?ufu(T ?)
?
?? ,
where Zmodel2 is the sum over the set of all pos-
sible parse trees for the sentence.
The third model is a hybrid of model 1 and the
previous model. The probabilities of the lexical
entries in the previous model are replaced with the
probabilities of lexical entry selection:
(Model 3)
pmodel3(T |w) =
1
Zmodel3 pmodel1(T |w) exp
(?
u
?ufu(T )
)
Zmodel3 =
?
T ?
pmodel1(T ?|w) exp
(?
u
?ufu(T ?)
)
.
In this study, the same model parameters used
in the previous model were used for phrase struc-
tures.
The probabilities of lexical entry selection,
p(li|w, i), are defined as follows:
(Probabilistic Model of Lexical Entry Selection)
p(li|w, i) = 1Zw exp
(?
u
?ufu(li,w, i)
)
158
fexlex =
?
wi?1, wi, wi+1,
pi?2, pi?1, pi, pi+1, pi+2
?
combinations of feature templates
?wi?1?, ?wi?, ?wi+1?,
?pi?2?, ?pi?1?, ?pi?, ?pi+1?, ?pi+2?, ?pi+3?,
?wi?1, wi?, ?wi, wi+1?,
?pi?1, wi?, ?pi, wi?, ?pi+1, wi?,
?pi, pi+1, pi+2, pi+3?, ?pi?2, pi?1, pi?,
?pi?1, pi, pi+1?, ?pi, pi+1, pi+2?
?pi?2, pi?1?, ?pi?1, pi?, ?pi, pi+1?, ?pi+1, pi+2?
Table 2: Features for the probabilities of lexical
entry selection.
procedure Parsing(?w1, . . . , wn?, ?L,R?, ?, ?, ?, ?, ?)
for i = 1 to n
foreach F ? ? {F |?wi, F ? ? L}
p =
?
u ?ufu(F
?)
pi[i? 1, i] ? pi[i? 1, i] ? {F ?}
if (p > ?[i? 1, i, F ?]) then
?[i? 1, i, F ?] ? p
LocalThresholding(i? 1, i,?, ?)
for d = 1 to n
for i = 0 to n? d
j = i+ d
for k = i+ 1 to j ? 1
foreach Fs ? ?[i, k], Ft ? ?[k, j], r ? R
if F = r(Fs, Ft) has succeeded
p = ?[i, k, Fs] + ?[k, j, Ft] +
?
u ?ufu(F )
pi[i, j] ? pi[i, j] ? {F}
if (p > ?[i, j, F ]) then
?[i, j, F ] ? p
LocalThresholding(i, j,?, ?)
GlobalThresholding(i, n, ?)
procedure IterativeParsing(w, G, ?0, ?0, ?0, ?0, ?0, ??, ??, ??,
??, ??, ?last, ?last, ?last, ?last, ?last)? ? ?0; ? ? ?0; ? ? ?0; ? ? ?0; ? ? ?0;
loop while ? ? ?last and ? ? ?last and ? ? ?last and ? ? ?last
and ? ? ?last
call Parsing(w, G, ?, ?, ?, ?, ?)
if pi[1, n] 6= ? then exit
? ? ?+??; ? ? ? +??;
? ? ?+??; ? ? ? +??; ? ? ? +??;
Figure 3: Pseudo-code of iterative parsing for
HPSG.
Zw =
?
l?
exp
(?
u
?ufu(l?,w, i)
)
,
where Zw is the sum over all possible lexical en-
tries for the word wi. The feature templates used
in our model are listed in Table 2 and are word
trigrams and POS 5-grams.
4 Experiments
4.1 Implementation
We implemented the iterative parsing algorithm
(Ninomiya et al, 2005) for the probabilistic HPSG
models. It first starts parsing with a narrow beam.
If the parsing fails, then the beam is widened, and
parsing continues until the parser outputs results
or the beam width reaches some limit. Though
the probabilities of lexical entry selection are in-
troduced, the algorithm for the presented proba-
bilistic models is almost the same as the original
iterative parsing algorithm.
The pseudo-code of the algorithm is shown in
Figure 3. In the figure, the pi[i, j] represents
the set of partial parse results that cover words
wi+1, . . . , wj , and ?[i, j, F ] stores the maximum
figure-of-merit (FOM) of partial parse result F
at cell (i, j). The probability of lexical entry
F is computed as ?u ?ufu(F ) for the previous
model, as shown in the figure. The probability
of a lexical entry for models 1, 2, and 3 is com-
puted as the probability of lexical entry selection,
p(F |w, i). The FOM of a newly created partial
parse, F , is computed by summing the values of
? of the daughters and an additional FOM of F if
the model is the previous model or model 3. The
FOM for models 1 and 2 is computed by only sum-
ming the values of ? of the daughters; i.e., weights
exp(?u) in the figure are assigned zero. The terms
? and ? are the thresholds of the number of phrasal
signs in the chart cell and the beam width for signs
in the chart cell. The terms ? and ? are the thresh-
olds of the number and the beam width of lexical
entries, and ? is the beam width for global thresh-
olding (Goodman, 1997).
4.2 Evaluation
We evaluated the speed and accuracy of parsing
with extremely lexicalized models by using Enju
2.1, the HPSG grammar for English (Miyao et al,
2005; Miyao and Tsujii, 2005). The lexicon of
the grammar was extracted from Sections 02-21 of
the Penn Treebank (Marcus et al, 1994) (39,832
sentences). The grammar consisted of 3,797 lex-
ical entries for 10,536 words1. The probabilis-
tic models were trained using the same portion of
the treebank. We used beam thresholding, global
thresholding (Goodman, 1997), preserved iterative
parsing (Ninomiya et al, 2005) and other tech-
1An HPSG treebank is automatically generated from the
Penn Treebank. Those lexical entries were generated by ap-
plying lexical rules to observed lexical entries in the HPSG
treebank (Nakanishi et al, 2004). The lexicon, however, in-
cluded many lexical entries that do not appear in the HPSG
treebank. The HPSG treebank is used for training the prob-
abilistic model for lexical entry selection, and hence, those
lexical entries that do not appear in the treebank are rarely
selected by the probabilistic model. The ?effective? tag set
size, therefore, is around 1,361, the number of lexical entries
without those never-seen lexical entries.
159
No. of tested sentences Total No. of Avg. length of tested sentences
? 40 words ? 100 words sentences ? 40 words ? 100 words
Section 23 2,162 (94.04%) 2,299 (100.00%) 2,299 20.7 22.2
Section 24 1,157 (92.78%) 1,245 (99.84%) 1,247 21.2 23.0
Table 3: Statistics of the Penn Treebank.
Section 23 (? 40 + Gold POSs) Section 23 (? 100 + Gold POSs)
LP LR UP UR Avg. time LP LR UP UR Avg. time
(%) (%) (%) (%) (ms) (%) (%) (%) (%) (ms)
previous model 87.65 86.97 91.13 90.42 468 87.26 86.50 90.73 89.93 604
model 1 87.54 86.85 90.38 89.66 111 87.23 86.47 90.05 89.27 129
model 2 87.71 87.02 90.51 89.80 109 87.38 86.62 90.17 89.39 130
model 3 89.79 88.97 92.66 91.81 132 89.48 88.58 92.33 91.40 152
Section 23 (? 40 + POS tagger) Section 23 (? 100 + POS tagger)
LP LR UP UR Avg. time LP LR UP UR Avg. time
(%) (%) (%) (%) (ms) (%) (%) (%) (%) (ms)
previous model 85.33 84.83 89.93 89.41 509 84.96 84.25 89.55 88.80 674
model 1 85.26 84.31 89.17 88.18 133 85.00 84.01 88.85 87.82 154
model 2 85.37 84.42 89.25 88.26 134 85.08 84.09 88.91 87.88 155
model 3 87.66 86.53 91.61 90.43 155 87.35 86.29 91.24 90.13 183
Table 4: Experimental results for Section 23.
niques for deep parsing2. The parameters for beam
searching were determined manually by trial and
error using Section 22: ?0 = 4,?? = 4, ?last =
20, ?0 = 1.0,?? = 2.5, ?last = 11.0, ?0 =
12,?? = 4, ?last = 28, ?0 = 6.0,?? =
2.25, ?last = 15.0, ?0 = 8.0,?? = 3.0, and
?last = 20.0. With these thresholding parame-
ters, the parser iterated at most five times for each
sentence.
We measured the accuracy of the predicate-
argument relations output of the parser. A
predicate-argument relation is defined as a tu-
ple ??,wh, a, wa?, where ? is the predicate type
(e.g., adjective, intransitive verb), wh is the head
word of the predicate, a is the argument label
(MODARG, ARG1, ..., ARG4), and wa is the
head word of the argument. Labeled precision
(LP)/labeled recall (LR) is the ratio of tuples cor-
rectly identified by the parser3. Unlabeled pre-
cision (UP)/unlabeled recall (UR) is the ratio of
tuples without the predicate type and the argu-
ment label. This evaluation scheme was the
same as used in previous evaluations of lexicalized
grammars (Hockenmaier, 2003; Clark and Cur-
2Deep parsing techniques include quick check (Malouf
et al, 2000) and large constituent inhibition (Kaplan et al,
2004) as described by Ninomiya et al (2005), but hybrid
parsing with a CFG chunk parser was not used. This is be-
cause we did not observe a significant improvement for the
development set by the hybrid parsing and observed only a
small improvement in the parsing speed by around 10 ms.
3When parsing fails, precision and recall are evaluated,
although nothing is output by the parser; i.e., recall decreases
greatly.
ran, 2004b; Miyao and Tsujii, 2005). The ex-
periments were conducted on an AMD Opteron
server with a 2.4-GHz CPU. Section 22 of the
Treebank was used as the development set, and
the performance was evaluated using sentences of
? 40 and 100 words in Section 23. The perfor-
mance of each parsing technique was analyzed us-
ing the sentences in Section 24 of ? 100 words.
Table 3 details the numbers and average lengths of
the tested sentences of ? 40 and 100 words in Sec-
tions 23 and 24, and the total numbers of sentences
in Sections 23 and 24.
The parsing performance for Section 23 is
shown in Table 4. The upper half of the table
shows the performance using the correct POSs in
the Penn Treebank, and the lower half shows the
performance using the POSs given by a POS tag-
ger (Tsuruoka and Tsujii, 2005). The left and
right sides of the table show the performances for
the sentences of ? 40 and ? 100 words. Our
models significantly increased not only the pars-
ing speed but also the parsing accuracy. Model
3 was around three to four times faster and had
around two points higher precision and recall than
the previous model. Surprisingly, model 1, which
used only lexical information, was very fast and
as accurate as the previous model. Model 2 also
improved the accuracy slightly without informa-
tion of phrase structures. When the automatic POS
tagger was introduced, both precision and recall
dropped by around 2 points, but the tendency to-
wards improved speed and accuracy was again ob-
160
76.00%
78.00%
80.00%
82.00%
84.00%
86.00%
88.00%
0 100 200 300 400 500 600 700 800 900
Parsing time (ms/sentence)
F
-
s
c
o
r
e
previous model
model 1
model 2
model 3
Figure 4: F-score versus average parsing time for sentences in Section 24 of ? 100 words.
served.
The unlabeled precisions and recalls of the pre-
vious model and models 1, 2, and 3 were signifi-
cantly different as measured using stratified shuf-
fling tests (Cohen, 1995) with p-values < 0.05.
The labeled precisions and recalls were signifi-
cantly different among models 1, 2, and 3 and
between the previous model and model 3, but
were not significantly different between the previ-
ous model and model 1 and between the previous
model and model 2.
The average parsing time and labeled F-score
curves of each probabilistic model for the sen-
tences in Section 24 of? 100 words are graphed in
Figure 4. The superiority of our models is clearly
observed in the figure. Model 3 performed sig-
nificantly better than the previous model. Models
1 and 2 were significantly faster with almost the
same accuracy as the previous model.
5 Discussion
5.1 Supertagging
Our probabilistic model of lexical entry selection
can be used as an independent classifier for select-
ing lexical entries, which is called the supertag-
ger (Bangalore and Joshi, 1999; Clark and Curran,
2004b). The CCG supertagger uses a maximum
entropy classifier and is similar to our model.
We evaluated the performance of our probabilis-
tic model as a supertagger. The accuracy of the re-
sulting supertagger on our development set (Sec-
tion 22) is given in Table 5 and Table 6. The test
sentences were automatically POS-tagged. Re-
sults of other supertaggers for automatically ex-
test data accuracy (%)
HPSG supertagger 22 87.51
(this paper)
CCG supertagger 00/23 91.70 / 91.45
(Curran and Clark, 2003)
LTAG supertagger 22/23 86.01 / 86.27
(Shen and Joshi, 2003)
Table 5: Accuracy of single-tag supertaggers. The
numbers under ?test data? are the PTB section
numbers of the test data.
? tags/word word acc. (%) sentence acc. (%)
1e-1 1.30 92.64 34.98
1e-2 2.11 95.08 46.11
1e-3 4.66 96.22 51.95
1e-4 10.72 96.83 55.66
1e-5 19.93 96.95 56.20
Table 6: Accuracy of multi-supertagging.
tracted lexicalized grammars are listed in Table 5.
Table 6 gives the average number of supertags as-
signed to a word, the per-word accuracy, and the
sentence accuracy for several values of ?, which is
a parameter to determine how many lexical entries
are assigned.
When compared with other supertag sets of au-
tomatically extracted lexicalized grammars, the
(effective) size of our supertag set, 1,361 lexical
entries, is between the CCG supertag set (398 cat-
egories) used by Curran and Clark (2003) and the
LTAG supertag set (2920 elementary trees) used
by Shen and Joshi (2003). The relative order based
on the sizes of the tag sets exactly matches the or-
der based on the accuracies of corresponding su-
pertaggers.
161
5.2 Efficacy of extremely lexicalized models
The implemented parsers of models 1 and 2 were
around four times faster than the previous model
without a loss of accuracy. However, what sur-
prised us is not the speed of the models, but
the fact that they were as accurate as the previ-
ous model, though they do not use any phrase-
structure-based probabilities. We think that the
correct parse is more likely to be selected if the
correct lexical entries are assigned high probabil-
ities because lexical entries include specific infor-
mation about subcategorization frames and syn-
tactic alternation, such as wh-movement and pas-
sivization, that likely determines the dominant
structures of parse trees. Another possible rea-
son for the accuracy is the constraints placed by
unification-based grammars. That is, incorrect
parse trees were suppressed by the constraints.
The best performer in terms of speed and ac-
curacy was model 3. The increased speed was,
of course, possible for the same reasons as the
speeds of models 1 and 2. An unexpected but
very impressive result was the significant improve-
ment of accuracy by two points in precision and
recall, which is hard to attain by tweaking param-
eters or hacking features. This may be because
the phrase structure information and lexical in-
formation complementarily improved the model.
The lexical information includes more specific in-
formation about the syntactic alternation, and the
phrase structure information includes information
about the syntactic structures, such as the dis-
tances of head words or the sizes of phrases.
Nasr and Rambow (2004) showed that the accu-
racy of LTAG parsing reached about 97%, assum-
ing that the correct supertags were given. We ex-
emplified the dominance of lexical information in
real syntactic parsing, i.e., syntactic parsing with-
out gold-supertags, by showing that the proba-
bilities of lexical entry selection dominantly con-
tributed to syntactic parsing.
The CCG supertagging demonstrated fast and
accurate parsing for the probabilistic CCG (Clark
and Curran, 2004a). They used the supertag-
ger for eliminating candidates of lexical entries,
and the probabilities of parse trees were calcu-
lated using the phrase-structure-based model with-
out the probabilities of lexical entry selection. Our
study is essentially different from theirs in that the
probabilities of lexical entry selection have been
demonstrated to dominantly contribute to the dis-
ambiguation of phrase structures.
We have not yet investigated whether our results
can be reproduced with other lexicalized gram-
mars. Our results might hold only for HPSG be-
cause HPSG has strict feature constraints and has
lexical entries with rich syntactic information such
as wh-movement.
6 Conclusion
We developed an extremely lexicalized probabilis-
tic model for fast and accurate HPSG parsing.
The model is very simple. The probabilities of
parse trees are defined with only the probabili-
ties of selecting lexical entries, which are trained
by the discriminative methods in the log-linear
model with features of word trigrams and POS 5-
grams as defined in the CCG supertagging. Ex-
periments revealed that the model achieved im-
pressive accuracy as high as that of the previous
model for the probabilistic HPSG and that the im-
plemented parser runs around four times faster.
This indicates that accurate and fast parsing is pos-
sible using rather simple mechanisms. In addi-
tion, we provided another probabilistic model, in
which the probabilities for the leaf nodes in a parse
tree are given by the probabilities of supertag-
ging, and the probabilities for the intermediate
nodes are given by the previous phrase-structure-
based model. The experiments demonstrated not
only speeds significantly increased by three to four
times but also impressive improvement in parsing
accuracy by around two points in precision and re-
call.
We hope that this research provides a novel ap-
proach to deterministic parsing in which only lex-
ical selection and little phrasal information with-
out packed representations dominates the parsing
strategy.
References
Steven P. Abney. 1997. Stochastic attribute-value
grammars. Computational Linguistics, 23(4):597?
618.
Srinivas Bangalore and Aravind Joshi. 1999. Su-
pertagging: An approach to almost parsing. Com-
putational Linguistics, 25(2):237?265.
Adam Berger, Stephen Della Pietra, and Vincent Della
Pietra. 1996. A maximum entropy approach to nat-
ural language processing. Computational Linguis-
tics, 22(1):39?71.
162
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and maxent discriminative
reranking. In Proc. of ACL?05, pages 173?180.
Stephen Clark and James R. Curran. 2004a. The im-
portance of supertagging for wide-coverage CCG
parsing. In Proc. of COLING-04.
Stephen Clark and James R. Curran. 2004b. Parsing
the WSJ using CCG and log-linear models. In Proc.
of ACL?04, pages 104?111.
Paul R. Cohen. 1995. Empirical Methods for Artificial
Intelligence. The MIT Press.
Michael Collins. 1999. Head-Driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis,
Univ. of Pennsylvania.
James R. Curran and Stephen Clark. 2003. Investigat-
ing GIS and smoothing for maximum entropy tag-
gers. In Proc. of EACL?03, pages 91?98.
Stuart Geman and Mark Johnson. 2002. Dynamic pro-
gramming for parsing and estimation of stochastic
unification-based grammars. In Proc. of ACL?02,
pages 279?286.
Joshua Goodman. 1997. Global thresholding and mul-
tiple pass parsing. In Proc. of EMNLP-1997, pages
11?25.
Julia Hockenmaier. 2003. Parsing with generative
models of predicate-argument structure. In Proc. of
ACL?03, pages 359?366.
Mark Johnson, Stuart Geman, Stephen Canon, Zhiyi
Chi, and Stefan Riezler. 1999. Estimators for
stochastic ?unification-based? grammars. In Proc.
of ACL ?99, pages 535?541.
R. M. Kaplan, S. Riezler, T. H. King, J. T. Maxwell
III, and A. Vasserman. 2004. Speed and accuracy
in shallow and deep stochastic parsing. In Proc. of
HLT/NAACL?04.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proc. of ACL?03,
pages 423?430.
Robert Malouf and Gertjan van Noord. 2004. Wide
coverage parsing with stochastic attribute value
grammars. In Proc. of IJCNLP-04 Workshop ?Be-
yond Shallow Analyses?.
Robert Malouf, John Carroll, and Ann Copestake.
2000. Efficient feature structure operations with-
out compilation. Journal of Natural Language En-
gineering, 6(1):29?46.
Robert Malouf. 2002. A comparison of algorithms for
maximum entropy parameter estimation. In Proc. of
CoNLL-2002, pages 49?55.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1994. Building a large annotated
corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19(2):313?330.
Yusuke Miyao and Jun?ichi Tsujii. 2002. Maximum
entropy estimation for feature forests. In Proc. of
HLT 2002, pages 292?297.
Yusuke Miyao and Jun?ichi Tsujii. 2005. Probabilis-
tic disambiguation models for wide-coverage HPSG
parsing. In Proc. of ACL?05, pages 83?90.
Yusuke Miyao, Takashi Ninomiya, and Jun?ichi Tsujii,
2005. Keh-Yih Su, Jun?ichi Tsujii, Jong-Hyeok Lee
and Oi Yee Kwong (Eds.), Natural Language Pro-
cessing - IJCNLP 2004 LNAI 3248, chapter Corpus-
oriented Grammar Development for Acquiring a
Head-driven Phrase Structure Grammar from the
Penn Treebank, pages 684?693. Springer-Verlag.
Hiroko Nakanishi, Yusuke Miyao, and Jun?ichi Tsujii.
2004. An empirical investigation of the effect of lex-
ical rules on parsing with a treebank grammar. In
Proc. of TLT?04, pages 103?114.
Alexis Nasr and Owen Rambow. 2004. Supertagging
and full parsing. In Proc. of the 7th International
Workshop on Tree Adjoining Grammar and Related
Formalisms (TAG+7).
Takashi Ninomiya, Yoshimasa Tsuruoka, Yusuke
Miyao, and Jun?ichi Tsujii. 2005. Efficacy of beam
thresholding, unification filtering and hybrid pars-
ing in probabilistic hpsg parsing. In Proc. of IWPT
2005, pages 103?114.
Carl Pollard and Ivan A. Sag. 1994. Head-Driven
Phrase Structure Grammar. University of Chicago
Press.
Stefan Riezler, Detlef Prescher, Jonas Kuhn, and Mark
Johnson. 2000. Lexicalized stochastic modeling
of constraint-based grammars using log-linear mea-
sures and EM training. In Proc. of ACL?00, pages
480?487.
Libin Shen and Aravind K. Joshi. 2003. A SNoW
based supertagger with application to NP chunking.
In Proc. of ACL?03, pages 505?512.
Yoshimasa Tsuruoka and Jun?ichi Tsujii. 2005. Bidi-
rectional inference with the easiest-first strategy for
tagging sequence data. In Proc. of HLT/EMNLP
2005, pages 467?474.
Wen Wang and Mary P. Harper. 2004. A statisti-
cal constraint dependency grammar (CDG) parser.
In Proc. of ACL?04 Incremental Parsing work-
shop: Bringing Engineering and Cognition To-
gether, pages 42?49.
Wen Wang. 2003. Statistical Parsing and Language
Modeling based on Constraint Dependency Gram-
mar. Ph.D. thesis, Purdue University.
163
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 447?456,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
A Discriminative Candidate Generator for String Transformations
Naoaki Okazaki?
okazaki@is.s.u-tokyo.ac.jp
Yoshimasa Tsuruoka?
yoshimasa.tsuruoka@manchester.ac.uk
Sophia Ananiadou?
sophia.ananiadou@manchester.ac.uk
Jun?ichi Tsujii??
tsujii@is.s.u-tokyo.ac.jp
?Graduate School of Information
Science and Technology
University of Tokyo
7-3-1 Hongo, Bunkyo-ku
Tokyo 113-8656, Japan
?School of Computer Science,
University of Manchester
National Centre for Text Mining (NaCTeM)
Manchester Interdisciplinary Biocentre
131 Princess Street, Manchester M1 7DN, UK
Abstract
String transformation, which maps a source
string s into its desirable form t?, is related
to various applications including stemming,
lemmatization, and spelling correction. The
essential and important step for string trans-
formation is to generate candidates to which
the given string s is likely to be transformed.
This paper presents a discriminative approach
for generating candidate strings. We use sub-
string substitution rules as features and score
them using an L1-regularized logistic regres-
sion model. We also propose a procedure to
generate negative instances that affect the de-
cision boundary of the model. The advantage
of this approach is that candidate strings can
be enumerated by an efficient algorithm be-
cause the processes of string transformation
are tractable in the model. We demonstrate
the remarkable performance of the proposed
method in normalizing inflected words and
spelling variations.
1 Introduction
String transformation maps a source string s into its
destination string t?. In the broad sense, string trans-
formation can include labeling tasks such as part-
of-speech tagging and shallow parsing (Brill, 1995).
However, this study addresses string transformation
in its narrow sense, in which a part of a source string
is rewritten with a substring. Typical applications of
this task include stemming, lemmatization, spelling
correction (Brill and Moore, 2000; Wilbur et al,
2006; Carlson and Fette, 2007), OCR error correc-
tion (Kolak and Resnik, 2002), approximate string
matching (Navarro, 2001), and duplicate record de-
tection (Bilenko and Mooney, 2003).
Recent studies have formalized the task in the dis-
criminative framework (Ahmad and Kondrak, 2005;
Li et al, 2006; Chen et al, 2007),
t? = argmax
t?gen(s)
P (t|s). (1)
Here, the candidate generator gen(s) enumerates
candidates of destination (correct) strings, and the
scorer P (t|s) denotes the conditional probability of
the string t for the given s. The scorer was modeled
by a noisy-channel model (Shannon, 1948; Brill and
Moore, 2000; Ahmad and Kondrak, 2005) and max-
imum entropy framework (Berger et al, 1996; Li et
al., 2006; Chen et al, 2007).
The candidate generator gen(s) also affects the
accuracy of the string transformation. Previous stud-
ies of spelling correction mostly defined gen(s),
gen(s) = {t | dist(s, t) < ?}. (2)
Here, the function dist(s, t) denotes the weighted
Levenshtein distance (Levenshtein, 1966) between
strings s and t. Furthermore, the threshold ? requires
the distance between the source string s and a can-
didate string t to be less than ?.
The choice of dist(s, t) and ? involves a tradeoff
between the precision, recall, and training/tagging
speed of the scorer. A less restrictive design of these
factors broadens the search space, but it also in-
creases the number of confusing candidates, amount
of feature space, and computational cost for the
scorer. Moreover, the choice is highly dependent on
the target task. It might be sufficient for a spelling
447
correction program to gather candidates from known
words, but a stemmer must handle unseen words ap-
propriately. The number of candidates can be huge
when we consider transformations from and to un-
seen strings.
This paper addresses these challenges by explor-
ing the discriminative training of candidate genera-
tors. More specifically, we build a binary classifier
that, when given a source string s, decides whether
a candidate t should be included in the candidate set
or not. This approach appears straightforward, but it
must resolve two practical issues. First, the task of
the classifier is not only to make a binary decision
for the two strings s and t, but also to enumerate a
set of positive strings for the string s,
gen(s) = {t | predict(s, t) = 1}. (3)
In other words, an efficient algorithm is necessary
to find a set of strings with which the classifier
predict(s, t) yields positive labels for the string s.
Another issue arises when we prepare a training
set. A discriminative model requires a training set
in which each instance (pair of strings) is annotated
with a positive or negative label. Even though some
existing resources (e.g., inflection table and query
log) are available for positive instances, such re-
sources rarely contain negative instances. Therefore,
we must generate negative instances that are effec-
tive for discriminative training.
To address the first issue, we design features that
express transformations from a source string s to its
destination string t. Feature selection and weight-
ing are performed using an L1-regularized logistic
regression model, which can find a sparse solution
to the classification model. We also present an al-
gorithm that utilizes the feature weights to enumer-
ate candidates of destination strings efficiently. We
deal with the second issue by generating negative
instances from unlabeled instances. We describe a
procedure to choose negative instances that affect
the decision boundary of the classifier.
This paper is organized as follows. Section 2 for-
malizes the task of the candidate generator as a bi-
nary classification modeled by logistic regression.
Features for the classifier are designed using the
rules of substring substitution. Therefore, we can
obtain, efficiently, candidates of destination strings
and negative instances for training. Section 3 re-
ports the remarkable performance of the proposed
method in various applications including lemmati-
zation, spelling normalization, and noun derivation.
We briefly review previous work in Section 4, and
conclude this paper in Section 5.
2 Candidate generator
2.1 Candidate classification model
In this section, we first introduce a binary classifier
that yields a label y ? {0, 1} indicating whether a
candidate t should be included in the candidate set
(1) or not (0), given a source string s. We express
the conditional probability P (y|s, t) using a logistic
regression model,
P (1|s, t) =
1
1 + exp (??TF (s, t))
, (4)
P (0|s, t) = 1? P (1|s, t). (5)
In these equations, F = {f1, ..., fK} denotes a vec-
tor of the Boolean feature functions; K is the num-
ber of feature functions; and ? = {?1, ..., ?K}
presents a weight vector of the feature functions.
We obtain the following decision rule to choose
the most probable label y? for a given pair ?s, t?,
y? = argmax
y?{0,1}
P (y|s, t) =
{
1
(
?TF (s, t) > 0
)
0 (otherwise)
.
(6)
Finally, given a source string s, the generator func-
tion gen(s) is defined to collect all strings to which
the classifier assigns positive labels:
gen(s) = {t | P (1|s, t) > P (0|s, t)}
= {t | ?TF (s, t) > 0}. (7)
2.2 Substitution rules as features
The binary classifier can include any arbitrary fea-
ture. This is exemplified by the Levenshtein dis-
tance and distributional similarity (Lee, 1999) be-
tween two strings s and t. These features can im-
prove the classification accuracy, but it is unrealistic
to compute these features for every possible string,
as in equation 7. For that reason, we specifically
examine substitution rules, with which the process
448
^oestrogen$
^estrogen$
^anaemia$
^anemia$
^studies$
^study$
('o', ''), ('^o', '^'), ('oe', 'e'),
('^oe', '^e'), ('^oes', '^es'), ...
('a', ''), ('na', 'n'), ('ae', 'e'),
('ana', 'an'), ('nae', 'ne'), ('aem', 'em'),
...
('ies', 'y'), ('dies', 'dy'), ('ies$', 'y$'),
('udies', 'udy'), ('dies$', 'dy$'), ...
S:
t:
S:
t:
S:
t:
(1)
(2)
(3)
Figure 1: Generating substitution rules.
of transforming a source string s into its destination
form t is tractable.
In this study, we assume that every string has a
prefix ??? and postfix ?$?, which indicate the head
and tail of a string. A substitution rule r = (?, ?)
replaces every occurrence of the substring ? in a
source string into the substring ?. Assuming that a
string s can be transformed into another string twith
a single substitution operation, substitution rules ex-
press the different portion between strings s and t.
Equation 8 defines a binary feature function with
a substitution rule between two strings s and t,
fk(s, t) =
{
1 (rule rk can convert s into t)
0 (otherwise)
.
(8)
We allow multiple substitution rules for a given pair
of strings. For instance, substitution rules (?a?,
??), (?na?, ?n?), (?ae?, ?e?), (?nae?, ?ne?), etc.
form feature functions that yield 1 for strings s =
??anaemia$? and t = ??anemia$?. Equation
6 produces a decision based on the sum of feature
weights, or scores of substitution rules, representing
the different portions between s and t.
Substitution rules for the given two strings s and
t are obtained as follows. Let l denote the longest
common prefix between strings s and t, and r the
longest common postfix. We define cs as the sub-
string in s that is not covered by the longest common
prefix l and postfix r, and define ct for t analogously.
In other words, strings s and t are divided into three
regions, lcsr and lctr, respectively. For strings s =
??anaemia$? and t = ??anemia$? in Figure 1
(2), we obtain cs = ?a? and ct = ?? because l =
??an? and r = ?emia$?.
Because substrings cs and ct express different
portions between strings s and t, we obtain the mini-
mum substitution rule (cs, ct), which can convert the
string s into t by replacing substrings cs in s with
ct; the minimum substitution rule for the same ex-
ample is (?a?, ??). However, replacing letters ?a?
in ??anaemia$? into empty letters does not pro-
duce the correct string ??anemia$? but ??nemi$?.
Furthermore, the rule might be inappropriate for ex-
pressing string transformation because it always re-
moves the letter ?a? from every string.
Therefore, we also obtain expanded substitution
rules, which insert postfixes of l to the head of min-
imum substitution rules, and/or append prefixes of
r to the rules. For example, we find an expanded
substitution rule (?na?, ?n?), by inserting a postfix
of l = ??an? to the head of the minimum substitu-
tion rule (?a?, ??); similarly, we obtain an expanded
substitution rule (?ae?, ?e?), by appending a prefix
of r = ?emia$? to the tail of the rule (?a?, ??).
Figure 1 displays examples of substitution rules
(the right side) for three pairs of strings (the left
side). Letters in blue, green, and red respectively
represent the longest common prefixes, longest com-
mon postfixes, and different portions. In this study,
we expand substitution rules such that the number of
letters in rules is does not pass a threshold ?1.
2.3 Parameter estimation
Given a training set that consists of N instances,
D =
(
(s(1), t(1), y(1)), ..., (s(N), t(N), y(N))
)
, we
optimize the feature weights in the logistic regres-
sion model by maximizing the log-likelihood of the
conditional probability distribution,
L? =
N?
i=1
logP (y(i)|s(i), t(i)). (9)
The partial derivative of the log-likelihood with re-
spect to a feature weight ?k is given as equation 10,
?L?
??k
=
N?
i=1
{
y(i) ? P (1|s(i), t(i))
}
fk(s
(i), t(i)).
(10)
The maximum likelihood estimation (MLE) is
known to suffer from overfitting the training set. The
1The number of letters for a substitution rule r = (?, ?) is
defined as the sum of the quantities of letters in ? and ?, i.e.,
|?|+ |?|. We determined the threshold ? = 12 experimentally.
449
common approach for addressing this issue is to use
the maximum a posteriori (MAP) estimation, intro-
ducing a regularization term of the feature weights
?, i.e., a penalty on large feature weights. In addi-
tion, the generation algorithm of substitution rules
might produce inappropriate rules that transform a
string incorrectly, or overly specific rules that are
used scarcely. Removing unnecessary substitution
rules not only speeds up the classifier but also the
algorithm for candidate generation, as presented in
Section 2.4.
In recent years, L1 regularization has received in-
creasing attention because it produces a sparse so-
lution of feature weights in which numerous fea-
ture weights are zero (Tibshirani, 1996; Ng, 2004).
Therefore, we regularize the log-likelihood with the
L1 norm of the weight vector ? and define the final
form the objective function to be minimized as
E? = ?L? +
|?|
?
. (11)
Here, ? is a parameter to control the effect of L1
regularization; the smaller the value we set to ?,
the more features the MAP estimation assigns zero
weights to: it removes a number of features from the
model. Equation 11 is minimized using the Orthant-
Wise Limited-memory Quasi-Newton (OW-LQN)
method (Andrew and Gao, 2007) because the second
term of equation 11 is not differentiable at ?k = 0.
2.4 Candidate generation
The advantage of our feature design is that we can
enumerate strings to which the classifier is likely to
assign positive labels. We start by observing the nec-
essary condition for t in equation 7,
?TF (s, t) > 0? ?k : fk(s, t) = 1 ? ?k > 0.
(12)
The classifier might assign a positive label to strings
s and t when at least one feature function whose
weight is positive can transform s to t.
Let R+ be a set of substitution rules to which
MAP estimation has assigned positive feature
weights. Because each feature corresponds to a sub-
stitution rule, we can obtain gen(s) for a given string
s by application of every substitution rule r ? R+,
gen(s) = {r(s) | r ? R+ ??TF (s, r(s)) > 0}.
(13)
Input: s = (s1, ..., sl): an input string s (series of letters)
Input: D: a trie dictionary containing positive features
Output: T : gen(s)
T = {};1
U = {};2
foreach i ? (1, ..., |s|) do3
F ? D.prefix search(s, i);4
foreach f ? F do5
if f /? U then6
t? f .apply(s);7
if classify(s, t) = 1 then8
add t to T ;9
end10
add f to U ;11
end12
end13
end14
return T ;15
Algorithm 1: A pseudo-code for gen(s).
Here, r(s) presents the string to which the substitu-
tion rule r transforms the source string s. We can
compute gen(s) with a small computational cost if
the MAP estimation with L1 regularization reduces
the number of active features.
Algorithm 1 represents a pseudo-code for obtain-
ing gen(s). To search for positive substitution rules
efficiently, the code stores a set of rules in a trie
structure. In line 4, the code obtains a set of positive
substitution rules F that can rewrite substrings start-
ing at offset #i in the source string s. For each rule
f ? F , we obtain a candidate string t by application
of the substitution rule f to the source string s (line
7). The candidate string t is qualified to be included
in gen(s) when the classifier assigns a positive label
to strings s and t (lines 8 and 9). Lines 6 and 11 pre-
vent the algorithm from repeating evaluation of the
same substitution rule.
2.5 Generating negative instances
The parameter estimation requires a training set D
in which each instance (pair of strings) is annotated
with a positive or negative label. Negative instances
(counter examples) are essential for penalizing in-
appropriate substitution rules, e.g. (?a?, ??). Even
though some existing resources (e.g. verb inflection
table) are available for positive instances, such re-
sources rarely contain negative instances.
A common approach for handling this situation
is to assume that every pair of strings in a resource
450
Input: D+ = [(s1, t1), ..., (sl, tl)]: positive instances
Input: V : a suffix array of all strings (vocabulary)
Output: D?: negative instances
Output: R: substitution rules (features)
D? = [];1
R = {};2
foreach d ? D+ do3
foreach r ? features(d) do4
add r to R;5
end6
end7
foreach r ? R do8
S ? V .search(r.src);9
foreach s ? S do10
t? r.apply(s);11
if (s, t) /? D+ then12
if t ? V then13
append (s, t) to D?;14
end15
end16
end17
end18
return D?, R;19
Algorithm 2: Generating negative instances.
is a negative instance; however, negative instances
amount to ca. V (V ? 1)/2, where V represents the
total number of strings. Moreover, substitution rules
expressing negative instances are innumerable and
sparse because the different portions are peculiar to
individual negative instances. For instance, the min-
imum substitution rule for unrelated words anaemia
and around is (?naemia?, ?round?), but the rule
cannot be too specific to generalize the conditions
for other negative instances.
In this study, we generate negative instances so
that they can penalize inappropriate rules and settle
the decision boundary of the classifier. This strat-
egy is summarized as follows. We consider every
pair of strings as candidates for negative instances.
We obtain substitution rules for the pair using the
same algorithm as that described in Section 2.2 if a
string pair is not included in the dictionary (i.e., not
in positive instances). The pair is used as a nega-
tive instance only when any substitution rule gener-
ated from the pair also exists in the substitution rules
generated from positive instances.
Algorithm 2 presents the pseudo-code that imple-
ments the strategy for generating negative instances
efficiently. First, we presume that we have positive
instances D+ = [(s1, t1), ..., (sl, tl)] and unlabeled
Table Description # Entries
LRSPL Spelling variants 90,323
LRNOM Nominalizations (derivations) 14,029
LRAGR Agreement and inflection 910,854
LRWD Word index (vocabulary) 850,236
Table 1: Excerpt of tables in the SPECIALIST Lexicon.
Data set # + # - # Rules
Orthography 15,830 33,296 11,098
Derivation 12,988 85,928 5,688
Inflection 113,215 124,747 32,278
Table 2: Characteristics of datasets.
strings V . For example, positive instance D+ repre-
sent orthographic variants, and unlabeled strings V
include all possible words (vocabulary). We insert
the vocabulary into a suffix array, which is used to
locate every occurrence of substrings in V .
The algorithm first generates substitution rules R
only from positive instances D+ (lines 3 to 7). For
each substitution rule r ? R, we enumerate known
strings S that contain the source substring r.src (line
9). We apply the substitution rule to each string s ?
S and obtain its destination string t (line 11). If the
pair of strings ?s, t? is not included in D+ (line 12),
and if the destination string t is known (line 13), the
substitution rule r might associate incorrect strings
s and t, which do not exist in D+. Therefore, we
insert the pair to the negative set D? (line 14).
3 Evaluation
3.1 Experiments
We evaluated the candidate generator using three
different tasks: normalization of orthographic vari-
ants, noun derivation, and lemmatization. The
datasets for these tasks were obtained from the
UMLS SPECIALIST Lexicon2, a large lexicon that
includes both commonly occurring English words
and biomedical vocabulary. Table 1 displays the list
of tables in the SPECIALIST Lexicon that were used
in our experiments. We prepared three datasets, Or-
thography, Derivation, and Inflection.
The Orthography dataset includes spelling vari-
ants (e.g., color and colour) in the LRSPL table. We
2UMLS SPECIALIST Lexicon:
http://specialist.nlm.nih.gov/
451
chose entries as positive instances in which spelling
variants are caused by (case-insensitive) alphanu-
meric changes3. The Derivation dataset was built di-
rectly from the LRNOM table, which includes noun
derivations such as abandon ? abandonment. The
LRAGR table includes base forms and their inflec-
tional variants of nouns (singular and plural forms),
verbs (infinitive, third singular, past, past participle
forms, etc), and adjectives/adverbs (positive, com-
parative, and superlative forms). For the Inflection
dataset, we extracted the entries in which inflec-
tional forms differ from their base forms4, e.g., study
? studies.
For each dataset, we applied the algorithm de-
scribed in Section 2.5 to generate substitution rules
and negative instances. Table 2 shows the number of
positive instances (# +), negative instances (# -), and
substitution rules (# Rules). We evaluated the per-
formance of the proposed method in two different
goals of the tasks: classification (Section 3.2) and
normalization (Section 3.3).
3.2 Experiment 1: Candidate classification
In this experiment, we measured the performance
of the classification task in which pairs of strings
were assigned with positive or negative labels.
We trained and evaluated the proposed method
by performing ten-fold cross validation on each
dataset5. Eight baseline systems were prepared
for comparison: Levenshtein distance (LD), nor-
malized Levenshtein distance (NLD), Dice coef-
ficient on letter bigrams (DICE) (Adamson and
Boreham, 1974), Longest Common Substring Ra-
tio (LCSR) (Melamed, 1999), Longest Common
Prefix Ratio (PREFIX) (Kondrak, 2005), Porter?s
stemmer (Porter, 1980), Morpha (Minnen et al,
2001), and CST?s lemmatiser (Dalianis and Jonge-
3LRSPL table includes trivial spelling variants that can be
handled using simple character/string operations. For example,
the table contains spelling variants related to case sensitivity
(e.g., deg and Deg) and symbols (e.g., Feb and Feb.).
4LRAGR table also provides agreement information even
when word forms do not change. For example, the table con-
tains an entry indicating that the first-singular present form of
the verb study is study, which might be readily apparent to En-
glish speakers.
5We determined the regularization parameter ? = 5 experi-
mentally. Refer to Figure 2 for the performance change.
jan, 2006)6.
The five systems LD, NLD, DICE, LCSR, and
PREFIX employ corresponding metrics of string
distance or similarity. Each system assigns a posi-
tive label to a given pair of strings ?s, t? if the dis-
tance/similarity of strings s and t is smaller/larger
than the threshold ? (refer to equation 2 for distance
metrics). The threshold of each system was chosen
so that the system achieves the best F1 score.
The remaining three systems assign a positive la-
bel only if the system transforms the strings s and
t into the identical string. For example, a pair of
two words studies and study is classified as positive
by Porter?s stemmer, which yields the identical stem
studi for these words. We trained CST?s lemmatiser
for each dataset to obtain flex patterns that are used
for normalizing word inflections.
To examine the performance of the L1-
regularized logistic regression as a discriminative
model, we also built two classifiers based on the
Support Vector Machine (SVM). These SVM
classifiers were implemented by the SVMperf 7 on
a linear kernel8. An SVM classifier employs the
same feature set (substitution rules) as the proposed
method so that we can directly compare the L1-
regularized logistic regression and the linear-kernel
SVM. Another SVM classifier incorporates the five
string metrics; this system can be considered as our
reproduction of the discriminative string similarity
proposed by Bergsma and Kondrak (2007).
Table 3 reports the precision (P), recall (R), and
F1 score (F1) based on the number of correct de-
cisions for positive instances. The proposed method
outperformed the baseline systems, achieving 0.919,
0.888, and 0.984 of F1 scores, respectively. Porter?s
stemmer worked on the Inflection set, but not on
the Orthography set, which is beyond the scope of
the stemming algorithms. CST?s lemmatizer suf-
fered from low recall on the Inflection set because
it removed suffixes of base forms, e.g., (cloning,
clone) ? (clone, clo). Morpha and CST?s lemma-
6We used CST?s lemmatiser version 2.13:
http://www.cst.dk/online/lemmatiser/uk/
index.html
7SVM for Multivariate Performance Measures (SVMperf ):
http://svmlight.joachims.org/svm_perf.html
8We determined the parameter C = 500 experimentally; it
controls the tradeoff between training error and margin.
452
System Orthography Derivation Inflection
P R F1 P R F1 P R F1
Levenshtein distance (? = 1) .319 .871 .467 .004 .006 .005 .484 .679 .565
Levenshtein distance .323 .999 .488 .131 1.00 .232 .479 .988 .646
Normalized Levenshtein distance .441 .847 .580 .133 .990 .235 .598 .770 .673
Dice coefficient (letter bigram) .401 .918 .558 .137 .984 .240 .476 1.00 .645
LCSR .322 1.00 .487 .156 .841 .263 .476 1.00 .645
PREFIX .418 .927 .576 .140 .943 .244 .476 1.00 .645
Porter stemmer (Porter, 1980) .084 .074 .079 .197 .846 .320 .926 .839 .881
Morpha (Minnen et al, 2001) .009 .007 .008 .012 .022 .016 .979 .836 .902
CST?s lemmatiser (Dalianis et al 2006) .119 .008 .016 .383 .682 .491 .821 .176 .290
Proposed method .941 .898 .919 .896 .880 .888 .985 .986 .984
Substitution rules trained with SVM .943 .890 .916 .894 .886 .890 .980 .987 .983
+ LD, NLD, DICE, LCSR, PREFIX .946 .906 .926 .894 .886 .890 .980 .987 .983
Table 3: Performance of candidate classification
Rank Src Dst Weight Examples
1 uss us 9.81 focussing
2 aev ev 9.56 mediaeval
3 aen en 9.53 ozaena
4 iae$ ae$ 9.44 gadoviae
5 nni ni 9.16 prorennin
6 nne ne 8.84 connexus
7 our or 8.54 colour
8 aea ea 8.31 paean
9 aeu eu 8.22 stomodaeum
10 ooll ool 7.79 woollen
Table 4: Feature weights for the Orthography set
tizer were not designed for orthographic variants and
noun derivations.
Levenshtein distance (? = 1) did not work for
the Derivation set because noun derivations often
append two or more letters (e.g., happy ? happi-
ness). No string similarity/distance metrics yielded
satisfactory results. Some metrics obtained the best
F1 scores with extreme thresholds only to classify
every instance as positive. These results imply the
difficulty of the string metrics for the tasks.
The L1-regularized logistic regression was com-
parable to the SVM with linear kernel in this exper-
iment. However, the presented model presents the
advantage that it can reduce the number of active
features (features with non-zero weights assigned);
the L1 regularization can remove 74%, 48%, and
82% of substitution rules in each dataset. The
performance improvements by incorporating string
metrics as features were very subtle (less than 0.7%).
What is worse, the distance/similarity metrics do not
specifically derive destination strings to which the
classifier is likely to assign positive labels. There-
fore, we can no longer use the efficient algorithm
as a candidate generator (in Section 2.4) with these
features.
Table 4 demonstrates the ability of our approach
to obtain effective features; the table shows the top
10 features with high weights assigned for the Or-
thography data. An interesting aspect of the pro-
posed method is that the process of the orthographic
variants is interpretable through the feature weights.
Figure 2 shows plots of the F1 scores (y-axis) for
the Inflection data when we change the number of
active features (x-axis) by controlling the regular-
ization parameter ? from 0.001 to 100. The larger
the value we set for ?, the better the classifier per-
forms, generally, with more active features. In ex-
treme cases, the number of active features drops to
97 with ? = 0.01; nonetheless, the classifier still
achieves 0.961 of the F1 score. The result suggests
that a small set of substitution rules can accommo-
date most cases of inflectional variations.
3.3 Experiment 2: String transformation
The second experiment examined the performance
of the string normalization tasks formalized in equa-
tion 1. In this task, a system was given a string s and
was required to yield either its transformed form t?
(s 6= t?) or the string s itself when the transforma-
tion is unnecessary for s. The conditional probabil-
ity distribution (scorer) in equation 1 was modeled
453
System Orthography Derivation Inflection XTAG morph 1.5
P R F1 P R F1 P R F1 P R F1
Morpha .078 .012 .021 .233 .016 .029 .435 .682 .531 .830 .587 .688
CST?s lemmatiser .135 .160 .146 .378 .732 .499 .367 .762 .495 .584 .589 .587
Proposed method .859 .823 .841 .979 .981 .980 .973 .979 .976 .837 .816 .827
Table 5: Performance of string transformation
0.96
0.965
0.97
0.975
0.98
0.985
0.99
0 1000 2000 3000 4000 5000 6000 7000
F1 s
core
Number of active features (with non-zero weights)
Spelling variation
Figure 2: Number of active features and performance.
by the maximum entropy framework. Features for
the maximum entropy model consist of: substitution
rules between strings s and t, letter bigrams and tri-
grams in s, and letter bigrams and trigrams in t.
We prepared four datasets, Orthography, Deriva-
tion, Inflection, and XTAG morphology. Each
dataset is a list of string pairs ?s, t? that indicate
the transformation of the string s into t. A source
string s is identical to its destination string t when
string s should not be changed. These instances
correspond to the case where string s has already
been lemmatized. For each string pair (s, t) in LR-
SPL9, LRNOM, and LRAGR tables, we generated
two instances ?s, t? and ?t, t?. Consequently, a sys-
tem is expected to leave the string t unchanged. We
also used XTAG morphology10 to perform a cross-
domain evaluation of the lemmatizer trained on the
Inflection dataset11. The entries in XTAG morphol-
9We define that s precedes t in dictionary order.
10XTAG morphology database 1.5:
ftp://ftp.cis.upenn.edu/pub/xtag/morph-1.
5/morph-1.5.tar.gz
11We found that XTAG morphology contains numerous in-
ogy that also appear in the Inflection dataset were
39,130 out of 317,322 (12.3 %). We evaluated
the proposed method and CST?s lemmatizer by per-
forming ten-fold cross validation.
Table 5 reports the performance based on the
number of correct transformations. The proposed
method again outperformed the baseline systems
with a wide margin. It is noteworthy that the pro-
posed method can accommodate morphological in-
flections in the XTAG morphology corpus with no
manual tuning or adaptation.
Although we introduced no assumptions about
target tasks (e.g. a known vocabulary), the aver-
age number of positive substitution rules relevant
to source strings was as small as 23.9 (in XTAG
morphology data). Therefore, the candidate gen-
erator performed 23.9 substitution operations for a
given string. It applied the decision rules (equa-
tion 7) 21.3 times, and generated 1.67 candidate
strings per source string. The experimental results
described herein demonstrated that the candidate
generator was modeled successfully by the discrim-
inative framework.
4 Related work
The task of string transformation has a long history
in natural language processing and information re-
trieval. As described in Section 1, this task is re-
lated closely to various applications. Therefore, we
specifically examine several prior studies that are
relevant to this paper in terms of technical aspects.
Some researchers have reported the effectiveness
of the discriminative framework of string similarity.
MaCallum et al (2005) proposed a method to train
the costs of edit operations using Conditional Ran-
dom Fields (CRFs). Bergsma and Kondrak (2007)
correct comparative and superlative adjectives, e.g., unpopular
? unpopularer ? unpopularest and refundable ? refundabler
? refundablest. Therefore, we removed inflection entries for
comparative and superlative adjectives from the dataset.
454
presented an alignment-based discriminative string
similarity. They extracted features from substring
pairs that are consistent to a character-based align-
ment of two strings. Aramaki et al (2008) also used
features that express the different segments of the
two strings. However, these studies are not suited for
a candidate generator because the processes of string
transformations are intractable in their discrimina-
tive models.
Dalianis and Jongejan (2006) presented a lem-
matiser based on suffix rules. Although they pro-
posed a method to obtain suffix rules from a training
data, the method did not use counter-examples (neg-
atives) for reducing incorrect string transformations.
Tsuruoka et al (2008) proposed a scoring method
for discovering a list of normalization rules for dic-
tionary look-ups. However, their objective was to
transform given strings, so that strings (e.g., studies
and study) referring to the same concept in the dic-
tionary are mapped into the same string (e.g., stud);
in contrast, this study maps strings into their destina-
tion strings that were specified by the training data.
5 Conclusion
We have presented a discriminative approach for
generating candidates for string transformation.
Unlike conventional spelling-correction tasks, this
study did not assume a fixed set of destination
strings (e.g. correct words), but could even generate
unseen candidate strings. We used anL1-regularized
logistic regression model with substring-substitution
features so that candidate strings for a given string
can be enumerated using the efficient algorithm. The
results of experiments described herein showed re-
markable improvements and usefulness of the pro-
posed approach in three tasks: normalization of or-
thographic variants, noun derivation, and lemmati-
zation.
The method presented in this paper allows only
one region of change in string transformation. A
natural extension of this study is to handle mul-
tiple regions of changes for morphologically rich
languages (e.g. German) and to handle changes
at the phrase/term level (e.g., ?estrogen receptor?
and ?receptor of oestrogen?). Another direction
would be to incorporate the methodologies for semi-
supervised machine learning to accommodate situa-
tions in which positive instances and/or unlabeled
strings are insufficient.
Acknowledgments
This work was partially supported by Grants-in-Aid
for Scientific Research on Priority Areas (MEXT,
Japan), and for Solution-Oriented Research for Sci-
ence and Technology (JST, Japan).
References
George W. Adamson and Jillian Boreham. 1974. The
use of an association measure based on character struc-
ture to identify semantically related pairs of words and
document titles. Information Storage and Retrieval,
10(7-8):253?260.
Farooq Ahmad and Grzegorz Kondrak. 2005. Learning
a spelling error model from search query logs. In Pro-
ceedings of the conference on Human Language Tech-
nology and Empirical Methods in Natural Language
Processing (HLT-EMNLP 2005), pages 955?962.
Galen Andrew and Jianfeng Gao. 2007. Scalable train-
ing of L1-regularized log-linear models. In Proceed-
ings of the 24th International Conference on Machine
Learning (ICML 2007), pages 33?40.
Eiji Aramaki, Takeshi Imai, Kengo Miyo, and Kazuhiko
Ohe. 2008. Orthographic disambiguation incorporat-
ing transliterated probability. In Proceedings of the
Third International Joint Conference on Natural Lan-
guage Processing (IJCNLP 2008), pages 48?55.
Adam L. Berger, Vincent J. Della Pietra, and Stephen
A. Della Pietra. 1996. A maximum entropy approach
to natural language processing. Computational Lin-
guistics, 22(1):39?71.
Shane Bergsma and Grzegorz Kondrak. 2007.
Alignment-based discriminative string similarity. In
Proceedings of the 45th Annual Meeting of the Associ-
ation of Computational Linguistics (ACL 2007), pages
656?663.
Mikhail Bilenko and Raymond J. Mooney. 2003. Adap-
tive duplicate detection using learnable string simi-
larity measures. In Proceedings of the ninth ACM
SIGKDD international conference on Knowledge dis-
covery and data mining (KDD 2003), pages 39?48.
Eric Brill and Robert C. Moore. 2000. An improved
error model for noisy channel spelling correction. In
Proceedings of the 38th Annual Meeting on the As-
sociation for Computational Linguistics (ACL 2000),
pages 286?293.
Eric Brill. 1995. Transformation-based error-driven
learning and natural language processing: a case study
455
in part-of-speech tagging. Computational Linguistics,
21(4):543?565.
Andrew Carlson and Ian Fette. 2007. Memory-based
context-sensitive spelling correction at web scale. In
Proceedings of the Sixth International Conference on
Machine Learning and Applications (ICMLA 2007),
pages 166?171.
Qing Chen, Mu Li, and Ming Zhou. 2007. Improv-
ing query spelling correction using web search results.
In Proceedings of the Joint Conference on Empirical
Methods in Natural Language Processing and Compu-
tational Natural Language Learning (EMNLP-CoNLL
2007), pages 181?189.
Hercules Dalianis and Bart Jongejan. 2006. Hand-
crafted versus machine-learned inflectional rules: The
euroling-siteseeker stemmer and cst?s lemmatiser. In
In Proceedings of the 6th International Conference
on Language Resources and Evaluation (LREC 2006),
pages 663?666.
Okan Kolak and Philip Resnik. 2002. OCR error correc-
tion using a noisy channel model. In Proceedings of
the second international conference on Human Lan-
guage Technology Research (HLT 2002), pages 257?
262.
Grzegorz Kondrak. 2005. Cognates and word alignment
in bitexts. In Proceedings of the Tenth Machine Trans-
lation Summit (MT Summit X), pages 305?312.
Lillian Lee. 1999. Measures of distributional similarity.
In Proceedings of the 37th Annual Meeting of the As-
sociation for Computational Linguistics (ACL 1999),
pages 25?32.
Vladimir I. Levenshtein. 1966. Binary codes capable of
correcting deletions, insertions, and reversals. Soviet
Physics Doklady, 10(8):707?710.
Mu Li, Yang Zhang, Muhua Zhu, and Ming Zhou. 2006.
Exploring distributional similarity based models for
query spelling correction. In Proceedings of the 21st
International Conference on Computational Linguis-
tics and the 44th Annual Meeting of the Association for
Computational Linguistics (Coling-ACL 2006), pages
1025?1032.
Andrew McCallum, Kedar Bellare, and Fernando Pereira.
2005. A conditional random field for discriminatively-
trained finite-state string edit distance. In Proceedings
of the 21st Conference on Uncertainty in Artificial In-
telligence (UAI 2005), pages 388?395.
I. Dan Melamed. 1999. Bitext maps and alignment
via pattern recognition. Computational Linguistics,
25(1):107?130.
Guido Minnen, John Carroll, and Darren Pearce. 2001.
Applied morphological processing of English. Natu-
ral Language Engineering, 7(3):207?223.
Gonzalo Navarro. 2001. A guided tour to approximate
string matching. ACM Computing Surveys (CSUR),
33(1):31?88.
Andrew Y. Ng. 2004. Feature selection, L1 vs. L2 regu-
larization, and rotational invariance. In Proceedings of
the twenty-first international conference on Machine
learning (ICML 2004), pages 78?85.
Martin F. Porter. 1980. An algorithm for suffix stripping.
Program, 14(3):130?137.
Claude E. Shannon. 1948. A mathematical theory
of communication. Bell System Technical Journal,
27(3):379?423.
Robert Tibshirani. 1996. Regression shrinkage and se-
lection via the lasso. Journal of the Royal Statistical
Society. Series B (Methodological), 58(1):267?288.
Yoshimasa Tsuruoka, John McNaught, and Sophia Ana-
niadou. 2008. Normalizing biomedical terms by min-
imizing ambiguity and variability. BMC Bioinformat-
ics, Suppl 3(9):S2.
W. John Wilbur, Won Kim, and Natalie Xie. 2006.
Spelling correction in the PubMed search engine. In-
formation Retrieval, 9(5):543?564.
456
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 56?64,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
A Discriminative Latent Variable Chinese Segmenter
with Hybrid Word/Character Information
Xu Sun
Department of Computer Science
University of Tokyo
sunxu@is.s.u-tokyo.ac.jp
Yaozhong Zhang
Department of Computer Science
University of Tokyo
yaozhong.zhang@is.s.u-tokyo.ac.jp
Takuya Matsuzaki
Department of Computer Science
University of Tokyo
matuzaki@is.s.u-tokyo.ac.jp
Yoshimasa Tsuruoka
School of Computer Science
University of Manchester
yoshimasa.tsuruoka@manchester.ac.uk
Jun?ichi Tsujii
Department of Computer Science, University of Tokyo, Japan
School of Computer Science, University of Manchester, UK
National Centre for Text Mining, UK
tsujii@is.s.u-tokyo.ac.jp
Abstract
Conventional approaches to Chinese word
segmentation treat the problem as a character-
based tagging task. Recently, semi-Markov
models have been applied to the problem, in-
corporating features based on complete words.
In this paper, we propose an alternative, a
latent variable model, which uses hybrid in-
formation based on both word sequences and
character sequences. We argue that the use of
latent variables can help capture long range
dependencies and improve the recall on seg-
menting long words, e.g., named-entities. Ex-
perimental results show that this is indeed the
case. With this improvement, evaluations on
the data of the second SIGHAN CWS bakeoff
show that our system is competitive with the
best ones in the literature.
1 Introduction
For most natural language processing tasks, words
are the basic units to process. Since Chinese sen-
tences are written as continuous sequences of char-
acters, segmenting a character sequence into a word
sequence is the first step for most Chinese process-
ing applications. In this paper, we study the prob-
lem of Chinese word segmentation (CWS), which
aims to find these basic units (words1) for a given
sentence in Chinese.
Chinese character sequences are normally am-
biguous, and out-of-vocabulary (OOV) words are a
major source of the ambiguity. Typical examples
of OOV words include named entities (e.g., orga-
nization names, person names, and location names).
Those named entities may be very long, and a dif-
ficult case occurs when a long word W (|W | ? 4)
consists of some words which can be separate words
on their own; in such cases an automatic segmenter
may split the OOV word into individual words. For
example,
(Computer Committee of International Federation of
Automatic Control) is one of the organization names
in the Microsoft Research corpus. Its length is 13
and it contains more than 6 individual words, but it
should be treated as a single word. Proper recogni-
tion of long OOV words are meaningful not only for
word segmentation, but also for a variety of other
purposes, e.g., full-text indexing. However, as is il-
lustrated, recognizing long words (without sacrific-
ing the performance on short words) is challenging.
Conventional approaches to Chinese word seg-
mentation treat the problem as a character-based la-
1Following previous work, in this paper, words can also refer
to multi-word expressions, including proper names, long named
entities, idioms, etc.
56
beling task (Xue, 2003). Labels are assigned to each
character in the sentence, indicating whether the
character xi is the start (Labeli = B), middle or end
of a multi-character word (Labeli = C). A popu-
lar discriminative model that have been used for this
task is the conditional random fields (CRFs) (Laf-
ferty et al, 2001), starting with the model of Peng
et al (2004). In the Second International Chinese
Word Segmentation Bakeoff (the second SIGHAN
CWS bakeoff) (Emerson, 2005), two of the highest
scoring systems in the closed track competition were
based on a CRF model (Tseng et al, 2005; Asahara
et al, 2005).
While the CRF model is quite effective compared
with other models designed for CWS, it may be lim-
ited by its restrictive independence assumptions on
non-adjacent labels. Although the window can in
principle be widened by increasing the Markov or-
der, this may not be a practical solution, because
the complexity of training and decoding a linear-
chain CRF grows exponentially with the Markov or-
der (Andrew, 2006).
To address this difficulty, a choice is to relax the
Markov assumption by using the semi-Markov con-
ditional random field model (semi-CRF) (Sarawagi
and Cohen, 2004). Despite the theoretical advan-
tage of semi-CRFs over CRFs, however, some pre-
vious studies (Andrew, 2006; Liang, 2005) explor-
ing the use of a semi-CRF for Chinese word seg-
mentation did not find significant gains over the
CRF ones. As discussed in Andrew (2006), the rea-
son may be that despite the greater representational
power of the semi-CRF, there are some valuable fea-
tures that could be more naturally expressed in a
character-based labeling model. For example, on
a CRF model, one might use the feature ?the cur-
rent character xi is X and the current label Labeli
is C?. This feature may be helpful in CWS for gen-
eralizing to new words. For example, it may rule
out certain word boundaries if X were a character
that normally occurs only as a suffix but that com-
bines freely with some other basic forms to create
new words. This type of features is slightly less nat-
ural in a semi-CRF, since in that case local features
?(yi, yi+1, x) are defined on pairs of adjacent words.
That is to say, information about which characters
are not on boundaries is only implicit. Notably, ex-
cept the hybrid Markov/semi-Markov system in An-
drew (2006)2, no other studies using the semi-CRF
(Sarawagi and Cohen, 2004; Liang, 2005; Daume?
III and Marcu, 2005) experimented with features of
segmenting non-boundaries.
In this paper, instead of using semi-Markov mod-
els, we describe an alternative, a latent variable
model, to learn long range dependencies in Chi-
nese word segmentation. We use the discrimina-
tive probabilistic latent variable models (DPLVMs)
(Morency et al, 2007; Petrov and Klein, 2008),
which use latent variables to carry additional infor-
mation that may not be expressed by those original
labels, and therefore try to build more complicated
or longer dependencies. This is especially meaning-
ful in CWS, because the used labels are quite coarse:
Label(y) ? {B,C}, where B signifies beginning a
word and C signifies the continuation of a word.3
For example, by using DPLVM, the aforementioned
feature may turn to ?the current character xi is X ,
Labeli = C, and LatentV ariablei = LV ?. The
current latent variable LV may strongly depend on
the previous one or many latent variables, and there-
fore we can model the long range dependencies
which may not be captured by those very coarse la-
bels. Also, since character and word information
have their different advantages in CWS, in our latent
variable model, we use hybrid information based on
both character and word sequences.
2 A Latent Variable Segmenter
2.1 Discriminative Probabilistic Latent
Variable Model
Given data with latent structures, the task is to
learn a mapping between a sequence of observa-
tions x = x1, x2, . . . , xm and a sequence of labels
y = y1, y2, . . . , ym. Each yj is a class label for the
j?th character of an input sequence, and is a mem-
ber of a set Y of possible class labels. For each se-
quence, the model also assumes a sequence of latent
variables h = h1, h2, . . . , hm, which is unobserv-
able in training examples.
The DPLVM is defined as follows (Morency et al,
2The system was also used in Gao et al (2007), with an
improved performance in CWS.
3In practice, one may add a few extra labels based on lin-
guistic intuitions (Xue, 2003).
57
2007):
P (y|x,?) =?
h
P (y|h,x,?)P (h|x,?), (1)
where ? are the parameters of the model. DPLVMs
can be seen as a natural extension of CRF models,
and CRF models can be seen as a special case of
DPLVMs that have only one latent variable for each
label.
To make the training and inference efficient, the
model is restricted to have disjoint sets of latent vari-
ables associated with each class label. Each hj is a
member in a set Hyj of possible latent variables for
the class label yj . H is defined as the set of all pos-
sible latent variables, i.e., the union of all Hyj sets.
Since sequences which have any hj /? Hyj will by
definition have P (y|x,?) = 0, the model can be
further defined4 as:
P (y|x,?) = ?
h?Hy1?...?Hym
P (h|x,?), (2)
where P (h|x,?) is defined by the usual conditional
random field formulation:
P (h|x,?) = exp??f(h,x)?
?h exp??f(h,x)
, (3)
in which f(h,x) is a feature vector. Given a training
set consisting of n labeled sequences, (xi,yi), for
i = 1 . . . n, parameter estimation is performed by
optimizing the objective function,
L(?) =
n?
i=1
log P (yi|xi,?) ? R(?). (4)
The first term of this equation is the conditional log-
likelihood of the training data. The second term is
a regularizer that is used for reducing overfitting in
parameter estimation.
For decoding in the test stage, given a test se-
quence x, we want to find the most probable label
sequence, y?:
y? = argmaxyP (y|x,??). (5)
For latent conditional models like DPLVMs, the best
label path y? cannot directly be produced by the
4It means that Eq. 2 is from Eq. 1 with additional definition.
Viterbi algorithm because of the incorporation of
hidden states. In this paper, we use a technique
based on A? search and dynamic programming de-
scribed in Sun and Tsujii (2009), for producing the
most probable label sequence y? on DPLVM.
In detail, an A? search algorithm5 (Hart et al,
1968) with a Viterbi heuristic function is adopted to
produce top-n latent paths, h1,h2, . . .hn. In addi-
tion, a forward-backward-style algorithm is used to
compute the exact probabilities of their correspond-
ing label paths, y1,y2, . . .yn. The model then tries
to determine the optimal label path based on the
top-n statistics, without enumerating the remaining
low-probability paths, which could be exponentially
enormous.
The optimal label path y? is ready when the fol-
lowing ?exact-condition? is achieved:
P (y1|x,?) ? (1 ?
?
yk?LPn
P (yk|x,?)) ? 0, (6)
where y1 is the most probable label sequence in
current stage. It is straightforward to prove that
y? = y1, and further search is unnecessary. This
is because the remaining probability mass, 1 ??
yk?LPn P (yk|x,?), cannot beat the current op-timal label path in this case. For more details of the
inference, refer to Sun and Tsujii (2009).
2.2 Hybrid Word/Character Information
We divide our main features into two types:
character-based features and word-based features.
The character-based features are indicator functions
that fire when the latent variable label takes some
value and some predicate of the input (at a certain
position) corresponding to the label is satisfied. For
each latent variable label hi (the latent variable la-
bel at position i), we use the predicate templates as
follows:
? Input characters/numbers/letters locating at po-
sitions i ? 2, i ? 1, i, i + 1 and i + 2
? The character/number/letter bigrams locating
at positions i ? 2, i ? 1, i and i + 1
5A? search and its variants, like beam-search, are widely
used in statistical machine translation. Compared to other
search techniques, an interesting point of A? search is that it
can produce top-n results one-by-one in an efficient manner.
58
? Whether xj and xj+1 are identical, for j = (i?
2) . . . (i + 1)
? Whether xj and xj+2 are identical, for j = (i?
3) . . . (i + 1)
The latter two feature templates are designed to de-
tect character or word reduplication, a morphologi-
cal phenomenon that can influence word segmenta-
tion in Chinese.
The word-based features are indicator functions
that fire when the local character sequence matches
a word or a word bigram. A dictionary containing
word and bigram information was collected from the
training data. For each latent variable label unigram
hi, we use the set of predicate template checking for
word-based features:
? The identity of the string xj . . . xi, if it matches
a word A from the word-dictionary of training
data, with the constraint i?6 < j < i; multiple
features will be generated if there are multiple
strings satisfying the condition.
? The identity of the string xi . . . xk, if it matches
a word A from the word-dictionary of training
data, with the constraint i < k < i+6; multiple
features could be generated.
? The identity of the word bigram (xj . . . xi?1,
xi . . . xk), if it matches a word bigram in the
bigram dictionary and satisfies the aforemen-
tioned constraints on j and k; multiple features
could be generated.
? The identity of the word bigram (xj . . . xi,
xi+1 . . . xk), if it matches a word bigram in the
bigram dictionary and satisfies the aforemen-
tioned constraints on j and k; multiple features
could be generated.
All feature templates were instantiated with val-
ues that occur in positive training examples. We
found that using low-frequency features that occur
only a few times in the training set improves perfor-
mance on the development set. We hence do not do
any thresholding of the DPLVM features: we simply
use all those generated features.
The aforementioned word based features can in-
corporate word information naturally. In addition,
following Wang et al (2006), we found using a
very simple heuristic can further improve the seg-
mentation quality slightly. More specifically, two
operations, merge and split, are performed on the
DPLVM/CRF outputs: if a bigram A B was not ob-
served in the training data, but the merged one AB
was, then A B will be simply merged into AB; on
the other hand, if AB was not observed but A B ap-
peared, then it will be split into A B. We found this
simple heuristic on word information slightly im-
proved the performance (e.g., for the PKU corpus,
+0.2% on the F-score).
3 Experiments
We used the data provided by the second Inter-
national Chinese Word Segmentation Bakeoff to
test our approaches described in the previous sec-
tions. The data contains three corpora from different
sources: Microsoft Research Asia (MSR), City Uni-
versity of Hong Kong (CU), and Peking University
(PKU).
Since the purpose of this work is to evaluate the
proposed latent variable model, we did not use ex-
tra resources such as common surnames, lexicons,
parts-of-speech, and semantics. For the generation
of word-based features, we extracted a word list
from the training data as the vocabulary.
Four metrics were used to evaluate segmentation
results: recall (R, the percentage of gold standard
output words that are correctly segmented by the de-
coder), precision (P , the percentage of words in the
decoder output that are segmented correctly), bal-
anced F-score (F ) defined by 2PR/(P + R), recall
of OOV words (R-oov). For more detailed informa-
tion on the corpora and these metrics, refer to Emer-
son (2005).
3.1 Training the DPLVM Segmenter
We implemented DPLVMs in C++ and optimized
the system to cope with large scale problems, in
which the feature dimension is beyond millions. We
employ the feature templates defined in Section 2.2,
taking into account those 3,069,861 features for the
MSR data, 2,634,384 features for the CU data, and
1,989,561 features for the PKU data.
As for numerical optimization, we performed
gradient decent with the Limited-Memory BFGS
59
(L-BFGS)6 optimization technique (Nocedal and
Wright, 1999). L-BFGS is a second-order Quasi-
Newton method that numerically estimates the cur-
vature from previous gradients and updates. With
no requirement on specialized Hessian approxima-
tion, L-BFGS can handle large-scale problems in an
efficient manner.
Since the objective function of the DPLVM model
is non-convex, we randomly initialized parameters
for the training.7 To reduce overfitting, we employed
an L2 Gaussian weight prior8 (Chen and Rosen-
feld, 1999). During training, we varied the L2-
regularization term (with values 10k, k from -3 to
3), and finally set the value to 1. We use 4 hidden
variables per label for this task, compromising be-
tween accuracy and efficiency.
3.2 Comparison on Convergence Speed
First, we show a comparison of the convergence
speed between the objective function of DPLVMs
and CRFs. We apply the L-BFGS optimization algo-
rithm to optimize the objective function of DPLVM
and CRF models, making a comparison between
them. We find that the number of iterations required
for the convergence of DPLVMs are fewer than for
CRFs. Figure 1 illustrates the convergence-speed
comparison on the MSR data. The DPLVM model
arrives at the plateau of convergence in around 300
iterations, with the penalized loss of 95K when
#passes = 300; while CRFs require 900 iterations,
with the penalized loss of 98K when #passes =
900.
However, we should note that the time cost of the
DPLVM model in each iteration is around four times
higher than the CRF model, because of the incorpo-
ration of hidden variables. In order to speed up the
6For numerical optimization on latent variable models, we
also experimented the conjugate-gradient (CG) optimization al-
gorithm and stochastic gradient decent algorithm (SGD). We
found the L-BFGS with L2 Gaussian regularization performs
slightly better than the CG and the SGD. Therefore, we adopt
the L-BFGS optimizer in this study.
7For a non-convex objective function, different parame-
ter initializations normally bring different optimization results.
Therefore, to approach closer to the global optimal point, it
is recommended to perform multiple experiments on DPLVMs
with random initialization and then select a good start point.
8We also tested the L-BFGS with L1 regularization, and we
found the L-BFGS with L2 regularization performs better in
this task.
0
300K
600K
900K
1200K
1500K
1800K
 100  200  300  400  500  600  700  800  900
O
bj.
 Fu
nc
. V
alu
e
Forward-Backward Passes
DPLVM
CRF
Figure 1: The value of the penalized loss based on the
number of iterations: DPLVMs vs. CRFs on the MSR
data.
Style #W.T. #Word #C.T. #Char
MSR S.C. 88K 2,368K 5K 4,050K
CU T.C. 69K 1,455K 5K 2,403K
PKU S.C. 55K 1,109K 5K 1,826K
Table 1: Details of the corpora. W.T. represents word
types; C.T. represents character types; S.C. represents
simplified Chinese; T.C. represents traditional Chinese.
training speed of the DPLVM model in the future,
one solution is to use the stochastic learning tech-
nique9. Another solution is to use a distributed ver-
sion of L-BFGS to parallelize the batch training.
4 Results and Discussion
Since the CRF model is one of the most successful
models in Chinese word segmentation, we compared
DPLVMs with CRFs. We tried to make experimen-
tal results comparable between DPLVMs and CRF
models, and have therefore employed the same fea-
ture set, optimizer and fine-tuning strategy between
the two. We also compared DPLVMs with semi-
CRFs and other successful systems reported in pre-
vious work.
4.1 Evaluation Results
Three training and test corpora were used in the test,
including the MSR Corpus, the CU Corpus, and the
9We have tried stochastic gradient decent, as described pre-
viously. It is possible to try other stochastic learning methods,
e.g., stochastic meta decent (Vishwanathan et al, 2006).
60
MSR data P R F R-oov
DPLVM (*) 97.3 97.3 97.3 72.2
CRF (*) 97.1 96.8 97.0 72.0
semi-CRF (A06) N/A N/A 96.8 N/A
semi-CRF (G07) N/A N/A 97.2 N/A
CRF (Z06-a) 96.5 96.3 96.4 71.4
Z06-b 97.2 96.9 97.1 71.2
ZC07 N/A N/A 97.2 N/A
Best05 (T05) 96.2 96.6 96.4 71.7
CU data P R F R-oov
DPLVM (*) 94.7 94.4 94.6 68.8
CRF (*) 94.3 93.9 94.1 65.8
CRF (Z06-a) 95.0 94.2 94.6 73.6
Z06-b 95.2 94.9 95.1 74.1
ZC07 N/A N/A 95.1 N/A
Best05 (T05) 94.1 94.6 94.3 69.8
PKU data P R F R-oov
DPLVM (*) 95.6 94.8 95.2 77.8
CRF (*) 95.2 94.2 94.7 76.8
CRF (Z06-a) 94.3 94.6 94.5 75.4
Z06-b 94.7 95.5 95.1 74.8
ZC07 N/A N/A 94.5 N/A
Best05 (C05) 95.3 94.6 95.0 63.6
Table 2: Results from DPLVMs, CRFs, semi-CRFs, and
other systems.
PKU Corpus (see Table 1 for details). The results
are shown in Table 2. The results are grouped into
three sub-tables according to different corpora. Each
row represents a CWS model. For each group, the
rows marked by ? represent our models with hy-
brid word/character information. Best05 represents
the best system of the Second International Chinese
Word Segmentation Bakeoff on the corresponding
data; A06 represents the semi-CRF model in An-
drew (2006)10, which was also used in Gao et al
(2007) (denoted as G07) with an improved perfor-
mance; Z06-a and Z06-b represents the pure sub-
word CRF model and the confidence-based com-
bination of CRF and rule-based models, respec-
tively (Zhang et al, 2006); ZC07 represents the
word-based perceptron model in Zhang and Clark
(2007); T05 represents the CRF model in Tseng et
al. (2005); C05 represents the system in Chen et al
10It is a hybrid Markov/semi-Markov CRF model which
outperforms conventional semi-CRF models (Andrew, 2006).
However, in general, as discussed in Andrew (2006), it is essen-
tially still a semi-CRF model.
(2005). The best F-score and recall of OOV words
of each group is shown in bold.
As is shown in the table, we achieved the best
F-score in two out of the three corpora. We also
achieved the best recall rate of OOV words on those
two corpora. Both of the MSR and PKU Corpus use
simplified Chinese, while the CU Corpus uses the
traditional Chinese.
On the MSR Corpus, the DPLVM model reduced
more than 10% error rate over the CRF model us-
ing exactly the same feature set. We also compared
our DPLVM model with the semi-CRF models in
Andrew (2006) and Gao et al (2007), and demon-
strate that the DPLVM model achieved slightly bet-
ter performance than the semi-CRF models. Andrew
(2006) and Gao et al (2007) only reported the re-
sults on the MSR Corpus.
In summary, tests for the Second International
Chinese Word Segmentation Bakeoff showed com-
petitive results for our method compared with the
best results in the literature. Our discriminative la-
tent variable models achieved the best F-scores on
the MSR Corpus (97.3%) and PKU Corpus (95.2%);
the latent variable models also achieved the best re-
calls of OOV words over those two corpora. We will
analyze the results by varying the word-length in the
following subsection.
4.2 Effect on Long Words
One motivation of using a latent variable model for
CWS is to use latent variables to more adequately
learn long range dependencies, as we argued in Sec-
tion 1. In the test data of the MSR Corpus, 19% of
the words are longer than 3 characters; there are also
8% in the CU Corpus and 11% in the PKU Corpus,
respectively. In the MSR Corpus, there are some ex-
tremely long words (Length > 10), while the CU
and PKU corpus do not contain such extreme cases.
Figure 2 shows the recall rate on different groups
of words categorized by their lengths (the number
of characters). As we expected, the DPLVM model
performs much better on long words (Length ? 4)
than the CRF model, which used exactly the same
feature set. Compared with the CRF model, the
DPLVM model exhibited almost the same level of
performance on short words. Both models have
the best performance on segmenting the words with
the length of two. The performance of the CRF
61
 0
 20
 40
 60
 80
 100
 0  2  4  6  8  10  12  14
R
ec
al
l-M
SR
 (%
)
Length of Word (MSR)
DPLVM
CRF
 0
 20
 40
 60
 80
 100
 0  2  4  6  8  10  12  14
R
ec
al
l-C
U 
(%
)
Length of Word (CU)
DPLVM
CRF
 40
 50
 60
 70
 80
 90
 100
 0  2  4  6  8  10  12  14
R
ec
al
l-P
KU
 (%
)
Length of Word (PKU)
DPLVM
CRF
Figure 2: The recall rate on words grouped by the length.
model deteriorates rapidly as the word length in-
creases, which demonstrated the difficulty on mod-
eling long range dependencies in CWS. Compared
with the CRF model, the DPLVM model performed
quite well in dealing with long words, without sacri-
ficing the performance on short words. All in all, we
conclude that the improvement of using the DPLVM
model came from the improvement on modeling
long range dependencies in CWS.
4.3 Error Analysis
Table 3 lists the major errors collected from the la-
tent variable segmenter. We examined the collected
errors and found that many of them can be grouped
into four types: over-generalization (the top row),
errors on named entities (the following three rows),
errors on idioms (the following three rows) and er-
rors from inconsistency (the two rows at the bottom).
Our system performed reasonably well on very
complex OOV words, such as
(Agricultural Bank of China,
Gold Segmentation Segmenter Output
//
Co-allocated org. names
(Chen Yao) //
(Chen Fei) //
(Vasillis) //
//
//
// //
Idioms
// (propagandist)
(desertification) //
Table 3: Error analysis on the latent variable seg-
menter. The errors are grouped into four types: over-
generalization, errors on named entities, errors on idioms
and errors from data-inconsistency.
Shijiazhuang-city Branch, the second sales depart-
ment) and (Science
and Technology Commission of China, National In-
stitution on Scientific Information Analysis). How-
ever, it sometimes over-generalized to long words.
For example, as shown in the top row,
(National Department of Environmental Protection)
and (The Central Propaganda Department)
are two organization names, but they are incorrectly
merged into a single word.
As for the following three rows, (Chen Yao)
and (Chen Fei) are person names. They are
wrongly segmented because we lack the features to
capture the information of person names (such use-
ful knowledge, e.g., common surname list, are cur-
rently not used in our system). In the future, such
errors may be solved by integrating open resources
into our system. (Vasillis) is a transliter-
ated foreign location name and is also wrongly seg-
mented.
For the corpora that considered 4 character idioms
as a word, our system successfully combined most
of new idioms together. This differs greatly from the
results of CRFs. However, there are still a number
of new idioms that failed to be correctly segmented,
as listed from the fifth row to the seventh row.
Finally, some errors are due to inconsistencies in
the gold segmentation. For example, // (pro-
pagandist) is two words, but a word with similar
62
structure, (theorist), is one word.
(desertification) is one word, but its synonym,
// (desertification), is two words in the gold seg-
mentation.
5 Conclusion and Future Work
We presented a latent variable model for Chinese
word segmentation, which used hybrid information
based on both word and character sequences. We
discussed that word and character information have
different advantages, and could be complementary
to each other. Our model is an alternative to the ex-
isting word based models and character based mod-
els.
We argued that using latent variables can better
capture long range dependencies. We performed
experiments and demonstrated that our model can
indeed improve the segmentation accuracy on long
words. With this improvement, tests on the data
of the Second International Chinese Word Segmen-
tation Bakeoff show that our system is competitive
with the best in the literature.
Since the latent variable model allows a wide
range of features, so the future work will consider
how to integrate open resources into our system. The
latent variable model handles latent-dependencies
naturally, and can be easily extended to other label-
ing tasks.
Acknowledgments
We thank Kun Yu, Galen Andrew and Xiaojun Lin
for the enlightening discussions. We also thank the
anonymous reviewers who gave very helpful com-
ments. This work was partially supported by Grant-
in-Aid for Specially Promoted Research (MEXT,
Japan).
References
Galen Andrew. 2006. A hybrid markov/semi-markov
conditional random field for sequence segmentation.
Proceedings of EMNLP?06, pages 465?472.
Masayuki Asahara, Kenta Fukuoka, Ai Azuma, Chooi-
Ling Goh, Yotaro Watanabe, Yuji Matsumoto, and
Takahashi Tsuzuki. 2005. Combination of machine
learning methods for optimum chinese word segmen-
tation. Proceedings of the fourth SIGHAN workshop,
pages 134?137.
Stanley F. Chen and Ronald Rosenfeld. 1999. A gaussian
prior for smoothing maximum entropy models. Tech-
nical Report CMU-CS-99-108, CMU.
Aitao Chen, Yiping Zhou, Anne Zhang, and Gordon Sun.
2005. Unigram language model for chinese word seg-
mentation. Proceedings of the fourth SIGHAN work-
shop.
Hal Daume? III and Daniel Marcu. 2005. Learn-
ing as search optimization: approximate large mar-
gin methods for structured prediction. Proceedings of
ICML?05.
Thomas Emerson. 2005. The second international chi-
nese word segmentation bakeoff. Proceedings of the
fourth SIGHAN workshop, pages 123?133.
Jianfeng Gao, Galen Andrew, Mark Johnson, and
Kristina Toutanova. 2007. A comparative study of pa-
rameter estimation methods for statistical natural lan-
guage processing. In Proceedings of the 45th Annual
Meeting of the Association of Computational Linguis-
tics (ACL?07), pages 824?831.
P.E. Hart, N.J. Nilsson, and B. Raphael. 1968. A formal
basis for the heuristic determination of minimum cost
path. IEEE Trans. On System Science and Cybernet-
ics, SSC-4(2):100?107.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic models
for segmenting and labeling sequence data. Proceed-
ings of ICML?01, pages 282?289.
Percy Liang. 2005. Semi-supervised learning for natural
language. Master?s thesis, Massachusetts Institute of
Technology.
Louis-Philippe Morency, Ariadna Quattoni, and Trevor
Darrell. 2007. Latent-dynamic discriminative mod-
els for continuous gesture recognition. Proceedings of
CVPR?07, pages 1?8.
Jorge Nocedal and Stephen J. Wright. 1999. Numerical
optimization. Springer.
F. Peng and A. McCallum. 2004. Chinese segmenta-
tion and new word detection using conditional random
fields. Proceedings of COLING?04.
Slav Petrov and Dan Klein. 2008. Discriminative log-
linear grammars with latent variables. Proceedings of
NIPS?08.
Sunita Sarawagi and William Cohen. 2004. Semi-
markov conditional random fields for information ex-
traction. Proceedings of ICML?04.
Xu Sun and Jun?ichi Tsujii. 2009. Sequential labeling
with latent variables: An exact inference algorithm and
its efficient approximation. Proceedings of the 12th
Conference of the European Chapter of the Associa-
tion for Computational Linguistics (EACL?09).
Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel
Jurafsky, and Christopher Manning. 2005. A condi-
tional random field word segmenter for sighan bakeoff
63
2005. Proceedings of the fourth SIGHAN workshop,
pages 168?171.
S.V.N. Vishwanathan, Nicol N. Schraudolph, Mark W.
Schmidt, and Kevin P. Murphy. 2006. Accelerated
training of conditional random fields with stochastic
meta-descent. Proceedings of ICML?06, pages 969?
976.
Xinhao Wang, Xiaojun Lin, Dianhai Yu, Hao Tian, and
Xihong Wu. 2006. Chinese word segmentation with
maximum entropy and n-gram language model. In
Proceedings of the fifth SIGHAN workshop, pages
138?141, July.
Nianwen Xue. 2003. Chinese word segmentation as
character tagging. International Journal of Computa-
tional Linguistics and Chinese Language Processing,
8(1).
Yue Zhang and Stephen Clark. 2007. Chinese segmen-
tation with a word-based perceptron algorithm. Pro-
ceedings of ACL?07.
Ruiqiang Zhang, Genichiro Kikui, and Eiichiro Sumita.
2006. Subword-based tagging by conditional random
fields for chinese word segmentation. Proceedings of
HLT/NAACL?06 companion volume short papers.
64
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 465?472,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Improving the Scalability of Semi-Markov Conditional
Random Fields for Named Entity Recognition
Daisuke Okanohara? Yusuke Miyao? Yoshimasa Tsuruoka ? Jun?ichi Tsujii???
?Department of Computer Science, University of Tokyo
Hongo 7-3-1, Bunkyo-ku, Tokyo, Japan
?School of Informatics, University of Manchester
POBox 88, Sackville St, MANCHESTER M60 1QD, UK
?SORST, Solution Oriented Research for Science and Technology
Honcho 4-1-8, Kawaguchi-shi, Saitama, Japan
{hillbig,yusuke,tsuruoka,tsujii}@is.s.u-tokyo.ac.jp
Abstract
This paper presents techniques to apply
semi-CRFs to Named Entity Recognition
tasks with a tractable computational cost.
Our framework can handle an NER task
that has long named entities and many
labels which increase the computational
cost. To reduce the computational cost,
we propose two techniques: the first is the
use of feature forests, which enables us to
pack feature-equivalent states, and the sec-
ond is the introduction of a filtering pro-
cess which significantly reduces the num-
ber of candidate states. This framework
allows us to use a rich set of features ex-
tracted from the chunk-based representa-
tion that can capture informative charac-
teristics of entities. We also introduce a
simple trick to transfer information about
distant entities by embedding label infor-
mation into non-entity labels. Experimen-
tal results show that our model achieves an
F-score of 71.48% on the JNLPBA 2004
shared task without using any external re-
sources or post-processing techniques.
1 Introduction
The rapid increase of information in the biomedi-
cal domain has emphasized the need for automated
information extraction techniques. In this paper
we focus on the Named Entity Recognition (NER)
task, which is the first step in tackling more com-
plex tasks such as relation extraction and knowl-
edge mining.
Biomedical NER (Bio-NER) tasks are, in gen-
eral, more difficult than ones in the news domain.
For example, the best F-score in the shared task of
Bio-NER in COLING 2004 JNLPBA (Kim et al,
2004) was 72.55% (Zhou and Su, 2004) 1, whereas
the best performance at MUC-6, in which systems
tried to identify general named entities such as
person or organization names, was an accuracy of
95% (Sundheim, 1995).
Many of the previous studies of Bio-NER tasks
have been based on machine learning techniques
including Hidden Markov Models (HMMs) (Bikel
et al, 1997), the dictionary HMM model (Kou et
al., 2005) and Maximum Entropy Markov Mod-
els (MEMMs) (Finkel et al, 2004). Among these
methods, conditional random fields (CRFs) (Laf-
ferty et al, 2001) have achieved good results (Kim
et al, 2005; Settles, 2004), presumably because
they are free from the so-called label bias problem
by using a global normalization.
Sarawagi and Cohen (2004) have recently in-
troduced semi-Markov conditional random fields
(semi-CRFs). They are defined on semi-Markov
chains and attach labels to the subsequences of a
sentence, rather than to the tokens2. The semi-
Markov formulation allows one to easily construct
entity-level features. Since the features can cap-
ture all the characteristics of a subsequence, we
can use, for example, a dictionary feature which
measures the similarity between a candidate seg-
ment and the closest element in the dictionary.
Kou et al (2005) have recently showed that semi-
CRFs perform better than CRFs in the task of
recognition of protein entities.
The main difficulty of applying semi-CRFs to
Bio-NER lies in the computational cost at training
1Krauthammer (2004) reported that the inter-annotator
agreement rate of human experts was 77.6% for bio-NLP,
which suggests that the upper bound of the F-score in a Bio-
NER task may be around 80%.
2Assuming that non-entity words are placed in unit-length
segments.
465
Table 1: Length distribution of entities in the train-
ing set of the shared task in 2004 JNLPBA
Length # entity Ratio
1 21646 42.19
2 15442 30.10
3 7530 14.68
4 3505 6.83
5 1379 2.69
6 732 1.43
7 409 0.80
8 252 0.49
>8 406 0.79
total 51301 100.00
because the number of named entity classes tends
to be large, and the training data typically contain
many long entities, which makes it difficult to enu-
merate all the entity candidates in training. Table
1 shows the length distribution of entities in the
training set of the shared task in 2004 JNLPBA.
Formally, the computational cost of training semi-
CRFs is O(KLN), where L is the upper bound
length of entities, N is the length of sentence and
K is the size of label set. And that of training in
first order semi-CRFs is O(K2LN). The increase
of the cost is used to transfer non-adjacent entity
information.
To improve the scalability of semi-CRFs, we
propose two techniques: the first is to intro-
duce a filtering process that significantly re-
duces the number of candidate entities by using
a ?lightweight? classifier, and the second is to
use feature forest (Miyao and Tsujii, 2002), with
which we pack the feature equivalent states. These
enable us to construct semi-CRF models for the
tasks where entity names may be long and many
class-labels exist at the same time. We also present
an extended version of semi-CRFs in which we
can make use of information about a preceding
named entity in defining features within the frame-
work of first order semi-CRFs. Since the preced-
ing entity is not necessarily adjacent to the current
entity, we achieve this by embedding the informa-
tion on preceding labels for named entities into the
labels for non-named entities.
2 CRFs and Semi-CRFs
CRFs are undirected graphical models that encode
a conditional probability distribution using a given
set of features. CRFs allow both discriminative
training and bi-directional flow of probabilistic in-
formation along the sequence. In NER, we of-
ten use linear-chain CRFs, which define the con-
ditional probability of a state sequence y = y1, ...,
yn given the observed sequence x = x1,...,xn by:
p(y|x, ?) = 1
Z(x) exp(?
n
i=1?j?jfj(yi?1, yi, x, i)),
(1)
where fj(yi?1, yi,x, i) is a feature function and
Z(x) is the normalization factor over all the state
sequences for the sequence x. The model parame-
ters are a set of real-valued weights ? = {?j}, each
of which represents the weight of a feature. All the
feature functions are real-valued and can use adja-
cent label information.
Semi-CRFs are actually a restricted version of
order-L CRFs in which all the labels in a chunk are
the same. We follow the definitions in (Sarawagi
and Cohen, 2004). Let s = ?s1, ..., sp? denote a
segmentation of x, where a segment sj = ?tj , uj ,
yj? consists of a start position tj , an end position
uj , and a label yj . We assume that segments have a
positive length bounded above by the pre-defined
upper bound L (tj ? uj , uj ? tj + 1 ? L) and
completely cover the sequence x without overlap-
ping, that is, s satisfies t1 = 1, up = |x|, and
tj+1 = uj + 1 for j = 1, ..., p ? 1. Semi-CRFs
define a conditional probability of a state sequence
y given an observed sequence x by:
p(y|x, ?) = 1
Z(x) exp(?j?i?ifi(sj)), (2)
where fi(sj) := fi(yj?1, yj ,x, tj , uj) is a fea-
ture function and Z(x) is the normalization factor
as defined for CRFs. The inference problem for
semi-CRFs can be solved by using a semi-Markov
analog of the usual Viterbi algorithm. The com-
putational cost for semi-CRFs is O(KLN) where
L is the upper bound length of entities, N is the
length of sentence and K is the number of label
set. If we use previous label information, the cost
becomes O(K2LN).
3 Using Non-Local Information in
Semi-CRFs
In conventional CRFs and semi-CRFs, one can
only use the information on the adjacent previ-
ous label when defining the features on a certain
state or entity. In NER tasks, however, informa-
tion about a distant entity is often more useful than
466
O protein O O DNA
O protein O-protein O-protein DNA
Figure 1: Modification of ?O? (other labels) to
transfer information on a preceding named entity.
information about the previous state (Finkel et al,
2005). For example, consider the sentence ?... in-
cluding Sp1 and CP1.? where the correct labels of
?Sp1? and ?CP1? are both ?protein?. It would be
useful if the model could utilize the (non-adjacent)
information about ?Sp1? being ?protein? to clas-
sify ?CP1? as ?protein?. On the other hand, in-
formation about adjacent labels does not necessar-
ily provide useful information because, in many
cases, the previous label of a named entity is ?O?,
which indicates a non-named entity. For 98.0% of
the named entities in the training data of the shared
task in the 2004 JNLPBA, the label of the preced-
ing entity was ?O?.
In order to incorporate such non-local informa-
tion into semi-CRFs, we take a simple approach.
We divide the label of ?O? into ?O-protein? and
?O? so that they convey the information on the
preceding named entity. Figure 1 shows an ex-
ample of this conversion, in which the two labels
for the third and fourth states are converted from
?O? to ?O-protein?. When we define the fea-
tures for the fifth state, we can use the informa-
tion on the preceding entity ?protein? by look-
ing at the fourth state. Since this modification
changes only the label set, we can do this within
the framework of semi-CRF models. This idea is
originally proposed in (Peshkin and Pfeffer, 2003).
However, they used a dynamic Bayesian network
(DBNs) rather than a semi-CRF, and semi-CRFs
are likely to have significantly better performance
than DBNs.
In previous work, such non-local information
has usually been employed at a post-processing
stage. This is because the use of long distance
dependency violates the locality of the model and
prevents us from using dynamic programming
techniques in training and inference. Skip-CRFs
(Sutton and McCallum, 2004) are a direct imple-
mentation of long distance effects to the model.
However, they need to determine the structure
for propagating non-local information in advance.
In a recent study by Finkel et al, (2005), non-
local information is encoded using an indepen-
dence model, and the inference is performed by
Gibbs sampling, which enables us to use a state-
of-the-art factored model and carry out training ef-
ficiently, but inference still incurs a considerable
computational cost. Since our model handles lim-
ited type of non-local information, i.e. the label
of the preceding entity, the model can be solved
without approximation.
4 Reduction of Training/Inference Cost
The straightforward implementation of this mod-
eling in semi-CRFs often results in a prohibitive
computational cost.
In biomedical documents, there are quite a few
entity names which consist of many words (names
of 8 words in length are not rare). This makes
it difficult for us to use semi-CRFs for biomedi-
cal NER, because we have to set L to be eight or
larger, where L is the upper bound of the length of
possible chunks in semi-CRFs. Moreover, in or-
der to take into account the dependency between
named entities of different classes appearing in a
sentence, we need to incorporate multiple labels
into a single probabilistic model. For example, in
the shared task in COLING 2004 JNLPBA (Kim
et al, 2004) the number of labels is six (?pro-
tein?, ?DNA?, ?RNA?, ?cell line?, ?cell type?
and ?other?). This also increases the computa-
tional cost of a semi-CRF model.
To reduce the computational cost, we propose
two methods (see Figure 2). The first is employing
a filtering process using a lightweight classifier to
remove unnecessary state candidates beforehand
(Figure 2 (2)), and the second is the using the fea-
ture forest model (Miyao and Tsujii, 2002) (Fig-
ure 2 (3)), which employs dynamic programming
at training ?as much as possible?.
4.1 Filtering with a naive Bayes classifier
We introduce a filtering process to remove low
probability candidate states. This is the first step
of our NER system. After this filtering step, we
construct semi-CRFs on the remaining candidate
states using a feature forest. Therefore the aim of
this filtering is to reduce the number of candidate
states, without removing correct entities. This idea
467
(1) Enumerate
Candidate States
(2) Filtering by
Na?ve Bayes
(3) Construct feature forest
Training/
Inference
: other : entity
: other with preceding entity information
Figure 2: The framework of our system. We first enumerate all possible candidate states, and then filter
out low probability states by using a light-weight classifier, and represent them by using feature forest.
Table 2: Features used in the naive Bayes Classi-
fier for the entity candidate: ws, ws+1, ..., we. spi
is the result of shallow parsing at wi.
Feature Name Example of Features
Start/End Word ws, we
Inside Word ws, ws+1, ... , we
Context Word ws?1, we+1
Start/End SP sps, spe
Inside SP sps, sps+1, ..., spe
Context SP sps?1, spe+1
is similar to the method proposed by Tsuruoka and
Tsujii (2005) for chunk parsing, in which implau-
sible phrase candidates are removed beforehand.
We construct a binary naive Bayes classifier us-
ing the same training data as those for semi-CRFs.
In training and inference, we enumerate all possi-
ble chunks (the max length of a chunk is L as for
semi-CRFs) and then classify those into ?entity?
or ?other?. Table 2 lists the features used in the
naive Bayes classifier. This process can be per-
formed independently of semi-CRFs
Since the purpose of the filtering is to reduce the
computational cost, rather than to achieve a good
F-score by itself, we chose the threshold probabil-
ity of filtering so that the recall of filtering results
would be near 100 %.
4.2 Feature Forest
In estimating semi-CRFs, we can use an efficient
dynamic programming algorithm, which is simi-
lar to the forward-backward algorithm (Sarawagi
and Cohen, 2004). The proposal here is a more
general framework for estimating sequential con-
ditional random fields.
This framework is based on the feature forest
DNA
protein
Other
DNA
protein
Other
: or node (disjunctive node)
: and node (conjunctive node)
pos i i+1
??
Figure 3: Example of feature forest representation
of linear chain CRFs. Feature functions are as-
signed to ?and? nodes.
protein
O-protein
protein
u
j
=8 
prev-entity:protein
u
j
=  8
prev-entity: protein
packed
pos
87 9
Figure 4: Example of packed representation of
semi-CRFs. The states that have the same end po-
sition and prev-entity label are packed.
model, which was originally proposed for disam-
biguation models for parsing (Miyao and Tsujii,
2002). A feature forest model is a maximum en-
tropy model defined over feature forests, which are
abstract representations of an exponential number
of sequence/tree structures. A feature forest is
an ?and/or? graph: in Figure 3, circles represent
468
?and? nodes (conjunctive nodes), while boxes de-
note ?or? nodes (disjunctive nodes). Feature func-
tions are assigned to ?and? nodes. We can use
the information of the previous ?and? node for de-
signing the feature functions through the previous
?or? node. Each sequence in a feature forest is
obtained by choosing a conjunctive node for each
disjunctive node. For example, Figure 3 represents
3 ? 3 = 9 sequences, since each disjunctive node
has three candidates. It should be noted that fea-
ture forests can represent an exponential number
of sequences with a polynomial number of con-
junctive/disjunctive nodes.
One can estimate a maximum entropy model for
the whole sequence with dynamic programming
by representing the probabilistic events, i.e. se-
quence of named entity tags, by feature forests
(Miyao and Tsujii, 2002).
In the previous work (Lafferty et al, 2001;
Sarawagi and Cohen, 2004), ?or? nodes are con-
sidered implicitly in the dynamic programming
framework. In feature forest models, ?or? nodes
are packed when they have same conditions. For
example, ?or? nodes are packed when they have
same end positions and same labels in the first or-
der semi-CRFs,
In general, we can pack different ?or? nodes that
yield equivalent feature functions in the follow-
ing nodes. In other words, ?or? nodes are packed
when the following states use partial information
on the preceding states. Consider the task of tag-
ging entity and O-entity, where the latter tag is ac-
tually O tags that distinguish the preceding named
entity tags. When we simply apply first-order
semi-CRFs, we must distinguish states that have
different previous states. However, when we want
to distinguish only the preceding named entity tags
rather than the immediate previous states, feature
forests can represent these events more compactly
(Figure 4). We can implement this as follows. In
each ?or? node, we generate the following ?and?
nodes and their feature functions. Then we check
whether there exist ?or? node which has same con-
ditions by using its information about ?end posi-
tion? and ?previous entity?. If so, we connect the
?and? node to the corresponding ?or? node. If not,
we generate a new ?or? node and continue the pro-
cess.
Since the states with label O-entity and entity
are packed, the computational cost of training in
our model (First order semi-CRFs) becomes the
half of the original one.
5 Experiments
5.1 Experimental Setting
Our experiments were performed on the training
and evaluation set provided by the shared task in
COLING 2004 JNLPBA (Kim et al, 2004). The
training data used in this shared task came from
the GENIA version 3.02 corpus. In the task there
are five semantic labels: protein, DNA, RNA,
cell line and cell type. The training set consists
of 2000 abstracts from MEDLINE, and the evalu-
ation set consists of 404 abstracts. We divided the
original training set into 1800 abstracts and 200
abstracts, and the former was used as the training
data and the latter as the development data. For
semi-CRFs, we used amis3 for training the semi-
CRF with feature-forest. We used GENIA taggar4
for POS-tagging and shallow parsing.
We set L = 10 for training and evaluation when
we do not state L explicitly , where L is the upper
bound of the length of possible chunks in semi-
CRFs.
5.2 Features
Table 3 lists the features used in our semi-CRFs.
We describe the chunk-dependent features in de-
tail, which cannot be encoded in token-level fea-
tures.
?Whole chunk? is the normalized names at-
tached to a chunk, which performs like the closed
dictionary. ?Length? and ?Length and End-
Word? capture the tendency of the length of a
named entity. ?Count feature? captures the ten-
dency for named entities to appear repeatedly in
the same sentence.
?Preceding Entity and Prev Word? are fea-
tures that capture specifically words for conjunc-
tions such as ?and? or ?, (comma)?, e.g., for the
phrase ?OCIM1 and K562?, both ?OCIM1? and
?K562? are assigned cell line labels. Even if
the model can determine only that ?OCIM1? is a
cell line , this feature helps ?K562? to be assigned
the label cell line.
5.3 Results
We first evaluated the filtering performance. Table
4 shows the result of the filtering on the training
3http://www-tsujii.is.s.u-tokyo.ac.jp/amis/
4http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/tagger/
Note that the evaluation data are not used for training the GE-
NIA tagger.
469
Table 3: Feature templates used for the chunk s := ws ws+1 ... we where ws and we represent the words
at the beginning and ending of the target chunk respectively. pi is the part of speech tag of wi and sci is
the shallow parse result of wi.
Feature Name description of features
Non-Chunk Features
Word/POS/SC with Position BEGIN + ws, END + we, IN + ws+1, ..., IN + we?1, BEGIN + ps,...
Context Uni-gram/Bi-gram ws?1, we+1, ws?2 + ws?1, we+1 + we+2, ws?1 + we+1
Prefix/Suffix of Chunk 2/3-gram character prefix of ws, 2/3/4-gram character suffix of we
Orthography capitalization and word formation of ws...we
Chunk Features
Whole chunk ws + ws+1 + ... + we
Word/POS/SC End Bi-grams we?1 + we, pe?1 + pe, sce?1 + sce
Length, Length and End Word |s|, |s|+we
Count Feature the frequency of wsws+1..we in a sentence is greater than one
Preceding Entity Features
Preceding Entity /and Prev Word PrevState, PrevState + ws?1
Table 4: Filtering results using the naive Bayes
classifier. The number of entity candidates for the
training set was 4179662, and that of the develop-
ment set was 418628.
Training set
Threshold probability reduction ratio recall
1.0 ? 10?12 0.14 0.984
1.0 ? 10?15 0.20 0.993
Development set
Threshold probability reduction ratio recall
1.0 ? 10?12 0.14 0.985
1.0 ? 10?15 0.20 0.994
and evaluation data. The naive Bayes classifiers
effectively reduced the number of candidate states
with very few falsely removed correct entities.
We then examined the effect of filtering on the
final performance. In this experiment, we could
not examine the performance without filtering us-
ing all the training data, because training on all
the training data without filtering required much
larger memory resources (estimated to be about
80G Byte) than was possible for our experimental
setup. We thus compared the result of the recog-
nizers with and without filtering using only 2000
sentences as the training data. Table 5 shows the
result of the total system with different filtering
thresholds. The result indicates that the filtering
method achieved very well without decreasing the
overall performance.
We next evaluate the effect of filtering, chunk
information and non-local information on final
performance. Table 6 shows the performance re-
sult for the recognition task. L means the upper
bound of the length of possible chunks in semi-
CRFs. We note that we cannot examine the re-
sult of L = 10 without filtering because of the in-
tractable computational cost. The row ?w/o Chunk
Feature? shows the result of the system which does
not employ Chunk-Features in Table 3 at training
and inference. The row ?Preceding Entity? shows
the result of a system which uses Preceding En-
tity and Preceding Entity and Prev Word fea-
tures. The results indicate that the chunk features
contributed to the performance, and the filtering
process enables us to use full chunk representation
(L = 10). The results of McNemar?s test suggest
that the system with chunk features is significantly
better than the system without it (the p-value is
less than 1.0 < 10?4). The result of the preceding
entity information improves the performance. On
the other hand, the system with preceding infor-
mation is not significantly better than the system
without it5. Other non-local information may im-
prove performance with our framework and this is
a topic for future work.
Table 7 shows the result of the overall perfor-
mance in our best setting, which uses the infor-
mation about the preceding entity and 1.0?10?15
threshold probability for filtering. We note that the
result of our system is similar to those of other sys-
5The result of the classifier on development data is 74.64
(without preceding information) and 75.14 (with preceding
information).
470
Table 5: Performance with filtering on the development data. (< 1.0 ? 10?12) means the threshold
probability of the filtering is 1.0 ? 10?12.
Recall Precision F-score Memory Usage (MB) Training Time (s)
Small Training Data = 2000 sentences
Without filtering 65.77 72.80 69.10 4238 7463
Filtering (< 1.0 ? 10.0?12) 64.22 70.62 67.27 600 1080
Filtering (< 1.0 ? 10.0?15) 65.34 72.52 68.74 870 2154
All Training Data = 16713 sentences
Without filtering Not available Not available
Filtering (< 1.0 ? 10.0?12) 70.05 76.06 72.93 10444 14661
Filtering (< 1.0 ? 10.0?15) 72.09 78.47 75.14 15257 31636
Table 6: Overall performance on the evaluation set. L is the upper bound of the length of possible chunks
in semi-CRFs.
Recall Precision F-score
L < 5 64.33 65.51 64.92
L = 10 + Filtering (< 1.0 ? 10.0?12) 70.87 68.33 69.58
L = 10 + Filtering (< 1.0 ? 10.0?15) 72.59 70.16 71.36
w/o Chunk Feature 70.53 69.92 70.22
+ Preceding Entity 72.65 70.35 71.48
tems in several respects, that is, the performance of
cell line is not good, and the performance of the
right boundary identification (78.91% in F-score)
is better than that of the left boundary identifica-
tion (75.19% in F-score).
Table 8 shows a comparison between our sys-
tem and other state-of-the-art systems. Our sys-
tem has achieved a comparable performance to
these systems and would be still improved by us-
ing external resources or conducting pre/post pro-
cessing. For example, Zhou et. al (2004) used
post processing, abbreviation resolution and exter-
nal dictionary, and reported that they improved F-
score by 3.1%, 2.1% and 1.2% respectively. Kim
et. al (2005) used the original GENIA corpus
to employ the information about other semantic
classes for identifying term boundaries. Finkel
et. al (2004) used gazetteers, web-querying, sur-
rounding abstracts, and frequency counts from
the BNC corpus. Settles (2004) used seman-
tic domain knowledge of 17 types of lexicon.
Since our approach and the use of external re-
sources/knowledge do not conflict but are com-
plementary, examining the combination of those
techniques should be an interesting research topic.
Table 7: Performance of our system on the evalu-
ation set
Class Recall Precision F-score
protein 77.74 68.92 73.07
DNA 69.03 70.16 69.59
RNA 69.49 67.21 68.33
cell type 65.33 82.19 72.80
cell line 57.60 53.14 55.28
overall 72.65 70.35 71.48
Table 8: Comparison with other systems
System Recall Precision F-score
Zhou et. al (2004) 75.99 69.42 72.55
Our system 72.65 70.35 71.48
Kim et.al (2005) 72.77 69.68 71.19
Finkel et. al (2004) 68.56 71.62 70.06
Settles (2004) 70.3 69.3 69.8
471
6 Conclusion
In this paper, we have proposed a single proba-
bilistic model that can capture important charac-
teristics of biomedical named entities. To over-
come the prohibitive computational cost, we have
presented an efficient training framework and a fil-
tering method which enabled us to apply first or-
der semi-CRF models to sentences having many
labels and entities with long names. Our results
showed that our filtering method works very well
without decreasing the overall performance. Our
system achieved an F-score of 71.48% without the
use of gazetteers, post-processing or external re-
sources. The performance of our system came
close to that of the current best performing system
which makes extensive use of external resources
and rule based post-processing.
The contribution of the non-local information
introduced by our method was not significant in
the experiments. However, other types of non-
local information have also been shown to be ef-
fective (Finkel et al, 2005) and we will examine
the effectiveness of other non-local information
which can be embedded into label information.
As the next stage of our research, we hope to ap-
ply our method to shallow parsing, in which seg-
ments tend to be long and non-local information is
important.
References
Daniel M. Bikel, Richard Schwartz, and Ralph
Weischedel. 1997. Nymble: a high-performance
learning name-finder. In Proc. of the Fifth Confer-
ence on Applied Natural Language Processing.
Jenny Finkel, Shipra Dingare, Huy Nguyen, Malv-
ina Nissim, Gail Sinclair, and Christopher Man-
ning. 2004. Exploiting context for biomedical en-
tity recognition: From syntax to the web. In Proc. of
JNLPBA-04.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by Gibbs
sampling. In Proc. of ACL 2005, pages 363?370.
Jin-Dong Kim, Tomoko Ohta, Yoshimasa Tsuruoka,
Yuka Tateisi, and Nigel Collier. 2004. Introduc-
tion to the bio-entity recognition task at JNLPBA.
In Proc. of JNLPBA-04, pages 70?75.
Seonho Kim, Juntae Yoon, Kyung-Mi Park, and Hae-
Chang Rim. 2005. Two-phase biomedical named
entity recognition using a hybrid method. In Proc. of
the Second International Joint Conference on Natu-
ral Language Processing (IJCNLP-05).
Zhenzhen Kou, William W. Cohen, and Robert F. Mur-
phy. 2005. High-recall protein entity recognition
using a dictionary. Bioinformatics 2005 21.
Micahel Krauthammer and Goran Nenadic. 2004.
Term identification in the biomedical literature. Jor-
nal of Biomedical Informatics.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proc. of ICML 2001.
Yusuke Miyao and Jun?ichi Tsujii. 2002. Maximum
entropy estimation for feature forests. In Proc. of
HLT 2002.
Peshkin and Pfeffer. 2003. Bayesian information ex-
traction network. In IJCAI.
Sunita Sarawagi and William W. Cohen. 2004. Semi-
markov conditional random fields for information
extraction. In NIPS 2004.
Burr Settles. 2004. Biomedical named entity recogni-
tion using conditional random fields and rich feature
sets. In Proc. of JNLPBA-04.
Beth M. Sundheim. 1995. Overview of results of the
MUC-6 evaluation. In Sixth Message Understand-
ing Conference (MUC-6), pages 13?32.
Charles Sutton and Andrew McCallum. 2004. Collec-
tive segmentation and labeling of distant entities in
information extraction. In ICML workshop on Sta-
tistical Relational Learning.
Yoshimasa Tsuruoka and Jun?ichi Tsujii. 2005. Chunk
parsing revisited. In Proceedings of the 9th Inter-
national Workshop on Parsing Technologies (IWPT
2005).
GuoDong Zhou and Jian Su. 2004. Exploring deep
knowledge resources in biomedical name recogni-
tion. In Proc. of JNLPBA-04.
472
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 1017?1024,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Semantic Retrieval for the Accurate Identification of Relational Concepts
in Massive Textbases
Yusuke Miyao? Tomoko Ohta? Katsuya Masuda? Yoshimasa Tsuruoka?
Kazuhiro Yoshida? Takashi Ninomiya? Jun?ichi Tsujii??
?Department of Computer Science, University of Tokyo
?School of Informatics, University of Manchester
?Information Technology Center, University of Tokyo
Hongo 7-3-1, Bunkyo-ku, Tokyo 113-0033 JAPAN
{yusuke,okap,kmasuda,tsuruoka,kyoshida,ninomi,tsujii}@is.s.u-tokyo.ac.jp
Abstract
This paper introduces a novel framework
for the accurate retrieval of relational con-
cepts from huge texts. Prior to retrieval,
all sentences are annotated with predicate
argument structures and ontological iden-
tifiers by applying a deep parser and a term
recognizer. During the run time, user re-
quests are converted into queries of region
algebra on these annotations. Structural
matching with pre-computed semantic an-
notations establishes the accurate and effi-
cient retrieval of relational concepts. This
framework was applied to a text retrieval
system for MEDLINE. Experiments on
the retrieval of biomedical correlations re-
vealed that the cost is sufficiently small for
real-time applications and that the retrieval
precision is significantly improved.
1 Introduction
Rapid expansion of text information has motivated
the development of efficient methods of access-
ing information in huge texts. Furthermore, user
demand has shifted toward the retrieval of more
precise and complex information, including re-
lational concepts. For example, biomedical re-
searchers deal with a massive quantity of publica-
tions; MEDLINE contains approximately 15 mil-
lion references to journal articles in life sciences,
and its size is rapidly increasing, at a rate of more
than 10% yearly (National Library of Medicine,
2005). Researchers would like to be able to
search this huge textbase for biomedical correla-
tions such as protein-protein or gene-disease asso-
ciations (Blaschke and Valencia, 2002; Hao et al,
2005; Chun et al, 2006). However, the framework
of traditional information retrieval (IR) has diffi-
culty with the accurate retrieval of such relational
concepts because relational concepts are essen-
tially determined by semantic relations between
words, and keyword-based IR techniques are in-
sufficient to describe such relations precisely.
The present paper demonstrates a framework
for the accurate real-time retrieval of relational
concepts from huge texts. Prior to retrieval, we
prepare a semantically annotated textbase by ap-
plying NLP tools including deep parsers and term
recognizers. That is, all sentences are annotated
in advance with semantic structures and are stored
in a structured database. User requests are con-
verted on the fly into patterns of these semantic
annotations, and texts are retrieved by matching
these patterns with the pre-computed semantic an-
notations. The accurate retrieval of relational con-
cepts is attained because we can precisely describe
relational concepts using semantic annotations. In
addition, real-time retrieval is possible because se-
mantic annotations are computed in advance.
This framework has been implemented for a
text retrieval system for MEDLINE. We first ap-
ply a deep parser (Miyao and Tsujii, 2005) and
a dictionary-based term recognizer (Tsuruoka and
Tsujii, 2004) to MEDLINE and obtain annotations
of predicate argument structures and ontological
identifiers of genes, gene products, diseases, and
events. We then provide a search engine for these
annotated sentences. User requests are converted
into queries of region algebra (Clarke et al, 1995)
extended with variables (Masuda et al, 2006) on
these annotations. A search engine for the ex-
tended region algebra efficiently finds sentences
having semantic annotations that match the input
queries. In this paper, we evaluate this system with
respect to the retrieval of biomedical correlations
1017
Symbol CRP
Name C-reactive protein, pentraxin-related
Species Homo sapiens
Synonym MGC88244, PTX1
Product C-reactive protein precursor, C-reactive
protein, pentraxin-related protein
External links EntrezGene:1401, GDB:119071, . . .
Table 1: An example GENA entry
and examine the effects of using predicate argu-
ment structures and ontological identifiers.
The need for the discovery of relational con-
cepts has been investigated intensively in Infor-
mation Extraction (IE). However, little research
has targeted on-demand retrieval from huge texts.
One difficulty is that IE techniques such as pat-
tern matching and machine learning require heav-
ier processing in order to be applied on the fly.
Another difficulty is that target information must
be formalized beforehand and each system is de-
signed for a specific task. For instance, an IE
system for protein-protein interactions is not use-
ful for finding gene-disease associations. Apart
from IE research, enrichment of texts with vari-
ous annotations has been proposed and is becom-
ing a new research area for information manage-
ment (IBM, 2005; TEI, 2004). The present study
basically examines this new direction in research.
The significant contribution of the present paper,
however, is to provide the first empirical results of
this framework for a real task with a huge textbase.
2 Background: Resources and Tools for
Semantic Annotations
The proposed system for the retrieval of relational
concepts is a product of recent developments in
NLP resources and tools. In this section, ontology
databases, deep parsers, and search algorithms for
structured data are introduced.
2.1 Ontology databases
Ontology databases are collections of words and
phrases in specific domains. Such databases have
been constructed extensively for the systematic
management of domain knowledge by organizing
textual expressions of ontological entities that are
detached from actual sentences.
For example, GENA (Koike and Takagi, 2004)
is a database of genes and gene products that
is semi-automatically collected from well-known
databases, including HUGO, OMIM, Genatlas,
Locuslink, GDB, MGI, FlyBase, WormBase,
Figure 1: An output of HPSG parsing
Figure 2: A predicate argument structure
CYGD, and SGD. Table 1 shows an example of
a GENA entry. ?Symbol? and ?Name? denote
short forms and nomenclatures of genes, respec-
tively. ?Species? represents the organism species
in which this gene is observed. ?Synonym? is a
list of synonyms and name variations. ?Product?
gives a list of products of this gene, such as pro-
teins coded by this gene. ?External links? pro-
vides links to other databases, and helps to obtain
detailed information from these databases. For
biomedical terms other than genes/gene products,
the Unified Medical Language System (UMLS)
meta-thesaurus (Lindberg et al, 1993) is a large
database that contains various names of biomedi-
cal and health-related concepts.
Ontology databases provide mappings be-
tween textual expressions and entities in the real
world. For example, Table 1 indicates that CRP,
MGC88244, and PTX1 denote the same gene con-
ceptually. Hence, these resources enable us to
canonicalize variations of textual expressions of
ontological entities.
2.2 Parsing technologies
Recently, state-of-the-art CFG parsers (Charniak
and Johnson, 2005) can compute phrase structures
of natural sentences at fairly high accuracy. These
parsers have been used in various NLP tasks in-
cluding IE and text mining. In addition, parsers
that compute deeper analyses, such as predicate
argument structures, have become available for
1018
the processing of real-world sentences (Miyao and
Tsujii, 2005). Predicate argument structures are
canonicalized representations of sentence mean-
ings, and express the semantic relations of words
explicitly. Figure 1 shows an output of an HPSG
parser (Miyao and Tsujii, 2005) for the sentence
?A normal serum CRP measurement does not ex-
clude deep vein thrombosis.? The dotted lines ex-
press predicate argument relations. For example,
the ARG1 arrow coming from ?exclude? points
to the noun phrase ?A normal serum CRP mea-
surement?, which indicates that the subject of ?ex-
clude? is this noun phrase, while such relations are
not explicitly represented by phrase structures.
Predicate argument structures are beneficial for
our purpose because they can represent relational
concepts in an abstract manner. For example, the
relational concept of ?CRP excludes thrombosis?
can be represented as a predicate argument struc-
ture, as shown in Figure 2. This structure is univer-
sal in various syntactic expressions, such as pas-
sivization (e.g., ?thrombosis is excluded by CRP?)
and relativization (e.g., ?thrombosis that CRP ex-
cludes?). Hence, we can abstract surface varia-
tions of sentences and describe relational concepts
in a canonicalized form.
2.3 Structural search algorithms
Search algorithms for structured texts have been
studied extensively, and examples include XML
databases with XPath (Clark and DeRose, 1999)
and XQuery (Boag et al, 2005), and region alge-
bra (Clarke et al, 1995). The present study fo-
cuses on region algebra extended with variables
(Masuda et al, 2006) because it provides an ef-
ficient search algorithm for tags with cross bound-
aries. When we annotate texts with various levels
of syntactic/semantic structures, cross boundaries
are inherently nonnegligible. In fact, as described
in Section 3, our system exploits annotations of
predicate argument structures and ontological en-
tities, which include substantial cross boundaries.
Region algebra is defined as a set of operators
on regions, i.e., word sequences. Table 2 shows
operators of the extended region algebra, where
A and B denote regions, and results of operations
are also regions. For example, ?A & B? denotes a
region that includes both A and B. Four contain-
ment operators, >, >>, <, and <<, represent an-
cestor/descendant relations in XML. For example,
?A > B? indicates that A is an ancestor of B. In
[tag] Region covered with ?<tag>?
A > B A containing B
A >> B A containing B (A is not nested)
A < B A contained by B
A << B A contained by B (B is not nested)
A - B Starting with A and ending with B
A & B A and B
A | B A or B
Table 2: Operators of the extended region algebra
[sentence] >>
(([word arg1="$subject"] > exclude) &
([phrase id="$subject"] > CRP))
Figure 3: A query of the extended region algebra
Figure 4: Matching with the query in Figure 3
search algorithms for region algebra, the cost of
retrieving the first answer is constant, and that of
an exhaustive search is bounded by the lowest fre-
quency of a word in a query (Clarke et al, 1995).
Variables in the extended region algebra allow
us to express shared structures and are necessary
in order to describe predicate argument structures.
For example, Figure 3 shows a formula in the ex-
tended region algebra that represents the predicate
argument structure of ?CRP excludes something.?
This formula indicates that a sentence contains a
region in which the word ?exclude? exists, the
first argument (?arg1?) phrase of which includes
the word ?CRP.? A predicate argument relation is
expressed by the variable, ?$subject.? Figure 4
shows a situation in which this formula is satisfied.
Three horizontal bars describe regions covered by
<sentence>, <phrase>, and <word> tags,
respectively. The dotted line denotes the relation
expressed by this variable. Given this formula as a
query, a search engine can retrieve sentences hav-
ing semantic annotations that satisfy this formula.
3 A Text Retrieval System for MEDLINE
While the above resources and tools have been de-
veloped independently, their collaboration opens
up a new framework for the retrieval of relational
concepts, as described below (Figure 5).
Off-line processing: Prior to retrieval, a deep
parser is applied to compute predicate argument
1019
Figure 5: Framework of semantic retrieval
structures, and a term recognizer is applied to cre-
ate mappings from textual expressions into identi-
fiers in ontology databases. Semantic annotations
are stored and indexed in a structured database for
the extended region algebra.
On-line processing: User input is converted into
queries of the extended region algebra. A search
engine retrieves sentences having semantic anno-
tations that match the queries.
This framework is applied to a text retrieval en-
gine for MEDLINE. MEDLINE is an exhaustive
database covering nearly 4,500 journals in the life
sciences and includes the bibliographies of arti-
cles, about half of which have abstracts. Research
on IE and text mining in biomedical science has
focused mainly on MEDLINE. In the present pa-
per, we target al articles indexed in MEDLINE at
the end of 2004 (14,785,094 articles). The follow-
ing sections explain in detail off-/on-line process-
ing for the text retrieval system for MEDLINE.
3.1 Off-line processing: HPSG parsing and
term recognition
We first parsed all sentences using an HPSG parser
(Miyao and Tsujii, 2005) to obtain their predi-
cate argument structures. Because our target is
biomedical texts, we re-trained a parser (Hara et
al., 2005) with the GENIA treebank (Tateisi et
al., 2005), and also applied a bidirectional part-of-
speech tagger (Tsuruoka and Tsujii, 2005) trained
with the GENIA treebank as a preprocessor.
Because parsing speed is still unrealistic for
parsing the entire MEDLINE on a single ma-
chine, we used two geographically separated com-
puter clusters having 170 nodes (340 Xeon CPUs).
These clusters are separately administered and not
dedicated for use in the present study. In order to
effectively use such an environment, GXP (Taura,
2004) was used to connect these clusters and dis-
tribute the load among them. Our processes were
given the lowest priority so that our task would not
disturb other users. We finished parsing the entire
MEDLINE in nine days (Ninomiya et al, 2006).
# entries (genes) 517,773
# entries (gene products) 171,711
# entries (diseases) 148,602
# expanded entries 4,467,855
Table 3: Sizes of ontologies used for term recog-
nition
Event type Expressions
influence effect, affect, role, response, . . .
regulation mediate, regulate, regulation, . . .
activation induce, activate, activation, . . .
Table 4: Event expression ontology
Next, we annotated technical terms, such as
genes and diseases, to create mappings to onto-
logical identifiers. A dictionary-based term recog-
nition algorithm (Tsuruoka and Tsujii, 2004) was
applied for this task. First, an expanded term
list was created by generating name variations of
terms in GENA and the UMLS meta-thesaurus1.
Table 3 shows the size of the original database and
the number of entries expanded by name varia-
tions. Terms in MEDLINE were then identified
by the longest matching of entries in this expanded
list with words/phrases in MEDLINE.
The necessity of ontologies is not limited to
nominal expressions. Various verbs are used for
expressing events. For example, activation events
of proteins can be expressed by ?activate,? ?en-
hance,? and other event expressions. Although the
numbers of verbs and their event types are much
smaller than those of technical terms, verbal ex-
pressions are important for the description of rela-
tional concepts. Since ontologies of event expres-
sions in this domain have not yet been constructed,
we developed an ontology from scratch. We inves-
tigated 500 abstracts extracted from MEDLINE,
and classified 167 frequent expressions, including
verbs and their nominalized forms, into 18 event
types. Table 4 shows a part of this ontology. These
expressions in MEDLINE were automatically an-
notated with event types.
As a result, we obtained semantically annotated
MEDLINE. Table 5 shows the size of the orig-
inal MEDLINE and semantic annotations. Fig-
ure 6 shows semantic annotations for the sentence
in Figure 1, where ?-? indicates nodes of XML,2
1We collected disease names by specifying a query with
the semantic type as ?Disease or Syndrome.?
2Although this example is shown in XML, this textbase
contains tags with cross boundaries because tags for predicate
argument structures and technical terms may overlap.
1020
# papers 14,785,094
# abstracts 7,291,857
# sentences 70,935,630
# words 1,462,626,934
# successfully parsed sentences 69,243,788
# predicate argument relations 1,510,233,701
# phrase tags 3,094,105,383
# terms (genes) 84,998,621
# terms (gene products) 27,471,488
# terms (diseases) 19,150,984
# terms (event expressions) 51,810,047
Size of the original MEDLINE 9.3 GByte
Size of the semantic annotations 292 GByte
Size of the index file for region algebra 954 GByte
Table 5: Sizes of the original and semantically an-
notated MEDLINE textbases
- <sentence sentence_id="e6e525">
- <phrase id="0" cat="S" head="15" lex_head="18">
- <phrase id="1" cat="NP" head="4" lex_head="14">
- <phrase id="2" cat="DT" head="3" lex_head="3">
- <word id="3" pos="DT" cat="DT" base="a" arg1="4">
- A
- <phrase id="4" cat="NP" head="7" lex_head="14">
- <phrase id="5" cat="AJ" head="6" lex_head="6">
- <word id="6" pos="JJ" cat="AJ" base="normal" arg1="7">
- normal
- <phrase id="7" cat="NP" head="10" lex_head="14">
- <phrase id="8" cat="NP" head="9" lex_head="9">
- <word id="9" pos="NN" cat="NP" base="serum" mod="10">
- serum
- <phrase id="10" cat="NP" head="13" lex_head="14">
- <phrase id="11" cat="NP" head="12" lex_head="12">
- <entity_name id="entity-1" type="gene"
gene_id="GHS003134" gene_symbol="CRP"
gene_name="C-reactive protein, pentraxin-related"
species="Homo sapiens"
db_site="EntrezGene:1401|GDB:119071|GenAtlas:CRP">
- <word id="12" pos="NN" cat="NP" base="crp" mod="13">
- CRP
- <phrase id="13" cat="NP" head="14" lex_head="14">
- <word id="14" pos="NN" cat="NP" base="measurement">
- measurement
- <phrase id="15" cat="VP" head="16" lex_head="18">
- <phrase id="16" cat="VP" head="17" lex_head="18">
- <phrase id="17" cat="VP" head="18" lex_head="18">
- <word id="18" pos="VBZ" cat="VP" base="do"
arg1="1" arg2="21">
- does
- <phrase id="19" cat="AV" head="20" lex_head="20">
- <word id="20" pos="RB" cat="AV" base="not" arg1="21">
- not
- <phrase id="21" cat="VP" head="22" lex_head="23">
- <phrase id="22" cat="VP" head="23" lex_head="23">
- <word id="23" pos="VB" cat="VP" base="exclude"
arg1="1" arg2="24">
- exclude
...
Figure 6: A semantically annotated sentence
although the latter half of the sentence is omitted
because of space limitations. Sentences are an-
notated with four tags,3 ?phrase,? ?word,? ?sen-
tence,? and ?entity name,? and their attributes as
given in Table 6. Predicate argument structures are
annotated as attributes, ?mod? and ?argX ,? which
point to the IDs of the argument phrases. For ex-
ample, in Figure 6, the <word> tag for ?exclude?
has the attributes arg1="1" and arg2="24",
which denote the IDs of the subject and object
phrases, respectively.
3Additional tags exist for representing document struc-
tures such as ?title? (details omitted).
Tag Attributes
phrase id, cat, head, lex head
word id, cat, pos, base, mod, argX , rel type
sentence sentence id
entity name id, type, gene id/disease id, gene symbol,
gene name, species, db site
Attribute Description
id unique identifier
cat syntactic category
head head daughter?s ID
lex head lexical head?s ID
pos part-of-speech
base base form of the word
mod ID of modifying phrase
argX ID of the X-th argument of the word
rel type event type
sentence id sentence?s ID
type whether gene, gene prod, or disease
gene id ID in GENA
disease id ID in the UMLS meta-thesaurus
gene symbol short form of the gene
gene name nomenclature of the gene
species species that have this gene
db site links to external databases
Table 6: Tags (upper) and attributes (lower) for
semantic annotations
3.2 On-line processing
The off-line processing described above results in
much simpler on-line processing. User input is
converted into queries of the extended region al-
gebra, and the converted queries are entered into a
search engine for the extended region algebra. The
implementation of a search engine is described in
detail in Masuda et al (2006).
Basically, given subject x, object y, and verb v,
the system creates the following query:
[sentence] >>
([word arg1="$subject" arg2="$object"
base="v"] &
([phrase id="$subject"] > x) &
([phrase id="$object"] > y))
Ontological identifiers are substituted for x, y,
and v, if possible. Nominal keywords, i.e., x and
y, are replaced by [entity_name gene_id="n"]
or [entity_name disease_id="n"], where n is
the ontological identifier of x or y. For verbal key-
words, base="v" is replaced by rel_type="r",
where r is the event type of v.
4 Evaluation
Our system is evaluated with respect to speed and
accuracy. Speed is indispensable for real-time in-
teractive text retrieval systems, and accuracy is key
for the motivation of semantic retrieval. That is,
our motivation for employing semantic retrieval
1021
Query No. User input
1 something inhibit ERK2
2 something trigger diabetes
3 adiponectin increase something
4 TNF activate IL6
5 dystrophin cause disease
6 macrophage induce something
7 something suppress MAP phosphorylation
8 something enhance p53 (negative)
Table 7: Queries for experiments
[sentence] >>
([word rel_type="activation"] &
[entity_name type="gene" gene_id="GHS019685"] &
[entity_name type="gene" gene_id="GHS009426"])
[sentence] >>
([word arg1="$subject" arg2="$object"
rel_type="activation"] &
([phrase id="$subject"] >
[entity_name type="gene" gene_id="GHS019685"]) &
([phrase cat="np" id="$object"] >
[entity_name type="gene" gene_id="GHS009426"]))
Figure 7: Queries of the extended region algebra
for Query 4-3 (upper: keyword search, lower: se-
mantic search)
was to provide a device for the accurate identifica-
tion of relational concepts. In particular, high pre-
cision is desired in text retrieval from huge texts
because users want to extract relevant information,
rather than collect exhaustive information.
We have two parameters to vary: whether to
use predicate argument structures and whether to
use ontological identifiers. The effect of using
predicate argument structures is evaluated by com-
paring ?keyword search? with ?semantic search.?
The former is a traditional style of IR, in which
sentences are retrieved by matching words in a
query with words in sentences. The latter is a
new feature of the present system, in which sen-
tences are retrieved by matching predicate argu-
ment relations in a query with those in a semanti-
cally annotated textbase. The effect of using onto-
logical identifiers is assessed by changing queries
of the extended region algebra. When we use the
term ontology, nominal keywords in queries are
replaced with ontological identifiers in GENA and
the UMLS meta-thesaurus. When we use the event
expression ontology, verbal keywords in queries
are replaced with event types.
Table 7 is a list of queries used in the follow-
ing experiments. Words in italics indicate a class
of words: ?something? indicates that any word
can appear, and disease indicates that any dis-
ease expression can appear. These queries were
selected by a biologist, and express typical re-
lational concepts that a biologist may wish to
find. Queries 1, 3, and 4 represent relations of
genes/proteins, where ERK2, adiponectin, TNF,
and IL6 are genes/proteins. Queries 2 and 5 de-
scribe relations concerning diseases, and Query 6
is a query that is not relevant to genes or diseases.
Query 7 expresses a complex relation concern-
ing a specific phenomena, i.e., phosphorylation,
of MAP. Query 8 describes a relation concerning
a gene, i.e., p53, while ?(negative)? indicates that
the target of retrieval is negative mentions. This is
expressed by ?not? modifying a predicate.
For example, Query 4 attempts to retrieve sen-
tences that mention the protein-protein interaction
?TNF activates IL6.? This is converted into queries
of the extended region algebra given in Figure 7.
The upper query is for keyword search and only
specifies the appearances of the three words. Note
that the keywords are translated into the ontolog-
ical identifiers, ?activation,? ?GHS019685,? and
?GHS009426.? The lower query is for semantic
search. The variables in ?arg1? and ?arg2? indi-
cate that ?GHS019685? and ?GHS009426? are the
subject and object, respectively, of ?activation?.
Table 8 summarizes the results of the experi-
ments. The postfixes of query numbers denote
whether ontological identifiers are used. X-1 used
no ontologies, and X-2 used only the term ontol-
ogy. X-3 used both the term and event expression
ontologies4. Comparison of X-1 and X-2 clarifies
the effect of using the term ontology. Comparison
of X-2 and X-3 shows the effect of the event ex-
pression ontology. The results for X-3 indicate
the maximum performance of the current system.
This table shows that the time required for the se-
mantic search for the first answer, shown as ?time
(first)? in seconds, was reasonably short. Thus,
the present framework is acceptable for real-time
text retrieval. The numbers of answers increased
when we used the ontologies, and this result indi-
cates the efficacy of both ontologies for obtaining
relational concepts written in various expressions.
Accuracy was measured by judgment by a bi-
ologist. At most 100 sentences were retrieved for
each query, and the results of keyword search and
semantic search were merged and shuffled. A bi-
ologist judged the shuffled sentences (1,839 sen-
tences in total) without knowing whether the sen-
4Query 5-1 is not tested because ?disease? requires
the term ontology, and Query 6-2 is not tested because
?macrophage? is not assigned an ontological identifier.
1022
Query Keyword search Semantic search
No. # ans. time (first/all) precision n-precision # ans. time (first/all) precision relative recall
1-1 252 0.00/ 1.5 74/100 (74%) 74/100 (74%) 143 0.01/ 2.5 96/100 (96%) 51/74 (69%)
1-2 348 0.00/ 1.9 61/100 (61%) 61/100 (61%) 174 0.01/ 3.1 89/100 (89%) 42/61 (69%)
1-3 884 0.00/ 3.2 50/100 (50%) 50/100 (50%) 292 0.01/ 5.3 91/100 (91%) 21/50 (42%)
2-1 125 0.00/ 1.8 45/100 (45%) 9/ 27 (33%) 27 0.02/ 2.9 23/ 27 (85%) 17/45 (38%)
2-2 113 0.00/ 2.9 40/100 (40%) 10/ 26 (38%) 26 0.06/ 4.0 22/ 26 (85%) 19/40 (48%)
2-3 6529 0.00/ 12.1 42/100 (42%) 42/100 (42%) 662 0.01/1527.4 76/100 (76%) 8/42 (19%)
3-1 287 0.00/ 1.5 20/100 (20%) 4/ 30 (13%) 30 0.05/ 2.4 23/ 30 (80%) 6/20 (30%)
3-2 309 0.01/ 2.1 21/100 (21%) 4/ 32 (13%) 32 0.10/ 3.5 26/ 32 (81%) 6/21 (29%)
3-3 338 0.01/ 2.2 24/100 (24%) 8/ 39 (21%) 39 0.05/ 3.6 32/ 39 (82%) 8/24 (33%)
4-1 4 0.26/ 1.5 0/ 4 (0%) 0/ 0 (?) 0 2.44/ 2.4 0/ 0 (?) 0/ 0 (?)
4-2 195 0.01/ 2.5 9/100 (9%) 1/ 6 (17%) 6 0.09/ 4.1 5/ 6 (83%) 2/ 9 (22%)
4-3 2063 0.00/ 7.5 5/100 (5%) 5/ 94 (5%) 94 0.02/ 10.5 89/ 94 (95%) 2/ 5 (40%)
5-2 287 0.08/ 6.3 73/100 (73%) 73/100 (73%) 116 0.05/ 14.7 97/100 (97%) 37/73 (51%)
5-3 602 0.01/ 15.9 50/100 (50%) 50/100 (50%) 122 0.05/ 14.2 96/100 (96%) 23/50 (46%)
6-1 10698 0.00/ 42.8 14/100 (14%) 14/100 (14%) 1559 0.01/3014.5 65/100 (65%) 10/14 (71%)
6-3 42106 0.00/3379.5 11/100 (11%) 11/100 (11%) 2776 0.01/5100.1 61/100 (61%) 5/11 (45%)
7 87 0.04/ 2.7 34/ 87 (39%) 7/ 15 (47%) 15 0.05/ 4.2 10/ 15 (67%) 10/34 (29%)
8 1812 0.01/ 7.6 19/100 (19%) 17/ 84 (20%) 84 0.20/ 29.2 73/ 84 (87%) 7/19 (37%)
Table 8: Number of retrieved sentences, retrieval time, and accuracy
tence was retrieved by keyword search or semantic
search. Without considering which words actually
matched the query, a sentence is judged to be cor-
rect when any part of the sentence expresses all of
the relations described by the query. The modality
of sentences was not distinguished, except in the
case of Query 8. These evaluation criteria may be
disadvantageous for the semantic search because
its ability to exactly recognize the participants of
relational concepts is not evaluated. Table 8 shows
the precision attained by keyword/semantic search
and n-precision, which denotes the precision of
the keyword search, in which the same number,
n, of outputs is taken as the semantic search. The
table also gives the relative recall of the semantic
search, which represents the ratio of sentences that
are correctly output by the semantic search among
those correctly output by the keyword search. This
does not necessarily represent the true recall be-
cause sentences not output by keyword search are
excluded. However, this is sufficient for the com-
parison of keyword search and semantic search.
The results show that the semantic search exhib-
ited impressive improvements in precision. The
precision was over 80% for most queries and was
nearly 100% for Queries 4 and 5. This indicates
that predicate argument structures are effective for
representing relational concepts precisely, espe-
cially for relations in which two entities are in-
volved. Relative recall was approximately 30?
50%, except for Query 2. In the following, we
will investigate the reasons for the residual errors.
Table 9 shows the classifications of the errors of
Disregarding of noun phrase structures 45
Term recognition errors 33
Parsing errors 11
Other reasons 8
Incorrect human judgment 7
Nominal expressions 41
Phrasal verb expressions 26
Inference required 24
Coreference resolution required 19
Parsing errors 16
Other reasons 15
Incorrect human judgment 10
Table 9: Error analysis (upper: 104 false positives,
lower: 151 false negatives)
semantic retrieval. The major reason for false pos-
itives was that our queries ignore internal struc-
tures of noun phrases. The system therefore re-
trieved noun phrases that do not directly mention
target entities. For example, ?the increased mor-
tality in patients with diabetes was caused by . . . ?
does not indicate the trigger of diabetes. Another
reason was term recognition errors. For exam-
ple, the system falsely retrieved sentences con-
taining ?p40,? which is sometimes, but not nec-
essarily used as a synonym for ?ERK2.? Ma-
chine learning-based term disambiguation will al-
leviate these errors. False negatives were caused
mainly by nominal expressions such as ?the in-
hibition of ERK2.? This is because the present
system does not convert user input into queries
on nominal expressions. Another major reason,
phrasal verb expressions such as ?lead to,? is also
a shortage of our current strategy of query cre-
ation. Because semantic annotations already in-
1023
clude linguistic structures of these expressions, the
present system can be improved further by creat-
ing queries on such expressions.
5 Conclusion
We demonstrated a text retrieval system for MED-
LINE that exploits pre-computed semantic anno-
tations5. Experimental results revealed that the
proposed system is sufficiently efficient for real-
time text retrieval and that the precision of re-
trieval was remarkably high. Analysis of resid-
ual errors showed that the handling of noun phrase
structures and the improvement of term recogni-
tion will increase retrieval accuracy. Although
the present paper focused on MEDLINE, the NLP
tools used in this system are domain/task indepen-
dent. This framework will thus be applicable to
other domains such as patent documents.
The present framework does not conflict with
conventional IR/IE techniques, and integration
with these techniques is expected to improve the
accuracy and usability of the proposed system. For
example, query expansion and relevancy feedback
can be integrated in a straightforward way in order
to improve accuracy. Document ranking is useful
for the readability of retrieved results. IE systems
can be applied off-line, in the manner of the deep
parser in our system, for annotating sentences with
target information of IE. Such annotations will en-
able us to retrieve higher-level concepts, such as
relationships among relational concepts.
Acknowledgment
This work was partially supported by Grant-in-Aid
for Scientific Research on Priority Areas ?Systems
Genomics? (MEXT, Japan), Genome Network
Project (NIG, Japan), and Solution-Oriented Re-
search for Science and Technology (JST, Japan).
References
C. Blaschke and A. Valencia. 2002. The frame-based
module of the SUISEKI information extraction sys-
tem. IEEE Intelligent Systems, 17(2):14?20.
S. Boag, D. Chamberlin, M. F. Ferna?ndez, D. Florescu,
J. Robie, and J. Sime?on. 2005. XQuery 1.0: An
XML query language.
E. Charniak and M. Johnson. 2005. Coarse-to-fine n-
best parsing and MaxEnt discriminative reranking.
In Proc. ACL 2005.
5A web-based demo of our system is available on-line at:
http://www-tsujii.is.s.u-tokyo.ac.jp/medie/
H.-W. Chun, Y. Tsuruoka, J.-D. Kim, R. Shiba, N. Na-
gata, T. Hishiki, and J. Tsujii. 2006. Extraction
of gene-disease relations from MedLine using do-
main dictionaries and machine learning. In Proc.
PSB 2006, pages 4?15.
J. Clark and S. DeRose. 1999. XML Path Language
(XPath) version 1.0.
C. L. A. Clarke, G. V. Cormack, and F. J. Burkowski.
1995. An algebra for structured text search and a
framework for its implementation. The Computer
Journal, 38(1):43?56.
Y. Hao, X. Zhu, M. Huang, and M. Li. 2005. Dis-
covering patterns to extract protein-protein interac-
tions from the literature: Part II. Bioinformatics,
21(15):3294?3300.
T. Hara, Y. Miyao, and J. Tsujii. 2005. Adapting
a probabilistic disambiguation model of an HPSG
parser to a new domain. In Proc. IJCNLP 2005.
IBM, 2005. Unstructed Information Management Ar-
chitecture (UIMA) SDK User?s Guide and Refer-
ence.
A. Koike and T. Takagi. 2004. Gene/protein/family
name recognition in biomedical literature. In Proc.
Biolink 2004, pages 9?16.
D. A. Lindberg, B. L. Humphreys, and A. T. Mc-
Cray. 1993. The Unified Medical Language Sys-
tem. Methods in Inf. Med., 32(4):281?291.
K. Masuda, T. Ninomiya, Y. Miyao, T. Ohta, and
J. Tsujii. 2006. Nested region algebra extended with
variables. In Preparation.
Y. Miyao and J. Tsujii. 2005. Probabilistic disam-
biguation models for wide-coverage HPSG parsing.
In Proc. 43rd ACL, pages 83?90.
National Library of Medicine. 2005. Fact Sheet MED-
LINE. Available at http://www.nlm.nih.
gov/pubs/factsheets/medline.html.
T. Ninomiya, Y. Tsuruoka, Y. Miyao, K. Taura, and
J. Tsujii. 2006. Fast and scalable HPSG parsing.
Traitement automatique des langues (TAL), 46(2).
Y. Tateisi, A. Yakushiji, T. Ohta, and J. Tsujii. 2005.
Syntax annotation for the GENIA corpus. In Proc.
IJCNLP 2005, Companion volume, pages 222?227.
K. Taura. 2004. GXP : An interactive shell for the grid
environment. In Proc. IWIA2004, pages 59?67.
TEI Consortium, 2004. Text Encoding Initiative.
Y. Tsuruoka and J. Tsujii. 2004. Improving the per-
formance of dictionary-based approaches in protein
name recognition. Journal of Biomedical Informat-
ics, 37(6):461?470.
Y. Tsuruoka and J. Tsujii. 2005. Bidirectional infer-
ence with the easiest-first strategy for tagging se-
quence data. In Proc. HLT/EMNLP 2005, pages
467?474.
1024
Proceedings of the BioNLP Workshop on Linking Natural Language Processing and Biology at HLT-NAACL 06, pages 136?137,
New York City, June 2006. c?2006 Association for Computational Linguistics
Subdomain adaptation of a POS tagger with a small corpus 
 
1 Introduction    
For the domain of biomedical research abstracts, 
two large corpora, namely GENIA (Kim et al
2003) and Penn BioIE (Kulik et al2004) are avail-
able. Both are basically in human domain and the 
performance of systems trained on these corpora 
when they are applied to abstracts dealing with 
other species is unknown. In machine-learning-
based systems, re-training the model with addition 
of corpora in the target domain has achieved prom-
ising results (e.g. Tsuruoka et al2005, Lease et al
2005). In this paper, we compare two methods for 
adaptation of POS taggers trained for GENIA and 
Penn BioIE corpora to Drosophila melanogaster 
(fruit fly) domain. 
2 Method 
Maximum Entropy Markov Models (MEMMs) 
(Ratnaparkhi 1996) and their extensions (Tutanova 
et al2003, Tsuruoka et al2005) have been success-
fully applied to English POS tagging. Here we use 
second-order standard MEMMs for learning POS. 
where the model parameters are determined with 
maximum entropy criterion in combination a regu-
larization method called inequality constraints 
(Kazama and Tsujii 2003). This regularization 
method has one non-negative meta-parameter 
called width-factor that controls the ?fitness? of the 
model parameters to the training data.
We used two methods of adapting a POS tagging 
model. One is to add the domain corpus to the 
training set. The other is to use the reference distri-
bution modeling, in which the training is per-
                                                          
    This work is partially supported by SORST program, Japan 
Science and Technology Agency. 
formed only on the domain corpus and the infor-
mation about the original training set is incorpo-
rated in the form of the reference distribution in 
the maximum entropy formulation (Johnson et al
2000, Hara et al2005). 
A set of 200 MEDLINE abstracts on D. 
melanogaster, was manually annotated with POS 
according to the scheme of the GENIA POS corpus 
(Tateisi et al2004) by one annotator. The new cor-
pus consists of 40,200 tokens in 1676 sentences. 
From this corpus which we call ?Fly? hereafter, 
1024 sentences are randomly taken and used for 
training. Half of the remaining is used for devel-
opment and the rest is used for testing.  
We measured the accuracy of the POS tagger 
trained in three settings:  
Original: The tagger is trained with the union of 
Wall Street Journal (WSJ) section of Penn 
Treebank (Marcus et al1993), GENIA, and 
Penn BioIE. In WSJ, Sections 0-18 for train-
ing, 19-21 for development, and 22-24 for 
test. In GENIA and Penn BioIE, 90% of the 
corpus is used for training and the rest is 
used for test. 
Combined: The tagger is trained with the union 
of the Original set plus N sentences from Fly.  
Refdist: Tagger is trained with N sentences 
from Fly, plus the Original set as reference. 
In Combined and Refdist settings, N is set to 8, 16, 
32, 64, 128, 256, 512, 1024 sentences to measure 
the learning curve. 
3 Results 
The accuracies of the tagger trained in the Origi-
nal setting were 96.4% on Fly, 96.7% on WSJ, 
Yuka Tateisi Yoshimasa Tsuruoka Jun-ichi Tsujii
Faculty of Informatics 
 Kogakuin University 
 Nishishinjuku 1-24-2 
Shinjuku-ku, Tokyo, 163-
8677, Japan 
School of Informatics 
 University of Manchester 
 Manchester M60 1QD, U.K. 
Dept. of Computer Science 
University of Tokyo 
Hongo 7-3-1, Bunkyo-ku, 
Tokyo 113-0033, Japan 
School of Informatics 
 University of Manchester 
 Manchester M60 1QD, U.K.
136
98.1% on GENIA and 97.7% on Penn BioIE cor-
pora respectively. In the Combined setting, the ac-
curacies were 97.9% on Fly, 96.7% on WSJ, 
98.1% on GENIA and 97.7% on Penn BioIE. With 
Refdist setting, the accuracy on the Fly corpus was 
raised but those for WSJ and Penn BioIE corpora 
dropped from Original. When the width factor w 
was 10, the accuracy was 98.1% on Fly, but 95.4% 
on WSJ, 98.3% on GENIA and 96.6% on Penn 
BioIE. When the tagger was trained only on WSJ 
the accuracies were 88.7% on Fly, 96.9% on WSJ, 
85.0% on GENIA and 86.0% on Penn BioIE. 
When the tagger was trained only on Fly, the accu-
racy on Fly was even lower (93.1%). The learning 
curve indicated that the accuracies on the Fly cor-
pus were still rising in both Combined and Refdist 
settings, but both accuracies are almost as high as 
those of the original tagger on the original corpora 
(WSJ, GENIA and Penn BioIE), so in practical 
sense, 1024 sentences is a reasonable size for the 
additional corpus. When the width factor was 
smaller (2.5 and 5) the accuracies on the Fly cor-
pus were saturated with N=1024 with lower values 
(97.8% with w=2.5 and 98.0% with w=5).  
The amount of resources required for the Com-
bined and the Refdist settings were drastically dif-
ferent. In the Combined setting, the learning time 
was 30632 seconds and the required memory size 
was 6.4GB. On the other hand, learning in the Ref-
dist setting took only 21 seconds and the required 
memory size was 157 MB. 
The most frequent confusions involved the con-
fusion between FW (foreign words) with another 
class. Further investigation revealed that most of 
the error involved Linnaean names of species. Lin-
naean names are tagged differently in the GENIA 
and Penn BioIE corpora. In the GENIA corpus, 
tokens that constitute a Linnaean name are tagged 
as FW (foreign word) but in the Penn BioIE corpus 
they are tagged as NNP (proper noun). This seems 
to be one of the causes of the drop of accuracy on 
the Penn BioIE corpus when more sentences from 
the Fly corpus, whose tagging scheme follows that 
of GENIA, are added for training.
4 Conclusions 
We compared two methods of adapting a POS tag-
ger trained on corpora in human domain to fly do-
main. Training in Refdist setting required much 
smaller resources to fit to the target domain, but 
the resulting tagger is less portable to other do-
mains. On the other hand, training in Combined 
setting is slower and requires huge memory, but 
the resulting tagger is more robust, and fits rea-
sonably to various domains. 
References 
Tadayoshi Hara, Yusuke Miyao and Jun'ichi Tsujii. 
2005. Adapting a probabilistic disambiguation model 
of an HPSG parser to a new domain. In Proceedings 
of  IJCNLP 2005, LNAI 3651, pp. 199-210. 
Mark Johnson and Stefan Riezler. 2000.  Exploiting 
auxiliary distributions in stochastic unification-based 
grammars. In Proceedings of 1st NAACL.  
Jun?ichi Kazama and Jun?ichi Tsujii. 2003. Evaluation 
and extension of maximum entropy models with ine-
quality constraints. In Proceedings of EMNLP 2003. 
Jin-Dong Kim, Tomoko Ohta, Yuka Tateisi, and 
Jun?ichi Tsujii. 2003. GENIA corpus ? a semanti-
cally annotated corpus for bio-textmining. Bioinfor-
matics, 19(Suppl. 1):i180?i182. 
Seth Kulick, Ann Bies, Mark Liberman, Mark Mandel, 
Ryan McDonald, Martha Palmer, Andrew Schein, 
Lyle Ungar, Scott Winters, and Pete White. 2004. In-
tegrated annotation for biomedical information ex-
traction. In Proceedings of BioLINK 2004, pp. 61?68. 
Matthew Lease and Eugene Charniak. 2005. Parsing 
Biomedical Literature, In Proceedings of  IJCNLP 
2005, LNAI 3651, pp. 58-69. 
Mitchell P. Marcus, Beatrice Sanorini and Mary Ann 
Marcinkiewicz. 1993. Building a large annotated 
corpus of English: the Penn Treebank.  Computa-
tional Linguistics, Vol.19, pp. 313-330.  
Adwait Ratnaparkhi. 1996. A Maximum Entropy Model 
for Part-Of-Speech Tagging. In Proceedings of 
EMNLP 1996. 
Yuka Tateisi and Jun'ichi Tsujii. (2004). Part-of-Speech 
Annotation of Biology Research Abstracts. In the 
Proceedings of LREC2004, vol. IV, pp. 1267-1270. 
Kristina Toutanova,  Dan Klein, Christopher Manning 
and Yoram Singer. 2003. Feature-Rich Part-of-
Speech Tagging with a Cyclic Dependency Network. 
In Proceedings of HLT-NAACL 2003, pp. 173-180. 
Yoshimasa Tsuruoka, Yuka Tateishi, Jin-Dong Kim, 
Tomoko Ohta, John McNaught, Sophia Ananiadou, 
and Jun'ichi Tsujii. 2005. Developing a Robust Part-
of-Speech Tagger for Biomedical Text. In Proceed-
ings of 10th Panhellenic Conference on Informatics, 
LNCS 3746, pp. 382-392.  
137
Proceedings of the Linguistic Annotation Workshop, pages 33?40,
Prague, June 2007. c?2007 Association for Computational Linguistics
An Annotation Type System for a Data-Driven NLP Pipeline
Udo Hahn Ekaterina Buyko Katrin Tomanek
Jena University Language & Information Engineering (JULIE) Lab
Fu?rstengraben 30, 07743 Jena, Germany
{hahn|buyko|tomanek}@coling-uni-jena.de
Scott Piao John McNaught Yoshimasa Tsuruoka Sophia Ananiadou
NaCTeM and School of Computer Science
University of Manchester
{scott.piao|john.mcnaught|yoshimasa.tsuruoka|sophia.ananiadou}@manchester.ac.uk
Abstract
We introduce an annotation type system for
a data-driven NLP core system. The specifi-
cations cover formal document structure and
document meta information, as well as the
linguistic levels of morphology, syntax and
semantics. The type system is embedded in
the framework of the Unstructured Informa-
tion Management Architecture (UIMA).
1 Introduction
With the maturation of language technology, soft-
ware engineering issues such as re-usability, in-
teroperability, or portability are getting more and
more attention. As dozens of stand-alone compo-
nents such as tokenizers, stemmers, lemmatizers,
chunkers, parsers, etc. are made accessible in vari-
ous NLP software libraries and repositories the idea
sounds intriguing to (re-)use them on an ?as is? basis
and thus save expenditure and manpower when one
configures a composite NLP pipeline.
As a consequence, two questions arise. First, how
can we abstract away from the specific code level of
those single modules which serve, by and large, the
same functionality? Second, how can we build NLP
systems by composing them, at the abstract level
of functional specification, from these already ex-
isting component building blocks disregarding con-
crete implementation matters? Yet another burning
issue relates to the increasing availability of multiple
metadata annotations both in corpora and language
processors. If alternative annotation tag sets are cho-
sen for the same functional task a ?data conversion?
problem is created which should be solved at the ab-
stract specification level as well (Ide et al, 2003).
Software engineering methodology points out that
these requirements are best met by properly identi-
fying input/output capabilities of constituent compo-
nents and by specifying a general data model (e.g.,
based on UML (Rumbaugh et al, 1999)) in or-
der to get rid of the low-level implementation (i.e.,
coding) layer. A particularly promising proposal
along this line of thought is the Unstructured Infor-
mation Management Architecture (UIMA) (Ferrucci
and Lally, 2004) originating from IBM research ac-
tivities.1 UIMA is but the latest attempt in a series
of proposals concerned with more generic NLP en-
gines such as ATLAS (Laprun et al, 2002) or GATE
(Cunningham, 2002). These frameworks have in
common a data-driven architecture and a data model
based on annotation graphs as an adaptation of the
TIPSTER architecture (Grishman, 1997). They suf-
fer, however, from a lack of standards for data ex-
change and abstraction mechanisms at the level of
specification languages.
This can be achieved by the definition of a com-
mon annotation scheme. We propose an UIMA
schema which accounts for a significant part of the
complete NLP cycle ? from the collection of doc-
uments and their internal formal structure, via sen-
tence splitting, tokenization, POS tagging, and pars-
ing, up until the semantic layer (still excluding dis-
course) ? and which aims at the implementation-
independent specification of a core NLP system.
1Though designed for any sort of unstructured data (text,
audio and video data), we here focus on special requirements
for the analysis of written documents.
33
2 Related work
Efforts towards the design of annotation schemata
for language resources and their standardization
have a long-standing tradition in the NLP commu-
nity. In the very beginning, this work often fo-
cused exclusively on subdomains of text analysis
such as document structure meta-information, syn-
tactic or semantic analysis. The Text Encoding Ini-
tiative (TEI)2 provided schemata for the exchange
of documents of various genres. The Dublin Core
Metadata Initiative3 established a de facto standard
for the Semantic Web.4 For (computational) lin-
guistics proper, syntactic annotation schemes, such
as the one from the Penn Treebank (Marcus et al,
1993), or semantic annotations, such as the one un-
derlying ACE (Doddington et al, 2004), are increas-
ingly being used in a quasi standard way.
In recent years, however, the NLP community is
trying to combine and merge different kinds of an-
notations for single linguistic layers. XML formats
play a central role here. An XML-based encod-
ing standard for linguistic corpora XCES (Ide et al,
2000) is based on CES (Corpus Encoding Standard)
as part of the EAGLES Guidelines.5 Work on TIGER
(Brants and Hansen, 2002) is an example for the li-
aison of dependency- and constituent-based syntac-
tic annotations. New standardization efforts such as
the Syntactic Annotation Framework (SYNAF) (De-
clerck, 2006) aim to combine different proposals and
create standards for syntactic annotation.
We also encounter a tendency towards multiple
annotations for a single corpus. Major bio-medical
corpora, such as GENIA (Ohta et al, 2002) or
PennBioIE,6 combine several layers of linguistic
information in terms of morpho-syntactic, syntac-
tic and semantic annotations (named entities and
events). In the meantime, the Annotation Compat-
ibility Working Group (Meyers, 2006) began to con-
centrate its activities on the mutual compatibility of
annotation schemata for, e.g., POS tagging, tree-
banking, role labeling, time annotation, etc.
The goal of these initiatives, however, has never
been to design an annotation scheme for a complete
2http://www.tei-c.org
3http://dublincore.org
4http://www.w3.org/2001/sw
5http://www.ilc.cnr.it/EAGLES96/
6http://bioie.ldc.upenn.edu
NLP pipeline as needed, e.g., for information ex-
traction or text mining tasks (Hahn and Wermter,
2006). This lack is mainly due to missing standards
for specifying comprehensive NLP software archi-
tectures. The MEANING format (Pianta et al, 2006)
is designed to integrate different levels of morpho-
syntactic annotations. The HEART OF GOLD mid-
dleware (Scha?fer, 2006) combines multidimensional
mark-up produced by several NLP components. An
XML-based NLP tool suite for analyzing and anno-
tating medical language in an NLP pipeline was also
proposed by (Grover et al, 2002). All these propos-
als share their explicit linkage to a specific NLP tool
suite or NLP system and thus lack a generic annota-
tion framework that can be re-used in other develop-
mental environments.
Buitelaar et al developed in the context of an in-
formation extraction project an XML-based multi-
layered annotation scheme that covers morpho-
syntactic, shallow parsing and semantic annotation
(Buitelaar et al, 2003). Their scheme borrows con-
cepts from object-oriented programming (e.g., ab-
stract types, polymorphism). The object-oriented
perspective already allows the development of a
domain-independent schema and extensions of core
types without affecting the base schema. This
schema is comprehensive indeed and covers a sig-
nificant part of advanced NLP pipelines but it is also
not connected to a generic framework.
It is our intention to come full circle within a
general annotation framework. Accordingly, we
cover a significant part of the NLP pipeline from
document meta information and formal document
structure, morpho-syntactic and syntactic analysis
up to semantic processing. The scheme we propose
is intended to be compatible with on-going work
in standardization efforts from task-specific annota-
tions and to adhere to object-oriented principles.
3 Data-Driven NLP Architecture
As the framework for our specification efforts, we
adopted the Unstructured Information Management
Architecture (UIMA) (Ferrucci and Lally, 2004). It
provides a formal specification layer based on UML,
as well as a run-time environment for the interpreta-
tion and use of these specifications. This dualism is
going to attract more and more researchers as a basis
34
for proper NLP system engineering.
3.1 UIMA-based Tool Suite
UIMA provides a platfrom for the integration
of NLP components (ANALYSIS ENGINES in the
UIMA jargon) and the deployment of complex
NLP pipelines. It is more powerful than other
prominent software systems for language engineer-
ing (e.g., GATE, ATLAS) as far as its pre- and
post-processing facilities are concerned ? so-called
COLLECTION READERS can be developed to handle
any kind of input format (e.g., WWW documents,
conference proceedings), while CONSUMERS, on
other hand, deal with the subsequent manipulation
of the NLP core results (e.g., automatic indexing).
Therefore, UIMA is a particularly suitable architec-
ture for advanced text analysis applications such as
text mining or information extraction.
We currently provide ANALYSIS ENGINES for
sentence splitting, tokenization, POS tagging, shal-
low and full parsing, acronym detection, named
entity recognition, and mapping from named enti-
ties to database term identifiers (the latter is mo-
tivated by our biological application context). As
we mainly deal with documents taken from the bio-
medical domain, our collection readers process doc-
uments from PUBMED,7 the most important liter-
ature resource for researchers in the life sciences.
PUBMED currently provides more than 16 million
bibliographic references to bio-medical articles. The
outcomes of ANALYSIS ENGINES are input for var-
ious CONSUMERS such as semantic search engines
or text mining tools.
3.2 Common Analysis System
UIMA is based on a data-driven architecture. This
means that UIMA components do not exchange or
share code, they rather exchange data only. The
components operate on common data referred to
as COMMON ANALYSIS SYSTEM (CAS)(Go?tz and
Suhre, 2004). The CAS contains the subject of anal-
ysis (document) and provides meta data in the form
of annotations. Analysis engines receive annotations
through a CAS and add new annotations to the CAS.
An annotation in the CAS then associates meta data
with a region the subject of the analysis occupies
7http://www.pubmed.gov
(e.g., the start and end positions in a document).
UIMA defines CAS interfaces for indexing, ac-
cessing and updating the CAS. CASes are modelled
independently from particular programming lan-
guages. However, JCAS, an object-oriented inter-
face to the CAS, was developed for JAVA. CASes are
crucial for the development and deployment of com-
plex NLP pipelines. All components to be integrated
in UIMA are characterized by abstract input/output
specifications, so-called capabilities. These speci-
fications are declared in terms of descriptors. The
components can be integrated by wrappers conform-
ing with the descriptors. For the integration task, we
define in advance what kind of data each component
may manipulate. This is achieved via the UIMA
annotation type system. This type system follows
the object-oriented paradigm. There are only two
kinds of data, viz. types and features. Features spec-
ify slots within a type, which either have primitive
values such as integers or strings, or have references
to instances of types in the CAS. Types, often called
feature structures, are arranged in an inheritance hi-
erarchy.
In the following section, we propose an ANNO-
TATION TYPE SYSTEM designed and implemented
for an UIMA Tool Suite that will become the back-
bone for our text mining applications. We distin-
guish between the design and implementation lev-
els, talking about the ANNOTATION SCHEME and
the TYPE SYSTEM, respectively.
4 Annotation Type System
The ANNOTATION SCHEME we propose currently
consists of five layers: Document Meta, Document
Structure & Style, Morpho-Syntax, Syntax and Se-
mantics. Accordingly, annotation types fall into five
corresponding categories. Document Meta and Doc-
ument Structure & Style contain annotations about
each document?s bibliography, organisation and lay-
out. Morpho-Syntax and Syntax describe the results
of morpho-syntactic and syntactic analysis of texts.
The results of lemmatisation, stemming and decom-
position of words can be represented at this layer, as
well. The annotations from shallow and full parsing
are represented at the Syntax layer. The appropri-
ate types permit the representation of dependency-
and constituency-based parsing results. Semantics
35
uima.tcas.Annotation
+begin: uima.cas.Integer
+end: uima.cas.Integer
Annotation
+componentId: uima.cas.String
+confidence: uima.cas.Double
Descriptor
pubmed.ManualDescriptor
+MeSHList: uma.cas.FSArray = MeSHHeading
+...
AutoDescriptor
+...
Header
+docType: uima.cas.String
+source: uima.cas.String
+docID: uima.cas.String
+language: uima.cas.String
+copyright: uima.cas.String
+authors: uima.cas.FSArray = AuthorInfo
+title: uima.cas.String
+pubTypeList: uima.cas.FSArray = PubType
+...
pubmed.Header
+citationStatus: uima.cas.String {...}
ManualDescriptor
+keywordList: uima.cas.FSArray = Keyword
+...
PubType
+name: uima.cas.Sting
Journal
+ISSN: uima.cas.String
+volume: uima.cas.String
+journalTitle: uima.cas.String
+impactFactor: uima.cas.String
Keyword
+name: uima.cas.String
+source: uima.cas.String
Token
+posTag: uima.cas.FSArray  = POSTag
+lemma: Lemma
+feats: GrammaticalFeats
+stemmedForm: StemmedForm
+depRelList: uima.cas.FSArray = DependencyRelation
+orthogr: uima.cas.FSArray = String
POSTag
+tagsetId: uima.cas.String
+language: uima.cas.String
+value: uima.cas.String
Lemma
+value: String
Acronym
Abbreviation
+expan: String
StemmedForm
+value: String
GrammaticalFeats
+language: uima.cas.String
DiscontinuousAnnotation
+value: FSArray = Annotation
PennPOSTag
NounFeats
+...
...
+...
...
+...
DependencyRelation
+head: Token
+projective: uima.cas.Boolean
+label: uima.cas.String
Relation
DepRelationSet...
Chunk
PhraseChunk
PTBConstituent
+formFuncDisc: uima.cas.String
+gramRole: uima.cas.String
+adv: uima.cas.String
+misc: uima.cas.String
+map: Constituent
+tpc: uima.cas.Boolean
+nullElement: uima.cas.String
+ref: Constituent
Constituent
+parent: Constituent
+head: Token
+cat: uima.cas.String
GENIAConstituent
+syn: uima.cas.String 
...
+...
...
+...
NP ...PP
Entity
+dbEntry: uima.cas.FSArray = DBEntry
+ontologyEntry: uima.cas.FSArray = OntologyEntry
+specificType: uima.cas.String
BioEntity
Cytokine
Organism VariationGene
...
LexiconEntry OntologyEntryDBEntry
ResourceEntry
+source: uima.cas.String
+entryId: uima.cas.String
+version: uima.cas.String
Zone 
Title TextBody Paragraph Figure
+caption: Caption
Section
+title: Title
+depth: uima.cas.Integer
Misc ... PersonOrganization
MUCEntity
...
2
3
4
5 6
1
CAS Core
Figure 1: Multi-Layered UIMA Annotation Scheme in UML Representation. 1: Basic Feature Structure and
Resource Linking. 2: Document Meta Information. 3: Morpho-Syntax. 4: Syntax. 5: Document Structure
& Style. 6: Semantics.
36
currently covers information about named entities,
events and relations between named entities.
4.1 Basic Feature Structure
All types referring to different linguistic lay-
ers derive from the basic type Annotation,
the root type in the scheme (cf. Figure 1-
1). The Annotation type itself derives infor-
mation from the default UIMA annotation type
uima.tcas.Annotation and, thus, inherits the
basic annotation features, viz. begin and end (mark-
ing spans of annotations in the subject of analysis).
Annotation extends this default feature structure
with additional features. The componentId marks
which NLP component actually computed this an-
notation. This attribute allows to manage multiple
annotations of the same type The unique linkage be-
tween an analysis component and an annotation item
is particularly relevant in cases of parallel annota-
tions. The component from which the annotation
originated also assigns a specific confidence score
to its confidence feature. Each type in the scheme is
at least supplied with these four slots inherited from
their common root type.
4.2 Document Meta Information
The Document Meta layer (cf. Figure 1-2) describes
the bibliographical and content information of a doc-
ument. The bibliographical information, often re-
trieved from the header of the analyzed document,
is represented in the type Header. The source
and docID attributes yield a unique identifier for
each document. We then adopted some Dublin Core
elements, e.g., language, title, docType. We dis-
tinguish between domain-independent information
such as language, title, document type and domain-
dependent information as relevant for text mining
in the bio-medical domain. Accordingly, the type
pubmed.Header was especially created for the
representation of PUBMED document information.
A more detailed description of the document?s pub-
lication data is available from types which specialize
PubType such as Journal. The latter contains
standard journal-specific attributes, e.g., ISSN, vol-
ume, journalTitle.
The description of the document?s content of-
ten comes with a list of keywords, informa-
tion assigned to the Descriptor type. We
clearly distinguish between content descriptors man-
ually provided by an author, indexer or cura-
tor, and items automatically generated by text
analysis components after document processing.
While the first kind of information will be stored
in the ManualDescriptor, the second one
will be represented in the AutoDescriptor.
The generation of domain-dependent descriptors is
also possible; currently the scheme contains the
pubmed.ManualDescriptor which allows to
assign attributes such as chemicals and genes.
4.3 Document Structure & Style
The Document Structure & Style layer (cf. Figure 1-
5) contains information about the organization and
layout of the analyzed documents. This layer en-
ables the marking-up of document structures such
as paragraphs, rhetorical zones, figures and tables,
as well as typographical information, such as italics
and special fonts. The focus of modeling this layer is
on the annotation of scientific documents, especially
in the life sciences. We adopted here the SCIXML8
annotation schema, which was especially developed
for marking-up scientific publications. The Zone
type refers to a distinct division of text and is the par-
ent type for various subtypes such as TextBody,
Title etc. While it seems impossible to predict all
of the potential formal text segments, we first looked
at types of text zones frequently occurring in sci-
entific documents. The type Section, e.g., repre-
sents a straightforward and fairly standard division
of scientific texts into introduction, methods and re-
sults sections. The divisions not covered by current
types can be annotated with Misc. The annotation
of tables and figures with corresponding types en-
ables to link text and additional non-textual infor-
mation, an issue which is gaining more and more
attention in the text mining field.
4.4 Morpho-Syntax
The Morpho-Syntax layer (cf. Figure 1-3) represents
the results of morpho-syntactic analysis such as to-
kenization, stemming, POS tagging. The small-
est annotation unit is Token which consists of five
attributes, including its part-of-speech information
8http://www.cl.cam.ac.uk/?aac10/
escience/sciborg.html
37
(posTag), stemmedForm, lemma, grammatical fea-
tures (feats), and orthographical information (or-
thogr).
With respect to already available POS tagsets,
the scheme allows corresponding extensions of
the supertype POSTag to, e.g., PennPOSTag
(for the Penn Tag Set (Marcus et al, 1993)) or
GeniaPOSTag (for the GENIA Tag Set (Ohta et
al., 2002)). The attribute tagsetId serves as a unique
identifier of the corresponding tagset. The value of
the POS tag (e.g., NN, VVD, CC) can be stored in
the attribute value. The potential values for the in-
stantiation of this attribute are always restricted to
the tags of the associated tagset. These constraints
enforce formal control on annotation processes.
As for morphologically normalized lexical items,
the Lemma type stores the canonical form of a lexi-
cal token which can be retrieved from a lexicon once
it is computed by a lemmatizer. The lemma value,
e.g., for the verb ?activates? would be ?activate?. The
StemmedForm represents a base form of a text to-
ken as produced by stemmers (e.g., ?activat-? for the
noun ?activation?).
Due to their excessive use in life science docu-
ments, abbreviations, acronyms and their expanded
forms have to be considered in terms of appropriate
types, as well. Accordingly, Abbreviation and
Acronym are defined, the latter one being a child
type of the first one. The expanded form of a short
one can easily be accessed from the attribute expan.
Grammatical features of tokens are represented
in those types which specialize the supertype
GrammaticalFeats. Its child types, viz.
NounFeats, VerbFeats, AdjectiveFeats,
PronounFeats (omitted from Figure 1-3) cover
the most important word categories. Attributes
of these types obviously reflect the properties
of particular grammatical categories. While
NounFeats comes with gender, case and num-
ber only, PronounFeats must be enhanced with
person. A more complex feature structure is asso-
ciated with VerbFeats which requires attributes
such as tense, person, number, voice and aspect. We
adapted here specifications from the TEI to allow
compatibility with other annotation schemata.
The type LexiconEntry (cf. Figure 1-1) en-
ables a link to the lexicon of choice. By designing
this type we achieve much needed flexibility in link-
ing text snaps (e.g., tokens, simplex forms, multi-
word terms) to external resources. The attributes
entryId and source yield, in combination, a unique
identifier of the current lexicon entry. Resource ver-
sion control is enabled through an attribute version.
Text annotations often mark disrupted text spans,
so-called discontinuous annotations. In coordinated
structures such as ?T and B cell?, the annotator
should mark two named entities, viz. ?T cell? and ?B
cell?, where the first one results from the combina-
tion of the disjoint parts ?T? and ?cell?. In order to
represent such discontinous annotations, we intro-
duced the type DiscontinuousAnnotation
(cf. Figure 1-1) which links through its attribute
value spans of annotations to an annotation unit.
4.5 Syntax
This layer of the scheme provides the types and at-
tributes for the representation of syntactic structures
of sentences (cf. Figure 1-4). The results from shal-
low and full parsing can be stored here.
Shallow parsing (chunking) aims at dividing
the flow of text into phrases (chunks) in a non-
overlapping and non-recursive manner. The type
Chunk accounts for different chunk tag sets by sub-
typing. Currently, the scheme supports Phrase-
Chunks with subtypes such as NP, VP, PP, or ADJP
(Marcus et al, 1993).
The scheme also reflects the most popular full
parsing approaches in NLP, viz. constituent-based
and dependency-based approaches. The results
from constituent-based parsing are represented in
a parse tree and can be stored as single nodes in
the Constituent type. The tree structure can
be reconstructed through links in the attribute par-
ent which stores the id of the parent constituent.
Besides the attribute parent, Constituent holds
the attributes cat which stores the complex syntac-
tic category of the current constituent (e.g., NP, VP),
and head which links to the head word of the con-
stituent. In order to account for multiple annota-
tions in the constituent-based approach, we intro-
duced corresponding constituent types which spe-
cialize Constituent. This parallels our approach
which we advocate for alternatives in POS tagging
and the management of alternative chunking results.
Currently, the scheme supports three differ-
ent constituent types, viz. PTBConstituent,
38
GENIAConstituent (Miyao and Tsujii, 2005)
and PennBIoIEConstituent. The attributes
of the type PTBConstituent cover the com-
plete repertoire of annotation items contained in
the Penn Treebank, such as functional tags for
form/function dicrepancies (formFuncDisc), gram-
matical role (gramRole), adverbials (adv) and mis-
cellaneous tags (misc). The representation of null
elements, topicalized elements and gaps with corre-
sponding references to the lexicalized elements in a
tree is reflected in attributes nullElement, tpc, map
and ref, respectively. GENIAConstituent and
PennBIoIEConstituent inherit from PTB-
Constituent all listed attributes and provide, in
the case of GENIAConstituent , an additional
attribute syn to specify the syntactic idiosyncrasy
(coordination) of constituents.
Dependency parsing results are directly linked to
the token level and are thus referenced in the Token
type. The DependencyRelation type inherits
from the general Relation type and introduces
additional features which are necessary for describ-
ing a syntactic dependency. The attribute label char-
acterizes the type of the analyzed dependency rela-
tion. The attribute head indicates the head of the
dependency relation attributed to the analyzed to-
ken. The attribute projective relates to the property
of the dependency relation whether it is projective
or not. As different dependency relation sets can be
used for parsing, we propose subtyping similar to
the constituency-based parsing approaches. In order
to account for alternative dependency relation sets,
we aggregate all possible annotations in the Token
type as a list (depRelList).
4.6 Semantics
The Semantics layer comprises currently the repre-
sentation of named entities, particularly for the bio-
medical domain. The entity types are hierarchically
organized. The supertype Entity (cf. Figure 1-
6) links annotated (named) entities to the ontologies
and databases through appropriate attributes, viz. on-
tologyEntry and sdbEntry. The attribute specific-
Type specifies the analyzed entity in a more detailed
way (e.g., Organism can be specified through
the species values ?human?, ?mouse?, ?rat?, etc.)
The subtypes are currently being developed in the
bio-medical domain and cover, e.g., genes, pro-
teins, organisms, diseases, variations. This hierar-
chy can easily be extended or supplemented with
entities from other domains. For illustration pur-
poses, we extended it here by MUC (Grishman
and Sundheim, 1996) entity types such as Person,
Organization, etc.
This scheme is still under construction and will
soon also incorporate the representation of relation-
ships between entities and domain-specific events.
The general type Relation will then be extended
with specific conceptual relations such as location,
part-of, etc. The representation of events will be
covered by a type which aggregates pre-defined re-
lations between entities and the event mention. An
event type such as InhibitionEventwould link
the text spans in the sentence ?protein A inhibits
protein B? in attributes agent (?protein A?), patient
(?protein B?), mention (?inhibits?).
5 Conclusion and Future work
In this paper, we introduced an UIMA annotation
type system which covers the core functionality
of morphological, syntactic and semantic analysis
components of a generic NLP system. It also in-
cludes type specifications which relate to the formal
document format and document style. Hence, the
design of this scheme allows the annotation of the
entire cycle of (sentence-level) NLP analysis (dis-
course phenomena still have to be covered).
The annotation scheme consists mostly of core
types which are designed in a domain-independent
way. Nevertheless, it can easily be extended with
types which fit other needs. The current scheme sup-
plies an extension for the bio-medical domain at the
document meta and structure level, as well as on the
semantic level. The morpho-syntactic and syntactic
levels provide types needed for the analysis of the
English language. Changes of attributes or attribute
value sets will lead to adaptations to other natural
languages.
We implemented the scheme as an UIMA type
system. The formal specifications are implemented
using the UIMA run-time environment. This direct
link of formal and implementational issues is a ma-
jor asset using UIMA unmatched by any previous
specification approach. Furthermore, all annotation
results can be converted to the XMI format within
39
the UIMA framework. XMI, the XML Metadata In-
terchange format, is an OMG9 standard for the XML
representation of object graphs.
The scheme also eases the representation of an-
notation results for the same task with alternative
and often competitive components. The identifica-
tion of the component which provided specific an-
notations can be retrieved from the attribute com-
ponentId. Furthermore, the annotation with alterna-
tive and multiple tag sets is supported as well. We
have designed for each tag set a type representing
the corresponding annotation parameters. The inher-
itance trees at almost all annotation layers support
the parallelism in annotation process (e.g., tagging
may proceed with different POS tagsets).
The user of the scheme can restrict the potential
values of the types or attributes. The current scheme
makes use of the customization capability for POS
tagsets, for all attributes of constituents and chunks.
This yields additional flexibility in the design and,
once specified, an increased potential for automatic
control for annotations.
The scheme also enables a straightforward con-
nection to external resources such as ontologies,
lexicons, and databases as evidenced by the corre-
sponding subtypes of ResourceEntry (cf. Figure
1-1). These types support the specification of a re-
lation between a concrete text span and the unique
item addressed in any of these resources.
With these considerations in mind, we strive for
the elaboration of a common standard UIMA type
system for NLP engines. The advantages of such a
standard include an easy exchange and integration
of different NLP analysis engines, the facilitation
of sophisticated evaluation studies (where, e.g., al-
ternative components for NLP tasks can be plugged
in and out at the spec level), and the reusability of
single NLP components developed in various labs.
Acknowledgments. This research was funded by the EC?s 6th Framework Programme
(4th call) within the BOOTStrep project under grant FP6-028099.
References
S. Brants and S. Hansen. 2002. Developments in the TIGER
annotation scheme and their realization in the corpus. In
Proc. of the 3rd LREC Conference, pages 1643?1649.
P. Buitelaar, T. Declerck, B. Sacaleanu, ?S. Vintar, D. Raileanu,
and C. Crispi. 2003. A multi-layered, XML-based approach
9http://www.omg.org
to the integration of linguistic and semantic annotations. In
Proc. of EACL 2003 Workshop NLPXML-03.
H. Cunningham. 2002. GATE, a general architecture for text
engineering. Computers and the Humanities, 36:223?254.
T. Declerck. 2006. SYNAF: Towards a standard for syntactic
annotation. In Proc. of the 5th LREC Conference.
G. Doddington, A. Mitchell, M. Przybocki, L. Ramshaw,
S. Strassel, and R. Weischedel. 2004. The Automatic Con-
tent Extraction (ACE) Program. In Proc. of the 4th LREC
Conference, pages 837?840.
D. Ferrucci and A. Lally. 2004. UIMA: an architectural ap-
proach to unstructured information processing in the corpo-
rate research environment. Natural Language Engineering,
10(3-4):327?348.
T. Go?tz and O. Suhre. 2004. Design and implementation of the
UIMA Common Analysis System. IBM Systems Journal,
43(3):476?489.
R. Grishman and B. Sundheim. 1996. Message Understand-
ing Conference ? 6: A brief history. In Proc. of the 16th
COLING, pages 466?471.
R. Grishman. 1997. Tipster architecture design document,
version 2.3. Technical report, Defense Advanced Research
Projects Agency (DARPA), U.S. Departement of Defense.
C. Grover, E. Klein, M. Lapata, and A. Lascarides. 2002.
XML-based NLP tools for analysing and annotating medi-
cal language. In Proc. of the 2nd Workshop NLPXML-2002,
pages 1?8.
U. Hahn and J. Wermter. 2006. Levels of natural language pro-
cessing for text mining. In S. Ananiadou and J. McNaught,
editors, Text Mining for Biology and Biomedicine, pages 13?
41. Artech House.
N. Ide, P. Bonhomme, and L. Romary. 2000. XCES: An XML-
based standard for linguistic corpora. In Proc. of the 2nd
LREC Conference, pages 825?830.
N. Ide, L. Romary, and E. de la Clergerie. 2003. International
standard for a linguistic annotation framework. In Proc. of
the HLT-NAACL 2003 SEALTS Workshop, pages 25?30.
C. Laprun, J. Fiscus, J. Garofolo, and S. Pajot. 2002. A prac-
tical introduction to ATLAS. In Proc. of the 3rd LREC Con-
ference, pages 1928?1932.
M. Marcus, B. Santorini, and M.A. Marcinkiewicz. 1993.
Building a large annotated corpus of English: The PENN
TREEBANK. Computational Linguistics, 19(2):313?330.
A. Meyers. 2006. Annotation compatibility working group re-
port. In Proc. of the COLING-ACL 2006 Workshop FLAC
2006?, pages 38?53.
Y. Miyao and J. Tsujii. 2005. Probabilistic disambiguation
models for wide-coverage HPSG parsing. In Proc. of the
ACL 2005, pages 83 ? 90.
T. Ohta, Y. Tateisi, and J.-D. Kim. 2002. The GENIA corpus:
An annotated research abstract corpus in molecular biology
domain. In Proc. of the 2nd HLT, pages 82?86.
E. Pianta, L. Bentivogli, C. Girardi, and B. Magnini. 2006.
Representing and accessing multilevel linguistic annotation
using the MEANING format. In Proc. of the 5th EACL-2006
Workshop NLPXML-2006, pages 77?80.
J. Rumbaugh, I. Jacobson, and G. Booch. 1999. The Unified
Modeling Language Reference Manual. Addison-Wesley.
U. Scha?fer. 2006. Middleware for creating and combining
multi-dimensional NLP markup. In Proc. of the 5th EACL-
2006 Workshop NLPXML-2006, pages 81?84.
40
BioNLP 2008: Current Trends in Biomedical Natural Language Processing, pages 30?37,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Accelerating the Annotation of Sparse Named Entities
by Dynamic Sentence Selection
Yoshimasa Tsuruoka1, Jun?ichi Tsujii1,2,3 and Sophia Ananiadou1,3
1 School of Computer Science, The University of Manchester, UK
2 Department of Computer Science, The University of Tokyo, Japan
3 National Centre for Text Mining (NaCTeM), Manchester, UK
yoshimasa.tsuruoka@manchester.ac.uk
tsujii@is.s.u-tokyo.ac.jp
sophia.ananiadou@manchester.ac.uk
Abstract
This paper presents an active learning-like
framework for reducing the human effort for
making named entity annotations in a corpus.
In this framework, the annotation work is per-
formed as an iterative and interactive process
between the human annotator and a proba-
bilistic named entity tagger. At each itera-
tion, sentences that are most likely to con-
tain named entities of the target category are
selected by the probabilistic tagger and pre-
sented to the annotator. This iterative anno-
tation process is repeated until the estimated
coverage reaches the desired level. Unlike ac-
tive learning approaches, our framework pro-
duces a named entity corpus that is free from
the sampling bias introduced by the active
strategy. We evaluated our framework by
simulating the annotation process using two
named entity corpora and show that our ap-
proach could drastically reduce the number
of sentences to be annotated when applied to
sparse named entities.
1 Introduction
Named entities play a central role in conveying im-
portant domain specific information in text, and
good named entity recognizers are often required
in building practical information extraction systems.
Previous studies have shown that automatic named
entity recognition can be performed with a reason-
able level of accuracy by using various machine
learning models such as support vector machines
(SVMs) or conditional random fields (CRFs) (Tjong
Kim Sang and De Meulder, 2003; Settles, 2004;
Okanohara et al, 2006).
However, the lack of annotated corpora, which are
indispensable for training machine learning models,
makes it difficult to broaden the scope of text mining
applications. In the biomedical domain, for exam-
ple, several annotated corpora such as GENIA (Kim
et al, 2003), PennBioIE (Kulick et al, 2004), and
GENETAG (Tanabe et al, 2005) have been created
and made publicly available, but the named entity
categories annotated in these corpora are tailored to
their specific needs and not always sufficient or suit-
able for text mining tasks that other researchers need
to address.
Active learning is a framework which can be used
for reducing the amount of human effort required to
create a training corpus (Dagan and Engelson, 1995;
Engelson and Dagan, 1996; Thompson et al, 1999;
Shen et al, 2004). In active learning, samples that
need to be annotated by the human annotator are
picked up by a machine learning model in an iter-
ative and interactive manner, considering the infor-
mativeness of the samples. Active learning has been
shown to be effective in several natural language
processing tasks including named entity recognition.
The problem with active learning is, however, that
the resulting annotated data is highly dependent on
the machine learning algorithm and the sampling
strategy employed, because active learning anno-
tates only a subset of the given corpus. This sam-
pling bias is not a serious problem if one is to use the
annotated corpus only for their own machine learn-
ing purpose and with the same machine learning al-
gorithm. However, the existence of bias is not desir-
able if one also wants the corpus to be used by other
applications or researchers. For the same reason, ac-
30
tive learning approaches cannot be used to enrich an
existing linguistic corpus with a new named entity
category.
In this paper, we present a framework that enables
one to make named entity annotations for a given
corpus with a reduced cost. Unlike active learn-
ing approaches, our framework aims to annotate all
named entities of the target category contained in
the corpus. Obviously, if we were to ensure 100%
coverage of annotation, there is no way of reducing
the annotation cost, i.e. the human annotator has to
go through every sentence in the corpus. However,
we show in this paper that it is possible to reduce
the cost by slightly relaxing the requirement for the
coverage, and the reduction can be drastic when the
target named entities are sparse.
We should note here that the purpose of this pa-
per is not to claim that our approach is superior to
existing active learning approaches. The goals are
different?while active learning aims at optimizing
the performance of the resulting machine learning-
based tagger, our framework aims to help develop
an unbiased named entity-annotated corpus.
This paper is organized as follows. Section 2 de-
scribes the overall annotation flow in our framework.
Section 3 presents how to select sentences using the
output of a probabilistic tagger. Section 4 describes
how to estimate the coverage during the course of
annotation. Experimental results using two named
entity corpora are presented in section 5. Section 6
describes related work and discussions. Concluding
remarks are given in section 7.
2 Annotating Named Entities by Dynamic
Sentence Selection
Figure 1 shows the overall flow of our annotation
framework. The framework is an iterative process
between the human annotator and a named entity
tagger based on CRFs. In each iteration, the CRF
tagger is trained using all annotated sentences avail-
able and is applied to the unannotated sentences to
select sentences that are likely to contain named
entities of the target category. The selected sen-
tences are then annotated by the human annotator
and moved to the pool of annotated sentences.
This overall flow of annotation framework is very
similar to that of active learning. In fact, the only
1. Select the first n sentences from the corpus and
annotate the named entities of the target cate-
gory.
2. Train a CRF tagger using all annotated sen-
tences.
3. Apply the CRF tagger to the unannotated sen-
tences in the corpus and select the top n sen-
tences that are most likely to contain target
named entities.
4. Annotate the selected sentences.
5. Go back to 2 (repeat until the estimated cover-
age reaches a satisfactory level).
Figure 1: Annotating named entities by dynamic sentence
selection.
differences are the criterion of sentence selection
and the fact that our framework uses the estimated
coverage as the stopping condition. In active learn-
ing, sentences are selected according to their infor-
mativeness to the machine learning algorithm. Our
approach, in contrast, selects sentences that are most
likely to contain named entities of the target cate-
gory. Section 3 elaborates on how to select sentences
using the output of the CRF-based tagger.
The other key in this annotation framework is
when to stop the annotation work. If we repeat the
process until all sentences are annotated, then obvi-
ously there is not merit of using this approach. We
show in section 4 that we can quite accurately esti-
mate how much of the entities in the corpus are al-
ready annotated and use this estimated coverage as
the stopping condition.
3 Selecting Sentences using the CRF
tagger
Our annotation framework takes advantage of the
ability of CRFs to output multiple probabilistic hy-
potheses. This section describes how we obtain
named entity candidates and their probabilities from
CRFs in order to compute the expected number of
named entities contained in a sentence 1.
1We could use other machine learning algorithms for this
purpose as long as they can produce probabilistic output. For
31
3.1 The CRF tagger
CRFs (Lafferty et al, 2001) can be used for named
entity recognition by representing the spans of
named entities using the ?BIO? tagging scheme, in
which ?B? represents the beginning of a named en-
tity, ?I? the inside, and ?O? the outside (See Table 2
for example). This representation converts the task
of named entity recognition into a sequence tagging
task.
A linear chain CRF defines a single log-linear
probabilistic distribution over the possible tag se-
quences y for a sentence x:
p(y|x) = 1Z(x) exp
T
?
t=1
K
?
k=1
?kfk(t, yt, yt?1,xt),
where fk(t, yt, yt?1,xt) is typically a binary func-
tion indicating the presence of feature k, ?k is the
weight of the feature, and Z(X) is a normalization
function:
Z(x) =
?
y
exp
T
?
t=1
K
?
k=1
?kfk(t, yt, yt?1,xt).
This modeling allows us to define features on states
(?BIO? tags) and edges (pairs of adjacent ?BIO?
tags) combined with observations (e.g. words and
part-of-speech (POS) tags).
The weights of the features are determined
in such a way that they maximize the condi-
tional log-likelihood of the training data2 L(?) =
?N
i=1 log p?(y(i)|x(i)). We use the L-BFGS algo-
rithm (Nocedal, 1980) to compute those parameters.
Table 1 lists the feature templates used in the CRF
tagger. We used unigrams of words/POS tags, and
prefixes and suffixes of the current word. The cur-
rent word is also normalized by lowering capital let-
ters and converting all numerals into ?#?, and used
as a feature. We created a word shape feature from
the current word by converting consecutive capital
letters into ?A?, small letters ?a?, and numerals ?#?.
example, maximum entropy Markov models are a possible al-
ternative. We chose the CRF model because it has been proved
to deliver state-of-the-art performance for named entity recog-
nition tasks by previous studies.
2In the actual implementation, we used L2 norm penalty for
regularization.
Word Unigram wi, wi?1, wi+1 & yi
POS Unigram pi, pi?1, pi+1 & yi
Prefix, Suffix prefixes of wi & yi
suffixes of wi & yi
(up to length 3)
Normalized Word N(wi) & yi
Word Shape S(wi) & yi
Tag Bi-gram true & yi?1yi
Table 1: Feature templates used in the CRF tagger.
3.2 Computing the expected number of named
entities
To select sentences that are most likely to contain
named entities of the target category, we need to
obtain the expected number of named entities con-
tained in each sentence. CRFs are well-suited for
this task as the output is fully probabilistic.
Suppose, for example, that the sentence is ?Tran-
scription factor GATA-1 and the estrogen receptor?.
Table 2 shows an example of the 5-best sequences
output by the CRF tagger. The sequences are rep-
resented by the aforementioned ?BIO? representa-
tion. For example, the first sequence indicates that
there is one named entity ?Transcription factor? in
the sequence. By summing up these probabilistic se-
quences, we can compute the probabilities for pos-
sible named entities in a sentence. From the five se-
quences in Table 2, we obtain the following three
named entities and their corresponding probabilities.
?Transcription factor? (0.677 + 0.242 = 0.916)
?estrogen receptor? (0.242 + 0.009 = 0.251)
?Transcription factor GATA-1? (0.012 + 0.009 =
0.021)
The expected number of named entities in this
sentence can then be calculated as 0.916 + 0.251 +
0.021 = 1.188.
In this example, we used 5-best sequences as an
approximation of all possible sequences output by
the tagger, which are needed to compute the exact
expected number of entities. One possible way to
achieve a good approximation is to use a large N for
N -best sequences, but there is a simpler and more
efficient way 3, which directly produces the exact
3We thank an anonymous reviewer for pointing this out.
32
Probability Transcription factor GATA-1 and the estrogen receptor
0.677 B I O O O O O
0.242 B I O O O B I
0.035 O O O O O O O
0.012 B I I O O O O
0.009 B I I O O B I
: : : : : : : :
Table 2: N-best sequences output by the CRF tagger.
expected number of entities. Recall that named enti-
ties are represented with the ?BIO? tags. Since one
entity always contains one ?B? tag, we can compute
the number of expected entities by simply summing
up the marginal probabilities for the ?B? tag on each
token in the sentence4.
Once we compute the expected number of enti-
ties for every unannotated sentence in the corpus,
we sort the sentences in descending order of the ex-
pected number of entities and choose the top n sen-
tences to be presented to the human annotator.
4 Coverage Estimation
To ensure the quality of the resulting annotated cor-
pus, it is crucial to be able to know the current cov-
erage of annotation at each iteration in the annota-
tion process. To compute the coverage, however,
one needs to know the total number of target named
entities in the corpus. The problem is that it is not
known until all sentences are annotated.
In this paper, we solve this dilemma by using
an estimated value for the total number of entities.
Then, the estimated coverage can be computed as
follows:
(estimated coverage) = mm + ?i?U Ei
(1)
where m is the number of entities actually annotated
so far and Ei is the expected number of entities in
sentence i, and U is the set of unannotated sentences
in the corpus. At any iteration, m is always known
and Ei is obtained from the output of the CRF tagger
as explained in the previous section.
4The marginal probabilities on each token can be computed
by the forward-backward algorithm, which is much more effi-
cient than computing N -best sequences for a large N .
# Entities Sentences (%)
CoNLL: LOC 7,140 5,127 (36.5%)
CoNLL: MISC 3,438 2,698 (19.2%)
CoNLL: ORG 6,321 4,587 (32.7%)
CoNLL: PER 6,600 4,373 (31.1%)
GENIA: DNA 2,017 5,251 (28.3%)
GENIA: RNA 225 810 ( 4.4%)
GENIA: cell line 835 2,880 (15.5%)
GENIA: cell type 1,104 5,212 (28.1%)
GENIA: protein 5,272 13,040 (70.3%)
Table 3: Statistics of named entities.
5 Experiments
We carried out experiments to see how our method
can improve the efficiency of annotation process
for sparse named entities. We evaluate our method
by simulating the annotation process using existing
named entity corpora. In other words, we use the
gold-standard annotations in the corpus as the anno-
tations that would be made by the human annotator
during the annotation process.
5.1 Corpus
We used two named entity corpora for the exper-
iments. One is the training data provided for the
CoNLL-2003 shared task (Tjong Kim Sang and
De Meulder, 2003), which consists of 14,041 sen-
tences and includes four named entity categories
(LOC, MISC, ORG, and PER) for the general do-
main. The other is the training data provided for
the NLPBA shared task (Kim et al, 2004), which
consists of 18,546 sentences and five named entity
categories (DNA, RNA, cell line, cell type, and pro-
tein) for the biomedical domain. This corpus is cre-
ated from the GENIA corpus (Kim et al, 2003) by
merging the original fine-grained named entity cate-
gories.
33
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  2000  4000  6000  8000  10000
Number of Sentences
Coverage
Estimated Coverage
Baseline
Figure 2: Annotation of LOC in the CoNLL corpus.
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  2000  4000  6000  8000  10000
Number of Sentences
Coverage
Estimated Coverage
Baseline
Figure 3: Annotation of MISC in the CoNLL corpus.
Table 3 shows statistics of the named entities in-
cluded in the corpora. The first column shows the
number of named entities for each category. The
second column shows the number of the sentences
that contain the named entities of each category. We
can see that some of the named entity categories are
very sparse. For example, named entities of ?RNA?
appear only in 4.4% of the sentences in the corpus.
In contrast, named entities of ?protein? appear in
more than 70% of the sentences in the corpus.
In the experiments reported in the following sec-
tions, we do not use the ?protein? category because
there is no merit of using our framework when most
sentences are relevant to the target category.
5.2 Results
We carried out eight sets of experiments, each of
which corresponds to one of those named entity cat-
egories shown in Table 3 (excluding the ?protein?
category). The number of sentences selected in each
iteration (the value of n in Figure 1) was set to 100
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  2000  4000  6000  8000  10000
Number of Sentences
Coverage
Estimated Coverage
Baseline
Figure 4: Annotation of ORG in the CoNLL corpus.
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  2000  4000  6000  8000  10000
Number of Sentences
Coverage
Estimated Coverage
Baseline
Figure 5: Annotation of PER in the CoNLL corpus.
throughout all experiments.
Figures 2 to 5 show the results obtained on the
CoNLL data. The figures show how the coverage
increases as the annotation process proceeds. The
x-axis shows the number of annotated sentences.
Each figure contains three lines. The normal line
represents the coverage actually achieved, which is
computed as follows:
(coverage) = entities annotatedtotal number of entities . (2)
The dashed line represents the coverage estimated
by using equation 1. For the purpose of comparison,
the dotted line shows the coverage achieved by the
baseline annotation strategy in which sentences are
selected sequentially from the beginning to the end
in the corpus.
The figures clearly show that our method can
drastically accelerate the annotation process in com-
parison to the baseline annotation strategy. The im-
provement is most evident in Figure 3, in which
34
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  2000  4000  6000  8000  10000
Number of Sentences
Coverage
Estimated Coverage
Baseline
Figure 6: Annotation of DNA in the GENIA corpus.
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  2000  4000  6000  8000  10000
Number of Sentences
Coverage
Estimated Coverage
Baseline
Figure 7: Annotation of RNA in the GENIA corpus.
named entities of the category ?MISC? are anno-
tated.
We should also note that coverage estimation was
surprisingly accurate. In all experiments, the differ-
ence between the estimated coverage and the real
coverage was very small. This means that we can
safely use the estimated coverage as the stopping
condition for the annotation work.
Figures 6 to 9 show the experimental results on
the GENIA data. The figures show the same char-
acteristics observed in the CoNLL data. The accel-
eration by our framework was most evident for the
?RNA? category.
Table 4 shows how much we can save the annota-
tion cost if we stop the annotation process when the
estimated coverage reaches 99%. The first column
shows the coverage actually achieved and the second
column shows the number and ratio of the sentences
annotated in the corpus. This table shows that, on
average, we can achieve a coverage of 99.0% by an-
notating 52.4% of the sentences in the corpus. In
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  2000  4000  6000  8000  10000
Number of Sentences
Coverage
Estimated Coverage
Baseline
Figure 8: Annotation of cell line in the GENIA corpus.
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  2000  4000  6000  8000  10000
Number of Sentences
Coverage
Estimated Coverage
Baseline
Figure 9: Annotation of cell type in the GENIA corpus.
other words, we could roughly halve the annotation
cost by accepting the missing rate of 1.0%.
As expected, the cost reduction was most drastic
when ?RNA?, which is the most sparse named entity
category (see Table 3), was targeted. The cost reduc-
tion was more than seven-fold. These experimental
results confirm that our annotation framework is par-
ticularly useful when applied to sparse named enti-
ties.
Table 4 also shows the timing information on the
experiments 5. One of the potential problems with
this kind of active learning-like framework is the
computation time required to retrain the tagger at
each iteration. Since the human annotator has to
wait while the tagger is being retrained, the compu-
tation time required for retraining the tagger should
not be very long. In our experiments, the worst
case (i.e. DNA) required 443 seconds for retrain-
ing the tagger at the last iteration, but in most cases
5We used AMD Opteron 2.2GHz servers for the experiments
and our CRF tagger is implemented in C++.
35
Coverage Sentences Annotated (%) Cumulative Time (second) Last Interval (second)
CoNLL: LOC 99.1% 7,600 (54.1%) 3,362 92
CoNLL: MISC 96.9% 5,400 (38.5%) 1,818 61
CoNLL: ORG 99.7% 8,900 (63.4%) 5,201 104
CoNLL: PER 98.0% 6,200 (44.2%) 2,300 75
GENIA: DNA 99.8% 11,900 (64.2%) 33,464 443
GENIA: RNA 99.2% 2,500 (13.5%) 822 56
GENIA: cell line 99.6% 9,400 (50.7%) 15,870 284
GENIA: cell type 99.3% 8,600 (46.4%) 13,487 295
Average 99.0% - (52.4%) - -
Table 4: Coverage achieved when the estimated coverage reached 99%.
the training time for each iteration was kept under
several minutes.
In this work, we used the BFGS algorithm for
training the CRF model, but it is probably possible to
further reduce the training time by using more recent
parameter estimation algorithms such as exponenti-
ated gradient algorithms (Globerson et al, 2007).
6 Discussion and Related Work
Our annotation framework is, by definition, not
something that can ensure a coverage of 100%. The
seriousness of a missing rate of, for example, 1% is
not entirely clear?it depends on the application and
the purpose of annotation. In general, however, it
is hard to achieve a coverage of 100% in real an-
notation work even if the human annotator scans
through all sentences, because there is often ambi-
guity in deciding whether a particular named entity
should be annotated or not. Previous studies report
that inter-annotator agreement rates with regards to
gene/protein name annotation are f-scores around
90% (Morgan et al, 2004; Vlachos and Gasperin,
2006). We believe that the missing rate of 1% can be
an acceptable level of sacrifice, given the cost reduc-
tion achieved and the unavoidable discrepancy made
by the human annotator.
At the same time, we should also note that our
framework could be used in conjunction with ex-
isting methods for semi-supervised learning to im-
prove the performance of the CRF tagger, which
in turn will improve the coverage. It is also pos-
sible to improve the performance of the tagger by
using external dictionaries or using more sophis-
ticated probabilistic models such as semi-Markov
CRFs (Sarawagi and Cohen, 2004). These enhance-
ments should further improve the coverage, keeping
the same degree of cost reduction.
The idea of improving the efficiency of annota-
tion work by using automatic taggers is certainly not
new. Tanabe et al (2005) applied a gene/protein
name tagger to the target sentences and modified
the results manually. Culotta and McCallum (2005)
proposed to have the human annotator select the
correct annotation from multiple choices produced
by a CRF tagger for each sentence. Tomanek et
al. (2007) discuss the reusability of named entity-
annotated corpora created by an active learning ap-
proach and show that it is possible to build a cor-
pus that is useful to different machine learning algo-
rithms to a certain degree.
The limitation of our framework is that it is use-
ful only when the target named entities are sparse
because the upper bound of cost saving is limited
by the proportion of the relevant sentences in the
corpus. Our framework may therefore not be suit-
able for a situation where one wants to make an-
notations for named entities of many categories si-
multaneously (e.g. creating a corpus like GENIA
from scratch). In contrast, our framework should be
useful in a situation where one needs to modify or
enrich named entity annotations in an existing cor-
pus, because the target named entities are almost al-
ways sparse in such cases. We should also note that
named entities in full papers, which recently started
to attract much attention, tend to be more sparse than
those in abstracts.
7 Conclusion
We have presented a simple but powerful framework
for reducing the human effort for making name en-
tity annotations in a corpus. The proposed frame-
work allows us to annotate almost all named entities
36
of the target category in the given corpus without
having to scan through all the sentences. The frame-
work also allows us to know when to stop the anno-
tation process by consulting the estimated coverage
of annotation.
Experimental results demonstrated that the frame-
work can reduce the number of sentences to be anno-
tated almost by half, achieving a coverage of 99.0%.
Our framework was particularly effective when the
target named entities were very sparse.
Unlike active learning, this work enables us to
create a named entity corpus that is free from the
sampling bias introduced by the active learning strat-
egy. This work will therefore be especially useful
when one needs to enrich an existing linguistic cor-
pus (e.g. WSJ, GENIA, or PennBioIE) with named
entity annotations for a new semantic category.
Acknowledgment
This work is partially supported by BBSRC grant
BB/E004431/1. The UK National Centre for Text
Mining is sponsored by the JISC/BBSRC/EPSRC.
References
Aron Culotta and Andrew McCallum. 2005. Reducing
labeling effort for structured prediction tasks. In Pro-
ceedings of AAAI-05, pages 746?751.
Ido Dagan and Sean P. Engelson. 1995. Committee-
based sampling for training probabilistic classifiers. In
Proceedings of ICML, pages 150?157.
Sean Engelson and Ido Dagan. 1996. Minimizing man-
ual annotation cost in supervised training from cor-
pora. In Proceedings of ACL, pages 319?326.
A. Globerson, T. Koo, X. Carreras, and M. Collins. 2007.
Exponentiated gradient algorithms for log-linear struc-
tured prediction. In Proceedings of ICML, pages 305?
312.
J.-D. Kim, T. Ohta, Y. Tateisi, and J. Tsujii. 2003. GE-
NIA corpus?a semantically annotated corpus for bio-
textmining. Bioinformatics, 19 (Suppl. 1):180?182.
J.-D. Kim, T. Ohta, Y. Tsuruoka, Y. Tateisi, and N. Col-
lier. 2004. Introduction to the bio-entity recogni-
tion task at JNLPBA. In Proceedings of the Interna-
tional Joint Workshop on Natural Language Process-
ing in Biomedicine and its Applications (JNLPBA),
pages 70?75.
Seth Kulick, Ann Bies, Mark Libeman, Mark Mandel,
Ryan McDonald, Martha Palmer, Andrew Schein, and
Lyle Ungar. 2004. Integrated annotation for biomed-
ical information extraction. In Proceedings of HLT-
NAACL 2004 Workshop: Biolink 2004, pages 61?68.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic mod-
els for segmenting and labeling sequence data. In Pro-
ceedings of ICML, pages 282?289.
Alexander A. Morgan, Lynette Hirschman, Marc
Colosimo, Alexander S. Yeh, and Jeff B. Colombe.
2004. Gene name identification and normalization us-
ing a model organism database. Journal of Biomedical
Informatics, 37:396?410.
Jorge Nocedal. 1980. Updating quasi-newton matrices
with limited storage. Mathematics of Computation,
35(151):773?782.
Daisuke Okanohara, Yusuke Miyao, Yoshimasa Tsu-
ruoka, and Jun?ichi Tsujii. 2006. Improving the scal-
ability of semi-markov conditional random fields for
named entity recognition. In Proceedings of COL-
ING/ACL, pages 465?472.
Sunita Sarawagi and William W. Cohen. 2004. Semi-
markov conditional random fields for information ex-
traction. In Proceedings of NIPS.
Burr Settles. 2004. Biomedical named entity recogni-
tion using conditional random fields and rich feature
sets. In COLING 2004 International Joint workshop
on Natural Language Processing in Biomedicine and
its Applications (NLPBA/BioNLP) 2004, pages 107?
110.
Dan Shen, Jie Zhang, Jian Su, Guodong Zhou, and Chew-
Lim Tan. 2004. Multi-criteria-based active learning
for named entity recognition. In Proceedings of ACL,
pages 589?596, Barcelona, Spain.
Lorraine Tanabe, Natalie Xie, Lynne H. Thom, Wayne
Matten, and W. John Wilbur. 2005. GENETAG: a
tagged corpus for gene/protein named entity recogni-
tion. BMC Bioinformatics, 6(Suppl 1):S3.
Cynthia A. Thompson, Mary Elaine Califf, and Ray-
mond J. Mooney. 1999. Active learning for natural
language parsing and information extraction. In Pro-
ceedings of ICML, pages 406?414.
Erik F. Tjong Kim Sang and Fien De Meulder. 2003.
Introduction to the conll-2003 shared task: Language-
independent named entity recognition. In Proceedings
of CoNLL-2003, pages 142?147.
Katrin Tomanek, Joachim Wermter, and Udo Hahn.
2007. An approach to text corpus construction which
cuts annotation costs and maintains reusability of an-
notated data. In Proceedings of EMNLP-CoNLL,
pages 486?495.
Andreas Vlachos and Caroline Gasperin. 2006. Boot-
strapping and evaluating named entity recognition in
the biomedical domain. In Proceedings of the HLT-
NAACL BioNLP Workshop on Linking Natural Lan-
guage and Biology, pages 138?145.
37
BioNLP 2008: Current Trends in Biomedical Natural Language Processing, pages 63?70,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
How to Make the Most of NE Dictionaries in Statistical NER
Yutaka Sasaki2 Yoshimasa Tsuruoka2 John McNaught1,2 Sophia Ananiadou1,2
1 National Centre for Text Mining
2 School of Computer Science, University of Manchester
MIB, 131 Princess Street, Manchester, M1 7DN, UK
Abstract
When term ambiguity and variability are very
high, dictionary-based Named Entity Recogni-
tion (NER) is not an ideal solution even though
large-scale terminological resources are avail-
able. Many researches on statistical NER have
tried to cope with these problems. However,
it is not straightforward how to exploit exist-
ing and additional Named Entity (NE) dictio-
naries in statistical NER. Presumably, addi-
tion of NEs to an NE dictionary leads to bet-
ter performance. However, in reality, the re-
training of NER models is required to achieve
this. We have established a novel way to im-
prove the NER performance by addition of
NEs to an NE dictionary without retraining.
We chose protein name recognition as a case
study because it most suffers the problems re-
lated to heavy term variation and ambiguity.
In our approach, first, known NEs are identi-
fied in parallel with Part-of-Speech (POS) tag-
ging based on a general word dictionary and
an NE dictionary. Then, statistical NER is
trained on the tagger outputs with correct NE
labels attached. We evaluated performance of
our NER on the standard JNLPBA-2004 data
set. The F-score on the test set has been im-
proved from 73.14 to 73.78 after adding the
protein names appearing in the training data to
the POS tagger dictionary without any model
retraining. The performance further increased
to 78.72 after enriching the tagging dictionary
with test set protein names. Our approach
has demonstrated high performance in pro-
tein name recognition, which indicates how
to make the most of known NEs in statistical
NER.
1 Introduction
The accumulation of online biomedical informa-
tion has been growing at a rapid pace, mainly at-
tributed to a rapid growth of a wide range of repos-
itories of biomedical data and literature. The auto-
matic construction and update of scientific knowl-
edge bases is a major research topic in Bioinformat-
ics. One way of populating these knowledge bases
is through named entity recognition (NER). Unfortu-
nately, biomedical NER faces many problems, e.g.,
protein names are extremely difficult to recognize
due to ambiguity, complexity and variability. A fur-
ther problem in protein name recognition arises at
the tokenization stage. Some protein names include
punctuation or special symbols, which may cause to-
kenization to lose some word concatenation infor-
mation in the original sentence. For example, IL-2
and IL - 2 fall into the same token sequence IL
- 2 as usually dash (or hyphen) is designated as a
token delimiter.
Research into NER is centred around three ap-
proaches: dictionary-based, rule-based and machine
learning-based approaches. To overcome the usual
NER pitfalls, we have opted for a hybrid approach
combining dictionary-based and machine learning
approaches, which we call dictionary-based statisti-
cal NER approach. After identifying protein names
in text, we link these to semantic identifiers, such as
UniProt accession numbers. In this paper, we focus
on the evaluation of our dictionary-based statistical
NER.
2 Methods
Our dictionary-based statistical approach consists of
two components: dictionary-based POS/PROTEIN
tagging and statistical sequential labelling. First,
63
dictionary-based POS/PROTEIN tagging finds can-
didates for protein names using a dictionary. The
dictionary maps strings to parts of speech (POS),
where the POS tagset is augmented with a tag
NN-PROTEIN. Then, sequential labelling applies
to reduce false positives and false negatives in the
POS/PROTEIN tagging results. Expandability is
supported through allowing a user of the NER tool to
improve NER coverage by adding entries to the dic-
tionary. In our approach, retraining is not required
after dictionary enrichment.
Recently, Conditional Random Fields (CRFs)
have been successfully applied to sequence labelling
problems, such as POS tagging and NER, and have
outperformed other machine learning techniques.
The main idea of CRFs is to estimate a conditional
probability distribution over label sequences, rather
than over local directed label sequences as with Hid-
den Markov Models (Baum and Petrie, 1966) and
Maximum Entropy Markov Models (McCallum et
al., 2000). Parameters of CRFs can be efficiently
estimated through the log-likelihood parameter esti-
mation using the forward-backward algorithm, a dy-
namic programming method.
2.1 Training and test data
Experiments were conducted using the training and
test sets of the JNLPBA-2004 data set(Kim et al,
2004).
Training data The training data set used in
JNLPBA-2004 is a set of tokenized sentences with
manually annotated term class labels. The sentences
are taken from the Genia corpus (version 3.02) (Kim
et al, 2003), in which 2,000 abstracts were manu-
ally annotated by a biologist, drawing on a set of
POS tags and 36 biomedical term classes. In the
JNLPBA-2004 shared task, performance in extract-
ing five term classes, i.e., protein, DNA, RNA, cell
line, and cell type classes, were evaluated.
Test Data The test data set used in JNLPBA-2004
is a set of tokenized sentences extracted from 404
separately collected MEDLINE abstracts, where the
term class labels were manually assigned, following
the annotation specification of the Genia corpus.
2.2 Overview of dictionary-based statistical
NER
Figure 1 shows the block diagram of dictionary-
based statistical NER. Raw text is analyzed by
a POS/PROTEIN tagger based on a CRF tagging
Figure 1: Block diagram of dictionary-based statistical
NER
Figure 2: Block diagram of training procedure
model and dictionary, and then converted into to-
ken sequences. Strings in the text that match with
protein names in the dictionary will be tagged as
NN-PROTEIN depending on the context around the
protein names. Since it is not realistic to enumer-
ate all protein names in the dictionary, due to their
high variability of form, instead previously unseen
forms are predicted to be protein names by statisti-
cal sequential labelling. Finally, protein names are
identified from the POS/PROTEIN tagged token se-
quences via a CRF labelling model.
Figure 2 shows the block diagram of the train-
ing procedure for both POS/PROTEIN tagging and
sequential labelling. The tagging model is created
using the Genia corpus (version 3.02) and a dic-
tionary. Using the tagging model, MEDLINE ab-
stracts used for the JNLPBA-2004 training data set
are then POS/PROTEIN-tagged. The output token
sequences over these abstracts are then integrated
with the correct protein labels of the JNLPBA-2004
training data. This process results in the preparation
of token sequences with features and correct protein
labels. A CRF labelling model is finally generated
by applying a CRF tool to these decorated token se-
quences.
64
IL/NNP
-/- 2/CD
-/-
mediated/VVD
mediated/VVN
activation/NN
IL-2/NN-PROTEIN
IL-2/NN-PROTEIN
-/-
2/CD
mediated/VVN
mediated/VVD
mediate/VVP
mediate/VV
activation/NN
IL/NNP
IL-2-mediated activation ...
POS/PROTEIN tagging
Lexicon
Figure 3: Dictionary based approach
2.2.1 Dictionary-based POS/PROTEIN tagging
The dictionary-based approach is beneficial when
a sentence contains some protein names that con-
flict with general English words. Otherwise, if the
POS tags of sentences are decided without consider-
ing possible occurrences of protein names, POS se-
quences could be disrupted. For example, in ?met
proto-oncogene precursor?, met might be falsely
recognized as a verb by a non dictionary-based tag-
ger.
Given a sentence, the dictionary-based approach
extracts protein names as follows. Find all word se-
quences that match the lexical entries, and create a
token graph (i.e., trellis) according to the word order.
Estimate the score of every path using the weights of
node and edges estimated by training using Condi-
tional Random Fields. Select the best path.
Figure 3 shows an example of our dictionary-
based approach. Suppose that the input is ?IL-
2-mediated activation?. A trellis is created based
on the lexical entries in a dictionary. The se-
lection criteria for the best path are determined
by the CRF tagging model trained on the Genia
corpus. In this example, IL-2/NN-PROTEIN
-/- mediated/VVN activation/NN is se-
lected as the best path. Following Kudo et al (Kudo
et al, 2004), we adapted the core engine of the
CRF-based morphological analyzer, MeCab1, to our
POS/PROTEIN tagging task. MeCab?s dictionary
databases employ double arrays (Aoe, 1989) which
enable efficient lexical look-ups.
The features used were:
? POS
? PROTEIN
1http://sourceforge.net/project/showfiles.php?group id=177856/
? POS-PROTEIN
? bigram of adjacent POS
? bigram of adjacent PROTEIN
? bigram of adjacent POS-PROTEIN
During the construction of the trellis, white space
is considered as the delimiter unless otherwise stated
within dictionary entries. This means that unknown
tokens are character sequences without spaces.
2.2.2 Dictionary construction
A dictionary-based approach requires the dictio-
nary to cover not only a wide variety of biomedical
terms but also entries with:
? all possible capitalization
? all possible linguistic inflections
We constructed a freely available, wide-coverage
English word dictionary that satisfies these condi-
tions. We did consider the MedPost pos-tagger
package2 which contains a free dictionary that has
downcased English words; however, this dictionary
is not well curated as a dictionary and the number of
entries is limited to only 100,000, including inflec-
tions.
Therefore, we started by constructing an English
word dictionary. Eventually, we created a dictionary
with about 266,000 entries for English words (sys-
tematically covering inflections) and about 1.3 mil-
lion entries for protein names.
We created the general English part of the dictio-
nary from WordNet by semi-automatically adding
POS tags. The POS tag set is a minor modifica-
tion of the Penn Treebank POS tag set3, in that pro-
tein names are given a new POS tag, NN-PROTEIN.
Further details on construction of the dictionary now
follow.
Protein names were extracted from the BioThe-
saurus4. After selecting only those terms
clearly stated as protein names, 1,341,992 pro-
tein names in total were added to the dictionary.
2ftp://ftp.ncbi.nlm.nih.gov/pub/lsmith/MedPost/
3ftp://ftp.cis.upenn.edu/pub/treebank/
doc/tagguide.ps.gz
4http://pir.georgetown.edu/iprolink/
biothesaurus/
65
Nouns were extracted from WordNet?s noun list.
Words starting with lower case and upper case
letters were determined as NN and NNP, re-
spectively. Nouns in NNS and NNPS cate-
gories were collected from the results of POS
tagging articles from Plos Biology Journal5
with TreeTagger6.
Verbs were extracted from WordNet?s verb list. We
manually curated VBD, VBN, VBG and VBZ
verbs with irregular inflections based on Word-
Net. Next, VBN, VBD, VBG and VBZ forms
of regular verbs were automatically generated
from the WordNet verb list.
Adjectives were extracted from WordNet?s adjec-
tive list. We manually curated JJ, JJR and JJS
of irregular inflections of adjectives based on
the WordNet irregular adjective list. Base form
(JJ) and regular inflections (JJR, JJS) of adjec-
tives were also created based on the list of ad-
jectives.
Adverbs were extracted from WordNet?s adverb
list. Both the original and capitalised forms
were added as RB.
Pronouns were manually curated. PRP and PRP$
words were added to the dictionary.
Wh-words were manually curated. As a result,
WDT, WP, WP$ and WRB words were added
to the dictionary.
Words for other parts of speech were manually
curated.
2.2.3 Statistical prediction of protein names
Statistical sequential labelling was employed to
improve the coverage of protein name recognition
and to remove false positives resulting from the pre-
vious stage (dictionary-based tagging).
We used the JNLPBA-2004 training data, which
is a set of tokenized word sequences with
IOB2(Tjong Kim Sang and Veenstra, 1999) protein
labels. As shown in Figure 2, POSs of tokens re-
sulting from tagging and tokens of the JNLPBA-
2004 data set are integrated to yield training data for
sequential labelling. During integration, when the
single token of a protein name found after tagging
5http://biology.plosjournals.org/
6http://www.ims.uni-stuttgart.de/projekte/
corplex/TreeTagger/DecisionTreeTagger.html/
corresponds to a sequence of tokens from JNLPBA-
2004, its POS is given as NN-PROTEIN1, NN-
PROTEIN2,..., according to the corresponding token
order in the JNLPBA-2004 sequence.
Following the data format of the JNLPBA-2004
training set, our training and test data use the IOB2
labels, which are ?B-protein? for the first token of
the target sequence, ?I-protein? for each remaining
token in the target sequence, and ?O? for other to-
kens. For example, ?Activation of the IL 2 precursor
provides? is analyzed by the POS/PROTEIN tagger
as follows.
Activation NN
of IN
the DT
IL 2 precursor NN-PROTEIN
provides VVZ
The tagger output is given IOB2 labels as follows.
Activation NN O
of IN O
the DT O
IL NN-PROTEIN1 B-protein
2 NN-PROTEIN2 I-protein
precursor NN-PROTEIN3 I-protein
provides VVZ O
We used CRF models to predict the IOB2 la-
bels. The following features were used in our ex-
periments.
? word feature
? orthographic features
? the first letter and the last four letters of
the word form, in which capital letters in
a word are normalized to ?A?, lower case
letters are normalized to ?a?, and digits are
replaced by ?0?, e.g., the word form of IL-
2 is AA-0.
? postfixes, the last two and four letters
? POS feature
? PROTEIN feature
The window size was set to ?2 of the current to-
ken.
3 Results and discussion
66
Table 1: Experimental Rusults
Tagging R P F
Full 52.91 43.85 47.96
(a) POS/PROTEIN tagging Left 61.48 50.95 55.72
Right 61.38 50.87 55.63
Sequential Labelling R P F
Full 63.23 70.39 66.62
(b) Word feature Left 68.15 75.86 71.80
Right 69.88 77.79 73.63
Full 77.17 67.52 72.02
(c) (b) + orthographic feature Left 82.51 72.20 77.01
Right 84.29 73.75 78.67
Full 76.46 68.41 72.21
(d) (c) + POS feature Left 81.94 73.32 77.39
Right 83.54 74.75 78.90
Full 77.58 69.18 73.14
(e) (d) + PROTEIN feature Left 82.69 73.74 77.96
Right 84.37 75.24 79.54
Full 79.85 68.58 73.78
(f) (e) + after adding protein names in the Left 84.82 72.85 78.38
training set to the dictionary Right 86.60 74.37 80.02
3.1 Protein name recognition performance
Table 1 shows our protein name recognition results,
showing the differential effect of various combina-
tions of strategies. Results are expressed accord-
ing to recall (R), precision (P), and F-measure (F),
which here measure how accurately our various ex-
periments determined the left boundary (Left), the
right boundary (Right), and both boundaries (Full)
of protein names. The baseline for tagging (row
(a)) shows the protein name detection performance
of our dictionary-based tagging using our large pro-
tein name dictionary, where no training for protein
name prediction was involved. The F-score of this
baseline tagging method was 47.96.
The baseline for sequential labelling (row (b))
shows the prediction performance when using only
word features where no orthographic and POS fea-
tures were used. The F-score of the baseline la-
belling method was 66.62. When orthographic fea-
ture was added (row (c)), the F-score increased by
5.40 to 72.02. When the POS feature was added
(row (d)), the F-score increased by 0.19 to 72.21.
Using all features (row (e)), the F-score reached
73.14. Surprisingly, adding protein names appear-
ing in the training data to the dictionary further im-
proved the F-score by 0.64 to 73.78, which is the
second best score for protein name recognition us-
ing the JNLPBA-2004 data set.
Table 2: After Dictionary Enrichment
Method R P F
Tagging Full 79.02 61.87 69.40
(+test set Left 82.28 64.42 72.26
protein names) Right 80.96 63.38 71.10
Labelling full 86.13 72.49 78.72
(+test set Left 89.58 75.40 81.88
protein names) Right 90.23 75.95 82.47
Tagging and labelling speeds were measured us-
ing an unloaded Linux server with quad 1.8 GHz
Opteron cores and 16GB memory. The dictionary-
based POS/PROTEIN tagger is very fast even
though the total size of the dictionary is more than
one million. The processing speed for tagging and
sequential labelling of the 4,259 sentences of the test
set data took 0.3 sec and 7.3 sec, respectively, which
means that in total it took 7.6 sec. for recognizing
protein names in the plain text of 4,259 sentences.
3.2 Dictionary enrichment
The advantage of the dictionary-based statistical ap-
proach is that it is versatile, as the user can easily
improve its performance with no retraining. We as-
sume the following situation as the ideal case: sup-
pose that a user needs to analyze a large amount of
text with protein names. The user wants to know
67
the maximum performance achievable for identify-
ing protein names with our dictionary-based statis-
tical recognizer which can be achieved by adding
more protein names to the current dictionary. Note
that protein names should be identified in context.
That is, recall of the NER results with the ideal dic-
tionary is not 100%. Some protein names in the ideal
dictionary are dropped during statistical tagging or
labelling.
Table 2 shows the scores after each step of dic-
tionary enrichment. The first block (Tagging) shows
the tagging performance after adding protein names
appearing in the test set to the dictionary. The sec-
ond block (Labelling) shows the performance of the
sequence labelling of the output of the first step.
Note that tagging and the sequence labelling mod-
els are not retrained using the test set.
3.3 Discussion
It is not possible in reality to train the recognizer
on target data, i.e., the test set, but it would be pos-
sible for users to add discovered protein names to
the dictionary so that they could improve the overall
performance of the recognizer without retraining.
Rule-based and procedural approaches are taken
in (Fukuda et al, 1998; Franzen et al, 2002). Ma-
chine learning-based approaches are taken in (Col-
lier et al, 2000; Lee et al, 2003; Kazama et al,
2002; Tanabe and Wilbur, 2002; Yamamoto et al,
2003; Tsuruoka, 2006; Okanohara et al, 2006).
Machine learning algorithms used in these studies
are Naive Bayes, C4.5, Maximum Entropy Models,
Support Vector Machines, and Conditional Random
Fields. Most of these studies applied machine learn-
ing techniques to tokenized sentences.
Table 3 shows the scores reported by other sys-
tems. Tsai et al (Tsai et al, 2006) and Zhou and
Su (Zhou and Su, 2004) combined machine learning
techniques and hand-crafted rules. Tsai et al (Tsai
et al, 2006) applied CRFs to the JNLPBA-2004
data. After applying pattern-based post-processing,
they achieved the best F-score (75.12) among those
reported so far. Kim and Yoon(Kim and Yoon, 2007)
also applied heuristic post-processing. Zhou and Su
(Zhou and Su, 2004) achieved an F-score of 73.77.
Purely machine learning-based approaches have
been investigated by several researchers. The
GENIA Tagger (Tsuruoka, 2006) is trained on
the JNLPBA-2004 Corpus. Okanohara et al
(Okanohara et al, 2006) employed semi-Markov
CRFs whose performance was evaluated against the
JNLPBA-2004 data set. Yamamoto et al (Ya-
mamoto et al, 2003) used SVMs for character-
based protein name recognition and sequential la-
belling. Their protein name extraction performance
was 69%. This paper extends the machine learning
approach with a curated dictionary and CRFs and
achieved high F-score 73.78, which is the top score
among the heuristics-free NER systems. Table 4
shows typical recognition errors found in the recog-
nition results that achieved F-score 73.78. In some
cases, protein name boundaries of the JNLPBA-
2004 data set are not consistent. It is also one of
the reasons for the recognition errors that the data
set contains general protein names, such as domain,
family, and binding site names as well as anaphoric
expressions, which are usually not covered by pro-
tein name repositories. Therefore, our impression on
the performance is that an F-score of 73.78 is suffi-
ciently high.
Furthermore, thanks to the dictionary-based ap-
proach, it has been shown that the upper bound per-
formance using ideal dictionary enrichment, with-
out any retraining of the models, has an F-score of
78.72.
4 Conclusions
This paper has demonstrated how to utilize known
named entities to achieve better performance in sta-
tistical named entity recognition. We took a two-
step approach where sentences are first tokenized
and tagged based on a biomedical dictionary that
consists of general English words and about 1.3 mil-
lion protein names. Then, a statistical sequence
labelling step predicted protein names that are not
listed in the dictionary and, at the same time, re-
duced false negatives in the POS/PROTEIN tagging
results. The significant benefit of this approach is
that a user, not a system developer, can easily en-
hance the performance by augmenting the dictio-
nary. This paper demonstrated that the state-of-
the-art F-score 73.78 on the standard JNLPBA-2004
data set was achieved by our approach. Further-
more, thanks to the dictionary-based NER approach,
the upper bound performance using ideal dictionary
enrichment, without any retraining of the models,
yielded F-score 78.72.
5 Acknowledgments
This research is partly supported by EC IST project
FP6-028099 (BOOTStrep), whose Manchester team
is hosted by the JISC/BBSRC/EPSRC sponsored
National Centre for Text Mining.
68
Table 3: Conventional results for protein name recognition
Authors R P F
Tsai et al(Tsai et al, 2006) 71.31 79.36 75.12
Our system 79.85 68.58 73.78
Zhou and Su(Zhou and Su, 2004) 69.01 79.24 73.77
Kim and Yoon(Kim and Yoon, 2007) 75.82 71.02 73.34
Okanohara et al(Okanohara et al, 2006) 77.74 68.92 73.07
Tsuruoka(Tsuruoka, 2006) 81.41 65.82 72.79
Finkel et al(Finkel et al, 2004) 77.40 68.48 72.67
Settles(Settles, 2004) 76.1 68.2 72.0
Song et al(Song et al, 2004) 65.50 73.04 69.07
Ro?ssler(Ro?ssler, 2004) 72.9 62.0 67.0
Park et al(Park et al, 2004) 69.71 59.37 64.12
References
J. Aoe, An Efficient Digital Search Algorithm by Using
a Double-Array Structure, IEEE Transactions on Soft-
ware Engineering, 15(9):1066?1077, 1989.
L.E. Baum and T. Petrie, Statistical inference for proba-
bilistic functions of finite state Markov chains, The An-
nals of Mathematical Statistics, 37:1554?1563, 1966.
J. Chang, H. Schutze, R. Altman, GAPSCORE: Finding
Gene and Protein names one Word at a Time, Bioin-
formatics, Vol. 20, pp. 216-225, 2004.
N. Collier, C. Nobata, J. Tsujii, Extracting the Names
of Genes and Gene Products with a Hidden Markov
Model, Proc. of the 18th International Conference
on Computational Linguistics (COLING?2000), Saar-
brucken, 2000.
Jenny Finkel, Shipra Dingare, Huy Nguyen, Malvina Nis-
sim, Gail Sinclair and Christopher Manning, Exploit-
ing Context for Biomedical Entity Recognition: From
Syntax to the Web, Proc. of the Joint Workshop on Nat-
ural Language Processing in Biomedicine and its Ap-
plications (JNLPBA-2004), pp. 88?91, 2004.
K. Franzen, G. Eriksson, F. Olsson, L. Asker, P. Liden,
and J. Koster, Protein Names and How to Find Them,
Int. J. Med. Inf., Vol. 67, pp. 49?61, 2002.
K. Fukuda, T. Tsunoda, A. Tamura, and T. Takagi,
Toward information extraction: identifying protein
names from biological papers, PSB, pp. 705-716,
1998.
J. Kazama, T. Makino, Y. Ohta, J. Tsujii, Tuning Support
Vector Machines for Biomedical Named Entity Recog-
nition, Proc. of ACL-2002 Workshop on Natural Lan-
guage Processing in the Biomedical Domain, pp. 1?8,
2002.
J.-D. Kim, T. Ohta, Y. Tateisi, J. Tsujii: GENIA corpus
- semantically annotated corpus for bio-textmining,
Bioinformatics 2003, 19:i180-i182.
Jin-Dong Kim, Tomoko Ohta, Yoshimasa Tsuruoka,
Yuka Tateisi, Introduction to the Bio-Entity Recogni-
tion Task at JNLPBA, Proc. of the Joint Workshop on
Natural Language Processing in Biomedicine and its
Applications (JNLPBA-2004), pp. 70?75, 2004.
S. Kim, J. Yoon: Experimental Study on a Two Phase
Method for Biomedical Named Entity Recognition,
IEICE Transactions on Informaion and Systems 2007,
E90-D(7):1103?1120.
Taku Kudo and Kaoru Yamamoto and Yuuji Matsumoto,
Applying Conditional Random Fields to Japanese
Morphological Analysis, Proc. of Empirical Methods
in Natural Language Processing (EMNLP), pp. 230?
237, 2004.
J. Lafferty, A. McCallum, and F. Pereira, Conditional
Random Fields: Probabilistic Models for Segment-
ing and Labeling Sequence Data, Proc. of ICML-2001,
pp.282?289, 2001
K. J. Lee, Y. S. Hwang and H. C. Rim (2003), Two-Phase
Biomedical NE Recognition based on SVMs, Proc. of
ACL 2003 Workshop on Natural Language Processing
in Biomedicine, Sapporo, 2003.
McCallum A, Freitag D, Pereira F.: Maximum entropy
Markov models for information extraction and seg-
mentation, Proceedings of the Seventeenth Interna-
tional Conference on Machine Learning, 2000:591-
598.
Daisuke, Okanohara, Yusuke Miyao, Yoshimasa Tsu-
ruoka and Jun?ichi Tsujii, Improving the Scalability of
Semi-Markov Conditional Random Fields for Named
Entity Recognition, Proc. of ACL 2006, Sydney, 2006.
Kyung-Mi Park, Seon-Ho Kim, Do-Gil Lee and
Hae-Chang Rim. Boosting Lexical Knowledge for
Biomedical Named Entity Recognition, Proc. of the
Joint Workshop on Natural Language Processing in
Biomedicine and its Applications (JNLPBA-2004), pp.
76-79, 2004.
Marc Ro?ssler, Adapting an NER-System for German to
the Biomedical Domain, Proc. of the Joint Workshop
on Natural Language Processing in Biomedicine and
its Applications (JNLPBA-2004), pp. 92?95, 2004.
Burr Settles, Biomedical Named Entity Recognition Us-
ing Conditional Random Fields and Novel Feature
69
Table 4: Error Analysis
False positives
Cause Correct extraction Identified term
1 dictionary - protein, binding sites
2 prefix word trans-acting factor common trans-acting factor
3 unknown word - ATTTGCAT
4 sequential labelling error - additional proteins
5 test set error - Estradiol receptors
False negatives
Cause Correct extraction Identified term
1 anaphoric (the) receptor, (the) binding sites -
2 coordination (and, or) transcription factors NF-kappa B and AP-1 transcription factors NF-kappa B
3 prefix word activation protein-1 protein-1
catfish STAT STAT
4 postfix word nuclear factor kappa B complex nuclear factor kappa B
5 plural protein tyrosine kinase(s) protein tyrosine kinase
6 family name, biding site, T3 binding sites -
and domain residues 639-656 -
7 sequential labelling error PCNA -
Chloramphenicol acetyltransferase -
8 test set error superfamily member -
Sets, Proc. of the Joint Workshop on Natural Lan-
guage Processing in Biomedicine and its Applications
(JNLPBA-2004), pp. 104?1007, 2004.
Yu Song, Eunju Kim, Gary Geunbae Lee and Byoung-
kee Yi, POSBIOTM-NER in the shared task of
BioNLP/NLPBA 2004, Proc. of the Joint Workshop on
Natural Language Processing in Biomedicine and its
Applications (JNLPBA-2004), pp. 100-103, 2004.
L. Tanabe and W. J. Wilbur, Tagging Gene and Protein
Names in Biomedical Text, Bioinformatics, 18(8), pp.
1124?1132, 2002.
E.F. Tjong Kim Sang and J. Veenstra, Representing Text
Chunks,EACL-99, pp. 173-179, 1999.
Richard Tzong-Han Tsai, W.-C. Chou, S.-H. Wu, T.-Y.
Sung, J. Hsiang, and W.-L. Hsu, Integrating Linguistic
Knowledge into a Conditional Random Field Frame-
work to Identify Biomedical Named Entities, Expert
Systems with Applications, 30 (1), 2006.
Yoshimasa Tsuruoka, GENIA Tagger 3.0,
http://www-tsujii.is.s.u-tokyo.ac.jp/
GENIA/tagger/, 2006.
K. Yamamoto, T. Kudo, A. Konagaya and Y. Matsumoto,
Protein Name Tagging for Biomedical Annotation in
Text, in Proc. of ACL-2003 Workshop on Natural Lan-
guage Processing in Biomedicine, Sapporo, 2003.
Guofeng Zhou and Jian Su, Exploring Deep Knowledge
Resources in Biomedical Name Recognition, Proceed-
ings of the Joint Workshop on Natural Language Pro-
cessing of Biomedicine and its Applications (JNLPBA-
2004), pp. 96-99, 2004.
70
Proceedings of the Ninth International Workshop on Parsing Technologies (IWPT), pages 133?140,
Vancouver, October 2005. c?2005 Association for Computational Linguistics
Chunk Parsing Revisited
Yoshimasa Tsuruoka
 
and Jun?ichi Tsujii  
 
CREST, JST (Japan Science and Technology Corporation)

Department of Computer Science, University of Tokyo

School of Informatics, University of Manchester

tsuruoka,tsujii  @is.s.u-tokyo.ac.jp
Abstract
Chunk parsing is conceptually appealing
but its performance has not been satis-
factory for practical use. In this pa-
per we show that chunk parsing can
perform significantly better than previ-
ously reported by using a simple sliding-
window method and maximum entropy
classifiers for phrase recognition in each
level of chunking. Experimental results
with the Penn Treebank corpus show that
our chunk parser can give high-precision
parsing outputs with very high speed (14
msec/sentence). We also present a pars-
ing method for searching the best parse by
considering the probabilities output by the
maximum entropy classifiers, and show
that the search method can further im-
prove the parsing accuracy.
1 Introduction
Chunk parsing (Tjong Kim Sang, 2001; Brants,
1999) is a simple parsing strategy both in imple-
mentation and concept. The parser first performs
chunking by identifying base phrases, and convert
the identified phrases to non-terminal symbols. The
parser again performs chunking on the updated se-
quence and convert the newly recognized phrases
into non-terminal symbols. The parser repeats this
procedure until there are no phrases to be chunked.
After finishing these chunking processes, we can
reconstruct the complete parse tree of the sentence
from the chunking results.
Although the conceptual simplicity of chunk pars-
ing is appealing, satisfactory performance for prac-
tical use has not yet been achieved with this pars-
ing strategy. Sang achieved an f-score of 80.49 on
the Penn Treebank by using the IOB tagging method
for each level of chunking (Tjong Kim Sang, 2001).
However, there is a very large gap between their per-
formance and that of widely-used practical parsers
(Charniak, 2000; Collins, 1999).
The performance of chunk parsing is heavily de-
pendent on the performance of phrase recognition in
each level of chunking. We show in this paper that
the chunk parsing strategy is indeed appealing in that
it can give considerably better performance than pre-
viously reported by using a different approach for
phrase recognition and that it enables us to build a
very fast parser that gives high-precision outputs.
This advantage could open up the possibility of
using full parsers for large-scale information extrac-
tion from the Web corpus and real-time information
extraction where the system needs to analyze the
documents provided by the users on run-time.
This paper is organized as follows. Section 2
introduces the overall chunk parsing strategy em-
ployed in this work. Section 3 describes the sliding-
window based method for identifying chunks. Two
filtering methods to reduce the computational cost
are presented in sections 4 and 5. Section 6 explains
the maximum entropy classifier and the feature set.
Section 7 describes methods for searching the best
parse. Experimental results on the Penn Treebank
corpus are given in Section 8. Section 10 offers
some concluding remarks.
133
Estimated  volume  was   a   light  2.4  million  ounces  .
VBN         NN    VBD DT  JJ    CD     CD NNS   .
QPNP
Figure 1: Chunk parsing, the 1st iteration.
volume          was   a   light    million       ounces .
NP             VBD DT  JJ          QP            NNS   .
NP
Figure 2: Chunk parsing, the 2nd iteration.
2 Chunk Parsing
For the overall strategy of chunk parsing, we fol-
low the method proposed by Sang (Tjong Kim Sang,
2001). Figures 1 to 4 show an example of chunk
parsing. In the first iteration, the chunker identifies
two base phrases, (NP Estimated volume) and (QP
2.4 million), and replaces each phrase with its non-
terminal symbol and head. The head word is identi-
fied by using the head-percolation table (Magerman,
1995). In the second iteration, the chunker identifies
(NP a light million ounces) and converts this phrase
into NP. This chunking procedure is repeated until
the whole sentence is chunked at the fourth itera-
tion, and the full parse tree is easily recovered from
the chunking history.
This parsing strategy converts the problem of full
parsing into smaller and simpler problems, namely,
chunking, where we only need to recognize flat
structures (base phrases). Sang used the IOB tag-
ging method proposed by Ramshow(Ramshaw and
Marcus, 1995) and memory-based learning for each
level of chunking and achieved an f-score of 80.49
on the Penn Treebank corpus.
3 Chunking with a sliding-window
approach
The performance of chunk parsing heavily depends
on the performance of each level of chunking. The
popular approach to this shallow parsing is to con-
vert the problem into a tagging task and use a variety
volume          was                    ounces          .
NP             VBD                    NP           .
VP
Figure 3: Chunk parsing, the 3rd iteration.
volume                           was                   .
NP                               VP                .
S
Figure 4: Chunk parsing, the 4th iteration.
of machine learning techniques that have been de-
veloped for sequence labeling problems such as Hid-
den Markov Models, sequential classification with
SVMs (Kudo and Matsumoto, 2001), and Condi-
tional Random Fields (Sha and Pereira, 2003).
One of our claims in this paper is that we should
not convert the chunking problem into a tagging
task. Instead, we use a classical sliding-window
method for chunking, where we consider all sub-
sequences as phrase candidates and classify them
with a machine learning algorithm. Suppose, for ex-
ample, we are about to perform chunking on the se-
quence in Figure 4.
NP-volume VBD-was .-.
We consider the following sub sequences as the
phrase candidates in this level of chunking.
1. (NP-volume) VBD-was .-.
2. NP-volume (VBD-was) .-.
3. NP-volume VBD-was (.-.)
4. (NP-volume VBD-was) .-.
5. NP-volume (VBD-was .-.)
6. (NP-volume VBD-was .-.)
The merit of taking the sliding window approach
is that we can make use of a richer set of features on
recognizing a phrase than in the sequential labeling
134
approach. We can define arbitrary features on the
target candidate (e.g. the whole sequence of non-
terminal symbols of the target) and the surrounding
context, which are, in general, not available in se-
quential labeling approaches.
We should mention here that there are some other
modeling methods for sequence labeling which al-
low us to define arbitrary features on the target
phrase. Semi-markov conditional random fields
(Semi-CRFs) are one of such modeling methods
(Sarawagi and Cohen, 2004). Semi-CRFs could
give better performance than the sliding-window
approach because they can incorporate features on
other phrase candidates on the same level of chunk-
ing. However, they require additional computational
resources for training and parsing, and the use of
Semi-CRFs is left for future work.
The biggest disadvantage of the sliding window
approach is the cost for training and parsing. Since
there are  

 	
 phrase candidates when the
length of the sequence is   , a naive application of
machine learning easily leads to prohibitive con-
sumption of memory and time.
In order to reduce the number of phrase candi-
dates to be considered by machine learning, we in-
troduce two filtering phases into training and pars-
ing. One is done by a rule dictionary. The other is
done by a naive Bayes classifier.
4 Filtering with the CFG Rule Dictionary
We use an idea that is similar to the method pro-
posed by Ratnaparkhi (Ratnaparkhi, 1996) for part-
of-speech tagging. They used a Tag Dictionary, with
which the tagger considers only the tag-word pairs
that appear in the training sentences as the candidate
tags.
A similar method can be used for reducing the
number of phrase candidates. We first construct a
rule dictionary consisting of all the CFG rules used
in the training data. In both training and parsing, we
filter out all the sub-sequences that do not match any
of the entry in the dictionary.
4.1 Normalization
The rules used in the training data do not cover all
the rules in unseen sentences. Therefore, if we take
a naive filtering method using the rule dictionary, we
Original Symbol Normalized Symbol
NNP, NNS, NNPS, PRP NN
RBR, RBS RB
JJR, JJS, PRP$ JJ
VBD, VBZ VBP
: ,
?, ? NULL
Table 1: Normalizing preterminals.
0
2000
4000
6000
8000
10000
12000
14000
16000
0 10000 20000 30000 40000
Si
ze
 o
f R
ul
e 
Di
ct
io
na
ry
Number of Sentences
Original
Normalized
Figure 5: Number of sentences vs the size of the rule
dictionary..
substantially lose recall in parsing unseen data.
To alleviate the problem of the coverage of rules,
we conduct normalization of the rules. We first con-
vert preterminal symbols into equivalent sets using
the conversion table provided in Table 1. This con-
version reduces the sparseness of the rules.
We further normalize the Right-Hand-Side (RHS)
of the rules with the following heuristics.

?X CC X? is converted to ?X?.

?X , X? is converted to ?X?.
Figure 5 shows the effectiveness of this normal-
ization method. The figure illustrates how the num-
ber of rules increases in the rule dictionary as we
add training sentences. Without the normalization,
the number of rules continues to grow rapidly even
when the entire training set is read. The normaliza-
tion methods reduce the growing rate, which con-
siderably alleviates the sparseness problem (i.e. the
problems of unknown rules).
135
5 Filtering with the Naive Bayes classifier
Although the use of the rule dictionary significantly
reduced the number of phrase candidates, we still
found it difficult to train the parser using the entire
training set when we used a rich set of features.
To further reduce the cost required for training
and parsing, we propose to use a naive Bayes classi-
fier for filtering the candidates. A naive Bayes clas-
sifier is simple and requires little storage and com-
putational cost.
We construct a binary naive Bayes classifier for
each phrase type using the entire training data. We
considered the following information as the features.
 The Right-Hand-Side (RHS) of the CFG rule
 The left-adjacent nonterminal symbol.
 The right-adjacent nonterminal symbol.
By assuming the conditional independence
among the features, we can compute the probability
for filtering as follows:
 
 	


 
	 

 


 
	


 
 

 
	 

 
 

 



 
 

 
 

 
 

 



where

is a binary output indicating whether the
candidate is a phrase of the target type or not,

is
the RHS of the CFG rule,

is the symbol on the
left, and

is the symbol on the right. We used
the Laplace smoothing method for computing each
probability. Note that the information about the re-
sult of the rule application, i.e., the LHS symbol, is
considered in this filtering scheme because different
naive Bayes classifiers are used for different LHS
symbols (phrase types).
Table 2 shows the filtering performance in train-
ing with sections 02-21 on the Penn Treebank. We
set the threshold probability for filtering to be 0.0001
for the experiments reported in this paper. The
naive Bayes classifiers effectively reduced the num-
ber of candidates with little positive samples that
were wrongly filtered out.
6 Phrase Recognition with a Maximum
Entropy Classifier
For the candidates which are not filtered out in the
above two phases, we perform classification with
maximum entropy classifiers (Berger et al, 1996).
We construct a binary classifier for each type of
phrases using the entire training set. The training
samples for maximum entropy consist of the phrase
candidates that have not been filtered out by the CFG
rule dictionary and the naive Bayes classifier.
One of the merits of using a maximum entropy
classifier is that we can obtain a probability from
the classifier in each decision. The probability of
each decision represents how likely the candidate is
a correct chunk. We accept a chunk only when the
probability is larger than the predefined threshold.
With this thresholding scheme, we can control the
trade-off between precision and recall by changing
the threshold value.
Regularization is important in maximum entropy
modeling to avoid overfitting to the training data.
For this purpose, we use the maximum entropy mod-
eling with inequality constraints (Kazama and Tsu-
jii, 2003). This modeling has one parameter to
tune as in Gaussian prior modeling. The parame-
ter is called the width factor. We set this parame-
ter to be 1.0 throughout the experiments. For nu-
merical optimization, we used the Limited-Memory
Variable-Metric (LMVM) algorithm (Benson and
More?, 2001).
6.1 Features
Table 3 lists the features used in phrase recognition
with the maximum entropy classifier. Information
about the adjacent non-terminal symbols is impor-
tant. We use unigrams, bigrams, and trigrams of the
adjacent symbols. Head information is also useful.
We use unigrams and bigrams of the neighboring
heads. The RHS of the CFG rule is also informa-
tive. We use the features on RHSs combined with
symbol features.
7 Searching the best parse
7.1 Deterministic parsing
The deterministic version of chunk parsing is
straight-forward. All we need to do is to repeat
chunking until there are no phrases to be chunked.
136
Symbol # candidates # remaining candidates # positives # false negative
ADJP 4,043,409 1,052,983 14,389 53
ADVP 3,459,616 1,159,351 19,765 78
NP 7,122,168 3,935,563 313,042 117
PP 3,889,302 1,181,250 94,568 126
S 3,184,827 1,627,243 95,305 99
VP 4,903,020 2,013,229 145,878 144
Table 2: Effectiveness of the naive Bayes filtering on some representative nonterminals.
Symbol Unigrams     ,    
Symbol Bigrams       ,   	       ,           ,         
Symbol Trigrams       	       ,               ,               ,             
Head Unigrams 
    , 
   
Head Bigrams 
  
   , 
	       , 
   
 
Symbol-Head Unigrams    
  ,    
  ,  

 

CFG Rule TOWARDS DATA AND GOAL ORIENTED ANALYSIS:  
TOOL INTER-OPERABILITY AND COMBINATORIAL 
COMPARISON 
Yoshinobu Kano1      Ngan Nguyen1      Rune S?tre1       Kazuhiro Yoshida1 
Keiichiro Fukamachi1      Yusuke Miyao1       Yoshimasa Tsuruoka3   
Sophia Ananiadou2,3        Jun?ichi Tsujii1,2,3 
 
1Department of Computer Science, University of Tokyo 
Hongo 7-3-1, Bunkyo-ku, Tokyo 113-0033 Tokyo 
 
2School of Computer Science, University of Manchester 
PO Box 88, Sackville St, MANCHESTER M60 1QD, UK 
 
3NaCTeM (National Centre for Text Mining), Manchester Interdisciplinary Biocentre, 
University of Manchester, 131 Princess St, MANCHESTER M1 7DN, UK 
 
{kano,nltngan,satre,kyoshida,keif,yusuke,tsujii} 
@is.s.u-tokyo.ac.jp 
{yoshimasa.tsuruoka,sophia.ananiadou}@manchester.ac.uk 
 
Abstract 
Recently, NLP researches have advanced 
using F-scores, precisions, and recalls with 
gold standard data as evaluation measures. 
However, such evaluations cannot capture 
the different behaviors of varying NLP 
tools or the different behaviors of a NLP 
tool that depends on the data and domain in 
which it works. Because an increasing 
number of tools are available nowadays, it 
has become increasingly important to grasp 
these behavioral differences, in order to 
select a suitable set of tools, which forms a 
complex workflow for a specific purpose. 
In order to observe such differences, we 
need to integrate available combinations of 
tools into a workflow and to compare the 
combinatorial results. Although generic 
frameworks like UIMA (Unstructured 
Information Management Architecture) 
provide interoperability to solve this 
problem, the solution they provide is only 
partial. In order for truly interoperable 
toolkits to become a reality, we also need 
sharable and comparable type systems with 
an automatic combinatorial comparison 
generator, which would allow systematic 
comparisons of available tools. In this 
paper, we describe such an environment, 
which we developed based on UIMA, and 
we show its feasibility through an example 
of a protein-protein interaction (PPI) 
extraction system. 
1 Introduction 
Recently, an increasing number of TM/NLP tools 
such as part-of-speech (POS) taggers (Tsuruoka et 
al., 2005), named entity recognizers (NERs) 
(Settles, 2005) syntactic parsers (Hara et al, 2005) 
and relation or event extractors (ERs) have been 
developed. Nevertheless, it is still very difficult to 
integrate independently developed tools into an 
aggregated application that achieves a specific 
task. The difficulties are caused not only by 
differences in programming platforms and 
different input/output data formats, but also by the 
lack of higher level interoperability among 
modules developed by different groups.  
859
UIMA, Unstructured Information Management 
Architecture (Lally and Ferrucci, 2004), which was 
originally developed by IBM and has recently 
become an open project in OASIS and Apache, 
provides a promising framework for tool 
integration. Although it has a set of useful 
functionalities, UIMA only provides a generic 
framework, thus it requires a user community to 
develop their own platforms with a set of actual 
software modules. A few attempts have already 
been made to establish platforms, e.g. the CMU 
UIMA component repository 1 , GATE 
(Cunningham et al, 2002) with its UIMA 
interoperability layer, etc.  
However, simply wrapping existing modules to 
be UIMA compliant does not offer a complete 
solution. Most of TM/NLP tasks are composite in 
nature, and can only be solved by combining 
several modules. Users need to test a large number 
of combinations of tools in order to pick the most 
suitable combination for their specific task. 
Although types and type systems are the only 
way to represent meanings in the UIMA 
framework, UIMA does not provide any specific 
types, except for a few purely primitive types. In 
this paper, we propose a way to design sharable 
type systems. A sharable type system designed in 
this way can provide the interoperability between 
independently developed tools with fewer losses in 
information, thus allowing for the combinations of 
tools and comparisons on these combinations. 
We show how our automatic comparison 
generator works based on a type system designed in 
that way. Taking the extraction of protein-protein 
                                                 
1 http://uima.lti.cs.cmu.edu/ 
interaction (PPI) as a typical example of a 
composite task, we illustrate how our platform 
helps users to observe the differences between 
tools and to construct a system for their own needs. 
2 Motivation and Background 
2.1 Goal and Data Oriented Evaluation, 
Module Selection and Inter-operability 
There are standard evaluation metrics for NLP 
modules such as precision, recall and F-value. For 
basic tasks like sentence splitting, POS tagging, 
and named-entity recognition, these metrics can be 
estimated using existing gold-standard test sets.  
Conversely, accuracy measurements based on 
the standard test sets are sometimes deceptive, 
since its accuracy may change significantly in 
practice, depending on the types of text and the 
actual tasks at hand. Because these accuracy 
metrics do not take into account the importance of 
the different types of errors to any particular 
application, the practical utility of two systems 
with seemingly similar levels of accuracy may in 
fact differ significantly. To users and developers 
alike, a detailed examination of how systems 
perform (on the text they would like to process) is 
often more important than standard metrics and 
test sets. Naturally, far greater weight is placed in 
measuring the end-to-end performance of a 
composite system than in measuring the 
performance of the individual components. 
In reality, because the selection of modules 
usually affects the performance of the entire 
system, it is crucial to carefully select modules that 
are appropriate for a given task. This is the main 
reason for having a collection of interoperable 
 
 
TOOL-SPECIFIC TYPES
PennPOS 
Penn verb1 ? ?
POS 
tcas.uima.Annotation 
-begin: int  -end: int 
SyntacticAnnotation SemanticAnnotation 
Sentence Phrase Token NamedEntity Relation 
-ent: FSArray<NamedEntity>
POSToken 
-pos: POS 
RichToken 
uima.jcas.cas.TOP 
UnknownPOS 
-base: String 
-posType: String 
ToolAToken
Verb Noun ?.. 
ToolBPOSToken
Protein 
ToolCProtein
ProteinProteinInteraction
ToolDPPI
Figure 1. Part of our type system 
860
modules. We need to show how the ultimate 
performance will be affected by the selection of 
different modules and show the best combination 
of modules in terms of the performance of the 
whole aggregated system for the task at hand. 
 Since the number of possible combinations of 
component modules is typically large, the system 
has to be able to enumerate and execute them 
semi-automatically. This requires a higher level of 
interoperability of individual modules than just 
wrapping them for UIMA.  
2.2 UIMA 
2.2.1 CAS and Type System 
The UIMA framework uses the ?stand-off 
annotation? style (Ferrucci et al, 2006). The raw 
text in a document is kept unchanged during the 
analysis process, and when the processing of the 
text is performed, the result is added as new stand-
off annotations with references to their positions in 
the raw text. A Common Analysis Structure (CAS) 
maintains a set of these annotations, which in itself 
are objects. The annotation objects in a CAS 
belong to types that are defined separately in a 
hierarchical type system. The features of an 
annotation2  object have values that are typed as 
well. 
2.2.2 Component and Capability 
Each UIMA Component has the capability 
property which describes what types of objects the 
component may take as the input and what types of 
objects it produces as the output. For example, a 
named entity recognizer detects named entities in 
                                                 
tools. Types should be defined in a distinct and 
2 In the UIMA framework, Annotation is a base type which 
has begin and end offset values. In this paper we call any 
objects (any subtype of TOP) as annotations. 
the text and outputs annotation objects of the type 
NamedEntity. 
It is possible to deploy any UIMA component as 
a SOAP web service, so that we can combine a 
remote component on a web service with the local 
component freely inside a UIMA-based system.  
3 Integration Platform and Comparators 
3.1 Sharable and Comparable Type System 
Although UIMA provides a set of useful 
functionalities for an integration platform of 
TM/NLP tools, users still have to develop the 
actual platform by using these functionalities 
effectively. There are several decisions for the 
designer to make an integration platform. 
Determining how to use types in UIMA is a 
crucial decision. Our decision is to keep different 
type systems by individual groups as they are, if 
necessary; we require that individual type systems 
have to be related through a sharable type system, 
which our platform defines. Such a shared type 
system can bridge modules with different type 
systems, though the bridging module may lose 
some information during the translation process.  
Whether such a sharable type system can be 
defined or not is dependent on the nature of each 
problem.  For example, a sharable type system for 
POS tags in English can be defined rather easily, 
since most of POS-related modules (such as POS 
taggers, shallow parsers, etc.) more or less follow 
the well established types defined by the Penn 
Treebank (Marcus et al, 1993) tag set. 
Figure 1 shows a part of our sharable type 
system. We deliberately define a highly organized 
type hierarchy as described above.  
Secondly we should consider that the type 
system may be used to compare a similar sort of 
Comparable Tools 
Sentence 
Detector
Deep 
Parser 
Named  
Entity 
Recognizer 
POS 
Tagger 
PPI 
Extractor 
AImed 
Collection 
Reader 
Comparator 
Evaluator 
Tokenizer 
Figure 2. PPI system workflow  
(conceptual) 
Figure 3.  
Basic example pattern
Comparable Tools
OpenNLP 
Sentence 
Detector 
Enju ABNER 
Stepp 
Tagger
UIMA 
Tokenizer
Figure 4.  
Complex tool example 
Comparable Tools 
GENIA 
Tagger 
OpenNLP 
Sentence 
Detector 
Enju NER 
POS 
Tagger
Tokenizer
Figure 5.  
Branch flow pattern 
Comparable Tools
OpenNLP 
S.D. 
UIMA 
Tokenizer
Enju ABNER 
Stepp 
Tagger
GENIA 
S.D. 
861
hierarchical manner. For example, both tokenizers 
and POS taggers output an object of type Token, 
but their roles are different when we assume a 
cascaded pipeline. We defined Token as a 
supertvpe, POSToken as subtypes of Token. Each 
tool should have an individual type to make clear 
which tool generated which instance, because each 
tool may have a slightly different definition. This 
is important because the capabilities are 
represented by these types, and the capabilities are 
the only attributes which are machine readable. 
3.2 General Combinatorial Comparison 
stem is defined in the previously 
tually shows the workflow of our 
wh
 pattern expansion mechanism which 
ge
cases, a single tool can play two or 
m
                                                
Generator 
Even if the type sy
described way, there are still some issues to 
consider when comparing tools. We illustrate these 
issues using the PPI workflow that we utilized in 
our experiments. 
Figure 2 concep
ole PPI system. If we can prepare two or more 
components for some type of the components in 
the workflow (e.g. two sentence detectors and three 
POS taggers), then we can make combinations of 
these tools to form a multiplied number of 
workflow patterns (2x3 = 6 patterns). See Table 1 
for the details of UIMA components used in our 
experiments. 
We made a
nerates possible workflow patterns automatically 
from a user-defined comparable workflow. A 
comparable workflow is a special workflow that 
explicitly specifies which set of components 
should be compared. Then, users just need to group 
comparable components (e.g. ABNER3 and MedT-
NER as a comparable NER group) without making 
any modifications to the original UIMA 
components. This aggregation of comparable 
components is controlled by our custom workflow 
controller.  
In some 
ore roles (e.g. the GENIA Tagger performs 
tokenization, POS tagging, and NER; see Figure 
4). It may be possible to decompose the original 
tool into single roles, but in most cases it is 
difficult and unnatural to decompose such a 
 
ponent requires two or more input 
ty
4 Experiments and Results 
 using our PPI 
e have several 
co
igure 6 show a part of the 
co
Table 2.   
3 In the example figures, ABNER requires Sentence to 
make the explanation clearer, though ABNER does not 
require it in actual usage. 
complex tool. We designed our comparator to 
detect possible input combinations automatically 
by the types of previously generated annotations, 
and the input capability of each posterior 
component. As described in the previous section, 
the component should have appropriate 
capabilities with proper types in order to permit 
this detection.  
When a com
pes (e.g. our PPI extractor requires outputs of a 
deep parser and a protein NER system), there 
could be different components used in the prior 
flow (e.g. OpenNLP and GENIA sentence 
detectors in Figure 5). Our comparator also 
calculates such cases automatically. 
 OO UO GOO U G A
UU 8 89 8
We have performed experiments
extraction system as an example (Kano et al, 
2008). It is similar to our BioCreative PPI system 
(S?tre et al, 2006) but differs in that we have 
deconstructed the original system into seven 
different components (Figure 2).  
As summarized in Table 1, w
mparable components and the AImed corpus as 
the gold standard data. In this case, possible 
combination workflow patterns are POSToken for 
36, PPI for 589, etc.   
Table 2, 3, 4 and F
mparison result screenshots between these 
patterns on 20 articles from the AImed corpus. In 
the tables, abbreviations like ?OOG? stands for a 
workflow of O(Sentence) -> O(Token) - 
Sentence
comparisons (%). 
Table 3. Part of Token
comparisons, 
precision/recall (%).
OOO UOS GOO 
UUO 87/74 81/68 85/68 
GUG 74/65 73/65 78/65 
GGO 92/95 81/84 97/95 
OGO 100/100 89/88 100/94 
G 0 0 - 85
U
 9/75 /75 8/70
GU 89/75 89/75 88/70
GG 92/95 91/95 97/95
OG 
86 - 0 7
A 6 6 60 -
O - 10 10/100 99/99 00/9481 0 7
Table 4. Part of POSToken comparisons, 
precision/recall (%) 
862
G(POSToken), where O stands for OpenNLP, G 
stands for Genia, U stands for UIMA, etc.  
When neither of the compared results include 
th
e comparison on Sentences 
sh
%  
0 
e gold standard data (AImed in this case), the 
comparison results show a similarity of the tools 
for this specific task and data, rather than an 
evaluation. Even if we lack an annotated corpus, it 
is possible to run the tools and compare the results 
in order to understand the characteristics of the 
tools depending on the corpus and the tool 
combinations.  
Although th
ows low scores of similarities, Tokens are 
almost the same; it means that input sentence 
boundaries do not affect tokenizations so much. 
POSToken similarities drop approximately 0-10
100 
  
                      100
Fi  6  NER (Protein) comp rison di
ences in 
5 Conclusion and Future Work 
ponents, 
 design, which the UIMA 
fra
   0  
gure . a stribution of 
precisions (x-axis, %) and recalls (y-axis, %). 
from the similarities in Token; the differ
Token are mainly apostrophes and punctuations; 
POSTokens are different because each POS 
tagger uses a slightly different set of tags: normal 
Penn tagset for Stepp tagger, BioPenn tagset 
(includes new tags for hyphenation) for GENIA 
tagger, and an original apostrophe tag for 
OpenNLP tagger. 
NLP tasks typically consist of many com
and it is necessary to show which set of tools are 
most suitable for each specific task and data. 
Although UIMA provides a general framework 
with much functionality for interoperability, we 
still need to build an environment that enables the 
combinations and comparisons of tools for a 
specific task.  
The type system
mework does not provide, is one of the most 
critical issues on interoperability. We have thus 
proposed a way to design a sharable and 
comparable type system. Such a type system allows 
for the automatic combinations of any UIMA 
compliant components and for the comparisons of 
these combinations, when the components have 
proper capabilities within the type system. We are 
Sentence Token POSToken RichToken Protein Phrase PPI
GENIA Tagger: Trained on the WSJ, GENIA and PennBioIE corpora (POS). Uses Maximum Entropy (Berger 
et al, 1996) classification, trained on JNLPBA (Kim et al, 2004) (NER). Trained on GENIA corpus (Sentence 
Splitter). 
Enju: HPSG parser with predicate argument structures as well as phrase structures. Although trained with Penn 
Treebank, it can compute accurate analyses of biomedical texts owing to its method for domain adaptation (Hara 
et al, 2005). 
STePP Tagger: Based on probabilistic models, tuned to biomedical text trained by WSJ, GENIA (Kim et al, 
2003)  and PennBioIE corpora. 
MedT-NER: Statistical recognizer trained on the JNLPBA data. 
ABNER: From the University of Wisconsin (Settles, 2005), wrapped by the Center for Computational 
Pharmacology at the University of Colorado.  
Akane++: A new version of the AKANE system (Yakushiji, 2006), trained with SVMlight-TK (Joachims, 1999; 
Bunescu and Mooney, 2006; Moschitti, 2006) and the AImed Corpus. 
UIMA Examples: Provided in the Apache UIMA example. Sentence Splitter and Tokenizer. 
OpenNLP Tools: Part of the OpenNLP project (http://opennlp.sourceforge.net/), from Apache UIMA examples. 
AImed Corpus: 225 Medline abstracts with proteins and PPIs annotated (Bunescu and Mooney, 2006).   
Legend:         Input type(s) required for that tool          Input type(s) required optionally          Output type(s)  
Table 1. List of UIMA Components used in our experiment. 
863
preparing to make a portion of the components and 
services described in this paper publicly available 
(http://www-tsujii.is.s.u-tokyo.ac.jp/uima/). 
The final system shows which combination of 
co
or this work includes 
co
cknowledgments 
e wish to thank Dr. Lawrence Hunter?s text 
References 
Vincent J. Della Pietra, and Stephen 
IT 
 Mooney. 
on." Edited 
tcheva, and V. 
ls and 
m Lally, Daniel Gruhl, and Edward 
RC24122. (2006). 
ilistic disambiguation model of an 
t, 
e 
." MIT Press, (1999): 169-
ls 
ser: a tool comparator, using protein-protein 
i. "Introduction to the Bio-Entity 
d 
ics 
 i180-
le Application with the Unstructured Information 
l 43, 
ng a Large Annotated Corpus of 
ractical 
. (2006). 
oko 
 
cally tagging genes, proteins, and other entity 
rsity 
, 
ust Part-of-
tion 
University of Tokyo, (2006).  
mponents has the best score, and also generates 
comparative results. This helps users to grasp the 
characteristics and differences among tools, which 
cannot be easily observed by the widely used F-
score evaluations only. 
Future directions f
mbining the output of several modules of the 
same kind (such as NERs) to obtain better results, 
collecting other tools developed by other groups 
using the sharable type system, making machine 
learning tools UIMA compliant, and making grid 
computing available with UIMA workflows to 
increase the entire performance without modifying 
the original UIMA components. 
 
A
 
W
mining group at the Center for Computational 
Pharmacology for discussing with us and making 
their tools available for this research. This work 
was partially supported by NaCTeM (the UK 
National Centre for Text Mining), Grant-in-Aid for 
Specially Promoted Research (MEXT, Japan) and 
Genome Network Project (MEXT, Japan). 
NaCTeM is jointly funded by 
JISC/BBSRC/EPSRC. 
Berger, Adam L., 
A. Della Pietra. "A maximum entropy approach to 
natural language processing." Comput. Linguist. (M
Press) 22, no. 1 (1996): 39-71. 
Bunescu, Razvan, and Raymond
"Subsequence Kernels for Relation Extracti
by Weiss Y., Scholkopf B. and Platt J., 171-178. 
Cambridge, MA: MIT Press, (2006). 
Cunningham, H., D. Maynard, K. Bon
Tablan. "GATE: A framework and graphical 
development environment for robust NLP too
applications." Proceedings of the 40th Anniversary 
Meeting of the Association for Computational 
Linguistics. (2002). 
Ferrucci, David, Ada
Epstein. "Towards an Interoperability Standard for Text 
and Multi-Modal Analytics." IBM Research Report, 
Hara, Tadayoshi, Yusuke Miyao, and Jun'ichi Tsujii. 
"Adapting a probab
HPSG parser to a new domain." Edited by Dale Rober
Wong Kam-Fai, Su Jian and Yee Oi. Natural Languag
Processing IJCNLP 2005. Jeju Island, Korea: Springer-
Verlag, (2005). 199-210. 
Joachims, Thorsten. "Making large-scale support vector 
machine learning practical
184. 
Kano, Yoshinobu, et al "Filling the gaps between too
and u
interaction as an example." Proceedings of The Pacific 
Symposium on Biocomputing (PSB). Hawaii, USA, To 
appear, (2008). 
Kim, Jin-Dong, Tomoko Ohta, Yoshimasa Tsuruoka, 
and Yuka Tateis
Recognition Task at JNLPBA." Proceedings of the 
International Workshop on Natural Language 
Processing. Geneva, Switzerland, (2004). 70-75. 
Kim, Jin-Dong, Tomoko Ohta, Yuka Teteisi, an
Jun'ichi Tsujii. "GENIA corpus - a semantically 
annotated corpus for bio-textmining." Bioinformat
(Oxford University Press) 19, no. suppl. 1 (2003):
i182. 
Lally, Adam, and David Ferrucci. "Building an 
Examp
Management Architecture." IBM Systems Journa
no. 3 (2004): 455-475. 
Marcus, Mitchell P., Beatrice Santorini, and Mary Ann 
Marcinkiewicz. "Buildi
English: The Penn Treebank." Computational 
Linguistics 19, no. 2 (1993): 313-330. 
Moschitti, Alessandro. "Making Tree Kernels P
for Natural Language Learning." EACL
S?tre, Rune, Kazuhiro Yoshida, Akane Yakushiji, 
Yusuke Miyao, Yuichiroh Matsubayashi, and Tom
Ohta. "AKANE System: Protein-Protein Interaction
Pairs in BioCreAtIvE2 Challenge." Proceedings of the 
Second BioCreative Challenge Evaluation Workshop. 
(2007). 
Settles, B. "ABNER: an open source tool for 
automati
names in text." Bioinformatics (Oxford Unive
Press) 21, no. 14 (2005): 3191-3192. 
Tsuruoka, Yoshimasa, Yuka Tateishi, Jin-Dong Kim
and Tomoko Ohta. "Developing a Rob
Speech Tagger for Biomedical Text." Advances in 
Informatics - 10th Panhellenic Conference on 
Informatics. Volos, Greece, (2005). 382-392. 
Yakushiji, Akane. "Relation Information Extrac
Using Deep Syntactic Analysis." PhD Thesis, 
864
Combining Multiple Models for Speech Information Retrieval 
Muath Alzghool and Diana Inkpen  
School of Information Technology and Engineering 
University of Ottawa 
{alzghool,diana}@ site.uottawa.ca  
Abstract 
In this article we present a method for combining different information retrieval models in order to increase the retrieval performance 
in a Speech Information Retrieval task. The formulas for combining the models are tuned on training data. Then the system is evaluated 
on test data. The task is particularly difficult because the text collection is automatically transcribed spontaneous speech, with many 
recognition errors. Also, the topics are real information needs, difficult to satisfy. Information Retrieval systems are not able to obtain 
good results on this data set, except for the case when manual summaries are included. 
 
1. Introduction  
Conversational speech such as recordings of interviews or 
teleconferences is difficult to search through. The 
transcripts produced with Automatic Speech Recognition 
(ASR) systems tend to contain many recognition errors, 
leading to low Information Retrieval (IR) performance 
(Oard et al, 2007). 
Previous research has explored the idea of combining 
the results of different retrieval strategies; the motivation is 
that each technique will retrieve different sets of relevant 
documents; therefore combining the results could produce 
a better result than any of the individual techniques. We 
propose new data fusion techniques for combining the 
results of different IR models. We applied our data fusion 
techniques to the Mallach collection (Oard et al, 2007) 
used in the Cross-Language Speech Retrieval (CLSR) task 
at Cross-Language Evaluation Forum (CLEF) 2007. The 
Mallach collection comprises 8104 ?documents? which are 
manually-determined topically-coherent segments taken 
from 272 interviews with Holocaust survivors, witnesses 
and rescuers, totalling 589 hours of speech. Figure 1 shows 
the document structure in CLSR test collection, two ASR 
transcripts are available for this data, in this work we use 
the ASRTEXT2004A field provided by IBM research with 
a word error rate of 38%. Additionally, metadata fields for 
each document include: two sets of 20 automatically 
assigned keywords determined using two different kNN 
classifiers (AK1 and AK2), a set of a varying number of 
manually-assigned keywords (MK), and a manual 
3-sentence summary written by an expert in the field.  A set 
of 63 training topics and 33 test topics were generated for 
this task. The topics provided with the collection were 
created in English from actual user requests. Topics were 
structured using the standard TREC format of Title, 
Description and Narrative fields. To enable CL-SR 
experiments the topics were translated into Czech, German, 
French, and Spanish by native speakers; Figure 2 and 3 
show two examples for English and its translation in 
French respectively. Relevance judgments were generated 
using a search-guided procedure and standard pooling 
methods. See (Oard et al, 2004) for full details of the 
collection design.  
We present results on the automatic transcripts for 
English queries and translated queries (cross-language) 
for two combination methods; we also present results 
when manual summaries and manual keywords are 
indexed. 
 
<DOC> 
<DOCNO>VHF[IntCode]-[SegId].[SequenceNum]</DOCNO\> 
<INTERVIEWDATA>Interviewee name(s) and 
birthdate</INTERVIEWDATA> 
<NAME>Full name of every person mentioned</NAME> 
<MANUALKEYWORD>Thesaurus keywords assigned to the 
segment</MANUALKEYWORD> 
<SUMMARY>3-sentence segment summary</SUMMARY> 
<ASRTEXT2004A>ASR transcript produced in 
2004</ASRTEXT2004A> 
<ASRTEXT2006A>ASR transcript produced in 
2006</ASRTEXT2006A> 
<AUTOKEYWORD2004A1>Thesaurus keywords from a kNN 
classifier</AUTOKEYWORD2004A1> 
<AUTOKEYWORD2004A2>Thesaurus keywords from a second 
kNN classifier</AUTOKEYWORD2004A2> 
</DOC> 
Figure 1. Document structure in CL-SR test collection. 
 
<top>  
<num>1159  
<title>Child survivors in Sweden  
<desc>Describe survival mechanisms of children born 
in 1930-1933 who spend the war in concentration 
camps or in hiding and who presently live in Sweden. 
 <narr>The relevant material should describe the 
circumstances and inner resources of the surviving 
children. The relevant material also describes how 
the wartime experience affected their post-war 
adult life. </top> 
Figure 2. Example for English topic in CL-SR test collection. 
 
<top>  
<num>1159  
<title>Les enfants survivants en Su?de  
<desc>Descriptions des m?canismes de survie des 
enfants n?s entre 1930 et 1933 qui ont pass? la 
guerre en camps de concentration ou cach?s et qui 
vivent actuellement en Su?de.  
<narr>? 
</top>  
Figure 3. Example for French topic in CL-SR test collection. 
2. System Description  
Our Cross-Language Information Retrieval systems 
were built with off-the-shelf components. For the retrieval 
part, the SMART (Buckley, Salton, &Allan, 1992; Salton 
&Buckley, 1988) IR system and the Terrier (Amati &Van 
Rijsbergen, 2002; Ounis et al, 2005) IR system were 
tested with many different weighting schemes for 
indexing the collection and the queries.  
SMART was originally developed at Cornell 
University in the 1960s. SMART is based on the vector 
space model of information retrieval. We use the standard 
notation: weighting scheme for the documents, followed 
by dot, followed by the weighting scheme for the queries, 
each term-weighting scheme is described as a 
combination of term frequency, collection frequency, and 
length normalization components where the schemes are 
abbreviated according to its components variations (n no 
normalization, c cosine, t idf, l log, etc.) We used nnn.ntn, 
ntn.ntn, lnn.ntn, ann.ntn, ltn.ntn, atn.ntn, ntn.nnn , 
nnc.ntc, ntc.ntc, ntc.nnc, lnc.ntc, anc.ntc, ltc.ntc, atc.ntc 
weighting schemes (Buckley, Salton, &Allan, 1992; 
Salton &Buckley, 1988);  lnn.ntn performs very well in 
CLEF-CLSR 2005 and 2006 (Alzghool &Inkpen, 2007; 
Inkpen, Alzghool, &Islam, 2006); lnn.ntn means that lnn 
was used for documents and ntn for queries according to 
the following formulas:  
0.1)ln(nln += tfweight        (1) 
tn
Ntfweight logntn ?=     (2)      
where tf denotes the term frequency of a term t in the 
document or query, N denotes the number of documents 
in the collection, and nt denotes the number of documents 
in which the term t occurs.  
Terrier was originally developed at the University of 
Glasgow. It is based on Divergence from Randomness 
models (DFR) where IR is seen as a probabilistic process 
(Amati &Van Rijsbergen, 2002; Ounis et al, 2005). We 
experimented with the In_expC2 (Inverse Expected 
Document Frequency model with Bernoulli after-effect 
and normalization) weighting model, one of Terrier?s 
DFR-based document weighting models.  
Using the In_expC2 model, the relevance score of a 
document d for a query q is given by the formula: 
                  (3) ?
?
=
qt
dtwqtfqdsim ),(.),(
where qtf is the frequency of term t in the query q, and w(t,d) 
is the relevance score of a document d for the query term t, 
given by: 
)
5.0
1log()
)1(
1(),( 2 +
+??+?
+=
e
e
et n
Ntfn
tfnn
Fdtw   (4) 
where 
-F is the term frequency of t in the whole collection. 
-N is the number of document in the whole collection.  
-nt is the document frequency of t. 
-ne is given by ))
1
(1( Fte N
n
Nn
???=  (5) 
- tfne is the normalized within-document frequency of the 
term t in the document d. It is given by the normalization 2 
(Amati &Van Rijsbergen, 2002; Ounis et al, 2005): 
)_1(log
l
lavgctftfn ee ?+?=     (6) 
where c is a parameter, tf is the within-document 
frequency of the term t in the document d, l is the 
document length, and avg_l is the average document 
length in the whole collection. 
We estimated the parameter c of the Terrier's 
normalization 2 formula by running some experiments on 
the training data, to get the best values for c depending on 
the topic fields used. We obtained the following values: 
c=0.75 for queries using the Title only, c=1 for queries 
using the Title and Description fields, and c=1 for queries 
using the Title, Description, and Narrative fields. We select 
the c value that has a best MAP score according to the 
training data. 
For translating the queries from French and Spanish 
into English, several free online machine translation tools 
were used. The idea behind using multiple translations is 
that they might provide more variety of words and 
phrases, therefore improving the retrieval performance. 
Seven online MT systems (Inkpen, Alzghool, &Islam, 
2006) were used for translating from Spanish and from 
French into English. We combined the outputs of the MT 
systems by simply concatenating all the translations. All 
seven translations of a title made the title of the translated 
query; the same was done for the description and narrative 
fields.  
We propose two methods for combining IR models. We 
use the sum of normalized weighted similarity scores of 15 
different IR schemes as shown in the following formulas: 
 
 ?
?
?+=
schemsIRi
iMAPr NormSimiWiWFusion )]()([1
34      (7) 
?
?
?=
schemsIRi
iMAPr NormSimiWiWFusion )(*)(2
34      (8)                         
where Wr(i) and WMAP(i) are experimentally determined 
weights based on the recall (the number of relevant 
documents retrieved) and precision (MAP score) values for 
each IR scheme computed on the training data. For 
example, suppose that two retrieval runs r1 and r2 give 0.3 
and 0.2 (respectively) as  MAP scores on training data; we 
normalize these scores by dividing them by the maximum 
MAP value: then WMAP(r1) is 1 and WMAP(r2) is 0.66 (then 
we compute the power 3 of these weights, so that one 
weight stays 1 and the other one decreases; we chose power 
3 for MAP score and power 4 for recall, because the MAP 
is more important than the recall). We hope that when we 
multiply the similarity values with the weights and take the 
summation over all the runs, the performance of the 
combined run will improve. NormSimi is the normalized 
similarity for each IR scheme. We did the normalization by 
dividing the similarity by the maximum similarity in the 
run. The normalization is necessary because different 
weighting schemes will generate different range of 
similarity values, so a normalization method should 
applied to each run.  Our method is differed than the work 
done by Fox and Shaw in (1994), and Lee in ( 1995); they 
combined the results by taking the summation of the 
similarity scores without giving any weight to each run. In 
our work we weight each run according to the precision 
and recall on the training data.  
3. Experimental Results 
We applied the data fusion methods described in section 2 
to 14 runs produced by SMART and one run produced by 
Terrier.  Performance results for each single run and fused 
runs are presented in Table 1, in which % change is given 
with respect to the run providing better effectiveness in 
each combination on the training data. The Manual 
English column represents the results when only the 
manual keywords and the manual summaries were used 
for indexing the documents using English topics, the 
Auto-English column represents the results when 
automatic transcripts are indexed from the documents, for 
English topics. For cross-languages experiments the 
results are represented in the columns Auto-French, and 
Auto-Spanish, when using the combined translations 
produced by the seven online MT tools, from French and 
Spanish into English. Since the result of combined 
translation for each language was better than when using 
individual translations from each MT tool on the training 
data (Inkpen, Alzghool, &Islam, 2006), we used only the 
combined translations in our experiments. 
Data fusion helps to improve the performance (MAP 
score) on the test data. The best improvement using data 
fusion (Fusion1) was on the French cross-language 
experiments with 21.7%, which is statistically significant 
while on monolingual the improvement was only 6.5% 
which is not significant. We computed these 
improvements relative to the results of the best 
single-model run, as measured on the training data. This 
supports our claim that data fusion improves the recall by 
bringing some new documents that were not retrieved by 
all the runs. On the training data, the Fusion2 method 
gives better results than Fusion1 for all cases except on 
Manual English, but on the test data Fusion1 is better than 
Fusion2. In general, the data fusion seems to help, 
because the performance on the test data in not always 
good for weighting schemes that obtain good results on 
the training data, but combining models allows the 
best-performing weighting schemes to be taken into 
consideration. 
The retrieval results for the translations from French 
were very close to the monolingual English results, 
especially on the training data, but on the test data the 
difference was significantly worse. For Spanish, the 
difference was significantly worse on the training data, 
but not on the test data.  
Experiments on manual keywords and manual 
summaries available in the test collection showed high 
improvements, the MAP score jumped from 0.0855 to 
0.2761 on the test data. 
4. Conclusion 
We experimented with two different systems: Terrier 
and SMART, with combining the various weighting 
schemes for indexing the document and query terms. We 
proposed two methods to combine different weighting 
scheme from different systems, based on weighted 
summation of normalized similarity measures; the weight 
for each scheme was based on the relative precision and 
recall on the training data. Data fusion helps to improve 
the retrieval significantly for some experiments 
(Auto-French) and for other not significantly (Manual 
English). Our result on automatic transcripts for English 
queries (the required run for the CLSR task at CLEF 
2007), obtained a MAP score of 0.0855. This result was 
significantly better than the other 4 systems that 
participated in the CLSR task at CLEF 2007(Pecina et al, 
2007). 
In future work we plan to investigate more methods of 
data fusion (to apply a normalization scheme scalable to 
unseen data), removing or correcting some of the speech 
recognition errors in the ASR content words, and to use 
speech lattices for indexing.  
5. References 
 
Alzghool, M. & Inkpen, D. (2007). Experiments for the 
cross language speech retrieval task at CLEF 2006. In 
C. Peters, (Ed.), Evaluation of multilingual and 
multi-modal information retrieval (Vol. 4730/2007, 
pp. 778-785). Springer. 
Amati, G. & Van Rijsbergen, C. J. (2002). Probabilistic 
models of information retrieval based on measuring 
the divergence from randomness (Vol. 20). ACM,  
New York. 
Buckley, C., Salton, G., & Allan, J. (1992). Automatic 
retrieval with locality information using smart. In 
Text retrieval conferenc (TREC-1) (pp. 59-72). 
Inkpen, D., Alzghool, M., & Islam, A. (2006). Using 
various indexing schemes and multiple translations in 
the CL-SR task at CLEF 2005. In C. Peters, (Ed.), 
Accessing multilingual information repositories 
(Vol. 4022/2006, pp. 760-768). Springer,  London. 
Lee, J. H. (1995). Combining multiple evidence from 
different properties of weighting schemes, 
Proceedings of the 18th annual international ACM 
SIGIR conference on Research and development in 
information retrieval. ACM, Seattle, Washington, 
United States. 
Oard, D. W., Soergel, D., Doermann, D., Huang, X., 
Murray, G. C., Wang, J., Ramabhadran, B., Franz, 
M., & Gustman, S. (2004). Building an information 
retrieval test collection for spontaneous 
conversational speech, Proceedings of the 27th 
annual international ACM SIGIR conference on 
Research and development in information retrieval. 
ACM, Sheffield, United Kingdom. 
Oard, D. W., Wang, J., Jones, G. J. F., White, R. W., 
Pecina, P., Soergel, D., Huang, X., & Shafran, I. 
(2007). Overview of the CLEF-2006 cross-language 
speech retrieval track. In C. Peters, (Ed.), Evaluation 
of multilingual and multi-modal information 
retrieval (Vol. 4730/2007, pp. 744-758). Springer,  
Heidelberg. 
Ounis, I., Amati, G., Plachouras, V., He, B., Macdonald, 
C., & Johnson, D. (2005). Terrier information 
retrieval platform In Advances in information 
retrieval (Vol. 3408/2005, pp. 517-519). Springer,  
Heidelberg. 
Pecina, P., Hoffmannov?a, P., Jones, G. J. F., Zhang, Y., 
& Oard, D. W. (2007). Overview of the CLEF-2007 
cross language speech retrieval track, Working Notes 
of the CLEF- 2007 Evaluation, . CLEF2007, 
Budapest-Hungary. 
Salton, G. & Buckley, C. (1988). Term weighting 
approaches in automatic text retrieval. Information 
Processing and Management, 24(5): 513-523. 
Shaw, J. A. & Fox, E. A. (1994). Combination of multiple 
searches. In Third text retrieval conference (trec-3) 
(pp. 105-108). National Institute of Standards and 
Technology Special Publication. 
 
 
Manual English Auto-English Auto-French Auto-Spanish Weighting 
scheme Training Test Training Test Training Test Training Test 
nnc.ntc 0.2546 0.2293 0.0888 0.0819 0.0792 0.055 0.0593 0.0614 
ntc.ntc 0.2592 0.2332 0.0892 0.0794 0.0841 0.0519 0.0663 0.0545 
lnc.ntc 0.2710 0.2363 0.0898 0.0791 0.0858 0.0576 0.0652 0.0604 
ntc.nnc 0.2344 0.2172 0.0858 0.0769 0.0745 0.0466 0.0585 0.062 
anc.ntc 0.2759 0.2343 0.0723 0.0623 0.0664 0.0376 0.0518 0.0398 
ltc.ntc 0.2639 0.2273 0.0794 0.0623 0.0754 0.0449 0.0596 0.0428 
atc.ntc 0.2606 0.2184 0.0592 0.0477 0.0525 0.0287 0.0437 0.0304 
nnn.ntn 0.2476 0.2228 0.0900 0.0852 0.0799 0.0503 0.0599 0.061 
ntn.ntn 0.2738 0.2369 0.0933 0.0795 0.0843 0.0507 0.0691 0.0578 
lnn.ntn 0.2858 0.245 0.0969 0.0799 0.0905 0.0566 0.0701 0.0589 
ntn.nnn 0.2476 0.2228 0.0900 0.0852 0.0799 0.0503 0.0599 0.061 
ann.ntn 0.2903 0.2441 0.0750 0.0670 0.0743 0.038 0.057 0.0383 
ltn.ntn 0.2870 0.2435 0.0799 0.0655 0.0871 0.0522 0.0701 0.0501 
atn.ntn 0.2843 0.2364 0.0620 0.0546 0.0722 0.0347 0.0586 0.0355 
In_expC2 0.3177 0.2737 0.0885 0.0744 0.0908 0.0487 0.0747 0.0614 
Fusion 1 0.3208 0.2761 0.0969 0.0855 0.0912 0.0622 0.0731 0.0682 
% change 1.0% 0.9% 0.0% 6.5% 0.4% 21.7% -2.2% 10.0% 
Fusion 2 0.3182 0.2741 0.0975 0.0842 0.0942 0.0602 0.0752 0.0619 
% change 0.2% 0.1% 0.6% 5.1% 3.6% 19.1% 0.7% 0.8% 
Table 1. Results (MAP scores) for 15 weighting schemes using Smart and Terrier (the In_expC2 model), and the results 
for the two Fusions Methods. In bold are the best scores for the 15 single runs on the training data and the corresponding 
results on the test data.  
 
Weighting 
scheme 
Manual English Auto-English Auto- French Auto- Spanish 
 Train. Test Train. Test Train. Test Train. Test 
nnc. ntc 2371 1827 1726 1306 1687 1122 1562 1178 
ntc.ntc 2402 1857 1675 1278 1589 1074 1466 1155 
lnc.ntc 2402 1840 1649 1301 1628 1111 1532 1196 
ntc.nnc 2354 1810 1709 1287 1662 1121 1564 1182 
anc.ntc 2405 1858 1567 1192 1482 1036 1360 1074 
ltc.ntc 2401 1864 1571 1211 1455 1046 1384 1097 
atc.ntc 2387 1858 1435 1081 1361 945 1255 1011 
nnn.ntn 2370 1823 1740 1321 1748 1158 1643 1190 
ntn.ntn 2432 1863 1709 1314 1627 1093 1502 1174 
lnn.ntn 2414 1846 1681 1325 1652 1130 1546 1194 
ntn.nnn 2370 1823 1740 1321 1748 1158 1643 1190 
ann.ntn 2427 1859 1577 1198 1473 1027 1365 1060 
ltn.ntn 2433 1876 1582 1215 1478 1070 1408 1134 
atn.ntn 2442 1859 1455 1101 1390 975 1297 1037 
In_expC2 2638 1823 1624 1286 1676 1061 1631 1172 
Fusion 1 2645 1832 1745 1334 1759 1147 1645 1219 
% change 0.3% 0.5 % 0.3% 1.0% 0.6% -1.0% 0.1% 2.4% 
Fusion 2 2647 1823 1727 1337 1736 1098 1631 1172 
% change 0.3% 0.0% 0.8% 1.2% -0.7% -5.5% -0.7% -1.5% 
Table 2. Results (number of relevant documents retrieved) for 15 weighting schemes using Terrier and SMART, and the 
results for the Fusions Methods. In bold are the best scores for the 15 single runs on training data and the corresponding 
test data. 
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 790?798,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Fast Full Parsing by Linear-Chain Conditional Random Fields
Yoshimasa Tsuruoka?? Jun?ichi Tsujii??? Sophia Ananiadou??
? School of Computer Science, University of Manchester, UK
? National Centre for Text Mining (NaCTeM), UK
? Department of Computer Science, University of Tokyo, Japan
{yoshimasa.tsuruoka,j.tsujii,sophia.ananiadou}@manchester.ac.uk
Abstract
This paper presents a chunking-based dis-
criminative approach to full parsing. We
convert the task of full parsing into a series
of chunking tasks and apply a conditional
random field (CRF) model to each level
of chunking. The probability of an en-
tire parse tree is computed as the product
of the probabilities of individual chunk-
ing results. The parsing is performed in a
bottom-up manner and the best derivation
is efficiently obtained by using a depth-
first search algorithm. Experimental re-
sults demonstrate that this simple parsing
framework produces a fast and reasonably
accurate parser.
1 Introduction
Full parsing analyzes the phrase structure of a sen-
tence and provides useful input for many kinds
of high-level natural language processing such as
summarization (Knight and Marcu, 2000), pro-
noun resolution (Yang et al, 2006), and infor-
mation extraction (Miyao et al, 2008). One of
the major obstacles that discourage the use of full
parsing in large-scale natural language process-
ing applications is its computational cost. For ex-
ample, the MEDLINE corpus, a collection of ab-
stracts of biomedical papers, consists of 70 million
sentences and would require more than two years
of processing time if the parser needs one second
to process a sentence.
Generative models based on lexicalized PCFGs
enjoyed great success as the machine learning
framework for full parsing (Collins, 1999; Char-
niak, 2000), but recently discriminative models
attract more attention due to their superior accu-
racy (Charniak and Johnson, 2005; Huang, 2008)
and adaptability to new grammars and languages
(Buchholz and Marsi, 2006).
A traditional approach to discriminative full
parsing is to convert a full parsing task into a series
of classification problems. Ratnaparkhi (1997)
performs full parsing in a bottom-up and left-to-
right manner and uses a maximum entropy clas-
sifier to make decisions to construct individual
phrases. Sagae and Lavie (2006) use the shift-
reduce parsing framework and a maximum en-
tropy model for local classification to decide pars-
ing actions. These approaches are often called
history-based approaches.
A more recent approach to discriminative full
parsing is to treat the task as a single structured
prediction problem. Finkel et al (2008) incor-
porated rich local features into a tree CRF model
and built a competitive parser. Huang (2008) pro-
posed to use a parse forest to incorporate non-local
features. They used a perceptron algorithm to op-
timize the weights of the features and achieved
state-of-the-art accuracy. Petrov and Klein (2008)
introduced latent variables in tree CRFs and pro-
posed a caching mechanism to speed up the com-
putation.
In general, the latter whole-sentence ap-
proaches give better accuracy than history-based
approaches because they can better trade off deci-
sions made in different parts in a parse tree. How-
ever, the whole-sentence approaches tend to re-
quire a large computational cost both in training
and parsing. In contrast, history-based approaches
are less computationally intensive and usually pro-
duce fast parsers.
In this paper, we present a history-based parser
using CRFs, by treating the task of full parsing as
a series of chunking problems where it recognizes
chunks in a flat input sequence. We use the linear-
790
Estimated  volume  was   a   light  2.4  million  ounces  .
VBN         NN    VBD DT  JJ    CD     CD NNS   .
QPNP
Figure 1: Chunking, the first (base) level.
volume          was   a   light    million       ounces .
NP             VBD DT  JJ          QP            NNS   .
NP
Figure 2: Chunking, the 2nd level.
chain CRF model to perform chunking.
Although our parsing model falls into the cat-
egory of history-based approaches, it is one step
closer to the whole-sentence approaches because
the parser uses a whole-sequence model (i.e.
CRFs) for individual chunking tasks. In other
words, our parser could be located somewhere
between traditional history-based approaches and
whole-sentence approaches. One of our motiva-
tions for this work was that our parsing model
may achieve a better balance between accuracy
and speed than existing parsers.
It is also worth mentioning that our approach is
similar in spirit to supertagging for parsing with
lexicalized grammar formalisms such as CCG and
HPSG (Clark and Curran, 2004; Ninomiya et al,
2006), in which significant speed-ups for parsing
time are achieved.
In this paper, we show that our approach is in-
deed appealing in that the parser runs very fast
and gives competitive accuracy. We evaluate our
parser on the standard data set for parsing exper-
iments (i.e. the Penn Treebank) and compare it
with existing approaches to full parsing.
This paper is organized as follows. Section 2
presents the overall chunk parsing strategy. Sec-
tion 3 describes the CRF model used to perform
individual chunking steps. Section 4 describes the
depth-first algorithm for finding the best derivation
of a parse tree. The part-of-speech tagger used in
the parser is described in section 5. Experimen-
tal results on the Penn Treebank corpus are pro-
vided in Section 6. Section 7 discusses possible
improvements and extensions of our work. Sec-
tion 8 offers some concluding remarks.
volume          was                    ounces          .
NP             VBD                    NP           .
VP
Figure 3: Chunking, the 3rd level.
volume                           was                   .
NP                               VP                .
S
Figure 4: Chunking, the 4th level.
2 Full Parsing by Chunking
This section describes the parsing framework em-
ployed in this work.
The parsing process is conceptually very sim-
ple. The parser first performs chunking by iden-
tifying base phrases, and converts the identified
phrases to non-terminal symbols. It then performs
chunking for the updated sequence and converts
the newly recognized phrases into non-terminal
symbols. The parser repeats this process until the
whole sequence is chunked as a sentence
Figures 1 to 4 show an example of a parsing pro-
cess by this framework. In the first (base) level,
the chunker identifies two base phrases, (NP Es-
timated volume) and (QP 2.4 million), and re-
places each phrase with its non-terminal symbol
and head1. In the second level, the chunker iden-
tifies a noun phrase, (NP a light million ounces),
and converts it into NP. This process is repeated
until the whole sentence is chunked at the fourth
level. The full parse tree is recovered from the
chunking history in a straightforward way.
This idea of converting full parsing into a se-
ries of chunking tasks is not new by any means?
the history of this kind of approach dates back to
1950s (Joshi and Hopely, 1996). More recently,
Brants (1999) used a cascaded Markov model to
parse German text. Tjong Kim Sang (2001) used
the IOB tagging method to represent chunks and
memory-based learning, and achieved an f-score
of 80.49 on the WSJ corpus. Tsuruoka and Tsu-
jii (2005) improved upon their approach by using
1The head word is identified by using the head-
percolation table (Magerman, 1995).
791
 0
 1000
 2000
 3000
 4000
 5000
 0  5  10  15  20  25  30
# 
se
nt
en
ce
s
Height
Figure 5: Distribution of tree height in WSJ sec-
tions 2-21.
a maximum entropy classifier and achieved an f-
score of 85.9. However, there is still a large gap
between the accuracy of chunking-based parsers
and that of widely-used practical parsers such as
Collins parser and Charniak parser (Collins, 1999;
Charniak, 2000).
2.1 Heights of Trees
A natural question about this parsing framework is
how many levels of chunking are usually needed to
parse a sentence. We examined the distribution of
the heights of the trees in sections 2-21 of the Wall
Street Journal (WSJ) corpus. The result is shown
in Figure 5. Most of the sentences have less than
20 levels. The average was 10.0, which means we
need to perform, on average, 10 chunking tasks to
obtain a full parse tree for a sentence if the parsing
is performed in a deterministic manner.
3 Chunking with CRFs
The accuracy of chunk parsing is highly depen-
dent on the accuracy of each level of chunking.
This section describes our approach to the chunk-
ing task.
A common approach to the chunking problem
is to convert the problem into a sequence tagging
task by using the ?BIO? (B for beginning, I for
inside, and O for outside) representation. For ex-
ample, the chunking process given in Figure 1 is
expressed as the following BIO sequences.
B-NP I-NP O O O B-QP I-QP O O
This representation enables us to use the linear-
chain CRF model to perform chunking, since the
task is simply assigning appropriate labels to a se-
quence.
3.1 Linear Chain CRFs
A linear chain CRF defines a single log-linear
probabilistic distribution over all possible tag se-
quences y for the input sequence x:
p(y|x) = 1Z(x) exp
T
?
t=1
K
?
k=1
?kfk(t, yt, yt?1,x),
where fk(t, yt, yt?1,x) is typically a binary func-
tion indicating the presence of feature k, ?k is the
weight of the feature, and Z(X) is a normalization
function:
Z(x) =
?
y
exp
T
?
t=1
K
?
k=1
?kfk(t, yt, yt?1,x).
This model allows us to define features on states
and edges combined with surface observations.
The weights of the features are determined in
such a way that they maximize the conditional log-
likelihood of the training data:
L? =
N
?
i=1
log p(y(i)|x(i)) + R(?),
where R(?) is introduced for the purpose of regu-
larization which prevents the model from overfit-
ting the training data. The L1 or L2 norm is com-
monly used in statistical natural language process-
ing (Gao et al, 2007). We used L1-regularization,
which is defined as
R(?) = 1C
K
?
k=1
|?k|,
where C is the meta-parameter that controls the
degree of regularization. We used the OWL-QN
algorithm (Andrew and Gao, 2007) to obtain the
parameters that maximize the L1-regularized con-
ditional log-likelihood.
3.2 Features
Table 1 shows the features used in chunking for
the base level. Since the task is basically identical
to shallow parsing by CRFs, we follow the feature
sets used in the previous work by Sha and Pereira
(2003). We use unigrams, bigrams, and trigrams
of part-of-speech (POS) tags and words.
The difference between our CRF chunker and
that in (Sha and Pereira, 2003) is that we could
not use second-order CRF models, hence we could
not use trigram features on the BIO states. We
792
Symbol Unigrams s?2, s?1, s0, s+1, s+2
Symbol Bigrams s?2s?1, s?1s0, s0s+1, s+1s+2
Symbol Trigrams s?3s?2s?1, s?2s?1s0, s?1s0s+1, s0s+1s+2, s+1s+2s+3
Word Unigrams h?2, h?1, h0, h+1, h+2
Word Bigrams h?2h?1, h?1h0, h0h+1, h+1h+2
Word Trigrams h?1h0h+1
Table 1: Feature templates used in the base level chunking. s represents a terminal symbol (i.e. POS tag)
and the subscript represents a relative position. h represents a word.
found that using second order CRFs in our task
was very difficult because of the computational
cost. Recall that the computational cost for CRFs
is quadratic to the number of possible states. In
our task, we need to consider the states for all non-
terminal symbols, whereas their work is only con-
cerned with noun phrases.
Table 2 shows feature templates used in the non-
base levels of chunking. In the non-base levels of
chunking, we can use a richer set of features than
the base-level chunking because the chunker has
access to the information about the partial trees
that have been already created. In addition to the
features listed in Table 1, the chunker looks into
the daughters of the current non-terminal sym-
bol and use them as features. It also uses the
words and POS tags around the edges of the re-
gion covered by the current non-terminal symbol.
We also added a special feature to better capture
PP-attachment. The chunker looks at the head of
the second daughter of the prepositional phrase to
incorporate the semantic head of the phrase.
4 Searching for the Best Parse
The probability for an entire parse tree is com-
puted as the product of the probabilities output by
the individual CRF chunkers:
score =
h
?
i=0
p(yi|xi), (1)
where i is the level of chunking and h is the height
of the tree. The task of full parsing is then to
choose the series of chunking results that maxi-
mizes this probability.
It should be noted that there are cases where
different derivations (chunking histories) lead to
the same parse tree (i.e. phrase structure). Strictly
speaking, therefore, what we describe here as the
probability of a parse tree is actually the proba-
bility of a single derivation. The probabilities of
the derivations should then be marginalized over
to produce the probability of a parse tree, but in
this paper we ignore this effect and simply focus
only on the best derivation.
We use a depth-first search algorithm to find the
highest probability derivation. Figure 6 shows the
algorithm in pseudo-code. The parsing process is
implemented with a recursive function. In each
level of chunking, the recursive function first in-
vokes a CRF chunker to obtain chunking hypothe-
ses for the given sequence. For each hypothesis
whose probability is high enough to have possibil-
ity of constituting the best derivation, the function
calls itself with the sequence updated by the hy-
pothesis. The parsing process is performed in a
bottom up manner and this recursive process ter-
minates if the whole sequence is chunked as a sen-
tence.
To extract multiple chunking hypotheses from
the CRF chunker, we use a branch-and-bound
algorithm rather than the A* search algorithm,
which is perhaps more commonly used in previous
studies. We do not give pseudo code, but the ba-
sic idea is as follows. It first performs the forward
Viterbi algorithm to obtain the best sequence, stor-
ing the upper bounds that are used for pruning in
branch-and-bound. It then performs a branch-and-
bound algorithm in a backward manner to retrieve
possible candidate sequences whose probabilities
are greater than the given threshold. Unlike A*
search, this method is memory efficient because it
is performed in a depth-first manner and does not
require priority queues for keeping uncompleted
hypotheses.
It is straightforward to introduce beam search in
this search algorithm?we simply limit the num-
ber of hypotheses generated by the CRF chunker.
We examine how the width of the beam affects the
parsing performance in the experiments.
793
Symbol Unigrams s?2, s?1, s0, s+1, s+2
Symbol Bigrams s?2s?1, s?1s0, s0s+1, s+1s+2
Symbol Trigrams s?3s?2s?1, s?2s?1s0, s?1s0s+1, s0s+1s+2, s+1s+2s+3
Head Unigrams h?2, h?1, h0, h+1, h+2
Head Bigrams h?2h?1, h?1h0, h0h+1, h+1h+2
Head Trigrams h?1h0h+1
Symbol & Daughters s0d01, ... s0d0m
Symbol & Word/POS context s0wj?1, s0pj?1, s0wk+1 , s0pk+1
Symbol & Words on the edges s0wj , s0wk
Freshness whether s0 has been created in the level just below
PP-attachment h?1h0m02 (only when s0 = PP)
Table 2: Feature templates used in the upper level chunking. s represents a non-terminal symbol. h
represents a head percolated from the bottom for each symbol. d0i is the ith daughter of s0. wj is the
first word in the range covered by s0. wj?1 is the word preceding wj . wk is the last word in the range
covered by s0. wk+1 is the word following wk. p represents POS tags. m02 represents the head of the
second daughter of s0.
Word Unigram w?2, w?1, w0, w+1, wi+2
Word Bigram w?1w0, w0w+1, w?1w+1
Prefix, Suffix prefixes of w0
suffixes of w0
(up to length 10)
Character features w0 has a hyphen
w0 has a number
w0 has a capital letter
w0 is all capital
Normalized word N(w0)
Table 3: Feature templates used in the POS tagger.
w represents a word and the subscript represents a
relative position.
5 Part-of-Speech Tagging
We use the CRF model also for POS tagging.
The CRF-based POS tagger is incorporated in the
parser in exactly the same way as the other lay-
ers of chunking. In other words, the POS tagging
process is treated like the bottom layer of chunk-
ing, so the parser considers multiple probabilistic
hypotheses output by the tagger in the search al-
gorithm described in the previous section.
5.1 Features
Table 3 shows the feature templates used in the
POS tagger. Most of them are standard features
commonly used in POS tagging for English. We
used unigrams and bigrams of neighboring words,
prefixes and suffixes of the current word, and some
characteristics of the word. We also normalized
the current word by lowering capital letters and
converting all the numerals into ?#?, and used the
normalized word as a feature.
6 Experiments
We ran parsing experiments using the Wall Street
Journal corpus. Sections 2-21 were used as the
training data. Section 22 was used as the devel-
opment data, with which we tuned the feature set
and parameters for learning and parsing. Section
23 was reserved for the final accuracy report.
The training data for the CRF chunkers were
created by converting each parse tree in the train-
ing data into a list of chunking sequences like
the ones presented in Figures 1 to 4. We trained
three CRF models, i.e., the POS tagging model,
the base chunking model, and the non-base chunk-
ing model. The training took about two days on a
single CPU.
We used the evalb script provided by Sekine and
Collins for evaluating the labeled recall/precision
of the parser outputs2. All experiments were car-
ried out on a server with 2.2 GHz AMD Opteron
processors and 16GB memory.
6.1 Chunking Performance
First, we describe the accuracy of individual
chunking processes. Table 4 shows the results
for the ten most frequently occurring symbols on
the development data. Noun phrases (NP) are the
2The script is available at http://nlp.cs.nyu.edu/evalb/. We
used the parameter file ?COLLINS.prm?.
794
1: procedure PARSESENTENCE(x)
2: PARSE(x, 1, 0)
3:
4: function PARSE(x, p, q)
5: if x is chunked as a complete sentence
6: return p
7: H ? PERFORMCHUNKING(x, q/p)
8: for h ? H in descending order of their
probabilities do
9: r ? p? h.probability
10: if r > q then
11: x? ? UPDATESEQUENCE(x, h)
12: s? PARSE(x?, r, q)
13: if s > q then
14: q ? s
15: return q
16:
17: function PERFORMCHUNKING(x, t)
18: perform chunking with a CRF chunker and
19: return a set of chunking hypotheses whose
20: probabilities are greater than t.
21:
22: function UPDATESEQUENCE(x, h)
23: update sequence x according to chunking
24: hypothesis h and return the updated
25: sequence.
Figure 6: Searching for the best parse with a
depth-first search algorithm. This pseudo-code il-
lustrates how to find the highest probability parse,
but in the real implementation, the function needs
to keep track of chunking histories as well as prob-
abilities.
most common symbol and consist of 55% of all
phrases. The accuracy of noun phrases recognition
was relatively high, but it may be useful to design
special features for this particular type of phrase,
considering the dominance of noun phrases in the
corpus. Although not directly comparable, Sha
and Pereira (2003) report almost the same level
of accuracy (94.38%) on noun phrase recognition,
using a much smaller training set. We attribute
their superior performance mainly to the use of
second-order features on state transitions. Table 4
also suggests that adverb phrases (ADVP) and ad-
jective phrases (ADJP) are more difficult to recog-
nize than other types of phrases, which coincides
with the result reported in (Collins, 1999).
It should be noted that the performance reported
in this table was evaluated using the gold standard
sequences as the input to the CRF chunkers. In the
Symbol # Samples Recall Prec. F-score
NP 317,597 94.79 94.16 94.47
VP 76,281 91.46 91.98 91.72
PP 66,979 92.84 92.61 92.72
S 33,739 91.48 90.64 91.06
ADVP 21,686 84.25 85.86 85.05
ADJP 14,422 77.27 78.46 77.86
QP 14,308 89.43 91.16 90.28
SBAR 11,603 96.42 96.97 96.69
WHNP 8,827 95.54 97.50 96.51
PRT 3,391 95.72 90.52 93.05
: : : : :
all 579,253 92.63 92.62 92.63
Table 4: Chunking performance (section 22, all
sentences).
Beam Recall Prec. F-score Time (sec)
1 86.72 87.83 87.27 16
2 88.50 88.85 88.67 41
3 88.69 89.08 88.88 61
4 88.72 89.13 88.92 92
5 88.73 89.14 88.93 119
10 88.68 89.19 88.93 179
Table 5: Beam width and parsing performance
(section 22, all sentences).
real parsing process, the chunkers have to use the
output from the previous (one level below) chun-
ker, so the quality of the input is not as good as
that used in this evaluation.
6.2 Parsing Performance
Next, we present the actual parsing performance.
The first set of experiments concerns the relation-
ship between the width of beam and the parsing
performance. Table 5 shows the results obtained
on the development data. We varied the width of
the beam from 1 to 10. The beam width of 1 cor-
responds to deterministic parsing. Somewhat un-
expectedly, the parsing accuracy did not drop sig-
nificantly even when we reduced the beam width
to a very small number such as 2 or 3.
One of the interesting findings was that re-
call scores were consistently lower than precision
scores throughout all experiments. A possible rea-
son is that, since the score of a parse is defined
as the product of all chunking probabilities, the
parser could prefer a parse tree that consists of
a small number of chunk layers. This may stem
795
from the history-based model?s inability of prop-
erly trading off decisions made by different chun-
kers.
Overall, the parsing speed was very high. The
deterministic version (beam width = 1) parsed
1700 sentences in 16 seconds, which means that
the parser needed only 10 msec to parse one sen-
tence. The parsing speed decreases as we increase
the beam width.
The parser was also memory efficient. Thanks
to L1 regularization, the training process did not
result in many non-zero feature weights. The num-
bers of non-zero weight features were 58,505 (for
the base chunker), 263,889 (for the non-base chun-
ker), and 42,201 (for the POS tagger). The parser
required only 14MB of memory to run.
There was little accuracy difference between the
beam width of 4 and 5, so we adopted the beam
width of 4 for the final accuracy report on the test
data.
6.3 Comparison with Previous Work
Table 6 shows the performance of our parser on
the test data and summarizes the results of previ-
ous work. Our parser achieved an f-score of 88.4
on the test data, which is comparable to the accu-
racy achieved by recent discriminative approaches
such as Finkel et al (2008) and Petrov & Klein
(2008), but is not as high as the state-of-the-art
accuracy achieved by the parsers that can incor-
porate global features such as Huang (2008) and
Charniak & Johnson (2005). Our parser was more
accurate than traditional history-based approaches
such as Sagae & Lavie (2006) and Ratnaparkhi
(1997), and was significantly better than previous
cascaded chunking approaches such as Tsuruoka
& Tsujii (2005) and Tjong Kim Sang (2001).
Although the comparison presented in the table
is not perfectly fair because of the differences in
hardware platforms, the results show that our pars-
ing model is a promising addition to the parsing
frameworks for building a fast and accurate parser.
7 Discussion
One of the obvious ways to improve the accuracy
of our parser is to improve the accuracy of in-
dividual CRF models. As mentioned earlier, we
were not able to use second-order features on state
transitions, which would have been very useful,
due to the problem of computational cost. Incre-
mental feature selection methods such as grafting
(Perkins et al, 2003) may help us to incorporate
such higher-order features, but the problem of de-
creased efficiency of dynamic programming in the
CRF would probably need to be addressed.
In this work, we treated the chunking problem
as a sequence labeling problem by using the BIO
representation for the chunks. However, semi-
Markov conditional random fields (semi-CRFs)
can directly handle the chunking problem by
considering all possible combinations of subse-
quences of arbitrary length (Sarawagi and Cohen,
2004). Semi-CRFs allow one to use a richer set
of features than CRFs, so the use of semi-CRFs
in our parsing framework should lead to improved
accuracy. Moreover, semi-CRFs would allow us to
incorporate some useful restrictions in producing
chunking hypotheses. For example, we could nat-
urally incorporate the restriction that every chunk
has to contain at least one symbol that has just
been created in the previous level3. It is hard for
the normal CRF model to incorporate such restric-
tions.
Introducing latent variables into the CRF model
may be another promising approach. This is the
main idea of Petrov and Klein (2008), which sig-
nificantly improved parsing accuracy.
A totally different approach to improving the
accuracy of our parser is to use the idea of ?self-
training? described in (McClosky et al, 2006).
The basic idea is to create a larger set of training
data by applying an accurate parser (e.g. rerank-
ing parser) to a large amount of raw text. We can
then use the automatically created treebank as the
additional training data for our parser. This ap-
proach suggests that accurate (but slow) parsers
and fast (but not-so-accurate) parsers can actually
help each other.
Also, since it is not difficult to extend our parser
to produce N-best parsing hypotheses, one could
build a fast reranking parser by using the parser as
the base (hypotheses generating) parser.
8 Conclusion
Although the idea of treating full parsing as a se-
ries of chunking problems has a long history, there
has not been a competitive parser based on this
parsing framework. In this paper, we have demon-
strated that the framework actually enables us to
3For example, the sequence VBD DT JJ in Figure 2 can-
not be a chunk in the current level because it would have been
already chunked in the previous level if it were.
796
Recall Precision F-score Time (min)
This work (deterministic) 86.3 87.5 86.9 0.5
This work (search, beam width = 4) 88.2 88.7 88.4 1.7
Huang (2008) 91.7 Unk
Finkel et al (2008) 87.8 88.2 88.0 >250*
Petrov & Klein (2008) 88.3 3*
Sagae & Lavie (2006) 87.8 88.1 87.9 17
Charniak & Johnson (2005) 90.6 91.3 91.0 Unk
Tsuruoka & Tsujii (2005) 85.0 86.8 85.9 2
Collins (1999) 88.1 88.3 88.2 39**
Tjong Kim Sang (2001) 78.7 82.3 80.5 Unk
Charniak (2000) 89.6 89.5 89.5 23**
Ratnaparkhi (1997) 86.3 87.5 86.9 Unk
Table 6: Parsing performance on section 23 (all sentences). * estimated from the parsing time on the
training data. ** reported in (Sagae and Lavie, 2006) where Pentium 4 3.2GHz was used to run the
parsers.
build a competitive parser if we use CRF mod-
els for each level of chunking and a depth-first
search algorithm to search for the highest proba-
bility parse.
Like other discriminative learning approaches,
one of the advantages of our parser is its general-
ity. The design of our parser is very generic, and
the features used in our parser are not particularly
specific to the Penn Treebank. We expect it to be
straightforward to adapt the parser to other projec-
tive grammars and languages.
This parsing framework should be useful when
one needs to process a large amount of text or
when real time processing is required, in which
the parsing speed is of top priority. In the deter-
ministic setting, our parser only needed about 10
msec to parse a sentence.
Acknowledgments
This work described in this paper has been
funded by the Biotechnology and Biological Sci-
ences Research Council (BBSRC; BB/E004431/1)
and the European BOOTStrep project (FP6 -
028099). The research team is hosted by the
JISC/BBSRC/EPSRC sponsored National Centre
for Text Mining.
References
Galen Andrew and Jianfeng Gao. 2007. Scalable train-
ing of L1-regularized log-linear models. In Pro-
ceedings of ICML, pages 33?40.
Thorsten Brants. 1999. Cascaded markov models. In
Proceedings of EACL.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of CoNLL-X, pages 149?164.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and maxent discriminative
reranking. In Proceedings of ACL, pages 173?180.
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of NAACL 2000,
pages 132?139.
Stephen Clark and James R. Curran. 2004. The impor-
tance of supertagging for wide-coverage CCG pars-
ing. In Proceedings of COLING 2004, pages 282?
288.
Michael Collins. 1999. Head-Driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania.
Jenny Rose Finkel, Alex Kleeman, and Christopher D.
Manning. 2008. Efficient, feature-based, condi-
tional random field parsing. In Proceedings of ACL-
08:HLT, pages 959?967.
Jianfeng Gao, Galen Andrew, Mark Johnson, and
Kristina Toutanova. 2007. A comparative study of
parameter estimation methods for statistical natural
language processing. In Proceedings of ACL, pages
824?831.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of
ACL-08:HLT, pages 586?594.
Aravind K. Joshi and Phil Hopely. 1996. A parser
from antiquity. Natural Language Engineering,
2(4):291?294.
797
Kevin Knight and Daniel Marcu. 2000. Statistics-
based summarization - step one: Sentence compres-
sion. In Proceedings of AAAI/IAAI, pages 703?710.
David M. Magerman. 1995. Statistical decision-tree
models for parsing. In Proceedings of ACL, pages
276?283.
David McClosky, Eugene Charniak, and Mark John-
son. 2006. Effective self-training for parsing. In
Proceedings of HLT-NAACL.
Yusuke Miyao, Rune Saetre, Kenji Sage, Takuya Mat-
suzaki, and Jun?ichi Tsujii. 2008. Task-oriented
evaluation of syntactic parsers and their representa-
tions. In Proceedings of ACL-08:HLT, pages 46?54.
Takashi Ninomiya, Takuya Matsuzaki, Yoshimasa Tsu-
ruoka, Yusuke Miyao, and Jun?ichi Tsujii. 2006.
Extremely lexicalized models for accurate and fast
HPSG parsing. In Proceedings of EMNLP 2006,
pages 155?163.
Simon Perkins, Kevin Lacker, and James Theiler.
2003. Grafting: fast, incremental feature selection
by gradient descent in function space. The Journal
of Machine Learning Research, 3:1333?1356.
Slav Petrov and Dan Klein. 2008. Discriminative
log-linear grammars with latent variables. In Ad-
vances in Neural Information Processing Systems 20
(NIPS), pages 1153?1160.
Adwait Ratnaparkhi. 1997. A linear observed time sta-
tistical parser based on maximum entropy models.
In Proceedings of EMNLP 1997, pages 1?10.
Kenji Sagae and Alon Lavie. 2006. A best-first proba-
bilistic shift-reduce parser. In Proceedings of COL-
ING/ACL, pages 691?698.
Sunita Sarawagi and William W. Cohen. 2004. Semi-
markov conditional random fields for information
extraction. In Proceedings of NIPS.
Fei Sha and Fernando Pereira. 2003. Shallow parsing
with conditional random fields. In Proceedings of
HLT-NAACL.
Erik Tjong Kim Sang. 2001. Transforming a chunker
to a parser. In J. Veenstra W. Daelemans, K. Sima?an
and J. Zavrel, editors, Computational Linguistics in
the Netherlands 2000, pages 177?188. Rodopi.
Yoshimasa Tsuruoka and Jun?ichi Tsujii. 2005. Chunk
parsing revisited. In Proceedings of IWPT, pages
133?140.
Xiaofeng Yang, Jian Su, and Chew Lim Tan. 2006.
Kernel-based pronoun resolution with structured
syntactic features. In Proceedings of COLING/ACL,
pages 41?48.
798
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 477?485,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Stochastic Gradient Descent Training for
L1-regularized Log-linear Models with Cumulative Penalty
Yoshimasa Tsuruoka?? Jun?ichi Tsujii??? Sophia Ananiadou??
? School of Computer Science, University of Manchester, UK
? National Centre for Text Mining (NaCTeM), UK
? Department of Computer Science, University of Tokyo, Japan
{yoshimasa.tsuruoka,j.tsujii,sophia.ananiadou}@manchester.ac.uk
Abstract
Stochastic gradient descent (SGD) uses
approximate gradients estimated from
subsets of the training data and updates
the parameters in an online fashion. This
learning framework is attractive because
it often requires much less training time
in practice than batch training algorithms.
However, L1-regularization, which is be-
coming popular in natural language pro-
cessing because of its ability to pro-
duce compact models, cannot be effi-
ciently applied in SGD training, due to
the large dimensions of feature vectors
and the fluctuations of approximate gra-
dients. We present a simple method to
solve these problems by penalizing the
weights according to cumulative values for
L1 penalty. We evaluate the effectiveness
of our method in three applications: text
chunking, named entity recognition, and
part-of-speech tagging. Experimental re-
sults demonstrate that our method can pro-
duce compact and accurate models much
more quickly than a state-of-the-art quasi-
Newton method for L1-regularized log-
linear models.
1 Introduction
Log-linear models (a.k.a maximum entropy mod-
els) are one of the most widely-used probabilistic
models in the field of natural language process-
ing (NLP). The applications range from simple
classification tasks such as text classification and
history-based tagging (Ratnaparkhi, 1996) to more
complex structured prediction tasks such as part-
of-speech (POS) tagging (Lafferty et al, 2001),
syntactic parsing (Clark and Curran, 2004) and se-
mantic role labeling (Toutanova et al, 2005). Log-
linear models have a major advantage over other
discriminative machine learning models such as
support vector machines?their probabilistic out-
put allows the information on the confidence of
the decision to be used by other components in the
text processing pipeline.
The training of log-liner models is typically per-
formed based on the maximum likelihood crite-
rion, which aims to obtain the weights of the fea-
tures that maximize the conditional likelihood of
the training data. In maximum likelihood training,
regularization is normally needed to prevent the
model from overfitting the training data,
The two most common regularization methods
are called L1 and L2 regularization. L1 regular-
ization penalizes the weight vector for its L1-norm
(i.e. the sum of the absolute values of the weights),
whereas L2 regularization uses its L2-norm. There
is usually not a considerable difference between
the two methods in terms of the accuracy of the
resulting model (Gao et al, 2007), but L1 regu-
larization has a significant advantage in practice.
Because many of the weights of the features be-
come zero as a result of L1-regularized training,
the size of the model can be much smaller than that
produced by L2-regularization. Compact models
require less space on memory and storage, and en-
able the application to start up quickly. These mer-
its can be of vital importance when the application
is deployed in resource-tight environments such as
cell-phones.
A common way to train a large-scale L1-
regularized model is to use a quasi-Newton
method. Kazama and Tsujii (2003) describe a
method for training a L1-regularized log-linear
model with a bound constrained version of the
BFGS algorithm (Nocedal, 1980). Andrew and
Gao (2007) present an algorithm called Orthant-
Wise Limited-memory Quasi-Newton (OWL-
QN), which can work on the BFGS algorithm
without bound constraints and achieve faster con-
vergence.
477
An alternative approach to training a log-linear
model is to use stochastic gradient descent (SGD)
methods. SGD uses approximate gradients esti-
mated from subsets of the training data and up-
dates the weights of the features in an online
fashion?the weights are updated much more fre-
quently than batch training algorithms. This learn-
ing framework is attracting attention because it of-
ten requires much less training time in practice
than batch training algorithms, especially when
the training data is large and redundant. SGD was
recently used for NLP tasks including machine
translation (Tillmann and Zhang, 2006) and syn-
tactic parsing (Smith and Eisner, 2008; Finkel et
al., 2008). Also, SGD is very easy to implement
because it does not need to use the Hessian infor-
mation on the objective function. The implemen-
tation could be as simple as the perceptron algo-
rithm.
Although SGD is a very attractive learning
framework, the direct application of L1 regular-
ization in this learning framework does not result
in efficient training. The first problem is the inef-
ficiency of applying the L1 penalty to the weights
of all features. In NLP applications, the dimen-
sion of the feature space tends to be very large?it
can easily become several millions, so the appli-
cation of L1 penalty to all features significantly
slows down the weight updating process. The sec-
ond problem is that the naive application of L1
penalty in SGD does not always lead to compact
models, because the approximate gradient used at
each update is very noisy, so the weights of the
features can be easily moved away from zero by
those fluctuations.
In this paper, we present a simple method for
solving these two problems in SGD learning. The
main idea is to keep track of the total penalty and
the penalty that has been applied to each weight,
so that the L1 penalty is applied based on the dif-
ference between those cumulative values. That
way, the application of L1 penalty is needed only
for the features that are used in the current sample,
and also the effect of noisy gradient is smoothed
away.
We evaluate the effectiveness of our method
by using linear-chain conditional random fields
(CRFs) and three traditional NLP tasks, namely,
text chunking (shallow parsing), named entity
recognition, and POS tagging. We show that our
enhanced SGD learning method can produce com-
pact and accurate models much more quickly than
the OWL-QN algorithm.
This paper is organized as follows. Section 2
provides a general description of log-linear mod-
els used in NLP. Section 3 describes our stochastic
gradient descent method for L1-regularized log-
linear models. Experimental results are presented
in Section 4. Some related work is discussed in
Section 5. Section 6 gives some concluding re-
marks.
2 Log-Linear Models
In this section, we briefly describe log-linear mod-
els used in NLP tasks and L1 regularization.
A log-linear model defines the following prob-
abilistic distribution over possible structure y for
input x:
p(y|x) = 1Z(x) exp
?
i
wifi(y,x),
where fi(y,x) is a function indicating the occur-
rence of feature i, wi is the weight of the feature,
and Z(x) is a partition (normalization) function:
Z(x) =
?
y
exp
?
i
wifi(y,x).
If the structure is a sequence, the model is called
a linear-chain CRF model, and the marginal prob-
abilities of the features and the partition function
can be efficiently computed by using the forward-
backward algorithm. The model is used for a va-
riety of sequence labeling tasks such as POS tag-
ging, chunking, and named entity recognition.
If the structure is a tree, the model is called a
tree CRF model, and the marginal probabilities
can be computed by using the inside-outside algo-
rithm. The model can be used for tasks like syn-
tactic parsing (Finkel et al, 2008) and semantic
role labeling (Cohn and Blunsom, 2005).
2.1 Training
The weights of the features in a log-linear model
are optimized in such a way that they maximize
the regularized conditional log-likelihood of the
training data:
Lw =
N
?
j=1
log p(yj |xj ;w)?R(w), (1)
where N is the number of training samples, yj is
the correct output for input xj , and R(w) is the
478
regularization term which prevents the model from
overfitting the training data. In the case of L1 reg-
ularization, the term is defined as:
R(w) = C
?
i
|wi|,
where C is the meta-parameter that controls the
degree of regularization, which is usually tuned by
cross-validation or using the heldout data.
In what follows, we denote by L(j,w)
the conditional log-likelihood of each sample
log p(yj |xj ;w). Equation 1 is rewritten as:
Lw =
N
?
j=1
L(j,w)? C
?
i
|wi|. (2)
3 Stochastic Gradient Descent
SGD uses a small randomly-selected subset of the
training samples to approximate the gradient of
the objective function given by Equation 2. The
number of training samples used for this approx-
imation is called the batch size. When the batch
size is N , the SGD training simply translates into
gradient descent (hence is very slow to converge).
By using a small batch size, one can update the
parameters more frequently than gradient descent
and speed up the convergence. The extreme case
is a batch size of 1, and it gives the maximum
frequency of updates and leads to a very simple
perceptron-like algorithm, which we adopt in this
work.1
Apart from using a single training sample to
approximate the gradient, the optimization proce-
dure is the same as simple gradient descent,2 so
the weights of the features are updated at training
sample j as follows:
wk+1 = wk + ?k
?
?w (L(j,w)?
C
N
?
i
|wi|),
where k is the iteration counter and ?k is the learn-
ing rate, which is normally designed to decrease
as the iteration proceeds. The actual learning rate
scheduling methods used in our experiments are
described later in Section 3.3.
1In the actual implementation, we randomly shuffled the
training samples at the beginning of each pass, and then
picked them up sequentially.
2What we actually do here is gradient ascent, but we stick
to the term ?gradient descent?.
3.1 L1 regularization
The update equation for the weight of each feature
i is as follows:
wik+1 = wik + ?k
?
?wi
(L(j,w)? CN |wi|).
The difficulty with L1 regularization is that the
last term on the right-hand side of the above equa-
tion is not differentiable when the weight is zero.
One straightforward solution to this problem is to
consider a subgradient at zero and use the follow-
ing update equation:
wik+1 = wik + ?k
?L(j,w)
?wi
? CN ?ksign(w
k
i ),
where sign(x) = 1 if x > 0, sign(x) = ?1 if x <
0, and sign(x) = 0 if x = 0. In this paper, we call
this weight updating method ?SGD-L1 (Naive)?.
This naive method has two serious problems.
The first problem is that, at each update, we need
to perform the application of L1 penalty to all fea-
tures, including the features that are not used in
the current training sample. Since the dimension
of the feature space can be very large, it can sig-
nificantly slow down the weight update process.
The second problem is that it does not produce
a compact model, i.e. most of the weights of the
features do not become zero as a result of train-
ing. Note that the weight of a feature does not be-
come zero unless it happens to fall on zero exactly,
which rarely happens in practice.
Carpenter (2008) describes an alternative ap-
proach. The weight updating process is divided
into two steps. First, the weight is updated with-
out considering the L1 penalty term. Then, the
L1 penalty is applied to the weight to the extent
that it does not change its sign. In other words,
the weight is clipped when it crosses zero. Their
weight update procedure is as follows:
wk+
1
2
i = wki + ?k
?L(j,w)
?wi
?
?
?
?
w=wk
,
if wk+
1
2
i > 0 then
wk+1i = max(0, w
k+ 12
i ?
C
N ?k),
else if wk+
1
2
i < 0 then
wk+1i = min(0, w
k+ 12
i +
C
N ?k).
In this paper, we call this update method ?SGD-
L1 (Clipping)?. It should be noted that this method
479
-0.1
-0.05
 0
 0.05
 0.1
 0  1000  2000  3000  4000  5000  6000
W
ei
gh
t
Updates
Figure 1: An example of weight updates.
is actually a special case of the FOLOS algorithm
(Duchi and Singer, 2008) and the truncated gradi-
ent method (Langford et al, 2009).
The obvious advantage of using this method is
that we can expect many of the weights of the
features to become zero during training. Another
merit is that it allows us to perform the applica-
tion of L1 penalty in a lazy fashion, so that we
do not need to update the weights of the features
that are not used in the current sample, which leads
to much faster training when the dimension of the
feature space is large. See the aforementioned pa-
pers for the details. In this paper, we call this effi-
cient implementation ?SGD-L1 (Clipping + Lazy-
Update)?.
3.2 L1 regularization with cumulative
penalty
Unfortunately, the clipping-at-zero approach does
not solve all problems. Still, we often end up with
many features whose weights are not zero. Re-
call that the gradient used in SGD is a crude ap-
proximation to the true gradient and is very noisy.
The weight of a feature is, therefore, easily moved
away from zero when the feature is used in the
current sample.
Figure 1 gives an illustrative example in which
the weight of a feature fails to become zero. The
figure shows how the weight of a feature changes
during training. The weight goes up sharply when
it is used in the sample and then is pulled back
toward zero gradually by the L1 penalty. There-
fore, the weight fails to become zero if the feature
is used toward the end of training, which is the
case in this example. Note that the weight would
become zero if the true (fluctuationless) gradient
were used?at each update the weight would go
up a little and be pulled back to zero straightaway.
Here, we present a different strategy for apply-
ing the L1 penalty to the weights of the features.
The key idea is to smooth out the effect of fluctu-
ating gradients by considering the cumulative ef-
fects from L1 penalty.
Let uk be the absolute value of the total L1-
penalty that each weight could have received up
to the point. Since the absolute value of the L1
penalty does not depend on the weight and we are
using the same regularization constant C for all
weights, it is simply accumulated as:
uk =
C
N
k
?
t=1
?t. (3)
At each training sample, we update the weights
of the features that are used in the sample as fol-
lows:
wk+
1
2
i = wki + ?k
?L(j,w)
?wi
?
?
?
?
w=wk
,
if wk+
1
2
i > 0 then
wk+1i = max(0, w
k+ 12
i ? (uk + qk?1i )),
else if wk+
1
2
i < 0 then
wk+1i = min(0, w
k+ 12
i + (uk ? qk?1i )),
where qki is the total L1-penalty that wi has actu-
ally received up to the point:
qki =
k
?
t=1
(wt+1i ? w
t+ 12
i ). (4)
This weight updating method penalizes the
weight according to the difference between uk and
qk?1i . In effect, it forces the weight to receive the
total L1 penalty that would have been applied if
the weight had been updated by the true gradients,
assuming that the current weight vector resides in
the same orthant as the true weight vector.
It should be noted that this method is basi-
cally equivalent to a ?SGD-L1 (Clipping + Lazy-
Update)? method if we were able to use the true
gradients instead of the stochastic gradients.
In this paper, we call this weight updating
method ?SGD-L1 (Cumulative)?. The implemen-
tation of this method is very simple. Figure 2
shows the whole SGD training algorithm with this
strategy in pseudo-code.
480
1: procedure TRAIN(C)
2: u? 0
3: Initialize wi and qi with zero for all i
4: for k = 0 to MaxIterations
5: ? ? LEARNINGRATE(k)
6: u? u + ?C/N
7: Select sample j randomly
8: UPDATEWEIGHTS(j)
9:
10: procedure UPDATEWEIGHTS(j)
11: for i ? features used in sample j
12: wi ? wi + ? ?L(j,w)?wi
13: APPLYPENALTY(i)
14:
15: procedure APPLYPENALTY(i)
16: z ? wi
17: if wi > 0 then
18: wi ? max(0, wi ? (u + qi))
19: else if wi < 0 then
20: wi ? min(0, wi + (u? qi))
21: qi ? qi + (wi ? z)
22:
Figure 2: Stochastic gradient descent training with
cumulative L1 penalty. z is a temporary variable.
3.3 Learning Rate
The scheduling of learning rates often has a major
impact on the convergence speed in SGD training.
A typical choice of learning rate scheduling can
be found in (Collins et al, 2008):
?k =
?0
1 + k/N , (5)
where ?0 is a constant. Although this scheduling
guarantees ultimate convergence, the actual speed
of convergence can be poor in practice (Darken
and Moody, 1990).
In this work, we also tested simple exponential
decay:
?k = ?0??k/N , (6)
where ? is a constant. In our experiments, we
found this scheduling more practical than that
given in Equation 5. This is mainly because ex-
ponential decay sweeps the range of learning rates
more smoothly?the learning rate given in Equa-
tion 5 drops too fast at the beginning and too
slowly at the end.
It should be noted that exponential decay is not
a good choice from a theoretical point of view, be-
cause it does not satisfy one of the necessary con-
ditions for convergence?the sum of the learning
rates must diverge to infinity (Spall, 2005). How-
ever, this is probably not a big issue for practition-
ers because normally the training has to be termi-
nated at a certain number of iterations in practice.3
4 Experiments
We evaluate the effectiveness our training algo-
rithm using linear-chain CRF models and three
NLP tasks: text chunking, named entity recogni-
tion, and POS tagging.
To compare our algorithm with the state-of-the-
art, we present the performance of the OWL-QN
algorithm on the same data. We used the publicly
available OWL-QN optimizer developed by An-
drew and Gao.4 The meta-parameters for learning
were left unchanged from the default settings of
the software: the convergence tolerance was 1e-4;
and the L-BFGS memory parameter was 10.
4.1 Text Chunking
The first set of experiments used the text chunk-
ing data set provided for the CoNLL 2000 shared
task.5 The training data consists of 8,936 sen-
tences in which each token is annotated with the
?IOB? tags representing text chunks such as noun
and verb phrases. We separated 1,000 sentences
from the training data and used them as the held-
out data. The test data provided by the shared task
was used only for the final accuracy report.
The features used in this experiment were uni-
grams and bigrams of neighboring words, and un-
igrams, bigrams and trigrams of neighboring POS
tags.
To avoid giving any advantage to our SGD al-
gorithms over the OWL-QN algorithm in terms of
the accuracy of the resulting model, the OWL-QN
algorithm was used when tuning the regularization
parameter C. The tuning was performed in such a
way that it maximized the likelihood of the heldout
data. The learning rate parameters for SGD were
then tuned in such a way that they maximized the
value of the objective function in 30 passes. We
first determined ?0 by testing 1.0, 0.5, 0.2, and 0.1.
We then determined ? by testing 0.9, 0.85, and 0.8
with the fixed ?0.
3This issue could also be sidestepped by, for example,
adding a small O(1/k) term to the learning rate.
4Available from the original developers? websites:
http://research.microsoft.com/en-us/people/galena/ or
http://research.microsoft.com/en-us/um/people/jfgao/
5http://www.cnts.ua.ac.be/conll2000/chunking/
481
Passes Lw/N # Features Time (sec) F-score
OWL-QN 160 -1.583 18,109 598 93.62
SGD-L1 (Naive) 30 -1.671 455,651 1,117 93.64
SGD-L1 (Clipping + Lazy-Update) 30 -1.671 87,792 144 93.65
SGD-L1 (Cumulative) 30 -1.653 28,189 149 93.68
SGD-L1 (Cumulative + Exponential-Decay) 30 -1.622 23,584 148 93.66
Table 1: CoNLL-2000 Chunking task. Training time and accuracy of the trained model on the test data.
-2.4
-2.2
-2
-1.8
-1.6
 0  10  20  30  40  50
O
bje
cti
ve
 fu
nc
tio
n
Passes
OWL-QN
SGD-L1 (Clipping)
SGD-L1 (Cumulative)
SGD-L1 (Cumulative + ED)
Figure 3: CoNLL 2000 chunking task: Objective
 0
 50000
 100000
 150000
 200000
 0  10  20  30  40  50
# 
Ac
tiv
e 
fe
at
ur
es
Passes
OWL-QN
SGD-L1 (Clipping)
SGD-L1 (Cumulative)
SGD-L1 (Cumulative + ED)
Figure 4: CoNLL 2000 chunking task: Number of
active features.
Figures 3 and 4 show the training process of
the model. Each figure contains four curves repre-
senting the results of the OWL-QN algorithm and
three SGD-based algorithms. ?SGD-L1 (Cumu-
lative + ED)? represents the results of our cumu-
lative penalty-based method that uses exponential
decay (ED) for learning rate scheduling.
Figure 3 shows how the value of the objec-
tive function changed as the training proceeded.
SGD-based algorithms show much faster conver-
gence than the OWL-QN algorithm. Notice also
that ?SGD-L1 (Cumulative)? improves the objec-
tive slightly faster than ?SGD-L1 (Clipping)?. The
result of ?SGD-L1 (Naive)? is not shown in this
figure, but the curve was almost identical to that
of ?SGD-L1 (Clipping)?.
Figure 4 shows the numbers of active features
(the features whose weight are not zero). It is
clearly seen that the clipping-at-zero approach
fails to reduce the number of active features, while
our algorithms succeeded in reducing the number
of active features to the same level as OWL-QN.
We then trained the models using the whole
training data (including the heldout data) and eval-
uated the accuracy of the chunker on the test data.
The number of passes performed over the train-
ing data in SGD was set to 30. The results are
shown in Table 1. The second column shows the
number of passes performed in the training. The
third column shows the final value of the objective
function per sample. The fourth column shows
the number of resulting active features. The fifth
column show the training time. The last column
shows the f-score (harmonic mean of recall and
precision) of the chunking results. There was no
significant difference between the models in terms
of accuracy. The naive SGD training took much
longer than OWL-QN because of the overhead of
applying L1 penalty to all dimensions.
Our SGD algorithms finished training in 150
seconds on Xeon 2.13GHz processors. The
CRF++ version 0.50, a popular CRF library de-
veloped by Taku Kudo,6 is reported to take 4,021
seconds on Xeon 3.0GHz processors to train the
model using a richer feature set.7 CRFsuite ver-
sion 0.4, a much faster library for CRFs, is re-
ported to take 382 seconds on Xeon 3.0GHz, using
the same feature set as ours.8 Their library uses the
OWL-QN algorithm for optimization. Although
direct comparison of training times is not impor-
6http://crfpp.sourceforge.net/
7http://www.chokkan.org/software/crfsuite/benchmark.html
8ditto
482
tant due to the differences in implementation and
hardware platforms, these results demonstrate that
our algorithm can actually result in a very fast im-
plementation of a CRF trainer.
4.2 Named Entity Recognition
The second set of experiments used the named
entity recognition data set provided for the
BioNLP/NLPBA 2004 shared task (Kim et al,
2004).9 The training data consist of 18,546 sen-
tences in which each token is annotated with the
?IOB? tags representing biomedical named enti-
ties such as the names of proteins and RNAs.
The training and test data were preprocessed
by the GENIA tagger,10 which provided POS tags
and chunk tags. We did not use any information on
the named entity tags output by the GENIA tagger.
For the features, we used unigrams of neighboring
chunk tags, substrings (shorter than 10 characters)
of the current word, and the shape of the word (e.g.
?IL-2? is converted into ?AA-#?), on top of the
features used in the text chunking experiments.
The results are shown in Figure 5 and Table
2. The trend in the results is the same as that of
the text chunking task: our SGD algorithms show
much faster convergence than the OWL-QN algo-
rithm and produce compact models.
Okanohara et al (2006) report an f-score of
71.48 on the same data, using semi-Markov CRFs.
4.3 Part-Of-Speech Tagging
The third set of experiments used the POS tag-
ging data in the Penn Treebank (Marcus et al,
1994). Following (Collins, 2002), we used sec-
tions 0-18 of the Wall Street Journal (WSJ) corpus
for training, sections 19-21 for development, and
sections 22-24 for final evaluation. The POS tags
were extracted from the parse trees in the corpus.
All experiments for this work, including the tun-
ing of features and parameters for regularization,
were carried out using the training and develop-
ment sets. The test set was used only for the final
accuracy report.
It should be noted that training a CRF-based
POS tagger using the whole WSJ corpus is not a
trivial task and was once even deemed impractical
in previous studies. For example, Wellner and Vi-
lain (2006) abandoned maximum likelihood train-
9The data is available for download at http://www-
tsujii.is.s.u-tokyo.ac.jp/GENIA/ERtask/report.html
10http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/tagger/
-3.8
-3.6
-3.4
-3.2
-3
-2.8
-2.6
-2.4
-2.2
 0  10  20  30  40  50
O
bje
cti
ve
 fu
nc
tio
n
Passes
OWL-QN
SGD-L1 (Clipping)
SGD-L1 (Cumulative)
SGD-L1 (Cumulative + ED)
Figure 5: NLPBA 2004 named entity recognition
task: Objective.
-2.8
-2.7
-2.6
-2.5
-2.4
-2.3
-2.2
-2.1
-2
-1.9
-1.8
 0  10  20  30  40  50
O
bje
cti
ve
 fu
nc
tio
n
Passes
OWL-QN
SGD-L1 (Clipping)
SGD-L1 (Cumulative)
SGD-L1 (Cumulative + ED)
Figure 6: POS tagging task: Objective.
ing because it was ?prohibitive? (7-8 days for sec-
tions 0-18 of the WSJ corpus).
For the features, we used unigrams and bigrams
of neighboring words, prefixes and suffixes of
the current word, and some characteristics of the
word. We also normalized the current word by
lowering capital letters and converting all the nu-
merals into ?#?, and used the normalized word as a
feature.
The results are shown in Figure 6 and Table 3.
Again, the trend is the same. Our algorithms fin-
ished training in about 30 minutes, producing ac-
curate models that are as compact as that produced
by OWL-QN.
Shen et al, (2007) report an accuracy of 97.33%
on the same data set using a perceptron-based bidi-
rectional tagging model.
5 Discussion
An alternative approach to producing compact
models for log-linear models is to reformulate the
483
Passes Lw/N # Features Time (sec) F-score
OWL-QN 161 -2.448 30,710 2,253 71.76
SGD-L1 (Naive) 30 -2.537 1,032,962 4,528 71.20
SGD-L1 (Clipping + Lazy-Update) 30 -2.538 279,886 585 71.20
SGD-L1 (Cumulative) 30 -2.479 31,986 631 71.40
SGD-L1 (Cumulative + Exponential-Decay) 30 -2.443 25,965 631 71.63
Table 2: NLPBA 2004 Named entity recognition task. Training time and accuracy of the trained model
on the test data.
Passes Lw/N # Features Time (sec) Accuracy
OWL-QN 124 -1.941 50,870 5,623 97.16%
SGD-L1 (Naive) 30 -2.013 2,142,130 18,471 97.18%
SGD-L1 (Clipping + Lazy-Update) 30 -2.013 323,199 1,680 97.18%
SGD-L1 (Cumulative) 30 -1.987 62,043 1,777 97.19%
SGD-L1 (Cumulative + Exponential-Decay) 30 -1.954 51,857 1,774 97.17%
Table 3: POS tagging on the WSJ corpus. Training time and accuracy of the trained model on the test
data.
problem as a L1-constrained problem (Lee et al,
2006), where the conditional log-likelihood of the
training data is maximized under a fixed constraint
of the L1-norm of the weight vector. Duchi et
al. (2008) describe efficient algorithms for pro-
jecting a weight vector onto the L1-ball. Although
L1-regularized and L1-constrained learning algo-
rithms are not directly comparable because the ob-
jective functions are different, it would be inter-
esting to compare the two approaches in terms
of practicality. It should be noted, however, that
the efficient algorithm presented in (Duchi et al,
2008) needs to employ a red-black tree and is
rather complex.
In SGD learning, the need for tuning the meta-
parameters for learning rate scheduling can be an-
noying. In the case of exponential decay, the set-
ting of ? = 0.85 turned out to be a good rule
of thumb in our experiments?it always produced
near best results in 30 passes, but the other param-
eter ?0 needed to be tuned. It would be very useful
if those meta-parameters could be tuned in a fully
automatic way.
There are some sophisticated algorithms for
adaptive learning rate scheduling in SGD learning
(Vishwanathan et al, 2006; Huang et al, 2007).
However, those algorithms use second-order infor-
mation (i.e. Hessian information) and thus need
access to the weights of the features that are not
used in the current sample, which should slow
down the weight updating process for the same
reason discussed earlier. It would be interesting
to investigate whether those sophisticated learning
scheduling algorithms can actually result in fast
training in large-scale NLP tasks.
6 Conclusion
We have presented a new variant of SGD that can
efficiently train L1-regularized log-linear models.
The algorithm is simple and extremely easy to im-
plement.
We have conducted experiments using CRFs
and three NLP tasks, and demonstrated empiri-
cally that our training algorithm can produce com-
pact and accurate models much more quickly than
a state-of-the-art quasi-Newton method for L1-
regularization.
Acknowledgments
We thank N. Okazaki, N. Yoshinaga, D.
Okanohara and the anonymous reviewers for their
useful comments and suggestions. The work de-
scribed in this paper has been funded by the
Biotechnology and Biological Sciences Research
Council (BBSRC; BB/E004431/1). The research
team is hosted by the JISC/BBSRC/EPSRC spon-
sored National Centre for Text Mining.
References
Galen Andrew and Jianfeng Gao. 2007. Scalable train-
ing of L1-regularized log-linear models. In Pro-
ceedings of ICML, pages 33?40.
484
Bob Carpenter. 2008. Lazy sparse stochastic gradient
descent for regularized multinomial logistic regres-
sion. Technical report, Alias-i.
Stephen Clark and James R. Curran. 2004. Parsing the
WSJ using CCG and log-linear models. In Proceed-
ings of COLING 2004, pages 103?110.
Trevor Cohn and Philip Blunsom. 2005. Semantic role
labeling with tree conditional random fields. In Pro-
ceedings of CoNLL, pages 169?172.
Michael Collins, Amir Globerson, Terry Koo, Xavier
Carreras, and Peter L. Bartlett. 2008. Exponen-
tiated gradient algorithms for conditional random
fields and max-margin markov networks. The Jour-
nal of Machine Learning Research (JMLR), 9:1775?
1822.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of EMNLP, pages 1?8.
Christian Darken and John Moody. 1990. Note on
learning rate schedules for stochastic optimization.
In Proceedings of NIPS, pages 832?838.
Juhn Duchi and Yoram Singer. 2008. Online and
batch learning using forward-looking subgradients.
In NIPS Workshop: OPT 2008 Optimization for Ma-
chine Learning.
Juhn Duchi, Shai Shalev-Shwartz, Yoram Singer, and
Tushar Chandra. 2008. Efficient projections onto
the l1-ball for learning in high dimensions. In Pro-
ceedings of ICML, pages 272?279.
Jenny Rose Finkel, Alex Kleeman, and Christopher D.
Manning. 2008. Efficient, feature-based, condi-
tional random field parsing. In Proceedings of ACL-
08:HLT, pages 959?967.
Jianfeng Gao, Galen Andrew, Mark Johnson, and
Kristina Toutanova. 2007. A comparative study of
parameter estimation methods for statistical natural
language processing. In Proceedings of ACL, pages
824?831.
Han-Shen Huang, Yu-Ming Chang, and Chun-Nan
Hsu. 2007. Training conditional random fields by
periodic step size adaptation for large-scale text min-
ing. In Proceedings of ICDM, pages 511?516.
Jun?ichi Kazama and Jun?ichi Tsujii. 2003. Evalua-
tion and extension of maximum entropy models with
inequality constraints. In Proceedings of EMNLP
2003.
J.-D. Kim, T. Ohta, Y. Tsuruoka, Y. Tateisi, and N. Col-
lier. 2004. Introduction to the bio-entity recognition
task at JNLPBA. In Proceedings of the International
Joint Workshop on Natural Language Processing in
Biomedicine and its Applications (JNLPBA), pages
70?75.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proceedings of ICML, pages 282?
289.
John Langford, Lihong Li, and Tong Zhang. 2009.
Sparse online learning via truncated gradient. The
Journal of Machine Learning Research (JMLR),
10:777?801.
Su-In Lee, Honglak Lee, Pieter Abbeel, and Andrew Y.
Ng. 2006. Efficient l1 regularized logistic regres-
sion. In Proceedings of AAAI-06, pages 401?408.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1994. Building a large annotated
corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19(2):313?330.
Jorge Nocedal. 1980. Updating quasi-newton matrices
with limited storage. Mathematics of Computation,
35(151):773?782.
Daisuke Okanohara, Yusuke Miyao, Yoshimasa Tsu-
ruoka, and Jun?ichi Tsujii. 2006. Improving
the scalability of semi-markov conditional random
fields for named entity recognition. In Proceedings
of COLING/ACL, pages 465?472.
Adwait Ratnaparkhi. 1996. A maximum entropy
model for part-of-speech tagging. In Proceedings
of EMNLP 1996, pages 133?142.
Libin Shen, Giorgio Satta, and Aravind Joshi. 2007.
Guided learning for bidirectional sequence classifi-
cation. In Proceedings of ACL, pages 760?767.
David Smith and Jason Eisner. 2008. Dependency
parsing by belief propagation. In Proceedings of
EMNLP, pages 145?156.
James C. Spall. 2005. Introduction to Stochastic
Search and Optimization. Wiley-IEEE.
Christoph Tillmann and Tong Zhang. 2006. A discrim-
inative global training algorithm for statistical MT.
In Proceedings of COLING/ACL, pages 721?728.
Kristina Toutanova, Aria Haghighi, and Christopher
Manning. 2005. Joint learning improves semantic
role labeling. In Proceedings of ACL, pages 589?
596.
S. V. N. Vishwanathan, Nicol N. Schraudolph, Mark W.
Schmidt, and Kevin P. Murphy. 2006. Accelerated
training of conditional random fields with stochastic
gradient methods. In Proceedings of ICML, pages
969?976.
Ben Wellner and Marc Vilain. 2006. Leveraging
machine readable dictionaries in discriminative se-
quence models. In Proceedings of LREC 2006.
485
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 467?474, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Bidirectional Inference with the Easiest-First Strategy
for Tagging Sequence Data
Yoshimasa Tsuruoka12 and Jun?ichi Tsujii231
1 CREST, JST (Japan Science and Technology Corporation)
Honcho 4-1-8, Kawaguchi-shi, Saitama 332-0012 Japan
2 Department of Computer Science, University of Tokyo
Hongo 7-3-1, Bunkyo-ku, Tokyo 113-0033 Japan
3 School of Informatics, University of Manchester
POBox 88, Sackville St, MANCHESTER M60 1QD, UK
{tsuruoka,tsujii}@is.s.u-tokyo.ac.jp
Abstract
This paper presents a bidirectional in-
ference algorithm for sequence label-
ing problems such as part-of-speech tag-
ging, named entity recognition and text
chunking. The algorithm can enumerate
all possible decomposition structures and
find the highest probability sequence to-
gether with the corresponding decomposi-
tion structure in polynomial time. We also
present an efficient decoding algorithm
based on the easiest-first strategy, which
gives comparably good performance to
full bidirectional inference with signifi-
cantly lower computational cost. Exper-
imental results of part-of-speech tagging
and text chunking show that the proposed
bidirectional inference methods consis-
tently outperform unidirectional inference
methods and bidirectional MEMMs give
comparable performance to that achieved
by state-of-the-art learning algorithms in-
cluding kernel support vector machines.
1 Introduction
The task of labeling sequence data such as part-of-
speech (POS) tagging, chunking (shallow parsing)
and named entity recognition is one of the most im-
portant tasks in natural language processing.
Conditional random fields (CRFs) (Lafferty et al,
2001) have recently attracted much attention be-
cause they are free from so-called label bias prob-
lems which reportedly degrade the performance of
sequential classification approaches like maximum
entropy markov models (MEMMs).
Although sequential classification approaches
could suffer from label bias problems, they have sev-
eral advantages over CRFs. One is the efficiency
of training. CRFs need to perform dynamic pro-
gramming over the whole sentence in order to com-
pute feature expectations in each iteration of numer-
ical optimization. Training, for instance, second-
order CRFs using a rich set of features can require
prohibitive computational resources. Max-margin
methods for structured data share problems of com-
putational cost (Altun et al, 2003).
Another advantage is that one can employ a vari-
ety of machine learning algorithms as the local clas-
sifier. There is huge amount of work about devel-
oping classification algorithms that have high gener-
alization performance in the machine learning com-
munity. Being able to incorporate such state-of-the-
art machine learning algorithms is important. In-
deed, sequential classification approaches with ker-
nel support vector machines offer competitive per-
formance in POS tagging and chunking (Gimenez
and Marquez, 2003; Kudo and Matsumoto, 2001).
One obvious way to improve the performance of
sequential classification approaches is to enrich the
information that the local classifiers can use. In stan-
dard decomposition techniques, the local classifiers
cannot use the information about future tags (e.g.
the right-side tags in left-to-right decoding), which
would be helpful in predicting the tag of the target
word. To make use of the information about fu-
ture tags, Toutanova et al proposed a tagging algo-
rithm based on bidirectional dependency networks
467
(Toutanova et al, 2003) and achieved the best ac-
curacy on POS tagging on the Wall Street Journal
corpus. As they pointed out in their paper, however,
their method potentially suffers from ?collusion? ef-
fects which make the model lock onto conditionally
consistent but jointly unlikely sequences. In their
modeling, the local classifiers can always use the in-
formation about future tags, but that could cause a
double-counting effect of tag information.
In this paper we propose an alternative way of
making use of future tags. Our inference method
considers all possible ways of decomposition and
chooses the ?best? decomposition, so the informa-
tion about future tags is used only in appropriate
situations. We also present a deterministic version
of the inference method and show their effective-
ness with experiments of English POS tagging and
chunking, using standard evaluation sets.
2 Bidirectional Inference
The task of labeling sequence data is to find the se-
quence of tags t1...tn that maximizes the following
probability given the observation o = o1...on
P (t1...tn|o). (1)
Observations are typically words and their lexical
features in the task of POS tagging. Sequential clas-
sification approaches decompose the probability as
follows,
P (t1...tn|o) =
n?
i=1
p(ti|t1...ti?1o). (2)
This is the left-to-right decomposition. If we
make a first-order markov assumption, the equation
becomes
P (t1...tn|o) =
n?
i=1
p(ti|ti?1o). (3)
Then we can employ a probabilistic classifier
trained with the preceding tag and observations in
order to obtain p(ti|ti?1o) for local classification. A
common choice for the local probabilistic classifier
is maximum entropy classifiers (Berger et al, 1996).
The best tag sequence can be efficiently computed
by using a Viterbi decoding algorithm in polynomial
time.
t1
(a)
t2 t3
o
t1
(b)
t2 t3
t1
(c)
t2 t3 t1
(d)
t2 t3
o
o o
Figure 1: Different structures for decomposition.
The right-to-left decomposition is
P (t1...tn|o) =
n?
i=1
p(ti|ti+1o). (4)
These two ways of decomposition are widely used
in various tagging problems in natural language pro-
cessing. The issue with such decompositions is that
you have only the information about the preceding
(or following) tags when performing local classifi-
cation.
From the viewpoint of local classification, we
want to give the classifier as much information as
possible because the information about neighboring
tags is useful in general.
As an example, consider the situation where we
are going to annotate a three-word sentence with
part-of-speech tags. Figure 1 shows the four possi-
ble ways of decomposition. They correspond to the
following equations:
(a) P (t1...t3|o) = P (t1|o)P (t2|t1o)P (t3|t2o) (5)
(b) P (t1...t3|o) = P (t3|o)P (t2|t3o)P (t1|t2o) (6)
(c) P (t1...t3|o) = P (t1|o)P (t3|o)P (t2|t3t1o) (7)
(d) P (t1...t3|o) = P (t2|o)P (t1|t2o)P (t3|t2o) (8)
(a) and (b) are the standard left-to-right and right-
to-left decompositions. Notice that in decomposi-
tion (c), the local classifier can use the information
about the tags on both sides when deciding t2. If,
for example, the second word is difficult to tag (e.g.
an unknown word), we might as well take the de-
composition structure (c) because the local classifier
468
can use rich information when deciding the tag of
the most difficult word. In general if we have an
n-word sentence and adopt a first-order markov as-
sumption, we have 2n?1 possible ways of decompo-
sition because each of the n ? 1 edges in the cor-
responding graph has two directions (left-to-right or
right-to-left).
Our bidirectional inference method is to consider
all possible decomposition structures and choose the
?best? structure and tag sequence. We will show in
the next section that this is actually possible in poly-
nomial time by dynamic programming.
As for the training, let us look at the equa-
tions of four different decompositions above. You
can notice that there are only four types of local
conditional probabilities: P (ti|ti?1o), P (ti|ti+1o),
P (ti|ti?1ti+1o), and P (ti|o).
This means that if we have these four types of lo-
cal classifiers, we can consider any decomposition
structures in the decoding stage. These local classi-
fiers can be obtained by training with corresponding
neighboring tag information. Training the first two
types of classifiers is exactly the same as the train-
ing of popular left-to-right and right-to-left sequen-
tial classification models respectively.
If we take a second-order markov assumption, we
need to train 16 types of local classifiers because
each of the four neighboring tags of a classification
target has two possibilities of availability. In gen-
eral, if we take a k-th order markov assumption, we
need to train 22k types of local classifies.
2.1 Polynomial Time Inference
This section describes an algorithm to find the de-
composition structure and tag sequence that give the
highest probability. The algorithm for the first-order
case is an adaptation of the algorithm for decoding
the best sequence on a bidirectional dependency net-
work introduced by (Toutanova et al, 2003), which
originates from the Viterbi decoding algorithm for
second-order markov models.
Figure 2 shows a polynomial time decoding al-
gorithm for our bidirectional inference. It enumer-
ates all possible decomposition structures and tag
sequences by recursive function calls, and finds the
highest probability sequence. Polynomial time is
achieved by caching. Note that for each local clas-
sification, the function chooses the appropriate local
function bestScore()
{
return bestScoreSub(n+2, ?end, end, end?, ?L,L?);
}
function bestScoreSub(i+1, ?ti?1, ti, ti+1?, ?di?1, di?)
{
// memorization
if (cached(i+1, ?ti?1, ti, ti+1?, ?di?1, di?))
return cache(i+1, ?ti?1, ti, ti+1?, ?di?1, di?);
// left boundary case
if (i = -1)
if (?ti?1, ti, ti+1? = ?start, start, start?) return 1;
else return 0;
// recursive case
P = localClassification(i, ?ti?1, ti, ti+1?, ?di?1, di?);
return maxdi?2 maxti?2 P?
bestScoreSub(i, ?ti?2, ti?1, ti?, ?di?2, di?1?);
}
function localClassification(i, ?ti?1, ti, ti+1?, ?di?1, di?)
{
if (di?1 = L & di = L) return P (ti|ti+1, o);
if (di?1 = L & di = R) return P (ti|o);
if (di?1 = R & di = L) return P (ti|ti?1ti+1, o);
if (di?1 = R & di = R) return P (ti|ti?1, o);
}
Figure 2: Pseudo-code for bidirectional inference
for the first-order conditional markov models. di is
the direction of the edge between ti and ti+1.
classifier by taking into account the directions of the
adjacent edges of the classification target.
The second-order case is similar but slightly more
complex. Figure 3 shows the algorithm. The recur-
sive function needs to consider the directions of the
four adjacent edges of the classification target, and
maintain the directions of the two neighboring edges
to enumerate all possible edge directions. In addi-
tion, the algorithm rules out cycles in the structure.
2.2 Decoding with the Easiest-First Strategy
We presented a polynomial time decoding algorithm
in the previous section. However, polynomial time is
not low enough in practice. Indeed, even the Viterbi
decoding of second-order markov models for POS
tagging is not practical unless some pruning method
is involved. The computational cost of the bidirec-
tional decoding algorithm presented in the previous
section is, of course, larger than that because it enu-
merates all possible directions of the edges on top of
the enumeration of possible tag sequences.
In this section we present a greedy version of the
decoding method for bidirectional inference, which
469
function bestScore()
{
return bestScoreSub(n+3, ?end, end, end, end, end?, ?L,L, L, L?, ?L,L?);
}
function bestScoreSub(i+2, ?ti?2, ti?1, ti, ti+1ti+2?, ?d?i?1, di?1, di, d?i+1?, ?di?2, d?i?)
{
// to avoid cycles
if (di?1 = di & di != d?i) return 0;
// memorization
if (cached(i+2, ?ti?2, ti?1, ti, ti+1ti+2?, ?d?i?1, di?1, di, d?i+1?, ?di?2, d?i?)
return cache(i+2, ?ti?2, ti?1, ti, ti+1ti+2?, ?d?i?1, di?1, di, d?i+1?, ?di?2, d?i?);
// left boundary case
if (i = -2)
if (?ti?2, ti?1, ti, ti+1, ti+2? = ?start, start, start, start, start?) return 1;
else return 0;
// recursive case
P = localClassification(i, ?ti?2, ti?1, ti, ti+1, ti+2?, ?d?i?1, di?1, di, d?i+1?);
return maxd?
i?2
maxdi?3 maxti?3 P? bestScoreSub(i+1, ?ti?3, ti?2, ti?1, titi+1?, ?d?i?2, di?2, di?1, d?i?, ?di?3, d?i?1?);
}
Figure 3: Pseudo-code for bidirectional inference for the second-order conditional markov models. di is the
direction of the edge between ti and ti+1. d?i is the direction of the edge between ti?1 and ti+1. We omit the
localClassification function because it is the obvious extension of that for the first-order case.
is extremely simple and significantly more efficient
than full bidirectional decoding.
Instead of enumerating all possible decomposi-
tion structures, the algorithm determines the struc-
ture by adopting the easiest-first strategy. The whole
decoding algorithm is given below.
1. Find the ?easiest? word to tag.
2. Tag the word.
3. Go back to 1. until all the words are tagged.
We assume in this paper that the ?easiest? word
to tag is the word for which the classifier outputs
the highest probability. In finding the easiest word,
we use the appropriate local classifier according to
the availability of the neighboring tags. Therefore,
in the first iteration, we always use the local classi-
fiers trained with no contextual tag information (i.e.
(P (ti|o)). Then, for example, if t3 has been tagged
in the first iteration in a three-word sentence, we use
P (t2|t3o) to compute the probability for tagging t2
in the second iteration (as in Figure 1 (b)).
A naive implementation of this algorithm requires
O(n2) invocations of local classifiers, where n is the
number of the words in the sentence, because we
need to update the probabilities over the words at
each iteration. However, a k-th order Markov as-
sumption obviously allows us to skip most of the
probability updates, resulting in O(kn) invocations
of local classifiers. This enables us to build a very
efficient tagger.
3 Maximum Entropy Classifier
For local classifiers, we used a maximum entropy
model which is a common choice for incorporating
various types of features for classification problems
in natural language processing (Berger et al, 1996).
Regularization is important in maximum entropy
modeling to avoid overfitting to the training data.
For this purpose, we use the maximum entropy
modeling with inequality constraints (Kazama and
Tsujii, 2003). The model gives equally good per-
formance as the maximum entropy modeling with
Gaussian priors (Chen and Rosenfeld, 1999), and
the size of the resulting model is much smaller than
that of Gaussian priors because most of the param-
eters become zero. This characteristic enables us
to easily handle the model data and carry out quick
decoding, which is convenient when we repetitively
perform experiments. This modeling has one param-
eter to tune, which is called the width factor. We
tuned this parameter using the development data in
each type of experiments.
470
Current word wi & ti
Previous word wi?1 & ti
Next word wi+1 & ti
Bigram features wi?1, wi & ti
wi, wi+1 & ti
Previous tag ti?1 & ti
Tag two back ti?2 & ti
Next tag ti+1 & ti
Tag two ahead ti+2 & ti
Tag Bigrams ti?2, ti?1 & ti
ti?1, ti+1 & ti
ti+1, ti+2 & ti
Tag Trigrams ti?2, ti?1, ti+1 & ti
ti?1, ti+1, ti+2 & ti
Tag 4-grams ti?2, ti?1, ti+1, ti+2 & ti
Tag/Word ti?1, wi & ti
combination ti+1, wi & ti
ti?1, ti+1, wi & ti
Prefix features prefixes of wi & ti
(up to length 10)
Suffix features suffixes of wi & ti
(up to length 10)
Lexical features whether wi has a hyphen & ti
whether wi has a number & ti
whether wi has a capital letter & ti
whether wi is all capital & ti
Table 1: Feature templates used in POS tagging ex-
periments. Tags are parts-of-speech. Tag features
are not necessarily used in all the models. For ex-
ample, ?next tag? features cannot be used in left-to-
right models.
4 Experiments
To evaluate the bidirectional inference methods pre-
sented in the previous sections, we ran experiments
on POS tagging and text chunking with standard En-
glish data sets.
Although achieving the best accuracy is not the
primary purpose of this paper, we explored useful
feature sets and parameter setting by using develop-
ment data in order to make the experiments realistic.
4.1 Part-of-speech tagging experiments
We split the Penn Treebank corpus (Marcus et al,
1994) into training, development and test sets as in
(Collins, 2002). Sections 0-18 are used as the train-
ing set. Sections 19-21 are the development set, and
sections 22-24 are used as the test set. All the ex-
periments were carried out on the development set,
except for the final accuracy report using the best
setting.
For features, we basically adopted the feature set
Method Accuracy Speed
(%) (tokens/sec)
Left-to-right (Viterbi) 96.92 844
Right-to-left (Viterbi) 96.89 902
Dependency Networks 97.06 1,446
Easiest-last 96.58 2,360
Easiest-first 97.13 2,461
Full bidirectional 97.12 34
Table 2: POS tagging accuracy and speed on the de-
velopment set.
Method Accuracy (%)
Dep. Networks (Toutanova et al, 2003) 97.24
Perceptron (Collins, 2002) 97.11
SVM (Gimenez and Marquez, 2003) 97.05
HMM (Brants, 2000) 96.48
Easiest-first 97.10
Full Bidirectional 97.15
Table 3: POS tagging accuracy on the test set (Sec-
tions 22-24 of the WSJ, 5462 sentences).
provided by (Toutanova et al, 2003) except for com-
plex features such as crude company-name detection
features because they are specific to the Penn Tree-
bank and we could not find the exact implementation
details. Table 1 lists the feature templates used in our
experiments.
We tested the proposed bidirectional methods,
conventional unidirectional methods and the bidirec-
tional dependency network proposed by Toutanova
(Toutanova et al, 2003) for comparison. 1. All
the models are second-order. Table 2 shows the
accuracy and tagging speed on the development
data 2. Bidirectional inference methods clearly out-
performed unidirectional methods. Note that the
easiest-first decoding method achieves equally good
performance as full bidirectional inference. Table 2
also shows that the easiest-last strategy, where we
select and tag the most difficult word at each itera-
tion, is clearly a bad strategy.
An example of easiest-first decoding is given be-
low:
1For dependency network and full bidirectional decoding,
we conducted pruning because the computational cost was too
large to perform exhaustive search. We pruned a tag candidate if
the zero-th order probability of the candidate P (ti|o) was lower
than one hundredth of the zero-th order probability of the most
likely tag at the token.
2Tagging speed was measured on a server with an AMD
Opteron 2.4GHz CPU.
471
The/DT/4 company/NN/7 had/VBD/11
sought/VBN/14 increases/NNS/13 total-
ing/VBG/12 $/$/2 80.3/CD/5 million/CD/8
,/,/1 or/CC/6 22/CD/9 %/NN/10 ././3
Each token represents Word/PoS/DecodingOrder.
Typically, punctuations and articles are tagged first.
Verbs are usually tagged in later stages because their
tags are likely to be ambiguous.
We applied our bidirectional inference methods
to the test data. The results are shown in Table 3.
The table also summarizes the accuracies achieved
by several other research efforts. The best accuracy
is 97.24% achieved by bidirectional dependency net-
works (Toutanova et al, 2003) with a richer set of
features that are carefully designed for the corpus. A
perceptron algorithm gives 97.11% (Collins, 2002).
Gimenez and Marquez achieve 97.05% with support
vector machines (SVMs). This result indicates that
bidirectional inference with maximum entropy mod-
eling can achieve comparable performance to other
state-of-the-art POS tagging methods.
4.2 Chunking Experiments
The task of chunking is to find non-recursive phrases
in a sentence. For example, a chunker segments the
sentence ?He reckons the current account deficit will
narrow to only 1.8 billion in September? into the fol-
lowing,
[NP He] [VP reckons] [NP the current account
deficit] [VP will narrow] [PP to] [NP only 1.8 bil-
lion] [PP in] [NP September] .
We can regard chunking as a tagging task by con-
verting chunks into tags on tokens. There are several
ways of representing text chunks (Sang and Veen-
stra, 1999). We tested the Start/End representation
in addition to the popular IOB2 representation since
local classifiers can have fine-grained information
on the neighboring tags in the Start/End represen-
tation.
For training and testing, we used the data set pro-
vided for the CoNLL-2000 shared task. The training
set consists of section 15-18 of the WSJ corpus, and
the test set is section 20. In addition, we made the
development set from section 21 3.
We basically adopted the feature set provided in
3We used the Perl script provided on
http://ilk.kub.nl/? sabine/chunklink/
Current word wi & ti
Previous word wi?1 & ti
Word two back wi?2 & ti
Next word wi+1 & ti
Word two ahead wi+2 & ti
Bigram features wi?2, wi?1 & ti
wi?1, wi & ti
wi, wi+1 & ti
wi+1, wi+2 & ti
Current POS pi & ti
Previous POS pi?1 & ti
POS two back pi?2 & ti
Next POS pi+1 & ti
POS two ahead pi+2 & ti
Bigram POS features pi?2, pi?1 & ti
pi?1, pi & ti
pi, pi+1 & ti
pi+1, pi+2 & ti
Trigram POS features pi?2, pi?1, pi & ti
pi?1, pi, pi+1 & ti
pi, pi+1, pi+2 & ti
Previous tag ti?1 & ti
Tag two back ti?2 & ti
Next tag ti+1 & ti
Tag two ahead ti+2 & ti
Bigram tag features ti?2, ti?1 & ti
ti?1, ti+1 & ti
ti+1, ti+2 & ti
Table 4: Feature templates used in chunking experi-
ments.
(Collins, 2002) and used POS-trigrams as well. Ta-
ble 4 lists the features used in chunking experiments.
Table 5 shows the results on the development set.
Again, bidirectional methods exhibit better perfor-
mance than unidirectional methods. The difference
is bigger with the Start/End representation. Depen-
dency networks did not work well for this chunking
task, especially with the Start/End representation.
We applied the best model on the development
set in each chunk representation type to the test
data. Table 6 summarizes the performance on the
test set. Our bidirectional methods achieved F-
scores of 93.63 and 93.70, which are better than the
best F-score (93.48) of the CoNLL-2000 shared task
(Sang and Buchholz, 2000) and comparable to those
achieved by other state-of-the-art methods.
5 Discussion
There are some reports that one can improve the
performance of unidirectional models by combining
outputs of multiple taggers. Shen et al (2003) re-
ported a 4.9% error reduction of supertagging by
472
Representation Method Order Recall Precision F-score Speed (tokens/sec)
IOB2 Left-to-right 1 93.17 93.05 93.11 1,775
2 93.13 92.90 93.01 989
Right-to-left 1 92.92 92.82 92.87 1,635
2 92.92 92.74 92.87 927
Dependency Networks 1 92.71 92.91 92.81 2,534
2 92.61 92.95 92.78 1,893
Easiest-first 1 93.17 93.04 93.11 2,441
2 93.35 93.32 93.33 1,248
Full Bidirectional 1 93.29 93.14 93.21 712
2 93.26 93.12 93.19 48
Start/End Left-to-right 1 92.98 92.69 92.83 861
2 92.96 92.67 92.81 439
Right-to-left 1 92.92 92.83 92.87 887
2 92.89 92.74 92.82 451
Dependency Networks 1 87.10 89.56 88.32 1,894
2 87.16 89.44 88.28 331
Easiest-first 1 93.33 92.95 93.14 1,950
2 93.31 92.95 93.13 1,016
Full Bidirectional 1 93.52 93.26 93.39 392
2 93.44 93.20 93.32 4
Table 5: Chunking F-scores on the development set.
Method Recall Precision F-score
SVM (Kudoh and Matsumoto, 2000) 93.51 93.45 93.48
SVM voting (Kudo and Matsumoto, 2001) 93.92 93.89 93.91
Regularized Winnow (with basic features) (Zhang et al, 2002) 93.60 93.54 93.57
Perceptron (Carreras and Marquez, 2003) 93.29 94.19 93.74
Easiest-first (IOB2, second-order) 93.59 93.68 93.63
Full Bidirectional (Start/End, first-order) 93.70 93.65 93.70
Table 6: Chunking F-scores on the test set (Section 20 of the WSJ, 2012 sentences).
pairwise voting between left-to-right and right-to-
left taggers. Kudo et al (2001) attained performance
improvement in chunking by conducting weighted
voting of multiple SVMs trained with distinct chunk
representations. The biggest difference between our
approach and such voting methods is that the lo-
cal classifier in our bidirectional inference methods
can have rich information for decision. Also, vot-
ing methods generally need many tagging processes
to be run on a sentence, which makes it difficult to
build a fast tagger.
Our algorithm can be seen as an ensemble classi-
fier by which we choose the highest probability one
among the different taggers with all possible decom-
position structures. Although choosing the highest
probability one is seemingly natural and one of the
simplest ways for combining the outputs of different
taggers, one could use a different method (e.g. sum-
ming the probabilities over the outputs which share
the same label sequence). Investigating the methods
for combination should be an interesting direction of
future work.
As for the computational cost for training, our
methods require us to train 22n types of classifiers
when we adopt an nth order markov assumption. In
many cases a second-order model is sufficient be-
cause further increase of n has little impact on per-
formance. Thus the training typically takes four or
16 times as much time as it would take for training a
single unidirectional tagger, which looks somewhat
expensive. However, because each type of classi-
fier can be trained independently, the training can
be performed completely in parallel and run with
the same amount of memory as that for training a
single classifier. This advantage contrasts with the
case for CRFs which requires substantial amount of
memory and computational cost if one tries to incor-
porate higher-order features about tag sequences.
Tagging speed is another important factor in
building a practical tagger for large-scale text min-
473
ing. Our inference algorithm with the easiest-first
strategy needs no Viterbi decoding unlike MEMMs
and CRFs, and makes it possible to perform very fast
tagging with high precision.
6 Conclusion
We have presented a bidirectional inference algo-
rithm for sequence labeling problems such as POS
tagging, named entity recognition and text chunk-
ing. The algorithm can enumerate all possible de-
composition structures and find the highest prob-
ability sequence together with the corresponding
decomposition structure in polynomial time. We
have also presented an efficient bidirectional infer-
ence algorithm based on the easiest-first strategy,
which gives comparable performance to full bidi-
rectional inference with significantly lower compu-
tational cost.
Experimental results of POS tagging and text
chunking show that the proposed bidirectional in-
ference methods consistently outperform unidi-
rectional inference methods and our bidirectional
MEMMs give comparable performance to that
achieved by state-of-the-art learning algorithms in-
cluding kernel support vector machines.
A natural extension of this work is to replace
the maximum entropy modeling, which was used as
the local classifiers, with other machine learning al-
gorithms. Support vector machines with appropri-
ate kernels is a good candidate because they have
good generalization performance as a single classi-
fier. Although SVMs do not output probabilities, the
easiest-first method would be easily applied by con-
sidering the margins output by SVMs as the confi-
dence of local classification.
References
Yasemin Altun, Ioannis Tsochantaridis, and Thomas
Hofmann. 2003. Hidden markov support vector ma-
chines. In Proceedings of ICML 2003, pages 3?10.
Adam L. Berger, Stephen A. Della Pietra, and Vincent
J. Della Pietra. 1996. A maximum entropy approach
to natural language processing. Computational Lin-
guistics, 22(1):39?71.
Thorsten Brants. 2000. TnT ? a statistical part-of-speech
tagger. In Proceedings of the 6th Applied NLP Con-
ference (ANLP).
Xavier Carreras and Lluis Marquez. 2003. Phrase recog-
nition by filtering and ranking with perceptrons. In
Proceedings of RANLP-2003.
Stanley F. Chen and Ronald Rosenfeld. 1999. A gaus-
sian prior for smoothing maximum entropy models.
Technical Report CMUCS -99-108, Carnegie Mellon
University.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings of
EMNLP 2002, pages 1?8.
Jesus Gimenez and Lluis Marquez. 2003. Fast and accu-
rate part-of-speech tagging: The SVM approach revis-
ited. In Proceedings of RANLP 2003, pages 158?165.
Jun?ichi Kazama and Jun?ichi Tsujii. 2003. Evaluation
and extension of maximum entropy models with in-
equality constraints. In Proceedings of EMNLP 2003.
Taku Kudo and Yuji Matsumoto. 2001. Chunking with
support vector machines. In Proceedings of NAACL
2001.
Taku Kudoh and Yuji Matsumoto. 2000. Use of support
vector learning for chunk identification. In Proceed-
ings of CoNLL-2000, pages 142?144.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic mod-
els for segmenting and labeling sequence data. In Pro-
ceedings of ICML 2001, pages 282?289.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1994. Building a large annotated cor-
pus of english: The penn treebank. Computational
Linguistics, 19(2):313?330.
Erik F. Tjong Kim Sang and Sabine Buchholz. 2000. In-
troduction to the conll-2000 shared task: Chunking.
In Proceedings of CoNLL-2000 and LLL-2000, pages
127?132.
Erik F. Tjong Kim Sang and Jorn Veenstra. 1999. Rep-
resenting text chunks. In Proceedings of EACL 1999,
pages 173?179.
Libin Shen and Aravind K. Joshi. 2003. A SNoW based
Supertagger with Application to NP Chunking. In
Proceedings of ACL 2003, pages 505?512.
Kristina Toutanova, Dan Klein, Christopher Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Pro-
ceedings of HLT-NAACL 2003, pages 252?259.
Tong Zhang, Fred Damereau, and David Johnson. 2002.
Text chunking based on a generalization of winnow.
Journal of Machine Learning Research, 2:615?638.
474
Training a Naive Bayes Classifier via the EM Algorithm with a Class
Distribution Constraint
Yoshimasa Tsuruoka?? and Jun?ichi Tsujii??
?Department of Computer Science, University of Tokyo
Hongo 7-3-1, Bunkyo-ku, Tokyo 113-0033 JAPAN
?CREST, JST (Japan Science and Technology Corporation)
Honcho 4-1-8, Kawaguchi-shi, Saitama 332-0012 JAPAN
{tsuruoka,tsujii}@is.s.u-tokyo.ac.jp
Abstract
Combining a naive Bayes classifier with the
EM algorithm is one of the promising ap-
proaches for making use of unlabeled data for
disambiguation tasks when using local con-
text features including word sense disambigua-
tion and spelling correction. However, the use
of unlabeled data via the basic EM algorithm
often causes disastrous performance degrada-
tion instead of improving classification perfor-
mance, resulting in poor classification perfor-
mance on average. In this study, we introduce
a class distribution constraint into the iteration
process of the EM algorithm. This constraint
keeps the class distribution of unlabeled data
consistent with the class distribution estimated
from labeled data, preventing the EM algorithm
from converging into an undesirable state. Ex-
perimental results from using 26 confusion sets
and a large amount of unlabeled data show
that our proposed method for using unlabeled
data considerably improves classification per-
formance when the amount of labeled data is
small.
1 Introduction
Many of the tasks in natural language processing can
be addressed as classification problems. State-of-the-
art machine learning techniques including Support Vec-
tor Machines (Vapnik, 1995), AdaBoost (Schapire and
Singer, 2000) and Maximum Entropy Models (Ratna-
parkhi, 1998; Berger et al, 1996) provide high perfor-
mance classifiers if one has abundant correctly labeled
examples.
However, annotating a large set of examples generally
requires a huge amount of human labor and time. This
annotation cost is one of the major obstacles to applying
machine learning techniques to real-world NLP applica-
tions.
Recently, learning algorithms called minimally super-
vised learning or unsupervised learning that can make use
of unlabeled data have received much attention. Since
collecting unlabeled data is generally much easier than
annotating data, such techniques have potential for solv-
ing the problem of annotation cost. Those approaches in-
clude a naive Bayes classifier combined with the EM al-
gorithm (Dempster et al, 1977; Nigam et al, 2000; Ped-
ersen and Bruce, 1998), Co-training (Blum and Mitchell,
1998; Collins and Singer, 1999; Nigam and Ghani, 2000),
and Transductive Support Vector Machines (Joachims,
1999). These algorithms have been applied to some
tasks including text classification and word sense disam-
biguation and their effectiveness has been demonstrated
to some extent.
Combining a naive Bayes classifier with the EM algo-
rithm is one of the promising minimally supervised ap-
proaches because its computational cost is low (linear to
the size of unlabeled data), and it does not require the
features to be split into two independent sets unlike co-
training.
However, the use of unlabeled data via the basic EM
algorithm does not always improve classification perfor-
mance. In fact, this often causes disastrous performance
degradation resulting in poor classification performance
on average. To alleviate this problem, we introduce a
class distribution constraint into the iteration process of
the EM algorithm. This constraint keeps the class dis-
tribution of unlabeled data consistent with the class dis-
tribution estimated from labeled data, preventing the EM
algorithm from converging into an undesirable state.
In order to assess the effectiveness of the proposed
method, we applied it to the problem of semantic disam-
biguation using local context features. Experiments were
conducted with 26 confusion sets and a large number of
unlabeled examples collected from a corpus of one hun-
dred million words.
This paper is organized as follows. Section 2 briefly
reviews the naive Bayes classifier and the EM algorithm
as means of using unlabeled data. Section 3 presents the
idea of using a class distribution constraint and how to
impose this constraint on the learning process. Section
4 describes the problem of confusion set disambiguation
and the features used in the experiments. Experimental
results are presented in Section 5. Related work is dis-
cussed in Section 6. Section 7 offers some concluding
remarks.
2 Naive Bayes Classifier
The naive Bayes classifier is a simple but effective classi-
fier which has been used in numerous applications of in-
formation processing such as image recognition, natural
language processing, information retrieval, etc. (Escud-
ero et al, 2000; Lewis, 1998; Nigam and Ghani, 2000;
Pedersen, 2000).
In this section, we briefly review the naive Bayes clas-
sifier and the EM algorithm that is used for making use
of unlabeled data.
2.1 Naive Bayes Model
Let x be a vector we want to classify, and ck be a possible
class. What we want to know is the probability that the
vector x belongs to the class ck. We first transform the
probability P (ck|x) using Bayes? rule,
P (ck|x) = P (ck)?
P (x|ck)
P (x)
. (1)
Class probability P (ck) can be estimated from training
data. However, direct estimation of P (ck|x) is impossi-
ble in most cases because of the sparseness of training
data.
By assuming the conditional independence of the ele-
ments of a vector, P (x|ck) is decomposed as follows,
P (x|ck) =
d
?
j=1
P (xj |ck), (2)
where xj is the jth element of vector x. Then Equation 1
becomes
P (ck|x) = P (ck)?
?d
j=1 P (xj |ck)
P (x)
. (3)
With this equation, we can calculate P (ck|x) and classify
x into the class with the highest P (ck|x).
Note that the naive Bayes classifier assumes the con-
ditional independence of features. This assumption how-
ever does not hold in most cases. For example, word oc-
currence is a commonly used feature for text classifica-
tion. However, obvious strong dependencies exist among
word occurrences. Despite this apparent violation of the
assumption, the naive Bayes classifier exhibits good per-
formance for various natural language processing tasks.
There are some implementation variants of the naive
Bayes classifier depending on their event models (Mc-
Callum and Nigam, 1998). In this paper, we adopt the
multi-variate Bernoulli event model. Smoothing was
done by replacing zero-probability with a very small con-
stant (1.0? 10?4).
2.2 EM Algorithm
The Expectation Maximization (EM) algorithm (Demp-
ster et al, 1977) is a general framework for estimating
the parameters of a probability model when the data has
missing values. This algorithm can be applied to min-
imally supervised learning, in which the missing values
correspond to missing labels of the examples.
The EM algorithm consists of the E-step in which the
expected values of the missing sufficient statistics given
the observed data and the current parameter estimates are
computed, and the M-step in which the expected values
of the sufficient statistics computed in the E-step are used
to compute complete data maximum likelihood estimates
of the parameters (Dempster et al, 1977).
In our implementation of the EM algorithm with the
naive Bayes classifier, the learning process using unla-
beled data proceeds as follows:
1. Train the classifier using only labeled data.
2. Classify unlabeled examples, assigning probabilistic
labels to them.
3. Update the parameters of the model. Each proba-
bilistically labeled example is counted as its proba-
bility instead of one.
4. Go back to (2) until convergence.
3 Class Distribution Constraint
3.1 Motivation
As described in the previous section, the naive Bayes
classifier can be easily extended to exploit unlabeled data
by using the EM algorithm. However, the use of unla-
beled data for actual tasks exhibits mixed results. The
performance is improved for some cases, but not in all
cases. In our preliminary experiments, using unlabeled
data by means of the EM algorithm often caused signifi-
cant deterioration of classification performance.
To investigate the cause of this, we observed the
change of class distribution of unlabeled data occuring in
the process of the EM algorithm. What we found is that
sometimes the class distribution of unlabeled data greatly
diverges from that of the labeled data. For example, when
the proportion of class A examples in labeled data was
about 0.9, the EM algorithm would sometimes converge
into states where the proportion of class A is about 0.7.
This divergence of class distribution clearly indicated the
EM algorithm converged into an undesirable state.
One of the possible remedies for this phenomenon is
that of forcing class distribution of unlabeled data not to
diverge from the class distribution estimated from labeled
data. In this work, we introduce a class distribution con-
straint (CDC) into the training process of the EM algo-
rithm. This constraint keeps the class distribution of un-
labeled data consistent with that of labeled data.
3.2 Calibrating Probabilistic Labels
We implement class distribution constraints by calibrat-
ing probabilistic labels assigned to unlabeled data in the
process of the EM algorithm. In this work, we consider
only binary classification: classes A and B.
Let pi be the probabilistic label of the ith example
representing the probability that this example belongs to
class A.
Let ? be the proportion of class A examples in the la-
beled data L. If the proportion of the class A examples
(the proportion of the examples whose p i is greater than
0.5) in unlabeled data U is different from ?, we consider
that the values of the probabilistic labels should be cali-
brated.
The basic idea of the calibration is to shift all the prob-
ability values of unlabeled data to the extent that the class
distribution of unlabeled data becomes identical to that of
labeled data. In order for the shifting of the probability
values not to cause the values to go outside of the range
from 0 to 1, we transform the probability values by an
inverse sigmoid function in advance. After the shifting,
the values are returned to probability values by a sigmoid
function.
The whole calibration process is given below:
1. Transform the probabilistic labels p
1
, ...pn by the in-
verse function of the sigmoid function,
f(x) =
1
1 + e?x
. (4)
into real value ranging from ?? to ?. Let the
transformed values be q
1
, ...qn.
2. Sort q
1
, ...qn in descending order. Then, pick up the
value qborder that is located at the position of pro-
portion ? in these n values.
3. Since qborder is located at the border between the
examples of label A and those of label B, the value
should be close to zero (= probability is 0.5). Thus
we calibrate all qi by subtracting qborder.
4. Transform q
1
, ...qn by a sigmoid function back into
probability values.
This calibration process is conducted between the E-
step and the M-step in the EM algorithm.
4 Confusion Set Disambiguation
We applied the naive Bayes classifier with the EM algo-
rithm to confusion set disambiguation. Confusion set dis-
ambiguation is defined as the problem of choosing the
correct word from a set of words that are commonly
confused. For example, quite may easily be mistyped
as quiet. An automatic proofreading system would
need to judge which is the correct use given the con-
text surrounding the target. Example confusion sets in-
clude: {principle, principal}, {then, than}, and {weather,
whether}.
Until now, many methods have been proposed for this
problem including winnow-based algorithms (Golding
and Roth, 1999), differential grammars (Powers, 1998),
transformation based learning (Mangu and Brill, 1997),
decision lists (Yarowsky, 1994).
Confusion set disambiguation has very similar char-
acteristics to a word sense disambiguation problem in
which the system has to identify the meaning of a pol-
ysemous word given the surrounding context. The merit
of using confusion set disambiguation as a test-bed for a
learning algorithm is that since one does not need to an-
notate the examples to make labeled data, one can con-
duct experiments using an arbitrary amount of labeled
data.
4.1 Features
As the input of the classifier, the context of the target must
be represented in the form of a vector. We use a binary
feature vector which contains only the values of 0 or 1 for
each element.
In this work, we use the local context surrounding the
target as the feature of an example. The features of a
target are the two preceding words and the two following
words. For example, if the disambiguation target is quiet
and the system is given the following sentence
?...between busy and quiet periods and it...?
the contexts of this example are represented as follows:
busy
?2
, and
?1
, periods
+1
, and
+2
In the input vector, only the elements corresponding to
these features are set to 1, while all the other elements are
set to 0.
Table 1: Confusion Sets used in the Experiments
Confusion Set Baseline #Unlabeled
I, me 86.4 474726
accept, except 53.2 14876
affect, effect 79.1 20653
among, between 80.1 101621
amount, number 76.1 50310
begin, being 93.0 82448
cite, sight 95.1 3498
country, county 80.8 17810
fewer, less 91.6 35413
its, it?s 83.7 177488
lead, led 53.5 25195
maybe, may be 92.4 36519
passed, past 66.8 24450
peace, piece 57.0 11219
principal, principle 61.7 8670
quiet, quite 88.8 29618
raise, rise 60.8 13392
sight, site 61.1 9618
site, cite 96.0 5594
than, then 63.8 216286
their, there 63.8 372471
there, they?re 96.4 146462
they?re, their 96.9 237443
weather, whether 87.5 29730
your, you?re 88.6 108185
AVERAGE 78.2 90147
5 Experiment
To conduct large scale experiments, we used the British
National Corpus 1 that is currently one of the largest cor-
pora available. The corpus contains roughly one hundred
million words collected from various sources.
The confusion sets used in our experiments are the
same as in Golding?s experiment (1999). Since our al-
gorithm requires the classification to be binary, we de-
composed three-class confusion sets into pairwise binary
classifications. Table 1 shows the resulting confusion sets
used in the following experiments. The baseline perfor-
mances, achieved by simply selecting the majority class,
are shown in the second column. The number of unla-
beled data are shown in the rightmost column.
The 1,000 test sets were randomly selected from the
corpus for each confusion set. They do not overlap the
labeled data or the unlabeled data used in the learning
process.
1Data cited herein has been extracted from the British Na-
tional Corpus Online service, managed by Oxford University
Computing Services on behalf of the BNC Consortium. All
rights in the texts cited are reserved.
Table 2: Results of Confusion Sets Disambiguation with
32 Labeled Data
NB + EM
Confusion Set NB NB+EM +CDC
I, me 87.4 96.3 96.0
accept, except 77.2 89.0 81.1
affect, effect 86.4 91.6 93.6
among, between 80.1 64.4 79.5
amount, number 69.6 61.6 68.8
begin, being 95.1 86.6 95.1
cite, sight 95.1 95.1 95.1
country, county 77.5 70.4 76.0
fewer, less 89.0 77.4 85.4
its, it?s 85.3 92.3 94.2
lead, led 65.3 64.2 63.7
maybe, may be 91.1 77.6 92.9
passed, past 77.9 70.2 82.0
peace, piece 78.4 81.5 82.1
principal, principle 72.8 88.7 79.4
quiet, quite 85.3 75.9 83.5
raise, rise 83.7 86.1 81.0
sight, site 67.7 68.7 67.9
site, cite 96.2 93.3 92.8
than, then 74.7 84.0 85.3
their, there 88.4 91.4 90.2
there, they?re 96.4 96.4 89.1
they?re, their 96.9 96.9 96.9
weather, whether 90.6 92.3 93.7
your, you?re 87.8 81.8 90.3
AVERAGE 83.8 82.9 85.4
The results are shown in Table 2 through Table 5.
These four tables correspond to the cases in which the
number of labeled examples is 32, 64, 128 and 256 as
indicated by the table captions. The first column shows
the confusion sets. The second column shows the clas-
sification performance of the naive Bayes classifier with
which only labeled data was used for training. The third
column shows the performance of the naive Bayes classi-
fier with which unlabeled data was used via the basic EM
algorithm. The rightmost column shows the performance
of the EM algorithm that was extended with our proposed
calibration process.
Notice that the effect of unlabeled data were very dif-
ferent for each confusion set. As shown in Table 2, the
precision was significantly improved for some confusion
sets including {I, me}, {accept, except} and {affect, ef-
fect} . However, disastrous performance deterioration
can be observed, especially that of the basic EM algo-
rithm, in some confusion sets including {among, be-
tween}, {country, county}, and {site, cite}.
On average, precision was degraded by the use of un-
Table 3: Results of Confusion Sets Disambiguation with
64 Labeled Data
NB + EM
Confusion Set NB NB+EM +CDC
I, me 89.4 96.8 95.7
accept, except 82.9 89.3 87.5
affect, effect 89.4 92.4 93.6
among, between 79.9 76.3 80.5
amount, number 71.5 68.7 69.1
begin, being 95.8 92.1 95.7
cite, sight 95.1 95.8 96.4
country, county 78.7 73.4 74.5
fewer, less 87.6 74.3 87.3
its, it?s 85.8 94.0 92.5
lead, led 76.2 66.8 72.8
maybe, may be 92.6 84.0 96.2
passed, past 79.7 72.5 88.4
peace, piece 81.1 81.2 82.4
principal, principle 75.2 90.2 89.8
quiet, quite 86.5 84.0 89.2
raise, rise 85.7 85.6 86.9
sight, site 71.9 69.0 69.0
site, cite 96.3 95.8 95.5
than, then 79.7 83.8 83.2
their, there 90.5 91.9 92.1
there, they?re 96.2 85.2 91.4
they?re, their 96.9 96.9 95.8
weather, whether 90.6 91.4 93.3
your, you?re 88.0 83.3 94.2
AVERAGE 85.7 84.6 87.7
labeled data via the basic EM algorithm (from 83.3% to
82.9%). On the other hand, the EM algorithm with the
class distribution constraint improved average classifica-
tion performance (from 83.3% to 85.4%). This improved
precision nearly reached the performance achieved by
twice the size of labeled data without unlabeled data (see
the average precision of NB in Table 3). This perfor-
mance gain indicates that the use of unlabeled data ef-
fectively doubles the labeled training data.
In Table 3, the tendency of performance improvement
(or degradation) in the use of unlabeled data is almost the
same as in Table 2. The basic EM algorithm degraded the
performance on average, while our method improved av-
erage performance (from 85.7% to 87.7%). This perfor-
mance gain effectively doubled the size of labeled data.
The results with 128 labeled examples are shown in Ta-
ble 4. Although the use of unlabeled examples by means
of our proposed method still improved average perfor-
mance (from 87.6% to 88.6%), the gain is smaller than
that for a smaller amount of labeled data.
With 256 labeled examples (Table 5), the average per-
Table 4: Results of Confusion Sets Disambiguation with
128 Labeled Data
NB + EM
Confusion Set NB NB+EM +CDC
I, me 90.7 96.9 96.4
accept, except 85.7 90.7 89.4
affect, effect 91.9 93.1 93.3
among, between 80.0 76.3 80.1
amount, number 78.2 68.9 69.3
begin, being 94.4 88.1 95.0
cite, sight 96.9 96.9 98.1
country, county 81.3 75.1 75.7
fewer, less 89.9 74.9 89.4
its, it?s 88.6 93.2 95.2
lead, led 80.5 82.5 82.2
maybe, may be 94.5 80.9 94.4
passed, past 81.8 74.1 85.5
peace, piece 84.1 81.3 82.5
principal, principle 79.8 89.8 89.5
quiet, quite 86.5 82.7 90.1
raise, rise 85.2 86.4 87.7
sight, site 75.6 70.3 70.5
site, cite 96.1 95.8 97.0
than, then 81.7 84.2 84.5
their, there 91.8 91.5 91.2
there, they?re 95.9 83.4 91.3
they?re, their 96.9 96.9 96.7
weather, whether 92.0 92.6 95.1
your, you?re 88.9 84.1 94.5
AVERAGE 87.6 85.2 88.6
formance gain was negligible (from 89.2% to 89.3%).
Figure 1 summarizes the average precisions for differ-
ent number of labeled examples. Average peformance
was improved by the use of unlabeled data with our pro-
posed method when the amount of labeled data was small
(from 32 to 256) as shown in Table 2 through Table
5. However, when the number of labeled examples was
large (more than 512), the use of unlabeled data degraded
average performance.
5.1 Effect of the amount of unlabeled data
When the use of unlabeled data improves classification
performance, the question of how much unlabeled data
are needed becomes very important. Although unlabeled
data are generally much more obtainable than labeled
data, acquiring more than several-thousand unlabeled ex-
amples is not always an easy task. As for confusion set
disambiguation, Table 1 indicates that it is sometimes im-
possible to collect tens of thousands examples even in a
very large corpus.
In order to investigate the effect of the amount of un-
Table 5: Results of Confusion Sets Disambiguation with
256 Labeled Data
NB + EM
Confusion Set NB NB+EM +CDC
I, me 93.4 96.6 96.4
accept, except 89.7 90.3 91.2
affect, effect 93.4 93.5 93.9
among, between 79.6 75.1 80.4
amount, number 81.4 68.9 69.2
begin, being 94.6 89.9 96.6
cite, sight 97.6 97.9 98.4
country, county 84.2 76.5 77.5
fewer, less 90.8 83.0 89.2
its, it?s 90.2 93.3 94.5
lead, led 82.9 79.8 82.6
maybe, may be 96.0 87.1 94.7
passed, past 83.5 74.6 86.3
peace, piece 84.6 81.4 85.7
principal, principle 83.4 90.5 90.5
quiet, quite 88.6 86.8 91.2
raise, rise 88.0 87.1 88.4
sight, site 79.2 71.7 73.2
site, cite 97.3 97.6 97.4
than, then 82.3 85.5 85.9
their, there 93.6 92.1 92.0
there, they?re 96.5 83.0 91.1
they?re, their 96.8 90.8 97.3
weather, whether 93.8 91.9 94.7
your, you?re 89.7 83.8 94.6
AVERAGE 89.2 85.9 89.3
labeled data, we conducted experiments by varying the
amount of unlabeled data for some confusion sets that ex-
hibited significant performance gain by using unlabeled
data.
Figure 2 shows the relationship between the classifica-
tion performance and the amount of unlabeled data for
three confusion sets: {I, me}, {principal, principle}, and
{passed, past}. The number of labeled examples in all
cases was 64.
Note that performance continued to improve even
when the number of unlabeled data reached more than
ten thousands. This suggests that we can further improve
the performance for some confusion sets by using a very
large corpus containing more than one hundred million
words.
Figure 2 also indicates that the use of unlabeled data
was not effective when the amount of unlabeled data was
smaller than one thousand. It is often the case with mi-
nor words that the number of occurrences does not reach
one thousand even in a one-hundred-million word corpus.
Thus, constructing a very very large corpus (containing
75
80
85
90
95
100
100 1000
Pr
ec
is
io
n 
(%
)
Number of Labeled Examples
NB
NB+EM
NB+EM+CDC
Figure 1: Relationship between Average Precision and
the Amount of Labeled Data
60
65
70
75
80
85
90
95
100
1000 10000 100000
Pr
ec
is
io
n 
(%
)
Number of Unlabeled Examples
I, me
principal, principle
passed, past
Figure 2: Relationship between Precision and the
Amount of Unlabeled Data
more than billions of words) appears to be beneficial for
infrequent words.
6 Related Work
Nigam et al(2000) reported that the accuracy of text clas-
sification can be improved by a large pool of unlabeled
documents using a naive Bayes classifier and the EM al-
gorithm. They presented two extensions to the basic EM
algorithm. One is a weighting factor to modulate the con-
tribution of the unlabeled data. The other is the use of
multiple mixture components per class. With these exten-
sions, they reported that the use of unlabeled data reduces
classification error by up to 30%.
Pedersen et al(1998) employed the EM algorithm and
Gibbs Sampling for word sense disambiguation by using
a naive Bayes classifier. Although Gibbs Sampling re-
sults in a small improvement over the EM algorithm, the
results for verbs and adjectives did not reach baseline per-
formance on average. The amount of unlabeled data used
in their experiments was relatively small (from several
hundreds to a few thousands).
Yarowsky (1995) presented an approach that signif-
icantly reduces the amount of labeled data needed for
word sense disambiguation. Yarowsky achieved accura-
cies of more than 90% for two-sense polysemous words.
This success was likely due to the use of ?one sense per
discourse? characteristic of polysemous words.
Yarowsky?s approach can be viewed in the context of
co-training (Blum and Mitchell, 1998) in which the fea-
tures can be split into two independent sets. For word
sense disambiguation, the sets correspond to the local
contexts of the target word and the ?one sense per dis-
course? characteristic. Confusion sets however do not
have the latter characteristic.
The effect of a huge amount of unlabeled data for
confusion set disambiguation is discussed in (Banko and
Brill, 2001). Bank and Brill conducted experiments of
committee-based unsupervised learning for two confu-
sion sets. Their results showed that they gained a slight
improvement by using a certain amount of unlabeled
data. However, test set accuracy began to decline as ad-
ditional data were harvested.
As for the performance of confusion set disambigua-
tion, Golding (1999) achieved over 96% by a winnow-
based approach. Although our results are not directly
comparable with their results since the data sets are
different, our results does not reach the state-of-the-
art performance. Because the performance of a naive
Bayes classifier is significantly affected by the smoothing
method used for paramter estimation, there is a chance to
improve our performance by using a more sophisticated
smoothing technique.
7 Conclusion
The naive Bayes classifier can be combined with the well-
established EM algorithm to exploit the unlabeled data
. However, the use of unlabeled data sometimes causes
disastrous degradation of classification performance.
In this paper, we introduce a class distribution con-
straint into the iteration process of the EM algorithm.
This constraint keeps the class distribution of unlabeled
data consistent with the true class distribution estimated
from labeled data, preventing the EM algorithm from
converging into an undesirable state.
Experimental results using 26 confusion sets and a
large amount of unlabeled data showed that combining
the EM algorithm with our proposed constraint consis-
tently reduced the average classification error rates when
the amount of labeled data is small. The results also
showed that use of unlabeled data is especially advan-
tageous when the amount of labeled data is small (up to
about one hundred).
7.1 Future Work
In this paper, we empirically demonstrated that a class
distribution constraint reduced the chance of undesirable
convergence of the EM algorithm. However, the theoret-
ical justification of this constraint should be clarified in
future work.
References
Michele Banko and Eric Brill. 2001. Scaling to very very
large corpora for natural language disambiguation. In
Proceedings of the Association for Computational Lin-
guistics.
Adam L. Berger, Stephen A. Della Pietra, and Vincent J.
Della Pietra. 1996. A maximum entropy approach to
natural language processing. Computational Linguis-
tics, 22(1):39?71.
Avrim Blum and Tom Mitchell. 1998. Combin-
ing labeled and unlabeled data with co-training. In
COLT: Proceedings of the Workshop on Computa-
tional Learning Theory, Morgan Kaufmann Publish-
ers.
Michael Collins and Yoram Singer. 1999. Unsupervised
models for named entity classification. In Proceedings
of the Joint SIGDAT Conference on Empirical Meth-
ods in Natural Language Processing and Very Large
Corpora.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the em
algorithm. Royal Statstical Society B 39, pages 1?38.
G. Escudero, L. arquez, and G. Rigau. 2000. Naive bayes
and exemplar-based approaches to word sense disam-
biguation revisited. In Proceedings of the 14th Euro-
pean Conference on Artificial Intelligence.
Andrew R. Golding and Dan Roth. 1999. A winnow-
based approach to context-sensitive spelling correc-
tion. Machine Learning, 34(1-3):107?130.
Thorsten Joachims. 1999. Transductive inference for
text classification using support vector machines. In
Proc. 16th International Conf. on Machine Learning,
pages 200?209. Morgan Kaufmann, San Francisco,
CA.
David D. Lewis. 1998. Naive Bayes at forty: The in-
dependence assumption in information retrieval. In
Claire Ne?dellec and Ce?line Rouveirol, editors, Pro-
ceedings of ECML-98, 10th European Conference on
Machine Learning, number 1398, pages 4?15, Chem-
nitz, DE. Springer Verlag, Heidelberg, DE.
Lidia Mangu and Eric Brill. 1997. Automatic rule acqui-
sition for spelling correction. In Proc. 14th Interna-
tional Conference on Machine Learning, pages 187?
194. Morgan Kaufmann.
Andrew McCallum and Kamal Nigam. 1998. A com-
parison of event models for naive bayes text classifica-
tion. In AAAI-98 Workshop on Learning for Text Cat-
egorization.
Kamal Nigam and Rayid Ghani. 2000. Analyzing the ef-
fectiveness and applicability of co-training. In CIKM,
pages 86?93.
Kamal Nigam, Andrew Kachites Mccallum, Sebastian
Thrun, and Tom Mitchell. 2000. Text classification
from labeled and unlabeled documents using EM. Ma-
chine Learning, 39(2/3):103?134.
Ted Pedersen and Rebecca Bruce. 1998. Knowledge
lean word-sense disambiguation. In AAAI/IAAI, pages
800?805.
Ted Pedersen. 2000. A simple approach to building en-
sembles of naive bayesian classifiers for word sense
disambiguation. In Proceedings of the First Annual
Meeting of the North American Chapter of the Asso-
ciation for Computational Linguistics, pages 63?69,
Seattle, WA, May.
David M. W. Powers. 1998. Learning and application
of differential grammars. In T. Mark Ellison, editor,
CoNLL97: Computational Natural Language Learn-
ing, pages 88?96. Association for Computational Lin-
guistics, Somerset, New Jersey.
Adwait Ratnaparkhi. 1998. Maximum Entropy Models
for Natural Language Ambiguity Resolution. Ph.D.
thesis, the University of Pennsylvania.
Robert E. Schapire and Yoram Singer. 2000. Boostex-
ter: A boosting-based system for text categorization.
Machine Learning, 39(2/3):135?168.
Vladimir N. Vapnik. 1995. The Nature of Statistical
Learning Theory. New York.
David Yarowsky. 1994. Decision lists for lexical ambi-
guity resolution: Application to accent restoration in
spanish and french. In Meeting of the Association for
Computational Linguistics, pages 88?95.
David Yarowsky. 1995. Unsupervised word sense dis-
ambiguation rivaling supervised methods. Proc. of the
33rd Annual Meeting of the Association for Computa-
tional Linguistics, pages 189?196.
Boosting Precision and Recall of Dictionary-Based Protein Name
Recognition
Yoshimasa Tsuruoka?? and Jun?ichi Tsujii??
?Department of Computer Science, University of Tokyo
Hongo 7-3-1, Bunkyo-ku, Tokyo 113-0033 Japan
?CREST, JST (Japan Science and Technology Corporation)
Honcho 4-1-8, Kawaguchi-shi, Saitama 332-0012 Japan
{tsuruoka,tsujii}@is.s.u-tokyo.ac.jp
Abstract
Dictionary-based protein name recogni-
tion is the first step for practical infor-
mation extraction from biomedical doc-
uments because it provides ID informa-
tion of recognized terms unlike machine
learning based approaches. However, dic-
tionary based approaches have two se-
rious problems: (1) a large number of
false recognitions mainly caused by short
names. (2) low recall due to spelling vari-
ation. In this paper, we tackle the for-
mer problem by using a machine learning
method to filter out false positives. We
also present an approximate string search-
ing method to alleviate the latter prob-
lem. Experimental results using the GE-
NIA corpus show that the filtering using
a naive Bayes classifier greatly improves
precision with slight loss of recall, result-
ing in a much better F-score.
1 Introduction
The rapid increase of machine readable biomedical
texts (e.g. MEDLINE) makes automatic information
extraction from those texts much more attractive.
Especially extracting information of protein-protein
interactions from MEDLINE abstracts is regarded as
one of the most important tasks today (Marcotte et
al., 2001; Thomas et al, 2000; Ono et al, 2001).
To extract information of proteins, one has to first
recognize protein names in a text. This kind of prob-
lem has been studied in the field of natural language
processing as named entity recognition tasks. Ohta
et al (2002) provided the GENIA corpus, an an-
notated corpus of MEDLINE abstracts, which can
be used as a gold-standard for evaluating and train-
ing named entity recognition algorithms. There
are some research efforts using machine learning
techniques to recognize biological entities in texts
(Takeuchi and Collier, 2002; Kim and Tsujii, 2002;
Kazama et al, 2002).
One drawback of these machine learning based
approaches is that they do not provide identification
information of recognized terms. For the purpose
of information extraction of protein-protein interac-
tion, the ID information of recognized proteins, such
as GenBank 1 ID or SwissProt 2 ID, is indispensable
to integrate the extracted information with the data
in other information sources.
Dictionary-based approaches, on the other hand,
intrinsically provide ID information because they
recognize a term by searching the most similar
(or identical) one in the dictionary to the target
term. This advantage currently makes dictionary-
based approaches particularly useful as the first step
for practical information extraction from biomedical
documents (Ono et al, 2001).
However, dictionary-based approaches have two
serious problems. One is a large number of false
positives mainly caused by short names, which sig-
nificantly degrade overall precision. Although this
problem can be avoided by excluding short names
from the dictionary, such a solution makes it impos-
sible to recognize short protein names. We tackle
1GenBank is one of the largest genetic sequence databases.
2The Swiss-Prot is an annotated protein sequence database.
this problem by using a machine learning technique.
Each recognized candidate is checked if it is really
protein name or not by a classifier trained on an an-
notated corpus.
The other problem of dictionary based approaches
is spelling variation. For example, the protein name
?NF-Kappa B? has many spelling variants such as
?NF Kappa B,? ?NF kappa B,? ?NF kappaB,? and
?NFkappaB.? Exact matching techniques, however,
regard these terms as completely different terms.
We alleviate this problem by using an approximate
string matching method in which surface-level sim-
ilarities between terms are considered.
This paper is organized as follows. Section 2
describes the overview of our method. Section 3
presents the approximate string searching algorithm
for candidate recognition. Section 3 describes how
to filter out false recognitions by a machine learning
method. Section 5 presents the experimental results
using the GENIA corpus. Some related work is de-
scribed in Section 6. Finally, Section 7 offers some
concluding remarks.
2 Method Overview
Our protein name recognition method consists of
two phases. In the first phase, we scan the text for
protein name candidates using a dictionary. In the
second phase, we check each candidate whether it is
really protein name or not using a machine learning
method. We call these two phases recognition phase
and filtering phase respectively. The overview of the
method is given below.
? Recognition phase
Protein name candidates are identified using a
protein name dictionary. To alleviate the prob-
lem of spelling variation, we use an approxi-
mate string matching technique.
? Filtering phase
Every protein name candidates is classified into
?accepted? or ?rejected? by a classifier. The
classifier uses the context of the term and the
term itself as the features for the classification.
Only ?accepted? candidates are recognized as
protein names.
In the following sections, we describe the details
of each phase.
21234-
223451
32123R
43212G
43211E
43210
2-RG
134
3451
313R
431G
4311E
4310
-RG
Figure 1: Dynamic Programming Matrix
3 Candidate Recognition
The most straightforward way to exploit a dictio-
nary for candidate recognition is the exact (longest)
match algorithm. For exact match, many fast match-
ing algorithms (e.g. Boyer-Moore algorithm (1977))
have been proposed. However, the existence of
many spelling variations for the same protein name
makes the exact matching less attractive. For exam-
ple, even a short protein name ?EGR-1? has at least
the six following variations:
EGR-1, EGR 1, Egr-1, Egr 1, egr-1, egr 1.
Since longer protein names have a huge number
of possible variations, it is impossible to enrich the
dictionary by expanding each protein name as de-
scribed above.
3.1 Approximate String Searching
To deal with the problem of spelling variation, we
need a kind of ?elastic? matching algorithm, by
which a recognition system scan a text to find a sim-
ilar term to (if any) a protein name in the dictio-
nary. We need a similarity measure to do such a task.
The most popular measure of similarity between
two strings is edit distance, which is the minimum
number of operations on individual characters (e.g.
substitutions, insertions, and deletions) required to
transform one string of symbols into another. For ex-
ample, the edit distance between ?EGR-1? and ?GR-
2? is two, because one substitution (1 for 2) and one
deletion (E) are required.
To calculate the edit distance between two strings,
we can use a dynamic programming technique. Fig-
ure 1 illustrates an example. For clarity of presen-
tation, all costs are assumed to be 1. The matrix
C0..|x|,0..|y| is filled, where Ci,j represents the mini-
mum number of operations needed to match x1..i to
y1..j . This is computed as follows (Navarro, 1998)
Ci,0 = i (1)
C0,j = j (2)
Ci,j = if (xi = yj) then Ci?1,j?1 (3)
else 1 + min(Ci?1,j , Ci,j?1, Ci?1,j?1)
The calculation can be done by either a row-
wise left-to-right traversal or a column-wise top-to-
bottom traversal.
There are many fast algorithms other than the dy-
namic programming for uniform-cost edit distance,
where the weight of each edit operation is constant
within the same type (Navarro, 2001). However,
what we expect is that the distance between ?EGR-
1? and ?EGR 1? will be smaller than that between
?EGR-1? and ?FGR-1?, while the uniform-cost edit
distances of them are equal.
The dynamic programming based method is flex-
ible enough to allow us to define arbitrary costs for
individual operations depending on a letter being op-
erated. For example, we can make the cost of the
substitution between a space and a hyphen much
lower than that of the substitution between ?E? and
?F.? Therefore, we use the dynamic programming
based method for our task.
Table 1 shows the cost function used in our ex-
periments. Both insertion and deletion costs are 100
except for spaces and hyphens. Substitution costs
for similar letters are 10. Substitution costs for the
other different letters are 50.
3.2 String Searching
We have described a method for calculating the
similarity between two strings in the previous sec-
tion. However, what we need is approximate string
searching in which the recognizer scans a text to
find a similar term to (if any) a term in the dictio-
nary. The dynamic programming based method can
be easily extended for approximate string searching.
The method is illustrated in Figure 2. The pro-
tein name to be matched is ?EGR-1? and the text
to be scanned is ?encoded by EGR include.? String
searching can be done by just setting the elements
corresponding separators (e.g. space) in the first row
Table 1: Cost Function
Operation Letter Cost
Insertion space or hyphen 10
Other letters 100
Deletion space or hyphen 10
Other letters 100
Substitution A letter for the same letter 0
A numeral for a numeral 10
space for hyphen 10
hyphen for space 10
A capital letter for the
corresponding small letter 10
A small letter for the
corresponding capital letter 10
Other letters 50
to zero. After filling the whole matrix, one can find
that ?EGR-1? can be matched to this text at the place
of ?EGR 1? with cost 1 by searching for the lowest
value in the bottom row.
To take into account the length of a term, we adopt
a normalized cost, which is calculated by dividing
the cost by the length of the term:
(nomalized cost) = (cost) + ?(length of the term) (4)
where ? is a constant value 3. When the costs of two
terms are the same, the longer one is preferred due
to this constant.
To recognize a protein name in a given text, we
perform the above calculation for every term con-
tained in the dictionary and select the term that has
the lowest normalized cost.
If the normalized cost is lower than the predefined
threshold. The corresponding range in the text is
recognized as a protein name candidate.
3.3 Implementation Issues for String Searching
A naive way for string searching using a dictionary
is to conduct the procedure described in the previ-
ous section one by one for every term in the dictio-
nary. However, since the size of the dictionary is
very large, this naive method takes too much time to
perform a large scale experiment.
3? was set to 0.4 in our experiments.
7654444321123444476544444-
76555432122345555765555551
6
6
6
6
d
7
7
7
7
e
54333321012333376543333R
54322222101222276543222G
54321111210121176543211E
54321010321021076543210
ulcni1RGEybdedocne
65444432112344446544444
655543212234555565555551
6
6
6
6
d e
5433332101233336543333R
5432222210122226543222G
5432111121012116543211E
5432101032102106543210
ulcni1RGEybdedocne
Figure 2: Example of String Searching using Dynamic Programming Matrix
Navarro (2001) have presented a way to reduce
redundant calculations by constructing a trie of the
dictionary. The trie is used as a device to avoid
repeating the computation of the cost against same
prefix of many patterns. Suppose that we have just
calculated the cost of the term ?EGR-1? and next we
have to calculate the cost of the term ?EGR-2,? it is
clear that we do not have to re-calculated the first
four rows in the matrix (see Figure 2). They also
pointed out that it is possible to determine, prior to
reaching the bottom of the matrix, that the current
term cannot produce any relevant match: if all the
values of the current row are larger than the thresh-
old, then a match cannot occur since we can only
increase the cost or at best keep it the same.
4 Filtering Candidates by a Naive Bayes
Classifier
One of the serious problems of dictionary-based
recognition is a large number of false recognitions
mainly caused by short entries in the dictionary. For
example, the dictionary constructed from GenBank
contains an entry ?NK.? However, the word ?NK?
is frequently used as a part of the term ?NK cells.?
In this case, ?NK? is an abbreviation of ?natural
killer? and is not a protein name. Therefore this en-
try makes a large number of false recognitions lead-
ing to low precision performance.
In the filtering phase, we use a classifier trained on
an annotated corpus to suppress such kind of false
recognition. The objective of this phase is to im-
prove precision without the loss of recall.
We conduct binary classification (?accept? or ?re-
ject?) on each candidate. The candidates that are
classified into ?rejected? are filtered out. In other
words, only the candidates that are classified into
?accepted? are recognized as protein names.
In this paper, we use a naive Bayes classifier for
this classification task.
4.1 Naive Bayes classifier
The naive Bayes classifier is a simple but effective
classifier which has been used in numerous applica-
tions of information processing such as image recog-
nition, natural language processing and information
retrieval (Lewis, 1998; Escudero et al, 2000; Peder-
sen, 2000; Nigam and Ghani, 2000).
Here we briefly review the naive Bayes model.
Let ~x be a vector we want to classify, and ck be a
possible class. What we want to know is the prob-
ability that the vector ~x belongs to the class ck. We
first transform the probability P (ck|~x) using Bayes?
rule,
P (ck|~x) = P (ck) ?
P (~x|ck)
P (~x) (5)
Class probability P (ck) can be estimated from train-
ing data. However, direct estimation of P (ck|~x) is
impossible in most cases because of the sparseness
of training data.
By assuming the conditional independence
among the elements of a vector, P (~x|ck) is
decomposed as follows,
P (~x|ck) =
d
?
j=1
P (xj|ck), (6)
where xj is the jth element of vector ~x. Then Equa-
tion 5 becomes
P (ck|~x) = P (ck) ?
?d
j=1 P (xj |ck)
P (~x) (7)
By this equation, we can calculate P (ck|~x) and clas-
sify ~x into the class with the highest P (ck|~x).
There are some implementation variants of the
naive Bayes classifier depending on their event mod-
els (McCallum and Nigam, 1998). In this paper, we
adopt the multi-variate Bernoulli event model.
4.2 Features
As the input of the classifier, the features of the tar-
get must be represented in the form of a vector. We
use a binary feature vector which contains only the
values of 0 or 1 for each element.
In this paper, we use the local context surround-
ing a candidate term and the words contained in the
term as the features. We call the former contextual
features and the latter term features.
The features used in our experiments are given be-
low.
? Contextual Features
W?1 : the preceding word.
W+1 : the following word.
? Term Features
Wbegin : the first word of the term.
Wend : the last word of the term.
Wmiddle : the other words of the term without
positional information (bag-of-words).
Suppose the candidate term is ?putative zinc fin-
ger protein, ? and the sentence is:
... encoding a putative zinc finger protein was
found to derepress beta- galactosidase ...
We obtain the following active features for this
example.
{W?1 a}, {W+1 was}, {Wbegin putative}, {Wend
protein}, {Wmiddle zinc}, {Wmiddle finger}.
4.3 Training
The training of the classifier is done with an anno-
tated corpus. We first scan the corpus for protein
name candidates by dictionary matching. If a recog-
nized candidate is annotated as a protein name, this
candidate and its context are used as a positive (?ac-
cepted?) example for training. Otherwise, it is used
as a negative (?rejected?) example.
5 Experiment
5.1 Corpus and Dictionary
We conducted experiments of protein name recogni-
tion using the GENIA corpus version 3.01 (Ohta et
al., 2002). The GENIA corpus is an annotated cor-
pus, which contains 2000 abstracts extracted from
MEDLINE database. These abstracts are selected
from the search results with MeSH terms Human,
Blood Cells, and Transcription Factors.
The biological entities in the corpus are annotated
according to the GENIA ontology. Although the
corpus has many categories such as protein, DNA,
RNA, cell line and tissue, we used only the protein
category. When a term was recursively annotated,
only the innermost (shortest) annotation was consid-
ered.
The test data was created by randomly selecting
200 abstracts from the corpus. The remaining 1800
abstracts were used as the training data. The protein
name dictionary was constructed from the training
data by gathering all the terms that were annotated
as proteins.
Each recognition was counted as correct if the
both boundaries of the recognized term exactly
matched the boundaries of an annotation in the cor-
pus.
5.2 Improving Precision by Filtering
We first conducted experiments to evaluate how
much precision is improved by the filtering process.
In the recognition phase, the longest matching algo-
rithm was used for candidate recognition.
The results are shown in Table 2. F-measure is de-
fined as the harmonic mean for precision and recall
as follows:
F = 2 ? precision ? recallprecision + recall (8)
Table 2: Precision Improvement by Filtering
Precision Recall F-measure
w/o filtering 48.6 70.7 57.6
with filtering 74.3 65.3 69.5
Table 3: Recall Improvement by Approximate
String Search
Threshold Precision Recall F-measure
1.0 72.6 39.5 51.2
2.0 73.7 63.7 68.3
3.0 74.0 66.5 70.1
4.0 73.9 66.8 70.2
5.0 73.4 67.1 70.1
6.0 73.6 67.1 70.2
7.0 73.5 67.2 70.2
8.0 73.1 67.4 70.2
9.0 72.9 67.8 70.2
10.0 72.6 67.7 70.0
The first row shows the performances achieved
without filtering. In this case, all the candidates
identified in the recognition phase are regarded as
protein names. The second row shows the perfor-
mance achieved with filtering by the naive Bayes
classifier. In this case, only the candidates that are
classified into ?accepted? are regarded as protein
names. Notice that the filtering significantly im-
proved the precision (from 48.6% to 74.3%) with
slight loss of the recall. The F-measure was also
greatly improved (from 57.6% to 69.5%).
5.3 Improving Recall by Approximate String
Search
We also conducted experiments to evaluate how
much we can further improve the recognition per-
formance by using the approximate string search-
ing method described in Section 3. Table 3 shows
the results. The leftmost columns show the thresh-
olds of the normalized costs for approximate string
searching. As the threshold increased, the preci-
sion degraded while the recall improved. The best
F-measure was 70.2%, which is better than that of
exact matching by 0.7% (see Table 2).
Table 4: Performance using Different Feature Set
Feature Set Precision Recall F-measure
Contextual 61.0 62.6 61.8
features
Term 71.3 67.9 69.5
features
All features 73.5 67.2 70.2
5.4 Efficacy of Contextual Features
The advantage of using a machine learning tech-
nique is that we can exploit the context of a candi-
date for deciding whether it is really protein name or
not. In order to evaluate the efficacy of contexts, we
conducted experiments using different feature sets.
The threshold of normalized cost was set to 7.0.
Table 4 shows the results. The first row shows the
performances achieved by using only contextual fea-
tures. The second row shows those achieved by us-
ing only term features. The performances achieved
by using both feature sets are shown in the third row.
The results indicate that candidate terms them-
selves are strong cues for classification. However,
the fact that the best performance was achieved
when both feature sets were used suggests that the
context of a candidate conveys useful information
about the semantic class of the candidate.
6 Related Work
Kazama et al (2002) reported an F-measure of
56.5% on the GENIA corpus (Version 1.1) using
Support Vector Machines. Collier et al (2001)
reported an F-measure of 75.9% evaluated on 100
MEDLINE abstracts using a Hidden Markov Model.
These research efforts are machine learning based
and do not provide ID information of recognized
terms.
Krauthammer et al (2000) proposed a dictionary-
based gene/protein name recognition method. They
used BLAST for approximate string matching by
mapping sequences of text characters into sequences
of nucleotides that can be processed by BLAST.
They achieved a recall of 78.8% and a precision of
71.1% by a partial match criterion, which is less
strict than our exact match criterion.
7 Conclusion
In this paper we propose a two-phase protein name
recognition method. In the first phase, we scan texts
for protein name candidates using a protein name
dictionary and an approximate string searching tech-
nique. In the second phase, we filter the candidates
using a machine learning technique.
Since our method is dictionary-based, it can pro-
vide ID information of recognized terms unlike ma-
chine learning based approaches. False recognition,
which is a common problem of dictionary-based ap-
proaches, is suppressed by a classifier trained on an
annotated corpus.
Experimental results using the GENIA corpus
show that the filtering using a naive Bayes classi-
fier greatly improves precision with slight loss of re-
call. We achieved an F-measure of 70.2% for protein
name recognition on the GENIA corpus.
The future direction of this research involves:
? Use of state-of-the-art classifiers
We have used a naive Bayes classifier in our
experiments because it requires a small com-
putational resource and exhibits good perfor-
mance. There is a chance, however, to improve
performance by using state-of-the-art machine
learning techniques including maximum en-
tropy models and support vector machines.
? Use of other elastic matching algorithms
We have restricted the computation of similar-
ity to edit distance. However, it is not uncom-
mon that the order of the words in a protein
name is altered, for example,
?beta-1 integrin?
?integrin beta-1?
The character-level edit distance cannot capture
this -kind of similarities.
References
Robert S. Boyer and J. Strother Moore. 1977. A fast
string searching algorithm. Communications of the
ACM, 20(10):762?772.
Nigel Collier, Chikashi Nobata, and Junichi Tsujii. 2001.
Automatic acquisition and classification of molecular
biology terminology using a tagged corpus. Journal of
Terminology, 7(2):239?258.
G. Escudero, L. arquez, and G. Rigau. 2000. Naive bayes
and exemplar-based approaches to word sense disam-
biguation revisited. In Proceedings of the 14th Euro-
pean Conference on Artificial Intelligence.
Jun?ichi Kazama, Takaki Makino, Yoshihiro Ohta, and
Jun?ichi Tsujii. 2002. Tuning support vector machines
for biomedical named entity recognition. In Proceed-
ings of the ACL-02 Workshop on Natural Language
Processing in the Biomedical Domain.
Jin Dong Kim and Jun?ichi Tsujii. 2002. Corpus-based
approach to biological entity recognition. In Text Data
Mining SIG (ISMB2002).
Michael Krauthammer, Andrey Rzhetsky, Pavel Moro-
zov, and Carol Friedman. 2000. Using BLAST for
identifying gene and protein names in journal articles.
Gene, 259:245?252.
David D. Lewis. 1998. Naive Bayes at forty: The inde-
pendence assumption in information retrieval. In Pro-
ceedings of ECML-98, 10th European Conference on
Machine Learning, number 1398, pages 4?15.
Edward M. Marcotte, Ioannis Xenarios, and David Eisen-
berg. 2001. Mining literature for protein-protein inter-
actions. BIOINFORMATICS, 17(4):359?363.
Andrew McCallum and Kamal Nigam. 1998. A com-
parison of event models for naive bayes text classifi-
cation. In AAAI-98 Workshop on Learning for Text
Categorization.
G. Navarro, R. Baeza-Yates, and J.M. Arcoverde. 2001.
Matchsimile: A flexible approximate matching tool for
personal names searching. In Proceedings of the XVI
Brazilian Symposium on Databases (SBBD?2001),
pages 228?242.
Gonzalo Navarro. 1998. Approximate Text Searching.
Ph.D. thesis, Dept. of Computer Science, Univ. of
Chile.
Gonzalo Navarro. 2001. A guided tour to approximate
string matching. ACM Computing Surveys, 33(1):31?
88.
Kamal Nigam and Rayid Ghani. 2000. Analyzing the ef-
fectiveness and applicability of co-training. In CIKM,
pages 86?93.
Tomoko Ohta, Yuka Tateishi, Hideki Mima, and Jun?ichi
Tsujii. 2002. Genia corpus: an annotated research
abstract corpus in molecular biology domain. In Pro-
ceedings of the Human Language Technology Confer-
ence.
Toshihide Ono, Haretsugu Hishigaki, Akira Tanigami,
and Toshihisa Takagi. 2001. Automated extraction
of information on protein-protein interactions from the
biological literature. BIOINFORMATICS, 17(2):155?
161.
Ted Pedersen. 2000. A simple approach to building en-
sembles of naive bayesian classifiers for word sense
disambiguation. In Proceedings of the First Annual
Meeting of the North American Chapter of the Associ-
ation for Computational Linguistics, pages 63?69.
K. Takeuchi and N. Collier. 2002. Use of support vec-
tor machines in extended named entity recognition. In
Proceedings of the 6th Conference on Natural Lan-
guage Learning 2002 (CoNLL-2002), pages 119?125.
James Thomas, David Milward, Christos Ouzounis,
Stephen Pulman, and Mark Carroll. 2000. Automatic
extraction of protein interactions from scientific ab-
stracts. In Proceedings of the Pacific Symposium on
Biocomputing (PSB2000), volume 5, pages 502?513.
Proceedings of the ACL-ISMB Workshop on Linking Biological Literature, Ontologies and Databases: Mining
Biological Semantics, pages 25?31, Detroit, June 2005. c?2005 Association for Computational Linguistics
A Machine Learning Approach to Acronym Generation
Yoshimasa Tsuruoka
 
 
CREST
Japan Science and Technology Agency
Japan
Sophia Ananiadou
School of Computing
Salford University
United Kingdom
tsuruoka@is.s.u-tokyo.ac.jp
S.Ananiadou@salford.ac.uk
tsujii@is.s.u-tokyo.ac.jp
Jun?ichi Tsujii  

Department of Computer Science
The University of Tokyo
Japan
Abstract
This paper presents a machine learning
approach to acronym generation. We for-
malize the generation process as a se-
quence labeling problem on the letters in
the definition (expanded form) so that a
variety of Markov modeling approaches
can be applied to this task. To con-
struct the data for training and testing, we
extracted acronym-definition pairs from
MEDLINE abstracts and manually anno-
tated each pair with positional informa-
tion about the letters in the acronym. We
have built an MEMM-based tagger using
this training data set and evaluated the
performance of acronym generation. Ex-
perimental results show that our machine
learning method gives significantly bet-
ter performance than that achieved by the
standard heuristic rule for acronym gen-
eration and enables us to obtain multi-
ple candidate acronyms together with their
likelihoods represented in probability val-
ues.
1 Introduction
Technical terms and named-entities play important
roles in knowledge integration and information re-
trieval in the biomedical domain. However, spelling
variations make it difficult to identify the terms con-
veying the same concept because they are written
in different manners. Acronyms constitute a major
part of spelling variations (Nenadic et al, 2002), so
proper management of acronyms leads to improved
performance of the information systems in this do-
main.
As for the methods for recognizing acronym-
definition pairs from running text, there are many
studies reporting high performance (e.g. over 96%
accuracy and 82% recall) (Yoshida et al, 2000; Ne-
nadic et al, 2002; Schwartz and Hearst, 2003; Za-
hariev, 2003; Adar, 2004). However, another aspect
that we have to consider for efficient acronym man-
agement is to generate acronyms from the given def-
inition (expanded form).
One obvious application of acronym generation
is to expand the keywords in information retrieval.
As reported in (Wren et al, 2005), for example,
you can retrieve only 25% of the documents con-
cerning the concept of ?JNK? by using the key-
word ?c-jun N-terminal kinase?. In more than 33%
of the documents the concept is written with its
acronym ?JNK?. To alleviate this problem, some
research efforts have been devoted to constructing
a database containing a large number of acronym-
definition pairs from running text of biomedical doc-
uments (Adar, 2004).
However, the major problem of this database-
building approach is that building the database offer-
ing complete coverage is nearly impossible because
not all the biomedical documents are publicly avail-
able. Although most of the abstracts of biomedical
papers are publicly available on MEDLINE, there
is still a large number of full-papers which are not
available.
In this paper, we propose an alternative approach
25
to providing acronyms from their definitions so
that we can obtain acronyms without consulting
acronym-definition databases.
One of the simplest way to generate acronyms
from definitions would be to choose the letters at the
beginning of each word and capitalize them. How-
ever, there are a lot of exceptions in the acronyms
appearing in biomedical documents. The followings
are some real examples of the definition-acronym
pairs that cannot be created with the simple heuristic
method.
RNA polymerase (RNAP)
antithrombin (AT)
melanoma cell adhesion molecule (Mel-CAM)
the xenoestrogen 4-tert-octylphenol (t-OP)
In this paper we present a machine learning ap-
proach to automatic generation of acronyms in order
to capture a variety of mechanisms of acronym gen-
eration. We formalize this problem as a sequence
labeling task such as part-of-speech tagging, chunk-
ing and other natural language tagging tasks so that
common Markov modeling approaches can be ap-
plied to this task.
2 Acronym Generation as a Sequence
Labeling Problem
Given the definition (expanded form), the mecha-
nism of acronym generation can be regarded as the
task of selecting the appropriate action on each letter
in the definition.
Figure 1 illustrates an example, where the defini-
tion is ?Duck interferon gamma? and the generated
acronym is ?DuIFN-gamma?. The generation pro-
ceeds as follows:
The acronym generator outputs the first
two letters unchanged and skips the fol-
lowing three letters. Then the generator
capitalizes ?i? and skip the following four
letters...
By assuming that an acronym is made up of alpha-
numeric letters, spaces and hyphens, the actions be-
ing taken by the generator are classified into the fol-
lowing five classes.
  SKIP
The generator skips the letter.
  UPPER
If the target letter is uppercase, the generator
outputs the same letter. If the target letter is
lowercase, the generator coverts the letter into
the corresponding upper letter.
  LOWER
If the target letter is lowercase, the generator
outputs the same letter. If the target letter is
uppercase, the generator coverts the letter into
the corresponding lowercase letter.
  SPACE
The generator convert the letter into a space.
  HYPHEN
The generator convert the letter into a hyphen.
From the probabilistic modeling point of view,
this task is to find the sequence of actions 
that maximizes the following probability given the
observation 	
	  	 


	
 (1)
Observations are the letters in the definition and
various types of features derived from them. We de-
compose the probability in a left-to-right manner.






	



ffProceedings of the Ninth International Workshop on Parsing Technologies (IWPT), pages 103?114,
Vancouver, October 2005. c?2005 Association for Computational Linguistics
Efficacy of Beam Thresholding, Unification Filtering and Hybrid
Parsing in Probabilistic HPSG Parsing
Takashi Ninomiya
CREST, JST
and
Department of Computer Science
The University of Tokyo
ninomi@is.s.u-tokyo.ac.jp
Yoshimasa Tsuruoka
CREST, JST
and
Department of Computer Science
The University of Tokyo
tsuruoka@is.s.u-tokyo.ac.jp
Yusuke Miyao
Department of Computer Science
The University of Tokyo
yusuke@is.s.u-tokyo.ac.jp
Jun?ichi Tsujii
Department of Computer Science
The University of Tokyo
and
School of Informatics
University of Manchester
and
CREST, JST
tsujii@is.s.u-tokyo.ac.jp
Abstract
We investigated the performance efficacy
of beam search parsing and deep parsing
techniques in probabilistic HPSG parsing
using the Penn treebank. We first tested
the beam thresholding and iterative pars-
ing developed for PCFG parsing with an
HPSG. Next, we tested three techniques
originally developed for deep parsing: quick
check, large constituent inhibition, and hy-
brid parsing with a CFG chunk parser. The
contributions of the large constituent inhi-
bition and global thresholding were not sig-
nificant, while the quick check and chunk
parser greatly contributed to total parsing
performance. The precision, recall and av-
erage parsing time for the Penn treebank
(Section 23) were 87.85%, 86.85%, and 360
ms, respectively.
1 Introduction
We investigated the performance efficacy of beam
search parsing and deep parsing techniques in
probabilistic head-driven phrase structure grammar
(HPSG) parsing for the Penn treebank. We first
applied beam thresholding techniques developed for
CFG parsing to HPSG parsing, including local
thresholding, global thresholding (Goodman, 1997),
and iterative parsing (Tsuruoka and Tsujii, 2005b).
Next, we applied parsing techniques developed for
deep parsing, including quick check (Malouf et al,
2000), large constituent inhibition (Kaplan et al,
2004) and hybrid parsing with a CFG chunk parser
(Daum et al, 2003; Frank et al, 2003; Frank, 2004).
The experiments showed how each technique con-
tributes to the final output of parsing in terms of
precision, recall, and speed for the Penn treebank.
Unification-based grammars have been extensively
studied in terms of linguistic formulation and com-
putation efficiency. Although they provide precise
linguistic structures of sentences, their processing is
considered expensive because of the detailed descrip-
tions. Since efficiency is of particular concern in prac-
tical applications, a number of studies have focused
on improving the parsing efficiency of unification-
based grammars (Oepen et al, 2002). Although sig-
nificant improvements in efficiency have been made,
parsing speed is still not high enough for practical
applications.
The recent introduction of probabilistic models of
wide-coverage unification-based grammars (Malouf
and van Noord, 2004; Kaplan et al, 2004; Miyao
and Tsujii, 2005) has opened up the novel possibil-
ity of increasing parsing speed by guiding the search
path using probabilities. That is, since we often re-
quire only the most probable parse result, we can
compute partial parse results that are likely to con-
tribute to the final parse result. This approach has
been extensively studied in the field of probabilistic
103
CFG (PCFG) parsing, such as Viterbi parsing and
beam thresholding.
While many methods of probabilistic parsing for
unification-based grammars have been developed,
their strategy is to first perform exhaustive pars-
ing without using probabilities and then select the
highest probability parse. The behavior of their al-
gorithms is like that of the Viterbi algorithm for
PCFG parsing, so the correct parse with the high-
est probability is guaranteed. The interesting point
of this approach is that, once the exhaustive pars-
ing is completed, the probabilities of non-local de-
pendencies, which cannot be computed during pars-
ing, are computed after making a packed parse for-
est. Probabilistic models where probabilities are as-
signed to the CFG backbone of the unification-based
grammar have been developed (Kasper et al, 1996;
Briscoe and Carroll, 1993; Kiefer et al, 2002), and
the most probable parse is found by PCFG parsing.
This model is based on PCFG and not probabilis-
tic unification-based grammar parsing. Geman and
Johnson (Geman and Johnson, 2002) proposed a dy-
namic programming algorithm for finding the most
probable parse in a packed parse forest generated by
unification-based grammars without expanding the
forest. However, the efficiency of this algorithm is
inherently limited by the inefficiency of exhaustive
parsing.
In this paper we describe the performance of beam
thresholding, including iterative parsing, in proba-
bilistic HPSG parsing for a large-scale corpora, the
Penn treebank. We show how techniques developed
for efficient deep parsing can improve the efficiency
of probabilistic parsing. These techniques were eval-
uated in experiments on the Penn Treebank (Marcus
et al, 1994) with the wide-coverage HPSG parser de-
veloped by Miyao et al (Miyao et al, 2005; Miyao
and Tsujii, 2005).
2 HPSG and probabilistic models
HPSG (Pollard and Sag, 1994) is a syntactic theory
based on lexicalized grammar formalism. In HPSG,
a small number of schemata describe general con-
struction rules, and a large number of lexical en-
tries express word-specific characteristics. The struc-
tures of sentences are explained using combinations
of schemata and lexical entries. Both schemata and
lexical entries are represented by typed feature struc-
tures, and constraints represented by feature struc-
tures are checked with unification.
Figure 1 shows an example of HPSG parsing of
the sentence ?Spring has come.? First, each of the
lexical entries for ?has? and ?come? is unified with a
daughter feature structure of the Head-Complement
Spring
HEAD  noun
SUBJ  < >
COMPS  < > 2
HEAD  verb
SUBJ  <    >
COMPS  <    >
1
has
HEAD  verb
SUBJ  <    >
COMPS  < >
1
come
2
head-comp
HEAD  verb
SUBJ  < >
COMPS  < >
HEAD  noun
SUBJ  < >
COMPS  < >
1
=?
Spring
HEAD  noun
SUBJ  < >
COMPS  < > 2
HEAD  verb
SUBJ  <    >
COMPS  <    >
1
has
HEAD  verb
SUBJ  <    >
COMPS  < >
1
come
2
HEAD  verb
SUBJ  <    >
COMPS  < >
1
HEAD  verb
SUBJ  < >
COMPS  < >
1
subject-head
head-comp
Figure 1: HPSG parsing
Schema. Unification provides the phrasal sign of
the mother. The sign of the larger constituent is
obtained by repeatedly applying schemata to lexi-
cal/phrasal signs. Finally, the parse result is output
as a phrasal sign that dominates the sentence.
Given set W of words and set F of feature struc-
tures, an HPSG is formulated as a tuple, G = ?L,R?,
where
L = {l = ?w,F ?|w ? W, F ? F} is a set of lexical
entries, and
R is a set of schemata, i.e., r ? R is a partial
function: F ? F ? F .
Given a sentence, an HPSG computes a set of phrasal
signs, i.e., feature structures, as a result of parsing.
Previous studies (Abney, 1997; Johnson et al,
1999; Riezler et al, 2000; Miyao et al, 2003; Mal-
ouf and van Noord, 2004; Kaplan et al, 2004; Miyao
and Tsujii, 2005) defined a probabilistic model of
unification-based grammars as a log-linear model or
maximum entropy model (Berger et al, 1996). The
probability of parse result T assigned to given sen-
tence w = ?w1, . . . , wn? is
p(T |w) = 1Zw
exp
(
?
i
?ifi(T )
)
Zw =
?
T ?
exp
(
?
i
?ifi(T ?)
)
,
where ?i is a model parameter, and fi is a feature
function that represents a characteristic of parse tree
T . Intuitively, the probability is defined as the nor-
malized product of the weights exp(?i) when a char-
acteristic corresponding to fi appears in parse result
T . Model parameters ?i are estimated using numer-
104
ical optimization methods (Malouf, 2002) so as to
maximize the log-likelihood of the training data.
However, the above model cannot be easily esti-
mated because the estimation requires the computa-
tion of p(T |w) for all parse candidates assigned to
sentence w. Because the number of parse candidates
is exponentially related to the length of the sentence,
the estimation is intractable for long sentences.
To make the model estimation tractable, Ge-
man and Johnson (Geman and Johnson, 2002) and
Miyao and Tsujii (Miyao and Tsujii, 2002) proposed
a dynamic programming algorithm for estimating
p(T |w). They assumed that features are functions
on nodes in a packed parse forest. That is, parse tree
T is represented by a set of nodes, i.e., T = {c}, and
the parse forest is represented by an and/or graph
of the nodes. From this assumption, we can redefine
the probability as
p(T |w) = 1Zw
exp
(
?
c?T
?
i
?ifi(c)
)
Zw =
?
T ?
exp
(
?
c?T ?
?
i
?ifi(c)
)
.
A packed parse forest has a structure similar to a
chart of CFG parsing, and c corresponds to an edge
in the chart. This assumption corresponds to the
independence assumption in PCFG; that is, only
a nonterminal symbol of a mother is considered in
further processing by ignoring the structure of its
daughters. With this assumption, we can compute
the figures of merit (FOMs) of partial parse results.
This assumption restricts the possibility of feature
functions that represent non-local dependencies ex-
pressed in a parse result. Since unification-based
grammars can express semantic relations, such as
predicate-argument relations, in their structure, the
assumption unjustifiably restricts the flexibility of
probabilistic modeling. However, previous research
(Miyao et al, 2003; Clark and Curran, 2004; Kaplan
et al, 2004) showed that predicate-argument rela-
tions can be represented under the assumption of
feature locality. We thus assumed the locality of fea-
ture functions and exploited it for the efficient search
of probable parse results.
3 Techniques for efficient deep
parsing
Many of the techniques for improving the parsing
efficiency of deep linguistic analysis have been de-
veloped in the framework of lexicalized grammars
such as lexical functional grammar (LFG) (Bresnan,
1982), lexicalized tree adjoining grammar (LTAG)
(Shabes et al, 1988), HPSG (Pollard and Sag, 1994)
or combinatory categorial grammar (CCG) (Steed-
man, 2000). Most of them were developed for ex-
haustive parsing, i.e., producing all parse results that
are given by the grammar (Matsumoto et al, 1983;
Maxwell and Kaplan, 1993; van Noord, 1997; Kiefer
et al, 1999; Malouf et al, 2000; Torisawa et al, 2000;
Oepen et al, 2002; Penn and Munteanu, 2003). The
strategy of exhaustive parsing has been widely used
in grammar development and in parameter training
for probabilistic models.
We tested three of these techniques.
Quick check Quick check filters out non-unifiable
feature structures (Malouf et al, 2000). Sup-
pose we have two non-unifiable feature struc-
tures. They are destructively unified by travers-
ing and modifying them, and then finally they
are found to be not unifiable in the middle of the
unification process. Quick check quickly judges
their unifiability by peeping the values of the
given paths. If one of the path values is not
unifiable, the two feature structures cannot be
unified because of the necessary condition of uni-
fication. In our implementation of quick check,
each edge had two types of arrays. One con-
tained the path values of the edge?s sign; we
call this the sign array. The other contained the
path values of the right daughter of a schema
such that its left daughter is unified with the
edge?s sign; we call this a schema array. When
we apply a schema to two edges, e1 and e2, the
schema array of e1 and the sign array of e2 are
quickly checked. If it fails, then quick check re-
turns a unification failure. If it succeeds, the
signs are unified with the schemata, and the re-
sult of unification is returned.
Large constituent inhibition (Kaplan et al,
2004) It is unlikely for a large medial edge to
contribute to the final parsing result if it spans
more than 20 words and is not adjacent to the
beginning or ending of the sentence. Large
constituent inhibition prevents the parser from
generating medial edges that span more than
some word length.
HPSG parsing with a CFG chunk parser A
hybrid of deep parsing and shallow parsing
was recently found to improve the efficiency
of deep parsing (Daum et al, 2003; Frank et
al., 2003; Frank, 2004). As a preprocessor, the
shallow parsing must be very fast and achieve
high precision but not high recall so that the
105
procedure Viterbi(?w1, . . . , wn?, ?L?, R?, ?, ?, ?)
for i = 1 to n
foreach Fu ? {F |?wi, F ? ? L}
? =
?
i ?ifi(Fu)
pi[i? 1, i]? pi[i? 1, i] ? {Fu}
if (? > ?[i? 1, i, Fu]) then
?[i? 1, i, Fu]? ?
for d = 1 to n
for i = 0 to n? d
j = i + d
for k = i + 1 to j ? 1
foreach Fs ? pi[i, k], Ft ? pi[k, j], r ? R
if F = r(Fs, Ft) has succeeded
? = ?[i, k, Fs] + ?[k, j, Ft] +
?
i ?ifi(F )
pi[i, j]? pi[i, j] ? {F}
if (? > ?[i, j, F ]) then
?[i, j, F ]? ?
Figure 2: Pseudo-code of Viterbi algorithms for probabilistic HPSG parsing
total parsing performance in terms of precision,
recall and speed is not degraded. Because there
is trade-off between speed and accuracy in
this approach, the total parsing performance
for large-scale corpora like the Penn treebank
should be measured. We introduce a CFG
chunk parser (Tsuruoka and Tsujii, 2005a) as a
preprocessor of HPSG parsing. Chunk parsers
meet the requirements for preprocessors; they
are very fast and have high precision. The
grammar for the chunk parser is automatically
extracted from the CFG treebank translated
from the HPSG treebank, which is generated
during grammar extraction from the Penn
treebank. The principal idea of using the chunk
parser is to use the bracket information, i.e.,
parse trees without non-terminal symbols, and
prevent the HPSG parser from generating edges
that cross brackets.
4 Beam thresholding for HPSG
parsing
4.1 Simple beam thresholding
Many algorithms for improving the efficiency of
PCFG parsing have been extensively investigated.
They include grammar compilation (Tomita, 1986;
Nederhof, 2000), the Viterbi algorithm, controlling
search strategies without FOM such as left-corner
parsing (Rosenkrantz and Lewis II, 1970) or head-
corner parsing (Kay, 1989; van Noord, 1997), and
with FOM such as the beam search, the best-first
search or A* search (Chitrao and Grishman, 1990;
Caraballo and Charniak, 1998; Collins, 1999; Rat-
naparkhi, 1999; Charniak, 2000; Roark, 2001; Klein
and Manning, 2003). The beam search and best-
first search algorithms significantly reduce the time
required for finding the best parse at the cost of los-
ing the guarantee of finding the correct parse.
The CYK algorithm, which is essentially a bottom-
up parser, is a natural choice for non-probabilistic
HPSG parsers. Many of the constraints are ex-
pressed as lexical entries in HPSG, and bottom-up
parsers can use those constraints to reduce the search
space in the early stages of parsing.
For PCFG, extending the CYK algorithm to out-
put the Viterbi parse is straightforward (Ney, 1991;
Jurafsky and Martin, 2000). The parser can effi-
ciently calculate the Viterbi parse by taking the max-
imum of the probabilities of the same nonterminal
symbol in each cell. With the probabilistic model
defined in Section 2, we can also define the Viterbi
search for unification-based grammars (Geman and
Johnson, 2002). Figure 2 shows the pseudo-code of
Viterbi algorithm. The pi[i, j] represents the set of
partial parse results that cover words wi+1, . . . , wj ,
and ?[i, j, F ] stores the maximum FOM of partial
parse result F at cell (i, j). Feature functions are
defined over lexical entries and results of rule appli-
cations, which correspond to conjunctive nodes in a
feature forest. The FOM of a newly created partial
parse, F , is computed by summing the values of ? of
the daughters and an additional FOM of F .
The Viterbi algorithm enables various pruning
techniques to be used for efficient parsing. Beam
thresholding (Goodman, 1997) is a simple and effec-
tive technique for pruning edges during parsing. In
each cell of the chart, the method keeps only a por-
tion of the edges which have higher FOMs compared
to the other edges in the same cell.
106
procedure BeamThresholding(?w1, . . . , wn?, ?L?, R?, ?, ?, ?)
for i = 1 to n
foreach Fu ? {F |?wi, F ? ? L}
? =
?
i ?ifi(Fu)
pi[i? 1, i]? pi[i? 1, i] ? {Fu}
if (? > ?[i? 1, i, Fu]) then
?[i? 1, i, Fu]? ?
for d = 1 to n
for i = 0 to n? d
j = i + d
for k = i + 1 to j ? 1
foreach Fs ? pi[i, k], Ft ? pi[k, j], r ? R
if F = r(Fs, Ft) has succeeded
? = ?[i, k, Fs] + ?[k, j, Ft] +
?
i ?ifi(F )
pi[i, j]? pi[i, j] ? {F}
if (? > ?[i, j, F ]) then
?[i, j, F ]? ?
LocalThresholding(?, ?)
GlobalThresholding(n, ?)
procedure LocalThresholding(?, ?)
sort pi[i, j] according to ?[i, j, F ]
pi[i, j]? {pi[i, j]1, . . . , pi[i, j]?}
?max = maxF ?[i, j, F ]
foreach F ? pi[i, j]
if ?[i, j, F ] < ?max ? ?
pi[i, j]? pi[i, j]\{F}
procedure GlobalThresholding(n, ?)
f [0..n]? {0,?? ??, . . . ,??}
b[0..n]? {??,??, . . . ,??, 0}
#forward
for i = 0 to n? 1
for j = i + 1 to n
foreach F ? pi[i, j]
f [j]? max(f [j], f [i] + ?[i, j, F ])
#backward
for i = n? 1 to 0
for j = i + 1 to n
foreach F ? pi[i, j]
b[i]? max(b[i], b[j] + ?[i, j, F ])
#global thresholding
?max = f [n]
for i = 0 to n? 1
for j = i + 1 to n
foreach F ? pi[i, j]
if f [i] + ?[i, j, F ] + b[j] < ?max ? ? then
pi[i, j]? pi[i, j]\{F}
Figure 3: Pseudo-code of local beam search and global beam search algorithms for probabilistic HPSG
parsing
107
procedure IterativeBeamThresholding(w, G, ?0, ?0, ?0, ??, ??, ??, ?last, ?last, ?last)
?? ?0; ? ? ?0; ? ? ?0
loop while ? ? ?last and ? ? ?last and ? ? ?last
call BeamThresholding(w, G, ?, ?, ?)
if pi[1, n] 6= ? then exit
?? ? + ??; ? ? ? + ??; ? ? ? + ??
Figure 4: Pseudo-code of iterative beam thresholding
We tested three selection schemes for deciding
which edges to keep in each cell.
Local thresholding by number of edges Each
cell keeps the top ? edges based on their FOMs.
Local thresholding by beam width Each cell
keeps the edges whose FOM is greater than
?max ? ?, where ?max is the highest FOM
among the edges in the cell.
Global thresholding by beam width Each cell
keeps the edges whose global FOM is greater
than ?max??, where ?max is the highest global
FOM in the chart.
Figure 3 shows the pseudo-code of local beam
search, and global beam search algorithms for prob-
abilistic HPSG parsing. The code for local thresh-
olding is inserted at the end of the computation for
each cell. In Figure 3, pi[i, j]k denotes the k-th ele-
ment in sorted set pi[i, j]. We first take the first ?
elements that have higher FOMs and then remove
the elements with FOMs lower than ?max ? ?.
Global thresholding is also used for pruning edges,
and was originally proposed for CFG parsing (Good-
man, 1997). It prunes edges based on their global
FOM and the best global FOM in the chart. The
global FOM of an edge is defined as its FOM plus its
forward and backward FOMs, where the forward and
backward FOMs are rough estimations of the outside
FOM of the edge. The global thresholding is per-
formed immediately after each line of the CYK chart
is completed. The forward FOM is calculated first,
and then the backward FOM is calculated. Finally,
all edges with a global FOM lower than ?max ? ?
are pruned. Figure 3 gives further details of the al-
gorithm.
4.2 Iterative beam thresholding
We tested the iterative beam thresholding proposed
by Tsuruoka and Tsujii (2005b). We started the
parsing with a narrow beam. If the parser output
results, they were taken as the final parse results. If
the parser did not output any results, we widened the
Table 1: Abbreviations used in experimental results
num local beam thresholding by number
width local beam thresholding by width
global global beam thresholding by width
iterative iterative parsing with local beam
thresholding by number and width
chp parsing with CFG chunk parser
beam, and reran the parsing. We continued widen-
ing the beam until the parser output results or the
beam width reached some limit.
The pseudo-code is presented in Figure 4. It calls
the beam thresholding procedure shown in Figure 3
and increases parameters ? and ? until the parser
outputs results, i.e., pi[1, n] 6= ?.
Preserved iterative parsing Our implemented
CFG parser with iterative parsing cleared the
chart and edges at every iteration although the
parser regenerated the same edges using those
generated in the previous iteration. This is
because the computational cost of regenerating
edges is smaller than that of reusing edges to
which the rules have already been applied. For
HPSG parsing, the regenerating cost is even
greater than that for CFG parsing. In our
implementation of HPSG parsing, the chart
and edges were not cleared during the iterative
parsing. Instead, the pruned edges were marked
as thresholded ones. The parser counted the
number of iterations, and when edges were
generated, they were marked with the iteration
number, which we call the generation. If
edges were thresholded out, the generation was
replaced with the current iteration number plus
1. Suppose we have two edges, e1 and e2. The
grammar rules are applied iff both e1 and e2 are
not thresholded out, and the generation of e1
or e2 is equal to the current iteration number.
Figure 5 shows the pseudo-code of preserved
iterative parsing.
108
procedure BeamThresholding(?w1, . . . , wn?, ?L?, R?, ?, ?, ?, iternum)
for i = 1 to n
foreach Fu ? {F |?wi, F ? ? L}
? =
?
i ?ifi(Fu)
pi[i? 1, i]? pi[i? 1, i] ? {Fu}
if (? > ?[i? 1, i, Fu]) then
?[i? 1, i, Fu]? ?
for d = 1 to n
for i = 0 to n? d
j = i + d
for k = i + 1 to j ? 1
foreach Fs ? ?[i, k], Ft ? ?[k, j], r ? R
if gen[i, k, Fs] = iternum ? gen[k, j, Ft] = iternum
if F = r(Fs, Ft) has succeeded
gen[i, j, F ]? iternum
? = ?[i, k, Fs] + ?[k, j, Ft] +
?
i ?ifi(F )
pi[i, j]? pi[i, j] ? {F}
if (? > ?[i, j, F ]) then
?[i, j, F ]? ?
LocalThresholding(?, ?, iternum)
GlobalThresholding(n, ?, iternum)
procedure LocalThresholding(?, ?, iternum)
sort pi[i, j] according to ?[i, j, F ]
?[i, j]? {pi[i, j]1, . . . , pi[i, j]?}
?max = maxF ?[i, j, F ]
foreach F ? ?[i, j]
if ?[i, j, F ] < ?max ? ?
?[i, j]? ?[i, j]\{F}
foreach F ? (pi[i, j]? ?[i, j])
gen[i, j, F ]? iternum + 1
procedure GlobalThresholding(n, ?, iternum)
f [0..n]? {0,?? ??, . . . ,??}
b[0..n]? {??,??, . . . ,??, 0}
#forward
for i = 0 to n? 1
for j = i + 1 to n
foreach F ? pi[i, j]
f [j]? max(f [j], f [i] + ?[i, j, F ])
#backward
for i = n? 1 to 0
for j = i + 1 to n
foreach F ? pi[i, j]
b[i]? max(b[i], b[j] + ?[i, j, F ])
#global thresholding
?max = f [n]
for i = 0 to n? 1
for j = i + 1 to n
foreach F ? ?[i, j]
if f [i] + ?[i, j, F ] + b[j] < ?max ? ? then
?[i, j]? ?[i, j]\{F}
foreach F ? (pi[i, j]? ?[i, j])
gen[i, j, F ]? iternum + 1
procedure IterativeBeamThresholding(w, G, ?0, ?0, ?0, ??, ??, ??, ?last, ?last, ?last)
?? ?0; ? ? ?0; ? ? ?0; iternum = 0
loop while ? ? ?last and ? ? ?last and ? ? ?last
call BeamThresholding(w, G, ?, ?, ?, iternum)
if pi[1, n] 6= ? then exit
?? ? + ??; ? ? ? + ??; ? ? ? + ??; iternum? iternum + 1
Figure 5: Pseudo-code of preserved iterative parsing for HPSG
109
Table 2: Experimental results for development set (section 22) and test set (section 23)
Precision Recall F-score Avg. Time (ms) No. of failed sentences
development set 88.21% 87.32% 87.76% 360 12
test set 87.85% 86.85% 87.35% 360 15









        
  	 
  	      	  
         



	














        
  	 
  	      	  
         



	





Figure 7: Parsing time for the sentences in Section 24 of less than 15 words of Viterbi parsing (none) (Left)
and iterative parsing (iterative) (Right)

 
 
 


 
 
	 

 
 
               
                      Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 841?848
Manchester, August 2008
Modeling Latent-Dynamic in Shallow Parsing:
A Latent Conditional Model with Improved Inference
Xu Sun? Louis-Philippe Morency? Daisuke Okanohara? Jun?ichi Tsujii??
?Department of Computer Science, The University of Tokyo, Hongo 7-3-1, Tokyo, Japan
?USC Institute for Creative Technologies, 13274 Fiji Way, Marina del Rey, USA
?School of Computer Science, The University of Manchester, 131 Princess St, Manchester, UK
?{sunxu, hillbig, tsujii}@is.s.u-tokyo.ac.jp ?morency@ict.usc.edu
Abstract
Shallow parsing is one of many NLP tasks
that can be reduced to a sequence la-
beling problem. In this paper we show
that the latent-dynamics (i.e., hidden sub-
structure of shallow phrases) constitutes a
problem in shallow parsing, and we show
that modeling this intermediate structure
is useful. By analyzing the automatically
learned hidden states, we show how the
latent conditional model explicitly learn
latent-dynamics. We propose in this paper
the Best Label Path (BLP) inference algo-
rithm, which is able to produce the most
probable label sequence on latent condi-
tional models. It outperforms two existing
inference algorithms. With the BLP infer-
ence, the LDCRF model significantly out-
performs CRF models on word features,
and achieves comparable performance of
the most successful shallow parsers on the
CoNLL data when further using part-of-
speech features.
1 Introduction
Shallow parsing identifies the non-recursive cores
of various phrase types in text. The paradigmatic
shallow parsing problem is noun phrase chunking,
in which the non-recursive cores of noun phrases,
called base NPs, are identified. As the represen-
tative problem in shallow parsing, noun phrase
chunking has received much attention, with the de-
velopment of standard evaluation datasets and with
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
extensive comparisons among methods (McDon-
ald 2005; Sha & Pereira 2003; Kudo & Matsumoto
2001).
Syntactic contexts often have a complex under-
lying structure. Chunk labels are usually far too
general to fully encapsulate the syntactic behavior
of word sequences. In practice, and given the lim-
ited data, the relationship between specific words
and their syntactic contexts may be best modeled
at a level finer than chunk tags but coarser than
lexical identities. For example, in the noun phrase
(NP) chunking task, suppose that there are two lex-
ical sequences, ?He is her ?? and ?He gave her
? ?. The observed sequences, ?He is her? and
?He gave her?, would both be conventionally la-
beled by ?BOB?, where B signifies the ?beginning
NP?, and O the ?outside NP?. However, this label-
ing may be too general to encapsulate their respec-
tive syntactic dynamics. In actuality, they have dif-
ferent latent-structures, crucial in labeling the next
word. For ?He is her ??, the NP started by ?her? is
still incomplete, so the label for ? is likely to be I,
which conveys the continuation of the phrase, e.g.,
?[He] is [her brother]?. In contrast, for ?He gave
her ??, the phrase started by ?her? is normally self-
complete, and makes the next label more likely to
be B, e.g., ?[He] gave [her] [flowers]?.
In other words, latent-dynamics is an interme-
diate representation between input features and la-
bels, and explicitly modeling this can simplify the
problem. In particular, in many real-world cases,
when the part-of-speech tags are not available, the
modeling on latent-dynamics would be particu-
larly important.
In this paper, we model latent-dynamics in
shallow parsing by extending the Latent-Dynamic
Conditional Random Fields (LDCRFs) (Morency
et al 2007), which offer advantages over previ-
841
y y y1 2 m h h h1 2 m
y y y1 2 m
CRF LDCRF
x x x1 2 mx x x1 2 m
Figure 1: Comparison between CRF and LDCRF.
In these graphical models, x represents the obser-
vation sequence, y represents labels and h repre-
sents hidden states assigned to labels. Note that
only gray circles are observed variables. Also,
only the links with the current observation are
shown, but for both models, long range dependen-
cies are possible.
ous learning methods by explicitly modeling hid-
den state variables (see Figure 1). We expect LD-
CRFs to be particularly useful in those cases with-
out POS tags, though this paper is not limited to
this.
The inference technique is one of the most im-
portant components for a structured classification
model. In conventional models like CRFs, the op-
timal label path can be directly searched by using
dynamic programming. However, for latent condi-
tional models like LDCRFs, the inference is kind
of tricky, because of hidden state variables. In this
paper, we propose an exact inference algorithm,
the Best Label Path inference, to efficiently pro-
duce the optimal label sequence on LDCRFs.
The following section describes the related
work. We then review LDCRFs, and propose the
BLP inference. We further present a statistical
interpretation on learned hidden states. Finally,
we show that LDCRF-BLP is particularly effective
when pure word features are used, and when POS
tags are added, as existing systems did, it achieves
comparable results to the best reported systems.
2 Related Work
There is a wide range of related work on shallow
parsing. Shallow parsing is frequently reduced to
sequence labeling problems, and a large part of
previous work uses machine learning approaches.
Some approaches rely on k-order generative proba-
bilistic models of paired input sequences and label
sequences, such as HMMs (Freitag & McCallum
2000; Kupiec 1992) or multilevel Markov mod-
els (Bikel et al 1999). The generative model
provides well-understood training and inference
but requires stringent conditional independence as-
sumptions.
To accommodate multiple overlapping features
on observations, some other approaches view the
sequence labeling problem as a sequence of clas-
sification problems, including support vector ma-
chines (SVMs) (Kudo & Matsumoto 2001) and a
variety of other classifiers (Punyakanok & Roth
2001; Abney et al 1999; Ratnaparkhi 1996).
Since these classifiers cannot trade off decisions at
different positions against each other (Lafferty et
al. 2001), the best classifier based shallow parsers
are forced to resort to heuristic combinations of
multiple classifiers.
A significant amount of recent work has shown
the power of CRFs for sequence labeling tasks.
CRFs use an exponential distribution to model the
entire sequence, allowing for non-local dependen-
cies between states and observations (Lafferty et
al. 2001). Lafferty et al (2001) showed that CRFs
outperform classification models as well as HMMs
on synthetic data and on POS tagging tasks. As for
the task of shallow parsing, CRFs also outperform
many other state-of-the-art models (Sha & Pereira
2003; McDonald et al 2005).
When the data has distinct sub-structures, mod-
els that exploit hidden state variables are advanta-
geous in learning (Matsuzaki et al 2005; Petrov
et al 2007). Sutton et al (2004) presented an
extension to CRF called dynamic conditional ran-
dom field (DCRF) model. As stated by the authors,
training a DCRF model with unobserved nodes
(hidden variables) makes their approach difficult
to optimize. In the vision community, the LD-
CRF model was recently proposed by Morency et
al. (2007), and shown to outperform CRFs, SVMs,
and HMMs for visual sequence labeling.
In this paper, we introduce the concept of latent-
dynamics for shallow parsing, showing how hid-
den states automatically learned by the model
present similar characteristics. We will also pro-
pose an improved inference technique, the BLP,
for producing the most probable label sequence in
LDCRFs.
3 Latent-Dynamic Conditional Random
Fields
The task is to learn a mapping between a sequence
of observations x = x
1
, x
2
, . . . , x
m
and a sequence
of labels y = y
1
, y
2
, . . . , y
m
. Each y
j
is a class la-
842
bel for the j?th token of a word sequence and is a
member of a set Y of possible class labels. For
each sequence, the model also assumes a vector of
hidden state variables h = {h
1
, h
2
, . . . , h
m
}, which
are not observable in the training examples.
Given the above definitions, we define a latent
conditional model as follows:
P(y|x,?) =
?
h
P(y|h, x,?)P(h|x,?), (1)
where ? are the parameters of the model. The LD-
CRF model can seem as a natural extension of the
CRF model, and the CRF model can seem as a spe-
cial case of LDCRFs employing one hidden state
for each label.
To keep training and inference efficient, we re-
strict the model to have disjointed sets of hidden
states associated with each class label. Each h
j
is
a member of a set H
y
j
of possible hidden states for
the class label y
j
. We define H, the set of all pos-
sible hidden states to be the union of all H
y
j
sets.
Since sequences which have any h
j
< H
y
j
will by
definition have P(y|x,?) = 0, we can express our
model as:
P(y|x,?) =
?
h?H
y
1
?...?H
y
m
P(h|x,?), (2)
where P(h|x,?) is defined using the usual con-
ditional random field formulation: P(h|x,?) =
exp ??f(h|x)/
?
?h
exp ??f(h|x), in which f(h|x) is
the feature vector. Given a training set consisting
of n labeled sequences (x
i
, y
i
) for i = 1 . . . n, train-
ing is performed by optimizing the objective func-
tion to learn the parameter ?
?
:
L(?) =
n
?
i=1
log P(y
i
|x
i
,?) ? R(?). (3)
The first term of this equation is the conditional
log-likelihood of the training data. The second
term is the regularizer.
4 BLP Inference on Latent Conditional
Models
For testing, given a new test sequence x, we want
to estimate the most probable label sequence (Best
Label Path), y
?
, that maximizes our conditional
model:
y
?
= argmax
y
P(y|x,?
?
). (4)
In the CRF model, y
?
can be simply searched by
using the Viterbi algorithm. However, for latent
conditional models like LDCRF, the Best Label
Path y
?
cannot directly be produced by the Viterbi
algorithm because of the incorporation of hidden
states.
In this paper, we propose an exact inference al-
gorithm, the Best Label Path inference (BLP), for
producing the most probable label sequence y
?
on
LDCRF. In the BLP schema, top-n hidden paths
HP
n
= {h
1
,h
2
. . . h
n
} over hidden states are effi-
ciently produced by using A
?
search (Hart et al,
1968), and the corresponding probabilities of hid-
den paths P(h
i
|x,?) are gained. Thereafter, based
on HP
n
, the estimated probabilities of various la-
bel paths, P(y|x,?), can be computed by summing
the probabilities of hidden paths, P(h|x,?), con-
cerning the association between hidden states and
each class label:
P(y|x,?) =
?
h: h?H
y
1
?...?H
y
m
?h?HP
n
P(h|x,?). (5)
By using the A
?
search, HP
n
can be extended in-
crementally in an efficient manner, until the algo-
rithm finds that the Best Label Path is ready, and
then the search stops and ends the BLP inference
with success. The algorithm judges that y
?
is ready
when the following condition is achieved:
P(y
1
|x,?) ? P(y
2
|x,?) +
?
h<H
y
1
?...?H
y
m
P(h|x,?), (6)
where y
1
is the most probable label sequence, and
y
2
is the second ranked label sequence estimated
by using HP
n
. It would be straightforward to prove
that y
?
= y
1
, and further search is unnecessary, be-
cause in this case, the unknown probability mass
can not change the optimal label path. The un-
known probability mass can be computed by using
?
h<H
y
1
?...?H
y
m
P(h|x,?) = 1 ?
?
h?H
y
1
?...?H
y
m
P(h|x,?). (7)
The top-n hidden paths of HP
n
produced by the
A
?
-search are exact, and the BLP inference is ex-
act. To guarantee HP
n
is exact in our BLP in-
ference, an admissible heuristic function should
be used in A
?
search (Hart et al, 1968). We use
a backward Viterbi algorithm (Viterbi, 1967) to
compute the heuristic function of the forward A
?
search:
Heu
i
(h
j
) = max
h
?
i
=h
j
?h
?
i
?HP
|h|
i
P
?
(h
?
|x,?
?
), (8)
843
where h
?
i
= h
j
represents a partial hidden path
started from the hidden state h
j
, and HP
|h|
i
rep-
resents all possible partial hidden paths from the
position i to the ending position |h| . Heu
i
(h
j
) is
an admissible heuristic function for the A
?
search
over hidden paths, therefore HP
n
is exact and BLP
inference is exact.
The BLP inference is efficient when the prob-
ability distribution among the hidden paths is in-
tensive. By combining the forward A
?
with the
backward Viterbi algorithm, the time complexity
of producing HP
n
is roughly a linear complexity
concerning its size. In practice, on the CoNLL test
data containing 2,012 sentences, the BLP infer-
ence finished in five minutes when using the fea-
ture set based on both word and POS information
(see Table 3). The memory consumption is also
relatively small, because it is an online style algo-
rithm and it is not necessary to preserve HP
n
.
In this paper, to make a comparison, we also
study the Best Hidden Path inference (BHP):
y
BHP
= argmax
y
P(h
y
|x,?
?
), (9)
where h
y
? H
y
1
? . . . ?H
y
m
. In other words, the
Best Hidden Path is the label sequence that is di-
rectly projected from the most probable hidden
path h
?
.
In (Morency et al 2007), y
?
is estimated by us-
ing the Best Point-wise Marginal Path (BMP). To
estimate the label y
j
of token j, the marginal prob-
abilities P(h
j
= a|x,?) are computed for possible
hidden states a ? H. Then, the marginal probabili-
ties are summed and the optimal label is estimated
by using the marginal probabilities.
The BLP produces y
?
while the BHP and the
BMP perform an estimation on y
?
. We will make
an experimental comparison in Section 6.
5 Analyzing Latent-Dynamics
The chunks in shallow parsing are represented with
the three labels shown in Table 1, and shallow pars-
ing is treated as a sequence labeling task with those
three labels. A challenge for most shallow parsing
approaches is to determine the concepts learned by
the model. In this section, we show how we can
analyze the latent-dynamics.
5.1 Analyzing Latent-Dynamics
In this section, we show how to analyze the charac-
teristics of the hidden states. Our goal is to find the
words characterizing a specific hidden state, and
B words beginning a chunk
I words continuing a chunk
O words being outside a chunk
Table 1: Shallow parsing labels.
then look at the selected words with their associ-
ated POS tags to determine if the LDCRF model
has learned meaningful latent-dynamics.
In the experiments reported in this section, we
did not use the features on POS tags in order to
isolate the model?s capability of learning latent dy-
namics. In other words, the model could simply
learn the dynamics of POS tags as the latent dy-
namics if the model is given the information about
POS tags. The features used in the experiments are
listed on the left side (Word Features) in Table 3.
The main idea is to look at the marginal proba-
bilities P(h
j
= a|x,?) for each word j, and select
the hidden state a
?
with the highest probability. By
counting how often a specific word selected a as
the optimal hidden state, i.e., ?(w, a), we can cre-
ate statistics about the relationship between hidden
states and words. We define relative frequency as
the number of times a specific word selected a hid-
den state while normalized by the global frequency
of this word:
RltFreq(w, h
j
) =
Freq( ?(w, h
j
) )
Freq(w)
. (10)
5.2 Learned Latent-Dynamics from CoNLL
In this subsection, we show the latent-dynamics
learned automatically from the CoNLL dataset.
The details of these experiments are presented in
the following section.
The most frequent three words corresponding to
the individual hidden states of the labels, B and O,
are shown in Table 2. As shown, the automati-
cally learned hidden states demonstrate prominent
characteristics. The extrinsic label B, which begins
a noun phrase, is automatically split into 4 sub-
categories: wh-determiners (WDT, such as ?that?)
together with wh-pronouns (WP, such as ?who?),
the determiners (DT, such as ?any, an, a?), the per-
sonal pronouns (PRP, such as ?they, we, he?), and
the singular proper nouns (NNP, such as ?Nasdaq,
Florida?) together with the plural nouns (NNS,
such as ?cities?). The results of B1 suggests that
the wh-determiners represented by ?that?, and the
wh-pronouns represented by ?who?, perform simi-
844
Labels HidStat Words POS RltFreq
B
That WDT 0.85
B1 who WP 0.49
Who WP 0.33
any DT 1.00
B2 an DT 1.00
a DT 0.98
They PRP 1.00
B3 we PRP 1.00
he PRP 1.00
Nasdaq NNP 1.00
B4 Florida NNP 0.99
cities NNS 0.99
O
But CC 0.88
O1 by IN 0.73
or IN 0.67
4.6 CD 1.00
O2 1 CD 1.00
11 CD 0.62
were VBD 0.94
O3 rose VBD 0.93
have VBP 0.92
been VBN 0.97
O4 be VB 0.94
to TO 0.92
Table 2: Latent-dynamics learned automatically by
the LDCRF model. This table shows the top three
words and their gold-standard POS tags for each
hidden states.
lar roles in modeling the dynamics in shallow pars-
ing. Further, the singular proper nouns and the
plural nouns are grouped together, suggesting that
they may perform similar roles. Moreover, we can
notice that B2 and B3 are highly consistent.
The label O is automatically split into the coordi-
nating conjunctions (CC) together with the prepo-
sitions (IN) indexed by O1, the cardinal numbers
(CD) indexed by O2, the past tense verbs (VBD)
together with the personal verbs (VBP) indexed by
O3, and another sub-category, O4. From the results
we can find that gold-standard POS tags may not
be adequate in modeling latent-dynamics in shal-
low parsing, as we can notice that three hidden
states out of four (O1, O3 and O4) contains relat-
ing but different gold-standard POS tags.
6 Experiments
Following previous studies on shallow parsing, our
experiments are performed on the CoNLL 2000
Word Features:
{w
i?2
, w
i?1
, w
i
, w
i+1
, w
i+2
, w
i?1
w
i
, w
i
w
i+1
}
?{h
i
, h
i?1
h
i
, h
i?2
h
i?1
h
i
}
POS Features:
{t
i?1
, t
i
, t
i+1
, t
i?2
t
i?1
, t
i?1
t
i
, t
i
t
i+1
, t
i+1
t
i+2
,
t
i?2
t
i?1
t
i
, t
i?1
t
i
t
i+1
, t
i
t
i+1
t
i+2
}
?{h
i
, h
i?1
h
i
, h
i?2
h
i?1
h
i
}
Table 3: Feature templates used in the experi-
ments. w
i
is the current word; t
i
is current POS
tag; and h
i
is the current hidden state (for the case
of latent models) or the current label (for the case
of conventional models).
data set (Sang & Buchholz 2000; Ramshow &
Marcus 1995). The training set consists of 8,936
sentences, and the test set consists of 2,012 sen-
tences. The standard evaluation metrics for this
task are precision p (the fraction of output chunks
matching the reference chunks), recall r (the frac-
tion of reference chunks returned), and the F-
measure given by F = 2pr/(p + r).
6.1 LDCRF for Shallow Parsing
We implemented LDCRFs in C++, and optimized
the system to cope with large scale problems, in
which the feature dimension is beyond millions.
We employ similar predicate sets defined in Sha
& Pereira (2003). We follow them in using predi-
cates that depend on words as well as POS tags in
the neighborhood of a given position, taking into
account only those 417,835 features which occur
at least once in the training data. The features are
listed in Table 3.
As for numerical optimization (Malouf 2002;
Wallach 2002), we performed gradient decent with
the Limited-Memory BFGS (L-BFGS) optimiza-
tion technique (Nocedal & Wright 1999). L-BFGS
is a second-order Quasi-Newton method that nu-
merically estimates the curvature from previous
gradients and updates. With no requirement on
specialized Hessian approximation, L-BFGS can
handle large-scale problems in an efficient manner.
We implemented an L-BFGS optimizer in C++ by
modifying the OWLQN package (Andrew & Gao
2007) developed by Galen Andrew. In our exper-
iments, storing 10 pairs of previous gradients for
the approximation of the function?s inverse Hes-
sian worked well, making the amount of the ex-
tra memory required modest. Using more pre-
vious gradients will probably decrease the num-
845
ber of iterations required to reach convergence,
but would increase memory requirements signifi-
cantly. To make a comparison, we also employed
the Conjugate-Gradient (CG) optimization algo-
rithm. For details of CG, see Shewchuk (1994).
Since the objective function of the LDCRF
model is non-convex, it is suggested to use the ran-
dom initialization of parameters for the training.
To reduce overfitting, we employed an L
2
Gaus-
sian weight prior (Chen & Rosenfeld 1999). Dur-
ing training and validation, we varied the number
of hidden states per label (from 2 to 6 states per
label), and also varied the L
2
-regularization term
(with values 10
k
, k from -3 to 3). Our experiments
suggested that using 4 or 5 hidden states per label
for the shallow parser is a viable compromise be-
tween accuracy and efficiency.
7 Results and Discussion
7.1 Performance on Word Features
As discussed in Section 4, it is preferred to not
use the features on POS tags in order to isolate
the model?s capability of learning latent dynam-
ics. In this sub-section, we use pure word fea-
tures with their counts above 10 in the training data
to perform experimental comparisons among dif-
ferent inference algorithms on LDCRFs, including
BLP, BHP, and existing BMP.
Since the CRF model is one of the success-
ful models in sequential labeling tasks (Lafferty et
al. 2001; Sha & Pereira 2003; McDonald et al
2005), in this section, we also compare LDCRFs
with CRFs. We tried to make experimental results
more comparable between LDCRF and CRF mod-
els, and have therefore employed the same fea-
tures set, optimizer and fine-tuning strategy be-
tween LDCRF and CRF models.
The experimental results are shown in Table 4.
In the table, Acc. signifies ?label accuracy?, which
is useful for the significance test in the follow-
ing sub-section. As shown, LDCRF-BLP outper-
forms LDCRF-BHP and LDCRF-BMP, suggesting
that BLP inference
1
is superior. The superiority
of BLP is statistically significant, which will be
shown in next sub-section. On the other side, all
the LDCRF models outperform the CRF model. In
particular, the gap between LDCRF-BLP and CRF
is 1.53 percent.
1
In practice, for efficiency, we approximated the BLP on a
few sentences by limiting the number of search steps.
Models: WF Acc. Pre. Rec. F
1
LDCRF-BLP 97.01 90.33 88.91 89.61
LDCRF-BHP 96.52 90.26 88.21 89.22
LDCRF-BMP 97.26 89.83 89.06 89.44
CRF 96.11 88.12 88.03 88.08
Table 4: Experimental comparisons among differ-
ent inference algorithms on LDCRFs, and the per-
formance of CRFs using the same feature set on
pure word features. The BLP inference outper-
forms the BHP and BMP inference. LDCRFs out-
perform CRFs.
Models F
1
Gap Acc. Gap Sig.
BLP vs. BHP 0.39 0.49 1e-10
BLP vs. CRF 1.53 0.90 5e-13
Table 5: The significance tests. LDCRF-BLP is
significantly more accurate than LDCRF-BHP and
CRFs.
7.2 Labeling Accuracy and Significance Test
As shown in Table 4, the accuracy rate for individ-
ual labeling decisions is over-optimistic as a mea-
sure for shallow parsing. Nevertheless, since test-
ing the significance of shallow parsers? F-measures
is tricky, individual labeling accuracy provides a
more convenient basis for statistical significance
tests (Sha & Pereira 2003). One such test is the
McNemar test on paired observations (Gillick &
Cox 1989). As shown in Table 5, for the LD-
CRF model, the BLP inference schema is sta-
tistically more accurate than the BHP inference
schema. Also, Evaluations show that the McNe-
mar?s value on labeling disagreement between the
LDCRF-BLP and CRF models is 5e-13, suggest-
ing that LDCRF-BLP is significantly more accu-
rate than CRFs.
On the other hand, the accuracy rate of BMP in-
ference is a special case. Since the BMP inference
is essentially an accuracy-first inference schema,
the accuracy rate and the F-measure have a differ-
ent relation in BMP. As we can see, the individual
labeling accuracy achieved by the LDCRF-BMP
model is as high as 97.26%, but its F-measure is
still lower than LDCRF-BLP.
7.3 Convergence Speed
It would be interesting to compare the convergence
speed between the objective loss function of LD-
CRFs and CRFs. We apply the L-BFGS optimiza-
846
 150
 200
 250
 300
 350
 400
 450
 500
 0  50  100  150  200  250
P
en
al
iz
ed
 L
os
s
Passes
LDCRF
CRF
Figure 2: The value of the penalized loss based on
the number of iterations: LDCRFs vs. CRFs.
 160
 180
 200
 220
 240
 260
 280
 0  50  100  150  200  250
P
en
al
iz
ed
 L
os
s
Passes
L-BFGS
CG
Figure 3: Training the LDCRF model: L-BFGS
vs. CG.
tion algorithm to optimize the loss function of LD-
CRF and CRF models, making a comparison be-
tween them. We find that the iterations required
for the convergence of LDCRFs is less than for
CRFs (see Figure 2). Normally, the LDCRF model
arrives at the plateau of convergence in 120-150
iterations, while CRFs require 210-240 iterations.
When we replace the L-BFGS optimizer by the CG
optimization algorithm, we observed as well that
LDCRF converges faster on iteration numbers than
CRF does.
On the contrary, however, the time cost of the
LDCRF model in each iteration is higher than the
CRF model, because of the incorporation of hid-
den states. The time cost of the LDCRF model
in each iteration is roughly a quadratic increase
concerning the increase of the number of hidden
states. Therefore, though the LDCRF model re-
quires less passes for the convergence, it is practi-
cally slower than the CRF model. Improving the
scalability of the LDCRF model would be a inter-
esting topic in the future.
Furthermore, we make a comparison between
Models: WF+POS Pre. Rec. F
1
LDCRF-BLP 94.65 94.03 94.34
CRF
N/A N/A 93.6
(Vishwanathan et al 06)
CRF
94.57 94.00 94.29
(McDonald et al 05)
Voted perceptron
N/A N/A 93.53
(Collins 02)
Generalized Winnow
93.80 93.99 93.89
(Zhang et al 02)
SVM combination
94.15 94.29 94.22
(Kudo & Matsumoto 01)
Memo. classifier
93.63 92.89 93.26
(Sang 00)
Table 6: Performance of the LDCRF-BLP model,
and the comparison with CRFs and other success-
ful approaches. In this table, all the systems have
employed POS features.
the L-BFGS and the CG optimizer for LDCRFs.
We observe that the L-BFGS optimizer is slightly
faster than CG on LDCRFs (see Figure 3), which
echoes the comparison between the L-BFGS and
the CG optimizing technique on the CRF model
(Sha & Pereira 2003).
7.4 Comparisons to Other Systems with POS
Features
Performance of the LDCRF-BLP model and some
of the best results reported previously are summa-
rized in Table 6. Our LDCRF model achieved
comparable performance to those best reported
systems in terms of the F-measure.
McDonald et al (2005) achieved an F-measure
of 94.29% by using a CRF model. By employing a
multi-model combination approach, Kudo & Mat-
sumoto (2001) also achieved a good performance.
They use a combination of 8 kernel SVMs with
a heuristic voting strategy. An advantage of LD-
CRFs over max-margin based approaches is that
LDCRFs can output N-best label sequences and
their probabilities using efficient marginalization
operations, which can be used for other compo-
nents in an information extraction system.
8 Conclusions and Future Work
In this paper, we have shown that automatic model-
ing on ?latent-dynamics? can be useful in shallow
parsing. By analyzing the automatically learned
847
hidden states, we showed how LDCRFs can natu-
rally learn latent-dynamics in shallow parsing.
We proposed an improved inference algorithm,
the BLP, for LDCRFs. We performed experiments
using the CoNLL data, and showed how the BLP
inference outperforms existing inference engines.
When further employing POS features as other
systems did, the performance of the LDCRF-BLP
model is comparable to those best reported results.
The LDCRF model demonstrates a significant ad-
vantage over other models on pure word features
in this paper. We expect it to be particularly useful
in the real-world tasks without rich features.
The latent conditional model handles latent-
dynamics naturally, and can be easily extended to
other labeling tasks. Also, the BLP inference algo-
rithm can be extended to other latent conditional
models for producing optimal label sequences. As
a future work, we plan to further speed up the BLP
algorithm.
Acknowledgments
Many thanks to Yoshimasa Tsuruoka for helpful
discussions on the experiments and paper-writing.
This research was partially supported by Grant-
in-Aid for Specially Promoted Research 18002007
(MEXT, Japan). The work at the USC Institute for
Creative Technology was sponsored by the U.S.
Army Research, Development, and Engineering
Command (RDECOM), and the content does not
necessarily reflect the position or the policy of the
Government, and no official endorsement should
be inferred.
References
Abney, S. 1991. Parsing by chunks. In R. Berwick, S. Ab-
ney, and C. Tenny, editors, Principle-based Parsing. Kluwer
Academic Publishers.
Abney, S.; Schapire, R. E. and Singer, Y. 1999. Boosting
applied to tagging and PP attachment. In Proc. EMNLP/VLC-
99.
Andrew, G. and Gao, J. 2007. Scalable training of L1-
regularized log-linear models. In Proc. ICML-07.
Bikel, D. M.; Schwartz, R. L. and Weischedel, R. M. 1999.
An algorithm that learns what?s in a name. Machine Learning,
34: 211-231.
Chen, S. F. and Rosenfeld, R. 1999. A Gaussian prior
for smooth-ing maximum entropy models. Technical Report
CMU-CS-99-108, CMU.
Collins, M. 2002. Discriminative training methods for hid-
den Markov models: Theory and experiments with perceptron
algorithms. In Proc. EMNLP-02.
Freitag, D. and McCallum, A. 2000. Information extrac-
tion with HMM structures learned by stochastic optimization.
In Proc. AAAI-00.
Gillick, L. and Cox, S. 1989. Some statistical issues in
the comparison of speech recognition algorithms. In Interna-
tional Conference on Acoustics Speech and Signal Process-
ing, v1, pages 532-535.
Hart, P.E.; Nilsson, N.J.; and Raphael, B. 1968. A formal
basis for the heuristic determination of minimum cost path.
IEEE Trans. On System Science and Cybernetics, SSC-4(2):
100-107.
Kudo, T. and Matsumoto, Y. 2001. Chunking with support
vector machines. In Proc. NAACL-01.
Kupiec, J. 1992. Robust part-of-speech tagging using a
hidden Markov model. Computer Speech and Language.
6:225-242.
Lafferty, J.; McCallum, A. and Pereira, F. 2001. Condi-
tional random fields: Probabilistic models for segmenting and
labeling sequence data. In Proc. ICML-01, pages 282-289.
Malouf, R. 2002. A comparison of algorithms for maxi-
mum entropy parameter estimation. In Proc. CoNLL-02.
Matsuzaki, T.; Miyao Y. and Tsujii, J. 2005. Probabilistic
CFG with Latent Annotations. In Proc. ACL-05.
McDonald, R.; Crammer, K. and Pereira, F. 2005. Flexible
Text Segmentation with Structured Multilabel Classification.
In Proc. HLT/EMNLP-05, pages 987- 994.
Morency, L.P.; Quattoni, A. and Darrell, T. 2007. Latent-
Dynamic Discriminative Models for Continuous Gesture
Recognition. In Proc. CVPR-07, pages 1- 8.
Nocedal, J. and Wright, S. J. 1999. Numerical Optimiza-
tion. Springer.
Petrov, S.; Pauls, A.; and Klein, D. 2007. Discriminative
log-linear grammars with latent variables. In Proc. NIPS-07.
Punyakanok, V. and Roth, D. 2001. The use of classifiers
in sequential inference. In Proc. NIPS-01, pages 995-1001.
MIT Press.
Ramshaw, L. A. and Marcus, M. P. 1995. Text chunking
using transformation-based learning. In Proc. Third Work-
shop on Very Large Corpora. In Proc. ACL-95.
Ratnaparkhi, A. 1996. A maximum entropy model for
part-of-speech tagging. In Proc. EMNLP-96.
Sang, E.F.T.K. 2000. Noun Phrase Representation by Sys-
tem Combination. In Proc. ANLP/NAACL-00.
Sang, E.F.T.K and Buchholz, S. 2000. Introduction to the
CoNLL-2000 shared task: Chunking. In Proc. CoNLL-00,
pages 127-132.
Sha, F. and Pereira, F. 2003. Shallow Parsing with Condi-
tional Random Fields. In Proc. HLT/NAACL-03.
Shewchuk, J. R. 1994. An introduction to the
conjugate gradient method without the agonizing pain.
http://www.2.cs.cmu.edu/jrs/jrspapers.html/#cg.
Sutton, C.; Rohanimanesh, K. and McCallum, A. 2004.
Dynamic conditional random fields: Factorized probabilistic
models for labeling and segmenting sequence data. In Proc.
ICML-04.
Viterbi, A.J. 1967. Error bounds for convolutional codes
and an asymptotically optimum decoding algorithm. IEEE
Transactions on Information Theory. 13(2):260-269.
Vishwanathan, S.; Schraudolph, N. N.; Schmidt, M.W. and
Murphy, K. 2006. Accelerated training of conditional random
fields with stochastic meta-descent. In Proc. ICML-06.
Wallach, H. 2002. Efficient training of conditional random
fields. In Proc. 6th Annual CLUK Research Colloquium.
Zhang, T.; Damerau, F. and Johnson, D. 2002. Text chunk-
ing based on a generalization of winnow. Journal of Machine
Learning Research, 2:615-637.
848
Coling 2010: Poster Volume, pages 126?134,
Beijing, August 2010
Improving Graph-based Dependency Parsing with Decision History
Wenliang Chen?, Jun?ichi Kazama?, Yoshimasa Tsuruoka?? and Kentaro Torisawa?
?Language Infrastructure Group, MASTAR Project, NICT
{chenwl, kazama, torisawa}@nict.go.jp
?School of Information Science, JAIST
tsuruoka@jaist.ac.jp
Abstract
This paper proposes an approach to im-
prove graph-based dependency parsing by
using decision history. We introduce a
mechanism that considers short dependen-
cies computed in the earlier stages of pars-
ing to improve the accuracy of long de-
pendencies in the later stages. This re-
lies on the fact that short dependencies are
generally more accurate than long depen-
dencies in graph-based models and may
be used as features to help parse long de-
pendencies. The mechanism can easily
be implemented by modifying a graph-
based parsing model and introducing a set
of new features. The experimental results
show that our system achieves state-of-
the-art accuracy on the standard PTB test
set for English and the standard Penn Chi-
nese Treebank (CTB) test set for Chinese.
1 Introduction
Dependency parsing is an approach to syntactic
analysis inspired by dependency grammar. In re-
cent years, interest in this approach has surged due
to its usefulness in such applications as machine
translation (Nakazawa et al, 2006), information
extraction (Culotta and Sorensen, 2004).
Graph-based parsing models (McDonald and
Pereira, 2006; Carreras, 2007) have achieved
state-of-the-art accuracy for a wide range of lan-
guages as shown in recent CoNLL shared tasks
(Buchholz et al, 2006; Nivre et al, 2007). How-
ever, to make parsing tractable, these models are
forced to restrict features over a very limited his-
tory of parsing decisions (McDonald and Pereira,
2006; McDonald and Nivre, 2007). Previous
work showed that rich features over a wide range
of decision history can lead to significant im-
provements in accuracy for transition-based mod-
els (Yamada and Matsumoto, 2003a; Nivre et al,
2004).
In this paper, we propose an approach to im-
prove graph-based dependency parsing by using
decision history. Here, we make an assumption:
the dependency relations between words with a
short distance are more reliable than ones between
words with a long distance. This is supported by
the fact that the accuracy of short dependencies
is in general greater than that of long dependen-
cies as reported in McDonald and Nivre (2007)
for graph-based models. Our idea is to use deci-
sion history, which is made in previous scans in a
bottom-up procedure, to help parse other words in
later scans. In the bottom-up procedure, short de-
pendencies are parsed earlier than long dependen-
cies. Thus, we introduce a mechanism in which
we treat short dependencies built earlier as deci-
sion history to help parse long dependencies in
later stages. It can easily be implemented by mod-
ifying a graph-based parsing model and designing
a set of features for the decision history.
To demonstrate the effectiveness of the pro-
posed approach, we present experimental results
on English and Chinese data. The results indi-
cate that the approach greatly improves the accu-
racy and that richer history-based features indeed
make large contributions. The experimental re-
sults show that our system achieves state-of-the-
art accuracy on the data.
2 Motivation
In this section, we present an example to show
the idea of using decision history in a dependency
parsing procedure.
Suppose we have two sentences in Chinese, as
shown in Figures 1 and 2, where the correct de-
pendencies are represented by the directed links.
For example, in Figure 1 the directed link from
126
w3:?(bought) to w5:?(books) mean that w3 is
the head and w5 is the dependent. In Chinese,
the relationship between clauses is often not made
explicit and two clauses may simply be put to-
gether with only a comma (Li and Thompson,
1997). This makes it hard to parse Chinese sen-
tences with several clauses.
ROOT
?? ? ? ? ? ??? ? ? ? ? ?(last year) (I) (bought) (NULL) (books) (,) (this year) (he) (also) (bought) (NULL) (books) w1         w2   w3         w4      w5      w6     w7        w8    w9     w10      w11      w12(Last year I bought some books and this year he also bought some books.)
Figure 1: Example A
ROOT
?? ? ? ? ? ??? ? ? ? ?(last year) (I) (bought) (NULL) (books) (,) (this year) (also) (bought) (NULL) (books) w1         w2   w3        w4        w5      w6     w7     w8        w9       w10        w11(Last year I bought some books and this year too)      
Figure 2: Example B
If we employ a graph-based parsing model,
such as the model of (McDonald and Pereira,
2006; Carreras, 2007), it is difficult to assign the
relations between w3 and w10 in Example A and
between w3 and w9 in Example B. For simplicity,
we use wAi to refer to wi of Example A and wBi to
refer to wi of Example B in what follows.
The key point is whether the second clauses are
independent in the sentences. The two sentences
are similar except that the second clause of Exam-
ple A is an independent clause but that of Exam-
ple B is not. wA10 is the root of the second clause
of Example A with subject wA8 , while wB9 is the
root of the second clause of Example B, but the
clause does not have a subject. These mean that
the correct decisions are to assign wA10 as the head
of wA3 and wB3 as the head of wB9 , as shown by the
dash-dot-lines in Figures 1 and 2.
However, the model can use very limited infor-
mation. Figures 3-(a) and 4-(a) show the right
dependency relation cases and Figures 3-(b) and
4-(b) show the left direction cases. For the right
direction case of Example A, the model has the
information about wA3 ?s rightmost child wA5 and
wA10?s leftmost child wA6 inside wA3 and wA10, but it
does not have information about the other children
?? ? ? ? ? ? ?? ? ? ? ? ?(last year) (I) (bought) (NULL) (books) (,) (this year) (he) (also) (bought) (NULL) (books) w1         w2   w3        w4        w5      w6     w7     w8    w9     w10        w11      w12(a)
?? ? ? ? ? ? ?? ? ? ? ? ?(last year) (I) (bought) (NULL) (books) (,) (this year) (he) (also) (bought) (NULL) (books)
(b)w1         w2   w3        w4        w5      w6     w7     w8    w9     w10        w11      w12
Figure 3: Example A: two directions
?? ? ? ? ? ??? ? ? ? ?(last year) (I) (bought) (NULL) (books) (,) (this year) (also) (bought) (NULL) (books) w1         w2   w3         w4    w5      w6  w7        w8     w9         w10        w11      (a)
?? ? ? ? ? ??? ? ? ? ?(last year) (I) (bought) (NULL) (books) ( ) (this year) (also) (bought) (NULL) (books)
(b)
,w1         w2   w3         w4    w5      w6  w7        w8     w9         w10        w11      
Figure 4: Example B: two directions
(such as wA8 ) of wA3 and wA10, which may be useful
for judging the relation between wA3 and wA10. The
parsing model can not find the difference between
the syntactic structures of two sentences for pairs
(wA3 ,wA10) and (wB3 ,wB9 ). If we can provide the in-
formation about the other children of wA3 and wA10
to the model, it becomes easier to find the correct
direction between wA3 and wA10.
Next, we show how to use decision history to
help parse wA3 and wA10 of Example A.
In a bottom up procedure, the relations between
the words inside [wA3 , wA10] are built as follows
before the decision for wA3 and wA10. In the first
round, we build relations for neighboring words
(word distance1=1), such as the relations between
wA3 and wA4 and between wA4 and wA5 . In the sec-
ond round, we build relations for words of dis-
tance 2, and then for longer distance words until
all the possible relations between the inside words
are built. Figure 5 shows all the possible relations
inside [wA3 , wA10] that we can build. To simplify,
we use undirected links to refer to both directions
1Word distance between wi and wj is |j ? i|.
127
of dependency relations between words in the fig-
ure.
?? ? ? ? ? ? ?? ? ? ? ? ?(last year) (I) (bought) (NULL) (books)   (,) (this year) (he) (also) (bought) (NULL) (books) w1         w2   w3         w4 w5        w6  w7        w8   w9     w10     w11      w12
Figure 5: Example A: first step
Then given those inside relations, we choose
the inside structure with the highest score for each
direction of the dependency relation between wA3
and wA10. Figure 6 shows the chosen structures.
Note that the chosen structures for two directions
could either be identical or different. In Figure
6-(a) and -(b), they are different.
?? ? ? ? ? ? ?? ? ? ? ? ?(last year) (I) (bought) (NULL) (books) (,) (this year) (he) (also) (bought) (NULL) (books) w1 w2 w3 w4 w5 w6 w7 w8 w9 w10 w11 w12(a)       
(b)
?? ? ? ? ? ? ?? ? ? ? ? ?(last year) (I) (bought) (NULL) (books) (,) (this year) (he) (also) (bought) (NULL) (books) w1         w2   w3        w4        w5      w6     w7     w8    w9     w10        w11      w12
Figure 6: Example A: second step
Finally, we use the chosen structures as deci-
sion history to help parse wA3 and wA10. For ex-
ample, the fact that wA8 is a dependent of wA10 is
a clue that suggests that the second clause may be
independent. This results in wA10 being the head of
wA3 .
This simple example shows how to use the de-
cision history to help parse the long distance de-
pendencies.
3 Background: graph-based parsing
models
Before we describe our method, we briefly intro-
duce the graph-based parsing models. We denote
input sentence w by w = (w0, w1, ..., wn), where
w0 = ROOT is an artificial root token inserted at
the beginning of the sentence and does not depend
on any other token in w and wi refers to a word.
We employ the second-order projective graph-
based parsing model of Carreras (2007), which is
an extension of the projective parsing algorithm of
Eisner (1996).
The parsing algorithms used in Carreras (2007)
independently find the left and right dependents of
a word and then combine them later in a bottom-
up style based on Eisner (1996). A subtree that
spans the words in [s, t] (and roots at s or t) is
represented by chart item [s, t, right/left, C/I],
where right (left) indicates that the root of the sub-
tree is s (t) and C means that the item is complete
while I means that the item is incomplete (Mc-
Donald, 2006). Here, complete item in the right
(left) direction means that the words other than s
(t) cannot have dependents outside [s, t] and in-
complete item in the right (left) direction, on the
other hand, means that t (s) may have dependents
outside [s, t]. In addition, t (s) is the direct depen-
dent of s (t) in the incomplete item with the right
(left) direction.
Larger chart items are created from pairs of
smaller chart items by the bottom-up procedure.
Figure 7 illustrates the cubic parsing actions of the
Eisner?s parsing algorithm (Eisner, 1996) in the
right direction, where s, r, and t refer to the start
and end indices of the chart items. In Figure 7-
(a), all the items on the left side are complete and
represented by triangles, where the triangle of [s,
r] is complete item [s, r,?, C] and the triangle of
[r + 1, t] is complete item [r + 1, t,?, C]. Then
the algorithm creates incomplete item [s, t,?, I]
(trapezoid on the right side of Figure 7-(a)) by
combining the chart items on the left side. This
action builds the dependency from s to t. In Fig-
ure 7-(b), the item of [s, r] is incomplete and
the item of [r, t] is complete. Then the algo-
rithm creates complete item [s, t,?, C]. For the
left direction case, the actions are similar. Note
that only the actions of creating the incomplete
chart items build new dependency relations be-
tween words, while the ones of creating the com-
plete items merge the existing structures without
building new relations.
Once the parser has considered the dependency
relations between words of distance 1, it goes on
128
to dependency relations between words of dis-
tance 2, and so on by the parsing actions. For
words of distance 2 and greater, it considers ev-
ery possible partition of the structures into two
parts and chooses the one with the highest score
for each direction. The score is the sum of the fea-
ture weights of the chart items. The features are
designed over edges of dependency trees and the
weights are given by model parameters (McDon-
ald and Pereira, 2006; Carreras, 2007). We store
the obtained chart items in a table. The chart item
includes the information on the optimal splitting
point of itself. Thus, by looking up the table, we
can obtain the best tree structure (with the highest
score) of any chart item.
s         r     r+1    t            s                   t
(a)
s         r     r t               s                 t
(b)
Figure 7: Cubic parsing actions of Eisner (1996)
4 Parsing with decision history
As mentioned above, the actions for creating
the incomplete items build the relations between
words. In this study, we only consider using his-
tory information when creating incomplete items.
4.1 Decision history
Suppose we are going to compute the scores of
the relations between ws and wt. There are two
possible directions for them.
By using the bottom-up style algorithm, the
scores of the structures between words with dis-
tance < |s?t| are computed in previous scans and
the structures are stored in the table. We divide
the decision history into two types: history-inside
and history-outside. The history-inside type is the
decision history made inside [s,t] and the history-
outside type is the history made outside [s,t].
4.1.1 History-inside
We obtain the structure with the highest score
for each direction of the dependency between ws
and wt. Figure 8-(b) shows the best solution (with
the highest score) of the left direction, where the
structure is split into two parts, [s, r1,?, C] and
[r1 + 1, t,?, C]. Figure 8-(c) shows the best so-
lution of the right case, where the structure is split
into two parts, [s, r2,?, C] and [r2 + 1, t,?, C].
s          r1 r1+1               t
ws ?                 wt (b)(a)
s r r +1 t2 2(c)
Figure 8: History-inside
By looking up the table, we have a subtree that
roots at ws on the right side of ws and a subtree
that roots at wt on the left side of wt. We use these
structures as the information on history-inside.
4.1.2 History-outside
For history-outside, we try to obtain the sub-
tree that roots at ws on the left side of ws and
the one that roots at wt on the right side of wt.
However, compared to history-inside, obtaining
history-outside is more complicated because we
do not know the boundaries and the proper struc-
tures of the subtrees. Here, we use an simple
heuristic method to find a subtree whose root is
at ws on the left side of ws and one whose root is
at wt on the right side of wt.
We introduce two assumptions: 1) The struc-
ture within a sub-sentence 2 is more reliable than
the one that goes across from sub-sentences. 2)
More context (more words) can result in a better
solution for determining subtree structures.
2To simplify, we split one sentence into sub-sentences
with punctuation marks.
129
Algorithm 1 Searching for history-outside
boundaries
1: Input: w, s, t
2: for k = s? 1 to 1 do
3: if(isPunct(wk)) break;
4: if(s? k >= t? s? 1) break
5: end for
6: bs = k
7: for k = t + 1 to |w| do
8: if(isPunct(wk)) break;
9: if(k ? t >= t? s? 1) break
10: end for
11: bt = k
12: Output: bs, bt
Under these two assumptions, Algorithm 1
shows the procedure for searching for history-
outside boundaries, where bs is the boundary for
for the descendants on the left side of ws , bt
is the boundary for searching the descendants on
the right side of wt, and isPunct is the function
that checks if the word is a punctuation mark. bs
should be in the same sub-sentence with s and
|s? bs| should be less than |t? s|. bt should be in
the same sub-sentence with t and |bt ? t| should
be less than |t? s|.
Next we try to find the subtree structures. First,
we collect the part-of-speech (POS) tags of the
heads of all the POS tags in training data and
remove the tags that occur fewer than 10 times.
Then, we determine the directions of the relations
by looking up the collected list. For bs and s, we
check if the POS tag of ws could be the head tag
of the POS tag of wbs by looking up the list. If
so, the direction d is ?. Otherwise, we check if
the POS tag of wbs could be the head tag of the
POS tag of ws. If so, d is ?, else d is ?. Fi-
nally, we obtain the subtree of ws from chart item
[bs, s, d, I]. Similarly, we obtain the subtree of wt.
Figure 9 shows the history-outside information for
ws and wt, where the relation between wbs and ws
and the relation between wbt and wt will be de-
termined by the above method. We have subtree
[rs, s, left, C] that roots at ws on the left side of
ws and subtree [t, rt, right, C] that roots at wt on
the right side of wt in Figure 9-(b) and (c).
4.2 Parsing algorithm
Then, we explain how to use these decision his-
tory in the parsing algorithm. We use Lst to rep-
bs rs s        t         rt bt(b)ws ?                 wt(a)
b r s t r b(c)s s t t
Figure 9: History-outside
resent the scores of basic features for the left di-
rection and Rst for the right case. Then we design
history-based features (described in Section 4.3)
based on the history-inside and history-outside in-
formation, as mentioned above. Finally, we up-
date the scores with the ones of the history-based
features by the following equations:
L+st = Lst + Ldfst (1)
R+st = Rst + Rdfst (2)
where L+st and R+st refer to the updated scores, Ldfst
and Rdfst refer to the scores of the history-based
features.
Algorithm 2 Parsing algorithm
1: Initialization: V [s, s, dir, I/C] = 0.0 ?s, dir
2: for k = 1 to n do
3: for s = 0 to n? k do
4: t = s + k
5: % Create incomplete items
6: Lst=V [s, t,?, I]= maxs?r<tV I(r);
7: Rst=V [s, t,?, I]= maxs?r<tV I(r);
8: Calculate Ldfst and Rdfst ;9: % Update the scores of incomplete chart items
10: V [s, t,?, I]=L+st=Lst + Ldfst
11: V [s, t,?, I]=R+st=Rst + Rdfst12: % Create complete items
13: V [s, t,?, C]= maxs?r<tV C(r);
14: V [s, t,?, C]= maxs<r?tV C(r);
15: end for
16: end for
Algorithm 2 is the parsing algorithm with
the history-based features, where V [s, t, dir, I/C]
refers to the score of chart item [s, t, dir, I/C],
V I(r) is a function to search for the optimal
sibling and grandchild nodes for the incomplete
items (line 6 and 7) (Carreras, 2007) given the
130
splitting point r and return the score of the struc-
ture, and V C(r) is a function to search for the op-
timal grandchild node for the complete items (line
13 and 14). Compared with the parsing algorithms
of Carreras (2007), Algorithm 2 uses history in-
formation by adding line 8, 10, and 11.
In Algorithm 2, it first creates chart items with
distance 1, then goes on to chart items with dis-
tance 2, and so on. In each round, it searches for
the structures with the highest scores for incom-
plete items shown at line 6 and 7 of Algorithm 2.
Then we update the scores with the history-based
features by Equation 1 and Equation 2 at line 10
and 11 of Algorithm 2. However, note that we can
not guarantee to find the candidate with the high-
est score with Algorithm 2 because new features
violate the assumptions of dynamic programming.
4.3 History-based features
In this section, we design features that capture the
history information in the recorded decisions.
For a dependency between two words, say s and
t, there are four subtrees that root at s or t. We de-
sign the features by combining s, twith each child
of s and t in the subtrees. The feature templates
are shown as follows: (In the following, c means
one of the children of s and t, and the nodes in the
templates are expanded to their lexical form and
POS tags to obtain actual features.):
C+Dir this feature template is a 2-tuple con-
sisting of (1) a c node and (2) the direction of the
dependency.
C+Dir+S/C+Dir+T this feature template is a 3-
tuple consisting of (1) a c node, (2) the direction
of the dependency, and (3) a s or t node.
C+Dir+S+T this feature template is a 4-tuple
consisting of (1) a c node, (2) the direction of the
dependency, (3) a s node, and (4) a t node.
s     csi r1 r1+1 cti tr2 cso cto r3
Figure 10: Structure of decision history
We use SHI to represent the subtree of s in
the history-inside, THI to represent the one of t
in the history-inside, SHO to represent the one
of s in the history-outside, and THO to represent
the one of t in the history-outside. Based on the
subtree types, the features are divided into four
sets: FSHI , FTHI , FSHO, and FTHO refer to the
features related to the children that are in subtrees
SHI , THI , SHO, and THO respectively.
Figure 10 shows the structure of decision his-
tory of a left dependency (between s and t) re-
lation. For the right case, the structure is simi-
lar. In the figure, SHI is chart item [s, r1,?, C],
THI is chart item [r1 + 1, t,?, C], SHO is
chart item [r2, s,?, C], and THO is chart item
[t, r3,?, C]. We use csi, cti, cso, and cto to repre-
sent a child of s/t in subtrees SHI , THI , SHO,
and THO respectively. The lexical form features
of FSHI and FSHO are listed as examples in Table
1, where ?L? refers to the left direction. We can
also expand the nodes in the templates to the POS
tags. Compared with the algorithm of Carreras
(2007) that only considers the furthest children of
s and t, Algorithm 2 considers all the children.
Table 1: Lexical form features of FSHI and FSHO
template FSHI FSHO
C+DIR word-csi+L word-cso+L
C+DIR+S word-csi+L+word-s word-cso+L+word-s
C+DIR+T word-csi+L+word-t word-cso+L+word-t
C+DIR word-csi+L word-cso+L
+S+T +word-s+word-t +word-s+word-t
4.4 Policy of using history
In practice, we define several policies to use the
history information for different word pairs as fol-
lows:
? All: Use the history-based features for all the
word pairs without any restriction.
? Sub-sentences: use the history-based fea-
tures only for the relation of two words from
sub-sentences. Here, we use punctuation
marks to split sentences into sub-sentences.
? Distance: use the history-based features for
the relation of two words within a predefined
distance. We set the thresholds to 3, 5, and
10.
131
5 Experimental results
In order to evaluate the effectiveness of the
history-based features, we conducted experiments
on Chinese and English data.
For English, we used the Penn Treebank (Mar-
cus et al, 1993) in our experiments and the tool
?Penn2Malt?3 to convert the data into dependency
structures using a standard set of head rules (Ya-
mada and Matsumoto, 2003a). To match previous
work (McDonald and Pereira, 2006; Koo et al,
2008), we split the data into a training set (sec-
tions 2-21), a development set (Section 22), and a
test set (section 23). Following the work of Koo
et al (2008), we used the MXPOST (Ratnaparkhi,
1996) tagger trained on training data to provide
part-of-speech tags for the development and the
test set, and we used 10-way jackknifing to gener-
ate tags for the training set.
For Chinese, we used the Chinese Treebank
(CTB) version 4.04 in the experiments. We also
used the ?Penn2Malt? tool to convert the data and
created a data split: files 1-270 and files 400-931
for training, files 271-300 for testing, and files
301-325 for development. We used gold stan-
dard segmentation and part-of-speech tags in the
CTB. The data partition and part-of-speech set-
tings were chosen to match previous work (Chen
et al, 2008; Yu et al, 2008).
We measured the parser quality by the unla-
beled attachment score (UAS), i.e., the percentage
of tokens with the correct HEAD 5. And we also
evaluated on complete dependency analysis.
In our experiments, we implemented our sys-
tems on the MSTParser6 and extended with
the parent-child-grandchild structures (McDonald
and Pereira, 2006; Carreras, 2007). For the base-
line systems, we used the first- and second-order
(parent-sibling) features that were used in Mc-
Donald and Pereira (2006) and other second-order
features (parent-child-grandchild) that were used
in Carreras (2007). In the following sections, we
call the second-order baseline systems Baseline
3http://w3.msi.vxu.se/?nivre/research/Penn2Malt.html
4http://www.cis.upenn.edu/?chinese/.
5As in previous work, English evaluation ignores any to-
ken whose gold-standard POS tag is one of {?? `` : , .} and
Chinese evaluation ignores any token whose tag is ?PU?.
6http://mstparser.sourceforge.net
and our new systems OURS.
5.1 Results with different feature settings
In this section, we test our systems with different
settings on the development data.
Table 2: Results with different policies
Chinese English
Baseline 89.04 92.43
D1 88.73 92.27
D3 88.90 92.36
D5 89.10 92.59
D10 89.32 92.57
Dsub 89.57 92.63
Table 2 shows the parsing results when we used
different policies defined in Section 4.4 with all
the types of features, where Dsub refers to apply-
ing the policy: sub-sentence, D1 refers to apply-
ing the policy: all, and D3|5|10 refers to applying
the policy: distance with the predefined distance
3, 5, or 10. The results indicated that the accu-
racies of our systems decreased if we used the
history information for short distance words. The
system with Dsub performed the best.
Table 3: Results with different types of Features
Chinese English
Baseline 89.04 92.43
+FSHI 89.14 92.53
+FTHI 89.33 92.35
+FSHO 89.25 92.47
+FTHO 88.99 92.54
Then we investigated the effect of different
types of the history-based features. Table 3 shows
the results with policy Dsub. From the table, we
found that FTHI provided the largest improve-
ment for Chinese and FTHO performed the best
for English.
In what follows, we used Dsub as the policy for
all the languages, the features FSHI + FTHI +
FSHO for Chinese, and the features FSHI +
FSHO + FTHO for English.
5.2 Main results
The main results are shown in the upper parts of
Tables 4 and 5, where the improvements by OURS
over the Baselines are shown in parentheses. The
results show that OURS provided better perfor-
mance over the Baselines by 1.02 points for Chi-
132
Table 4: Results for Chinese
UAS Complete
Baseline 88.41 48.85
OURS 89.43(+1.02) 50.86
OURS+STACK 89.53 49.42
Zhao2009 87.0 ?
Yu2008 87.26 ?
STACK 88.95 49.42
Chen2009 89.91 48.56
nese and 0.29 points for English. The improve-
ments of (OURS) were significant in McNemar?s
Test with p < 10?4 for Chinese and p < 10?3 for
English.
5.3 Comparative results
Table 4 shows the comparative results for Chinese,
where Zhao2009 refers to the result of (Zhao et
al., 2009), Yu2008 refers to the result of Yu et
al. (2008), Chen2009 refers to the result of Chen
et al (2009) that is the best reported result on
this data, and STACK refers to our implementa-
tion of the combination parser of Nivre and Mc-
Donald (2008) using our baseline system and the
MALTParser7. The results indicated that OURS
performed better than Zhao2009, Yu2008, and
STACK, but worse than Chen2009 that used large-
scale unlabeled data (Chen et al, 2009). We also
implemented the combination system of OURS
and the MALTParser, referred as OURS+STACK
in Table 4. The new system achieved further im-
provement. In future work, we can combine our
approach with the parser of Chen et al (2009).
Table 5 shows the comparative results for En-
glish, where Y&M2003 refers to the parser of Ya-
mada and Matsumoto (2003b), CO2006 refers to
the parser of Corston-Oliver et al (2006), Z&C
2008 refers to the combination system of Zhang
and Clark (2008), STACK refers to our implemen-
tation of the combination parser of Nivre and Mc-
Donald (2008), KOO2008 refers to the parser of
Koo et al (2008), Chen2009 refers to the parser
of Chen et al (2009), and Suzuki2009 refers to
the parser of Suzuki et al (2009) that is the best
reported result for this data. The results shows
that OURS outperformed the first two systems that
were based on single models. Z&C 2008 and
STACK were the combination systems of graph-
7http://www.maltparser.org/
Table 5: Results for English
UAS Complete
Baseline 91.92 44.28
OURS 92.21 (+0.29) 45.24
Y&M2003 90.3 38.4
CO2006 90.8 37.6
Z&C2008 92.1 45.4
STACK 92.53 47.06
KOO2008 93.16 ?
Chen2009 93.16 47.15
Suzuki2009 93.79 ?
based and transition-based models. OURS per-
formed better than Z&C 2008, but worse than
STACK. The last three systems that used large-
scale unlabeled data performed better than OURS.
6 Related work
There are several studies that tried to overcome
the limited feature scope of graph-based depen-
dency parsing models .
Nakagawa (2007) proposed a method to deal
with the intractable inference problem in a graph-
based model by introducing the Gibbs sampling
algorithm. Compared with their approach, our ap-
proach is much simpler yet effective. Hall (2007)
used a re-ranking scheme to provide global fea-
tures while we simply augment the features of an
existing parser.
Nivre and McDonald (2008) and Zhang and
Clark (2008) proposed stacking methods to com-
bine graph-based parsers with transition-based
parsers. One parser uses dependency predictions
made by another parser. Our results show that our
approach can be used in the stacking frameworks
to achieve higher accuracy.
7 Conclusions
This paper proposes an approach for improving
graph-based dependency parsing by using the de-
cision history. For the graph-based model, we
design a set of features over short dependen-
cies computed in the earlier stages to improve
the accuracy of long dependencies in the later
stages. The results demonstrate that our proposed
approach outperforms baseline systems by 1.02
points for Chinese and 0.29 points for English.
133
References
Buchholz, S., E. Marsi, A. Dubey, and Y. Kry-
molowski. 2006. CoNLL-X shared task on
multilingual dependency parsing. Proceedings of
CoNLL-X.
Carreras, X. 2007. Experiments with a higher-order
projective dependency parser. In Proceedings of
the CoNLL Shared Task Session of EMNLP-CoNLL
2007, pages 957?961.
Chen, WL., D. Kawahara, K. Uchimoto, YJ. Zhang,
and H. Isahara. 2008. Dependency parsing with
short dependency relations in unlabeled data. In
Proceedings of IJCNLP 2008.
Chen, WL., J. Kazama, K. Uchimoto, and K. Torisawa.
2009. Improving dependency parsing with subtrees
from auto-parsed data. In Proceedings of EMNLP
2009, pages 570?579, Singapore, August.
Corston-Oliver, S., A. Aue, Kevin. Duh, and Eric Ring-
ger. 2006. Multilingual dependency parsing using
bayes point machines. In HLT-NAACL2006.
Culotta, A. and J. Sorensen. 2004. Dependency tree
kernels for relation extraction. In Proceedings of
ACL 2004, pages 423?429.
Eisner, J. 1996. Three new probabilistic models for
dependency parsing: An exploration. In Proc. of
COLING 1996, pages 340?345.
Hall, Keith. 2007. K-best spanning tree parsing. In
Proc. of ACL 2007, pages 392?399, Prague, Czech
Republic, June. Association for Computational Lin-
guistics.
Koo, T., X. Carreras, and M. Collins. 2008. Simple
semi-supervised dependency parsing. In Proceed-
ings of ACL-08: HLT, Columbus, Ohio, June.
Li, Charles N. and Sandra A. Thompson. 1997. Man-
darin Chinese - A Functional Reference Grammar.
University of California Press.
Marcus, M., B. Santorini, and M. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: the Penn Treebank. Computational Linguis-
ticss, 19(2):313?330.
McDonald, R. and J. Nivre. 2007. Characterizing the
errors of data-driven dependency parsing models.
In Proceedings of EMNLP-CoNLL, pages 122?131.
McDonald, R. and F. Pereira. 2006. Online learning
of approximate dependency parsing algorithms. In
Proc. of EACL2006.
McDonald, Ryan. 2006. Discriminative Training and
Spanning Tree Algorithms for Dependency Parsing.
Ph.D. thesis, University of Pennsylvania.
Nakagawa, Tetsuji. 2007. Multilingual dependency
parsing using global features. In Proceedings of
the CoNLL Shared Task Session of EMNLP-CoNLL
2007, pages 952?956.
Nakazawa, T., K. Yu, D. Kawahara, and S. Kurohashi.
2006. Example-based machine translation based on
deeper NLP. In Proceedings of IWSLT 2006, pages
64?70, Kyoto, Japan.
Nivre, J. and R. McDonald. 2008. Integrating graph-
based and transition-based dependency parsers. In
Proceedings of ACL-08: HLT, Columbus, Ohio,
June.
Nivre, J., J. Hall, and J. Nilsson. 2004. Memory-
based dependency parsing. In Proc. of CoNLL
2004, pages 49?56.
Nivre, J., J. Hall, S. Ku?bler, R. McDonald, J. Nilsson,
S. Riedel, and D. Yuret. 2007. The CoNLL 2007
shared task on dependency parsing. In Proceed-
ings of the CoNLL Shared Task Session of EMNLP-
CoNLL 2007, pages 915?932.
Ratnaparkhi, A. 1996. A maximum entropy model for
part-of-speech tagging. In Proceedings of EMNLP,
pages 133?142.
Suzuki, Jun, Hideki Isozaki, Xavier Carreras, and
Michael Collins. 2009. An empirical study of semi-
supervised structured conditional models for depen-
dency parsing. In Proc. of EMNLP 2009, pages
551?560, Singapore, August. Association for Com-
putational Linguistics.
Yamada, H. and Y. Matsumoto. 2003a. Statistical de-
pendency analysis with support vector machines. In
Proceedings of IWPT2003, pages 195?206.
Yamada, H. and Y. Matsumoto. 2003b. Statistical de-
pendency analysis with support vector machines. In
Proceedings of IWPT2003, pages 195?206.
Yu, K., D. Kawahara, and S. Kurohashi. 2008. Chi-
nese dependency parsing with large scale automati-
cally constructed case structures. In Proceedings of
Coling 2008, pages 1049?1056, Manchester, UK,
August.
Zhang, Y. and S. Clark. 2008. A tale of two
parsers: Investigating and combining graph-based
and transition-based dependency parsing. In Pro-
ceedings of EMNLP 2008, pages 562?571, Hon-
olulu, Hawaii, October.
Zhao, Hai, Yan Song, Chunyu Kit, and Guodong
Zhou. 2009. Cross language dependency parsing
using a bilingual lexicon. In Proceedings of ACL-
IJCNLP2009, pages 55?63, Suntec, Singapore, Au-
gust. Association for Computational Linguistics.
134
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 73?83,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
SMT Helps Bitext Dependency Parsing
Wenliang Chen??, Jun?ichi Kazama?, Min Zhang?, Yoshimasa Tsuruoka??,
Yujie Zhang??, Yiou Wang?, Kentaro Torisawa? and Haizhou Li?
?Human Language Technology, Institute for Infocomm Research, Singapore
?National Institute of Information and Communications Technology (NICT), Japan
?School of Information Science, JAIST, Japan
?Beijing Jiaotong University, China
{wechen, mzhang, hli}@i2r.a-star.edu.sg
{kazama, torisawa, yujie, wangyiou}@nict.go.jp
tsuruoka@jaist.ac.jp
Abstract
We propose a method to improve the accuracy
of parsing bilingual texts (bitexts) with the
help of statistical machine translation (SMT)
systems. Previous bitext parsing methods use
human-annotated bilingual treebanks that are
hard to obtain. Instead, our approach uses an
auto-generated bilingual treebank to produce
bilingual constraints. However, because the
auto-generated bilingual treebank contains er-
rors, the bilingual constraints are noisy. To
overcome this problem, we use large-scale
unannotated data to verify the constraints and
design a set of effective bilingual features for
parsing models based on the verified results.
The experimental results show that our new
parsers significantly outperform state-of-the-
art baselines. Moreover, our approach is still
able to provide improvement when we use a
larger monolingual treebank that results in a
much stronger baseline. Especially notable
is that our approach can be used in a purely
monolingual setting with the help of SMT.
1 Introduction
Recently there have been several studies aiming to
improve the performance of parsing bilingual texts
(bitexts) (Smith and Smith, 2004; Burkett and Klein,
2008; Huang et al, 2009; Zhao et al, 2009; Chen
et al, 2010). In bitext parsing, we can use the in-
formation based on ?bilingual constraints? (Burkett
and Klein, 2008), which do not exist in monolingual
sentences. More accurate bitext parsing results can
be effectively used in the training of syntax-based
machine translation systems (Liu and Huang, 2010).
Most previous studies rely on bilingual treebanks
to provide bilingual constraints for bitext parsing.
Burkett and Klein (2008) proposed joint models on
bitexts to improve the performance on either or both
sides. Their method uses bilingual treebanks that
have human-annotated tree structures on both sides.
Huang et al (2009) presented a method to train a
source-language parser by using the reordering in-
formation on words between the sentences on two
sides. It uses another type of bilingual treebanks
that have tree structures on the source sentences and
their human-translated sentences. Chen et al (2010)
also used bilingual treebanks and made use of tree
structures on the target side. However, the bilingual
treebanks are hard to obtain, partly because of the
high cost of human translation. Thus, in their experi-
ments, they applied their methods to a small data set,
the manually translated portion of the Chinese Tree-
bank (CTB) which contains only about 3,000 sen-
tences. On the other hand, many large-scale mono-
lingual treebanks exist, such as the Penn English
Treebank (PTB) (Marcus et al, 1993) (about 40,000
sentences in Version 3) and the latest version of CTB
(over 50,000 sentences in Version 7).
In this paper, we propose a bitext parsing ap-
proach in which we produce the bilingual constraints
on existing monolingual treebanks with the help of
SMT systems. In other words, we aim to improve
source-language parsing with the help of automatic
translations.
In our approach, we first use an SMT system
to translate the sentences of a source monolingual
treebank into the target language. Then, the target
sentences are parsed by a parser trained on a tar-
get monolingual treebank. We then obtain a bilin-
gual treebank that has human annotated trees on the
source side and auto-generated trees on the target
side. Although the sentences and parse trees on the
73
target side are not perfect, we expect that we can
improve bitext parsing performance by using this
newly auto-generated bilingual treebank. We build
word alignment links automatically using a word
alignment tool. Then we can produce a set of bilin-
gual constraints between the two sides.
Because the translation, parsing, and word align-
ment are done automatically, the constraints are not
reliable. To overcome this problem, we verify the
constraints by using large-scale unannotated mono-
lingual sentences and bilingual sentence pairs. Fi-
nally, we design a set of bilingual features based on
the verified results for parsing models.
Our approach uses existing resources including
monolingual treebanks to train monolingual parsers
on both sides, bilingual unannotated data to train
SMT systems and to extract bilingual subtrees,
and target monolingual unannotated data to extract
monolingual subtrees. In summary, we make the fol-
lowing contributions:
? We propose an approach that uses an auto-
generated bilingual treebank rather than
human-annotated bilingual treebanks used in
previous studies (Burkett and Klein, 2008;
Huang et al, 2009; Chen et al, 2010). The
auto-generated bilingual treebank is built with
the help of SMT systems.
? We verify the unreliable constraints by using
the existing large-scale unannotated data and
design a set of effective bilingual features over
the verified results. Compared to Chen et al
(2010) that also used tree structures on the tar-
get side, our approach defines the features on
the auto-translated sentences and auto-parsed
trees, while theirs generates the features by
some rules on the human-translated sentences.
? Our parser significantly outperforms state-of-
the-art baseline systems on the standard test
data of CTB containing about 3,000 sentences.
Moreover, our approach continues to achieve
improvement when we build our system us-
ing the latest version of CTB (over 50,000 sen-
tences) that results in a much stronger baseline.
? We show the possibility that we can improve
the performance even if the test set has no hu-
man translation. This means that our proposed
approach can be used in a purely monolingual
setting with the help of SMT. To our knowl-
edge, this paper is the first one that demon-
strates this widened applicability, unlike the
previous studies that assumed that the parser is
applied only on the bitexts made by humans.
Throughout this paper, we use Chinese as the
source language and English as the target language.
The rest of this paper is organized as follows. Sec-
tion 2 introduces the motivation of this work. Sec-
tion 3 briefly introduces the parsing model used in
the experiments. Section 4 describes a set of bilin-
gual features based on the bilingual constraints and
Section 5 describes how to use large-scale unanno-
tated data to verify the bilingual constraints and de-
fine another set of bilingual features based on the
verified results. Section 6 explains the experimental
results. Finally, in Section 7 we draw conclusions.
2 Motivation
Here, bitext parsing is the task of parsing source sen-
tences with the help of their corresponding transla-
tions. Figure 1-(a) shows an example of the input
of bitext parsing, where ROOT is an artificial root
token inserted at the beginning and does not depend
on any other token in the sentence, the dashed undi-
rected links are word alignment links, and the di-
rected links between words indicate that they have
a dependency relation. Given such inputs, we build
dependency trees for the source sentences. Figure
1-(b) shows the output of bitext parsing for the ex-
ample in 1-(a).
ROOT!? ?? ?? ? ? ?? ?? ? ?? ??ta gaodu pingjia le yu lipeng zongli de huitan jieguo!! !!!
ROOT H hi hl d d h l f h f i h P Li!! e! g y!commen e !t e!resu ts!!o !!!t e!con erence!!!!w t !! eng
(a)
ROOT!? ?? ?? ? ? ?? ?? ? ?? ??ta gaodu pingjia le yu lipeng zongli de huitan jieguo!! !!(b)Figure 1: Input and output of our approach
In bitext parsing, some ambiguities exist on the
source side, but they may be unambiguous on the
74
target side. These differences are expected to help
improve source-side parsing.
Suppose we have a Chinese sentence shown in
Figure 2-(a). In this sentence, there is a nomi-
nalization case (Li and Thompson, 1997) in which
the particle ??(de)/nominalizer? is placed after the
verb compound ???(peiyu)??(qilai)/cultivate?
to modify ???(jiqiao)/skill?. This nominaliza-
tion is a relative clause, but does not have a clue
about its boundary. That is, it is very hard to deter-
mine which word is the head of ???(jiqiao)/skill?.
The head may be ???(fahui)/demonstrate? or ??
?(peiyu)/cultivate?, as shown in Figure 2-(b) and
-(c), where (b) is correct.
? ?? ?? ??? ?? ?? ?? ?? ?? ? ?? ? ??ta xiwang quanti yundongyuan chongfeng fahui pingshi peiyu qilai de!!liliang he!jiqiaoPN!!!!!!!VV!!!!!!!!!DT!!!!!!!!!!!!!!!NN!!!!!!!!!!!!!!!AD!!!!!!!!!!!!!!VV!!!!!!AD!!!!!!!VV!!!!VV DEC NN!!!!CC!!!NN(a)
? ?? ?? ??? ?? ?? ?? ?? ?? ? ?? ? ??ta xiwang quanti yundongyuan chongfeng fahui pingshi peiyu qilai de!!liliang he!jiqiao(b)
? ?? ?? ??? ?? ?? ?? ?? ?? ? ?? ? ??ta xiwang quanti yundongyuan chongfeng fahui pingshi peiyu qilai de!!liliang he!jiqiao(c)Figure 2: Example of an ambiguity on the Chinese side
In its English translation (Figure 3), word ?that? is
a clue indicating the relative clause which shows the
relation between ?skill? and ?cultivate?, as shown in
Figure 3. The figure shows that the translation can
provide useful bilingual constraints. From the de-
pendency tree on the target side, we find that the
word ?skill? corresponding to ???(jiqiao)/skill?
depends on the word ?demonstrate? corresponding
to ???(fahui)/demonstrate?, while the word ?cul-
tivate? corresponding to ???(peiyu)/cultivate? is a
grandchild of ?skill?. This is a positive evidence for
supporting ???(fahui)/demonstrate? as being the
head of ???(jiqiao)/skill?.
The above case uses the human translation on
the target side. However, there are few human-
annotated bilingual treebanks and the existing bilin-
gual treebanks are usually small. In contrast, there
are large-scale monolingual treebanks, e.g., the PTB
and the latest version of CTB. So we want to use
existing resources to generate a bilingual treebank
with the help of SMT systems. We hope to improve
source side parsing by using this newly built bilin-
gual treebank.
? ?? ?? ??? ?? ?? ?? ?? ?? ? ?? ? ??ta xiwang quanti yundongyuan chongfeng fahui pingshi peiyu qilai de!!liliang he!jiqiao
He!hoped!that!all!the!athletes!would!!fully!demonstrate!the!strength!and!skill!that!they!cultivate!daily
Figure 3: Example of human translation
? ?? ?? ??? ?? ?? ?? ?? ?? ? ?? ? ??ta xiwang quanti yundongyuan chongfeng fahui pingshi peiyu qilai de!!liliang he!jiqiao
he!expressed!the!hope!that!all!athletes!used!to!give!full!play!to!the!country!'s!strength!and!skills!
Figure 4: Example of Moses translation
Figure 4 shows an example of a translation us-
ing a Moses-based system, where the target sen-
tence is parsed by a monolingual target parser. The
translation contains some errors, but it does contain
some correct parts that can be used for disambigua-
tion. In the figure, the word ?skills? corresponding
to ???(jiqiao)/skill? is a grandchild of the word
?play? corresponding to ???(fahui)/demonstrate?.
This is a positive evidence for supporting ??
?(fahui)/demonstrate? as being the head of ??
?(jiqiao)/skill?.
From this example, although the sentences and
parse trees on the target side are not perfect, we
still can explore useful information to improve bitext
parsing. In this paper, we focus on how to design
a method to verify such unreliable bilingual con-
straints.
3 Parsing model
In this paper, we implement our approach based
on graph-based parsing models (McDonald and
Pereira, 2006; Carreras, 2007). Note that our ap-
proach can also be applied to transition-based pars-
ing models (Nivre, 2003; Yamada and Matsumoto,
2003).
The graph-based parsing model is to search for
the maximum spanning tree (MST) in a graph (Mc-
Donald and Pereira, 2006). The formulation defines
the score of a dependency tree to be the sum of edge
scores,
75
s(x, y) =
?
g?y
score(w, x, g) =
?
g?y
w ?f(x, g) (1)
where x is an input sentence, y is a dependency
tree for x, and g is a spanning subgraph of y. f(x, g)
can be based on arbitrary features of the subgraph
and the input sequence x and the feature weight
vector w are the parameters to be learned by using
MIRA (Crammer and Singer, 2003) during training.
In our approach, we use two types of features
for the parsing model. One is monolingual fea-
tures based on the source sentences. The mono-
lingual features include the first- and second- order
features presented in McDonald and Pereira (2006)
and the parent-child-grandchild features used in Car-
reras (2007). The other one is bilingual features (de-
scribed in Sections 4 and 5) that consider the bilin-
gual constraints.
We call the parser with the monolingual features
on the source side Parsers, and the parser with the
monolingual features on the target side Parsert.
4 Original bilingual features
In this paper, we generate two types of bilingual fea-
tures, original and verified bilingual features. The
original bilingual features (described in this section)
are based on the bilingual constraints without being
verified by large-scale unannotated data. And the
verified bilingual features (described in Section 5)
are based on the bilingual constraints verified by us-
ing large-scale unannotated data.
4.1 Auto-generated bilingual treebank
Assuming that we have monolingual treebanks on
the source side, an SMT system that can translate
the source sentences into the target language, and a
Parsert trained on the target monolingual treebank.
We first translate the sentences of the source
monolingual treebank into the target language using
the SMT system. Usually, SMT systems can output
the word alignment links directly. If they can not, we
perform word alignment using some publicly avail-
able tools, such as Giza++ (Och and Ney, 2003) or
Berkeley Aligner (Liang et al, 2006; DeNero and
Klein, 2007). The translated sentences are parsed by
the Parsert. Then, we have a newly auto-generated
bilingual treebank.?
4.2 Bilingual constraint functions
In this paper, we focus on the first- and second-
order graph models (McDonald and Pereira, 2006;
Carreras, 2007). Thus we produce the constraints
for bigram (a single edge) and trigram (adjacent
edges) dependencies in the graph model. For the tri-
gram dependencies, we consider the parent-sibling
and parent-child-grandchild structures described in
McDonald and Pereira (2006) and Carreras (2007).
We leave the third-order models (Koo and Collins,
2010) for a future study.
Suppose that we have a (candidate) dependency
relation rs that can be a bigram or trigram de-
pendency. We examine whether the corresponding
words of the source words of rs have a dependency
relation rt in the target trees. We also consider the
direction of the dependency relation. The corre-
sponding word of the head should also be the head
in rt. We define a binary function for this bilingual
constraint: Fbn(rsn : rtk), where n and k refers to
the types of the dependencies (2 for bigram and 3 for
trigram). For example, in rs2 : rt3, rs2 is a bigram
dependency on the source side and rt3 is a trigram
dependency on the target side.
4.2.1 Bigram constraint function: Fb2
For rs2, we consider two types of bilingual con-
straints. The first constraint function, denoted as
Fb2(rs2 : rt2), checks if the corresponding words
also have a direct dependency relation rt2. Figure
5 shows an example, where the source word ??
?(quanti)? depends on ????(yundongyuan)?
and word ?all? corresponding to ???(quanti)? de-
pends on word ?athletes? corresponding to ???
?(yundongyuan)?. In this case, Fb2(rs2 : rt2) =
+. However, when the source words are ??(ta)?
and ???(xiwang)?, this time their corresponding
words ?He? and ?hope? do not have a direct depen-
dency relation. In this case, Fb2(rs2 : rt2)=?.
The second constraint function, denoted as
Fb2(rs2 : rt3), checks if the corresponding words
form a parent-child-grandchild relation that often
occurs in translation (Koehn et al, 2003). Figure 6
shows an example. The source word ???(jiqiao)?
depends on ???(fahui)? while its corresponding
word ?skills? indirectly depends on ?play? which
corresponds to ???(fahui)? via ?to?. In this case,
Fb2(rs2 : rt3)=+.
76
? ?? ?? ??? ?? ?? ?? ?? ?? ? ?? ? ??ta xiwang quanti yundongyuan chongfeng fahui pingshi peiyu qilai de!!liliang he!jiqiao
he!expressed!the!hope!that!all!athletes!used!to!give!full!play!to!the!country!'s!strength!and!skills!
Figure 5: Example of bilingual constraints (2to2)
? ?? ?? ??? ?? ?? ?? ?? ?? ? ?? ? ??ta xiwang quanti yundongyuan chongfeng fahui pingshi peiyu qilai de!!liliang he!jiqiao
he!expressed!the!hope!that!all!athletes!used!to!give!full!play!to!the!country!'s!strength!and!skills!
Figure 6: Example of bilingual constraints (2to3)
4.2.2 Trigram constraint function: Fb3
For a second-order relation on the source side,
we consider one type of constraint. We have three
source words that form a second-order relation and
all of them have the corresponding words. We
define function Fb3(rs3 : rt3) for this constraint.
The function checks if the corresponding words
form a trigram dependencies structure. An exam-
ple is shown in Figure 7. The source words ??
?(liliang)?, ??(he)?, and ???(jiqiao)? form a
parent-sibling structure, while their corresponding
words ?strength?, ?and?, and ?skills? also form a
parent-sibling structure on the target side. In this
case, function Fb3(rs3 : rt3)=+.
? ?? ?? ??? ?? ?? ?? ?? ?? ? ?? ? ??ta xiwang quanti yundongyuan chongfeng fahui pingshi peiyu qilai de!!liliang he!jiqiao
he!expressed!the!hope!that!all!athletes!used!to!give!full!play!to!the!country!'s!strength!and!skills!
Figure 7: Example of bilingual constraints (3to3)
4.3 Bilingual reordering function: Fro
Huang et al (2009) proposed features based on
reordering between languages for a shift-reduce
parser. They define the features based on word-
alignment information to verify whether the corre-
sponding words form a contiguous span to resolve
shift-reduce conflicts. We also implement similar
features in our system. For example, in Figure 1-
(a) the source span is [??(huitan), ??(jieguo)],
which maps onto [results, conference]. Because no
word within this target span is aligned to a source
word outside of the source span, this span is a con-
tiguous span. In this case, function Fro =+, other-
wise Fro=?.
4.4 Original bilingual features
We define original bilingual features based on the
bilingual constraint functions and the bilingual re-
ordering function.
Table 1 lists the original features, where Dir
refers to the directions1 of the source-side dependen-
cies, Fb2 can be Fb2(rs2 : rt2) and Fb2(rs2 : rt3),
and Fb3 is Fb3(rs3 : rt3). Each line of the table
defines a feature template that is a combination of
functions.
First-order features Second-order features
?Fro?
?Fb2, Dir? ?Fb3, Dir?
?Fb2, Dir, Fro? ?Fb3, Dir, Fro?
Table 1: Original bilingual features
We use an example to show how to generate the
original bilingual features in practice. In Figure 4,
we want to define the bilingual features for the bi-
gram dependency (rs2) between ???(fahui)? and
???(jiqiao)?. The corresponding words form a tri-
gram relation rt3 in the target dependency tree. The
direction of the bigram dependency is right. Then
we have feature ??Fb2(rs2 : rt3)=+, RIGHT ?? for
the second first-order feature template in Table 1.
5 Verified bilingual features
However, because the bilingual treebank is gener-
ated automatically, using the bilingual constraints
alone is not reliable. Therefore, in this section we
verify the constraints by using large-scale unanno-
tated data to overcome this problem. More specifi-
cally, rtk of the constraint is verified by checking a
list of target monolingual subtrees and rsn : rtk is
verified by checking a list of bilingual subtrees. The
subtrees are extracted from the large-scale unanno-
tated data. The basic idea is as follows: if the de-
pendency structures of a bilingual constraint can be
found in the list of the target monolingual subtrees
1For the second order features, Dir is the combination of
the directions of two dependencies.
77
or bilingual subtrees, this constraint will probably be
reliable.
We first parse the large-scale unannotated mono-
lingual and bilingual data. Subsequently, we ex-
tract the monolingual and bilingual subtrees from
the parsed data. We then verify the bilingual con-
straints using the extracted subtrees. Finally, we
generate the bilingual features based on the verified
results for the parsing models.
5.1 Verified constraint functions
5.1.1 Monolingual target subtrees
Chen et al (2009) proposed a simple method to
extract subtrees from large-scale monolingual data
and used them as features to improve monolingual
parsing. Following their method, we parse large
unannotated data with the Parsert and obtain the sub-
tree list (STt) on the target side. We extract two
types of subtrees: bigram (two words) subtree and
trigram (three words) subtree.
H b ht b h b k
ROOT!!He!!!!!bought!!!!!a!!!!book
e!!!!! oug oug t!! oo !
a book b ht b k!!!!!
(a) (b)
oug !!!a!!!!! oo !
Figure 8: Example of monolingual subtree extraction
From the dependency tree in Figure 8-(a), we ob-
tain the subtrees shown in Figure 8-(b) where the
first three are bigram subtrees and the last one is
a trigram subtree. After extraction, we obtain the
subtree list STt that includes two sets, one for bi-
gram subtrees, and the other one for trigram sub-
trees. We remove the subtrees occurring only once
in the data. For each set, we assign labels to the
extracted subtrees according to their frequencies by
using the same method as that of Chen et al (2009).
If the frequency of a subtree is in the top 10% in the
corresponding set, it is labeled HF. If the frequency
is between the top 20% and 30%, it is labeled MF.
We assign the label LF to the remaining subtrees.
We use Type(stt) to refer to the label of a subtree,
stt.
5.1.2 Verified target constraint function:
Fvt(rtk)
We use the extracted target subtrees to verify the
rtk of the bilingual constraints. In fact, rtk is a can-
didate subtree. If the rtk is included in STt, func-
tion Fvt(rtk) = Type(rtk), otherwise Fvt(rtk) =
ZERO. For example, in Figure 5 the bigram struc-
ture of ?all? and ?athletes? can form a bigram sub-
tree that is included STt and its label is HF. In this
case, Fvt(rt2)= HF .
5.1.3 Bilingual subtrees
We extract bilingual subtrees from a bilingual
corpus, which is parsed by the Parsers and Parsert
on both sides. We extract three types of bilingual
subtrees: bigram-bigram (stbi22), bigram-trigram
(stbi23), and trigram-trigram (stbi33) subtrees. For
example, stbi22 consists of a bigram subtree on the
source side and a bigram subtree on the target side.
? ? ? ?? ? ? ? ??ROOT! ?ta shi yi ming xuesheng
ROOT!!He!!!!!is!!!!!a!!!!!student He!!!!!is is!!!!!student
(a) (b)
Figure 9: Example of bilingual subtree extraction
From the dependency tree in Figure 9-(a), we
obtain the bilingual subtrees shown in Figure 9-
(b). Figure 9-(b) shows the extracted bigram-bigram
bilingual subtrees. After extraction, we obtain the
bilingual subtrees STbi. We remove the subtrees oc-
curring only once in the data.
5.1.4 Verified bilingual constraint function:
Fvb(rbink)
We use the extracted bilingual subtrees to verify
the rsn : rtk (rbink in short) of the bilingual con-
straints. rsn and rtk form a candidate bilingual sub-
tree stbink. If the stbink is included in STbi, function
Fvb(rbink)=+, otherwise Fvb(rbink)=?.
5.2 Verified bilingual features
Then, we define another set of bilingual features by
combining the verified constraint functions. We call
these bilingual features ?verified bilingual features?.
78
Table 2 lists the verified bilingual features used in
our experiments, where each line defines a feature
template that is a combination of functions.
We use an example to show how to generate the
verified bilingual features in practice. In Figure 4,
we want to define the verified features for the bi-
gram dependency (rs2) between ???(fahui)? and
???(jiqiao)?. The corresponding words form a
trigram relation rt3. The direction of the bigram
dependency is right. Suppose we can find rt3 in
STt with label MF and can not find the candidate
bilingual subtree in STbi. Then we have feature
??Fb2(rs2 : rt3) = +, Fvt(rt3) = MF,RIGHT ??
for the third first-order feature template and feature
??Fb2(rs2 : rt3)=+, Fvb(rbi23)=?, RIGHT ?? for
the fifth in Table 2.
First-order features Second-order features
?Fro?
?Fb2, Fvt(rtk)? ?Fb3, Fvt(rtk)?
?Fb2, Fvt(rtk), Dir? ?Fb3, Fvt(rtk), Dir?
?Fb2, Fvb(rbink)? ?Fb3, Fvb(rbink)?
?Fb2, Fvb(rbink), Dir? ?Fb3, Fvb(rbink), Dir?
?Fb2, Fro, Fvb(rbink)?
Table 2: Verified bilingual features
6 Experiments
We evaluated the proposed method on the translated
portion of the Chinese Treebank V2 (referred to as
CTB2tp) (Bies et al, 2007), articles 1-325 of CTB,
which have English translations with gold-standard
parse trees. The tool ?Penn2Malt?2 was used to con-
vert the data into dependency structures. Following
the studies of Burkett and Klein (2008), Huang et
al. (2009) and Chen et al (2010), we used the ex-
act same data split: 1-270 for training, 301-325 for
development, and 271-300 for testing. Note that we
did not use human translation on the English side
of this bilingual treebank to train our new parsers.
For testing, we used two settings: a test with hu-
man translation and another with auto-translation.
To process unannotated data, we trained a first-order
Parsers on the training data.
To prove that the proposed method can work on
larger monolingual treebanks, we also tested our
2http://w3.msi.vxu.se/?nivre/research/Penn2Malt.html
methods on the CTB7 (LDC2010T07) that includes
much more sentences than CTB2tp. We used arti-
cles 301-325 for development, 271-300 for testing,
and the other articles for training. That is, we eval-
uated the systems on the same test data as CTB2tp.
Table 3 shows the statistical information on the data
sets.
Train Dev Test
CTB2tp 2,745 273 290
CTB7 50,747 273 290
Table 3: Number of sentences of data sets used
We built Chinese-to-English SMT systems based
on Moses3. Minimum error rate training (MERT)
with respect to BLEU score was used to tune the de-
coder?s parameters. The translation model was cre-
ated from the FBIS corpus (LDC2003E14). We used
SRILM4 to train a 5-gram language model. The lan-
guage model was trained on the target side of the
FBIS corpus and the Xinhua news in English Gi-
gaword corpus (LDC2009T13). The development
and test sets were from NIST MT08 evaluation cam-
paign5. We then used the SMT systems to translate
the training data of CTB2tp and CTB7.
To directly compare with the results of Huang
et al (2009) and Chen et al (2010), we also used
the same word alignment tool, Berkeley Aligner
(Liang et al, 2006; DeNero and Klein, 2007), to
perform word alignment for CTB2tp and CTB7.
We trained a Berkeley Aligner on the FBIS corpus
(LDC2003E14). We removed notoriously bad links
in {a, an, the}?{?(de),?(le)} following the work
of Huang et al (2009).
To train an English parser, we used the PTB
(Marcus et al, 1993) in our experiments and the
tool ?Penn2Malt? to convert the data. We split the
data into a training set (sections 2-21), a develop-
ment set (section 22), and a test set (section 23).
We trained first-order and second-order Parsert on
the training data. The unlabeled attachment score
(UAS) of the second-order Parsert was 91.92, in-
dicating state-of-the-art accuracy on the test data.
We used the second-order Parsert to parse the auto-
translated/human-made target sentences in the CTB
3http://www.statmt.org/moses/
4http://www.speech.sri.com/projects/srilm/download.html
5http://www.itl.nist.gov/iad/mig//tests/mt/2008/
79
data.
To extract English subtrees, we used the BLLIP
corpus (Charniak et al, 2000) that contains about
43 million words of WSJ texts. We used the MX-
POST tagger (Ratnaparkhi, 1996) trained on train-
ing data to assign POS tags and used the first-order
Parsert to process the sentences of the BLLIP cor-
pus. To extract bilingual subtrees, we used the FBIS
corpus and an additional bilingual corpus contain-
ing 800,000 sentence pairs from the training data of
NIST MT08 evaluation campaign. On the Chinese
side, we used the morphological analyzer described
in (Kruengkrai et al, 2009) trained on the training
data of CTBtp to perform word segmentation and
POS tagging and used the first-order Parsers to parse
all the sentences in the data. On the English side, we
used the same procedure as we did for the BLLIP
corpus. Word alignment was performed using the
Berkeley Aligner.
We reported the parser quality by the UAS, i.e.,
the percentage of tokens (excluding all punctuation
tokens) with correct HEADs.
6.1 Experimental settings
For baseline systems, we used the monolingual fea-
tures mentioned in Section 3. We called these fea-
tures basic features. To compare the results of (Bur-
kett and Klein, 2008; Huang et al, 2009; Chen et
al., 2010), we used the test data with human trans-
lation in the following three experiments. The tar-
get sentences were parsed by using the second-order
Parsert. We used PAG to refer to our parsers trained
on the auto-generated bilingual treebank.
6.2 Training with CTB2tp
Order-1 Order-2
Baseline 84.35 87.20
PAGo 84.71(+0.36) 87.85(+0.65)
PAG 85.37(+1.02) 88.49(+1.29)
ORACLE 85.79(+1.44) 88.87(+1.67)
Table 4: Results of training with CTB2tp
First, we conducted the experiments on the stan-
dard data set of CTB2tp, which was also used in
other studies (Burkett and Klein, 2008; Huang et al,
2009; Chen et al, 2010). The results are given in
Table 4, where Baseline refers to the system with
the basic features, PAGo refers to that after adding
the original bilingual features of Table 1 to Baseline,
PAG refers to that after adding the verified bilingual
features of Table 2 to Baseline, and ORACLE6 refers
to using human-translation for training data with
adding the features of Table 1. We obtained an ab-
solute improvement of 1.02 points for the first-order
model and 1.29 points for the second-order model by
adding the verified bilingual features. The improve-
ments of the final systems (PAG) over the Baselines
were significant in McNemar?s Test (p < 0.001 for
the first-order model and p < 0.0001 for the second-
order model). If we used the original bilingual fea-
tures (PAGo), the system dropped 0.66 points for the
first-order and 0.64 points for the second-order com-
pared with system PAG. This indicated that the ver-
ified bilingual constraints did provide useful infor-
mation for the parsing models.
We also found that PAG was about 0.3 points
lower than ORACLE. The reason is mainly due
to the imperfect translations, although we used
the large-scale subtree lists to help verify the con-
straints. We tried adding the features of Table 2 to
the ORACLE system, but the results were worse.
These facts indicated that our approach obtained the
benefits from the verified constraints, while using
the bilingual constraints alone was enough for OR-
ACLE.
6.3 Training with CTB7
 0.83
 0.84
 0.85
 0.86
 0.87
 0.88
 0.89
 0.9
 0.91
 0.92
 5  10  20  30  40  50
U
A
S
Amount of training data (K)
Baseline1PAG1Baseline2PAG2
Figure 10: Results of using different sizes of training data
Here, we demonstrate that our approach is still
able to provide improvement, even if we use larger
6Note that we also used the tool to perform the word align-
ment automatically.
80
Baseline D10 D20 D50 D100 GTran
BLEU n/a 14.71 15.84 16.92 17.95 n/a
UAS 87.20 87.63 87.67 88.20 88.49 88.58
Table 5: Results of using different translations
training data that result in strong baseline systems.
We incrementally increased the training sentences
from the CTB7. Figure 10 shows the results of us-
ing different sizes of CTB7 training data, where the
numbers of the x-axis refer to the sentence numbers
of training data used, Baseline1 and Baseline2 re-
fer to the first- and second-order baseline systems,
and PAG1 and PAG2 refer to our first- and second-
order systems. The figure indicated that our sys-
tem always outperformed the baseline systems. For
small data sizes, our system performed much better
than the baselines. For example, when using 5,000
sentences, our second-order system provided a 1.26
points improvement over the second-order baseline.
Finally, when we used all of the CTB7 training
data, our system achieved 91.66 for the second-order
model, while the baseline achieved 91.10.
6.4 With different settings of SMT systems
We investigated the effects of different settings of
SMT systems. We randomly selected 10%, 20%,
and 50% of FBIS to train the Moses systems and
used them to translate CTB2tp. The results are in
Table 5, where D10, D20, D50, and D100 refer to
the system with 10%, 20%, 50%, and 100% data re-
spectively. For reference, we also used the Google-
translate online system7, indicated as GTran in the
table, to translate the CTB2tp.
From the table, we found that our system outper-
formed the Baseline even if we used only 10% of the
FBIS corpus. The BLEU and UAS scores became
higher, when we used more data of the FBIS corpus.
And the gaps among the results of D50, D100, and
GTran were small. This indicated that our approach
was very robust to the noise produced by the SMT
systems.
6.5 Testing with auto-translation
We also translated the test data into English using
the Moses system and tested the parsers on the new
7http://translate.google.com/
test data. Table 6 shows the results. The results
showed that PAG outperformed the baseline systems
for both the first- and second-order models. This
indicated that our approach can provide improve-
ment in a purely monolingual setting with the help
of SMT.
Order-1 Order-2
Baseline 84.35 87.20
PAG 84.88(+0.53) 87.89(+0.69)
Table 6: Results of testing with auto-translation (training
with CTB2tp)
6.6 Comparison results
With CTB2tp With CTB7
Type System UAS System UAS
M Baseline 87.20 Baseline 91.10
HA
Huang2009 86.3 n/a
Chen2010BI 88.56
Chen2010ALL 90.13
AG PAG 88.49 PAG 91.66PAG+STs 89.75
Table 7: Comparison of our results with other pre-
vious reported systems. Type M denotes training on
monolingual treebank. Types HA and AG denote training
on human-annotated and auto-generated bilingual tree-
banks respectively.
We compared our results with the results reported
previously for the same data. Table 7 lists the re-
sults, where Huang2009 refers to the result of Huang
et al (2009), Chen2010BI refers to the result of
using bilingual features in Chen et al (2010), and
Chen2010ALL refers to the result of using all of
the features in Chen et al (2010). The results
showed that our new parser achieved better accuracy
than Huang2009 and comparable to Chen2010BI .
To achieve higher performance, we also added the
source subtree features (Chen et al, 2009) to our
system: PAG+STs. The new result is close to
Chen2010ALL. Compared with the approaches of
81
Huang et al (2009) and Chen et al (2010), our
approach used an auto-generated bilingual treebank
while theirs used a human-annotated bilingual tree-
bank. By using all of the training data of CTB7, we
obtained a more powerful baseline that performed
much better than the previous reported results. Our
parser achieved 91.66, much higher accuracy than
the others.
7 Conclusion
We have presented a simple yet effective approach
to improve bitext parsing with the help of SMT sys-
tems. Although we trained our parser on an auto-
generated bilingual treebank, we achieved an accu-
racy comparable to the systems trained on human-
annotated bilingual treebanks on the standard test
data. Moreover, our approach continued to pro-
vide improvement over the baseline systems when
we used a much larger monolingual treebank (over
50,000 sentences) where target human translations
are not available and very hard to construct. We also
demonstrated that the proposed approach can be ef-
fective in a purely monolingual setting with the help
of SMT.
Acknowledgments
This study was started when Wenliang Chen, Yu-
jie Zhang, and Yoshimasa Tsuruoka were members
of Language Infrastructure Group, National Insti-
tute of Information and Communications Technol-
ogy (NICT), Japan. We would also thank the anony-
mous reviewers for their detailed comments, which
have helped us to improve the quality of this work.
References
Ann Bies, Martha Palmer, Justin Mott, and Colin Warner.
2007. English Chinese Translation Treebank V 1.0,
LDC2007T02. Linguistic Data Consortium.
David Burkett and Dan Klein. 2008. Two languages are
better than one (for syntactic parsing). In Proceedings
of EMNLP 2008, pages 877?886, Honolulu, Hawaii,
October. Association for Computational Linguistics.
Xavier Carreras. 2007. Experiments with a higher-order
projective dependency parser. In Proceedings of the
CoNLL Shared Task Session of EMNLP-CoNLL 2007,
pages 957?961, Prague, Czech Republic, June. Asso-
ciation for Computational Linguistics.
Eugene Charniak, Don Blaheta, Niyu Ge, Keith Hall,
John Hale, and Mark Johnson. 2000. BLLIP 1987-
89 WSJ Corpus Release 1, LDC2000T43. Linguistic
Data Consortium.
Wenliang Chen, Jun?ichi Kazama, Kiyotaka Uchimoto,
and Kentaro Torisawa. 2009. Improving dependency
parsing with subtrees from auto-parsed data. In Pro-
ceedings of EMNLP 2009, pages 570?579, Singapore,
August.
Wenliang Chen, Jun?ichi Kazama, and Kentaro Torisawa.
2010. Bitext dependency parsing with bilingual sub-
tree constraints. In Proceedings of ACL 2010, pages
21?29, Uppsala, Sweden, July. Association for Com-
putational Linguistics.
Koby Crammer and Yoram Singer. 2003. Ultraconser-
vative online algorithms for multiclass problems. J.
Mach. Learn. Res., 3:951?991.
John DeNero and Dan Klein. 2007. Tailoring word
alignments to syntactic machine translation. In Pro-
ceedings of ACL 2007, pages 17?24, Prague, Czech
Republic, June. Association for Computational Lin-
guistics.
Liang Huang, Wenbin Jiang, and Qun Liu. 2009.
Bilingually-constrained (monolingual) shift-reduce
parsing. In Proceedings of EMNLP 2009, pages 1222?
1231, Singapore, August. Association for Computa-
tional Linguistics.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceedings of
NAACL 2003, pages 48?54. Association for Computa-
tional Linguistics.
Terry Koo and Michael Collins. 2010. Efficient third-
order dependency parsers. In Proceedings of ACL
2010, pages 1?11, Uppsala, Sweden, July. Association
for Computational Linguistics.
Canasai Kruengkrai, Kiyotaka Uchimoto, Jun?ichi
Kazama, Yiou Wang, Kentaro Torisawa, and Hitoshi
Isahara. 2009. An error-driven word-character hybrid
model for joint Chinese word segmentation and POS
tagging. In Proceedings of ACL-IJCNLP2009, pages
513?521, Suntec, Singapore, August. Association for
Computational Linguistics.
Charles N. Li and Sandra A. Thompson. 1997. Man-
darin Chinese - A Functional Reference Grammar.
University of California Press.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of NAACL 2006,
pages 104?111, New York City, USA, June. Associa-
tion for Computational Linguistics.
Yang Liu and Liang Huang. 2010. Tree-based and forest-
based translation. In Tutorial Abstracts of ACL 2010,
page 2, Uppsala, Sweden, July. Association for Com-
putational Linguistics.
82
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: the Penn Treebank. Computational
Linguisticss, 19(2):313?330.
Ryan McDonald and Fernando Pereira. 2006. On-
line learning of approximate dependency parsing algo-
rithms. In Proceedings of EACL 2006, pages 81?88.
Joakim Nivre. 2003. An efficient algorithm for
projective dependency parsing. In Proceedings of
IWPT2003, pages 149?160.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Adwait Ratnaparkhi. 1996. A maximum entropy model
for part-of-speech tagging. In Proceedings of EMNLP
1996, pages 133?142.
David A. Smith and Noah A. Smith. 2004. Bilingual
parsing with factored estimation: Using English to
parse Korean. In Proceedings of EMNLP 2004, pages
49?56.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statistical
dependency analysis with support vector machines. In
Proceedings of IWPT 2003, pages 195?206.
Hai Zhao, Yan Song, Chunyu Kit, and Guodong Zhou.
2009. Cross language dependency parsing us-
ing a bilingual lexicon. In Proceedings of ACL-
IJCNLP2009, pages 55?63, Suntec, Singapore, Au-
gust. Association for Computational Linguistics.
83
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1372?1376,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Simple Customization of Recursive Neural Networks
for Semantic Relation Classification
Kazuma Hashimoto?, Makoto Miwa??, Yoshimasa Tsuruoka?, and Takashi Chikayama?
?The University of Tokyo, 3-7-1 Hongo, Bunkyo-ku, Tokyo, Japan
{hassy, tsuruoka, chikayama}@logos.t.u-tokyo.ac.jp
??The University of Manchester, 131 Princess Street, Manchester, M1 7DN, UK
makoto.miwa@manchester.ac.uk
Abstract
In this paper, we present a recursive neural
network (RNN) model that works on a syn-
tactic tree. Our model differs from previous
RNN models in that the model allows for an
explicit weighting of important phrases for the
target task. We also propose to average param-
eters in training. Our experimental results on
semantic relation classification show that both
phrase categories and task-specific weighting
significantly improve the prediction accuracy
of the model. We also show that averaging the
model parameters is effective in stabilizing the
learning and improves generalization capacity.
The proposed model marks scores competitive
with state-of-the-art RNN-based models.
1 Introduction
Recursive Neural Network (RNN) models are
promising deep learning models which have been
applied to a variety of natural language processing
(NLP) tasks, such as sentiment classification, com-
pound similarity, relation classification and syntactic
parsing (Hermann and Blunsom, 2013; Socher et al,
2012; Socher et al, 2013). RNN models can repre-
sent phrases of arbitrary length in a vector space of
a fixed dimension. Most of them use minimal syn-
tactic information (Socher et al, 2012).
Recently, Hermann and Blunsom (2013) pro-
posed a method for leveraging syntactic informa-
tion, namely CCG combinatory operators, to guide
composition of phrases in RNN models. While their
models were successfully applied to binary senti-
ment classification and compound similarity tasks,
there are questions yet to be answered, e.g., whether
such enhancement is beneficial in other NLP tasks
as well, and whether a similar improvement can
be achieved by using syntactic information of more
commonly available types such as phrase categories
and syntactic heads.
In this paper, we present a supervised RNN model
for a semantic relation classification task. Our model
is different from existing RNN models in that impor-
tant phrases can be explicitly weighted for the task.
Syntactic information used in our model includes
part-of-speech (POS) tags, phrase categories and
syntactic heads. POS tags are used to assign vec-
tor representations to word-POS pairs. Phrase cate-
gories are used to determine which weight matrices
are chosen to combine phrases. Syntactic heads are
used to determine which phrase is weighted during
combining phrases. To incorporate task-specific in-
formation, phrases on the path between entity pairs
are further weighted.
The second contribution of our work is the intro-
duction of parameter averaging into RNN models.
In our preliminary experiments, we observed that
the prediction performance of the model often fluc-
tuates significantly between training iterations. This
fluctuation not only leads to unstable performance
of the resulting models, but also makes it difficult to
fine-tune the hyperparameters of the model. Inspired
by Swersky et al (2010), we propose to average the
model parameters in the course of training. A re-
cent technique for deep learning models of similar
vein is dropout (Hinton et al, 2012), but averaging
is simpler to implement.
Our experimental results show that our model per-
1372
Figure 1: A recursive representations of a phrase ?a
word vector? with POS tags of the words (DT, NN and
NN respectively). For example, the two word-POS pairs
?word NN? and ?vector NN? with a syntactic category
N are combined to represent the phrase ?word vector?.
forms better than standard RNN models. By av-
eraging the model parameters, our model achieves
performance competitive with the MV-RNN model
in Socher et al (2012), without using computation-
ally expensive word-dependent matrices.
2 An Averaged RNN Model with Syntax
Our model is a supervised RNN that works on a bi-
nary syntactic tree. As our first step to leverage in-
formation available in the tree, we distinguish words
with the same spelling but POS tags in the vector
space. Our model also uses different weight ma-
trices dependent on the phrase categories of child
nodes (phrases or words) in combining phrases. Our
model further weights those nodes that appear to be
important.
Compositional functions of our model follow
those of the SU-RNN model in Socher et al (2013).
2.1 Word-POS Vector Representations
Our model simply assigns vector representations to
word-POS pairs. For example, a word ?caused?
can be represented in two ways: ?caused VBD? and
?caused VBN?. The vectors are represented as col-
umn vectors in a matrix We ? Rd?|V|, where d is
the dimension of a vector and V is a set of all word-
POS pairs we consider.
2.2 Compositional Functions with Syntax
In construction of parse trees, we associate each of
the tree node with its d-dimensional vector represen-
tation computed from vector representations of its
subtrees. For leaf nodes, we look up word-POS vec-
tor representations in V. Figure 1 shows an example
of such recursive representations. A parent vector
p ? Rd?1 is computed from its direct child vectors
cl and cr? Rd?1:
p = tanh(?lW
Tcl ,Tcr
l cl+?rW
Tcl ,Tcrr cr+bTcl ,Tcr ),
where W Tcl ,Tcrl and W
Tcl ,Tcrr ? Rd?d are weight
matrices that depend on the phrase categories of cl
and cr. Here, cl and cr have phrase categories Tcl
and Tcr respectively (such as N, V, etc.). bTcl ,Tcr ?
Rd?1 is a bias vector. To incorporate the impor-
tance of phrases into the model, two subtrees of a
node may have different weights ?l ? [0, 1] and
?r(= 1 ? ?l), taking phrase importance into ac-
count. The value of ?l is manually specified and
automatically applied to all nodes based on prior
knowledge about the task. In this way, we can com-
pute vector representations for phrases of arbitrary
length. We denote a set of such matrices as Wlr and
bias vectors as b.
2.3 Objective Function and Learning
As with other RNN models, we add on the top of a
node x a softmax classifier. The classifier is used to
predict a K-class distribution d(x) ? RK?1 over a
specific task to train our model:
d(x) = softmax(W labelx+ blabel), (1)
where W label ? RK?d is a weight matrix and
blabel ? RK?1 is a bias vector. We denote t(x) ?
RK?1 as the target distribution vector at node x.
t(x) has a 0-1 encoding: the entry at the correct la-
bel of t(x) is 1, and the remaining entries are 0. We
then compute the cross entropy error between d(x)
and t(x):
E(x) = ?
K
?
k=1
tk(x)logdk(x),
and define an objective function as the sum of E(x)
over all training data:
J(?) =
?
x
E(x) + ?
2
???2,
where ? = (We,Wlr, b,W label, blabel) is the set of
our model parameters that should be learned. ? is a
vector of regularization parameters.
1373
To compute d(x), we can directly leverage any
other nodes? feature vectors in the same tree. We
denote such additional feature vectors as x?i ? Rd?1,
and extend Eq. (1):
d(x) = softmax(W labelx+
?
i
W addi x?i +blabel),
where W addi ? RK?d are weight matrices for addi-
tional features. We denote these matrices W addi as
W add. We also add W add to ?:
? = (We,Wlr, b,W label,W add, blabel).
The gradient of J(?)
?J(?)
??
=
?
x
?E(x)
??
+ ??
is efficiently computed via backpropagation through
structure (Goller and Ku?chler, 1996). To minimize
J(?), we use batch L-BFGS1 (Hermann and Blun-
som, 2013; Socher et al, 2012).
2.4 Averaging
We use averaged model parameters
? = 1
T + 1
T
?
t=0
?t
at test time, where ?t is the vector of model parame-
ters after t iterations of the L-BFGS optimization.
Our preliminary experimental results suggest that
averaging ? except We works well.
3 Experimental Settings
We used the Enju parser (Miyao and Tsujii, 2008)
for syntactic parsing. We used 13 phrase categories
given by Enju.
3.1 Task: Semantic Relation Classification
We evaluated our model on a semantic relation clas-
sification task: SemEval 2010 Task 8 (Hendrickx et
al., 2010). Following Socher et al (2012), we re-
garded the task as a 19-class classification problem.
There are 8,000 samples for training, and 2,717 for
1We used libLBFGS provided at http://www.
chokkan.org/software/liblbfgs/.
Figure 2: Classifying the relation between two entities.
test. For the validation set, we randomly sampled
2,182 samples from the training data.
To predict a class label, we first find the minimal
phrase that covers the target entities and then use the
vector representation of the phrase (Figure 2).
As explained in Section 2.3, we can directly con-
nect features on any other nodes to the softmax clas-
sifier. In this work, we used three such internal fea-
tures: two vector representations of target entities
and one averaged vector representation of words be-
tween the entities2.
3.2 Weights on Phrases
We tuned the weight ?l (or ?r) introduced in Sec-
tion 2.2 for this particular task. There are two fac-
tors: syntactic heads and syntactic path between tar-
get entities. Our model puts a weight ? ? [0.5, 1]
on head phrases, and 1 ? ? on the others. For re-
lation classification tasks, syntactic paths between
target entities are important (Zhang et al, 2006), so
our model also puts another weight ? ? [0.5, 1] on
phrases on the path, and 1 ? ? on the others. When
both child nodes are on the path or neither of them
on the path, we set ? = 0.5. The two weight fac-
tors are summed up and divided by 2 to be the final
weights ?l and ?r to combine the phrases. For ex-
ample, we set ?l = (1??)+?2 and ?r =
?+(1??)
2
when the right child node is the head and the left
child node is on the path.
3.3 Initialization of Model Parameters and
Tuning of Hyperparameters
We initialized We with 50-dimensional word vec-
tors3 trained with the model of Collobert et
2Socher et al (2012) used richer features including words
around entity pairs in their implementation.
3The word vectors are provided at http://ronan.
collobert.com/senna/. We used the vectors without any
modifications such as normalization.
1374
Method F1 (%)
Our model 79.4
RNN 74.8
MV-RNN 79.1
RNN w/ POS, WordNet, NER 77.6
MV-RNN w/ POS, WordNet, NER 82.4
SVM w/ bag-of-words 73.1
SVM w/ lexical and semantic features 82.2
Table 1: Comparison of our model with other methods on
SemEval 2010 task 8.
Method F1 (%)
Our model 79.4
Our model w/o phrase categories (PC) 77.7
Our model w/o head weights (HW) 78.8
Our model w/o path weights (PW) 78.7
Our model w/o averaging (AVE) 76.9
Our model w/o PC, HW, PW, AVE 74.1
Table 2: Contributions of syntactic and task-specific in-
formation and averaging.
al. (2011), and Wlr with I2 + ?, where I ? Rd?d
is an identity matrix. Here, ? is zero-mean gaussian
random variable with a variance of 0.01. The ini-
tialization of Wlr is the same as that of Socher et
al. (2013). The remaining model parameters were
initialized with 0.
We tuned hyperparameters in our model using the
validation set for each experimental setting. The hy-
perparameters include the regularization parameters
for We, Wlr, W label and W add, and the weights ?
and ?. For example, the best performance for our
model with all the proposed methods was obtained
with the values: 10?6, 10?4, 10?3, 10?3, 0.7 and
0.9 respectively.
4 Results and Discussion
Table 1 shows the performance of our model and that
of previously reported systems on the test set. The
performance of an SVM system with bag-of-words
features was reported in Rink and Harabagiu (2010),
and the performance of the RNN and MV-RNN
models was reported in Socher et al (2012). Our
model achieves an F1 score of 79.4% and outper-
forms the RNN model (74.8% F1) as well as the
simple SVM-based system (73.1% F1). More no-
Figure 3: F1 vs Training iterations.
tably, the score of our model is competitive with that
of the MV-RNN model (79.1% F1), which is com-
putationally much more expensive. Readers are re-
ferred to Hermann and Blunsom (2013) for the dis-
cussion about the computational complexity of the
MV-RNN model. We improved the performance of
RNN models on the task without much increasing
the complexity. This is a significant practical advan-
tage of our model, although its expressive power is
not the same as that of the MV-RNN model.
Our model outperforms the RNN model with one
lexical and two semantic external features: POS
tags, WordNet hypernyms and named entity tags
(NER) of target word pairs (external features). The
MV-RNN model with external features shows bet-
ter performance than our model. An SVM with rich
lexical and semantic features (Rink and Harabagiu,
2010) also outperforms ours. Note, however, that
this is not a fair comparison because those mod-
els use rich external resources such as WordNet and
named entity tags.
4.1 Contributions of Proposed Methods
We conducted additional experiments to quantify the
contributions of phrase categories, heads, paths and
averaging to our classification score. As shown in
Table 2, our model without phrase categories, heads
or paths still outperforms the RNN model with ex-
ternal features. On the other hand, our model with-
out averaging yields a lower score than the RNN
model with external features, though it is still bet-
1375
ter than the RNN model alone. Without utiliz-
ing these four properties, our model obtained only
74.1% F1. These results indicate that syntactic and
task-specific information and averaging contribute
to the performance improvement. The improvement
is achieved by a simple modification of composi-
tional functions in RNN models.
4.2 Effects of Averaging in Training
Figure 3 shows the training curves in terms of F1
scores. These curves clearly demonstrate that pa-
rameter averaging helps to stabilize the learning and
improve generalization capacity.
5 Conclusion
We have presented an averaged RNN model for se-
mantic relation classification. Our experimental re-
sults show that syntactic information such as phrase
categories and heads improves the performance, and
the task-specific weighting is also beneficial. The
results also demonstrate that averaging the model
parameters not only stabilizes the learning but also
improves the generalization capacity of the model.
As future work, we plan to combine deep learning
models with richer information such as predicate-
argument structures.
Acknowledgments
We thank the anonymous reviewers for their insight-
ful comments.
References
Ronan Collobert, Jason Weston, Le?on Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011.
Natural Language Processing (almost) from Scratch.
In JMLR, 12:2493?2537.
Christoph Goller and Andreas Ku?chler. 1996. Learning
Task-Dependent Distributed Representations by Back-
propagation Through Structure. In ICNN.
Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva, Preslav
Nakov, Diarmuid ?O Se?aghdha, Sebastian Pado?, Marco
Pennacchiotti, Lorenza Romano and Stan Szpakowicz.
2010. SemEval-2010 Task 8: Multi-Way Classication
of Semantic Relations Between Pairs of Nominals. In
SemEval 2010.
Karl Moritz Hermann and Phil Blunsom. 2013. The Role
of Syntax in Vector Space Models of Compositional Se-
mantics. In ACL.
Geoffrey E. Hinton, Nitish Srivastava, Alex Krizhevsky,
Ilya Sutskever and Ruslan R. Salakhutdinov. 2012.
Improving neural networks by preventing co-
adaptation of feature detectors. In arXiv:1207.0580.
Yusuke Miyao and Jun?ichi Tsujii. 2008. Feature Forest
Models for Probabilistic HPSG Parsing. In Computa-
tional Linguistics, 34(1):35?80, MIT Press.
Bryan Rink and Sanda Harabagiu. 2010. UTD: Clas-
sifying Semantic Relations by Combining Lexical and
Semantic Resources. In SemEval 2010.
Richard Socher, Brody Huval, Christopher D. Manning
and Andrew Y. Ng. 2012. Semantic Compositionality
Through Recursive Matrix-Vector Spaces. In EMNLP.
Richard Socher, John Bauer, Christopher D. Manning and
Andrew Y. Ng. 2013. Parsing with Compositional
Vector Grammars. In ACL.
Kevin Swersky, Bo Chen, Ben Marlin and Nando de Fre-
itas. 2010. A tutorial on stochastic approximation al-
gorithms for training Restricted Boltzmann Machines
and Deep Belief Nets. In ITA workshop.
Min Zhang, Jie Zhang, Jian Su and Guodong Zhou. 2006.
A Composite Kernel to Extract Relations between En-
tities with Both Flat and Structured Features. In COL-
ING/ACL.
1376
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1544?1555,
October 25-29, 2014, Doha, Qatar. c?2014 Association for Computational Linguistics
Jointly Learning Word Representations and Composition Functions
Using Predicate-Argument Structures
Kazuma Hashimoto?, Pontus Stenetorp?, Makoto Miwa?, and Yoshimasa Tsuruoka?
?The University of Tokyo, 3-7-1 Hongo, Bunkyo-ku, Tokyo, Japan
{hassy,pontus,tsuruoka}@logos.t.u-tokyo.ac.jp
?Toyota Technological Institute, 2-12-1 Hisakata, Tempaku-ku, Nagoya, Japan
makoto-miwa@toyota-ti.ac.jp
Abstract
We introduce a novel compositional lan-
guage model that works on Predicate-
Argument Structures (PASs). Our model
jointly learns word representations and
their composition functions using bag-
of-words and dependency-based con-
texts. Unlike previous word-sequence-
based models, our PAS-based model com-
poses arguments into predicates by using
the category information from the PAS.
This enables our model to capture long-
range dependencies between words and
to better handle constructs such as verb-
object and subject-verb-object relations.
We verify this experimentally using two
phrase similarity datasets and achieve re-
sults comparable to or higher than the pre-
vious best results. Our system achieves
these results without the need for pre-
trained word vectors and using a much
smaller training corpus; despite this, for
the subject-verb-object dataset our model
improves upon the state of the art by as
much as ?10% in relative performance.
1 Introduction
Studies on embedding single words in a vector
space have made notable successes in capturing
their syntactic and semantic properties (Turney
and Pantel, 2010). These embeddings have also
been found to be a useful component for Natural
Language Processing (NLP) systems; for exam-
ple, Turian et al. (2010) and Collobert et al. (2011)
demonstrated how low-dimensional word vectors
learned by Neural Network Language Models
(NNLMs) are beneficial for a wide range of NLP
tasks.
Recently, the main focus of research on vector
space representation is shifting from word repre-
sentations to phrase representations (Baroni and
Zamparelli, 2010; Grefenstette and Sadrzadeh,
2011; Mitchell and Lapata, 2010; Socher et al.,
2012). Combining the ideas of NNLMs and se-
mantic composition, Tsubaki et al. (2013) intro-
duced a novel NNLM incorporating verb-object
dependencies. More recently, Levy and Goldberg
(2014) presented a NNLM that integrated syntac-
tic dependencies. However, to the best of our
knowledge, there is no previous work on integrat-
ing a variety of syntactic and semantic dependen-
cies into NNLMs in order to learn composition
functions as well as word representations. The fol-
lowing question thus arises naturally:
Can a variety of dependencies be used to
jointly learn both stand-alone word vectors
and their compositions, embedding them in
the same vector space?
In this work, we bridge the gap between
purely context-based (Levy and Goldberg, 2014;
Mikolov et al., 2013b; Mnih and Kavukcuoglu,
2013) and compositional (Tsubaki et al., 2013)
NNLMs by using the flexible set of categories
from Predicate-Argument-Structures (PASs).
More specifically, we propose a Compositional
Log-Bilinear Language Model using PASs (PAS-
CLBLM), an overview of which is shown in
Figure 1. The model is trained by maximizing
the accuracy of predicting target words from their
bag-of-words and dependency-based context,
which provides information about selectional
preference. As shown in Figure 1 (b), one of the
advantages of the PAS-CLBLM is that the model
can treat not only word vectors but also composed
vectors as contexts. Since the composed vectors
1544
Figure 1: An overview of the proposed model: PAS-CLBLM. (a) The PAS-LBLM predicts target words
from their bag-of-words and dependency-based context words. (b) The PAS-CLBLM predicts target
words using not only context words but also composed vector representations derived from another level
of predicate-argument structures. Underlined words are target words and we only depict the bag-of-
words vector for the PAS-CLBLM.
are treated as input to the language model in
the same way as word vectors, these composed
vectors are expected to become similar to word
vectors for words with similar meanings.
Our empirical results demonstrate that the pro-
posed model has the ability to learn meaning-
ful representations for adjective-noun, noun-noun,
and (subject-) verb-object dependencies. On three
tasks of measuring the semantic similarity be-
tween short phrases (adjective-noun, noun-noun,
and verb-object), the learned composed vectors
achieve scores (Spearman?s rank correlation ?)
comparable to or higher than those of previ-
ous models. On a task involving more complex
phrases (subject-verb-object), our learned com-
posed vectors achieve state-of-the-art performance
(? = 0.50) with a training corpus that is an order
of magnitude smaller than that used by previous
work (Tsubaki et al., 2013; Van de Cruys et al.,
2013). Moreover, the proposed model does not
require any pre-trained word vectors produced by
external models, but rather induces word vectors
jointly while training.
2 Related Work
There is a large body of work on how to represent
the meaning of a word in a vector space. Distri-
butional approaches assume that the meaning of
a word is determined by the contexts in which it
appears (Firth, 1957). The context of a word is of-
ten defined as the words appearing in a window
of fixed-length (bag-of-words) and a simple ap-
proach is to treat the co-occurrence statistics of a
word w as a vector representation for w (Mitchell
and Lapata, 2008; Mitchell and Lapata, 2010); al-
ternatively, dependencies between words can be
used to define contexts (Goyal et al., 2013; Erk
and Pado?, 2008; Thater et al., 2010).
In contrast to distributional representations,
NNLMs represent words in a low-dimensional
vector space (Bengio et al., 2003; Collobert et al.,
2011). Recently, Mikolov et al. (2013b) and Mnih
and Kavukcuoglu (2013) proposed highly scalable
models to learn high-dimensional word vectors.
Levy and Goldberg (2014) extended the model of
Mikolov et al. (2013b) by treating syntactic depen-
dencies as contexts.
Mitchell and Lapata (2008) investigated a vari-
ety of compositional operators to combine word
vectors into phrasal representations. Among these
operators, simple element-wise addition and mul-
tiplication are now widely used to represent short
phrases (Mitchell and Lapata, 2010; Blacoe and
Lapata, 2012). The obvious limitation with these
simple approaches is that information about word
order and syntactic relations is lost.
To incorporate syntactic information into com-
position functions, a variety of compositional
models have been proposed. These include recur-
sive neural networks using phrase-structure trees
(Socher et al., 2012; Socher et al., 2013b) and
models in which words have a specific form of
parameters according to their syntactic roles and
composition functions are syntactically dependent
on the relations of input words (Baroni and Zam-
parelli, 2010; Grefenstette and Sadrzadeh, 2011;
Hashimoto et al., 2013; Hermann and Blunsom,
2013; Socher et al., 2013a).
More recently, syntactic dependency-based
1545
compositional models have been proposed (Pa-
perno et al., 2014; Socher et al., 2014; Tsub-
aki et al., 2013). One of the advantages of these
models is that they are less restricted by word or-
der. Among these, Tsubaki et al. (2013) intro-
duced a novel compositional NNLM mainly fo-
cusing on verb-object dependencies and achieved
state-of-the-art performance for the task of mea-
suring the semantic similarity between subject-
verb-object phrases.
3 PAS-CLBLM: A Compositional
Log-Bilinear Language Model Using
Predicate-Argument Structures
In some recent studies on representing words as
vectors, word vectors are learned by solving word
prediction tasks (Mikolov et al., 2013a; Mnih and
Kavukcuoglu, 2013). More specifically, given tar-
get words and their context words, the basic idea
is to train classifiers to discriminate between each
target word and artificially induced negative tar-
get words. The feature vector of the classifiers are
calculated using the context word vectors whose
values are optimized during training. As a result,
vectors of words in similar contexts become simi-
lar to each other.
Following these studies, we propose a novel
model to jointly learn representations for words
and their compositions by training word predic-
tion classifiers using PASs. In this section, we
first describe the predicate-argument structures as
they serve as the basis of our model. We then
introduce a Log-Bilinear Language Model us-
ing Predicate-Argument Structures (PAS-LBLM)
to learn word representations using both bag-of-
words and dependency-based contexts. Finally,
we propose integrating compositions of words into
the model. Figure 1 (b) shows the overview of the
proposed model.
3.1 Predicate-Argument Structures
Due to advances in deep parsing technologies,
syntactic parsers that can produce predicate-
argument structures are becoming accurate and
fast enough to be used for practical applications.
In this work, we use the probabilistic HPSG
parser Enju (Miyao and Tsujii, 2008) to obtain the
predicate-argument structures of individual sen-
tences. In its grammar, each word in a sentence
is treated as a predicate of a certain category with
zero or more arguments. Table 1 shows some ex-
Category predicate arg1 arg2
adj arg1 heavy rain
noun arg1 car accident
verb arg12 cause rain accident
prep arg12 at eat restaurant
Table 1: Examples of predicates of different cate-
gories from the grammar of the Enju parser. arg1
and arg2 denote the first and second arguments.
amples of predicates of different categories.1 For
example, a predicate of the category verb arg12
expresses a verb with two arguments. A graph can
be constructed by connecting words in predicate-
argument structures in a sentence; in general, these
graphs are acyclic.
One of the merits of using predicate-argument
structures is that they can capture dependencies
between more than two words, while standard syn-
tactic dependency structures are limited to depen-
dencies between two words. For example, one of
the predicates in the phrase ?heavy rain caused car
accidents? is the verb ?cause?, and it has two ar-
guments (?rain? and ?accident?). Furthermore, ex-
actly the same predicate-argument structure (pred-
icate: cause, first argument: rain, second argu-
ment: accident) is extracted from the passive form
of the above phrase: ?car accidents were caused
by heavy rain?. This is helpful when capturing
semantic dependencies between predicates and ar-
guments, and in extracting facts or relations de-
scribed in a sentence, such as who did what to
whom.
3.2 A Log-Bilinear Language Model Using
Predicate-Argument Structures
3.2.1 PAS-based Word Prediction
The PAS-LBLM predicts a target word given its
PAS-based context. We assume that each word
w in the vocabulary V is represented with a d-
dimensional vector v(w). When a predicate of
category c is extracted from a sentence, the PAS-
LBLM computes the predicted d-dimensional vec-
tor p(w
t
) for the target word w
t
from its context
words w
1
, w
2
, . . . , w
m
:
p(w
t
) = f
(
m
?
i=1
h
c
i
? v(w
i
)
)
, (1)
1The categories of the predicates in the Enju parser are
summarized at http://kmcs.nii.ac.jp/
?
yusuke/
enju/enju-manual/enju-output-spec.html.
1546
where hc
i
? R
d?1 are category-specific weight
vectors and ? denotes element-wise multiplica-
tion. f is a non-linearity function; in this work
we define f as tanh.
As an example following Figure 1 (a), when
the predicate ?cause? is extracted with its first
and second arguments ?rain? and ?accident?, the
PAS-LBLM computes p(cause) ? Rd following
Eq. (1):
p(cause) = f(h
verb arg12
arg1
? v(rain)+
h
verb arg12
arg2
? v(accident)).
(2)
In Eq. (2), the predicate is treated as the target
word, and its arguments are treated as the con-
text words. In the same way, an argument can be
treated as a target word:
p(rain) = f(h
verb arg12
verb
? v(cause)+
h
verb arg12
arg2
? v(accident)).
(3)
Relationship to previous work. If we omit the
the category-specific weight vectors hc
i
in Eq. (1),
our model is similar to the CBOW model in
Mikolov et al. (2013a). CBOW predicts a tar-
get word given its surrounding bag-of-words con-
text, while our model uses its PAS-based context.
To incorporate the PAS information in our model
more efficiently, we use category-specific weight
vectors. Similarly, the vLBL model of Mnih and
Kavukcuoglu (2013) uses different weight vec-
tors depending on the position relative to the tar-
get word. As with previous neural network lan-
guage models (Collobert et al., 2011; Huang et al.,
2012), our model and vLBL can use weight ma-
trices rather than weight vectors. However, as dis-
cussed by Mnih and Teh (2012), using weight vec-
tors makes the training significantly faster than us-
ing weight matrices. Despite the simple formula-
tion of the element-wise operations, the category-
specific weight vectors efficiently propagate PAS-
based context information as explained next.
3.2.2 Training Word Vectors
To train the PAS-LBLM, we use a scoring function
to evaluate how well the target word w
t
fits the
given context:
s(w
t
, p(w
t
)) = v?(w
t
)
T
p(w
t
), (4)
where v?(w
t
) ? R
d?1 is the scoring weight vector
for w
t
. Thus, the model parameters in the PAS-
LBLM are (V, ?V ,H). V is the set of word vec-
tors v(w), and ?V is the set of scoring weight vec-
tors v?(w). H is the set of the predicate-category-
specific weight vectors hc
i
.
Based on the objective in the model of Collobert
et al. (2011), the model parameters are learned by
minimizing the following hinge loss:
N
?
n=1
max(1? s(w
t
, p(w
t
)) + s(w
n
, p(w
t
)), 0),
(5)
where the negative sample w
n
is a randomly sam-
pled word other than w
t
, and N is the number
of negative samples. In our experiments we set
N = 1. Following Mikolov et al. (2013b), nega-
tive samples were drawn from the distribution over
unigrams that we raise to the power 0.75 and then
normalize to once again attain a probability distri-
bution. We minimize the loss function in Eq. (5)
using AdaGrad (Duchi et al., 2011). For further
training details, see Section 4.5.
Relationship to softmax regression models.
The model parameters can be learned by maximiz-
ing the log probability of the target word w
t
based
on the softmax function:
p(w
t
|context) =
exp(s(w
t
, p(w
t
)))
?
|V|
i=1
exp(s(w
i
, p(w
t
)))
. (6)
This is equivalent to a softmax regression model.
However, when the vocabulary V is large, com-
puting the softmax function in Eq. (6) is compu-
tationally expensive. If we do not need probabil-
ity distributions over words, we are not necessar-
ily restricted to using the probabilistic expressions.
Recently, several methods have been proposed to
efficiently learn word representations rather than
accurate language models (Collobert et al., 2011;
Mikolov et al., 2013b; Mnih and Kavukcuoglu,
2013), and our objective follows the work of Col-
lobert et al. (2011). Mikolov et al. (2013b) and
Mnih and Kavukcuoglu (2013) trained their mod-
els using word-dependent scoring weight vectors
which are the arguments of our scoring function
in Eq. (4). During development we also trained
our model using the negative sampling technique
of Mikolov et al. (2013b); however, we did not ob-
serve any significant performance difference.
Intuition behind the PAS-LBLM. Here we
briefly explain how each class of the model pa-
rameters of the PAS-LBLM contributes to learning
word representations at each stochastic gradient
1547
decent step. The category-specific weight vectors
provide the PAS information for context word vec-
tors which we would like to learn. During train-
ing, context word vectors having the same PAS-
based syntactic roles are updated similarly. The
word-dependent scoring weight vectors propagate
the information on which words should, or should
not, be predicted. In effect, context word vectors
making similar contributions to word predictions
are updated similarly. The non-linear function f
provides context words with information on the
other context words in the same PAS. In this way,
word vectors are expected to be learned efficiently
by the PAS-LBLM.
3.3 Learning Composition Functions
As explained in Section 3.1, predicate-argument
structures inherently form graphs whose nodes are
words in a sentence. Using the graphs, we can in-
tegrate relationships between multiple predicate-
argument structures into our model.
When the context word w
i
in Eq. (1), excluding
predicate words, has another predicate-argument
of category c? as a dependency, we replace v(w
i
)
with the vector produced by the composition func-
tion for the predicate category c?. For example,
as shown in Figure 1 (b), when the first argument
?rain? of the predicate ?cause? is also the argu-
ment of the predicate ?heavy?, we first compute
the d-dimensional composed vector representation
for ?heavy? and ?rain?:
g
c
?
(v(heavy), v(rain)), (7)
where c? is the category adj arg1, and g
c
? is a func-
tion to combine input vectors for the predicate-
category c?. We can use any composition func-
tion that produces a representation of the same
dimensionality as its inputs, such as element-
wise addition/multiplication (Mitchell and Lap-
ata, 2008) or neural networks (Socher et al.,
2012). We then replace v(rain) in Eq. (2) with
g
c
?
(v(heavy), v(rain)). When the second argu-
ment ?accident? in Eq. (2) is also the argument
of the predicate ?car?, v(accident) is replaced
with g
c
??
(v(car), v(accident)). c
?? is the predi-
cate category noun arg1. These multiple relation-
ships of predicate-argument structures should pro-
vide richer context information. We refer to the
PAS-LBLM with composition functions as PAS-
CLBLM.
3.4 Bag-of-Words Sensitive PAS-CLBLM
Both the PAS-LBLM and PAS-CLBLM can take
meaningful relationships between words into ac-
count. However, at times, the number of context
words can be limited and the ability of other mod-
els to take ten or more words from a fixed con-
text in a bag-of-words (BoW) fashion could com-
pensate for this sparseness. Huang et al. (2012)
combined local and global contexts in their neural
network language models, and motivated by their
work, we integrate bag-of-words vectors into our
models. Concretely, we add an additional input
term to Eq. (1):
p(w
t
) = f
(
m
?
i=1
h
c
i
? v(w
i
) + h
c
BoW
? v(BoW)
)
,
(8)
where hc
BoW
? R
d?1 are additional weight vec-
tors, and v(BoW) ? Rd?1 is the average of the
word vectors in the same sentence. To construct
the v(BoW) for each sentence, we average the
word vectors of nouns and verbs in the same sen-
tence, excluding the target and context words.
4 Experimental Settings
4.1 Training Corpus
We used the British National Corpus (BNC) as our
training corpus, extracted 6 million sentences from
the original BNC files, and parsed them using the
Enju parser described in Section 3.1.
4.2 Word Sense Disambiguation Using
Part-of-Speech Tags
In general, words can have multiple syntactic us-
ages. For example, the word cause can be a
noun or a verb depending on its context. Most
of the previous work on learning word vectors
ignores this ambiguity since word sense disam-
biguation could potentially be performed after the
word vectors have been trained (Huang et al.,
2012; Kartsaklis and Sadrzadeh, 2013). Some re-
cent work explicitly assigns an independent vec-
tor for each word usage according to its part-of-
speech (POS) tag (Hashimoto et al., 2013; Kart-
saklis and Sadrzadeh, 2013). Alternatively, Baroni
and Zamparelli (2010) assigned different forms of
parameters to adjectives and nouns.
In our experiments, we combined each word
with its corresponding POS tags. We used the
base-forms provided by the Enju parser rather than
1548
Figure 2: Two PAS-CLBLM training samples.
the surface-forms, and used the first two charac-
ters of the POS tags. For example, VB, VBP,
VBZ, VBG, VBD, VBN were all mapped to VB.
This resulted in two kinds of cause: cause NN and
cause VB and we used the 100,000 most frequent
lowercased word-POS pairs in the BNC.
4.3 Selection of Training Samples Based on
Categories of Predicates
To train the PAS-LBLM and PAS-CLBLM, we
could use all predicate categories. However, our
preliminary experiments showed that these cate-
gories covered many training samples which are
not directly relevant to our experimental setting,
such as determiner-noun dependencies. We thus
manually selected the categories used in our ex-
periments. The selected predicates are listed in
Table 1: adj arg1, noun arg1, prep arg12, and
verb arg12. These categories should provide
meaningful information on selectional preference.
For example, the prep arg12 denotes prepositions
with two arguments, such as ?eat at restaurant?
which means that the verb ?eat? is related to the
noun ?restaurant? by the preposition ?at?. Prepo-
sitions are one of the predicates whose arguments
can be verbs, and thus prepositions are important
in training the composition functions for (subject-)
verb-object dependencies as described in the next
paragraph.
Another point we had to consider was how
to construct the training samples for the PAS-
CLBLM. We constructed compositional training
samples as explained in Section 3.3 when c? was
adj arg1, noun arg1, or verb arg12. Figure 2
shows two examples in addition to the example
in Figure 1 (b). Using such training samples, the
PAS-CLBLM could, for example, recognize from
the two predicate-argument structures, ?eat food?
and ?eat at restaurant?, that eating foods is an ac-
tion that occurs at restaurants.
Model Composition Function
Add
l
v(w
1
) + v(w
2
)
Add
nl
tanh(v(w
1
) + v(w
2
))
Wadd
l
m
c
adj
? v(w
1
) + m
c
arg1
? v(w
2
)
Wadd
nl
tanh(m
c
adj
?v(w
1
)+m
c
arg1
?v(w
2
))
Table 2: Composition functions used in this work.
The examples are shown as the adjective-noun de-
pendency between w
1
=?heavy? and w
2
=?rain?.
4.4 Selection of Composition Functions
As described in Section 3.3, we are free to se-
lect any composition functions in Eq. (7). To
maintain the fast training speed of the PAS-
LBLM, we avoid dense matrix-vector multiplica-
tion in our composition functions. In Table 2,
we list the composition functions used for the
PAS-CLBLM. Add
l
is element-wise addition and
Add
nl
is element-wise addition with the non-
linear function tanh. The subscripts l and nl de-
note the words linear and non-linear. Similarly,
Wadd
l
is element-wise weighted addition and
Wadd
nl
is element-wise weighted addition with
the non-linear function tanh. The weight vec-
tors mc
i
? R
d?1 in Table 2 are predicate-category-
specific parameters which are learned during train-
ing. We investigate the effects of the non-linear
function tanh for these composition functions.
In the formulations of the backpropagation algo-
rithm, non-linear functions allow the input vectors
to weakly interact with each other.
4.5 Initialization and Optimization of Model
Parameters
We assigned a 50-dimensional vector for each
word-POS pair described in Section 4.2 and ini-
tialized the vectors and the scoring weight vec-
tors using small random values. In part inspired
by the initialization method of the weight matrices
in Socher et al. (2013a), we initialized all values
in the compositional weight vectors of the Wadd
l
and Wadd
nl
as 1.0. The context weight vectors
were initialized using small random values.
We minimized the loss function in Eq. (5) us-
ing mini-batch SGD and AdaGrad (Duchi et al.,
2011). Using AdaGrad, the SGD?s learning rate
is adapted independently for each model parame-
ter. This is helpful in training the PAS-LBLM and
PAS-CLBLM, as they have conditionally depen-
dent model parameters with varying frequencies.
1549
The mini-batch size was 32 and the learning rate
was 0.05 for each experiment, and no regulariza-
tion was used. To verify the semantics captured by
the proposed models during training and to tune
the hyperparameters, we used the WordSim-3532
word similarity data set (Finkelstein et al., 2001).
5 Evaluation on Phrase Similarity Tasks
5.1 Evaluation Settings
The learned models were evaluated on four tasks
of measuring the semantic similarity between
short phrases. We performed evaluation using the
three tasks (AN, NN, and VO) in the dataset3 pro-
vided by Mitchell and Lapata (2010), and the SVO
task in the dataset4 provided by Grefenstette and
Sadrzadeh (2011).
The datasets include pairs of short phrases ex-
tracted from the BNC. AN, NN, and VO con-
tain 108 phrase pairs of adjective-noun, noun-
noun, and verb-object. SVO contains 200 pairs of
subject-verb-object phrases. Each phrase pair has
multiple human-ratings: the higher the rating is,
the more semantically similar the phrases. For ex-
ample, the subject-verb-object phrase pair of ?stu-
dent write name? and ?student spell name? has a
high rating. The pair ?people try door? and ?peo-
ple judge door? has a low rating.
For evaluation we used the Spearman?s rank
correlation ? between the human-ratings and the
cosine similarity between the composed vector
pairs. We mainly used non-averaged human-
ratings for each pair, and as described in Section
5.3, we also used averaged human-ratings for the
SVO task. Each phrase pair in the datasets was an-
notated by more than two annotators. In the case
of averaged human ratings, we averaged multiple
human-ratings for each phrase pair, and in the case
of non-averaged human-ratings, we treated each
human-rating as a separate annotation.
With the PAS-CLBLM, we represented each
phrase using the composition functions listed in
Table 2. When there was no composition present,
we represented the phrase using element-wise ad-
dition. For example, when we trained the PAS-
CLBLM with the composition function Wadd
nl
,
2http://www.cs.technion.ac.il/
?
gabr/
resources/data/wordsim353/
3http://homepages.inf.ed.ac.uk/
s0453356/share
4http://www.cs.ox.ac.uk/activities/
compdistmeaning/GS2011data.txt
Model AN NN VO
PAS-CLBLM (Add
l
) 0.52 0.44 0.35
PAS-CLBLM (Add
nl
) 0.52 0.46 0.45
PAS-CLBLM (Wadd
l
) 0.48 0.39 0.34
PAS-CLBLM (Wadd
nl
) 0.48 0.40 0.39
PAS-LBLM 0.41 0.44 0.39
word2vec 0.52 0.48 0.42
BL w/ BNC 0.48 0.50 0.35
HB w/ BNC 0.41 0.44 0.34
KS w/ ukWaC n/a n/a 0.45
K w/ BNC n/a n/a 0.41
Human agreement 0.52 0.49 0.55
Table 3: Spearman?s rank correlation scores ? for
the three tasks: AN, NN, and VO.
the composed vector for each phrase was com-
puted using the Wadd
nl
function, and when we
trained the PAS-LBLM, we used the element-wise
addition function. To compute the composed vec-
tors using the Wadd
l
and Wadd
nl
functions, we
used the categories of the predicates adj arg1,
noun arg1, and verb arg12 listed in Table 1.
As a strong baseline, we trained the Skip-gram
model of Mikolov et al. (2013b) using the pub-
licly available word2vec5 software. We fed the
POS-tagged BNC into word2vec since our models
utilize POS tags and trained 50-dimensional word
vectors using word2vec. For each phrase we then
computed the representation using vector addition.
5.2 AN, NN, and VO Tasks
Table 3 shows the correlation scores ? for the AN,
NN, and VO tasks. Human agreement denotes the
inter-annotator agreement. The word2vec baseline
achieves unexpectedly high scores for these three
tasks. Previously these kinds of models (Mikolov
et al., 2013b; Mnih and Kavukcuoglu, 2013) have
mainly been evaluated for word analogy tasks and,
to date, there has been no work using these word
vectors for the task of measuring the semantic sim-
ilarity between phrases. However, this experimen-
tal result suggests that word2vec can serve as a
strong baseline for these kinds of tasks, in addi-
tion to word analogy tasks.
In Table 3, BL, HB, KS, and K denote the work
of Blacoe and Lapata (2012), Hermann and Blun-
som (2013), Kartsaklis and Sadrzadeh (2013), and
Kartsaklis et al. (2013) respectively. Among these,
5https://code.google.com/p/word2vec/
1550
Averaged Non-averaged
Model Corpus SVO-SVO SVO-V SVO-SVO SVO-V
PAS-CLBLM (Add
l
) 0.29 0.34 0.24 0.28
PAS-CLBLM (Add
nl
) 0.27 0.32 0.24 0.28
PAS-CLBLM (Wadd
l
) BNC 0.25 0.26 0.21 0.23
PAS-CLBLM (Wadd
nl
) 0.42 0.50 0.34 0.41
PAS-LBLM 0.21 0.06 0.18 0.08
word2vec BNC 0.12 0.32 0.12 0.28
Grefenstette and Sadrzadeh (2011) BNC n/a n/a 0.21 n/a
Tsubaki et al. (2013) ukWaC n/a 0.47 n/a n/a
Van de Cruys et al. (2013) ukWaC n/a n/a 0.32 0.37
Human agreement 0.75 0.62
Table 4: Spearman?s rank correlation scores ? for the SVO task. Averaged denotes the ? calculated by
averaged human ratings, and Non-averaged denotes the ? calculated by non-averaged human ratings.
only Kartsaklis and Sadrzadeh (2013) used the
ukWaC corpus (Baroni et al., 2009) which is an or-
der of magnitude larger than the BNC. As we can
see in Table 3, the PAS-CLBLM (Add
nl
) achieves
scores comparable to and higher than those of the
baseline and the previous state-of-the-art results.
In relation to these results, the Wadd
l
and Wadd
nl
variants of the PAS-CLBLM do not achieve great
improvements in performance. This indicates that
simple word vector addition can be sufficient to
compose representations for phrases consisting of
word pairs.
5.3 SVO Task
Table 4 shows the correlation scores ? for the SVO
task. The scores ? for this task are reported for
both averaged and non-averaged human ratings.
This is due to a disagreement in previous work
regarding which metric to use when reporting re-
sults. Hence, we report the scores for both settings
in Table 4. Another point we should consider is
that some previous work reported scores based on
the similarity between composed representations
(Grefenstette and Sadrzadeh, 2011; Van de Cruys
et al., 2013), and others reported scores based on
the similarity between composed representations
and word representations of landmark verbs from
the dataset (Tsubaki et al., 2013; Van de Cruys et
al., 2013). For completeness, we report the scores
for both settings: SVO-SVO and SVO-V in Table 4.
The results show that the weighted addition
model with the non-linear function tanh (PAS-
CLBLM (Wadd
nl
)) is effective for the more com-
plex phrase task. While simple vector addition is
sufficient for phrases consisting of word pairs, it is
clear from our experimental results that they fall
short for more complex structures such as those
involved in the SVO task.
Our PAS-CLBLM (Wadd
nl
) model outperforms
the previous state-of-the-art scores for the SVO
task as reported by Tsubaki et al. (2013) and
Van de Cruys et al. (2013). As such, there are three
key points that we would like to emphasize:
(1) the difference of the training corpus size,
(2) the necessity of the pre-trained word vectors,
(3) the modularity of deep learning models.
Tsubaki et al. (2013) and Van de Cruys et al.
(2013) used the ukWaC corpus. This means our
model works better, despite using a considerably
smaller corpora. It should also be noted that, like
us, Grefenstette and Sadrzadeh (2011) used the
BNC corpus.
The model of Tsubaki et al. (2013) is based on
neural network language models which use syn-
tactic dependencies between verbs and their ob-
jects. While their novel model, which incorpo-
rates the idea of co-compositionality, works well
with pre-trained word vectors produced by exter-
nal models, it is not clear whether the pre-trained
vectors are required to achieve high scores. In
contrast, we have achieved state-of-the-art results
without the use of pre-trained word vectors.
Despite our model?s scalability, we trained 50-
dimensional vector representations for words and
their composition functions and achieved high
scores using this low dimensional vector space.
1551
model d AN NN VO SVO
Add
l
50 0.52 0.44 0.35 0.24
1000 0.51 0.51 0.43 0.31
Add
nl
50 0.52 0.46 0.45 0.24
1000 0.51 0.50 0.45 0.31
Wadd
l
50 0.48 0.39 0.34 0.21
1000 0.50 0.49 0.43 0.32
Wadd
nl
50 0.48 0.40 0.39 0.34
1000 0.51 0.48 0.48 0.34
Table 5: Comparison of the PAS-CLBLM between
d = 50 and d = 1000.
This maintains the possibility to incorporate re-
cently developed deep learning composition func-
tions into our models, such as recursive neural
tensor networks (Socher et al., 2013b) and co-
compositional neural networks (Tsubaki et al.,
2013). While such complex composition functions
slow down the training of compositional models,
richer information could be captured during train-
ing.
5.4 Effects of the Dimensionality
To see how the dimensionality of the word vectors
affects the scores, we trained the PAS-CLBLM for
each setting using 1,000-dimensional word vectors
and set the learning rate to 0.01. Table 5 shows
the scores for all four tasks. Note that we only re-
port the scores for the setting non-averaged SVO-
SVO here. As shown in Table 5, the scores consis-
tently improved with a few exceptions. The scores
? = 0.51 for the NN task and ? = 0.48 for the
VO task are the best results to date. However, the
score ? = 0.34 for the SVO task did not improve
by increasing the dimensionality. This means that
simply increasing the dimensionality of the word
vectors does not necessarily lead to better results
for complex phrases.
5.5 Effects of Bag-of-Words Contexts
Lastly, we trained the PAS-CLBLM without the
bag-of-words contexts described in Section 3.4
and used 50-dimensional word vectors. As can be
seen in Table 6, large score improvements were
observed only for the VO and SVO tasks by in-
cluding the bag-of-words contexts and the non-
linearity function. It is likely that the results de-
pend on how the bag-of-words contexts are con-
structed. However, we leave this line of analysis
as future work. Both adjective-noun and noun-
model BoW AN NN VO SVO
Add
l
w/ 0.52 0.44 0.35 0.24
w/o 0.48 0.46 0.38 0.23
Add
nl
w/ 0.52 0.46 0.45 0.24
w/o 0.50 0.47 0.41 0.15
Wadd
l
w/ 0.48 0.39 0.34 0.21
w/o 0.47 0.39 0.38 0.21
Wadd
nl
w/ 0.48 0.40 0.39 0.34
w/o 0.52 0.42 0.33 0.26
Table 6: Scores of the PAS-CLBLM with and
without BoW contexts.
noun phrase are noun phrases, and (subject-) verb-
object phrases can be regarded as complete sen-
tences. Therefore, different kinds of context infor-
mation might be required for both groups.
6 Qualitative Analysis on Composed
Vectors
An open question that remains is to what ex-
tent composition affects the representations pro-
duced by our PAS-CLBLM model. To evalu-
ate this we assigned a vector for each composed
representation. For example, the adjective-noun
dependency ?heavy rain? would be assigned an
independent vector. We added the most fre-
quent 100,000 adjective-noun, noun-noun, and
(subject-) verb-object tuples to the vocabulary and
the resulting vocabulary contained 400,000 to-
kens (100,000+3?100,000). A similar method
for treating frequent neighboring words as single
words was introduced by Mikolov et al. (2013b).
However, some dependencies, such as (subject-)
verb-object phrases, are not always captured when
considering only neighboring words.
Table 7 (No composition) shows some examples
of predicate-argument dependencies with their
closest neighbors in the vector space according
to the cosine similarity. The table shows that the
learned vectors of multiple words capture seman-
tic similarity. For example, the vector of ?heavy
rain? is close to the vectors of words which ex-
press the phenomena heavily raining. The vector
of ?new york? captures the concept of a major city.
The vectors of (subject-) verb-object dependencies
also capture the semantic similarity, which is the
main difference to previous approaches, such as
that of Mikolov et al. (2013b), which only consider
neighboring words. These results suggest that the
PAS-CLBLM can learn meaningful composition
1552
Query No composition Composition
rain rain
(AN) thunderstorm sunshine
heavy downpour storm
rain blizzard drizzle
much rain chill
general manager executive
(AN) vice president director
chief executive director representative
executive project manager officer
managing director administrator
second war war
(NN) plane crash world
world riot race
war last war holocaust
great war warfare
oslo york
(NN) paris toronto
new birmingham paris
york moscow edinburgh
madrid glasgow
make order make
(VO) carry survey allow
make pay tax demand
payment pay produce
impose tax bring
achieve objective solve
(VO) bridge gap alleviate
solve improve quality overcome
problem deliver information resolve
encourage development circumvent
hold meeting take
(SVO) event take place get
meeting end season win
take discussion take place put
place do work gain
Table 7: Nearest neighbor vectors for multiple
words. POS-tags are not shown for simplicity.
category predicate arg1 arg2
adj arg1 2.38 6.55 -
noun arg1 3.37 5.60 -
verb arg12 6.78 2.57 2.18
Table 8: L2-norms of the 50-dimensional weight
vectors of the composition function Wadd
nl
.
functions since the composition layers receive the
same error signal via backpropagation.
We then trained the PAS-CLBLM using Wadd
nl
to learn composition functions. Table 7 (Compo-
sition) shows the nearest neighbor words for each
composed vector, and as we can see, the learned
composition function emphasizes the head words
and captures some sort of semantic similarity. We
then inspected the L2-norms of the weight vectors
of the composition function. As shown in Table 8,
head words are strongly emphasized. Emphasiz-
ing head words is helpful in representing com-
posed meanings, but in the case of verbs it may
not always be sufficient. This can be observed in
Table 3 and Table 4, which demonstrates that verb-
related tasks are more difficult than noun-phrase
tasks.
While No composition captures the seman-
tic similarity well using independent parameters,
there is the issue of data sparseness. As the size of
the vocabulary increases, the number of tuples of
word dependencies increases rapidly. In this ex-
periment, we used only the 300,000 most frequent
tuples. In contrast to this, the learned composi-
tion functions can capture similar information us-
ing only word vectors and a small set of predicate
categories.
7 Conclusion and Future Work
We have presented a compositional log-bilinear
language model using predicate-argument struc-
tures that incorporates both bag-of-words and
dependency-based contexts. In our experiments
the learned composed vectors achieve state-of-the-
art scores for the task of measuring the semantic
similarity between short phrases. For the subject-
verb-object phrase task, the result is achieved
without any pre-trained word vectors using a cor-
pus an order of magnitude smaller than that of the
previous state of the art. For future work, we will
investigate how our models and the resulting vec-
tor representations can be helpful for other unsu-
pervised and/or supervised tasks.
Acknowledgments
We thank the anonymous reviewers for their help-
ful comments and suggestions. This work was
supported by JSPS KAKENHI Grant Number
13F03041.
References
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, pages
1183?1193.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi,
and Eros Zanchetta. 2009. The WaCky Wide Web:
A Collection of Very Large Linguistically Processed
Web-Crawled Corpora. Language Resources and
Evaluation, 43(3):209?226.
Yoshua Bengio, Re?jean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A Neural Probabilistic Lan-
1553
guage Model. Journal of Machine Learning Re-
search, 3:1137?1155.
William Blacoe and Mirella Lapata. 2012. A Com-
parison of Vector-based Representations for Seman-
tic Composition. In Proceedings of the 2012 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, pages 546?556.
Ronan Collobert, Jason Weston, Le?on Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural Language Processing (Almost) from
Scratch. Journal of Machine Learning Research,
12:2493?2537.
John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive Subgradient Methods for Online Learning
and Stochastic Optimization. Journal of Machine
Learning Research, 12:2121?2159.
Katrin Erk and Sebastian Pado?. 2008. A Structured
Vector Space Model for Word Meaning in Context.
In Proceedings of the 2008 Conference on Empiri-
cal Methods in Natural Language Processing, pages
897?906.
Lev Finkelstein, Gabrilovich Evgenly, Matias Yossi,
Rivlin Ehud, Solan Zach, Wolfman Gadi, and Rup-
pin Eytan. 2001. Placing Search in Context: The
Concept Revisited. In Proceedings of the Tenth In-
ternational World Wide Web Conference.
John Rupert Firth. 1957. A synopsis of linguistic
theory 1930-55. In Studies in Linguistic Analysis,
pages 1?32.
Kartik Goyal, Sujay Kumar Jauhar, Huiying Li, Mrin-
maya Sachan, Shashank Srivastava, and Eduard
Hovy. 2013. A Structured Distributional Seman-
tic Model : Integrating Structure with Semantics. In
Proceedings of the Workshop on Continuous Vector
Space Models and their Compositionality, pages 20?
29.
Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011.
Experimental Support for a Categorical Composi-
tional Distributional Model of Meaning. In Pro-
ceedings of the 2011 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1394?
1404.
Kazuma Hashimoto, Makoto Miwa, Yoshimasa Tsu-
ruoka, and Takashi Chikayama. 2013. Simple Cus-
tomization of Recursive Neural Networks for Se-
mantic Relation Classification. In Proceedings of
the 2013 Conference on Empirical Methods in Nat-
ural Language Processing, pages 1372?1376.
Karl Moritz Hermann and Phil Blunsom. 2013. The
Role of Syntax in Vector Space Models of Composi-
tional Semantics. In Proceedings of the 51st Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers), pages 894?904.
Eric Huang, Richard Socher, Christopher Manning,
and Andrew Ng. 2012. Improving Word Represen-
tations via Global Context and Multiple Word Proto-
types. In Proceedings of the 50th Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers), pages 873?882.
Dimitri Kartsaklis and Mehrnoosh Sadrzadeh. 2013.
Prior Disambiguation of Word Tensors for Con-
structing Sentence Vectors. In Proceedings of the
2013 Conference on Empirical Methods in Natural
Language Processing, pages 1590?1601.
Dimitri Kartsaklis, Mehrnoosh Sadrzadeh, and Stephen
Pulman. 2013. Separating Disambiguation from
Composition in Distributional Semantics. In Pro-
ceedings of 17th Conference on Natural Language
Learning (CoNLL), pages 114?123.
Omer Levy and Yoav Goldberg. 2014. Dependency-
Based Word Embeddings. In Proceedings of the
52nd Annual Meeting of the Association for Compu-
tational Linguistics (Volume 2: Short Papers), pages
302?308.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient Estimation of Word Repre-
sentations in Vector Space. In Proceedings of Work-
shop at the International Conference on Learning
Representations.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013b. Distributed Represen-
tations of Words and Phrases and their Composition-
ality. In Advances in Neural Information Processing
Systems 26, pages 3111?3119.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
46th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, pages 236?244.
Jeff Mitchell and Mirella Lapata. 2010. Composition
in Distributional Models of Semantics. Cognitive
Science, 34(8):1388?1439.
Yusuke Miyao and Jun?ichi Tsujii. 2008. Feature for-
est models for probabilistic HPSG parsing. Compu-
tational Linguistics, 34(1):35?80.
Andriy Mnih and Koray Kavukcuoglu. 2013. Learning
word embeddings efficiently with noise-contrastive
estimation. In Advances in Neural Information Pro-
cessing Systems 26, pages 2265?2273.
Andriy Mnih and Yee Whye Teh. 2012. A fast
and simple algorithm for training neural probabilis-
tic language models. In John Langford and Joelle
Pineau, editors, Proceedings of the 29th Interna-
tional Conference on Machine Learning (ICML-12),
ICML ?12, pages 1751?1758.
Denis Paperno, Nghia The Pham, and Marco Baroni.
2014. A practical and linguistically-motivated ap-
proach to compositional distributional semantics. In
1554
Proceedings of the 52nd Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), pages 90?99.
Richard Socher, Brody Huval, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Semantic Compo-
sitionality through Recursive Matrix-Vector Spaces.
In Proceedings of the 2012 Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
pages 1201?1211.
Richard Socher, John Bauer, Christopher D. Manning,
and Ng Andrew Y. 2013a. Parsing with Compo-
sitional Vector Grammars. In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
455?465.
Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D. Manning, Andrew Ng, and
Christopher Potts. 2013b. Recursive Deep Mod-
els for Semantic Compositionality Over a Sentiment
Treebank. In Proceedings of the 2013 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 1631?1642.
Richard Socher, Quoc V. Le, Christopher D. Manning,
and Andrew Y. Ng. 2014. Grounded Compositional
Semantics for Finding and Describing Images with
Sentences. Transactions of the Association for Com-
putational Linguistics, 2:207?218.
Stefan Thater, Hagen Fu?rstenau, and Manfred Pinkal.
2010. Contextualizing Semantic Representations
Using Syntactically Enriched Vector Models. In
Proceedings of the 48th Annual Meeting of the As-
sociation for Computational Linguistics, pages 948?
957.
Masashi Tsubaki, Kevin Duh, Masashi Shimbo, and
Yuji Matsumoto. 2013. Modeling and Learning Se-
mantic Co-Compositionality through Prototype Pro-
jections and Neural Networks. In Proceedings of the
2013 Conference on Empirical Methods in Natural
Language Processing, pages 130?140.
Joseph Turian, Lev-Arie Ratinov, and Yoshua Bengio.
2010. Word Representations: A Simple and General
Method for Semi-Supervised Learning. In Proceed-
ings of the 48th Annual Meeting of the Association
for Computational Linguistics, pages 384?394.
Peter D. Turney and Patrick Pantel. 2010. From Fre-
quency to Meaning: Vector Space Models of Se-
mantics. Journal of Artificial Intelligence Research,
37(1):141?188.
Tim Van de Cruys, Thierry Poibeau, and Anna Korho-
nen. 2013. A Tensor-based Factorization Model of
Semantic Compositionality. In Proceedings of the
2013 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 1142?1151.
1555
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 88?92, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
UTTime: Temporal Relation Classification using Deep Syntactic Features
Natsuda Laokulrat
The University of Tokyo
3-7-1 Hongo, Bunkyo-ku,
Tokyo, Japan
natsuda@logos.t.u-tokyo.ac.jp
Makoto Miwa
The University of Manchester
131 Princess Street,
Manchester, M1 7DN, UK
makoto.miwa@manchester.ac.uk
Yoshimasa Tsuruoka
The University of Tokyo
3-7-1 Hongo, Bunkyo-ku,
Tokyo, Japan
tsuruoka@logos.t.u-tokyo.ac.jp
Takashi Chikayama
The University of Tokyo
3-7-1 Hongo, Bunkyo-ku,
Tokyo, Japan
chikayama@logos.t.u-tokyo.ac.jp
Abstract
In this paper, we present a system, UTTime,
which we submitted to TempEval-3 for Task
C: Annotating temporal relations. The sys-
tem uses logistic regression classifiers and ex-
ploits features extracted from a deep syntactic
parser, including paths between event words in
phrase structure trees and their path lengths,
and paths between event words in predicate-
argument structures and their subgraphs. UT-
Time achieved an F1 score of 34.9 based
on the graphed-based evaluation for Task C
(ranked 2nd) and 56.45 for Task C-relation-
only (ranked 1st) in the TempEval-3 evalua-
tion.
1 Introduction
Temporal annotation is the task of identifying tem-
poral relationships between pairs of temporal enti-
ties, namely temporal expressions and events, within
a piece of text. The temporal relationships are im-
portant to support other NLP applications such as
textual entailment, document summarization, and
question answering. The temporal annotation task
consists of several subtasks, including temporal ex-
pression extraction, event extraction, and temporal
link identification and relation classification.
In TempEval-3, there are three subtasks of the
temporal annotation process offered, i.e., Task A:
Temporal expression extraction and normalization,
Task B: Event extraction, and Task C: Annotating
temporal relations. This paper presents a system
to handle Task C. Based on the annotated data pro-
vided, this subtask requires identifying pairs of tem-
poral entities and classifying the pairs into one of the
14 relation types according to TimeML (Pustejovsky
et al, 2005), i.e., BEFORE, AFTER, IMMEDIATELY BE-
FORE, IMMEDIATELY AFTER, INCLUDES, IS INCLUDED,
DURING, DURING INVERSE, SIMULTANEOUS, IDENTITY,
BEGINS, BEGUN BY, END, and ENDED BY.
The motivation behind our work is to utilize syn-
tactic and semantic relationships between a pair of
temporal entities in the temporal relation classifica-
tion task, since we believe that these relationships
convey the temporal relation. In addition to general
features, which are easily extracted from sentences
(e.g., part of speech tags, lemmas, synnonyms), we
use features extracted using a deep syntactic parser.
The features from the deep parser can be divided into
two groups: features from phrase structure trees and
features from predicate-argument structures. These
features are only applicable in the case that the tem-
poral entities appear in the same sentence, so we use
only the general features for inter-sentence relations.
Predicate-argument structure expresses semantic
relations between words. This information can be
extracted from a deep syntactic parser. Features
from predicate-argument structures can capture im-
portant temporal information (e.g., prepositions of
time) from sentences effectively.
The remaining part of this paper is organized as
follows. We explain our approach in detail in Sec-
tion 2 and then show the evaluation and results in
Section 3. Finally, we conclude with directions for
future work in Section 4.
2 Approach
Our system, UTTime, is based on a supervised ma-
chine learning approach. UTTime performs two
tasks; TLINK identification and classification. In
88
other words, UTTime identifies pairs of temporal en-
tities and classifies these pairs into temporal relation
types.
2.1 TLINK identification
A pair of temporal entities that have a temporal rela-
tion is called a TLINK. The system first determines
which pairs of temporal entities are linked by using
a ruled-based approach as a baseline approach.
All the TempEval-3?s possible pairs of temporal
entities are extracted by a set of simple rules; pairs
of temporal entities that satisfy one of the following
rules are considered as TLINKs.
? Event and document creation time
? Events in the same sentence
? Event and temporal expression in the same sen-
tence
? Events in consecutive sentences
2.2 TLINK classification
Each TLINK is classified into a temporal relation
type. We use a machine learning approach for the
temporal relation classification. Two L2-regularized
logistic regression classifiers, LIBLINEAR (Fan et
al., 2008), are used; one for event-event TLINKs,
and another one for event-time TLINKs. In addition
to general features at different linguistic levels, fea-
tures extracted by a deep syntactic parser are used.
The general features we employed are:
? Event and timex attributes
All attributes associated with events (class,
tense, aspect, modality, and polarity) and
temporal expressions (type, value, func-
tionInDocument, and temporalFunction) are
used. For event-event TLINKs, we also use
tense/class/aspect match, tense/class/aspect bi-
grams as features (Chambers et al, 2007).
? Morphosyntactic information
Words, part of speech tags, lemmas within a
window before/after event words are extracted
using Stanford coreNLP (Stanford NLP Group,
2012).
? Lexical semantic information
Figure 1: Phrase structure tree
Synonyms of event word tokens from WordNet
lexical database (Fellbaum, 1998) are used as
features.
? Event-Event information
For event-event TLINKs, we use
same sentence feature to differentiate pairs
of events in the same sentence from pairs of
events from different sentences (Chambers et
al., 2007).
In the case that temporal entities of a particu-
lar TLINK are in the same sentence, we extract
two new types of sentence-level semantic informa-
tion from a deep syntactic parser. We use the Enju
parser (Miyao and Tsujii, 2008). It analyzes syn-
tactic/semantic structures of sentences and provides
phrase structures and predicate-argument structures.
The features we extract from the deep parser are
? Paths between event words in the phrase struc-
ture tree, and up(?)/down(?) lengths of paths.
We use 3-grams of paths as features instead of
full paths since these are too sparse. An ex-
ample is shown in Figure 1. In this case, the
path between the event words, estimates and
worth, is VBZ?, VX?, VP?, VP?, VP, PP?, PX?, IN?.
The 3-grams of the path are, therefore, {VBZ?-
VX?-VP?, VX?-VP?-VP?, VP?-VP?-VP, VP?-VP-PP?,
VP-PP?-PX?, PP?-PX-?-IN?}. The up/down path
89
Figure 2: Predicate argument structure
lengths are 4 (VBZ?, VX?, VP?, VP?) and 3 (PP?,
PX?, IN?) respectively.
? Paths between event words in predicate-
argument structure, and their subgraphs.
For the previous example, we can express the
relations in predicate-argument structure repre-
sentation as
? verb arg12: estimate (she, properties)
? prep arg12: worth (estimate, dollars)
In this case, the path between the event words,
estimates and worth, is?prep arg12:arg1. That
is, the type of the predicate worth is prep arg12
and it has estimate as the first argument (arg1).
The path from estimate to worth is in reverse
direction (?).
The next example sentence, John saw mary be-
fore the meeting, gives an idea of a more com-
plex predicate-argument structure as shown
in Figure 2. The path between the event
words, saw and meeting is ?prep arg12:arg1,
prep arg12:arg2.
We use (v, e, v) and (e, v, e) tuples of the
edges and vertices on the path as features.
For example, in Figure 2, the (v,e,v) tuples
are (see, ?prep arg12:arg1, before) and (be-
fore, prep arg12:arg2, meeting). In the same
way, the (e,v,e) tuple is (?prep arg12:arg1,
before, prep arg12:arg2). The subgraphs
of (v, e, v) and (e, v, e) tuples are also
used, including (see, ?prep arg12:arg1,
*), (*, ?prep arg12:arg1, before), (*,
?prep arg12:arg1, *), (*, prep arg12:arg2,
meeting), (before, prep arg12:arg2, *), (*,
prep arg12:arg2, *), (*, before, prep arg12:arg2),
(?prep arg12:arg1, before, *), (*, before, *).
From the above example, the features from pred-
icate argument structure can properly capture the
preposition before. It can also capture a preposi-
tion from a compound sentence such as John met
Mary before he went back home. The path between
the event words met and went are (?conj arg12:arg1,
conj arg12:arg2) and the (v, e, v) and (e, v, e)
tuples are (met, ?conj arg12:arg1, before), (before,
conj arg12:arg2, went), and (?prep arg12:arg1, be-
fore, prep arg12:arg2).
2.3 Hybrid approach
The rule-based approach described in Section 2.1
produces many unreasonable and excessive links.
We thus use a machine learning approach to filter
out those unreasonable links by training the model
in Section 2.2 with an additional relation type, UN-
KNOWN, for links that satisfy the rules in Section
2.1 but do not appear in the training data.
In this way, for Task C, we first extract all the links
that satisfy the rules and classify the relation types of
those links. After classifying temporal relations, we
remove the links that are classified as UNKNOWN.
3 Evaluation
The scores are calculated by the graph-based eval-
uation metric proposed by UzZaman and Allen
(2011). We trained the models with TimeBank and
AQUAINT corpora. We also trained our models on
the training set with inverse relations. The perfor-
mance analysis is based on 10-fold cross validation
on the development data.
3.1 Task C
In Task C, a system has to identify appropriate tem-
poral links and to classify each link into one tempo-
ral relation type. For Task C evaluation, we compare
the results of the models trained with and without the
features from the deep parser. The results are shown
in Table 1. The rule-based approach gives a very low
precision.
3.2 Task C-relation-only
Task C-relation-only provides a system with all the
appropriate temporal links and only needs the sys-
tem to classify the relation types. Since our goal is to
exploit the features from the deep parser, in Task C-
relation-only, we measured the contribution of those
features to temporal relation classification in Table
2.
90
Features F1 P R
gen. (rule) 22.51 14.32 52.58
gen. + ph. + pas. (rule) 22.61 14.30 54.01
gen. + ph. + pas. (hyb.) 33.52 36.23 31.19
gen. + ph. + pas. (hyb. + inv.) 39.53 37.56 41.70
Table 1: Result of Task C. (rule: rule-based approach,
hyb.: hybrid approach, gen.: general features, ph.:phrase
structure tree features, pas.:predicate-argument structure
features, and inv.: Inverse relations are used for training.)
Features F1 P R
gen. 64.42 64.59 64.25
gen. + ph. 65.24 65.42 65.06
gen. + pas. 66.40 66.55 66.25
gen. + ph. + pas. 66.39 66.55 66.23
gen. + ph. + pas. (inv.) 65.30 65.39 65.20
Table 2: Result of Task C-relation-only. (gen.:
general features, ph.:phrase structure tree features,
pas.:predicate-argument structure features, and inv.: In-
verse relations are used for training.)
The predicate-argument-structure features con-
tributed to the improvement more than those of
phrase structures in both precision and recall. The
reason is probably that the features from phrase
structures that we used did not imply a temporal re-
lation of events in the sentence. For instance, the
sentence ?John saw Mary before the meeting? gives ex-
actly the same path as of the sentence ?John sawMary
after the meeting?.
3.3 Results on test data
Tables 3 and 4 show the results on the test data,
which were manually annotated and provided by the
TempEval-3 organizer. We also show the scores of
the other systems in the tables. For the evaluation
on the test data, we used the models trained with
general features, phrase structure tree features, and
predicate-argument structure features.
UTTime-5 ranked 2nd best in Task C. Interest-
ingly, training the models with inverse relations im-
proved the system only when using the hybrid ap-
proach. This means that the inverse relations did not
improve the temporal classification but helped the
system filter out unreasonable links (UNKNOWN)
in the hybrid approach. As expected, the ruled-based
approach got a very high recall score at the expense
of precision. UTTime-1, although it achieved the F1
Approach F1 P R
rule (UTTime-1) 24.65 15.18 65.64
rule + inv (UTTime-3) 24.28 15.1 61.99
hyb. (UTTime-4) 28.81 37.41 23.43
hyb. + inv. (UTTime-5) 34.9 35.94 33.92
cleartk 36.26 37.32 35.25
NavyTime 31.06 35.48 27.62
JU-CSE 26.41 21.04 35.47
KUL-KULTaskC 24.83 23.35 26.52
Table 3: Result of Tack C on test data. (rule: rule-based
approach, hyb.: hybrid approach, and inv.: Inverse rela-
tions are used for training.)
Approach F1 P R
gen. + ph. + pas. (UTTime-1) 56.45 55.58 57.35
gen. + ph. + pas. (UTTime-2) 54.26 53.2 55.36
gen. + ph. + pas. (inv.) (UTTime-3) 54.7 53.85 55.58
NavyTime 46.83 46.59 47.07
JU-CSE 34.77 35.07 34.48
Table 4: Result of Task C-relation-only on test data.
(gen.: general features, ph.:phrase structure tree features,
pas.:predicate-argument structure features, and inv.: In-
verse relations are used for training.)
score of only 24.65, got the highest recall among all
the systems.
For Task C-relation-only, we achieved the highest
F1 score, precision, and recall. UTTime-2 basically
had the same models as that of UTTime-1, but we
put different weights for each relation type. The re-
sults show that using the weights did not improve
the score in graph-based evaluation.
4 Conclusion
The system, UTTime, identifying temporal links and
classifying temporal relation, is proposed. The links
were identified based on the rule-based approach
and then some links were filtered out by a classi-
fier. The filtering helped improve the system consid-
erably. For the relation classification task, the fea-
tures extracted from phrase structures and predicate-
argument structures were proposed, and the features
improved the classification in precision, recall, and
F-score.
In future work, we hope to improve the classifica-
tion performance by constructing timegraphs (Miller
and Schubert, 1999), so that the system can use in-
formation from neighbor TLINKs as features.
91
References
James Pustejovsky, Robert Ingria, Roser Saur??, Jose?
Castan?o, Jessica Littman, Rob Gaizauskas, Andrea
Setzer, Graham Katz, Inderjeet Mani 2005. The spec-
ification language TimeML. The Language of Time: A
reader, pages 545?557
Stanford Natural Language Processing Group. 2012.
Stanford CoreNLP.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. Cambridge, MA: MIT Press.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A Li-
brary for Large Linear Classification.
Nathanael Chambers, Shan Wang and Dan Jurafsky.
2007. Classifying Temporal Relations between
Events. In ACL 2007, pages 173?176.
Yusuke Miyao and Jun?ichi Tsujii. 2008. Feature Forest
Models for Probabilistic HPSG Parsing. In Computa-
tional Linguistics. 34(1). pages 35?80, MIT Press.
Naushad UzZaman and James F. Allen. 2011. Temporal
Evaluation. In ACL 2011, pages 351?356.
Stephanie A. Miller and Lenhart K. Schubert. 1999.
Time Revisited. In Computational Intelligence 6,
pages 108?118.
92
Proceedings of the Fifteenth Conference on Computational Natural Language Learning, pages 238?246,
Portland, Oregon, USA, 23?24 June 2011. c?2011 Association for Computational Linguistics
Learning with Lookahead:
Can History-Based Models Rival Globally Optimized Models?
Yoshimasa Tsuruoka?? Yusuke Miyao?? Jun?ichi Kazama?
? Japan Advanced Institute of Science and Technology (JAIST), Japan
? National Institute of Informatics (NII), Japan
? National Institute of Information and Communications Technology (NICT), Japan
tsuruoka@jaist.ac.jp yusuke@nii.ac.jp kazama@nict.go.jp
Abstract
This paper shows that the performance of
history-based models can be significantly im-
proved by performing lookahead in the state
space when making each classification deci-
sion. Instead of simply using the best ac-
tion output by the classifier, we determine
the best action by looking into possible se-
quences of future actions and evaluating the
final states realized by those action sequences.
We present a perceptron-based parameter op-
timization method for this learning frame-
work and show its convergence properties.
The proposed framework is evaluated on part-
of-speech tagging, chunking, named entity
recognition and dependency parsing, using
standard data sets and features. Experimental
results demonstrate that history-based models
with lookahead are as competitive as globally
optimized models including conditional ran-
dom fields (CRFs) and structured perceptrons.
1 Introduction
History-based models have been a popular ap-
proach in a variety of natural language process-
ing (NLP) tasks including part-of-speech (POS) tag-
ging, named entity recognition, and syntactic pars-
ing (Ratnaparkhi, 1996; McCallum et al, 2000; Ya-
mada and Matsumoto, 2003; Nivre et al, 2004).
The idea is to decompose the complex structured
prediction problem into a series of simple classifi-
cation problems and use a machine learning-based
classifier to make each decision using the informa-
tion about the past decisions and partially completed
structures as features.
Although history-based models have many prac-
tical merits, their accuracy is often surpassed by
globally optimized models such as CRFs (Lafferty
et al, 2001) and structured perceptrons (Collins,
2002), mainly due to the label bias problem. To-
day, vanilla history-based models such as maximum
entropy Markov models (MEMMs) are probably not
the first choice for those who are looking for a ma-
chine learning model that can deliver the state-of-
the-art accuracy for their NLP task. Globally opti-
mized models, by contrast, are gaining popularity in
the community despite their relatively high compu-
tational cost.
In this paper, we argue that history-based mod-
els are not something that should be left behind
in research history, by demonstrating that their ac-
curacy can be significantly improved by incorpo-
rating a lookahead mechanism into their decision-
making process. It should be emphasized that we
use the word ?lookahead? differently from some lit-
erature on syntactic parsing in which lookahead sim-
ply means looking at the succeeding words to choose
the right parsing actions. In this paper, we use the
word to refer to the process of choosing the best ac-
tion by considering different sequences of future ac-
tions and evaluating the structures realized by those
sequences. In other words, we introduce a looka-
head mechanism that performs a search in the space
of future actions.
We present a perceptron-based training algorithm
that can work with the lookahead process, together
with a proof of convergence. The algorithm enables
us to tune the weight of the perceptron in such a way
that we can correctly choose the right action for the
238
State Operation Stack Queue
0 I saw a dog with eyebrows
1 shift I saw a dog with eyebrows
2 shift I saw a dog with eyebrows
3 reduceL saw(I) a dog with eyebrows
. . .
4 saw(I) dog(a) with eyebrows
5 shift saw(I) dog(a) with eyebrows
6 shift saw(I) dog(a) with eyebrows
7 reduceR saw(I) dog(a) with(eyebrows)
8 reduceR saw(I) dog(a, with(eyebrows))
5? reduceR saw(I, dog(a)) with eyebrows
6? shift saw(I, dog(a)) with eyebrows
7? shift saw(I, dog(a)) with eyebrows
8? reduce R saw(I, dog(a)) with(eyebrows)
9? reduce R saw(I, dog(a), with(eyebrows))
Figure 1: Shift-reduce dependency parsing
current state at each decision point, given the infor-
mation obtained from a search.
To answer the question of whether the history-
based models enhanced with lookahead can actually
compete with globally optimized models, we eval-
uate the proposed framework with a range of stan-
dard NLP tasks, namely, POS tagging, text chunking
(a.k.a. shallow parsing), named entity recognition,
and dependency parsing.
This paper is organized as follows. Section 2
presents the idea of lookahead with a motivating
example from dependency parsing. Section 3 de-
scribes our search algorithm for lookahead and a
perceptron-based training algorithm. Experimen-
tal results on POS tagging, chunking, named entity
recognition, and dependency parsing are presented
in Section 4. We discuss relationships between our
approach and some related work in Section 5. Sec-
tion 6 offers concluding remarks with some potential
research directions.
2 Motivation
This section describes an example of dependency
parsing that motivates the introduction of lookahead
in history-based models.
A well-known history-based approach to depen-
dency parsing is shift-reduce parsing. This al-
gorithm maintains two data structures, stack and
queue: A stack stores intermediate parsing results,
and a queue stores words to read. Two operations
(actions), shift and reduce, on these data structures
construct dependency relations one by one.
For example, assume that we are given the follow-
ing sentence.
I saw a dog with eyebrows.
In the beginning, we have an empty stack, and a
queue filled with a list of input words (State 0 in Fig-
ure 1). The shift operation moves the left-most ele-
ment of the queue to the stack. In this example, State
1 is obtained by applying shift to State 0. After the
two shift operations, we reach State 2, in which the
stack has two elements. When we have two or more
elements in the stack, we can apply the other opera-
tion, reduce, which merges the two stack elements
by creating a dependency relation between them.
When we apply reduceL, which means to have the
left element as a dependent of the right element, we
reach State 3: The word ?I? has disappeared from
the stack and instead it is attached to its head word
?saw?.1 In this way, the shift-reduce parsing con-
structs a dependency tree by reading words from the
queue and constructing dependency relations on the
stack.
1In Figure 1, H(D1, D2, . . .) indicates that D1, D2, . . . are
the dependents of the head H .
239
Let?s say we have now arrived at State 4 after sev-
eral operations. At this state, we cannot simply de-
termine whether we should shift or reduce. In such
cases, conventional methods rely on a multi-class
classifier to determine the next operation. That is,
a classifier is used to select the most plausible oper-
ation, by referring to the features about the current
state, such as surface forms and POSs of words in
the stack and the queue.
In the lookahead strategy, we make this decision
by referring to future states. For example, if we ap-
ply shift to State 4, we will reach State 8 in the end,
which indicates that ?with? attaches to ?dog?. The
other way, i.e., applying reduceR to State 4, eventu-
ally arrives at State 9?, indicating ?with? attaches to
?saw?. These future states indicate that we were im-
plicitly resolving PP-attachment ambiguity at State
4. While conventional methods attempt to resolve
such ambiguity using surrounding features at State
4, the lookahead approach resolves the same ambi-
guity by referring to the future states, for example,
State 8 and 9?. Because future states can provide ad-
ditional and valuable information for ambiguity res-
olution, improved accuracy is expected.
It should be noted that Figure 1 only shows one
sequence of operations for each choice of operation
at State 4. In general, however, the number of poten-
tial sequences grows exponentially with the looka-
head depth, so the lookahead approach requires us to
pay the price as the increase of computational cost.
The primary goal of this paper is to demonstrate that
the cost is actually worth it.
3 Learning with Lookahead
This section presents our framework for incorporat-
ing lookahead in history-based models. In this pa-
per, we focus on deterministic history-based models
although our method could be generalized to non-
deterministic cases.
We use the word ?state? to refer to a partially
completed analysis as well as the collection of his-
torical information available at each decision point
in deterministic history-based analysis. State transi-
tions are made by ?actions? that are defined at each
state. In the example of dependency parsing pre-
sented in Section 2, a state contains all the infor-
mation about past operations, stacks, and queues as
1: Input
2: d: remaining depth of search
3: S0: current state
4: Output
5: S: state of highest score
6: v: highest score
7:
8: function SEARCH(d, S0)
9: if d = 0 then
10: return (S0,w ? ?(S0))
11: (S, v)? (null,??)
12: for each a ? POSSIBLEACTIONS(S0)
13: S1 ? UPDATESTATE(S0, a)
14: (S?, v?)? SEARCH(d? 1, S1)
15: if v? > v then
16: (S, v)? (S?, v?)
17: return (S, v)
Figure 2: Search algorithm.
well as the observation (i.e. the words in the sen-
tence). The possible actions are shift, reduceR, and
reduceL. In the case of POS tagging, for example, a
state is the words and the POS tags assigned to the
words on the left side of the current target word (if
the tagging is conducted in the left-to-right manner),
and the possible actions are simply defined by the
POS tags in the annotation tag set.
3.1 Search
With lookahead, we choose the best action at each
decision point by considering possible sequences of
future actions and the states realized by those se-
quences. In other words, we need to perform a
search for each possible action.
Figure 2 describes our search algorithm in pseudo
code. The algorithm performs a depth-first search to
find the state of the highest score among the states in
its search space, which is determined by the search
depth d. This search process is implemented with
a recursive function, which receives the remaining
search depth and the current state as its input and
returns the state of the highest score together with
its score.
We assume a linear scoring model, i.e., the score
of each state S can be computed by taking the dot
product of the current weight vector w and ?(S),
the feature vector representation of the state. The
240
1: Input
2: C: perceptron margin
3: D: depth of lookahead search
4: S0: current state
5: ac: correct action
6:
7: procedure UPDATEWEIGHT(C,D, S0, ac)
8: (a?, S?, v)? (null, null,??)
9: for each a ? POSSIBLEACTIONS(S0)
10: S1 ? UPDATESTATE(S0, a)
11: (S?, v?)?SEARCH(D,S1)
12: if a = ac then
13: v? ? v? ? C
14: S?c ? S?
15: if v? > v then
16: (a?, S?, v)? (a, S?, v?)
17: if a? 6= ac then
18: w ? w + ?(S?c )? ?(S?)
Figure 3: Perceptron weight update
scores are computed at each leaf node of the search
tree and backed up to the root.2
Clearly, the time complexity of determinis-
tic tagging/parsing with this search algorithm is
O(nmD+1), where n is the number of actions
needed to process the sentence, m is the (average)
number of possible actions at each state, and D is
the search depth. It should be noted that the time
complexity of k-th order CRFs is O(nmk+1), so
a history-based model with k-depth lookahead is
comparable to k-th order CRFs in terms of train-
ing/testing time.
Unlike CRFs, our framework does not require the
locality of features since it is history-based, i.e., the
decisions can be conditioned on arbitrary features.
One interpretation of our learning framework is that
it trades off the global optimality of the learned pa-
rameters against the flexibility of features.
3.2 Training a margin perceptron
We adapt a learning algorithm for margin percep-
trons (Krauth and Mezard, 1987) to our purpose of
2In actual implementation, it is not efficient to compute the
score of a state from scratch at each leaf node. For most of
the standard features used in tagging and parsing, it is usually
straight-forward to compute the scores incrementally every time
the state is updated with an action.
optimizing the weight parameters for the lookahead
search. Like other large margin approaches such
as support vector machines, margin perceptrons are
known to produce accurate models compared to per-
ceptrons without a margin (Li et al, 2002).
Figure 3 shows our learning algorithm in pseudo
code. The algorithm is very similar to the standard
training algorithm for margin perceptrons, i.e., we
update the weight parameters with the difference of
two feature vectors (one corresponding to the cor-
rect action, and the other the action of the highest
score) when the perceptron makes a mistake. The
feature vector for the second best action is also used
when the margin is not large enough. Notice that the
feature vector for the second best action is automat-
ically selected by using a simple trick of subtracting
the margin parameter from the score for the correct
action (Line 13 in Figure 3).
The only difference between our algorithm and
the standard algorithm for margin perceptrons is that
we use the states and their scores obtained from
lookahead searches (Line 11 in Figure 3), which are
backed up from the leaves of the search trees. In Ap-
pendix A, we provide a proof of the convergence of
our training algorithm and show that the margin will
approach at least half the true margin (assuming that
the training data are linearly separable).
As in many studies using perceptrons, we average
the weight vector over the whole training iterations
at the end of the training (Collins, 2002).
4 Experiments
This section presents four sets of experimental re-
sults to show how the lookahead process improves
the accuracy of history-based models in common
NLP tasks.
4.1 Sequence prediction tasks
First, we evaluate our framework with three se-
quence prediction tasks: POS tagging, chunking,
and named entity recognition. We compare our
method with the CRF model, which is one of the de
facto standard machine learning models for such se-
quence prediction tasks. We trained L1-regularized
first-order CRF models using the efficient stochastic
gradient descent (SGD)-based training method pre-
sented in Tsuruoka et al (2009). Since our main in-
241
terest is not in achieving the state-of-the-art results
for those tasks, we did not conduct feature engineer-
ing to come up with elaborate features?we sim-
ply adopted the feature sets described in their paper
(with an exception being tag trigram features tested
in the POS tagging experiments). The experiments
for these sequence prediction tasks were carried out
using one core of a 3.33GHz Intel Xeon W5590 pro-
cessor.
The first set of experiments is about POS tagging.
The training and test data were created from theWall
Street Journal corpus of the Penn Treebank (Marcus
et al, 1994). Sections 0-18 were used as the training
data. Sections 19-21 were used for tuning the meta
parameters for learning (the number of iterations and
the margin C). Sections 22-24 were used for the
final accuracy reports.
The experimental results are shown in Table 1.
Note that the models in the top four rows use exactly
the same feature set. It is clearly seen that the looka-
head improves tagging accuracy, and our history-
based models with lookahead is as accurate as the
CRF model. We also created another set of models
by simply adding tag trigram features, which can-
not be employed by first-order CRF models. These
features have slightly improved the tagging accu-
racy, and the final accuracy achieved by a search
depth of 3 was comparable to some of the best re-
sults achieved by pure supervised learning in this
task (Shen et al, 2007; Lavergne et al, 2010).
The second set of experiments is about chunking.
We used the data set for the CoNLL 2000 shared
task, which contains 8,936 sentences where each to-
ken is annotated with the ?IOB? tags representing
text chunks. The experimental results are shown
in Table 2. Again, our history-based models with
lookahead were slightly more accurate than the CRF
model using exactly the same set of features. The
accuracy achieved by the lookahead model with a
search depth of 2 was comparable to the accuracy
achieved by a computationally heavy combination
of max-margin classifiers (Kudo and Matsumoto,
2001). We also tested the effectiveness of additional
features of tag trigrams using the development data,
but there was no improvement in the accuracy.
The third set of experiments is about named en-
tity recognition. We used the data provided for
the BioNLP/NLPBA 2004 shared task (Kim et al,
2004), which contains 18,546 sentences where each
token is annotated with the ?IOB? tags representing
biomedical named entities. We performed the tag-
ging in the right-to-left fashion because it is known
that backward tagging is more accurate than forward
tagging on this data set (Yoshida and Tsujii, 2007).
Table 3 shows the experimental results, together
with some previous performance reports achieved
by pure machine leaning methods (i.e. without rule-
based post processing or external resources such as
gazetteers). Our history-based model with no looka-
head was considerably worse than the CRF model
using the same set of features, but it was signifi-
cantly improved by the introduction of lookahead
and resulted in accuracy figures better than that of
the CRF model.
4.2 Dependency parsing
We also evaluate our method in dependency parsing.
We follow the most standard experimental setting
for English dependency parsing: The Wall Street
Journal portion of Penn Treebank is converted to de-
pendency trees by using the head rules of Yamada
and Matsumoto (2003).3 The data is split into train-
ing (section 02-21), development (section 22), and
test (section 23) sets. The parsing accuracy was eval-
uated with auto-POS data, i.e., we used our looka-
head POS tagger (depth = 2) presented in the previ-
ous subsection to assign the POS tags for the devel-
opment and test data. Unlabeled attachment scores
for all words excluding punctuations are reported.
The development set is used for tuning the meta pa-
rameters, while the test set is used for evaluating the
final accuracy.
The parsing algorithm is the ?arc-standard?
method (Nivre, 2004), which is briefly described in
Section 2. With this algorithm, state S corresponds
to a parser configuration, i.e., the stack and the
queue, and action a corresponds to shift, reduceL,
and reduceR. In this experiment, we use the same
set of feature templates as Huang and Sagae (2010).
Table 4 shows training time, test time, and parsing
accuracy. In this table, ?No lookahead (depth = 0)?
corresponds to a conventional shift-reduce parsing
method without any lookahead search. The results
3Penn2Malt is applied for this conversion, while depen-
dency labels are removed.
242
Training Time (sec) Test Time (sec) Accuracy
CRF (L1 regularization & SGD training) 847 3 97.11 %
No lookahead (depth = 0) 85 5 97.00 %
Lookahead (depth = 1) 294 9 97.19 %
Lookahead (depth = 2) 8,688 173 97.19 %
No lookahead (depth = 0) + tag trigram features 88 5 97.11 %
Lookahead (depth = 1) + tag trigram features 313 10 97.22 %
Lookahead (depth = 2) + tag trigram features 10,034 209 97.28 %
Structured perceptron (Collins, 2002) n/a n/a 97.11 %
Guided learning (Shen et al, 2007) n/a n/a 97.33 %
CRF with 4 billion features (Lavergne et al, 2010) n/a n/a 97.22 %
Table 1: Performance of English POS tagging (training times and accuracy scores on test data)
Training time (sec) Test time (sec) F-measure
CRF (L1 regularization & SGD training) 74 1 93.66
No lookahead (depth = 0) 22 1 93.53
Lookahead (depth = 1) 73 1 93.77
Lookahead (depth = 2) 1,113 9 93.81
Voting of 8 SVMs (Kudo and Matsumoto, 2001) n/a n/a 93.91
Table 2: Performance of text chunking (training times and accuracy scores on test data).
clearly demonstrate that the lookahead search boosts
parsing accuracy. As expected, training and test
speed decreases, almost by a factor of three, which
is the branching factor of the dependency parser.
The table also lists accuracy figures reported in
the literature on shift-reduce dependency parsing.
Most of the latest studies on shift-reduce depen-
dency parsing employ dynamic programing or beam
search, which implies that deterministic methods
were not as competitive as those methods. It should
also be noted that all of the listed studies learn struc-
tured perceptrons (Collins and Roark, 2004), while
our parser learns locally optimized perceptrons. In
this table, our parser without lookahead search (i.e.
depth = 0) resulted in significantly lower accuracy
than the previous studies. In fact, it is worse than the
deterministic parser of Huang et al (2009), which
uses (almost) the same set of features. This is pre-
sumably due to the difference between locally opti-
mized perceptrons and globally optimized structured
perceptrons. However, our parser with lookahead
search is significantly better than their determinis-
tic parser, and its accuracy is close to the levels of
the parsers with beam search.
5 Discussion
The reason why we introduced a lookahead mech-
anism into history-based models is that we wanted
the model to be able to avoid making such mistakes
that can be detected only in later stages. Probabilis-
tic history-based models such as MEMMs should be
able to avoid (at least some of) such mistakes by per-
forming a Viterbi search to find the highest proba-
bility path of the actions. However, as pointed out
by Lafferty et al (2001), the per-state normaliza-
tion of probabilities makes it difficult to give enough
penalty to such incorrect sequences of actions, and
that is primarily why MEMMs are outperformed by
CRFs.
Perhaps the most relevant to our work in terms
of learning is the general framework for search and
learning problems in history-based models proposed
by Daume? III and Marcu (2005). This framework,
called LaSO (Learning as Search Optimization), can
include many variations of search strategies such as
beam search and A* search as a special case. In-
deed, our lookahead framework could be regarded
as a special case in which each search node con-
243
Training time (sec) Test time (sec) F-measure
CRF (L1 regularization & SGD training) 235 4 71.63
No lookahead (depth = 0) 66 4 70.17
Lookahead (depth = 1) 91 4 72.28
Lookahead (depth = 2) 302 7 72.00
Lookahead (depth = 3) 2,419 33 72.21
Semi-Markov CRF (Okanohara et al, 2006) n/a n/a 71.48
Reranking (Yoshida and Tsujii, 2007) n/a n/a 72.65
Table 3: Performance of biomedical named entity recognition (training times and accuracy scores on test data).
Training time (sec) Test time (sec) Accuracy
No lookahead (depth = 0) 1,937 4 89.73
Lookahead (depth = 1) 4,907 13 91.00
Lookahead (depth = 2) 12,800 31 91.10
Lookahead (depth = 3) 31,684 79 91.24
Beam search (k = 64) (Zhang and Clark, 2008) n/a n/a 91.4
Deterministic (Huang et al, 2009) n/a n/a 90.2
Beam search (k = 16) (Huang et al, 2009) n/a n/a 91.3
Dynamic programming (Huang and Sagae, 2010) n/a n/a 92.1
Table 4: Performance of English dependency parsing (training times and accuracy scores on test data).
sists of the next and lookahead actions4, although
the weight updating procedure differs in several mi-
nor points. Daume? III and Marcu (2005) did not try
a lookahead search strategy, and to the best of our
knowledge, this paper is the first that demonstrates
that lookahead actually works well for various NLP
tasks.
Performing lookahead is a very common tech-
nique for a variety of decision-making problems
in the field of artificial intelligence. In computer
chess, for example, programs usually need to per-
form a very deep search in the game tree to find a
good move. Our decision-making problem is sim-
ilar to that of computer Chess in many ways, al-
though chess programs perform min-max searches
rather than the ?max? searches performed in our al-
gorithm. Automatic learning of evaluation functions
for chess programs can be seen as the training of
a machine learning model. In particular, our learn-
ing algorithm is similar to the supervised approach
4In addition, the size of the search queue is always truncated
to one for the deterministic decisions presented in this paper.
Note, however, that our lookahead framework can also be com-
bined with other search strategies such as beam search. In that
case, the search queue is not necessarily truncated.
(Tesauro, 2001; Hoki, 2006) in that the parameters
are optimized based on the differences of the feature
vectors realized by the correct and incorrect actions.
In history-based models, the order of actions is of-
ten very important. For example, backward tagging
is considerably more accurate than forward tagging
in biomedical named entity recognition. Our looka-
head method is orthogonal to more elaborate tech-
niques for determining the order of actions such as
easy-first tagging/parsing strategies (Tsuruoka and
Tsujii, 2005; Elhadad, 2010). We expect that incor-
porating such elaborate techniques in our framework
will lead to improved accuracy, but we leave it for
future work.
6 Conclusion
We have presented a simple and general framework
for incorporating a lookahead process in history-
based models and a perceptron-based training algo-
rithm for the framework. We have conducted ex-
periments using standard data sets for POS tagging,
chunking, named entity recognition and dependency
parsing, and obtained very promising results?the
accuracy achieved by the history-based models en-
244
hanced with lookahead was as competitive as glob-
ally optimized models including CRFs.
In most of the experimental results, steady im-
provement in accuracy has been observed as the
depth of the search is increased. Although it is
not very practical to perform deeper searches with
our current implementation?we naively explored
all possible sequences of actions, future work should
encompass extending the depths of search space
by introducing elaborate pruning/search extension
techniques.
In this work, we did not conduct extensive feature
engineering for improving the accuracy of individ-
ual tasks because our primary goal with this paper is
to present the learning framework itself. However,
one of the major merits of using history-based mod-
els is that we are allowed to define arbitrary features
on the partially completed structure. Another inter-
esting direction of future work is to see how much
we could improve the accuracy by performing ex-
tensive feature engineering in this particular learning
framework.
Appendix A: Convergence of the Learning
Procedure
Let {xi, aic}Ki=1 be the training examples where aic
is the correct first action for decision point xi, and
let Si be the set of all the states at the leaves of
the search trees for xi generated by the lookahead
searches and Sic be the set of all the states at the
leaves of the search tree for the correct action aic.
We also define Si = Si \ Sic. We write the weight
vector before the k-th update as wk. We define
S?c = argmax
S?Sic
w??(S) and S? = argmax
S?Si
w??(S)5.
Then the update rule can be interpreted as wk+1 =
wk +(?(S?c )??(S?)). Note that this update is per-
formed only when ?(Sc) ?wk?C < ?(S?) ?wk for
all Sc ? Sc since otherwise S? in the learning algo-
rithm cannot be a state with an incorrect first action.
In other words, ?(Sc) ?w ? ?(S?) ?w ? C for all
Sc ? Sc after convergence.
Given these definitions, we prove the convergence
for the separable case. That is, we assume the exis-
tence of a weight vector u (with ||u|| = 1), ? (> 0),
5S?c and S? depend on the weight vector at each point, but
we omit it from the notation for brevity.
and R (> 0) that satisfy:
?i,?Sc ? Sic,?S ? Si ?(Sc) ? u? ?(S) ? u ? ?,
?i,?Sc ? Sic,?S ? Si ||?(Sc)? ?(S)|| ? R.
The proof is basically an adaptation of the proofs
in Collins (2002) and Li et al (2002). First, we ob-
tain the following relation:
wk+1 ? u = wk ? u+ (?(S?c ) ? u? ?(S?) ? u)
= wk ? u+ ? ? w1 ? u+ k? = k?.
Therefore, ||wk+1 ? u||2 = ||wk+1||2 ? (k?)2 ?
(1). We assumed w1 = 0 but this is not an essential
assumption.
Next, we also obtain:
||wk+1||2 ? ||wk||2 + 2(?(S?c )? ?(S?)) ?wk
+||?(S?c )? ?(S?)||2
? ||wk||2 + 2C +R2
? ||w1||2 + k(R2 + 2C) = k(R2 + 2C)? (2)
Combining (1) and (2), we obtain k ? (R2 +
2C)/?2. That is, the number of updates is bounded
from above, meaning that the learning procedure
converges after a finite number of updates. Substi-
tuting this into (2) gives ||wk+1|| ? (R2 + 2C)/?
? (3).
Finally, we analyze the margin achieved by the
learning procedure after convergence. The margin,
?(w), is defined as follows in this case.
?(w) = min
xi
min
Sc?Sic,S?Si
?(Sc) ?w ? ?(S) ?w
||w||
= min
xi
min
Sc?Sic
?(Sc) ?w ? ?(S?) ?w
||w||
After convergence (i.e., w = wk+1), ?(Sc) ? w ?
?(S?)?w ? C for all Sc ? Sc as we noted. Together
with (3), we obtain the following bound:
?(w) ? min
xi
?C
2C +R2
= ?C
2C +R2
=
(
?
2
)(
1? R
2
2C +R2
)
As can be seen, the margin approaches at least half
the true margin, ?/2 as C ? ? (at the cost of infi-
nite number of updates).
245
References
Michael Collins and Brian Roark. 2004. Incremental
parsing with the perceptron algorithm. In Proceedings
of ACL, pages 111?118.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings of
EMNLP, pages 1?8.
Hal Daume? III and Daniel Marcu. 2005. Learning as
search optimization: Approximate large margin meth-
ods for structured prediction. In Proceedings of ICML,
pages 169?176.
Yoav Goldbergand Michael Elhadad. 2010. An effi-
cient algorithm for easy-first non-directional depen-
dency parsing. In Proceedings of NAACL-HLT, pages
742?750.
Kunihito Hoki. 2006. Optimal control of minimax
search results to learn positional evaluation. In Pro-
ceedings of the 11th Game Programming Workshop
(GPW), pages 78?83 (in Japanese).
Liang Huang and Kenji Sagae. 2010. Dynamic program-
ming for linear-time incremental parsing. In Proceed-
ings of ACL, pages 1077?1086.
Liang Huang, Wenbin Jiang, and Qun Liu. 2009.
Bilingually-constrained (monolingual) shift-reduce
parsing. In Proceedings of EMNLP, pages 1222?1231.
J.-D. Kim, T. Ohta, Y. Tsuruoka, Y. Tateisi, and N. Col-
lier. 2004. Introduction to the bio-entity recogni-
tion task at JNLPBA. In Proceedings of the Interna-
tional Joint Workshop on Natural Language Process-
ing in Biomedicine and its Applications (JNLPBA),
pages 70?75.
W Krauth and M Mezard. 1987. Learning algorithms
with optimal stability in neural networks. Journal of
Phisics A, 20(11):L745?L752.
Taku Kudo and Yuji Matsumoto. 2001. Chunking with
support vector machines. In Proceedings of NAACL.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic mod-
els for segmenting and labeling sequence data. In Pro-
ceedings of ICML, pages 282?289.
Thomas Lavergne, Olivier Cappe?, and Franc?ois Yvon.
2010. Practical very large scale CRFs. In Proceed-
ings of ACL, pages 504?513.
Yaoyong Li, Hugo Zaragoza, Ralf Herbrich, John Shawe-
Taylor, and Jaz S. Kandola. 2002. The perceptron
algorithm with uneven margins. In Proceedings of
ICML, pages 379?386.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1994. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19(2):313?330.
Andrew McCallum, Dayne Freitag, and Fernando
Pereira. 2000. Maximum entropy markov models for
information extraction and segmentation. In Proceed-
ings of ICML, pages 591?598.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2004.
Memory-based dependency parsing. In Proceedings
of CoNLL, pages 49?56.
Joakim Nivre. 2004. Incrementality in deterministic de-
pendency parsing. In ACL 2004 Workshop on Incre-
mental Parsing: Bringing Engineering and Cognition
Together, pages 50?57.
Daisuke Okanohara, Yusuke Miyao, Yoshimasa Tsu-
ruoka, and Jun?ichi Tsujii. 2006. Improving the scal-
ability of semi-markov conditional random fields for
named entity recognition. In Proceedings of COL-
ING/ACL, pages 465?472.
Adwait Ratnaparkhi. 1996. A maximum entropy model
for part-of-speech tagging. In Proceedings of EMNLP
1996, pages 133?142.
Libin Shen, Giorgio Satta, and Aravind Joshi. 2007.
Guided learning for bidirectional sequence classifica-
tion. In Proceedings of ACL, pages 760?767.
Gerald Tesauro, 2001. Comparison training of chess
evaluation functions, pages 117?130. Nova Science
Publishers, Inc.
Yoshimasa Tsuruoka and Jun?ichi Tsujii. 2005. Bidirec-
tional inference with the easiest-first strategy for tag-
ging sequence data. In Proceedings of HLT/EMNLP
2005, pages 467?474.
Yoshimasa Tsuruoka, Jun?ichi Tsujii, and Sophia Ana-
niadou. 2009. Stochastic gradient descent training
for l1-regularized log-linear models with cumulative
penalty. In Proceedings of ACL-IJCNLP, pages 477?
485.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statistical
dependency analysis with support vector machines. In
Proceedings of IWPT, pages 195?206.
Kazuhiro Yoshida and Jun?ichi Tsujii. 2007. Reranking
for biomedical named-entity recognition. In Proceed-
ings of ACL Workshop on BioNLP, pages 209?216.
Yue Zhang and Stephen Clark. 2008. A tale of
two parsers: investigating and combining graphbased
and transition-based dependency parsing using beam-
search. In Proceedings of EMNLP, pages 562?571.
246
Proceedings of BioNLP Shared Task 2011 Workshop, pages 94?101,
Portland, Oregon, USA, 24 June, 2011. c?2011 Association for Computational Linguistics
Extracting Bacteria Biotopes with Semi-supervised Named Entity
Recognition and Coreference Resolution
Nhung T. H. Nguyen and Yoshimasa Tsuruoka
School of Information Science
Japan Advanced Institute of Science and Technology
1-1 Asahidai, Nomi, Ishikawa 923-1292 Japan
{nthnhung,tsuruoka}@jaist.ac.jp
Abstract
This paper describes our event extraction sys-
tem that participated in the bacteria biotopes
task in BioNLP Shared Task 2011. The sys-
tem performs semi-supervised named entity
recognition by leveraging additional informa-
tion derived from external resources including
a large amount of raw text. We also perform
coreference resolution to deal with events hav-
ing a large textual scope, which may span over
several sentences (or even paragraphs). To
create the training data for coreference resolu-
tion, we have manually annotated the corpus
with coreference links. The overall F-score of
event extraction was 33.2 at the official eval-
uation of the shared task, but it has been im-
proved to 33.8 thanks to the refinement made
after the submission deadline.
1 Introduction
In this paper, we present a machine learning-based
approach for bacteria biotopes extraction of the
BioNLP Shared Task 2011 (Bossy et al , 2011).
The task consists of extracting bacteria localization
events, namely, mentions of given species and the
place where it lives. Places related to bacteria lo-
calization events range from plant or animal hosts
for pathogenic or symbiotic bacteria to natural envi-
ronments like soil or water1. This task also targets
specific environments of interest such as medical en-
vironments (hospitals, surgery devices, etc.), pro-
cessed food (dairy) and geographical localizations.
1https://sites.google.com/site/bionlpst/
home/bacteria-biotopes
The task of extracting bacteria biotopes involves
two steps: Named Entity Recognition (NER) and
event detection. The current dominant approach to
NER problems is to use supervised machine learning
models such as Maximum Entropy Markov Models
(MEMMs), Support Vector Machines (SVMs) and
Conditional Random Fields (CRFs). These models
have been shown to work reasonably well when a
large amount of training data is available (Nadeau
and Sekine, 2007). However, because the anno-
tated corpus delivered for this particular subtask in
the shared task is very small (78 documents with
1754 sentences), we have decided to use a semi-
supervised learning method in our system. Our NER
module uses a CRF model with enhanced features
created from external resources. More specifically,
we use additional features created from the output
of HMM clustering performed on a large amount of
raw text, and word senses from WordNet for tag-
ging.
The target events in this shared task are divided
into two types. The first is Localization events
which relates a bacterium to the place where it lives.
The second is PartOf events which denotes an or-
gan that belongs to an organism. As in Bossy et
al. (2010), the largest possible scope of the men-
tion of a relation is the whole document, and thus
it may span over several sentences (or even para-
graphs). This observation motivated us to perform
coreference resolution as a pre-processing step, so
that each event can be recognized within a narrower
textual scope. There are two common approaches to
coreference resolution: one mainly relies on heuris-
tics, and the other employs machine learning. Some
94
instances of the heuristics-based approach are de-
scribed in (Harabagiu et al, 2001; Markert and
Nissim, 2005; Yang and Su, 2007), where they
use lexical and encyclopedic knowledge. Machine
learning-based methods (Soon and Ng, 2001; Ng
and Cardie, 2002; Yang et al , 2003; Luo et al
, 2004; Daume and Marcu, 2005) train a classi-
fier or search model using a corpus annotated with
anaphoric pairs. In our system, we employ the sim-
ple supervised method presented in Soon and Ng
(2001). To create the training data, we have man-
ually annotated the corpus with coreference infor-
mation about bacteria.
Our approach, consequently, has three processes:
NER, coreference resolution of bacterium entities,
and event extraction. The latter two processes can be
formulated as classification problems. Coreference
resolution is to determine the relation between can-
didate noun phrases and bacterium entities, and the
event extraction is to detect the relation between two
entities. It should be noted that our official submis-
sion in the shared task was carried out without using
a coreference resolution module, and the system has
been improved after the submission deadline.
Our contribution in this paper is two-fold. In the
methodology aspect, we use an unsupervised learn-
ing method to create additional features for the CRF
model and perform coreference resolution to narrow
the scope of events. In the resource aspect, the man-
ual annotations for training our coreference resolu-
tion module will be made available to the research
community.
The remainder of this paper is organized as fol-
lowed. Section 2, 3 and 4 describe details about the
implementation of our system. Section 5 presents
the experimental results with some error analysis.
Finally, we conclude our approach and discuss fu-
ture work in section 6.
2 Semi-supervised NER
According to the task description, the NER task
consists of detecting the phrases that denote bacte-
rial taxon names and localizations which are bro-
ken into eight types: Host, HostPart, Geographical,
Food, Water, Soil, Medical and Environment. In
this work, we use a CRF model to perform NER.
CFRs (Lafferty et. al., 2001) are a sequence model-
ing framework that not only has all the advantages
of MEMMs but also solves the label bias problem
in a principled way. This model is suitable for la-
beling sequence data, especially for NER. Based on
this model, our CRF tagger is trained with a stochas-
tic gradient descent-based method described in Tsu-
ruoka et al (2009), which can produce a compact
and accurate model.
Due to the small size of the training corpus and
the complexity of their category, the entities cannot
be easily recognized by standard supervised learn-
ing. Therefore, we enhance our learning model by
incorporating related information from other exter-
nal resources. On top of the lexical and syntactic
features, we use two additional types of information,
which are expected to alleviate the data sparseness
problem. In summary, we use four types of features
including lexical and syntactic features, word clus-
ter and word sense features as the input for the CRF
model.
2.1 Word cluster features
The idea of enhancing a supervised learning model
with word cluster information is not new. Kamaza
et. al. (2001) use a hidden Markov model (HMM)
to produce word cluster features for their maximum
entropy model for part-of-speech tagging. Koo et al
(2008) implement the Brown clustering algorithm
to produce additional features for their dependency
parser. For our NER task, we use an HMM to pro-
duce word cluster features for our CRF model.
We employed an open source library2 for learn-
ing HMMs with the online Expectation Maximiza-
tion (EM) algorithm proposed by Liang and Klein
(2009). The online EM algorithm is much more ef-
ficient than the standard batch EM algorithm and al-
lows us to use a large amount of data. For each hid-
den state, words that are produced by this state with
the highest probability are written. We use this result
of word clustering as a feature for NER. The optimal
number of hidden states is selected by evaluating its
effectiveness on NER using the development set.
To prepare the raw text for HMM clustering, we
downloaded 686 documents (consisting of both full
documents and abstracts) about bacteria biotopes
2http://www-tsujii.is.s.u-tokyo.ac.jp/
?hillbig/ohmm.htm
95
Figure 1: Sample of HMM clustering result.
from MicrobeWiki, JGI Genome Portal, Genoscope,
2Can bacteria pages at EBI and NCBI Genome
Project (the training corpus is also downloaded from
these five webpages). In addition, we use the
100,000 latest MEDLINE abstracts containing the
string ?bacteri? in our clustering. In total, the raw
text consists of more than 100,000 documents with
more than 2 million sentences.
A part of the result of HMM clustering is shown
in Figure 1. According to this result, the word ?Bi-
fidobacterium? belongs to cluster number 9, and its
feature value is ?Cluster-9?. The word cluster fea-
tures of the other words are extracted in the same
way.
2.2 Word sense features
We used WordNet to produce additional features on
word senses. Although WordNet3 is a large lexi-
cal database, it only comprises words in the general
genre, to which only the localization entities belong.
Since it does not contain the bacterial taxon names,
the most important entities in this task, we used an-
other dictionary for bacteria names. The dictionary
was extracted from the genomic BLAST page of
NCBI 4. To connect these two resources, we simply
place all entries from the NCBI dictionary under the
?bacterium? sense of WordNet. Table 1 illustrates
some word sense features employed in our model.
2.3 Pre-processing for bacteria names
In biomedical documents, the bacteria taxon names
are written in many forms. For example, they are
3http://wordnet.princeton.edu/
4http://www.ncbi.nlm.nih.gov/sutils/
genom_table.cgi
Word POS Sense
chromosome NN body
colonize VBP social
detected VBN perception
fly NN animal
gastrointestinal JJ pert
infant NN person
longum FW bacterium
maintaining VBG stative
milk NN food
onion NN plant
proterins NNS substance
USA NNP location
Table 1: Sample of word sense features given by Word-
Net and NCBI dictionary.
presented in a full name like ?Bacillius cereus?, or
in a short form such as ?B. cereus?, or even in an ab-
breviation as ?GSB? (green sulfur bacteria). More-
over, the bacteria names are often modified with
some common strings such as ?strain?, ?spp.?, ?sp.?,
etc. ?Borrelia hermsii strain DAH?, ?Bradyrhizo-
bium sp. BTAi1?, and ?Spirochaeta spp.? are ex-
amples of this kind. In order to tackle this prob-
lem, we apply a pre-processing step before NER. Al-
though there are many previous studies solving this
kind of problem, in our system, we apply a simple
method for this step.
? Retrieving the full form of bacteria names. We
assume that (a) both short form and full form
must occur in the same document; (b) a token
is considered as an abbreviation if it is writ-
ten in upper case and its length is shorter than
4 characters. When a token satisfies condition
(b) (which means it is an abbreviation), the pro-
cessing retrieves its full form by identifying all
sequences containing tokens initialized by its
abbreviated character. In case of short form
like ?B. cereus?, the selected sequence must in-
clude the right token (which is ?cereus? in ?B.
cereus?).
? Making some common strings transparent. As
our observation on the training data, there are
8 common strings in bacteria names, including
?strain?, ?str?, ?str.?, ?subsp?, ?spp.?, ?spp?,
?sp.?, ?sp?. All of these strings will be removed
before NER and recovered after that.
96
3 Coreference Resolution as Binary
Classification
Coreference resolution is the process of determin-
ing whether different nominal phrases are used to
refer to the same real world entity or concept. Our
approach basically follows the learning method de-
scribed in Soon and Ng (2001). In this approach,
we build a binary classifier using the coreferring en-
tities in the training corpus. The classifier takes a
pair of candidates and returns true if they refer to
the same real world entity and false otherwise. In
this paper, we limit our module to detecting the bac-
teria?s coreference, and hence the candidates consist
of noun phrases (NPs) (starting by a determiner),
pronouns, possessive adjective and name of bacte-
ria.
In addition to producing the candidates, the pre-
processing step creates a set of features for each
anaphoric pair. These features are used by the clas-
sifier to determine if two candidates have a corefer-
ence relation or not.
The following features are extracted from each
candidate pair.
? Pronoun: 1 if one of the candidates is a pro-
noun; 0 otherwise.
? Exact or Partial Match: 1 if the two strings of
the candidates are identical, 2 if they are partial
matching; 0 otherwise.
? Definite Noun Phrase: 1 if one of the candi-
dates is a definite noun phrases; 0 otherwise.
? Demonstrative Noun Phrase: 1 if one of the
candidates is a demonstrative noun phrase; 0
otherwise.
? Number Agreement: 1 if both candidates are
singular or plural; 0 otherwise.
? Proper Name: 1 if both candidates are bac-
terium entities or proper names; 0 otherwise.
? Character Distance: count the number of the
characters between two candidates.
? Possessive Adjective: 1 if one of the candidates
is possessive adjective; 0 otherwise.
Figure 2: Example of annotating coreference resolution.
T16 is a bacterium which is delivered in *.a2 file, T24
and T25 are anaphoric expressions. There are two coref-
erence relations of T16 and T24, T16 and T25.
? Exist in Coreference Dictionary: 1 if the candi-
date exists in the dictionary extracted from the
training data; 0 otherwise. This feature aims to
remove noun phrases which are unlikely to be
related to the bacterium entities.
The first five features are exactly the same as those
in Soon and Ng (2001), while the others are refined
or added to make it suitable for our specific task.
In the testing phase, we used the best-first
clustering as in Ng and Cardie (2002). Rather
than performing a right-to-left search from each
anaphoric NP for the first coreferent NP, a right-to-
left search for a highly likely antecedent was per-
formed. Hence, the classifier was modified to select
the antecedent of NP with the coreference likelihood
score above a threshold. This threshold was tuned by
evaluating it on the development set.
3.1 Corpus annotation
To create the training data for coreference resolu-
tion, we have manually annotated the corpus based
on the gold-standard named entity annotations deliv-
ered by the organizer. Due to our decision to focus
on bacteria names, only the coreference of these en-
tities are labeled. We use a format similar to those of
the organizer, i.e. the standoff presentation and text-
bound annotations. The coreference annotation file
consists of two parts, one part for anaphoric expres-
sions and the other for coreference relation. Figure 2
shows an example of a coreference annotation with
the original text.
97
4 Event Extraction
The bacteria biotopes, as mentioned earlier, are di-
vided into two types. The first type of events,
namely localization events, relates a bacterium to
the place where it lives, and has two mandatory ar-
guments: a Bacterium type and a localization type.
The second type of events, i.e. PartOf events, de-
note an organ that belongs to an organism, and has
two mandatory arguments of type HostPart and Host
respectively. We view this step as determining the
relationship between two specific entities. Because
of no ambiguity between the two types of event, the
event extraction can be solved as the binary classifi-
cation of pairs of entities. The classifier is trained on
the training data with four types of feature extracted
from the context between two entities: distance in
sentences, the number of entities, the nearest left and
right verbs.
Generating Training Examples. Given the
coreference information on bacterium entities, the
system considers all the entities belonging to the
coreference chains as real bacteria and generates
event instances. Since about 96% of all annotated
events occur in the same paragraph, we restrict our
method to detecting events within one paragraph.
? Localization Event. The system creates a rela-
tionship between a bacterium and a localization
entity with minimum distance between them
by the following priorities:
(1) The bacterium precedes the localization en-
tity in the same sentence.
(2) The bacterium precedes the localization en-
tity in the same paragraph.
? PartOf Event. All possible relationships be-
tween Host and HostPart entities are generated
if they are in the same paragraph.
5 Experiments and Discussion
The training and evaluation data used in these exper-
iments are provided by the shared task organizers.
The token and syntactic information are extracted
from the supporting resources (Stenetorp et. al. ,
2011). More detail, the tokenized text was done by
GENIA tools, and the syntactic analyses was cre-
ated by the McClosky-Charinak parser (McClosky
Experiment Acc. Pre. Re. F-score
Baseline 94.28 76.32 35.51 48.47
Word cluster 94.46 78.23 39.59 52.57
Word sense 94.63 74.15 44.49 55.61
All Features 94.70 77.62 45.31 57.22
Table 2: Performance of Named Entity Recognition in
terms of Accuracy, Precision, Recall and F-score with
different features on the development set.
and Charniak, 2008), trained on the GENIA Tree-
bank corpus (Tateisi et al, 2005), which is one of the
most accurate parsers for biomedical documents.
For both classification of anaphoric pairs in coref-
erence resolution and determining relationship of
two entites, we used the SVMlight library 5, a state-
of-the-art classifier, with the linear kernel.
In order to find the best parameters and features
for our final system, we conducted a series of exper-
iments at each step of the approach.
5.1 Named Entity Recognition
We evaluated the impact of additional featues on
NER by running four experiments. The Baseline ex-
periment was conducted by using the original CRF
tagger, which did not use any additional features de-
rived from external resources. The other three ex-
periments were conducted by incrementally adding
more features to the CRF tagger. Table 2 shows the
results on the development set6.
Through these experiments we have realized that
using the external resources is very effective. The
word cluster and word sense features are used like
a dictionary. The first one can be considered as the
dictionary of specific classes of entity in the same
domain with this task, which mainly supports the
precision, whereas the latter is a general dictionary
boosting the recall. With regard to F-score, the word
sense features outperform the word cluster features.
When we combine all of them, the F-score is im-
proved significantly by nearly 9 points.
The detailed results of individual classes in Ta-
ble 3 show that the Environment entities are the
hardest to recognize. Because of their general char-
acteristic, these entities are often confused with Host
5http://svmlight.joachims.org/
6These scores were generated by using the CoNLL 2000
evaluation script.
98
Class Gold Pre. Re. F-score
Bacterium 86 70.00 40.23 51.09
Host 78 78.57 56.41 65.67
HostPart 44 91.67 50.00 64.71
Geographical 8 71.43 62.50 66.67
Environment 8 0.00 0.00 0.00
Food 0 N/A N/A N/A
Medical 2 100.00 50.00 66.67
Water 17 100.00 17.65 30.00
Soil 1 100.00 100.00 100.00
All 244 77.62 45.31 57.22
Table 3: Results of NER using all features on the de-
velopment set. The ?Gold? column shows the number
of entities of that class in the gold-standard corpus. The
score of Food entities is not available because there is no
positive instance in the development set.
Detection Linking
Precision 24.18 20.48
Recall 91.36 33.71
F-score 38.24 25.48
Table 4: Result of coreference resolution on the develop-
ment set achieved with gold-standard named entity anno-
tations.
or Water. In contrast, the Geographical category is
easier than the others if we have gazetteers and ad-
ministrative name lists.
5.2 Coreference Resolution
We next evaluated the accuracy of coreference reso-
lution for bacterium entities. The evaluation7 is car-
ried out in two steps: evaluation of mention detec-
tion, and evaluation of mention linking to produce
coreference links. The exact matching criterion was
used when evaluating the accuracy of the two steps.
Table 4 shows the performance of the coreference
resolution module when taking annotated entites as
input. As mentioned in section 3, the first step of this
module considers all NPs beginning with a deter-
miner and bacterium entities as candidates. There-
fore, the number of the candidate NPs is vastly larger
than that of the positive ones. This is the reason
why the precision of mention detection is low, while
the recall is high. This high recall leads to a large
number of generated linkings and raises the com-
7http://sites.google.com/site/bionlpst/
home/protein-gene-coreference-task
Experiment Pre. Re. F-score
No Coref. 42.11 27.34 33.15
With Coref. 43.40 27.64 33.77
Table 5: Comparative results of event extraction with and
without coreference information on the test set.
Type of event
Num. of addition Num. of ruled out
True False True False
Localization 17 1 6 20
PartOf 6 5 1 0
Total 29 27
Table 6: Contribution of coreference resolution to event
extraction.
plexity of linking detection. In order to obtain more
accurate results, we had to remove weak linkings
whose classification score is under 0.7 (this is the
best threshold on the development set). However, as
shown in Table 4, the performance of mention link-
ing was not satisfactory.
5.3 Event Extraction
Finally, we carried out two experiments on the test
set to investigate the effect of coreference resolution
on event extraction. The results shown in Table 5 in-
dicate that the contribution of coreference resolution
in this particular experiment is not significant. The
coreference information helps the module to add 29
more events (23 true and 6 false events) and rule out
27 events (20 false and 7 true events) compared with
the experiment with no coreference resolution. De-
tail about this contribution is presented in Table 6.
We further analyzed the result of event extraction
and found that there exist two kinds of Localization
events, which we call direct and indirect events. The
direct events are the ones that are easily recogniz-
able on the surface level of textual expressions. The
three Localization events in Figure 3 belong to this
type. Our module is able to detect most of the di-
rect events, especially when we have the coreference
information on bacteria ? it is straight-forward be-
cause the two arguments of the event occur in the
same sentence. In constrast, the indirect events
are more complicated. They appear implicitly in the
document and we need to infer them through an in-
termediate agent. For example, a bacterium causes
a disease, and this disease infects the humans or an-
99
Figure 3: Example of direct events. The solid line is the
Localization event, the dash line is the PartOf event.
Figure 4: Example of indirect events. The solid line is
the Localization event, the arrow shows the causative re-
lation.
imals. Therefore, it can be considered that the bac-
terium locates in the humans or animals. Figure 4
illustrates this case. In this example, the Bacillus
anthracis causes Anthrax, Humans contract the dis-
ease (which refers to Anthrax), and the Bacillus an-
thracis locates in Humans. These events are very
difficult to recognize since, in this context, we do
not have any information about the disease. Events
of this type provide an interesting challenge for bac-
teria biotopes extraction.
6 Conclusion and Future Work
We have presented our machine learning-based ap-
proach for extracting bacteria biotopes. The system
is implemented with modules for three tasks: NER,
coreference resolution and event extraction.
For NER, we used a CRF tagger with four types
of features: lexical and syntactic features, the word
cluster and word sense extracted from the external
resources. Although we achieved a significant im-
provement by employing WordNet and the HMM
clustering on raw text, there is still much room for
improvement. For example, because all extracted
knowledge used in this NER module belongs to the
general knowlegde, its performance is not as good as
our expectation. We envisage that the performance
of the module will be improved if we can find useful
biological features.
We have attempted to use the information ob-
tained from the coreference resolution of bacteria to
narrow the event?s scope. On the test set, although it
does not improve the system significantly, the coref-
erence information has shown to be useful in event
extraction. 8
In this work, we simply used binary classifiers
with standard features for both coreference resolu-
tion and event detection. More advanced machine
learning approaches for structured prediction may
lead to better performance, but we leave it for future
work.
References
Robert Bossy, Claire Nedellec, and Julien Jourde. 2010.
Guidelines for Annotation of Bacteria Biotopes.
Robert Bossy, Julien Jourde, Philippe Bessie`res, Marteen
van de Guchte, and Claire Ne?dellec. 2011. BioNLP
Shared Task 2011 - Bacteria Biotope, In Proceedings
of the BioNLP 2011 Workshop Companion Volume for
Shared Task. Portland, Oregon, Association for Com-
putational Linguistics.
Hal Daume? III and Daniel Marcu. 2005. A Large-scale
Exploration of Effective Global Features for a Joint
Entity Detection and Tracking Model. In Proceedings
of HLT-EMNLP 2005, pp. 97-104.
Sanda M. Harabagiu, Razvan C. Bunescu and Steven J.
Maiorano. 2001. Text and Knowlegde Mining for Co-
reference Resolution. In Proceedings of NAACL 2001,
pp. 1-8.
Jun?ichi Kazama, Yusuke Miyao, and Jun?ichi Tsujii.
2001. A Maximum Entropy Tagger with Unsuper-
vised Hidden Markov Models. In Proceedings of NL-
PRS 2001, pp. 333-340.
Terry Koo, Xavier Carreras, and Michael Collins. 2008.
Simple Semi-supervised Dependency Parsing. In Pro-
ceedings of ACL-08: HLT, pp. 595-603.
John Lafferty, Andrew McCallum and Fernando Pereira.
2001. Conditional Random Fields: Probabilistic Mod-
els for Segmenting and Labeling Sequence Data. In
Proceedings of ICML?01, pp. 282-289.
Percy Liang and Dan Klein. 2009. Online EM for Unsu-
pervised Models. In Proceedings of NAACL 2009, pp.
611-619.
Xiaoqiang Luo, Abe Ittycheriah, Hongyan Jing, Nanda
Kambhatla and Salim Roukos. 2004. A
Mention-Synchronous Co-reference Resolution Algo-
rithm based on the Bell Tree. In Proceedings of ACL
2004, pp. 135-142.
Katja Markert and Malvina Nissim. 2005. Comparing
Knowledge Sources for Nominal Anaphora Resolu-
tion. In Computational Linguistics, Volume 31 Issue
3, pp. 367-402.
8If you are interesting in the annotated corpus used for our
coreference resolution model, please request us by email.
100
David McClosky and Eugene Charniak. 2008. Self-
Training for Biomedical Parsing. Proceedings of the
Association for Computational Linguistics (ACL 2008,
short papers), Columbus, Ohio, pp. 101-104.
David Nadeau and Satoshi Sekine. 2007. A survey of
named entity recognition and classification. Linguisti-
cae Investigationes, Volume 30(1), pp. 326.
Vincent Ng and Claire Cardie. 2002. Improving Ma-
chine Learning Approach to Co-reference Resolution.
In Proceedings of ACL 2002, pp. 104-111.
Wee Meng Soon and Hwee Tou Ng. 2001. A Ma-
chine Learning Approach to Co-reference Resolution
of Noun Phrases. Computational Linguistics 2001,
Volume 27 Issue 4, pp. 521-544.
Pontus Stenetorp, Goran Topic?, Sampo Pyysalo, Tomoko
Ohta, Jin-Dong Kim, and Jun?ichi Tsujii. 2011.
BioNLP Shared Task 2011: Supporting Resources.
InProceedings of the BioNLP 2011 Workshop Com-
panion Volume for Shared Task, Portland, Oregon, As-
sociation for Computational Linguistics.
Yuka Tateisi, Akane Yakushiji, Tomoko Ohta and Junichi
Tsujii. 2005. Syntax Annotation for the GENIA cor-
pus. In Proceedings of IJCNLP 2005 (Companion vol-
ume), pp. 222-227.
Yoshimasa Tsuruoka, Jun?ichi Tsujii, and Sophia Ana-
niadou. 2009. Stochastic Gradient Descent Training
for L1-regularized Log-linear Models with Cumulative
Penalty. In Proceedings of ACL-IJCNLP, pp. 477-485.
Xiaofeng Yang, Guodong Zhou, Jian Su and Chew Lim
Tan. 2003. Co-reference Resolution using Competi-
tion Learning Approach. In Proceedings of ACL 2003,
pp. 176-183.
Xiaofeng Yang and Jian Su. 2007. Coreference Reso-
lution Using Semantic Relatedness Information from
Automatically Discovered Patterns. In Proceedings of
ACL 2007, pp. 528-535.
101
Proceedings of TextGraphs-9: the workshop on Graph-based Methods for Natural Language Processing, pages 6?14,
October 29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Exploiting Timegraphs in Temporal Relation Classification
Natsuda Laokulrat
?
, Makoto Miwa
?
, and Yoshimasa Tsuruoka
?
?
The University of Tokyo, 3-7-1 Hongo, Bunkyo-ku, Tokyo, Japan
{natsuda,tsuruoka}@logos.t.u-tokyo.ac.jp
?
Toyota Technological Institute, 2-12-1 Hisakata, Tempaku-ku, Nagoya, Japan
miwa@toyota-ti.ac.jp
Abstract
Most of the recent work on machine
learning-based temporal relation classifi-
cation has been done by considering only
a given pair of temporal entities (events or
temporal expressions) at a time. Entities
that have temporal connections to the pair
of temporal entities under inspection are
not considered even though they provide
valuable clues to the prediction. In this
paper, we present a new approach for ex-
ploiting knowledge obtained from nearby
entities by making use of timegraphs and
applying the stacked learning method to
the temporal relation classification task.
By performing 10-fold cross validation
on the Timebank corpus, we achieved an
F1 score of 59.61% based on the graph-
based evaluation, which is 0.16 percent-
age points higher than that of the local
approach. Our system outperformed the
state-of-the-art system that utilizes global
information and achieved about 1.4 per-
centage points higher accuracy.
1 Introduction
Temporal relationships between entities, namely
temporal expressions and events, are regarded as
important information for deep understanding of
documents. Being able to predict temporal re-
lations between events and temporal expressions
within a piece of text can support various NLP ap-
plications such as textual entailment (Bos et al.,
2005), multi-document summarization (Bollegala
et al., 2010), and question answering (Ravichan-
dran and Hovy, 2002).
Temporal relation classification, which is one of
the subtasks TempEval-3 (UzZaman et al., 2013),
aims to classify temporal relationships between
pairs of temporal entities into one of the 14 re-
lation types according to the TimeML specifica-
tion (Pustejovsky et al., 2005), e.g., BEFORE, AF-
TER, DURING, and BEGINS.
The Timebank corpus introduced by Puste-
jovsky et al. (2003) has enabled the machine
learning-based classification of temporal relation-
ship. By learning from the annotated relation
types in the documents, it is possible to predict
the temporal relation of a given pair of temporal
entities (Mani et al., 2006).
However, most of the existing machine
learning-based systems use local information
alone, i.e., they consider only a given pair of tem-
poral entities at a time. Entities that have tem-
poral connections to the entities in the given pair
are not considered at all even though they provide
valuable clues to the prediction. Hence, the lo-
cal approach often produces contradictions. For
instance, the system may predict that A happens
before B, that B happens before C, and that A hap-
pens after C, which are mutually contradictory.
In order to tackle the contradiction problem,
global approaches have been proposed by Cham-
bers and Jurafsky (2008) and Yoshikawa et al.
(2009). Chamber and Jurafsky proposed a global
model based on Integer Linear Programming that
combines the output of local classifiers and max-
imizes the global confidence scores. While they
focused only on the temporal relations between
events, Yoshikawa et al. proposed a Markov Logic
model to jointly predict the temporal relations be-
tween events and time expressions.
In this paper, we propose an approach that
utilizes timegraphs (Miller and Schubert, 1999),
which represent temporal connectivity of all tem-
poral entities in each document, for the relation
classification. Our method differs from the pre-
vious work in that their methods used transition
rules to enforce consistency within each triplet of
relations, but our method can also work with a set
consisting of more than three relations. Moreover,
6
Figure 1: An example from the Timebank corpus
in our work, the full set of temporal relations spec-
ified in TimeML are used, rather than the reduced
set used in the previous work.
We evaluate our method on the TempEval-3?s
Task C-relation-only data, which provides a sys-
tem with all the appropriate temporal links and
only needs the system to classify the relation
types. The result shows that by exploiting the
timegraph features in the stacked learning ap-
proach, the classification performance improves
significantly. By performing 10-fold cross valida-
tion on the Timebank corpus, we can achieve an F1
score of 59.61% based on the graph-based evalu-
ation, which is 0.16 percentage points (pp) higher
than that of the local approach. We compared the
results of our system to those of Yoshikawa et al.
(2009) and achieved about 1.4 pp higher accuracy.
The remainder of the paper is organized as fol-
lows. Section 2 explains the temporal relation
classification task and the pairwise classifier. Sec-
tion 3 and Section 4 describe our proposed time-
graph features and the application to the stacked
learning approach. Section 5 shows the experi-
ment setup and presents the results. Finally, we
discuss the results in 6 and conclude with direc-
tions for future work in Section 7.
2 Temporal Relation Classification
According to TempEval-3, a temporal annotation
task consists of several subtasks, including tempo-
ral expression extraction (Task A), event extrac-
tion (Task B), and temporal link identification and
relation classification (Task C). Our work, as with
the previous work mentioned in Section 1, only
focuses on the relation classification task (Task C-
relation only). The system does not extract events
and temporal expressions automatically.
A pair of temporal entities, including events and
temporal expressions, that is annotated as a tem-
poral relation is called a TLINK. Temporal rela-
tion classification is a task to classify TLINKs into
temporal relation types.
Following TempEval-3, all possible TLINKs
are between:
? Event and Document Creation Time (DCT)
? Events in the same sentence
? Event and temporal expression in the same
sentence
? Events in consecutive sentences
2.1 The Timebank corpus
The Timebank corpus is a human-annotated cor-
pus commonly used in training and evaluating a
temporal relation classifier. It is annotated follow-
ing the TimeML specification to indicate events,
temporal expressions, and temporal relations. It
also provides five attributes, namely, class, tense,
aspect, modality, and polarity, associated with
each event (EVENT), and four attributes, namely,
type, value, functionInDocument, and temporal-
Function, associated with each temporal expres-
sion (TIMEX3). An example of the annotated event
and temporal expression is shown in Figure 1.
The sentence is brought from wsj 0292.tml in the
Timebank corpus.
There is no modal word in the sentence, so the
attribute modality does not appear.
We use the complete set of the TimeML rela-
tions, which has 14 types of temporal relations in-
cluding BEFORE, AFTER, IMMEDIATELY BEFORE, IM-
MEDIATELY AFTER, INCLUDES, IS INCLUDED, DUR-
ING, DURING INVERSE, SIMULTANEOUS, IDENTITY,
BEGINS, BEGUN BY, END, and ENDED BY. However,
in TempEval-3, SIMULTANEOUS and IDENTITY are
regarded as the same relation type, so we change
all IDENTITY relations into SIMULTANEOUS.
Given the example mentioned above, the tem-
poral relation is annotated as shown in the last line
of Figure 1. From the annotated relation, the event
rose (e30) happens DURING the temporal expres-
sion the first nine months (t88).
7
Feature E-E E-T Description
Event attributes
Class X X
All attributes associated with events. The ex-
planation of each attribute can be found in
(Pustejovsky et al., 2005).
Tense X X
Aspect X X
Modality X X
Polarity X X
Timex attributes
Type X
All attributes associated with temporal ex-
pressions. The explanation of each attribute
can be found in (Pustejovsky et al., 2005).
Value X
FunctionInDocument X
TemporalFunction X
Morphosyntactic information
Words X X
Words, POS, lemmas within a window be-
fore/after event words extracted using Stan-
ford coreNLP (Stanford NLP Group, 2012)
Part of speech tags X X
Lemmas X X
Lexical semantic information
Synonyms of event word tokens X X
WordNet lexical database (Fellbaum, 1998)
Synonyms of temporal expressions X
Event-Event information
Class match X
Details are described in (Chambers et al.,
2007)
Tense match X
Aspect match X
Class bigram X
Tense bigram X
Aspect bigram X
Same sentence X X True if both temporal entities are in the same
sentence
Deep syntactic information
Phrase structure X X Deep syntactic information extracted from
Enju Parser (Miyao and Tsujii, 2008). The
details are described in (Laokulrat et al.,
2013)
Predicate-argument structure X X
Table 1: Local features
Feature E-E E-T Description
Adjacent nodes and links X X
The details are described in Subsection 3.2
Other paths X X
Generalized paths X X
(E,V,E) tuples X X
(V,E,V) tuples X X
Table 2: Timegraph features
8
Figure 2: path length ? 2
Figure 3: path length ? 3
3 Proposed method
Rather than using only local information on
two entities in a TLINK, our goal is to exploit
more global information which can be extracted
from a document?s timegraph. Our motivation
is that temporal relations of nearby TLINKs in
a timegraph provide very useful information for
predicting the relation type of a given TLINK. For
instance, consider the following sentence and the
temporal connectivity shown in Figure 2.
About 500 people attended (e1) a Sunday
night memorial for the Buffalo-area physician
who performed abortions, one year (t1) after he
was killed (e2) by a sniper?s bullet.
It can be seen that the relation between e1 and
t1 and the relation between t1 and e2 are useful
for predicting the relation between e1 and e2.
Another more-complicated example is shown
below with temporal connectivity in Figure 3.
?The Congress of the United States is af-
fording(e1) Elian Gonzalez what INS and this
administration has not, which is his legal right
and his right to due process,? said(e2) Jorge
Mas Santos, chairman of the Cuban American
National Foundation. ?This gives(e3) him the
protection that he will not be repatriated(e4) to
Cuba between now and Feb. 10.?
Figure 5: Local pairwise classification. Each
TLINK is classified separately.
Figure 6: Timegraph constructed from a docu-
ment?s TLINKs
Again, the relation between e4 and e3
can be inferred from the nearby relations,
i.e., (1) e4 AFTER e2 and e2 AFTER e1
imply e4 AFTER e1, (2) e4 AFTER e1 and
e1 SIMULTANEOUS e3 imply e4 AFTER e3.
3.1 Overview of our framework
Our framework is based on the stacked learn-
ing method (Wolpert, 1992), which employs two
stages of classification as illustrated in Figure 4.
3.1.1 Local pairwise model
In a local pairwise model, temporal relation clas-
sification is done by considering only a given pair
of temporal entities at a time as illustrated in Fig-
ure 5. We use a supervised machine learning ap-
proach and employ the basic feature set that can
be easily extracted from the document?s text and
the set of features proposed in our previous work
(Laokulrat et al., 2013), which utilizes deep syn-
tactic information, as baselines. The local features
at different linguistic levels are listed in Table 1.
Two classifiers are used: one for Event-Event
TLINKs (E-E), and the other for Event-Time
TLINKs (E-T).
3.1.2 Stacked learning
Stacked learning is a machine learning method
that enables the learner to be aware of the labels
of nearby examples.
9
Figure 4: Stacked learning. The output from the first stage is treated as features for the second stage.
The final output is predicted using label information of nearby TLINKs.
The first stage, as shown in Figure 5, uses the
local classifiers and predicts the relation types of
all TLINKs. In the second stage, the document?s
timegraph is constructed and the output from the
first stage is associated with TLINKs in the graph.
The classifiers in the second stage use the infor-
mation from the nearby TLINKs and predict the
final output. We exploit features extracted from
the documents? timegraphs, as listed in Section 3.2
in the second stage of the stacked learning.
An example of a document?s timegraph is
shown in Figure 6.
3.2 Timegraph features
We treat timegraphs as directed graphs and double
the number of edges by adding new edges with
opposite relation types/directions to every existing
edge. For example, if the graph contains an edge
e1 BEFORE e2, we add a new edge e2 AFTER e1.
Our proposed timegraph features are described
below.
? Adjacent nodes and links
The features are the concatenation of the di-
rections to the adjacent links to the pair of en-
tities, the relation types of the links, and the
information on the adjacent nodes, i.e., word
tokens, part of speech tags, lemmas. For ex-
ample, the features for predicting the relation
between e1 and e2 in Figure 6 are SRC OUT-
IS INCLUDED-(Type of t0), DEST IN-BEFORE-
(Type of t0), and so on.
In this work, only Type of temporal expres-
sion (an attribute given in the Timebank cor-
pus), Tense and Part-of-speech tag are ap-
plied but other attributes could also be used.
? Other paths
Paths with certain path lengths (in this work,
2 ? path length ? 4) between the temporal
entities are used as features. The paths must
not contain cycles. For example, the path
features of the relation between e1 and e2
are IS INCLUDED-BEFORE and SIMULTANEOUS-
BEFORE-BEFORE.
? Generalized paths
A generalized version of the path features,
e.g., the IS INCLUDED-BEFORE path is gener-
alized to *-BEFORE and IS INCLUDED-*.
? (E,V,E) tuples
The (E,V,E) tuples of the edges and ver-
tices on the path are used as features, e.g.,
IS INCLUDED (Type of t0) BEFORE.
? (V,E,V) tuples
The (V,E,V) tuples of the edges and vertices
on the path are used as features, e.g., (Tense
of e1) IS INCLUDED (Type of t0) and (Type of
t0) BEFORE (Tense of e2).
The summary of the timegraph features is
shown in Table 2.
4 Relation inference and time-time
connection
We call TLINKs that have more than one path be-
tween the temporal entities ?multi-path TLINKs?.
The coverage of the multi-path TLINKs is pre-
sented in Table 3. The annotated entities in
10
the Timebank corpus create loosely connected
timegraphs as we can see from the table that only
5.65% of all the annotated TLINKs have multiple
paths between given pairs of temporal entities.
Since most of the timegraph features are only
applicable for multi-path TLINKs, it is important
to have dense timegraphs. In order to increase
the numbers of connections, we employ two ap-
proaches: relation inference and time-time con-
nection.
4.1 Relation inference
We create new E-E and E-T connections between
entities in a timegraph by following a set of infer-
ence rules. For example, if e1 happens AFTER e2
and e2 happens IMMEDIATELY AFTER e3, then we
infer a new temporal relation ?e1 happens AFTER
e3?. In this paper, we add a new connection only
when the inference gives only one type of tem-
poral relation as a result from the relation infer-
ence. Figure 7b shows the timegraph after adding
new inference relations to the original timegraph
in Figure 7a.
4.2 Time-time connection
As with Chambers et al. (2007) and Tatu and
Srikanth (2008), we also create new connections
between time entities in a timegraph by applying
some rules to normalized values of time entities
provided in the corpus.
Figure 7c shows the timegraph after adding a
time-time link and new inference relations to the
original timegraph in Figure 7a. When the nor-
malized value of t2 is more than the value of t1,
a TLINK with the relation type AFTER is added
between them. After that, as introduced in Sub-
section 4.2, new inference relations (e1-e2, e1-e3,
e2-e3) are added.
As the number of relations grows too large af-
ter performing time-time connection and infer-
ence relation recursively, we limited the number of
TLINKs for each document?s timegraph to 10,000
relations. The total number of TLINKs for all doc-
uments in the corpus is presented in Table 4. The
first row is the number of the human-annotated re-
lations. The second and third rows show the to-
tal number after performing relation inference and
time-time connection.
(a) Original timegraph
(b) After relation inference. Two relations (e1-e2, e1-e3)
are added.
after 
after 
(c) After time-time connection (t1-t2) and relation inference.
Three relations (e1-e2, e1-e3, e2-e3) are added.
Figure 7: Increasing number of TLINKs
No. of TLINKs E-E E-T Total
All TLINKs 2,520 2,463 4,983
Multi-path TLINKs 119 163 282
Percentage 4.72 6.62 5.65
Table 3: Coverage of multi-path TLINKs
11
Approach
Graph-based evaluation
F1(%) P(%) R(%)
Local - baseline features 58.15 58.17 58.13
Local - baseline + deep features 59.45 59.48 59.42
Stacked - baseline features 58.33 58.37 58.29
Stacked (inference) - baseline features 58.30 58.32 58.27
Stacked (inference, time-time) - baseline features 58.29 58.31 58.27
Stacked - baseline + deep features 59.55 59.51 59.58
Stacked (inference) - baseline + deep features 59.55 59.57 59.52
Stacked (inference, time-time) - baseline + deep features 59.61 59.63 59.58
Table 5: Ten-fold cross validation results on the training set
No. of TLINKs Total
Annotated 4,983
+Inference 24,788
+Inference + time-time connection 87,992
Table 4: Number of TLINKs in the Timebank cor-
pus
5 Evaluation
For the baselines and both stages of the stacked
learning, we have used the LIBLINEAR (Fan
et al., 2008) and configured it to work as L2-
regularized logistic regression classifiers.
We trained our models on the Timebank corpus,
introduced in Subsection 2.1, which was provided
by the TempEval-3 organiser. The corpus contains
183 newswire articles in total.
5.1 Results on the training data
The performance analysis is performed based on
10-fold cross validation over the training data. The
classification F1 score improves by 0.18 pp and
0.16 pp compared to the local pairwise models
with/without deep syntactic features.
We evaluated the system using a graph-based
evaluation metric proposed by UzZaman and
Allen (2011). Table 5 shows the classification
accuracy over the training set using graph-based
evaluation.
The stacked model affected the relation classi-
fication output of the local model, changing the
relation types of 390 (out of 2520) E-E TLINKs
and 169 (out of 2463) E-T TLINKs.
5.2 Comparison with the state of the art
We compared our system to that of Yoshikawa
et al. (2009) which uses global information to
improve the accuracy of temporal relation clas-
sification. Their system was evaluated based on
TempEval-2?s rules and data set (Verhagen et al.,
2007), in which the relation types were reduced to
six relations: BEFORE, OVERLAP, AFTER, BEFORE-
OR-OVERLAP, OVERLAP-OR-AFTER, and VAGUE. The
evaluation was done using 10-fold cross validation
over the same data set as that of their reported re-
sults.
According to TempEval-2?s rules, there are
three tasks as follows:
? Task A: Temporal relations between events
and all time expressions appearing in the
same sentence.
? Task B: Temporal relations between events
and the DCT.
? Task C: Temporal relations betweeen main
verbs of adjacent sentences.
The number of TLINKs annotated by the orga-
nizer, after relation inference, and after time-time
connection for each task is summarized in Table
7. Table 8 shows the number of TLINKs after per-
forming relation inference and time-time connec-
tion.
As shown in Table 6, our system can achieve
better results in task B and C even without deep
syntactic features but performs worse than their
system in task A. Compared to the baselines, the
overall improvement is statistically significant* (p
< 10
?4
, McNemar?s test, two-tailed) without deep
syntactic features and gets more statistically sig-
nificant** (p< 10
?5
, McNemar?s test, two-tailed)
when applying deep syntactic information to the
system. The overall result has about 1.4 pp higher
accuracy than the result from their global model.
Note that Yoshikawa et al. (2009) did not apply
deep syntactic features in their system.
12
Approach Task A Task B Task C Overall
Yoshikawa et al. (2009) (local) 61.3 78.9 53.3 66.7
Yoshikawa et al. (2009) (global) 66.2 79.9 55.2 68.9
Our system (local) - baseline features 59.9 80.3 58.5 68.5
Our system (local) - baseline + deep features 62.1 80.3 58.4 69.0
Our system (stacked) - baseline features 59.5 79.9 58.5 68.2
Our system (stacked, inference) - baseline features 59.9 80.0 59.7 68.7
Our system (stacked, inference, time-time) - baseline fea-
tures
63.8 80.0 58.9 69.5*
Our system (stacked) - baseline + deep features 63.5 79.4 58.0 68.9
Our system (stacked, inference) - baseline + deep features 63.7 80.3 59.2 69.7
Our system (stacked, inference, time-time) - baseline +
deep features
65.9 80.5 58.9 70.3**
Table 6: Comparison of the stacked model to the state of the art and to our local model (F1 score(%))
No. of TLINKs Task A Task B Task C
Annotated 1,490 2,556 1,744
Table 7: TempEval-2 data set
No. of TLINKs Total
Annotated 5,970
+Inference 156,654
+Inference + time-time connection 167,875
Table 8: Number of relations in TempEval-2 data
set
The stacked model enhances the classification
accuracy of task A when timegraphs are dense
enough. Deep syntactic features can be extracted
only when temporal entities are in the same sen-
tences so they improve the model for task A
(event-time pairs in the same sentences) but these
features clearly lower the accuracy of task C, since
there are very few event-event pairs that appear
in the same sentences (and break the definition
of task C). This is probably because the sparse-
ness of the deep features degrades the performance
in task C. Moreover, these features do not help
task B in the local model because we cannot ex-
tract any deep syntactic features from TLINKs be-
tween events and DCT. However, they contribute
slightly to the improvement in the stacked model
since deep syntactic features increase the accuracy
of the prediction of task A in the first stage of the
stacked model. As a result, timegraph features ex-
tracted from the output of the first stage are better
than those extracted from the local model trained
on only baseline features.
6 Discussion
As we can see from Table 5 and 6, although
deep syntactic features can improve the classifi-
cation accuracy significantly, some additional pre-
processing is required. Moreover, deep parsers
are not able to parse sentences in some specific
domains. Thus, sometimes it is not practical to
use this kind of features in real-world temporal
relation classification problems. By applying the
stacked learning approach to the temporal relation
classification task, the system with only baseline
features is able to achieve good classification re-
sults compared to the system with deep syntactic
features.
Again, from Table 5 and 6, the inference and
time-time connection, described in Section 4,
sometimes degrade the performance. This is pre-
sumably because the number of features increases
severely as the number of TLINKs increased.
The stacked model also has another advantage
that it is easy to build and does not consume too
much training time compared to MLNs used by
Yoshikawa et al. (2009), which are, in general,
computationally expensive and infeasible for large
training sets.
7 Conclusion
In this paper, we present an approach for exploit-
ing timegraph features in the temporal relation
classification task. We employ the stacked learn-
ing approach to make use of information obtained
from nearby entities in timegraphs. The results
13
show that our system can outperform the state-of-
the-art system and achieve good accuracy by us-
ing only baseline features. We also apply the rela-
tion inference rules and the time-time connection
to tackle the timegraphs? sparseness problem.
In future work, we hope to improve the classi-
fication performance by making use of probability
values of prediction results obtained from the first
stage of the stacked learning and applying the full
set of inference relations to the system.
Acknowledgement
The authors would like to thank the anonymous re-
viewers for their insightful comments and sugges-
tions, which were helpful in improving the quality
of the paper.
References
Danushka Bollegala, Naoaki Okazaki, and Mitsuru
Ishizuka. 2010. A bottom-up approach to sentence
ordering for multi-document summarization. In In-
formation Processing & Management, Volume 46,
Issue 1, January 2010, pages 89?109.
Johan Bos and Katja Markert. 2005. Recognis-
ing textual entailment with logical inference. In
HLT/EMNLP 2005, pages 628?635.
Nathanael Chambers, Shan Wang and Dan Juraf-
sky. 2007. Classifying temporal relations between
events. In ACL 2007, pages 173?176.
Nathanael Chambers and Dan Jurafsky. 2008. Jointly
combining implicit constraints improves temporal
ordering. In EMNLP 2008, pages 698?706.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A library for large linear classification.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. Cambridge, MA: MIT Press.
Natsuda Laokulrat, Makoto Miwa, Yoshimasa Tsu-
ruoka and Takashi Chikayama. 2013. UTTime:
Temporal relation classification using deep syntac-
tic features. In SemEval 2013, pages 89?92.
Inderjeet Mani, Marc Verhagen, Ben Wellner, Chong
Min Lee and James Pustejovsky. 2006. Machine
Learning of Temporal Relations. In ACL 2006,
pages 753?760.
Stephanie A. Miller and Lenhart K. Schubert. 1999.
Time revisited. In Computational Intelligence 6,
pages 108?118.
Yusuke Miyao and Jun?ichi Tsujii. 2008. Feature for-
est models for probabilistic HPSG parsing. In Com-
putational Linguistics. 34(1). pages 35?80, MIT
Press.
James Pustejovsky, Patrick Hanks, Roser Saur??, An-
dew See, Rob Gaizauskas, Andrea Setzer, Dragomir
Radev, Beth Sundheim, David Day, Lisa Ferro and
Marcia Lazo. 2003. The TIMEBANK Corpus.
In Proceedings of Corpus Linguistics 2003 (March
2003), pages 545?557.
James Pustejovsky, Robert Ingria, Roser Saur??, Jos?e
Casta?no, Jessica Littman, Rob Gaizauskas, Andrea
Setzer, Graham Katz and Inderjeet Mani. 2005. The
specification language TimeML. In The Language
of Time: A reader, pages 545?557.
Deepak Ravichandran and Eduard Hovy. 2002. Learn-
ing surface text patterns for a question answering
system. In ACL 2002, pages 41?47.
Stanford Natural Language Processing Group. 2012.
Stanford CoreNLP.
Marta Tatu and Munirathnam Srikanth. 2008. Experi-
ments with reasoning for temporal relations between
events. In COLING 2008, pages 857?864.
Naushad UzZaman and James F. Allen. 2011. Tempo-
ral evaluation. In ACL 2011, pages 351?356.
Naushad UzZaman, Hector Llorens, Leon Derczyn-
ski, Marc Verhagen, James Allen and James Puste-
jovsky. 2013. SemEval-2013 Task 1: TempEval-3:
Evaluating time expressions, events, and temporal
relations. In SemEval 2013, pages 2?9.
Marc Verhagen, Robert Gaizauskas, Frank Schilder,
Mark Hepple, Graham Katz and James Pustejovsky.
2007. SemEval-2007 task 15: TempEval temporal
relation identification. In SemEval 2007, pages 75?
80.
David H. Wolpert. 1992. Stacked generalization. In
Neural Networks, volume 5, pages 241?259.
Katsumasa Yoshikawa, Sebastian Riedel ,Masayuki
Asahara and Yuji Matsumoto. 2009. Jointly iden-
tifying temporal relations with Markov Logic. In
ACL 2009, pages 405?413.
14
