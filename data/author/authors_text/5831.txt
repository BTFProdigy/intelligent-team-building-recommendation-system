Layout and Language: Integrating Spatial and Linguistic 
Knowledge for Layout Understanding Tasks 
Matthew Hurs t  and Tetsuya Nasukawa 
IBM Research,  Tokyo  Research  Laboratory  
Abstract 
Complex documents tored in a flat or partially 
marked up file format require layout sensitive pre- 
processing before any natural language processing 
can be carried out on their textual content. Con- 
temporary technology for the discovery of basic tex- 
tual units is based on either spatial or other con- 
tent insensitive methods. However, there are many 
cases where knowledge of both the language and lay- 
out is required in order to establish the boundaries 
of the basic textual blocks. This paper describes 
a number of these cases and proposes the applica- 
tion of a general method combining knowledge about 
language with knowledge about the spatial arrange- 
ment of text. We claim that the comprehensive un- 
derstanding of layout can only be achieved through 
the exploitation of layout knowledge and language 
knowledge in an inter-dependent maimer. 
1 Introduction 
There is currently a significant amount of work be- 
ing carried out on applications which aim to de- 
duce layout information fl'om a spatial descrit/tion 
of a document. The tasks vary in detail, however 
they generally take as input a document description 
which presents areas of text (including titles, head- 
lags, paragraphs, lists and tables) marked implicitly 
by position. A simple example is a flat text docu- 
ment which uses white space to demonstrate align- 
meat at the edges of textual blocks and blank lines 
to indicate vertical spatial cohesion and separatiou 
between blocks. 1 
Rus and Summers ((Rus and Su,nmers, 1994)) 
state that "the non-teztual content of documents 
\[complement\] thetcztual content and should play art 
equal role". This is clearly desirable: textual and 
spatial properties, as described in tiffs paper, are 
inter-related and it is in fact highly beneficial to ex- 
ploit the relationships which exist between them. In 
1The term spatiM cohesion is motivated by the work on 
lexical cohesion by Morris and Hirst ((Morris and Hirst, 
1991)). Text which is cohesive is text which has a quality 
of unity (p. 21). Objects which have spatial cohesion have a 
quality of unity indicated by spatial features; in the words of 
Morris and Hirst: they "stick together". 
algorithmic terms, this implies implementing solu- 
tions which use both spatial and linguistic features 
to detect coherent extual objects its the raw text. 
Apt)roaches to tile problem are limited to those ex- 
ploiting spatial cohesion. There are two techuiques 
for achieving this. The first looks for features of 
space, identifying rivers of space which ruts around 
text blocks in some memfingflfl maimer. Tim second 
looks at non-linguistic qualities of the text includ- 
ing alignment of tokens between lines as well as cer- 
tain types of global interactions (e.g. (Kieninger and 
Dengel, 1998)). Although this second type focuses 
on the characters rather than the spaces in the text, 
tim features that it detects are implications of tile 
spatial arrangement of tile text: judging two words 
to be overlapping in the horizontal axis is not a fea- 
ture of tile words in terms of their content~ but of 
their position. Elements of the above basic methods 
may be combined and, as with any f'eatnre vector 
type of mmlysis, machine learning algorithins may 
be applied (e.g. (Ng el; al., 1999)). 
2 A New Method 
Tile methods based on spatial cohesion outlined 
above make assumptions about the application of 
layout to the textual content of the document in or- 
der to derive features indicating higher order struc- 
ture. These assumptions rely on tile realisation of 
layout as space and do not always hold (see, e.g., 
Figure 4: Grid Quantization), and may result in atn- 
biguities. However, there is another source of infor- 
mation which can be exploited to recover layout. 
Though layout imt)oses patial etfects, it has lit- 
tle or no effect on low level language phenomena 
witlfin the distinct layout document objects: we do 
not ca'poet the layout of tea;t o render it ungr'ammat- 
ical? Conversely, we do not expect grmmnaticality 
to persist in an incorrect interpretation of layout. 
For example, applying this observation to the seg- 
mentation of a double colmnn of text will indicate 
2It is clear that layout does has very definite consequences 
for tile content of textual document elements, however those 
features we are concerned with here are below even this rudi- 
mentary level of language analysis. 
334 
the line breaks, see Figure 4: Double Cohunns. :3The 
aI)l)lication of a low level hmguage model to the in- 
terpretation of spatially distinct textual areas can be 
applied in many cases where a tmrely spatial algo- 
rithm may fail. The following is an incomplete list 
of possible cases of application (concrete xamples 
may be found in Figure 4): 
Mu l t i  Co lumn Text  When the cohmms are sepa- 
rated by only one space, a language model may 
be aI)l)lied to determine if and where the blocks 
exist. These m W be confused with False Space 
Pos i t ives  where, by chance, the text tbnnat- 
ting introduces treams of white space within 
contiguous text. 
Apposed/Marg ina l  Mater ia l  Text which is off'- 
set from the main body of text, similarly to 
multi column text, will contain its own line 
breaks. 
Unmarked  Headers  Headers may be unmarked 
and appear silnilm' to single line 1)aragraphs. 
Doub le  Spac ing  The introduction of more tlmn 
one line of spacing within contiguous text causes 
ambiguities with paragraph set)aration, headers 
al ld so oi1. 
El l ipt ica l  L ists When text continues through a 
layout device, a language model may 1)e used 
to detect it. a 
Shor t  Paragraphs  When a t)aragral)hs is 1)artic- 
ularly short, the insertion of a line break may 
(:ause prot)lems. 
Another exmnple, and a usefifl at)plication, is that 
to the 1)rot)lore of table segmentation. Once a table 
has been located using this method or other meth- 
ods, the cells must be located. 
Mu l t i -Cohunn Cells A cell sl)ans multit)le 
cohmms. This may easily 1)e conflised with 
Mu l t i -Row Cells where a cell contains more 
than one line and must be groul)ed according 
to the line breaks. 
E l l ipt ica l  Cel l  Contents  Cells which tbrm a dis- 
junctiou of possible contilmations to the content 
of another cell can be identified using a language 
model. 
Gr id  Quant i za t ion  When a plain text table con- 
tains ceils which arc not wholly aligned with 
aIn Figure 4: Doubh,. Cohmms, we know, through 
the al)plicatlon of a language model, that there in a line 
t)reak after paragraph as a paragraph of text  in more likely 
than a paragraph Applying, and Applying this of text 
is grammatically. 
4'l'his bares similm'ities with a simple list, lint the language. 
is that of the textual lint; which uses flmctional words and 
lmnctuation to indicate disjmmtion. 
other cells in the stone grid row or column, it is 
difficult to associate the cells correctly. 
Languages which permit vertical and horizon- 
tal orthography (such as Japanese) pose additional 
t)roblems when extracting layout features from l)lain 
text data. 
Or ientat ion  Detect ion  With mixed orientation, 
a language model may be used to distinguish 
vertical and horizontal text blocks) 
We can hyl)othesise that spatially cohesive areas 
of the document are renderings of some underlying 
textual representation. If, at some level, the text 
is set)arated from the layout (the text is linearised 
by removing line breaks), then we may observe cer- 
tain linguistic phenomena which are characteristic 
of the bmguage. Reversing this allows us to identify 
the sl)atially cohesive objects in the document )y 
discovering the transfonnatioll to the text (the ap- 
plication of layout, i.e. the insertion of spacing and 
line breaks) which preserve our observations about 
the language. One such observation is the ordering 
of words. Consequently, we can apply a language 
model to a line of text in a docuinent o determine 
where line breaks have been inserted into the text 
for layout purt)oses by observing where the language 
model breaks down and where our simt)le notion of 
layout 1)ased on sl)atial features i)ermits text block 
segmelttation. This is an ideal. In fact, knowledge 
of layout and lan.q'uagc is required to over'co'me th, e 
short comings of each,. 
There are many tyt)es of language model which 
may be applied to the problem being considered, 
ranging from the analytical - which provide an in- 
dication of linguisti(" structure), to tile classi\[ying - 
which indicate if (and to what extent) the intmt tits 
the model. The analytical, such as a context free 
grmnmar, are not appropriate for this problem as 
they require a broad intmt and are not suited to the 
fraglnents of" int)ut envisioned for this at)t)lications. 
The 1)rime purpose of the language model we wish 
to use is to t)rovide some ranking of candidate con- 
timmtions of a particular set of one or more tokens. 
A simple examI)le is the bigrmn model. This uses 
fl'equency counts of pairs of words derived froln a 
corlms. Although there are advantages and disad- 
vantages to this model, it will serve as an exmnI)le 
though other more Sol)histicated and reliable models 
inay easily be at)i)lied. 
5In Figure 4: Orientation Variation, the. column of text 
on the left of the tattle is a vertically orientated label (%W~m 
nlgeTk2L) whereas the remainder of the table is horizontally 
orientated. The apparent cohnnn oll the right of the tal)le is 
an artifact of the spacing and has no linguistic cohesion. 
335 
3 Bas ic  Algor i thm 
The problem can be generally described in the fol- 
lowing manner: given a set of objects distributed 
in a two dimensional grid, for each pair of objects, 
determine if they belong to tile same cohesive set. 
TILe objects are tokens, or words, and the measm'e 
of cohesion is that one word follows from the other 
in accordance with the the nature of the language, 
the content of the document, and tlm idiom of the 
particular document element within which they may 
be contained and that the spatial model of the lay- 
out of the docmnent permits cohesion. In summary, 
the cohesion is spatial and linguistic. 
However, such a general description is not com- 
putationally sensible and the search space will be 
reduced if we consider the cases where we expect 
ambiguities to occur. This can be approached by 
recognising that when there is the potential for am- 
biguity there is often present some artifact, which 
tory well help identify the domain of the ambiguity: 
these are generally the markers of spatial cohesion; 
e.g., where there arc double cohnnns, we may also 
identify left justification. Consequently, for a given 
word in tile the double column area, tim mnbiguity 
may be resolved by inspecting tile word to the right, 
or tile set of words which may be left justified with 
the line currently under inspection on the line below. 
Therefore, tile application of tile language model to 
tile disambiguation problems mentioned above takes 
place between a small set of candidate continuation 
positions. 
These continuation points are located as pre- 
scribed by the markers of the spatial layout of text. 
Consequently, any algorithm using linguistic knowl- 
edge must exploit layout knowledge in order to 1)oth 
arrive at an economic sohltion, and also to be ro- 
bust to weaknesses in tile language model. The gen- 
eral method described here relies on and determines 
both spatial and linguistic information in a tightly 
integrated manner. Tile algorithm falls ill to the 
following broad steps: 
1. detect potential for ambiguity. 
2. compute the set of possible continuation points 
by using knowledge of spatial layout. 
3. disambiguate using a combination of hmguagc 
and layout knowledge. 
For examtfle, the words marked with a clear box 
in Figure 2, upper, are those which, according to a 
naive spatial algorithm, m'e possibly in close prox- 
imity to tile right edge of a text block. Hav- 
ing detected them, tile possit)le continuation points, 
shaded boxes, are comlmted (here for a single word 
for illustration). A language model may then be ap- 
plied to determine the most likely contimmtion. 
Care must be taken wlmn discovering equally 
likely continuations as opt)osed to a single most 
likely (me. Figure 2, lower, contains two examples. 
Tile first illustrates tile case when there is 11o con- 
tinuation appropriate (there are three equally likely 
continuations; as none is the most likely, no contin- 
nation should be proposed). In the second example, 
a unique continuation is preferred. The general al- 
gorithln above provides ammtation to the tokens in 
tile document which may then be used to drive a 
text-block recognition algorithm. 
Detect ing the Potential for Ambiguity The 
potential for ambiguity occurs when a feature of the 
document is discovered which may indicate the im- 
mediate boundary of a text block. As we arc dealing 
with the basic element of a token (or word), the po- 
tential for ambiguity may occur at the end of a word, 
or between any two words in a sequence on tile line. 
However, we only need to consider those cases where 
a spatial algorithnl may determine a block boundary 
(correctly or incorrectly). In order to do this we need 
a characterisation f a spatial algorithln in terms of 
the features it uses to determine text block bound- 
aries.These are naturally related to space in the text, 
and so onr algorithm will be concerned with the fol- 
lowing three types of space: 1) Between words where 
there is a vertical river of white space which contin- 
ues above and below according to some threshold; 
2) Between words larger than a nfinimum amount 
of space; 3) At; tim right hand side of the document 
when no more tokens are found. These describe po- 
tential points for line break insertion into text and 
constitute a partial fllnctionat model of layout. 
Computing the Set; of Continuation Points 
The set of continuation points is comtmted accord- 
ing to the assumptions used to deterinine if there is 
the potential for ambiguity. The continuation point 
from a point of potential ambiguity are: 1) The next 
word to tile right; 2) The first word on tile next line; 
3) All tile continuation points on the next line which 
are to the left of the current word. These represent 
the complement to the above functional model of 
1wout. Thus we have a model of 1wout which is 
intentionally over general as it uses local features 
which are ambiguous. 
D isambiguat ion  Disambiguation may be carried 
out in a number of ways depending on the extent re- 
quired by the language model being employed. How- 
ever, regardless of what range of history or looka- 
head is required by tile language model, the process 
of dismnbiguation is not a simple matter of selecting 
tile best possible continuation as proposed by the 
statistical or other elements of the language model. 
The interactions between layout and language re- 
quire that a nmnber of constraints be considered. 
These constraints model tile ambiguities cruised by 
336 
the layout and the language. 
For any 1)otential point of anlbiguity, a single (or 
null) l)oint of continuation must be found. And for 
any l)oint of continuation, a single source of its his- 
tory is required. If token A has potential continua- 
tion points X and Y, and token B has potential con- 
tinuation points Y and Z, mid the best; continuation 
as predicted by the model for A is X and that tor B 
is also X, then both A mid B Call not be succeeded 
by their respective best continuations. The selection 
of continuation points nmst 1)e l)ased on the set of 
possible continuation points for the connected graph 
in which a potential point of ambiguity occurs (see 
Figure 3). An additional constraint inlposed by the 
1wout of the text is that links representing contin- 
uation cmmot cross. This constraint is a feature of 
the interaction between tile spatial layotlt all(1 the 
linguistic model. 
3.1 Extens ions  
The above algorittnn is not callable, of capturing 
all types of continuation observed in the basic text; 
blocks of certain document elements. Specifically, 
there is an imi)licit restriction on a uni(lue continua- 
tion of the language through certain layout features. 
This may be called the one to one model of the inter- 
action t)etween layout and language. I\]owever, the 
less fre(luent~ though equally inlt)ortant (:ases of one 
to many and many to one intera(:tions must also l)e 
considered. In Figure d: Many to One, exanll)les of 
1)oth are given. Significantly, these cases exists at 
the boundaries between t)asi(: textual COml)onents of 
large (loculnent ol).ie(:ts (here tables). It is suggested, 
the, n, that the detection of equally likely contimla- 
tion 1)oints may be used to dete, ct boml(larie, s where 
there is little or no sl)atial separation. (; 
3.2 Ex i )e r imentat ion  
Ill order to test the lmsic ideas described in this pa- 
1)er, a siml)le systenl was imt)lenlented. A (:orplts 
of documents was collected from the SEC archive 
(www.sec.gov). These docmnents are rich ill va.r- 
ious docunlent elenmnts inchlding tables, lists and 
headers. The documents are essentially flat, though 
there is solne anlount of header information encoded 
in XML as well as a nfinimal anlount of nmrkul) in 
the document; body. 
A simple 1)igram nlodel of the, language used was 
created. This was (:onstructed 1)artly from general 
texts (a corlms of English literature.) of which it was 
assumed there was no complex content, and 1)artly 
from tile SEC docunmnts. 7 A system was iml)le- 
?This begs a definition of equally likely - which would be, 
dependent on the language model and implementation. 
7An import;ant i)rocess in the creation of a language model 
for 1wout problems i the identification ofusable language in
the COl'pll8. ~\]~o these nds, the SEC (loculne.nl, s were marke(l 
up by hand to identiI~, i)aragrai)h text. These, text blocks 
mented which marked the potential points of mnbi- 
gully and tile continuation points and then at)plied 
the chlster and selection algorithln to determine the 
presence of spatio-linguistically cohesive text blocks 
(see example output ill Figure 1). 
As yet, no formal ewfluation of the implementa- 
tion is available. It can be asserted, however, that 
tile results obtained fl'om this preliminary implemen- 
tation indicate that the general method produces 
significant results, and that the basic notion of com- 
bining spatial and linguistic infornmtion tbr the de- 
ternfination of cohesive lements in a conlplex doe- 
unlent is a powerful one. 
Another experiment investigated tlle utility of the 
mettlods described in this paper. We wanted to de- 
termine how often mnbiguities occurred and how inl- 
1)ortant correct resolution was. Looking at; the am- 
biguity in table stub (:ells - tile mnlfiguity between 
multi-row ceils and multiple ceils below a header -
resulted ill some significant results. For a sample of 
28 tables (1704 ceils); ill tile 131 stub cells we found 
68 examl)les of multi-row cells, and 35 of headers 
to multiple cells (note tlmt these are not disjoint 
sets). Using the SEC bigram model, the cases were 
disanll)iguated l)y hand, resulting in a 74 % success 
rate,. This sinlple investigation demonstrates that 
tim disalnbiguation is required and that linguistic 
inforination cm~ 1)e applied successflfily. 
4 Conclus ions 
This l)aper has outlined a set of problents 1)articu- 
lar to the encoding of complex docmneng eh',ments 
in tlat or partially marked up files. The at)l)lit:a- 
tion of ~ siml)h', language nlodel in conjunction witll 
algorithms ensitive to the layout chal'acteristics of 
the docuulent elenlents ill terms of spatial ti;atures 
is in'oposed as a general solution to these problems. 
The, method relies on the, persistence of the language 
ill which the document is written in tel'ms of the 
ulodel used to recognize it. 
ill the flltul'e, we intend to al)ply this approach to 
the implementation f a general layout analysis pre- 
processor. An interesting Dature of the interaction 
between tile language model and the 1wout of the 
document ix that the 1)erformance ofa syst, enl ix (lilly 
sensitive to the quality of the language model at tile 
I)oints at wtfich it interacts with tile layout of tile 
docunlent. Consequently, a gelmral imrl)ose model 
built fronl a corpus of marked Ill) docmnents may 
be used to deternline a subset of the cohesive text- 
blocks ill a document. Those blocks may then be 
used to derive more language data, possil)ly specific 
to the documellt, and then tim process repeated until 
no nlore interactions are left ambiguous. 
were then used for the creation of a simple bigram model. 
337 
References  
T. Kieninger and Andreas Dengel. 1998. A paper-to- 
html table converting system. In Proceedings of Doc- 
ument Analysis Systems (DAS) 98, Nagano, Japan, 
November. 
Jane Morris and Graeme Hirst. 1991. Lexical cohesion 
computed by thesaural relations as an indicator of the 
structure of text. Computational Li.quistics. 
Hwee Tou Ng, Chung Yong Lira, and Jessica Li Teng 
Koo. 1999. Learning to recognize tables in fi'ee text. 
In Proceedings of the 37th Annual Meeting oJ' the Asso- 
ciation for Computational Linguistics, pages 443-450, 
Maryland, USA, June. 
Daniela R~us ai~d Kristen Summers. 1994. Using white 
space for automated document structuring. Technical 
Report TR94-1452, Cornell University, Del)artmeilt of
Computer Science, July. 
338 
I~eh~ic~l  per  sonne IEI 
~l l i~ ,  zener~l  andE\]  
J~ Ho~th~ ended 
. . . . . . . . . . . . . . . .  h 
\[ELI gOL\] \[Z<L\] CdEb 
\[55\] \[E~ EL\] ,k~ 
f ' igure 1: Example port ion of ()utt)ut from prototype system 
For example a paracNrapli occur. Applying ithis7 
of text is gram(ng-@i-dgI 'observettion to the\] 
wherever the line \[br6~aks} segmetltation of a double 
collm~n of text will indicaEe 
., - where the \].ine b~eaks 6ccur:.' 
Foriexample, a paragraph occur. Applying this 
of t'ext is \[gramrfiatical:. hlS~-r~ft:'~26i~ to the 
the line breaks seglnentation of a double 
column of text will indicate 
where the line breaks occur. 
!s0mdtime? sentences  may consp i re  to fo rm ifais~ 
!posit ive s of r i vers  of wh i te  spaceiwl~ic~ 
appear  itO separate  blocks!. 
Somet imes .sentences  may consp i re  to forra fa l se  
ibbs i t iV6s \ ]~ r ivers  of wh i te  space  wh ich  
a~_~___~ to separate  b locks .  
b~umber ~6f Date \[Of 
\[13og~j \ [ - (~ j  ~ I~_~ Name ~ Address 
t,'ignre 2: goc~tin4g Pote.ntial Ambiguity tlnd Comput ing Contiint~,tion Points 
If a higr~un model is use.d, the probal)ility that word ~,J is followed by word w' mary 
be expressed as; a probability as p(w' I w) and assigned a value between 0 and 1. If 
the probabilities are those shown in to the right then the continuation for A would 
be X and the contimtation point for B would be Y. 
p(XlA)=0.8 p(XlB)=0.5 
A B 
p(YIA)=0.4 p(YIB)=0.3 
Figure 3: Sort ing cont inuat ion dei)end,; oil the potent ia l  ayout of tile document 
339 
1 
p~ 
0 
0 
0 
0 
! 
0 
o 
..n 
o 
Zi 
m 
@ 
o 
r~ 
o 
o~o~ ~o~ 
.~'~ 
Ng~g~ 
oz"  
0 
0 
om 
oo  
oe~ 
0 
o 
~ o 
o 
~.~ o 
.o 
L~ 
0 '  
c~ 
4~ 
o 
0 
o 
? ~.~ ,~ 
u ? 
? .~ .~ 
~o.~ 
o 
4~ 
0 
0 0 
~o 
Y. 
~ H 
o ~) ,~ 
o 
o ~ .~ 
o 
o ~ ~ 
~.~ 
0 
~.~ ~ 
. 
0 
r~ 
,.~ 
o 
o o 
4~ 
oN 
,g 
~ i ~ ~ 
u ~ u u 
i , l 
0 
?.) 
bO 
b.O 
p 
.# 
m 
b.o 
340 
Deeper Sentiment Analysis
Using Machine Translation Technology
KANAYAMA Hiroshi NASUKAWA Tetsuya
WATANABE Hideo
Tokyo Research Laboratory, IBM Japan, Ltd.
1623-14 Shimotsuruma, Yamato-shi, Kanagawa-ken, 242-8502 Japan
{hkana,nasukawa,hiwat}@jp.ibm.com
Abstract
This paper proposes a new paradigm for senti-
ment analysis: translation from text documents
to a set of sentiment units. The techniques of
deep language analysis for machine translation
are applicable also to this kind of text mining
task. We developed a high-precision sentiment
analysis system at a low development cost, by
making use of an existing transfer-based ma-
chine translation engine.
1 Introduction
Sentiment analysis (SA) (Nasukawa and Yi, 2003; Yi
et al, 2003) is a task to obtain writers? feelings as ex-
pressed in positive or negative comments, questions,
and requests, by analyzing large numbers of docu-
ments. SA is becoming a useful tool for the com-
mercial activities of both companies and individual
consumers, because they want to sort out opinions
about products, services, or brands that are scat-
tered in online texts such as product review articles,
replies given to questionnaires, and messages in bul-
letin boards on the WWW.
This paper describes a method to extract a set
of sentiment units from sentences, which is the key
component of SA. A sentiment unit is a tuple of
a sentiment1, a predicate, and its arguments. For
example, these sentences in a customer?s review of
a digital camera (1) contained three sentiment units
(1a), (1b), and (1c). Apparently these units indicate
that the camera has good features in its lens and
recharger, and a bad feature in its price.
It has excellent lens, but the price is too high.
I don?t think the quality of the recharger
has any problem.
?
?
? (1)
[favorable] excellent (lens) (1a)
[unfavorable] high (price) (1b)
[favorable] problematic+neg (recharger) (1c)
The extraction of these sentiment units is not a
trivial task because many syntactic and semantic op-
erations are required. First, the structure of a pred-
icate and its arguments may be changed from the
1Possible values of a sentiment are ?favorable?, ?unfavor-
able?, ?question?, and ?request?. In this paper the discussion
is mostly focused on the first two values.
syntactic form as in (1a) and (1c). Also modal, as-
pectual, and negation information must be handled,
as in (1c). Second, a sentiment unit should be con-
structed as the smallest possible informative unit so
that it is easy to handle for the organizing processes
after extraction. In (1b) the degree adverb ?too? is
omitted to normalize the expression. For (1c), the
predicate ?problematic? has the argument ?recharger?
instead of the head word of the noun phrase ?the
quality of the recharger?, because just using ?qual-
ity? is not informative to describe the sentiment of
the attribute of a real-world object. Moreover, dis-
ambiguation of sentiments is necessary: in (1b) the
adjective ?high? has the ?unfavorable? feature, but
?high? can be treated as ?favorable? in the expression
?resolution is high?.
We regard this task as translation from text to
sentiment units, because we noticed that the deep
language analysis techniques which are required for
the extraction of sentiment units are analogous to
those which have been studied for the purpose of
language translation. We implemented an accurate
sentiment analyzer by making use of an existing
transfer-based machine translation engine (Watan-
abe, 1992), replacing the translation patterns and
bilingual lexicons with sentiment patterns and a sen-
timent polarity lexicon. Although we used many
techniques for deep language analysis, the system
was implemented at a surprisingly low development
cost because the techniques for machine translation
could be reused in the architecture described in this
paper.
We aimed at the high precision extraction of senti-
ment units. In other words, our SA system attaches
importance to each individual sentiment expression,
rather than to the quantitative tendencies of repu-
tation. This is in order to meet the requirement of
the SA users who want to know not only the over-
all goodness of an object, but also the breakdown of
opinions. For example, when there are many posi-
tive opinions and only one negative opinion, the neg-
ative one should not be ignored because of its low
percentage, but should be investigated thoroughly
since valuable knowledge is often found in such a
minority opinion. Figure 1 illustrates an image of
the SA output. The outliner organizes positive and
negative opinions by topic words, and provides ref-
erences to the original text.
Favorable Unfavorable
battery long life - battery (3)
good - battery (2)
:
not good - battery (1)
s
lens
:
nice - lens (2)
:
(original document)
When I bought this camera,
I thought the battery
was not good, but the
problem was solved after
I replaced it with new one.
Figure 1: An image of an outliner which uses SA output.
Users can refer to the original text by clicking on the
document icons.
MT SA
Japanese
sentence
?parser
Japanese
tree structure
? transfer j
English
tree structure
Sentiment
fragments
? generator ?English
sentence
Sentiment
units
Transfer
patterns
Fragment
patterns
Bilingual
lexicon
Polarity
lexicon
Figure 2: The concept of the machine translation en-
gine and the sentiment analyzer. Some components are
shared between them. Also other components are similar
between MT and SA.
This means that the approach for SA should be
switched from the rather shallow analysis techniques
used for text mining (Hearst, 1999; Nasukawa and
Nagano, 2001), where some errors can be treated as
noise, into deep analysis techniques such as those
used for machine translation (MT) where all of the
syntactic and semantic phenomena must be handled.
We implemented a Japanese SA system using a
Japanese to English translation engine. Figure 2 il-
lustrates our SA system, which utilizes a MT engine,
where techniques for parsing and pattern matching
on the tree structures are shared between MT and
SA.
Section 2 reviews previous studies of sentiment
analysis. In Section 3 we define the sentiment unit
to be extracted for sentiment analysis. Section 4
presents the implementation of our system, compar-
ing the operations and resources with those used
for machine translation. Our system is evaluated
in Section 5. In the rest of paper we mainly use
Japanese examples because some of the operations
depend on the Japanese language, but we also use
English examples to express the sentiment units and
some language-independent issues, for understand-
ability.
2 Previous work on Sentiment
Analysis
Some prior studies on sentiment analysis focused on
the document-level classification of sentiment (Tur-
ney, 2002; Pang et al, 2002) where a document
is assumed to have only a single sentiment, thus
these studies are not applicable to our goal. Other
work (Subasic and Huettner, 2001; Morinaga et al,
2002) assigned sentiment to words, but they relied
on quantitative information such as the frequencies
of word associations or statistical predictions of fa-
vorability.
Automatic acquisition of sentiment expressions
have also been studied (Hatzivassiloglou and McKe-
own, 1997), but limited to adjectives, and only one
sentiment could be assigned to each word.
Yi et al (2003) pointed out that the multiple
sentiment aspects in a document should be ex-
tracted. This paper follows that approach, but ex-
ploits deeper analysis in order to avoid the analytic
failures reported by Nasukawa and Yi (2003), which
occurred when they used a shallow parser and only
addressed a limited number of syntactic phenomena.
In our in-depth approach described in the next sec-
tion, two types of errors out of the four reported by
Nasukawa and Yi (2003) were easily removed2.
3 Sentiment Unit
This section describes the sentiment units which are
extracted from text, and their roles in the sentiment
analysis and its applications.
A sentiment unit consists of a sentiment, a predi-
cate, its one or more arguments, and a surface form.
Formally it is expressed as in Figure 3.
The ?sentiment? feature categorizes a sentiment
unit into four types: ?favorable? [fav], ?unfavorable?
[unf], ?question? [qst], and ?request? [req]. A predi-
cate is a word, typically a verb or an adjective, which
conveys the main notion of the sentiment unit. An
argument is also a word, typically a noun, which
modifies the predicate with a case postpositional in
Japanese. They roughly correspond to a subject and
an object of the predicate in English.
For example, from the sentence (2)3, the extracted
sentiment unit is (2a).
ABC123-ha renzu-ga subarashii.
ABC123-TOPIC lens-NOM excellent
?ABC123 has an excellent lens.?
(2)
[fav] excellent ? ABC123, lens ? (2a)
The sentiment unit (2a) stands for the sentiment
is ?favorable?, the predicate is ?excellent? and its ar-
guments are ?ABC123? and ?lens?. In this case, both
?ABC123? and ?lens? are counted as words which are
associated with a favorable sentiment. Arguments
are used as the keywords in the outliner, as in the
leftmost column in Figure 1. Predicates with no ar-
gument are ignored, because they have no effects on
the view and often become noise.
2Though this paper handles Japanese SA, we also imple-
mented an English version of SA using English-French trans-
lation techniques, and that system solved the problems which
were mentioned in Nasukawa and Yi?s paper.
3?ABC123? is a fictitious product name.
<sentiment unit> ::= <sentiment> <predicate> <argument>+ <surface>
<sentiment> ::= favorable | unfavorable | question | request
<predicate> ::= <word> <feature>*
<argument> ::= <word> <feature>*
<surface> ::= <string>
Figure 3: The definition of a sentiment unit.
The predicate and its arguments can be different
from the surface form in the original text. Seman-
tically similar representations should be aggregated
to organize extracted sentiments, so the examples in
this paper use English canonical forms to represent
predicates and arguments, while the actual imple-
mentation uses Japanese expressions.
Predicates may have features, such as negation,
facility, difficulty, etc. For example, ?ABC123
doesn?t have an excellent lens.? brings a sentiment
unit ?[unf] excellent+neg ? ABC123, lens ??. Also
the facility/difficulty feature affects the sentiments
such as ?[unf] break+facil? for ?easy to break? and
?[unf] learn+diff? ?difficult to learn?.
The surface string is the corresponding part in the
original text. It is used for reference in the view of
the output of SA, because the surface string is the
most understandable notation of each sentiment unit
for humans.
We use the term sentiment polarity for the se-
lection of the two sentiments [fav] and [unf]. The
other two sentiments, [qst] and [req] are important
in applications, e.g. the automatic creation of FAQ.
Roughly speaking, [qst] is extracted from an inter-
rogative sentence, and [req] is used for imperative
sentences or expressions such as ?I want ...? and
?I?d like you to ...?. From a pragmatic point of view
it is difficult to distinguish between them4, but we
classify them using simple rules.
4 Implementation
This section describes operations and resources de-
signed for the extraction of sentiment units. There
are many techniques analogous to those for machine
translation, so first we show the architecture of the
transfer-based machine translation engine which is
used as the basis of the extraction of sentiment units.
4.1 Transfer-based Machine Translation
Engine
As illustrated on the left side of Figure 2, the
transfer-based machine translation system consists
of three parts: a source language syntactic parser,
a bilingual transfer which handles the syntactic tree
structures, and a target language generator. Here
the flow of the Japanese to English translation is
shown with the following example sentence (3).
4For example, the interrogative sentence ?Would you read
it?? implies a request.
kare hon ki
iru
watashi
ha wo ni
no
Figure 4: The Japanese syntactic tree for the sen-
tence (3).
Kare-ha watashi-no
He-TOPIC I-GEN
hon-wo ki-ni iru.
book-ACC mind-DAT enter
?He likes my book.?
(3)
First the syntactic parser parses the sentence (3)
to create the tree structure as shown in Figure 4.
Next, the transfer converts this Japanese parse
tree into an English one by applying the translation
patterns as in Figure 5. A translation pattern con-
sists of a tree of the source language, a tree of the
target language, and the word correspondences be-
tween both languages.
The patterns (a) and (b) in Figure 5 match with
the subtrees in Figure 4, as Figure 6 illustrates.
This matching operation is very complicated because
there can be an enormous number of possible combi-
nations of patterns. The fitness of the pattern com-
binations is calculated according to the similarity of
the source tree and the left side of the translation
pattern, the specificity of the translation pattern,
and so on. This example also shows the process
of matching the Japanese case markers (postposi-
tional particles). The source tree and the pattern
(a) match even though the postpositional particles
are different (?ha? and ?ga?). This process may be
much more complicated when a verb is transformed
into special forms e.g. passive or causative. Besides
this there are many operations to handle syntactic
and semantic phenomena, but here we take them for
granted because of space constraints.
Now the target fragments have been created as in
Figure 6, using the right side of the matched trans-
lation patterns as in Figure 5. The two fragments
are attached at the shared node ? noun2 ?, and lexi-
calized by using the bilingual lexicon. Finally the
target sentence ?He likes my book.? is generated by
the target language generator.
iru
noun noun ki
ga wo ni
like
noun noun
SUBJ OBJ
(a)
noun
no
watashi
noun
my
(b)
Figure 5: Two examples of Japanese-English trans-
lation patterns. The left side and the right side are
Japanese and English syntactic trees, respectively.
The ? noun ? works as a wildcard which matches with
any noun. Curves stand for correspondences be-
tween Japanese and English words.
kare hon ki
iru
watashi
ha wo ni
no
(a)
(b)
like
noun1 noun2
SUBJ OBJ
noun2
my
Figure 6: Transferring the Japanese tree in Figure 4
into the English tree. The patterns in Figure 5 create
two English fragments, and they are attached at the
nodes ? noun2 ? which share the same correspondent
node in the source language tree.
4.2 Techniques Required for Sentiment
Analysis
Our aim is to extract sentiment units with high pre-
cision. Moreover, the set of arguments of each pred-
icate should be selected necessarily and sufficiently.
Here we show that the techniques to meet these re-
quirements are analogous to the techniques for ma-
chine translation which have been reviewed in Sec-
tion 4.1.
4.2.1 Full parsing and top-down tree
matching
Full syntactic parsing plays an important role to ex-
tract sentiments correctly, because the local struc-
tures obtained by a shallow parser are not always
reliable. For example, expressions such as ?I don?t
think X is good?, ?I hope that X is good? are not fa-
vorable opinions about X, even though ?X is good?
appears on the surface. Therefore we use top-down
pattern matching on the tree structures from the full
parsing in order to find each sentiment fragment,
that is potentially a part of a sentiment unit.
In our method, initially the top node is examined
to see whether or not the node and its combination
of children nodes match with one of the patterns
in the pattern repository. In this top-down manner,
the nodes ?don?t think? and ?hope? in the above ex-
amples are examined before ?X is good?, and thus
the above expressions won?t be misunderstood to ex-
press favorable sentiments.
There are three types of patterns: principal pat-
terns, auxiliary patterns, and nominal patterns. Fig-
ure 7 illustrates examples of principal patterns: the
noun
warui
ga [unf]
bad ? noun ?
(c)
noun
iru
ki
wo ni [fav]
like ? noun ?
(d)
Figure 7: Examples of principal patterns.
declinable
to
omowa
nai
unit
+neg
(e)
declinable
monono
declinable
unit
unit
(f)
Figure 8: Examples of auxiliary patterns.
? declinable ? denotes a verb or an adjective in
Japanese. Note that the two unit s on the right side
of (f) are not connected. This means two separated
sentiment units can be obtained.
pattern (c) converts a Japanese expression ? noun -
ga warui? to a sentiment unit ?[unf] bad ? noun ??.
The pattern (d) converts an expression ? noun -wo
ki-ni iru? to a sentiment unit ?[fav] like ? noun ??,
where the subject (the noun preceding the postpo-
sitional ga) is excluded from the arguments because
the subject of ?like? is usually the author, who is not
the target of sentiment analysis.
Another type is the auxiliary pattern, which ex-
pands the scope of matching. Figure 8 has two
examples. The pattern (e) matches with phrases
such as ?X-wa yoi-to omowa-nai. ((I) don?t think
X is good.)? and produces a sentiment unit with the
negation feature. When this pattern is attached to
a principal pattern, its favorability is inverted. The
pattern (f) allows us to obtain two separate senti-
ment units from sentences such as ?Dezain-ga warui-
monono, sousasei-ha yoi. (The design is bad, but the
usability is good.)?.
4.2.2 Informative noun phrase
The third type of pattern is a nominal pattern. Fig-
ure 9 shows three examples. The pattern (g) is used
to avoid a formal noun (nominalizer) being an argu-
ment. Using this pattern, from the sentence ?Kawaii
no-ga suki-da. ((I) like pretty things)?, ?[fav] like
? pretty ?? can be extracted instead of ?[fav] like
? thing ??. The pattern (h) is used to convert a
noun phrase ?renzu-no shitsu (quality of the lens)?
into just ?lens?. Due to this operation, from Sen-
tence (4), an informative sentiment unit (4a) can be
obtained instead of a less informative one (4b).
Renzu-no shitsu-ga yoi.
lens-GEN quality-NOM good
?The quality of the lens is good.?
(4)
[fav] good ? lens ? (4a)
? [fav] good ? quality ? (4b)
adj
no
adj
(g)
noun
no
shitsu
noun
(h)
noun
noun
noun
noun
(i)
Figure 9: Examples of nominal patterns.
The pattern (i) is for compound nouns such as
?juuden jikan (recharging time)?. A sentiment
unit ?long ? time ?? is not informative, but ?long
? recharging time ?? can be regarded as a [unf] sen-
timent.
4.2.3 Disambiguation of sentiment polarity
Some adjectives and verbs may be used for both fa-
vorable and unfavorable predicates. This variation
of sentiment polarity can be disambiguated natu-
rally in the same manner as the word sense dis-
ambiguation in machine translation. The adjective
?takai (high)? is a typical example, as in (5a) and
(5b). In this case the sentiment polarity depends on
the noun preceding the postpositional particle ?ga?:
favorable if the noun is ?kaizoudo (resolution)?, unfa-
vorable if the noun is a product name. The semantic
category assigned to a noun holds the information
used for this type of disambiguation.
Kaizoudo-ga takai.
resolution-NOM high
?The resolution is high.?
? [fav] (5a)
ABC123-ga takai.
ABC123-NOM high (price)
?ABC123 is expensive.?
? [unf] (5b)
4.2.4 Aggregation of synonymous
expressions
In contrast to disambiguation, aggregation of syn-
onymous expressions is important to organize ex-
tracted sentiment units. If the different expressions
which convey the same (or similar) meanings are
aggregated into a canonical one, the frequency in-
creases and one can easily find frequently mentioned
opinions.
Using the translation architecture, any forms can
be chosen as the predicates and arguments by ad-
justing the patterns and lexicons. That is, monolin-
gual word translation is done in our method.
4.3 Resources for Sentiment Analysis
We prepared the following resources for sentiment
analysis:
Principal patterns: The verbal and adjectival
patterns for machine translation were converted
to principal patterns for sentiment analysis.
The left sides of the patterns are compatible
with the source language parts of the original
patterns, so we just assigned a sentiment po-
larity to each word. A total of 3752 principal
patterns were created.
Auxiliary/Nominal patterns: A total of 95 aux-
iliary patterns and 36 nominal patterns were
created manually.
Polarity lexicon: Some nouns were assigned sen-
timent polarity, e.g. [unf] for ?noise?. This po-
larity is used in expressions such as ?... ga ooi.
(There are many ...)?. This lexicon is also used
for the aggregation of words.
Some patterns and lexicons are domain-
dependent. The situation is the same as in
machine translation. Fortunately the translation
engine used here has a function to selectively use
domain-dependent dictionaries, and thus we can
prepare patterns which are especially suited for the
messages on bulletin boards, or for the domain of
digital cameras. For example, ?The size is small.?
is a desirable feature of a digital camera. We can
assign the appropriate sentiment (in this case, [fav])
by using a domain-specific principal pattern.
5 Evaluation
We conducted two experiments on the extraction of
sentiment units from bulletin boards on the WWW
that are discussing digital cameras. A total of 200
randomly selected sentences were analyzed by our
system. The resources were created by looking at
other parts of the same domain texts, and therefore
this experiment is an open test.
Experiment 1 measured the precision of the sen-
timent polarity, and Experiment 2 evaluated the in-
formativeness of the sentiment units. In this section
we handled only the sentiments [fav] and [unf] senti-
ments, thus the other two sentiments [qst] and [req]
were not evaluated.
5.1 Experiment 1: Precision and Recall
In order to see the reliability of the extracted sen-
timent polarities, we evaluated the following three
metrics:
Weak precision: The coincidence rate of the senti-
ment polarity between the system?s output and
manual output when both the system and the
human evaluators assigned either a favorable or
unfavorable sentiment.
Strong precision: The coincidence rate of the sen-
timent polarity between the system?s output
and manual output when the system assigned
either a favorable or unfavorable sentiment.
Recall: The detection rate of sentiment units
within the manual output.
These metrics are measured by using two meth-
ods: (A) our proposed method based on the machine
translation engine, and (B) the lexicon-only method,
which emulates the shallow parsing approach. The
latter method used the simple polarity lexicon of ad-
jectives and verbs, where an adjective or a verb had
only one sentiment polarity, then no disambigua-
tion was done. Except for the direct negation of
(A) MT (B) Lexicon only
Weak prec. 100% (31/31) 80% (41/51)
Strong prec. 89% (31/35) 44% (41/93)
Recall 43% (31/72) 57% (41/72)
Table 1: Precision and recall for the extraction of
sentiment units from 200 sentences.
(A) MT
Manual
f n u
f 20 3 0
Sy
ste
m
n 27 - 14
u 0 1 11
(B) Lexicon only
Manual
f n u
f 26 19 6
Sy
ste
m
n 14 - 7
u 4 23 15
Table 2: The breakdown of the results of Experi-
ment 1. The columns and rows show the manual
output and the system output, respectively (f: favor-
able, n: non-sentiment, u: unfavorable). The sum of
the bold numbers equals the numerators of the pre-
cision and recall.
an adjective or a verb5, no translation patterns were
used. Instead of the top-down pattern matching,
sentiment units were extracted from any part of the
tree structures (the results of full-parsing were used
also here).
Table 1 shows the results. With the MT frame-
work, the weak precision was perfect, and also the
strong precision was much higher, while the recall
was lower than for the lexicon-only method. Their
breakdowns in the two parts of Table 2 indicate that
most of errors where the system wrongly assigned
either of sentiments (i.e. human regarded an expres-
sion as non-sentiment) have been reduced with the
MT framework.
All of the above results are consistent with intu-
ition. The MT method outputs a sentiment unit
only when the expression is reachable from the root
node of the syntactic tree through the combina-
tion of sentiment fragments, while the lexicon-only
method picks up sentiment units from any node in
the syntactic tree. The sentence (6) is an exam-
ple where the lexicon-only method output the wrong
sentiment unit (6a). The MT method did not out-
put this sentiment unit, thus the precision values of
the MT method did not suffer from this example.
... gashitsu-ga kirei-da-to iu hyouka-ha
uke-masen-deshi-ta. (6)
?There was no opinion that the picture was sharp.?
? [fav] clear ? picture ? (6a)
In the lexicon-only method, some errors occurred
due to the ambiguity in sentiment polarity of an ad-
jective or a verb, e.g. ?Kanousei-ga takai. (Capa-
bilities are high.)? since ?takai (high/expensive)? is
always assigned the [unf] feature.
5?He doesn?t like it.? is regarded as negation, but ?I don?t
think it is good.? is not.
declinable
noun noun noun
ga wo ni
Figure 10: A na??ve predicate-argument structure
used by the system (C). Nouns preceding three ma-
jor postpositional particles ?ga?, ?wo?, and ?ni? are
supported as the slots of arguments. On the other
hand, in the system (A), there are over 3,000 prin-
cipal patterns that have information on appropriate
combinations for each verb and adjective.
(A) MT (C) Na??ve
Less redundant 2/35 0/35
More informative 13/35 1/35
Both 1/35 0/35
Table 3: Comparison of scope of sentiment units.
The numbers mean the counts of the better output
for each system among 35 sentiment units. The re-
mainder is the outputs that were the same in both
systems.
The recall was not so high, especially in the MT
method, but according to our error analysis the re-
call can be increased by adding auxiliary patterns.
On the other hand, it is almost impossible to increase
the precision without our deep analysis techniques.
Consequently, our proposed method outperforms the
shallow (lexicon-only) approach.
5.2 Experiment 2: Scope of Sentiment Unit
We also compared the appropriateness of the scope
of the extracted sentiment units between (A) the
proposed method with the MT framework and
(C) a method that supports only na??ve predicate-
argument structures as in Figure 10 and doesn?t use
any nominal patterns.
According to the results shown in Table 3, the MT
method produced less redundant or more informa-
tive sentiment units than did relying on the na??ve
predicate-argument structures in about half of the
cases among the 35 extracted sentiment units.
The following example (7) is a case where the sen-
timent unit output by the MT method (7a) was less
redundant than that output by the na??ve method
(7b). The translation engine understood that the
phrase ?kyonen-no 5-gatsu-ni (last May)? held tem-
poral information, therefore it was excluded from
the arguments of the predicate ?enhance?, while both
?function? and ?May? were the arguments of ?enhance?
in (7b). Apparently the argument ?May? is not nec-
essary here.
... kyonen-no 5-gatsu-ni kinou-ga
kairyou-sare-ta you-desu. (7)
?It seems the function was enhanced last May.?
[fav] enhance ? function ? (7a)
? [fav] enhance ? function, May ? (7b)
Example (8) is another case where the sentiment
unit output by the MT method (8a) was more infor-
mative than that output by the na??ve method (8b).
Than the Japanese functional noun ?hou?, its modi-
fier ?zoom? was more informative. The MT method
successfully selected the noun ?zoom? as the argu-
ment of ?desirable?.
... zuum-no hou-ga nozomashii. (8)
?A zoom is more desirable.?
[fav] desirable ? zoom ? (8a)
? [fav] desirable ? hou ? (8b)
The only one case we encountered where the
MT method extracted a less informative sentiment
unit was the sentence ?Botan-ga satsuei-ni pittari-
desu (The shutter is suitable for taking photos)?.
The na??ve method could produce the sentiment unit
?[fav] suitable ? shutter, photo ??, but the MT
method created ?[fav] suitable ? shutter ??. This is
due to the lack of a noun phrase preceding the post-
positional particle ?ni? in the principal pattern. Such
problems can be avoided by modifying the patterns,
and thus the effect of the combination of patterns
for SA has been shown here.
6 Conclusion
This paper has proposed a new approach to senti-
ment analysis: the translation from text to a set of
semantic fragments. We have shown that the deep
syntactic and semantic analysis makes possible the
reliable extraction of sentiment units, and the out-
lining of sentiments became useful because of the
aggregation of the variations in expressions, and the
informative outputs of the arguments. The experi-
mental results have shown that the precision of the
sentiment polarity was much higher than for the con-
ventional methods, and the sentiment units created
by our system were less redundant and more infor-
mative than when using na??ve predicate-argument
structures. Even though we exploited many advan-
tages of deep analysis, we could create a sentiment
analysis system at a very low development cost, be-
cause many of the techniques for machine translation
can be reused naturally when we regard the extrac-
tion of sentiment units as a kind of translation.
Many techniques which have been studied for the
purpose of machine translation, such as word sense
disambiguation (Dagan and Itai, 1994; Yarowsky,
1995), anaphora resolution (Mitamura et al, 2002),
and automatic pattern extraction from corpora
(Watanabe et al, 2003), can accelerate the further
enhancement of sentiment analysis, or other NLP
tasks. Therefore this work is the first step towards
the integration of shallow and wide NLP, with deep
NLP.
References
Ido Dagan and Alon Itai. 1994. Word sense dis-
ambiguation using a second language monolingual
corpus. Computational Linguistics, 20(4):563?
596.
Vasileios Hatzivassiloglou and Kathleen R. McKe-
own. 1997. Predicting the semantic orientation
of adjectives. In Proceedings of the 35th Annual
Meeting of the ACL and the 8th Conference of the
European Chapter of the ACL, pages 174?181.
Marti A. Hearst. 1999. Untangling text data min-
ing. In Proc. of the 37th Annual Meeting of the
Association for Computational Linguistics.
Teruko Mitamura, Eric Nyberg, Enrique Torrejon,
Dave Svoboda, Annelen Brunner, and Kathryn
Baker. 2002. Pronominal anaphora resolution in
the kantoo multilingual machine translation sys-
tem. In Proc. of TMI 2002, pages 115?124.
Satoshi Morinaga, Kenji Yamanishi, Kenji Tateishi,
and Toshikazu Fukushima. 2002. Mining product
reputations on the web. In Proc. of the 8th ACM
SIGKDD Conference.
Tetsuya Nasukawa and Tohru Nagano. 2001. Text
analysis and knowledge mining system. IBM Sys-
tems Journal, 40(4):967?984.
Tetsuya Nasukawa and Jeonghee Yi. 2003. Senti-
ment analysis: Capturing favorability using nat-
ural language processing. In Proc. of the Second
International Conferences on Knowledge Capture,
pages 70?77.
Bo Pang, Lillian Lee, and Shivakumar
Vaithyanathan. 2002. Thumbs up? Sentiment
classification using machine learning techniques.
In Proceedings of the 2002 Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP), pages 79?86.
Pero Subasic and Alison Huettner. 2001. Affect
analysis of text using fuzzy semantic typing. IEEE
Trans. on Fussy Systems.
Peter D. Turney. 2002. Thumbs up or thumbs
down? Semantic orientation applied to unsuper-
vised classification of reviews. In Proc. of the 40th
ACL Conf., pages 417?424.
Hideo Watanabe, Sadao Kurohashi, and Eiji Ara-
maki. 2003. Finding translation patterns from de-
pendency structures. In Michael Carl and Andy
Way, editors, Recent Advances in Example-based
Machine Translation, pages 397?420. Kluwer Aca-
demic Publishers.
Hideo Watanabe. 1992. A similarity-driven trans-
fer system. In Proc. of the 14th COLING, Vol. 2,
pages 770?776.
David Yarowsky. 1995. Unsupervised word sense
disambiguation rivaling supervised methods. In
Meeting of the Association for Computational
Linguistics, pages 189?196.
Jeonghee Yi, Tetsuya Nasukawa, Razvan Bunescu,
and Wayne Niblack. 2003. Sentiment analyzer:
Extracting sentiments about a given topic using
natural language processing techniques. In Pro-
ceedings of the Third IEEE International Confer-
ence on Data Mining, pages 427?434.
Term Aggregation:
Mining Synonymous Expressions using Personal Stylistic Variations
Akiko Murakami Tetsuya Nasukawa
IBM Research, Tokyo Research Laboratory
1623-14 Shimotsuruma, Yamato, Kanagawa 242-8502, Japan
 akikom, nasukawa@jp.ibm.com
Abstract
We present a text mining method for finding
synonymous expressions based on the distribu-
tional hypothesis in a set of coherent corpora.
This paper proposes a new methodology to im-
prove the accuracy of a term aggregation system
using each author?s text as a coherent corpus.
Our approach is based on the idea that one per-
son tends to use one expression for one mean-
ing. According to our assumption, most of the
words with similar context features in each au-
thor?s corpus tend not to be synonymous expres-
sions. Our proposedmethod improves the accu-
racy of our term aggregation system, showing
that our approach is successful.
1 Introduction
The replacement of words with a representative
synonymous expression dramatically enhances text
analysis systems. We developed a text mining sys-
tem called TAKMI (Nasukawa, 2001) which can
find valuable patterns and rules in text that indicate
trends and significant features about specific top-
ics using not only word frequency but also using
predicate-argument pairs that indicate dependencies
among terms. The dependency information helps
to distinguish between sentences by their meaning.
Here are some examples of sentences from a PC call
center?s logs, along with the extracted dependency
pairs:
  customer broke a tp
 customer...break,
break...tp
  end user broke a ThinkPad
 end user...break,
break...ThinkPad
In these examples, ?customer? and ?end user?
and ?tp? and ?ThinkPad? can be assumed to have
the same meaning in terms of this analysis for the
call center?s operations. Thus, these two sentences
have the same meaning, but the differences in ex-
pressions prevent us from recognizing their iden-
tity. The variety of synonymous expressions causes
a lack of consistency in expressions. Other exam-
ples of synonymous expressions are:
customer = cu = cus = cust = end user = user =
eu
Windows95 = Win95 = w95
One way to address this problem is by assign-
ing canonical forms to synonymous expressions and
variations of inconsistent expressions. The goal of
this paper is to find those of synonymous expres-
sions and variations of inconsistent expressions that
can be replaced with a canonical form for text analy-
sis. We call this operation ?term aggregation?. Term
aggregation is different from general synonym find-
ing. For instance, ?customer? and ?end user? may
not be synonyms in general, but we recognize these
words as ?customer? in the context of a manufac-
turers? call center logs. Thus, the words we want
to aggregate may not be synonyms, but their role in
the sentences are the same in the target domain from
the mining perspective. Yet, we can perform term
aggregation using the same methods as in synonym
finding, such as using word feature similarities.
There are several approaches for the automatic
extraction of synonymous expressions, such as us-
ing word context features, but the results of such
approaches tend to contain some antonymous ex-
pressions as noise. For instance, a system may ex-
tract ?agent? as a synonymous expression for ?cus-
tomer?, since they share the same feature of being
human, and since both words appear as subjects of
the same predicates, such as ?talk?, ?watch?, and
?ask?.
In general, it is difficult to distinguish synony-
mous expressions from antonymous expressions
based on their context. However, if we have a co-
herent corpus, one in which the use of expressions
is consistent for the same meaning, the words ex-
tracted from that corpus are guaranteed to have dif-
ferent meanings from each other.
Figure 1: Synonym Extraction System using Coherent Corpus
Figure 1 illustrates the idea of such coherent cor-
pora. Words with similar contexts within incoher-
ent corpora consist of various expressions including
synonyms and antonyms, as in the left hand side of
this figure, because of the use of synonymous ex-
pressions as in the upper right box of the figure.
In contrast, words with similar contexts within each
coherent corpus do not contain synonymous expres-
sions, as in the lower right box of the figure.
By using the information about non-synonymous
expressions with similar contexts, we can deduce
the synonymous expressions from the words with
similar contexts within incoherent corpora by re-
moving the non-synonymous expressions.
In this paper, we use a set of textual data written
by the same author as a coherent corpus. Our as-
sumption is that one person tends to use one expres-
sion to represent one meaning. For example, ?user?
for ?customer? and ?agt? for ?agent? as in Figure 1.
Our method has three steps: extraction of syn-
onymous expression candidates, extraction of noise
candidates, and re-evaluation with these candidates.
In order to evaluate the performance of our method,
we conducted some experiments on extracting term
aggregation sets. The experimental results indicate
that our method leads to better precision than the ba-
sic synonym extraction approach, though the recall
rates are slightly reduced.
The rest of this paper is organized as follows.
First we describe the personal stylistic variations in
each author?s text in Section 2, and in Section 3 we
will give an overview of our system. We will present
the experimental results and discussion in Section
4. We review related work in Section 5 and we con-
sider future work and conclude the paper in Section
6.
2 Personal Stylistic Variations in Each
Authors? Corpora
According to our assumption, each author uses a
unique expression to represent one semantic con-
cept, even though various expressions can be used
for representing the same meaning. To evaluate
this assumption, we analyzed a call center?s corpus,
which was typed in by the call takers in a personal
computer service call center 1.
Call Taker A B C D E
customer 31 62 32 31 286
cust 6 335 2 3 2
eu 345 89 179 402 62
user 5 20 2 3 13
Table 1: The Variation of the Expressions for ?cus-
tomer? in each Call Taker?s Text.
Table1 shows variations of the expressions for ?
customer? which were used by the call takers. This
table shows that each call taker mainly used one
1The IBM PC Help Center
unique expression to represent one meaning with a
consistency ratio of about 80%, but the other 20%
are other expressions.
These results show our assumption holds for the
tendency for one expression to have one mean-
ing within the same author?s corpus. However, it
also demonstrated that multiple expressions for the
same meaning appear within the same author?s cor-
pus even though the distribution of the appearences
clearly leans toward one expression. Thus, we
should consider this fact when we apply this as-
sumption.
3 Experiments
3.1 Data Overview
In our experiments we used one month?s worth
of data stored in the call center, containing about
five million words. The number of unique nouns
was 29,961, and the number of unique verbs was
11,737, and 3,350,200 dependency pairs were ex-
tracted from the data. We then created ten subcor-
pora in such a manner that each of them contains
data provided by the same call taker. The average
number of predicate-argument pairs in each subcor-
pus was 37,454. In our experiments, we selected
ten authors? corpus according to their size from the
larger one.
To evaluate the experiments, we manually created
some evaluation data sets. The evaluation data sets
were made for ten target words, and the average
number of variants was 7.8 words for each target
word. Some examples are shown in Table2.
target concept variants
customer customer, cu, cus,
cust, end user,
user, eu
HDD harddisk, hdd drive,
HD, HDD, hdds,
harddrive, hd, H.D
battery Battery, batteyr, battery,
battary, batt, bat
screen display, monitor,
moniter, Monitor
Table 2: Examples of Evaluation Data
For the cannonical expressions for each target
word, we simply selected the most frequent expres-
sion from the variants.
3.2 Text Analysis Tool for Noisy Data
In the call center data there are some difficulties for
natural language processing because the data con-
tains a lot of informal writing. The major problems
are;
  Words are often abbreviated
  There are many spelling errors
  Case is used inconsistently
Shallow processing is suitable for such noisy
data, so we used a Markov-model-based tagger, es-
sentially the same as the one described in (Char-
niak, 1993) in our experiments 2. This tagger as-
signs a POS based on the distribution of the candi-
date POSs for each word and the probability of POS
transitions extracted from a training corpus, and we
used a manually annotated corpus of articles from
the Wall Street Journal in the Penn Treebank corpus
3 as a training corpus. This tagger treats an unknown
word that did not appear in the training corpus as a
noun. In addition, it assigns a canonical form to
words without inflections.
After POS tagging for each sequence of words
in a document, it is possible to apply a cascaded
set of rules, successively identifying more and more
complex phrasal groups. Therefore, simple patterns
will be identified as simple noun groups and verb
groups, and these can be composed into a variety of
complex NP configurations. At a still higher level,
clause boundaries can be marked, and even (nomi-
nal) arguments for (verb) predicates can be identi-
fied. The accuracy of these analyses is lower than
the accuracy of the POS assignment.
3.3 Term Aggregation using Personal Stylistic
Variations
In this section we explain how to aggregate words
using these word features. We have three steps for
the term aggregation: creating noun feature vectors,
extracting synonymous expressions and noise can-
didates, and a re-evaluation.
3.3.1 Creating Noun Feature Vectors
There is a number of research reports on word
similarities, and the major approach is comparing
their contexts in the texts. Contexts can be de-
fined in two different ways: syntactic-based and
window-based techniques. Syntactic-based tech-
niques consider the linguistic information about
part-of-speech categories and syntactic groupings/
relationships. Window-based techniques consider
an arbitrary number of words around the given
2This shallow syntactic parser is called CCAT based on the
TEXTRACT architecture (Neff, 2003) developed at IBM Wat-
son Research Center.
3http:// www.cis.upenn.edu/ treebank/
rank candidate
1 batt
2 batterie
3 bat
4
    
cover
5 BTY
6 batterry
7  
 
 
 
 
 
 
 
 
 
adapter
8 bezel
9  
 
 
 
 
 
 
 
 
 
cheque
10
    
screw
Table 3: battery?s Synonymous Expression Candidates from the Entire Corpus
Author A
rank candidate
1 battery
2 controller
3  
 
 
 
 
 
 
 
Cover
4 APM
5
    
screw
6 mark
7  
 
 
 
 
 
 
 
 
 
cheque
8 diskette
9 checkmark
10 boot
Author B
rank candidate
1 batt
2 form
3 protector
4 DISKETTE
5 Mwave
6  
 
 
 
 
 
 
 
 
 
adapter
7 mouse
8  
 
 
 
 
 
 
 
 
 
cheque
9 checkmark
10 process
Table 4: Noise Candidates from Each Author?s Corpus
word. The words we want to aggregate for text
analysis are not rigorous synonyms, but the ?role?
is the same, so we have to consider the syntactic re-
lation based on the assumptions that words with the
same role tend to modify or be modified by similar
words (Hindle, 1990; Strzalkowski, 1992). On the
other hand, window-based techniques are not suit-
able for our data, because the documents are written
by several authors who have a variety of different
writing styles (e.g. selecting different prepositions
and articles). Therefore we consider only syntactic
features: dependency pairs, which consist of nouns,
verbs, and their relationships. A dependency pair is
written as (noun, verb(with its relationship)) as in
the following examples.
(customer, boot)
(customer, shut off)
(tp, shut off)
The symbol  means the noun modifies the verb,
and  means the verb modifies the noun. By us-
ing these extracted pairs, we can assign a frequency
value to each noun and verb as in a vector space
model. We use a noun feature vector (NFV) to eval-
uate the similarities between nouns. The NFVs are
made for each authors? corpora and for the entire
corpus, which contains all of the author?s corpora.
3.3.2 Extract Synonymous Expression
Candidates and Noise Candidates
The similarity between two nouns that we used in
our approach is defined as the cosine coefficient of
the two NFVs. Then we can get the relevant can-
didate lists that are sorted by word similarities be-
tween nouns and the target word. The noun list from
the entire corpus is based on the similarities be-
tween the target?s NFV in the entire corpus and the
NFVs in the entire corpus. These words are the syn-
onymous expression candidates, which is the base-
line system. The noun lists from the authors? cor-
pora are extracted based on the similarities between
the target?s NFV in the entire corpus and the NFVs
in each authors? corpora. The most similar word in
an author?s corpus is accepted as a synonymous ex-
pression for the target word, and the other similar
words in the author?s corpus are taken to not have
the same meaning as the target word, even though
the features are similar. These words are then taken
as the noise candidates, except for the most relevant
words in each candidate list. If there are N authors,
then N lists are extracted.
3.3.3 Re-evaluation
On the basis of our assumption, we propose a simple
approach for re-evaluation: deleting the noise can-
didates in the synonymous expression candidates.
However, as shown in Section 2, each author does
not necessarily use only one expression for one
meaning. For instance, while the call taker B in
Table 1 mostly uses ?cust?, he/she also uses other
expressions to a considerable degree. Accordingly
if we try to delete all noise candidates, such syn-
onymous expressions will be eliminated from the fi-
nal result. To avoid this kind of over-deleting, we
classified words into three types, ?Absolute Term?,
?Candidate Term?, and ?Noise Candidate?. First,
we assigned the ?Candidate Term? type to all of
the extracted terms from the entire corpus. Sec-
ond, the most relevant word extracted from each au-
thor?s corpus was turned into an ?Absolute Term?.
Third, the words extracted from all of the authors?
corpora, except for the most relevant word in each
author?s corpus, were turned into the ?Noise Can-
didate? type. In this step an ?Absolute Term? does
not change if the word is a noise candidate. Then
the words listed as ?Absolute Term? or ?Candi-
date Term? are taken as the final results of the re-
evaluation.
3.4 An Actual Example
In this section we will show an actual example of
how our system works. In this example, the target
word is ?battery?. First, the synonymous expression
candidates are extracted from the entire corpus us-
ing the NFV of the target word in the entire corpus
and the NFVs in the entire corpus. The relevant list
is shown in Table 3. In this candidate list, we can
find many synonymous expressions for ?battery?,
such as ?batt?, ?batterie?, etc, however we also see
some noise, such as ?cover?, ?adapter?, etc. In this
step these words are tentavely assigned as ?Candi-
date Term?.
Second, the noise candidates are extracted from
each authors? corpora by estimating the similarities
between the target word?s NFV in the entire corpus
and the NFVs in the author?s corpora. The noise
candidate lists from two authors are shown in Table
4. The most relevant words in each author?s cor-
pora are ?battery? and ?batt?, so the same words in
the extracted ?Candidate Term? list are turned into
?Absolute Term? and remain undeleted even when
?battery? and ?batt? appear in the same author?s cor-
pus. The rest of the words in the noise candidate
lists are noise, so the same words in the ?Candi-
date Term? list are turned into ?Noise Candidate?,
such as ?cover?, ?adapter?, ?cheque?, and ?screw?.
Finally, we can get the term aggregation result as a
list consisting of the words marked ?Absolute Term?
and ?Candidate Term?. The results are shown in Ta-
ble 5.
batt
batterie
bat
BTY
batterry
bazel
Table 5: Results after Removing the Noise
4 Experimental Results and Discussion
For the evaluation, we used general evaluation met-
rics, precision 4 , recall 5, and the F-measure 6. To
measure the system?s performance, we calculated
the precision and the recall for the top N significant
words of the baseline system and the re-evaluated
system.
4.1 Estimate of the Size of Cut-off Term
In our experiments, we used the metrics of preci-
sion and recall to evaluate our method. These met-
rics are based on the number of synonymous expres-
sions correctly extracted in the top N ranking. To
define this cut-off term rank N for the data, we did
some preliminary experiments with a small amount
of data.
With the simple noise deletion approach we ex-
pect to increase the precision, however, the recall is
not expected to be increased by using this method.
We defined the maximum top value of N as satia-
tion.
Figure 2 shows the performance against rank N
for the entire corpus. We can see the satiation point
at 20 in the figure. Therefore, we set N equal to
20 in our experiments for synonymous expression
extraction from the entire corpus.
At the same time, we want to know the highest
value of n to obtain the noise candidates. In each
author?s corpus a lower recall is acceptable, because
we will remove these words as noise from the results
of the entire corpus.
These results lead to the conclusion that the win-
dow size of the rank N for the entire corpus and the
4
   
   	

	 	 
   	

	 
5
	

  
   	

	 	 
   	

	 
 
 
6
  	  
   



00.1
0.2
0.3
0.4
0.5
0.6
0.7
5 10 15 20 25 30 35 40
R
ec
al
l
rank
Recall
Figure 2: The Recalls of the Synonymous Extrac-
tion System Against the Rank
rank n for each corpus should have the same value,
20. During the evaluation, we extracted the synony-
mous expressions with the top 20 similarities from
the entire corpus and removed the noise candidates
with the top 20 similarities from each author?s cor-
pora.
4.2 Most Relevant Word Approach
The basic idea of this method is that one author
mostly uses a unique expression to represent one
meaning. According to this idea, the most similar
words in each authors? corpora tend to be synony-
mous expression candidates. Comparing these two
methods, one is a system for removing noise and
the other is a system for extracting the most similar
word.
According to the assumption of one person
mostly using one unique expression to represent one
meaning, we can extract the synonymous expres-
sions that are the most similar word to the target
word in each author?s corpus. In comparison with
the approach using the most similar word in each
author?s corpus and removing the noise, we calcu-
lated the recall rates for the most similar word ap-
proach. Table 6 shows the recall rates for the sys-
tem with the entire corpus, the system using the top
word from three authors? corpora, five authors? cor-
pora, and ten authors? corpora.
entire 3 5 10
corpus authors authors authors
Recall 0.624 0.114 0.114 0.143
Table 6: The Recall when Defining the Most Similar
Words as Answers
These results show that the most similar words
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0 2 4 6 8 10 12
Number of Authors
Recall
Precision
F-Measure
Figure 3: The Results After Noise Reduction by Us-
ing Authors? Corpora
in the authors? corpora are not necessarily synony-
mous expressions for the target word, since some
authors use other expressions in their corpus.
4.3 Noise Deletion Approach
For evaluating the deleting noise approach, the per-
formance against the number of authors is shown
in Figure 3. We extracted the top 20 synonymous
expression candidates from the entire corpus, and
removed the top 20 (except for the most similar
words) noise candidates from the authors? corpora.
Figure 3 contains the entire corpus result, and the
results after removing the noise from three authors?
corpora, five authors? corpora, and ten authors? cor-
pora.
This figure shows that the noise reduction ap-
proach leads to better precision than the basic ap-
proach, but the recall rates are slightly reduced. This
is because they sometimes remove words that are
not noise, when an author used several expressions
for the same word. In spite of that, the F-measures
are increased, showing the method improves the ac-
curacy by 37% (when using 10 authors? corpora).
In addition, the table indicates that the improvement
relative to the number of authors is not yet at a max-
imum.
5 Related Work
There have been many approachs to automatic de-
tection of similar words from text. Our method
is similar to (Hindle, 1990), (Lin, 1998), and
(Gasperin, 2001) in the use of dependency relation-
ships as the word features. Another approach used
the words? distribution to cluster the words (Pereira,
1993), and Inoue (Inoue, 1991) also used the word
distributional information in the Japanese-English
word pairs to resolve the polysemous word prob-
lem.
Wu (Wu, 2003) shows one approach to collect
synonymous collocation by using translation infor-
mation. This time we considered only synonymous
expression terms, but the phrasal synonymous ex-
pression should be the target of aggregation in text
analysis.
Not only synonymous expressions, but abbrevi-
ation is one of the most important issues in term
aggregation. Youngja (Youngja, 2001) proposed a
method for finding abbreviations and their defini-
tions, using the pattern-based rules which were gen-
erated automatically and/or manually.
To re-evaluate the baseline synonym extraction
system, we used the authors? writing styles, and
there are some researches using this approach. The
most famous usage for them is the identification of
a unknown author of a certain document (Thisted,
1987).
6 Conclusion and Future Work
This paper describes how to use the coherent corpus
for term aggregation. In this paper we used the per-
sonal stylistic variations based on the idea that one
person mostly uses one expression for one meaning.
Although variations of personal writing styles are
cause of the synonymous expressions in general, we
managed to take advantage of such personal writing
styles in order to reduce noise for term aggregation
system.
We argued mainly about synonymous expres-
sions in this paper, we can extract abbreviations and
frequent missspelled words, and they should be con-
sidered as terms in term aggregation. We have to
consider not only role-based word similarities, but
also string-based similarities.
In general, a wide range of variations in expres-
sions for the same meaning is a problematic feature
of noisy data. However, in our method, we exploit
these problematic variations for useful information
for improving the accuracy of the system. This
noise removal approach is effective when the data
contains various expressions coming from various
authors. Gasperin (Gasperin, 2001) indicated the
specific prepositions are relevant to characterize the
significant syntactic contexts used for the measure-
ment of word similarity, considering what preposi-
tions do and do not depend on personal writing style
remains as future work.
In this paper, our work is based on the call cen-
ter?s logs, but this method is suitable for data from
other domains. For example we anticipate that
patent application data will be a suitable resource,
because this data includes various expressions, and
the expressions are based on each company?s ter-
minology. On the other hand, e-mail data does not
seem suitable for our approach because other au-
thors influence the expressions used. While we re-
stricted ourselves in this work to this specific data,
our future work will include an investigation of
the character of the data and how it influences our
method.
References
Charniak, E. 1993. Statistical Language Learning.
MIT press.
Caroline Gasperin, Pablo Gamallo, Alexandre
Agustini, Gabriel Lopes, and Vera de Lima
2001. Using Syntactic Contexts for Measuring
Word Similarity In the Workshop on Semantic
Knowledge Acquisition & Categorisation (ESS-
LLI 2001)
Donald Hindle 1990. Noun Classification From
Predicate-Argument Structures. Proceedings of
the 28th Annual Meeting of ACL, pp.268-275
Naomi Inoue 1991. Automatic Noun Classification
by Using Japanese-English Word Pairs. Proceed-
ings of the 29th Annual Meeting of ACL, pp. 201-
208
Dekang Lin 1998. Automatic Retrieval and Clus-
tering of Similar Words COLING - ACL, pp768-
774,
Nasukawa T. and Nagano, T. 2001. Text analysis
and knowledge mining system. In IBM Systems
Journal, Vol. 40, No. 4, pp. 967?984.
Mary S. Neff, Roy J. Byrd, and Branimir K. Bogu-
raev. 2003. The Talent System: TEXTRACTAr-
chitecture and Data Model. In Proceedings of the
HLT-NAACL 2003 Workshop on Software Engi-
neering and Architecture of Language Technol-
ogy systems (SEALTS), pp. 1?8.
Youngja Park and Roy J. Byrd 2001. Hybrid text
mining for finding abbreviations and their defi-
nitions. Proceedings of the 2001 Conference on
EMNLP, pp.126-133
Fernando Pereira and Naftali Tishby 1993. Distri-
butional Clustering of English Words Proceed-
ings of the 31th Annual Meeting of ACL, pp. 183-
190
Strzalkowski T. and Vauthey B. 1992. Information
Retrieval Using Robust Natural Language Pro-
cessing. Proceedings of ACL-92, pp.104-111.
B. Thisted and R. Efron. 1987. Did Shakespeare
write a newly discovered poem?. Biometrika, pp.
445?455
Hua Wu and Ming Zhou 2003. Synonymous Collo-
cation Extraction Using Translation Information
Proceedings of the 41st Annual Meeting of ACL,
pp.120-127
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 409?416
Manchester, August 2008
Textual Demand Analysis:
Detection of Users? Wants and Needs from Opinions
Hiroshi Kanayama Tetsuya Nasukawa
Tokyo Research Laboratory, IBM Japan, Ltd.
1623-14 Shimotsuruma, Yamato-shi, Kanagawa-ken, 242-8502 Japan
{hkana,nasukawa}@jp.ibm.com
Abstract
This paper tackles textual demand analy-
sis, the task of capturing what people want
or need, rather than identifying what they
like or dislike, on which much conven-
tional work has focused. It exploits syn-
tactic patterns as clues to detect previously
unknown demands, and requires domain-
dependent knowledge to get high recall. To
build such patterns we created an unsuper-
vised pattern induction method relying on
the hypothesis that there are commonly de-
sired aspects throughout a domain corpus.
Experimental results show that the pro-
posed method detects twice to four times
as many demand expressions in Japanese
discussion forums compared to a baseline
method.
1 Introduction
Increasingly we can access many opinions towards
products, services, or companies through elec-
tronic documents including questionnaires, call
logs, and other consumer-generated media (CGM)
such as Internet discussion forums and blogs. It is
very important for companies to get insights from
their customers? opinions by analyzing such docu-
ments in large numbers.
The most popular way to utilize such data has
involved sentiment analysis (SA) (Nasukawa and
Yi, 2003; Yi et al, 2003), which is the task of rec-
ognizing the writers? feelings as expressed in pos-
itive or negative comments. Typically, a SA sys-
tem focuses on expressions to identify the strong
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
or weak points of the subjects as in (1) or in the
writers? evaluations as in (2).
(1) I think the pictures are beautiful.
(2) I don?t like this camera very much.
Here we call them polar expressions because
they convey positive or negative polarities. By
counting the polar expressions related to products
or services, one can quantitatively compare the
goodness of competing services, find the draw-
backs of specific products, and so on.
In addition to polar expressions, there are other
types of expressions that provide valuable infor-
mation, especially for the supplier side rather than
the consumer side. Examples (3) and (4) express
the demands of the writers.
(3) I?d be happy if it is equipped with
a crisp LCD.
(4) I?m waiting for a single-lens reflex less
than 30,000 yen to come on the market.
We call such expressions ?demand expres-
sions?, and the underlined phrases ?demand tar-
gets.?
While sentiment analysis reveals evaluations of
existing products or services, the task proposed
here, textual demand analysis
1
, gives more direct
suggestions to companies: things they should do
to attract customers. For example, by investigating
demand targets, companies can add new functions
to products on the market or plan new services to
satisfy customers. These activities should lead to
positive evaluations in the future.
Interestingly, demand expressions may be noise
in sentiment analysis, because the demand ex-
pressions do not actually convey positive or neg-
1
Note that textual demand analysis is different from the
demand analysis in the field of marketing or software engi-
neering.
409
Consumers
Company
Opinions
?
Textual
Demand
Analysis
?
......
......
......
......
Demands Outliner
Syntactic
Patterns
Pattern
Extraction
Frequent
Demand
Instances
-
ff
??
?
Pattern Induction
Figure 1: A demand analysis system and the flow
of the pattern induction method.
ative evaluations of existing products or services,
even though these demand expressions often con-
tain positive or negative words, as in Example (3)
which contains the positive expressions ?happy?
and ?crisp LCD?.
The detection of novel demand targets requires
deep syntactic information because such demand
targets themselves can not be predefined. For ex-
ample, to regard the underlined parts of (3) and
(4) as demand targets, the non-underlined parts of
these sentences have to be properly interpreted as
triggers. This is a major difference from sentiment
analysis where the polar expressions can be de-
fined in the lexicon.
The left parts of Figure 1 illustrate the concepts
of a system that visualizes the users? demands de-
scribed in the input opinion data, where the main
analysis component processes the documents and
extracts the demand targets. The output of the sys-
tem is created by a demand outliner, which the
company uses to grasp the trends of consumers?
demands.
The syntactic patterns that can be used as clues
to demand expressions depend on the topic domain
or the writing style. To organize this linguistic
knowledge we propose an unsupervised induction
method for syntactic patterns. The right part of
Figure 1 shows the flow of pattern induction.
In the next section, we review related work, and
Section 3 defines our task more formally. In Sec-
tion 4 we describe a naive approach to the task and
Section 5 shows a form of unsupervised pattern in-
duction used to cover more demand expressions.
Section 6 gives the experimental results and we
conclude in Section 7.
2 Related Work
Sentiment analysis (SA) and related topics have
been extensively studied in recent years. The tex-
tual demand analysis proposed in this paper shares
some properties with phrase-level SA, the detec-
tion of sentiments and evaluations expressed in
phrases, rather than document-level SA, the clas-
sification of documents in terms of goodness of
reputation. Yi et al (2003) pointed out that the
multiple sentiment aspects in a document should
be extracted, and Nasukawa and Yi (2003) clar-
ified the need for deep syntactic analysis for the
phrase-level SA.
The acquisition of clues is a key technology in
these research efforts, as seen in learning meth-
ods for document-level SA (Hatzivassiloglou and
McKeown, 1997; Turney, 2002) and for phrase-
level SA (Wilson et al, 2005; Kanayama and Na-
sukawa, 2006).
As well as the sentiment expressions leading to
evaluations, there are many semantic aspects to be
extracted from documents which contain writers?
opinions, such as subjectivity (Wiebe and Mihal-
cea, 2006), comparative sentences (Jindal and Liu,
2006), or predictive expressions (Kim and Hovy,
2007). However, the extraction of the contents of
writers? demands which this paper handles is less
studied while this type of information is very valu-
able for commercial applications.
For the tasks of information extraction and re-
lation extraction, bootstrapping approaches have
been proven successful (Yangarber, 2003; Pantel
and Pennacchiotti, 2006). The pattern induction
method in this paper exploits their ideas, but their
application to the demand detection is not trivial,
because some instances of demands are previously
unknown and do not appear frequently, so they
have to be abstracted effectively.
The work by Inui et al (2003) handles seman-
tics of a type similar to ours. They aimed to detect
the requests in the responses to open-ended ques-
tionnaires, seeking direct requests such as ?...
? (?[I] would like you to ...?) and other forms
which can be paraphrased as direct requests. They
classified sentences into requests or non-requests,
where their source documents were responses to
questionnaires, and where more than 60% of the
utterances could be regarded as requests of some
sort. In contrast, our method detects the content
of the demands in the form of noun phrases, and
handles more general target documents including
410
CGM documents that contain much more diverse
utterances.
3 Task Definition
As shown in Section 1, our goal is to create a sys-
tem to enumerate in an easily understandable way
the demand targets in the input text. This section
describes the definition of a demand target and its
representation format.
3.1 Demand targets
Demands or requests written in opinion texts can
be represented by verb phrases as in e.g. ?I want
to V.? and ?I want you to V.?, or noun phrases
as in ?I want N.?
2
In this paper we focus on the
last type, i.e. noun phrases which represent desired
objects, because they are easier to aggregate and
grasp than verb phrases. Another reason is that
some demands represented with a verb phrase only
describe the objects that are desired. For example,
?I want to buy N? and ?I want you to provide N? can
be simply interpreted as meaning that N is what the
writer wants. We call such a noun phrase a demand
target, and these are the outputs of our system.
For applications, the demand targets to be de-
tected by the system depend on the type of in-
put documents. For example, from a consumers?
forum on digital cameras, the underlined parts in
Examples (3) and (4) from Section 1 apparently
describe the writer?s demands, so they are valu-
able information for such users of demand anal-
ysis such as the makers of digital camera. How-
ever, the request in Example (5) does not express
the author?s demands about any digital camera, but
rather it is written for other participants in the fo-
rum. This type should be excluded from the out-
put.
(5) Please give me a good advice.
In contrast, when the responses to a question-
naire about the activities of an organization are
processed, statements such as Example (5) should
be regarded as a demand target, since the writer
wrote it as a request to the sponsor of the question-
naire and the ?advice? is indeed a thing that can be
provided by the sponsoring organization.
3.2 Representation of demand targets
A demand target tends to be expressed by a noun
phrase with modifiers, as seen in Examples (3) and
2
?V? and ?N? indicate a verb phrase and a noun phrase,
respectively.
7
?happy?

?if?
!
?equip?


-NOM
2J
?LCD?
?
db
?crisp?

?standpoint?

F
?I?

?want?
H
?sell?
#
-ACC
-)
?reflex?
?
T!
?can buy?

?with?
30,0003
?-yen?
15
?single-lens?
Figure 2: Syntactic trees for the sentences (6) and
(7). ?*? indicates the headword of the demand tar-
gets.
(4), rather than by a single noun. The headwords
of such phrases (e.g. ?LCD? in (3) and ?reflex? in
(4)) represent the main categories of the demanded
objects, but they are not distinctive enough to rec-
ognize as knowledge of the authors? demands.
Therefore the key task of this research was to
find ways to markup the headword of a noun
phrase that represents the content of a demand in
the syntactic parse tree. Examples (6) and (7) are
the original Japanese sentences corresponding to
Examples (3) and (4).
(6) Fdb2J
!7
?I?d be happy if it is equipped with a crisp LCD.?
(7) Z3T!15-)#H
?[I?m] waiting for a single-lens reflex
less than 30,000 yen to come on the market.?
Figure 2 represents the parse trees correspond-
ing to sentences (6) and (7), where the demand tar-
gets are identified by the mark ?*?.
This simple representation is advantageous for
both the collection of and the deeper investigation
of the demand targets. One can easily grasp the
content of a demand if the application shows the
whole surface structure of the subtree under ?*? in
the tree, e.g. the underlined parts of Examples (6)
and (7). At the same time the tree structure sup-
ports the further analysis of the trends of the de-
mands by picking up the headwords or modifiers
prominent in the subtrees that were detected as de-
mand targets.
4 Baseline Method of Textual Demand
Analysis
This section describes an algorithm to extract de-
mand targets with high precision and describes
a preliminary experiment to measure the perfor-
mance.
411
N?


-ACC
]
?want?
(a)
V

?that?
E
?think?
(b)
V

?though?
V
(c)
Figure 3: (a) is a demand pattern which indicates
that the noun in N
?
is detected as a demand target.
(b) and (c) are auxiliary patterns, where V indicates
the node matches any verb.
4.1 Syntactic patterns and top-down
matching
A major purpose of textual demand analysis is
to discover novel demands embedded in the text,
thus the triggers of their detection should not be
a predefined set of demand targets but should be
their surrounding syntactic information. We use
two types of syntactic patterns shown in Figure 3.
Those patterns are compared with the syntactic
tree as the parsing result of the input sentence.
The pattern (a) in Figure 3 is a demand pattern,
which is used to search for demand targets. The
node with the ??? mark indicates the correspond-
ing node will be the headword of a demand tar-
get. Hence we write the pattern (a) as ?N
?
-
-]
? for simplicity. The patterns are applied in
a top-down manner, that is, initially the top node
of the input tree is examined to see whether or
not the node and its combination of children nodes
match with one of the patterns in the pattern repos-
itory. This method supports higher precision in the
detection than the surface pattern matching. For
example, the expression ?WaV
]K
? (?There is no one who wants low quality
goods?) should not be misunderstood to express a
demand.
The patterns (b) and (c) in Figure 3 are auxil-
iary patterns. These are used to apply the demand
patterns to nodes other than the root of the syn-
tactic tree. For example, by applying the patterns
(b) and (c), the pattern (a) can then be applied to
the expressions ?N
]E? (?I
think that I want N?) and ?N
]	
;
O ? (?Though I want N, I don?t have
enough money?), respectively, even though ?N
?
-

-]? doesn?t appear at the top of the trees.
In other words, the auxiliary patterns contribute to
generate variations of the demand patterns.
In addition, simple rules can be applied to fil-
ter out certain meaningless outputs. When a noun
phrase that matched to the ??? part of the demand
Table 1: The result on the small gold standard with
DP
1
. PM signifies surface pattern matching, TM
signifies tree matching. ?+AP? means that auxil-
iary patterns are used.
Method Precision Recall
PM 39% (14/36) 25% (14/56)
TM 92% (11/12) 20% (11/56)
TM+AP 94% (17/18) 30% (17/56)
pattern was a pronoun or very common noun (e.g.
?camera? in the camera domain) without any mod-
ifier, it is not output as a demand target.
4.2 Preliminary experiment
We conducted a preliminary experiment to assess
the feasibility of our approach.
We prepared a small gold-standard dataset
which consists of 1,152 sentences from a discus-
sion forum on digital cameras, for which two hu-
man annotators attached marks to the demand tar-
gets. There were 56 demand targets that at least
one of the annotators detected, and the sentence-
level agreement value
3
was ? = 0.73, which is
regarded as a good level of agreement. There was
no sentence in which the two annotators attached
marks to different nouns.
First, we made a minimum set of demand pat-
terns DP
1
, which contained only one basic pattern
?N
?
-
-]4? (?I want N??).
To see the effect of the top-down matching and
the auxiliary patterns described in Section 4.1,
demand targets in the gold-standard corpus were
automatically detected using three methods: pat-
tern matching with surface strings like ?
]
? (PM), tree matching without the auxiliary pat-
terns (TM), and tree matching with the auxiliary
patterns
5
(TM+AP).
Table 1 shows the results. The top-down match-
ing on the syntactic tree resulted in much higher
precision than the surface pattern matching, and
the auxiliary patterns improved the recall. The
only misdetection in the tree matching method was
due to an error in the sentence segmentation.
However, all of them show low recall values,
3
The agreement on whether or not the sentence has a de-
mand target.
4
Apparent character variations like ?]? and ??,
and alternative forms of particles were aggregated in the pars-
ing process.
5
A total of 95 auxiliary patterns which Kanayama et al
(2004) used for the sentiment analysis.
412
Table 2: The list of augmented demand patterns
DP
q
.
N
?
-
-] (I want N?), N?-#-Y (I hope N?),
N
?
-#-	6-! (Please [give] N?), N?-#-6
(I wish N
?
), N
?
-#--4 (Please do N?),
N
?
-#-^ (I ask [you] N?), N?-
-!-Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 458?467, Prague, June 2007. c?2007 Association for Computational Linguistics
Automatic Identification of Important Segments and Expressions for Mining
of Business-Oriented Conversations at Contact Centers
Hironori Takeuchi?, L Venkata Subramaniam?, Tetsuya Nasukawa?, and Shourya Roy?
?IBM Research, Tokyo Research Laboratory ?IBM Research, India Research Laboratory
Shimotsuruma 1623-14, Yamato-shi Institutional Area 4, Block-C, Vasant Kunj
Kanagawa 2428502 Japan New Delhi 110070 India
{hironori, nasukawa}@jp.ibm.com {lvsubram, rshourya}@in.ibm.com
Abstract
Textual records of business-oriented conver-
sations between customers and agents need
to be analyzed properly to acquire useful
business insights that improve productivity.
For such an analysis, it is critical to iden-
tify appropriate textual segments and ex-
pressions to focus on, especially when the
textual data consists of complete transcripts,
which are often lengthy and redundant. In
this paper, we propose a method to iden-
tify important segments from the conversa-
tions by looking for changes in the accuracy
of a categorizer designed to separate differ-
ent business outcomes. We extract effective
expressions from the important segments to
define various viewpoints. In text mining a
viewpoint defines the important associations
between key entities and it is crucial that the
correct viewpoints are identified. We show
the effectiveness of the method by using real
datasets from a car rental service center.
1 Introduction
?Contact center? is a general term for customer ser-
vice centers, help desks, and information phone
lines. Many companies operate contact centers to
sell their products, handle customer issues, and ad-
dress product-related and services-related issues. In
contact centers, analysts try to get insights for im-
proving business processes from stored customer
contact data. Gigabytes of customer contact records
are produced every day in the form of audio record-
ings of speech, transcripts, call summaries, email,
etc. Though analysis by experts results in insights
that are very deep and useful, such analysis usually
covers only a very small (1-2%) fraction of the total
call volume and yet requires significant workload.
The demands for extracting trends and knowledge
from the whole text data collection by using text
mining technology, therefore, are increasing rapidly.
In order to acquire valuable knowledge through
text mining, it is generally critical to identify im-
portant expressions to be monitored and compared
within the textual data. For example, given a large
collection of contact records at the contact center
of a manufacturer, the analysis of expressions for
products and expressions for problems often leads to
business value by identifying specific problems in a
specific product. If 30% of the contact records with
expressions for a specific product such as ?ABC?
contain expressions about a specific trouble such
as ?cracked?, while the expressions about the same
trouble appear in only 5% of the contact records for
similar products, then it should be a clue that the
product ?ABC? may actually have a crack-related
problem. An effective way to facilitate this type
of analysis is to register important expressions in a
lexicon such as ?ABC? and ?cracked? as associated
respectively with their categories such as ?product?
and ?problem? so that the behavior of terms in the
same category can be compared easily. It is actu-
ally one of the most important steps of text mining
to identify such relevant expressions and their cate-
gories that can potentially lead to some valuable in-
sights. A failure in this step often leads to a failure
in the text mining. Also, it has been considered an
artistic task that requires highly experienced consul-
458
tants to define such categories, which are often de-
scribed as the viewpoint for doing the analysis, and
their corresponding expressions through trial and er-
ror.
In this paper, we propose a method to identify im-
portant segments of textual data for analysis from
full transcripts of conversations. Compared to the
written summary of a conversation, a transcription
of an entire conversation tends to be quite lengthy
and contains various forms of redundancy. Many
of the terms appearing in the conversation are not
relevant for specific analysis. For example, the
terms for greeting such as ?Hello? and ?Welcome
to (Company A)? are unlikely to be associated with
specific business results such as purchased-or-not
and satisfied-or-not, especially because the conver-
sation is transcribed without preserving the nonver-
bal moods such as tone of voice, emotion etc. Thus
it is crucial to identify key segments and notable
expressions within conversations for analysis to ac-
quire valuable insights.
We exploit the fact that business conversations
follow set patterns such as an opening followed by a
request and the confirmation of details followed by
a closing, etc. By taking advantage of this feature of
business conversations, we have developed a method
to identify key segments and the notable expressions
within conversations that tend to discriminate be-
tween the business results. Such key segments, the
trigger segments, and the notable expressions asso-
ciated with certain business results lead us to easily
understand appropriate viewpoints for analysis.
Application of our method for analyzing nearly
one thousand conversations from a rental car reser-
vation office enabled us to acquire novel insights for
improving agent productivity and resulted in an ac-
tual increase in revenues.
Organization of the Paper: We start by describ-
ing the properties of the conversation data used in
this paper. Section 3 describes the method for iden-
tifying useful viewpoints and expressions that meet
the specified purpose. Section 4 provides the results
using conversational data. After the discussion in
Section 5, we conclude the paper in Section 6.
2 Business-Oriented Conversation Data
We consider business-oriented conversation data
collected at contact centers handling inbound tele-
phone sales and reservations. Such business oriented
conversations have the following properties.
? Each conversation is a one-to-one interaction
between a customer and an agent.
? For many contact center processes the conver-
sation flow is well defined in advance.
? There are a fixed number of outcomes and each
conversation has one of these outcomes.
For example, in car rentals, the following conversa-
tion flow is pre-defined for the agent. In practice
most calls to a car rental center follow this call flow.
? Opening - contains greeting, brand name, name
of agent
? Pick-up and return details - agent asks location,
dates and times of pick up and return, etc.
? Offering car and rate - agent offers a car spec-
ifying rate and mentions applicable special of-
fers.
? Personal details - agent asks for customer?s in-
formation such as name, address, etc.
? Confirm specifications - agent recaps reserva-
tion information such as name, location, etc.
? Mandatory enquiries - agent verifies clean driv-
ing record, valid license, etc.
? Closing - agent gives confirmation number and
thanks the customer for calling.
In these conversations the participants speak in turns
and the segments can be clearly identified. Figure 1
shows part of a transcribed call.
Each call has a specific outcome. For example,
each car rental transaction has one of two call types,
reservation or unbooked, as an outcome.
Because the call process is pre-defined, the con-
versations look similar in spite of having different
results. In such a situation, finding the differences in
the conversations that have effects on the outcomes
459
is very important, but it is very expensive and dif-
ficult to find such unknown differences by human
analysis. We show that it is possible to define proper
viewpoints and corresponding expressions leading
to insights on how to change the outcomes of the
calls.
AGENT: Welcome to CarCompanyA. My name is Albert. How may I
help you?
.........
AGENT: Allright may i know the location you want to pick the
car from.
CUSTOMER: Aah ok I need it from SFO.
AGENT: For what date and time.
.........
AGENT: Wonderful so let me see ok mam so we have a 12 or 15
passenger van avilable on this location on those dates and
for that your estimated total for those three dates just
300.58$ this is with Taxes with surcharges and with free
unlimited free milleage.
.........
AGENT : alright mam let me recap the dates you want to pick
it up from SFO on 3rd August and drop it off on august 6th in
LA alright
CUSTOMER : and one more questions Is it just in states or
could you travel out of states
.........
AGENT : The confirmation number for your booking is 221 384.
CUSTOMER : ok ok Thank you
Agent : Thank you for calling CarCompanyA and you have a
great day good bye
Figure 1: Transcript of a car rental dialog (partial)
3 Trigger Segment Detection and Effective
Expression Extraction
In this section, we describe a method for automat-
ically identifying valuable segments and concepts
from the data for the user-specified difference anal-
ysis. First, we present a model to represent the con-
versational data. After that we introduce a method
to detect the segments where the useful concepts for
the analysis appear. Finally, we select useful expres-
sions in each detected trigger segment.
3.1 Data Model
Each conversational data record in the collection D
is defined as di. Each di can be seen as a sequence
of conversational turns in the conversational data,
and then di can be divided as
di = d1i + d2i + ? ? ?+ dMii , (1)
where dki is the k-th turn in di and Mi is the total
number of turns in di. The + operator in the above
equation can be seen as an equivalent of the string
concatenation operator. We define d?ji as the por-
tion of di from the beginning to turn j. Using the
same notation, d?ji = d1i + d2i + ? ? ? + dji . The
collection of d?mki constitutes the Chronologically
Cumulative Data up to turn mk (Dk). Dk is repre-
sented as
Dk = (d?mk1 ,d?mk2 , . . . ,d?mkn ). (2)
Figure 2 shows an image of the data model. We set
some mk and prepare the chronologically cumula-
tive data set as shown in Figure 3. We represent bi-
nary mutually exclusive business outcomes such as
success and failure resulting from the conversations
as ?A? and ?not A?.
di= di1+?+diMi
Number of turns0 1 2 3 Mi
di1 di2 di3 diMimk
di~mk= i1+?+dimk
Figure 2: Conversation data model
m5 turnm1 m2 m3 m40 1 2 5 10 15
Ddidi~m5
D5di~m4
D4di~m3
D3
D2
D1
di~m2
di~m1
m1=1, m2=2, m3=5, m4=10, m5=15
Figure 3: Chronologically cumulative conversa-
tional data
3.2 Trigger Segment Detection
Trigger segments can be viewed as portions of the
data which have important features which distin-
guish data of class ?A? from data of class ?not A?.
460
To detect such segments, we divide each chrono-
logically cumulative data set Dk into two data sets,
training data Dtrainingk and test data Dtestk . Start-
ing from D1, for each Dk we trained a classifier
using Dtrainingk and evaluated it on Dtestk . Using
accuracy, the fraction of correctly classified docu-
ments, as a metric of performance (Yang and Liu,
1999), we denote the evaluation result of the cat-
egorization as acc(categorizer(Dk)) for each Dk
and plot it along with its turn. Figure 4 shows the
effect of gradually increasing the training data for
the classification. The distribution of expressions
m1 m2 m3 m4 m5
acc(categorizer(Di))
trigger trigger
D1
D2 D3
D4
D5 D all
turn
Figure 4: Plot of acc(categorizer(Dk))
in a business-oriented conversation will change al-
most synchronously because the call flow is pre-
defined. Therefore acc(categorizer(Dk)) will in-
crease if features that contribute to the categorization
appear in Dk. In contrast, acc(categorizer(Dk))
will decrease if no features that contribute to
the categorization are in Dk. Therefore, from
the transitions of acc(categorizer(Dk)), we can
identify the segments with increases as triggers
where the features that have an effect on the out-
come appear. We denote a trigger segment as
seg(start position, end position). Because the to-
tal numbers of turns can be different, we do not
detect the last section as a trigger. In Figure 4,
seg(m1,m2) and seg(m4,m5) are triggers. It is
important to note that using the cumulative data is
key to the detection of trigger segments. Using non-
cumulative segment data would give us the catego-
rization accuracy for the features within that seg-
ment but would not tell us whether the features of
this segment are improving the accuracy or decreas-
ing it. It is this gradient information between seg-
ments that is key to identifying trigger segments.
Many approaches have been proposed for docu-
ment classification (Yang and Liu, 1999). In this
research, however, we are not interested in the clas-
sification accuracy itself but in the increase and de-
crease of the accuracy within particular segments.
For example, the greeting, or the particular method
of payment may not affect the outcome, but the
mention of a specific feature of the product may
have an effect on the outcome. Therefore in our
research we are interested in identifying the partic-
ular portion of the call where this product feature
is mentioned, along with its mention, which has an
effect on the outcome of the call. In our experi-
ments we used the SVM (Support Vector Machine)
classifier (Joachims, 1998), but almost any classifier
should work because our approach does not depend
on the classification method.
3.3 Effective Expression Extraction
In this section, we describe our method to extract
effective expressions from the detected trigger seg-
ments.
The effective expressions in Dk are those which
are representative in the selected documents and
appear for the first time in the trigger segments
seg(mi,mj). Numerous methods to select features
exist (Hisamitsu and Niwa, 2002) (Yang and Ped-
ersen, 1997). We use the ?2 statistic for each ex-
pression in Dk as a representative metric. For the
two-by-two contingency table of a expression w and
a class ?A? shown in Table 1, the ?2 statistic is cal-
culated as
Table 1: Contingency table for calculating the ?2
statistic
# of documents # of documents
including w not including w
A n11 n12
not-A n21 n22
?2 = N(n11n22 ? n12n21)
2
(n11 + n12)(n11 + n21)(n12 + n22)(n21 + n22) (3)
where N is the number of documents. This statis-
tic can be compared to the ?2 distribution with one
degree of freedom to judge representativeness.
We also want to extract the expressions that have
not had an effect on the outcome before Dk. To de-
tect the new expressions in Dk, we define the metric
461
new(w) = w(Dk)max(w(Dk?1), 1)/
mk
mk?1
?sign(w(DAk )? w(DnotAk )), (4)
where w(Dk) is the frequency of expression w in
the chronologically cumulative data Dk, max(a, b)
selects the larger value in the arguments, mk is the
number of turns in Dk, w(DAk ) is the frequency of
w in Dk with the outcome of the corresponding data
being ?A?, and sign(?) is the signum function. When
w in class ?A? appears in Dk much more frequently
than Dk?1 compared with the ratio of their turns,
this metric will be more than 1. We detect signifi-
cant expressions by considering the combined score
?2(w) ? new(w). Using this combined score, we
can filter out the representative expressions that have
already appeared before Dk and distinguish signifi-
cant expressions that first appear in Dk for each class
?A? and ?not A?.
3.4 Appropriate Viewpoint Selection
In a text mining system, to get an association that
leads to a useful insight, we have to define appro-
priate viewpoints. Viewpoints refer to objects in re-
lation to other objects. In analysis using a conven-
tional text mining system (Nasukawa and Nagano,
2001), the viewpoints are selected based on expres-
sions in user dictionaries prepared by domain ex-
perts. We have identified important segments of the
conversations by seeing changes in the accuracy of a
categorizer designed to segregate different business
outcomes. We have also been able to extract effec-
tive expressions from these trigger segments to de-
fine various viewpoints. Hence, viewpoint selection
is now based on the trigger segments and effective
expressions identified automatically based on speci-
fied business outcomes. In the next section we apply
our technique to a real life dataset and show that we
can successfully select useful viewpoints.
4 Experiments and Results
4.1 Experiment Data and System
We collected 914 recorded calls from the car rental
help desk and manually transcribed them. Figure 1
shows part of a call that has been transcribed.
There are three types of calls:
1. Reservation Calls: Calls which got converted.
Here, ?converted? means the customer made a
reservation for a car. Reserved cars can get
picked-up or not picked-up, so some reserved
cars do not eventually get picked-up by cus-
tomers (no shows and cancellations).
2. Unbooked Calls: Calls which did not get con-
verted.
3. Service Calls: Customers changing or enquir-
ing about a previous booking.
The distribution of the calls is given in Table 2.
Table 2: Distribution of calls
Unbooked Calls 461
Reservation Calls (Picked-Up) 72
Reservation Calls (Not Picked-Up) 65
Service Calls 326
Total Calls 914
The reservation calls are most important in this
context, so we focus on those 137 calls. In the reser-
vation calls, there are two types of outcomes, car
picked-up and car not picked-up. All reservation
calls look similar in spite of having different out-
comes (in terms of pick up). The reservation hap-
pens during the call but the pick up happens at a
later date. If we can find differences in the conver-
sation that affect the outcome, it is expected that we
could improve the agent productivity. Reservation
calls follow the pre-defined reservation call flow that
we mentioned in Section 2 and it is very difficult
to find differences between them manually. In this
experiment, by using the proposed method, we try
to extract trigger segments and expressions to find
viewpoints that affect the outcome of the reservation
calls.
For the analysis, we constructed a text mining sys-
tem for the difference analysis ?picked-up? vs. ?not
picked-up?. The experimental system consists of
two parts, an information extraction part and a text
mining part. In the information extraction part we
define dictionaries and templates to identify useful
expressions. In the text mining part we define appro-
priate viewpoints based on the identified expressions
to get useful associations leading to useful insights.
462
4.2 Results of Trigger Segment Detection and
Effective Expression Extraction
Based on the pre-defined conversation flow de-
scribed in Section 2, we set m1=1, m2=2,
m3=5, m4=10, m5=15, and m6=20 and prepared
D1, . . . , D6 and D. The features of di consist of
nouns, compound nouns, specified noun phrases
(e.g. adjective+noun), and verbs. For each Dk
we calculated acc(categorizer(Dk)) for the classes
?picked-up? and ?not picked-up.? In this process, we
use a SVM-based document categorizer (Joachims,
2002). Of the 137 calls, we used 100 calls for
training the categorizer and 37 calls for trigger
segment detection. Figure 5 shows the results of
acc(categorizer(Dk)) for picked-up. The accuracy
of classification using the data of entire conversa-
tions (acc(categorizer(D)) is 67.6% but we are try-
ing to detect important segments by considering not
the accuracy values themselves but the gradients be-
tween segments. From these results, seg(1, 2) and
0
10
20
30
40
50
60
70
80
0 5 10 15 20 25 30 35 40 45
Turn (m_j)
A
c
c
u
r
a
c
y
 
[
%
]
D1
D2
D3
D4
D5
D
D6
Figure 5: Result of acc(categorizer(Dk))
seg(10, 15) are detected as trigger segments. We
now know that these segments are highly correlated
to the outcome of the call.
For each detected trigger segment, we extract ef-
fective expressions in each class using the metric de-
scribed in Section 3.3. Table 3 shows some expres-
sions with high values for the metric for each trigger.
In this table, ?just NUMERIC dollars? is a canonical
expression and an expression such as ?just 160 dol-
lars? is mapped to this canonical expression in the
information extraction process. From this result, in
seg(1, 2), ?make?, ?reservation? are correlated with
?pick up? and ?rate? and ?check? are correlated with
Table 3: Selected expressions in trigger segments
Trigger Selected expressions
pick up not picked up
seg(1, 2) make, return, tomorrow, rate, check, see
day, airport, look, want, week
assist, reservation, tonight
seg(10, 15) number, corporate program, go, impala
contract, card, have,
tax surcharge,
just NUMERIC dollars,
discount, customer club,
good rate, economy
?not-picked up?. By looking at some documents
containing these expressions, we found customer in-
tention phrases such as ?would like to make a reser-
vation?, ?want to check a rate?, etc. Therefore, it
can be induced that the way a customer starts the
call may have an impact on the outcome. From ex-
pressions in seg(10, 15), it can be said that discount-
related phrases and mentions of the good rates by the
agent can have an effect on the outcome.
We can directly apply the conventional methods
for representative feature selection to D. The fol-
lowing expressions were selected as the top 20 ex-
pressions from whole conversational data by using
the ?2 metric defined in (3).
corporate program, contract, counter, September,
mile, rate, economy, last name,
valid driving license,BRAND NAME, driving,
telephone, midsize, tonight, use, credit, moment,
airline, afternoon
From these results, we see that looking at the call as
a whole does not point us to the fact that discount-
related phrases, or the first customers-utterance, af-
fect the outcome. Detecting trigger segments and
extracting important expressions from each trigger
segment are key to identifying subtle differences be-
tween very similar looking calls that have entirely
opposite outcomes.
4.3 Results of Text Mining Analysis using
Selected Viewpoints and Expressions
From the detected segments and expressions we de-
termined that the customer?s first utterance along
with discount phrases and value selling phrases af-
fected the call outcomes. Under these hypotheses,
we prepared the following semantic categories.
463
? Customer intention at start of call: From the
customer?s first utterance, we extract the fol-
lowing intentions based on the patterns.
? strong start: would like to make a booking,
need to pick up a car, . . .
? weak start: would like to check the rates,
want to know the rate for vans, . . .
Under our hypotheses, the customer with a
strong start has the intention of booking a car
and we classify such a customer as a book-
ing customer. The customer with a weak start
usually just wants to know the rates and is clas-
sified as a rates customer.
? discount-related phrases: discount, corporate
program, motor club, buying club . . . are reg-
istered into the domain dictionary as discount-
related phrases.
? value selling phrases: we extract phrases men-
tioning good rates and good vehicles by match-
ing patterns related to such utterances.
? mentions of good rates: good rate, won-
derful price, save money, just need to pay
this low amount, . . .
? mentions of good vehicles: good car, fan-
tastic car, latest model, . . .
Using these three categories, we tried to find insights
to improve agent productivity.
Table 4 shows the result of two-dimensional as-
sociation analysis for 137 reservation calls. This ta-
ble shows the association between customer types
based on customer intention at the start of a call
and pick up information. From these results, 67%
Table 4: Association between customer types and
pick up information
Customer types extracted from texts Pick up information
based on customer intent at start of call pick up not-picked up
booking customer (w/ strong start) (70) 47 23
rates customer (w/ weak start) (37) 13 24
(47 out of 70) of the booking customers picked up
the reserved car and only 35% (13 out of 37) of the
rates customers picked it up. This supports our hy-
pothesis and means that pick up is predictable from
the customer?s first or second utterance.
It was found that cars booked by rates customers
tend to be ?not picked up,? so if we can find any
actions by agents that convert such customers into
?pick up,? then the revenue will improve. In the
booking customer case, to keep the ?pick up? high,
we need to determine specific agent actions that con-
cretize the customer?s intent.
Table 5 shows how mentioning discount-related
phrases affects the pick up ratios for rates customers
and booking customers. From this table, it can
Table 5: Association between mention of discount
phrases and pick up information
Rates customer Pick up information
Mention of discount phrases by agents pick up not-picked up
yes (21) 10 11
no (16) 3 13
Booking customer Pick up information
Mention of discount phrases by agents pick up not picked up
yes (40) 30 10
no (30) 17 13
be seen that mentioning discount phrases affects
the final status of both types of customers. In the
rates customer case, the probability that the booked
car will be picked up, P (pick-up) is improved to
0.476 by mentioning discount phrases. This means
customers are attracted by offering discounts and
this changes their intention from ?just checking rate?
to ?make a reservation here?. We found similar
trends for the association between mention of value
selling phrases and pick up information.
4.4 Improving Agent Productivity
From the results of the text mining analysis experi-
ment, we derived the following actionable insights:
? There are two types of customers in reservation
calls.
? Booking customer (with strong start)
tends to pick up the reserved car.
? Rates customer (with weak start) tends
not to pick up the reserved car.
? In the rates customer case, ?pick up? is im-
proved by mentioning discount phrases.
By implementing the actionable insights derived
from the analysis in an actual car rental process, we
verified improvements in pick up. We divided the
83 agents in the car rental reservation center into
two groups. One of them, consisting of 22 agents,
was trained based on the insights from the text min-
ing analysis. The remaining 61 agents were not
told about these findings. By comparing these two
464
groups over a period of one month we hoped to see
how the actionable insights contributed to improv-
ing agent performance. As the evaluation metric, we
used the pick up ratio - that is the ratio of the number
of ?pick-ups? to the number of reservations.
Following the training the pick up ratio of the
trained agents increased by 4.75%. The average
pick up ratio for the remaining agents increased by
2.08%. Before training the ratios of both groups
were comparable. The seasonal trends in this indus-
try mean that depending on the month the bookings
and pickups may go up or down. We believe this
is why the average pick up ratio for the remaining
agents also increased. Considering this, it can be es-
timated that by implementing the actionable insights
the pick up ratio for the pilot group was improved by
about 2.67%. We confirmed that this difference is
meaningful because the p-value of the t-test statistic
is 0.0675 and this probability is close to the stan-
dard t-test (?=0.05). Seeing this, the contact center
trained all of its agents based on the insights from
the text mining analysis.
5 Discussion
There has been a lot of work on specific tools for
analyzing the conversational data collected at con-
tact centers. These include call type classification
for the purpose of categorizing calls (Tang et al,
2003) (Zweig et al, 2006), call routing (Kuo and
Lee, 2003) (Haffner et al, 2003), obtaining call log
summaries (Douglas et al, 2005), agent assisting
and monitoring (Mishne et al, 2005), and building
of domain models (Roy and Subramaniam, 2006).
Filtering problematic dialogs automatically from an
automatic speech recognizer has also been studied
(Hastie et al, 2002) (Walker et al, 2002). In con-
trast to these technologies, in this paper we con-
sider the task of trying to find insights from a col-
lection of complete conversations. In (Nasukawa
and Nagano, 2001), such an analysis was attempted
for agent-entered call summaries of customer con-
tacts by extracting phrases based on domain-expert-
specified viewpoints. In our work we have shown
that even for conversational data, which is more
complex, we could identify proper viewpoints and
prepare expressions for each viewpoint. Call sum-
maries by agents tend to mask the customers? inten-
tion at the start of the call. We get more valuable
insights from the text mining analysis of conversa-
tional data. For such an analysis of conversational
data, our proposed method has an important role.
With our method, we find the important segments
in the data for doing analyses. Also our analyses are
closely linked to the desired outcomes.
In trigger detection, we created a chronologically
cumulative data set based on turns. We can also
use the segment information such as the ?opening?
and ?enquiries? described in Section 2. We prepared
data with segment information manually assigned,
made the chronologically cumulative data and ap-
plied our trigger detection method. Figure 6 shows
the results of acc(categorizer(Dk)). The trend in
40
45
50
55
60
call start -->
opening
call start -->
details
call start -->
offering
call start -->
personal
details
call start -->
confirmation,
mandatory
questions,
closing
Conversation flow
A
c
c
u
r
a
c
y
 
[
%
]
Figure 6: Result of acc(categorizer(Dk)) using
segment information
Figure 6 is similar to that in Figure 5. From this
result, it is observed that ?opening? and ?offering?
segments are trigger segments. Usually, segmenta-
tion is not done in advance and to assign such infor-
mation automatically we need data with labeled seg-
mentation information. The results show that even
in the absence of labeled data our trigger detection
method identifies the trigger segments. In the exper-
iments in Section 4, we set turns for each chrono-
logically cumulative data by taking into account the
pre-defined call flow.
In Figure 5 we observe that the accuracy of the
categorizer is decreasing even when using increas-
ing parts of the call. Even the accuracy using the
complete call is less than using only the first turn.
This indicates that the first turn is very informative,
but it also indicates that the features are not being
used judiciously. In a conventional classification
task, the number of features are sometimes restricted
465
when constructing a categorizer. It is known that se-
lecting only significant features improves the clas-
sification accuracy (Yang and Pedersen, 1997). We
used Information Gain for selecting features from
the document collection. This method selects the
most discriminative features between two classes.
As expected the classification accuracy improved
significantly as we reduced the total number of fea-
tures from over 2,000 to the range of 100 to 300.
Figure 7 shows the changes in accuracy. In the pro-
40
45
50
55
60
65
70
75
80
85
90
0 5 10 15 20 25 30 35 40 45
Turn (m_j)
A
c
c
u
r
a
c
y
 
[
%
}
100
200
300
D1
D2
D3
D4 D5 D
D6
Figure 7: Result of acc(categorizer(Dk)) with top
100 to 300 features selected using information gain
posed method, we detect trigger segments using the
increases and decreases of the classification accu-
racy. By selecting features, the noisy features are not
added in the segments. Decreasing portions, there-
fore are not observed. In this situation, as a trigger
segment, we can detect the portion where the gra-
dient of the accuracy curve increases. Also using
feature selection, we find that the classification ac-
curacy is highest when using the entire document,
which is expected. However, we notice that the trig-
ger segments obtained with and without feature se-
lection are almost the same.
In the experiment, we use manually transcribed
data. As future work we would like to use the noisy
output of an automatic speech recognition system to
obtain viewpoints and expressions.
6 Conclusion
In this paper, we have proposed methods for iden-
tifying appropriate segments and expressions auto-
matically from the data for user specified difference
analysis. We detected the trigger segments using the
property that a business-oriented conversation fol-
lows a pre-defined flow. After that, we identified
the appropriate expressions from each trigger seg-
ment. It was found that in a long business-priented
conversation there are important segments affecting
the outcomes that can not been easily detected by
just looking through the conversation, but such seg-
ments can be detected by monitoring the changes
of the categorization accuracy. For the trigger seg-
ment detection, we do not use semantic segment in-
formation but only the positional segment informa-
tion based on the conversational turns. Because our
method does not rely on the semantic information in
the data, therefore our method can be seen as robust.
Through experiments with real conversational data,
using identified segments and expressions we were
able to define appropriate viewpoints and concepts
leading to insights for improving the car rental busi-
ness process.
Acknowledgment
The authors would like to thank Sreeram Balakr-
ishnan, Raghuram Krishnapuram, Hideo Watanabe,
and Koichi Takeda at IBM Research for their sup-
port. The authors also appreciate the efforts of Jatin
Joy Giri at IBM India in providing domain knowl-
edge about the car rental process and thank him for
help in constructing the dictionaries.
References
S. Douglas, D. Agarwal, T. Alonso, R. M. Bell,
M. Gilbert, D. F. Swayne, and C. Volinsky. 2005.
Mining customer care dialogs for ?daily news?.
IEEE Transaction on Speech and Audio Processing,
13(5):652?660.
P. Haffner, G. Tur, and J. H. Wright. 2003. Optimizing
svms for complex call classification. In Proceedings of
IEEE International Conference on Acoustics, Speech,
and Signal Processing (ICASSP), pages 632?635.
H. W. Hastie, R. Prasad, and M. A. Walker. 2002. What?s
the trouble: Automatically identifying problematic di-
alogues in darpa communicator dialogue systems. In
Proceedings of the 40th Annual Meeting of the ACL,
pages 384?391.
T. Hisamitsu and Y. Niwa. 2002. A measure of term rep-
resentativeness based on the number of co-occurring
sailent words. In Proceedings of the 19th International
Conference on Computational Linguistics (COLING),
pages 1?7.
466
T. Joachims. 1998. Text categorization with support vec-
tor machines: Learning with many relevant features.
In Proceedings of the 10th European Conference on
Machine Learning (ECML), pages 137?142.
T. Joachims. 2002. Optimizing search engines using
clickthrough data. In Proceedings of the ACM Con-
ference on Knowledge Discovery and Data Mining
(KDD), pages 133?142.
H.-K J. Kuo and C.-H. Lee. 2003. Discriminative train-
ing of natural language call routers. IEEE Transaction
on Speech and Audio Processing, 11(1):24?35.
G. Mishne, D. Carmel, R. Hoory, A. Roytman, and
A. Soffer. 2005. Automatic analysis of call-center
conversations. In Proceedings of ACM Conference
on Information and Knowledge Management (CIKM),
pages 453?459.
T. Nasukawa and T. Nagano. 2001. Text analysis and
knowledge mining system. IBM Systems Journal,
pages 967?984.
S. Roy and L. V. Subramaniam. 2006. Automatic
generation of domain models for call centers from
noisy transcriptions. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics
and 44th Annual Meeting of the ACL (COLING/ACL),
pages 737?744.
M. Tang, B. Pellom, and K. Hacioglu. 2003. Call-
type classification and unsupervised training for the
call center domain. In Proceesings of IEEE Workshop
on Automatic Speech Recognition and Understanding,
pages 204?208.
M. A. Walker, I. Langkilde-Geary, H. W. Hastie,
J. Wright, and A. Gorin. 2002. Automatically train-
ing a problematic dialogue predictor for a spoken di-
alogue system. Journal of Artificial Intelligence Re-
search, 16:393?319.
Y. Yang and X. Liu. 1999. A re-examination of text cate-
gorization methods. In Proceedings of the 22th Annual
International ACM SIGIR Conference on Research
and Development in Information Retrieval, pages 42?
49.
Y. Yang and J. O. Pedersen. 1997. A comparative study
on feature selection in text categorization. In Proceed-
ings of the 14th International Conference on Machine
Learning (ICML), pages 412?420.
G. Zweig, O. Shiohan, G. Saon, B. Ramabhadran,
D. Povey, L. Mangu, and B. Kingsbury. 2006. Au-
tomatic analysis of call-center conversations. In Pro-
ceedings of IEEE Internatinal Conference of Acous-
tics, Speech and Signal Processing (ICASSP), pages
589?592.
467
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 355?363,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Fully Automatic Lexicon Expansion
for Domain-oriented Sentiment Analysis
Hiroshi Kanayama Tetsuya Nasukawa
Tokyo Research Laboratory, IBM Japan, Ltd.
1623-14 Shimotsuruma, Yamato-shi, Kanagawa-ken, 242-8502 Japan
{hkana,nasukawa}@jp.ibm.com
Abstract
This paper proposes an unsupervised
lexicon building method for the detec-
tion of polar clauses, which convey pos-
itive or negative aspects in a specific
domain. The lexical entries to be ac-
quired are called polar atoms, the min-
imum human-understandable syntactic
structures that specify the polarity of
clauses. As a clue to obtain candidate
polar atoms, we use context coherency,
the tendency for same polarities to ap-
pear successively in contexts. Using
the overall density and precision of co-
herency in the corpus, the statistical
estimation picks up appropriate polar
atoms among candidates, without any
manual tuning of the threshold values.
The experimental results show that the
precision of polarity assignment with
the automatically acquired lexicon was
94% on average, and our method is ro-
bust for corpora in diverse domains and
for the size of the initial lexicon.
1 Introduction
Sentiment Analysis (SA) (Nasukawa and Yi,
2003; Yi et al, 2003) is a task to recognize
writers? feelings as expressed in positive or
negative comments, by analyzing unreadably
large numbers of documents. Extensive syn-
tactic patterns enable us to detect sentiment
expressions and to convert them into seman-
tic structures with high precision, as reported
by Kanayama et al (2004). From the exam-
ple Japanese sentence (1) in the digital cam-
era domain, the SA system extracts a senti-
ment representation as (2), which consists of
a predicate and an argument with positive (+)
polarity.
(1) Kono kamera-ha subarashii-to omou.
?I think this camera is splendid.?
(2) [+] splendid(camera)
SA in general tends to focus on subjec-
tive sentiment expressions, which explicitly de-
scribe an author?s preference as in the above
example (1). Objective (or factual) expres-
sions such as in the following examples (3) and
(4) may be out of scope even though they de-
scribe desirable aspects in a specific domain.
However, when customers or corporate users
use SA system for their commercial activities,
such domain-specific expressions have a more
important role, since they convey strong or
weak points of the product more directly, and
may influence their choice to purchase a spe-
cific product, as an example.
(3) Kontorasuto-ga kukkiri-suru.
?The contrast is sharp.?
(4) Atarashii kishu-ha zuumu-mo tsuite-iru.
?The new model has a zoom lens, too.?
This paper addresses the Japanese ver-
sion of Domain-oriented Sentiment Analysis,
which identifies polar clauses conveying good-
ness and badness in a specific domain, in-
cluding rather objective expressions. Building
domain-dependent lexicons for many domains
is much harder work than preparing domain-
independent lexicons and syntactic patterns,
because the possible lexical entries are too
numerous, and they may differ in each do-
main. To solve this problem, we have devised
an unsupervised method to acquire domain-
dependent lexical knowledge where a user has
only to collect unannotated domain corpora.
The knowledge to be acquired is a domain-
dependent set of polar atoms. A polar atom is
a minimum syntactic structure specifying po-
larity in a predicative expression. For exam-
ple, to detect polar clauses in the sentences (3)
355
and (4)1, the following polar atoms (5) and (6)
should appear in the lexicon:
(5) [+] kukkiri-suru
?to be sharp?
(6) [+] tsuku ? zuumu-ga
?to have ? zoom lens-NOM?
The polar atom (5) specified the positive po-
larity of the verb kukkiri-suru. This atom can
be generally used for this verb regardless of
its arguments. In the polar atom (6), on the
other hand, the nominative case of the verb
tsuku (?have?) is limited to a specific noun zu-
umu (?zoom lens?), since the verb tsuku does
not hold the polarity in itself. The automatic
decision for the scopes of the atoms is one of
the major issues.
For lexical learning from unannotated cor-
pora, our method uses context coherency in
terms of polarity, an assumption that polar
clauses with the same polarity appear suc-
cessively unless the context is changed with
adversative expressions. Exploiting this ten-
dency, we can collect candidate polar atoms
with their tentative polarities as those adja-
cent to the polar clauses which have been
identified by their domain-independent polar
atoms in the initial lexicon. We use both intra-
sentential and inter-sentential contexts to ob-
tain more candidate polar atoms.
Our assumption is intuitively reasonable,
but there are many non-polar (neutral) clauses
adjacent to polar clauses. Errors in sentence
delimitation or syntactic parsing also result in
false candidate atoms. Thus, to adopt a can-
didate polar atom for the new lexicon, some
threshold values for the frequencies or ratios
are required, but they depend on the type of
the corpus, the size of the initial lexicon, etc.
Our algorithm is fully automatic in the
sense that the criteria for the adoption of po-
lar atoms are set automatically by statistical
estimation based on the distributions of co-
herency: coherent precision and coherent den-
sity. No manual tuning process is required,
so the algorithm only needs unannotated do-
main corpora and the initial lexicon. Thus
our learning method can be used not only by
the developers of the system, but also by end-
users. This feature is very helpful for users to
1The English translations are included only for con-
venience.
analyze documents in new domains.
In the next section, we review related work,
and Section 3 describes our runtime SA sys-
tem. In Section 4, our assumption for unsu-
pervised learning, context coherency and its
key metrics, coherent precision and coherent
density are discussed. Section 5 describes our
unsupervised learning method. Experimental
results are shown in Section 6, and we conclude
in Section 7.
2 Related Work
Sentiment analysis has been extensively stud-
ied in recent years. The target of SA in this
paper is wider than in previous work. For ex-
ample, Yu and Hatzivassiloglou (2003) sepa-
rated facts from opinions and assigned polari-
ties only to opinions. In contrast, our system
detects factual polar clauses as well as senti-
ments.
Unsupervised learning for sentiment analy-
sis is also being studied. For example, Hatzi-
vassiloglou and McKeown (1997) labeled ad-
jectives as positive or negative, relying on se-
mantic orientation. Turney (2002) used col-
location with ?excellent? or ?poor? to obtain
positive and negative clues for document clas-
sification. In this paper, we use contextual
information which is wider than for the con-
texts they used, and address the problem of
acquiring lexical entries from the noisy clues.
Inter-sentential contexts as in our approach
were used as a clue also for subjectivity anal-
ysis (Riloff and Wiebe, 2003; Pang and Lee,
2004), which is two-fold classification into sub-
jective and objective sentences. Compared to
it, this paper solves a more difficult problem:
three-fold classification into positive, negative
and non-polar expressions using imperfect co-
herency in terms of sentiment polarity.
Learning methods for phrase-level sentiment
analysis closely share an objective of our ap-
proach. Popescu and Etzioni (2005) achieved
high-precision opinion phrases extraction by
using relaxation labeling. Their method itera-
tively assigns a polarity to a phrase, relying on
semantic orientation of co-occurring words in
specific relations in a sentence, but the scope
of semantic orientation is limited to within a
sentence. Wilson et al (2005) proposed su-
pervised learning, dividing the resources into
356
Document
to analyze -
Sentence
Delimitation ...
?
??
??
Sentences
?Proposition Detection
Propositions
Clauses
?Polarity Assignment
+
?
Polarities
Polar Clauses
Modality
Patterns
Conjunctive
Patterns
*
Polar
Atoms
-
Figure 1: The flow of the clause-level SA.
prior polarity and context polarity, which are
similar to polar atoms and syntactic patterns
in this paper, respectively. Wilson et al pre-
pared prior polarities from existing resources,
and learned the context polarities by using
prior polarities and annotated corpora. There-
fore the prerequisite data and learned data
are opposite from those in our approach. We
took the approach used in this paper because
we want to acquire more domain-dependent
knowledge, and context polarity is easier to
access in Japanese2. Our approach and their
work can complement each other.
3 Methodology of Clause-level SA
As Figure 1 illustrates, the flow of our sen-
timent analysis system involves three steps.
The first step is sentence delimitation: the in-
put document is divided into sentences. The
second step is proposition detection: proposi-
tions which can form polar clauses are identi-
fied in each sentence. The third step is polarity
assignment: the polarity of each proposition
is examined by considering the polar atoms.
This section describes the last two processes,
which are based on a deep sentiment analy-
sis method analogous to machine translation
(Kanayama et al, 2004) (hereafter ?the MT
method?).
3.1 Proposition Detection
Our basic tactic for clause-level SA is the high-
precision detection of polar clauses based on
deep syntactic analysis. ?Clause-level? means
that only predicative verbs and adjectives such
2For example, indirect negation such as caused by
a subject ?nobody? or a modifier ?seldom? is rare in
Japanese.
as in (7) are detected, and adnominal (attribu-
tive) usages of verbs and adjectives as in (8)
are ignored, because utsukushii (?beautiful?) in
(8) does not convey a positive polarity.
(7) E-ga utsukushii.
?The picture is beautiful.?
(8) Utsukushii hito-ni aitai.
?I want to meet a beautiful person.?
Here we use the notion of a proposition as a
clause without modality, led by a predicative
verb or a predicative adjective. The proposi-
tions detected from a sentence are subject to
the assignment of polarities.
Basically, we detect a proposition only at
the head of a syntactic tree3. However, this
limitation reduces the recall of sentiment anal-
ysis to a very low level. In the example (7)
above, utsukushii is the head of the tree, while
those initial clauses in (9) to (11) below are
not. In order to achieve higher recall while
maintaining high precision, we apply two types
of syntactic patterns, modality patterns and
conjunctive patterns4, to the tree structures
from the full-parsing.
(9) Sore-ha utsukushii-to omou.
?I think it is beautiful.?
(10) Sore-ha utsukushiku-nai.
?It is not beautiful.?
(11) Sore-ga utsukushii-to yoi.
?I hope it is beautiful.?
Modality patterns match some auxiliary
verbs or corresponding sentence-final expres-
sions, to allow for specific kinds of modality
and negation. One of the typical patterns is
[ v to omou] (?I think v ?)5, which allows ut-
sukushii in (9) to be a proposition. Also nega-
tion is handled with a modality pattern, such
as [ v nai] (?not v ?). In this case a neg fea-
ture is attached to the proposition to identify
utsukushii in (10) as a negated proposition.
On the other hand, no proposition is identi-
fied in (11) due to the deliberate absence of
a pattern [ v to yoi] (?I hope v ?). We used
a total of 103 domain-independent modality
patterns, most of which are derived from the
3This is same as the rightmost part of the sentence
since all Japanese modification is directed left to right.
4These two types of patterns correspond to auxil-
iary patterns in the MT method, and can be applied
independent of domains.
5 v denotes a verb or an adjective.
357
coordinative (roughly ?and?)
-te, -shi, -ueni, -dakedenaku, -nominarazu
causal (roughly ?because?)
-tame, -kara, -node
adversative (roughly ?but?)
-ga, -kedo, -keredo, - monono, -nodaga
Table 1: Japanese conjunctions used for con-
junctive patterns.
MT method, and some patterns are manually
added for this work to achieve higher recall.
Another type of pattern is conjunctive pat-
terns, which allow multiple propositions in a
sentence. We used a total of 22 conjunctive
patterns also derived from the MT method, as
exemplified in Table 1. In such cases of coordi-
native clauses and causal clauses, both clauses
can be polar clauses. On the other hand, no
proposition is identified in a conditional clause
due to the absence of corresponding conjunc-
tive patterns.
3.2 Polarity Assignment Using Polar
Atoms
To assign a polarity to each proposition, po-
lar atoms in the lexicon are compared to the
proposition. A polar atom consists of po-
larity, verb or adjective, and optionally, its
arguments. Example (12) is a simple polar
atom, where no argument is specified. This
atom matches any proposition whose head is
utsukushii. Example (13) is a complex polar
atom, which assigns a negative polarity to any
proposition whose head is the verb kaku and
where the accusative case is miryoku.
(12) [+] utsukushii
?to be beautiful?
(13) [?] kaku ? miryoku-wo
?to lack ? attraction-ACC?
A polarity is assigned if there exists a polar
atom for which verb/adjective and the argu-
ments coincide with the proposition, and oth-
erwise no polarity is assigned. The opposite
polarity of the polar atom is assigned to a
proposition which has the neg feature.
We used a total of 3,275 polar atoms, most
of which are derived from an English sentiment
lexicon (Yi et al, 2003).
According to the evaluation of the MT
method (Kanayama et al, 2004), high-
precision sentiment analysis had been achieved
using the polar atoms and patterns, where the
splendid
light have-zoom
small-LCD ? satisfied
?
high-price
Iff
?
Inter-sentential
Context
6
6
Intra-sentential
Context
Figure 2: The concept of the intra- and inter-
sentential contexts, where the polarities are
perfectly coherent. The symbol ??? denotes
the existence of an adversative conjunction.
system never took positive sentiment for neg-
ative and vice versa, and judged positive or
negative to neutral expressions in only about
10% cases. However, the recall is too low, and
most of the lexicon is for domain-independent
expressions, and thus we need more lexical en-
tries to grasp the positive and negative aspects
in a specific domain.
4 Context Coherency
This section introduces the intra- and inter-
sentential contexts in which we assume context
coherency for polarity, and describes some pre-
liminary analysis of the assumption.
4.1 Intra-sentential and
Inter-sentential Context
The identification of propositions described
in Section 3.1 clarifies our viewpoint of the
contexts. Here we consider two types of
contexts: intra-sentential context and inter-
sentential context. Figure 2 illustrates the
context coherency in a sample discourse (14),
where the polarities are perfectly coherent.
(14) Kono kamera-ha subarashii-to omou.
?I think this camera is splendid.?
Karui-shi, zuumu-mo tsuite-iru.
?It?s light and has a zoom lens.?
Ekishou-ga chiisai-kedo, manzoku-da.
?Though the LCD is small, I?m satisfied.?
Tada, nedan-ga chotto takai.
?But, the price is a little high.?
The intra-sentential context is the link be-
tween propositions in a sentence, which are
detected as coordinative or causal clauses. If
there is an adversative conjunction such as
-kedo (?but?) in the third sentence in (14), a
flag is attached to the relation, as denoted
with ??? in Figure 2. Though there are dif-
ferences in syntactic phenomena, this is sim-
358
shikashi (?however?), demo (?but?), sorenanoni
(?even though?), tadashi (?on condition that?),
dakedo (?but?), gyakuni (?on the contrary?),
tohaie (?although?), keredomo (?however?),
ippou (?on the other hand?)
Table 2: Inter-sentential adversative expres-
sions.
Domain Post. Sent. Len.
digital cameras 263,934 1,757,917 28.3
movies 163,993 637,054 31.5
mobile phones 155,130 609,072 25.3
cars 159,135 959,831 30.9
Table 3: The corpora from four domains
used in this paper. The ?Post.? and ?Sent.?
columns denote the numbers of postings and
sentences, respectively. ?Len.? is the average
length of sentences (in Japanese characters).
ilar to the semantic orientation proposed by
Hatzivassiloglou and McKeown (1997).
The inter-sentential context is the link be-
tween propositions in the main clauses of pairs
of adjacent sentences in a discourse. The po-
larities are assumed to be the same in the
inter-sentential context, unless there is an ad-
versative expression as those listed in Table 2.
If no proposition is detected as in a nominal
sentence, the context is split. That is, there is
no link between the proposition of the previous
sentence and that of the next sentence.
4.2 Preliminary Study on Context
Coherency
We claim these two types of context can be
used for unsupervised learning as clues to as-
sign a tentative polarity to unknown expres-
sions. To validate our assumption, we con-
ducted preliminary observations using various
corpora.
4.2.1 Corpora
Throughout this paper we used Japanese
corpora from discussion boards in four differ-
ent domains, whose features are shown in Ta-
ble 3. All of the corpora have clues to the
boundaries of postings, so they were suitable
to identify the discourses.
4.2.2 Coherent Precision
How strong is the coherency in the con-
text proposed in Section 4.1? Using the polar
clauses detected by the SA system with the
initial lexicon, we observed the coherent pre-
cision of domain d with lexicon L, defined as:
cp(d, L) = #(Coherent)#(Coherent)+#(Conflict) (15)
where #(Coherent) and #(Conflict) are oc-
currence counts of the same and opposite po-
larities observed between two polar clauses as
observed in the discourse. As the two polar
clauses, we consider the following types:
Window. A polar clause and the nearest po-
lar clause which is found in the preceding
n sentences in the discourse.
Context. Two polar clauses in the intra-
sentential and/or inter-sentential context
described in Section 4.1. This is the view-
point of context in our method.
Table 4 shows the frequencies of coherent
pairs, conflicting pairs, and the coherent pre-
cision for half of the digital camera domain
corpus. ?Baseline? is the percentage of posi-
tive clauses among the polar clauses6.
For the ?Window? method, we tested for
n=0, 1, 2, and ?. ?0? means two propositions
within a sentence. Apparently, the larger the
window size, the smaller the cp value. When
the window size is ???, implying anywhere
within a discourse, the ratio is larger than the
baseline by only 2.7%, and thus these types
of coherency are not reliable even though the
number of clues is relatively large.
?Context? shows the coherency of the two
types of context that we considered. The
cp values are much higher than those in the
?Window? methods, because the relationships
between adjacent pairs of clauses are handled
more appropriately by considering syntactic
trees, adversative conjunctions, etc. The cp
values for inter-sentential and intra-sentential
contexts are almost the same, and thus both
contexts can be used to obtain 2.5 times more
clues for the intra-sentential context. In the
rest of this paper we will use both contexts.
We also observed the coherent precision for
each domain corpus. The results in the cen-
ter column of Table 5 indicate the number
is slightly different among corpora, but all of
them are far from perfect coherency.
6If there is a polar clause whose polarity is unknown,
the polarity is correctly predicted with at least 57.0%
precision by assuming ?positive?.
359
Model Coherent Conflict cp(d, L)
Baseline 57.0%
Window
n = 0 3,428 1,916 64.1%
n = 1 11,448 6,865 62.5%
n = 2 16,231 10,126 61.6%
n = ? 26,365 17,831 59.7%
Context
intra. 2,583 996 72.2%
inter. 3,987 1,533 72.2%
both 6,570 2,529 72.2%
Table 4: Coherent precision with various view-
points of contexts.
Domain cp(d, L) cd(d, L)
digital cameras 72.2% 7.23%
movies 76.7% 18.71%
mobile phones 72.9% 7.31%
cars 73.4% 7.36%
Table 5: Coherent precision and coherent den-
sity for each domain.
4.2.3 Coherent Density
Besides the conflicting cases, there are many
more cases where a polar clause does not ap-
pear in the polar context. We also observed
the coherent density of the domain d with the
lexicon L defined as:
cd(d, L) = #(Coherent)#(Polar) (16)
This indicates the ratio of polar clauses that
appear in the coherent context, among all of
the polar clauses detected by the system.
The right column of Table 5 shows the co-
herent density in each domain. The movie
domain has notably higher coherent density
than the others. This indicates the sentiment
expressions are more frequently used in the
movie domain.
The next section describes the method of
our unsupervised learning using this imperfect
context coherency.
5 Unsupervised Learning for
Acquisition of Polar Atoms
Figure 3 shows the flow of our unsupervised
learning method. First, the runtime SA sys-
tem identifies the polar clauses, and the can-
didate polar atoms are collected. Then, each
candidate atom is validated using the two met-
rics in the previous section, cp and cd, which
are calculated from all of the polar clauses
found in the domain corpus.
Domain
Corpus d
-
Initial
Lexicon L
*
SA
6
Polar
Clauses
context-
?U
Candidate
Polar Atoms
f(a), p(a), n(a)
cd(d, L)
cp(d, L)
?test
6
R?
N ?
? test-
?
- ? - New
Lexicon
Figure 3: The flow of the learning process.
ID Candidate Polar Atom f(a) p(a) n(a)
1* chiisai ?to be small? 3,014 226 227
2 shikkari-suru ?to be firm? 246 54 10
3 chiisai ? bodii-ga 11 4 0?to be small ? body-NOM?
4* todoku ? mokuyou-ni 2 0 2?to be delivered?on Thursday?
Table 6: Examples of candidate polar atoms
and their frequencies. ?*? denotes that it
should not be added to the lexicon. f(a), p(a),
and n(a) denote the frequency of the atom and
in positive and negative contexts, respectively.
5.1 Counts of Candidate Polar Atoms
From each proposition which does not have a
polarity, candidate polar atoms in the form of
simple atoms (just a verb or adjective) or com-
plex atoms (a verb or adjective and its right-
most argument consisting of a pair of a noun
and a postpositional) are extracted. For each
candidate polar atom a, the total appearances
f(a), and the occurrences in positive contexts
p(a) and negative contexts n(a) are counted,
based on the context of the adjacent clauses
(using the method described in Section 4.1).
If the proposition has the neg feature, the po-
larity is inverted. Table 6 shows examples of
candidate polar atoms with their frequencies.
5.2 Determination for Adding to
Lexicon
Among the located candidate polar atoms,
how can we distinguish true polar atoms,
which should be added to the lexicon, from
fake polar atoms, which should be discarded?
As shown in Section 4, both the coherent
precision (72-77%) and the coherent density
(7-19%) are so small that we cannot rely on
each single appearance of the atom in the po-
lar context. One possible approach is to set
the threshold values for frequency in a polar
context, max(p(a), n(a)) and for the ratio of
appearances in polar contexts among the to-
360
tal appearances, max(p(a),n(a))f(a) . However, the
optimum threshold values should depend on
the corpus and the initial lexicon.
In order to set general criteria, here we as-
sume that a true positive polar atom a should
have higher p(a)f(a) than its average i.e. coher-
ent density, cd(d, L+a), and also have higher
p(a)
p(a)+n(a) than its average i.e. coherent preci-
sion, cp(d, L+a) and these criteria should be
met with 90% confidence, where L+a is the
initial lexicon with a added. Assuming the bi-
nomial distribution, a candidate polar atom is
adopted as a positive polar atom7 if both (17)
and (18) are satisfied8.
q > cd(d, L),
where
p(a)?
k=0
f(a)Ckqk(1? q)f(a)?k = 0.9
(17)
r > cp(d, L) or n(a) = 0,
where
p(a)?
k=0
p(a)+n(a)Ckrk(1? r)p(a)+n(a)?k= 0.9
(18)
We can assume cd(d, L+a) ' cd(d, L), and
cp(d, L+a) ' cp(d, L) when L is large. We
compute the confidence interval using approx-
imation with the F-distribution (Blyth, 1986).
These criteria solve the problems in mini-
mum frequency and scope of the polar atoms
simultaneously. In the example of Table 6, the
simple atom chiisai (ID=1) is discarded be-
cause it does not meet (18), while the complex
atom chiisai ? bodii-ga (ID=3) is adopted
as a positive atom. shikkari-suru (ID=2)
is adopted as a positive simple atom, even
though 10 cases out of 64 were observed in the
negative context. On the other hand, todoku
? mokuyou-ni (ID=4) is discarded because it
does not meet (17), even though n(a)f(a) = 1.0,
i.e. always observed in negative contexts.
6 Evaluation
6.1 Evaluation by Polar Atoms
First we propose a method of evaluation of the
lexical learning.
7The criteria for the negative atoms are analogous.
8nCr notation is used here for combination (n
choose k).
Annotator B
Positive Neutral Negative
Anno- Positive 65 11 3
tator Neutral 3 72 0
A Negative 1 4 41
Table 7: Agreement of two annotators? judg-
ments of 200 polar atoms. ?=0.83.
It is costly to make consistent and large
?gold standards? in multiple domains, espe-
cially in identification tasks such as clause-
level SA (cf. classification tasks). Therefore
we evaluated the learning results by asking hu-
man annotators to classify the acquired polar
atoms as positive, negative, and neutral, in-
stead of the instances of polar clauses detected
with the new lexicon. This can be done be-
cause the polar atoms themselves are informa-
tive enough to imply to humans whether the
expressions hold positive or negative meanings
in the domain.
To justify the reliability of this evaluation
method, two annotators9 evaluated 200 ran-
domly selected candidate polar atoms in the
digital camera domain. The agreement results
are shown in Table 7. The manual classifi-
cation was agreed upon in 89% of the cases
and the Kappa value was 0.83, which is high
enough to be considered consistent.
Using manual judgment of the polar atoms,
we evaluated the performance with the follow-
ing three metrics.
Type Precision. The coincidence rate of the
polarity between the acquired polar atom
and the human evaluators? judgments. It
is always false if the evaluators judged it
as ?neutral.?
Token Precision. The coincidence rate of
the polarity, weighted by its frequency in
the corpus. This metric emulates the pre-
cision of the detection of polar clauses
with newly acquired poler atoms, in the
runtime SA system.
Relative Recall. The estimated ratio of the
number of detected polar clauses with the
expanded lexicon to the number of de-
tected polar clauses with the initial lex-
9For each domain, we asked different annotators
who are familiar with the domain. They are not the
authors of this paper.
361
Domain # Type Token RelativePrec. Prec. Recall
digital cameras 708 65% 96.5% 1.28
movies 462 75% 94.4% 1.19
mobile phones 228 54% 92.1% 1.13
cars 487 68% 91.5% 1.18
Table 8: Evaluation results with our method.
The column ?#? denotes the number of polar
atoms acquired in each domain.
icon. Relative recall will be 1 when no
new polar atom is acquired. Since the pre-
cision was high enough, this metric can
be used for approximation of the recall,
which is hard to evaluate in extraction
tasks such as clause-/phrase-level SA.
6.2 Robustness for Different
Conditions
6.2.1 Diversity of Corpora
For each of the four domain corpora, the an-
notators evaluated 100 randomly selected po-
lar atoms which were newly acquired by our
method, to measure the precisions. Relative
recall is estimated by comparing the numbers
of detected polar clauses from randomly se-
lected 2,000 sentences, with and without the
acquired polar atoms. Table 8 shows the re-
sults. The token precision is higher than 90%
in all of the corpora, including the movie do-
main, which is considered to be difficult for SA
(Turney, 2002). This is extremely high preci-
sion for this task, because the correctness of
both the extraction and polarity assignment
was evaluated simultaneously. The relative re-
call 1.28 in the digital camera domain means
the recall is increased from 43%10 to 55%. The
difference was smaller in other domains, but
the domain-dependent polar clauses are much
informative than general ones, thus the high-
precision detection significantly enhances the
system.
To see the effects of our method, we con-
ducted a control experiment which used pre-
set criteria. To adopt the candidate atom a,
the frequency of polarity, max(p(a), n(a)) was
required to be 3 or more, and the ratio of po-
larity, max(p(a),n(a))f(a) was required to be higher
than the threshold ?. Varying ? from 0.05 to
10The human evaluation result for digital camera do-
main (Kanayama et al, 2004).
6
? -
Relative recall
Token
precision
0.5
1
1.0 1.1 1.2
?
? ? = 0.05
? ? = 0.1??
? = 0.3
??
??
?? = 0.8
? ?digital cameras
?
? ? = 0.05?? = 0.1
?
? ? = 0.3?
?
???
? ?movies
(our method)
?Y
Figure 4: Relative recall vs. token precision
with various preset threshold values ? for the
digital camera and movie domains. The right-
most star and circle denote the performance of
our method.
0.8, we evaluated the token precision and the
relative recall in the domains of digital cam-
eras and movies. Figure 4 shows the results.
The results showed both relative recall and
token precision were lower than in our method
for every ?, in both corpora. The optimum ?
was 0.3 in the movie domain and 0.1 in the
digital camera domain. Therefore, in this pre-
set approach, a tuning process is necessary for
each domain. Our method does not require
this tuning, and thus fully automatic learning
was possible.
Unlike the normal precision-recall tradeoff,
the token precision in the movie domain got
lower when the ? is strict. This is due to the
frequent polar atoms which can be acquired
at the low ratios of the polarity. Our method
does not discard these important polar atoms.
6.2.2 Size of the Initial Lexicon
We also tested the performance while vary-
ing the size of the initial lexicon L. We pre-
pared three subsets of the initial lexicon, L0.8,
L0.5, and L0.2, removing polar atoms ran-
domly. These lexicons had 0.8, 0.5, 0.2 times
the polar atoms, respectively, compared to
L. Table 9 shows the precisions and recalls
using these lexicons for the learning process.
Though the cd values vary, the precision was
stable, which means that our method was ro-
bust even for different sizes of the lexicon. The
smaller the initial lexicon, the higher the rela-
tive recall, because the polar atoms which were
removed from L were recovered in the learning
process. This result suggests the possibility of
362
lexicon cd Token Prec. Relative Rec.
L 7.2% 96.5% 1.28
L0.8 6.1% 97.5% 1.41
L0.5 3.9% 94.2% 2.10
L0.2 3.6% 84.8% 3.55
Table 9: Evaluation results for various sizes of
the initial lexicon (the digital camera domain).
the bootstrapping method from a small initial
lexicon.
6.3 Qualitative Evaluation
As seen in the agreement study, the polar
atoms used in our study were intrinsically
meaningful to humans. This is because the
atoms are predicate-argument structures de-
rived from predicative clauses, and thus hu-
mans could imagine the meaning of a polar
atom by generating the corresponding sen-
tence in its predicative form.
In the evaluation process, some interesting
results were observed. For example, a nega-
tive atom nai ? kerare-ga (?to be free from
vignetting?) was acquired in the digital cam-
era domain. Even the evaluator who was fa-
miliar with digital cameras did not know the
term kerare (?vignetting?), but after looking up
the dictionary she labeled it as negative. Our
learning method could pick up such technical
terms and labeled them appropriately.
Also, there were discoveries in the error
analysis. An evaluator assigned positive to aru
? kamera-ga (?to have camera?) in the mobile
phone domain, but the acquired polar atom
had the negative polarity. This was actually
an insight from the recent opinions that many
users want phones without camera functions11.
7 Conclusion
We proposed an unsupervised method to ac-
quire polar atoms for domain-oriented SA, and
demonstrated its high performance. The lex-
icon can be expanded automatically by us-
ing unannotated corpora, and tuning of the
threshold values is not required. Therefore
even end-users can use this approach to im-
prove the sentiment analysis. These features
allow them to do on-demand analysis of more
narrow domains, such as the domain of digital
11Perhaps because cameras tend to consume battery
power and some users don?t need them.
cameras of a specific manufacturer, or the do-
main of mobile phones from the female users?
point of view.
References
C. R. Blyth. 1986. Approximate binomial confi-
dence limits. Journal of the American Statistical
Asscoiation, 81(395):843?855.
Vasileios Hatzivassiloglou and Kathleen R. McKe-
own. 1997. Predicting the semantic orientation
of adjectives. In Proceedings of the 35th ACL
and the 8th EACL, pages 174?181.
Hiroshi Kanayama, Tetsuya Nasukawa, and Hideo
Watanabe. 2004. Deeper sentiment analysis us-
ing machine translation technology. In Proceed-
ings of the 20th COLING, pages 494?500.
Tetsuya Nasukawa and Jeonghee Yi. 2003. Senti-
ment analysis: Capturing favorability using nat-
ural language processing. In Proceedings of the
Second K-CAP, pages 70?77.
Bo Pang and Lillian Lee. 2004. A sentimental
education: Sentiment analysis using subjectiv-
ity summarization based on minimum cuts. In
Proceedings of the 42nd ACL, pages 271?278.
Ana-Maria Popescu and Oren Etzioni. 2005. Ex-
tracting product features and opinions from
reviews. In Proceedings of HLT/EMNLP-05,
pages 339?346.
Ellen Riloff and Janyee Wiebe. 2003. Learning ex-
traction patterns for subjective expressions. In
Proceedings of EMNLP-03, pages 105?112.
Peter D. Turney. 2002. Thumbs up or thumbs
down? Semantic orientation applied to unsuper-
vised classification of reviews. In Proceedings of
the 40th ACL, pages 417?424.
Theresa Wilson, Janyce Wiebe, and Paul Hoff-
mann. 2005. Recognizing contextual polarity in
phrase-level sentiment analysis. In Proceedings
of HLT/EMNLP-05, pages 347?354.
Jeonghee Yi, Tetsuya Nasukawa, Razvan Bunescu,
and Wayne Niblack. 2003. Sentiment analyzer:
Extracting sentiments about a given topic using
natural language processing techniques. In Pro-
ceedings of the Third IEEE International Con-
ference on Data Mining, pages 427?434.
Hong Yu and Vasileios Hatzivassiloglou. 2003. To-
wards answering opinion questions: Separating
facts from opinions and identifying the polarity
of opinion sentences. In Proceedings of EMNLP-
2003, pages 129?136.
363
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 19?27,
Beijing, August 2010
Robust Measurement and Comparison of Context Similarity for Finding
Translation Pairs
Daniel Andrade?, Tetsuya Nasukawa?, Jun?ichi Tsujii?
?Department of Computer Science, University of Tokyo
{daniel.andrade, tsujii}@is.s.u-tokyo.ac.jp
?IBM Research - Tokyo
nasukawa@jp.ibm.com
Abstract
In cross-language information retrieval it
is often important to align words that are
similar in meaning in two corpora writ-
ten in different languages. Previous re-
search shows that using context similar-
ity to align words is helpful when no
dictionary entry is available. We sug-
gest a new method which selects a sub-
set of words (pivot words) associated with
a query and then matches these words
across languages. To detect word associa-
tions, we demonstrate that a new Bayesian
method for estimating Point-wise Mutual
Information provides improved accuracy.
In the second step, matching is done in
a novel way that calculates the chance of
an accidental overlap of pivot words us-
ing the hypergeometric distribution. We
implemented a wide variety of previously
suggested methods. Testing in two con-
ditions, a small comparable corpora pair
and a large but unrelated corpora pair,
both written in disparate languages, we
show that our approach consistently out-
performs the other systems.
1 Introduction
Translating domain-specific, technical terms from
one language to another can be challenging be-
cause they are often not listed in a general dictio-
nary. The problem is exemplified in cross-lingual
information retrieval (Chiao and Zweigenbaum,
2002) restricted to a certain domain. In this case,
the user might enter only a few technical terms.
However, jargons that appear frequently in the
data set but not in general dictionaries, impair the
usefulness of such systems. Therefore, various
means to extract translation pairs automatically
have been proposed. They use different clues,
mainly
? Spelling distance or transliterations, which
are useful to identify loan words (Koehn and
Knight, 2002).
? Context similarity, helpful since two words
with identical meaning are often used in sim-
ilar contexts across languages (Rapp, 1999).
The first type of information is quite specific; it
can only be helpful in a few cases, and can thereby
engender high-precision systems with low recall,
as described for example in (Koehn and Knight,
2002). The latter is more general. It holds for
most words including loan words. Usually the
context of a word is defined by the words which
occur around it (bag-of-words model).
Let us briefly recall the main idea for using
context similarity to find translation pairs. First,
the degree of association between the query word
and all content words is measured with respect to
the corpus at hand. The same is done for every
possible translation candidate in the target cor-
pus. This way, we can create a feature vector
for the query and all its possible translation can-
didates. We can assume that, for some content
words, we have valid translations in a general dic-
tionary, which enables us to compare the vectors
across languages. We will designate these content
words as pivot words. The query and its trans-
lation candidates are then compared using their
feature vectors, where each dimension in the fea-
ture vector contains the degree of association to
19
one pivot word. We define the degree of associa-
tion, as a measurement for finding words that co-
occur, or which do not co-occur, more often than
we would expect by pure chance.1
We argue that common ways for comparing
similarity vectors across different corpora perform
worse because they assume that degree of associa-
tions are very similar across languages and can be
compared without much preprocessing. We there-
fore suggest a new robust method including two
steps. Given a query word, in the first step we
determine the set of pivots that are all positively
associated with statistical significance. In the sec-
ond step, we compare this set of pivots with the set
of pivots extracted for a possible translation can-
didate. For extracting positively associated piv-
ots, we suggest using a new Bayesian method for
estimating the critical Pointwise Mutual Informa-
tion (PMI) value. In the second step, we use a
novel measure to compare the sets of extracted
pivot words which is based on an estimation of
the probability that pivot words overlap by pure
chance. Our approach engenders statistically sig-
nificant improved accuracy for aligning transla-
tion pairs, when compared to a variety of previ-
ously suggested methods. We confirmed our find-
ings using two very different pairs of comparable
corpora for Japanese and English.
In the next section, we review previous related
work. In Section 3 we explain our method in
detail, and argue that it overcomes subtle weak-
nesses of several previous efforts. In Section 4, we
show with a series of cross-lingual experiments
that our method, in some settings, can lead to con-
siderable improvement in accuracy. Subsequently
in Section 4.2, we analyze our method in contrast
to the baseline by giving two examples. We sum-
marize our findings in Section 5.
2 Related Work
Extracting context similarity for nouns and then
matching them across languages to find trans-
lation pairs was pioneered in (Rapp, 1999) and
(Fung, 1998). The work in (Chiao and Zweigen-
baum, 2002), which can be regarded as a varia-
1For example ?car? and ?tire? are expected to have a high
(positive) degree of association, and ?car? and ?apple? is ex-
pected to have a high (negative) degree of association.
tion of (Fung, 1998), uses tf.idf, but suggests to
normalize the term frequency by the maximum
number of co-occurrences of two words in the cor-
pus. All this work is closely related to our work
because they solely consider context similarity,
whereas context is defined using a word window.
The work in (Rapp, 1999; Fung, 1998; Chiao and
Zweigenbaum, 2002) will form the baselines for
our experiments in Section 4.2 This baseline is
also similar to the baseline in (Gaussier et al,
2004), which showed that it can be difficult to beat
such a feature vector approach.
In principle our method is not restricted to how
context is defined; we could also use, for exam-
ple, modifiers and head words, as in (Garera et
al., 2009). Although, we found in a preliminary
experiment that using a dependency parser to dif-
ferentiate between modifiers and head words like
in (Garera et al, 2009), instead of a bag-of-words
model, in our setting, actually decreased accuracy
due to the narrow dependency window. How-
ever, our method could be combined with a back-
translation step, which is expected to improve
translation quality as in (Haghighi et al, 2008),
which performs indirectly a back-translation by
matching all nouns mutually exclusive across cor-
pora. Notably, there also exist promising ap-
proaches which use both types of information,
spelling distance, and context similarity in a joint
framework, see (Haghighi et al, 2008), or (De?jean
et al, 2002) which include knowledge of a the-
saurus. In our work here, we concentrate on the
use of degrees of association as an effective means
to extract word translations.
In this application, to measure association ro-
bustly, often the Log-Likelihood Ratio (LLR)
measurement is suggested (Rapp, 1999; Morin et
al., 2007; Chiao and Zweigenbaum, 2002). The
occurrence of a word in a document is modeled
as a binary random variable. The LLR measure-
ment measures stochastic dependency between
2Notable differences are that we neglected word order, in
contrast to (Rapp, 1999), as it is little useful to compare it
between Japanese and English. Furthermore in contrast to
(Fung, 1998) we use only one translation in the dictionary,
which we select by comparing the relative frequencies. We
also made a second run of the experiments where we man-
ually selected the correct translations for the first half of the
most frequent pivots ? Results did not change significantly.
20
two such random variables (Dunning, 1993), and
is known to be equal to Mutual Information that is
linearly scaled by the size of the corpus (Moore,
2004). This means it is a measure for how much
the occurrence of word A makes the occurrence
of word B more likely, which we term positive
association, and how much the absence of word
A makes the occurrence of word B more likely,
which we term negative association. However, our
experiments show that only positive association is
beneficial for aligning words cross-lingually. In
fact, LLR can still be used for extracting posi-
tive associations by filtering in a pre-processing
step words with possibly negative associations
(Moore, 2005). Nevertheless a problem which
cannot be easily remedied is that confidence es-
timates using LLR are unreliable for small sample
sizes (Moore, 2004). We suggest a more princi-
pled approach that measures from the start only
how much the occurrence of word A makes the
occurrence of word B more likely, which is des-
ignated as Robust PMI.
Another point that is common to (Rapp, 1999;
Morin et al, 2007; Chiao and Zweigenbaum,
2002; Garera et al, 2009; Gaussier et al, 2004)
is that word association is compared in a fine-
grained way, i.e. they compare the degree of asso-
ciation3 with every pivot word, even when it is low
or exceptionally high. They suggest as a compar-
ison measurement Jaccard similarity, Cosine sim-
ilarity, and the L1 (Manhattan) distance.
3 Our Approach
We presume that rather than similarity between
degree (strength of) of associations, the existence
of common word associations is a more reliable
measure for word similarity because the degrees
of association are difficult to compare for the fol-
lowing reasons:
? Small differences in the degree of associa-
tion are not statistically significant
Taking, for example, two sample sets from
3To clarify terminology, where possible, we will try to
distinguish between association and degree of association.
For example word ?car? has the association ?tire?, whereas
the degree of association with ?tire? is a continuous number,
like 5.6.
the same corpus, we will in general measure
different degrees of association.
? Differences in sub-domains / sub-topics
Corpora sharing the same topic can still dif-
fer in sub-topics.
? Differences in style or language
Differences in word usage. 4
Other information that is used in vector ap-
proaches such as that in (Rapp, 1999) is nega-
tive association, although negative association is
less informative than positive. Therefore, if it is
used at all, it should be assigned a much smaller
weight.
Our approach caters to these points, by first de-
ciding whether a pivot word is positively associ-
ated (with statistical significance) or whether it
is not, and then uses solely this information for
finding translation pairs in comparable corpora. It
is divisible into two steps. In the first, we use a
Bayesian estimated PointwiseMutual Information
(PMI) measurement to find the pivots that are pos-
itively associated with a certain word with high
confidence. In the second step, we compare two
words using their associated pivots as features.
The similarity of feature sets is calculated using
pointwise entropy. The words for which feature
sets have high similarity are assumed to be related
in meaning.
3.1 Extracting positively associated words ?
Feature Sets
To measure the degree of positive association be-
tween two words x and y, we suggest the use
of information about how much the occurrence
of word x makes the occurrence of word y more
likely. We express this using Pointwise Mutual
Information (PMI), which is defined as follows:
PMI(x, y) = log p(x, y)p(x) ? p(y) = log
p(x|y)
p(x) .
Therein, p(x) is the probability that word x oc-
curs in a document; p(y) is defined analogously.
Furthermore, p(x, y) is the probability that both
4For example, ?stop? is not the only word to describe the
fact that a car halted.
21
words occur in the same document. A positive as-
sociation is given if p(x|y) > p(x). In related
works that use the PMI (Morin et al, 2007), these
probabilities are simply estimated using relative
frequencies, as
PMI(x, y) = log
f(x,y)
n
f(x)
n
f(y)
n
,
where f(x), f(y) is the document frequency
of word x and word y, and f(x, y) is the co-
occurrence frequency; n is the number of docu-
ments. However, using relative frequencies to es-
timate these probabilities can, for low-frequency
words, produce unreliable estimates for PMI
(Manning and Schu?tze, 2002). It is therefore nec-
essary to determine the uncertainty of PMI esti-
mates. The idea of defining confidence intervals
over PMI values is not new (Johnson, 2001); how-
ever, the problem is that exact calculation is very
computationally expensive if the number of docu-
ments is large, in which case one can approximate
the binomial approximation for example with a
Gaussian, which is, however only justified if n
is large and p, the probability of an occurrence,
is not close to zero (Wilcox, 2009). We suggest
to define a beta distribution over each probabil-
ity of the binary events that word x occurs, i.e.
[x], and analogously [x|y]. It was shown in (Ross,
2003) that a Bayesian estimate for Bernoulli trials
using the beta distribution delivers good credibil-
ity intervals5, importantly, when sample sizes are
small, or when occurrence probabilities are close
to 0. Therefore, we assume that
p(x|y) ? beta(??x|y, ??x|y), p(x) ? beta(??x, ??x)
where the parameters for the two beta distribu-
tions are set to
??x|y = f(x, y) + ?x|y ,
??x|y = f(y) ? f(x, y) + ?x|y , and
??x = f(x) + ?x, ??x = n ? f(x) + ?x .
Prior information related to p(x) and the con-
ditional probability p(x|y) can be incorporated
5In the Bayesian notation we refer here to credibility in-
tervals instead of confidence intervals.
by setting the hyper-parameters of the beta-
distribtutions.6 These can, for example, be
learned from another unrelated corpora pair and
then weighted appropriately by setting ?+ ?. For
our experiments, we use no information beyond
the given corpora pair; the conditional priors are
therefore set equal to the prior for p(x). Even if
we do not know which word x is, we have a notion
about p(x) because Zipf?s law indicates to us that
we should expect it to be small. A crude estima-
tion is therefore the mean word occurrence proba-
bility in our corpus as
? = 1|all words|
?
x?{all words}
f(x)
n .
We give this estimate a total weight of one obser-
vation. That is, we set
? = ? , ? = 1 ? ? .
From a practical perspective, this can be inter-
preted as a smoothing when sample sizes are
small, which is often the case for p(x|y). Because
we assume that p(x|y) and p(x) are random vari-
ables, PMI is consequently also a random variable
that is distributed according to a beta distribution
ratio.7 For our experiments, we apply a general
sampling strategy. We sample p(x|y) and p(x) in-
dependently and then calculate the ratio of times
PMI > 0 to determine P (PMI > 0).8 We will
refer to this method as Robust PMI (RPMI).
Finally we can calculate, for any word x, the set
of pivot words which have most likely a positive
association with word x. We require that this set
be statistically significant: the probability of one
or more words being not a positive association is
smaller than a certain p-value.9
6The hyper-parameters ? and ?, can be intuitively inter-
preted in terms of document frequency. For example ?x is
the number of times we belief the word x occurs, and ?x the
number of times we belief that x does not occur in a corpus.
Analogously ?x|y and ?x|y can be interpreted with respect
to the subset of the corpus where the word y occurs, instead
of the whole corpus. Note however, that ? and ? do not nec-
essarily have to be integers.
7The resulting distribution for the general case of a beta
distribution ratio was derived in (Pham-Gia, 2000). Unfortu-
nately, it involves the calculation of a Gauss hyper-geometric
function that is computationally expensive for large n.
8For experiments, we used 100, 000 samples for each es-
timate of P (PMI > 0).
9We set, for all of our experiments, the p-value to 0.01.
22
As an alternative for determining the probabil-
ity of a positive association using P (PMI > 0),
we calculate LLR and assume that approximately
LLR ? ?2 with one degree of freedom (Dunning,
1993). Furthermore, to ensure that only positive
association counts, we set the probability to zero
if p(x, y) < p(x) ? p(y), where the probabilities
are estimated using relative frequencies (Moore,
2005). We refer to this as LLR(P); lacking this
correction, it is LLR.
3.2 Comparing Word Feature Sets Across
Corpora
So far, we have explained a robust means to ex-
tract the pivot words that have a positive associa-
tion with the query. The next task is to find a sen-
sible way to use these pivots to compare the query
with candidates from the target corpus. A simple
means to match a candidate with a query is to see
how many pivots they have in common, i.e. using
the matching coefficient (Manning and Schu?tze,
2002) to score candidates. This similarity mea-
sure produces a reasonable result, as we will show
in the experiment section; however, in our error
analysis, we found out that this gives a bias to
candidates with higher frequencies, which is ex-
plainable as follows. Assuming that a word A has
a fixed number of pivots that are positively associ-
ated, then depending on the sample size?the doc-
ument frequency in the corpus?not all of these
are statistically significant. Therefore, not all true
positive associations are included in the feature
set to avoid possible noise. If the document fre-
quency increases, then we can extract more sta-
tistically significant positive associations and the
cardinality of the feature set increases. This con-
sequently increases the likelihood of having more
pivots that overlap with pivots from the query?s
feature set. For example, imagine two candidate
words A and B, for which feature sets of both in-
clude the feature set of the query, i.e. a complete
match, howeverA?s feature set is much larger than
B?s feature set. In this case, the information con-
veyed by having a complete match with the query
word?s feature set is lower in the case of A?s fea-
ture set than in case of B?s feature set. Therefore,
we suggest its use as a basis of our similarity mea-
sure, the degree of pointwise entropy of having an
estimate of m matches, as
Information(m, q, c) = ? log(P (matches = m)).
Therein, P (matches = m) is the likelihood that a
candidate word with c pivots has m matches with
the query word, which has q pivots. Letting w be
the total number of pivot words, we can then cal-
culate that the probability that the candidate with
c pivots was selected by chance
P (matches = m) =
( q
m
)
?
(w?q
c?m
)
(w
c
) .
Note that this probability equals a hypergeometric
distribution.10 The smaller P (matches = m) is,
the less likely it is that we obtain m matches by
pure chance. In other words, if P (matches = m)
is very small, m matches are more than we would
expect to occur by pure chance.11
Alternatively, in our experiments, we also con-
sider standard similarity measurements (Manning
and Schu?tze, 2002) such as the Tanimoto coeffi-
cient, which also lowers the score of candidates
that have larger feature sets.
4 Experiments
In our experiments, we specifically examine trans-
lating nouns, mostly technical terms, which occur
in complaints about cars collected by the Japanese
Ministry of Land, Infrastructure, Transport and
Tourism (MLIT)12, and in complaints about cars
collected by the USA National Highway Traffic
Safety Administration (NHTSA)13. We create for
each data collection a corpus for which a doc-
ument corresponds to one car customer report-
ing a certain problem in free text. The com-
plaints are, in general, only a few sentences long.
10` q
m
? is the number of possible combinations of pivots
which the candidate has in common with the query. There-
fore, ` qm
?
?
`w?q
c?m
? is the number of possible different feature
sets that the candidate can have such that it sharesm common
pivots with the query. Furthermore, `wc
? is the total number
of possible feature sets the candidate can have.
11The discussion is simplified here. It can also be that
P (matches = m) is very small, if there are less occur-
rences of m that we would expect to occur by pure chance.
However, this case can be easily identified by looking at the
gradient of P (matches = m).
12http://www.mlit.go.jp/jidosha/carinf/rcl/defects.html
13http://www-odi.nhtsa.dot.gov/downloads/index.cfm
23
To verify whether our results can be generalized
over other pairs of comparable corpora, we ad-
ditionally made experiments using two corpora
extracted from articles of Mainichi Shinbun, a
Japanese newspaper, in 1995 and English articles
from Reuters in 1997. There are two notable dif-
ferences between those two pairs of corpora: the
content is much less comparable, Mainichi re-
ports more national news than world news, and
secondly, Mainichi and Reuters corpora are much
larger than MLIT/NHTSA.14
For both corpora pairs, we extracted a
gold-standard semi-automatically by looking at
Japanese nouns and their translations with docu-
ment frequency of at least 50 for MLIT/NHTSA,
and 100 for Mainichi/Reuters. As a dictionary we
used the Japanese-English dictionary JMDic15.
In general, we preferred domain-specific terms
over very general terms, i.e. for example for
MLIT/NHTSA the noun ?? ?injection? was
preferred over ???? ?installation?. We ex-
tracted 100 noun pairs for MLIT/NHTSA and
Mainichi/Reuters, each. Each Japanese noun
which is listed in the gold-standard forms a query
which is input into our system. The resulting
ranking of the translation candidates is automat-
ically evaluated using the gold-standard. There-
fore, synonyms that are not listed in the gold stan-
dard are not recognized, engendering a conserva-
tive estimation of the translation accuracy. Be-
cause all methods return a ranked list of trans-
lation candidates, the accuracy is measured us-
ing the rank of the translation listed in the gold-
standard.16 The Japanese corpora are prepro-
cessed with MeCab (Kudo et al, 2004); the En-
glish corpora with Stepp Tagger (Tsuruoka et al,
2005) and Lemmatizer (Okazaki et al, 2008). As
a dictionary we use the Japanese-English dictio-
nary JMDic17. In line with related work (Gaussier
et al, 2004), we remove a word pair (Japanese
noun s, English noun t) from the dictionary, if s
occurs in the gold-standard. Afterwards we define
14MLIT/MLIT has each 20,000 documents.
Mainichi/Reuters corpora 75,935 and 148,043 documents,
respectively.
15http://www.csse.monash.edu.au/ jwb/edict doc.html
16In cases for which there are several translations listed for
one word, the rank of the first is used.
17http://www.csse.monash.edu.au/ jwb/edict doc.html
the pivot words by consulting the remaining dic-
tionary.
4.1 Crosslingual Experiment
We compare our approach used for extract-
ing cross-lingual translation pairs against several
baselines. We compare to LLR + Manhattan
(Rapp, 1999) and our variation LLR(P) + Man-
hattan. Additionally, we compare TFIDF(MSO)
+ Cosine, which is the TFIDF measure, whereas
the Term Frequency is normalized using the max-
imal word frequency and the cosine similarity
for comparison suggested in (Fung, 1998). Fur-
thermore, we implemented two variations of this,
TFIDF(MPO) + Cosine and TFIDF(MPO) + Jac-
card coefficient, which were suggested in (Chiao
and Zweigenbaum, 2002). In fact, TFIDF(MPO)
is the TFIDF measure, whereas the Term Fre-
quency is normalized using the maximal word pair
frequency. The results are displayed in Figure 1.
Our approach clearly outperforms all baselines;
notably it has Top 1 accuracy of 0.14 and Top 20
accuracy of 0.55, which is much better than that
for the best baseline, which is 0.11 and 0.44, re-
spectively.
experiment that are similar to those of our cross-
lingual experi ent, we use the same pivot words
and the same gold standard as that used for the
MLIT/NHTSA experiments, for which a pair (A,
translation of A) is changed to (A, A): that is, the
word becomes the translation of itself. The result
of the monolingual experiment in Table 2 shows
that our method performs slightly worse than the
baseline, LLR + Manhattan, i.e. LLR with L1 nor-
malization and L1 distance(Rapp, 1999). Further-
more, LLR(P) + Manhattan using only positive as-
sociations also performs slightly worse.
Top 1 Top 10 Top 20
LLR + Manhattan 0.94 0.99 0.99
LLR(P) + Man attan 0.89 1.0 1.0
RPMI + Entropy 0.79 0.94 0.95
Table 2: Monolingual NHTSA experiment.
In our main experiment, we compare our ap-
proach used for extracting cross-lingual transla-
ti n pairs ag inst seve al baselines. As before,
we compare LLR + Manhattan (Rapp, 1999) and
the variation LLR(P) + Manhattan. Addition-
ally, we compare TFIDF(MSO) + Cosine, which
is the TFIDF measure, whereas the Term Fre-
quency is normalized using the maximal word fre-
quency and the cosine similarity for comparison
suggested in (Fung, 1998). Furthermore, we im-
plemented two variations of this, TFIDF(MPO) +
Cosine and TFIDF(MPO) + Jaccard coefficient,
which were suggested in (Chiao and Zweigen-
baum, 2002). In fact, TFIDF(MPO) is the TFIDF
measure, whereas the Term Frequency is normal-
ized using the maximal word pair frequency.14
The results are displayed in Figure 1. Our ap-
proach clearly outperforms all baselines; notably
it has top 1 accuracy of 0.14 and top 20 accuracy
of 0.55, which is much better than that for the best
baseline, which is 0.11 and 0.44, respectively.
We next leave the proposed framework con-
stant, but change the mode of estimating positive
associations and the way to match feature sets.
As alternatives for estimating the probability that
there is a positive association, we test LLR(P) and
LLR. As alternatives for comparing feature sets,
we investigate the matching coefficient (match-
ing), cosine similarity (cosine), Tanimoto coeffi-
14We tried, like originally suggested, using maximum
count of every occurring word pair, i.e. (content word, con-
tent word), but using maximum of all pairs (content word,
pivot word) improves always slightly accuracy. Therefore for
we chose the latter as a baseline.
?
??
??
??
??
??
??
? ? ?? ?? ?? ??
???????????????????????????????????????????????????
Figure 1: Percentile ranking of our approach
RPMI + Entropy against various previous sug-
gested methods.
cient (tani), and overlap coefficient (over) (Man-
ning and Schu?tze, 2002). The result of every com-
bination is displayed concisely in Table 3 using the
median rank. In our experience, the median rank
is a good choice of measure of location for our
problem because we have, in general, a skewed
distribution over the ranks. The cases in which
the median ranks are close to RPMI + entropy are
magnified in 4.
It is readily apparent that most alternatives per-
form clearly worse. Looking at Table 4, we can
see that only RPMI + Entropy, and LLR(P) +
Entropy, perform similar. Pointwise entropy in-
creases the accuracy (Top 1) over the matching
coefficient and is clearly superior to other similar-
ity measures. Overlap similarity performs well in
contrast to other standard measurements because
other measures punish words with a high number
of associated pivots too severely. However, our
approach of using pointwise entropy as a measure
of similarity performs best because it more ade-
quately punishes words with a high number of as-
sociated pivots. Finally, LLR(P) presents a clear
edge over LLR, which suggests that indeed only
positive associations seem to matter in a cross-
lingual setting.
Entropy Matching Cosine Tani Over
RPMI 13.0 17.0 24.0, 37.5 36.0
LLR(P) 16.0 15.0 22.5 34.0 25.5
LLR 23.5 22.0 27.5 50.5 50.0
Table 3: Evaluation Matrix
Finally, we aim to clarify whether these re-
sults are specific to a certain type of compara-
ble corpora pair or if they hold more generally.
Therefore, we conduct the same experiments us-
ing the very different comparable corpora pair
Mainichi/Reuters. When comparing to the best
Figure 1: Crosslingual Experiment
MLIT/NHTSA ? Percentile Ranking of RPMI
+ Entropy Against Various Previous Suggested
Methods.
We next leave the proposed framework con-
stant, but change the mode of estimating posi-
tive associations and the way to match feature
sets. As alternatives for estimating the proba-
bility that there is a positive association, we test
LLR(P) and LLR. As alternatives for comparing
feature sets, we investigate the matching coef-
ficient (match), cosine similarity (cosine), Tan-
imoto coefficient (tani), and overlap coefficient
24
(over) (Manning and Schu?tze, 2002). The re-
sult of every combination is displayed concisely
in Table 1 using the median rank18. The cases
in which the median ranks are close to RPMI +
Entropy are magnified in Table 2. We can see
there that RPMI + Entropy, and LLR(P) + En-
tropy perform nearly equally. All other combina-
tions perform worse, especially in Top 1 accuracy.
Finally, LLR(P) presents a clear edge over LLR,
which suggests that indeed only positive associa-
tions seem to matter in a cross-lingual setting.
Entropy Match Cosine Tani Over
RPMI 13.0 17.0 24.0 37.5 36.0
LLR(P) 16.0 15.0 22.5 34.0 25.5
LLR 23.5 22.0 27.5 50.5 50.0
Table 1: Crosslingual experiment MLIT/NHTSA
? Evaluation matrix showing the median ranks of
several combinations of association and similarity
measures.
Top 1 Top 10 Top 20
RPMI + Entropy 0.14 0.46 0.55
RPMI + Matching 0.08 0.41 0.57
LLR(P) + Entropy 0.14 0.46 0.55
LLR(P) + Matching 0.08 0.44 0.55
Table 2: Accuracies for crosslingual experiment
MLIT/NHTSA.
Finally we conduct an another experiment using
the corpora pair Mainichi/Reuters which is quite
different from MLIT/NHTSA. When comparing
to the best baselines in Table 3 we see that our
approach again performs best. Furthermore, the
experiments displayed in Table 4 suggest that Ro-
bust PMI and pointwise entropy are better choices
for positive association measurement and similar-
ity measurement, respectively. We can see that
Top 1 Top 10 Top 20
RPMI + Entropy 0.15 0.38 0.46
LLR(P) + Manhattan 0.10 0.26 0.33
TFIDF(MPO) + Cos 0.05 0.12 0.18
Table 3: Accuracies for crosslingual experiment
Mainichi/Reuters ? Comparison to best baselines.
18A median rank of i, means that 50% of the correct trans-
lations have a rank higher than i.
Top 1 Top 10 Top 20
RPMI + Entropy 0.15 0.38 0.46
RPMI + Matching 0.08 0.30 0.35
LLR(P) + Entropy 0.13 0.36 0.47
LLR(P) + Matching 0.08 0.29 0.37
Table 4: Accuracies for crosslingual experiment
Mainichi/Reuters ? Comparison to alternatives.
the overall best baseline turns out to be LLR(P) +
Manhattan. Comparing the rank from each word
from the gold-standard pairwise, we see that our
approach, RPMI + Entropy, is significantly better
than this baseline in MLIT/NHTSA as well as in
Mainichi/Reuters.19
4.2 Analysis
In this section, we provide two representative ex-
amples extracted from the previous experiments
which sheds light into a weakness of the stan-
dard feature vector approach which was used as a
baseline before. The two example queries and the
corresponding responses of LLR(P) + Manhattan
and our approach are listed in Table 5. Further-
more in Table 6 we list the pivot words with the
highest degree of association (here LLR values)
for the query and its correct translation. We can
see that a query and its translation shares some
pivots which are associated with statistical signif-
icance20. However it also illustrates that the ac-
tual LLR value is less insightful and can hardly be
compared across these two corpora.
Let us analyze the two examples in more de-
tail. In Table 6, we see that the first query ??
?gear?21 is highly associated with??? ?shift?.
However, on the English side we see that gear is
most highly associated with the pivot word gear.
Note that here the word gear is also a pivot word
corresponding to the Japanese pivot word ??
?gear (wheel)?.22 Since in English the word gear
(shift) and gear (wheel) is polysemous, the surface
forms are the same leading to a high LLR value of
19Using pairwise test with p-value 0.05.
20Note that for example, an LLR value bigger than 11.0
means the chances that there is no association is smaller than
0.001 using that LLR ? ?2.
21For a Japanese word, we write the English translation
which is appropriate in our context, immediately after it.
22In other words, we have the entry (??, gear) in our
dictionary but not the entry (??, gear). The first pair is
used as a pivot, the latter word pair is what we try to find.
25
gear. Finally, the second example query ???
?pedal? shows that words which, not necessarily
always, but very often co-occur, can cause rela-
tively high LLR values. The Japanese verb ??
?to press? is associated with ??? with a high
LLR value ? 4 times higher than ?? ?return?
? which is not reflected on the English side. In
summary, we can see that in both cases the degree
of associations are rather different, and cannot be
compared without preprocessing. However, it is
also apparent that in both examples a simple L1
normalization of the degree of associations does
not lead to more similarity, since the relative dif-
ferences remain.
?? ?gear?
Method Top 3 candidates Rank
baseline jolt, lever, design 284
filtering reverse, gear, lever 2
??? ?pedal?
Method Top 3 candidates Rank
baseline mj, toyota, action 176
filtering pedal, situation, occasion 1
Table 5: List of translation suggestions using
LLR(P) + Manhattan (baseline) and our method
(filtering). The third column shows the rank of
the correct translation.
?? gear
Pivots LLR(P) Pivots LLR(P)
?? ?shift? 154 gear 7064
??? ?shift? 144 shift 1270
??? ?come out? 116 reverse 314
??? pedal
Pivots LLR(P) Pivots LLR(P)
?? ?press? 628 floor 1150
?? ?return? 175 stop 573
? ?foot? 127 press 235
Table 6: Shows the three pivot words which have
the highest degree of association with the query
(left side) and the correct translation (right side).
5 Conclusions
We introduced a new method to compare con-
text similarity across comparable corpora using a
Bayesian estimate for PMI (Robust PMI) to ex-
tract positive associations and a similarity mea-
surement based on the hypergeometric distribu-
tion (measuring pointwise entropy). Our experi-
ments show that, for finding cross-lingual trans-
lations, the assumption that words with similar
meaning share positive associations with the same
words is more appropriate than the assumption
that the degree of association is similar. Our ap-
proach increases Top 1 and Top 20 accuracy of
up to 50% and 39% respectively, when compared
to several previous methods. We also analyzed
the two components of our method separately. In
general, Robust PMI yields slightly better per-
formance than the popular LLR, and, in contrast
to LLR, allows to extract positive associations as
well as to include prior information in a principled
way. Pointwise entropy for comparing feature sets
cross-lingually improved the translation accuracy
clearly when compared with standard similarity
measurements.
Acknowledgment
We thank Dr. Naoaki Okazaki and the anony-
mous reviewers for their helpful comments. Fur-
thermore we thank Daisuke Takuma, IBM Re-
search - Tokyo, for mentioning previous work
on statistical corrections for PMI. This work was
partially supported by Grant-in-Aid for Specially
Promoted Research (MEXT, Japan). The first au-
thor is supported by the MEXT Scholarship and
by an IBM PhD Scholarship Award.
References
Chiao, Y.C. and P. Zweigenbaum. 2002. Looking
for candidate translational equivalents in special-
ized, comparable corpora. In Proceedings of the In-
ternational Conference on Computational Linguis-
tics, pages 1?5. International Committee on Com-
putational Linguistics.
De?jean, H., E?. Gaussier, and F. Sadat. 2002. An ap-
proach based on multilingual thesauri and model
combination for bilingual lexicon extraction. In
Proceedings of the International Conference on
Computational Linguistics, pages 1?7. International
Committee on Computational Linguistics.
Dunning, T. 1993. Accurate methods for the statistics
of surprise and coincidence. Computational Lin-
guistics, 19(1):61?74.
Fung, P. 1998. A statistical view on bilingual
lexicon extraction: from parallel corpora to non-
parallel corpora. Lecture Notes in Computer Sci-
ence, 1529:1?17.
26
Garera, N., C. Callison-Burch, and D. Yarowsky.
2009. Improving translation lexicon induction from
monolingual corpora via dependency contexts and
part-of-speech equivalences. In Proceedings of the
Conference on Computational Natural Language
Learning, pages 129?137. Association for Compu-
tational Linguistics.
Gaussier, E., J.M. Renders, I. Matveeva, C. Goutte,
and H. Dejean. 2004. A geometric view on bilin-
gual lexicon extraction from comparable corpora.
In Proceedings of the Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 526?
533. Association for Computational Linguistics.
Haghighi, A., P. Liang, T. Berg-Kirkpatrick, and
D. Klein. 2008. Learning bilingual lexicons from
monolingual corpora. In Proceedings of the Annual
Meeting of the Association for Computational Lin-
guistics, pages 771?779. Association for Computa-
tional Linguistics.
Johnson, M. 2001. Trading recall for precision with
confidence-sets. Technical report, Brown Univer-
sity.
Koehn, P. and K. Knight. 2002. Learning a translation
lexicon from monolingual corpora. In Proceedings
of ACL Workshop on Unsupervised Lexical Acquisi-
tion, volume 34, pages 9?16. Association for Com-
putational Linguistics.
Kudo, T., K. Yamamoto, and Y. Matsumoto. 2004.
Applying conditional random fields to Japanese
morphological analysis. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 230?237. Association for Com-
putational Linguistics.
Manning, C.D. and H. Schu?tze. 2002. Foundations
of Statistical Natural Language Processing. MIT
Press.
Moore, R.C. 2004. On log-likelihood-ratios and the
significance of rare events. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 333?340. Association for
Computational Linguistics.
Moore, R.C. 2005. A discriminative framework for
bilingual word alignment. In Proceedings of the
Conference on Human Language Technology and
Empirical Methods in Natural Language Process-
ing, pages 81?88. Association for Computational
Linguistics.
Morin, E., B. Daille, K. Takeuchi, and K. Kageura.
2007. Bilingual terminology mining-using brain,
not brawn comparable corpora. In Proceedings of
the Annual Meeting of the Association for Compu-
tational Linguistics, volume 45, pages 664?671. As-
sociation for Computational Linguistics.
Okazaki, N., Y. Tsuruoka, S. Ananiadou, and J. Tsu-
jii. 2008. A discriminative candidate generator for
string transformations. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 447?456. Association for Com-
putational Linguistics.
Pham-Gia, T. 2000. Distributions of the ratios of in-
dependent beta variables and applications. Com-
munications in Statistics. Theory and Methods,
29(12):2693?2715.
Rapp, R. 1999. Automatic identification of word
translations from unrelated English and German
corpora. In Proceedings of the Annual Meeting
of the Association for Computational Linguistics,
pages 519?526. Association for Computational Lin-
guistics.
Ross, T.D. 2003. Accurate confidence intervals for
binomial proportion and Poisson rate estimation.
Computers in Biology and Medicine, 33(6):509?
531.
Tsuruoka, Y., Y. Tateishi, J. Kim, T. Ohta, J. Mc-
Naught, S. Ananiadou, and J. Tsujii. 2005. De-
veloping a robust part-of-speech tagger for biomed-
ical text. Lecture Notes in Computer Science,
3746:382?392.
Wilcox, R.R. 2009. Basic Statistics: Understanding
Conventional Methods and Modern Insights. Ox-
ford University Press.
27
