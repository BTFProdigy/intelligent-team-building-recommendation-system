Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 630?639,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Phrase Translation Probabilities with ITG Priors
and Smoothing as Learning Objective
Markos Mylonakis
Language and Computation, ILLC
Faculty of Science
University of Amsterdam
m.mylonakis@uva.nl
Khalil Sima?an
Language and Computation, ILLC
Faculty of Science
University of Amsterdam
k.simaan@uva.nl
Abstract
The conditional phrase translation probabil-
ities constitute the principal components of
phrase-based machine translation systems.
These probabilities are estimated using a
heuristic method that does not seem to opti-
mize any reasonable objective function of the
word-aligned, parallel training corpus. Ear-
lier efforts on devising a better understood
estimator either do not scale to reasonably
sized training data, or lead to deteriorating
performance. In this paper we explore a new
approach based on three ingredients (1) A
generative model with a prior over latent
segmentations derived from Inversion Trans-
duction Grammar (ITG), (2) A phrase ta-
ble containing all phrase pairs without length
limit, and (3) Smoothing as learning ob-
jective using a novel Maximum-A-Posteriori
version of Deleted Estimation working with
Expectation-Maximization. Where others
conclude that latent segmentations lead to
overfitting and deteriorating performance,
we show here that these three ingredients
give performance equivalent to the heuristic
method on reasonably sized training data.
1 Motivation
A major component in phrase-based statistical Ma-
chine translation (PBSMT) (Zens et al, 2002;
Koehn et al, 2003) is the table of conditional prob-
abilities of phrase translation pairs. The pervading
method for estimating these probabilities is a sim-
ple heuristic based on the relative frequency of the
phrase pair in the multi-set of the phrase pairs ex-
tracted from the word-aligned corpus (Koehn et al,
2003). While this heuristic estimator gives good em-
pirical results, it does not seem to optimize any intu-
itively reasonable objective function of the (word-
aligned) parallel corpus (see e.g., (DeNero et al,
2006)) The mounting number of efforts attacking
this problem over the last few years (DeNero et al,
2006; Marcu and Wong, 2002; Birch et al, 2006;
Moore and Quirk, 2007; Zhang et al, 2008) exhibits
its difficulty. So far, none has lead to an alternative
method that performs as well as the heuristic on rea-
sonably sized data (approx. 1000k sentence pair).
Given a parallel corpus, an estimator for phrase-
tables in PBSMT involves two interacting decisions
(1) which phrase pairs to extract, and (2) how to as-
sign probabilities to the extracted pairs. The heuris-
tic estimator employs word-alignment (Giza++)
(Och and Ney, 2003) and a few thumb rules for
defining phrase pairs, and then extracts a multi-set
of phrase pairs and estimates their conditional prob-
abilities based on the counts in the multi-set. Us-
ing this method for extracting a set of phrase pairs,
(DeNero et al, 2006; Moore and Quirk, 2007) aim
at defining a better estimator for the probabilities.
Generally speaking, both efforts report deteriorating
translation performance relative to the heuristic.
Instead of employing word-alignment to guide
phrase pair extraction, it is theoretically more ap-
pealing to aim at phrase alignment as part of the esti-
mation process (Marcu and Wong, 2002; Birch et al,
2006). This way, phrase pair extraction goes hand-
in-hand with estimating the probabilities. How-
ever, in practice, due to the huge number of possi-
ble phrase pairs, this task is rather challenging, both
computationally and statistically. It is hard to define
630
both a manageable phrase pair translation model and
a well-founded training regime that would scale up
to reasonably sized parallel corpora (see e.g., (Birch
et al, 2006)). It remains to be seen whether this the-
oretically interesting approach will lead to improved
phrase probability estimates.
In this paper we also start out from a stan-
dard phrase extraction procedure based on word-
alignment and aim solely at estimating the condi-
tional probabilities for the phrase pairs and their
reverse translation probabilities. Unlike preceding
work, we extract all phrase pairs from the training
corpus and estimate their probabilities, i.e., without
limit on length. We present a novel formulation of
a conditional translation model that works with a
prior over segmentations and a bag of conditional
phrase pairs. We use binary Synchronous Context-
Free Grammar (bSCFG), based on Inversion Trans-
duction Grammar (ITG) (Wu, 1997; Chiang, 2005a),
to define the set of eligible segmentations for an
aligned sentence pair. We also show how the num-
ber of spurious derivations per segmentation in this
bSCFG can be used for devising a prior probabil-
ity over the space of segmentations, capturing the
bias in the data towards monotone translation. The
heart of the estimation process is a new smoothing
estimator, a penalized version of Deleted Estima-
tion, which averages the temporary probability es-
timates of multiple parallel EM processes at each
joint iteration.
For evaluation we use a state-of-the-art baseline
system (Moses) (Hoang and Koehn, 2008) which
works with a log-linear interpolation of feature func-
tions optimized by MERT (Och, 2003). We sim-
ply substitute our own estimates for the heuristic
phrase translation estimates (both directions and the
phrase penalty score) and compare the two within
the Moses decoder. While our estimates differ sub-
stantially from the heuristic, their performance is on
par with the heuristic estimates. This is remark-
able given the fact that comparable previous work
(DeNero et al, 2006; Moore and Quirk, 2007) did
not match the performance of the heuristic estima-
tor using large training sets. We find that smooth-
ing is crucial for achieving good estimates. This
is in line with earlier work on consistent estimation
for similar models (Zollmann and Sima?an, 2006),
and agrees with the most up-to-date work that em-
ploys Bayesian priors over the estimates (Zhang et
al., 2008).
2 Related work
Marcu and Wong (Marcu and Wong, 2002) realize
that the problem of extracting phrase pairs should
be intertwined with the method of probability esti-
mation. They formulate a joint phrase-based model
in which a source-target sentence pair is generated
jointly. However, the huge number of possible
phrase-alignments prohibits scaling up the estima-
tion by Expectation-Maximization (EM) (Dempster
et al, 1977) to large corpora. Birch et al(Birch et
al., 2006) provide soft measures for including word-
alignments in the estimation process and obtain im-
proved results only on small data sets.
Coming up-to-date, (Blunsom et al, 2008) at-
tempt a related estimation problem to (Marcu and
Wong, 2002), using the expanded phrase pair set
of (Chiang, 2005a), working with an exponential
model and concentrating on marginalizing out the
latent segmentation variable. Also most up-to-date,
(Zhang et al, 2008) report on a multi-stage model,
without a latent segmentation variable, but with a
strong prior preferring sparse estimates embedded in
a Variational Bayes (VB) estimator and concentrat-
ing the efforts on pruning both the space of phrase
pairs and the space of (ITG) analyses. The latter two
efforts report improved performance, albeit again on
a limited training set (approx. 140k sentences up to
a certain length).
DeNero et al(2006) have explored estimation us-
ing EM of phrase pair probabilities under a con-
ditional translation model based on the original
source-channel formulation. This model involves a
hidden segmentation variable that is set uniformly
(or to prefer shorter phrases over longer ones). Fur-
thermore, the model involves a reordering compo-
nent akin to the one used in IBM model 3. De-
spite this, the heuristic estimator remains superior
because ?EM learns overly determinized segmen-
tations and translation parameters, overfitting the
training data and failing to generalize?. More re-
cently, (Moore and Quirk, 2007) devise a estimator
working with a model that does not include a hid-
den segmentation variable but works with a heuris-
tic iterative procedure (rather than MLE or EM). The
631
translation results remain inferior to the heuristic but
the authors note an interesting trade-off between de-
coding speed and the various settings of this estima-
tor.
Our work expands on the general approach taken
by (DeNero et al, 2006; Moore and Quirk, 2007)
but arrives at insights similar to those of the most
recent work (Zhang et al, 2006), albeit in a com-
pletely different manner. The present work differs
from all preceding work in that it employs the set
of all phrase pairs during training. It differs from
(Zhang et al, 2008) in that it does postulate a la-
tent segmentation variable and puts the prior di-
rectly over that variable rather than over the ITG
synchronous rule estimates. Our method neither
excludes phrase pairs before estimation nor does it
prune the space of possible segmentations/analyses
during training/estimation. As well as smoothing,
we find (in the same vein as (Zhang et al, 2008))
that setting effective priors/smoothing is crucial for
EM to arrive at better estimates.
3 The Translation Model
Given a word-aligned parallel corpus of source-
target sentences, it is common practice to extract a
set of phrase pairs using extraction heuristics (cf.
(Koehn et al, 2003; Och and Ney, 2004)). These
heuristics define a phrase pair to consist of a source
and target ngrams of a word-aligned source-target
sentence pair such that if one end of an alignment
is in the one ngram, the other end is in the other
ngram (and there is at least one such alignment)
(Och and Ney, 2004; Koehn et al, 2003). For ef-
ficiency and sparseness, the practitioners of PBSMT
constrain the length of the source phrase to a certain
maximum number of words.
An All Phrase Pairs Model: In this work we train
a phrase-translation table that consists of all phrase-
pairs that can be extracted from the word-aligned
training data according to the standard phrase ex-
traction heuristic. After training, we can still limit
the set of phrase pairs to those selected by a cut-off
on phrase length. The reason for using all phrase
pairs during training is that it gives a clear point of
reference for an estimator, without implicit, acciden-
tal biases that might emerge due to length cut-off1.
The Generative Model: Given a word-aligned
source-target sentence pair ?f , e,a?, the generative
story underlying our model goes as follows:
1. Abiding by the word-alignments in a, segment
the source-target sentence pair ?f , e? into a se-
quence of I containers ?I1 , and a bag of I
phrase pairs ?I1(f , e) = {?fj , ej?}Ij=1. Each
container ?j = ?lf , rf , le, re? consists of the
start lf and end rf positions2 for a phrase in
f and the start le and end re positions for an
aligned phrase in e.
2. For a given segmentation ?I1 , for every con-
tainer ?j (1 ? j ? I) generate the phrase-pair
?fj, ej?, independently from all other phrase-
pairs.
This leads to the following probabilistic model:
P (f | e;a) =
?
?I1??(a)
P (?I1)
?
?fj ,ej???I1 (f ,e)
P (fj | ej) (1)
Where ?(a) is the set of binarizable segmenta-
tions (defined next) that are eligible according to the
word-alignments a between f and e. These segmen-
tations into bilingual containers (where segmenta-
tions are taken inside the containers) are different
from the monolingual segmentations used in earlier
comparable conditional models (e.g., (DeNero et al,
2006)) which must generate the alignment on top of
the segmentations. Note how the different phrase
pairs ?fj, ej? are generated from their bilingual con-
tainers in the given segmentation ?I1 . We will dis-
cuss our choice of prior probability over segmenta-
tions P (?I1) after we discuss the definition of the bi-
narizable segmentations ?(a).
3.1 Binarizable segmentations ?(a)
Following (Zhang et al, 2006; Huang et al, 2008),
every sequence of phrase alignments can be viewed
1For example, if the cut-off on phrase pairs is ten words, all
sentence pairs smaller than ten words in the training data will
be included as phrase pairs as well. These sentences are treated
differently from longer sentences, which are not allowed to be
phrase pairs.
2The NULL alignments (word-to-NULL) in the training
data can also be marked with actual positions on both sides in
order to allow for this definition of containers.
632
as a sequence of integers 1, . . . I together with a
permuted version of this sequence pi(1), . . . , pi(I),
where the two copies of an integer in the two se-
quences are assumed aligned/paired together. For
example, possible permutations of {1, 2, 3, 4} are
{2, 1, 3, 4} and {2, 4, 1, 3}. Because a segmenta-
tion ?I1 of a sentence pair is also a sequence of
aligned phrases, it also constitutes a permuted se-
quence. A binarizable permutation x is either of
length one, or can be properly split into two binariz-
able sub-sequences y and z such that either3 z < y
or y < z. For example, one way to binarize the
permutation {2, 1, 3, 4} is to introduce a proper split
into {2, 1; 3, 4}, then recursively another proper split
of {2, 1} into {2; 1} and {3, 4} into {3; 4}. In con-
trast, the permutation {2, 4, 1, 3} is non-binarizable.
<>
<>
2 1
[]
3 4
[]
[]
<>
2 1
3
4
Figure 1: Multiple ways to binarize a permutation
Graphically speaking, the recursive definition of
binarizable permutations can be depicted as a bi-
nary tree structure where the nodes correspond to
recursive proper splits of the permutation, and the
leaves are decorated with the naturals. Figure 1 ex-
hibits two possible binarizations of the same permu-
tation where <> and [] denote inverted and mono-
tone proper splits respectively. Note that the number
of possible binarizations of a binarizable permuta-
tion is a recursive function of the number of possi-
ble proper splits and reaches its maximum for fully
monotone permutations (all binary trees, which is a
factorial function of the length of the permutation).
By definition (cf. (Zhang et al, 2006; Huang et
al., 2008)), a binarizable segmentation/permutation
can be recognized by a binarized Synchronous
Context-Free Grammar (SCFG), i.e., an SCFG in
which the right hand sides of all non-lexical rules
constitute binarizable permutations. In particular,
this holds for the SCFG implementing Inversion
3For two sequences of numbers, the notation y < z stands
for ?y ? y,?z ? z : y < z.
Transduction Grammar (Wu, 1997). This SCFG
(Chiang, 2005b) has two binary synchronous rules
that correspond resp. to the contiguous monotone
and inverted alignments:
XP ? XP 1 XP 2 , XP 1 XP 2 (2)
XP ? XP 1 XP 2 , XP 2 XP 1
The boxed integers in the superscripts on the non-
terminal XP denote synchronized rewritings. In
this work, we employ a binary SCFG (bSCFG)
working with these two synchronous rules to-
gether with a set of lexical rules {XP ?
f, e | ?f, e? is a phrase pair}.
In this bSCFG, every derivation corresponds to a
binarization of a segmentation of the input. Note
that the bSCFG defined in equation 2 generates all
possible binarizations for every segmentation of the
input. It is possible to constrain this bSCFG such
that it generates a single, canonical derivation per
segmentation. However, in section 3.2 we show that
the number of such derivations is a good measure of
phrase pair productivity.
It is well known that there are alignments and
segmentations that this bSCFG does not cover (see
(Huang et al, 2008)). Recently, strong evidence
emerged (e.g., (Huang et al, 2008)) showing that
most word-alignments of actual parallel corpora can
be covered by a binarized SCFG of the ITG type.
Furthermore, because our model employs the set of
all phrase-pairs that can be extracted from a given
training set, it will always find segmentations that
cover every sentence pair in the training data4. This
implies that while our model might discard non-
binarizable segmentations for certain complex word
alignments, we do manage to train the model on the
binarizable segementations of all sentence pairs.
Up to the prior over segmentations (see next), we
implement the above model using a weighted ver-
sion of the binary SCFG as follows:
? The weight for lexical rules is given by
P (XP ? f, e) := P (f | e), where ?f, e? is
a phrase-pair. These are the trainable parame-
ters of our model.
4In the worst case the whole sentence pair is a phrase pair
with a trivial segmentation.
633
11
5
5
1
1
5
52 3 4
3 4 23 4 2
2 43
Figure 2: Two segmentations of an align-
ment/permutation. Both segmentations have the
same number of binarizations despite differences in
container sizes.
? The weights for the two non-lexical rules in
equation 2 are fixed at 1.0. These weights are
not trained at all.
Where we use the notation P (.) for the weight of a
synchronous rule.
3.2 Prior over segmentations
As it has been found out by (DeNero et al, 2006),
it is not easy to come up with a simple, effec-
tive prior distribution over segmentations that al-
lows for improved phrase pair estimates. Within a
Maximum-Likelihood estimator, preference for seg-
mentations ?I1 consisting of longer containers could
lead to overfitting as we will explain in section 4.
Alternatively, it is tempting to have preference for
segmentations ?I1 that consist of shorter contain-
ers, because (generally speaking) shorter contain-
ers have higher expected coverage of new sentence
pairs. However, mere bias for shorter containers
will not give better estimates as observed by (DeN-
ero et al, 2006). One case where this bias clearly
fails is the case of a contiguous sequence of con-
tainers with a complex alignment structure (cross-
ing alignments). For example (see figure 2), for
the alignment {1, 3, 4, 2, 5} there is a segmentation
into five containers {1; 3; 4; 2; 5}, and another into
three {1; 3, 4, 2; 5}. The first segmentation involves
shorter containers that have crossing brackets among
them, while the second one consists of three con-
tainers including a longer container {3, 4, 2}. In
the first segmentation, due to their crossing align-
ments, each of the containers {3}, {4} and {2} will
not combine with the surrounding context ({1} and
{5}) on its own, i.e., without the other two contain-
ers. Furthermore, there is only a single binariza-
tion of {3, 4, 2}. Hence, while the first segmen-
tation involves shorter containers than the second
one, these shorter containers are as productive as
the large container {3, 4, 2}, i.e., they combine with
surrounding containers in the same number of ways
as the large container. In such and similar cases,
there are no grounds for the bias towards shorter
phrases/containers.
The notion of container productivity (the num-
ber of ways in which it combines with surrounding
containers during training) seems to correlate with
the expected number of ways a container can be
used during decoding, which should be correlated
with expected coverage. During training, contain-
ers that are often surrounded by other, monotoni-
cally aligned containers are expected to be more pro-
ductive than alternative containers that are often sur-
rounded by crossing alignments. Hence, the num-
ber of binarizations that a segmentation has under
the bSCFG is a direct function of the ways in which
the containers combine among themselves (mono-
tone vs. inverted/crossing) within segmentations,
and provides a more accurate measure of container
productivity than container length. Hence, the final
model we employ is the following:
P (f | e;a) =
?
?I1??(a)
N(?I1)
Z(?(a))
?
?fj ,ej???I1(f ,e)
P (fj | ej) (3)
Where N(?I1) is the number of binary deriva-
tions/trees that ?I1 has in the binary SCFG (bSCFG),
and Z(?(a)) = ??J1 ??(a) N(?
J
1 ), i.e., this prior is
the ratio of number of derivations of ?I1 to the to-
tal number of derivations that ?f , e,a? has under the
bSCFG.
3.3 Contrast with similar models:
In contrast with the model of (DeNero et al, 2006),
who define the segmentations over the source sen-
tence f alone, our model employs bilingual con-
tainers thereby segmenting both source and target
sides simultaneously. Therefore, unlike (DeNero
et al, 2006), our model does not need to gener-
ate the word-alignments explicitly, as these are em-
bedded in the segmentations. Similarly, our model
does not include explicit penalty terms for reorder-
634
ing/inversion but includes a related bias in the prior
probabilities over segmentations P (?I1).
In a way, the segmentations and bilingual contain-
ers we use can be viewed as similar to the concepts
used in the Joint Model of Marcu and Wong (Marcu
and Wong, 2002). Unlike (Marcu and Wong, 2002),
however, our model works with conditional proba-
bilities and starts out from the word-alignments.
The novel aspects of our model are three (1) It de-
fines the set of segmentations using a bSCFG, (2) It
includes a novel, refined prior probability over seg-
mentations, and (3) It employs all phrase pairs that
can be extracted from a word-aligned training par-
allel corpus. For these novel elements to produce
reasonable estimates, we devise our own estimator.
4 Estimation by Smoothing
In principle, we are dealing here with a translation
model that employs all phrase pairs (of unbounded
size), extracted from a word-aligned parallel cor-
pus. Under this model, where a phrase pair and
its sub-phrase pairs are included in the model, the
MLE can be expected to overfit the data5 unless a
suitable prior probability over segmentations is em-
ployed. Indeed, the prior over segmentations defined
in the preceding section prevents the MLE from
completely overfitting the training data. However,
we find empirical evidence that this prior is insuffi-
cient for avoiding overfitting.
Our model behaves like a memory-based model
because it memorizes all extractable phrase pairs
found in the training data including the training sen-
tence pairs themselves. Such memory-based mod-
els are related to nonparametric models such as
K-NN and kernel methods (Hastie et al, 2001).
For memory-based models, consistent estimation for
novel instances proceeds by local density estimation
from the surroundings of the instance, which is akin
to smoothing for parametric models. Hence, next we
describe our own version of a smoothed Maximum-
Likelihood estimator for phrase translation probabil-
5One trivial MLE solution would give the longest container,
consisting of the longest phrase pairs, a probability of one, at
the cost of all shorter alternatives. A similar problem arises in
Data-Oriented Parsing, see (Sima?an and Buratto, 2003; Zoll-
mann and Sima?an, 2006). Note that models that employ an
upperbound on phrase pair length will still risk overfitting train-
ing sentences of lengths that fall within this upperbound.
???????????????????-
INPUT: Word-aligned parallel training data T
OUTPUT: Estimates pi for all P (f | e)
{
Split training data T into equal parts H1, . . . ,H10.
For 1 ? i ? 10 do
Extract from Ei = ?j 6=iHj all phrase pairs pii
Initialize p?i0i to uniform conditional probs
Let j = 0
Repeat
Let j = j + 1 // EM iteration counter
For 1 ? i ? 10 do
E-step: calculate expected counts for pairs
in piji on Hi using counts from p?i
j?1
i .
M-step: calculate probabilities for pairs in
piji from the expected counts
For 1 ? i ? 10 do p?iji := 110
?10
i=1 piji
Until pi := {p?ij1, . . . , p?i
j
10} has converged
}
???????????????????-
Figure 3: Penalized Deleted Estimation
ities.
For a latent variable model, it is usually common
to employ Expectation-Maximization (EM) (Demp-
ster et al, 1977) as a search method for a (local)
maximum-likelihood estimate (MLE) of the train-
ing data. Instead of mere EM we opt for a smoothed
version: we present a new method, that combines
Deleted Estimation (Jelinek and Mercer, 1980) with
the Jackknife (Duda et al, 2001) as the core estima-
tor.
Figure 3 shows the pseudo-code for our estima-
tor. Like in Deleted Estimation, we split the training
data into ten equal portions. This way we create ten
different splits of extraction/heldout sets of respec-
tively 90%/10% of the training set. For every split
1 ? i ? 10, we extract a set of phrase pairs pii from
the extraction set Ei and train it (under our model)
on the heldout set Hi. Naturally, the phrase pair sets
pii (1 ? i ? 10) are subsets of (or equal to) the set
of phrase pairs pi = ?ipii extracted from the total
training data (i.e., pi is the set of model parameters).
The training of the different pii?s, each on its corre-
sponding heldout set Hi, is done by ten separate EM
processes, which are synchronized in their initializa-
635
tion, their iterations as well as stop condition. The
EM processes start out from uniform conditional es-
timates of the phrase pairs in all pii. After every EM
iteration j, when the M-step has finished, the esti-
mates in all piji (1 ? i ? 10) are set to the average
(over 1 ? i ? 10) of the estimates in piji leading to
p?iji (following the Jackknife method). The resulting
averaged probabilities in p?iji are then used as the cur-
rent phrase pair estimates, which feed into the next
iteration j + 1 of the different EM processes (each
working on a different heldout set Hi with a differ-
ent set of phrase pairs pii).
There are two special boundary cases which de-
mand special attention during estimation:
Sparse distributions: For a phrase e that does oc-
cur both in Hi and Ei, there could be a phrase
pair ?f, e? that does occur in Hi but does not
occur in pii. To prevent EM from giving the
extra probability mass to all other pairs ?f, e??
unjustifiably, we apply smoothing. We add the
missing pair ?f, e? to pii and set its probability
to a fixed number 10?5?len, where len is the
length of the phrase pair. In effect, we backoff
our model (equation 1) to a word-level model
with fixed word translation probability (10?5).
Zero distributions: When a phrase e does not oc-
cur in Hi, all its pairs ?f, e? in pii will have
zero counts. During each EM iteration, when
the M-step is applied, the distribution P (? | e)
is undefined by MLE, since it is irrelevant for
the likelihood of Hi. In this case any choice
of proper distribution P (? | e) will constitute an
MLE solution. We choose to set this case to a
uniform distribution every time again.
Since our model and estimator are implemented
within the bSCFG framework, we use a bilingual
CYK parser (Younger, 1967) under the grammar
in equation 2. This parser builds for every input
?f ,a, e? all binarizations/derivations for every seg-
mentation in ?(a). For implementing EM, we em-
ploy the Inside-Outside algorithm (Lari and Young,
1990; Goodman, 1998). During estimation, because
the input, output and word-alignment are known
in advance, the time and space requirements re-
main manageable despite the worst-case complexity
O(n6) in target sentence length n.
Penalized Deleted Estimation: In contrast with
our method, Deleted Estimation sums the expected
counts (rather than probabilities) obtained from
the different splits before applying the M-step
(normalization). While the rationale behind Deleted
Estimation comes from MLE over the original
training data, our method has a smoothing objective
(inspired by the Jackknife ): generally speaking, the
averages over different heldout sets (under different
subsets of the model) give less sharp estimates than
MLE. By averaging the different heldout estimates,
this estimator employs a penalty term that depends
on the marginal count of e in the heldout set6.
Interestingly, when the phrase e is very frequent7,
it will approximately occur almost as often in the
different heldout sets. In this case, our method
reduces to Deleted Estimation, where it effectively
sums the counts8. Yet, when the target phrase e
does occur only very few times, it is likely that its
count in some splits will be zero. In our method, at
every EM iteration, during the Maximization step,
we set such cases back to uniform. By averaging the
probabilities from the different splits over many EM
iterations, setting these cases to uniform constitutes
a kind of prior that prevents the final estimates
from falling too far from uniform. In contrast, in
Deleted Interpolation the zero counts are simply
summed with the other corresponding counts of the
same phrase pair, which leads to sharper probability
distributions. In all experiments that we conducted,
our method (which we call Penalized Deleted
Estimation) gave more successful estimates than
mere Deleted Estimation.
On the theoretical side, the choice for a fixed
6Define county(x) to be the count of event x
in data y. The Deleted Estimation (DE) estimate is?
H countH (f, e)/countT (e), which can be written as?
H [countH (f, e)/countH (e)][countH(e)/countT (e)] =?
H piH(f |e)[countH (e)/countT (e)] where piH(f |e) is the
estimate from heldout set H . Hence, DE linearly interpolated
piH with factors countH (e)/countT (e). Our estimator em-
ploys uniform interpolation factors instead, thereby penalizing
the DI counts (hence Penalized DI).
7Theoretically speaking, when the training data is unbound-
edly large, our estimator will converge to the same estimates
as the Deleted Estimation. When the data is still sparse, our
estimator is biased, unlike the MLE which will overfit.
8When calculating the conditional probabilities, the denom-
inators used are approximately equal to one another.
636
prior over segmentations (ITG prior) implies that our
model cannot be estimated to converge (in proba-
bility) to the relative frequency estimates (RFE) of
source-target sentence pairs in the limit of the train-
ing data (a sufficiently large parallel corpus). A prior
probability over segmentations that would allow our
estimator to converge in the limit to the RFE must
gradually prefer segmentations consisting of larger
containers as the data grows large. We set the de-
sign and estimation of such a prior aside for future
work.
5 Empirical experiments
Decoding and Baseline Model: In this work
we employ an existing decoder, Moses (Hoang
and Koehn, 2008), which defines a log-linear
model interpolating feature functions, with interpo-
lation scores ?f e? = argmaxe
?
f?? ?fHf (f , e).
The ?f are optimized by Minimum-Error Training
(MERT) (Och, 2003). The set ? consists of the
following feature functions (see (Hoang and Koehn,
2008)): a 5-gram target language model, the stan-
dard reordering scores, the word and phrase penalty
scores, the conditional lexical estimates obtained
from the word-alignment in both directions, and the
conditional phrase translation estimates in both di-
rections P (f | e) and P (e | f). Keeping the other
five feature functions fixed, we compare our esti-
mates of P (f | e) and P (e | f) (and the phrase
penalty) to the commonly used heuristic estimates.
Because our model employs a latent segmenta-
tion variable, this variable should be marginalized
out during decoding to allow selecting the highest
probability translation given the input. This turns
out crucial for improved results (cf. (Blunsom et al,
2008)). However, such a marginalization can be NP-
Complete, in analogy to a similar problem in Data-
Oriented Parsing (Sima?an, 2002)9. We do not have
a decoder yet that can approximate this marginaliza-
tion efficiently and we employ the standard Moses
decoder for this work.
Experimental Setup: The training, development
and test data all come from the French-English
translation shared task of the ACL 2007 Second
9A reduction of simple instances of the first problem to in-
stances of the latter problem should be possible.
Phrases System BLEU
? 7 Baseline PBSMT 33.03
? 10 Baseline PBSMT 33.03
All Baseline PBSMT 33.00
? 7 EM + ITG Prior 32.50
? 7 EM + Del. Est. 32.67
? 7 EM + Del. Est. + ITG Prior 32.73
? 7 EM + Pen. Del. Est. + ITG Prior 33.02
? 10 EM + Pen. Del. Est. + ITG Prior 33.14
All EM + Pen. Del. Est. + ITG Prior 32.98
Table 1: Results: data from ACL07 2nd Wkshp on SMT
Workshop on Statistical Machine Translation 10. Af-
ter pruning sentence pairs with word length more
than 40 on either side, we are left with 949K sen-
tence pairs for training. The development and test
data are composed of 2K sentence pairs each. All
data sets are lower-cased.
For both the baseline system and our method,
we produce word-level alignments for the parallel
training corpus using GIZA++. We use 5 iterations
of each IBM Model 1 and HMM alignment mod-
els, followed by 3 iterations of each Model 3 and
Model 4. From this aligned training corpus, we ex-
tract the phrase pairs according to the heuristics in
(Koehn et al, 2003). The baseline system extracts
all phrase-pairs upto a certain maximum length on
both sides and employs the heuristic estimator. The
language model used in all systems is a 5-gram lan-
guage model trained on the English side of the paral-
lel corpus. Minimum-Error Rate Training (MERT)
is applied on the development set to obtain opti-
mal log-linear interpolation weights for all systems.
Performance is measured by computing the BLEU
scores (Papineni et al, 2002) of the system?s trans-
lations, when compared against a single reference
translation per sentence.
Results: We compare different versions of our
system against the baseline system using the heuris-
tic estimator. We observe the effects of the ITG prior
in the translation model as well as the method of es-
timation (Deleted Estimation vs. Penalized Deleted
Estimation).
Table 1 exhibits the BLEU scores for the sys-
10http://www.statmt.org/wmt07
637
tems. Our own system (with ITG prior and Pe-
nalized Deleted Estimation and maximum phrase-
length ten words) scores (33.14), slightly outper-
forming the best baseline system (33.03). When us-
ing straight Deleted Estimation over EM, this leads
to deterioration (32.73). When also the ITG prior is
excluded (by having a single derivation per segmen-
tation) this leads to further deterioration (32.67). By
using mere EM with an ITG prior, performance goes
down to 32.50, exhibiting the crucial role of the es-
timation by smoothing. Clearly, Penalized Deleted
Estimation and the ITG prior are important for the
improved phrase translation estimates.
As table 1 shows we also varied the phrase length
cutoff (seven, ten or none=all phrase pairs). The
length cutoff pertains to both sides of a phrase-pair.
For our estimator, we always train all phrase pairs,
applying the length cutoff only after training (no re-
normalization is applied at that point).
Interestingly, we find out that the heuristic estima-
tor cannot benefit performance by including longer
phrase pairs. Our estimator does benefit perfor-
mance by including phrase pairs of length upto ten
words, but then it degrades again when including
all phrase pairs. We take the latter finding to sig-
nal remaining overfitting that proved resistant to the
smoothing applied by our estimator. The heuristic
estimator exhibits a similar degradation.
We also tried to vary the treatment of Sparse Dis-
tributions (section 4, page 7) during heldout estima-
tion from fixed word-translation probabilities to the
lexical model probabilities. This lead to slight dete-
rioration of results (32.94). It is unclear whether this
deterioration is meaningful or not. We did not ex-
plore mere EM without any smoothing or ITG prior,
as we expect it will directly overfit the training data
as reported by (DeNero et al, 2006).
We note that for French-English translation it is
hard to outperform the heuristic within the PBSMT
framework, since it already performs very well.
Preliminary, most recent experiments on German-
English (also WMT07 data) exhibit that our estima-
tor outperforms the heuristic.
6 Discussion and Future Research
The most similar efforts to ours, mainly (DeNero
et al, 2006), conclude that segmentation variables
in the generative translation model lead to overfit-
ting while attaining higher likelihood of the train-
ing data than the heuristic estimator. Based on this
advise (Moore and Quirk, 2007) exclude the latent
segmentation variables and opt for a heuristic train-
ing procedure. In this work we also start out from a
generative model with latent segmentation variables.
However, we find out that concentrating the learning
effort on smoothing is crucial for good performance.
For this, we devise ITG-based priors over segmenta-
tions and employ a penalized version of Deleted Es-
timation working with EM at its core. The fact that
our results (at least) match the heuristic estimates on
a reasonably sized data set (947k parallel sentence
pairs) is rather encouraging.
The work in (Zhang et al, 2008) has a simi-
lar flavor to our work, yet the two differ substan-
tially. Both depart from Maximum-Likelihood to-
wards non-overfitting estimators. Where Zhang et al
choose for sparse priors (leading to sharp phrase dis-
tributions) and put the smoothing burden on the ITG
rule parameters and a pruning strategy, we choose
for a prior over segmentations determined by the
ITG derivation space and smooth the MLE directly
with a penalized version of Deleted Estimation. It
remains to be seen how the two biases compare to
one another on the same task.
There are various strands of future research.
Firstly, we plan to explore our estimator on other
language pairs in order to obtain more evidence on
its behavior. Secondly, as (Blunsom et al, 2008)
show, marginalizing out the different segmentations
during decoding leads to improved performance. We
plan to build our own decoder (based on ITG) where
different ideas can be tested including tractable ways
for achieving a marginalization effect. Apart from a
new decoder, it will be worthwhile adapting the prior
probability in our model to allow for consistent es-
timation. Finally, it would be interesting to study
properties of the penalized Deleted Estimation used
in this paper.
Acknowledgments: Both authors are supported
by a VIDI grant (nr. 639.022.604) from The Nether-
lands Organization for Scientific Research (NWO).
David Chiang and Andy Way are acknowledged for
stimulating discussions on machine translation and
parsing.
638
References
A. Birch, Ch. Callison-Burch, M. Osborne, and Ph.
Koehn. 2006. Constraining the phrase-based, joint
probability statistical translation model. In Proceed-
ings on the Workshop on Statistical Machine Trans-
lation, pages 154?157. Association for Computational
Linguistics.
P. Blunsom, T. Cohn, and M. Osborne. 2008. A discrim-
inative latent variable model for statistical machine
translation. In Proceedings of ACL-08: HLT, pages
200?208. Association for Computational Linguistics.
D. Chiang. 2005a. A hierarchical phrase-based model
for statistical machine translation. In In Proceedings
of ACL 2005, pages 263?270.
D. Chiang. 2005b. An introduction to synchronous
grammars. Technical report, Univeristy of Maryland.
A.P. Dempster, N.M. Laird, and D.B. Rubin. 1977. Max-
imum likelihood from incomplete data via the em al-
gorithm. Journal of the Royal Statistical Society, Se-
ries B, 39(1):1?38.
J. DeNero, D. Gillick, J. Zhang, and D. Klein. 2006.
Why generative phrase models underperform surface
heuristics. In Proceedings on the Workshop on Sta-
tistical Machine Translation, pages 31?38, New York
City. Association for Computational Linguistics.
R.O. Duda, P.E. Hart, and D.G. Stork. 2001. Pattern
Classification. John Wiley & Sons, NY, USA.
J.T. Goodman. 1998. Parsing Inside-Out. PhD thesis,
Departement of Computer Science, Harvard Univer-
sity, Cambridge, Massachusetts.
T. Hastie, R. Tibshirani, and J. H. Friedman. 2001. The
Elements of Statistical Learning. Springer.
H. Hoang and Ph. Koehn. 2008. Design of the moses de-
coder for statistical machine translation. In ACL Work-
shop on Software engineering, testing, and quality as-
surance for NLP 2008.
L. Huang, H. Zhang, D. Gildea, and K. Knight.
2008. Binarization of synchronous context-free
grammars. Submitted to Computational Linguistics.
http://www.cis.upenn.edu/ lhuang3/opt.pdf.
F. Jelinek and R. L. Mercer. 1980. Interpolated estima-
tion of markov source parameters from sparse data. In
In Proceedings of the Workshop on Pattern Recogni-
tion in Practice.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In HLT-NAACL.
K. Lari and S.J. Young. 1990. The estimation of stochas-
tic context-free grammars using the inside-outside al-
gorithm. Computer, Speech and Language, 4:35?56.
D. Marcu and W. Wong. 2002. A phrase-based, joint
probability model for statistical machine translation.
In Proceedings of Empirical methods in natural lan-
guage processing, pages 133?139. Association for
Computational Linguistics.
R. Moore and Ch. Quirk. 2007. An iteratively-trained
segmentation-free phrase translation model for statisti-
cal machine translation. In Proceedings of the Second
Workshop on Statistical Machine Translation, pages
112?119, Prague, Czech Republic. Association for
Computational Linguistics.
F. J. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1):19?51.
F. J. Och and H. Ney. 2004. The alignment template
approach to statistical machine translation. Computa-
tional Linguistics, 30(4):417?449.
F. J. Och. 2003. Minimum error rate training in statistical
machine translation. In ACL, pages 160?167.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. In ACL, pages 311?318.
K. Sima?an and L. Buratto. 2003. Backoff Parame-
ter Estimation for the DOP Model. In H. Blockeel
N. Lavra ?C, D. Gamberger and L. Todorovski, editors,
Proceedings of the 14th European Conference on Ma-
chine Learning (ECML?03), Lecture Notes in Artifi-
cial Intelligence (LNAI 2837), pages 373?384, Cavtat-
Dubrovnik, Croatia. Springer.
K. Sima?an. 2002. Computational complexity of proba-
bilistic disambiguation. Grammars, 5(2):125?151.
D. Wu. 1997. Stochastic inversion transduction gram-
mars and bilingual parsing of parallel corpora. Com-
putational Linguistics, 23(3):377?403.
D.H. Younger. 1967. Recognition and parsing of
context-free languages in time n3. Information and
Control, 10(2):189?208.
R. Zens, F. J. Och, and H. Ney. 2002. Phrase-based sta-
tistical machine translation. In Matthias Jarke, Jana
Koehler, and Gerhard Lakemeyer, editors, KI 2002:
Advances in Artificial Intelligence, 25th Annual Ger-
man Conference on AI (KI 2002), volume 2479 of
Lecture Notes in Computer Science, pages 18?32.
Springer.
H. Zhang, L. Huang, D. Gildea, and K. Knight. 2006.
Synchronous binarization for machine translation. In
HLT-NAACL.
H. Zhang, Ch. Quirk, R. C. Moore, and D. Gildea.
2008. Bayesian learning of non-compositional phrases
with synchronous parsing. In Proceedings of ACL-08:
HLT, pages 97?105, Columbus, Ohio, June. Associa-
tion for Computational Linguistics.
A. Zollmann and K. Sima?an. 2006. An efficient and
consistent estimator for data-oriented parsing. Journal
of Automata, Languages and Combinatorics (JALC),
10 (2005) Number 2/3:367?388.
639
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 642?652,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Learning Hierarchical Translation Structure with Linguistic Annotations
Markos Mylonakis
ILLC
University of Amsterdam
m.mylonakis@uva.nl
Khalil Sima?an
ILLC
University of Amsterdam
k.simaan@uva.nl
Abstract
While it is generally accepted that many trans-
lation phenomena are correlated with linguis-
tic structures, employing linguistic syntax for
translation has proven a highly non-trivial
task. The key assumption behind many ap-
proaches is that translation is guided by the
source and/or target language parse, employ-
ing rules extracted from the parse tree or
performing tree transformations. These ap-
proaches enforce strict constraints and might
overlook important translation phenomena
that cross linguistic constituents. We propose
a novel flexible modelling approach to intro-
duce linguistic information of varying gran-
ularity from the source side. Our method
induces joint probability synchronous gram-
mars and estimates their parameters, by select-
ing and weighing together linguistically moti-
vated rules according to an objective function
directly targeting generalisation over future
data. We obtain statistically significant im-
provements across 4 different language pairs
with English as source, mounting up to +1.92
BLEU for Chinese as target.
1 Introduction
Recent advances in Statistical Machine Translation
(SMT) are widely centred around two concepts:
(a) hierarchical translation processes, frequently
employing Synchronous Context Free Grammars
(SCFGs) and (b) transduction or synchronous
rewrite processes over a linguistic syntactic tree.
SCFGs in the form of the Inversion-Transduction
Grammar (ITG) were first introduced by (Wu, 1997)
as a formalism to recursively describe the trans-
lation process. The Hiero system (Chiang, 2005)
utilised an ITG-flavour which focused on hierarchi-
cal phrase-pairs to capture context-driven translation
and reordering patterns with ?gaps?, offering com-
petitive performance particularly for language pairs
with extensive reordering. As Hiero uses a single
non-terminal and concentrates on overcoming trans-
lation lexicon sparsity, it barely explores the recur-
sive nature of translation past the lexical level. Nev-
ertheless, the successful employment of SCFGs for
phrase-based SMT brought translation models as-
suming latent syntactic structure to the spotlight.
Simultaneously, mounting efforts have been di-
rected towards SMT models employing linguistic
syntax on the source side (Yamada and Knight,
2001; Quirk et al, 2005; Liu et al, 2006), target
side (Galley et al, 2004; Galley et al, 2006) or both
(Zhang et al, 2008; Liu et al, 2009; Chiang, 2010).
Hierarchical translation was combined with target
side linguistic annotation in (Zollmann and Venu-
gopal, 2006). Interestingly, early on (Koehn et al,
2003) exemplified the difficulties of integrating lin-
guistic information in translation systems. Syntax-
based MT often suffers from inadequate constraints
in the translation rules extracted, or from striving to
combine these rules together towards a full deriva-
tion. Recent research tries to address these issues,
by re-structuring training data parse trees to bet-
ter suit syntax-based SMT training (Wang et al,
2010), or by moving from linguistically motivated
synchronous grammars to systems where linguistic
plausibility of the translation is assessed through ad-
ditional features in a phrase-based system (Venu-
gopal et al, 2009; Chiang et al, 2009), obscuring
the impact of higher level syntactic processes.
While it is assumed that linguistic structure does
correlate with some translation phenomena, in this
642
work we do not employ it as the backbone of trans-
lation. In place of linguistically constrained trans-
lation imposing syntactic parse structure, we opt for
linguistically motivated translation. We learn latent
hierarchical structure, taking advantage of linguistic
annotations but shaped and trained for translation.
We start by labelling each phrase-pair span in the
word-aligned training data with multiple linguisti-
cally motivated categories, offering multi-grained
abstractions from its lexical content. These phrase-
pair label charts are the input of our learning al-
gorithm, which extracts the linguistically motivated
rules and estimates the probabilities for a stochastic
SCFG, without arbitrary constraints such as phrase
or span sizes. Estimating such grammars under
a Maximum Likelihood criterion is known to be
plagued by strong overfitting leading to degener-
ate estimates (DeNero et al, 2006). In contrast,
our learning objective not only avoids overfitting
the training data but, most importantly, learns joint
stochastic synchronous grammars which directly
aim at generalisation towards yet unseen instances.
By advancing from structures which mimic lin-
guistic syntax, to learning linguistically aware latent
recursive structures targeting translation, we achieve
significant improvements in translation quality for 4
different language pairs in comparison with a strong
hierarchical translation baseline.
Our key contributions are presented in the fol-
lowing sections. Section 2 discusses the weak in-
dependence assumptions of SCFGs and introduces
a joint translation model which addresses these is-
sues and separates hierarchical translation structure
from phrase-pair emission. In section 3 we consider
a chart over phrase-pair spans filled with source-
language linguistically motivated labels. We show
how we can employ this crucial input to extract and
train a hierarchical translation structure model with
millions of rules. Section 4 demonstrates decoding
with the model by constraining derivations to lin-
guistic hints of the source sentence and presents our
empirical results. We close with a discussion of re-
lated work and our conclusions.
2 Joint Translation Model
Our model is based on a probabilistic Synchronous
CFG (Wu, 1997; Chiang, 2005). SCFGs define a
SBAR ? [WHNP SBAR\WHNP] (a)
SBAR\WHNP ? ?VP/NPL NPR? (b)
NPR ? [NP PP] (c)
WHNP ? WHNPP (d)
WHNPP ? which / der (e)
VP/NPL ? VP/NPLP (f)
VP/NPLP ? is / ist (g)
NPR ? NPRP (h)
NPRP ? the solution / die Lo?sung (i)
NP ? NPP (j)
NPP ? the solution / die Lo?sung (k)
PP ? PPP (l)
PPP ? to the problem / fu?r das Problem (m)
Figure 1: English-German SCFG rules for the relative
clause(s) ?which is the solution (to the problem) / der die
Lo?sung (fu?r das Problem) ist?, [ ] signify monotone trans-
lation, ? ? a swap reordering.
language over string pairs, which are generated be-
ginning from a start symbol S and recursively ex-
panding pairs of linked non-terminals across the two
strings using the grammar?s rule set. By crossing the
links between the non-terminals of the two sides re-
ordering phenomena are captured. We employ bi-
nary SCFGs, i.e. grammars with a maximum of two
non-terminals on the right-hand side. Also, for this
work we only used grammars with either purely lexi-
cal or purely abstract rules involving one or two non-
terminal pairs. An example can be seen in Figure 1,
using an ITG-style notation and assuming the same
non-terminal labels for both sides.
We utilise probabilistic SCFGs, where each rule
is assigned a conditional probability of expanding
the left-hand side symbol with the rule?s right-hand
side. Phrase-pairs are emitted jointly and the over-
all probabilistic SCFG is a joint model over parallel
strings.
2.1 SCFG Reordering Weaknesses
An interesting feature of all probabilistic SCFGs
(i.e. not only binary ones), which has received sur-
prisingly little attention, is that the reordering pat-
643
tern between the non-terminal pairs (or in the case
of ITGs the choice between monotone and swap ex-
pansion) are not conditioned on any other part of a
derivation. The result is that, the reordering pattern
with the highest probability will always be preferred
(e.g. in the Viterbi derivation) over the rest, irre-
spective of lexical or abstract context. As an ex-
ample, a probabilistic SCFG will always assign a
higher probability to derivations swapping or mono-
tonically translating nouns and adjectives between
English and French, only depending on which of the
two rules NP ? [NN JJ ], NP ? ?NN JJ?
has a higher probability. The rest of the (sometimes
thousands of) rule-specific features usually added to
SCFG translation models do not directly help either,
leaving reordering decisions disconnected from the
rest of the derivation.
While in a decoder this is somehow mitigated by
the use of a language model, we believe that the
weakness of straightforward applications of SCFGs
to model reordering structure at the sentence level
misses a chance to learn this crucial part of the
translation process during grammar induction. As
(Mylonakis and Sima?an, 2010) note, ?plain? SCFGs
seem to perform worse than the grammars described
next, mainly due to wrong long-range reordering de-
cisions for which the language model can hardly
help.
2.2 Hierarchical Reordering SCFG
We address the weaknesses mentioned above by re-
lying on an SCFG grammar design that is similar to
the ?Lexicalised Reordering? grammar of (Mylon-
akis and Sima?an, 2010). As in the rules of Fig-
ure 1, we separate non-terminals according to the
reordering patterns in which they participate. Non-
terminals such as BL, CR take part only in swap-
ping right-hand sides ?BL CR? (with BL swap-
ping from the source side?s left to the target side?s
right, CR swapping in the opposite direction), while
non-terminals such as B, C take part solely in mono-
tone right-hand side expansions [B C]. These non-
terminal categories can appear also on the left-hand
side of a rule, as in rule (c) of Figure 1.
In contrast with (Mylonakis and Sima?an, 2010),
monotone and swapping non-terminals do not emit
phrase-pairs themselves. Rather, each non-terminal
NT is expanded to a dedicated phrase-pair emit-
A ? [B C] A ? ?BL CR?
AL ? [B C] AL ? ?BL CR?
AR ? [B C] AR ? ?BL CR?
A ? AP AP ? ? / ?
AL ? ALP A
L
P ? ? / ?
AR ? ARP A
R
P ? ? / ?
Figure 2: Recursive Reordering Grammar rule cate-
gories; A, B, C non-terminals; ?, ? source and target
strings respectively.
ting non-terminal NTP, which generates all phrase-
pairs for it and nothing more. In this way, the pref-
erence of non-terminals to either expand towards
a (long) phrase-pair or be further analysed recur-
sively is explicitly modelled. Furthermore, this set
of pre-terminals allows us to separate the higher or-
der translation structure from the process that emits
phrase-pairs, a feature we employ next.
In (Mylonakis and Sima?an, 2010) this grammar
design mainly contributed to model lexical reorder-
ing preferences. While we retain this function, for
the rich linguistically-motivated grammars used in
this work this design effectively propagates reorder-
ing preferences above and below the current rule ap-
plication (e.g. Figure 1, rules (a)-(c)), allowing to
learn and apply complex reordering patterns.
The different types of grammar rules are sum-
marised in abstract form in Figure 2. We will subse-
quently refer to this grammar structure as Hierarchi-
cal Reordering SCFG (HR-SCFG).
2.3 Generative Model
We arrive at a probabilistic SCFG model which
jointly generates source e and target f strings, by
augmenting each grammar rule with a probability,
summing up to one for every left-hand side. The
probability of a derivation D of tuple ?e, f? begin-
ning from start symbol S is equal to the product of
the probabilities of the rules used to recursively gen-
erate it.
We separate the structural part of the derivation
D, down to the pre-terminals NTP, from the phrase-
emission part. The grammar rules pertaining to the
644
X, SBAR, WHNP+VP, WHNP+VBZ+NP
X, VBZ+NP, VP, SBAR\WHNP
X, SBAR/NN, WHNP+VBZ+DT
X, VBZ+DT, VP/NN
X, WHNP+VBZ, X, NP,
SBAR/NP VP\VBZ
X, WHNP, X, VBZ, X, DT, X, NN,
SBAR/VP VP/NP NP/NN NP\DT
which is the problem
Figure 3: The label chart for the source fragment ?which
is the problem?. Only a sample of the entries is listed.
structural part and their associated probabilities de-
fine a model p(?) over the latent variable ? de-
termining the recursive, reordering and phrase-pair
segmenting structure of translation, as in Figure 4.
Given ?, the phrase-pair emission part merely gener-
ates the phrase-pairs utilising distributions from ev-
ery NTP to the phrase-pairs that it covers, thereby
defining a model over all sentence-pairs generated
given each translation structure. The probabilities of
a derivation and of a sentence-pair are then as fol-
lows:
p(D) =p(?)p(e, f |?) (1)
p(e, f) =
?
D:D
?
??e,f?
p(D) (2)
By splitting the joint model in a hierarchical struc-
ture model and a lexical emission one we facilitate
estimating the two models separately. The following
section discusses this.
3 Learning Translation Structure
3.1 Phrase-Pair Label Chart
The input to our learning algorithm is a word-
aligned parallel corpus. We consider as phrase-
pair spans those that obey the word-alignment con-
straints of (Koehn et al, 2003). For every train-
ing sentence-pair, we also input a chart containing
one or more labels for every synchronous span, such
as that of Figure 3. Each label describes differ-
ent properties of the phrase pair (syntactic, semantic
etc.), possibly in relation to its context, or supply-
ing varying levels of abstraction (phrase-pair, deter-
miner with noun, noun-phrase, sentence etc.). We
aim to induce a recursive translation structure ex-
plaining the joint generation of the source and target
sentence taking advantage of these phrase-pair span
labels.
For this work we employ the linguistically mo-
tivated labels of (Zollmann and Venugopal, 2006),
albeit for the source language. Given a parse of the
source sentence, each span is assigned the following
kind of labels:
Phrase-Pair All phrase-pairs are assigned the X
label
Constituent Source phrase is a constituent A
Concatenation of Constituents Source phrase la-
belled A+B as a concatenation of constituents A and
B, similarly for 3 constituents.
Partial Constituents Categorial grammar (Bar-
Hillel, 1953) inspired labels A/B, A\B, indicating
a partial constituent A missing constituent B right or
left respectively.
An important point is that we assign all applica-
ble labels to every span. In this way, each label set
captures the features of the source side?s parse-tree
without being bounded by the actual parse structure,
as well as provides a coarse to fine-grained view of
the source phrase.
3.2 Grammar Extraction
From every word-aligned sentence-pair and its la-
bel chart, we extract SCFG rules as those of Figure
2. Binary rules are extracted from adjoining syn-
chronous spans up to the whole sentence-pair level,
with the non-terminals of both left and right-hand
side derived from the label names plus their reorder-
ing function (monotone, left/right swapping) in the
span examined. A single unary rule per non-terminal
NT generates the phrase-pair emitting NTP. Unary
rules NTP ? ? / ? generating the phrase-pair are
created for all the labels covering it.
While we label the phrase-pairs similarly to (Zoll-
mann and Venugopal, 2006), the extracted grammar
is rather different. We do not employ rules that are
grounded to lexical context (?gap? rules), relying in-
stead on the reordering-aware non-terminal set and
related unary and binary rules. The result is a gram-
mar which can both capture a rich array of trans-
lation phenomena based on linguistic and lexical
grounds and explicitly model the balance between
645
SBAR
WHNP
WHNPP
which
der
< SBAR\WHNP >
VP/NPL
VP/NPLP
is
ist
NPR
NP
NPP
the solution
die Lo?sung
PP
PPP
to the problem
fu?r das Problem
Figure 4: A derivation of a sentence fragment with the
grammar of Figure 1.
memorising long phrase-pairs and generalising over
yet unseen ones, as shown in the next example.
The derivation in Figure 4 illustrates some of the
formalism?s features. A preference to reorder based
on lexical content is applied for is / ist. Noun phrase
NPR is recursively constructed with a preference to
constitute the right branch of an order swapping non-
terminal expansion. This is matched with VP/NPL
which reorders in the opposite direction. The labels
VP/NP and SBAR\WHNP allow linguistic syntax
context to influence the lexical and reordering trans-
lation choices. Crucially, all these lexical, attach-
ment and reordering preferences (as encoded in the
model?s rules and probabilities) must be matched to-
gether to arrive at the analysis in Figure 4.
3.3 Parameter Estimation
We estimate the parameters for the phrase-emission
model p(e, f |?) using Relative Frequency Estima-
tion (RFE) on the label charts induced for the train-
ing sentence-pairs, after the labels have been aug-
mented by the reordering indications. In the RFE
estimate, every rule NTP ? ? / ? receives a prob-
ability in proportion with the times that ? / ? was
covered by the NT label.
On the other hand, estimating the parameters un-
der Maximum-Likelihood Estimation (MLE) for the
latent translation structure model p(?) is bound to
overfit towards memorising whole sentence-pairs as
discussed in (Mylonakis and Sima?an, 2010), with
the resulting grammar estimate not being able to
generalise past the training data. However, apart
from overfitting towards long phrase-pairs, a gram-
mar with millions of structural rules is also liable to
overfit towards degenerate latent structures which,
while fitting the training data well, have limited ap-
plicability to unseen sentences.
We avoid both pitfalls by estimating the grammar
probabilities with the Cross-Validating Expectation-
Maximization algorithm (CV-EM) (Mylonakis and
Sima?an, 2008; Mylonakis and Sima?an, 2010). CV-
EM is a cross-validating instance of the well known
EM algorithm (Dempster et al, 1977). It works it-
eratively on a partition of the training data, climb-
ing the likelihood of the training data while cross-
validating the latent variable values, considering for
every training data point only those which can be
produced by models built from the rest of the data
excluding the current part. As a result, the estima-
tion process simulates maximising future data likeli-
hood, using the training data to directly aim towards
strong generalisation of the estimate.
For our probabilistic SCFG-based translation
structure variable ?, implementing CV-EM boils
down to a synchronous version of the Inside-Outside
algorithm, modified to enforce the CV criterion. In
this way we arrive at cross-validated ML estimate of
the ? parameters while keeping the phrase-emission
parameters of p(e, f |?) fixed. The CV-criterion,
apart from avoiding overfitting, results in discarding
the structural rules which are only found in a single
part of the training corpus, leading to a more com-
pact grammar while still retaining millions of struc-
tural rules that are more hopeful to generalise.
Unravelling the joint generative process, by mod-
elling latent hierarchical structure separately from
phrase-pair emission, allows us to concentrate our
inference efforts towards the hidden, higher-level
translation mechanism.
4 Experiments
4.1 Decoding Model
The induced joint translation model can be used
to recover argmaxe p(e|f), as it is equal to
argmaxe p(e, f). We employ the induced proba-
bilistic HR-SCFGG as the backbone of a log-linear,
feature based translation model, with the derivation
probability p(D) under the grammar estimate being
646
one of the features. This is augmented with a small
number n of additional smoothing features ?i for
derivation rules r: (a) conditional phrase translation
probabilities, (b) lexical phrase translation probabil-
ities, (c) word generation penalty, and (d) a count
of swapping reordering operations. Features (a), (b)
and (c) are applicable to phrase-pair emission rules
and features for both translation directions are used,
while (d) is only triggered by structural rules.
These extra features assess translation quality past
the synchronous grammar derivation and learning
general reordering or word emission preferences
for the language pair. As an example, while our
probabilistic HR-SCFG maintains a separate joint
phrase-pair emission distribution per non-terminal,
the smoothing features (a) above assess the condi-
tional translation of surface phrases irrespective of
any notion of recursive translation structure.
The final feature is the language model score
for the target sentence, mounting up to the follow-
ing model used at decoding time, with the feature
weights ? trained by Minimum Error Rate Training
(MERT) (Och, 2003) on a development corpus.
p(D
?
? ?e, f?) ? p(e)?lmpG(D)
?G
n?
i=1
?
r?D
?i(r)
?i
4.2 Decoding Modifications
We use a customised version of the Joshua SCFG
decoder (Li et al, 2009) to translate, with the fol-
lowing modifications:
Source Labels Constraints As for this work the
phrase-pair labels used to extract the grammar are
based on the linguistic analysis of the source side,
we can construct the label chart for every input sen-
tence from its parse. We subsequently use it to con-
sider only derivations with synchronous spans which
are covered by non-terminals matching one of the
labels for those spans. This applies both for the non-
terminals covering phrase-pairs as well as the higher
level parts of the derivation.
In this manner we not only constrain the trans-
lation hypotheses resulting in faster decoding time,
but, more importantly, we may ground the hypothe-
ses more closely to the available linguistic informa-
tion of the source sentence. This is of particular
interest as we move up the derivation tree, where
an initial wrong choice below could propagate to-
wards hypotheses wildly diverging from the input
sentence?s linguistic annotation.
Per Non-Terminal Pruning The decoder uses a
combination of beam and cube-pruning (Huang and
Chiang, 2007). As our grammar uses non-terminals
in the hundreds of thousands, it is important not
to prune away prematurely non-terminals covering
smaller spans and to leave more options to be con-
sidered as we move up the derivation tree.
For this, for every cell in the decoder?s chart, we
keep a separate bin per non-terminal and prune to-
gether hypotheses leading to the same non-terminal
covering a cell. This allows full derivations to be
found for all input sentences, as well as avoids ag-
gressive pruning at an early stage. Given the source
label constraint discussed above, this does not in-
crease running times or memory demands consid-
erably as we allow only up to a few tens of non-
terminals per span.
Expected Counts Rule Pruning To compact the
hierarchical structure part of the grammar prior to
decoding, we prune rules that fail to accumulate
10?8 expected counts during the last CV-EM iter-
ation. For English to German, this brings the struc-
tural rules from 15M down to 1.2M. Note that we
do not prune the phrase-pair emitting rules. Over-
all, we consider this a much more informed pruning
criterion than those based on probability values (that
are not comparable across left-hand sides) or right-
hand side counts (frequent symbols need many more
expansions than a highly specialised one).
4.3 Experimental Setting & Baseline
We evaluate our method on four different lan-
guage pairs with English as the source language
and French, German, Dutch and Chinese as tar-
get. The data for the first three language pairs are
derived from parliament proceedings sourced from
the Europarl corpus (Koehn, 2005), with WMT-
07 development and test data for French and Ger-
man. The data for the English to Chinese task is
composed of parliament proceedings and news arti-
cles. For all language pairs we employ 200K and
400K sentence pairs for training, 2K for develop-
ment and 2K for testing (single reference per source
sentence). Both the baseline and our method decode
647
Training English to
French German Dutch Chinese
set size BLEU NIST BLEU NIST BLEU NIST BLEU NIST
200K
josh-base 29.20 7.2123 18.65 5.8047 21.97 6.2469 22.34 6.5540
lts 29.43 7.2611** 19.10** 5.8714** 22.31* 6.2903* 23.67** 6.6595**
400K
josh-base 29.58 7.3033 18.86 5.8818 22.25 6.2949 23.24 6.7402
lts 29.83 7.4000** 19.49** 5.9374** 22.92** 6.3727** 25.16** 6.9005**
Table 1: Experimental results for training sets of 200K and 400K sentence pairs. Statistically significant score im-
provements from the baseline at the 95% confidence level are labelled with a single star, at the 99% level with two.
with a 3-gram language model smoothed with modi-
fied Knesser-Ney discounting (Chen and Goodman,
1998), trained on around 1M sentences per target
language. The parses of the source sentences em-
ployed by our system during training and decod-
ing are created with the Charniak parser (Charniak,
2000).
We compare against a state-of-the-art hierarchi-
cal translation (Chiang, 2005) baseline, based on the
Joshua translation system under the default training
and decoding settings (josh-base). Apart of eval-
uating against a state-of-the-art system, especially
on the English-Chinese language pair, the compar-
ison has an added interesting aspect. The heuristi-
cally trained baseline takes advantage of ?gap rules?
to reorder based on lexical context cues, but makes
very limited use of the hierarchical structure above
the lexical surface. In contrast, our method induces
a grammar with no such rules, relying on lexical
content and the strength of a higher level translation
structure instead.
4.4 Training & Decoding Details
To train our Latent Translation Structure (LTS) sys-
tem, we used the following settings. CV-EM cross-
validated on a 10-part partition of the training data
and performed 10 iterations. The structural rule
probabilities were initialised to uniform per left-
hand side.
The decoder does not employ any ?glue grammar?
as is usual with hierarchical translation systems to
limit reordering up to a certain cut-off length. In-
stead, we rely on our LTS grammar to reorder and
construct the translation output up to the full sen-
tence length.
In summary, our system?s experimental pipeline is
as follows. All input sentences are parsed and label
charts are created from these parses. The Hierarchi-
cal Reordering SCFG is extracted and its parame-
ters are estimated employing CV-EM. The structural
rules of the estimate are pruned according to their
expected counts and smoothing features are added to
all rules. We train the feature weights under MERT
and decode with the resulting log-linear model.
The overall training and decoding setup is appeal-
ing also regarding computational demands. On an
8-core 2.3GHz system, training on 200K sentence-
pairs demands 4.5 hours while decoding runs on 25
sentences per minute.
4.5 Results
Table 1 presents the results for the baseline and our
method for the 4 language pairs, for training sets of
both 200K and 400K sentence pairs. Our system
(lts) outperforms the baseline for all 4 language
pairs for both BLEU and NIST scores, by a margin
which scales up to +1.92 BLEU points for English to
Chinese translation when training on the 400K set.
In addition, increasing the size of the training data
from 200K to 400K sentence pairs widens the per-
formance margin between the baseline and our sys-
tem, in some cases considerably. All but one of the
performance improvements are found to be statis-
tically significant (Koehn, 2004) at the 95% confi-
dence level, most of them also at the 99% level.
We selected an array of target languages of
increasing reordering complexity with English as
source. Examining the results across the target lan-
guages, LTS performance gains increase the more
challenging the sentence structure of the target lan-
guage is in relation to the source?s, highlighted when
translating to Chinese. Even for Dutch and German,
which pose additional challenges such as compound
words and morphology which we do not explicitly
treat in the current system, LTS still delivers signif-
icant improvements in performance. Additionally,
648
System 200K 400K
(a)
lts-nolabels 22.50 24.24
lts 23.67** 25.16**
(b)
josh-base-lm4 23.81 24.77
lts-lm4 24.48** 26.35**
Table 2: Additional experiments for English to Chi-
nese translation examining (a) the impact of the linguis-
tic annotations in the LTS system (lts), when com-
pared with an instance not employing such annotations
(lts-nolabels) and (b) decoding with a 4th-order
language model (-lm4). BLEU scores for 200K and
400K training sentence pairs.
the robustness of our system is exemplified by deliv-
ering significant performance increases for all lan-
guage pairs.
For the English to Chinese translation task, we
performed further experiments along two axes. We
first investigate the contribution of the linguistic
annotations, by comparing our complete system
(lts) with an otherwise identical implementation
(lts-nolabels) which does not employ any lin-
guistically motivated labels. The latter system then
uses a labels chart as that of Figure 3, which however
labels all phrase-pair spans solely with the generic
X label. The results in Table 2(a) indicate that a
large part of the performance improvement can be
attributed to the use of the linguistic annotations ex-
tracted from the source parse trees, indicating the
potential of the LTS system to take advantage of
such additional annotations to deliver better trans-
lations.
The second additional experiment relates to the
impact of employing a stronger language model dur-
ing decoding, which may increase performance but
slows down decoding speed. Notably, as can be seen
in Table 2(b), switching to a 4-gram LM results in
performance gains for both the baseline and our sys-
tem and while the margin between the two systems
decreases, our system continues to deliver a con-
siderable and significant improvement in translation
BLEU scores.
5 Related Work
In this work, we focus on the combination of
learning latent structure with syntax and linguistic
annotations, exploring the crossroads of machine
learning, linguistic syntax and machine translation.
Training a joint probability model was first dis-
cussed in (Marcu and Wong, 2002). We show that
a translation system based on such a joint model
can perform competitively in comparison with con-
ditional probability models, when it is augmented
with a rich latent hierarchical structure trained ade-
quately to avoid overfitting.
Earlier approaches for linguistic syntax-based
translation such as (Yamada and Knight, 2001; Gal-
ley et al, 2006; Huang et al, 2006; Liu et al, 2006)
focus on memorising and reusing parts of the struc-
ture of the source and/or target parse trees and con-
straining decoding by the input parse tree. In con-
trast to this approach, we choose to employ lin-
guistic annotations in the form of unambiguous syn-
chronous span labels, while discovering ambiguous
translation structure taking advantage of them.
Later work (Marton and Resnik, 2008; Venugopal
et al, 2009; Chiang et al, 2009) takes a more flex-
ible approach, influencing translation output using
linguistically motivated features, or features based
on source-side linguistically-guided latent syntactic
categories (Huang et al, 2010). A feature-based ap-
proach and ours are not mutually exclusive, as we
also employ a limited set of features next to our
trained model during decoding. We find augment-
ing our system with a more extensive feature set an
interesting research direction for the future.
An array of recent work (Chiang, 2010; Zhang et
al., 2008; Liu et al, 2009) sets off to utilise source
and target syntax for translation. While for this work
we constrain ourselves to source language syntax
annotations, our method can be directly applied to
employ labels taking advantage of linguistic annota-
tions from both sides of translation. The decoding
constraints of section 4.2 can then still be applied on
the source part of hybrid source-target labels.
For the experiments in this paper we employ a la-
bel set similar to the non-terminals set of (Zollmann
and Venugopal, 2006). However, the synchronous
grammars we learn share few similarities with those
that they heuristically extract. The HR-SCFG we
adopt allows capturing more complex reordering
phenomena and, in contrast to both (Chiang, 2005;
Zollmann and Venugopal, 2006), is not exposed to
the issues highlighted in section 2.1. Nevertheless,
our results underline the capacity of linguistic anno-
649
tations similar to those of (Zollmann and Venugopal,
2006) as part of latent translation variables.
Most of the aforementioned work does concen-
trate on learning hierarchical, linguistically moti-
vated translation models. Cohn and Blunsom (2009)
sample rules of the form proposed in (Galley et al,
2004) from a Bayesian model, employing Dirich-
let Process priors favouring smaller rules to avoid
overfitting. Their grammar is however also based
on the target parse-tree structure, with their system
surpassing a weak baseline by a small margin. In
contrast to the Bayesian approach which imposes
external priors to lead estimation away from degen-
erate solutions, we take a data-driven approach to
arrive to estimates which generalise well. The rich
linguistically motivated latent variable learnt by our
method delivers translation performance that com-
pares favourably to a state-of-the-art system.
Mylonakis and Sima?an (2010) also employ the
CV-EM algorithm to estimate the parameters of an
SCFG, albeit a much simpler one based on a hand-
ful of non-terminals. In this work we employ some
of their grammar design principles for an immensely
more complex grammar with millions of hierarchi-
cal latent structure rules and show how such gram-
mar can be learnt and applied taking advantage of
source language linguistic annotations.
6 Conclusions
In this work we contribute a method to learn and
apply latent hierarchical translation structure. To
this end, we take advantage of source-language lin-
guistic annotations to motivate instead of constrain
the translation process. An input chart over phrase-
pair spans, with each cell filled with multiple lin-
guistically motivated labels, is coupled with the HR-
SCFG design to arrive at a rich synchronous gram-
mar with millions of structural rules and the capacity
to capture complex linguistically conditioned trans-
lation phenomena. We address overfitting issues by
cross-validating climbing the likelihood of the train-
ing data and propose solutions to increase the effi-
ciency and accuracy of decoding.
An interesting aspect of our work is delivering
competitive performance for difficult language pairs
such as English-Chinese with a joint probability
generative model and an SCFG without ?gap rules?.
Instead of employing hierarchical phrase-pairs, we
invest in learning the higher-order hierarchical syn-
chronous structure behind translation, up to the full
sentence length. While these choices and the related
results challenge current MT research trends, they
are not mutually exclusive with them. Future work
directions include investigating the impact of hierar-
chical phrases for our models as well as any gains
from additional features in the log-linear decoding
model.
Smoothing the HR-SCFG grammar estimates
could prove a possible source of further perfor-
mance improvements. Learning translation and re-
ordering behaviour with respect to linguistic cues
is facilitated in our approach by keeping separate
phrase-pair emission distributions per emitting non-
terminal and reordering pattern, while the employ-
ment of the generic X non-terminals already allows
backing off to more coarse-grained rules. Neverthe-
less, we still believe that further smoothing of these
sparse distributions, e.g. by interpolating them with
less sparse ones, could in the future lead to an addi-
tional increase in translation quality.
Finally, we discuss in this work how our method
can already utilise hundreds of thousands of phrase-
pair labels and millions of structural rules. A fur-
ther promising direction is broadening this set with
labels taking advantage of both source and target-
language linguistic annotation or categories explor-
ing additional phrase-pair properties past the parse
trees such as semantic annotations.
Acknowledgments
Both authors are supported by a VIDI grant (nr.
639.022.604) from The Netherlands Organization
for Scientific Research (NWO). The authors would
like to thank Maxim Khalilov for helping with
experimental data and Andreas Zollmann and the
anonymous reviewers for their valuable comments.
References
Yehoshua Bar-Hillel. 1953. A quasi-arithmetical nota-
tion for syntactic description. Language, 29(1):47?58.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of the North American Asso-
ciation for Computational Linguistics (HLT/NAACL),
Seattle, Washington, USA, April.
650
Stanley Chen and Joshua Goodman. 1998. An empirical
study of smoothing techniques for language modeling.
Technical Report TR-10-98, Harvard University, Au-
gust.
David Chiang, Kevin Knight, and Wei Wang. 2009.
11,001 new features for statistical machine transla-
tion. In Proceedings of Human Language Technolo-
gies: The 2009 Annual Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics, pages 218?226, Boulder, Colorado, June. As-
sociation for Computational Linguistics.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
ACL 2005, pages 263?270.
David Chiang. 2010. Learning to translate with source
and target syntax. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, pages 1443?1452, Uppsala, Sweden, July. Asso-
ciation for Computational Linguistics.
Trevor Cohn and Phil Blunsom. 2009. A Bayesian model
of syntax-directed tree to string grammar induction.
In Proceedings of the 2009 Conference on Empiri-
cal Methods in Natural Language Processing, pages
352?361, Singapore, August. Association for Compu-
tational Linguistics.
A.P. Dempster, N.M. Laird, and D.B. Rubin. 1977. Max-
imum likelihood from incomplete data via the em al-
gorithm. Journal of the Royal Statistical Society, Se-
ries B, 39(1):1?38.
John DeNero, Dan Gillick, James Zhang, and Dan Klein.
2006. Why generative phrase models underperform
surface heuristics. In Proceedings on the Workshop
on Statistical Machine Translation, pages 31?38, New
York City. Association for Computational Linguistics.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In
Daniel Marcu Susan Dumais and Salim Roukos, ed-
itors, HLT-NAACL 2004: Main Proceedings, pages
273?280, Boston, Massachusetts, USA, May. Associ-
ation for Computational Linguistics.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proceed-
ings of the 21st International Conference on Computa-
tional Linguistics and 44th Annual Meeting of the As-
sociation for Computational Linguistics, pages 961?
968, Sydney, Australia, July. Association for Compu-
tational Linguistics.
Liang Huang and David Chiang. 2007. Forest rescoring:
Faster decoding with integrated language models. In
Proceedings of the 45th Annual Meeting of the Asso-
ciation of Computational Linguistics, pages 144?151,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In Proceedings of the 7th Biennial
Conference of the Association for Machine Translation
in the Americas (AMTA), Boston, MA, USA.
Zhongqiang Huang, Martin Cmejrek, and Bowen Zhou.
2010. Soft syntactic constraints for hierarchical
phrase-based translation using latent syntactic distri-
butions. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Processing,
pages 138?147, Cambridge, MA, October. Associa-
tion for Computational Linguistics.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In HLT-
NAACL 2003.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Dekang Lin and
Dekai Wu, editors, Proceedings of EMNLP 2004,
pages 388?395, Barcelona, Spain, July. Association
for Computational Linguistics.
Philipp Koehn. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. In MT Summit 2005.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Sanjeev
Khudanpur, Lane Schwartz, Wren Thornton, Jonathan
Weese, and Omar Zaidan. 2009. Joshua: An open
source toolkit for parsing-based machine translation.
In Proceedings of the Fourth Workshop on Statistical
Machine Translation, pages 135?139, Athens, Greece,
March. Association for Computational Linguistics.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-
string alignment template for statistical machine trans-
lation. In Proceedings of the 21st International Con-
ference on Computational Linguistics and 44th Annual
Meeting of the Association for Computational Linguis-
tics, pages 609?616, Sydney, Australia, July. Associa-
tion for Computational Linguistics.
Yang Liu, Yajuan Lu?, and Qun Liu. 2009. Improving
tree-to-tree translation with packed forests. In Pro-
ceedings of the Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International Joint
Conference on Natural Language Processing of the
AFNLP, pages 558?566, Suntec, Singapore, August.
Association for Computational Linguistics.
Daniel Marcu andWilliamWong. 2002. A phrase-based,
joint probability model for statistical machine transla-
tion. In Proceedings of Empirical methods in natural
language processing, pages 133?139. Association for
Computational Linguistics.
Yuval Marton and Philip Resnik. 2008. Soft syntactic
constraints for hierarchical phrased-based translation.
In Proceedings of ACL-08: HLT, pages 1003?1011,
651
Columbus, Ohio, June. Association for Computational
Linguistics.
Markos Mylonakis and Khalil Sima?an. 2008. Phrase
translation probabilities with ITG priors and smooth-
ing as learning objective. In Proceedings of the 2008
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 630?639, Honolulu, USA,
October.
Markos Mylonakis and Khalil Sima?an. 2010. Learn-
ing probabilistic synchronous CFGs for phrase-based
translation. In Fourteenth Conference on Computa-
tional Natural Language Learning, Uppsala, Sweden,
July.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Annual Meeting of the Association for Compu-
tational Linguistics, pages 160?167, Sapporo, Japan,
July. Association for Computational Linguistics.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. De-
pendency treelet translation: Syntactically informed
phrasal smt. In Proceedings of 43rd Annual Meeting
of the Association for Computational Linguistics, Ann
Arbor, Michigan, USA, June.
Ashish Venugopal, Andreas Zollmann, Noah A. Smith,
and Stephan Vogel. 2009. Preference grammars: Soft-
ening syntactic constraints to improve statistical ma-
chine translation. In Proceedings of Human Language
Technologies: The 2009 Annual Conference of the
North American Chapter of the Association for Com-
putational Linguistics, pages 236?244, Boulder, Col-
orado, June. Association for Computational Linguis-
tics.
Wei Wang, Jonathan May, Kevin Knight, and Daniel
Marcu. 2010. Re-structuring, re-labeling, and re-
aligning for syntax-based machine translation. Com-
putational Linguistics, 36(2):247?277.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?403.
Kenji Yamada and Kevin Knight. 2001. A syntax-based
statistical translation model. In Proceedings of 39th
Annual Meeting of the Association for Computational
Linguistics, pages 523?530, Toulouse, France, July.
Association for Computational Linguistics.
Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li,
Chew Lim Tan, and Sheng Li. 2008. A tree sequence
alignment-based tree-to-tree translation model. In
Proceedings of ACL-08: HLT, pages 559?567, Colum-
bus, Ohio, June. Association for Computational Lin-
guistics.
Andreas Zollmann and Ashish Venugopal. 2006. Syntax
augmented machine translation via chart parsing. In
Proceedings on the Workshop on Statistical Machine
Translation, pages 138?141, New York City, June. As-
sociation for Computational Linguistics.
652
Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 117?125,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Learning Probabilistic Synchronous CFGs for Phrase-based Translation
Markos Mylonakis
ILLC
University of Amsterdam
m.mylonakis@uva.nl
Khalil Sima?an
ILLC
University of Amsterdam
k.simaan@uva.nl
Abstract
Probabilistic phrase-based synchronous
grammars are now considered promis-
ing devices for statistical machine transla-
tion because they can express reordering
phenomena between pairs of languages.
Learning these hierarchical, probabilistic
devices from parallel corpora constitutes a
major challenge, because of multiple la-
tent model variables as well as the risk
of data overfitting. This paper presents
an effective method for learning a family
of particular interest to MT, binary Syn-
chronous Context-Free Grammars with in-
verted/monotone orientation (a.k.a. Bi-
nary ITG). A second contribution con-
cerns devising a lexicalized phrase re-
ordering mechanism that has complimen-
tary strengths to Chiang?s model. The
latter conditions reordering decisions on
the surrounding lexical context of phrases,
whereas our mechanism works with the
lexical content of phrase pairs (akin to
standard phrase-based systems). Surpris-
ingly, our experiments on French-English
data show that our learning method ap-
plied to far simpler models exhibits per-
formance indistinguishable from the Hiero
system.
1 Introduction
A fundamental problem in phrase-based machine
translation concerns the learning of a probabilistic
synchronous context-free grammar (SCFG) over
phrase pairs from an input parallel corpus. Chi-
ang?s Hiero system (Chiang, 2007) exemplifies
the gains to be had by combining phrase-based
translation (Och and Ney, 2004) with the hierar-
chical reordering capabilities of SCFGs, particu-
larly originating from Binary Inversion Transduc-
tion Grammars (BITG) (Wu, 1997). Yet, exist-
ing empirical work is largely based on successful
heuristic techniques, and the learning of Hiero-like
BITG/SCFG remains an unsolved problem,
The difficulty of this problem stems from the
need for simultaneously learning of two kinds of
preferences (see Fig.1) (1) lexical translation prob-
abilities (P (?e, f? | X)) of source (f ) and target
(e) phrase pairs, and (2) phrase reordering prefer-
ences of a target string relative to a source string,
expressed in synchronous productions probabil-
ities (for monotone or switching productions).
Theoretically speaking, both kinds of preferences
may involve latent structure relative to the paral-
lel corpus. The mapping between source-target
sentence pairs can be expressed in terms of la-
tent phrase segmentations and latent word/phrase-
alignments, and the hierarchical phrase reorder-
ing can be expressed in terms of latent binary
synchronous hierarchical structures (cf. Fig. 1).
But each of these three kinds of latent structures
may be made explicit using external resources:
word-alignment could be considered solved us-
ing Giza++ (Och and Ney, 2003)), phrase pairs
can be obtained from these word-alignments (Och
and Ney, 2004), and the hierarchical synchronous
structure can be grown over source/target linguis-
tic syntactic trees output by an existing parser.
The Joint Phrase Translation Model (Marcu and
Wong, 2002) constitutes a specific case, albeit
without the hierarchical, synchronous reordering
Start S ? X 1 / X 1 (1)
Monotone X ? X 1 X 2 /X 1 X 2 (2)
Switching X ? X 1 X 2 /X 2 X 1 (3)
Emission X ? e / f (4)
Figure 1: A phrase-pair SCFG (BITG)
117
component. Other existing work, e.g. (Chiang,
2007), assumes the word-alignments are given in
the parallel corpus, but the problem of learning
phrase translation probabilities is usually avoided
by using surface counts of phrase pairs (Koehn et
al., 2003). The problem of learning the hierar-
chical, synchronous grammar reordering rules is
oftentimes addressed as a learning problem in its
own right assuming all the rest is given (Blunsom
et al, 2008b).
A small number of efforts has been dedicated
to the simultaneous learning of the probabilities
of phrase translation pairs as well as hierarchi-
cal reordering, e.g., (DeNero et al, 2008; Zhang
et al, 2008; Blunsom et al, 2009). Of these,
some concentrate on evaluating word-alignment,
directly such as (Zhang et al, 2008) or indirectly
by evaluating a heuristically trained hierarchical
translation system from sampled phrasal align-
ments (Blunsom et al, 2009). However, very
few evaluate on actual translation performance of
induced synchronous grammars (DeNero et al,
2008). In the majority of cases, the Hiero system,
which constitutes the yardstick by which hierar-
chical systems are measured, remains superior in
translation performance, see e.g. (DeNero et al,
2008).
This paper tackles the problem of learning gen-
erative BITG models as translation models assum-
ing latent segmentation and latent reordering: this
is the most similar setting to the training of Hiero.
Unlike all other work that heuristically selects a
subset of phrase pairs, we start out from an SCFG
that works with all phrase pairs in the training set
and concentrate on the aspects of learning. This
learning problem is fraught with the risks of over-
fitting and can easily result in inadequate reorder-
ing preferences (see e.g. (DeNero et al, 2006)).
Almost instantly, we find that the translation
performance of all-phrase probabilistic SCFGs
learned in this setting crucially depends on the in-
terplay between two aspects of learning:
? Defining a more constrained parameter
space, where the reordering productions
are phrase-lexicalised and made sensitive to
neighbouring reorderings, and
? Defining an objective function that effec-
tively smoothes the maximum-likelihood cri-
terion.
One contribution of this paper is in devis-
ing an effective, data-driven smoothed Maximum-
Likelihood that can cope with a model working
with all phrase pair SCFGs. This builds upon
our previous work on estimating parameters of a
?bag-of-phrases? model for Machine Translation
(Mylonakis and Sima?an, 2008). However, learn-
ing SCFGs poses significant novel challenges, the
core of which lies on the hierarchical nature of a
stochastic SCFG translation model and the rele-
vant additional layer of latent structure. We ad-
dress these issues in this work. Another important
contribution is in defining a lexicalised reorder-
ing component within BITG that captures order
divergences orthogonal to Chiang?s model (Chi-
ang, 2007) but somewhat akin to Phrase-Based
Statistical Machine Translation reordering models
(Koehn et al, 2003).
Our analysis shows that the learning difficul-
ties can be attributed to a rather weak generative
model. Yet, our best system exhibits Hiero-level
performance on French-English Europarl data us-
ing an SCFG-based decoder (Li et al, 2009). Our
findings should be insightful for others attempting
to make the leap from shallow phrase-based sys-
tems to hierarchical SCFG-based translation mod-
els using learning methods, as opposed to heuris-
tics.
The rest of the paper is structured as follows.
Section 2 briefly introduces the SCFG formalism
and discusses its adoption in the context of Statis-
tical Machine Translation (SMT). In section 3, we
consider some of the pitfalls of stochastic SCFG
grammar learning and address them by introduc-
ing a novel learning objective and algorithm. In
the section that follows we browse through latent
translation structure choices, while in section 5 we
present our empirical experiments on evaluating
the induced stochastic SCFGs on a translation task
and compare their performance with a hierarchical
translation baseline. We close with a comparison
of related work and a final discussion including fu-
ture research directions.
2 Synchronous Grammars for Machine
Translation
Synchronous Context Free Grammars (SCFGs)
provide an appealing formalism to describe the
translation process, which explains the generation
of parallel strings recursively and allows capturing
long-range reordering phenomena. Formally, an
SCFG G is defined as the tuple (N,E, F,R, S),
118
where N is the finite set of non-terminals with
S ? N the start symbol, F and E are finite sets
of words for the source and target language and R
is a finite set of rewrite rules. Every rule expands
a left-hand side non-terminal to a right-hand side
pair of strings, a source language string over the
vocabulary F?N and a target language string over
E ? N . The number of non-terminals in the two
strings is equal and the rule is complemented with
a mapping between them.
String pairs in the language of the SCFG are
those with a valid derivation, consisting of a se-
quence of rule applications, starting from S and
recursively expanding the linked non-terminals at
the right-hand side of rules. Stochastic SCFGs
augment every rule in R with a probability, under
the constraint that probabilities of rules with the
same left-hand side sum up to one. The probabil-
ity of each derived string pair is then the product
of the probabilities of rules used in the derivation.
Unless otherwise stated, for the rest of the paper
when we refer to SCFGs we will be pointing to
their stochastic extension.
The rank of an SCFG is defined as the maxi-
mum number of non-terminals in a grammar?s rule
right-hand side. Contrary to monolingual Context
Free Grammars, there does not always exist a con-
version of an SCFG of a higher rank to one of a
lower rank with the same language of string pairs.
For this, most machine translation applications fo-
cus on SCFGs of rank two (binary SCFGs), or
binarisable ones witch can be converted to a bi-
nary SCFG, given that these seem to cover most
of the translation phenomena encountered in lan-
guage pairs (Wu, 1997) and the related processing
algorithms are less demanding computationally.
Although SCFGS were initially introduced for
machine translation as a stochastic word-based
translation process in the form of the Inversion-
Transduction Grammar (Wu, 1997), they were ac-
tually able to offer state-of-the-art performance in
their latter phrase-based implementation by Chi-
ang (Chiang, 2005). Chiang?s Hiero hierarchi-
cal translation system is based on a synchronous
grammar with a single non-terminal X covering
all learned phrase-pairs. Beginning from the start
symbol S, an initial phrase-span structure is con-
structed monotonically using a simple ?glue gram-
mar?:
S ?S 1 X 2 / S 1 X 2
S ?X 1 / X 1
The true power of the system lies in expanding
these initial phrase-spans with a set of hierarchi-
cal translation rules, which allow conditioning re-
ordering decisions based on lexical context. For
the French to English language pair, some exam-
ples would be:
S ? X 1 e?conomiques / financial X 1
S ? cette X 1 de X 2 / this X 1 X 2
S ? politique X 1 commune de X 2 /
X 2
? s common X 1 policy
Further work builds on the Hiero grammar to ex-
pand it with constituency syntax motivated non-
terminals (Zollmann and Venugopal, 2006).
3 Synchronous Grammar Learning
The learning of phrase-based stochastic SCFGs
with a Maximum Likelihood objective is exposed
to overfitting as other all-fragment models such as
Phrase-Based SMT (PBSMT) (Marcu and Wong,
2002; DeNero et al, 2006) and Data-Oriented
Parsing (DOP) (Bod et al, 2003; Zollmann and
Sima?an, 2006). Maximum Likelihood Estima-
tion (MLE) returns degenerate grammar estimates
that memorise well the parallel training corpus but
generalise poorly to unseen data.
The bias-variance decomposition of the gener-
alisation error Err sheds light on this learning
problem. For an estimator p? with training data D,
Err can be expressed as the expected Kullback-
Leibler (KL) divergence between the target distri-
bution q and that the estimate p?. This error decom-
poses into bias and variance terms (Heskes, 1998):
Err =
bias
? ?? ?
KL(q, p?)+
variance
? ?? ?
EDKL(p?, p?) (5)
Bias is the KL-divergence between q and the mean
estimate over all training data p? = EDp?(D). Vari-
ance is the expected divergence between the av-
erage estimate and the estimator?s actual choice.
MLE estimators for all-fragment models are zero-
biased with zero divergence between the average
estimate and the true data distribution. In contrast,
their variance is unboundedly large, leading to un-
bounded generalisation error on unseen cases.
119
3.1 Cross Validated MLE
A well-known method for estimating generalisa-
tion error is k-fold Cross-Validation (CV) (Hastie
et al, 2001). By partitioning the training data D
into k parts Hk1 , we estimate Err as the expected
error over all 1 ? i ? k, when testing on Hi with
a model trained by MLE on the rest of the data
D?i = ?j 6=iHj .
Here we use CV to leverage the bias-variance
trade-off for learning stochastic all-phrase SCFGs.
Given an input all-phrase SCFG grammar with
phrase-pairs extracted from the training data, we
maximise training data likelihood (MLE) subject
to CV smoothing: for each data part Hi (1 ? i ?
k), we consider only derivations which employ
grammar rules extracted from the rest of the data
D?i. Other work (Mylonakis and Sima?an, 2008)
has also explored MLE under CV for a ?bag-of-
phrases model? that does not deal with reordering
preferences, does not employ latent hierarchical
structure and works with a non-hierarchical de-
coder, and partially considers the sparsity issues
that arise within CV training. The present paper
deals with these issues.
Because of the latent segmentation and hi-
erarchical variables, CV-smoothed MLE cannot
be solved analytically and we devise a CV in-
stance of the Expectation-Maximization (EM) al-
gorithm, with an implementation based on a syn-
chronous version of the Inside-Outside algorithm
(see Fig. 2). For each word-aligned sentence pair
in a partitionHi, the set of eligible derivations (de-
noted D?i) are those that can be built using only
phrase-pairs and productions found inD?i. An es-
sential part of the learning process involves defin-
ing the grammar extractor G(D), a function from
data to an all-phrase SCFG. We will discuss vari-
ous extractors in section 4.
Our CV-EM algorithm is an EM instance, guar-
anteeing convergence and a non-decreasing CV-
smoothed data likelihood after each iteration. The
running time remains O(n6), where n is input
length, but by considering only derivation spans
which do not cross word-alignment points, this
runs in reasonable times for relatively large cor-
pora.
3.2 Bayesian Aspects of CV-MLE
Beside being an estimator, the CV-MLE learning
algorithm has the added value of being a grammar
learner focusing on reducing generalisation error,
INPUT: Word-aligned parallel training data D
Grammar extractor G
The number of parts k to partition D
OUTPUT: SCFG G with rule probabilities p?
Partition training data D into parts H1, . . . ,Hk.
For 1 ? i ? k do
Extract grammar rules set Gi = G(Hi)
Initialise G = ?iGi, p?0 uniform
Let j = 0
Repeat
Let j = j + 1
E-step:
For 1 ? i ? k do
Calculate expected counts given G, p?j?1,
for derivations D?i of Hi
using rules from ?k 6=iG(k)
M-step: set p?j to ML estimate given
expected counts
Until convergence
Figure 2: The CV Expectation Maximization al-
gorithm
in the sense that probabilities of grammar produc-
tions should reflect the frequency with which these
productions are expected to be used for translating
future data. Additionally, since the CV criterion
prohibits for every data point derivations that use
rules only extracted from the same data part, such
rules are assigned zero probabilities in the final es-
timate and are effectively excluded from the gram-
mar. In this way, the algorithm ?shapes? the input
grammar, concentrating probability mass on pro-
ductions that are likely to be used with future data.
One view point of CV-MLE is that each par-
tition D?i and Hi induces a prior probability
Prior(pi; D?i) on every parameter assignment pi,
obtained from D?i. This prior assigns zero prob-
ability to all pi parameter sets with non-zero prob-
abilities for rules not in G(D?i), and uniformly
distributes probability to the rest of the parameter
sets. In light of this, the CV-MLE objective can be
written as follows:
argmax
pi
?
i
Prior(pi; D?i)? P (Hi | pi) (6)
This data-driven prior aims to directly favour pa-
rameter sets which are expected to better gener-
alise according to the CV criterion, without rely-
ing on arbitrary constraints such as limiting the
120
length of phrase pairs in the right-hand side of
grammar rules. Furthermore, other frequently em-
ployed priors such as the Dirichlet distribution and
the Dirichlet Process promote better generalising
rule probability distributions based on externally
set hyperparameter values, whose selection is fre-
quently sensitive in terms of language pairs, or
even the training corpus itself. In contrast, the CV-
MLE prior aims for a data-driven Bayesian model,
focusing on getting information from the data, in-
stead of imposing external human knowledge on
them (see also (Mackay and Petoy, 1995)).
3.3 Smoothing the Model
One remaining wrinkle in the CV-EM scheme is
the treatment of boundary cases. There will often
be sentence-pairs in Hi, that cannot be fully de-
rived by the grammar extracted from the rest of the
data D?i either because of (1) ?unknown? words
(i.e. not appearing in other parts of the CV parti-
tion) or (2) complicated combinations of adjacent
word-alignments. We employ external smoothing
of the grammar, prior to learning.
Our solution is to extend the SCFG extracted
fromD?i with new emission productions deriving
the ?unknown? phrase-pairs (i.e., found in Hi but
not in D?i). Crucially, the probabilities of these
productions are drawn from a fixed smoothing dis-
tribution, i.e., they remain constant throughout es-
timation. Our smoothing distribution of phrase-
pairs for all pre-terminals considers source-target
phrase lengths drawn from a Poisson distribution
with unit mean, drawing subsequently the words
of each of the phrases uniformly from the vocab-
ulary of each language, similar to (Blunsom et al,
2009).
psmooth(f/e) =
ppoisson(|f |; 1) ppoisson(|e|; 1)
V |f |f V
|e|
e
Since the smoothing distribution puts stronger
preference on shorter phrase-pairs and avoids
competing with the ?known? phrase-pairs, it leads
the learner to prefer using as little as possible such
smoothing rules, covering only the phrase-pairs
required to complete full derivations.
4 Parameter Spaces and Grammar
Extractors
A Grammar Extractor (GE) plays a major role in
our probabilistic SCFG learning pipeline. A GE is
a function from a word-aligned parallel corpus to a
probabilistic SCFG model. Together with the con-
straints that render a proper probabilistic SCFG1,
this defines the parameter space.
The extractors used in this paper create SCFGs
productions of two different kinds: (a) hierarchi-
cal synchronous productions that define the space
of possible derivations up to the level of the SCFG
pre-terminals, and (2) the phrase-pair emission
rules that expand the pre-terminals to phrase-pairs
of varying lengths. Given the word-alignments,
the set of phrase-pairs extracted is the set of all
translational equivalents (without length upper-
bound) under the word-alignment as defined in
(Och and Ney, 2004; Koehn et al, 2003).
Below we focus on the two grammar extrac-
tors employed in our experiments. We start out
from the most generic, BITG-like formulation,
and aim at incremental refinement of the hierar-
chical productions in order to capture relevant,
content-based phrase-pair reordering preferences
in the training data.
Single non-terminal SCFG This is a phrase-
based binary SCFG grammar employing a single
non-terminal X covering each extracted phrase-
pair. The other productions consist of monotone
and switching expansions of phrase-pair spans
covered by X . Finally, the whole sentence-pair is
considered to be covered by X . We will call this
?plain SCFG? extractor. See Fig. 1.
Lexicalised Reordering SCFG One weakness
of the plain SCFG is that the reordering deci-
sions in the derivations are made without reference
to lexical content of the phrases; this is because
all phrase-pairs are covered by the same non-
terminal. As a refinement, we propose a gram-
mar extractor that aims at modelling the reordering
behaviour of phrase-pairs by taking their content
into account. This time, the X non-terminal is re-
served for phrase-pairs and spans which will take
part in monotonic productions only. Two fresh
non-terminals, XSL and XSR, are used for cov-
ering phrase-pairs that participate in order switch-
ing with other, adjacent phrase-pairs. The non-
terminal XSL covers phrase-pairs which appear
first in the source language order, and the latter
those which follow them. The grammar rules pro-
duced by this GE, dubbed ?switch grammar?, are
listed in Fig. 3.
1The sum of productions that have the same left-hand la-
bel must be one.
121
Start S ? X 1 /X 1
Monotone Expansion
X ? X 1 X 2 /X 1 X 2
XSL ? X 1 X 2 / X 1 X 2
XSR ? X 1 X 2 /X 1 X 2
Switching Expansion
X ? XSL 1 XSR 2 /XSR 2 XSL 1
XSL ? XSL 1 XSR 2 /XSR 2 XSL 1
XSR ? XSL 1 XSR 2 /XSR 2 XSL 1
Phrase-Pair Emission
X ? e/f
XSL ? e / f
XSR ? e / f
Figure 3: Lexicalised-Reordering SCFG
The reordering information captured by the
switch grammar is in a sense orthogonal to that
of Hiero-like systems utilising rules such as those
listed in section 2. Hiero rules encode hier-
archical reordering patterns based on surround-
ing context. In contrast, the switch grammar
models the reordering preferences of the phrase-
pairs themselves, similarly to the monotone-swap-
discontinuous reordering models of Phrase-based
SMT models (Koehn et al, 2003). Furthermore, it
strives to match pairs of such preferences, combin-
ing together phrase-pairs with compatible reorder-
ing preferences.
5 Experiments
In this section we proceed to integrate our esti-
mates within an SCFG-based decoder. We subse-
quently evaluate our performance in relation to a
state-of-the-art Hiero baseline on a French to En-
glish translation task.
5.1 Decoding
The joint model of bilingual string derivations pro-
vided by the learned SCFG grammar can be used
for translation given a input source sentence, since
argmaxe p(e|f) = argmaxe p(e, f). We use our
learned stochastic SCFG grammar with the decod-
ing component of the Joshua SCFG toolkit (Li
et al, 2009). The full translation model inter-
polates log-linearly the probability of a grammar
derivation together with the language model prob-
ability of the target string. The model is further
smoothed, similarly to phrase-based models and
the Hiero system, with smoothing features ?i such
as the lexical translation scores of the phrase-pairs
involved and rule usage penalties. As usual with
statistical translation, we aim for retrieving the tar-
get sentence e corresponding to the most probable
derivation D
?
? (f, e) with rules r, with:
p(D) ? p(e)?lmpscfg(e, f)
?scfg
?
i
?
r?D
?i(r)
?i
The interpolation weights are tuned using Mini-
mum Error Rate Training (Och, 2003).
5.2 Results
We test empirically the learner?s output gram-
mars for translating from French to English, us-
ing k = 5 for the Cross Validation data partition-
ing. The training material is a GIZA++ word-
aligned corpus of 200K sentence-pairs from the
Europarl corpus (Koehn, 2005), with our devel-
opment and test parallel corpora of 2K sentence-
pairs stemming from the same source. Train-
ing the grammar parameters until convergence de-
mands around 6 hours on an 8-core 2.26 GHz Intel
Xeon system. Decoding employs a 4-gram lan-
guage model, trained on English Europarl data of
19.5M words, smoothed using modified Kneser-
Ney discounting (Chen and Goodman, 1998), and
lexical translation smoothing features based on the
GIZA++ alignments.
In a sense, the real baseline to which we might
compare against should be a system employing the
MLE estimate for the grammar extracted from the
whole training corpus. However, as we have al-
ready discussed, this assigns zero probability to all
sentence-pairs outside of the training data and is
subsequently bound to perform extremely poorly,
as decoding would then completely rely on the
smoothing features. Instead, we opt to compare
against a hierarchical translation baseline provided
by the Joshua toolkit, trained and tuned on the
same data as our learning algorithm. The grammar
used by the baseline is much richer than the ones
learned by our algorithm, also employing rules
which translate with context, as shown in section
2. Nevertheless, since it is not clear how the re-
ordering rules probabilities of a grammar similar
to the ones we use could be trained heuristically,
we choose to relate the performance of our learned
stochastic SCFG grammars to the particular, state-
of-the-art in SCFG-based translation, system.
Table 1 presents the translation performance re-
sults of our systems and the baseline. On first
122
System
Lexical
BLEU
Smoothing
joshua-baseline No 27.79
plain scfg No 28.04
switch scfg No 28.48
joshua-baseline Yes 29.96
plain scfg Yes 29.75
switch scfg Yes 29.88
Table 1: Empirical results, with and without addi-
tional lexical translation smoothing features dur-
ing decoding
observation, it is evident that our learning algo-
rithm outputs stochastic SCFGs which manage to
generalise, avoiding the degenerate behaviour of
plain MLE training for these models. Given the
notoriety of the estimation process, this is note-
worthy on its own. Having a learning algorithm
at hand which realises in a reasonable extent the
potential of each stochastic grammar design (as
implemented in the relevant grammar extractors),
we can now compare between the two grammar
extractors used in our experiments. The results
table highlights the importance of conditioning
the reordering process on lexical grounds. The
plain grammar with the single phrase-pair non-
terminal cannot accomplish this and achieves a
lower BLEU score. On the other hand, the switch
SCFG allows such conditioning. The learner takes
advantage of this feature to output a grammar
which performs better in taking reordering deci-
sions, something that is reflected in both the actual
translations as well as the BLEU score achieved.
Furthermore, our results highlight the impor-
tance of the smoothing decoding features. The
unsmoothed baseline system itself scores consid-
erably less when employing solely the heuristic
translation score. Our unsmoothed switch gram-
mar decoding setup improves on the baseline by
a considerable difference of 0.7 BLEU. Subse-
quently, when adding the smoothing lexical trans-
lation features, both systems record a significant
increase in performance, reaching comparable lev-
els of performance.
The degenerate behaviour of MLE for SCFGs
can be greatly limited by constraining ourselves
to grammars employing minimal phrase-pairs
; phrase-pairs which cannot be further broken
down into smaller ones according to the word-
alignment. One could argue that it is enough to
perform plain MLE with such minimal phrase-pair
SCFGs, instead of using our more elaborate learn-
ing algorithm with phrase-pairs of all lengths. To
investigate this, for our final experiment we used
a plain MLE estimate of the switch grammar to
translate, limiting the grammar?s phrase-pair emis-
sion rules to only those which involve minimal
phrase-pairs. The very low score of 17.82 BLEU
(without lexical smoothing) not only highlights
the performance gains of using longer phrase-pairs
in hierarchical translation models, but most impor-
tantly provides a strong incentive to address the
overfitting behaviour of MLE estimators for such
models, instead of avoiding it.
6 Related work
Most learning of phrase-based models, e.g.,
(Marcu and Wong, 2002; DeNero et al, 2006;
Mylonakis and Sima?an, 2008), works without hi-
erarchical components (i.e., not based on the ex-
plicit learning of an SCFG/BITG). These learning
problems pose other kinds of learning challenges
than the ones posed by explicit learning of SCFGs.
Chiang?s original work (Chiang, 2007) is also re-
lated. Yet, the learning problem is not expressed in
terms of an explicit objective function because sur-
face heuristic counts are used. It has been very dif-
ficult to match the performance of Chiang?s model
without use of these heuristic counts.
A somewhat related work, (Blunsom et al,
2008b), attempts learning new non-terminal labels
for synchronous productions in order to improve
translation. This work differs substantially from
our work because it employs a heuristic estimate
for the phrase pair probabilities, thereby concen-
trating on a different learning problem: that of re-
fining the grammar symbols. Our approach might
also benefit from such a refinement but we do not
attempt this problem here. In contrast, (Blunsom
et al, 2008a) works with the expanded phrase pair
set of (Chiang, 2005), formulating an exponential
model and concentrating on marginalising out the
latent segmentation variables. Again, the learning
problem is rather different from ours. Similarly,
the work in (Zhang et al, 2008) reports on a multi-
stage model, without a latent segmentation vari-
able, but with a strong prior preferring sparse esti-
mates embedded in a Variational Bayes (VB) esti-
mator. This work concentrates the efforts on prun-
ing both the space of phrase pairs and the space of
(ITG) analyses.
123
To the best of our knowledge, this work is the
first to attempt learning probabilistic phrase-based
BITGs as translation models in a setting where
both a phrase segmentation component and a hi-
erarchical reordering component are assumed la-
tent variables. Like this work, (Mylonakis and
Sima?an, 2008; DeNero et al, 2008) also employ
an all-phrases model. Our paper shows that it is
possible to train such huge grammars under itera-
tive schemes like CV-EM, without need for sam-
pling or pruning. At the surface of it, our CV-
EM estimator is also a kind of Bayesian learner,
but in reality it is a more specific form of regu-
larisation, similar to smoothing techniques used in
language modelling (Chen and Goodman, 1998;
Mackay and Petoy, 1995).
7 Discussion and Future Research
Phrase-based stochastic SCFGs provide a rich for-
malism to express translation phenomena, which
has been shown to offer competitive performance
in practice. Since learning SCFGs for machine
translation has proven notoriously difficult, most
successful SCFGmodels for SMT rely on rules ex-
tracted from word-alignment patterns and heuris-
tically computed rule scores, with the impact and
the limits imposed by these choices yet unknown.
Some of the reasons behind the challenges of
SCFG learning can be traced back to the introduc-
tion of latent variables at different, competing lev-
els: word and phrase-alignment as well as hier-
archical reordering structure, with larger phrase-
pairs reducing the need for extensive reordering
structure and vice versa. While imposing priors
such as the often used Dirichlet distribution or the
Dirichlet Process provides a method to overcome
these pitfalls, we believe that the data-driven reg-
ularisation employed in this work provides an ef-
fective alternative to them, focusing more on the
data instead of importing generic external human
knowledge.
We believe that this work makes a significant
step towards learning synchronous grammars for
SMT. This is an objective not only worthy be-
cause of promises of increased performance, but,
most importantly, also by increasing the depth of
our understanding on SCFGs as vehicles of latent
translation structures. Our usage of the induced
grammars directly for translation, instead of an in-
termediate task such as phrase-alignment, aims ex-
actly at this.
While the latent structures that we explored
in this paper were relatively simple in compar-
ison with Hiero-like SCFGs, they take a differ-
ent, content-driven approach on learning reorder-
ing preferences than the context-driven approach
of Hiero. We believe that these approaches are not
merely orthogonal, but could also prove comple-
mentary. Taking advantage of the possible syner-
gies between content and context-driven reorder-
ing learning is an appealing direction of future re-
search. This is particularly promising for other
language pairs, such as Chinese to English, where
Hiero-like grammars have been shown to perform
particularly well.
Acknowledgments: Both authors are supported
by a VIDI grant (nr. 639.022.604) from The
Netherlands Organization for Scientific Research
(NWO).
References
P. Blunsom, T. Cohn, and M. Osborne. 2008a. A dis-
criminative latent variable model for statistical ma-
chine translation. In Proceedings of ACL-08: HLT,
pages 200?208. Association for Computational Lin-
guistics.
Phil Blunsom, Trevor Cohn, and Miles Osborne.
2008b. Bayesian synchronous grammar induction.
In Advances in Neural Information Processing Sys-
tems 21, Vancouver, Canada, December.
Phil Blunsom, Trevor Cohn, Chris Dyer, and Miles
Osborne. 2009. A gibbs sampler for phrasal syn-
chronous grammar induction. In Proceedings of the
47th Annual Meeting of the Association of Compu-
tational Linguistics, Singapore, August. Association
for Computational Linguistics.
R. Bod, R. Scha, and K. Sima?an, editors. 2003. Data
Oriented Parsing. CSLI Publications, Stanford Uni-
versity, Stanford, California, USA.
S. Chen and J. Goodman. 1998. An empirical study of
smoothing techniques for language modeling. Tech-
nical Report TR-10-98, Harvard University, August.
D. Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
ACL 2005, pages 263?270.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33.
J. DeNero, D. Gillick, J. Zhang, and D. Klein. 2006.
Why generative phrase models underperform sur-
face heuristics. In Proceedings on the Workshop on
Statistical Machine Translation, pages 31?38, New
York City. Association for Computational Linguis-
tics.
124
John DeNero, Alexandre Bouchard-Co?te?, and Dan
Klein. 2008. Sampling alignment structure un-
der a Bayesian translation model. In Proceedings
of the 2008 Conference on Empirical Methods in
Natural Language Processing, pages 314?323, Hon-
olulu, Hawaii, October. Association for Computa-
tional Linguistics.
T. Hastie, R. Tibshirani, and J. H. Friedman. 2001. The
Elements of Statistical Learning. Springer.
Tom Heskes. 1998. Bias/variance decompositions for
likelihood-based estimators. Neural Computation,
10:1425?1433.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In HLT-NAACL 2003.
P. Koehn. 2005. Europarl: A Parallel Corpus for Sta-
tistical Machine Translation. In MT Summit 2005.
Zhifei Li, Chris Callison-Burch, Chris Dyer, San-
jeev Khudanpur, Lane Schwartz, Wren Thornton,
Jonathan Weese, and Omar Zaidan. 2009. Joshua:
An open source toolkit for parsing-based machine
translation. In Proceedings of the Fourth Workshop
on Statistical Machine Translation, pages 135?139,
Athens, Greece, March. Association for Computa-
tional Linguistics.
David J. C. Mackay and Linda C. Bauman Petoy. 1995.
A hierarchical dirichlet language model. Natural
Language Engineering, 1:1?19.
D. Marcu and W. Wong. 2002. A phrase-based, joint
probability model for statistical machine translation.
In Proceedings of Empirical methods in natural lan-
guage processing, pages 133?139. Association for
Computational Linguistics.
Markos Mylonakis and Khalil Sima?an. 2008. Phrase
translation probabilities with itg priors and smooth-
ing as learning objective. In Proceedings of the 2008
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 630?639, Honolulu, USA,
October.
F. J. Och and H. Ney. 2003. A systematic comparison
of various statistical alignment models. Computa-
tional Linguistics, 29(1):19?51.
F. J. Och and H. Ney. 2004. The alignment template
approach to statistical machine translation. Compu-
tational Linguistics, 30(4):417?449.
Franz Josef Och. 2003. Minimum error rate train-
ing in statistical machine translation. In Proceed-
ings of the 41st Annual Meeting of the Association
for Computational Linguistics, pages 160?167, Sap-
poro, Japan, July. Association for Computational
Linguistics.
D. Wu. 1997. Stochastic inversion transduction gram-
mars and bilingual parsing of parallel corpora. Com-
putational Linguistics, 23(3):377?403.
H. Zhang, Ch. Quirk, R. C. Moore, and D. Gildea.
2008. Bayesian learning of non-compositional
phrases with synchronous parsing. In Proceedings
of ACL-08: HLT, pages 97?105, Columbus, Ohio,
June. Association for Computational Linguistics.
A. Zollmann and K. Sima?an. 2006. An efficient
and consistent estimator for data-oriented parsing.
Journal of Automata, Languages and Combinatorics
(JALC), 10 (2005) Number 2/3:367?388.
Andreas Zollmann and Ashish Venugopal. 2006. Syn-
tax augmented machine translation via chart parsing.
In Proceedings on the Workshop on Statistical Ma-
chine Translation, pages 138?141, New York City,
June. Association for Computational Linguistics.
125
