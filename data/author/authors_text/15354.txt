Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 180?189,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
A New Dataset and Method for Automatically Grading ESOL Texts
Helen Yannakoudakis
Computer Laboratory
University of Cambridge
United Kingdom
Helen.Yannakoudakis@cl.cam.ac.uk
Ted Briscoe
Computer Laboratory
University of Cambridge
United Kingdom
Ted.Briscoe@cl.cam.ac.uk
Ben Medlock
iLexIR Ltd
Cambridge
United Kingdom
ben@ilexir.co.uk
Abstract
We demonstrate how supervised discrimina-
tive machine learning techniques can be used
to automate the assessment of ?English as a
Second or Other Language? (ESOL) examina-
tion scripts. In particular, we use rank prefer-
ence learning to explicitly model the grade re-
lationships between scripts. A number of dif-
ferent features are extracted and ablation tests
are used to investigate their contribution to
overall performance. A comparison between
regression and rank preference models further
supports our method. Experimental results on
the first publically available dataset show that
our system can achieve levels of performance
close to the upper bound for the task, as de-
fined by the agreement between human exam-
iners on the same corpus. Finally, using a set
of ?outlier? texts, we test the validity of our
model and identify cases where the model?s
scores diverge from that of a human examiner.
1 Introduction
The task of automated assessment of free text fo-
cuses on automatically analysing and assessing the
quality of writing competence. Automated assess-
ment systems exploit textual features in order to
measure the overall quality and assign a score to a
text. The earliest systems used superficial features,
such as word and sentence length, as proxies for
understanding the text. More recent systems have
used more sophisticated automated text processing
techniques to measure grammaticality, textual co-
herence, prespecified errors, and so forth.
Deployment of automated assessment systems
gives a number of advantages, such as the reduced
workload in marking texts, especially when applied
to large-scale assessments. Additionally, automated
systems guarantee the application of the same mark-
ing criteria, thus reducing inconsistency, which may
arise when more than one human examiner is em-
ployed. Often, implementations include feedback
with respect to the writers? writing abilities, thus fa-
cilitating self-assessment and self-tutoring.
Implicitly or explicitly, previous work has mostly
treated automated assessment as a supervised text
classification task, where training texts are labelled
with a grade and unlabelled test texts are fitted to the
same grade point scale via a regression step applied
to the classifier output (see Section 6 for more de-
tails). Different techniques have been used, includ-
ing cosine similarity of vectors representing text in
various ways (Attali and Burstein, 2006), often com-
bined with dimensionality reduction techniques such
as Latent Semantic Analysis (LSA) (Landauer et al,
2003), generative machine learning models (Rudner
and Liang, 2002), domain-specific feature extraction
(Attali and Burstein, 2006), and/or modified syntac-
tic parsers (Lonsdale and Strong-Krause, 2003).
A recent review identifies twelve different auto-
mated free-text scoring systems (Williamson, 2009).
Examples include e-Rater (Attali and Burstein,
2006), Intelligent Essay Assessor (IEA) (Landauer
et al, 2003), IntelliMetric (Elliot, 2003; Rudner et
al., 2006) and Project Essay Grade (PEG) (Page,
2003). Several of these are now deployed in high-
stakes assessment of examination scripts. Although
there are many published analyses of the perfor-
180
mance of individual systems, as yet there is no pub-
lically available shared dataset for training and test-
ing such systems and comparing their performance.
As it is likely that the deployment of such systems
will increase, standardised and independent evalua-
tion methods are important. We make such a dataset
of ESOL examination scripts available1 (see Section
2 for more details), describe our novel approach to
the task, and provide results for our system on this
dataset.
We address automated assessment as a supervised
discriminative machine learning problem and par-
ticularly as a rank preference problem (Joachims,
2002). Our reasons are twofold:
Discriminative classification techniques often
outperform non-discriminative ones in the context of
text classification (Joachims, 1998). Additionally,
rank preference techniques (Joachims, 2002) allow
us to explicitly learn an optimal ranking model of
text quality. Learning a ranking directly, rather than
fitting a classifier score to a grade point scale after
training, is both a more generic approach to the task
and one which exploits the labelling information in
the training data efficiently and directly.
Techniques such as LSA (Landauer and Foltz,
1998) measure, in addition to writing competence,
the semantic relevance of a text written in response
to a given prompt. However, although our corpus
of manually-marked texts was produced by learners
of English in response to prompts eliciting free-text
answers, the marking criteria are primarily based on
the accurate use of a range of different linguistic
constructions. For this reason, we believe that an
approach which directly measures linguistic compe-
tence will be better suited to ESOL text assessment,
and will have the additional advantage that it may
not require retraining for new prompts or tasks.
As far as we know, this is the first application
of a rank preference model to automated assess-
ment (hereafter AA). In this paper, we report exper-
iments on rank preference Support Vector Machines
(SVMs) trained on a relatively small amount of data,
on identification of appropriate feature types derived
automatically from generic text processing tools, on
comparison with a regression SVM model, and on
the robustness of the best model to ?outlier? texts.
1http://www.ilexir.com/
We report a consistent, comparable and replicable
set of results based entirely on the new dataset and
on public-domain tools and data, whilst also exper-
imentally motivating some novel feature types for
the AA task, thus extending the work described in
(Briscoe et al, 2010).
In the following sections we describe in more de-
tail the dataset used for training and testing, the sys-
tem developed, the evaluation methodology, as well
as ablation experiments aimed at studying the con-
tribution of different feature types to the AA task.
We show experimentally that discriminative models
with appropriate feature types can achieve perfor-
mance close to the upper bound, as defined by the
agreement between human examiners on the same
test corpus.
2 Cambridge Learner Corpus
The Cambridge Learner Corpus2 (CLC), developed
as a collaborative project between Cambridge Uni-
versity Press and Cambridge Assessment, is a large
collection of texts produced by English language
learners from around the world, sitting Cambridge
Assessment?s English as a Second or Other Lan-
guage (ESOL) examinations3.
For the purpose of this work, we extracted scripts
produced by learners taking the First Certificate in
English (FCE) exam, which assesses English at an
upper-intermediate level. The scripts, which are
anonymised, are annotated using XML and linked
to meta-data about the question prompts, the candi-
date?s grades, native language and age. The FCE
writing component consists of two tasks asking
learners to write either a letter, a report, an article,
a composition or a short story, between 200 and 400
words. Answers to each of these tasks are anno-
tated with marks (in the range 1?40), which have
been fitted to a RASCH model (Fischer and Mole-
naar, 1995) to correct for inter-examiner inconsis-
tency and comparability. In addition, an overall
mark is assigned to both tasks, which is the one we
use in our experiments.
Each script has been also manually tagged with
information about the linguistic errors committed,
2http://www.cup.cam.ac.uk/gb/elt/catalogue/subject/custom/
item3646603/Cambridge-International-Corpus-Cambridge-
Learner-Corpus/?site locale=en GB
3http://www.cambridgeesol.org/
181
using a taxonomy of approximately 80 error types
(Nicholls, 2003). The following is an example error-
coded sentence:
In the morning, you are <NS type = ?TV?>
waken|woken</NS> up by a singing puppy.
In this sentence, TV denotes an incorrect tense of
verb error, where waken can be corrected to woken.
Our data consists of 1141 scripts from the year
2000 for training written by 1141 distinct learners,
and 97 scripts from the year 2001 for testing written
by 97 distinct learners. The learners? ages follow
a bimodal distribution with peaks at approximately
16?20 and 26?30 years of age.
The prompts eliciting the free text are provided
with the dataset. However, in this paper we make
no use of prompt information and do not make any
attempt to check that the text answer is appropriate
to the prompt. Our focus is on developing an accu-
rate AA system for ESOL text that does not require
prompt-specific or topic-specific training. There is
no overlap between the prompts used in 2000 and in
2001. A typical prompt taken from the 2000 training
dataset is shown below:
Your teacher has asked you to write a story for the
school?s English language magazine. The story must
begin with the following words: ?Unfortunately, Pat
wasn?t very good at keeping secrets?.
3 Approach
We treat automated assessment of ESOL text (see
Section 2) as a rank preference learning problem
(see Section 1). In the experiments reported here
we use Support Vector Machines (SVMs) (Vap-
nik, 1995) through the SVMlight package (Joachims,
1999). Using the dataset described in Section 2, a
number of linguistic features are automatically ex-
tracted and their contribution to overall performance
is investigated.
3.1 Rank preference model
SVMs have been extensively used for learning clas-
sification, regression and ranking functions. In its
basic form, a binary SVM classifier learns a linear
threshold function that discriminates data points of
two categories. By using a different loss function,
the ?-insensitive loss function (Smola, 1996), SVMs
can also perform regression. SVMs in regression
mode estimate a function that outputs a real number
based on the training data. In both cases, the model
generalises by computing a hyperplane that has the
largest (soft-)margin.
In rank preference SVMs, the goal is to learn a
ranking function which outputs a score for each data
point, from which a global ordering of the data is
constructed. This procedure requires a setR consist-
ing of training samples ~xn and their target rankings
rn:
R = {(~x1, r1), (~x2, r2), ..., (~xn, rn)} (1)
such that ~xi R ~xj when ri < rj , where
1 ? i, j ? n and i 6= j.
A rank preference model is not trained directly on
this set of data objects and their labels; rather a set of
pair-wise difference vectors is created. The goal of
a linear ranking model is to compute a weight vec-
tor ~w that maximises the number of correctly ranked
pairs:
?(~xi R ~xj) : ~w(~xi ? ~xj) > 0 (2)
This is equivalent to solving the following opti-
misation problem:
Minimise:
1
2
?~w?2 + C
?
?ij (3)
Subject to the constraints:
?(~xi R ~xj) : ~w(~xi ? ~xj) ? 1? ?ij (4)
?ij ? 0 (5)
The factor C allows a trade-off between the train-
ing error and the margin size, while ?ij are non-
negative slack variables that measure the degree of
misclassification.
The optimisation problem is equivalent to that for
the classification model on pair-wise difference vec-
tors. In this case, generalisation is achieved by max-
imising the differences between closely-ranked data
pairs.
The principal advantage of applying rank prefer-
ence learning to the AA task is that we explicitly
182
model the grade relationships between scripts and
do not need to apply a further regression step to fit
the classifier output to the scoring scheme. The re-
sults reported in this paper are obtained by learning
a linear classification function.
3.2 Feature set
We parsed the training and test data (see Section
2) using the Robust Accurate Statistical Parsing
(RASP) system with the standard tokenisation and
sentence boundary detection modules (Briscoe et al,
2006) in order to broaden the space of candidate fea-
tures suitable for the task. The features used in our
experiments are mainly motivated by the fact that
lexical and grammatical features should be highly
discriminative for the AA task. Our full feature set
is as follows:
i. Lexical ngrams
(a) Word unigrams
(b) Word bigrams
ii. Part-of-speech (PoS) ngrams
(a) PoS unigrams
(b) PoS bigrams
(c) PoS trigrams
iii. Features representing syntax
(a) Phrase structure (PS) rules
(b) Grammatical relation (GR) distance mea-
sures
iv. Other features
(a) Script length
(b) Error-rate
Word unigrams and bigrams are lower-cased and
used in their inflected forms. PoS unigrams, bigrams
and trigrams are extracted using the RASP tagger,
which uses the CLAWS4 tagset. The most proba-
ble posterior tag per word is used to construct PoS
ngram features, but we use the RASP parser?s op-
tion to analyse words assigned multiple tags when
the posterior probability of the highest ranked tag is
less than 0.9, and the next n tags have probability
greater than 150 of it.
4http://ucrel.lancs.ac.uk/claws/
Based on the most likely parse for each identified
sentence, we extract the rule names from the phrase
structure (PS) tree. RASP?s rule names are semi-
automatically generated and encode detailed infor-
mation about the grammatical constructions found
(e.g. V1/modal bse/+-, ?a VP consisting of a modal
auxiliary head followed by an (optional) adverbial
phrase, followed by a VP headed by a verb with base
inflection?). Moreover, rule names explicitly repre-
sent information about peripheral or rare construc-
tions (e.g. S/pp-ap s-r, ?a S with preposed PP with
adjectival complement, e.g. for better or worse, he
left?), as well as about fragmentary and likely extra-
grammatical sequences (e.g. T/txt-frag, ?a text unit
consisting of 2 or more subanalyses that cannot be
combined using any rule in the grammar?). There-
fore, we believe that many (longer-distance) gram-
matical constructions and errors found in texts can
be (implicitly) captured by this feature type.
In developing our AA system, a number of dif-
ferent grammatical complexity measures were ex-
tracted from parses, and their impact on the accuracy
of the system was explored. For the experiments re-
ported here, we use complexity measures represent-
ing the sum of the longest distance in word tokens
between a head and dependent in a grammatical re-
lation (GR) from the RASP GR output, calculated
for each GR graph from the top 10 parses per sen-
tence. In particular, we extract the mean and median
values of these distances per sentence and use the
maximum values per script. Intuitively, this feature
captures information about the grammatical sophis-
tication of the writer. However, it may also be con-
founded in cases where sentence boundaries are not
identified through, for example, poor punctuation.
Although the CLC contains information about the
linguistic errors committed (see Section 2), we try
to extract an error-rate in a way that doesn?t require
manually tagged data. However, we also use an
error-rate calculated from the CLC error tags to ob-
tain an upper bound for the performance of an auto-
mated error estimator (true CLC error-rate).
In order to estimate the error-rate, we build a tri-
gram language model (LM) using ukWaC (ukWaC
LM) (Ferraresi et al, 2008), a large corpus of En-
glish containing more than 2 billion tokens. Next,
we extend our language model with trigrams ex-
tracted from a subset of the texts contained in the
183
Features
Pearson?s Spearman?s
correlation correlation
word ngrams 0.601 0.598
+PoS ngrams 0.682 0.687
+script length 0.692 0.689
+PS rules 0.707 0.708
+complexity 0.714 0.712
Error-rate features
+ukWaC LM 0.735 0.758
+CLC LM 0.741 0.773
+true CLC error-rate 0.751 0.789
Table 1: Correlation between the CLC scores and the AA
system predicted values.
CLC (CLC LM). As the CLC contains texts pro-
duced by second language learners, we only extract
frequently occurring trigrams from highly ranked
scripts to avoid introducing erroneous ones to our
language model. A word trigram in test data is
counted as an error if it is not found in the language
model. We compute presence/absence efficiently us-
ing a Bloom filter encoding of the language models
(Bloom, 1970).
Feature instances of types i and ii are weighted
using the tf*idf scheme and normalised by the L2
norm. Feature type iii is weighted using frequency
counts, while iii and iv are scaled so that their final
value has approximately the same order of magni-
tude as i and ii.
The script length is based on the number of words
and is mainly added to balance the effect the length
of a script has on other features. Finally, features
whose overall frequency is lower than four are dis-
carded from the model.
4 Evaluation
In order to evaluate our AA system, we use two cor-
relation measures, Pearson?s product-moment cor-
relation coefficient and Spearman?s rank correla-
tion coefficient (hereafter Pearson?s and Spearman?s
correlation respectively). Pearson?s correlation de-
termines the degree to which two linearly depen-
dent variables are related. As Pearson?s correlation
is sensitive to the distribution of data and, due to
outliers, its value can be misleading, we also re-
port Spearman?s correlation. The latter is a non-
parametric robust measure of association which is
Ablated Pearson?s Spearman?s
feature correlation correlation
none 0.741 0.773
word ngrams 0.713 0.762
PoS ngrams 0.724 0.737
script length 0.734 0.772
PS rules 0.712 0.731
complexity 0.738 0.760
ukWaC+CLC LM 0.714 0.712
Table 2: Ablation tests showing the correlation between
the CLC and the AA system.
sensitive only to the ordinal arrangement of values.
As our data contains some tied values, we calculate
Spearman?s correlation by using Pearson?s correla-
tion on the ranks.
Table 1 presents the Pearson?s and Spearman?s
correlation between the CLC scores and the AA sys-
tem predicted values, when incrementally adding
to the model the feature types described in Sec-
tion 3.2. Each feature type improves the model?s
performance. Extending our language model with
frequent trigrams extracted from the CLC improves
Pearson?s and Spearman?s correlation by 0.006 and
0.015 respectively. The addition of the error-rate ob-
tained from the manually annotated CLC error tags
on top of all the features further improves perfor-
mance by 0.01 and 0.016. An evaluation of our best
error detection method shows a Pearson correlation
of 0.611 between the estimated and the true CLC er-
ror counts. This suggests that there is room for im-
provement in the language models we developed to
estimate the error-rate. In the experiments reported
hereafter, we use the ukWaC+CLC LM to calculate
the error-rate.
In order to assess the independent as opposed to
the order-dependent additive contribution of each
feature type to the overall performance of the sys-
tem, we run a number of ablation tests. An ablation
test consists of removing one feature of the system
at a time and re-evaluating the model on the test set.
Table 2 presents Pearson?s and Spearman?s correla-
tion between the CLC and our system, when remov-
ing one feature at a time. All features have a positive
effect on performance, while the error-rate has a big
impact, as its absence is responsible for a 0.061 de-
crease of Spearman?s correlation. In addition, the
184
Model
Pearson?s Spearman?s
correlation correlation
Regression 0.697 0.706
Rank preference 0.741 0.773
Table 3: Comparison between regression and rank pref-
erence model.
removal of either the word ngrams, the PS rules, or
the error-rate estimate contributes to a large decrease
in Pearson?s correlation.
In order to test the significance of the improved
correlations, we ran one-tailed t-tests with a = 0.05
for the difference between dependent correlations
(Williams, 1959; Steiger, 1980). The results showed
that PoS ngrams, PS rules, the complexity measures,
and the estimated error-rate contribute significantly
to the improvement of Spearman?s correlation, while
PS rules also contribute significantly to the improve-
ment of Pearson?s correlation.
One of the main approaches adopted by previ-
ous systems involves the identification of features
that measure writing skill, and then the application
of linear or stepwise regression to find optimal fea-
ture weights so that the correlation with manually
assigned scores is maximised. We trained a SVM
regression model with our full set of feature types
and compared it to the SVM rank preference model.
The results are given in Table 3. The rank preference
model improves Pearson?s and Spearman?s correla-
tion by 0.044 and 0.067 respectively, and these dif-
ferences are significant, suggesting that rank prefer-
ence is a more appropriate model for the AA task.
Four senior and experienced ESOL examiners re-
marked the 97 FCE test scripts drawn from 2001 ex-
ams, using the marking scheme from that year (see
Section 2). In order to obtain a ceiling for the perfor-
mance of our system, we calculate the average corre-
lation between the CLC and the examiners? scores,
and find an upper bound of 0.796 and 0.792 Pear-
son?s and Spearman?s correlation respectively.
In order to evaluate the overall performance of our
system, we calculate its correlation with the four se-
nior examiners in addition to the RASCH-adjusted
CLC scores. Tables 4 and 5 present the results ob-
tained.
The average correlation of the AA system with the
CLC and the examiner scores shows that it is close
CLC E1 E2 E3 E4 AA
CLC - 0.820 0.787 0.767 0.810 0.741
E1 0.820 - 0.851 0.845 0.878 0.721
E2 0.787 0.851 - 0.775 0.788 0.730
E3 0.767 0.845 0.775 - 0.779 0.747
E4 0.810 0.878 0.788 0.779 - 0.679
AA 0.741 0.721 0.730 0.747 0.679 -
Avg 0.785 0.823 0.786 0.782 0.786 0.723
Table 4: Pearson?s correlation of the AA system predicted
values with the CLC and the examiners? scores, where E1
refers to the first examiner, E2 to the second etc.
CLC E1 E2 E3 E4 AA
CLC - 0.801 0.799 0.788 0.782 0.773
E1 0.801 - 0.809 0.806 0.850 0.675
E2 0.799 0.809 - 0.744 0.787 0.724
E3 0.788 0.806 0.744 - 0.794 0.738
E4 0.782 0.850 0.787 0.794 - 0.697
AA 0.773 0.675 0.724 0.738 0.697 -
Avg 0.788 0.788 0.772 0.774 0.782 0.721
Table 5: Spearman?s correlation of the AA system pre-
dicted values with the CLC and the examiners? scores,
where E1 refers to the first examiner, E2 to the second
etc.
to the upper bound for the task. Human?machine
agreement is comparable to that of human?human
agreement, with the exception of Pearson?s correla-
tion with examiner E4 and Spearman?s correlation
with examiners E1 and E4, where the discrepancies
are higher. It is likely that a larger training set and/or
more consistent grading of the existing training data
would help to close this gap. However, our system is
not measuring some properties of the scripts, such as
discourse cohesion or relevance to the prompt elicit-
ing the text, that examiners will take into account.
5 Validity tests
The practical utility of an AA system will depend
strongly on its robustness to subversion by writers
who understand something of its workings and at-
tempt to exploit this to maximise their scores (in-
dependently of their underlying ability). Surpris-
ingly, there is very little published data on the ro-
bustness of existing systems. However, Powers et
al. (2002) invited writing experts to trick the scoring
185
capabilities of an earlier version of e-Rater (Burstein
et al, 1998). e-Rater (see Section 6 for more de-
tails) assigns a score to a text based on linguistic fea-
ture types extracted using relatively domain-specific
techniques. Participants were given a description of
these techniques as well as of the cue words that the
system uses. The results showed that it was easier
to fool the system into assigning higher than lower
scores.
Our goal here is to determine the extent to which
knowledge of the feature types deployed poses a
threat to the validity of our system, where certain
text generation strategies may give rise to large pos-
itive discrepancies. As mentioned in Section 2, the
marking criteria for FCE scripts are primarily based
on the accurate use of a range of different grammati-
cal constructions relevant to specific communicative
goals, but our system assesses this indirectly.
We extracted 6 high-scoring FCE scripts from the
CLC that do not overlap with our training and test
data. Based on the features used by our system and
without bias towards any modification, we modified
each script in one of the following ways:
i. Randomly order:
(a) word unigrams within a sentence
(b) word bigrams within a sentence
(c) word trigrams within a sentence
(d) sentences within a script
ii. Swap words that have the same PoS within a
sentence
Although the above modifications do not ex-
haust the potential challenges a deployed AA system
might face, they represent a threat to the validity of
our system since we are using a highly related fea-
ture set. In total, we create 30 such ?outlier? texts,
which were given to an ESOL examiner for mark-
ing. Using the ?outlier? scripts as well as their origi-
nal/unmodified versions, we ran our system on each
modification separately and calculated the correla-
tion between the predicted values and the examiner?s
scores. Table 6 presents the results.
The predicted values of the system have a high
correlation with the examiner?s scores when tested
on ?outlier? texts of modification types i(a), i(b) and
Modification
Pearson?s Spearman?s
correlation correlation
i(a) 0.960 0.912
i(b) 0.938 0.914
i(c) 0.801 0.867
i(d) 0.08 0.163
ii 0.634 0.761
Table 6: Correlation between the predicted values and the
examiner?s scores on ?outlier? texts.
i(c). However, as i(c) has a lower correlation com-
pared to i(a) and i(b), it is likely that a random order-
ing of ngrams with N > 3 will further decrease per-
formance. A modification of type ii, where words
with the same PoS within a sentence are swapped,
results in a Pearson and Spearman correlation of
0.634 and 0.761 respectively.
Analysis of the results showed that our system
predicted higher scores than the ones assigned by the
examiner. This can be explained by the fact that texts
produced using modification type ii contain a small
portion of correct sentences. However, the marking
criteria are based on the overall writing quality. The
final case, where correct sentences are randomly or-
dered, receives the lowest correlation. As our sys-
tem is not measuring discourse cohesion, discrepan-
cies are much higher; the system?s predicted scores
are high whilst the ones assigned by the examiner
are very low. However, for a writer to be able to
generate text of this type already requires significant
linguistic competence, whilst a number of generic
methods for assessing text and/or discourse cohe-
sion have been developed and could be deployed in
an extended version of our system.
It is also likely that highly creative ?outlier? essays
may give rise to large negative discrepancies. Recent
comments in the British media have focussed on this
issue, reporting that, for example, one deployed es-
say marking system assigned Winston Churchill?s
speech ?We Shall Fight on the Beaches? a low score
because of excessive repetition5. Our model pre-
dicted a high passing mark for this text, but not the
highest one possible, that some journalists clearly
feel it deserves.
5http://news.bbc.co.uk/1/hi/education/8356572.stm
186
6 Previous work
In this section we briefly discuss a number of the
more influential and/or better described approaches.
Pe?rez-Mar??n et al (2009), Williamson (2009), Dikli
(2006) and Valenti et al (2003) provide a more de-
tailed overview of existing AA systems.
Project Essay Grade (PEG) (Page, 2003), one of
the earliest systems, uses a number of manually-
identified mostly shallow textual features, which are
considered to be proxies for intrinsic qualities of
writing competence. Linear regression is used to as-
sign optimal feature weights that maximise the cor-
relation with the examiner?s scores. The main is-
sue with this system is that features such as word
length and script length are easy to manipulate in-
dependently of genuine writing ability, potentially
undermining the validity of the system.
In e-Rater (Attali and Burstein, 2006), texts
are represented using vectors of weighted features.
Each feature corresponds to a different property of
texts, such as an aspect of grammar, style, discourse
and topic similarity. Additional features, represent-
ing stereotypical grammatical errors for example,
are extracted using manually-coded task-specific de-
tectors based, in part, on typical marking criteria. An
unmarked text is scored based on the cosine simi-
larity between its weighted vector and the ones in
the training set. Feature weights and/or scores can
be fitted to a marking scheme by stepwise or lin-
ear regression. Unlike our approach, e-Rater mod-
els discourse structure, semantic coherence and rel-
evance to the prompt. However, the system contains
manually developed task-specific components and
requires retraining or tuning for each new prompt
and assessment task.
Intelligent Essay Assessor (IEA) (Landauer et al,
2003) uses Latent Semantic Analysis (LSA) (Lan-
dauer and Foltz, 1998) to compute the semantic sim-
ilarity between texts, at a specific grade point, and
a test text. In LSA, text is represented by a ma-
trix, where rows correspond to words and columns
to context (texts). Singular Value Decomposition
(SVD) is used to obtain a reduced dimension matrix
clustering words and contexts. The system is trained
on topic and/or prompt specific texts while test texts
are assigned a score based on the ones in the training
set that are most similar. The overall score, which is
calculated using regression techniques, is based on
the content score as well as on other properties of
texts, such as style, grammar, and so forth, though
the methods used to assess these are not described
in any detail in published work. Again, the system
requires retraining or tuning for new prompts and
assessment tasks.
Lonsdale and Strong-Krause (2003) use a mod-
ified syntactic parser to analyse and score texts.
Their method is based on a modified version of
the Link Grammar parser (Sleator and Templerley,
1995) where the overall score of a text is calculated
as the average of the scores assigned to each sen-
tence. Sentences are scored on a five-point scale
based on the parser?s cost vector, which roughly
measures the complexity and deviation of a sentence
from the parser?s grammatical model. This approach
bears some similarities to our use of grammatical
complexity and extragrammaticality features, but
grammatical features represent only one component
of our overall system, and of the task.
The Bayesian Essay Test Scoring sYstem
(BETSY) (Rudner and Liang, 2002) uses multino-
mial or Bernoulli Naive Bayes models to classify
texts into different classes (e.g. pass/fail, grades A?
F) based on content and style features such as word
unigrams and bigrams, sentence length, number of
verbs, noun?verb pairs etc. Classification is based
on the conditional probability of a class given a set
of features, which is calculated using the assumption
that each feature is independent of the other. This
system shows that treating AA as a text classifica-
tion problem is viable, but the feature types are all
fairly shallow, and the approach doesn?t make effi-
cient use of the training data as a separate classifier
is trained for each grade point.
Recently, Chen et al (2010) has proposed an un-
supervised approach to AA of texts addressing the
same topic, based on a voting algorithm. Texts are
clustered according to their grade and given an ini-
tial Z-score. A model is trained where the initial
score of a text changes iteratively based on its sim-
ilarity with the rest of the texts as well as their Z-
scores. The approach might be better described as
weakly supervised as the distribution of text grades
in the training data is used to fit the final Z-scores to
grades. The system uses a bag-of-words represen-
tation of text, so would be easy to subvert. Never-
187
theless, exploration of the trade-offs between degree
of supervision required in training and grading ac-
curacy is an important area for future research.
7 Conclusions and future work
Though many of the systems described in Section
6 have been shown to correlate well with examin-
ers? marks on test data in many experimental con-
texts, no cross-system comparisons are available be-
cause of the lack of a shared training and test dataset.
Furthermore, none of the published work of which
we are aware has systematically compared the con-
tribution of different feature types to the AA task,
and only one (Powers et al, 2002) assesses the ease
with which the system can be subverted given some
knowledge of the features deployed.
We have shown experimentally how rank prefer-
ence models can be effectively deployed for auto-
mated assessment of ESOL free-text answers. Based
on a range of feature types automatically extracted
using generic text processing techniques, our sys-
tem achieves performance close to the upper bound
for the task. Ablation tests highlight the contribu-
tion of each feature type to the overall performance,
while significance of the resulting improvements in
correlation with human scores has been calculated.
A comparison between regression and rank prefer-
ence models further supports our approach. Prelim-
inary experiments based on a set of ?outlier? texts
have shown the types of texts for which the system?s
scoring capability can be undermined.
We plan to experiment with better error detection
techniques, since the overall error-rate of a script is
one of the most discriminant features. Briscoe et
al. (2010) describe an approach to automatic off-
prompt detection which does not require retraining
for each new question prompt and which we plan
to integrate with our system. It is clear from the
?outlier? experiments reported here that our system
would benefit from features assessing discourse co-
herence, and to a lesser extent from features as-
sessing semantic (selectional) coherence over longer
bounds than those captured by ngrams. The addition
of an incoherence metric to the feature set of an AA
system has been shown to improve performance sig-
nificantly (Miltsakaki and Kukich, 2000; Miltsakaki
and Kukich, 2004).
Finally, we hope that the release of the training
and test dataset described here will facilitate further
research on the AA task for ESOL free text and, in
particular, precise comparison of different systems,
feature types, and grade fitting methods.
Acknowledgements
We would like to thank Cambridge ESOL, a division
of Cambridge Assessment, for permission to use and
distribute the examination scripts. We are also grate-
ful to Cambridge Assessment for arranging for the
test scripts to be remarked by four of their senior ex-
aminers. Finally, we would like to thank Marek Rei,
?istein Andersen and the anonymous reviewers for
their useful comments.
References
Yigal Attali and Jill Burstein. 2006. Automated essay
scoring with e-rater v.2. Journal of Technology, Learn-
ing, and Assessment, 4(3):1?30.
Burton H. Bloom. 1970. Space/time trade-offs in hash
coding with allowable errors. Communications of the
ACM, 13(7):422?426, July.
E.J. Briscoe, J. Carroll, and R Watson. 2006. The second
release of the RASP system. In ACL-Coling?06 In-
teractive Presentation Session, pages 77?80, Sydney,
Australia.
E.J. Briscoe, B. Medlock, and ?. Andersen. 2010. Au-
tomated Assessment of ESOL Free Text Examinations.
Cambridge University, Computer Laboratory, TR-790.
Jill Burstein, Karen Kukich, Susanne Wolff, Chi Lu,
Martin Chodorow, Lisa Braden-Harder, and Mary Dee
Harris. 1998. Automated scoring using a hybrid fea-
ture identification technique. Proceedings of the 36th
annual meeting on Association for Computational Lin-
guistics, pages 206?210.
YY Chen, CL Liu, TH Chang, and CH Lee. 2010.
An Unsupervised Automated Essay Scoring System.
IEEE Intelligent Systems, pages 61?67.
Semire Dikli. 2006. An overview of automated scoring
of essays. Journal of Technology, Learning, and As-
sessment, 5(1).
S. Elliot. 2003. IntelliMetric: From here to validity. In
M.D. Shermis and J.C. Burstein, editors, Automated
essay scoring: A cross-disciplinary perspective, pages
71?86.
Adriano Ferraresi, Eros Zanchetta, Marco Baroni, and
Silvia Bernardini. 2008. Introducing and evaluating
ukWaC, a very large web-derived corpus of English.
188
In S. Evert, A. Kilgarriff, and S. Sharoff, editors, Pro-
ceedings of the 4th Web as Corpus Workshop (WAC-4).
G.H. Fischer and I.W. Molenaar. 1995. Rasch models:
Foundations, recent developments, and applications.
Springer.
Thorsten Joachims. 1998. Text categorization with sup-
port vector machines: Learning with many relevant
features. In Proceedings of the European Conference
on Machine Learning, pages 137?142. Springer.
Thorsten Joachims. 1999. Making large scale SVM
learning practical. In B. Scho?lkopf, C. Burges, and
A. Smola, editors, Advances in Kernel Methods - Sup-
port Vector Learning. MIT Press.
Thorsten Joachims. 2002. Optimizing search engines
using clickthrough data. In Proceedings of the ACM
Conference on Knowledge Discovery and Data Mining
(KDD), pages 133?142. ACM.
T.K. Landauer and P.W. Foltz. 1998. An introduction to
latent semantic analysis. Discourse processes, pages
259?284.
T.K. Landauer, D. Laham, and P.W. Foltz. 2003. Au-
tomated scoring and annotation of essays with the In-
telligent Essay Assessor. In M.D. Shermis and J.C.
Burstein, editors, Automated essay scoring: A cross-
disciplinary perspective, pages 87?112.
Deryle Lonsdale and D. Strong-Krause. 2003. Auto-
mated rating of ESL essays. In Proceedings of the
HLT-NAACL 2003 Workshop: Building Educational
Applications Using Natural Language Processing.
Eleni Miltsakaki and Karen Kukich. 2000. Automated
evaluation of coherence in student essays. In Proceed-
ings of LREC 2000.
Eleni Miltsakaki and Karen Kukich. 2004. Evaluation
of text coherence for electronic essay scoring systems.
Natural Language Engineering, 10(01):25?55, March.
D. Nicholls. 2003. The Cambridge Learner Corpus: Er-
ror coding and analysis for lexicography and ELT. In
Proceedings of the Corpus Linguistics 2003 confer-
ence, pages 572?581.
E.B. Page. 2003. Project essay grade: PEG. In M.D.
Shermis and J.C. Burstein, editors, Automated essay
scoring: A cross-disciplinary perspective, pages 43?
54.
D. Pe?rez-Mar??n, Ismael Pascual-Nieto, and P. Rodr??guez.
2009. Computer-assisted assessment of free-text
answers. The Knowledge Engineering Review,
24(04):353?374, December.
D.E. Powers, J.C. Burstein, M. Chodorow, M.E. Fowles,
and K. Kukich. 2002. Stumping e-rater: challenging
the validity of automated essay scoring. Computers in
Human Behavior, 18(2):103?134.
L.M. Rudner and Tahung Liang. 2002. Automated essay
scoring using Bayes? theorem. The Journal of Tech-
nology, Learning and Assessment, 1(2):3?21.
L.M. Rudner, Veronica Garcia, and Catherine Welch.
2006. An Evaluation of the IntelliMetric Essay Scor-
ing System. Journal of Technology, Learning, and As-
sessment, 4(4):1?21.
D.D.K. Sleator and D. Templerley. 1995. Parsing En-
glish with a link grammar. Proceedings of the 3rd In-
ternational Workshop on Parsing Technologies, ACL.
AJ Smola. 1996. Regression estimation with support
vector learning machines. Master?s thesis, Technische
Universita?t Munchen.
J.H. Steiger. 1980. Tests for comparing elements of a
correlation matrix. Psychological Bulletin, 87(2):245?
251.
Salvatore Valenti, Francesca Neri, and Alessandro Cuc-
chiarelli. 2003. An overview of current research
on automated essay grading. Journal of Information
Technology Education, 2:3?118.
Vladimir N. Vapnik. 1995. The nature of statistical
learning theory. Springer.
E. J. Williams. 1959. The Comparison of Regression
Variables. Journal of the Royal Statistical Society. Se-
ries B (Methodological), 21(2):396?399.
DM Williamson. 2009. A Framework for Implement-
ing Automated Scoring. In Annual Meeting of the
American Educational Research Association and the
National Council on Measurement in Education, San
Diego, CA.
189
Proceedings of the EACL 2012 Joint Workshop of LINGVIS & UNCLH, pages 35?43,
Avignon, France, April 23 - 24 2012. c?2012 Association for Computational Linguistics
Automating Second Language Acquisition Research:
Integrating Information Visualisation and Machine Learning
Helen Yannakoudakis
Computer Laboratory
University of Cambridge
United Kingdom
Helen.Yannakoudakis@cl.cam.ac.uk
Ted Briscoe
Computer Laboratory
University of Cambridge
United Kingdom
Ted.Briscoe@cl.cam.ac.uk
Theodora Alexopoulou
DTAL
University of Cambridge
United Kingdom
ta259@cam.ac.uk
Abstract
We demonstrate how data-driven ap-
proaches to learner corpora can support
Second Language Acquisition research
when integrated with visualisation tools.
We present a visual user interface support-
ing the investigation of a set of linguistic
features discriminating between pass and
fail ?English as a Second or Other Lan-
guage? exam scripts. The system displays
directed graphs to model interactions
between features and supports exploratory
search over a set of learner scripts. We
illustrate how the interface can support
the investigation of the co-occurrence
of many individual features, and discuss
how such investigations can shed light on
understanding the linguistic abilities that
characterise different levels of attainment
and, more generally, developmental aspects
of learner grammars.
1 Introduction
The Common European Framework of Reference
for Languages (CEFR)1 is an international bench-
mark of language attainment at different stages of
learning. The English Profile (EP)2 research pro-
gramme aims to enhance the learning, teaching
and assessment of English as an additional lan-
guage by creating detailed reference level descrip-
tions of the language abilities expected at each
level. As part of our research within that frame-
work, we modify and combine techniques devel-
oped for information visualisation with method-
ologies from computational linguistics to support
a novel and more empirical perspective on CEFR
1http://www.coe.int/t/dg4/linguistic/cadre en.asp
2http://www.englishprofile.org/
levels. In particular, we build a visual user in-
terface (hereafter UI) which aids the develop-
ment of hypotheses about learner grammars us-
ing graphs of linguistic features discriminating
pass/fail exam scripts for intermediate English.
Briscoe et al (2010) use supervised discrimi-
native machine learning methods to automate the
assessment of ?English as a Second or Other Lan-
guage? (ESOL) exam scripts, and in particular, the
First Certificate in English (FCE) exam, which
assesses English at an upper-intermediate level
(CEFR level B2). They use a binary discrimina-
tive classifier to learn a linear threshold function
that best discriminates passing from failing FCE
scripts, and predict whether a script can be clas-
sified as such. To facilitate learning of the clas-
sification function, the data should be represented
appropriately with the most relevant set of (lin-
guistic) features. They found a discriminative fea-
ture set includes, among other feature types, lexi-
cal and part-of-speech (POS) ngrams. We extract
the discriminative instances of these two feature
types and focus on their linguistic analysis3. Ta-
ble 1 presents a small subset ordered by discrimi-
native weight.
The investigation of discriminative features can
offer insights into assessment and into the linguis-
tic properties characterising the relevant CEFR
level. However, the amount and variety of data
potentially made available by the classifier is con-
siderable, as it typically finds hundreds of thou-
sands of discriminative feature instances. Even
if investigation is restricted to the most discrim-
inative ones, calculations of relationships be-
3Briscoe et al (2010) POS tagged and parsed the data
using the RASP toolkit (Briscoe et al, 2006). POS tags are
based on the CLAWS tagset.
35
tween features can rapidly grow and become over-
whelming. Discriminative features typically cap-
ture relatively low-level, specific and local prop-
erties of texts, so features need to be linked to the
scripts they appear in to allow investigation of the
contexts in which they occur. The scripts, in turn,
need to be searched for further linguistic prop-
erties in order to formulate and evaluate higher-
level, more general and comprehensible hypothe-
ses which can inform reference level descriptions
and understanding of learner grammars.
The appeal of information visualisation is to
gain a deeper understanding of important phe-
nomena that are represented in a database (Card et
al., 1999) by making it possible to navigate large
amounts of data for formulating and testing hy-
potheses faster, intuitively, and with relative ease.
An important challenge is to identify and assess
the usefulness of the enormous number of pro-
jections that can potentially be visualised. Explo-
ration of (large) databases can lead quickly to nu-
merous possible research directions; lack of good
tools often slows down the process of identifying
the most productive paths to pursue.
In our context, we require a tool that visu-
alises features flexibly, supports interactive inves-
tigation of scripts instantiating them, and allows
statistics about scripts, such as the co-occurrence
of features or presence of other linguistic proper-
ties, to be derived quickly. One of the advantages
of using visualisation techniques over command-
line database search tools is that Second Lan-
guage Acquisition (SLA) researchers and related
users, such as assessors and teachers, can access
scripts, associated features and annotation intu-
itively without the need to learn query language
syntax.
We modify previously-developed visualisation
techniques (Di Battista et al, 1999) and build a
visual UI supporting hypothesis formation about
learner grammars. Features are grouped in terms
of their co-occurrence in the corpus and directed
graphs are used in order to illustrate their rela-
tionships. Selection of different feature combi-
nations automatically generates queries over the
data and returns the relevant scripts as well as as-
sociations with meta-data and different types of
errors committed by the learners4. In the next sec-
4Our interface integrates a command-line Lucene search
tool (Gospodnetic and Hatcher, 2004) developed by Gram
and Buttery (2009).
Feature Example
VM RR (POS bigram: +) could clearly
, because (word bigram: ?) , because of
necessary (word unigram: +) it is necessary that
the people (word bigram: ?) *the people are clever
VV? VV? (POS bigram: ?) *we go see film
NN2 VVG (POS bigram: +) children smiling
Table 1: Subset of features ordered by discriminative
weight; + and ? show their association with either
passing or failing scripts.
tions we describe in detail the visualiser, illustrate
how it can support the investigation of individual
features, and discuss how such investigations can
shed light on the relationships between features
and developmental aspects of learner grammars.
To the best of our knowledge, this is the first
attempt to visually analyse as well as perform
a linguistic interpretation of discriminative fea-
tures that characterise learner English. We also
apply our visualiser to a set of 1,244 publically-
available FCE ESOL texts (Yannakoudakis et al,
2011) and make it available as a web service to
other researchers5.
2 Dataset
We use texts produced by candidates taking the
FCE exam, which assesses English at an upper-
intermediate level. The FCE texts, which are
part of the Cambridge Learner Corpus6, are pro-
duced by English language learners from around
the world sitting Cambridge Assessment?s ESOL
examinations7. The texts are manually tagged
with information about linguistic errors (Nicholls,
2003) and linked to meta-data about the learners
(e.g., age and native language) and the exam (e.g.,
grade).
3 The English Profile visualiser
3.1 Basic structure and front-end
The English Profile (EP) visualiser is developed
in Java and uses the Prefuse library (Heer et
al., 2005) for the visual components. Figure 1
shows its front-end. Features are represented
5Available by request: http://ilexir.co.uk/applications/ep-
visualiser/
6http://www.cup.cam.ac.uk/gb/elt/catalogue/subject/
custom/item3646603/
7http://www.cambridgeesol.org/
36
Figure 1: Front-end of the EP visualiser.
by a labelled node and displayed in the central
panel; positive features (i.e., those associated with
passing the exam) are shaded in a light green
colour while negative ones are light red8. A field
at the bottom right supports searching for fea-
tures/nodes that start with specified characters and
highlighting them in blue. An important aspect is
the display of feature patterns, discussed in more
detail in the next section (3.2).
3.2 Feature relations
Crucial to understanding discriminative features
is finding the relationships that hold between
them. We calculate co-occurrences of features at
the sentence-level in order to extract ?meaningful?
relations and possible patterns of use. Combi-
nations of features that may be ?useful? are kept
while the rest are discarded. ?Usefulness? is mea-
sured as follows:
Consider the set of all the sentences in the cor-
pus S = {s1, s2, ..., sN} and the set of all the fea-
tures F = {f1, f2, ..., fM}. A feature fi ? F is
associated with a feature fj ? F , where i 6= j
and 1 ? i, j ? M , if their relative co-occurrence
score is within a predefined range:
score(fj , fi) =
?N
k=1 exists(fj , fi, sk)
?N
k=1 exists(fi, sk)
(1)
8Colours can be customised by the user.
where sk ? S, 1 ? k ? N , exists() is a
binary function that returns 1 if the input fea-
tures occur in sk, and 0 ? score(fj , fi) ? 1.
We group features in terms of their relative co-
occurrence within sentences in the corpus and dis-
play these co-occurrence relationships as directed
graphs. Two nodes (features) are connected by
an edge if their score, based on Equation (1), is
within a user-defined range (see example below).
Given fi and fj , the outgoing edges of fi are mod-
elled using score(fj , fi) and the incoming edges
using score(fi, fj). Feature relations are shown
via highlighting of features when the user hovers
the cursor over them, while the strength of the re-
lations is visually encoded in the edge width.
For example, one of the highest-weighted pos-
itive discriminative features is VM RR (see Ta-
ble 1), which captures sequences of a modal
auxiliary followed by an adverb as in will al-
ways (avoid) or could clearly (see). Investigat-
ing its relative co-occurrence with other features
using a score range of 0.8?1 and regardless of
directionality, we find that VM RR is related to
the following: (i) POS ngrams: RR VB? AT1,
VM RR VB?, VM RR VH?, PPH1 VM RR,
VM RR VV?, PPIS1 VM RR, PPIS2 VM RR,
RR VB?; (ii) word ngrams: will also, can only,
can also, can just. These relations show us the
37
syntactic environments of the feature (i) or its
characteristic lexicalisations (ii).
3.3 Dynamic creation of graphs via selection
criteria
Questions relating to a graph display may include
information about the most connected nodes, sep-
arate components of the graph, types of intercon-
nected features, etc. However, the functionality,
usability and tractability of graphs is severely lim-
ited when the number of nodes and edges grows
by more than a few dozen (Fry, 2007). In order
to provide adequate information, but at the same
time avoid overly complex graphs, we support dy-
namic creation and visualisation of graphs using
a variety of selection criteria. The EP visualiser
supports the flexible investigation of the top 4,000
discriminative features and their relations.
The Menu item on the top left of the UI in Fig-
ure 1 activates a panel that enables users to select
the top N features to be displayed. The user can
choose whether to display positive and/or neg-
ative features and set thresholds for, as well as
rank by discriminative weight, connectivity with
other features (i.e., the number of features it is
connected to), and frequency. For instance, a
user can choose to investigate features that have
a connectivity between 500 and 900, rank them
by frequency and display the top 100. Highly-
connected features might tell us something about
the learner grammar while infrequent features, al-
though discriminative, might not lead to useful
linguistic insights. Additionally, users can in-
vestigate feature relations and set different score
ranges according to Equation (1), which controls
the edges to be displayed.
Figure 2(a) presents the graph of the 5 most
frequent negative features, using a score range
of 0.8?1. The system displays only one edge,
while the rest of the features are isolated. How-
ever, these features might be related to other fea-
tures from the list of 4,000 (which are not dis-
played since they are not found in the top N
list of features). Blue aggregation markers in the
shape of a circle, located at the bottom right of
each node, are used to visually display that in-
formation. When a node with an aggregation
marker is selected, the system automatically ex-
pands the graph and displays the related features.
The marker shape of an expanded node changes
to a star, while a different border stroke pattern
(a) Graph of the top 5 most fre-
quent negative features using a
score range of 0.8?1.
(b) Expanded graph when the aggregation marker for the
feature VVD II is selected.
Figure 2: Dynamic graph creation.
is used to visually distinguish the revealed nodes
from the top N . Figure 2(b) presents the ex-
panded graph when the aggregation marker for the
feature VVD II is selected. If the same aggrega-
tion marker is selected twice, the graph collapses
and returns to its original form.
3.4 Feature?Error relations
The FCE texts have been manually error-coded
(Nicholls, 2003) so it is possible to find associa-
tions between discriminative features and specific
error types. The Feature?Error relations compo-
nent on the left of Figure 1 displays a list of the
features, ranked by their discriminative weight,
together with statistics on their relations with er-
rors. Feature?error relations are computed at the
sentence level by calculating the proportion of
sentences containing a feature that also contain
a specific error (similar to Equation (1)). In the
example in Figure 1, we see that 27% of the sen-
tences that contain the feature bigram the people
also have an unnecessary determiner (UD) error,
while 14% have a replace verb (RV) error9.
9In the example image we only output the top 5 errors
(can be customised by the user).
38
Figure 3: Sentences, split by grade, containing occurrences of how to and RGQ TO VV?. The list on the left
gives error frequencies for the matching scripts, including the frequencies of lemmata and POSs inside an error.
3.5 Searching the data
In order to allow the user to explore how fea-
tures are related to the data, the EP visualiser
supports browsing operations. Selecting multiple
features ? highlighted in yellow ? and clicking
on the button get scripts returns relevant scripts.
The right panel of the front-end in Figure 1 dis-
plays a number of search and output options.
Users can choose to output the original/error-
coded/POS-tagged text and/or the grammatical
relations found by the RASP parser (Briscoe et
al., 2006), while different colours are used in or-
der to help readability. Data can be retrieved at
the sentence or script level and separated accord-
ing to grade. Additionally, Boolean queries can be
executed in order to examine occurrences of (se-
lected features and) specific errors only10. Also,
users can investigate scripts based on meta-data
information such as learner age.
Figure 3 shows the display of the system when
the features how to and RGQ TO VV? (how to
followed by a verb in base form) are selected. The
text area in the centre displays sentences instanti-
ating them. A search box at the top supports nav-
10For example, users can activate the Scripts with errors:
option and type ?R OR W?. This will return sentences con-
taining replace or word order errors.
igation, highlighting search terms in red, while
a small text area underneath displays the current
search query, the size of the database and the num-
ber of matching scripts or sentences. The Errors
by decreasing frequency pane on the left shows
a list of the errors found in the matching scripts,
ordered by decreasing frequency. Three different
tabs (lemma, POS and lemma POS) provide in-
formation about and allow extraction of counts of
lemmata and POSs inside an error tag.
3.6 Learner native language
Research on SLA highlights the possible effect of
a native language (L1) on the learning process.
Using the Menu item on the top left corner of
Figure 1, users can select the language of inter-
est while the system displays a new window with
an identical front-end and functionality. Feature?
error statistics are now displayed per L1, while
selecting multiple features returns scripts written
by learners speaking the chosen L1.
4 Interpreting discriminative features: a
case study
We now illustrate in greater depth how the EP vi-
sualiser can support interpretation of discrimina-
tive features: the POS trigram RG JJ NN1 (?) is
39
the 18th most discriminative (negative) feature. It
corresponds to a sequence of a degree adverb fol-
lowed by an adjective and a singular noun as in
very good boy. The question is why such a fea-
ture is negative since the string is not ungrammat-
ical. Visualisation of this feature using the ?dy-
namic graph creation? component of the visualiser
allows us to see the features it is related to. This
offers an intuitive and manageable way of inves-
tigating the large number of underlying discrimi-
native features.
We find that RG JJ NN1 is related to its dis-
criminative lexicalisation, very good (?), which
is the 513th most discriminative feature. Also,
it is related to JJ NN1 II (?) (e.g., difficult sport
at), ranked 2,700th, which suggests a particular
context for RG JJ NN1 when the noun is fol-
lowed by a preposition. Searching for this con-
junction of features in scripts, we get production
examples like 1a,b,c. Perhaps more interestingly,
RG JJ NN1 is related to VBZ RG (?) (ranked
243rd): is followed by a degree adverb. This
relation suggests a link with predicative struc-
tures since putting the two ngrams together yields
strings VBZ RG JJ NN1 corresponding to exam-
ples like 1c,d; if we also add II we get examples
like 1c.
1a It might seem to be very difficult sport at the
beginning.
1b We know a lot about very difficult situation
in your country.
1c I think it?s very good idea to spending vaca-
tion together.
1d Unix is very powerful system but there is one
thing against it.
The associations between features already give
an idea of the source of the problem. In the se-
quences including the verb be the indefinite ar-
ticle is omitted. So the next thing to investigate
is if indeed RG JJ NN1 is associated with ar-
ticle omission, not only in predicative contexts,
but more generally. The Feature?Error relations
component of the UI reveals an association with
MD (missing determiner) errors: 23% of sen-
tences that contain RG JJ NN1 also have a MD
error. The same holds for very good, JJ NN1 II
and VBZ RG with percentages 12%, 14% and
Language f1 f2 f3 f4
all 0.26 0.40 0.02 0.03
Turkish 0.29 0.48 0.04 0.03
Japanese 0.17 0.39 0.02 0.02
Korean 0.30 0.58 0.06 0.03
Russian 0.35 0.52 0.03 0.03
Chinese 0.25 0.56 0.02 0.03
French 0.21 0.41 0.00 0.03
German 0.19 0.41 0.00 0.02
Spanish 0.27 0.32 0.00 0.03
Greek 0.30 0.35 0.02 0.02
Table 2: f1/2/3/4:doc ratios for different L1s.
15% respectively. We then compared the num-
ber of MD errors per script across different types
of scripts. Across all scripts the ratio MD:doc
is 2.18, that is, approximately 2 MD errors per
script; in RG JJ NN1 scripts this ratio goes up
to 2.75, so that each script has roughly 3 MD
errors. VBZ RG follows with 2.68, JJ NN1 II
with 2.48, and very good with 2.32. In scripts
containing all features the ratio goes up to 4.02
(3.68 without very good), and in scripts contain-
ing VBZ RG JJ the ratio goes up to 2.73. Also,
in most of these scripts the error involves the in-
definite article. The emerging picture then is that
there is a link between these richer nominal struc-
tures that include more than one modifier and the
omission of the article. Two questions arise: (i)
why these richer nominals should associate with
article omission and (ii) why only singular nouns
are implicated in this feature.
Article omission errors are typical of learn-
ers coming from L1s lacking an article sys-
tem (Robertson, 2000; Ionin and Montrul, 2010;
Hawkins and Buttery, 2010). Trenkic (2008) pro-
poses that such learners analyse articles as adjecti-
val modifiers rather than as a separate category of
determiners or articles. When no adjective is in-
volved, learners may be aware that bare nominals
are ungrammatical in English and provide the ar-
ticle. However, with complex adjectival phrases,
learners may omit the article because of the pres-
ence of a degree adverb. In order to evaluate this
hypothesis further we need to investigate if arti-
cle omission is indeed more pronounced in our
data with more complex adjectival phrases e.g.,
very difficult situation than with simpler ones e.g.,
nice boy and whether this is primarily the case for
40
learners from L1s lacking articles.
Again, using the Errors by decreasing fre-
quency pane we found that the MD:doc ratio in
scripts containing the bigram JJ NN1 is 2.20. Ad-
ditionally, in scripts containing JJ NN1 and not
RG JJ NN1 it goes down to 2.04. These results
are much lower compared to the MD:doc ratio
in scripts containing RG JJ NN1 and/or the fea-
tures with which it is related (see above), fur-
ther supporting our hypothesis. We also found
the ratio of RG JJ NN1 (f1) occurrences per doc-
ument across different L1s, as well as the ratio
of VBZ RG JJ (f2), VBZ RG JJ NN1 (f3) and
RG JJ NN1 II (f4). As shown in Table 2 there
is no correlation between these features and the
L1, with the exception of f1 and f2 which are
more pronounced in Korean and Russian speak-
ers, and of f3 which seems completely absent
from French, German and Spanish which all have
articles. The exception is Greek which has articles
but uses bare nominals in predicative structures.
However, a more systematic pattern is revealed
when relations with MD errors are considered (us-
ing the Feature?Error relations and Errors by de-
creasing frequency components for different L1s).
As shown in Table 3, there is a sharp contrast be-
tween L1s with articles (French, German, Spanish
and Greek) and those without (Turkish, Japanese,
Korean, Russian, Chinese), which further sup-
ports our hypothesis. A further question is why
only the singular article is implicated in this fea-
ture. The association with predicative contexts
may provide a clue. Such contexts select nomi-
nals which require the indefinite article only in the
singular case; compare Unix is (a) very powerful
system with Macs are very elegant machines.
In sum, navigating the UI, we formed some
initial interpretations for why a particular feature
is negatively discriminative. In particular, nomi-
nals with complex adjectival phrases appear par-
ticularly susceptible to article omission errors by
learners of English with L1s lacking articles. The
example illustrates not just the usefulness of visu-
alisation techniques for navigating and interpret-
ing large amounts of data, but, more generally
the relevance of features weighted by discrimina-
tive classifiers. Despite being superficial in their
structure, POS ngrams can pick up syntactic envi-
ronments linked to particular phenomena. In this
case, the features do not just identify a high rate of
article omission errors, but, importantly, a partic-
sentences% MD:doc
Language f1 f2 f1 f2
all 23.0 15.6 2.75 2.73
Turkish 45.2 29.0 5.81 5.82
Japanese 44.4 22.3 4.48 3.98
Korean 46.7 35.0 5.48 5.31
Russian 46.7 23.4 5.42 4.59
Chinese 23.4 13.5 3.58 3.25
French 6.9 6.7 1.32 1.49
German 2.1 3.0 0.91 0.92
Spanish 10.0 9.6 1.18 1.35
Greek 15.5 12.9 1.60 1.70
Table 3: f1/2 relations with MD errors for different
L1s, where sentences% shows the proportion of sen-
tences containing f1/2 that also contain a MD.
ular syntactic environment triggering higher rates
of such errors.
5 Previous work
To the best of our knowledge, this is the first at-
tempt to visually analyse as well as perform a
linguistic interpretation of discriminative features
that characterise learner English.
Collins (2010) in his dissertation addresses vi-
sualisation for NLP research. The Bubble Sets vi-
sualisation draws secondary set relations around
arbitrary collections of items, such as a linguis-
tic parse tree. VisLink provides a general plat-
form within which multiple visualisations of lan-
guage (e.g., a force-directed graph and a radial
graph) can be connected, cross-queried and com-
pared. Moreover, he explores the space of content
analysis. DocuBurst is an interactive visualisation
of document content, which spatially organizes
words using an expert-created ontology (e.g.,
WordNet). Parallel Tag Clouds combine keyword
extraction and coordinated visualisations to pro-
vide comparative overviews across subsets of a
faceted text corpus. Recently, Rohrdantz et al
(2011) proposed a new approach to detecting and
investigating changes in word senses by visually
modelling and plotting aggregated views about
the diachronic development in word contexts.
Visualisation techniques have been success-
fully used in other areas including the humanities
(e.g., Plaisant et al (2006) and Don et al (2007)),
as well as genomics (e.g., Meyer et al (2010a)
and Meyer et al (2010b)). For example, Meyer
41
et al (2010a) present a system that supports the
inspection and curation of data sets showing gene
expression over time, in conjunction with the spa-
tial location of the cells where the genes are ex-
pressed.
Graph layouts have been effectively used in
the analysis of domains such as social networks
(e.g., terrorism network) to allow for a system-
atic exploration of a variety of Social Network
Analysis measures (e.g., Gao et al (2009) and
Perer and Shneiderman (2006)). Heer and Boyd
(2005) have implemented Vizster, a visualisation
system for the exploration of on-line social net-
works (e.g., facebook) designed to facilitate the
discovery of people, promote awareness of com-
munity structure etc. Van Ham et al (2009) intro-
duce Phrase Net, a system that analyses unstruc-
tured text by taking as input a predefined pattern
and displaying a graph whose nodes are words
and whose edges link the words that are found as
matches.
We believe our integration of highly-weighted
discriminative features identified by a supervised
classifier into a graph-based visualiser to support
linguistic SLA research is, however, novel.
6 Conclusions
We have demonstrated how a data-driven ap-
proach to learner corpora can support SLA re-
search when guided by discriminative features
and augmented with visualisation tools. We de-
scribed a visual UI which supports exploratory
search over a corpus of learner texts using di-
rected graphs of features, and presented a case
study of how the system allows SLA researchers
to investigate the data and form hypotheses about
intermediate level learners. Although the use-
fulness of the EP visualiser should be con-
firmed through more rigorous evaluation tech-
niques, such as longitudinal case studies (Shnei-
derman and Plaisant, 2006; Munzner, 2009) with
a broad field of experts, these initial explorations
are encouraging. One of the main advantages of
using visualisation techniques over command-line
database search tools is that SLA researchers can
start developing and testing hypotheses without
the need to learn a query syntax first.
We would also like to point out that we adopted
a user-driven development of the visualiser based
on the needs of the third author, an SLA re-
searcher who acted as a design partner during
the development of the tool and was eager to use
and test it. There were dozens of meetings over
a period of seven months, and the feedback on
early interfaces was incorporated in the version
described here. After the prototype reached a sat-
isfactory level of stability, the final version overall
felt enjoyable and inviting, as well as allowed her
to form hypotheses and draw on different types of
evidence in order to substantiate it (Alexopoulou
et al, 2012). Future work will include the devel-
opment, testing and evaluation of the UI with a
wider range of users, as well as be directed to-
wards investigation and evaluation of different vi-
sualisation techniques of machine learned or ex-
tracted features that support hypothesis formation
about learner grammars.
Acknowledgments
We are grateful to Cambridge ESOL for support-
ing this research. We would like to thank Marek
Rei, ?istein Andersen, Paula Buttery and Ange-
liki Salamoura for fruitful discussions and feed-
back, Tim Parish for making the tool available on
the web, as well as the anonymous reviewers for
their valuable comments and suggestions.
References
Theodora Alexopoulou, Helen Yannakoudakis, and
Angeliki Salamoura. 2012. Classifying interme-
diate Learner English: a data-driven approach to
learner corpora. to appear.
Ted Briscoe, John Carroll, and Rebecca Watson. 2006.
The second release of the RASP system. In Pro-
ceedings of the COLING/ACL, volume 6.
Ted Briscoe, Ben Medlock, and ?istein Andersen.
2010. Automated Assessment of ESOL Free Text
Examinations. University of Cambridge, Computer
Laboratory, TR-790.
Stuart K. Card, Jock D. Mackinlay, and Ben Shneider-
man. 1999. Readings in information visualization:
using vision to think. Morgan Kaufmann.
Christopher M. Collins. 2010. Interactive Visualiza-
tions of natural language. Ph.D. thesis, University
of Toronto.
Giuseppe Di Battista, Peter Eades, Roberto Tamassia,
and Ioannis G. Tollis. 1999. Graph Drawing: Al-
gorithms for the Visualization of Graphs. Prentice
Hall Press.
Anthony Don, Elena Zheleva, Machon Gregory,
Sureyya Tarkan, Loretta Auvil, Tanya Clement, Ben
Shneiderman, and Catherine Plaisant. 2007. Dis-
covering interesting usage patterns in text collec-
tions: integrating text mining with visualization. In
42
Proceedings of the sixteenth ACM conference on in-
formation and knowledge management, pages 213?
222. ACM.
Ben Fry. 2007. Visualizing Data: Exploring and
Explaining Data with the Processing Environment.
O?Reilly Media.
Jie Gao, Kazuo Misue, and Jiro Tanaka. 2009. A
Multiple-Aspects Visualization Tool for Exploring
Social Networks. Human Interface and the Man-
agement of Information, pages 277?286.
Otis Gospodnetic and Erik Hatcher. 2004. Lucene in
Action. Manning Publications.
Lu Gram and Paula Buttery. 2009. A tutorial intro-
duction to iLexIR Search. unpublished.
John Hawkins and Paula Buttery. 2010. Criterial fea-
tures in Learner Corpora: theory and illustrations.
English Profile Journal, 1(1):1?23.
Jeffrey Heer and Danah Boyd. 2005. Vizster: visual-
izing online social networks. IEEE Symposium on
Information Visualization (INFOVIS), pages 32?39.
Jeffrey Heer, Stuart K. Card, and James A. Landay.
2005. Prefuse: a toolkit for interactive informa-
tion visualization. In Proceedings of the SIGCHI
conference on Human factors in computing systems,
pages 421?430, New York, USA. ACM.
Tania Ionin and Silvina Montrul. 2010. The role
of l1 transfer in the interpretation of articles with
definite plurals in l2 english. Language Learning,
60(4):877?925.
Miriah Meyer, Tamara Munzner, Angela DePace, and
Hanspeter Pfister. 2010a. MulteeSum: a tool for
comparative spatial and temporal gene expression
data. IEEE transactions on visualization and com-
puter graphics, 16(6):908?17.
Miriah Meyer, Bang Wong, Mark Styczynski, Tamara
Munzner, and Hanspeter Pfister. 2010b. Pathline:
A tool for comparative functional genomics. Com-
puter Graphics, 29(3).
Tamara Munzner. 2009. A Nested Model for Visual-
ization Design and Validation. IEEE Transactions
on Visualization and Computer Graphics, 15(6).
Diane Nicholls. 2003. The Cambridge Learner
Corpus-error coding and analysis for lexicography
and ELT. In Proceedings of the Corpus Linguistics
2003 conference, pages 572?581.
Adam Perer and Ben Shneiderman. 2006. Balanc-
ing Systematic and Flexible Exploration of Social
Networks. IEEE Transactions on Visualization and
Computer Graphics, 12(5):693?700.
Catherine Plaisant, James Rose, Bei Yu, Loretta Auvil,
Matthew G. Kirschenbaum, Martha N. Smith,
Tanya Clement, and Greg Lord. 2006. Exploring
erotics in Emily Dickinson?s correspondence with
text mining and visual interfaces. In Proceedings of
the 6th ACM/IEEE-CS joint conference on Digital
libraries, pages 141?150. ACM.
Daniel Robertson. 2000. Variability in the use of the
English article system by Chinese learners of En-
glish. Second Language Research, 2:135?172.
Christian Rohrdantz, Annette Hautli, Thomas Mayer,
and Miriam Butt. 2011. Towards tracking seman-
tic change by visual analytics. Proceedings of the
49th Meeting of the Association for Computational
Linguistics, pages 305?310.
Ben Shneiderman and Catherine Plaisant. 2006.
Strategies for evaluating information visualization
tools: multi-dimensional in-depth long-term case
studies. In Proceedings of the 2006 AVI workshop
on BEyond time and errors: novel evaluation meth-
ods for information visualization. ACM.
Danijela Trenkic. 2008. The representation of English
articles in second language grammars: Determiners
or adjectives? Bilingualism: Language and Cogni-
tion, 11(01):1?18.
Frank Van Ham, Martin Wattenberg, and Fernanda B.
Vie?gas. 2009. Mapping text with phrase nets.
IEEE Transactions on Visualization and Computer
Graphics, 15(6):1169?76.
Helen Yannakoudakis, Ted Briscoe, and Ben Medlock.
2011. A New Dataset and Method for Automati-
cally Grading ESOL Texts. In Proceedings of the
49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies.
43
The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 33?43,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Modeling coherence in ESOL learner texts
Helen Yannakoudakis
Computer Laboratory
University of Cambridge
United Kingdom
Helen.Yannakoudakis@cl.cam.ac.uk
Ted Briscoe
Computer Laboratory
University of Cambridge
United Kingdom
Ted.Briscoe@cl.cam.ac.uk
Abstract
To date, few attempts have been made to de-
velop new methods and validate existing ones
for automatic evaluation of discourse coher-
ence in the noisy domain of learner texts.
We present the first systematic analysis of
several methods for assessing coherence un-
der the framework of automated assessment
(AA) of learner free-text responses. We ex-
amine the predictive power of different coher-
ence models by measuring the effect on per-
formance when combined with an AA system
that achieves competitive results, but does not
use discourse coherence features, which are
also strong indicators of a learner?s level of at-
tainment. Additionally, we identify new tech-
niques that outperform previously developed
ones and improve on the best published result
for AA on a publically-available dataset of En-
glish learner free-text examination scripts.
1 Introduction
Automated assessment (hereafter AA) systems of
English learner text assign grades based on textual
features which attempt to balance evidence of writ-
ing competence against evidence of performance er-
rors. Previous work has mostly treated AA as a
supervised text classification or regression task. A
number of techniques have been investigated, in-
cluding cosine similarity of feature vectors (Attali
and Burstein, 2006), often combined with dimen-
sionality reduction techniques such as Latent Se-
mantic Analysis (LSA) (Landauer et al, 2003), and
generative machine learning models (Rudner and
Liang, 2002) as well as discriminative ones (Yan-
nakoudakis et al, 2011). As multiple factors influ-
ence the linguistic quality of texts, such systems ex-
ploit features that correspond to different properties
of texts, such as grammar, style, vocabulary usage,
topic similarity, and discourse coherence and cohe-
sion.
Cohesion refers to the use of explicit linguistic
cohesive devices (e.g., anaphora, lexical semantic
relatedness, discourse markers, etc.) within a text
that can signal primarily suprasentential discourse
relations between textual units (Halliday and Hasan,
1976). Cohesion is not the only mechanism of dis-
course coherence, which may also be inferred from
meaning without presence of explicit linguistic cues.
Coherence can be assessed locally in terms of tran-
sitions between adjacent clauses, parentheticals, and
other textual units capable of standing in discourse
relations, or more globally in terms of the overall
topical coherence of text passages.
There is a large body of work that has investi-
gated a number of different coherence models on
news texts (e.g., Lin et al (2011), Elsner and Char-
niak (2008), and Soricut and Marcu (2006)). Re-
cently, Pitler et al (2010) presented a detailed survey
of current techniques in coherence analysis of ex-
tractive summaries. To date, however, few attempts
have been made to develop new methods and vali-
date existing ones for automatic evaluation of dis-
course coherence and cohesion in the noisy domain
of learner texts, where spelling and grammatical er-
rors are common.
Coherence quality is typically present in marking
criteria for evaluating learner texts, and it is iden-
33
tified by examiners as a determinant of the overall
score. Thus we expect that adding a coherence met-
ric to the feature set of an AA system would better
reflect the evaluation performed by examiners and
improve performance. The goal of the experiments
presented in this paper is to measure the effect a
number of (previously-developed and new) coher-
ence models have on performance when combined
with an AA system that achieves competitive results,
but does not use discourse coherence features.
Our contribution is threefold: 1) we present the
first systematic analysis of several methods for as-
sessing discourse coherence in the framework of
AA of learner free-text responses, 2) we identify
new discourse features that serve as proxies for the
level of (in)coherence in texts and outperform pre-
viously developed techniques, and 3) we improve
the best results reported by Yannakoudakis et al
(2011) on the publically available ?English as a Sec-
ond or Other Language? (ESOL) corpus of learner
texts (to date, this is the only public-domain corpus
that contains grades). Finally, we explore the utility
of our best model for assessing the incoherent ?out-
lier? texts used in Yannakoudakis et al (2011).
2 Experimental Design & Background
We examine the predictive power of a number of
different coherence models by measuring the effect
on performance when combined with an AA system
that achieves state-of-the-art results, but does not
use discourse coherence features. Specifically, we
describe a number of different experiments improv-
ing on the AA system presented in Yannakoudakis
et al (2011); AA is treated as a rank preference
supervised learning problem and ranking Support
Vector Machines (SVMs) (Joachims, 2002) are used
to explicitly model the grade relationships between
scripts. This system uses a number of different lin-
guistic features that achieve good performance on
the AA task. However, these features only focus on
lexical and grammatical properties, as well as errors
within individual sentences, ignoring discourse co-
herence, which is also present in marking criteria for
evaluating learner texts, as well as a strong indicator
of a writer?s understanding of a language.
Also, in Yannakoudakis et al (2011), experiments
are presented that test the validity of the system
using a number of automatically-created ?outlier?
texts. The results showed that the model is vulner-
able to input where individually high-scoring sen-
tences are randomly ordered within a text. Failing to
identify such pathological cases makes AA systems
vulnerable to subversion by writers who understand
something of its workings, thus posing a threat to
their validity. For example, an examinee might learn
by rote a set of well-formed sentences and repro-
duce these in an exam in the knowledge that an AA
system is not checking for prompt relevance or co-
herence1.
3 Dataset & Experimental Setup
We use the First Certificate in English (FCE) ESOL
examination scripts2 (upper-intermediate level as-
sessment) described in detail in Yannakoudakis et al
(2011), extracted from the Cambridge Learner Cor-
pus3 (CLC). The dataset consists of 1,238 texts be-
tween 200 and 400 words produced by 1,238 distinct
learners in response to two different prompts. An
overall mark has been assigned in the range 1?40.
For all experiments, we use a series of 5-fold
cross-validation runs on 1,141 texts from the exami-
nation year 2000 to evaluate performance as well as
generalization of numerous models. Moreover, we
identify the best model on year 2000 and we also test
it on 97 texts from the examination year 2001, previ-
ously used in Yannakoudakis et al (2011) to report
the best published results. Validating the results on
a different examination year tests generalization to
some prompts not used in 2000, and also allows us to
test correlation between examiners and the AA sys-
tem. Again, we treat AA as a rank preference learn-
ing problem and use SVMs, utilizing the SVMlight
package (Joachims, 2002), to facilitate comparison
with Yannakoudakis et al (2011).
4 Discourse Coherence
We focus on the development and evaluation of (au-
tomated) methods for assessing coherence in learner
1Powers et al (2002) report the results of a related exper-
iment with the AA system e-Rater, in which experts tried to
subvert the system by submitting essays they believed would be
inaccurately scored.
2http://ilexir.co.uk/applications/clc-fce-dataset/
3http://www.cup.cam.ac.uk/gb/elt/catalogue/subject/custom
/item3646603/
34
texts under the framework of AA. Most of the meth-
ods we investigate require syntactic analysis. As in
Yannakoudakis et al (2011), we analyze all texts us-
ing the RASP toolkit (Briscoe et al, 2006)4.
4.1 ?Superficial? Proxies
In this section we introduce diverse classes of ?su-
perficial? cohesive features that serve as proxies for
coherence. Surface text properties have been as-
sessed in the framework of automatic summary eval-
uation (Pitler et al, 2010), and have been shown to
significantly correlate with the fluency of machine-
translated sentences (Chae and Nenkova, 2009).
4.1.1 Part-of-Speech (POS) Distribution
The AA system described in Yannakoudakis et
al. (2011) exploited features based on POS tag se-
quences, but did not consider the distribution of POS
types across grades. In coherent texts, constituent
clauses and sentences are related and depend on each
other for their interpretation. Anaphors such as pro-
nouns link the current sentence to those where the
entities were previously mentioned. Pronouns can
be directly related to (lack of) coherence and make
intuitive sense as cohesive devices. We compute the
number of pronouns in a text and use it as a shallow
feature for capturing coherence.
4.1.2 Discourse Connectives
Discourse connectives (such as but or because) re-
late propositions expressed by different clauses or
sentences. The presence of such items in a text
should be indicative of (better) coherence. We thus
compute a number of shallow cohesive features as
proxies for coherence, based on fixed lists of words
belonging to the following categories: (a) Addition
(e.g., additionally), (b) Comparison (e.g., likewise),
(c) Contrast (e.g., whereas) and (d) Conclusion (e.g.,
therefore), and use the frequencies of these four cat-
egories as features.
4.1.3 Word Length
The previous AA system treated script length as
a normalizing feature, but otherwise avoided such
?superficial? proxies of text quality. However, many
cohesive words are longer than average, especially
for the closed-class functional component of English
4http://ilexir.co.uk/applications/rasp/
vocabulary. We thus assess the minimum, maximum
and average word length as a superficial proxy for
coherence.
4.2 Semantic Similarity
We explore the utility of inter-sentential feature
types for assessing discourse coherence. Among the
features used in Yannakoudakis et al (2011), none
explicitly captures coherence and none models inter-
sentential relationships. Incremental Semantic anal-
ysis (ISA) (Baroni et al, 2007) is a word-level dis-
tributional model that induces a semantic space from
input texts. ISA is a fully-incremental variation of
Random Indexing (RI) (Sahlgren, 2005), which can
efficiently capture second-order effects in common
with other dimensionality-reduction methods based
on singular value decomposition, but does not rely
on stoplists or global statistics for weighting pur-
poses.
Utilizing the S-Space package (Jurgens and
Stevens, 2010), we trained an ISA model5 using a
subset of ukWaC (Ferraresi et al, 2008), a large cor-
pus of English containing more than 2 billion tokens.
We used the POS tagger lexicon provided with the
RASP system to discard documents whose propor-
tion of valid English words to total words is less than
0.4; 78,000 documents were extracted in total and
were then preprocessed replacing URLs, email ad-
dresses, IP addresses, numbers and emoticons with
special markers. To measure local coherence we de-
fine the similarity between two sentences si and si+1
as the maximum cosine similarity between the his-
tory vectors of the words they contain. The overall
coherence of a text T is then measured by taking the
mean of all sentence-pair scores:
coherence(T ) =
?n?1
i=1 maxk,j sim(s
k
i , s
j
i+1)
n? 1
(1)
where sim(ski , s
j
i+1) is the cosine similarity between
the history vectors of the kth word in si and the
jth word in si+1, and n is the total number of
sentences6. We investigate the efficacy of ISA by
adding this coherence score, as well as the maximum
5The parameters of our ISA model are fairly standard: 1800
dimensions, a context window of 3 words, impact rate i =
0.0003 and decay rate km = 50.
6We exclude articles, conjunctions, prepositions and auxil-
iary verbs from the calculation of sentence similarity.
35
sim value found over the entire text, to the vectors
of features associated with a text. The hypothesis
is that the degree of semantic relatedness between
adjoining sentences serves as a proxy for local dis-
course coherence; that is, coherent text units contain
semantically-related words.
Higgins et al (2004) and Higgins and Burstein
(2007) use RI to determine the semantic similarity
between sentences of same/different discourse seg-
ments (e.g., from the essay thesis and conclusion, or
between sentences and the essay prompt), and assess
the percentage of sentences that are correctly clas-
sified as related or unrelated. The main differences
from our approach are that we assess the utility of se-
mantic space models for predicting the overall grade
for a text, in contrast to binary classification at the
sentence-level, and we use ISA rather than RI7.
4.3 Entity-based Coherence
The entity-based coherence model, proposed by
Barzilay and Lapata (2008), is one of the most pop-
ular statistical models of inter-sentential coherence,
and learns coherence properties similar to those em-
ployed by Centering Theory (Grosz et al, 1995).
Local coherence is modeled on the basis of se-
quences of entity mentions that are labeled with
their syntactic roles (e.g., subject, object). We con-
struct the entity grids using the Brown Coherence
Toolkit8,9 (Elsner and Charniak, 2011b), and use as
features the probabilities of different entity transi-
tion types, defined in terms of their role in adja-
cent sentences10. Burstein et al (2010) show how
the entity-grid can be used to discriminate high-
coherence from low-coherence learner texts. The
main difference with our approach is that we eval-
uate the entity-grid model in the context of AA text
grading, rather than binary classification.
7We also used RI in addition to ISA, and found that it did
not yield significantly different results. In particular, we trained
a RI model with 2,000 dimensions and a context window of 3
on the same ukWaC data. Below we only report results for the
fully-incremental ISA model.
8https://bitbucket.org/melsner/browncoherence
9The tool does not perform full coreference resolution; in-
stead, coreference is approximated by linking entities that share
a head noun.
10We represent entities with specified roles (Subject, Object,
Neither, Absent), use transition probabilities of length 2, 3 and
4, and a salience option of 2.
4.4 Pronoun Coreference Model
Pronominal anaphora is another important aspect
of coherence. Charniak and Elsner (2009) present
an unsupervised generative model of pronominal
anaphora for coherence modeling. In their imple-
mentation, they model each pronoun as generated by
an antecedent somewhere in the previous two sen-
tences. If a ?good? antecedent is found, the probabil-
ity of a pronoun will be high; otherwise, the proba-
bility will be low. The overall probability of a text
is then calculated as the probability of the result-
ing sequence of pronoun assignments. In our ex-
periments, we use the pre-trained model distributed
by Charniak and Elsner (2009) for news text to esti-
mate the probability of a text and include it as a fea-
ture. However, this model is trained on high-quality
texts, so performance may deteriorate when applied
to learner texts. It is not obvious how to train such
a model on learner texts and we leave this for future
research.
4.5 Discourse-new Model
Elsner and Charniak (2008) apply a discourse-new
classifier to model coherence. Their classifier dis-
tinguishes NPs whose referents have not been pre-
viously mentioned in the discourse from those that
have been already introduced, using a number of
syntactic and lexical features. To model coher-
ence, they assign each NP in a text a label Lnp ?
{new, old}11, and calculate the probability of a text
as ?np:NPsP (Lnp|np). Again, we use the pre-
trained model distributed by Charniak and Elsner
(2009) for news text to find the probability of a text
following Elsner and Charniak (2008) and include it
as a feature.
4.6 IBM Coherence Model
Soricut and Marcu (2006) adapted the IBM model
1 (Brown et al, 1994) used in machine translation
(MT) to model local discourse coherence. The intu-
ition behind the IBM model in MT is that the use of
certain words in a source language is likely to trig-
ger the use of certain words in a target language.
Instead, they hypothesized that the use of certain
words in a sentence tends to trigger the use of cer-
tain words in an adjoining sentence. In contrast to
11NPs with the same head are considered to be coreferent.
36
semantic space models such as ISA or RI (discussed
above), this method models the intuition that local
coherence is signaled by the identification of word
co-occurrence patterns across adjacent sentences.
We compute two features introduced by Soricut
and Marcu (2006): the forward likelihood and the
backward likelihood. The first refers to the likeli-
hood of observing the words in sentence si+1 condi-
tioned on si, and the latter to the likelihood of ob-
serving the words in si conditioned on si+1. We
extract 3 million adjacent sentences from ukWaC12,
and use the GIZA++ (Och and Ney, 2000) imple-
mentation of IBM model 1 to obtain the probabili-
ties of recurring patterns. The forward and backward
probabilities are calculated over the entire text, and
their values are used as features in our feature vec-
tors13. We further extend the above model and incor-
porate syntactic aspects of text coherence by train-
ing on POS tags instead of lexical items. We try to
model the intuition that local coherence is signaled
by the identification of POS co-occurrence patterns
across adjacent sentences, where the use of certain
POS tags in a sentence tends to trigger the use of
other POS tags in an adjacent sentence. We analyze
3 million adjacent sentences using the RASP POS
tagger and train the same IBM model to obtain the
probabilities of recurring POS patterns.
4.7 Lemma/POS Cosine Similarity
A simple method of incorporating (syntactic) as-
pects of text coherence is to use cosine similarity
between vectors of lemma and/or POS-tag counts in
adjacent sentences. We experiment with both: each
sentence is represented by a vector whose dimen-
sion depends on the total number of lemmas/POS-
types. The sentence vectors are weighted using
lemma/POS frequency, and the cosine similarity be-
tween adjacent sentences is calculated. The coher-
ence of a text T is then calculated as the average
value of cosine similarity over the entire text14:
coherence(T ) =
?n?1
i=1 sim(si, si+1)
n? 1
(2)
12We use the same subset of documents as the ones used to
train our ISA model in Section 4.2.
13Pitler et al (2010) have also investigated the IBM model to
measure text quality in automatically-generated texts.
14Pitler et al (2010) use POS cosine similarity to measure
continuity in automatically-generated texts.
4.8 Locally-Weighted Bag-of-Words
The popular bag-of-words (BOW) assumption rep-
resents a text as a histogram of word occurrences.
While computationally efficient, such a represen-
tation is unable to maintain any sequential infor-
mation. The locally-weighted bag-of-words (LOW-
BOW) framework, introduced by Lebanon et al
(2007), is a sequentially-sensitive alternative to
BOW. In BOW, we represent a text as a histogram
over the vocabulary used to generate that text. In
LOWBOW, a text is represented by a set of lo-
cal histograms computed across the whole text, but
smoothed by kernels centered on different locations.
More specifically, a smoothed characterization
of the local histogram is obtained by integrating a
length-normalized document with respect to a non-
uniform measure that is concentrated around a par-
ticular location ? ? [0, 1]. In accordance with the
statistical literature on non-parametric smoothing,
we refer to such a measure as a smoothing kernel.
The kernel parameters ? and ? specify the local his-
togram?s position in the text (i.e., where it is cen-
tered) and its scale (i.e., to what extent it is smoothed
over the surrounding region) respectively. In con-
trast to BOW or n-grams, which keep track of fre-
quently occurring patterns independent of their po-
sitions, this representation is able to robustly capture
medium and long range sequential trends in a text by
keeping track of changes in the histograms from its
beginning to end.
Geometrically, LOWBOW uses local smoothing
to embed texts as smooth curves in the multinomial
simplex. These curves summarize the progression
of semantic and/or statistical trends through the text.
By varying the amount of smoothing we obtain a
family of sequential representations possessing dif-
ferent sequential resolutions or scales. Low resolu-
tion representations capture topic trends and shifts
while ignoring finer details. High resolution repre-
sentations capture fine sequential details but make it
difficult to grasp the general trends within the text15.
Since coherence involves both cohesive lexical
devices and sequential progression within a text, we
believe that LOWBOW can be used to assess the se-
quential content and the global structure and coher-
15For more details regarding LOWBOW and its geometric
properties see Lebanon et al (2007).
37
ence of texts. We use a publically-available LOW-
BOW implementation16 to create local histograms
over word unigrams. For the LOWBOW kernel
smoothing function (see above), we use the Gaus-
sian probability density function restricted to [0, 1]
and re-normalized, and a smoothing ? value of 0.02.
Additionally, we consider a total number of 9 local
histograms (discourse segments). We further extend
the above model and incorporate syntactic aspects of
text coherence by using local histograms over POS
unigrams. This representation is able to capture se-
quential trends abstracted into POS tags. We try
to model the hypothesis that coherence is signaled
by sequential, mostly inter-sentential progression of
POS types.
Since each text is represented by a set of local
histrograms/vectors, and standard SVM kernels can-
not work with such input spaces, we use instead a
kernel defined over sets of vectors: the diffusion
kernel (Lafferty and Lebanon, 2005) compares lo-
cal histograms in a one-to-one fashion (i.e., his-
tograms at the same locations are compared to each
other), and has proven to be useful for related tasks
(Lebanon et al, 2007; Escalante et al, 2011). To the
best of our knowledge, LOWBOW representations
have not been investigated for coherence evaluation
(under the AA framework). So far, they have been
applied to discourse segmentation (AMIDA, 2007),
text categorization (Lebanon et al, 2007), and au-
thorship attribution (Escalante et al, 2011).
5 Evaluation
We examine the predictive power of each of the co-
herence models/features described in Section 4 by
measuring the effect on performance when com-
bined with an AA system that achieves state-of-the-
art results on the FCE dataset, but does not use dis-
course coherence features. In particular, we use the
system described in Yannakoudakis et al (2011) as
our baseline AA system. Discourse coherence is a
strong indicator of thorough knowledge of a second
language and thus we expect coherence features to
further improve performance of AA systems.
We evaluate the grade predictions of our mod-
els against the gold standard grades in the dataset
using Pearson?s product-moment correlation coeffi-
16http://goo.gl/yQ0Q0
cient (r) and Spearman?s rank correlation coefficient
(?) as is standard in AA research (Briscoe et al,
2010). Table 1 gives results obtained by augmenting
the baseline model with each of the coherence fea-
tures described above. In each of these experiments,
we perform 5-fold cross-validation17 using all 1,141
texts from the exam year 2000 (see Section 3).
Most of the resulting models have minimal ef-
fect on performance18. However, word length, ISA,
LOWBOWlex, and the IBM modelPOSf derived mod-
els all improve performance, while larger differ-
ences are observed in r. The highest performance
? 0.675 and 0.678 ? is obtained with ISA, while the
second best feature is word length. The entity-grid,
the pronoun model and the discourse-new model do
not improve on the baseline. Although these mod-
els have been successfully used as components in
state-of-the-art systems for discriminating coherent
from incoherent news documents (Elsner and Char-
niak, 2011b), and the entity-grid model has also
been successfully applied to learner text (Burstein
et al, 2010), they seem to have minimal impact
on performance, while the discourse-new model de-
creases ? by?0.01. On the other hand, LOWBOWlex
and LOWBOWPOS give an increase in performance,
which confirms our hypothesis that local histograms
are useful. Also, the former seems to perform
slightly better than the latter.
Our adapted version of the IBM model ? IBM
modelPOS ? performs better than its lexicalized ver-
sion, which does not have an impact on perfor-
mance, while larger differences are observed in r.
Additionally, the increase in performance is larger
than the one obtained with the entity-grid, pro-
noun or discourse-new model. The forward ver-
sion of IBM modelPOS seems to perform slightly
better than the backward one, while the results are
comparable to LOWBOWPOS and outperformed by
LOWBOWlex. The rest of the models do not perform
as well; the number of pronouns or discourse con-
nectives gives low results, while lemma and POS co-
sine similarity between adjacent sentences are also
17We compute mean values of correlation coefficients by first
applying the r-to-Z Fisher transformation, and then using the
Fisher weighted mean correlation coefficient (Faller, 1981).
18Significance tests in averaged correlations are omitted as
variable estimates are produced, whose variance is hard to be
estimated unbiasedly.
38
r ?
0 Baseline 0.651 0.670
1 POS distr. 0.653 0.670
2 Disc. connectives 0.648 0.668
3 Word length 0.667 0.676
4 ISA 0.675 0.678
5 EGrid 0.650 0.668
6 Pronoun 0.650 0.668
7 Disc-new 0.646 0.662
8 LOWBOWlex 0.663 0.677
9 LOWBOWPOS 0.659 0.674
10 IBM modellexf 0.649 0.668
11 IBM modellexb 0.649 0.667
12 IBM modelPOSf 0.661 0.672
13 IBM modelPOSb 0.658 0.669
14 Lemma cosine 0.651 0.667
15 POS cosine 0.650 0.665
16 5+6+7+10+11 0.648 0.665
17 All 0.677 0.671
Table 1: 5-fold cross-validation performance on texts
from year 2000 when adding different coherence features
on top of the baseline AA system.
among the weakest predictors.
Elsner and Charniak (2011b) have shown that
combining the entity-grid with the pronoun,
discourse-new and lexicalized IBM models gives
state-of-the-art results for discriminating news docu-
ments and their random permutations. We also com-
bine these models and assess their performance un-
der the AA framework. Row 16 of Table 1 shows
that the combination does not give an improvement
over the individual models. Moreover, combining
all feature classes together in row 17 does not yield
higher results than those obtained with ISA, while ?
is no better than the baseline.
In the following experiments, we evaluate the best
model identified on year 2000 on a set of 97 texts
from the exam year 2001, previously used in Yan-
nakoudakis et al (2011) to report results of the fi-
nal best system. Validating the model on a different
exam year also shows us the extent to which it gen-
eralizes between years. Table 2 presents the results.
The published correlations on this dataset are 0.741
and 0.773 r and ? respectively. Adding ISA on top
of the previous system significantly improves19 the
19Calculated using one-tailed tests for the difference between
r ?
Baseline 0.741 0.773
ISA 0.749 0.790?
Table 2: Performance on the exam scripts drawn from the
examination year 2001. ? indicates a significant improve-
ment at ? = 0.05.
published results on the 2001 texts, getting closer to
the upper-bound. The upper-bound on this dataset20
is 0.796 and 0.792 r and ? respectively, calculated
by taking the average correlation between the FCE
grades and the ones provided by 4 senior ESOL ex-
aminers21. Table 3 also presents the average corre-
lation between our extended AA system?s predicted
grades and the 4 examiners? grades, in addition to
the original FCE grades from the dataset. Again,
our extended model improves over the baseline.
Finally, we explore the utility of our best model
for assessing the publically available ?outlier? texts
used in Yannakoudakis et al (2011). The previous
AA system is unable to downgrade appropriately
?outlier? scripts containing individually high-scoring
sentences with poor overall coherence, created by
randomly ordering a set of highly-marked texts. To
test our best system, we train an SVM rank prefer-
ence model with the ISA-derived coherence feature,
which can explicitly capture such sequential trends.
A generic model for flagging putative ?outlier? texts
? whose predicted score is lower than a predefined
threshold ? for manual checking might be used as
the first stage of a deployed AA system. The ISA
model improves r and ? by 0.320 and 0.463 respec-
tively for predicting a score on this type of ?outlier?
texts and their original version (Table 4).
6 Analysis & Discussion
In the previous section, we evaluated various co-
hesion and coherence features on learner data, and
found different patterns of performance compared to
those previously reported on news texts (see Section
7 for more details). Although most of the models ex-
amined gave a minimal effect on AA performance,
ISA, LOWBOWlex, IBM modelPOSf and word length
dependent correlations (Williams, 1959; Steiger, 1980).
20See Yannakoudakis et al (2011) for details.
21The examiners? scores are also distributed with the FCE
dataset.
39
r ?
Baseline 0.723 0.721
ISA 0.727 0.736
Table 3: Average correlation between the AA model, the
FCE dataset grades, and 4 examiners on the exam scripts
from year 2000.
r ?
Baseline 0.08 0.163
ISA 0.400 0.626
Table 4: Performance of the ISA AA model on outliers.
gave a clear improvement in correlation, with larger
differences in r. Our results indicate that coherence
metrics further improve the performance of a com-
petitive AA system. More specifically, we found the
ISA-derived feature to be the most effective contrib-
utor to the prediction of text quality. This suggests
that incoherence in FCE texts might be due to topic
discontinuities. Also, the improvement obtained by
LOWBOW suggests that patterns of sequential pro-
gression within a text can be useful: coherent texts
appear to use similar token distributions at similar
locations across different documents.
The word length feature was successfully used as
a proxy for coherence, perhaps because many cohe-
sive words are longer than average. However, such
a feature can also capture further aspects of texts,
such as lexical complexity, so further investigation
is needed to identify the extent to which it measures
different properties. On the other hand, the minimal
effect of the entity-grid, pronoun and discourse-new
model suggests that infelicitous use of pronominal
forms or sequences of entities may not be an issue
in FCE texts. Preliminary investigation of the scripts
showed that learners tend to repeat the same entity
names or descriptions rather than use pronouns or
shorter descriptions.
A possible explanation for the difference in per-
formance between the lexicalized and POS IBM
model is that the latter abstracts away from lexi-
cal information and thus avoids misspellings and
reduces sparsity. Also, our discourse connective
classes do not seem to have a predictive power. This
may be because our manually-built word lists do not
have sufficient coverage.
7 Previous Work
Comparatively few metrics have been investigated
for evaluating coherence in (ESOL) learner texts.
Miltsakaki and Kukich (2004) employ e-Rater (At-
tali and Burstein, 2006), an essay scoring system,
and show that Centering Theory?s Rough-Shift tran-
sitions (Grosz et al, 1995) contribute significantly to
the assessment of learner texts. Higgins et al (2004)
and Higgins and Burstein (2007) use RI to deter-
mine the semantic similarity between sentences of
same/different discourse segments. Their model is
based on a number of different semantic similarity
scores and assesses the percentage of sentences that
are correctly classified as (un)related. Among their
results, they found that it is hard to beat the baseline
(as 98.1% of the sentences were annotated as ?highly
related?) and identify sentences which are not related
to other ones in the same discourse segment. We
demonstrate that the related fully-incremental ISA
model can be used to improve AA grading accuracy
on the FCE dataset, as opposed to classifying the
(non-)relatedness of sentences.
Burstein et al (2010) show how the entity-grid
can be used to discriminate high-coherence from
low-coherence learner texts. They augment this
model with additional features related to writing
quality and word usage, and show a positive effect
in performance for automated coherence prediction
of student essays of different populations. On the
FCE dataset used here, entity-grids do not improve
AA grading accuracy. This may be because the texts
are shorter or because grading is a more difficult task
than binary classification. Application of their aug-
mented entity-grid model to FCE texts would be an
interesting avenue for future research.
Foltz et al (1998) examine local coherence in
textbooks and articles using Latent Semantic Anal-
ysis (LSA) (Landauer et al, 2003). They assess se-
mantic relatedness using vector-based similarity be-
tween adjacent sentences. They argue that LSA may
be more appropriate for comparing the relative qual-
ity of texts; for determining the overall text coher-
ence it may be difficult to set a criterion for the co-
herence value since it depends on a variety of dif-
ferent factors, such as the size of the text units to be
compared. Nevertheless, our results show that ISA,
a similar distributional semantic model with dimen-
40
sionality reduction, improves FCE grading accuracy.
Barzilay and Lee (2004) implement lexicalized
content models that represent global text proper-
ties on news articles and narratives using Hidden
Markov Models (HMMs). In the HMM, states cor-
respond to distinct topics, and transitions between
states represent the probability of moving from one
topic to another. This approach has the advantage
of capturing the order in which different topics ap-
pear in texts; however, the HMMs are highly domain
specific and would probably need retraining for each
distinct essay prompt.
Soricut and Marcu (2006) use a log-linear model
that combines local and global models of coher-
ence and show that it outperforms each of the in-
dividual ones on news articles and accident reports.
Their global model is based on the document con-
tent model proposed by Barzilay and Lee (2004).
Their local model of discourse coherence is based
on the entity-grid (Barzilay and Lapata, 2008), as
well as on the lexicalized IBM model (see Section
4.6 above); we have experimented with both, and
showed that they have a minimal effect on grading
performance with the FCE dataset.
Elsner and Charniak (2008;2011a) apply a
discourse-new classifier and a pronoun coreference
system to model coherence (see Section 4) on dia-
logue and news texts. They found that combining
these models with the entity-grid achieves state-of-
the-art performance. We found that such a combina-
tion, as well as the individual models do not perform
well for grading the FCE texts.
Recently, Elsner and Charniak (2011a) proposed a
variation of the entity-grid intended to integrate top-
ical information. They use Latent Dirichlet Alloca-
tion (Blei et al, 2003) to learn topic-to-word distri-
butions, and model coherence by generalizing the bi-
nary history features of the entity-grid and comput-
ing a real-valued feature which represents the simi-
larity between an entity and the subject(s) of the pre-
vious sentence. Also, Lin et al (2011) proposed a
model that assesses the coherence of a text based on
discourse relation transitions. The underlying idea
is that coherent texts exhibit measurable preferences
for specific intra- and inter-discourse relation order-
ing. They found their model to be complementary to
the entity-grid, as it encodes the notion of preferen-
tial ordering of discourse relations, and thus tackles
local coherence from a different perspective. Apply-
ing the above models to AA on learner texts would
also be an interesting avenue for future work.
8 Conclusion
We presented the first systematic analysis of a wide
variety of models for assessing discourse coherence
on learner data, and evaluated their individual per-
formance as well as their combinations for the AA
grading task. We adapted the LOWBOW model for
assessing sequential content in texts, and showed
evidence supporting our hypothesis that local his-
tograms are useful. We also successfully adapted
ISA, an efficient and incremental variant distribu-
tional semantic model, to this task. ISA, LOWBOW,
the POS IBM model and word length are the best in-
dividual features for assessing coherence.
A significant improvement over the AA system
presented in Yannakoudakis et al (2011) and the
best published result on the FCE dataset was ob-
tained by augmenting the system with an ISA-based
local coherence feature. However, it is quite likely
that further experimentation with LOWBOW fea-
tures, given the large range of possible parameter
settings, would yield better results too.
We also explored the robustness of the ISA model
of local coherence on ?outlier? texts and achieved
much better correlations with the examiner?s grades
for these texts in the FCE dataset. This should facil-
itate development of an automated system to detect
essays consisting of high-quality but incoherent se-
quences of sentences.
All our results are specific to ESOL FCE texts and
may not generalize to other genres or ESOL attain-
ment levels. Future work should also investigate a
wider range of (learner) texts and further coherence
models, such as that of Elsner and Charniak (2011a)
and Lin et al (2011).
Acknowledgments
We are grateful to Cambridge ESOL, a division
of Cambridge Assessment, for supporting this re-
search. We would like to thank Marek Rei and ?is-
tein Andersen for their valuable comments and sug-
gestions, Yi Mao for giving us access to her code,
as well as the anonymous reviewers for their useful
feedback.
41
References
AMIDA. 2007. Augmented multi-party interaction
with distance access. Available from www. amidapro-
ject.org/, AMIDA Report.
Yigal Attali and Jill Burstein. 2006. Automated essay
scoring with e-rater v.2. Journal of Technology, Learn-
ing, and Assessment, 4(3):1?30.
Marco Baroni, Alessandro Lenci, and Luca Onnis. 2007.
ISA meets Lara: An incremental word space model for
cognitively plausible simulations of semantic learning.
In Proceedings of the Association for Computational
Linguistics.
Regina Barzilay and Mirella Lapata. 2008. Modeling
Local Coherence: An Entity-Based Approach. Com-
putational Linguistics, 34(1):1?34.
Regina Barzilay and Lillian Lee. 2004. Catching the
drift: Probabilistic content models, with applications
to generation and summarization. In Proceedings of
the Human Language Technology Conference of the
North American Chapter of the Association for Com-
putational Linguistics.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. Journal of Machine
Learning Research.
Ted Briscoe, John Carroll, and Rebecca Watson. 2006.
The second release of the RASP system. In Proceed-
ings of the COLING/ACL, volume 6.
Ted Briscoe, Ben Medlock, and ?istein Andersen. 2010.
Automated assessment of ESOL free text examina-
tions. Technical Report UCAM-CL-TR-790, Univer-
sity of Cambridge, Computer Laboratory, November.
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. 1994. The mathe-
matic of statistical machine translation: Parameter es-
timation. Computational linguistics, 19(2):263?311.
Jill Burstein, Joel Tetreault, and Slava Andreyev. 2010.
Using entity-based features to model coherence in stu-
dent essays. In Proceedings of the Human Language
Technology Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 681?684.
Jieun Chae and Ani Nenkova. 2009. Predicting the flu-
ency of text with shallow structural features: case stud-
ies of machine translation and human-written text. In
Proceedings of the 12th Conference of the European
Chapter of the Association for Computational Linguis-
tics, pages 139?147.
Eugene Charniak and Micha Elsner. 2009. EM works for
pronoun anaphora resolution. In Proceedings of the
12th Conference of the European Chapter of the Asso-
ciation for Computational Linguistics, pages 148?156.
Micha Elsner and Eugene Charniak. 2008. Coreference-
inspired coherence modeling. In Proceedings of the
46th Annual Meeting of the Association for Computa-
tional Linguistics on Human Language Technologies,
pages 41?44.
Micha Elsner and Eugene Charniak. 2011a. Disentan-
gling chat with local coherence models. In Proceed-
ings of the 49th Annual Meeting of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 1179?1189.
Micha Elsner and Eugene Charniak. 2011b. Extending
the entity grid with entity-specific features. In Pro-
ceedings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Language
Technologies, pages 125?129.
Hugo J. Escalante, Thamar Solorio, and Manuel Montes-
y Go?mez. 2011. Local Histograms of Character N-
grams for Authorship Attribution. In Proceedings of
the 49th Annual Meeting on Association for Computa-
tional Linguistics, pages 288?298.
Alan J. Faller. 1981. An Average Correlation Coeffi-
cient. Journal of Applied Meteorology.
Adriano Ferraresi, Eros Zanchetta, Marco Baroni, and
Silvia Bernardini. 2008. Introducing and evaluating
ukWaC, a very large web-derived corpus of English.
In S. Evert, A. Kilgarriff, and S. Sharoff, editors, Pro-
ceedings of the 4th Web as Corpus Workshop.
Peter W. Foltz, Walter Kintsch, and Thomas K. Lan-
dauer. 1998. The measurement of textual coherence
with latent semantic analysis. Discourse processes,
25(2):285?308.
Barbara J. Grosz, Scott Weinstein, and Aravind K. Joshi.
1995. Centering: A framework for modeling the lo-
cal coherence of discourse. Computational linguistics,
21(2):203?225.
Michael A. K. Halliday and Ruqaiya Hasan. 1976. Co-
hesion in English . Longman Pub Group.
Derrick Higgins and Jill Burstein. 2007. Sentence sim-
ilarity measures for essay coherence. In Proceedings
of the 7th International Workshop on Computational
Semantics, pages 1?12.
Derrick Higgins, Jill Burstein, Daniel Marcu, and Clau-
dia Gentile. 2004. Evaluating multiple aspects of co-
herence in student essays. In Proceedings of the Hu-
man Language Technology Conference of the North
American Chapter of the Association for Computa-
tional Linguistics.
Thorsten Joachims. 2002. Optimizing search engines
using clickthrough data. In Proceedings of the ACM
Conference on Knowledge Discovery and Data Min-
ing, pages 133?142.
David Jurgens and Keith Stevens. 2010. The S-Space
package: an open source package for word space mod-
els. In Proceedings of the Association for Computa-
tional Linguistics 2010 System Demonstrations, pages
30?35.
42
John Lafferty and Guy Lebanon. 2005. Diffusion kernels
on statistical manifolds. Journal of Machine Learning
Research, 6:129?163.
Thomas K. Landauer, Darrell Laham, and Peter W. Foltz.
2003. Automated scoring and annotation of essays
with the Intelligent Essay Assessor. In M.D. Shermis
and J.C. Burstein, editors, Automated essay scoring: A
cross-disciplinary perspective, pages 87?112.
Guy Lebanon, Yi Mao, and Joshua Dillon. 2007. The
locally weighted bag-of-words framework for docu-
ment representation. Journal of Machine Learning Re-
search, 8(10):2405?2441.
Ziheng Lin, Hwee T. Ng, and Min-Yen Kan. 2011. Auto-
matically Evaluating Text Coherence Using Discourse
Relations. In Proceedings of the 49th Annual Meeting
on Association for Computational Linguistics.
Eleni Miltsakaki and Karen Kukich. 2004. Evaluation
of text coherence for electronic essay scoring systems.
Natural Language Engineering, 10(01):25?55.
Franz J. Och and Hermann Ney. 2000. Improved statisti-
cal alignment models. In Proceedings of the 38th An-
nual Meeting on Association for Computational Lin-
guistics, pages 440?447.
Emily Pitler, Annie Louis, and Ani Nenkova. 2010.
Automatic evaluation of linguistic quality in multi-
document summarization. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics, pages 544?554.
Donald E. Powers, Jill C. Burstein, Martin Chodorow,
Mary E. Fowles, and Karen Kukich. 2002. Stump-
ing e-rater: challenging the validity of automated essay
scoring. Computers in Human Behavior, 18(2):103?
134.
Lawrence M. Rudner and Tahung Liang. 2002. Auto-
mated essay scoring using Bayes? theorem. The Jour-
nal of Technology, Learning and Assessment, 1(2):3?
21.
Magnus Sahlgren. 2005. An introduction to random in-
dexing. In Methods and Applications of Semantic In-
dexing Workshop at the 7th International Conference
on Terminology and Knowledge Engineering, pages 1?
9. Citeseer.
Radu Soricut and Daniel Marcu. 2006. Discourse gener-
ation using utility-trained coherence models. In Pro-
ceedings of the COLING/ACL on Main conference
poster sessions, pages 803?810.
James H. Steiger. 1980. Tests for comparing ele-
ments of a correlation matrix. Psychological Bulletin,
87(2):245?251.
Evan J. Williams. 1959. The Comparison of Regression
Variables. Journal of the Royal Statistical Society. Se-
ries B (Methodological), 21(2):396?399.
Helen Yannakoudakis, Ted Briscoe, and Ben Medlock.
2011. A New Dataset and Method for Automatically
Grading ESOL Texts. In The 49th Annual Meeting
of the Association for Computational Linguistics: Hu-
man Language Technologies.
43
Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 32?41,
Atlanta, Georgia, June 13 2013. c?2013 Association for Computational Linguistics
Developing and testing
a self-assessment and tutoring system
?istein E. Andersen
iLexIR
Streets, 62 Hills Road
Cambridge, CB2 1LA
and@ilexir.co.uk
Helen Yannakoudakis
Cambridge English
1 Hills Road
Cambridge, CB1 2EU
yannakoudakis.h
@cambridgeenglish.org
Fiona Barker
Cambridge English
1 Hills Road
Cambridge, CB1 2EU
barker.f
Tim Parish
iLexIR
Streets, 62 Hills Road
Cambridge, CB2 1LA
tim@ilexir.co.uk
Abstract
Automated feedback on writing may be a use-
ful complement to teacher comments in the
process of learning a foreign language. This
paper presents a self-assessment and tutoring
system which combines an holistic score with
detection and correction of frequent errors and
furthermore provides a qualitative assessment
of each individual sentence, thus making the
language learner aware of potentially prob-
lematic areas rather than providing a panacea.
The system has been tested by learners in
a range of educational institutions, and their
feedback has guided its development.
1 Introduction
Learning to write a foreign language well requires
a considerable amount of practice and appropriate
feedback. Good teachers are essential, but their time
is limited. As recently shown in a study by Wang et
al. (in press) conducted amongst first-year students
of English at a Taiwanese university, automated
writing evaluation can lead to increased learner au-
tonomy and higher writing accuracy. In this pa-
per, we investigate the merits of a self-assessment
and tutoring (SAT) system specifically aimed at in-
termediate learners of English, at around B2 level
in the Common European Framework of Reference
for Languages (CEFR) (Council of Europe, 2001).
There are a large number of students at this level,
and they should have sufficient knowledge of the
language to benefit from the system whilst at the
same time committing errors which can be identified
reliably.
The system provides automated feedback on
learners? writing at three different levels of gran-
ularity: an overall assessment of their proficiency,
a score for each individual sentence, highlighting
well-written passages as well as ones requiring more
work, and specific comments on local issues includ-
ing spelling and word choice.
Computer-based writing tools have been around
for a long time, with Criterion (Burstein et al, 2003,
which also provides a number of features for teach-
ers) and ESL Assistant (Gamon et al, 2009, not
currently available) aimed specifically at second-
language learners, but the idea of indicating the rel-
ative quality of different parts of a text (sentences in
our case) has, to the best of our knowledge, not been
implemented previously. This kind of non-specific
feedback does not provide a precise diagnosis or im-
mediate cure, but might have the advantage of fos-
tering learning.
In addition to describing the SAT system itself, we
present a series of three trials in which learners of
English in a number of educational contexts used the
system as a tool to work on written responses to spe-
cific tasks and improve their writing skills.
2 System
The SAT system is made available to students learn-
ing English as a Web service to which they can
sign up with a code (?class key?) provided by their
teacher. Once they have filled in a short demo-
graphic questionnaire, the users can respond to one,
two, three or more writing tasks. The students can
save their work at any time and ask the system to
assess the current version of their text, which will
32
Figure 1: SAT system screen where students can see the automated feedback and revise their piece of writing. The
?score feedback? and ?error feedback? views are shown in Figures 2 and 3.
give feedback as shown in Figure 1 and described
in more detail in the following subsections. Assess-
ment times are currently around 15sec, which facil-
itates incremental and exploratory editing of a text
to improve it, giving the students the ability to try
out different ways of correcting a problematic turn
of phrase. The teacher can see which students have
signed up and look at the last saved version of their
responses. Finally, the students are asked to answer
a few questions about their experience with the sys-
tem.
2.1 Text assessment
The SAT system provides an overall assessment of
someone?s proficiency by automatically analysing
and scoring the text as a whole. There is a large
body of literature with regard to automated text scor-
ing systems (Page, 1968; Rudner and Liang, 2002;
Attali and Burstein, 2006; Briscoe et al, 2010). Ex-
isting systems, overviews of which have been pub-
lished in various studies (Dikli, 2006; Williamson,
2009; Shermis and Hamner, 2012), involve a large
range of techniques, such as discriminative and gen-
erative machine learning, clustering algorithms and
vectorial semantics, as well as syntactic parsers.
We approach automated text assessment as a su-
pervised machine learning problem, which enables
us to take advantage of existing annotated data. We
use the publically-available First Certificate in En-
glish (FCE) dataset of upper-intermediate learner En-
glish (Yannakoudakis et al, 2011) and focus on as-
sessing general linguistic competence. Systems that
measure English competence directly are easier and
faster to deploy, since they are more likely to be re-
usable and generalise better across different genres
than topic-specific ones, which are not immediately
33
usable when new tasks are added, since the model
cannot be applied until a substantial amount of man-
ually annotated responses have been collected for a
specific prompt.
Following previous research, we employ discrim-
inative ranking, which has been shown to achieve
state-of-the-art results on the task of assessing
free-text writing competence (Yannakoudakis et al,
2011). The underlying idea is that high-scoring texts
(or ?scripts?) should receive a higher rank than low-
scoring ones. We train a linear ranking perceptron
(Bo?s and Opper, 1998) on features derived from pre-
vious work (namely, lexical and grammatical prop-
erties of text) and compare it to our previous model
(Yannakoudakis et al, 2011), which is trained using
ranking Support Vector Machines (Joachims, 2002).
Our new perceptron model achieves 0.740 and 0.765
Pearson product-moment (r) and Spearman?s rank
correlation coefficient (?) respectively between the
gold and predicted scores; this is comparable to
our previous SVM model, which achieves 0.741 and
0.773, and the differences are not significant.
In order to provide scoring feedback1 based on
the predictions of our model, we use visual presen-
tations. Visualisation techniques allow us to go be-
yond the mere display of a number, can stimulate the
learners? visual perceptions, and, when used appro-
priately, information can be displayed in an intuitive
and easily interpretable way. Furthermore, aesthet-
ics in computer-based interfaces have been shown to
have an effect on the users. For example, Ben-Bassat
et al (2006) have found an interdependence between
perceived aesthetics and usability in questionnaire-
based assessments, and have shown that users? pref-
erences are not necessarily based only upon perfor-
mance; aesthetics also play a role.
More specifically, we assign an overall score on
a scale from red for a text that looks like it may be
at intermediate level or below to green for a text that
shows some evidence of being at upper-intermediate
level (the level assessed by the FCE exam) or above
(i.e., advanced). This is illustrated in Figure 1 below
the Overall score section, where an arrow is used to
indicate the level of text quality on a colour gradient
defined by the two extreme points, red and green.
1Note that ranks can be transformed to scores through linear
regression, while correlation remains unaltered as it is invariant
to linear transformations.
A text with the highest score possible would indi-
cate that the learner has potentially shown evidence
of being at a level higher than that assessed by FCE,
the latter, of course, being dependent on the extent
to which higher-order linguistic skills are elicited by
the prompts. On the contrary, a very low score in-
dicates poor linguistic abilities corresponding to a
lower level.
Although exams that encompass the full range of
language proficiency exhibited at different stages of
learning are hard to design, the FCE exam, bench-
marked at the B2 level and reserving some of its
score range for performances beneath and beyond,
allows us to roughly estimate someone?s proficiency
as being far below, just below, around or above an
upper intermediate level. The task of predicting at-
tainment levels has recently started to receive atten-
tion (Dickinson et al, 2012; Hawkins and Filipovic?,
2012).
2.2 Sentence evaluation
The second component of the SAT system automat-
ically assesses and scores the quality of individual
sentences, independently of their context. The chal-
lenge of assessing intra-sentential quality lies in the
limited linguistic evidence that can be extracted au-
tomatically from relatively short sentences for them
to be assessed reliably, in addition to the difficulty
in acquiring annotated data, since rating a response
sentence by sentence is not something examiners
typically do and would therefore require an addi-
tional and expensive manual annotation effort.
Previous work has primarily focused on automatic
content scoring of short answers, ranging from a few
words to a few sentences (Pulman and Sukkarieh,
2005; Attali et al, 2008; Mohler et al, 2011; Ziai
et al, 2012). On the other hand, scoring of individ-
ual sentences with respect to their linguistic quality,
specifically in learner texts, has received consider-
ably less attention. Higgins et al (2004) devised
guidelines for the manual annotation of sentences in
learner texts, and evaluated a rule-based approach
that classifies sentences with respect to clarity of ex-
pression based on grammar, mechanics and word us-
age errors; however, their system performs binary
classification, whereas we are focusing on scoring
sentences. Writing instruction tools, such as Crite-
rion (Burstein et al, 2003), give advice on stylistic
34
and organisational issues and automatically detect a
variety of errors in the text, though they do not ex-
plicitly allow for an overall evaluation of sentences
with respect to various writing aspects. The latter,
used in combination with an error feedback compo-
nent (see Section 2.3), can be a useful instrument
informing learners about the severity of their mis-
takes; for example, although sentences may contain
some errors, they may still maintain a certain level
of acceptability that does not impede communica-
tion. Moreover, indicating problematic regions may
be better from a pedagogic point of view than detect-
ing and correcting all errors identified in the text.
To date, there is no publically available annotated
dataset consisting of sentences marked with a score
representing their linguistic quality. Manual annota-
tion is typically expensive and time-consuming, and
a certain amount of annotator training is generally
required. Instead, we exploit already available an-
notated data ? scores and error annotation in the FCE
dataset ? and evaluate various approaches, two of
which are: a) to use the script-level model (see Sec-
tion 2.1) to predict sentence quality scores, and b) to
use the script-level score divided by the total num-
ber of (manually annotated) errors in a sentence as
pseudo-gold labels to train a sentence-level model.
As the models above are expected to contain a cer-
tain amount of noise, it is imperative that we iden-
tify evaluation measures that are indicative of our
application ? that is, assign higher scores to high-
quality sentences compared to low-quality ones ?
and not only depend on the labels they have been
trained on. More specifically, we use correlation
with pseudo-gold scores (rg and ?g; not applicable
to the script-level model), correlation with the script-
level scores by first averaging predicted sentence-
level scores (rs and ?s), correlation with error counts
(re and ?e), average precision (AP) and pairwise ac-
curacy. AP is a measure used in information retrieval
to evaluate systems that return a ranked list of doc-
uments. Herein, sentences are ranked by their pre-
dicted scores, precision is calculated at each correct
sentence (that is, containing no errors), and aver-
aged over all correct sentences (in other words, we
treat sentences with no errors as the ?relevant doc-
uments?). Pairwise accuracy is calculated based on
the number of times the corrected sentence (avail-
able through the error annotation in the FCE dataset)
is ranked higher than the original one written by the
candidate, ignoring sentences without errors. Corre-
lation with error counts, average precision and pair-
wise accuracy are particularly important as they re-
flect more directly the extent to which good and bad
sentences are discriminated. Again, in both cases,
we employ a linear ranking perceptron.
We conducted a series of experiments on a sep-
arate development set to evaluate the performance
of features beyond the ones used in the script-level
model. The final results, reported in Table 1, are
calculated on the FCE test set (Yannakoudakis et al,
2011).
Our best configuration is model b, which achieves
the highest results according to most evaluation
measures with a feature space consisting of 1) er-
ror counts identified through the absence of word
trigrams in a large background corpus, 2) phrase-
structure rules, 3) presence of frequent errors, as
well as the number of words defining an error, as
described in Section 2.3, 4) the presence of main
verbs, nouns, adjectives, subordinating conjuctions
and adverbs, 5) affixes and 6) the presence of clausal
subjects and modifiers. The texts were parsed using
RASP (Briscoe et al, 2006).
Model a, the script-level model, does not work as
well at the sentence level. However, it does perform
better when evaluated against script-level scores (rs
and ?s), and this is expected given that it is trained
directly on gold script-level scores. On the other
hand, this evaluation measure is not as indicative of
good performance in our application as the others,
as it does not take into account the varying quality
of individual sentences within a script.
Training the script-level model with different fea-
ture sets (including those utilised in the sentence-
level model) did not yield an improvement in per-
formance (the results are omitted due to space re-
strictions). Additional experiments were conducted
to investigate the effect of training the sentence-level
model with different pseudo-gold labels (e.g., addi-
tive/subtractive pseudo-gold scores rather than divi-
sive/multiplicative), but the results are not reported
here as the difference in performance was not sub-
stantial.
Table 1 shows that better performance can be
achieved with our pseudo-gold labels, used to train
a model at the sentence level, rather than gold la-
35
Model a Model b
rg ? 0.550
?g ? 0.646
rs 0.572 0.385
?s 0.578 0.301
re ?0.111 ?0.750
?e ?0.078 ?0.702
AP 0.393 0.747
Pairwise
Correct 0.608 0.703
Incorrect 0.359 0.204
Table 1: Results on the FCE test set for the script-level
model (a) and our model (b).
bels at the script level. To evaluate this further,
we trained a sentence-level model using the script-
level scores as labels (that is, sentences within the
same script are all assigned the same label/score).
However, this did not improve performance (again,
the results are omitted due to space restrictions).
We also point out that the best-performing feature
space (described above) is based on text properties
that are more likely to be present in relatively short
sentences (e.g., the presence of main verbs), com-
pared to those used for script-level models in previ-
ous work (Yannakoudakis et al, 2011), such as word
and part-of-speech bigrams and trigrams, which may
be too sparse for a sentence-level model.
Analogously to what we did to present the over-
all score, we developed a sentence score feedback
view to indicate the general quality of the sentences,
as given by our best model, by highlighting each of
them with a background colour ranging from green
for a well-written sentence, via yellow and orange
for a sentence which the system thinks is accept-
able, to dark orange and red for a sentence which
may have a few problems. Figure 2 shows how the
SAT system evaluates and colour-codes a few au-
thentic student-written sentences containing errors,
as well as their corrected counterparts based on the
error-coding in the FCE test set. Overall, the system
correctly identifies correct and incorrect versions of
each sentence, attributing a higher score (greener
colour) to the corrected sentence in each pair.
2.3 Word-level feedback
Basic spelling checkers have been around since the
1970s and grammar checkers since the 1980s (Ku-
kich, 1992), but misleading ?corrections? may be be-
wildering (Galletta et al, 2005), and the systems do
not always focus on the kinds of error frequently
committed, even less so in the case of learners as
was pointed out early on by Liou (1992), who tested
commercial grammar checkers on and developed a
system for detecting common errors in Taiwanese
learners? writing.
For word-level feedback within the SAT system,
we have implemented a method similar to one we
have used earlier in the context of pre-annotation of
learner corpora (Andersen, 2011). To ensure high
precision and good coverage of local errors typi-
cally committed by learners, error rules are gen-
erated from the Cambridge Learner Corpus (CLC)
(Nicholls, 2003) to detect word unigrams, bigrams
and trigrams which have been annotated as incorrect
at least five times and at least ninety per cent of the
times they occur. This way, rules can be extracted
from the existing error annotation in the corpus,
obviating the need for manually constructed mal-
rules, although the rules obtained by the two differ-
ent methods may to some extent be complementary.
In addition to corpus-derived rules, many classes of
incorrect but plausible derivational and inflectional
morphology are detected by means of rules derived
from a machine-readable dictionary. Many mistakes
are still not detected, but precision has been found to
be more important in terms of learning effect (Na-
gata and Nakatani, 2010), and errors missed by this
module will often give lower sentence scores.
Figure 3 illustrates some types of error detected
by the system. The feedback text is generated from
a small number of templates corresponding to differ-
ent categories of error marked up in the CLC.
We are currently working on extending this part
of the system with more general rules in addition to
word n-grams, e.g., part-of-speech tags and gram-
matical relations, in order to detect more errors with-
out loss in precision.
3 Trials
After the SAT system had been developed, a series
of trials were set up in order to test the online sys-
36
Figure 2: Examples of correct sentences (top) and incorrect ones (bottom) colour-coded by the SAT system.
Figure 3: The error feedback view identifies specific words that may have been used incorrectly. Explanations and
suggested corrections are provided in a separate column. The system actually proposes two different corrections for
and etc., namely etc. and and so on; the user will have to choose one or the other. The confusion between the verb see
and the noun sea is identified, but the the is not actually unnecessary; in this case, the system has been led astray by
the surrounding errors.
tem and to collect feedback from language learners
and their teachers in a variety of contexts. Three tri-
als were undertaken in November 2012, December
2012 and in March 2013, with changes made to the
system between each pair of trials.
English Profile Network member institutions
were contacted who had access to language learners
and who had previously participated in data collec-
tion for the English Profile Programme2. Teachers at
universities, secondary schools and private language
schools signed up for two or more trials so that their
learners could use and provide feedback on several
iterations of the SAT system. Certificates of partici-
2See www.englishprofile.org
pation were offered to encourage involvement in the
trials.
Ten institutions were involved from nine coun-
tries, namely Belgium, the Czech Republic, France,
Lithuania, Poland, Romania, Russia, Slovakia and
Spain. Eight universities, one secondary school and
one private language school were represented, in-
cluding specialist and generalist institutions of ed-
ucational sciences, agricultural science, veterinary
medicine and foreign languages. Each trial had be-
tween 4 and 8 institutions taking part, and each in-
stitution participated in two or three trials with many
students undertaking more than one trial.
All students who took part in the trials, over 450
37
in total, were expected to be at or above the upper-
intermediate (CEFR B2) level as this was the level at
which the SAT system was designed to function.
Three initial sets of tasks were developed for the
planned system trials, each set consisting of three
short written prompts which asked the users to write
on a specified topic for a particular purpose, for ex-
ample:
Daily life
Your English class is going to make a
short video about daily life in your town.
Write a report for your teacher, suggest-
ing which activities should be filmed, and
why.
Tasks were based on retired questions from an in-
ternational proficiency test at B2 level of the CEFR.
Each task was given a short name which was shown
in the SAT system in order for the users to select the
most interesting or relevant task for themselves.
A short set of instructions was produced for both
teachers and students which was emailed to the main
contact in each institution and passed on to their col-
leagues, teachers and students who were interested
in taking part in the trial.
The trials operated as follows:
? The main institutional contact receives an invi-
tation to participate in the trials.
? Interested institutions receive instructions and
confirm the number of class keys required
(sign-up codes for the system).
? Main contact and teachers at each institution
log in and work through the system as if they
are a language learner, by completing a demo-
graphic questionnaire, writing 1?3 tasks which
are assessed by the system, and finally complet-
ing a short user satisfaction questionnaire.
? Students work through the SAT system either
with the support of their teacher in class or re-
motely.
3.1 SAT system usage
During Trial 1, on the busiest day there were 155
submissions and the highest number of users on
a single day was 32. These figures indicate that
Revisions Count
1 292
2 272
3 142
4 78
5 50
6 28
7 15
8 25
9 11
10 14
11?15 21
16?20 6
20? 5
Table 2: Number of revisions per task response.
all users were submitting their work for assessment
more than once, which suggests that the system is
being used in an iterative fashion as envisaged. Dur-
ing Trial 2, the busiest day saw more than twice as
many submissions as during the first trial (442), and
the most people online on any one day almost dou-
bled to 62. Across both trials we collected around
3000 submissions in total, including revisions; the
average number of revisions for a submitted piece
of writing is 3.2 with the highest figure being 54
revisions (see Table 2 for details). This suggests
that some users write their first response, then make
changes to one word or phrase at a time, resulting in
such a large number of revisions. When more than
one revision has been submitted, the score given by
the system to the last revision is higher than that
given to the initial revision in over 80% of the cases.
Current changes to the system allowing system ad-
ministrators to check on intermediate versions of
submitted texts are underway.
3.2 Feedback
In addition to looking at the writing submitted by
users of the system, there was both numerical and
written feedback available to the system developers.
This was used to suggest changes to the system at
subsequent trials.
As can be seen from Table 3, user satisfaction
scores were generally high and increased from Trial
1 to Trial 2. In the first pilot, the written feed-
back from instructors was generally positive whilst
38
Trial 1 Trial 2
Using the SAT system helps me to write better in English. 3.80 3.92
I find the SAT system useful for understanding my mistakes. 3.74 3.96
I think the sentence colouring is useful. 3.74 4.15
I think the word-level information [error feedback] is useful. 3.86 4.12
The SAT system is easy to use. 4.45 4.49
The feedback on my writing is clear. 3.80 3.93
If you have used the SAT system before, has it improved since the last time? 3.86
Table 3: Average feedback scores on a scale from 1 (strongly disagree) to 5 (strongly agree).
the learner feedback was mixed, especially when it
comes to sentence evaluation:
In summary, I liked this system, because
the sentence colouring suggests me to
think about my writing style, mistakes,
what I should improve, change. This sys-
tem is not like a teacher, who checks all
our errors, but makes us develop our crit-
ical thinking, which is the most important
for writing especially. [...]
It?s okay the way of colouring system, the
problem is that it doesn?t tell you specifi-
cally what?s wrong with constructions so
you have think what you failed.
The fact that the system provides almost immediate
feedback has been appreciated:
I like that the paragraphs which I wrote
assesed so quickly. . . . Secondly, I really
like that student can correct his text till it
gets ideal.
Users have also made suggestions for improve-
ments, which have been essential for deciding which
parts of the system should be developed further.
3.3 System changes
As a result of feedback and the team?s extensive use
of the system, after each trial changes were made
both to the on-screen experience and behind the
scenes. After Trial 1, the system was amended to
enable users to see paragraph breaks in the corrected
version (which before had not been shown in the as-
sessed view of the text). There was also a new error
view with permanently visible explanations and ex-
amples and an additional question on the feedback
questionnaire which asked whether users felt the
Words Count
0? 99 540
100?199 1,294
200?299 928
300?399 201
400?499 67
500?999 26
1,000? 36
Table 4: Number of words per submission.
system had improved since the previous time they
used it. Behind the scenes, the server was upgraded
to cope with anticipated demand and code was writ-
ten so that administrators could review statistics on
usage.
At the time of writing the third SAT system trial
was underway. In the first two trials the total number
of words collected was over 600,000 with an average
response length of around 1100 characters or 200
words. Encouragingly, there were many longer re-
sponses including twelve over 1080 words in length
and the longest written to date is 1773 words. These
figures indicate that the system is not restrictive, but
encourages and inspires students to write. Table 4
gives an overview of the script length distribution.
Following two successful trials, the third trial
aimed to involve new and existing users and to pro-
vide more detailed teacher feedback.
4 Conclusions
In this paper, we described a tool that provides feed-
back to learners of English at three different levels
of granularity: an overall assessment of their profi-
ciency, assessment of individual sentences, and di-
agnostic feedback on local issues including spelling
and word choice. We argued that the use of visual-
39
isation techniques is important, as they allow us to
go beyond the mere display of a number, can stimu-
late the learners? visual perceptions, and can display
information in an intuitive and easily interpretable
way. The usefulness and usability of the tool as a
whole, as well as of its components, was confirmed
through questionnaire-based evaluations, where, for
example, the perceived usefulness of the sentence
colouring received an average of 4.15 on a 5-point
scale.
The first component of the SAT system, script-
level assessment, uses a machine learner to predict
a score for a text and roughly estimate someone?s
proficiency level based on lexical and grammatical
features. The second component allows for an auto-
matic evaluation of the linguistic quality of individ-
ual sentences. We proposed a method for generat-
ing sentence-level scores, which we use for training
our model. Using this method, we were able to learn
what features can be used to evaluate linguistic qual-
ity of (relatively short) sentences. Indicating prob-
lematic regions via highlighting of sentences may be
better from a pedagogic point of view than detecting
and correcting all errors identified in the text. The
third component automatically provides diagnostic
feedback on local errors with high precision on the
basis of a few templates, without relying on manu-
ally crafted rules.
The trials undertaken so far have improved the
functionality of the system in regard to what is on
offer to teachers and their students, but they have
also provided the basis for further research and de-
velopment to enhance the system?s functionality and
design and move towards wider deployment. We
plan to continue improving the methodologies used
for providing feedback to learners, as well as adding
further functionality, such as L1-specific feedback.
Another logical next step would be to continue to-
wards lower levels of granularity, moving from the
sentence as the unit of assessment to clauses and
phrases, which may be particularly beneficial for
more advanced language users who write longer and
more complex sentences.
Acknowledgements
Special thanks to Ted Briscoe and Marek Rei, as
well as to the anonymous reviewers, for their valu-
able contributions at various stages.
References
?istein E. Andersen. 2011. Semi-automatic ESOL error
annotation. English Profile Journal, 2.
Yigal Attali and Jill Burstein. 2006. Automated essay
scoring with e-Rater v.2.0. Journal of Technology,
Learning, and Assessment, 4(3):1?30.
Yigal Attali, Don Powers, Marshall Freedman, Marissa
Harrison, and Susan Obetz. 2008. Automated Scoring
of short-answer open-ended GRE subject test items.
Technical Report 04, ETS.
Tamar Ben-Bassat, Joachim Meyer, and Noam Tractin-
sky. 2006. Economic and subjective measures
of the perceived value of aesthetics and usability.
ACM Transactions on Computer-Human Interaction,
13(2):210?234.
Siegfried Bo?s and Manfred Opper. 1998. Dynamics of
batch training in a perceptron. Journal of Physics A:
Mathematical and General, 31(21):4835?4850.
Ted Briscoe, John Carroll, and Rebecca Watson. 2006.
The second release of the RASP system. In ACL-
Coling?06 Interactive Presentation Session, pages 77?
80.
Ted Briscoe, Ben Medlock, and ?istein E. Andersen.
2010. Automated assessment of ESOL free text exam-
inations. Technical Report UCAM-CL-TR-790, Uni-
versity of Cambridge, Computer Laboratory.
Jill Burstein, Martin Chodorow, and Claudia Leacock.
2003. Criterion: Online essay evaluation: An appli-
cation for automated evaluation of student essays. In
Proceedings of the fifteenth annual conference on in-
novative applications of artificial intelligence, pages
3?10.
Council of Europe. 2001. Common European Frame-
work of Reference for Languages: Learning, Teaching,
Assessment. Cambridge University Press.
Markus Dickinson, Sandra Ku?bler, and Anthony Meyer.
2012. Predicting learner levels for online exercises of
Hebrew. In Proceedings of the Seventh Workshop on
Innovative Use of NLP for Building Educational Ap-
plications, pages 95?104. Association for Computa-
tional Linguistics.
Semire Dikli. 2006. An overview of automated scoring
of essays. Journal of Technology, Learning, and As-
sessment, 5(1).
Dennis F. Galletta, Alexandra Durcikova, Andrea Ever-
ard, and Brian M. Jones. 2005. Does spell-checking
software need a warning label? Communications of
the ACM, 48(7):82?86.
Michael Gamon, Claudia Leacock, Chris Brockett,
William B Dolan, Jianfeng Gao, Dmitriy Belenko, and
40
Alexandre Klementiev. 2009. Using statistical tech-
niques and web search to correct ESL errors. Calico
Journal, 26(3):491?511.
John A. Hawkins and Luna Filipovic?. 2012. Criterial
Features in L2 English: Specifying the Reference Lev-
els of the Common European Framework. English
Profile Studies. Cambridge University Press.
Derrick Higgins, Jill Burstein, Daniel Marcu, and Clau-
dia Gentile. 2004. Evaluating multiple aspects of co-
herence in student essays. In Proceedings of the Hu-
man Language Technology Conference of the North
American Chapter of the Association for Computa-
tional Linguistics.
Thorsten Joachims. 2002. Optimizing search engines
using clickthrough data. In Proceedings of the ACM
Conference on Knowledge Discovery and Data Min-
ing, pages 133?142.
Karen Kukich. 1992. Techniques for automatically
correcting words in text. ACM Computing Surveys,
24(4):377?439.
Hsien-Chin Liou. 1992. An automatic text-analysis
project for EFL writing revision. System: The Inter-
national Journal of Educational Technology and Lan-
guage Learning Systems, 20(4):481?492.
Michael A.G. Mohler, Razvan Bunescu, and Rada Mi-
halcea. 2011. Learning to grade short answer ques-
tions using semantic similarity measures and depen-
dency graph alignments. In Proceedings of the 49th
Annual Meeting of the Association for Computational
Linguistics: Human Language Technologies.
Ryo Nagata and Kazuhide Nakatani. 2010. Evaluating
performance of grammatical error detection to maxi-
mize learning effect. In Proceedings of the 23rd In-
ternational Conference on Computational Linguistics:
Posters, COLING ?10, pages 894?900, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Diane Nicholls. 2003. The Cambridge Learner Cor-
pus: Error coding and analysis for lexicography and
ELT. In Dawn Archer, Paul Rayson, Andrew Wilson,
and Tony McEnery, editors, Proceedings of the Cor-
pus Linguistics conference, volume 16 of Technical
Papers, pages 572?581. University Centre For Com-
puter Corpus Research on Lanugage, Lancaster Uni-
versity, Lancaster.
Ellis B. Page. 1968. The use of the computer in analyz-
ing student essays. International Review of Education,
14(2):210?225.
Stephen G. Pulman and Jana Z. Sukkarieh. 2005. Au-
tomatic short answer marking. In Proceedings of the
second workshop on Building Educational Applica-
tions Using natural language processing, pages 9?16.
Lawrence M. Rudner and Tahung Liang. 2002. Auto-
mated essay scoring using Bayes? theorem. The Jour-
nal of Technology, Learning and Assessment, 1(2):3?
21.
Mark D. Shermis and Ben Hamner. 2012. Contrasting
state-of-the-art automated scoring of essays: analysis.
Technical report, The University of Akron and Kaggle.
Ying-Jian Wang, Hui-Fang Shang, and Paul Briody. In
press. Exploring the impact of using automated writ-
ing evaluation in English as a foreign language univer-
sity students? writing. Computer Assisted Language
Learning.
David M. Williamson. 2009. A framework for imple-
menting automated scoring. In Proceedings of the An-
nual Meeting of the American Educational Research
Association and the National Council on Measurement
in Education, San Diego, CA.
Helen Yannakoudakis, Ted Briscoe, and Ben Medlock.
2011. A new dataset and method for automatically
grading ESOL texts. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies.
Ramon Ziai, Niels Ott, and Detmar Meurers. 2012.
Short answer assessment: Establishing links between
research strands. In Proceedings of the workshop on
Building Educational Applications Using natural lan-
guage processing, pages 190?200.
41
Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task, pages 15?24,
Baltimore, Maryland, 26-27 July 2014.
c?2014 Association for Computational Linguistics
Grammatical error correction using hybrid systems and type filtering
Mariano Felice Zheng Yuan ?istein E. Andersen
Helen Yannakoudakis Ekaterina Kochmar
Computer Laboratory, University of Cambridge, United Kingdom
{mf501,zy249,oa223,hy260,ek358}@cl.cam.ac.uk
Abstract
This paper describes our submission to the
CoNLL 2014 shared task on grammatical
error correction using a hybrid approach,
which includes both a rule-based and an
SMT system augmented by a large web-
based language model. Furthermore, we
demonstrate that correction type estima-
tion can be used to remove unnecessary
corrections, improving precision without
harming recall. Our best hybrid system
achieves state-of-the-art results, ranking
first on the original test set and second on
the test set with alternative annotations.
1 Introduction
Grammatical error correction has attracted con-
siderable interest in the last few years, especially
through a series of ?shared tasks?. These efforts
have helped to provide a common ground for eval-
uating and comparing systems while encouraging
research in the field. These shared tasks have pri-
marily focused on English as a second or foreign
language and addressed different error types. The
HOO 2011 task (Dale and Kilgarriff, 2011), for
example, included all error types whereas HOO
2012 (Dale et al., 2012) and the CoNLL 2013
shared task (Ng et al., 2013) were restricted to only
two and five types respectively.
In this paper, we describe our submission to the
CoNLL 2014 shared task (Ng et al., 2014), which
involves correcting all the errors in essays writ-
ten in English by students at the National Univer-
sity of Singapore. An all-type task poses a greater
challenge, since correcting open-class types (such
as spelling or collocation errors) requires different
correction strategies than those in closed classes
(such as determiners or prepositions).
In this scenario, hybrid systems or combinations
of correction modules seem more appropriate and
typically produce good results. In fact, most of
the participating teams in previous shared tasks
have used a combination of modules or systems
for their submissions, even for correcting closed-
class types (Dahlmeier et al., 2011; Bhaskar et
al., 2011; Rozovskaya et al., 2011; Ivanova et al.,
2011; Rozovskaya et al., 2013; Yoshimoto et al.,
2013; Xing et al., 2013; Kunchukuttan et al., 2013;
Putra and Szabo, 2013; Xiang et al., 2013).
In line with previous research, we present a hy-
brid approach that employs a rule-based error cor-
rection system and an ad-hoc statistical machine
translation (SMT) system, as well as a large-scale
language model to rank alternative corrections and
an error type filtering technique.
The remainder of this paper is organised as fol-
lows: Section 2 describes our approach and each
component in detail, Section 3 presents our experi-
ments using the CoNLL 2014 shared task develop-
ment set and Section 4 reports our official results
on the test set. Finally, we discuss the performance
of our system and present an error analysis in Sec-
tion 5 and conclude in Section 6.
2 Approach
We tackle the error correction task using a pipeline
of processes that combines results from multiple
systems. Figure 1 shows the interaction of the
components in our final hybrid system, producing
the results submitted to the CoNLL 2014 shared
task. The following sections describe each of these
components in detail.
2.1 Rule-based error correction system
(RBS)
The rule-based system is a component of the Self-
Assessment and Tutoring (SAT) system, a web
service developed at the University of Cambridge
aimed at helping intermediate learners of English
15
Figure 1: Overview of components and interac-
tions in our final hybrid system.
in their writing tasks
1
(Andersen et al., 2013). The
original SAT system provides three main function-
alities: 1) text assessment, producing an overall
score for a piece of text, 2) sentence evaluation,
producing a sentence-level quality score, and 3)
word-level feedback, suggesting specific correc-
tions for frequent errors. Since the focus of the
shared task is on strict correction (as opposed to
detection), we only used the word-level feedback
component of the SAT system.
This module uses rules automatically derived
from the Cambridge Learner Corpus
2
(CLC)
(Nicholls, 2003) that are aimed at detecting error-
ful unigrams, bigrams and trigrams. In order to
ensure high precision, rules are based on n-grams
that have been annotated as incorrect at least five
times and at least ninety per cent of the times
they occur. In addition to these corpus-derived
rules, many cases of incorrect but plausible deriva-
tional and inflectional morphology are detected by
means of rules derived from a machine-readable
dictionary. For further details on specific compo-
nents, we refer the reader to the aforementioned
paper.
Given an input text, the rule-based system pro-
duces an XML file containing a list of suggested
corrections. These corrections can either be ap-
plied to the original text or used to generate mul-
tiple correction candidates, as described in Sec-
tion 2.3.
2.2 SMT system
We follow a similar approach to the one described
by Yuan and Felice (2013) in order to train an SMT
1
The latest version of the system, called ?Write
& Improve?, is available at http://www.cambridge
english.org/writeandimprovebeta/.
2
More information at http://www.cambridge
.org/elt/catalogue/subject/custom/item36
46603/
system that can ?translate? from incorrect into cor-
rect English. Our training data comprises a set of
different parallel corpora, where the original (in-
correct) sentences constitute the source side and
corrected versions based on gold standard anno-
tations constitute the target side. These corpora
include:
? the NUCLE v3.1 corpus (Dahlmeier et al.,
2013), containing around 1,400 essays writ-
ten in English by students at the National
University of Singapore (approx. 1,220,257
tokens in 57,152 sentences),
? phrase alignments involving corrections ex-
tracted automatically from the NUCLE cor-
pus (with up to 7 tokens per side), which are
used to boost the probability of phrase align-
ments that involve corrections so as to im-
prove recall,
? the CoNLL 2014 shared task development
set, containing 50 essays from the previous
year?s test set (approx. 29,207 tokens in 1,382
sentences),
? the First Certificate in English (FCE) cor-
pus (Yannakoudakis et al., 2011), contain-
ing 1,244 exam scripts and 2 essays per
script (approx. 532,033 tokens in 16,068 sen-
tences),
? a subset of the International English Lan-
guage Testing System (IELTS) examination
dataset extracted from the CLC corpus, con-
taining 2,498 exam scripts and 2 essays per
script (approx. 1,361,841 tokens in 64,628
sentences), and
? a set of sentences from the English Vo-
cabulary Profile
3
(EVP), which have been
modified to include artificially generated er-
rors (approx. 351,517 tokens in 18,830 sen-
tences). The original correct sentences are a
subset of the CLC and come from examina-
tions at different proficiency levels. The ar-
tificial error generation method aims at repli-
cating frequent error patterns observed in the
NUCLE corpus on error-free sentences, as
described by Yuan and Felice (2013).
3
Sentences were automatically scraped from http://
www.englishprofile.org/index.php?option=
com_content&view=article&id=4&Itemid=5
16
Word alignment was carried out using pialign
(Neubig et al., 2011), after we found it outper-
formed GIZA++ (Och and Ney, 2000; Och and
Ney, 2003) and Berkeley Aligner (Liang et al.,
2006; DeNero and Klein, 2007) in terms of pre-
cision and F
0.5
on the development set. Instead
of using heuristics to extract phrases from the
word alignments learnt by GIZA++ or Berker-
ley Aligner, pialign created a phrase table directly
from model probabilities.
In addition to the features already defined by pi-
align, we added character-level Levenshtein dis-
tance to each mapping in the phrase table. This
was done to allow for the fact that, in error correc-
tion, most words translate into themselves and er-
rors are often similar to their correct forms. Equal
weights were assigned to these features.
We then built a lexical reordering model using
the alignments created by pialign. The maximum
phrase length was set to 7, as recommended in the
SMT literature (Koehn et al., 2003; Koehn, 2014).
The IRSTLM Toolkit (Federico et al., 2008)
was used to build a 4-gram target language model
with Kneser?Ney smoothing (Kneser and Ney,
1995) on the correct sentences from the NUCLE,
full CLC and EVP corpora.
Decoding was performed with Moses (Koehn et
al., 2007), using the default settings and weights.
No tuning process was applied. The resulting sys-
tem was used to produce the 10 best correction
candidates for each sentence in the dataset, which
were further processed by other modules.
Segmentation, tokenisation and part-of-speech
tagging were performed using NLTK (Bird et
al., 2009) for consistency with the shared task
datasets.
2.3 Candidate generation
In order to integrate corrections from multiple sys-
tems, we developed a method to generate all the
possible corrected versions of a sentence (candi-
dates). Candidates are generated by computing all
possible combinations of corrections (irrespective
of the system from which they originate), includ-
ing the original tokens to allow for a ?no correc-
tion? option. The list of candidates produced for
each sentence always includes the original (un-
modified) sentence plus any other versions derived
from system corrections.
In order for a combination of corrections to gen-
erate a valid candidate, all the corrections must be
Figure 2: An example showing the candidate gen-
eration process.
Model CE ME UE P R F
0.5
SMT IRSTLM 651 2766 1832 0.2621 0.1905 0.2438
Microsoft Web
N-grams
666 2751 1344 0.3313 0.1949 0.2907
Table 1: Performance of language models on the
development set after ranking the SMT system?s
10-best candidates per sentence. CE: correct ed-
its, ME: missed edits, UE: unnecessary edits, P:
precision, R: recall.
compatible; otherwise, the candidate is discarded.
We consider two or more corrections to be com-
patible if they do not overlap, in an attempt to
avoid introducing accidental errors. In addition,
if different correction sets produce the same can-
didate, we only keep one. Figure 2 illustrates the
candidate generation process.
2.4 Language model ranking
Generated candidates are ranked using a language
model (LM), with the most probable candidate be-
ing selected as the final corrected version.
We tried two different alternatives for ranking:
1) using the target LM embedded in our SMT sys-
tem (described in Section 2.2) and 2) using a large
n-gram LM built from web data. In the latter
case, we used Microsoft Web N-gram Services,
which provide access to large smoothed n-gram
language models (with n=2,3,4,5) built from web
documents (Gao et al., 2010). All our experiments
are based on the 5-gram ?bing-body:apr10? model.
The ranking performance of these two models
was evaluated on the 10-best hypotheses generated
by the SMT system for each sentence in the devel-
opment set. Table 1 shows the results from the
M
2
Scorer (Dahlmeier and Ng, 2012), the official
scorer for the shared task that, unlike previous ver-
sions, weights precision twice as much as recall.
Results show that using Microsoft?s Web LM
yields better performance, which is unsurprising
given the vast amounts of data used to build that
17
System CE ME UE P R F
0.5
RBS 95 3322 107 0.4703 0.0278 0.1124
SMT 452 2965 690 0.3958 0.1323 0.2830
Table 2: Results of individual systems on the de-
velopment set.
model. For this reason, we adopt Microsoft?s
model for all further experiments.
We also note that without normalisation, higher
probabilities may be assigned to shorter sentences,
which can introduce a bias towards preferring
deletions or skipping insertions.
2.5 Type filtering
Analysing performance by error type is very valu-
able for system development and tuning. How-
ever, this can only be performed for corrections
in the gold standard (either matched or missed).
To estimate types for unnecessary corrections, we
defined a set of heuristics that analyse differences
in word forms and part-of-speech tags between
the original phrases and their system corrections,
based on common patterns observed in the train-
ing data. We had previously used a similar strat-
egy to classify errors in our CoNLL 2013 shared
task submission (Yuan and Felice, 2013) but have
now included a few improvements and rules for
new types. Estimation accuracy is 50.92% on the
training set and 67.57% on the development set,
which we consider to be acceptable for our pur-
poses given that the final test set is more similar to
the development set.
Identifying types for system corrections is not
only useful during system development but can
also be exploited to filter out and reduce the num-
ber of proposed corrections. More specifically, if
a system proposes a much higher number of un-
necessary corrections than correct suggestions for
a specific error type, we can assume the system is
actually degrading the quality of the original text,
in which case it is preferable to filter out those er-
ror types. Such decisions will lower the total num-
ber of unnecessary edits, thus improving overall
precision. However, they will also harm recall,
unless the number of matched corrections for the
error type is zero (i.e. unless P
type
= 0). To avoid
this, only corrections for types having zero preci-
sion should be removed.
3 Experiments and results
We carried out a series of experiments on the de-
velopment set using different pipelines and com-
binations of systems in order to find an optimal
setting. The following sections describe them in
detail.
3.1 Individual system performance
Our first set of experiments were aimed at inves-
tigating individual system performance on the de-
velopment set, which is reported in Table 2. Re-
sults show that the SMT system has much better
performance, which is expected given that it has
been trained on texts similar to those in the test
set.
3.2 Pipelines
Since corrections from the RBS and SMT systems
are often complementary, we set out to explore
combination schemes that would integrate correc-
tions from both systems. Table 3 shows results for
different combinations, where RBS and SMT in-
dicate all corrections from the respective systems,
subscript ?c? indicates candidates generated from
a system?s individual corrections, subscript ?10-
best? indicates the 10-best list of candidates pro-
duced by the SMT system, ?>? indicates a pipeline
where the output of one system is the input to the
other and ?+? indicates a combination of candi-
dates from different systems. All these pipelines
use the RBS system as the first processing step in
order to perform an initial correction, which is ex-
tremely beneficial for the SMT system.
Results reveal that the differences between
these pipelines are small in terms of F
0.5
, although
there are noticeable variations in precision and re-
call. The best results are achieved when the 10
best hypotheses from the SMT system are ranked
with Microsoft?s LM, which confirms our results
in Table 1 showing that the SMT LM is outper-
formed by a larger web-based model.
A simple pipeline using the RBS system first
and the SMT system second (#3) yields per-
formance that is better than (or comparable to)
pipelines #1, #2 and #4, suggesting that there is no
real benefit in using more sophisticated pipelines
when only the best hypothesis from the SMT sys-
tem is used. However, performance is improved
when the 10 best SMT hypotheses are considered.
The only difference between pipelines #5 and #6
lies in the way corrections from the RBS system
18
# Pipeline CE ME UE P R F
0.5
?
1 RBS > SMT
c
> LM 372 3045 481 0.4361 0.1088 0.2723
2 RBS
c
+ SMT
c
> LM 400 3017 485 0.4520 0.1171 0.2875
3 RBS > SMT 476 2941 738 0.3921 0.1393 0.2877
4 RBS
c
> LM > SMT 471 2946 718 0.3961 0.1378 0.2881
5 RBS > SMT
10-best
> LM 678 2739 1368 0.3314 0.1984 0.2922
6 RBS
c
> LM > SMT
10-best
> LM 681 2736 1366 0.3327 0.1993 0.2934
Table 3: Results for different system pipelines on the development set.
System CE ME UE P R F
0.5
RBS
c
> LM > SMT
10-best
> LM 681 2736 1366 0.3327 0.1993 0.2934
RBS
c
> LM > SMT
10-best
> LM > Filter 681 2736 1350 0.3353 0.1993 0.2950
Table 4: Results for individual systems on the development set.
are handled. In the first case, all corrections are
applied at once whereas in the second, the sug-
gested corrections are used to generate candidates
that are subsequently ranked by our LM, often dis-
carding some of the suggested corrections.
3.3 Filtering
As described in Section 2.5, we can evaluate per-
formance by error type in order to identify and re-
move unnecessary corrections. In particular, we
tried to optimise our best hybrid system (#6) by
filtering out types with zero precision. Table 5
shows type-specific performance for this system,
where three zero-precision types can be identi-
fied: Reordering (a subset of Others that we treat
separately), Srun (run-ons/comma splices) and Wa
(acronyms). Although reordering was explicitly
disabled in our SMT system, a translation table
can still include this type of mappings if they are
observed in the training data (e.g. ?you also can?
? ?you can also?).
In order to remove such undesired corrections,
the following procedure was applied: first, in-
dividual corrections were extracted by compar-
ing the original and corrected sentences; second,
the type of each extracted correction was pre-
dicted, subsequently deleting those that matched
unwanted types (i.e. reordering, Srun or Wa); fi-
nally, the set of remaining corrections was applied
to the original text. This method improves pre-
cision while preserving recall (see Table 4), al-
though the resulting improvement is not statisti-
cally significant (paired t-test, p > 0.05).
4 Official evaluation results
Our submission to the CoNLL 2014 shared task is
the result of our best hybrid system, described in
the previous section and summarised in Figure 1.
The official test set comprised 50 new essays (ap-
prox. 30,144 tokens in 1,312 sentences) written in
response to two prompts, one of which was also
included in the training data.
Systems were evaluated using the M
2
Scorer,
which uses F
0.5
as its overall measure. As in previ-
ous years, there were two evaluation rounds. The
first one was based on the original gold-standard
annotations provided by the shared-task organis-
ers whereas the second was based on a revised
version including alternative annotations submit-
ted by the participating teams. Our submitted sys-
tem achieved the first and second place respec-
tively. The official results of our submission in
both evaluation rounds are reported in Table 6.
5 Discussion and error analysis
In order to assess how our system performed per
error type on the test set, we ran our type estima-
tion script and obtained the results shown in Ta-
ble 7. Although these results are estimated and
therefore not completely accurate,
4
they can still
provide valuable insights, at least at a coarse level.
The following sections discuss our main findings.
5.1 Type performance
According to Table 7, our system achieves the best
performance for types WOadv (adverb/adjective
position) and Wtone (tone), but these results are
4
Estimation accuracy was found to be 57.90% on the test
set.
19
Error type CE ME UE P R F
0.5
ArtOrDet 222 465 225 0.4966 0.3231 0.4485
Cit 0 6 0 ? 0.0000 ?
Mec 31 151 15 0.6739 0.1703 0.4235
Nn 138 256 136 0.5036 0.3503 0.4631
Npos 4 25 45 0.0816 0.1379 0.0889
Others 1 34 12 0.0769 0.0286 0.0575
Pform 1 25 22 0.0435 0.0385 0.0424
Pref 1 38 5 0.1667 0.0256 0.0794
Prep 61 249 177 0.2563 0.1968 0.2417
Reordering 0 1 12 0.0000 0.0000 ?
Rloc- 13 115 80 0.1398 0.1016 0.1300
SVA 32 86 25 0.5614 0.2712 0.4624
Sfrag 0 4 0 ? 0.0000 ?
Smod 0 16 0 ? 0.0000 ?
Spar 4 30 0 1.0000 0.1176 0.4000
Srun 0 55 28 0.0000 0.0000 ?
Ssub 7 64 15 0.3182 0.0986 0.2201
Trans 13 128 36 0.2653 0.0922 0.1929
Um 0 34 0 ? 0.0000 ?
V0 2 16 3 0.4000 0.1111 0.2632
Vform 28 90 68 0.2917 0.2373 0.2789
Vm 9 86 41 0.1800 0.0947 0.1525
Vt 18 137 53 0.2535 0.1161 0.2050
WOadv 0 12 0 ? 0.0000 ?
WOinc 2 35 71 0.0274 0.0541 0.0304
Wa 0 5 2 0.0000 0.0000 ?
Wci 28 400 241 0.1041 0.0654 0.0931
Wform 65 161 54 0.5462 0.2876 0.4630
Wtone 1 12 0 1.0000 0.0769 0.2941
TOTAL 681 2736 1366 0.3327 0.1993 0.2934
Table 5: Type-specific performance of our best hy-
brid system on the development set. Types with
zero precision are marked in bold.
Test set CE ME UE P R F
0.5
Original 772 1793 1172 0.3971 0.3010 0.3733
Revised 913 1749 1042 0.4670 0.3430 0.4355
Table 6: Official results of our system on the orig-
inal and revised test sets.
not truly representative as they only account for a
small fraction of the test data (0.64% and 0.36%
respectively).
The third best performing type is Mec, which
comprises mechanical errors (such as punctuation,
capitalisation and spelling mistakes) and repre-
sents 11.58% of the errors in the data. The remark-
ably high precision obtained for this error type
suggests that our system is especially suitable for
correcting such errors.
We also found that our system was particularly
good at enforcing different types of agreement, as
demonstrated by the results for SVA (subject?verb
agreement), Pref (pronoun reference), Nn (noun
number) and Vform (verb form) types, which add
up to 22.80% of the errors. The following example
shows a successful correction:
Error type CE ME UE P R F
0.5
ArtOrDet 185 192 206 0.4731 0.4907 0.4766
Mec 86 219 16 0.8431 0.2820 0.6031
Nn 122 106 143 0.4604 0.5351 0.4736
Npos 2 13 59 0.0328 0.1333 0.0386
Others 0 30 10 0.0000 0.0000 ?
Pform 8 26 21 0.2759 0.2353 0.2667
Pref 19 77 12 0.6129 0.1979 0.4318
Prep 100 159 144 0.4098 0.3861 0.4049
Reordering 0 0 7 0.0000 ? ?
Rloc- 23 89 116 0.1655 0.2054 0.1722
SVA 38 85 31 0.5507 0.3089 0.4762
Sfrag 0 4 0 ? 0.0000 ?
Smod 0 2 0 ? 0.0000 ?
Spar 0 10 0 ? 0.0000 ?
Srun 0 14 1 0.0000 0.0000 ?
Ssub 8 39 19 0.2963 0.1702 0.2581
Trans 17 54 39 0.3036 0.2394 0.2881
Um 2 21 0 1.0000 0.0870 0.3226
V0 8 20 15 0.3478 0.2857 0.3333
Vform 31 93 46 0.4026 0.2500 0.3588
Vm 7 27 35 0.1667 0.2059 0.1733
Vt 26 108 40 0.3939 0.1940 0.3266
WOadv 10 11 0 1.0000 0.4762 0.8197
WOinc 1 33 37 0.0263 0.0294 0.0269
Wci 33 305 146 0.1844 0.0976 0.1565
Wform 42 49 29 0.5915 0.4615 0.5600
Wtone 4 7 0 1.0000 0.3636 0.7407
TOTAL 772 1793 1172 0.3971 0.3010 0.3733
Table 7: Type-specific performance of our submit-
ted system on the original test set.
ORIGINAL SENTENCE:
He or she has the right not to tell anyone .
SYSTEM HYPOTHESIS:
They have the right not to tell anyone .
GOLD STANDARD:
They have the right not to tell anyone .
In other cases, our system seems to do a good
job despite gold-standard annotations:
ORIGINAL SENTENCE:
This is because his or her relatives have the
right to know about this .
SYSTEM HYPOTHESIS:
This is because their relatives have the right
to know about this .
GOLD STANDARD:
This is because his or her relatives have the
right to know about this . (unchanged)
The worst performance is observed for Others
(including Reordering) and Srun, which only ac-
count for 1.69% of the errors. We also note that
Reordering and Srun errors, which had explicitly
been filtered out, still appear in our final results,
20
which is due to differences in the edit extraction
algorithms used by the M
2
Scorer and our own im-
plementation. According to our estimations, our
system has poor performance on the Wci type (the
second most frequent), suggesting it is not very
successful at correcting idioms and collocations.
Corrections for more complex error types such
as Um (unclear meaning), which are beyond the
scope of this shared task, are inevitably missed.
5.2 Deletions
We have also observed that many mismatches be-
tween our system?s corrections and the gold stan-
dard are caused by unnecessary deletions, as in the
following example:
ORIGINAL SENTENCE:
I could understand the feeling of the carrier .
SYSTEM HYPOTHESIS:
I understand the feeling of the carrier .
GOLD STANDARD:
I could understand the feeling of the carrier .
(unchanged)
This effect is the result of using 10-best hy-
potheses from the SMT system together with LM
ranking. Hypotheses from an SMT system can in-
clude many malformed sentences which are effec-
tively discarded by the embedded target language
model and additional heuristics. However, rank-
ing these raw hypotheses with external systems
can favour deletions, as language models will gen-
erally assign higher probabilities to shorter sen-
tences. A common remedy for this is normali-
sation but we found it made no difference in our
experiments.
In other cases, deletions can be ascribed to dif-
ferences in the domain of the training and test sets,
as observed in this example:
ORIGINAL SENTENCE:
Nowadays , social media are able to dissemi-
nate information faster than any other media .
SYSTEM HYPOTHESIS:
Nowadays , the media are able to disseminate
information faster than any other media .
GOLD STANDARD:
Nowadays , social media are able to dissemi-
nate information faster than any other media .
(unchanged)
5.3 Uncredited corrections
Our analysis also reveals a number of cases where
the system introduces changes that are not in-
cluded in the gold standard but we consider im-
prove the quality of a sentence. For example:
ORIGINAL SENTENCE:
Demon is not easily to be defeated and it is
required much of energy and psychological
support .
SYSTEM HYPOTHESIS:
Demon is not easily defeated and it requires
a lot of energy and psychological support .
GOLD STANDARD:
The demon is not easily defeated and it re-
quires much energy and psychological sup-
port .
Adding alternative corrections to the gold stan-
dard alleviates this problem, although the list of
alternatives will inevitably be incomplete.
There are also a number of cases where the sen-
tences are considered incorrect as part of a longer
text but are acceptable when they are evaluated in
isolation. Consider the following examples:
ORIGINAL SENTENCE:
The opposite is also true .
SYSTEM HYPOTHESIS:
The opposite is true .
GOLD STANDARD:
The opposite is also true . (unchanged)
ORIGINAL SENTENCE:
It has erased the boundaries of distance and
time .
SYSTEM HYPOTHESIS:
It has erased the boundaries of distance and
time . (unchanged)
GOLD STANDARD:
They have erased the boundaries of distance
and time .
In both cases, system hypotheses are perfectly
grammatical but they are considered incorrect
when analysed in context. Such mismatch is the
result of discrepancies between the annotation and
evaluation criteria: while the gold standard is an-
notated taking discourse into account, system cor-
21
rections are proposed in isolation, completely de-
void of discursive context.
Finally, the inability of the M
2
Scorer to com-
bine corrections from different annotators (as op-
posed to selecting only one annotator?s corrections
for the whole sentence) can also result in underes-
timations of performance. However, it is clear that
exploring these combinations during evaluation is
a challenging task itself.
6 Conclusions
We have presented a hybrid approach to error cor-
rection that combines a rule-based and an SMT
error correction system. We have explored dif-
ferent combination strategies, including sequen-
tial pipelines, candidate generation and ranking.
In addition, we have demonstrated that error type
estimations can be used to filter out unnecessary
corrections and improve precision without harm-
ing recall.
Results of our best hybrid system on the offi-
cial CoNLL 2014 test set yield F
0.5
=0.3733 for
the original annotations and F
0.5
=0.4355 for alter-
native corrections, placing our system in the first
and second place respectively.
Error analysis reveals that our system is partic-
ularly good at correcting mechanical errors and
agreement but is often penalised for unnecessary
deletions. However, a thorough inspection shows
that the system tends to produce very fluent sen-
tences, even if they do not match gold standard
annotations.
Acknowledgements
We would like to thank Marek Rei for his valuable
feedback and suggestions as well as Cambridge
English Language Assessment, a division of Cam-
bridge Assessment, for supporting this research.
References
?istein E. Andersen, Helen Yannakoudakis, Fiona
Barker, and Tim Parish. 2013. Developing and test-
ing a self-assessment and tutoring system. In Pro-
ceedings of the Eighth Workshop on Innovative Use
of NLP for Building Educational Applications, BEA
2013, pages 32?41, Atlanta, GA, USA, June. Asso-
ciation for Computational Linguistics.
Pinaki Bhaskar, Aniruddha Ghosh, Santanu Pal, and
Sivaji Bandyopadhyay. 2011. May I check the
English of your paper!!! In Proceedings of the
Generation Challenges Session at the 13th Euro-
pean Workshop on Natural Language Generation,
pages 250?253, Nancy, France, September. Associ-
ation for Computational Linguistics.
Steven Bird, Edward Loper, and Ewan Klein.
2009. Natural Language Processing with Python.
O?Reilly Media Inc.
Daniel Dahlmeier and Hwee Tou Ng. 2012. Bet-
ter evaluation for grammatical error correction. In
Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
NAACL 2012, pages 568?572, Montreal, Canada.
Daniel Dahlmeier, Hwee Tou Ng, and Thanh Phu Tran.
2011. NUS at the HOO 2011 Pilot Shared Task. In
Proceedings of the Generation Challenges Session
at the 13th European Workshop on Natural Lan-
guage Generation, pages 257?259, Nancy, France,
September. Association for Computational Linguis-
tics.
Daniel Dahlmeier, Hwee Tou Ng, and Siew Mei Wu.
2013. Building a Large Annotated Corpus of
Learner English: The NUS Corpus of Learner En-
glish. In Proceedings of the 8th Workshop on Inno-
vative Use of NLP for Building Educational Appli-
cations, BEA 2013, pages 22?31, Atlanta, Georgia,
USA, June.
Robert Dale and Adam Kilgarriff. 2011. Helping
Our Own: The HOO 2011 Pilot Shared Task. In
Proceedings of the Generation Challenges Session
at the 13th European Workshop on Natural Lan-
guage Generation, pages 242?249, Nancy, France,
September. Association for Computational Linguis-
tics.
Robert Dale, Ilya Anisimoff, and George Narroway.
2012. HOO 2012: A Report on the Preposition
and Determiner Error Correction Shared Task. In
Proceedings of the Seventh Workshop on Building
Educational Applications Using NLP, pages 54?62,
Montr?eal, Canada, June. Association for Computa-
tional Linguistics.
John DeNero and Dan Klein. 2007. Tailoring word
alignments to syntactic machine translation. In Pro-
ceedings of the 45th Annual Meeting of the Asso-
ciation of Computational Linguistics, pages 17?24,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Marcello Federico, Nicola Bertoldi, and Mauro Cet-
tolo. 2008. IRSTLM: an open source toolkit for
handling large scale language models. In Proceed-
ings of the 9th Annual Conference of the Interna-
tional Speech Communication Association, INTER-
SPEECH 2008, pages 1618?1621, Brisbane, Aus-
tralia, September. ISCA.
Jianfeng Gao, Patrick Nguyen, Xiaolong Li, Chris
Thrasher, Mu Li, and Kuansan Wang. 2010. A
Comparative Study of Bing Web N-gram Language
Models for Web Search and Natural Language Pro-
cessing. In Web N-gram Workshop, Workshop of the
22
33rd Annual International ACM SIGIR Conference
(SIGIR 2010), pages 16?21, Geneva, Switzerland,
July.
Elitza Ivanova, Delphine Bernhard, and Cyril Grouin.
2011. Handling Outlandish Occurrences: Using
Rules and Lexicons for Correcting NLP Articles. In
Proceedings of the Generation Challenges Session
at the 13th European Workshop on Natural Lan-
guage Generation, pages 254?256, Nancy, France,
September. Association for Computational Linguis-
tics.
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In Pro-
ceedings of the IEEE International Conference on
Acoustics, Speech and Signal Processing, volume I,
pages 181?184, Detroit, Michigan, May.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the 2003 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics on Human Language Technology, vol-
ume 1 of NAACL ?03, pages 48?54, Edmonton,
Canada. Association for Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ond?rej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
ACL ?07, pages 177?180, Prague, Czech Republic.
Association for Computational Linguistics.
Philipp Koehn, 2014. Moses: Statistical Ma-
chine Translation System ? User Manual and Code
Guide. University of Edinburgh, April. Available
online at http://www.statmt.org/moses/
manual/manual.pdf.
Anoop Kunchukuttan, Ritesh Shah, and Pushpak Bhat-
tacharyya. 2013. IITB System for CoNLL 2013
Shared Task: A Hybrid Approach to Grammati-
cal Error Correction. In Proceedings of the Seven-
teenth Conference on Computational Natural Lan-
guage Learning: Shared Task, pages 82?87, Sofia,
Bulgaria, August. Association for Computational
Linguistics.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of the Human
Language Technology Conference of the NAACL,
Main Conference, pages 104?111, New York City,
USA, June. Association for Computational Linguis-
tics.
Graham Neubig, Taro Watanabe, Eiichiro Sumita,
Shinsuke Mori, and Tatsuya Kawahara. 2011. An
unsupervised model for joint phrase alignment and
extraction. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies, pages 632?
641, Portland, Oregon, USA, June. Association for
Computational Linguistics.
Hwee Tou Ng, Siew Mei Wu, Yuanbin Wu, Christian
Hadiwinoto, and Joel Tetreault. 2013. The CoNLL-
2013 Shared Task on Grammatical Error Correction.
In Proceedings of the Seventeenth Conference on
Computational Natural Language Learning: Shared
Task, pages 1?12, Sofia, Bulgaria, August. Associa-
tion for Computational Linguistics.
Hwee Tou Ng, Siew Mei Wu, Ted Briscoe, Christian
Hadiwinoto, Raymond Hendy Susanto, and Christo-
pher Bryant. 2014. The CoNLL-2014 Shared Task
on Grammatical Error Correction. In Proceedings of
the Eighteenth Conference on Computational Natu-
ral Language Learning: Shared Task (CoNLL-2014
Shared Task), Baltimore, Maryland, USA, June. As-
sociation for Computational Linguistics. To appear.
Diane Nicholls. 2003. The Cambridge Learner Cor-
pus: Error coding and analysis for lexicography and
ELT. In Dawn Archer, Paul Rayson, Andrew Wil-
son, and Tony McEnery, editors, Proceedings of
the Corpus Linguistics 2003 conference, pages 572?
581, Lancaster, UK. University Centre for Computer
Corpus Research on Language, Lancaster Univer-
sity.
Franz Josef Och and Hermann Ney. 2000. Improved
statistical alignment models. In Proceedings of the
38th Annual Meeting on Association for Computa-
tional Linguistics, ACL ?00, pages 440?447, Hong
Kong, October. Association for Computational Lin-
guistics.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Comput. Linguist., 29(1):19?51, March.
Desmond Darma Putra and Lili Szabo. 2013. UdS
at CoNLL 2013 Shared Task. In Proceedings of
the Seventeenth Conference on Computational Natu-
ral Language Learning: Shared Task, pages 88?95,
Sofia, Bulgaria, August. Association for Computa-
tional Linguistics.
Alla Rozovskaya, Mark Sammons, Joshua Gioja, and
Dan Roth. 2011. University of Illinois System in
HOO Text Correction Shared Task. In Proceedings
of the Generation Challenges Session at the 13th Eu-
ropean Workshop on Natural Language Generation,
pages 263?266, Nancy, France, September. Associ-
ation for Computational Linguistics.
Alla Rozovskaya, Kai-Wei Chang, Mark Sammons,
and Dan Roth. 2013. The University of Illinois
System in the CoNLL-2013 Shared Task. In Pro-
ceedings of the Seventeenth Conference on Compu-
tational Natural Language Learning: Shared Task,
pages 13?19, Sofia, Bulgaria, August. Association
for Computational Linguistics.
23
Yang Xiang, Bo Yuan, Yaoyun Zhang, Xiaolong Wang,
Wen Zheng, and Chongqiang Wei. 2013. A hy-
brid model for grammatical error correction. In Pro-
ceedings of the Seventeenth Conference on Compu-
tational Natural Language Learning: Shared Task,
pages 115?122, Sofia, Bulgaria, August. Associa-
tion for Computational Linguistics.
Junwen Xing, Longyue Wang, Derek F. Wong, Lidia S.
Chao, and Xiaodong Zeng. 2013. UM-Checker: A
Hybrid System for English Grammatical Error Cor-
rection. In Proceedings of the Seventeenth Confer-
ence on Computational Natural Language Learn-
ing: Shared Task, pages 34?42, Sofia, Bulgaria, Au-
gust. Association for Computational Linguistics.
Helen Yannakoudakis, Ted Briscoe, and Ben Medlock.
2011. A new dataset and method for automatically
grading esol texts. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
180?189, Portland, Oregon, USA, June. Association
for Computational Linguistics.
Ippei Yoshimoto, Tomoya Kose, Kensuke Mitsuzawa,
Keisuke Sakaguchi, Tomoya Mizumoto, Yuta
Hayashibe, Mamoru Komachi, and Yuji Matsumoto.
2013. NAIST at 2013 CoNLL Grammatical Er-
ror Correction Shared Task. In Proceedings of the
Seventeenth Conference on Computational Natural
Language Learning: Shared Task, pages 26?33,
Sofia, Bulgaria, August. Association for Computa-
tional Linguistics.
Zheng Yuan and Mariano Felice. 2013. Constrained
grammatical error correction using statistical ma-
chine translation. In Proceedings of the Seventeenth
Conference on Computational Natural Language
Learning: Shared Task, pages 52?61, Sofia, Bul-
garia, August. Association for Computational Lin-
guistics.
24
