Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume, pages 292?295,
New York City, June 2006. c?2006 Association for Computational Linguistics
AUTOMATED QUALITY MONITORING FOR CALL CENTERS USING SPEECH AND NLP
TECHNOLOGIES
G. Zweig, O. Siohan, G. Saon, B. Ramabhadran, D. Povey, L. Mangu and B. Kingsbury
IBM T.J. Watson Research Center, Yorktown Heights, NY 10598
ABSTRACT
This paper describes an automated system for assigning qual-
ity scores to recorded call center conversations. The system com-
bines speech recognition, pattern matching, and maximum entropy
classification to rank calls according to their measured quality.
Calls at both ends of the spectrum are flagged as ?interesting? and
made available for further human monitoring. In this process, the
ASR transcript is used to answer a set of standard quality control
questions such as ?did the agent use courteous words and phrases,?
and to generate a question-based score. This is interpolated with
the probability of a call being ?bad,? as determined by maximum
entropy operating on a set of ASR-derived features such as ?max-
imum silence length? and the occurrence of selected n-gram word
sequences. The system is trained on a set of calls with associated
manual evaluation forms. We present precision and recall results
from IBM?s North American Help Desk indicating that for a given
amount of listening effort, this system triples the number of bad
calls that are identified, over the current policy of randomly sam-
pling calls. The application that will be demonstrated is a research
prototype that was built in conjunction with IBM?s North Ameri-
can call centers.
1. INTRODUCTION
Every day, tens of millions of help-desk calls are recorded at call
centers around the world. As part of a typical call center operation
a random sample of these calls is normally re-played to human
monitors who score the calls with respect to a variety of quality
related questions, e.g.
? Was the account successfully identified by the agent?
? Did the agent request error codes/messages to help deter-
mine the problem?
? Was the problem resolved?
? Did the agent maintain appropriate tone, pitch, volume and
pace?
This process suffers from a number of important problems: first,
the monitoring at least doubles the cost of each call (first an opera-
tor is paid to take it, then a monitor to evaluate it). This causes the
second problem, which is that therefore only a very small sample
of calls, e.g. a fraction of a percent, is typically evaluated. The
third problem arises from the fact that most calls are ordinary and
uninteresting; with random sampling, the human monitors spend
most of their time listening to uninteresting calls.
This work describes an automated quality-monitoring system
that addresses these problems. Automatic speech recognition is
used to transcribe 100% of the calls coming in to a call center,
and default quality scores are assigned based on features such as
key-words, key-phrases, the number and type of hesitations, and
the average silence durations. The default score is used to rank
the calls from worst-to-best, and this sorted list is made available
to the human evaluators, who can thus spend their time listening
only to calls for which there is some a-priori reason to expect that
there is something interesting.
The automatic quality-monitoring problem is interesting in
part because of the variability in how hard it is to answer the ques-
tions. Some questions, for example, ?Did the agent use courteous
words and phrases?? are relatively straightforward to answer by
looking for key words and phrases. Others, however, require es-
sentially human-level knowledge to answer; for example one com-
pany?s monitors are asked to answer the question ?Did the agent
take ownership of the problem?? Our work focuses on calls from
IBM?s North American call centers, where there is a set of 31 ques-
tions that are used to evaluate call-quality. Because of the high de-
gree of variability found in these calls, we have investigated two
approaches:
1. Use a partial score based only on the subset of questions
that can be reliably answered.
2. Use a maximum entropy classifier to map directly from
ASR-generated features to the probability that a call is bad
(defined as belonging to the bottom 20% of calls).
We have found that both approaches are workable, and we present
final results based on an interpolation between the two scores.
These results indicate that for a fixed amount of listening effort,
the number of bad calls that are identified approximately triples
with our call-ranking approach. Surprisingly, while there has been
significant previous scholarly research in automated call-routing
and classification in the call center , e.g. [1, 2, 3, 4, 5], there has
been much less in automated quality monitoring per se.
2. ASR FOR CALL CENTER TRANSCRIPTION
2.1. Data
The speech recognition systems were trained on approximately
300 hours of 6kHz, mono audio data collected at one of the IBM
call centers located in Raleigh, NC. The audio was manually tran-
scribed and speaker turns were explicitly marked in the word tran-
scriptions but not the corresponding times. In order to detect
speaker changes in the training data, we did a forced-alignment of
the data and chopped it at speaker boundaries. The test set consists
of 50 calls with 113 speakers totaling about 3 hours of speech.
2.2. Speaker Independent System
The raw acoustic features used for segmentation and recognition
are perceptual linear prediction (PLP) features. The features are
292
Segmentation/clustering Adaptation WER
Manual Off-line 30.2%
Manual Incremental 31.3%
Manual No Adaptation 35.9%
Automatic Off-line 33.0%
Automatic Incremental 35.1%
Table 1. ASR results depending on segmentation/clustering and
adaptation type.
Accuracy Top 20% Bottom 20%
Random 20% 20%
QA 41% 30%
Table 2. Accuracy for the Question Answering system.
mean-normalized 40-dimensional LDA+MLLT features. The SI
acoustic model consists of 50K Gaussians trained with MPE and
uses a quinphone cross-word acoustic context. The techniques are
the same as those described in [6].
2.3. Incremental Speaker Adaptation
In the context of speaker-adaptive training, we use two forms
of feature-space normalization: vocal tract length normalization
(VTLN) and feature-space MLLR (fMLLR, also known as con-
strained MLLR) to produce canonical acoustic models in which
some of the non-linguistic sources of speech variability have been
reduced. To this canonical feature space, we then apply a discrim-
inatively trained transform called fMPE [7]. The speaker adapted
recognition model is trained in this resulting feature space using
MPE.
We distinguish between two forms of adaptation: off-line and
incremental adaptation. For the former, the transformations are
computed per conversation-side using the full output of a speaker
independent system. For the latter, the transformations are updated
incrementally using the decoded output of the speaker adapted sys-
tem up to the current time. The speaker adaptive transforms are
then applied to the future sentences. The advantage of incremental
adaptation is that it only requires a single decoding pass (as op-
posed to two passes for off-line adaptation) resulting in a decoding
process which is twice as fast. In Table 1, we compare the per-
formance of the two approaches. Most of the gain of full offline
adaptation is retained in the incremental version.
2.3.1. Segmentation and Speaker Clustering
We use an HMM-based segmentation procedure for segmenting
the audio into speech and non-speech prior to decoding. The rea-
son is that we want to eliminate the non-speech segments in order
to reduce the computational load during recognition. The speech
segments are clustered together in order to identify segments com-
ing from the same speaker which is crucial for speaker adaptation.
The clustering is done via k-means, each segment being modeled
by a single diagonal covariance Gaussian. The metric is given by
the symmetric K-L divergence between two Gaussians. The im-
pact of the automatic segmentation and clustering on the error rate
is indicated in Table 1.
Accuracy Top 20% Bottom 20%
Random 20% 20%
ME 49% 36%
Table 3. Accuracy for the Maximum Entropy system.
Accuracy Top 20% Bottom 20%
Random 20% 20%
ME + QA 53% 44%
Table 4. Accuracy for the combined system.
3. CALL RANKING
3.1. Question Answering
This section presents automated techniques for evaluating call
quality. These techniques were developed using a train-
ing/development set of 676 calls with associated manually gen-
erated quality evaluations. The test set consists of 195 calls.
The quality of the service provided by the help-desk represen-
tatives is commonly assessed by having human monitors listen to
a random sample of the calls and then fill in evaluation forms. The
form for IBM?s North American Help Desk contains 31 questions.
A subset of the questions can be answered easily using automatic
methods, among those the ones that check that the agent followed
the guidelines e.g.
? Did the agent follow the appropriate closing script?
? Did the agent identify herself to the customer?
But some of the questions require human-level knowledge of the
world to answer, e.g.
? Did the agent ask pertinent questions to gain clarity of the
problem?
? Were all available resources used to solve the problem?
We were able to answer 21 out of the 31 questions using pat-
tern matching techniques. For example, if the question is ?Did
the agent follow the appropriate closing script??, we search for
?THANK YOU FOR CALLING?, ?ANYTHING ELSE? and
?SERVICE REQUEST?. Any of these is a good partial match for
the full script, ?Thank you for calling, is there anything else I can
help you with before closing this service request?? Based on the
answer to each of the 21 questions, we compute a score for each
call and use it to rank them. We label a call in the test set as being
bad/good if it has been placed in the bottom/top 20% by human
evaluators. We report the accuracy of our scoring system on the
test set by computing the number of bad calls that occur in the
bottom 20% of our sorted list and the number of good calls found
in the top 20% of our list. The accuracy numbers can be found in
Table 2.
3.2. Maximum Entropy Ranking
Another alternative for scoring calls is to find arbitrary features in
the speech recognition output that correlate with the outcome of a
call being in the bottom 20% or not. The goal is to estimate the
probability of a call being bad based on features extracted from
the automatic transcription. To achieve this we build a maximum
293
Fig. 1. Display of selected calls.
entropy based system which is trained on a set of calls with asso-
ciated transcriptions and manual evaluations. The following equa-
tion is used to determine the score of a call C using a set of N
predefined features:
P (class/C) = 1Z exp(
N
X
i=1
?ifi(class, C)) (1)
where class ? {bad, not ? bad}, Z is a normalizing factor, fi()
are indicator functions and {?i}{i=1,N} are the parameters of the
model estimated via iterative scaling [8].
Due to the fact that our training set contained under 700 calls,
we used a hand-guided method for defining features. Specifi-
cally, we generated a list of VIP phrases as candidate features,
e.g. ?THANK YOU FOR CALLING?, and ?HELP YOU?. We
also created a pool of generic ASR features, e.g. ?number of hes-
itations?, ?total silence duration?, and ?longest silence duration?.
A decision tree was then used to select the most relevant features
and the threshold associated with each feature. The final set of fea-
tures contained 5 generic features and 25 VIP phrases. If we take a
look at the weights learned for different features, we can see that if
a call has many hesitations and long silences then most likely the
call is bad.
We use P (bad|C) as shown in Equation 1 to rank all the calls.
Table 3 shows the accuracy of this system for the bottom and top
20% of the test calls.
At this point we have two scoring mechanisms for each call:
one that relies on answering a fixed number of evaluation ques-
tions and a more global one that looks across the entire call for
hints. These two scores are both between 0 and 1, and therefore
can be interpolated to generate one unique score. After optimizing
the interpolation weights on a held-out set we obtained a slightly
higher weight (0.6) for the maximum entropy model. It can be
seen in Table 4 that the accuracy of the combined system is greater
that the accuracy of each individual system, suggesting the com-
plementarity of the two initial systems.
4. END-TO-END SYSTEM PERFORMANCE
4.1. Application
This section describes the user interface of the automated quality
monitoring application. As explained in Section 1, the evalua-
Fig. 2. Interface to listen to audio and update the evaluation form.
tor scores calls with respect to a set of quality-related questions
after listening to the calls. To aid this process, the user interface
provides an efficient mechanism for the human evaluator to select
calls, e.g.
? All calls from a specific agent sorted by score
? The top 20% or the bottom 20% of the calls from a specific
agent ranked by score
? The top 20% or the bottom 20% of all calls from all agents
The automated quality monitoring user interface is a J2EE web
application that is supported by back-end databases and content
management systems 1 The displayed list of calls provides a link
to the audio, the automatically filled evaluation form, the overall
score for this call, the agent?s name, server location, call id, date
and duration of the call (see Figure 1). This interface now gives
the agent the ability to listen to interesting calls and update the
answers in the evaluation form if necessary (audio and evaluation
form illustrated in 2). In addition, this interface provides the eval-
uator with the ability to view summary statistics (average score)
and additional information about the quality of the calls. The over-
all system is designed to automatically download calls from mul-
tiple locations on a daily-basis, transcribe and index them, thereby
making them available to the supervisors for monitoring. Calls
spanning a month are available at any given time for monitoring
purposes.
4.2. Precision and Recall
This section presents precision and recall numbers for the
identification of ?bad? calls. The test set consists of 195 calls that
were manually evaluated by call center personnel. Based on these
manual scores, the calls were ordered by quality, and the bottom
20% were deemed to be ?bad.? To retrieve calls for monitoring,
we sort the calls based on the automatically assigned quality score
and return the worst. In our summary figures, precision and recall
are plotted as a function of the number of calls that are selected
for monitoring. This is important because in reality only a small
number of calls can receive human attention. Precision is the ratio
1In our case, the backend consists of DB2 and IBM?s Websphere Infor-
mation Integrator for Content and the application is hosted on Websphere
5.1.)
294
020
40
60
80
100
0 20 40 60 80 100
Observed
Ideal
Random
Fig. 3. Precision for the bottom 20% of the calls as a function of
the number of calls retrieved.
0
20
40
60
80
100
0 20 40 60 80 100
Observed
Ideal
Random
Fig. 4. Recall for the bottom 20% of the calls.
of bad calls retrieved to the total number of calls monitored, and
recall is the ratio of the number of bad calls retrieved to the total
number of bad calls in the test set. Three curves are shown in each
plot: the actually observed performance, performance of random
selection, and oracle or ideal performance. Oracle performance
shows what would happen if a perfect automatic ordering of the
calls was achieved.
Figure 3 shows precision performance. We see that in the
monitoring regime where only a small fraction of the calls are
monitored, we achieve over 60% precision. (Further, if 20% of
the calls are monitored, we still attain over 40% precision.)
Figure 4 shows the recall performance. In the regime of low-
volume monitoring, the recall is midway between what could be
achieved with an oracle, and the performance of random-selection.
Figure 5 shows the ratio of the number of bad calls found with
our automated ranking to the number found with random selection.
This indicates that in the low-monitoring regime, our automated
technique triples efficiency.
4.3. Human vs. Computer Rankings
As a final measure of performance, in Figure 6 we present a
scatterplot comparing human to computer rankings. We do not
have calls that are scored by two humans, so we cannot present a
human-human scatterplot for comparison.
5. CONCLUSION
This paper has presented an automated system for quality moni-
toring in the call center. We propose a combination of maximum-
entropy classification based on ASR-derived features, and question
answering based on simple pattern-matching. The system can ei-
ther be used to replace human monitors, or to make them more
1
1.5
2
2.5
3
3.5
4
4.5
5
0 20 40 60 80 100
Observed
Ideal
Fig. 5. Ratio of bad calls found with QTM to Random selection as
a function of the number of bad calls retrieved.
0
20
40
60
80
100
120
140
160
180
200
0 20 40 60 80 100 120 140 160 180 200
Fig. 6. Scatter plot of Human vs. Computer Rank.
efficient. Our results show that we can triple the efficiency of hu-
man monitors in the sense of identifying three times as many bad
calls for the same amount of listening effort.
6. REFERENCES
[1] J. Chu-Carroll and B. Carpenter, ?Vector-based natural lan-
guage call routing,? Computational Linguistics, 1999.
[2] P. Haffner, G. Tur, and J. Wright, ?Optimizing svms for com-
plex call classification,? 2003.
[3] M. Tang, B. Pellom, and K. Hacioglu, ?Call-type classifica-
tion and unsupervised training for the call center domain,? in
ARSU-2003, 2003.
[4] D. Hakkani-Tur, G. Tur, M. Rahim, and G. Riccardi, ?Unsu-
pervised and active learning in automatic speech recognition
for call classification,? in ICASSP-04, 2004.
[5] C. Wu, J. Kuo, E.E. Jan, V. Goel, and D. Lubensky, ?Improv-
ing end-to-end performance of call classification through data
confusion reduction and model tolerance enhancement,? in
Interspeech-05, 2005.
[6] H. Soltau, B. Kingsbury, L. Mangu, D. Povey, G. Saon, and
G. Zweig, ?The ibm 2004 conversational telephony system
for rich transcription,? in Eurospeech-2005, 2005.
[7] D. Povey, B. Kingsbury, L. Mangu, G. Saon, H. Soltau,
and G. Zweig, ?fMPE: Discriminatively trained features for
speech recognition,? in ICASSP-2005, 2004.
[8] A. Berger, S. Della Pietra, and V. Della Pietra, ?A maximum
entropy approach to natural language processing,? Computa-
tional Linguistics, vol. 22, no. 1, 1996.
295
Proceedings of NAACL HLT 2009: Short Papers, pages 277?280,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Fast decoding for open vocabulary spoken term detection
1B. Ramabhadran,1A. Sethy, 2J. Mamou?1 B. Kingsbury, 1 U. Chaudhari
1IBM T. J. Watson Research Center
Yorktown Heights,NY
2IBM Haifa Research Labs
Mount Carmel,Haifa
Abstract
Information retrieval and spoken-term detec-
tion from audio such as broadcast news, tele-
phone conversations, conference calls, and
meetings are of great interest to the academic,
government, and business communities. Mo-
tivated by the requirement for high-quality in-
dexes, this study explores the effect of using
both word and sub-word information to find
in-vocabulary and OOV query terms. It also
explores the trade-off between search accu-
racy and the speed of audio transcription. We
present a novel, vocabulary independent, hy-
brid LVCSR approach to audio indexing and
search and show that using phonetic confu-
sions derived from posterior probabilities es-
timated by a neural network in the retrieval
of OOV queries can help in reducing misses.
These methods are evaluated on data sets from
the 2006 NIST STD task.
1 Introduction
Indexing and retrieval of speech content in vari-
ous forms such as broadcast news, customer care
data and on-line media has gained a lot of interest
for a wide range of applications from market in-
telligence gathering, to customer analytics and on-
line media search. Spoken term detection (STD) is
a key information retrieval technology which aims
open vocabulary search over large collections of
spoken documents. An approach for solving the out-
of-vocabulary (OOV) issues (Saraclar and Sproat,
2004) consists of converting speech into phonetic,
?TThe work done by J. Mamou was partially funded by the
EU projects SAPIR and HERMES
syllabic or word-fragment transcripts and represent-
ing the query as a sequence of phones, syllables or
word-fragments respectively. Popular approaches
include subword decoding (Clements et al, 2002;
Mamou et al, 2007; Seide et al, 2004; Siohan and
Bacchiani, 2005) and representations enhanced with
phone confusion probabilities and approximate sim-
ilarity measures (Chaudhari and Picheny, 2007).
2 Fast Decoding Architecture
The first step in converting speech to a searchable in-
dex involves the use of an ASR system that produces
word, word-fragment or phonetic transcripts. In
this paper, the LVCSR system is a discriminatively
trained speaker-independent recognizer using PLP-
derived features and a quinphone acoustic model
with approximately 1200 context dependent states
and 30000 Gaussians. The acoustic model is trained
on 430 hours of audio from the 1996 and 1997 En-
glish Broadcast News Speech corpus (LDC97S44,
LDC98S71) and the TDT4 Multilingual Broadcast
News Speech corpus (LDC2005S11).
The language model used for decoding is a tri-
gram model with 84087 words trained on a collec-
tion of 335M words from the following data sources:
Hub4 Language Model data, EARS BN03 closed
captions and GALE Broadcast news and conversa-
tions data. A word-fragment language model is built
on this same data after tokenizing the text to frag-
ments using a fragment inventory of size 21000. A
greedy search algorithm assigns the longest possi-
ble matching fragment first and iteratively uses the
next longest possible fragment until the entire pro-
nunciation of the OOV term has been represented
277
0 5 10 15 20 25 30
30
40
50
60
70
80
90
Real Time Factor
W
ER
Figure 1: Speed vs WER
by sub-word units.
The speed and accuracy of the decoding are con-
trolled using two forms of pruning. The first is the
standard likelihood-based beam pruning that is used
in many Viterbi decoders. The second is a form
of Gaussian shortlisting in which the Gaussians in
the acoustic model are clustered into 1024 clusters,
each of which is represented by a single Gaussian.
When the decoder gets a new observation vector, it
computes the likelihood of the observation under all
1024 cluster models and then ranks the clusters by
likelihood. Observation likelihoods are then com-
puted only for those mixture components belonging
to the top maxL1 clusters; for components outside
this set a default, low likelihood is used. To illus-
trate the trade-offs in speed vs. accuracy that can
be achieved by varying the two pruning parame-
ters, we sweep through different values for the pa-
rameters and measure decoding accuracy, reported
as word error rate (WER), and decoding speed, re-
ported as times faster than real time (xfRT). For ex-
ample, a system that operates at 20xfRT will require
one minute of time (measured as elapsed time) to
process 20 minutes of speech. Figure 1 illustrates
this effect on the NIST 2006 Spoken Term Detec-
tion Dev06 test set.
3 Lucene Based Indexing and Search
The main difficulty with retrieving information from
spoken data is the low accuracy of the transcription,
particularly on terms of interest such as named en-
tities and content words. Generally, the accuracy
of a transcript is measured by its word error rate
(WER), which is characterized by the number of
substitutions, deletions, and insertions with respect
to the correct audio transcript. Mamou (Mamou
et al, 2007) presented the enhancement in recall
and precision by searching on word confusion net-
works instead of considering only the 1-best path
word transcript. We used this model for searching
in-vocabulary queries.
To handle OOV queries, a combination of
word and phonetic search was presented by
Mamou (Mamou et al, 2007). In this paper, we ex-
plore fuzzy phonetic search extending Lucene1, an
Apache open source search library written in Java,
for indexing and search. When searching for these
OOVs in word-fragment indexes, they are repre-
sented phonetically (and subsequently using word-
fragments) using letter-to-phoneme (L2P) rules.
3.1 Indexing
Each transcript is composed of basic units (e.g.,
word, word-fragment, phones) associated with a be-
gin time, duration and posterior probability. An
inverted index is used in a Lucene-based indexing
scheme. Each occurrence of a unit of indexing u in
a transcript D is indexed on its timestamp. If the
posterior probability is provided, we store the confi-
dence level of the occurrence of u at the time t that
is evaluated by its posterior probability Pr(u|t,D).
Otherwise, we consider its posterior probability to
be one. This representation allows the indexing of
different types of transcripts into a single index.
3.2 Retrieval
Since the vocabulary of the ASR system used to gen-
erate the word transcripts is known, we can easily
identify IV and OOV parts of the query. We present
two different algorithms, namely, exact and fuzzy
search on word-fragment transcripts. For search
on word-fragment or phonetic transcripts, the query
terms are converted to their word-fragment or pho-
netic representation.
Candidate lists of each query unit are extracted
from the inverted index. For fuzzy search, we re-
trieve several fuzzy matches from the inverted in-
dex for each unit of the query using the edit distance
weighted by the substitution costs provided by the
confusion matrix. Only the matches whose weighted
1http://lucene.apache.org/
278
edit distance is below a given threshold are returned.
We use a dynamic programming algorithm to incor-
porate the confusion costs specified in the matrix
in the distance computation. Our implementation is
fail-fast since the procedure is aborted if it is discov-
ered that the minimal cost between the sequences is
greater than a certain threshold.
The score of each occurrence aggregates the pos-
terior probability of each indexed unit. The occur-
rence of each unit is also weighted (user defined
weight) according to its type, for example, a higher
weight can be assigned to word matches instead of
word-fragment or phonetic matches. Given the na-
ture of the index, a match for any query term cannot
span across two consecutively indexed units.
3.3 Hybrid WordFragment Indexing
For the hybrid system we limited the word portion
of the ASR system?s lexicon to the 21K most fre-
quent (frequency greater than 5) words in the acous-
tic training data. This resulted in roughly 11M
(3.1%) OOV tokens in the hybrid LM training set
and 1127(2.5%) OOV tokens in the evaluation set.
A relative entropy criterion described in (Siohan and
Bacchiani, 2005) based on a 5-gram phone language
model was used to identify fragments. We selected
21K fragments to complement the 21K words result-
ing in a composite 42K vocabulary. The language
model text (11M (3.1%) fragment tokens and 320M
word tokens) was tokenized to contain words and
word-fragments (for the OOVs) and the resulting hy-
brid LM was used in conjunction with the acoustic
models described in Section 2.
4 Neural Network Based Posteriors for
Fuzzy Search
In assessing the match of decoded transcripts with
search queries, recognition errors must be accounted
for. One method relies on converting both the de-
coded transcripts and queries into phonetic represen-
tations and modeling the confusion between phones,
typically represented as a confusion matrix. In this
work, we derive this matrix from broadcast news de-
velopment data. In particular, two systems: HMM
based automatic speech recognition (ASR) (Chaud-
hari and Picheny, 2007) and a neural network based
acoustic model (Kingsbury, 2009), are used to ana-
lyze the data and the results are compared to produce
confusion estimates.
Let X = {xt} represent the input feature frames
and S the set of context dependent HMM states.
Associated with S is a many to one map M from
each member sj ? S to a phone in the phone set
pk ? P. This map collapses the beginning, mid-
dle, and end context dependent states to the central
phone identity. The ASR system is used to generate
a state based alignment of the development data to
the training transcripts. This results in a sequence
of state labels (classes) {st}, st ? S , one for each
frame of the input data. Note that the aligned states
are collapsed to the phone identity with M, so the
frame class labels are given by {ct}, ct ? P.
Corresponding to each frame, we also use the
state posteriors derived from the output of a Neu-
ral Network acoustic model and the prior probabil-
ities computed on the training set. Define Xt =
{. . . , xt, . . .} to be the sub-sequence of the input
speech frames centered around time index t. The
neural network takes Xt as input and produces
lt(sj) = y(sj|Xt)? l(sj), sj ? S
where y is the neural network output and l is the
prior probability, both in the log domain. Again, the
state labels are mapped using M, so the above pos-
terior is interpreted as that for the collapsed phone:
lt(sj) ? lt(M(sj)) = lt(pj), pj = M(sj).
The result of both analyses gives the following set of
associations:
c0 ? l0(p0), l0(p1), l0(p2), . . .
c1 ? l1(p0), l1(p1), l1(p2), . . .
.
.
ct ? lt(p0), lt(p1), lt(p2), . . .
Each log posterior li(pj) is converted into a count
ni,j = ceil[N ? eli(pj)],
where N is a large constant, i ranges over the
time index, and j ranges over the context dependent
states. From the counts, the confusion matrix entries
are computed. The total count for each state is
nj(k) =
?
i:ci=pj
ni,k,
279
where k is an index over the states.
?
????
n1(1) n1(2) . . .
n2(1) n2(2) . . .
.
.
?
????
The rows of the above matrix correspond to the ref-
erence and the columns to the observations. By nor-
malizing the rows, the entries can be interpreted as
?probability? of an observed phone (indicated by the
column) given the true phone.
5 Experiments and Results
The performance of a spoken term detection system
is measured using DET curves that plot the trade-off
between false alarms (FAs) and misses. This NIST
STD 2006 evaluation metric used Actual/Maximum
Term Weighted Value (ATWV/MTWV) that allows
one to weight FAs and Misses per the needs of the
task at hand (NIST, 2006).
Figure 2 illustrates the effect of speed on ATWV
on the NIST STD 2006 Dev06 data set using 1107
query terms. As the speed of indexing is increased to
many times faster than real time, the WER increases,
which in turn decreases the ATWV measure. It can
be seen that the use of word-fragments improves
the performance on OOV queries thus making the
combined search better than simple word search.
The primary advantage of using a hybrid decoding
scheme over a separate word and fragment based
decoding scheme is the speed of transforming the
audio into indexable units. The blue line in the fig-
ure illustrates that when using a hybrid setup, the
same performance can be achieved at speeds twice
as fast. For example, with the combined search
on two different decodes, an ATWV of 0.1 can be
achieved when indexing at a speed 15 times faster
than real time, but with a hybrid system, the same
performance can be reached at an indexing speed 30
times faster than real time. The ATWV on the hybrid
system also degrades gracefully with faster speeds
when compared to separate word and word-fragment
systems. Preliminary results indicate that fuzzy
search on one best output gives the same ATWV
performance as exact search (Figure 2) on consen-
sus output. Also, a closer look at the retrieval results
of OOV terms revealed that many more OOVs are
retrieved with the fuzzy search.
0 5 10 15 20 25 30 35
?0.4
?0.2
0
0.2
0.4
0.6
0.8
1
Real Time Factor
AT
W
V
 
 
exactWord
exactWordAndFrag
exactHybrid
Figure 2: Effect of WER on ATWV. Note that the cuves
for exactWord and exactWordAndFrag lie on top of each
other.
6 CONCLUSION
In this paper, we have presented the effect of rapid
decoding on a spoken term detection task. We
have demonstrated that hybrid systems perform well
and fuzzy search with phone confusion probabilities
help in OOV retrieval.
References
U. V. Chaudhari and M. Picheny. 2007. Improvements in
phone based audio search via constrained match with
high order confusion estimates. In Proc. of ASRU.
M. Clements, S. Robertson, and M. S. Miller. 2002.
Phonetic searching applied to on-line distance learning
modules. In Proc. of IEEE Digital Signal Processing
Workshop.
B. Kingsbury. 2009. Lattice-based optimization
of sequence classification criteria for neural-network
acoustic modeling. In Proc. of ICASSP.
J. Mamou, B. Ramabhadran, and O. Siohan. 2007. Vo-
cabulary independent spoken term detection. In Proc.
of ACM SIGIR.
NIST. 2006. The spoken term de-
tection (STD) 2006 evaluation plan.
http://www.nist.gov/speech/tests/std/docs/std06-
evalplan-v10.pdf.
M. Saraclar and R. Sproat. 2004. Lattice-based search
for spoken utterance retrieval. In Proc. HLT-NAACL.
F. Seide, P. Yu, C. Ma, and E. Chang. 2004. Vocabulary-
independent search in spontaneous speech. In Proc. of
ICASSP.
O. Siohan and M. Bacchiani. 2005. Fast vocabulary in-
dependent audio search using path based graph index-
ing. In Proc. of Interspeech.
280
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 190?197,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Unsupervised Model Adaptation using Information-Theoretic Criterion
Ariya Rastrow1, Frederick Jelinek1, Abhinav Sethy2 and Bhuvana Ramabhadran2
1Human Language Technology Center of Excellence, and
Center for Language and Speech Processing, Johns Hopkins University
{ariya, jelinek}@jhu.edu
2IBM T.J. Watson Research Center, Yorktown Heights, NY, USA
{asethy, bhuvana}@us.ibm.com
Abstract
In this paper we propose a novel general
framework for unsupervised model adapta-
tion. Our method is based on entropy which
has been used previously as a regularizer in
semi-supervised learning. This technique in-
cludes another term which measures the sta-
bility of posteriors w.r.t model parameters, in
addition to conditional entropy. The idea is to
use parameters which result in both low con-
ditional entropy and also stable decision rules.
As an application, we demonstrate how this
framework can be used for adjusting language
model interpolation weight for speech recog-
nition task to adapt from Broadcast news data
to MIT lecture data. We show how the new
technique can obtain comparable performance
to completely supervised estimation of inter-
polation parameters.
1 Introduction
All statistical and machine learning techniques for
classification, in principle, work under the assump-
tion that
1. A reasonable amount of training data is avail-
able.
2. Training data and test data are drawn from the
same underlying distribution.
In fact, the success of statistical models is cru-
cially dependent on training data. Unfortunately,
the latter assumption is not fulfilled in many appli-
cations. Therefore, model adaptation is necessary
when training data is not matched (not drawn from
same distribution) with test data. It is often the case
where we have plenty of labeled data for one specific
domain/genre (source domain) and little amount of
labeled data (or no labeled data at all) for the de-
sired domain/genre (target domain). Model adapta-
tion techniques are commonly used to address this
problem. Model adaptation starts with trained mod-
els (trained on source domain with rich amount of la-
beled data) and then modify them using the available
labeled data from target domain (or instead unla-
beled data). A survey on different methods of model
adaptation can be found in (Jiang, 2008).
Information regularization framework has been
previously proposed in literature to control the la-
bel conditional probabilities via input distribution
(Szummer and Jaakkola, 2003). The idea is that la-
bels should not change too much in dense regions
of the input distribution. The authors use the mu-
tual information between input features and labels as
a measure of label complexity. Another framework
previously suggested is to use label entropy (condi-
tional entropy) on unlabeled data as a regularizer to
Maximum Likelihood (ML) training on labeled data
(Grandvalet and Bengio, 2004).
Availability of resources for the target domain cat-
egorizes these techniques into either supervised or
unsupervised. In this paper we propose a general
framework for unsupervised adaptation using Shan-
non entropy and stability of entropy. The assump-
tion is that in-domain and out-of-domain distribu-
tions are not too different such that one can improve
the performance of initial models on in-domain data
by little adjustment of initial decision boundaries
(learned on out-of-domain data).
190
2 Conditional Entropy based Adaptation
In this section, conditional entropy and its relation
to classifier performance are first described. Next,
we introduce our proposed objective function for do-
main adaptation.
2.1 Conditional Entropy
Considering the classification problem whereX and
Y are the input features and the corresponding class
labels respectively, the conditional entropy is a mea-
sure of the class overlap and is calculated as follows
H(Y|X) = EX[H(Y|X = x)] =
?
?
p(x)
(
?
y
p(y|x) log p(y|x)
)
dx (1)
Through Fano?s Inequality theorem, one can see
how conditional entropy is related to classification
performance.
Theorem 1 (Fano?s Inequality) Suppose
Pe = P{Y? 6= Y} where Y? = g(X) are the
assigned labels for the data points, based on the
classification rule. Then
Pe ?
H(Y|X)? 1
log(|Y| ? 1)
where Y is the number of possible classes and
H(Y |X) is the conditional entropy with respect to
true distibution.
The proof to this theorem can be found in (Cover and
Thomas, 2006). This inequality indicates thatY can
be estimated with low probability of error only if the
conditional entropy H(Y|X) is small.
Although the above theorem is useful in a sense
that it connects the classification problem to Shan-
non entropy, the true distributions are almost never
known to us1. In most classification methods, a spe-
cific model structure for the distributions is assumed
and the task is to estimate the model parameters
within the assumed model space. Given the model
1In fact, Theorem 1 shows how relevant the input features
are for the classification task by putting a lower bound on the
best possible classifier performance. As the overlap between
features from different classes increases, conditional entropy in-
creases as well, thus lowering the performance of the best pos-
sible classifier.
structure and parameters, one can modify Fano?s In-
equality as follows,
Corollary 1
Pe(?) = P{Y? 6= Y |?} ? H?(Y|X)? 1log(|Y| ? 1)
(2)
where Pe(?) is the classifier probability of error
given model parameters, ? and
H?(Y|X) =
?
?
p(x)
(
?
y
p?(y|x) log p?(y|x)
)
dx
Here, H?(Y|X) is the conditional entropy imposed
by model parameters.
Eqn. 2 indicates the fact that models with low
conditional entropy are preferable. However, a low
entropy model does not necessarily have good per-
formance (this will be reviewed later on) 2
2.2 Objective Function
Minimization of conditional entropy as a framework
in the classification task is not a new concept and
has been tried by researchers. In fact, (Grandvalet
and Bengio, 2004) use this along with the maxi-
mum likelihood criterion in a semi-supervised set
up such that parameters with both maximum like-
lihood on labeled data and minimum conditional en-
tropy on unlabeled data are chosen. By minimiz-
ing the entropy, the method assumes a prior which
prefers minimal class overlap. Entropy minimiza-
tion is used in (Li et al, 2004) as an unsupervised
non-parametric clustering method and is shown to
result in significant improvement over k-mean, hier-
archical clustering and etc.
These methods are all based on the fact that mod-
els with low conditional entropy have their decision
boundaries passing through low-density regions of
the input distribution, P (X). This is consistent with
the assumption that classes are well separated so that
one can expect to take advantage of unlabeled exam-
ples (Grandvalet and Bengio, 2004).
In many cases shifting from one domain to an-
other domain, initial trained decision boundaries (on
2Imagine a model which classifies any input as class 1.
Clearly for this model H?(Y|X) = 0.
191
out-of-domain data) result in high conditional en-
tropy for the new domain, due to mismatch be-
tween distributions. Therefore, there is a need to
adjust model parameters such that decision bound-
aries goes through low-density regions of the distri-
bution. This motivates the idea of using minimum
conditional entropy criterion for adapting to a new
domain. At the same time, two domains are often
close enough that one would expect that the optimal
parameters for the new domain should not deviate
too much from initial parameters. In order to formu-
late the technique mentioned in the above paragraph,
let us define ?init to be the initial model parame-
ters estimated on out-of-domain data (using labeled
data). Assuming the availability of enough amount
of unlabeled data for in-domain task, we try to min-
imize the following objective function w.r.t the pa-
rameters,
?new = argmin
?
H?(Y|X) + ? ||? ? ?init||p
(3)
where ||? ? ?init||p is an Lp regularizer and tries to
prevent parameters from deviating too much from
their initial values3.
Once again the idea here is to adjust the param-
eters (using unlabeled data) such that low-density
separation between the classes is achieved. In the
following section we will discuss the drawback of
this objective function for adaptation in realistic sce-
narios.
3 Issues with Minimum Entropy Criterion
It is discussed in Section 2.2 that the model param-
eters are adapted such that a minimum conditional
entropy is achieved. It was also discussed how this is
related to finding decision boundaries through low-
density regions of input distribution. However, the
obvious assumption here is that the classes are well
separated and there in fact exists low-density regions
between classes which can be treated as boundaries.
Although this is a suitable/ideal assumption for clas-
sification, in most practical problems this assump-
tion is not satisfied and often classes overlap. There-
fore, we can not expect the conditional entropy to be
3The other reason for using a regularizer is to prevent trivial
solutions of minimum entropy criterion
convex in this situation and to achieve minimization
w.r.t parameters (other than the trivial solutions).
Let us clarify this through an example. Consider
X to be generated by mixture of two 2-D Gaus-
sians (each with a particular mean and covariance
matrix) where each Gaussian corresponds to a par-
ticular class ( binary class situation) . Also in order
to have linear decision boundaries, let the Gaussians
have same covariance matrix and let the parameter
being estimated be the prior for class 1, P (Y = 1).
Fig. 1 shows two different situations with over-
lapping classes and non-overlapping classes. The
left panel shows a distribution in which classes are
well separated whereas the right panel corresponds
to the situation where there is considerable overlap
between classes. Clearly, in the later case there is
no low-density region separating the classes. There-
fore, as we change the parameter (here, the prior on
the class Y = 1), there will not be any well defined
point with minimum entropy. This can be seen from
Fig. 2 where model conditional entropy is plotted
vs. class prior parameter for both cases. In the case
of no-overlap between classes, entropy is a convex
function w.r.t the parameter (excluding trivial solu-
tions which happens at P (Y = 1) = 0, 1) and is
minimum at P (Y = 1) = 0.7 which is the true prior
with which the data was generated.
We summarize issues with minimum entropy cri-
terion and our proposed solutions as follows:
? Trivial solution: this happens when we put de-
cision boundaries such that both classes are
considered as one class (this can be avoided us-
ing the regularizer in Eqn. 3 and the assump-
tion that initial models have a reasonable solu-
tion, e.g. close to the optimal solution for new
domain )
? Overlapped Classes: As it was discussed in
this section, if the overlap is considerable then
the entropy will not be convex w.r.t to model
parameters. We will address this issue in
the next section by introducing the entropy-
stability concept.
4 Entropy-Stability
It was discussed in the previous section that a mini-
mum entropy criterion can not be used (by itself) in
192
?3 ?2 ?1 0 1 2 3 4 5 6 7?4
?2
0
2
4
6
8
10
X1
X 2
?3 ?2 ?1 0 1 2 3 4 5 6 7?3
?2
?1
0
1
2
3
4
5
6
7
X1
X 2
Figure 1: Mixture of two Gaussians and the corresponding Bayes decision boundary: (left) with no class overlap
(right) with class overlap
0	 ?
0.05	 ?
0.1	 ?
0.15	 ?
0.2	 ?
0.25	 ?
0.3	 ?
0	 ?
0.005	 ?
0.01	 ?
0.015	 ?
0.02	 ?
0.025	 ?
0.03	 ?
0.035	 ?
0	 ? 0.1	 ? 0.2	 ? 0.3	 ? 0.4	 ? 0.5	 ? 0.6	 ? 0.7	 ? 0.8	 ? 0.9	 ? 1	 ?
Con
di?n
al	 ?E
ntro
py	 ?
P(Y=1)	 ?
without	 ?overlap	 ?
with	 ?overlap	 ?
Figure 2: Condtional entropy vs. prior parameter, P (Y =
1)
situations where there is a considerable amount of
overlap among classes. Assuming that class bound-
aries happen in the regions close to the tail of class
distributions, we introduce the concept of Entropy-
Stability and show how it can be used to detect
boundary regions. Define Entropy-Stability to be the
reciprocal of the following
?
?
?
?
?
?
?
?
?H?(Y|X)
??
?
?
?
?
?
?
?
?
p
=
?
?
?
?
?
?
?
?
?
?
?
?
?
p(x)
?
(?
y p?(y|x) log p?(y|x)
)
?? dx
?
?
?
?
?
?
?
?
?
?
?
?
p
(4)
Recall: since ? is a vector of parameters, ?H?(Y|X)??
will be a vector and by using Lp norm Entropy-
stability will be a scalar.
The introduced concept basically measures the
stability of label entropies w.r.t the model parame-
ters. The idea is that we prefer models which not
only have low-conditional entropy but also have sta-
ble decision rules imposed by the model. Next, we
show through the following theorem how Entropy-
Stability measures the stability over posterior prob-
abilities (decision rules) of the model.
Theorem 2
?
?
?
?
?
?
?
?
?H?(Y|X)
??
?
?
?
?
?
?
?
?
p
=
?
?
?
?
?
?
?
?
?
?
?
p(x)
(
?
y
?p?(y|x)
?? log p?(y|x)
)
dx
?
?
?
?
?
?
?
?
?
?
p
where the term inside the parenthesis is the weighted
sum (by log-likelihood) over the gradient of poste-
rior probabilities of labels for a given sample x
Proof The proof is straight forward and uses the fact
that
? ?p?(y|x)
?? = ?(
P
p?(y|x))
?? = 0 .
Using Theorem 2 and Eqn. 4, it should be clear
how Entropy-Stability measures the expected sta-
bility over the posterior probabilities of the model.
A high value of
?
?
?
?
?
?
?H?(Y|X)
??
?
?
?
?
?
?
p
implies models with
less stable decision rules. In order to explain how
this is used for detecting boundaries (overlapped
193
regions) we once again refer back to our mixture
of Gaussians? example. As the decision boundary
moves from class specific regions to overlapped re-
gions (by changing the parameter which is here class
prior probability) we expect the entropy to continu-
ously decrease (due to the assumption that the over-
laps occur at the tail of class distributions). How-
ever, as we get close to the overlapping regions the
added data points from other class(es) will resist
changes in the entropy. resulting in stability over the
entropy until we enter the regions specific to other
class(es).
In the following subsection we use this idea to
propose a new objective function which can be used
as an unsupervised adaptation method even for the
case of input distribution with overlapping classes.
4.1 Better Objective Function
The idea here is to use the Entropy-Stability con-
cept to accept only regions which are close to the
overlapped parts of the distribution (based on our
assumption, these are valid regions for decision
boundaries) and then using the minimum entropy
criterion we find optimum solutions for our parame-
ters inside these regions. Therefore, we modify Eqn.
3 such that it also includes the Entropy-Stability
term
?new = argmin
?
(
H?(Y|X) + ?
?
?
?
?
?
?
?
?
?H?(Y|X)
??
?
?
?
?
?
?
?
?
p?
+ ? ||? ? ?init||p
)
(5)
The parameter ? and ? can be tuned using small
amount of labeled data (Dev set).
5 Speech Recognition Task
In this section we will discuss how the proposed
framework can be used in a speech recognition task.
In the speech recognition task, Y is the sequence
of words and X is the input speech signal. For a
given speech signal, almost every word sequence is
a possible output and therefore there is a need for
a compact representation of output labels (words).
For this, word graphs (Lattices) are generated dur-
ing the recognition process. In fact, each lattice is
an acyclic directed graph whose nodes correspond
to particular instants of time, and arcs (edges con-
necting nodes) represent possible word hypotheses.
Associated with each arc is an acoustic likelihood
and language model likelihood scores. Fig. 3 shows
an example of recognition lattice 4 (for the purpose
of demonstration likelihood scores are not shown).L. Mangu et al: Finding Consensus in Speech Recognition 6
(a) Input lattice (?SIL? marks pauses)
SIL
SIL
SIL
SIL
SIL
SIL
VEAL
VERY
HAVE
MOVE
HAVE
HAVE
IT
MOVE
HAVE IT
VERY
VERY
VERY
VERY
OFTEN
OFTEN
FINE
FINE
FAST
I
I
I
(b) Multiple alignment (?-? marks deletions)
- -
I
MOVE
HAVE IT VEAL 
VERY
FINE
OFTEN
FAST
Figure 1: Sample recognition lattice and corresponding multiple alignment represented as
confusion network.
alignment (which gives rise to the standard string edit distance WE (W,R)) with
a modified, multiple string alignment. The new approach incorporates all lattice
hypotheses7 into a single alignment, and word error between any two hypotheses
is then computed according to that one alignment. The multiple alignment thus
defines a new string edit distance, which we will call MWE (W,R). While the
new alignment may in some cases overestimate the word error between two
hypotheses, as we will show in Section 5 it gives very similar results in practice.
The main benefit of the multiple alignment is that it allows us to extract
the hypothesis with the smallest expected (modified) word error very efficiently.
To see this, consider an example. Figure 1 shows a word lattice and the corre-
sponding hypothesis alignment. Each word hypothesis is mapped to a position
in the alignment (with deletions marked by ?-?). The alignment also supports
the computation of word posterior probabilities. The posterior probability of a
word hypothesis is the sum of the posterior probabilities of all lattice paths of
which the word is a part. Given an alignment and posterior probabilities, it is
easy to see that the hypothesis with the lowest expected word error is obtained
by picking the word with the highest posterior at each position in the alignment.
We call this the consensus hypothesis.
7In practice we apply some pruning of the lattice to remove low probability word hypotheses
(see Section 3.4).
Figure 3: Lattice Example
Since lattices contain all the likely hypotheses
(unlikely hypotheses are pruned during recognition
and will not be included in the lattice), conditional
entropy for any given input speech signal, x, can be
approximated by the conditional entropy of the lat-
tice. That is,
H?(Y|X = xi) = H?(Y|Li)
whereLi is the corresponding decoded lattice (given
speech recognizer parameters) of utterance xi.
For the calculation of entropy we need to
know the distribution of X because H?(Y|X) =
EX [H?(Y|X = x)] and since this distribution is not
known to us, we use Law of Large Numbers to ap-
proximate it by the empirical average
H?(Y|X) ? ? 1N
N?
i=1
?
y?Li
p?(y|Li) log p?(y|Li) (6)
Here N indicates the number of unlabeled utter-
ances for which we calculate the empirical value of
conditional entropy. Similarly, expectation w.r.t in-
put distribution in entropy-stability term is also ap-
proximated by the empirical average of samples.
Since the number of paths (hypotheses) in the lat-
tice is very large, it would be computationally infea-
sibl to c ute the conditi nal entropy y enumer-
ating all possible paths in the lattice and calculating
4The figure is adopted from (Mangu et al, 1999)
194
Element ?p, r?
?p1, r1?? ?p2, r2? ?p1p2, p1r2 + p2r1?
?p1, r1?? ?p2, r2? ?p1 + p2, r1 + r2?
0 ?0, 0?
1 ?1, 0?
Table 1: First-Order (Expectation) semiring: Defining
multiplication and sum operations for first-order semir-
ings.
their corresponding posterior probabilities. Instead
we use Finite-State Transducers (FST) to represent
the hypothesis space (lattice). To calculate entropy
and the gradient of entropy, the weights for the FST
are defined to be First- and Second-Order semirings
(Li and Eisner, 2009). The idea is to use semirings
and their corresponding operations along with the
forward-backward algorithm to calculate first- and
second-order statistics to compute entropy and the
gradient of entropy respectively. Assume we are in-
terested in calculating the entropy of the lattice,
H(p) = ??
d?Li
p(d)
Z log(
p(d)
Z )
= logZ ? 1Z
?
d?Li
p(d) log p(d)
= logZ ? r?Z (7)
where Z is the total probability of all the paths in
the lattice (normalization factor). In order to do so,
we need to compute ?Z, r?? on the lattice. It can
be proved that if we define the first-order semir-
ing ?pe, pe log pe? (pe is the non-normalized score of
each arc in the lattice) as our FST weights and define
semiring operations as in Table. 1, then applying the
forward algorithm will result in the calculation of
?Z, r?? as the weight (semiring weight) for the final
node.
The details for using Second-Order semirings for
calculating the gradient of entropy can be found
in (Li and Eisner, 2009). The same paper de-
scribes how to use the forward-backward algorithm
to speed-up the this procedure.
6 Language Model Adaptation
Language Model Adaptation is crucial when the
training data does not match the test data being de-
coded. This is a frequent scenario for all Automatic
Speech Recognition (ASR) systems. The applica-
tion domain very often contains named entities and
N-gram sequences that are unique to the domain of
interest. For example, conversational speech has
a very different structure than class-room lectures.
Linear Interpolation based methods are most com-
monly used to adapt LMs to a new domain. As
explained in (Bacchiani et al, 2003), linear inter-
polation is a special case of Maximum A Posterior
(MAP) estimation, where an N-gram LM is built on
the adaptation data from the new domain and the two
LMs are combined using:
p(wi|h) = ?pB(wi|h) + (1? ?)pA(wi|h)
0 ? ? ? 1
where pB refers to out-of-domain (background)
models and pA is the adaptation (in-domain) mod-
els. Here ? is the interpolation weight.
Conventionally, ? is calculated by optimizing per-
plexity (PPL) or Word Error Rate (WER) on some
held-out data from target domain. Instead using
our proposed framework, we estimate ? on enough
amount of unlabeled data from target domain. The
idea is that resources on the new domain have al-
ready been used to build domain specific models
and it does not make sense to again use in-domain
resources for estimating the interpolation weight.
Since we are trying to just estimate one parameter
and the performance of the interpolated model is
bound by in-domain/out-of-domain models, there is
no need to include a regularization term in Eqn. 5.
Also
?
?
?
?
?
?
?H?(Y|X)
??
?
?
?
?
?
?
p
= |?H?(Y|X)?? | because we only
have one parameter. Therefore, interpolation weight
will be chosen by the following criterion
?? = argmin
0???1
H?(Y|X) + ?|?H?(Y|X)?? | (8)
For the purpose of estimating one parameter ?, we
use ? = 1 in the above equation
7 Experimental Setup
The large vocabulary continuous speech recognition
(LVCSR) system used throughout this paper is based
on the 2007 IBM Speech transcription system for
GALE Distillation Go/No-go Evaluation (Chen et
al., 2006). The acoustic models used in this system
195
are state-of-the-art discriminatively trained models
and are the same ones used for all experiments pre-
sented in this paper.
For LM adaptation experiments, the out-of-
domain LM (pB , Broadcast News LM) training
text consists of 335M words from the follow-
ing broadcast news (BN) data sources (Chen et
al., 2006): 1996 CSR Hub4 Language Model
data, EARS BN03 closed captions, GALE Phase
2 Distillation GNG Evaluation Supplemental Mul-
tilingual data, Hub4 acoustic model training tran-
scripts, TDT4 closed captions, TDT4 newswire, and
GALE Broadcast Conversations and GALE Broad-
cast News. This language model is of order 4-gram
with Kneser-Ney smoothing and contains 4.6M n-
grams based on a lexicon size of 84K.
The second source of data is the MIT lectures data
set (J. Glass, T. Hazen, S. Cyphers, I. Malioutov, D.
Huynh, and R. Barzilay, 2007) . This serves as the
target domain (in-domain) set for language model
adaptation experiments. This set is split into 8 hours
for in-domain LM building, another 8 hours served
as unlabeled data for interpolation weight estimation
using criterion in Eqn. 8 (we refer to this as unsuper-
vised training data) and finally 2.5 hours Dev set for
estimating the interpolation weight w.r.t WER (su-
pervised tuning) . The lattice entropy and gradient
of entropy w.r.t ? are calculated on the unsupervised
training data set. The results are discussed in the
next section.
8 Results
In order to optimize the interpolation weight ? based
on criterion in Eqn. 8, we devide [0, 1] to 20 differ-
ent points and evaluate the objective function (Eqn.
8) on those points. For this, we need to calculate
entropy and gradient of the entropy on the decoded
lattices of the ASR system on 8 hours of MIT lecture
set which is used as an unlabeled training data. Fig.
4 shows the value of the objective function against
different values of model parameters (interpolation
weight ?). As it can be seen from this figure just
considering the conditional entropy will result in a
non-convex objective function whereas adding the
entropy-stability term will make the objective func-
tion convex. For the purpose of the evaluation, we
show the results for estimating ? directly on the tran-
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Model Entropy
Model Entropy+Entropy-Stability
BN-LM MIT-LM?
Figure 4: Objective function with and without including
Entropy-Stability term vs. interpolation weight ? on 8
hours MIT lecture unlabeled data
scription of the 8 hour MIT lecture data and compare
it to estimated value using our framework. The re-
sults are shown in Fig. 5. Using ? = 0 and ? = 1
the WERs are 24.7% and 21.1% respectively. Us-
ing the new proposed objective function, the optimal
? is estimated to be 0.6 with WER of 20.1% (Red
circle on the figure). Estimating ? w.r.t 8 hour train-
ing data transcription (supervised adaptation) will
result in ? = 0.7 (green circle) andWER of 20.0%.
Instead ? = 0.8 will be chosen by tuning the inter-
polation weight on 2.5 hour Dev set with compara-
ble WER of 20.1%. Also it is clear from the figure
that the new objective function can be used to pre-
dict the WER trend w.r.t the interpolation weight
parameter.
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Model Entropy + Entropy Stability
WER
24.7%
20.0%
21.1%
supervised tuning
?
Figure 5: Estimating ? based on WER vs. the
information-theoretic criterion
Therefore, it can be seen that the new unsuper-
196
vised method results in the same performance as su-
pervised adaptation in speech recognition task.
9 Conclusion and Future Work
In this paper we introduced the notion of entropy
stability and presented a new criterion for unsu-
pervised adaptation which combines conditional en-
tropy minimization with entropy stability. The en-
tropy stability criterion helps in selecting parameter
settings which correspond to stable decision bound-
aries. Entropy minimization on the other hand tends
to push decision boundaries into sparse regions of
the input distributions. We show that combining
the two criterion helps to improve unsupervised pa-
rameter adaptation in real world scenario where
class conditional distributions show significant over-
lap. Although conditional entropy has been previ-
ously proposed as a regularizer, to our knowledge,
the gradient of entropy (entropy-stability) has not
been used previously in the literature. We presented
experimental results where the proposed criterion
clearly outperforms entropy minimization. For the
speech recognition task presented in this paper, the
proposed unsupervised scheme results in the same
performance as the supervised technique.
As a future work, we plan to use the proposed
criterion for adapting log-linear models used in
Machine Translation, Conditional Random Fields
(CRF) and other applications. We also plan to ex-
pand linear interpolation Language Model scheme
to include history specific (context dependent)
weights.
Acknowledgments
The Authors want to thank Markus Dreyer and
Zhifei Li for their insightful discussions and sugges-
tions.
References
M. Bacchiani, B. Roark, and M. Saraclar. 2003. Un-
supervised language model adaptation. In Proc.
ICASSP, pages 224?227.
S. Chen, B. Kingsbury, L. Mangu, D. Povey, G. Saon,
H. Soltau, and G. Zweig. 2006. Advances in speech
transcription at IBM under the DARPA EARS pro-
gram. IEEE Transactions on Audio, Speech and Lan-
guage Processing, pages 1596?1608.
Thomas M. Cover and Joy A. Thomas. 2006. Elements
of information theory. Wiley-Interscience, 3rd edition.
Yves Grandvalet and Yoshua Bengio. 2004. Semi-
supervised learning by entropy minimization. In
Advances in neural information processing systems
(NIPS), volume 17, pages 529?536.
J. Glass, T. Hazen, S. Cyphers, I. Malioutov, D. Huynh,
and R. Barzilay. 2007. Recent progress in MIT spo-
ken lecture processing project. In Proc. Interspeech.
Jing Jiang. 2008. A literature survey on domain adapta-
tion of statistical classifiers, March.
Zhifei Li and Jason Eisner. 2009. First- and second-order
expectation semirings with applications to minimum-
risk training on translation forests. In EMNLP.
Haifeng Li, Keshu Zhang, and Tao Jiang. 2004. Min-
imum entropy clustering and applications to gene ex-
pression analysis. In Proceedings of IEEE Computa-
tional Systems Bioinformatics Conference, pages 142?
151.
Lidia Mangu, Eric Brill, and Andreas Stolcke. 1999.
Finding consensus among words: Lattice-based word
error minimization. In Sixth European Conference on
Speech Communication and Technology.
M. Szummer and T. Jaakkola. 2003. Information regu-
larization with partially labeled data. In Advances in
Neural Information Processing Systems, pages 1049?
1056.
197
NAACL-HLT 2012 Workshop: Will We Ever Really Replace the N-gram Model? On the Future of Language Modeling for HLT, pages 20?28,
Montre?al, Canada, June 8, 2012. c?2012 Association for Computational Linguistics
Deep Neural Network Language Models
Ebru Ar?soy, Tara N. Sainath, Brian Kingsbury, Bhuvana Ramabhadran
IBM T.J. Watson Research Center
Yorktown Heights, NY, 10598, USA
{earisoy, tsainath, bedk, bhuvana}@us.ibm.com
Abstract
In recent years, neural network language mod-
els (NNLMs) have shown success in both
peplexity and word error rate (WER) com-
pared to conventional n-gram language mod-
els. Most NNLMs are trained with one hid-
den layer. Deep neural networks (DNNs) with
more hidden layers have been shown to cap-
ture higher-level discriminative information
about input features, and thus produce better
networks. Motivated by the success of DNNs
in acoustic modeling, we explore deep neural
network language models (DNN LMs) in this
paper. Results on a Wall Street Journal (WSJ)
task demonstrate that DNN LMs offer im-
provements over a single hidden layer NNLM.
Furthermore, our preliminary results are com-
petitive with a model M language model, con-
sidered to be one of the current state-of-the-art
techniques for language modeling.
1 Introduction
Statistical language models are used in many natural
language technologies, including automatic speech
recognition (ASR), machine translation, handwrit-
ing recognition, and spelling correction, as a crucial
component for improving system performance. A
statistical language model represents a probability
distribution over all possible word strings in a lan-
guage. In state-of-the-art ASR systems, n-grams are
the conventional language modeling approach due
to their simplicity and good modeling performance.
One of the problems in n-gram language modeling
is data sparseness. Even with large training cor-
pora, extremely small or zero probabilities can be
assigned to many valid word sequences. Therefore,
smoothing techniques (Chen and Goodman, 1999)
are applied to n-grams to reallocate probability mass
from observed n-grams to unobserved n-grams, pro-
ducing better estimates for unseen data.
Even with smoothing, the discrete nature of n-
gram language models make generalization a chal-
lenge. What is lacking is a notion of word sim-
ilarity, because words are treated as discrete enti-
ties. In contrast, the neural network language model
(NNLM) (Bengio et al, 2003; Schwenk, 2007) em-
beds words in a continuous space in which proba-
bility estimation is performed using single hidden
layer neural networks (feed-forward or recurrent).
The expectation is that, with proper training of the
word embedding, words that are semantically or gra-
matically related will be mapped to similar loca-
tions in the continuous space. Because the prob-
ability estimates are smooth functions of the con-
tinuous word representations, a small change in the
features results in a small change in the probabil-
ity estimation. Therefore, the NNLM can achieve
better generalization for unseen n-grams. Feed-
forward NNLMs (Bengio et al, 2003; Schwenk
and Gauvain, 2005; Schwenk, 2007) and recur-
rent NNLMs (Mikolov et al, 2010; Mikolov et al,
2011b) have been shown to yield both perplexity and
word error rate (WER) improvements compared to
conventional n-gram language models. An alternate
method of embedding words in a continuous space
is through tied mixture language models (Sarikaya
et al, 2009), where n-grams frequencies are mod-
eled similar to acoustic features.
To date, NNLMs have been trained with one hid-
20
den layer. A deep neural network (DNN) with mul-
tiple hidden layers can learn more higher-level, ab-
stract representations of the input. For example,
when using neural networks to process a raw pixel
representation of an image, lower layers might de-
tect different edges, middle layers detect more com-
plex but local shapes, and higher layers identify ab-
stract categories associated with sub-objects and ob-
jects which are parts of the image (Bengio, 2007).
Recently, with the improvement of computational
resources (i.e. GPUs, mutli-core CPUs) and better
training strategies (Hinton et al, 2006), DNNs have
demonstrated improved performance compared to
shallower networks across a variety of pattern recog-
nition tasks in machine learning (Bengio, 2007;
Dahl et al, 2010).
In the acoustic modeling community, DNNs
have proven to be competitive with the well-
established Gaussian mixture model (GMM) acous-
tic model. (Mohamed et al, 2009; Seide et al, 2011;
Sainath et al, 2012). The depth of the network (the
number of layers of nonlinearities that are composed
to make the model) and the modeling a large number
of context-dependent states (Seide et al, 2011) are
crucial ingredients in making neural networks com-
petitive with GMMs.
The success of DNNs in acoustic modeling leads
us to explore DNNs for language modeling. In this
paper we follow the feed-forward NNLM architec-
ture given in (Bengio et al, 2003) and make the neu-
ral network deeper by adding additional hidden lay-
ers. We call such models deep neural network lan-
guage models (DNN LMs). Our preliminary experi-
ments suggest that deeper architectures have the po-
tential to improve over single hidden layer NNLMs.
This paper is organized as follows: The next sec-
tion explains the architecture of the feed-forward
NNLM. Section 3 explains the details of the baseline
acoustic and language models and the set-up used
for training DNN LMs. Our preliminary results are
given in Section 4. Section 5 summarizes the related
work to our paper. Finally, Section 6 concludes the
paper.
2 Neural Network Language Models
This section describes a general framework for feed-
forward NNLMs. We will follow the notations given
























  
 
	

	

	

	
