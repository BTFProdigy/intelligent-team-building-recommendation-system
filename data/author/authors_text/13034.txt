Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 80?88,
Beijing, August 2010
Self-Annotation for Fine-Grained Geospatial Relation Extraction
Andre Blessing Hinrich Schu?tze
Institute for Natural Language Processing
Universita?t Stuttgart
ner@ifnlp.org
Abstract
A great deal of information on the Web is
represented in both textual and structured
form. The structured form is machine-
readable and can be used to augment the
textual data. We call this augmentation
? the annotation of texts with relations
that are included in the structured data ?
self-annotation. In this paper, we intro-
duce self-annotation as a new supervised
learning approach for developing and im-
plementing a system that extracts fine-
grained relations between entities. The
main benefit of self-annotation is that it
does not require manual labeling. The in-
put of the learned model is a represen-
tation of the free text, its output struc-
tured relations. Thus, the model, once
learned, can be applied to any arbitrary
free text. We describe the challenges for
the self-annotation process and give re-
sults for a sample relation extraction sys-
tem. To deal with the challenge of fine-
grained relations, we implement and eval-
uate both shallow and deep linguistic anal-
ysis, focusing on German.
1 Introduction
In the last years, information extraction has be-
come more important in domains like context-
aware systems (e.g. Nexus (Du?rr et al, 2004)) that
need a rich knowledge base to make the right de-
cisions in different user contexts. Geospatial data
are one of the key features in such systems and
need to be represented on different levels of de-
tail. Data providers do not cover all these lev-
els completely. To overcome this problem, fine-
grained information extraction (IE) methods can
be used to acquire the missing knowledge. We
define fine-grained IE as methods that recognize
entities at a finer grain than standard categories
like person, location, and organization. Further-
more, the quality of the data in context-aware sys-
tems plays an important role and updates by an in-
formation extraction component can increase the
overall user acceptance.
For both issues an information extraction sys-
tem is required that can handle fine-grained rela-
tions, e.g., ?X is a suburb of Y? or ?the river X
is a tributary of Y? ? as opposed to simple con-
tainment. The World Wide Web offers a wealth of
information about geospatial data and can be used
as source for the extraction task. The extraction
component can be seen as a kind of sensor that we
call text senor (Blessing et al, 2006).
In this paper, we address the problem of de-
veloping a flexible system for the acquisition of
relations between entities that meets the above
desiderata. We concentrate on geospatial entities
on a fine-grained level although the approach is
in principle applicable to any domain. We use
a supervised machine learning approach, includ-
ing several features on different linguistic lev-
els, to build our system. Such a system highly
depends on the quality and amount of labeled
data in the training phase. The main contri-
bution of this paper is the introduction of self-
annotation, a novel approach that allows us to
eliminate manual labeling (although training set
creation also involves costs other than labeling).
Self-annotation is based on the fact that Word
Wide Web sites like Wikipedia include, in addi-
80
tion to unstructured text, structured data. We use
structured data sources to automatically annotate
unstructured texts. In this paper, we use German
Wikipedia data because it is a good source for the
information required for our context-aware sys-
tem and show that a system created without man-
ual labeling has good performance.
Our trained model only uses text, not the struc-
tured data (or any other markup) of the input doc-
uments. This means that we can train an informa-
tion extractor on Wikipedia and then apply it to
any text, regardless of whether this text also con-
tains structured information.
In the first part of this paper, we discuss
the challenges of self-annotation including some
heuristics which can easily be adapted to different
relation types. We then describe the architecture
of the extraction system. The components we de-
velop are based on the UIMA (Unstructured In-
formation Management Architecture) framework
(Hahn et al, 2008) and include two linguistic en-
gines (OpenNLP1, FSPar). The extraction task is
performed by a supervised classifier; this classi-
fier is also implemented as a UIMA component
and uses the ClearTK framework. We evaluate our
approach on two types of fine-grained relations.
2 Related work
Jiang (2009) also addresses the issue of super-
vised relation extraction when no large manually
labeled data set is available. They use only a few
seed instances of the target relation type to train
a supervised relation extraction system. However,
they use multi-task transfer learning including a
large amount of labeled instances of other relation
types for training their system. In contrast, our
work eliminates manual labeling by using struc-
tured data to annotate the relations.
Wu and Weld (2007) extract facts from in-
foboxes and link them with their corresponding
representation in the text. They discuss several is-
sues that occur when using infoboxes as a knowl-
edge base, in particular, (i) the fact that infoboxes
are incomplete; and (ii) schema drift. Schema
drift occurs when authors over time use differ-
ent attribute names to model facts or the same
1http://opennlp.sourceforge.net/
attributes are used to model different facts. So
the semantics of the infoboxes changes slightly
and introduces noise into the structured informa-
tion. Their work differs from self-annotation in
that they are not interested in the creation of self-
annotated corpora that can be used as training data
for other tasks. Their goal is to develop methods
that make infoboxes more consistent.
Zhang and Iria (2009) use a novel entity extrac-
tion method to automatically generate gazetteers
from seed lists using Wikipedia as knowledge
source. In contrast to our work they need struc-
tured data for the extraction while our system fo-
cuses on the extraction of information from un-
structured text. Methods that are applicable to
any unstructured text (not just the text in the
Wikipedia) are needed to increase coverage be-
yond the limited number of instances covered in
Wikipedia.
Nothman et al (2009) also annotate
Wikipedia?s unstructured text using struc-
tured data. The type of structured data they use is
hyperlinking (as opposed to infoboxes) and they
use it to derive a labeled named entity corpus.
They show that the quality of the annotation is
comparable to other manually labeled named
entity recognition gold standards. We interpret
their results as evidence that self-annotation can
be used to create high quality gold standards.
3 Task definition
In this section, we describe the annotation task;
give a definition of the relation types covered in
this paper; and introduce the extraction model.
We focus on binary relations between two re-
lation arguments occurring in the same sentence.
To simplify the self-annotation process we restrict
the first argument of the relation to the main en-
tity of the Wikipedia article. As we are building
text sensors for a context aware system, relations
between geospatial entities are of interest. Thus
we consider only relations that use a geospatial
named entity as second argument.
We create the training set by automatically
identifying all correct binary relations in the text.
To this end, we extract the relations from the
structured part of the Wikipedia, the infoboxes.
Then we automatically find the corresponding
81
sentences in the text and annotate the relations
(see section 4). All other not yet marked binary
relations between the main entity and geospatial
entities are annotated as negative samples. The
result of this step is a self-annotated training set.
In the second step of our task, the self-
annotated training set is used to train the extrac-
tion model. The model only takes textual features
as input and can be applied to any free text.
3.1 Classification task and relations used
Our relation extraction task is modeled as a classi-
fication task which considers a pair of named en-
tities and decides whether they occur in the re-
quested relation or not. The classifier uses ex-
tracted features for this decision. Features be-
long to three different classes. The first class con-
tains token-based features and their linguistic la-
bels like part-of-speech, lemma, stem. In the sec-
ond class, we have chunks that aggregate one or
more tokens into complex units. Dependency re-
lations between the tokens are represented in the
third class.
Our classifier is applicable to a wide spectrum
of geospatial relation types. For the purposes of
a focused evaluation, we selected two relations.
The first type contains rivers and the bodies of
water into which they flow. We call it river-
bodyOfWater relation. Our second type is com-
posed of relations between towns and the corre-
sponding suburb. We call this town-suburb rela-
tion.
3.2 Wikipedia as resource
Wikipedia satisfies all corpus requirements for our
task. It contains a lot of knowledge about geospa-
tial data with unstructured (textual) and structured
information. We consider only German Wikipedia
articles because our target application is a German
context aware system. In relation extraction for
German, we arguably face more challenges ? e.g.,
more complex morphology and freer word order ?
than we would in English.
For this work we consider only a subset of the
German Wikipedia. We use all articles that belong
to the following categories: Rivers by country,
Mountains by country, Valleys by country, Islands
by country, Mountain passes by country, Forests
by country and Settlements by country.
For the annotation task we use the structural
content of Wikipedia articles. Most articles be-
longing to the same categories use similar tem-
plates to represent structured information. One
type of template is the infobox, which con-
tains pairs of attributes and their values. These
attribute-value pairs specify a wide range of
geospatial relation types including fine-grained
relations. In this work we consider only the in-
fobox data and the article names from the struc-
tured data.
For context-aware systems fine-grained relation
types are particularly relevant. Such relations are
not represented in resources like DBPedia (Auer
et al, 2007) or Yago (Suchanek et al, 2007) al-
though they also consist of infobox data. Hence,
we have to build our own extraction component
(see section 5.2) when using infoboxes.
4 Self-Annotation
Self-annotation is a two-fold task. First, the struc-
tured data, in our case the infoboxes of Wikipedia
articles, must be analyzed to get al relevant
attribute-value pairs. Then all relevant geospatial
entities are marked and extracted. In a second step
these entities must be matched with the unstruc-
tured data.
In most cases, the extraction of the named en-
tities that correspond to the required relations is
trivial because the values in the infoboxes con-
sist only of one single entity or one single link.
But in some cases the values contain mixed con-
tent which can include links, entities and even
free text. In order to find an accurate extraction
method for those values we have developed sev-
eral heuristics. See section 5.2 for discussion.
The second task links the extracted structured
data to tokens in the textual data. Pattern based
string matching methods are not sufficient to iden-
tify all relations in the text. In many cases, mor-
phological rules need to be applied to identify
the entities in the text. In other cases, the pre-
processed text must be retokenized because the
borders of multi-word expressions are not consis-
tent with the extracted names in step one. One
other issue is that some named entities are a subset
of other named entities (Lonau vs. kleine Lonau;
82
Figure 1: Infobox of the German Wikipedia article
about Gollach.
similar to York vs. New York). We have to use a
longest match strategy to avoid such overlapping
annotations.
The main goal of the self-annotation task is
to reach the highest possible annotation quality.
Thus, only complete extracted relations are used
for the annotation process while incomplete data
are excluded from the training set. This procedure
reduces the noise in the labeled data.
4.1 Example
We use the river-bodyOfWater relation between
the two rivers Gollach and Tauber to describe the
self-annotation steps.
Figure 1 depicts a part of the infobox for the
German Wikipedia article about the river Gollach.
For this relation the attribute Mu?ndung ?mouth? is
relevant. The value contains unstructured infor-
mation (i.e., text, e.g. bei ?at? Bieberehren) and
structured information (the link from Bieberehren
to its Wikipedia page). The relation we want to
extract is that the river Gollach flows into the river
Tauber.
Bieberehrensie
sie
Tauber
Gollach
Gollach Tauber
Sie
Gollach
Tauber
Figure 2: Textual content of the German
Wikipedia article about Gollach. All named enti-
ties which are relevant for the river-bodyOfWater
relation are highlighted. This article contains two
instances for the relation between Gollach and
Tauber.
Figure 2 shows the textual content of the Gol-
lach article. We have highlighted all relevant
named entities for the self-annotation process.
This includes the name of the article and instances
of the pronoun sie referring to Gollach. Our
matching algorithm identifies two sentences as
positive samples for the relation between Gollach
and Tauber:
? (i) Die Gollach ist ein rechter Nebenfluss der
Tauber in Mittel- und Unterfranken. (The
Gollach is a right tributary of the Tauber in
Middle and Lower Franconia.)
? (ii) Schlie?lich mu?ndet sie in Bieberehren
auf 244 m in die Tauber. (Finally, it dis-
charges in Bieberehren at 244 m above MSL
into the Tauber.)
5 Processing
In this section we describe how the self-annotation
method and relation extraction is implemented.
First we introduce the interaction with the
Wikipedia resource to acquire the structured
and unstructured information for the processing
83
pipeline. Second we present the components of
the UIMA pipeline which are used for the relation
extraction task.
5.1 Wikipedia interaction
We use the JWPL API (Zesch et al, 2008) to
pre-process the Wikipedia data. This interface
provides functions to extract structured and un-
structured information from Wikipedia. How-
ever, many Wikipedia articles do not adhere to
valid Wikipedia syntax (missing closing brack-
ets etc.). The API also does not correctly handle
all Wikipedia syntax constructions. We therefore
have enhanced the API for our extraction task to
get high quality data for German Wikipedia arti-
cles.
5.2 Infobox extraction
As discussed in section 4 infoboxes are the key
resource for the self-annotation step. However
the processing of infoboxes that include attribute-
value pairs with mixed content is not trivial.
For each new relation type an initial manual ef-
fort is required. However, in comparison to the
complete annotation of a training corpus, this ef-
fort is small. First the attributes used in the in-
foboxes of the Wikipedia articles relevant for a
specific relation have to be analyzed. The results
of this analysis simplify the choice of the cor-
rect attributes. Next, the used values of these at-
tributes must be investigated. If they contain only
single entries (links or named entities) the extrac-
tion is trivial. However, if they consist of mixed
content (see section 4.1) then specific extraction
methods have to be applied. We investigated dif-
ferent heuristics for the self-annotation process to
get a method that can easily be adapted to new re-
lation types.
Our first heuristic includes a set of rules spec-
ifying the extraction of the values from the in-
foboxes. This heuristic gives an insufficient basis
for the self-annotation task because the rich mor-
phology and free word order in German can not
be modeled with simple rules. Moreover, hand-
crafted rules are arguably not as robust and main-
tainable as a statistical classifier trained on self-
annotated training material.
Our second heuristic is a three step process. In
step one we collect all links in the mixed con-
tent and replace them by a placeholder. In the
second step we tag the remaining content with
the OpenNLP tokenizer to get al named entities.
Both collected lists are then looked up in a lexicon
that contains named entities and the correspond-
ing geospatial classes. This process requires a nor-
malization procedure that includes the application
of morphological methods. The second method
can be easily adapted to new relation types.
5.3 UIMA
The self-annotated corpora are processed by sev-
eral components of the UIMA (Mu?ller et al,
2008) pipeline. The advantage of exchangeable
collection readers is that they seamlessly handle
structured and unstructured data. Another advan-
tage of using UIMA is the possibility to share
components with other research groups. We can
easily exchange different components, like the us-
age of the commonly known OpenNLP process-
ing tools or the FSPar NLP engine (Schiehlen,
2003) (which includes the TreeTagger (Schmid,
1995)). This allows us to experiment with dif-
ferent approaches, e.g., shallow vs. deep analy-
sis. The components we use provide linguistic
analysis on different levels: tokens, morphology,
part of speech (POS), chunking and partial depen-
dency analysis. Figure 4 shows the results after
the linguistic processing of our sample sentence.
For this work only a few annotations are wrapped
as UIMA types: token (incl. lemma, POS), multi-
word, sentence, NP, PP and dependency relations
(labeled edges between tokens). We will intro-
duce our machine learning component in section
5.5. Finally, the CAS consumers allow us to store
extracted facts in a context model.
Figure 3 shows the article about Gollach after
linguistic processing. In the legend all annotated
categories are listed. We highlighted all marked
relations, all references to the article name (re-
ferred to as subject in the figure) and links. After
selection of the Tauber relation, all annotations for
this token are listed in the right panel.
5.4 Coreference resolution
Using anaphora to refer to the main entity is a
common practice of the authors of Wikipedia ar-
84
Figure 3: Screenshot of the UIMA Annotation-
Viewer.
ticles. Coreference resolution is therefore neces-
sary for our annotation task. A shallow linguis-
tic analysis showed that the writing style is simi-
lar throughout Wikipedia articles. Based on this
observation, we empirically investigated some
geospatial articles and came to the conclusion that
a simple heuristic is sufficient for our coreference
resolution problem. In almost all articles, pro-
nouns refer to the main entity of the article. In
addition we include some additional rules to be
able to establish coreference of markables such as
der Fluss ?the river? or der Bach ?the creek? with
the main entity.
5.5 Supervised relation extraction
We use the ClearTK (Ogren et al, 2008) toolkit,
which is also an UIMA component, for the rela-
tion extraction task. It contains wrappers for dif-
ferent machine learning suites. Our initial exper-
iments showed that the MaximumEntropy clas-
sifier achieved the best results for our classifi-
cation task. The toolkit provides additional ex-
tensible feature methods. Because we view self-
annotation and fine-grained named entity recogni-
tion as our main contributions, not feature selec-
tion, we only give a brief overview of the features
we use.
F1 is a window based bag-of-words feature
(window size = 3). It considers lemma and part-
of-speech tag of the tokens. F2 is a phrase based
extractor that uses the parent phrase of both enti-
ties (max 2 levels). F3 is a representation of all
sie
sheSchlie?lichFinally
auf
on
Meter
meter
244
inin
Bieberehren
in
Tauber
die
the
1
2
3
4
1 3 1 2 3 41 2 2
TOP
m?ndenflow
SUBJADV
Figure 4: Dependency parser output of the FSPar
framework.
linguistic effort description
F1 pos-tagging window size 3, LEMMA
F2 chunk-parse parent chunks
F3 dependency-parse dependency paths betw. NEs
Table 1: List of feature types
possible dependency paths between the article?s
main entity and a target entity, where each path
is represented as a feature vector. In most cases,
more than one path is returned by the partial de-
pendency parser (which makes no disambiguation
decisions) and included in the feature representa-
tion. Figure 4 depicts the dependency parser out-
put of our sample sentence. Each pair of square
and circle with the same number corresponds to
one dependency. These different possible depen-
dency combinations give rise to 8 possible paths
between the relation entities Tauber and sie ?she?
although our example sentence is a very simple
sentence.
6 Evaluation
We evaluate the system in two experiments. The
first considers the relation between suburbs and
their parent towns. In the second experiment the
river-bodyOfWater relation is extracted. The ex-
periments are based on the previously described
extracted Wikipedia corpus. For each experiment
a new self-annotated corpus is created that is split
into three parts. The first part (60%) is used as
training corpus. The second part (20%) is used
as development corpus. The remaining 20% is
used for the final evaluation and was not inspected
while we were developing the extraction algo-
rithms.
85
6.1 Metric used
Our gold standard includes all relations of each
article. Our metric works on the level of type
and is independent of how often the same relation
occurs in the article. The metric counts a rela-
tion as true positive (TP) if the system extracted
it at least once. If the relation was not found by
the system a false negative (FN) is counted. A
false positive (FP) is given if the system extracts
a relation between two entities that is not part of
the (infobox-derived) gold standard for the article.
All three measures are used to calculate precision
(P = TPTP+FP ), recall (R = TPTP+FN ), and F1-
score (F1 = 2 P?RP+R ).
6.2 Town-suburb extraction
The town-suburb extractor uses one attribute of
the infobox to identify the town-suburb relation.
There is no schema drift in the infobox data and
the values contain only links. Therefore the self-
annotation works almost perfectly. The only ex-
ceptions are articles without an infobox which
cannot be used for training. However, this is not a
real issue because the amount of remaining data is
sufficient: 9000 articles can be used for this task.
The results in table 2 show that the classifier that
uses F1, F2 and F3 (that is, including the depen-
dency features) performs best.
engine features F1 recall precision
FSPar F1 64.9 79.0% 55.7%
FSPar F1, F2 89.6 90.2% 89.5%
FSPar F1, F2, F3 98.3 98.8% 97.8%
Table 2: Results of different feature combinations
on the test set for town-suburb relation
6.3 River-bodyOfWater extraction
For the extraction of the river-bodyOfWater re-
lation the infobox processing is more difficult.
We have to handle more attributes because there
is schema drift between the different users. It
is hence necessary to merge information coming
from different attribute values. The other diffi-
culty is the usage of mixed contents in the values.
Another main difference to the town-suburb rela-
tion is that the river-bodyOfWater relation is often
not mentioned in the first sentence (which usually
gives a short definition about the the main entity).
Thus, the self-annotation method has to deal with
the more complex sentences that are common later
in the article. This also contributes to a more chal-
lenging extraction task.
Our river-bodyOfWater relation corpus consists
of 3000 self-annotated articles.
Table 3 shows the performance of the extrac-
tor using two different linguistic components as
described in section 5.3. As in the case of town-
suburb extraction the classifier that uses all fea-
tures, including dependency features, performs
best.
engine features F1 recall precision
FSPar F1 51.8% 56.6% 47.8%
FSPar F1,F2 72.1% 68.9% 75.7%
FSPar F1,F2,F3 78.3% 74.1% 83.0%
OpenNLP F1 48.0% 62.8% 38.8%
OpenNLP F1,F2 73.3% 71.7% 74.7%
Table 3: Results of different feature combinations
on the test set for river-bodyOfWater extraction
6.4 Evaluation of self-annotation
To evaluate the quality of self-annotation, we ran-
domly selected one set of 100 self-annotated ar-
ticles from each data set and labeled these sets
manually. These annotations are used to calcu-
late the inter-annotator agreement between the hu-
man annotated and machine annotated instances.
We use Cohen?s ? as measure and get a result of
1.00 for the town-suburb relation. For the river-
bodyOfWater relation we got a ?-value of 0.79,
which also indicates good agreement.
We also use a gazetteer to evaluate the qual-
ity of all town-suburb relations that were extracted
for our self-annotated training set. The accuracy
is nearly perfect (only one single error), which is
good evidence for the high quality of Wikipedia.
Required size of self-annotated training set.
The performance of a supervised system depends
on the size of the training data. In the self-
annotation step a minimum of instances has to be
annotated, but it is not necessary to self-annotate
all available articles.
We reduced the number of articles used in
the training size to test this hypothesis. Reduc-
ing the entire training set of 9000 (respectively,
3000) self-annotated articles to 1000 reduces F1
86
by 2.0% for town-suburb and by 2.4% for river-
bodyOfWater; a reduction to 100 reduces F1 by
8.5% for town-suburb and by 9.3% for river-
bodyOfWater (compared to the 9000/3000 base-
line).
7 Discussion
Wu and Weld (2007) observed schema drift in
their work: Wikipedia authors do not not use in-
fobox attributes in a consistent manner. However,
we did not find schema drift to be a large prob-
lem in our experiments. The variation we found
can easily be handled with a small number of
rules. This can be due to the fact that the qual-
ity of Wikipedia articles improved a lot in the last
years through the introduction of automatic main-
tenance tools like bots2. Nevertheless, the devel-
opment of self-annotation for a new relation type
requires some manual work. The developer has to
check the quality of the extraction relations in the
infoboxes. This can lead to some additional adap-
tation work for the used attributes such as merging
or creating rules. However, a perfect coverage is
not required because the extraction system is only
used for training purposes; we only need to find
a sufficiently large number of positive training in-
stances and do not require exhaustive labeling of
all articles.
It is important to note that considering par-
tially found relations as negative samples has to
be avoided. Wrong negative samples have a gen-
erally unwanted impact on the performance of the
learned extraction model. A developer has to be
aware of this fact. In one experiment, the learned
classifiers were applied to the training data and
returned a number of false positive results ? 40
in case of the river-bodyOfWater relation. 31 of
these errors were not actual errors because the
self-annotation missed some true instances. Nev-
ertheless, the trained model recognizes these sam-
ples as correct; this could perhaps be used to fur-
ther improve the quality of self-annotation.
Manually labeled data also includes noise and
the benefit of self-annotation is substantial when
2See en.wikipedia.org/wiki/Wikipedia:Bots. The edit his-
tory of many articles shows that there is a lot of automatic
maintenance by bots to avoid schema drift.
the aim is to build a fine-grained relation extrac-
tion system in a fast and cheap way.
The difference of the results between OpenNLP
and FSPar engines are smaller than expected.
Although sentence splitting is poorly done by
OpenNLP the effect on the extraction result is
rather low. Another crucial point is that the
lexicon-based named entity recognizer of the FS-
Par engine that was optimized for named entities
used in Wikipedia has no significant impact on the
overall performance. Thus, a basic set of NLP
components with moderate error rates may be suf-
ficient for effective self-annotation.
8 Conclusion
This paper described a new approach to develop-
ing and implementing a complete system to ex-
tract fine-grained geospatial relations by using a
supervised machine learning approach without ex-
pensive manual labeling. Using self-annotation,
systems can be rapidly developed and adapted for
new relations without expensive manual annota-
tion. Only some manual work has to be done
to find the right attributes in the infoboxes. The
matching process between infoboxes and text is
not in all cases trivial and for some attributes ad-
ditional rules have to be modeled.
9 Acknowledgment
This project was funded by DFG as part of Nexus
(Collaborative Research Centre, SFB 627).
References
Auer, So?ren, Christian Bizer, Georgi Kobilarov, Jens
Lehmann, and Zachary Ives. 2007. Dbpedia: A
nucleus for a web of open data. In In 6th Intl Se-
mantic Web Conference, Busan, Korea, pages 11?
15. Springer.
Blessing, Andre, Stefan Klatt, Daniela Nicklas, Stef-
fen Volz, and Hinrich Schu?tze. 2006. Language-
derived information and context models. In Pro-
ceedings of 3rd IEEE PerCom Workshop on Context
Modeling and Reasoning (CoMoRea) (at 4th IEEE
International Conference on Pervasive Computing
and Communication (PerCom?06)).
Du?rr, Frank, Nicola Ho?nle, Daniela Nicklas, Christian
Becker, and Kurt Rothermel. 2004. Nexus?a plat-
form for context-aware applications. In Roth, Jo?rg,
87
editor, 1. Fachgespra?ch Ortsbezogene Anwendun-
gen und Dienste der GI-Fachgruppe KuVS, pages
15?18, Hagen, Juni. Informatik-Bericht der Fer-
nUniversita?t Hagen.
Hahn, Udo, Ekaterina Buyko, Rico Landefeld,
Matthias Mu?hlhausen, Michael Poprat, Katrin
Tomanek, and Joachim Wermter. 2008. An
overview of JCoRe, the JULIE lab UIMA compo-
nent repository. In Proceedings of the LREC?08
Workshop ?Towards Enhanced Interoperability for
Large HLT Systems: UIMA for NLP?, Marrakech,
Morocco, May.
Jiang, Jing. 2009. Multi-task transfer learning for
weakly-supervised relation extraction. In ACL-
IJCNLP ?09: Proceedings of the Joint Conference of
the 47th Annual Meeting of the ACL and the 4th In-
ternational Joint Conference on Natural Language
Processing of the AFNLP: Volume 2, pages 1012?
1020, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Mu?ller, Christof, Torsten Zesch, Mark-Christoph
Mu?ller, Delphine Bernhard, Kateryna Ignatova,
Iryna Gurevych, and Max Mu?hlha?user. 2008. Flex-
ible uima components for information retrieval re-
search. In Proceedings of the LREC 2008 Work-
shop ?Towards Enhanced Interoperability for Large
HLT Systems: UIMA for NLP?, Marrakech, Mo-
rocco, May 31, 2008. 24?27.
Nothman, Joel, Tara Murphy, and James R. Curran.
2009. Analysing wikipedia and gold-standard cor-
pora for ner training. In EACL ?09: Proceedings
of the 12th Conference of the European Chapter
of the Association for Computational Linguistics,
pages 612?620, Morristown, NJ, USA. Association
for Computational Linguistics.
Ogren, Philip V., Philipp G. Wetzler, and Steven
Bethard. 2008. Cleartk: A uima toolkit for sta-
tistical natural language processing. In UIMA for
NLP workshop at Language Resources and Evalua-
tion Conference (LREC).
Schiehlen, Michael. 2003. Combining deep and shal-
low approaches in parsing german. In ACL ?03:
Proceedings of the 41st Annual Meeting on Asso-
ciation for Computational Linguistics, pages 112?
119, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Schmid, Helmut. 1995. Improvements in part-of-
speech tagging with an application to german. In In
Proceedings of the ACL SIGDAT-Workshop, pages
47?50.
Suchanek, Fabian M., Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: A Core of Semantic Knowl-
edge. In 16th international World Wide Web con-
ference (WWW 2007), New York, NY, USA. ACM
Press.
Wu, Fei and Daniel S. Weld. 2007. Autonomously
semantifying wikipedia. In Proceedings of the Six-
teenth ACM Conference on Information and Knowl-
edge Management, CIKM 2007, Lisbon, Portugal,
November 6-10, 2007, pages 41?50.
Zesch, Torsten, Christof Mu?ller, and Iryna Gurevych.
2008. Extracting Lexical Semantic Knowledge
from Wikipedia and Wiktionary. In Proceedings of
the Conference on Language Resources and Evalu-
ation (LREC).
Zhang, Ziqi and Jose? Iria. 2009. A novel approach to
automatic gazetteer generation using wikipedia. In
People?s Web ?09: Proceedings of the 2009 Work-
shop on The People?s Web Meets NLP, pages 1?9,
Morristown, NJ, USA. Association for Computa-
tional Linguistics.
88
Proceedings of the 7th Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities, pages 55?64,
Sofia, Bulgaria, August 8 2013. c?2013 Association for Computational Linguistics
Towards a Tool for Interactive Concept Building for Large Scale Analysis
in the Humanities
Andre Blessing1 Jonathan Sonntag2 Fritz Kliche3
Ulrich Heid3 Jonas Kuhn1 Manfred Stede2
1Institute for Natural Language Processing
Universitaet Stuttgart, Germany
2Institute for Applied Computational Linguistics
University of Potsdam, Germany
3Institute for Information Science and Natural Language Processing
University of Hildesheim, Germany
Abstract
We develop a pipeline consisting of var-
ious text processing tools which is de-
signed to assist political scientists in find-
ing specific, complex concepts within
large amounts of text. Our main focus is
the interaction between the political scien-
tists and the natural language processing
groups to ensure a beneficial assistance for
the political scientists and new application
challenges for NLP. It is of particular im-
portance to find a ?common language? be-
tween the different disciplines. Therefore,
we use an interactive web-interface which
is easily usable by non-experts. It inter-
faces an active learning algorithm which
is complemented by the NLP pipeline to
provide a rich feature selection. Political
scientists are thus enabled to use their own
intuitions to find custom concepts.
1 Introduction
In this paper, we give examples of how NLP meth-
ods and tools can be used to provide support for
complex tasks in political sciences. Many con-
cepts of political science are complex and faceted;
they tend to come in different linguistic realiza-
tions, often in complex ones; many concepts are
not directly identifiable by means of (a small set
of) individual lexical items, but require some in-
terpretation.
Many researchers in political sciences either
work qualitatively on small amounts of data which
they interpret instance-wise, or, if they are in-
terested in quantitative trends, they use compara-
tively simple tools, such as keyword-based search
in corpora or text classification on the basis of
terms only; this latter approach may lead to im-
precise results due to a rather unspecific search as
well as semantically invalid or ambigious search
words. On the other hand, large amounts of e.g.
news texts are available, also over longer periods
of time, such that e.g. tendencies over time can
be derived. The corpora we are currently working
on contain ca. 700,000 articles from British, Irish,
German and Austrian newspapers, as well as (yet
unexplored) material in French.
Figure 1 depicts a simple example of a quantita-
tive analysis.1 The example shows how often two
terms, Friedensmission(?peace operation?), and
Auslandseinsatz(?foreign intervention?) are used
in the last two decades in newspaper texts about
interventions and wars. The long-term goal of the
project is to provide similar analysis for complex
concepts. An example of a complex concept is
the evocation of collective identities in political
contexts, as indirect in the news. Examples for
such collective identities are: the Europeans, the
French, the Catholics.
The objective of the work we are going to dis-
cuss in this paper is to provide NLP methods and
tools for assisting political scientists in the ex-
ploration of large data sets, with a view to both,
a detailed qualitative analysis of text instances,
and a quantitative overview of trends over time,
at the level of corpora. The examples discussed
here have to do with (possibly multiple) collective
identities. Typical context of such identities tend
to report communication, as direct or as indirect
speech. Examples of such contexts are given in 1.
(1) Die
The
Europa?er
Europeans
wu?rden
would
die
the
Lu?cke
gap
fu?llen,
fill,
1The figure shows a screenshot of our web-based
prototype.
55
Figure 1: The screenshot of our web-based system shows a simple quantitative analysis of the frequency
of two terms in news articles over time. While in the 90s the term Friedensmission (peace operation) was
predominant a reverse tendency can be observed since 2001 with Auslandseinsatz (foreign intervention)
being now frequently used.
sagte
said
Ru?he.
Ru?he.
,,The Europeans would fill the gap, Ru?he said.?
The tool support is meant to be semi-automatic,
as the automatic tools propose candidates that
need to be validated or refused by the political sci-
entists.
We combine a chain of corpus processing tools
with classifier-based tools, e.g. for topic clas-
sifiers, commentary/report classifiers, etc., make
the tools interoperable to ensure flexible data ex-
change and multiple usage scenarios, and we em-
bed the tool collection under a web (service) -
based user interface.
The remainder of this paper is structured as fol-
lows. In section 2, we present an outline of the ar-
chitecture of our tool collection, and we motivate
the architecture. Section 3 presents examples of
implemented modules, both from corpus process-
ing and search and retrieval of instances of com-
plex concepts. We also show how our tools are re-
lated to the infrastructural standards in use in the
CLARIN community. In section 4, we exemplify
the intended use of the methods with case studies
about steps necessary for identifying evocation:
being able to separate reports from comments, and
strategies for identifying indirect speech. Section
6 is devoted to a conclusion and to the discussion
of future work.
2 Project Goals
A collaboration between political scientists and
computational linguists necessarily involves find-
ing a common language in order to agree on
the precise objectives of a project. For exam-
ple, social scientists use the term codebook for
manual annotations of text, similar to annotation
schemes or guidelines in NLP. Both disciplines
share methodologies of interactive text analysis
which combine term based search, manual an-
notation and learning-based annotation of large
amounts of data. In this section, we give a brief
56
summary of the goals from the perspective of each
of the two disciplines, and then describe the text
corpus that is used in the project. Section 3 will
describe our approach to devising a system archi-
tecture that serves to realize the goals.
2.1 Social Science Research Issue
Given the complexity of the underlying research
issues (cf. Section 1) and the methodological tra-
dition of manual text coding by very well-trained
annotators in the social science and particular in
political science, our project does not aim at any
fully-automatic solution for empirical issues in po-
litical science. Instead, the goal is to provide as
much assistance to the human text analyst as possi-
ble, by means of a workbench that integrates many
tasks that otherwise would have to be carried out
with different software tools (e.g., corpus prepro-
cessing, KWIC searches, statistics). In our project,
the human analyst is concerned specifically with
manifestations of collective identities in newspa-
per texts on issues of war and military interven-
tions: who are the actors in political crisis man-
agement or conflict? How is this perspective of
responsible actors characterized in different news-
papers (with different political orientation; in dif-
ferent countries)? The analyst wants to find doc-
uments that contain facets of such constellations,
which requires search techniques involving con-
cepts on different levels of abstraction, ranging
from specific words or named entities (which may
appear with different names in different texts) to
event types (which may be realized with different
verb-argument configurations). Thus the text cor-
pus should be enriched with information relevant
to such queries, and the workbench shall provide
a comfortable interface for building such queries.
Moreover, various types and (possibly concurrent)
layers of human annotations have to complement
the automatic analysis, and the manual annota-
tion would benefit from automatic control of code-
book2 compliance and the convergence of coding
decisions.
2.2 Natural Language Processing Research
Issue
Large collections of text provide an excellent op-
portunity for computational linguists to scale their
methods. In the scenario of a project like ours, this
becomes especially challenging, because standard
2or, in NLP terms: annotation scheme.
automatic analysis components have to be com-
bined with manual annotation or interactive inter-
vention of the human analyst.
In addition to this principled challenge, there
may be more mundane issues resulting from pro-
cessing corpora whose origin stretches over many
years. In our case, the data collection phase coin-
cided with a spelling reform in German-speaking
countries. Many aspects of spelling changed twice
(in 1996 and in 2006), and thus it is the responsi-
bility of the NLP branch of the project to provide
an abstraction over such changes and to enable to-
day?s users to run a homogeneous search over the
texts using only the current spelling. While this
might be less important for generic web search ap-
plications, it is of great importance for our project,
where the overall objective is a combination of
quantitative and qualitative text analysis.
In our processing chain, we first need to harmo-
nize the data formats so that the processing tools
operate on a common format. Rather than defin-
ing these from scratch, we aim at compatibility
with the standardization efforts of CLARIN3 and
DARIAH4, two large language technology infras-
tructure projects in Europe that in particular target
eHumanities applications. One of the objectives
is to provide advanced tools to discover, explore,
exploit, annotate, analyse or combine textual re-
sources. In the next section we give more details
about how we interact which the CLARIN-D in-
frastructure (Boehlke et al, 2013).
3 Architecture
The main goal is to provide a web-based user-
interface to the social scientist to avoid any soft-
ware installation. Figure 2 presents the workflow
of the different processing steps in this project.
The first part considers format issues that occur
if documents from different sources are used. The
main challenge is to recognize metadata correctly.
Date and source name are two types of metadata
which are required for analyses in the social sci-
ences. But also the separation of document con-
tent (text) and metadata is important to ensure that
only real content is processed with the NLP meth-
ods. The results are stored in a repository which
uses a relational database as a back-end. All fur-
ther modules are used to add more annotations to
the textual data. First a complex linguistic pro-
3http://www.clarin.eu/
4http://www.dariah.eu/
57
cessing chain is used to provide state-of-the-art
corpus linguistic annotations (see Section 3.2 for
details). Then, to ensure that statistics over oc-
currence counts of words, word combinations and
constructions are valid and not blurred by the mul-
tiple presence of texts or text passages in the cor-
pus, we filter duplicates. Duplicates can occur
if our document set contains the same document
twice or if two documents are very similar, e.g.
they differ in only one sentence.
Raw documents
Repository:MetadataStructural dataTextual data Topic filter
Duplicate filter
Linguistic analysisSentence splitter Tokenizer
Web-basedUserinterface
Tagger ParserCoref NER
ImportExploration Workbench
Concept detection
Complex Concept Builder
Figure 2: Overview of the complete processing
chain.
We split the workflow for the user into two
parts: The first part is only used if the user im-
ports new data into the repository. For that he
can use the exploration workbench (Section 3.1).
Secondly, all steps for analyzing the data are done
with the Complex Concept Builder (Section 3.2).
3.1 Exploration Workbench
Formal corpus inhomogeneity (e.g. various data
formats and inconsistent data structures) are a ma-
jor issue for researchers working on text corpora.
The web-based ?Exploration Workbench? allows
for the creation of a consistent corpus from vari-
ous types of data and prepares data for further pro-
cessing with computational linguistic tools. The
workbench can interact with to existing computa-
tional linguistic infrastructure (e.g. CLARIN) and
provides input for the repository also used by the
Complex Concept Builder.
The workbench converts several input formats
(TXT, RTF, HTML) to a consistent XML repre-
sentation. The conversion tools account for differ-
ent file encodings and convert input files to Uni-
code (UTF-8). We currently work on newspa-
per articles wrapped with metadata. Text mining
components read out those metadata and identify
text content in the documents. Metadata appear
at varying positions and in diverse notations, e.g.
for dates, indications of authors or newspaper sec-
tions. The components account for these varia-
tions and convert them to a consistent machine
readable format. The extracted metadata are ap-
pended to the XML representation. The result-
ing XML is the starting point for further compu-
tational linguistic processing of the source docu-
ments.
The workbench contains a tool to identify text
duplicates and semi-duplicates via similarity mea-
sures of pairs of articles (Kantner et al, 2011).
The method is based on a comparison of 5-grams,
weighted by significance (tf-idf measure (Salton
and Buckley, 1988)). For a pair of documents it
yields a value on a ?similarity scale? ranging from
0 to 1. Values at medium range (0.4 to 0.8) are
considered semi-duplicates.
Data cleaning is important for the data-driven
studies. Not only duplicate articles have a nega-
tive impact, also articles which are not of interest
for the given topic have to be filtered out. There
are different approaches to classify articles into a
range of predefined topics. In the last years LDA
(Blei et al, 2003; Niekler and Ja?hnichen, 2012)
is one of the most successful methods to find top-
ics in articles. But for social scientists the cate-
gories typically used in LDA are not sufficient. We
follow the idea of Dualist (Settles, 2011; Settles
and Zhu, 2012) which is an interactive method for
classification. The architecture of Dualist is based
on MALLET (McCallum, 2002) which is easily
integrable into our architecture. Our goal is to
design the correct feature to find relevant articles
for a given topic. Word features are not sufficient
since we have to model more complex features (cf.
Section 2.1).
The workbench is not exclusively geared to the
data of the current project. We chose a modular
set-up of the tools of the workbench and provide
user-modifiable templates for the extraction of var-
ious kinds of metadata, in order to keep the work-
bench adaptable to new data and to develop tools
suitable for data beyond the scope of the current
corpus.
58
3.2 Complex Concept Builder
A central problem for political scientists who in-
tend to work on large corpora is the linguistic va-
riety in the expression of technical terms and com-
plex concepts. An editorial or a politician cited
in a news item can mobilize a collective identity
which can be construed from e.g. regional or so-
cial affiliation, nationality or religion. A reason-
able goal in the context of the search for collec-
tive identity evocation contexts is therefore to find
all texts which (possibly) contain collective iden-
tities. Moreover, while we are training our inter-
active tools on a corpus on wars and military in-
terventions the same collective identities might be
expressed in different ways in a corpus i.e. on the
Eurocrisis.
From a computational point of view, many dif-
ferent tools need to be joined to detect interest-
ing texts. An example application could be a case
where a political scientist intends to extract news-
paper articles that cite a politician who tries to
rally support for his political party. In order to
detect such text, we need a system to identify di-
rect and indirect speech and a sentiment system to
determine the orientation of the statement. These
systems in turn need various kinds of preprocess-
ing starting from tokenization over syntactic pars-
ing up to coreference resolution. The Complex
Concept Builder is the collection of all these sys-
tems with the goal to assist the political scientists.
So far, the Complex Concept Builder imple-
ments tokenization (Schmid, 2009), lemmatisation
(Schmid, 1995), part-of-speech tagging (Schmid
and Laws, 2008), named entity detection (Faruqui
and Pado?, 2010), syntactical parsing (Bohnet,
2010), coreference analysis for German (Lappin
and Leass, 1994; Stuckardt, 2001), relation extrac-
tion (Blessing et al, 2012) and sentiment analysis
for English (Taboada et al, 2011).
It is important for a researcher of the humanities
to be able to adapt existing classification systems
according to his own needs. A common procedure
in both, NLP and political sciences, is to annotate
data. Therefore, one major goal of the project and
the Complex Concept Builder is to provide ma-
chine learning systems with a wide range of pos-
sible features ? including high level information
like sentiment, text type, relations to other texts,
etc. ? that can be used by non-experts for semi-
automatic annotation and text selection. Active
learning is used to provide immediate results that
can then be improved continuously. This aspect
of the Complex Concept Builder is especially im-
portant because new or adapted concepts that may
be looked for can be found without further help of
natural language processing experts.
3.3 Implementation
We decided to use a web-based platform for our
system since the social scientist needs no software
installation and we are independent of the used
operating system. Only a state-of-the-art web-
browser is needed. On the server side, we use a
tomcat installation that interacts with our UIMA
pipeline (Ferrucci and Lally, 2004). A HTML-
rendering component designed in the project (and
parametrizable) allows for a flexible presentation
of the data. A major issue of our work is interac-
tion. To solve this, we use JQuery and AJAX to
dynamically interact between client- and server-
side.
4 Case Study
In this section we explore the interaction between
various sub-systems and how they collaborate to
find complex political concepts. The following
Section 4.1 describes the detection of direct and
indirect speech and its evaluation follows in Sec-
tion 4.2. Section 4.3 is a general exploration of a
few selected sub-systems which require, or benefit
from direct and indirect speech. Finally, Section
4.4 discusses a specific usage scenario for indirect
speech.
4.1 Identifying Indirect Speech
The Complex Concept Builder provides analy-
ses on different linguistic levels (currently mor-
phosyntax, dependency syntax, named entities) of
annotation. We exploit this knowledge to identify
indirect speech along with a mentioned speaker.
Our indirect speech recognizer is based on three
conditions: i) Consider all sentences that contain
at least one word which is tagged as subjunctive
(i.e. ?*.SUBJ?) by the RFTagger. ii) This verb
has to be a direct successor of another verb in the
dependency tree. iii) This verb needs to have a
subject.
Figure 3 depicts the dependency parse tree of
sentence 2.
(2) Der Einsatz werde wegen der Risiken fu?r die
unbewaffneten Beobachter ausgesetzt, teilte
59
Einsatzmission
theDer
,,
ausgesetztstopped
werde
wegenbecause of
Risikorisks
teilteinformed
will be Missionschefhead of mission
MoodMood
RobertRobert
mitam
SaturdaySamstag
on
..
SBOC
VFIN.Aux.3.Sg.Pres.Subj
VFIN.Full.3.Sg.Past.IndRFTags
Figure 3: Dependency parse of a sentence that
contains indirect speech (see Sentence 2).
Missionschef Robert Mood am Samstag mit.
The mission will be stopped because of the risks to the
unarmed observers, informed Head of Mission Robert
Mood on Saturday.
The speaker of the indirect speech in Sentence
2 is correctly identified as Missionschef (Head of
Mission) and the corresponding verb is teilte mit
(from mitteilen) (to inform).
The parsing-based analysis helps to identify the
speaker of the citation which is a necessary in-
formation for the later interpretation of the cita-
tion. As a further advantage, such an approach
helps to minimize the need of lexical knowledge
for the identification of indirect speech. Our er-
ror analysis below will show that in some cases
a lexicon can help to avoid false positives. A lexi-
con of verbs of communication can easily be boot-
strapped by using our approach to identify candi-
dates for the list of verbs which then restrict the
classifier in order to achieve a higher precision.
4.2 Indirect Speech Evaluation
For a first impression, we present a list of sen-
tences which were automatically annotated as pos-
itive instances by our indirect speech detector.
The sentences were rated by political scientists.
Additionally, for each sentence we extracted the
speaker and the used verb of speech. We man-
ually evaluated 200 extracted triples (sentence,
speaker, verb of speech): The precision of our
system is: 92.5%
Examples 2, 3 and 4 present good candidates
which are helpful for further investigations on col-
lective identities. In example 3 Cardinal Lehmann
is a representative speaker of the Catholic commu-
nity which is a collective identity. Our extracted
sentences accelerate the search for such candidates
which amounts to looking manually for needles in
a haystack.
example speaker verb of speech
(2) Robert Mood teilte (told)
(3) Kardinal Karl Lehmann sagte (said)
(4) Sergej Ordzhonikidse sagte (said)
(5) Bild (picture) tru?ben (tarnish)
(6) sein (be) sein (be)
Examples 5 and 6 show problems of our first
approach. In this case, the speaker is not a person
or an organisation, and the verb is not a verb of
speech.
(3) Ein Angriffskrieg jeder Art sei ? sit-
tlich verwerflich ?, sagte der Vorsitzende
der Bischoffskonferenz, Kardinal Karl
Lehmann.
Any kind of war of aggression is ?morally reprehen-
sible,? said the chairman of the Bishops? Conference,
Cardinal Karl Lehmann.
(4) Derartige Erkla?rungen eines Staatschefs
seien im Rahmen der internationalen
Beziehungen inakzeptabel, sagte der UN-
Generaldirektor Sergej Ordzhonikidse
gestern in Genf.
Such statements of heads of states are unacceptable in
the context of international relations, said UN General
Director Sergei Ordzhonikidse in Geneva yesterday.
(5) Wu?rden die Wahlen verschoben, tru?bte sich
das gescho?nte Bild.
Would the elections be postponed, the embellished im-
age would tarnish.
(6) Dies sei alles andere als einfach, ist aus Of-
fizierskreisen zu ho?ren.
This is anything but simple, is to hear from military
circles.
60
EinsatzEimosheDrhsatzs,uDigao
psgasatzEdidsowshgbcdsatzhsu hdo
pgddsgDsatzEcihsoEbchsgwsatzfhgdso
ciwsatzciRsofshksatzfgDDo
wsd asatzspuciEglsoshlrcDsatzdsDDo
MSMMy.SMMy BMSMMyB.SMMyOMSMMyO.SMMyCMSMMyC.SMMyVMSMMy
p EdtFEsktEussbctRshwE
Figure 4: 10 most used verbs (lemma) in indirect
speech.
4.3 Using Indirect Speech
Other modules benefit from the identification of
indirect speech, as can be seen from Sentence 7.
The sentiment system assigns a negative polarity
of ?2.15 to the sentence. The nested sentiment
sources, as described by (Wiebe et al, 2005), of
this sentence require a) a direct speech with the
speaker ?Mazower? and b) an indirect speech with
the speaker ?no one? to be found.5
(7) ?There were serious arguments about what
should happen to the Slavs and Poles in east-
ern Europe,? says Mazower, ?and how many
of them should be sent to the camps and what
proportion could be Germanised . . . No one
ever came out and directly said Hitler had got
it wrong, but there was plenty of implied crit-
icism through comparisons with the Roman
empire. [...]?6
A collective identity evoked in Sentence 7 is
?the Germans?? although the term is not explic-
itly mentioned. This collective identity is de-
scribed as non-homogeneous in the citation and
can be further explored manually by the political
scientists.
The following are further applications of the
identified indirect speeches a) using the frequency
of speeches per text as a feature for classifica-
tion; e.g. a classification system for news re-
ports/commentaries as described in Section 4.4 b)
a project-goal is to find texts in which collective
5The reported sentiment value for the whole sentence is
applicable only to the direct speech. The indirect speech (i.e.
?Hitler had got it wrong?) needs a more fine-grained polarity
score. Since our Complex Concept Builder is very flexible, it
is trivial to score each component separately.
6http://www.guardian.co.uk/education/2008/jul
/01/academicexperts.highereducationprofile
identities are mobilised by entities of political de-
bate (i.e. persons, organisations, etc.); the detec-
tion of indirect speech is mandatory for any such
analysis.
4.4 Commentary/Report Classification
A useful distinction for political scientists dealing
with newspaper articles is the distinction between
articles that report objectively on events or back-
grounds and editorials or press commentaries.
We first extracted opinionated and objective
texts from DeReKo corpus (Stede, 2004; Kupietz
et al, 2010). Some texts were removed in order to
balance the corpus. The balanced corpus contains
2848 documents and has been split into a develop-
ment and a training and test set. 570 documents
were used for the manual creation of features. The
remaining 2278 documents were used to train and
evaluate classifiers using 10-fold cross-validation
with the WEKA machine learning toolkit (Hall et
al., 2009) and various classifiers (cf. Table 1).
The challenge is that the newspaper articles
from the training and evaluation corpus come from
different newspapers and, of course, from differ-
ent authors. Commentaries in the yellow press
tend to have a very different style and vocabulary
than commentaries from broadsheet press. There-
fore, special attention needs to be paid to the in-
dependence of the classifier from different authors
and different newspapers. For this reason, we use
hand-crafted features tailored to this problem. In
return, this means omitting surface-form features
(i.e. words themselves).
The support vector machine used the SMO al-
gorithm (Platt and others, 1998) with a polynomial
kernel K(x, y) =< x, y > e with e = 2. All other
algorithms were used with default settings.
precision recall f-score
SVM 0.819 0.814 0.813
Naive Bayes 0.79 0.768 0.764
Multilayer Percep-
tron
0.796 0.795 0.794
Table 1: Results of a 10-fold cross-validation for
various machine learning algorithms.
A qualitative evaluation shows that direct and
indirect speech is a problem for the classifier.
Opinions voiced via indirect speech should not
lead to a classification as ?Commentary?, but
should be ignored. Additionally, the number of
61
uses of direct and indirect speech by the author can
provide insight into the intention of the author. A
common way to voice one?s own opinion, without
having to do so explicitly, is to use indirect speech
that the author agrees with. Therefore, the number
of direct and indirect speech uses will be added
to the classifier. First experiments indicate that the
inclusion of direct and indirect speech increase the
performance of the classifier.
5 Related Work
Many approaches exist to assist social scientists in
dealing with large scale data. We discuss some
well-known ones and highlight differences to the
approach described above.
The Europe Media Monitor (EMM) (Stein-
berger et al, 2009) analyses large amounts of
newspaper articles and assists anyone interested in
news. It allows its users to search for specific top-
ics and automatically clusters articles from differ-
ent sources. This is a key concept of the EMM,
because it collects about 100, 000 articles in ap-
proximately 50 languages per day and it is impos-
sible to scan through these by hand. EMM users
are EU institutions, national institutions of the EU
member states, international organisations and the
public (Steinberger et al, 2009).
The topic clusters provide insight into ?hot?
topics by simply counting the amount of articles
per cluster or by measuring the amount of news on
a specific topic with regards to its normal amount
of news. Articles are also data-mined for geo-
graphical information, e.g. to update in which
geographical region the article was written and
where the topic is located. Social network infor-
mation is gathered and visualised as well.
Major differences between the EMM and our
approach are the user group and the domain of
the corpus. The complex concepts political sci-
entists are interested in are much more nuanced
than the concepts relevant for topic detection and
the construction of social networks. Additionally,
the EMM does not allow its users to look for their
own concepts and issues, while this interactivity
is a central contribution of our approach (cf. Sec-
tions 1, 2.1 and 3.2).
The CLARIN-D project also provides a web-
based platform to create NLP-chains. It is called
WebLicht (Hinrichs et al, 2010), but in its cur-
rent form, the tool is not immediately usable for
social scientists as the separation of metadata and
textual data and the encoding of the data is hard
for non-experts. Furthermore, WebLicht does not
yet support the combination of manual and au-
tomatic annotation needed for text exploration in
the social science. Our approach is based on the
webservices used by WebLicht. But in contrast to
WebLicht, we provide two additional components
that simplify the integration (exploration work-
bench) and the interpretation (complex concept
builder) of the research data. The former is in-
tended, in the medium term, to be made available
in the CLARIN framework.
6 Conclusion and Outlook
We developed and implemented a pipeline of var-
ious text processing tools which is designed to as-
sist political scientists in finding specific, complex
concepts within large amounts of text. Our case
studies showed that our approach can provide ben-
eficial assistance for the research of political sci-
entists as well as researcher from other social sci-
ences and the humanities. A future aspect will be
to find metrics to evaluate our pipeline. In recently
started annotation experiments on topic classifica-
tion Cohen?s kappa coefficient (Carletta, 1996) is
mediocre. It may very well be possible that the
complex concepts, like multiple collective identi-
ties, are intrinsically hard to detect, and the anno-
tations cannot be improved substantially.
The extension of the NLP pipeline will be an-
other major working area in the future. Examples
are sentiment analysis for German, adding world
knowledge about named entities (e.g. persons and
events), identification of relations between enti-
ties.
Finally, all these systems need to be evaluated
not only in terms of f-score, precision and recall,
but also in terms of usability for the political scien-
tists. This also includes a detailed investigation of
various political science concepts and if they can
be detected automatically or if natural language
processing can help the political scientists to de-
tect their concepts semi-automatically. The defini-
tion of such evaluation is an open research topic in
itself.
Acknowledgements
The research leading to these results has been
done in the project eIdentity which is funded from
the Federal Ministry of Education and Research
(BMBF) under grant agreement 01UG1234.
62
References
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. J. Mach. Learn.
Res., 3:993?1022, March.
Andre Blessing, Jens Stegmann, and Jonas Kuhn.
2012. SOA meets relation extraction: Less may be
more in interaction. In Proceedings of the Work-
shop on Service-oriented Architectures (SOAs) for
the Humanities: Solutions and Impacts, Digital Hu-
manities, pages 6?11.
Volker Boehlke, Gerhard Heyer, and Peter Wittenburg.
2013. IT-based research infrastructures for the hu-
manities and social sciences - developments, exam-
ples, standards, and technology. it - Information
Technology, 55(1):26?33, February.
Bernd Bohnet. 2010. Top accuracy and fast depen-
dency parsing is not a contradiction. In Proceedings
of the 23rd International Conference on Computa-
tional, pages 89?97.
Jean Carletta. 1996. Assessing agreement on classi-
fication tasks: The kappa statistic. Computational
Linguistics, 22(2):249?254.
Manaal Faruqui and Sebastian Pado?. 2010. Train-
ing and evaluating a german named entity recog-
nizer with semantic generalization. In Proceedings
of KONVENS 2010, Saarbru?cken, Germany.
D. Ferrucci and A. Lally. 2004. UIMA: an architec-
tural approach to unstructured information process-
ing in the corporate research environment. Natural
Language Engineering, 10(3-4):327?348.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H Witten.
2009. The weka data mining software: an update.
ACM SIGKDD Explorations Newsletter, 11(1):10?
18.
Erhard W. Hinrichs, Marie Hinrichs, and Thomas Za-
strow. 2010. WebLicht: Web-Based LRT Services
for German. In Proceedings of the ACL 2010 System
Demonstrations, pages 25?29.
Cathleen Kantner, Amelie Kutter, Andreas Hilde-
brandt, and Mark Puettcher. 2011. How to get rid
of the noise in the corpus: Cleaning large samples
of digital newspaper texts. International Relations
Online Working Paper, 2, July.
Marc Kupietz, Cyril Belica, Holger Keibel, and An-
dreas Witt. 2010. The german reference corpus
dereko: a primordial sample for linguistic research.
In Proceedings of the 7th conference on interna-
tional language resources and evaluation (LREC
2010), pages 1848?1854.
Shalom Lappin and Herbert J Leass. 1994. An algo-
rithm for pronominal anaphora resolution. Compu-
tational linguistics, 20(4):535?561.
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://www.cs.umass.edu/ mccallum/mallet.
Andreas Niekler and Patrick Ja?hnichen. 2012. Match-
ing results of latent dirichlet alocation for text.
In Proceedings of ICCM 2012, 11th International
Conference on Cognitive Modeling, pages 317?322.
Universita?tsverlag der TU Berlin.
John Platt et al 1998. Sequential minimal optimiza-
tion: A fast algorithm for training support vector
machines. technical report msr-tr-98-14, Microsoft
Research.
Gerard Salton and Christopher Buckley. 1988. Term-
weighting approaches in automatic text retrieval. In-
formation processing & management, 24(5):513?
523.
Helmut Schmid and Florian Laws. 2008. Estima-
tion of conditional probabilities with decision trees
and an application to fine-grained POS tagging. In
Proceedings of the 22nd International Conference
on Computational Linguistics (Coling 2008), pages
777?784, Manchester, UK, August.
Helmut Schmid. 1995. Improvements in part-of-
speech tagging with an application to german. In
Proceedings of the ACL SIGDAT-Workshop, pages
47?50.
Helmut Schmid, 2009. Corpus Linguistics: An In-
ternational Handbook, chapter Tokenizing and Part-
of-Speech Tagging. Handbooks of Linguistics and
Communication Science. Walter de Gruyter, Berlin.
Burr Settles and Xiaojin Zhu. 2012. Behavioral fac-
tors in interactive training of text classifiers. In Pro-
ceedings of the 2012 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
563?567. Association for Computational Linguis-
tics.
Burr Settles. 2011. Closing the loop: Fast, inter-
active semi-supervised annotation with queries on
features and instances. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 1467?1478. Association for Com-
putational Linguistics.
Manfred Stede. 2004. The potsdam commentary
corpus. In Proceedings of the 2004 ACL Work-
shop on Discourse Annotation, DiscAnnotation ?04,
pages 96?102, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Ralf Steinberger, Bruno Pouliquen, and Erik Van
Der Goot. 2009. An introduction to the europe me-
dia monitor family of applications. In Proceedings
of the Information Access in a Multilingual World-
Proceedings of the SIGIR 2009 Workshop, pages 1?
8.
63
Roland Stuckardt. 2001. Design and enhanced evalua-
tion of a robust anaphor resolution algorithm. Com-
putational Linguistics, 27(4):479?506.
Maite Taboada, Julian Brooke, Milan Tofiloski, Kim-
berly Voll, and Manfred Stede. 2011. Lexicon-
based methods for sentiment analysis. Computa-
tional linguistics, 37(2):267?307.
Janyce Wiebe, Theresa Wilson, and Claire Cardie.
2005. Annotating expressions of opinions and emo-
tions in language. Language Resources and Evalu-
ation, 39(2-3):165?210.
64
