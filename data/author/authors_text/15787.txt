Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1027?1037, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Ensemble Semantics for Large-scale Unsupervised Relation Extraction 
 
 
Bonan Min1* Shuming Shi2 Ralph Grishman1 Chin-Yew Lin2 
  
1New York University 2Microsoft Research Asia 
New York, NY, USA Beijing, China 
{min,grishman}@cs.nyu.edu {shumings,cyl}@microsoft.com 
  
Abstract 
Discovering significant types of relations 
from the web is challenging because of its 
open nature. Unsupervised algorithms are 
developed to extract relations from a cor-
pus without knowing the relations in ad-
vance, but most of them rely on tagging 
arguments of predefined types. Recently, 
a new algorithm was proposed to jointly 
extract relations and their argument se-
mantic classes, taking a set of relation in-
stances extracted by an open IE algorithm 
as input. However, it cannot handle poly-
semy of relation phrases and fails to 
group many similar (?synonymous?) rela-
tion instances because of the sparseness of 
features. In this paper, we present a novel 
unsupervised algorithm that provides a 
more general treatment of the polysemy 
and synonymy problems. The algorithm 
incorporates various knowledge sources 
which we will show to be very effective 
for unsupervised extraction. Moreover, it 
explicitly disambiguates polysemous rela-
tion phrases and groups synonymous 
ones. While maintaining approximately 
the same precision, the algorithm achieves 
significant improvement on recall com-
pared to the previous method. It is also 
very efficient. Experiments on a real-
world dataset show that it can handle 14.7 
million relation instances and extract a 
very large set of relations from the web.  
1 Introduction 
Relation extraction aims at discovering semantic 
relations between entities. It is an important task 
that has many applications in answering factoid 
questions, building knowledge bases and improv-
ing search engine relevance. The web has become 
a massive potential source of such relations. How-
ever, its open nature brings an open-ended set of 
relation types. To extract these relations, a system 
should not assume a fixed set of relation types, nor 
rely on a fixed set of relation argument types.  
The past decade has seen some promising solu-
tions, unsupervised relation extraction (URE) algo-
rithms that extract relations from a corpus without 
knowing the relations in advance. However, most 
algorithms (Hasegawa et al 2004, Shinyama and 
Sekine, 2006, Chen et. al, 2005) rely on tagging 
predefined types of entities as relation arguments, 
and thus are not well-suited for the open domain.  
Recently, Kok and Domingos (2008) proposed 
Semantic Network Extractor (SNE), which gener-
ates argument semantic classes and sets of synon-
ymous relation phrases at the same time, thus 
avoiding the requirement of tagging relation argu-
ments of predefined types. However, SNE has 2 
limitations: 1) Following previous URE algo-
rithms, it only uses features from the set of input 
relation instances for clustering.  Empirically we 
found that it fails to group many relevant relation 
instances. These features, such as the surface forms 
of arguments and lexical sequences in between, are 
very sparse in practice. In contrast, there exist sev-
eral well-known corpus-level semantic resources 
that can be automatically derived from a source 
corpus and are shown to be useful for generating 
the key elements of a relation: its 2 argument se-
mantic classes and a set of synonymous phrases. 
For example, semantic classes can be derived from 
a source corpus with contextual distributional simi-
larity and web table co-occurrences. The ?synony-
my? 1  problem for clustering relation instances 
                                                          
* Work done during an internship at Microsoft Research Asia 
1027
could potentially be better solved by adding these 
resources. 2) SNE assumes that each entity or rela-
tion phrase belongs to exactly one cluster, thus is 
not able to effectively handle polysemy of relation 
phrases2. An example of a polysemous phrase is be 
the currency of  as in 2 triples <Euro, be the cur-
rency of, Germany> and <authorship, be the cur-
rency of, science>. As the target corpus expands 
from mostly news to the open web, polysemy be-
comes more important as input covers a wider 
range of domains. In practice, around 22% (section 
3) of relation phrases are polysemous. Failure to 
handle these cases significantly limits its effective-
ness. 
    To move towards a more general treatment of 
the polysemy and synonymy problems, we present a 
novel algorithm WEBRE for open-domain large-
scale unsupervised relation extraction without pre-
defined relation or argument types. The contribu-
tions are: 
? WEBRE incorporates a wide range of cor-
pus-level semantic resources for improving rela-
tion extraction. The effectiveness of each 
knowledge source and their combination are stud-
ied and compared. To the best of our knowledge, it 
is the first to combine and compare them for unsu-
pervised relation extraction. 
? WEBRE explicitly disambiguates polyse-
mous relation phrases and groups synonymous 
phrases, thus fundamentally it avoids the limitation 
of previous methods. 
? Experiments on the Clueweb09 dataset 
(lemurproject.org/clueweb09.php) show that 
WEBRE is effective and efficient. We present a 
large-scale evaluation and show that WEBRE can 
extract a very large set of high-quality relations. 
Compared to the closest prior work, WEBRE sig-
nificantly improves recall while maintaining the 
same level of precision. WEBRE is efficient. To 
the best of our knowledge, it handles the largest 
triple set to date (7-fold larger than largest previous 
effort). Taking 14.7 million triples as input, a com-
plete run with one CPU core takes about a day.  
 
 
 
                                                                                           
1 We use the term synonymy broadly as defined in Section 3. 
2 A cluster of relation phrases can, however, act as a whole as 
the phrase cluster for 2 different relations in SNE. However, 
this only accounts for 4.8% of the polysemous cases. 
2 Related Work 
Unsupervised relation extraction (URE) algorithms 
(Hasegawa et al 2004; Chen et al 2005; Shinya-
ma and Sekine, 2006) collect pairs of co-occurring 
entities as relation instances, extract features for 
instances and then apply unsupervised clustering 
techniques to find the major relations of a corpus. 
These UREs rely on tagging a predefined set of 
argument types, such as Person, Organization, and 
Location, in advance. Yao et al2011 learns fine-
grained argument classes with generative models, 
but they share the similar requirement of tagging 
coarse-grained argument types. Most UREs use a 
quadratic clustering algorithm such as Hierarchical 
Agglomerate Clustering (Hasegawa et al 2004, 
Shinyama and Sekine, 2006), K-Means (Chen et 
al., 2005), or both (Rosenfeld and Feldman, 2007); 
thus they are not scalable to very large corpora.  
As the target domain shifts to the web, new 
methods are proposed without requiring predefined 
entity types. Resolver (Yates and Etzioni, 2007) 
resolves objects and relation synonyms. Kok and 
Domingos (2008) proposed Semantic Network Ex-
tractor (SNE) to extract concepts and relations. 
Based on second-order Markov logic, SNE used a 
bottom-up agglomerative clustering algorithm to 
jointly cluster relation phrases and argument enti-
ties. However, both Resolver and SNE require 
each entity and relation phrase to belong to exactly 
one cluster. This limits their ability to handle poly-
semous relation phrases. Moreover, SNE only uses 
features in the input set of relation instances for 
clustering, thus it fails to group many relevant in-
stances. Resolver has the same sparseness problem 
but it is not affected as much as SNE because of its 
different goal (synonym resolution).  
As the preprocessing instance-detection step for 
the problem studied in this paper, open IE extracts 
relation instances (in the form of triples) from the 
open domain (Etzioni et al 2004; Banko et al 
2007; Fader et al 2011; Wang et al2011). For 
efficiency, they only use shallow features. Reverb 
(Fader et al 2011) is a state-of-the-art open do-
main extractor that targets verb-centric relations, 
which have been shown in Banko and Etzioni 
(2008) to cover over 70% of open domain rela-
tions. Taking their output as input, algorithms have 
been proposed to resolve objects and relation syn-
onyms (Resolver),  extract semantic networks 
1028
(SNE), and map extracted relations into an existing 
ontology (Soderland and Mandhani, 2007).  
Recent work shows that it is possible to con-
struct semantic classes and sets of similar phrases 
automatically with data-driven approaches. For 
generating semantic classes, previous work applies 
distributional similarity (Pasca, 2007; Pantel et al 
2009), uses a few linguistic patterns (Pasca 2004; 
Sarmento et al 2007), makes use of structure in 
webpages (Wang and Cohen 2007, 2009), or com-
bines all of them (Shi et al 2010). Pennacchiotti 
and Pantel (2009) combines several sources and 
features. To find similar phrases, there are 2 close-
ly related tasks: paraphrase discovery and recog-
nizing textual entailment. Data-driven paraphrase 
discovery methods (Lin and Pantel, 2001; Pasca 
and Dienes, 2005; Wu and Zhou, 2003; Sekine, 
2005) extends the idea of distributional similarity 
to phrases. The Recognizing Textual Entailment 
algorithms (Berant et al2011) can also be used to 
find related phrases since they find pairs of phrases 
in which one entails the other.  
To efficiently cluster high-dimensional datasets, 
canopy clustering (McCallum et al 2000) uses a 
cheap, approximate distance measure to divide da-
ta into smaller subsets, and then cluster each subset 
using an exact distance measure. It has been ap-
plied to reference matching. The second phase of 
WEBRE applies the similar high-level idea of par-
tition-then-cluster for speeding up relation cluster-
ing. We design a graph-based partitioning 
subroutine that uses various types of evidence, 
such as shared hypernyms.  
3 Problem Analysis 
The basic input is a collection of relation instances 
(triples) of the form <ent1, ctx, ent2>. For each tri-
ple, ctx is a relational phrase expressing the rela-
tion between the first argument ent1 and the second 
argument ent2. An example triple is <Obama, win 
in, NY>. The triples can be generated by an open 
IE extractor such as TextRunner or Reverb. Our 
goal is to automatically build a list of relations 
? = {< ent1, ???, ent2 >} ? 3 < ?1,?,?2 >  where P 
is the set of relation phrases, and ?1  and  ?2  are 
two argument classes. Examples of triples and rela-
tions R (as Type B) are shown in Figure 1. 
                                                          
3 This approximately equal sign connects 2 possible represen-
tations of a relation: as a set of triple instances or a triple with 
2 entity classes and a relation phrase class. 
The first problem is the polysemy of relation 
phrases, which means that a relation phrase ctx can 
express different relations in different triples. For 
example, the meaning of be the currency of in the 
following two triples is quite different: <Euro, be 
the currency of, Germany> and <authorship, be 
the currency of, science>. It is more appropriate to 
assign these 2 triples to 2 relations ?a currency is 
the currency of a country? and ?a factor is im-
portant in an area? than to merge them into one. 
Formally, a relation phrase ctx is polysemous if 
there exist 2 different relations < ?1,?,?2 >  and 
< ?1
?
,??,?2
?
> where ??? ? ? ? ??. In the previ-
ous example, be the currency of  is polysemous 
because it appears in 2 different relations.  
Polysemy of relation phrases is not uncommon. 
We generate clusters from a large sample of triples 
with the assistance of a soft clustering algorithm, 
and found that around 22% of relation phrases can 
be put into at least 2 disjoint clusters that represent 
different relations. More importantly, manual in-
spection reveals that some common phrases are 
polysemous. For example, be part of can be put 
into a relation ?a city is located in a country? when 
connecting Cities to Countries, and another rela-
tion ?a company is a subsidiary of a parent com-
pany? when connecting Companies to Companies. 
Failure to handle polysemous relation phrases fun-
damentally limits the effectiveness of an algorithm. 
The WEBRE algorithm described later explicitly 
handles polysemy and synonymy of relation 
phrases in its first and second phase respectively. 
The second problem is the ?synonymy? of rela-
tion instances. We use the term synonymy broadly 
and we say 2 relation instances are synonymous if 
they express the same semantic relation between 
the same pair of semantic classes. For example, 
both <Euro, be the currency used in, Germany> 
and <Dinar, be legal tender in, Iraq> express the 
relation <Currencies, be currency of, Countries>. 
Solving this problem requires grouping synony-
mous relation phrases and identifying argument 
semantic classes for the relation.  
Various knowledge sources can be derived from 
the source corpus for this purpose. In this paper we 
pay special attention to incorporating various se-
mantic resources for relation extraction. We will 
show that these semantic sources can significantly 
improve the coverage of extracted relations and the  
 
1029
Figure 1. Overview of the WEBRE algorithm (Illustrated with examples sampled from experiment results). The tables and rec-
tangles with a database sign show knowledge sources, shaded rectangles show the 2 phases, and the dotted shapes show the sys-
tem output, a set of Type A relations and a set of Type B relations. The orange arrows denote resources used in phase 1 and the 
green arrows show the resources used in phase 2. 
 
best performance is achieved when various re-
sources are combined together.  
4 Mining Relations from the Web 
We first describe relevant knowledge sources, and 
then introduce the WEBRE algorithm, followed by 
a briefly analysis on its computational complexity.  
4.1 Knowledge Sources 
Entity similarity graph We build two similarity 
graphs for entities: a distributional similarity (DS) 
graph and a pattern-similarity (PS) graph. The DS 
graph is based on the distributional hypothesis 
(Harris, 1985), saying that terms sharing similar 
contexts tend to be similar. We use a text window 
of size 4 as the context of a term, use Pointwise 
Mutual Information (PMI) to weight context fea-
tures, and use Jaccard similarity to measure the 
similarity of term vectors. The PS graph is gener-
ated by adopting both sentence lexical patterns and 
HTML tag patterns (Hearst, 1992; Kozareva et al 
2008; Zhang et al 2009; Shi et al 2010). Two 
terms (T) tend to be semantically similar if they co-
occur in multiple patterns. One example of sen-
tence lexical patterns is (such as | including) 
T{,T}* (and|,|.). HTML tag patterns include tables, 
dropdown boxes, etc. In these two graphs, nodes 
are entities and the edge weights indicate entity 
similarity. In all there are about 29.6 million nodes 
and 1.16 billion edges. 
Hypernymy graph Hypernymy relations are 
very useful for finding semantically similar term 
pairs. For example, we observed that a small city 
in UK and another small city in Germany share 
common hypernyms such as city, location, and 
place. Therefore the similarity between the two 
cities is large according to the hypernymy graph, 
while their similarity in the DS graph and the PS 
graph may be very small. Following existing work 
(Hearst, 1992, Pantel & Ravichandran 2004; Snow 
et al 2005; Talukdar et al 2008; Zhang et al 
2011), we adopt a list of lexical patterns to extract 
hypernyms. The patterns include NP {,} (such as) 
{NP,}* {and|or} NP, NP (is|are|was|were|being) 
(a|an|the) NP, etc. The hypernymy graph is a bi-
partite graph with two types of nodes: entity nodes 
and label (hypernym) nodes. There is an edge (T, 
L) with weight w if L is a hypernym of entity T 
with probability w. There are about 8.2 million 
nodes and 42.4 million edges in the hypernymy 
graph. In this paper, we use the terms hypernym 
and label interchangeably. 
Relation phrase similarity: To generate the pair-
wise similarity graph for relation phrases with re-
gard to the probability of expressing the same 
relation, we apply a variant of the DIRT algorithm 
(Lin and Pantel, 2001). Like DIRT, the paraphrase 
discovery relies on the distributional hypothesis, 
but there are a few differences: 1) we use stemmed 
lexical sequences (relation phrases) instead of de-
pendency paths as phrase candidates because of the 
very large scale of the corpus. 2) We used ordered 
1030
pairs of arguments as features of phrases while 
DIRT uses them as independent features. We em-
pirically tested both feature schemes and found 
that using ordered pairs results in likely para-
phrases but using independent features the result 
contains general inference rules4. 
4.2 WEBRE for Relation Extraction 
WEBRE consists of two phases. In the first 
phase, a set of semantic classes are discovered and 
used as argument classes for each relation phrase. 
This results in a large collection of relations whose 
arguments are pairs of semantic classes and which 
have exactly one relation phrase. We call these 
relations the Type A relations. An example Type A 
relation is <{New York, London?}, be locate in, 
{USA, England, ?}>. During this phase, polyse-
mous relation phrases are disambiguated and 
placed into multiple Type A relations. The second 
phase is an efficient algorithm which groups simi-
lar Type A relations together. This step enriches 
the argument semantic classes and groups synon-
ymous relation phrases to form relations with mul-
tiple expressions, which we called Type B 
relations. Both Type A and Type B relations are 
system outputs since both are valuable resources 
for downstream applications such as QA and Web 
Search. An overview of the algorithm is shown in 
Figure 1. Here we first briefly describe a clustering 
subroutine that is used in both phases, and then 
describe the algorithm in detail. 
To handle polysemy of objects (e.g., entities or 
relations) during the clustering procedure, a key 
building block is an effective Multi-Membership 
Clustering algorithm (MMClustering). For simplic-
ity and effectiveness, we use a variant of Hierar-
chical Agglomerative Clustering (HAC), in which 
we first cluster objects with HAC, and then reas-
sign each object to additional clusters when its 
similarities with these clusters exceed a certain 
threshold5. In the remainder of this paper, we use 
{C} = MMClustering({object}, SimFunc, ?) to rep-
resent running MMClustering over a set of objects, 
                                                          
4 For example, be part of  has ordered argument pairs <A, B> 
and <C, D>, and be not part of has ordered argument pairs 
<A, D> and <B, C>. If arguments are used as independent 
features, these two phrases shared the same set of features {A, 
B, C, D}. However, they are inferential (complement relation-
ship) rather than being similar phrases. 
5 This threshold should be slightly greater than the clustering 
threshold for HAC to avoid generating duplicated clusters. 
with threshold ? to generate a list of clusters {C} of 
the objects, given the pairwise object similarity 
function SimFunc. Our implementation uses HAC 
with average linkage since empirically it performs 
well. 
Discovering Type A Relations The first phase 
of the relation extraction algorithm generates Type 
A relations, which have exactly one relation phrase 
and two argument entity semantic classes. For each 
relation phrase, we apply a clustering algorithm on 
each of its two argument sets to generate argument 
semantic classes. The Phase 1 algorithm processes 
relation phrases one by one. For each relation 
phrase ctx, step 4 clusters the set {ent1} using 
MMClustering to find left-hand-side argument se-
mantic classes {C1}. Then for each cluster C in 
{C1}, it gathers the right-hand-side arguments 
which appeared in some triple whose left hand-
side-side argument is in C, and puts them into 
{ent2?}. Following this, it clusters {ent2?} to find 
right-hand-side argument semantic classes. This 
results in pairs of semantic classes which are ar-
guments of ctx. Each relation phrase can appear in 
multiple non-overlapping Type A relations. For 
example, <Cities, be part of, Countries> and 
<Companies, be part of, Companies> are different 
Type A relations which share the same relation 
phrase be part of. In the pseudo code, SimEntFunc 
is encoded in the entity similarity graphs.  
 
Algorithm Phase 1: Discovering Type A relations 
Input:  set of triples T={<ent1, ctx, ent2>} 
 entity similarity function SimEntFunc 
 Similarity threshold ? 
Output:  list of Type A relations {<C1, ctx, C2>} 
Steps:  
01. For each relation phrase ctx 
02.     {ent1, ctx, ent2} = set of triples sharing ctx 
03.     {ent1} = set of ent1 in {ent1, ctx, ent2} 
04.     {C1} = MMClustering({ent1}, SimEntFunc, ?) 
05.     For each C in { C1} 
06.         {ent2?} = set of ???2 ?. ?.?< ???1, ???, ???2 > ?
 ? ? ???1 ? ?1 
07.         {C2} = MMClustering({ent2?}, SimEntFunc, ?) 
08.         For each C2 in {C2} 
09.             Add <C1, ctx, C2> into {<C1, ctx, C2>} 
10. Return {<C1, ctx, C2>} 
 
    Discovering Type B Relations  The goal of 
phase 2 is to merge similar Type A relations, such 
as <Cities, be locate in, Countries> and <Cities, 
be city of, Countries>, to produce Type B relations, 
which have a set of synonymous relation phrases 
and more complete argument entity classes. The 
challenge for this phase is to cluster a very large 
1031
set of Type A relations, on which it is infeasible to 
run a clustering algorithm that does pairwise all 
pair comparison. Therefore, we designed an evi-
dence-based partition-then-cluster algorithm. 
The basic idea is to heuristically partition the 
large set of Type A relations into small subsets, 
and run clustering algorithms on each subset. It is 
based on the observation that most pairs of Type A 
relations are not similar because of the sparseness 
in the entity class and the relation semantic space. 
If there is little or no evidence showing that two 
Type A relations are similar, they can be put into 
different partitions. Once partitioned, the clustering 
algorithm only has to be run on each much smaller 
subset, thus computation complexity is reduced.  
The 2 types of evidence we used are shared 
members and shared hypernyms of relation argu-
ments. For example, 2 Type A relations 
r1=<Cities, be city of, Countries> and r2=<Cities, 
be locate in, Countries> share a pair of arguments 
<Tokyo, Japan>, and a pair of hypernyms <?city?, 
?country?>. These pieces of evidence give us hints 
that they are likely to be similar. As shown in the 
pseudo code, shared arguments and hypernyms are 
used as independent evidence to reduce sparseness. 
 
Algorithm Phase 2: Discovering Type B relations 
Input:  Set of Type A relations {r}={<C1, ctx, C2>} 
 Relation similarity function SimRelationFunc 
 Map from entities to their hypernyms: Mentity2label 
 Similarity threshold ? 
Edge weight threshold ? 
Variables G(V, E) = weighted graph in which V={r} 
Output:  Set of Type B relations {<C1, P, C2>} 
Steps:  
01. {<ent, {r?}>} = build  inverted index from argument 
ent to set of Type A relations {r?} on {<C1, ctx, C2>}  
02 {<l, {r?}>} = build  inverted index from hypernym l 
of arguments to set of Type A relations {r?} on {<C1, 
ctx, C2>} with map Mentity2label  
03. For each ent in {<ent, {r?}>} 
04.     For each pair of r1 and r2  s.t. ?1 ? {?
?} ? ?2 ? {??}    
05.        weight_edge(<r1, r2>) += weight (ent) 
06. For each l in {<l, {r?}>} 
07.     For each pair of r1 and r2  s.t. ?1 ? {?
?} ? ?2 ? {??}    
08.        weight_edge(<r1, r2>) += weight (l) 
09. For each edge <r1, r2> in G 
10.     If weight_edge(<r1, r2>) < ? 
11.         Remove edge <r1, r2> from G 
12. {CC}= DFS(G) 
13. For each connected component CC in {CC} 
14.     {<C1, ctx, C2>} = vertices in CC 
15. {<C1?, P?, C2?>} = MMClustering({<C1, ctx, C2>},  
  SimRelationFunc, ?) 
16.     Add {<C1?, P?, C2?>} into {<C1, P, C2>} 
17. Return {<C1, P, C2>} 
 
Steps 1 and 2 build an inverted index from evi-
dence to sets of Type A relations. On the graph G 
whose vertices are Type A relations, steps 3 to 8 
set the value of edge weights based on the strength 
of evidence that shows the end-points are related. 
The weight of evidence E is calculated as follows: 
 
??????(?) =
# ?????? ?????? ?? ????? ? ??????? ?? 
max(# ??????? ? ??????? ??)
 
 
The idea behind this weighting scheme is similar 
to that of TF-IDF in that the weight of evidence is 
higher if it appears more frequently and is less am-
biguous (appeared in fewer semantic classes during 
clustering of phase 1). The weighting scheme is 
applied to both shared arguments and labels. 
After collecting evidence, we prune (steps 9 to 
11) the edges with a weight less than a threshold ? 
to remove noise. Then a Depth-First Search (DFS) 
is called on G to find all Connected Components 
CC of the graph. These CCs are the partitions of 
likely-similar Type A relations. We run MMClus-
tering on each CC in {CC} and generate Type B  
relations (step 13 to step 16).  The similarity of 2 
relations (SimRelationFunc) is defined as follows: 
???(< ?1,?,?2 >, < ?1
?,??,?2
? >) 
 
= ?
0,     ?? ???(?,??) <  ?
min????(?1,?1
?), ???(?2,?2
?)? ,   ???? 
  
4.3 Computational Complexity 
WEBRE is very efficient since both phases de-
compose the large-clustering task into much small-
er clustering tasks over partitions. Given n objects 
for clustering, a hierarchical agglomerative cluster-
ing algorithm requires ?(?2)  pairwise compari-
sons. Assuming the clustering task is split into 
subtasks of size ?1, ?2, ?, ??, thus the computa-
tional complexity is reduced to ?(? ??
2?
1 ). Ideally 
each subtask has an equal size of ?/?, so the com-
putational complexity is reduced to O(?2/?) , a 
factor of ? speed up. In practice, the sizes of parti-
tions are not equal. Taking the partition sizes ob-
served in the experiment with 0.2 million Type A 
relations as input, the phase 2 algorithm achieves 
around a 100-fold reduction in pairwise compari-
sons compared to the agglomerative clustering al-
gorithm. The combination of phase 1 and phase 2 
achieves more than a 1000-fold reduction in pair-
wise comparison, compared to running an agglom-
erative clustering algorithm directly on 14.7 
million triples. This reduction of computational 
1032
complexity makes the unsupervised extraction of 
relations on a large dataset a reality. In the experi-
ments with 14.7 million triples as input, phase 1 
finished in 22 hours, and the phase 2 algorithm 
finished in 4 hours with one CPU core. 
Furthermore, both phases can be run in parallel 
in a distributed computing environment because 
data is partitioned. Therefore it is scalable and effi-
cient for clustering a very large number of relation 
instances from a large-scale corpus like the web.  
5 Experiment 
Data preparation We tested WEBRE on re-
sources extracted from the English subset of the 
Clueweb09 Dataset, which contains 503 million 
webpages. For building knowledge resources, all 
webpages are cleaned and then POS tagged and 
chunked with in-house tools. We implemented the 
algorithms described in section 4.1 to generate the 
knowledge sources, including a hypernym graph, 
two entity similarity graphs and a relation phrase 
similarity graph. 
We used Reverb Clueweb09 Extractions 1.1 
(downloaded from reverb.cs.washington.edu) as 
the triple store (relation instances). It is the com-
plete extraction of Reverb over Clueweb09 after 
filtering low confidence and low frequency triples. 
It contains 14.7 million distinct triples with 3.3 
million entities and 1.3 million relation phrases. 
We choose it because 1) it is extracted by a state-
of-the-art open IE extractor from the open-domain, 
and 2) to the best of our knowledge, it contains the 
largest number of distinct triples extracted from the 
open-domain and which is publicly available. 
 
Evaluation setup The evaluations are organized as 
follows: we evaluate Type A relation extraction 
and Type B relation extraction separately, and then 
we compare WEBRE to its closest prior work 
SNE.  Since both phases are essentially clustering 
algorithms, we compare the output clusters with 
human labeled gold standards and report perfor-
mance measures, following most previous work 
such as Kok and Domingos (2008) and Hasegawa 
et al(2004). Three gold standards are created for 
evaluating Type A relations, Type B relations and 
the comparison to SNE, respectively. In the exper-
iments, we set ?=0.6, ?=0.1 and ?=0.02 based on 
trial runs on a small development set of 10k rela-
tion instances. We filtered out the Type A relations 
and Type B relations which only contain 1 or 2 
triples since most of these relations are not differ-
ent from a single relation instance and are not very 
interesting. Overall, 0.2 million Type A relations 
and 84,000 Type B relations are extracted. 
 
Evaluating Type A relations To understand the 
effectiveness of knowledge sources, we run Phase 
1 multiple times taking entity similarity graphs 
(matrices) constructed with resources listed below: 
? TS: Distributional similarity based on the triple 
store. For each triple <ent1, ctx, ent2>, features 
of ent1 are {ctx} and {ctx ent2}; features of ent2 
are {ctx} and {ent1 ctx}. Features are weighted 
with PMI. Cosine is used as similarity measure.  
? LABEL: The similarity between two entities is 
computed according to the percentage of top 
hypernyms they share. 
? SIM: The similarity between two entities is the 
linear combination of their similarity scores in 
the distributional similarity graph and in the 
pattern similarity graph. 
? SIM+LABEL SIM and LABEL are combined. 
Observing that SIM generates high quality but 
overly fine-grained semantic classes, we modify 
the entity clustering procedure to cluster argu-
ment entities based on SIM first, and then fur-
ther clustering the results based on LABEL. 
The outputs of these runs are pooled and mixed 
for labeling. We randomly sampled 60 relation 
phrases. For each phrase, we select the 5 most fre-
quent Type A relations from each run (4?5=206 
Type A relations in all). For each relation phrase, 
we ask a human labeler to label the mixed pool of 
Type A relations that share the phrase: 1) The la-
belers7 are asked to first determine the major se-
mantic relation of each Type A relation, and then 
label the triples as good, fair or bad based on 
whether they express the major relation. 2) The 
labeler also reads all Type A relations and manual-
ly merges the ones that express the same relation. 
These 2 steps are repeated for each phrase. After 
labeling, we create a gold standard GS1, which 
contains roughly 10,000 triples for 60 relation 
phrases. On average, close to 200 triples are manu-
                                                          
6  Here 4 means the 4 methods (the bullet items above) of 
computing similarity. 
7 4 human labelers perform the task. A portion of the judg-
ments were independently dual annotated; inter-annotator 
agreement is 79%. Moreover, each judgment is cross-checked 
by at least one more annotator, further improving quality. 
1033
ally labeled and clustered for each phrase. This 
creates a large data set for evaluation.  
We report micro-average of precision, recall and 
F1 on the 60 relation phrases for each method. Pre-
cision (P) and Recall (R) of a given relation phrase 
is defined as follows. Here ?? and ??
?  represents a 
Type A relation in the algorithm output and GS1, 
respectively. We use t for triples and s(t) to repre-
sent the score of the labeled triple t. s(t) is set to 
1.0, 0.5 or 0 for t labeled as good, fair and bad, 
respectively. 
 
? =
? ? ?(?) ????  ??
? |??|??
, ? =
? ? ?(?) ????  ??
? ? ?(??) ?????
???
?
 
 
The results are in table 1. Overall, LABEL per-
forms 53% better than TS in F-measure, and 
SIM+LABEL performs the best, 8% better than 
LABEL. Applying a simple sign test shows both 
differences are clearly significant (p<0.001). Sur-
prisingly, SIM, which uses the similarity matrix 
extracted from full text, has a F1 of 0.277, which is 
lower than TS. We also tried combining TS and 
LABEL but did not find encouraging performance 
compared to SIM+LABEL. 
 
Algorithm Precision Recall F1 
TS 0.842 (0.886) 0.266 0.388 
LABEL 0.855 (0.870) 0.481 0.596 
SIM 0.755 (0.964) 0.178 0.277 
SIM+LABEL 0.843 (0.872) 0.540 0.643 
 
Table 1. Phase 1 performance (averaged on multiple runs) of 
the 4 methods. The highest performance numbers are in bold. 
(The number in parenthesis is the micro-average when empty-
result relation phrases are not considered for the method). 
 
Among the 4 methods, SIM has the highest preci-
sion (0.964) when relation phrases for which it 
fails to generate any Type A relations are exclud-
ed, but its recall is low. Manual checking shows 
that SIM tends to generate overly fine-grained ar-
gument classes. If fine-grained argument classes or 
extremely high-precision Type A relations are pre-
ferred, SIM is a good choice. LABEL performs 
significantly better than TS, which shows that hy-
pernymy information is very useful for finding ar-
gument semantic classes. However, it has coverage 
problems in that the hypernym finding algorithm 
failed to find any hypernym from the corpus for 
some entities. Following up, we found that 
SIM+LABEL has similar precision and the highest 
recall. This shows that the combination of semantic 
spaces is very helpful. The significant recall im-
provement from TS to SIM+LABEL shows that 
the corpus-based knowledge resources significant-
ly reduce the data sparseness, compared to using 
features extracted from the triple store only. The 
result of the phase 1 algorithm with SIM+LABEL 
is used as input for phase 2. 
 
Evaluating Type B relations The goal is 2-fold: 
1) to evaluate the phase 2 algorithm. This involves 
comparing system output to a gold standard con-
structed by hand, and reporting performance; 2) to 
evaluate the quality of Type B relations. For this, 
we will also report triple-level precision. 
    We construct a gold standard GS28 for evaluat-
ing Type B relations as follows: We randomly 
sampled 178 Type B relations, which contain 1547 
Type A relations and more than 100,000 triples. 
Since the number of triples is very large, it is in-
feasible for labelers to manually cluster triples to 
construct a gold standard. To report precision, we 
asked the labelers to label each Type A relation 
contained in this Type B relation as good, fair or 
bad based on whether it expresses the same rela-
tion. For recall evaluation, we need to know how 
many Type A relations are missing from each Type 
B relation. We provide the full data set of Type A 
relations along with three additional resources: 1) a 
tool which, given a Type A relation, returns a 
ranked list of similar Type A relations based on the 
pairwise relation similarity metric in section 4, 2) 
DIRT paraphrase collection, 3) WordNet (Fell-
baum, 1998) synsets. The labelers are asked to find 
similar phrases by checking phrases which contain 
synonyms of the tokens in the query phrase. Given 
a Type B relation, ideally we expect the labelers to 
find all missing Type A relations using these re-
sources. We report precision (P) and recall (R) as 
follows. Here ??  and ??
?  represent Type B rela-
tions in the algorithm output and GS2, respective-
ly. ??  and ??
?  represent Type A relations. ?(??) 
denotes the score of ??. It is set to 1.0, 0.5 and 0 
for good, fair or bad respectively.  
 
? =
? ? |??|??(??) ???????
? ? |??|  ???????
, ? =
? ? |??|??(??)  ???????
? ? ???
? ???
? ???
???
?
 
 
We also ask the labeler to label at most 50 ran-
domly sampled triples from each Type B relation, 
and calculate triple-level precision as the ratio of 
the sum of scores of triples over the number of  
                                                          
8 3 human labelers performed the task. A portion of the judg-
ments were independently dual annotated; inter-annotator 
agreement is 73%. Similar to labeling Type A relations, each 
judgment is cross-checked by at least one more annotator, 
further improving quality. 
1034
Argument 1 Relation phrase Argument 2 
marijuana, caffeine, nicotine? result in, be risk factor for, be major cause of? insomnia, emphysema, breast cancer,? 
C# 2.0, php5, java, c++, ? allow the use of, also use, introduce the concept of? destructors, interfaces, template,? 
clinton, obama, mccain, ? win, win in, take, be lead in,? ca, dc, fl, nh, pa, va, ga, il, nc,? 
Table 3. Sample Type B relations extracted. 
 
sampled triples. We use ???? to represent the preci-
sion calculated based on labeled triples. Moreover, 
as we are interested in how many phrases are 
found by our algorithm, we also include ???????, 
which is the recall of synonymous phrases. Results 
are shown in Table 2.  
 
Interval P R (???????) F1 ???? count 
[3, 5) 0.913 0.426 (0.026) 0.581 0.872 52149 
[5, 10) 0.834 0.514 (0.074) 0.636 0.863 21981 
[10, 20) 0.854 0.569 (0.066) 0.683 0.883 6277 
[20, 50) 0.899 0.675 (0.406) 0.771 0.894 2630 
[50, +?) 0.922 0.825 (0.594) 0.871 0.929 1089 
Overall 0.897 0.684 (0.324) 0.776 0.898 84126 
Table 2. Performance for Type B relation extraction. The first 
column shows the range of the maximum sizes of Type A 
relations in the Type B relation. The last column shows the 
number of Type B relations that are in this range. The number 
in parenthesis in the third column is the recall of phrases.  
 
The result shows that WEBRE can extract Type B 
relations at high precision (both P and ????). The 
overall recall is 0.684. Table 2 also shows a trend 
that if the maximum number of Type A relation in 
the target Type B relation is larger, the recall is 
better. This shows that the recall of Type B rela-
tions depends on the amount of data available for 
that relation. Some examples of Type B relations 
extracted are shown in Table 3. 
  
Comparison with SNE We compare WEBRE?s 
extracted Type B relations to the relations extract-
ed by its closest prior work SNE9. We found SNE 
is not able to handle the 14.7 million triples in a 
foreseeable amount of time, so we randomly sam-
pled 1 million (1M) triples 10 and test both algo-
rithms on this set. We also filtered out result 
clusters which have only 1 or 2 triples from both 
system outputs. For comparison purposes, we con-
structed a gold standard GS3 as follows: randomly 
select 30 clusters from both system outputs, and 
then find similar clusters from the other system 
output, followed by manually refining the clusters 
                                                          
9 Obtained from alchemy.cs.washington.edu/papers/kok08 
10 We found that SNE?s runtime on 1M triples varies from 
several hours to over a week, depending on the parameters. 
The best performance is achieved with runtime of approxi-
mately 3 days. We also tried SNE with 2M triples, on which 
many runs take several days and show no sign of convergence. 
For fairness, the comparison was done on 1M triples. 
by merging similar ones and splitting non-coherent 
clusters. GS3 contains 742 triples and 135 clusters. 
We report triple-level pairwise precision, recall 
and F1 for both algorithms against GS3, and report 
results in Table 4. We fine-tuned SNE (using grid 
search, internal cross-validation, and coarse-to-fine 
parameter tuning), and report its best performance. 
 
Algorithm Precision Recall F1 
WEBRE 0.848 0.734 0.787 
SNE 0.850 0.080 0.146 
 
Table 4. Pairwise precision/recall/F1 of WEBRE and SNE.  
 
Table 4 shows that WEBRE outperforms SNE 
significantly in pairwise recall while having similar 
precision. There are two reasons. First, WEBRE 
makes use of several corpus-level semantic sources 
extracted from the corpus for clustering entities 
and phrases while SNE uses only features in the 
triple store. These semantic resources significantly 
reduced data sparseness. Examination of the output 
shows that SNE is unable to group many triples 
from the same generally-recognized fine-grained 
relations. For example, SNE placed relation in-
stances <Barbara, grow up in, Santa Fe> and 
<John, be raised mostly in, Santa Barbara> into 2 
different clusters because the arguments and 
phrases do not share features nor could be grouped 
by SNE?s mutual clustering. In contrast, WEBRE 
groups them together. Second, SNE assumes a re-
lation phrase to be in exactly one cluster. For ex-
ample, SNE placed be part of in the phrase cluster 
be city of and failed to place it in another cluster be 
subsidiary of. This limits SNE?s ability to placing 
relation instances with polysemous phrases into 
correct relation clusters. 
It should be emphasized that we use pairwise 
precision and recall in table 4 to be consistent with 
the original SNE paper. Pairwise metrics are much 
more sensitive than instance-level metrics, and pe-
nalize recall exponentially in the worst case11 if an 
algorithm incorrectly splits a coherent cluster; 
therefore the absolute pairwise recall difference 
                                                          
11 Pairwise precision and recall are calculated on all pairs that 
are in the same cluster, thus are very sensitive. For example, if 
an algorithm incorrectly split a cluster of size N to a smaller 
main cluster of size N/2 and some constant-size clusters, pair-
wise recall could drop to as much as ? of its original value. 
1035
should not be interpreted as the same as the in-
stance-level recall reported in previous experi-
ments. On 1 million triples, WEBRE generates 
12179 triple clusters with an average size12 of 13 
while SNE generate 53270 clusters with an aver-
age size 5.1. In consequence, pairwise recall drops 
significantly. Nonetheless, at above 80% pairwise 
precision, it demonstrates that WEBRE can group 
more related triples by adding rich semantics har-
vested from the web and employing a more general 
treatment of polysemous relation phrases.  On 1M 
triples, WEBRE finished in 40 minutes, while the 
run time of SNE varies from 3 hours to a few days. 
6 Conclusion 
We present a fully unsupervised algorithm 
WEBRE for large-scale open-domain relation ex-
traction. WEBRE explicitly handles polysemy rela-
tions and achieves a significant improvement on 
recall by incorporating rich corpus-based semantic 
resources. Experiments on a large data set show 
that it can extract a very large set of high-quality 
relations. 
 
Acknowledgements 
Supported in part by the Intelligence Advanced 
Research Projects Activity (IARPA) via Air Force 
Research Laboratory (AFRL) contract number 
FA8650-10-C-7058. The U.S. Government is au-
thorized to reproduce and distribute reprints for 
Governmental purposes notwithstanding any copy-
right annotation thereon. The views and conclu-
sions contained herein are those of the authors and 
should not be interpreted as necessarily represent-
ing the official policies or endorsements, either 
expressed or implied, of IARPA, AFRL, or the 
U.S. Government. 
 
References 
Michele Banko, Michael J. Cafarella, Stephen Soder-
land, Matt Broadhead, and Oren Etzioni. 2007. Open 
Information Extraction from the Web. In Proceedings 
of IJCAI 2007. 
                                                          
12 The clusters which have only 1 or 2 triples are removed and 
not counted here for both algorithms. 
Michele Banko and Oren Etzioni. 2008. The Tradeoffs 
Between Open and Traditional Relation Extraction. 
In Proceedings of ACL 2008. 
Jonathan Berant, Ido Dagan and Jacob Goldberger. 
2011. Global Learning of Typed Entailment Rules. In 
Proceedings of ACL 2011. 
Razvan Bunescu and Raymond J. Mooney. 2004. Col-
lective Information Extraction with Relational Mar-
kov Networks. In Proceedings of ACL 2004. 
Jinxiu Chen, Donghong Ji, Chew Lim Tan, Zhengyu 
Niu. 2005. Unsupervised Feature Selection for Rela-
tion Extraction. In Proceedings of IJCNLP 2005. 
Oren Etzioni, Michael Cafarella, Doug Downey, Stan-
ley Kok, Ana-Maria Popescu, Tal Shaked, Stephen 
Soderland, Daniel S. Weld, and Alexander Yates. 
2004. Web-scale information extraction in 
KnowItAll (preliminary results). In Proceedings of 
WWW 2004. 
Oren Etzioni, Michael Cafarella, Doug Downey, 
AnaMaria Popescu, Tal Shaked, Stephen Soderland, 
Daniel S. Weld and Alexander Yates. 2005. Unsu-
pervised named-entity extraction from the Web: An 
Experimental Study. In Artificial Intelligence, 
165(1):91-134. 
Anthony Fader, Stephen Soderland, and Oren Etzioni. 
2011. Identifying Relations for Open Information Ex-
traction. In Proceedings of EMNLP 2011. 
Christiane Fellbaum (Ed.). 1998. WordNet: An Elec-
tronic Lexical Database. Cambridge, MA: MIT Press. 
Zelig S. Harris. 1985. Distributional Structure. The Phi-
losophy of Linguistics. New York: Oxford Uni-
versity Press. 
Takaaki Hasegawa, Satoshi Sekine, Ralph Grishman . 
2004.Discovering Relations among Named Entities 
from Large Corpora. In Proceedings of ACL 2004. 
Marti A. Hearst. 1992. Automatic  Acquisition of  Hy-
ponyms from Large Text Corpora. In Proceedings of 
COLING 1992. 
Stanley Kok and Pedro Domingos. 2008. Extracting 
Semantic Networks from Text via Relational Cluster-
ing. In Proceedings of ECML 2008. 
Zornitsa Kozareva, Ellen Riloff, Eduard Hovy. 2008. 
Semantic Class Learning from the Web with Hypo-
nym Pattern Linkage Graphs. In Proceedings of ACL 
2008. 
Dekang Lin and Patrick Pantel. 2001. DIRT ? Discov-
ery of Inference Rules from Text. In Proceedings of 
KDD 2001. 
Andrew McCallum, Kamal Nigam and Lyle Ungar. 
2000. Efficient Clustering of High-Dimensional Data 
Sets with Application to Reference Matching. In Pro-
ceedings of KDD 2000. 
Patrick Pantel, Eric Crestan, Arkady Borkovsky, Ana-
Maria Popescu and Vishnu Vyas. 2009. Web-Scale 
Distributional Similarity and Entity Set Expansion. In 
Proceedings of EMNLP 2009. 
1036
Patrick Pantel and Dekang Lin. 2002. Discovering word 
senses from text. In Proceedings of KDD2002. 
Patrick Pantel and Deepak Ravichandran. 2004. Auto-
matically Labeling Semantic Classes. In Proceedings 
of HLT/NAACL-2004. 
Marius Pasca. 2004. Acquisition of Categorized Named 
Entities for Web Search, In Proceedings of CIKM 
2004. 
Marius Pasca. 2007. Weakly-supervised discovery of 
named entities using web search queries. In Proceed-
ings of CIKM 2007. 
Marius Pasca and Peter Dienes. 2005. Aligning needles 
in a haystack: Paraphrase acquisition across the Web. 
In Proceedings of IJCNLP 2005. 
Marco Pennacchiotti and Patrick Pantel. 2009. Entity 
Extraction via Ensemble Semantics. In Proceedings 
of EMNLP 2009. 
Benjamin Rosenfeld and Ronen Feldman. 2007. Clus-
tering for Unsupervised Relation Identification. In 
Proceedings of CIKM 2007. 
Luis Sarmento, Valentin Jijkoun, Maarten de Rijke and 
Eugenio Oliveira. 2007. ?More like these?: growing 
entity classes from seeds. In Proceedings of CIKM 
2007. 
Satoshi Sekine. 2005. Automatic paraphrase discovery 
based on context and keywords between NE pairs. In 
Proceedings of the International Workshop on Para-
phrasing, 2005. 
Shuming Shi, Huibin Zhang, Xiaojie Yuan, Ji-Rong 
Wen. 2010. Corpus-based Semantic Class Mining: 
Distributional vs. Pattern-Based Approaches. In Pro-
ceedings of COLING 2010. 
Yusuke Shinyama, Satoshi Sekine. 2006. Preemptive 
Information Extraction using Unrestricted Relation 
Discovery, In Proceedings of NAACL 2006. 
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2005. 
Learning Syntactic Patterns for Automatic Hypernym 
Discovery. In Proceedings of  In NIPS 17, 2005. 
Stephen Soderland and Bhushan Mandhani. 2007. Mov-
ing from Textual Relations to Ontologized Relations. 
In Proceedings of the 2007 AAAI Spring Symposium 
on Machine Reading. 
Partha Pratim Talukdar, Joseph Reisinger, Marius Pasca, 
Deepak Ravichandran, Rahul Bhagat and Fernando 
Pereira. 2008. Weakly-Supervised Acquisition of La-
beled Class Instances using Graph Random Walks. In 
Proceedings of EMNLP 2008. 
David Vickrey, Oscar Kipersztok and Daphne Koller. 
2010. An Active Learning Approach to Finding Re-
lated Terms. In Proceedings of ACL 2010. 
Vishnu Vyas and Patrick Pantel. 2009. SemiAutomatic 
Entity Set Refinement. In Proceedings of 
NAACL/HLT 2009. 
Vishnu Vyas, Patrick Pantel and Eric Crestan. 2009, 
Helping Editors Choose Better Seed Sets for Entity 
Set Expansion, In Proceedings of CIKM 2009. 
Richard C. Wang and William W. Cohen. 2007. Lan-
guage- Independent Set Expansion of Named Entities 
Using the Web. In Proceedings of ICDM 2007. 
Richard C. Wang and William W. Cohen. 
2009. Automatic Set Instance Extraction using the 
Web. In Proceedings of ACL-IJCNLP 2009. 
Wei Wang, Romaric Besan?on and Olivier Ferret. 2011. 
Filtering and Clustering Relations for Unsupervised 
Information Extraction in Open Domain. In Proceed-
ings of CIKM 2011. 
Fei Wu and Daniel S. Weld. 2010. Open information 
extraction using Wikipedia. In Proceedings of ACL 
2010. 
Hua Wu and Ming Zhou. 2003. Synonymous colloca-
tion extraction using translation information. In Pro-
ceedings of the ACL Workshop on Multiword 
Expressions: Integrating Processing 2003. 
Limin Yao, Aria Haghighi, Sebastian Riedel, Andrew 
McCallum. 2011. Structured Relation Discovery Us-
ing Generative Models. In Proceedings of EMNLP 
2011.  
Alexander Yates and Oren Etzioni. 2007. Unsupervised 
Resolution of Objects and Relations on the Web.  In 
Proceedings of HLT-NAACL 2007.  
Fan Zhang, Shuming Shi, Jing Liu, Shuqi Sun, Chin-
Yew Lin. 2011. Nonlinear Evidence Fusion and 
Propagation for Hyponymy Relation Mining. In Pro-
ceedings of ACL 2011. 
Huibin Zhang, Mingjie Zhu, Shuming Shi, and Ji-Rong 
Wen. 2009. Employing Topic Models for Pattern-
based Semantic Class Discovery. In Proceedings of 
ACL 2009. 
1037
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 194?203,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Compensating for Annotation Errors in Training a Relation Extractor 
Bonan Min Ralph Grishman 
New York University New York University 
715 Broadway, 7th floor 715 Broadway, 7th floor 
New York, NY 10003 USA New York, NY 10003 USA 
min@cs.nyu.edu grishman@cs.nyu.edu 
  
Abstract 
The well-studied supervised Relation 
Extraction algorithms require training 
data that is accurate and has good 
coverage. To obtain such a gold standard, 
the common practice is to do independent 
double annotation followed by 
adjudication. This takes significantly 
more human effort than annotation done 
by a single annotator. We do a detailed 
analysis on a snapshot of the ACE 2005 
annotation files to understand the 
differences between single-pass 
annotation and the more expensive nearly 
three-pass process, and then propose an 
algorithm that learns from the much 
cheaper single-pass annotation and 
achieves a performance on a par with the 
extractor trained on multi-pass annotated 
data. Furthermore, we show that given 
the same amount of human labor, the 
better way to do relation annotation is not 
to annotate with high-cost quality 
assurance, but to annotate more.  
1. Introduction 
Relation Extraction aims at detecting and 
categorizing semantic relations between pairs of 
entities in text. It is an important NLP task that 
has many practical applications such as 
answering factoid questions, building knowledge 
bases and improving web search.  
    Supervised methods for relation extraction 
have been studied extensively since rich 
annotated linguistic resources, e.g. the Automatic 
Content Extraction1 (ACE) training corpus, were 
released. We will give a summary of related 
methods in section 2. Those methods rely on 
accurate and complete annotation. To obtain high 
quality annotation, the common wisdom is to let 
                                                 
1 http://www.itl.nist.gov/iad/mig/tests/ace/ 
two annotators independently annotate a corpus, 
and then asking a senior annotator to adjudicate 
the disagreements 2 . This annotation procedure 
roughly requires 3 passes3 over the same corpus. 
Therefore it is very expensive. The ACE 2005 
annotation on relations is conducted in this way. 
    In this paper, we analyzed a snapshot of ACE 
training data and found that each annotator 
missed a significant fraction of relation mentions 
and annotated some spurious ones. We found 
that it is possible to separate most missing 
examples from the vast majority of true-negative 
unlabeled examples, and in contrast, most of the 
relation mentions that are adjudicated as 
incorrect contain useful expressions for learning 
a relation extractor. Based on this observation, 
we propose an algorithm that purifies negative 
examples and applies transductive inference to 
utilize missing examples during the training 
process on the single-pass annotation. Results 
show that the extractor trained on single-pass 
annotation with the proposed algorithm has a 
performance that is close to an extractor trained 
on the 3-pass annotation. We further show that 
the proposed algorithm trained on a single-pass 
annotation on the complete set of documents has 
a higher performance than an extractor trained on 
3-pass annotation on 90% of the documents in 
the same corpus, although the effort of doing a 
single-pass annotation over the entire set costs 
less than half that of doing 3 passes over 90% of 
the documents. From the perspective of learning 
a high-performance relation extractor, it suggests 
that a better way to do relation annotation is not 
to annotate with a high-cost quality assurance, 
but to annotate more. 
                                                 
2 The senior annotator also found some missing examples as 
shown in figure 1. 
3 In this paper, we will assume that the adjudication pass has 
a similar cost compared to each of the two first-passes. The 
adjudicator may not have to look at as many sentences as an 
annotator, but he is required to review all instances found by 
both annotators. Moreover, he has to be more skilled and 
may have to spend more time on each instance to be able to 
resolve disagreements.  
194
2. Background 
2.1 Supervised Relation Extraction 
One of the most studied relation extraction tasks 
is the ACE relation extraction evaluation 
sponsored by the U.S. government. ACE 2005 
defined 7 major entity types, such as PER 
(Person), LOC (Location), ORG (Organization). 
A relation in ACE is defined as an ordered pair 
of entities appearing in the same sentence which 
expresses one of the predefined relations. ACE 
2005 defines 7 major relation types and more 
than 20 subtypes. Following previous work, we 
ignore sub-types in this paper and only evaluate 
on types when reporting relation classification 
performance. Types include General-affiliation 
(GEN-AFF), Part-whole (PART-WHOLE), 
Person-social (PER-SOC), etc. ACE provides a 
large corpus which is manually annotated with 
entities (with coreference chains between entity 
mentions annotated), relations, events and 
values. Each mention of a relation is tagged with 
a pair of entity mentions appearing in the same 
sentence as its arguments. More details about the 
ACE evaluation are on the ACE official website. 
    Given a sentence s and two entity mentions 
arg1 and arg2 contained in s, a candidate relation 
mention r with argument arg1 preceding arg2 is 
defined as r=(s, arg1, arg2). The goal of Relation 
Detection and Classification (RDC) is to 
determine whether r expresses one of the types 
defined. If so, classify it into one of the types. 
Supervised learning treats RDC as a 
classification problem and solves it with 
supervised Machine Learning algorithms such as 
MaxEnt and SVM. There are two commonly 
used learning strategies (Sun et al 2011). Given 
an annotated corpus, one could apply a flat 
learning strategy, which trains a single multi-
class classifier on training examples labeled as 
one of the relation types or not-a-relation, and 
apply it to determine its type or output not-a 
relation for each candidate relation mention 
during testing. The examples of each type are the 
relation mentions that are tagged as instances of 
that type, and the not-a-relation examples are 
constructed from pairs of entities that appear in 
the same sentence but are not tagged as any of 
the types. Alternatively, one could apply a 
hierarchical learning strategy, which trains two 
classifiers, a binary classifier RD for relation 
detection and the other a multi-class classifier RC 
for relation classification. RD is trained by 
grouping tagged relation mentions of all types as 
positive instances and using all the not-a-relation 
cases (same as described above) as negative 
examples. RC is trained on the annotated 
examples with their tagged types. During testing, 
RD is applied first to identify whether an 
example expresses some relation, then RC is 
applied to determine the most likely type only if 
it is detected as correct by RD. 
    State-of-the-art supervised methods for 
relation extraction also differ from each other on 
data representation. Given a relation mention, 
feature-based methods (Miller et al 2000;  
Kambhatla, 2004; Boschee et al 2005; 
Grishman et al 2005; Zhou et al 2005; Jiang 
and Zhai, 2007; Sun et al 2011) extract a rich 
list of structural, lexical, syntactic and semantic 
features to represent it; in contrast, the kernel 
based methods (Zelenko et al 2003; Bunescu 
and Mooney, 2005a; Bunescu and Mooney, 
2005b; Zhao and Grishman, 2005; Zhang et al 
2006a; Zhang et al 2006b; Zhou et al 2007; 
Qian et al 2008) represent each instance with an 
object such as augmented token sequences or a 
parse tree, and used a carefully designed kernel 
function, e.g. subsequence kernel (Bunescu and 
Mooney, 2005b) or convolution tree kernel 
(Collins and Duffy, 2001),  to calculate their 
similarity. These objects are usually augmented 
with features such as semantic features. 
In this paper, we use the hierarchical learning 
strategy since it simplifies the problem by letting 
us focus on relation detection only. The relation 
classification stage remains unchanged and we 
will show that it benefits from improved 
detection. For experiments on both relation 
detection and relation classification, we use 
SVM4 (Vapnik 1998) as the learning algorithm 
since it can be extended to support transductive 
inference as discussed in section 4.3. However, 
for the analysis in section 3.2 and the purification 
preprocess steps in section 4.2, we use a 
MaxEnt5 model since it outputs probabilities6 for 
its predictions.  For the choice of features, we use 
the full set of features from Zhou et al(2005) 
since it is reported to have a state-of-the-art 
performance (Sun et al 2011).  
2.2 ACE 2005 annotation 
The ACE 2005 training data contains 599 articles 
                                                 
4 SVM-Light is used. http://svmlight.joachims.org/ 
5 OpenNLP MaxEnt package is used. 
http://maxent.sourceforge.net/about.html 
6 SVM also outputs a value associated with each prediction. 
However, this value cannot be interpreted as probability.  
195
from newswire, broadcast news, weblogs, usenet 
newsgroups/discussion forum, conversational 
telephone speech and broadcast conversations. 
The annotation process is conducted as follows: 
two annotators working independently annotate 
each article and complete all annotation tasks 
(entities, values, relations and events). After two 
annotators both finished annotating a file, all 
discrepancies are then adjudicated by a senior 
annotator. This results in a high-quality 
annotation file. More details can be found in the 
documentation of ACE 2005 Multilingual 
Training Data V3.0. 
    Since the final release of the ACE training 
corpus only contains the final adjudicated 
annotations, in which all the traces of the two 
first-pass annotations are removed, we use a 
snapshot of almost-finished annotation, ACE 
2005 Multilingual Training Data V3.0, for our 
analysis. In the remainder of this paper, we will 
call the two independent first-passes of 
annotation fp1 and fp2. The higher-quality data 
done by merging fp1 and fp2 and then having 
disagreements adjudicated by the senior 
annotator is called adj. From this corpus, we 
removed the files that have not been completed 
for all three passes. On the final corpus 
consisting of 511 files, we can differentiate the 
annotations on which the three annotators have 
agreed and disagreed.  
    A notable fact of ACE relation annotation is 
that it is done with arguments from the list of 
annotated entity mentions. For example, in a 
relation mention tyco's ceo and president dennis 
kozlowski which expresses an EMP-ORG 
relation, the two arguments tyco and dennis 
kozlowski must have been tagged as entity 
mentions previously by the annotator. Since fp1 
and fp2 are done on all tasks independently, their 
disagreement on entity annotation will be 
propagated to relation annotation; thus we need 
to deal with these cases specifically.  
3. Analysis of data annotation 
3.1 General statistics 
As discussed in section 2, relation mentions are 
annotated with entity mentions as arguments, and 
the lists of annotated entity mentions vary in fp1, 
fp2 and adj. To estimate the impact propagated 
from entity annotation, we first calculate the ratio 
of overlapping entity mentions between entities 
annotated in fp1/fp2 with adj. We found that 
fp1/fp2 each agrees with adj on around 89% of 
the entity mentions. Following up, we checked 
the relation mentions7 from fp1 and fp2 against 
the adjudicated list of entity mentions from adj 
and found that 682 and 665 relation mentions 
respectively have at least one argument which 
doesn?t appear in the list of adjudicated entity 
mentions. 
    Given the list of relation mentions with both 
arguments appearing in the list of adjudicated 
entity mentions, figure 1 shows the inter-
annotator agreement of the ACE 2005 relation 
annotation. In this figure, the three circles 
represent the list of relation mentions in fp1, fp2 
and adj, respectively. 
3065
1486 1525
645 538
47
383
fp1 fp2
adj  
Figure 1. Inter-annotator agreement of ACE 2005 relation 
annotation. Numbers are the distinct relation mentions 
whose both arguments are in the list of adjudicated entity 
mentions. 
 
    It shows that each annotator missed a 
significant number of relation mentions 
annotated by the other. Considering that we 
removed 682/665 relation mentions from fp1/fp2 
because we generate this figure based on the list 
of adjudicated entity mentions, we estimate that 
fp1 and fp2 both missed around 18.3-28.5%8 of 
the relation mentions. This clearly shows that 
both of the annotators missed a significant 
fraction of the relation mentions. They also 
annotated some spurious relation mentions (as 
adjudicated in adj), although the fraction is 
smaller (close to 10% of all relation mentions in 
adj). 
    ACE 2005 relation annotation guidelines 
(ACE English Annotation Guidelines for 
Relations, version 5.8.3) defined 7 syntactic 
classes and the other class. We plot the 
distribution of syntactic classes of the annotated 
                                                 
7 This is done by selecting the relation mentions whose both 
arguments are in the list of adjudicated entity mentions. 
8 We calculate the lower bound by assuming that the 682 
relation mentions removed from fp1 are found in fp2, 
although with different argument boundary and headword 
tagged. The upper bound is calculated by assuming that they 
are all irrelevant and erroneous relation mentions. 
196
relations in figure 2 (3 of the classes, accounting 
together for less than 10% of the cases, are 
omitted) and the other class. It seems that it is 
generally easier for the annotators to find and 
agree on relation mentions of the type 
Preposition/PreMod/Possessives but harder to 
find and agree on the ones belonging to Verbal 
and Other. The definition and examples of these 
syntactic classes can be found in the annotation 
guidelines.  
    In the following sections, we will show the 
analysis on fp1 and adj since the result is similar 
for fp2. 
 
Figure 2. Percentage of examples of major syntactic classes. 
3.2 Why the differences? 
To understand what causes the missing 
annotations and the spurious ones, we need 
methods to find how similar/different the false 
positives are to true positives and also how 
similar/different the false negatives (missing 
annotations) are to true negatives. If we adopt a 
good similarity metric, which captures the 
structural, lexical and semantic similarity 
between relation mentions, this analysis will help 
us to understand the similarity/difference from an 
extraction perspective. 
    We use a state-of-the-art feature space (Zhou 
et al 2005) to represent examples (including all 
correct examples, erroneous ones and untagged 
examples) and use MaxEnt as the weight 
learning model since it shows competitive 
performance in relation extraction (Jiang and 
Zhai, 2007) and outputs probabilities associated 
with each prediction. We train a MaxEnt model 
for relation detection on true positives and true 
negatives, which respectively are the subset of 
correct examples annotated by fp1 (and 
adjudicated as correct ones) and negative 
examples that are not annotated in adj, and use it 
to make predictions on the mixed pool of correct 
examples, missing examples and spurious ones. 
To illustrate how distinguishable the missing 
examples (false negatives) are from the true 
negative ones, 1) we apply the MaxEnt model on 
both false negatives and true negatives, 2) put 
them together and rank them by the model-
predicted probabilities of being positive, 3) 
calculate their relative rank in this pool. We plot 
the Cumulative distribution of frequency (CDF) 
of the ranks (as percentages in the mixed pools) 
of false negatives in figure 3. We took similar 
steps for the spurious ones (false positives) and 
plot them in figure 3 as well (However, they are 
ranked by model-predicted probabilities of being 
negative). 
 
 
Figure 3: cumulative distribution of frequency (CDF) of the 
relative ranking of model-predicted probability of being 
positive for false negatives in a pool mixed of false 
negatives and true negatives; and the CDF of the relative 
ranking of model-predicted probability of being negative for 
false positives in a pool mixed of false positives and true 
positives. 
 
    For false negatives, it shows a highly skewed 
distribution in which around 75% of the false 
negatives are ranked within the top 10%. That 
means the missing examples are lexically, 
structurally or semantically similar to correct 
examples, and are distinguishable from the true 
negative examples. However, the distribution of 
false positives (spurious examples) is close to 
uniform (flat curve), which means they are 
generally indistinguishable from the correct 
examples. 
3.3 Categorize annotation errors 
The automatic method shows that the errors 
(spurious annotations) are very similar to the 
correct examples but provides little clue as to 
why that is the case. To understand their causes, 
we sampled 65 examples from fp1 (10% of the 
645 errors), read the sentences containing these 
197
Category Percentage 
Example 
Relation 
Type 
Sampled text of spurious examples in fp1 
Notes (examples are similar 
ones in adj for comparison) 
Duplicate 
relation 
mention for 
coreferential 
entity mentions 
49.2% ORG-AFF 
? his budding friendship with US      President 
George W. Bush in the face of ? 
? his budding friendship 
with US      President George 
W. Bush in the face of ? 
Correct 20% 
PHYS 
Hundreds of thousands of demonstrators took to 
the streets in Britain? 
 
PER-SOC 
The dead included the quack doctor, 55-year-old 
Nityalila Naotia, his teenaged son and? 
(Symmetric relation)  
The dead included the quack 
doctor, 55-year-old Nityalila 
Naotia, his teenaged son 
Argument not 
in list 
15.4% 
 
PER-SOC 
Putin had even secretly invited British Prime 
Minister Tony Blair, Bush's staunchest backer      
in the war on Iraq? 
 
Violate 
reasonable 
reader rule 
6.2% PHYS 
"The      amazing thing is they are going to turn 
San Francisco into ground zero for every criminal 
who wants to profit at their chosen profession", 
Paredes said. 
 
Errors 6.1% 
PART-
WHOLE 
?a likely candidate to run Vivendi Universal's 
entertainment unit in the United States? 
Arguments are tagged 
reversed 
PART-
WHOLE 
 
Khakamada argued that the United 
States would also need Russia's help "to make the 
new Iraqi government seem legitimate. 
Relation type error 
illegal 
promotion 
through 
?blocked? 
categories 
3% 
PHYS 
 
Up to 20,000 protesters thronged the plazas and 
streets of San Francisco, where? 
Up to 20,000 protesters 
thronged the plazas and 
streets of San Francisco, 
where? 
Table 1. Categories of spurious relation mentions in fp1 (on a sample of 10% of relation mentions), ranked by the percentage 
of the examples in each category. In the sample text, red text (also marked with dotted underlines) shows head words of the 
first arguments and the underlined text shows head words of the second arguments. 
 
erroneous relation mentions and compared them 
to the correct relation mentions in the same 
sentence; we categorized these examples and 
show them in table 1. The most common type of 
error is duplicate relation mention for 
coreferential entity mentions. The first row in 
table 1 shows an example, in which there is a 
relation ORG-AFF tagged between US and 
George W. Bush in adj. Because President and 
George W. Bush are coreferential, the example 
<US, President > from fp1 is adjudicated as 
incorrect. This shows that if a relation is 
expressed repeatedly across relation mentions 
whose arguments are coreferential, the 
adjudicator only tags one of the relation mentions 
as correct, although the other is correct too. This 
shared the same principle with another type of 
error illegal promotion through ?blocked? 
categories 9  as defined in the annotation 
guideline. The second largest category is correct, 
by which we mean the example is a correct 
relation mention and the adjudicator made a 
                                                 
9 For example, in sentence Smith went to a hotel in Brazil, 
(Smith, hotel) is a taggable PHYS Relation but (Smith, 
Brazil) is not, because to get the second relationship, one 
would have to ?promote? Brazil through hotel. For the 
precise definition of annotation rules, please refer to ACE 
(Automatic Content Extraction) English Annotation 
Guidelines for Relations, version 5.8.3. 
mistake. The third largest category is argument 
not in list, by which we mean that at least one of 
the arguments is not in the list of adjudicated 
entity mentions. 
    Based on Table 1, we can see that as many as 
72%-88% of the examples which are adjudicated 
as incorrect are actually correct if viewed from a 
relation learning perspective, since most of them 
contain informative expressions for tagging 
relations.  The annotation guideline is designed 
to ensure high quality while not imposing too 
much burden on human annotators. To reduce 
annotation effort, it defined rules such as illegal 
promotion through ?blocked? categories. The 
annotators? practice suggests that they are 
following another rule not to annotate duplicate 
relation mention for coreferential entity 
mentions. This follows the similar principle of 
reducing annotation effort but is not explicitly 
stated in the guideline: to avoid propagation of a 
relation through a coreference chain. However, 
these examples are useful for learning more ways 
to express a relation. Moreover, even for the 
erroneous examples (as shown in table 1 as 
violate reasonable reader rule and errors), most 
of them have some level of similar structures or 
semantics to the targeted relation. Therefore, it is 
very hard to distinguish them without human 
proofreading. 
198
Exp # Training 
data 
Testing 
data 
Detection (%) Classification (%) 
Precision Recall F1 Precision Recall F1 
1 fp1 adj 83.4 60.4 70.0 75.7 54.8 63.6 
2 fp2 adj 83.5 60.5 70.2 76.0 55.1 63.9 
3 adj adj 80.4 69.7 74.6 73.4 63.6 68.2 
Table 2. Performance of RDC trained on fp1/fp2/adj, and tested on adj. 
 
3.4 Why missing annotations and how 
many examples are missing? 
For the large number of missing annotations, 
there are a couple of possible reasons. One 
reason is that it is generally easier for a human 
annotator to annotate correctly given a well-
defined guideline, but it is hard to ensure 
completeness, especially for a task like relation 
extraction.  Furthermore, the ACE 2005 
annotation guideline defines more than 20 
relation subtypes. These many subtypes make it 
hard for an annotator to keep all of them in mind 
while doing the annotation, and thus it is 
inevitable that some examples are missed. 
    Here we proceed to approximate the number 
of missing examples given limited knowledge. 
Let each annotator annotate n examples and 
assume that each pair of annotators agrees on a 
certain fraction p of the examples. Assuming the 
examples are equally likely to be found by an 
annotator, therefore the total number of unique 
examples found by ?  annotators is ? (1???=0
?)??. If we had an infinite number of annotators 
(? ? ?), the total number of unique examples 
will be 
?
?
, which is the upper bound of the total 
number of examples. In the case of the ACE 
2005 relation mention annotation, since the two 
annotators annotate around 4500 examples and 
they agree on 2/3 of them, the total number of all 
positive examples is around 6750. This is close 
to the number of relation mentions in the 
adjudicated list: 6459. Here we assume the 
adjudicator is doing a more complex task than an 
annotator, resolving the disagreements and 
completing the annotation (as shown in figure 1).  
    The assumption of the calculation is a little 
crude but reasonable given the limited number of 
passes of annotation we have. Recent research (Ji 
et al2010) shows that, by adding annotators for 
IE tasks, the merged annotation tends to 
converge after having 5 annotators. To 
understand the annotation behavior better, in 
particular whether annotation will converge after 
adding a few annotators, more passes of 
annotation need to be collected. We leave this as 
future work. 
 
4. Relation extraction with low-cost 
annotation 
4.1 Baseline algorithm 
To see whether a single-pass annotation is useful 
for relation detection and classification, we did 
5-fold cross validation (5-fold CV) with each of 
fp1, fp2 and adj as the training set, and tested on 
adj. The experiments are done with the same 511 
documents we used for the analysis. As shown in 
table 2, we did 5-fold CV on adj for experiment 
3. For fairness, we use settings similar to 5-fold 
CV for experiment 1 and 2. Take experiment 1 as 
an example: we split both of fp1 and adj into 5 
folds, use 4 folds from fp1 as training data, and 1 
fold from adj as testing data and does one train-
test cycle. We rotate the folds (both training and 
testing) and repeat 5 times. The final results are 
averaged over the 5 runs. Experiment 2 was 
conducted similarly. In the reminder of the paper, 
5-fold CV experiments are all conducted in this 
way. 
    Table 2 shows that a relation tagger trained on 
the single-pass annotated data fp1 performs 
worse than the one trained on merged and 
adjudicated data adj, with 4.6 points lower F 
measure in relation detection, and 4.6 points 
lower relation classification. For detection, 
precision on fp1 is 3 points higher than on adj 
but recall is much lower (close to 10 points). The 
recall difference shows that the missing 
annotations contain expressions that can help to 
find more correct examples during testing. The 
small precision difference indirectly shows that 
the spurious ones in fp1 (as adjudicated) do not 
hurt precision. Performance on classification 
shows a similar trend because the relation 
classifier takes the examples predicted by the 
detector as correct as its input. Therefore, if there 
is an error, it gets propagated to this stage. Table 
2 also shows similar performance differences 
between fp2 and adj.  
    In the remainder of this paper, we will discuss 
a few algorithms to improve a relation tagger 
trained on single-pass annotated data10. Since we 
                                                 
10 We only use fp1 and adj in the following experiments 
because we observed that fp1 and fp2 are similar in general 
in the analysis, though a fraction of the annotation in fp1 
199
already showed that most of the spurious 
annotations are not actually errors from an 
extraction perspective and table 2 shows that 
they do not hurt precision, we will only focus on 
utilizing the missing examples, in other words, 
training with an incomplete annotation. 
4.2 Purify the set of negative examples 
As discussed in section 2, traditional supervised 
methods find all pairs of entity mentions that 
appear within a sentence, and then use the pairs 
that are not annotated as relation mentions as the 
negative examples for the purpose of training a 
relation detector. It relies on the assumption that 
the annotators annotated all relation mentions 
and missed no (or very few) examples. However, 
this is not true for training on a single-pass 
annotation, in which a significant portion of 
relation mentions are left not annotated. If this 
scheme is applied, all of the correct pairs which 
the annotators missed belong to this ?negative? 
category. Therefore, we need a way to purify the 
?negative? set of examples obtained by this 
conventional approach. 
    Li and Liu (2003) focuses on classifying 
documents with only positive examples. Their 
algorithm initially sets all unlabeled data to be 
negative and trains a Rocchio classifier, selects 
negative examples which are closer to the 
negative centroid than positive centroid as the 
purified negative examples, and then retrains the 
model. Their algorithm performs well for text 
classification. It is based on the assumption that 
there are fewer unannotated positive examples 
than negative ones in the   unlabeled set, so true 
negative examples still dominate the set of noisy 
?negative? examples in the purification step. 
Based on the same assumption, our purification 
process consists of the following steps: 
1) Use annotated relation mentions as 
positive examples; construct all possible 
relation mentions that are not annotated, and 
initially set them to be negative. We call this 
noisy data set D. 
2) Train a MaxEnt relation detection model 
Mdet on D. 
3) Apply Mdet  on all unannotated 
examples, and rank them by the model-
predicted probabilities of being positive, 
4) Remove the top N examples from D. 
These preprocessing steps result in a purified 
data set  ?????. We can use ????? for the normal 
                                                                          
and fp2 is different. Moreover, algorithms trained on them 
show similar performance. 
training process of a supervised relation 
extraction algorithm. 
    The algorithm is similar to Li and Liu 2003. 
However, we drop a few noisy examples instead 
of choosing a small purified subset since we have 
relatively few false negatives compared to the 
entire set of unannotated examples. Moreover, 
after step 3, most false negatives are clustered 
within the small region of top ranked examples 
which has a high model-predicated probability of 
being positive. The intuition is similar to what 
we observed from figure 3 for false negatives 
since we also observed very similar distribution 
using the model trained with noisy data. 
Therefore, we can purify negatives by removing 
examples in this noisy subset.  
    However, the false negatives are still mixed 
with true negatives. For example, still slightly 
more than half of the top 2000 examples are true 
negatives. Thus we cannot simply flip their 
labels and use them as positive examples. In the 
following section, we will use them in the form 
of unlabeled examples to help train a better 
model. 
4.3 Transductive inference on unlabeled 
examples 
Transductive SVM (Vapnik, 1998; Joachims, 
1999) is a semi-supervised learning method 
which learns a model from a data set consisting 
of both labeled and unlabeled examples. 
Compared to its popular antecedent SVM, it also 
learns a maximum margin classification 
hyperplane, but additionally forces it to separate 
a set of unlabeled data with large margin. The 
optimization function of Transductive  SVM 
(TSVM) is the following: 
 
 
Figure 4. TSVM optimization function for non-separable 
case (Joachims, 1999) 
 
    TSVM can leverage an unlabeled set of 
examples to improve supervised learning. As 
shown in section 3, a significant number of 
relation mentions are missing from the single-
pass annotation data. Although it is not possible 
to find all missing annotations without human 
effort, we can improve the model by further 
200
utilizing the fact that some unannotated examples 
should have been annotated.  
    The purification process discussed in the 
previous section removes N examples which 
have a high density of false negatives. We further 
utilize the N examples as follows: 
1) Construct a training corpus ??????? from 
?????  by taking a random sample
11 of N*(1-
p)/p (p is the ratio of annotated examples to 
all examples; p=0.05 in fp1) negatively 
labeled examples in ????? and setting them to 
be unlabeled. In addition, the N examples 
removed by the purification process are added 
back as unlabeled examples.  
2) Train TSVM on ???????.  
    The second step trained a model which 
replaced the detection model in the hierarchical 
detection-classification learning scheme we used. 
We will show in the next section that this 
improves the model. 
 
5. Experiments 
 
Experiments were conducted over the same set of 
documents on which we did analysis: the 511 
documents which have completed annotation in 
all of the fp1, fp2 and adj from the ACE 2005 
Multilingual Training Data V3.0. To 
reemphasize, we apply the hierarchical learning 
scheme and we focus on improving relation 
detection while keeping relation classification 
unchanged (results show that its performance is 
improved because of the improved detection). 
We use SVM as our learning algorithm with the 
full feature set from Zhou et al(2005).  
    Baseline algorithm: The relation detector is 
unchanged. We follow the common practice, 
which is to use annotated examples as positive 
ones and all possible untagged relation mentions 
as negative ones. We sub-sampled the negative 
data by ? since that shows better performance. 
    +purify:  This algorithm adds an additional 
purification preprocessing step (section 4.2) 
before the hierarchical learning RDC algorithm. 
After purification, the RDC algorithm is trained 
on the positive examples and purified negative 
examples. We set N=200012 in all experiments. 
                                                 
11 We included this large random sample so that the balance 
of positive to negative examples in the unlabeled set would 
be similar to that of the labeled data. The test data is not 
included in the unlabeled set. 
12 We choose 2000 because it is close to the number of 
relations missed from each single-pass annotation. In 
practice, it contains more than 70% of the false negatives, 
and it is less than 10% of the unannotated examples. To 
estimate how many examples are missing (section 3.4), one 
    +tSVM: First, the same purification process of 
+purify is applied. Then we follow the steps 
described in section 4.3 to construct the set of 
unlabeled examples, and set althe rest of 
purified negative examples to be negative. 
Finally, we train TSVM on both labeled and 
unlabeled data and replace the relation detection 
in the RDC algorithm. The relation classification 
is unchanged. 
    Table 3 shows the results. All experiments are 
done with 5-fold cross validation13 using testing 
data from adj. The first three rows show 
experiments trained on fp1, and the last row 
(ADJ) shows the unmodified RDC algorithm 
trained on adj for comparison. The purification 
of negative examples shows significant 
performance gain, 3.7% F1 on relation detection 
and 3.4% on relation classification. The precision 
decreases but recall increases substantially since 
the missing examples are not treated as 
negatives. Experiment shows that the purification 
process removes more than 60% of the false 
negatives. Transductive SVM further improved 
performance by a relatively small margin. This 
shows that the latent positive examples can help 
refine the model. Results also show that 
transductive inference can find around 17% of 
missing relation mentions. We notice that the 
performance of relation classification is 
improved since by improving relation detection, 
some examples that do not express a relation are 
removed. The classification performance on 
single-pass annotation is close to the one trained 
on adj due to the help from a better relation 
detector trained with our algorithm.  
    We also did 5-fold cross validation with a 
model trained on a fraction of the 4/5 (4 folds) of 
adj data (each experiment shown in table 4 uses 
4 folds of adj documents for training since one 
fold is left for cross validation). The documents 
are sampled randomly. Table 4 shows results for 
varying training data size. Compared to the 
results shown in the ?+tSVM? row of table 3, we 
can see that our best model trained on single-pass 
annotation outperforms SVM trained on 90% of 
the dual-pass, adjudicated data in both relation 
detection and classification, although it costs less 
than half the 3-pass annotation. This suggests 
that given the same amount of human effort for 
                                                                          
should perform multiple passes of independent annotation 
on a small dataset and measure inter-annotator agreements. 
13 Details about the settings for 5-fold cross validation are in 
section 4.1. 
201
Algorithm 
Detection (%) Classification (%) 
Precision Recall F1 Precision Recall F1 
Baseline 83.4 60.4 70.0 75.7 54.8 63.6 
+purify 76.8 70.9 73.7 69.8 64.5 67.0 
+tSVM 76.4 72.1 74.2 69.4 65.2 67.2 
ADJ (on adj) 80.4 69.7 74.6 73.4 63.6 68.2 
Table 3. 5-fold cross-validation results. All are trained on fp1 (except the last row showing the unchanged algorithm trained 
on adj for comparison), and tested on adj. McNemar's test show that the improvement from +purify to +tSVM, and from 
+tSVM to ADJ are statistically significant (with p<0.05). 
 
Percentage of 
adj used 
Detection (%) Classification (%) 
Precision Recall F1 Precision Recall F1 
60% ? 4/5 86.9 41.2 55.8 78.6 37.2 50.5 
70% ? 4/5 85.5 51.3 64.1 77.7 46.6 58.2 
80% ? 4/5 83.3 58.1 68.4 75.8 52.9 62.3 
90% ? 4/5 82.0 64.9 72.5 74.9 59.4 66.2 
Table 4. Performance with SVM trained on a fraction of adj. It shows 5 fold cross validation results. 
 
relation annotation, annotating more documents 
with single-pass offers advantages over 
annotating less data with high quality assurance 
(dual passes and adjudication). 
6. Related work 
Dligach et al(2010) studied WSD annotation 
from a cost-effectiveness viewpoint. They 
showed empirically that, with same amount of 
annotation dollars spent, single-annotation is 
better than dual-annotation and adjudication. The 
common practice for quality control of WSD 
annotation is similar to Relation annotation. 
However, the task of WSD annotation is very 
different from relation annotation. WSD requires 
that every example must be assigned some tag, 
whereas that is not required for relation tagging. 
Moreover, relation tagging requires identifying 
two arguments and correctly categorizing their 
types.  
The purified approach applied in this paper is 
related to the general framework of learning from 
positive and unlabeled examples. Li and Liu 
(2003) initially set alunlabeled data to be 
negative and train a Rocchio classifier, then 
select negative examples which are closer to the 
negative centroid than positive centroid as the 
purified negative examples.  We share a similar 
assumption with Li and Liu (2003) but we use a 
different method to select negative examples 
since the false negative examples show a very 
skewed distribution, as described in section 5.2.  
Transductive SVM was introduced by Vapnik 
(1998) and later refined in Joachims (1999). A 
few related methods were studied on the subtask 
of relation classification (the second stage of the 
hierarchical learning scheme) in Zhang (2005).   
Chan and Roth (2011) observed the similar 
phenomenon that ACE annotators rarely 
duplicate a relation link for coreferential 
mentions. They use an evaluation scheme to 
avoid being penalized by the relation mentions 
which are not annotated because of this behavior. 
 
7. Conclusion 
 
We analyzed a snapshot of the ACE 2005 
relation annotation and found that each single-
pass annotation missed around 18-28% of 
relation mentions and contains around 10% 
spurious mentions. A detailed analysis showed 
that it is possible to find some of the false 
negatives, and that most spurious cases are 
actually correct examples from a system 
builder?s perspective. By automatically purifying 
negative examples and applying transductive 
inference on suspicious examples, we can train a 
relation classifier whose performance is 
comparable to a classifier trained on the dual-
annotated and adjudicated data. Furthermore, we 
show that single-pass annotation is more cost-
effective than annotation with high quality 
assurance. 
Acknowledgments 
Supported by the Intelligence Advanced 
Research Projects Activity (IARPA) via Air 
Force Research Laboratory (AFRL) contract 
number FA8650-10-C-7058. The U.S. 
Government is authorized to reproduce and 
distribute reprints for Governmental purposes 
notwithstanding any copyright annotation 
thereon. The views and conclusions contained 
herein are those of the authors and should not be 
interpreted as necessarily representing the 
official policies or endorsements, either 
expressed or implied, of IARPA, AFRL, or the 
U.S. Government. 
202
References 
ACE. http://www.itl.nist.gov/iad/mig/tests/ace/ 
ACE (Automatic Content Extraction) English 
Annotation Guidelines for Relations, version 5.8.3. 
2005.  http://projects.ldc.upenn.edu/ace/. 
ACE 2005 Multilingual Training Data V3.0. 2005. 
LDC2005E18. LDC Catalog. 
Elizabeth Boschee, Ralph Weischedel, and Alex 
Zamanian. 2005. Automatic information extraction. 
In Proceedings of the International Conference on 
Intelligence Analysis. 
Razvan C. Bunescu and Raymond J. Mooney. 2005a. 
A shortest path dependency kenrel for relation 
extraction. In Proceedings of HLT/EMNLP-2005. 
Razvan C. Bunescu and Raymond J. Mooney. 2005b. 
Subsequence kernels for relation extraction. In 
Proceedings of NIPS-2005. 
Yee Seng Chan and Dan Roth. 2011. Exploiting 
Syntactico-Semantic Structures for Relation 
Extraction. In Proceedings of ACL-2011. 
Michael Collins and Nigel Duffy. Convolution 
Kernels for Natural Language. In Proceedings of 
NIPS-2001. 
Dmitriy Dligach, Rodney D. Nielsen and Martha 
Palmer. 2010. To annotate more accurately or to 
annotate more. In Proceedings of Fourth Linguistic 
Annotation Workshop at ACL 2010 
Ralph Grishman, David Westbrook and Adam 
Meyers. 2005. NYU?s English ACE 2005 System 
Description. In Proceedings of ACE 2005 
Evaluation Workshop 
Scott Miller, Heidi Fox, Lance Ramshaw, and Ralph 
Weischedel. 2000. A novel use of statistical 
parsing to extract information from text In 
Proceedings of NAACL-2010. 
Heng Ji, Ralph Grishman, Hoa Trang Dang and Kira 
Griffitt. 2010. An Overview of the TAC2010 
Knowledge Base Population Track. In Proceedings 
of TAC-2010 
Jing  Jiang  and ChengXiang Zhai. 2007. A systematic 
exploration of the feature space for relation 
extraction. In Proceedings of HLT-NAACL-2007. 
Thorsten Joachims. 1999. Transductive Inference for 
Text Classification using Support Vector 
Machines. In Proceedings of ICML-1999. 
Nanda Kambhatla. 2004. Combining lexical, 
syntactic, and semantic features with maximum 
entropy models for information extraction. In 
Proceedings of ACL-2004 
Xiao-Li Li and Bing Liu. 2003. Learning to classify 
text using positive and unlabeled data. In 
Proceedings of IJCAI-2003. 
Longhua Qian,  Guodong Zhou,  Qiaoming Zhu and 
Peide Qian. 2008. Exploiting constituent 
dependencies for tree kernel-based semantic 
relation extraction . In Proc. of COLING-2008. 
Ang Sun, Ralph Grishman and Satoshi Sekine. 2011. 
Semi-supervised Relation Extraction with Large-
scale Word Clustering. In Proceedings of ACL-
2011. 
Vladimir N. Vapnik. 1998. Statistical Learning 
Theory. John Wiley. 
Dmitry Zelenko, Chinatsu Aone, and Anthony 
Richardella. 2003. Kernel methods for relation 
extraction. Journal of Machine Learning Research. 
Min Zhang, Jie Zhang and Jian Su. 2006a. Exploring 
syntactic features for relation extraction using a 
convolution tree kernel, In Proceedings of HLT-
NAACL-2006. 
Min Zhang, Jie Zhang, Jian Su, and GuoDong Zhou. 
2006b. A composite kernel to extract relations 
between entities with both flat and structured 
features. In Proceedings of COLING-ACL-2006. 
Zhu Zhang. 2005. Mining Inter-Entity Semantic 
Relations Using Improved Transductive Learning. 
In Proceedings of ICJNLP-2005. 
Shubin Zhao and Ralph Grishman, 2005. Extracting 
Relations with Integrated Information Using Kern 
el Methods. In Proceedings of ACL-2005. 
Guodong Zhou, Jian Su, Jie Zhang and Min Zhang. 
2005. Exploring various knowledge in relation 
extraction. In Proceedings of ACL-2005. 
Guodong Zhou, Min Zhang, DongHong Ji, and 
QiaoMing Zhu. 2007. Tree kernel-based relation 
extraction with context-sensitive structured parse 
tree information. In Proceedings of 
EMNLP/CoNLL-2007. 
203
Proceedings of NAACL-HLT 2013, pages 777?782,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Distant Supervision for Relation Extraction
with an Incomplete Knowledge Base
Bonan Min, Ralph Grishman, Li Wan
New York University
New York, NY 10003
{min,grishman,wanli}
@cs.nyu.edu
Chang Wang, David Gondek
IBM T. J. Watson Research Center
Yorktown Heights, NY 10598
{wangchan,dgondek}
@us.ibm.com
Abstract
Distant supervision, heuristically labeling a
corpus using a knowledge base, has emerged
as a popular choice for training relation ex-
tractors. In this paper, we show that a sig-
nificant number of ?negative? examples gen-
erated by the labeling process are false neg-
atives because the knowledge base is incom-
plete. Therefore the heuristic for generating
negative examples has a serious flaw. Building
on a state-of-the-art distantly-supervised ex-
traction algorithm, we proposed an algorithm
that learns from only positive and unlabeled
labels at the pair-of-entity level. Experimental
results demonstrate its advantage over existing
algorithms.
1 Introduction
Relation Extraction is a well-studied problem
(Miller et al, 2000; Zhou et al, 2005; Kambhatla,
2004; Min et al, 2012a). Recently, Distant Super-
vision (DS) (Craven and Kumlien, 1999; Mintz et
al., 2009) has emerged to be a popular choice for
training relation extractors without using manually
labeled data. It automatically generates training ex-
amples by labeling relation mentions1 in the source
corpus according to whether the argument pair is
listed in the target relational tables in a knowledge
base (KB). This method significantly reduces human
efforts for relation extraction.
The labeling heuristic has a serious flaw. Knowl-
edge bases are usually highly incomplete. For exam-
1An occurrence of a pair of entities with the source sentence.
ple, 93.8% of persons from Freebase2 have no place
of birth, and 78.5% have no nationality (section 3).
Previous work typically assumes that if the argument
entity pair is not listed in the KB as having a re-
lation, all the corresponding relation mentions are
considered negative examples.3 This crude assump-
tion labeled many entity pairs as negative when in
fact some of their mentions express a relation. The
number of such false negative matches even exceeds
the number of positive pairs, by 3 to 10 times, lead-
ing to a significant problem for training. Previous
approaches (Riedel et al, 2010; Hoffmann et al,
2011; Surdeanu et al, 2012) bypassed this problem
by heavily under-sampling the ?negative? class.
We instead deal with a learning scenario where we
only have entity-pair level labels that are either posi-
tive or unlabeled. We proposed an extension to Sur-
deanu et al (2012) that can train on this dataset. Our
contribution also includes an analysis on the incom-
pleteness of Freebase and the false negative match
rate in two datasets of labeled examples generated
by DS. Experimental results on a realistic and chal-
lenging dataset demonstrate the advantage of the al-
gorithm over existing solutions.
2 Related Work
Distant supervision was first proposed by Craven
and Kumlien (1999) in the biomedical domain.
2Freebase is a large collaboratively-edited KB. It is available
at http://www.freebase.com.
3There are variants of labeling heuristics. For example, Sur-
deanu et al (2011) and Sun et al (2011) use a pair < e, v >
as a negative example, when it is not listed in Freebase, but e is
listed with a different v?. These assumptions are also problem-
atic in cases where the relation is not functional.
777
Since then, it has gain popularity (Mintz et al, 2009;
Bunescu and Mooney, 2007; Wu and Weld, 2007;
Riedel et al, 2010; Hoffmann et al, 2011; Sur-
deanu et al, 2012; Nguyen and Moschitti, 2011).
To tolerate noisy labels in positive examples, Riedel
et al (2010) use Multiple Instance Learning (MIL),
which assumes only at-least-one of the relation men-
tions in each ?bag? of mentions sharing a pair of ar-
gument entities which bears a relation, indeed ex-
presses the target relation. MultiR (Hoffmann et
al., 2011) and Multi-Instance Multi-Label (MIML)
learning (Surdeanu et al, 2012) further improve it
to support multiple relations expressed by different
sentences in a bag. Takamatsu et al (2012) mod-
els the probabilities of a pattern showing relations,
estimated from the heuristically labeled dataset.
Their algorithm removes mentions that match low-
probability patterns. Sun et al (2011) and Min et
al. (2012b) also estimate the probablities of patterns
showing relations, but instead use them to relabel ex-
amples to their most likely classes. Their approach
can correct highly-confident false negative matches.
3 Problem Definition
Distant Supervision: Given a KB D (a collection
of relational tables r(e1, e2), in which r?R (R is the
set of relation labels), and < e1, e2 > is a pair of
entities that is known to have relation r) and a cor-
pus C, the key idea of distant supervision is that we
align D to C, label each bag4 of relation mentions
that share argument pair < e1, e2 > with r, other-
wise OTHER. This generates a dataset that has labels
on entity-pair (bag) level. Then a relation extractor
is trained with single-instance learning (by assum-
ing all mentions have the same label as the bag), or
Multiple-Instance Learning (by assuming at-least-
one of the mentions expresses the bag-level label),
or Multi-Instance Multi-Label learning (further as-
suming a bag can have multiple labels) algorithms.
All of these works treat the OTHER class as exam-
ples that are labeled as negative.
The incomplete KB problem: KBs are usually
incomplete because they are manually constructed,
and it is not possible to cover all human knowledge
4A bag is defined as a set of relation mentions sharing the
same entity pair as relation arguments. We will use the terms
bag and entity pair interchangeably in this paper.
nor stay current. We took frequent relations, which
involve an entity of type PERSON, from Freebase
for analysis. We define the incompleteness ?(r) of a
relation r as follows:
?(r) = |{e}|?|{e|?e?,s.t.r(e,e?)?D}||{e}|
?(r) is the percentage of all persons {e} that do
not have an attribute e? (with which r(e, e?) holds).
Table 1 shows that 93.8% of persons have no place
of birth, and 78.5% of them have no nationality.
These are must-have attributes for a person. This
shows that Freebase is highly incomplete.
Freebase relation types Incompleteness
/people/person/education 0.792
/people/person/employment history 0.923
/people/person/nationality* 0.785
/people/person/parents* 0.988
/people/person/place of birth* 0.938
/people/person/places lived* 0.966
Table 1: The incompleteness of Freebase (* are must-
have attributes for a person).
We further investigate the rate of false negative
matches, as the percentage of entity-pairs that are
not listed in Freebase but one of its mentions gen-
erated by DS does express a relation in the tar-
get set of types. We randomly picked 200 unla-
beled bags5 from each of the two datasets (Riedel
et al, 2010; Surdeanu et al, 2012) generated by DS,
and we manually annotate all relation mentions in
these bags. The result is shown in Table 2, along
with a few examples that indicate a relation holds in
the set of false negative matches (bag-level). Both
datasets have around 10% false negative matches in
the unlabeled set of bags. Taking into considera-
tion that the number of positive bags and unlabeled
bags are highly imbalanced (1:134 and 1:37 in the
Riedel and KBP dataset respectively, before under-
sampling the unlabeled class), the number of false
negative matches are 11 and 4 times the number
of positive bags in Reidel and KBP dataset, respec-
tively. Such a large ratio shows false negatives do
have a significant impact on the learning process.
4 A semi-supervised MIML algorithm
Our goal is to model the bag-level label noise,
caused by the incomplete KB problem, in addition
585% and 95.7% of the bags in the Riedel and KBP datasets
have only one relation mention.
778
Dataset
(train-
ing)
# pos-
itive
bags
# positive :
# unlabeled
% are
false
negatives
# positive
: # false
negative
has human
assessment
Examples of false negative mentions
Riedel 4,700 1:134(BD*) 8.5% 1:11.4 no
(/location/location/contains)... in Brooklyn ?s Williamsburg.
(/people/person/place lived) Cheryl Rogowski , a farmer from
Orange County ...
KBP 183,062 1:37(BD*) 11.5% 1:4 yes
(per:city of birth) Juan Martn Maldacena (born September
10, 1968) is a theoretical physicist born in Buenos Aires
(per:employee of)Dave Matthews, from the ABC News, ...
Table 2: False negative matches on the Riedel (Riedel et al, 2010) and KBP dataset (Surdeanu et al, 2012). All
numbers are on bag (pairs of entities) level. BD* are the numbers before downsampling the negative set to 10% and
5% in Riedel and KBP dataset, respectively.
to modeling the instance-level noise using a 3-layer
MIL or MIML model (e.g., Surdeanu et al (2012)).
We propose a 4-layer model as shown in Figure 1.
The input to the model is a list of n bags with a
vector of binary labels, either Positive (P), or Un-
labled (U) for each relation r. Our model can be
viewed as a semi-supervised6 framework that ex-
tends a state-of-the-art Multi-Instance Multi-Label
(MIML) model (Surdeanu et al, 2012). Since the
input to previous MIML models are bags with per-
relation binary labels of either Positive (P) or Neg-
ative (N), we add a set of latent variables ? which
models the true bag-level labels, to bridge the ob-
served bag labels y and the MIML layers. We con-
sider this as our main contribution to the model. Our
hierarchical model is shown in Figure 1.
Figure 1: Plate diagram of our model.
Let i, j be the index in the bag and mention level,
respectively. Following Surdeanu et al (2012), we
model mention-level extraction p(zrij |xij ;wz) and
multi-instance multi-label aggregation p(?ri |zi;wr?)
in the bottom 3 layers. We define:
? r is a relation label. r?R ? {OTHER}, in
which OTHER denotes no relation expressed.
? yri ?{P,U}: r holds for ith bag or the bag is
unlabeled.
6We use the term semi-supervised because the algorithm
uses unlabeled bags but existing solutions requires bags to be
labeled either positive or negative.
? ?ri ?{P,N}: a hidden variable that denotes
whether r holds for the ith bag.
? ? is an observed constant controlling the total
number of bags whose latent label is positive.
We define the following conditional probabilities:
? p(yri |?ri ) =
?
?
?
?
?
?
?
1/2 if yri = P ? ?ri = P ;
1/2 if yri = U ? ?ri = P ;
1 if yri = U ? ?ri = N ;
0 otherwise ;
It encodes the constraints between true bag-
level labels and the entity pair labels in the KB.
? p(?|?) ? N (
?n
i=1
?
r?R ?(?ri ,P )
n , 1k ) where
?(x, y) = 1 if x = y, 0 otherwise. k is a large
number. ? is the fraction of the bags that are
positive. It is an observed parameter that de-
pends on both the source corpus and the KB
used.
Similar to Surdeanu et al (2012), we also define
the following parameters and conditional probabili-
ties (details are in Surdeanu et al (2012)):
? zij?R ? {OTHER}: a latent variable that de-
notes the relation type of the jth mention in the
ith bag.
? xij is the feature representation of the jth rela-
tion mention in the ith bag. We use the set of
features in Surdeanu et al (2012).
? wz is the weight vector for the multi-class rela-
tion mention-level classifier.
? wr? is the weight vector for the rth binary top-
level aggregation classifier (from mention la-
bels to bag-level prediction). We usew? to rep-
resent w1? ,w2? , ...w
|R|
? .
? p(?ri |zi;wr?) ? Bern(f?(wr? , zi)) where f? is
probability produced by the rth top-level clas-
sifier, from the mention-label level to the bag-
label level.
? p(zrij |xij ;wz) ? Multi(fz(wz,xij)) where fz
779
is probability produced by the mention-level
classifier, from the mentions to the mention-
label level.7
4.1 Training
We use hard Expectation-Maximization (EM) algo-
rithm for training the model. Our objective function
is to maximize log-likelihood:
L(wz,w?) = logp(y, ?|x;wz,w?)
= log
?
?
p(y, ?, ?|x;wz,w?)
Since solving it exactly involves exploring an expo-
nential assignment space for ?, we approximate and
iteratively set ?? = arg? max p(?|y, ?,x;wz,w?)
p(?|y, ?,x;wz,w?) ? p(y, ?, ?|x;wz,w?)
= p(y, ?|?,x)p(?|x;wz,w?)
= p(y|?)p(?|?)p(?|x;wz,w?)
Rewriting in log form:
logp(?|y, ?,x;wz,w?)
= logp(y|?) + logp(?|?) + logp(?|x;wz,w?)
=
n
?
i=1
?
r?R
logp(yri |?ri )? k(
n
?
i=1
?
r?R
?(?ri , P )
n ? ?)
2
+
n
?
i=1
?
r?R
logp(?ri |xi;wz,w?) + const
Algorithm 1 Training (E-step:2-11; M-step:12-15)
1: for i = 1, 2 to T do
2: ?ri ? N for all yri = U and r?R
3: ?ri ? P for all yri = P and r?R
4: I = {< i, r > |?ri = N}; I ? = {< i, r > |?ri = P}
5: for k = 0, 1 to ?n? |I ?| do
6: < i?, r? >= argmax<i,r>?I p(?ri |xi;wz,w?)
7: ?r?i? ? P ; I = I\{< i?, r? >}
8: end for
9: for i = 1, 2 to n do
10: z?i = argmaxzi p(zi|?i,xi;wz,w?)
11: end for
12: w?z = argmaxwz
?n
i=1
?|xi|
j=1 logp(zij |xij ,wz)
13: for all r?R do
14: w
r(?)
? = argmaxwr?
?n
i=1 p(?ri |zi,wr?)
15: end for
16: end for
17: return wz,w?
7All classifiers are implemented with L2-regularized logistic
regression with Stanford CoreNLP package.
In the E-step, we do a greedy search (steps 5-8
in algorithm 1) in all p(?ri |xi;wz,w?) and update ?ri
until the second term is maximized. wz , w? are the
model weights learned from the previous iteration.
After fixed ?, we seek to maximize:
logp(?|xi;wz,w?) =
n
?
i=1
logp(?i|xi;wz,w?)
=
n
?
i=1
log
?
zi
p(?i, zi|xi;wz,w?)
which can be solved with an approxi-
mate solution in Surdeanu et al (2012)
(step 9-11): update zi independently with:
z?i = argmaxzi p(zi|?i,xi;wz,w?). More details
can be found in Surdeanu et al (2012).
In the M-step, we retrain both of the mention-
level and the aggregation level classifiers.
The full EM algorithm is shown in algorithm 1.
4.2 Inference
Inference on a bag xi is trivial. For each mention:
z?ij = argzij?R?{OTHER} max p(zij |xij ,wz)
Followed by the aggregation (directly with w?):
yr(?)i = argyri ?{P,N} max p(y
r
i |zi;wr?)
4.3 Implementation details
We implement our model on top of the
MIML(Surdeanu et al, 2012) code base.8 We
use the same mention-level and aggregate-level
feature sets as Surdeanu et al (2012). We adopt
the same idea of using cross validation for the E
and M steps to avoid overfitting. We initialize our
algorithm by sampling 5% unlabeled examples as
negative, in essence using 1 epoch of MIML to
initialize. Empirically it performs well.
5 Experiments
Data set: We use the KBP (Ji et al, 2011)
dataset9 prepared and publicly released by Surdeanu
et al (2012) for our experiment since it is 1) large
and realistic, 2) publicly available, 3) most im-
portantly, it is the only dataset that has associated
human-labeled ground truth. Any KB held-out eval-
uation without manual assessment will be signif-
icantly affected by KB incompleteness. In KBP
8Available at http://nlp.stanford.edu/software/mimlre.shtml
9Available from Linguistic Data Consortium (LDC).
http://projects.ldc.upenn.edu/kbp/data/
780
Figure 2: Performance on the KBP dataset. The figures on the left, middle and right show MIML, Hoffmann, and
Mintz++ compared to the same MIML-Semi curve, respectively. MIML-Semi is shown in red curves (lighter curves in
black and white) while other algorithms are shown in black curves (darker curves in black and white).
dataset, the training bags are generated by mapping
Wikipedia (http://en.wikipedia.org) infoboxes (after
merging similar types following the KBP 2011 task
definition) into a large unlabeled corpus (consisting
of 1.5M documents from the KBP source corpus and
a complete snapshot of Wikipedia). The KBP shared
task provided 200 query named entities with their as-
sociated slot values (in total several thousand pairs).
We use 40 queries as development dataset (dev), and
the rest (160 queries) as evaluation dataset. We set
? = 0.25 by tuning on the dev set and use it in the
experiments. For a fair comparison, we follow Sur-
deanu et al (2012) and begin by downsampling the
?negative? class to 5%. We also set T=8 and use
the following noisy-or (for ith bag) of mention-level
probability to rank predicted types (r) of pairs and
plot the precision-recall curves for all experiments.
Probi(r) = 1?
?
j
(1? p(zij = r|xij ;wz))
Evaluation: We compare our algorithm (MIML-
semi) to three algorithms: 1) MIML (Surdeanu et
al., 2012), the Multiple-Instance Multiple Label al-
gorithm which labels the bags directly with the KB
(y = ?). 2) MultiR (denoted as Hoffmann) (Hoff-
mann et al, 2011), a Multiple-Instance algorithm
that supports overlapping relations. It also imposes
y = ?. 3) Mintz++ (Surdeanu et al, 2012), a vari-
ant of the single-instance learning algorithm (section
3). The first two are stat-of-the-art Multi-Instance
Multi-Label algorithms. Mintz++ is a strong base-
line (Surdeanu et al, 2012) and an improved ver-
sion of Mintz et al (2009). Figure 2 shows that
our algorithm consistently outperforms all three al-
gorithms at almost all recall levels (with the excep-
tion of a very small region in the PR-curve). This
demonstrates that by treating unla-beled data set dif-
ferently and leveraging the missing positive bags,
MIML-semi is able to learn a more accurate model
for extraction. Although the proposed solution is a
specific algorithm, we believe the idea of treating
unlabeled data differently can be incorporated into
any of these algorithms that only use unlabeled data
as negative examples.
6 Conclusion
We show that the distant-supervision labeling pro-
cess generates a significant number of false nega-
tives because the knowledge base is incomplete. We
proposed an algorithm that learns from only positive
and unlabeled bags. Experimental results demon-
strate its advantage over existing algorithms.
Acknowledgments
Supported in part by the Intelligence Advanced Re-
search Projects Activity (IARPA) via Department of
Interior National Business Center contract number
D11PC20154. The U.S. Government is authorized
to reproduce and distribute reprints for Governmen-
tal purposes notwithstanding any copyright annota-
tion thereon. The views and conclusions contained
herein are those of the authors and should not be
interpreted as necessarily representing the official
policies or endorsements, either expressed or im-
plied, of IARPA, DoI/NBC, or the U.S. Government.
781
References
Razvan Bunescu and Raymond Mooney. 2007. Learning
to extract relations from the web using minimal super-
vision. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics.
Mark Craven and Johan Kumlien. 1999. Constructing bi-
ological knowledge bases by extracting information
from text sources. In Proceedings of the Seventh Inter-
national Conference on Intelligent Systems for Molec-
ular Biology.
Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke
Zettlemoyer, and Daniel S. Weld. 2011. Knowledge-
based weak supervision for information extraction of
overlapping relations. In Proceedings of the Annual
Meeting of the Association for Computational Linguis-
tics.
Heng Ji, Ralph Grishman, and Hoa T. Dang. 2011.
Overview of the TAC 2011 knowledge base popula-
tion track. In Proceedings of the Text Analytics Con-
ference.
Jing Jiang and ChengXiang Zhai. 2007. A systematic ex-
ploration of the feature space for relation extraction. In
Proceedings of HLT-NAACL-2007.
Nanda Kambhatla. 2004. Combining lexical, syntactic,
and semantic features with maximum entropy mod-
els for information extraction. In Proceedings of ACL-
2004.
Scott Miller, Heidi Fox, Lance Ramshaw, and Ralph
Weischedel. 2000. A novel use of statistical parsing
to extract information from text. In Proceedings of
NAACL-2000.
Bonan Min, Shuming Shi, Ralph Grishman and Chin-
Yew Lin. 2012a. Ensemble Semantics for Large-scale
Unsupervised Relation Extraction. In Proceedings of
EMNLP-CoNLL 2012.
Bonan Min, Xiang Li, Ralph Grishman and Ang Sun.
2012b. New York University 2012 System for KBP
Slot Filling. In Proceedings of the Text Analysis Con-
ference (TAC) 2012.
Mike Mintz, Steven Bills, Rion Snow, and Daniel Juraf-
sky. 2009. Distant supervision for relation extraction
without labeled data. In Proceedings of the 47th An-
nual Meeting of the Association for Computational
Linguistics.
Truc Vien T. Nguyen and Alessandro Moschitti. 2011.
End-to-end relation extraction using distant supervi-
sion from external semantic repositories. In Proceed-
ings of the 49th Annual Meeting of the Association for
Computational Linguistics: Human Language Tech-
nologies.
Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Modeling relations and their mentions without
labeled text. In Proceedings of the European Confer-
ence on Machine Learning and Knowledge Discovery
in Databases (ECML PKDD 10).
Ang Sun, Ralph Grishman, Wei Xu, and Bonan Min.
2011. New York University 2011 system for KBP slot
filling. In Proceedings of the Text Analytics Confer-
ence.
Mihai Surdeanu, Sonal Gupta, John Bauer, David Mc-
Closky, Angel X. Chang, Valentin I. Spitkovsky, and
Christopher D. Manning. 2011. Stanfords distantly-
supervised slot-filling system. In Proceedings of the
Text Analytics Conference.
Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati,
Christopher D. Manning. 2012. Multi-instance Multi-
label Learning for Relation Extraction. In Proceed-
ings of the 2012 Conference on Empirical Methods in
Natural Language Processing and Natural Language
Learning.
TAC KBP 2011 task definition. 2011. http://nlp
.cs.qc.cuny.edu/kbp/2011/KBP2011 TaskDefinition.pdf
Shingo Takamatsu, Issei Sato, Hiroshi Nakagawa. 2012.
ReducingWrong Labels in Distant Supervision for Re-
lation Extraction. In Proceedings of 50th Annual Meet-
ing of the Association for Computational Linguistics.
Fei Wu and Daniel S. Weld. 2007. Autonomously seman-
tifying wikipedia. In Proceedings of the International
Conference on Information and Knowledge Manage-
ment (CIKM-2007).
Guodong Zhou, Jian Su, Jie Zhang and Min Zhang. 2005.
Exploring various knowledge in relation extraction. In
Proceedings of ACL-2005.
782
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 732?738,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Infusion of Labeled Data into Distant Supervision for Relation Extraction
Maria Pershina
+
Bonan Min
? ?
Wei Xu
#
Ralph Grishman
+
+
New York University, New York, NY
{pershina, grishman}@cs.nyu.edu
?
Raytheon BBN Technologies, Cambridge, MA
bmin@bbn.com
#
University of Pennsylvania, Philadelphia, PA
xwe@cis.upenn.edu
Abstract
Distant supervision usually utilizes only
unlabeled data and existing knowledge
bases to learn relation extraction models.
However, in some cases a small amount
of human labeled data is available. In this
paper, we demonstrate how a state-of-the-
art multi-instance multi-label model can
be modified to make use of these reli-
able sentence-level labels in addition to
the relation-level distant supervision from
a database. Experiments show that our ap-
proach achieves a statistically significant
increase of 13.5% in F-score and 37% in
area under the precision recall curve.
1 Introduction
Relation extraction is the task of tagging semantic
relations between pairs of entities from free text.
Recently, distant supervision has emerged as an
important technique for relation extraction and has
attracted increasing attention because of its effec-
tive use of readily available databases (Mintz et
al., 2009; Bunescu and Mooney, 2007; Snyder and
Barzilay, 2007; Wu and Weld, 2007). It automat-
ically labels its own training data by heuristically
aligning a knowledge base of facts with an unla-
beled corpus. The intuition is that any sentence
which mentions a pair of entities (e
1
and e
2
) that
participate in a relation, r, is likely to express the
fact r(e
1
,e
2
) and thus forms a positive training ex-
ample of r.
One of most crucial problems in distant super-
vision is the inherent errors in the automatically
generated training data (Roth et al, 2013). Ta-
ble 1 illustrates this problem with a toy exam-
ple. Sophisticated multi-instance learning algo-
rithms (Riedel et al, 2010; Hoffmann et al, 2011;
?
Most of the work was done when this author was at
New York University
Surdeanu et al, 2012) have been proposed to ad-
dress the issue by loosening the distant supervision
assumption. These approaches consider all men-
tions of the same pair (e
1
,e
2
) and assume that at-
least-one mention actually expresses the relation.
On top of that, researchers further improved per-
formance by explicitly adding preprocessing steps
(Takamatsu et al, 2012; Xu et al, 2013) or addi-
tional layers inside the model (Ritter et al, 2013;
Min et al, 2013) to reduce the effect of training
noise.
True Positive ... to get information out of captured
al-Qaida leader Abu Zubaydah.
False Positive ...Abu Zubaydah and former Taliban
leader Jalaluddin Haqqani ...
False Negative ...Abu Zubaydah is one of Osama bin
Laden?s senior operational planners...
Table 1: Classic errors in the training data gener-
ated by a toy knowledge base of only one entry
personTitle(Abu Zubaydah, leader).
However, the potential of these previously pro-
posed approaches is limited by the inevitable
gap between the relation-level knowledge and the
instance-level extraction task. In this paper, we
present the first effective approach, Guided DS
(distant supervision), to incorporate labeled data
into distant supervision for extracting relations
from sentences. In contrast to simply taking the
union of the hand-labeled data and the corpus la-
beled by distant supervision as in the previous
work by Zhang et al (2012), we generalize the
labeled data through feature selection and model
this additional information directly in the latent
variable approaches. Aside from previous semi-
supervised work that employs labeled and unla-
beled data (Yarowsky, 2013; Blum and Mitchell,
1998; Collins and Singer, 2011; Nigam, 2001, and
others), this is a learning scheme that combines
unlabeled text and two training sources whose
quantity and quality are radically different (Liang
et al, 2009).
To demonstrate the effectiveness of our pro-
732
Guideline g = {g
i
|i = 1, 2, 3}: Relation r(g)
types of entities, dependency path, span word (optional)
person person, nsubj ?? dobj, married personSpouse
person organization, nsubj ?? prep of , became personMemberOf
organization organization, nsubj ?? prep of , company organizationSubsidiaries
person person, poss?? appos, sister personSiblings
person person, poss?? appos, father personParents
person title,? nn personTitle
organization person, prep of ? appos? organizationTopMembersEmployees
person cause, nsubj ?? prep of personCauseOfDeath
person number,? appos personAge
person date, nsubjpass?? prep on? num personDateOfBirth
Table 2: Some examples from the final set G of extracted guidelines.
posed approach, we extend MIML (Surdeanu et
al., 2012), a state-of-the-art distant supervision
model and show a significant improvement of
13.5% in F-score on the relation extraction bench-
mark TAC-KBP (Ji and Grishman, 2011) dataset.
While prior work employed tens of thousands of
human labeled examples (Zhang et al, 2012) and
only got a 6.5% increase in F-score over a logistic
regression baseline, our approach uses much less
labeled data (about 1/8) but achieves much higher
improvement on performance over stronger base-
lines.
2 The Challenge
Simply taking the union of the hand-labeled data
and the corpus labeled by distant supervision is not
effective since hand-labeled data will be swamped
by a larger amount of distantly labeled data. An
effective approach must recognize that the hand-
labeled data is more reliable than the automatically
labeled data and so must take precedence in cases
of conflict. Conflicts cannot be limited to those
cases where all the features in two examples are
the same; this would almost never occur, because
of the dozens of features used by a typical relation
extractor (Zhou et al, 2005). Instead we propose
to perform feature selection to generalize human
labeled data into training guidelines, and integrate
them into latent variable model.
2.1 Guidelines
The sparse nature of feature space dilutes the dis-
criminative capability of useful features. Given
the small amount of hand-labeled data, it is im-
portant to identify a small set of features that are
general enough while being capable of predicting
quite accurately the type of relation that may hold
between two entities.
We experimentally tested alternative feature
sets by building supervised Maximum Entropy
(MaxEnt) models using the hand-labeled data (Ta-
ble 3), and selected an effective combination of
three features from the full feature set used by Sur-
deanu et al, (2011):
? the semantic types of the two arguments (e.g.
person, organization, location, date, title, ...)
? the sequence of dependency relations along the
path connecting the heads of the two arguments
in the dependency tree.
? a word in the sentence between the two argu-
ments
These three features are strong indicators of the
type of relation between two entities. In some
cases the semantic types of the arguments alone
narrows the possibilities to one or two relation
types. For example, entity types such as person
and title often implies the relation personTitle.
Some lexical items are clear indicators of partic-
ular relations, such as ?brother? and ?sister? for a
sibling relationship
We extract guidelines from hand-labeled data.
Each guideline g={g
i
|i=1,2,3} consists of a pair
of semantic types, a dependency path, and option-
ally a span word and is associated with a partic-
ular relation r(g). We keep only those guidelines
Model Precision Recall F-score
MaxEnt
all
18.6 6.3 9.4
MaxEnt
two
24.13 10.75 14.87
MaxEnt
three
40.27 12.40 18.97
Table 3: Performance of a MaxEnt, trained on
hand-labeled data using all features (Surdeanu et
al., 2011) vs using a subset of two (types of en-
tities, dependency path), or three (adding a span
word) features, and evaluated on the test set.
733
which make the correct prediction for all and at
least k=3 examples in the training corpus (thresh-
old 3 was obtained by running experiments on the
development dataset). Table 2 shows some exam-
ples in the final set G of extracted guidelines.
3 Guided DS
Our goal is to jointly model human-labeled ground
truth and structured data from a knowledge base
in distant supervision. To do this, we extend the
MIML model (Surdeanu et al, 2012) by adding a
new layer as shown in Figure 1.
The input to the model consists of (1) distantly
supervised data, represented as a list of n bags
1
with a vector y
i
of binary gold-standard labels, ei-
ther Positive(P ) or Negative(N) for each rela-
tion r?R; (2) generalized human-labeled ground
truth, represented as a set G of feature conjunc-
tions g={g
i
|i=1,2,3} associated with a unique re-
lation r(g). Given a bag of sentences, x
i
, which
mention an ith entity pair (e
1
, e
2
), our goal is to
correctly predict which relation is mentioned in
each sentence, or NR if none of the relations under
consideration are mentioned. The vector z
i
con-
tains the latent mention-level classifications for the
ith entity pair. We introduce a set of latent vari-
ables h
i
which model human ground truth for each
mention in the ith bag and take precedence over
the current model assignment z
i
.
G
|R|
|xi|
n
zi
hi
yi
xi
9
>
=
>
;
{
relation
level
mention
level
Figure 1: Plate diagram of Guided DS
Let i, j be the index in the bag and the men-
tion level, respectively. We model mention-
level extraction p(z
ij
|x
ij
;w
z
), human relabel-
ing h
ij
(x
ij
, z
ij
) and multi-label aggregation
p(y
r
i
|h
i
;w
y
). We define:
? y
r
i
?{P,N} : r holds for the ith bag or not.
? x
ij
is the feature representation of the jth rela-
tion mention in the ith bag. We use the same set
of features as in Surdeanu et al (2012).
1
A bag is a set of mentions sharing same entity pair.
? z
ij
?R ? NR: a latent variable that denotes the
relation of the jth mention in the ith bag
? h
ij
?R ?NR: a latent variable that denotes the
refined relation of the mention x
ij
We define relabeled relations h
ij
as following:
h
ij
(x
ij
, z
ij
)=
{
r(g), if ?!g?G s.t.g={g
k
}?{x
ij
}
z
ij
, otherwise
Thus, relation r(g) is assigned to h
ij
iff there
exists a unique guideline g ? G, such that the
feature vector x
ij
contains all constituents of g,
i.e. entity types, a dependency path and maybe a
span word, if g has one. We use mention relation
z
ij
inferred by the model only in case no such a
guideline exists or there is more than one match-
ing guideline. We also define:
? w
z
is the weight vector for the multi-class rela-
tion mention-level classifier
2
? w
r
y
is the weight vector for the rth binary top-
level aggregation classifier (from mention labels
to bag-level prediction). We use w
y
to represent
w
1
y
,w
2
y
, . . . ,w
|R|
y
.
Our approach is aimed at improving the mention-
level classifier, while keeping the multi-instance
multi-label framework to allow for joint modeling.
4 Training
We use a hard expectation maximization algorithm
to train the model. Our objective function is to
maximize log-likelihood of the data:
LL(w
y
,w
z
) =
n
?
i=1
log p(y
i
|x
i
,w
y
,w
z
,G)
=
n
?
i=1
log
?
h
i
p(y
i
,h
i
|x
i
,w
y
,w
z
,G)
=
n
?
i=1
log
?
h
i
|h
i
|
?
j=1
p(h
ij
|x
ij
,w
z
,G)
?
r?P
i
?N
i
p(y
r
i
|h
i
,w
r
y
)
where the last equality is due to conditional
independence. Because of the non-convexity
of LL(w
y
,w
z
) we approximate and maximize
the joint log-probability p(y
i
,h
i
|x
i
,w
y
,w
z
,G) for
each entity pair in the database:
log p(y
i
,h
i
|x
i
,w
y
,w
z
,G)
=
|h
i
|
?
j=1
log p(h
ij
|x
ij
,w
z
,G)+
?
r?P
i
?N
i
log p(y
r
i
|h
i
,w
r
y
).
2
All classifiers are implemented using L2-regularized lo-
gistic regression with Stanford CoreNLP package.
734
Iteration 1 2 3 4 5 6 7 8
(a) Corrected relations: 2052 718 648 596 505 545 557 535
(b) Retrieved relations: 10219 860 676 670 621 599 594 592
Total relabelings 12271 1578 1324 1264 1226 1144 1153 1127
Table 4: Number of relabelings for each training iteration of Guided DS: (a) relabelings due to cor-
rected relations, e.g. personChildren? personSiblings (b) relabelings due to retrieved relations, e.g.
notRelated(NR)?personTitle
Algorithm 1 : Guided DS training
1: Phase 1: build set G of guidelines
2: Phase 2: EM training
3: for iteration = 1, . . . , T do
4: for i = 1, . . . , n do
5: for j = 1, . . . , |x
i
| do
6: z
?
ij
= argmax
z
ij
p(z
ij
|x
i
,y
i
,w
z
,w
y
)
7: h
?
ij
=
{
r(g), if ?!g?G :{g
k
}?{x
ij
}
z
ij
?
, otherwise
8: update h
i
with h
?
ij
9: end for
10: end for
11: w
?
z
=argmax
w
?
n
i=1
?
|x
i
|
j=1
log p(h
ij
|x
ij
,w)
12: for r ? R do
13: w
r?
y
=argmax
w
?
1?i?n s.t. r?P
i
?N
i
log p(y
r
i
|h
i
,w)
14: end for
15: end for
16: return w
z
,w
y
The pseudocode is presented as algorithm 1.
The following approximation is used for infer-
ence at step 6:
p(z
ij
|x
i
,y
i
,w
z
,w
y
) ? p(y
i
, z
ij
|x
i
,w
y
,w
z
)
? p(z
ij
|x
ij
,w
z
)p(y
i
|h
?
i
,w
y
)
= p(z
ij
|x
ij
,w
z
)
?
r?P
i
?N
i
p(y
r
i
|h
?
i
,w
r
y
),
where h
?
i
contains previously inferred and
maybe further relabeled mention labels for group
i (steps 5-10), with the exception of component j
whose label is replaced by z
ij
. In the M-step (lines
12-15) we optimize model parameters w
z
,w
y
,
given the current assignment of mention-level la-
bels h
i
.
Experiments show that Guided DS efficiently
learns new model, resulting in a drastically de-
creasing number of needed relabelings for further
iterations (Table 4). At the inference step we first
classify all mentions:
z
?
ij
= argmax
z?R?NR
p(z|x
ij
,w
z
)
Then final relation labels for ith entity tuple are
obtained via the top-level classifiers:
y
r?
i
= argmax
y?{P,N}
p(y|z
?
i
,w
r
y
)
5 Experiments
5.1 Data
We use the KBP (Ji and Grishman, 2011) dataset
3
which is preprocessed by Surdeanu et al (2011)
using the Stanford parser
4
(Klein and Manning,
2003). This dataset is generated by mapping
Wikipedia infoboxes into a large unlabeled corpus
that consists of 1.5M documents from KBP source
corpus and a complete snapshot of Wikipedia.
The KBP 2010 and 2011 data includes 200
query named entities with the relations they are
involved in. We used 40 queries as development
set and the rest 160 queries (3334 entity pairs that
express a relation) as the test set. The official KBP
evaluation is performed by pooling the system re-
sponses and manually reviewing each response,
producing a hand-checked assessment data. We
used KBP 2012 assessment data to generate guide-
lines since queries from different years do not
overlap. It contains about 2500 labeled sentences
of 41 relations, which is less than 0.09% of the
size of the distantly labeled dataset of 2M sen-
tences. The final set G consists of 99 guidelines
(section 2.1).
5.2 Models
We implement Guided DS on top of the MIML
(Surdeanu et al, 2012) code base
5
. Training
MIML on a simple fusion of distantly-labeled
and human-labeled datasets does not improve the
maximum F-score since this hand-labeled data is
swamped by a much larger amount of distant-
supervised data of much lower quality. Upsam-
pling the labeled data did not improve the perfor-
mance either. We experimented with different up-
sampling ratios and report best results using ratio
1:1 in Figure 2.
3
Available from Linguistic Data Consortium (LDC) at
http://projects.ldc.upenn.edu/kbp/data.
4
http://nlp.stanford.edu/software/lex-parser.shtml
5
Available at http://nlp.stanford.edu/software/mimlre.shtml.
735
a)
b) 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
Recall
Prec
ision
Student Version of MATLAB
0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
Recall
Pre
cis
ion
 
 
Guided DS
MIML
Mintz++
MultiR
Student Version of MATLAB
0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.50.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
Recall
Prec
ision
Student Version of MATLAB
0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
Recall
Pr
ec
isio
n
 
 
Guided DS
Semi?MIML
DS+upsampling
MaxEnt
Student Version of MATLAB
Model P R F1 AUC Model P R F1 AUC
MaxEnt 40.27 12.40 18.97 1.97 MultiR 30.64 19.79 24.05 6.4
DS+upsampling 32.26 24.31 27.72 12.00 Mintz++ 25.17 25.87 25.51 10.94
Semi MIML 30.02 26.21 27.98 12.31 MIML 28.06 28.64 28.35 11.74
Guided DS 31.9 32.46 32.19 16.1
Model P R F1 AUC
s
t
a
t
e
-
o
f
-
a
r
t
Model P R F1 AUC
MaxEnt 40.27 12.40 18.97 1.97 MultiR 30.64 19.79 24.05 6.4
DS+upsampling 32.26 24.31 27.72 12.00 Mintz++ 25.17 25.87 25.51 10.94
Semi MIML 30.02 26.21 27.98 12.31 MIML 28.06 28.64 28.35 11.74
Guided DS 31.9 32.46 32.19 16.1
b
a
s
e
l
i
n
e
Model P R F1 AUC
s
t
a
t
e
-
o
f
-
a
r
t
Model P R F1 AUC
MaxEnt 40.27 12.40 18.97 1.97 MultiR 30.64 19.79 24.05 6.4
DS+upsampling 32.26 24.31 27.72 12.00 Mintz++ 25.17 25.87 25.51 10.94
Semi MIML 30.02 26.21 27.98 12.31 MIML 28.06 28.64 28.35 11.74
Guided DS 31.9 32.46 32.19 16.1
b
a
s
e
l
i
n
e
Model P R F1 AUC
s
t
a
t
e
-
o
f
-
a
r
t
Model P R F1 AUC
MaxEnt 40.27 12.40 18.97 1.97 MultiR 30.64 19.79 24.05 6.4
DS+upsampling 32.26 24.31 27.72 12.00 Mintz++ 25.17 25.87 25.51 10.94
Semi MIML 30.02 26.21 27.98 12.31 MIML 28.06 28.64 28.35 11.74
Guided DS 31.9 32.46 32.19 16.1
1
Figure 2: Performance of Guided DS on KBP task compared to a) baselines: MaxEnt, DS+upsampling,
Semi-MIML (Min et al, 2013) b) state-of-art models: Mintz++ (Mintz et al, 2009), MultiR (Hoffmann
et al, 2011), MIML (Surdeanu et al, 2012)
Our baselines: 1) MaxEnt is a supervised maxi-
mum entropy baseline trained on a human-labeled
data; 2) DS+upsampling is an upsampling ex-
periment, where MIML was trained on a mix of
a distantly-labeled and human-labeled data; 3)
Semi-MIML is a recent semi-supervised exten-
sion. We also compare Guided DS with three
state-of-the-art models: 1) MultiR and 2) MIML
are two distant supervision models that support
multi-instance learning and overlapping relations;
3) Mintz++ is a single-instance learning algorithm
for distant supervision. The difference between
Guided DS and all other systems is significant
with p-value less than 0.05 according to a paired
t-test assuming a normal distribution.
5.3 Results
We scored our model against all 41 relations and
thus replicated the actual KBP evaluation. Figure
2 shows that our model consistently outperforms
all six algorithms at almost all recall levels and im-
proves the aximum F -score by more than 13.5%
relative to MIML (from 28.35% to 32.19%) as well
as increases the area under precision-recall curve
by more than 37% (from 11.74 to 16.1). Also,
Guided DS improves the overall recall by more
than 9% absolute (from 30.9% to 39.93%) at a
comparable level of precision (24.35% for MIML
vs 23.64% for Guided DS), while increases the
running time of MIML by only 3%. Thus, our
approach outperforms state-of-the-art model for
relation extraction using much less labeled data
that was used by Zhang et al, (2012) to outper-
form logistic regression baseline. Performance
of Guided DS also compares favorably with best
scored hand-coded systems for a similar task such
as Sun et al, (2011) system for KBP 2011, which
reports an F-score of 25.7%.
6 Conclusions and Future Work
We show that relation extractors trained with dis-
tant supervision can benefit significantly from a
small number of human labeled examples. We
propose a strategy to generate and select guide-
lines so that they are more generalized forms of
labeled instances. We show how to incorporate
these guidelines into an existing state-of-art model
for relation extraction. Our approach significantly
improves performance in practice and thus opens
up many opportunities for further research in RE
where only a very limited amount of labeled train-
ing data is available.
Acknowledgmen s
Supported by the Intelligence Advanced Research
Projects Activity ( IARPA) via Air Force Research
Laboratory (AFRL) contract number FA8650-10-
C-7058. The U.S. Government is authorized to
reproduce and distribute reprints for Governmen-
tal purposes notwithstanding any copyright anno-
tation thereon. The views and conclusions con-
tained herein are those of the authors and should
not be interpreted as necessarily representing the
official policies or endorsements, either expressed
or implied, of IARPA, AFRL, or the U.S. Govern-
ment.
736
References
Avrim Blum and Tom M. Mitchell. 1998. Combin-
ing labeled and unlabeled sata with co-training. In
Proceedings of the 11th Annual Conference on Com-
putational Learning Theory (COLT), pages 92?100.
Razvan C. Bunescu and Raymond J. Mooney. 2007.
Learning to extract relations from the web using
minimal supervision. In Proceedings of the 45th An-
nual Meeting of the Association for Computational
Linguistics (ACL).
Michael Collins and Yorav Singer. 1999. Unsuper-
vised models for named entity classification. Pro-
ceedings of the Conference on Empirical Methods
in Natural Language Processing (EMNLP-VLC). ,
Mark Craven and Johan Kumlien. 1999. Constructing
biological knowledge bases by extracting informa-
tion from text sources. In Proceedings of the Sev-
enth International Conference on Intelligent Systems
for Molecular Biology (ISMB), pages 77?86.
Oren Etzioni, Michele Banko, Stephen Soderland, and
Daniel S. Weld. 2008. Open information extrac-
tion from the web. Communications of the ACM,
51(12):68?74.
Raphael Hoffmann, Congle Zhang, and Daniel S.
Weld. 2010. Learning 5000 relational extractors.
In Proceedings of the 49th Annual Meetings of the
Association for Computational Linguistics (ACL),
pages 286?295.
Raphael Hoffmann, Congle Zhang, Xiao Ling,
Luke S. Zettlemoyer, and Daniel S. Weld. 2011.
Knowledge-based weak supervision for information
extraction of overlapping relations. In Proceedings
of the 49th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 541?550.
Heng Ji and Ralph Grishman. 2011. Knowledge base
population: Successful approaches and challenges.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics (ACL),
pages 1148?1158.
Heng Ji, Ralph Grishman, and Hoa Trang Dang. 2011.
Overview of the TAC-2011 knowledge base popula-
tion track. In Text Analysis Conference Workshop.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41th Annual Meetings of the Association for Com-
putational Linguistics (ACL).
Percy Liang, Michael I.Jordan and Dan Klein. 2009.
Learning From Measurements in Exponential Fami-
lies. In Proceedings of the 26th Annual International
Conference on Machine Learning (ICML), pages =
641?648
Bonan Min, Ralph Grishman, Li Wan, Chang Wang,
and David Gondek. 2013. Distant supervision for
relation extraction with an incomplete knowledge
base. In Proceedings of the Conference of the North
American Chapter of the Association for Computa-
tional Linguistics (NAACL).
Mike Mintz, Steven Bills, Rion Snow, and Daniel Ju-
rafsky. 2009. Distant supervision for relation ex-
traction without labeled data. In Proceedigns of the
47th Annual Meeting of the Association for Compu-
tational Linguistics and the 4th International Joint
Conference on Natural Language Processing (ACL),
pages 1003?1011.
Ramesh Nallapati. 2004. Discriminative models for
information retrieval. In Proceedigns of the 27th An-
nual International ACM SIGIR Conference on Re-
search and Development in Information Retrieval
(SIGIR), pages 64?71.
Truc-Vien T. Nguyen and Alessandro Moschitti. 2011.
End-to-end relation extraction using distant super-
vision from external semantic repositories. In Pro-
ceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), pages
277?282.
Kamal Paul Nigam. 2001. Using Unlabeled Data to
Improve Text Classification. Ph.D. thesis, School of
Computer Science, Carnegie Mellon University.
Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Modeling relations and their mentions with-
out labeled text. In Proceedigns of the European
Conference on Machine Learning and Principles
and Practice of Knowledge Discovery in Databases
(ECML/PKDD), pages 148?163.
Alan Ritter, Luke Zettlemoyer, Mausam, and Oren Et-
zioni. 2013. Modeling missing data in distant su-
pervision for information extraction. Transactions
of the Association for Computational Linguistics.
Benjamin Roth, Tassilo Barth, Michael Wiegand, and
Dietrich Klakow 2013. A Survey of Noise Reduc-
tion Methods for Distant Supervision. In Proceed-
ings of Conference on Information and Knowledge
Management (CIKM-AKBC).
Benjamin Snyder and Regina Barzilay 2007.
Database-text alignment via structured multilabel
classification. In Proceedings of IJCAI.
Ang Sun, Ralph Grishman, Wei Xu, and Bonan Min.
2011. New york university 2011 system for kbp slot
filling. In Text Analysis Conference (TAC-KBP).
Mihai Surdeanu, J. Turmo, and A. Ageno. 2006. A
hybrid approach for the acquisition of information
extraction patterns. In Proceedings of the 11th Con-
ference of the European Chapter of the Associate
for Computational Linguistics Workshop on Adap-
tive Text Extraction and Mining (EACL).
Mihai Surdeanu, Sonal Gupta, John Bauer, David Mc-
Closky, Angel X. Chang, Valentin I. Spitkovsky,
and Christopher D.Manning. 2011. Stanford?s
737
Distantly-Supervised Slot-Filling System. In Pro-
ceedings of the Text Analysis Conference (TAC-
KBP).
Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati,
and Christopher D. Manning. 2012. Multi-instance
multi-label learning for relation extraction. In Pro-
ceedings of the 50th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), pages
455?465.
Shingo Takamatsu, Issei Sato, and Hiroshi Nakagawa.
2012. Reducing wrong labels in distant supervi-
sion for relation extraction. In Proceedings of the
50th Annual Meeting of the Association for Compu-
tational Linguistics (ACL), pages 721?729.
Fei Wu and Daniel S. Weld. 2007. Autonomously se-
mantifying wikipedia. In Proceedings of the Inter-
national Conference on Information and Knowledge
Management (CIKM), pages 41?50.
Wei Xu, Raphael Hoffmann, Zhao Le, and Ralph Gr-
ishman. 2013. Filling knowledge base gaps for dis-
tant supervision of relation extraction. In Proceed-
ings of the 51th Annual Meeting of the Association
for Computational Linguistics (ACL).
David Yarowsky. 1995. Unsupervised word sense dis-
ambiguation rivaling supervised methods. In Pro-
ceedings of the 33th Annual Meeting of the Associa-
tion for Computational Linguistics (ACL).
Ce Zhang, Feng Niu, Christopher R?e, and Jude Shav-
lik. 2012. Big data versus the crowd: Looking for
relationships in all the right places. In Proceedings
of the 50th Annual Meeting of the Association for
Computational Linguistics, pages 825?834. Associ-
ation for Computational Linguistics.
Guodong Zhou, Jian Su, Jie Zhang, and Min Zhang.
2005. Exploring various knowledge in relation ex-
traction. In Proceedings of the Annual Meeting
of the Association for Computational Linguistics
(ACL).
738
