Proceedings of the 2013 Workshop on Biomedical Natural Language Processing (BioNLP 2013), pages 36?44,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Corpus-Driven Terminology Development: Populating Swedish
SNOMED CT with Synonyms Extracted from Electronic Health Records
Aron Henriksson1, Maria Skeppstedt1, Maria Kvist1,2, Martin Duneld1, Mike Conway3
1Department of Computer and Systems Sciences (DSV), Stockholm University, Sweden
2Department of Learning, Informatics, Management and Ethics (LIME), Karolinska Institute, Sweden
3Division of Biomedical Informatics, University of California San Diego, USA
Abstract
The various ways in which one can re-
fer to the same clinical concept needs to
be accounted for in a semantic resource
such as SNOMED CT. Developing termi-
nological resources manually is, however,
prohibitively expensive and likely to re-
sult in low coverage, especially given the
high variability of language use in clinical
text. To support this process, distributional
methods can be employed in conjunction
with a large corpus of electronic health
records to extract synonym candidates for
clinical terms. In this paper, we exem-
plify the potential of our proposed method
using the Swedish version of SNOMED
CT, which currently lacks synonyms. A
medical expert inspects two thousand term
pairs generated by two semantic spaces ?
one of which models multiword terms in
addition to single words ? for one hundred
preferred terms of the semantic types dis-
order and finding.
1 Introduction
In recent years, the adoption of standardized ter-
minologies for the representation of clinical con-
cepts ? and their textual instantiations ? has en-
abled meaning-based retrieval of information from
electronic health records (EHRs). By identify-
ing and linking key facts in health records, the
ever-growing stores of clinical documentation now
available to us can more readily be processed
and, ultimately, leveraged to improve the qual-
ity of care. SNOMED CT1 has emerged as the
de facto international terminology for represent-
ing clinical concepts in EHRs and is today used
in more than fifty countries, despite only being
1http://www.ihtsdo.org/snomed-ct/
available in a handful of languages2. Translations
into several other languages are, however, under
way3. This translation effort is essential for more
widespread integration of SNOMED CT in EHR
systems globally.
Translating a comprehensive4 terminology such
as SNOMED CT to an additional language is,
however, a massive and expensive undertaking. A
substantial part of this process involves enrich-
ing the terminology with synonyms in the tar-
get language. SNOMED CT has, for instance,
recently been translated into Swedish; however,
the Swedish version does not as yet contain syn-
onyms. Methods and tools that can accelerate the
language porting process in general and the syn-
onym identification task in particular are clearly
needed, not only to lower costs but also to in-
crease the coverage of SNOMED CT in clinical
text. Methods that can account for real-world lan-
guage use in the clinical setting, then, as well as to
changes over time, are particularly valuable.
This paper evaluates a semi-automatic method
for the extraction of synonyms of SNOMED CT
preferred terms using models of distributional se-
mantics to induce semantic spaces from a large
corpus of clinical text. In contrast to most ap-
proaches that exploit the notion of distributional
similarity for synonym extraction, this method ad-
dresses the key problem of identifying synonymy
between terms of varying length: a simple solution
is proposed that effectively incorporates the notion
of paraphrasing in a distributional framework. The
semantic spaces ? and, by extension, the method ?
are evaluated for their ability to extract synonyms
of SNOMED CT terms of the semantic types dis-
order and finding in Swedish.
2SNOMED CT is currently available in US English, UK
English, Spanish, Danish and Swedish.
3http://www.ihtsdo.org/snomed-ct/
snomed-ct0/different-languages/
4SNOMED CT contains more than 300,000 active con-
cepts and over a million relations.
36
2 Background
Synonymy is an aspect of semantics that con-
cerns the fact that concepts can be instantiated
using multiple linguistic expressions, or, viewed
conversely, that multiple linguistic expressions
can refer to the same concept. As synonymous
expressions do not necessarily consist of single
words, we sometimes speak of paraphrasing rather
than synonymy (Androutsopoulos and Malakasio-
tis, 2010). This variability of language use needs
to be accounted for in order to build high-quality
natural language processing (NLP) and text min-
ing systems. This is typically achieved by us-
ing thesauri or encoding textual instantiations of
concepts in a semantic resource, e.g. an ontol-
ogy. Creating such resources manually is, how-
ever, prohibitively expensive and likely to lead
to low coverage, especially in the clinical genre
where language use variability is exceptionally
high (Meystre et al, 2008).
2.1 Synonym Extraction
As a result, the task of extracting synonyms ?
and other semantic relations ? has long been a
central challenge in the NLP research commu-
nity, not least in the biomedical (Cohen and Hersh,
2005) and clinical (Meystre et al, 2008) do-
mains. A wide range of techniques has been pro-
posed for relation extraction in general and syn-
onym extraction in particular ? lexico-syntactic
patterns (Hearst, 1992), distributional semantics
(Dumais and Landauer, 1997) and graph-based
models (Blondel et al, 2004) ? from a variety
of sources, including dictionaries (Blondel et al,
2004), linked data such as Wikipedia (Nakayama
et al, 2007), as well as both monolingual (Hindle,
1990) and multilingual (van der Plas and Tiede-
mann, 2006) corpora. In recent years, ensemble
methods have been applied to obtain better perfor-
mance on the synonym extraction task, combin-
ing models from different families (Peirsman and
Geeraerts, 2009), with different parameter settings
(Henriksson et al, 2012) and induced from differ-
ent data sources (Wu and Zhou, 2003).
In the context of biomedicine, the goal has of-
ten been to extract synonyms of gene and pro-
tein names from the biomedical literature (Yu and
Agichtein, 2003; Cohen et al, 2005; McCrae and
Collier, 2008). In the clinical domain, Conway
and Chapman (2012) used a rule-based approach
to generate potential synonyms from the BioPor-
tal ontology web service, verifying candidate syn-
onyms against a large clinical corpus. Zeng et
al. (2012) used three query expansion methods
for information retrieval of clinical documents and
found that a model of distributional semantics ?
LDA-based topic modeling ? generated the best
synonyms. Henriksson et al (2012) combined
models of distributional semantics ? random in-
dexing and random permutation ? to extract syn-
onym candidates for Swedish MeSH5 terms and
possible abbreviation-definition pairs. In the con-
text of SNOMED CT, distributional methods have
been applied to capture synonymous relations be-
tween terms of varying length: 16-24% of English
SNOMED CT synonyms present in a large clini-
cal corpus were successfully identified in a list of
twenty suggestions (Henriksson et al, 2013).
2.2 Distributional Semantics
Models of distributional semantics (see Cohen and
Widdows (2009) for an overview of methods and
their application in the biomedical domain) were
initially motivated by the inability of the vector
space model to account for synonymy, which had
a negative impact on recall in information retrieval
systems (Deerwester et al, 1990). The theoretical
foundation underpinning such models of seman-
tics is the distributional hypothesis (Harris, 1954),
according to which words with similar meanings
tend to appear in similar contexts. By exploiting
the availability of large corpora, the meaning of
terms can be modeled based on their distribution
in different contexts. An estimate of the semantic
relatedness between terms can then be quantified,
thereby, in some sense, rendering semantics com-
putable.
An obvious application of distributional seman-
tics is the extraction of semantic relations between
terms, such as synonymy, hyp(o/er)nymy and co-
hyponymy (Panchenko, 2013). As synonyms are
interchangeable in some contexts ? and thus have
similar distributional profiles ? synonymy is cer-
tainly a semantic relation that should be captured.
However, since hyp(o/er)nyms and co-hyponyms
? in fact, even antonyms ? are also likely to have
similar distributional profiles, such semantic rela-
tions will be extracted too.
Many models of distributional semantics dif-
fer in how context vectors, representing term
5Medical Subject Headings (http://www.nlm.nih.
gov/mesh).
37
meaning, are constructed. They are typically de-
rived from a term-context matrix that contains
the (weighted, normalized) frequency with which
terms occur in different contexts. Partly due
to the intractability of working with such high-
dimensional data, it is projected into a lower-
dimensional (semantic) space, while approxi-
mately preserving the relative distances between
data points. Methods that rely on computa-
tionally expensive dimensionality reduction tech-
niques suffer from scalability issues.
Random Indexing
Random indexing (RI) (Kanerva et al, 2000) is
a scalable and computationally efficient alterna-
tive in which explicit dimensionality reduction is
avoided: a lower dimensionality d is instead cho-
sen a priori as a model parameter and the d-
dimensional context vectors are then constructed
incrementally. Each unique term in the corpus is
assigned a static index vector, consisting of ze-
ros and a small number of randomly placed 1s
and -1s6. Each term is also assigned an initially
empty context vector, which is incrementally up-
dated by adding the index vectors of the surround-
ing words within a sliding window, weighted by
their distance to the target term. The semantic re-
latedness between two terms is then estimated by
calculating, for instance, the cosine similarity be-
tween their context vectors.
Random Permutation
Random permutation (RP) (Sahlgren et al, 2008)
is a modification of RI that attempts to take into
account term order information by simply permut-
ing (i.e. shifting) the index vectors according to
their direction and distance from the target term
before they are added to the context vector. RP
has been shown to outperform RI on the synonym
part of the TOEFL7 test.
Model Parameters
The model parameters need to be configured for
the task that the semantic space is to be used
for. For instance, with a document-level con-
text definition, syntagmatic relations are mod-
eled, i.e. terms that belong to the same topic
(<car, motor, race>), whereas, with a sliding win-
dow context definition, paradigmatic relations are
6By generating sparse vectors of a sufficiently high di-
mensionality in this way, the context representations will be
nearly orthogonal.
7Test Of English as a Foreign Language
modeled (<car, automobile, vehicle>) (Sahlgren,
2006). Synonymy is an instance of a paradigmatic
relation.
The dimensionality has also been shown to be
potentially very important, especially when the
size of the vocabulary and the number of contexts8
are large (Henriksson and Hassel, 2013).
3 Materials and Methods
The task of semi-automatically identifying syn-
onyms of SNOMED CT preferred terms is here
approached by, first, statistically identifying mul-
tiword terms in the data and treating them as com-
pounds; then, performing a distributional analysis
of a preprocessed clinical corpus to induce a se-
mantic term space; and, finally, extracting the se-
mantically most similar terms for each preferred
term of interest.
The experimental setup can be broken down
into the following steps: (1) data preparation, (2)
term recognition, (3) model parameter tuning and
(4) evaluation. Semantic spaces are induced with
different parameter configurations on two dataset
variants: one with unigram terms only and one that
also includes multiword terms. The model param-
eters are tuned using MeSH, which contains syn-
onyms for Swedish. The best parameter settings
for each of the two dataset variants are then em-
ployed in the final evaluation, where a medical ex-
pert inspects one hundred term lists extracted for
SNOMED CT preferred terms belonging to the se-
mantic types disorder and finding.
3.1 Data Preparation
The data used to induce the semantic spaces is ex-
tracted from the Stockholm EPR Corpus (Dalia-
nis et al, 2009), which contains Swedish health
records from the Karolinska University Hospital
in Stockholm9. The subset (?33 million tokens)
used in these experiments comprises all forms of
text-based records ? i.e., clinical notes ? from
a large variety of clinical practices. The docu-
ments in the corpus are initially preprocessed by
simply lowercasing tokens and removing punctu-
ation and digits. Lemmatization is not performed,
as we want to be able to capture morphological
8The vocabulary size and the number of contexts are
equivalent when employing a window context definition.
9This research has been approved by the Regional Ethical
Review Board in Stockholm (Etikpro?vningsna?mnden i Stock-
holm), permission number 2012/834-31/5.
38
variants of terms; stop-word filtering is not per-
formed, as traditional stop words ? for instance,
high-frequency function words ? could potentially
be constituents of multiword terms.
3.2 Term Recognition
Multiword terms are extracted statistically from
the corpus using the C-value statistic (Frantzi and
Ananiadou, 1996; Frantzi et al, 2000). This tech-
nique has been used successfully for term recog-
nition in the biomedical domain, largely due to
its ability to handle nested terms (Zhang et al,
2008). Using the C-value statistic for term recog-
nition first requires a list of candidate terms, for
which the C-value can then be calculated. Here,
this is simply produced by extracting n-grams ?
unigrams, bigrams and trigrams ? from the corpus
with TEXT-NSP (Banerjee and Pedersen, 2003).
The statistic is based on term frequency and term
length (number of words); if a candidate term is
part of a longer candidate term (as will be the case
for practically all unigram and bigram terms), the
number and frequency of those longer terms are
also taken into account (Figure 1).
In order to improve the quality of the extracted
terms, a number of filtering rules is applied to the
generated term list: terms that begin and/or end
with certain words, e.g. prepositions and articles,
are removed. The term list ? ranked according to
C-value ? is further modified by giving priority to
terms of particular interest, e.g. SNOMED CT dis-
order and finding preferred terms: these are moved
to the top of the list, regardless of their C-value.
As a result, the statistical foundation on which the
distributional method bases its semantic represen-
tation will effectively be strengthened.
The term list is then used to perform exact string
matching on the entire corpus: multiword terms
with a higher C-value than their constituents are
concatenated. We thereby treat multiword terms as
separate (term) types with distinct distributions in
the data, different from those of their constituents.
3.3 Model Parameter Tuning
Term spaces with different parameter configu-
rations are induced from the two dataset vari-
ants: one containing only unigram terms (Uni-
gram Word Spaces) and one containing also mul-
tiword terms (Multiword Term Spaces). The fol-
lowing model parameters are tuned:
? Distributional Model: Random indexing (RI)
vs. Random permutation (RP)
? Context Window Size: 2+2, 4+4, 8+8 sur-
rounding terms (left+right of the target term)
? Dimensionality: 1000, 2000, 3000
As the Swedish version of SNOMED CT cur-
rently does not contain synonyms, it cannot be
used to perform the parameter tuning automat-
ically. This is instead done with the Swedish
version of MeSH, which is one of the very few
standard terminologies that contains synonyms for
medical terms in Swedish. However, as the op-
timal parameter configurations for capturing syn-
onymy are not necessarily identical for all seman-
tic types, the parameter tuning is performed by
evaluating the semantic spaces for their ability to
identify synonyms of MeSH terms that belong to
the categories Disease or Syndrome and Sign or
Symptom. These particular categories are simply
chosen as they, to a reasonable extent, seem to
correspond to the SNOMED CT semantic types
studied in this paper, namely Disorder and Find-
ing. Only synonym pairs that appear at least fifty
times in each of the dataset variants are included
(155 for Unigram Word Spaces and 123 for Mul-
tiword Term Spaces), as the statistical foundation
for terms that only occur rarely in the data may
not be sufficiently solid. In these Multiword Term
Spaces, the MeSH terms ? but not the synonyms
? are given precedence in the term list. A term
is provided as input to a semantic space and the
twenty semantically most similar terms are out-
put, provided that they also appear at least fifty
times in the data. Recall Top 20 is calculated for
each input term: what proportion of the MeSH
synonyms are identified in a list of twenty sugges-
tions? Since each synonym pair must appear at
least fifty times in the corresponding dataset vari-
ant, it should be duly noted that the optimization
sets will not be identical, which in turn means that
the results of the Unigram Word Spaces and the
Multiword Term Spaces are not directly compara-
ble. The optimal parameter configuration, then,
may be different when also multiword terms are
modeled.
3.4 Evaluation
The optimal parameter configuration for each
dataset variant is employed in the final evaluation.
In this Multiword Term Space, the SNOMED CT
39
C-value(a) =
{
log2 |a| ? f(a) if a is not nested
log2 |a| ? (f(a)?
1
P (Ta)
?
bTa f(b)) otherwise
a = candidate term Ta = set of extracted candidate terms that contain a
b = longer candidate terms P (Ta) = number of candidate terms in Ta
f(a) = term frequency of a f(b) = term frequency of longer candidate term b
|a| = length of candidate term (number of words)
Figure 1: C-Value Formula. The formula for calculating C-value of candidate terms.
preferred terms of interest, rather than the MeSH
terms, are prioritized in the term list. The seman-
tic spaces ? and, in effect, the method ? are pri-
marily evaluated for their ability to identify syn-
onyms of SNOMED CT preferred terms, in this
case of concepts that belong to the semantic types
disorder and finding. The need to identify syn-
onyms for these semantic types is clear, as it has
been shown that the coverage of SNOMED CT
for mentions of disorders (38%) and, in particu-
lar, findings (23%) in Swedish clinical text is low
(Skeppstedt et al, 2012). Since the Swedish ver-
sion of SNOMED CT currently lacks synonyms,
the evaluation reasonably needs to be manual, as
there is no reference standard. One option, then,
could be to choose a random sample of preferred
terms to use in the evaluation. A potential draw-
back of such a(n) (unguided) selection is that many
concepts in the English version of SNOMED CT
do not have any synonymous terms, which might
lead to evaluators spending valuable time looking
for something which does not exist. An alterna-
tive approach, which is assumed here, is to inspect
concepts that have many synonyms in the English
version of SNOMED CT. The fact that some con-
cepts have many textual instantiations in one lan-
guage does not necessarily imply that they also
have many textual instantiations in another lan-
guage. This, however, seems to be the case when
comparing the English and Swedish versions of
MeSH: terms10 that have the most synonyms in the
English version tend to have at least one synonym
in the Swedish version to a larger extent than a ran-
dom selection of terms (60% and 62% of the terms
in the Swedish version have at least one synonym
when looking at the top 100 and top 50 terms with
the most synonyms in the English version, com-
pared to 41% overall in the Swedish version).
For the two dataset variants, we thus select 25
SNOMED CT preferred terms for each semantic
10These calculations are based on MeSH terms that belong
to the categories Disease or Syndrome and Sign or Symptom.
type ? disorder and finding ? that (1) have the most
synonyms in the English version and (2) occur at
least fifty times in the data. In total, fifty terms
are input to the Unigram Word Space and another
fifty terms (potentially with some overlap) are in-
put to the Multiword Term Space. A medical ex-
pert inspects the twenty semantically most simi-
lar terms for each input term. Synonymy is here
the primary semantic relation of interest, but the
semantic spaces are also evaluated for their abil-
ity, or tendency, to extract other semantic term re-
lations: hypernyms or hyponyms, co-hyponyms,
antonyms, as well as disorder-finding relations.
4 Results
The term recognition and concatenation of mul-
tiword terms naturally affect some properties of
the dataset variants, such as the vocabulary size
(number of types) and the type-token ratio. The
Unigram Word Space contains 381,553 types and
an average of 86.54 tokens/type, while the Mul-
tiword Term Space contains 2,223,953 types and
an average of 9.72 tokens/type. This, in turn, may
have an effect on which parameter configuration is
?optimal? for the synonym extraction task. In fact,
this seems to be the case when tuning the parame-
ters for the two dataset variants. For the Unigram
Word Spaces, random indexing with a sliding con-
text window of 8+8 terms and a dimensionality of
2000 seems to work best, whereas for the Mul-
tiword Term Spaces, random permutation with a
sliding window context of 4+4 terms and a dimen-
sionality of 3000 works better (Table 1).
When these parameter configurations are ap-
plied to the SNOMED CT terms, a total of 40 syn-
onyms are extracted by the Unigram Word Space
and 33 synonyms by the Multiword Term Space
(Table 2). On average, 0.80 and 0.66 synonyms
are extracted per preferred term, respectively. The
number of identified synonyms per input term
varies significantly: for some, none; for others, up
to ten. Other semantic relations are also extracted
40
Unigram Word Spaces Multiword Term Spaces
RI RP RI RP
Sliding Window? 2+2 4+4 8+8 2+2 4+4 8+8 2+2 4+4 8+8 2+2 4+4 8+8
1000 dimensions 0.43 0.47 0.48 0.41 0.45 0.42 0.21 0.25 0.26 0.25 0.26 0.24
2000 dimensions 0.43 0.48 0.49 0.48 0.48 0.43 0.21 0.24 0.25 0.25 0.25 0.24
3000 dimensions 0.44 0.47 0.48 0.46 0.45 0.43 0.22 0.24 0.24 0.23 0.27 0.25
Table 1: Model Parameter Tuning. Results, reported as recall top 20, for MeSH synonyms that appear
at least 50 times in each of the dataset variants (unigram vs. multiword). Random indexing (RI) and
Random permutation (RP) term spaces were built with different context window sizes (2+2, 4+4, 8+8
surrounding terms) and dimensionality (1000, 2000, 3000).
by the semantic spaces: mainly co-hyponyms,
but also hypernyms and hyponyms, antonyms and
disorder-finding relations. The Unigram Word
Space extracts, on average, 0.52 hypernyms or hy-
ponyms, 1.8 co-hyponyms, 0.1 antonyms and 0.34
disorder-finding relations. The Multiword Term
Space extracts, on average, 0.16 hypernyms or
hyponyms, 1.1 co-hyponyms, 0.14 antonyms and
0.66 disorder-finding relations. In general, more
of the above semantic relations are extracted by
the Unigram Word Space than by the Multiword
Term Space (178 vs. 136). It is, however, inter-
esting to note that almost twice as many disorder-
finding relations are extracted by the latter com-
pared to the former. Of course, none of the re-
lations extracted by the Unigram Word Space in-
volve a multiword term; on the other hand, more
than half (around 57%) of the relations extracted
by the Multiword Term Space involve at least one
multiword term.
Both semantic spaces identify more synonyms
of preferred terms that belong to the semantic type
finding than disorder (in total 56 vs. 39). The same
holds true for hyp(er/o)nyms and co-hypnoyms;
however, the converse is true for antonyms and
disorder-finding relations.
5 Discussion
The results demonstrate that it is indeed possible
to extract synonyms of medical terms by perform-
ing a distributional analysis of a large corpus of
clinical text ? unigram-unigram relations, as well
as unigram-multiword and multiword-unigram re-
lations. It is also clear, however, that other se-
mantically related terms share distributional pro-
files to a similar degree as synonymous terms. The
predominance of the other semantic relations, ex-
cept for antonymy, in the term lists can reason-
ably be explained by the simple fact that there
exist more hypernyms, hyponyms, co-hyponyms
and disorder-finding relations than synonyms (or
antonyms).
It is also evident that more semantic relations,
and indeed more synonyms, are extracted by the
Unigram Word Space than the Multiword Term
Space. Again, it is important to underline that the
results cannot be compared without due qualifica-
tion since the evaluation sets are not identical: the
Unigram Word Space does not contain any mul-
tiword terms, for instance. The ability to model
multiword terms in a distributional framework and
to handle semantic composition ? i.e., how mean-
ing is, and sometimes is not, composed by the
meaning of its constituents ? has long been an en-
deavor in the NLP research community (Sag et al,
2002; Baroni and Zamparelli, 2010; Grefenstette
and Sadrzadeh, 2011; Mitchell, 2011). Treating
multiword terms as compound tokens is a simple
and rather straightforward approach, which also
makes intuitive sense: rather than treat individ-
ual words as clearly delineated bearers of mean-
ing, identify semantic units ? regardless of term
length ? and model their distributional profiles.
Unfortunately, there are problems with this ap-
proach. First, the attendant increase in vocabu-
lary size entails a lower tokens-type ratio, which
in turn means that the statistical foundation for
terms will weaken. In this case, the average token-
type ratio decreased from 86.54 to 9.72. This ap-
proach therefore requires access to a sufficiently
large corpus. Second, the inflation in vocabulary
size entails a corresponding increase in the num-
ber of vectors in the semantic space. This not only
requires more memory; to ensure that the crucial
near-orthogonality property11 of RI-based models
is maintained, the dimensionality has to be suffi-
11Random indexing assumes that the index vectors ? rep-
resenting distinct contexts ? are nearly orthogonal.
41
Unigram Word Space Multiword Term Space
DISORDER FINDING DISORDER FINDING
Synonyms
sum 18 22 16 17
average 0.72 0.88 0.64 0.68
? 1 / preferred term 12 12 8 6
involves mwe - - 10 13
Hyp(er/o)nyms
sum 12 14 4 4
average 0.48 0.56 0.16 0.16
? 1 / preferred term 6 8 4 3
involves mwe - - 3 3
Co-hyponyms
sum 34 56 22 33
average 1.36 2.24 0.88 1.32
? 1 / preferred term 14 17 10 13
involves mwe - - 19 15
Antonyms
sum 3 2 4 3
average 0.12 0.08 0.16 0.12
? 1 / preferred term 3 2 3 3
involves mwe - - 0 1
Disorder-Finding
sum 11 6 28 5
average 0.44 0.24 1.12 0.2
? 1 / preferred term 6 5 12 5
involves mwe - - 11 2
Table 2: Evaluation Results. The types of semantic relations extracted among the twenty most se-
mantically similar terms of 25 DISORDER and 25 FINDING SNOMED CT preferred terms from each
semantic space. Sum is the total number of identified relevant terms. Average is the average number of
relevant terms per preferred term. ? 1 / preferred term is the number of preferred terms for which at
least one relevant term is identified. Involves mwe is the number of relevant relations where either the
preferred term or the relevant term is a multiword expression.
ciently large in relation to the number of contexts
(represented by index vectors). In the Multiword
Term Space the vocabulary size is over two million
(compared to less than 400,000 in the Unigram
Word Space). A dimensionality of 3000 is likely
insufficient to ensure that each term type has an
initial distinct and uncorrelated representation. In
the evaluation, there were several examples where
two groups of terms ? semantically homogenous
within each group, but semantically heterogenous
across groups ? co-existed in the same term list:
these ?topics? had seemingly collapsed into the
same subspace. Despite these problems, it should
be recognized that the Multiword Term Space is, in
fact, able to retrieve 23 synonymous relations that
involve at least one multiword term. The Unigram
Word Space cannot retrieve any such relations.
The ability to extract high-quality terms would
seem to be an important prerequisite for this ap-
proach to modeling multiword terms in a distribu-
tional framework. However, despite employing a
rather simple means of extracting terms ? without
using any syntactic information ? the terms that
actually appeared in the lists of semantically re-
lated terms were mostly reasonable. This perhaps
indicates that the term recognition task does not
need to be perfect: terms of interest, of course,
need to be identified, but some noise in the form
of bad terms might be acceptable. A weakness
of the term recognition part is, however, that too
many terms were identified, which in turn led to
the aforementioned inflation in vocabulary size.
42
Limiting the number of multiword terms in the ini-
tial term list ? for instance by extracting syntactic
phrases as candidate terms ? could provide a pos-
sible solution to this problem.
Overall, more synonyms were identified for the
semantic type finding than for disorder. One pos-
sible explanation for this could be that there are
more ways of describing a finding than a disorder
? not all semantic types can be assumed to have
the same number of synonyms. The same holds
true for all other semantic relations except for dis-
order-finding, where disorders generated a much
larger number of distributionally similar findings
than vice versa. This could perhaps also be ex-
plained by the possible higher number of syn-
onyms for finding than disorder.
When this method was evaluated using the
English version of SNOMED CT, 16-24% of
known synonyms were identified (Henriksson et
al., 2013). In this case, however, we extracted
synonym candidates for terms that may or may
not have synonyms. This is thus a scenario that
more closely resembles how this method would
actually be used in a real-life setting to populate
a terminology with synonyms. Although the com-
parison with MeSH showed that terms with many
synonyms in English also tend to have at least one
synonym in Swedish, approximately 40% of them
did not have any synonyms. It is thus not certain
that the terms used in this evaluation all have at
least one synonym, which was also noted by the
evaluator in this study.
6 Conclusions
In this study, we have demonstrated a method
that could potentially be used to expedite the lan-
guage porting process of terminologies such as
SNOMED CT. With access to a large corpus of
clinical text in the target language and an initial
set of terms, this language-independent method is
able to extract and present candidate synonyms to
the lexicographer, thereby providing valuable sup-
port for semi-automatic terminology development.
A means to model multiword terms in a distri-
butional framework is an important feature of the
method and is crucial for the synonym extraction
task.
Acknowledgments
This work was partly supported by the Swedish
Foundation for Strategic Research through the
project High-Performance Data Mining for Drug
Effect Detection (ref. no. IIS11-0053) at Stock-
holm University, Sweden, and partly funded by
the Stockholm University Academic Initiative
through the Interlock project. Finally, we would
like to thank the reviewers for their constructive
feedback.
References
Ion Androutsopoulos and Prodromos Malakasiotis.
2010. A Survey of Paraphrasing and Textual En-
tailment Methods. Journal of Artificial Intelligence
Research, 38:135?187.
Satanjeev Banerjee and Ted Pedersen. 2003. The De-
sign, Implementation, and Use of the Ngram Statis-
tic Package. In Proceedings of CICLing, pages 370?
381.
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are Vectors, Adjectives are Matrices: Representing
Adjective-Noun Constructions in Semantic Space.
In Proceedings of EMNLP, pages 1183?1193.
Vincent D. Blondel, Anah?? Gajardo, Maureen Hey-
mans, Pierre Senellart, and Paul Van Dooren.
2004. A Measure of Similarity between Graph Ver-
tices: Applications to Synonym Extraction and Web
Searching. SIAM Review, 46(4):647?666.
Aaron M. Cohen and William R. Hersh. 2005. A Sur-
vey of Current Work in Biomedical Text Mining.
Briefings in Bioinformatics, 6(1):57?71.
Trevor Cohen and Dominic Widdows. 2009. Empirical
Distributional Semantics: Methods and Biomedical
Applications. J Biomed Inform, 42(2):390?405.
AM Cohen, WR Hersh, C Dubay, and K Spackman.
2005. Using co-occurrence network structure to
extract synonymous gene and protein names from
medline abstracts. BMC Bioinformatics, 6(1):103.
Mike Conway and Wendy W. Chapman. 2012. Dis-
covering Lexical Instantiations of Clinical Con-
cepts using Web Services, WordNet and Corpus Re-
sources. In AMIA Fall Symposium, page 1604.
Hercules Dalianis, Martin Hassel, and Sumithra
Velupillai. 2009. The Stockholm EPR Corpus:
Characteristics and Some Initial Findings. In Pro-
ceedings of ISHIMR, pages 243?249.
Scott Deerwester, Susan T. Dumais, George W Fur-
nas, Thomas K Landauer, and Richard Harshman.
1990. Indexing by Latent Semantic Analysis. Jour-
nal of the American Society for Information Science,
41(6):391?407.
Susan T. Dumais and Thomas K. Landauer. 1997. A
Solution to Plato?s Problem: The Latent Semantic
43
Analysis Theory of Acquisition, Induction and Rep-
resentation of Knowledge. Psychological Review,
104(2):211?240.
Katerina Frantzi and Sophia Ananiadou. 1996. Ex-
tracting Nested Collocations. In Proceedings of
COLING, pages 41?46.
Katerina Frantzi, Sophia Ananiadou, and Hideki
Mima. 2000. Automatic Recognition of Multi-
Word Terms: The C-value/NC-value Method. In-
ternational Journal on Digital Libraries, 3(2):115?
130.
Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011.
Experimental Support for a Categorical Composi-
tional Distributional Model of Meaning. In Pro-
ceedings of EMNLP, pages 1394?1404.
Zellig S. Harris. 1954. Distributional Structure. Word,
10:146?162.
Marti Hearst. 1992. Automatic Acquisition of Hy-
ponyms from Large Text Corpora. In Proceedings
of COLING, pages 539?545.
Aron Henriksson and Martin Hassel. 2013. Optimiz-
ing the Dimensionality of Clinical Term Spaces for
Improved Diagnosis Coding Support. In Proceed-
ings of Louhi.
Aron Henriksson, Hans Moen, Maria Skeppstedt, Ann-
Marie Eklund, and Vidas Daudaravicius. 2012.
Synonym Extraction of Medical Terms from Clini-
cal Text Using Combinations of Word Space Mod-
els. In Proceedings of SMBM, pages 10?17.
Aron Henriksson, Mike Conway, Martin Duneld, and
Wendy W. Chapman. 2013. Identifying Syn-
onymy between SNOMED Clinical Terms of Vary-
ing Length Using Distributional Analysis of Elec-
tronic Health Records. In AMIA Annual Symposium
(submitted).
Donald Hindle. 1990. Noun Classification from
Predicate-Argument Structures. In Proceedings of
ACL, pages 268?275.
Pentti Kanerva, Jan Kristofersson, and Anders Holst.
2000. Random Indexing of Text Samples for Latent
Semantic Analysis. In Proceedings CogSci, page
1036.
John McCrae and Nigel Collier. 2008. Synonym
Set Extraction from the Biomedical Literature by
Lexical Pattern Discovery. BMC Bioinformatics,
9(1):159.
Ste?phane M. Meystre, Guergana K. Savova, Karin C.
Kipper-Schuler, John F. Hurdle, et al 2008. Ex-
tracting Information from Textual Documents in the
Electronic Health Record: A Review of Recent Re-
search. Yearb Med Inform, 35:128?44.
Jeffrey Mitchell. 2011. Composition in Distributional
Models of Semantics. Ph.D. thesis, University of
Edinburgh.
Kotaro Nakayama, Takahiro Hara, and Shojiro Nishio.
2007. Wikipedia Mining for an Association Web
Thesaurus Construction. In Proceedings of WISE,
pages 322?334.
Alexander Panchenko. 2013. Similarity Measures for
Semantic Relation Extraction. Ph.D. thesis, PhD
thesis, Universite? catholique de Louvain & Bauman
Moscow State Technical University.
Yves Peirsman and Dirk Geeraerts. 2009. Predicting
Strong Associations on the Basis of Corpus Data. In
Proceedings of EACL, pages 648?656.
Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann
Copestake, and Dan Flickinger. 2002. Multiword
Expressions: A Pain in the Neck for NLP. In Pro-
ceedings of CICLing, pages 1?15.
Magnus Sahlgren, Anders Holst, and Pentti Kanerva.
2008. Permutations as a Means to Encode Order
in Word Space. In Proceedings of CogSci, pages
1300?1305.
Magnus Sahlgren. 2006. The Word-Space Model:
Using Distributional Analysis to Represent Syntag-
matic and Paradigmatic Relations between Words in
High-Dimensional Vector Spaces. Ph.D. thesis, PhD
thesis, Stockholm University.
Maria Skeppstedt, Maria Kvist, and Hercules Dalianis.
2012. Rule-based Entity Recognition and Coverage
of SNOMED CT in Swedish Clinical Text. In Pro-
ceedings of LREC, pages 1250?1257.
Lonneke van der Plas and Jo?rg Tiedemann. 2006.
Finding Synonyms Using Automatic Word Align-
ment and Measures of Distributional Similarity. In
Proceedings of COLING/ACL, pages 866?873.
Hua Wu and Ming Zhou. 2003. Optimizing Syn-
onym Extraction Using Monolingual and Bilingual
Resources. In Proceedings of the Second Interna-
tional Workshop on Paraphrasing, pages 72?79.
Hong Yu and Eugene Agichtein. 2003. Extracting
Synonymous Gene and Protein Terms from Biolog-
ical Literature. Bioinformatics, 19(suppl 1):i340?
i349.
Qing T Zeng, Doug Redd, Thomas Rindflesch, and
Jonathan Nebeker. 2012. Synonym, Topic Model
and Predicate-Based Query Expansion for Retriev-
ing Clinical Documents. In Proceedings AMIA An-
nual Symposium, pages 1050?9.
Ziqi Zhang, Jose? Iria, Christopher Brewster, and Fabio
Ciravegna. 2008. A Comparative Evaluation of
Term Recognition Algorithms. In Proceedings of
LREC.
44
Proceedings of the 3rd Workshop on Predicting and Improving Text Readability for Target Reader Populations (PITR) @ EACL 2014, pages 57?65,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Medical text simplification using synonym replacement:
Adapting assessment of word difficulty to a compounding language
Emil Abrahamsson
1
Timothy Forni
1
Maria Skeppstedt
1
Maria Kvist
1,2
1
Department of Computer and Systems Sciences (DSV)
Stockholm University, Sweden
{emab6827, tifo6794, mariask}@dsv.su.se
2
Department of Learning, Informatics, Management and Ethics (LIME)
Karolinska Institutet, Sweden
maria.kvist@karolinska.se
Abstract
Medical texts can be difficult to under-
stand for laymen, due to a frequent occur-
rence of specialised medical terms. Re-
placing these difficult terms with eas-
ier synonyms can, however, lead to im-
proved readability. In this study, we have
adapted a method for assessing difficulty
of words to make it more suitable to med-
ical Swedish. The difficulty of a word
was assessed not only by measuring the
frequency of the word in a general cor-
pus, but also by measuring the frequency
of substrings of words, thereby adapt-
ing the method to the compounding na-
ture of Swedish. All words having a
MeSH synonym that was assessed as eas-
ier, were replaced in a corpus of medical
text. According to the readability measure
LIX, the replacement resulted in a slightly
more difficult text, while the readability
increased according to the OVIX measure
and to a preliminary reader study.
1 Introduction
Our health, and the health of our family and
friends, is something that concerns us all. To be
able to understand texts from the medical domain,
e.g. our own health record or texts discussing sci-
entific findings related to our own medical prob-
lems, is therefore highly relevant for all of us.
Specialised terms, often derived from latin or
greek, as well as specialised abbreviations, are,
however, often used in medical texts (Kokkinakis
and Toporowska Gronostaj, 2006). This has the
effect that medical texts can be difficult to compre-
hend (Keselman and Smith, 2012). Comprehend-
ing medical text might be particularly challenging
for those laymen readers who are not used to look-
ing up unknown terms while reading. A survey of
Swedish Internet users showed, for instance, that
users with a long education consult medical infor-
mation available on the Internet to a much larger
extent than users with a shorter education (Find-
ahl, 2010, pp. 28?35). This discrepancy between
different user groups is one indication that meth-
ods for simplifying medical texts are needed, to
make the medical information accessible to every-
one.
Previous studies have shown that replacing dif-
ficult words with easier synonyms can reduce the
level of difficulty in a text. The level of diffi-
culty of a word was, in these studies, determined
by measuring its frequency in a general corpus of
the language; a measure based on the idea that
frequent words are easier than less frequent, as
they are more familiar to the reader. This syn-
onym replacement method has been evaluated on
medical English text (Leroy et al., 2012) as well
as on Swedish non-medical text (Keskis?arkk?a and
J?onsson, 2012). To the best of our knowledge, this
method has, however, not previously been evalu-
ated on medical text written in Swedish. In ad-
dition, as Swedish is a compounding language,
laymen versions of specialised medical terms are
often constructed by compounds of every-day
Swedish words. Whether a word consists of easily
understandable constituents, is a factor that also
ought to be taken into account when assessing the
difficulty of a word.
The aim of our study was, therefore, to in-
vestigate if synonym replacement based on term
frequency could be successfully applied also on
Swedish medical text, as well as if this method
could be further developed by adapting it to the
compounding nature of Swedish.
2 Background
The level of difficulty varies between different
types of medical texts (Leroy et al., 2006), but
studies have shown that even brochures intended
57
for patients, or websites about health issues, can be
difficult to comprehend (Kokkinakis et al., 2012;
Leroy et al., 2012). Bio-medical texts, such as
medical journals, are characterised by sentences
that have high informational and structural com-
plexity, thus containing a lot of technical terms
(Friedman et al., 2002). An abundance of med-
ical terminology and a frequent use of abbrevia-
tions form, as previously mentioned, a strong bar-
rier for comprehension when laymen read medical
text. Health literacy is a much larger issue than
only the frequent occurrence of specialised terms;
an issue that includes many socio-economic fac-
tors. The core of the issue is, however, the read-
ability of the text, and adapting word choice to the
reader group (Zeng et al., 2005; Leroy et al., 2012)
is a possible method to at least partly improve the
readability of medical texts.
Semi-automatic adaption of word choice has
been evaluated on English medical text (Leroy et
al., 2012) and automatic adaption on Swedish non-
medical text (Keskis?arkk?a and J?onsson, 2012).
Both studies used synonym lexicons and replaced
words that were difficult to understand with more
easily understandable synonyms. The level of dif-
ficulty of a word was determined by measuring its
frequency in a general corpus. The English study
based its figures for word frequency on the number
of occurrences of a word in Google?s index of En-
glish language websites, while the Swedish study
used the frequency of a word in the Swedish Pa-
role corpus (Gellerstam et al., 2000), which is a
corpus compiled from several sources, e.g. news-
paper texts and fiction.
The English study used English WordNet as the
synonym resource, and difficult text was trans-
formed by a medical librarian, who chose eas-
ier replacements for difficult words among candi-
dates that were presented by the text simplifica-
tion system. Also hypernyms from semantic cat-
egories in WordNet, UMLS and Wiktionary were
used, but as clarifications for difficult words (e.g.
in the form: ?difficult word, a kind of semantic cat-
egory?). A frequency cut-off in the Google Web
Corpus was used for distinguishing between easy
and difficult words. The study was evaluated by
letting readers 1) assess perceived difficulty in 12
sentences extracted from medical texts aimed at
patients, and 2) answer multiple choice questions
related to paragraphs of texts from the same re-
source, in order to measure actual difficulty. The
evaluations showed that perceived difficulty was
significantly higher before the transformation, and
that actual difficulty was significantly higher for
one combination of medical topic and test setting.
The Swedish study used the freely available
SynLex as the resource for synonyms, and one
of the studied methods was synonym replacement
based on word frequency. The synonym replace-
ment was totally automatic and no cut-off was
used for distinguishing between familiar and rare
words. The replacement algorithm instead re-
placed all words which had a synonym with a
higher frequency in the Parole corpus than the fre-
quency of the original word. The effect of the
frequency-based synonym replacement was auto-
matically evaluated by applying the two Swedish
readability measures LIX and OVIX on the orig-
inal and on the modified text. Synonym replace-
ment improved readability according to these two
measures for all of the four studied Swedish text
genres: newspaper texts, informative texts from
the Swedish Social Insurance Agency, articles
from a popular science magazine and academic
texts.
For synonym replacement to be a meaningful
method for text simplification, there must exist
synonyms that are near enough not to change the
content of what is written. Perfect synonyms are
rare, as there is typically at least one aspect in
which two separate words within a language dif-
fer; if it is not a small difference in meaning, it
might be in the context in which they are typi-
cally used (Saeed, 1997). For describing med-
ical concepts, there is, however, often one set
of terms that are used by health professionals,
whereas another set of laymen?s terms are used by
patients (Leroy and Chen, 2001; Kokkinakis and
Toporowska Gronostaj, 2006). This means that
synonym replacement could have a large poten-
tial for simplifying medical text, as there are many
synonyms within this domain, for which the dif-
ference mainly lies in the context in which they
are typically used.
The availability of comprehensive synonym re-
sources is another condition for making it possi-
ble to implement synonym replacement for text
simplification. For English, there is a consumer
health vocabulary initiative connecting laymen?s
expressions to technical terminology (Keselman
et al., 2008), as well as several medical termi-
58
Original Med r?ontgen kan man se en ?okad trabekulering, osteoporos
samt pseudofrakturer.
Transformed Med r?ontgen kan man se en ?okad trabekulering, bensk?orhet
samt pseudofrakturer.
Translated original With X-ray, one can see an increased trabeculation, osteoporosis
and pseudo-fractures.
Translated transformed With X-ray, one can see an increased trabeculation, bone-brittleness
and pseudo-fractures.
Table 1: An example of how the synonym replacement changes a word in a sentence.
nologies containing synonymic expressions, e.g.
MeSH
1
and SNOMED CT
2
. Swedish, with fewer
speakers, also has fewer lexical resources than En-
glish, and although SNOMED CT was recently
translated to Swedish, the Swedish version does
not contain any synonyms. MeSH on the other
hand, which is a controlled vocabulary for index-
ing biomedical literature, is available in Swedish
(among several other languages), and contains
synonyms and abbreviations for medical concepts
(Karolinska Institutet, 2012).
Swedish is, as previously mentioned, a com-
pounding language, with the potential to create
words expressing most of all imaginable concepts.
Laymen?s terms for medical concepts are typi-
cally descriptive and often consist of compounds
of words used in every-day language. The word
humerusfraktur (humerus fracture), for instance,
can also be expressed as ?overarmsbenbrott, for
which a literal translation would be upper-arm-
bone-break. That a compound word with many
constituents occurring in standard language could
be easier to understand than the technical terms
of medical terminology, forms the basis for our
adaption of word difficulty assessment to medical
Swedish.
3 Method
We studied simplification of one medical text
genre; medical journal text. The replacement
method, as well as the main evaluation method,
was based on the previous study by Keskis?arkk?a
and J?onsson (2012). The method for assessing
word difficulty was, however, further developed
compared to this previous study.
As medical journal text, a subset of the journal
L?akartidningen, the Journal of the Swedish Med-
ical Association (Kokkinakis, 2012), was used.
1
www.nlm.nih.gov/mesh/
2
www.ihtsdo.org
The subset consisted of 10 000 randomly selected
sentences from issues published in 1996. As syn-
onym lexicon, the Swedish version of MeSH was
used. This resource contains 10 771 synonyms,
near synonyms, multi-word phrases with a very
similar meaning and abbreviation/expansion pairs
(all denoted as synonyms here), belonging to 8 176
concepts.
Similar to the study by Keskis?arkk?a and J?onsson
(2012), the Parole corpus was used for frequency
statistics. For each word in the L?akartidningen
subset, it was checked whether the word had a syn-
onym in MeSH. If that was the case, and if the
synonym was more frequently occurring in Parole
than the original word, then the original word was
replaced with the synonym. An example of a sen-
tence changed by synonym replacement is shown
in Table 1.
There are many medical words that only rarely
occur in general Swedish, and therefore are not
present as independent words in a corpus of stan-
dard Swedish, even if constituents of the words
frequently occur in the corpus. The method used
by Keskis?arkk?a and J?onsson was further developed
to handle these cases. This development was built
on the previously mentioned idea that a compound
word with many constituents occurring in standard
language is easier to understand than a rare word
for which this is not the case. When neither the
original word, nor the synonym, occurred in Pa-
role, a search in Parole was therefore instead car-
ried out for substrings of the words. The original
word was replaced by the synonym, in cases when
the synonym consisted of a larger number of sub-
strings present in Parole than the original word.
To insure that the substrings were relevant words,
they had to consist of a least four characters.
Exemplified by a sentence containing the word
hemangiom (hemangioma), the extended replace-
ment algorithm would work as follows: The al-
59
gorithm first detects that hemangiom has the syn-
onym blodk?arlstum?or (blood-vessel-tumour) in
MeSH. It thereafter establishes that neither he-
mangiom nor blodk?arlstum?or is included in the
Parole corpus, and therefore instead tries to find
substrings of the two words in Parole. For he-
mangiom, no substrings are found, while four
substrings are found for blodk?arlstum?or (Table
2), and therefore hemangiom is replaced by
blodk?arlstum?or.
Word 1 2 3 4
hemangiom - - - -
blodk?arlstum?or blod k?arl blodk?arl tum?or
Table 2: Example of found substrings
As the main evaluation of the effect of the syn-
onym replacement, the two readability measures
used by Keskis?arkk?a and J?onsson were applied,
on the original as well as on the modified text.
LIX (l?asbarhetsindex, readability measure) is the
standard metric used for measuring readability of
Swedish texts, while OVIX (ordvariationsindex,
word variation index) measures lexical variance,
thereby reflecting the size of vocabulary in the text
(Falkenjack et al., 2013).
The two metrics are defined as follows
(M?uhlenbock and Johansson Kokkinakis, 2009):
LIX =
O
M
+
L ? 100
O
Where:
? O = number of words in the text
? M = number of sentences in the text
? L = number of long words in the text (more
than 6 characters)
OVIX =
log(O)
log(2?
log(U)
log(O)
)
Where:
? O = number of words in the text
? U = number of unique words in the text
The interpretation of the LIX value is shown in
Table 3, while OVIX scores ranging from 60 to 69
indicate easy-to-read texts (M?uhlenbock and Jo-
hansson Kokkinakis, 2009).
LIX-value Genre
less than 25 Children?s books
25-30 Easy texts
30-40 Normal text/fiction
40-50 Informative texts
50-60 Specialist literature
more than 60 Research, dissertations
Table 3: The LIX-scale, from M?uhlenbock and Jo-
hansson Kokkinakis (2009)
To obtain preliminary results from non-
automatic methods, a very small manual evalua-
tion of correctness and perceived readability was
also carried out. A randomly selected subset of
the sentences in which at least one term had been
replaced were classified into three classes by a
physician: 1) The original meaning was retained
after the synonym replacement, 2) The original
meaning was only slightly altered after the syn-
onym replacement, and 3) The original meaning
was altered more than slightly after the synonym
replacement. Sentences classified into the first cat-
egory by the physician were further categorised
for perceived readability by two other evaluators;
both with university degrees in non-life science
disciplines. The original and the transformed sen-
tence were presented in random order, and the
evaluators were only informed that the simplifica-
tion was built on word replacement. The follow-
ing categories were used for the evaluation of per-
ceived readability: 1) The two presented sentences
are equally easy/difficult to understand, 2) One of
the sentences is easier to understand than the other.
In the second case, the evaluator indicated which
sentence was easier.
4 Results
In the used corpus subset, which contained
150 384 tokens (26 251 unique), 4 909 MeSH
terms for which there exist a MeSH synonym were
found. Among these found terms, 1 154 were
replaced with their synonym. The 15 most fre-
quently replaced terms are shown in Table 4, many
of them being words typical for a professional lan-
guage that have been replaced with compounds
of every-day Swedish words, or abbreviations that
have been replaced by an expanded form.
The total number of words increased from
150 384 to 150 717 after the synonym replace-
60
Original term (English) Replaced with (Literal translation) n
aorta (aorta) kroppspuls?ader (body-artery) 34
kolestas (cholestasis) gallstas (biliary-stasis) 33
angio?odem (angioedema) angioneurotiskt ?odem (angio-neurotic-oedema) 29
stroke (stroke) slaganfall (strike-seizure) 29
TPN (TPN) parenteral n?aring, total (parenteral nutrition, total) 26
GCS (GCS) Glasgow Coma Scale (Glasgow Coma Scale) 20
mortalitet (mortality) d?odlighet (deathrate) 20
?odem (oedema) svullnad (swelling) 20
legitimation (licensure) licens (certificate) 18
RLS (RLS) rastl?osa ben-syndrom (restless legs-syndrome) 18
anemi (anemia) blodbrist (blood-shortage) 17
anh?origa (family) familj (family) 17
ekokardiografi (echocardiography) hj?artultraljuds- (heart-ultrasound 17
unders?okning -examination)
artrit (arthritis) ledinflammation (joint-inflammation) 16
MHC (MHC) histokompatibilitets- (histocompatibility- 15
komplex complex)
Table 4: The 15 most frequently replaced terms. As the most frequent synonym (or synonym with most
known substrings) is always chosen for replacement, the same choice among a number of synonyms, or
a number of abbreviation expansions, will always be made. The column n contains the number of times
the original term was replaced with this synonym.
ment. Also the number of long words (more than
six characters) increased from 51 530 to 51 851.
This resulted in an increased LIX value, as can be
seen in Table 5. Both before and after the transfor-
mation, the LIX-value lies on the border between
the difficulty levels of informative texts and non-
fictional texts. The replacement also had the effect
that the number of unique words decreased with
138 words, which resulted in a lower OVIX, also
to be seen in Table 5.
For the manual evaluation, 195 sentences, in
which at least one term had been replaced, were
randomly selected. For 17% of these sentences,
the original meaning was slightly altered, and for
10%, the original meaning was more than slightly
altered. The rest of the sentences, which re-
tained their original meaning, were used for mea-
suring perceived readability, resulting in the fig-
ures shown in Table 6. Many replaced terms oc-
curred more than once among the evaluated sen-
tences. Therefore, perceived difficulty was also
measured for a subset of the evaluation data, in
which it was ensured that each replaced term oc-
curred exactly once, by only including the sen-
tence in which it first appeared. These subset fig-
ures (denoted Unique in Table 6) did, however,
only differ marginally from the figures for the en-
tire set. Although there was a large difference be-
tween the two evaluators in how they assessed the
effect of the synonym replacement, they both clas-
sified a substantially larger proportion of the sen-
tences as easier to understand after the synonym
replacement.
LIX OVIX
Original text 50 87.2
After synonym replacement 51 86.9
Table 5: LIX and OVIX before and after synonym
replacement
5 Discussion
According to the LIX measure, the medical text
became slightly more difficult to read after the
transformation, which is the opposite result to
that achieved in the study by Keskis?arkk?a and
J?onsson (2012). Similar to this previous study,
however, the text became slightly easier to read
according to the OVIX measure, as the number
of unique words decreased. As words longer
than six characters result in a higher LIX value, a
very plausible explanation for the increased LIX-
value, is that short words derived from Greek or
Latin have been replaced with longer compounds
61
Perceived effect Evaluator 1 Evaluator 2
of replacement All (Unique) All (Unique)
No difference 51% (52%) 29% (28%)
Easier 42% (42%) 54% (52%)
More difficult 7% (7%) 17% (21%)
Table 6: Results for the manual classification
of perceived difficulty. Evaluator 1 classified
143 sentences and Evaluator 2 classified 140 sen-
tences. The (Unique) column contains results
when only the first a occurrence of a replacement
of a particular term is included. A binomial sign
test (Newbold et al., 2003, p. 532) was performed
on the effect of the replacement, with the null hy-
pothesis that the probability of creating a more dif-
ficult sentence was equal to that of creating an eas-
ier one. This hypothesis could be rejected for both
evaluators; when including all sentences and also
when only including the (Unique) subset, show-
ing that the differences were statistically signifi-
cant (p0.01).
of every-day words. Replacing an abbreviation or
an acronym with its expanded long form has the
same effect. Expanding acronyms also increases
the number of words per sentence, which also re-
sults in a higher LIX value.
Studies on English medical text indicate, how-
ever, that simple surface measures do not accu-
rately reflect the readability (Zeng-Treitler et al.,
2007; Wu et al., 2013), and user studies have been
performed to construct readability measures bet-
ter adapted to the domain of medical texts (Kim et
al., 2007; Leroy and Endicott, 2012). Therefore,
although the manual evaluation was very limited
in scope, the results from this evaluation might
give a better indication of the effects of the sys-
tem. This evaluation showed that the perceived
readability often improved with synonym replace-
ment, although there were also replacements that
resulted in a decrease of perceived readability.
Further studies are required to determine whether
these results are generalisable to a larger group of
readers. Such studies should also include an eval-
uation of actual readability, using methods similar
to those of Leroy et al. (2012). The cases, in which
the synonym replacement resulted in a perceived
decrease in readability should also be further stud-
ied. It might, for instance, be better to use a fre-
quency cut-off for distinguishing between rare and
frequent words, as applied by Leroy et al. (2012),
rather than always replacing a word with a more
frequent synonym.
The manual evaluation also showed that the
original semantic meaning had been at least
slightly altered in almost a third of the sentences,
which shows that the set of synonyms in Swedish
MeSH might need to be adapted to make the syn-
onyms suitable to use in a text simplification sys-
tem. The replacements in Table 4 show three
types of potential problems. First, there are also
distant synonyms, as exemplified by oedema and
swelling, where oedema means a specific type of
swelling in the form of increased amount of liq-
uid in the tissues, as opposed to e.g. increased
amount of fat. Second, the MeSH terms are not
always written in a form that is appropriate to use
in running text, such as the term parenteral nu-
trition, total. Such terms need to be transformed
to another format before they can be used for au-
tomatic synonym replacement. Third, although
the abbreviations included in the manual evalua-
tion were all expanded to the correct form, ab-
breviations within the medical domain are often
overloaded with a number of different meanings
(Liu et al., 2002). For instance, apart from be-
ing an acronym for restless legs syndrome, RLS
can also mean reaction level scale (Cederblom,
2005). Therefore, in order to include abbrevia-
tions and acronyms in the synonym replacement
method studied here, an abbreviation disambigua-
tion needs to be carried out first (Gaudan et al.,
2005; Savova et al., 2008). An alternative could
be to automatically detect which abbreviations and
acronyms that are defined in the text when they
first are mentioned (Dann?ells, 2006), and restrict
the replacement method to those.
The sentence in Table 1 shows an example of
a successful synonym replacement, replacing a
word typically used by health professionals (os-
teoporosis) with a word typically used in every-
day language (bone-brittleness). This sentence
also gives an example of when not enough is
replaced in the sentence for it to be easy to
understand. Neither trabeculation, nor pseudo-
fractures, are included in MeSH, which shows the
importance of having access to comprehensive ter-
minological resources for the method of synonym
replacement to be successful. Extracting terms
that are frequently occurring within the text genre
that is to be simplified, but which are neither in-
cluded in the used terminology, nor in a corpus
62
of standard language such as Parole, could be a
method for finding candidates for expanding the
terminological resources. Semi-automatic meth-
ods could be applied for finding synonyms to these
new candidate terms, as well as to existing terms
within the terminology for which no synonyms are
provided (Henriksson et al., 2013).
Table 1 also exemplifies a further issue not ad-
dressed here, namely the frequent occurrence of
inflected words in Swedish text. No morphologic
normalisation, e.g. lemmatisation, was performed
of the text that was to be simplified or of the
terms in MeSH (e.g. normalising pseudo-fractures
to pseudo-fracture). Such a normalisation would
have the potential of matching, and thereby replac-
ing, a larger number of words, but it would also re-
quire that the replaced word is inflected to match
the grammatical form of the original word.
An alternative to using frequency in the Parole
corpus, or occurrence of substrings in a word in
Parole, for determining when a synonym is to be
replaced, is to use the frequency in a medical cor-
pus. That corpus then has to be targeted towards
laymen, as word frequency in texts targeted to-
wards health professionals would favour word re-
placements with words typical to the professional
language. Examples of such patient corpora could
be health related web portals for patients (Kokki-
nakis, 2011). However, as also texts targeted to-
wards patients have been shown to be difficult to
understand, the method of searching for familiar
words in substrings of medical terms might be
relevant for assessing word difficulty also if easy
medical corpora would be used.
6 Future work
A number of points for future work have al-
ready been mentioned, among which evaluating
the method on a large set of target readers has
the highest priority. Adapting the method to han-
dle inflected words, studying how near synonyms
and ambiguity of abbreviations affect the content
of the transformed sentences, as well as studying
methods for semi-automatic expansion of termi-
nologies, are other topics that have already been
mentioned.
It might also be the case that what synonym re-
placements are suitable are dependent on the con-
text in which a word occurs. Methods for adapting
assessment of word difficulty to context have been
studied within the Semeval-2012 shared task on
English lexical simplification (Specia et al., 2012),
although it was shown that infrequent words are
generally perceived as more difficult, regardless of
context.
In addition to these points, it should be noted
that we in this study have focused on one type
medical text, i.e. medical journal text. As men-
tioned in the introduction, there is, however,
another medical text type on which applying
text simplification would also be highly relevant,
namely health record text (Kvist and Velupillai,
2013; Kandula et al., 2010). The electronic health
record is nowadays made available to patients via
e-services in a number of countries, and there is
also an on-going project constructing such a ser-
vice in Sweden. Apart from health record text
also containing many words derived from greek
and latin, there are additional challenges associ-
ated with this type of text. As health record text is
written under time pressure, it is often written in
a telegraphic style with incomplete sentences and
many abbreviations (Friedman et al., 2002; Aan-
taa, 2012). As was exemplified among the top 15
most frequently replaced words, abbreviations is
one of the large problems when using the synonym
replacement method for text simplification, as they
are often overloaded with a number of meanings.
Future work, therefore, also includes the eval-
uation of synonym replacement on health record
text. It also includes the study of writing tools for
encouraging health professionals to produce text
that is easier to understand for the patient, or at
least easier to transform into more patient-friendly
texts with methods similar to the method studied
here (Ahltorp et al., 2013).
7 Conclusion
A method used in previous studies for assess-
ing difficulty of words in Swedish text was fur-
ther developed. The difficulty of a word was as-
sessed not only by measuring the frequency of
the word in a general corpus, but also by measur-
ing the frequency of substrings of words, thereby
adapting the method to the compounding nature of
Swedish. The replacement was mainly evaluated
by the two readability measures LIX and OVIX,
showing a slightly decreased OVIX but a slightly
increased LIX. A preliminary study on readers
showed, however, an increased perceived readabil-
ity after the synonym replacement. Studies on a
larger reader group are required to draw any con-
63
clusions on the general effect of the method for as-
sessment of word difficult. The preliminary results
are, however, encouraging, showing that a method
that replaces specialised words derived from latin
and greek by compounds of every-day Swedish
words can result in a increase of the perceived
readability.
Acknowledgements
We would like to thank the three reviewers for
many useful comments. This work was partly sup-
ported by a grant from the V?ardal Foundation
References
Kirsi Aantaa. 2012. Mot patientv?anligare epikriser,
en kontrastiv unders?okning [towards more patient
friendly discharge letters, a contrastive study]. Mas-
ter?s thesis, Department of Nordic Languages, Uni-
versity of Turku.
Magnus Ahltorp, Maria Skeppstedt, Hercules Dalianis,
and Maria Kvist. 2013. Using text prediction for fa-
cilitating input and improving readability of clinical
text. Stud Health Technol Inform, 192:1149.
Staffan Cederblom. 2005. Medicinska f?orkortningar
och akronymer (In Swedish). Studentlitteratur,
Lund.
Dana Dann?ells. 2006. Automatic acronym recogni-
tion. In Proceedings of the 11th conference on Eu-
ropean chapter of the Association for Computational
Linguistics (EACL).
Johan Falkenjack, Katarina Heimann M?uhlenbock, and
Arne J?onsson. 2013. Features indicating readability
in Swedish text. In Proceedings of the 19th Nordic
Conference of Computational Linguistics (NODAL-
IDA 2013), pages 27?40.
Olle Findahl. 2010. Svenskarna och Internet. .se.
Carol Friedman, Pauline Kra, and Andrey Rzhetsky.
2002. Two biomedical sublanguages: a description
based on the theories of zellig harris. J Biomed In-
form, 35(4):222?35, Aug.
S. Gaudan, H. Kirsch, and D. Rebholz-Schuhmann.
2005. Resolving abbreviations to their senses
in medline. Bioinformatics, 21(18):3658?3664,
September.
M Gellerstam, Y Cederholm, and T Rasmark. 2000.
The bank of Swedish. In LREC 2000. The 2nd In-
ternational Conference on Language Resources and
Evaluation, pages 329?333, Athens, Greece.
Aron Henriksson, Mike Conway, Martin Duneld, and
Wendy W. Chapman. 2013. Identifying syn-
onymy between SNOMED clinical terms of vary-
ing length using distributional analysis of electronic
health records. In Proceedings of the Annual Sym-
posium of the American Medical Informatics Asso-
ciation (AMIA 2013), Washington DC, USA.
Sasikiran Kandula, Dorothy Curtis, and Qing Zeng-
Treitler. 2010. A semantic and syntactic text sim-
plification tool for health content. AMIA Annu Symp
Proc, 2010:366?70.
Karolinska Institutet. 2012. Hur man anv?ander
den svenska MeSHen (In Swedish, trans-
lated as: How to use the Swedish MeSH).
http://mesh.kib.ki.se/swemesh/manual se.html.
Accessed 2012-03-10.
Alla Keselman and Catherine Arnott Smith. 2012. A
classification of errors in lay comprehension of med-
ical documents. Journal of Biomedical Informatics,
45(6):1151?1163.
Alla Keselman, Robert Logan, Catherine Arnott Smith,
Gondy Leroy, and Qing Zeng-Treitler. 2008. Devel-
oping informatics tools and strategies for consumer-
centered health communication. In J Am Med In-
form Assoc, volume 15:4, pages 473?483.
Robin Keskis?arkk?a and Arne J?onsson. 2012. Auto-
matic text simplification via synonym replacement.
In Proceedings of Swedish Language Technology
Conference 2012.
Hyeoneui Kim, Sergey Goryachev, Graciela Rosem-
blat, Allen Browne, Alla Keselman, and Qing Zeng-
Treitler. 2007. Beyond surface characteristics: a
new health text-specific readability measurement.
AMIA Annu Symp Proc, pages 418?422.
Dimitrios Kokkinakis and Maria Toporowska Gronos-
taj. 2006. Lay language versus professional lan-
guage within the cardiovascular subdomain - a con-
trastive study. In Proceedings of the 2006 WSEAS
Int. Conf. on Cellular & Molecular Biology, Bio-
physics & Bioengineering.
Dimitrios Kokkinakis, Markus Forsberg, Sofie Johans-
son Kokkinakis, Frida Smith, and Joakim
?
Ohl?en.
2012. Literacy demands and information to cancer
patients. In Proceedings of the 15th International
Conference on Text, Speech and Dialogue, pages
64?71.
Dimitrios Kokkinakis. 2011. Evaluating the coverage
of three controlled health vocabularies with focus on
findings, signs and symptoms. In NEALT Proceed-
ings Series, editor, NODALIDA, volume 12, pages
27?31.
Dimitrios Kokkinakis. 2012. The journal of the
Swedish medical association - a corpus resource for
biomedical text mining in Swedish. In The Third
Workshop on Building and Evaluating Resources for
Biomedical Text Mining (BioTxtM), an LREC Work-
shop. Turkey.
64
Maria Kvist and Sumithra Velupillai. 2013. Profes-
sional language in swedish radiology reports ? char-
acterization for patient-adapted text simplification.
In Scandinavian Conference on Health Informatics,
Copenhagen, Denmark, August.
Gondy Leroy and Hsinchun Chen. 2001. Meeting
medical terminology needs-the ontology-enhanced
medical concept mapper. IEEE Transactions on
Information Technology in Biomedicine, 5(4):261?
270.
Gondy Leroy and James E. Endicott. 2012. Com-
bining nlp with evidence-based methods to find text
metrics related to perceived and actual text difficulty.
In IHI, pages 749?754.
Gondy Leroy, Evren Eryilmaz, and Benjamin T.
Laroya. 2006. Health information text character-
istics. In AMIA Annu Symp Proc, pages 479?483.
Gondy Leroy, James E Endicott, Obay Mouradi, David
Kauchak, and Melissa L Just. 2012. Improving
perceived and actual text difficulty for health infor-
mation consumers using semi-automated methods.
AMIA Annu Symp Proc, 2012:522?31.
Hongfang Liu, Alan R Aronson, and Carol Friedman.
2002. A study of abbreviations in medline abstracts.
Proc AMIA Symp, pages 464?8.
Katarina M?uhlenbock and Sofie Johansson Kokkinakis.
2009. Lix 68 revisited - an extended readabil-
ity measure. In Proceedings of Corpus Linguistics
2009.
Paul Newbold, William L. Carlson, and Betty Thorne.
2003. Statistics for business and economics.
Prentice-Hall, Upper Saddle River, N. J., 5. ed. edi-
tion.
John I. Saeed. 1997. Semantics. Blackwell Publishers,
Oxford.
Guergana K. Savova, Anni Coden, Igor L. Sominsky,
Rie Johnson, Philip V. Ogren, Piet C. de Groen, and
Christopher G. Chute. 2008. Word sense disam-
biguation across two domains: Biomedical literature
and clinical notes. Journal of Biomedical Informat-
ics, 41(6):1088?1100.
Lucia Specia, Sujay Kumar Jauhar, and Rada Mihal-
cea. 2012. Semeval-2012 task 1: English lexical
simplification. In *SEM, First Joint Conference on
Lexical and Computational Semantics, pages 347?
355, Montr?eal, Canada.
Danny T Y Wu, David A Hanauer, Qiaozhu Mei, Pa-
tricia M Clark, Lawrence C An, Jianbo Lei, Joshua
Proulx, Qing Zeng-Treitler, and Kai Zheng. 2013.
Applying multiple methods to assess the readability
of a large corpus of medical documents. Stud Health
Technol Inform, 192:647?51.
Qing T. Zeng, Tony Tse, Jon Crowell, Guy Divita,
Laura Roth, and Allen C. Browne. 2005. Identify-
ing consumer-friendly display (cfd) names for health
concepts. In Proceedings of AMIA Annual Sympo-
sium, pages 859?863.
Qing Zeng-Treitler, Hyeoneui Kim, Sergey Goryachev,
Alla Keselman, Laura Slaughter, and Catherine. A.
Smith. 2007. Text characteristics of clinical reports
and their implications for the readability of personal
health records. Medinfo, 12(Pt 2):1117?1121.
65
Proceedings of the 3rd Workshop on Predicting and Improving Text Readability for Target Reader Populations (PITR) @ EACL 2014, pages 74?83,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Improving Readability of Swedish Electronic Health Records
through Lexical Simplification: First Results
Gintar
?
e Grigonyt
?
e
a
, Maria Kvist
bc
, Sumithra Velupillai
b
, Mats Wir
?
en
a
a
Department of Linguistics, Stockholm University, Sweden
b
Department of Computer and Systems Sciences, Stockholm University, Sweden
c
Department of Learning, Informatics, Management and Ethics, Karolinska Institutet, Sweden
gintare@ling.su.se, maria.kvist@karolinska.se,
sumithra@dsv.su.se, mats.wiren@ling.su.se
Abstract
This paper describes part of an ongo-
ing effort to improve the readability of
Swedish electronic health records (EHRs).
An EHR contains systematic documenta-
tion of a single patient?s medical history
across time, entered by healthcare pro-
fessionals with the purpose of enabling
safe and informed care. Linguistically,
medical records exemplify a highly spe-
cialised domain, which can be superfi-
cially characterised as having telegraphic
sentences involving displaced or missing
words, abundant abbreviations, spelling
variations including misspellings, and ter-
minology. We report results on lexical
simplification of Swedish EHRs, by which
we mean detecting the unknown, out-of-
dictionary words and trying to resolve
them either as compounded known words,
abbreviations or misspellings.
1 Introduction
An electronic health record (EHR; Swedish: pa-
tientjournal) contains systematic documentation
of a single patient?s medical history across time,
entered by healthcare professionals with the pur-
pose of enabling safe and informed care. The
value of EHRs is further increased by the fact that
they provide a source of information for statis-
tics and research, and a documentation for the pa-
tient through the Swedish Patient Data Act. EHRs
collect information from a range of sources, such
as administration of drugs and therapies, test re-
sults, preoperative notes, operative notes, progress
notes, discharge notes, etc.
EHRs contain both structured parts (such as
details about the patient, lab results, diagnostic
codes, etc.) and unstructured parts (in the form of
free text). The free-text part of EHRs is referred
to as clinical text, as opposed to the kind of gen-
eral medical text found in medical journals, books
or web pages containing information about health
care. Clinical texts have many subdomains de-
pending on the medical speciality of the writer and
the intended reader. There are more formal kinds
of EHRs, such as discharge summaries and radiol-
ogy reports, directed to other physicians, and more
informal kinds such as daily notes, produced by
nurses and physicians (as memory notes for them-
selves or for the team). In spite of the Patient Data
Act, the patient is seldom seen as a receiver or
reader of the document.
Linguistically, health records exemplify a
highly specialised domain, which can be super-
ficially characterised as having telegraphic sen-
tences involving displaced or missing words,
abundant abbreviations, undisputed misspellings,
spelling variation which may or may not amount to
misspellings depending on the degree of prescrip-
tivism, and terminology. While this specialised
style has evolved as an efficient means of com-
munication between healthcare professionals, it
presents formidable challenges for laymen trying
to decode it.
In spite of this, there has been no previous work
on the problem of automatically improving the
readability of Swedish EHRs. As an initial at-
tempt in this direction, we provide an automatic
approach to the problem of lexical simplification,
by which we mean detecting the unknown, out of
dictionary words and trying to resolve them either
as compounds generated from known words, as
abbreviations or as misspellings. As an additional
result, we obtain a distribution of how prevalent
these problems are in the clinical domain.
2 Lexical challenges to readability of
EHRs
A major reason for the obstacles to readability of
EHRs for laymen stems from the fact that they
74
are written under time pressure by professionals,
for professionals (Kvist et al., 2011). This re-
sults in a telegraphic style, with omissions, ab-
breviations and misspellings, as reported for sev-
eral languages including Swedish, Finnish, En-
glish, French, Hungarian and German (Laippala
et al., 2009; Friedman et al., 2002; Hag`ege et
al., 2011; Surj?an and H?eja, 2003; Bretschneider et
al., 2013). The omitted words are often subjects,
verbs, prepositions and articles (Friedman et al.,
2002; Bretschneider et al., 2013).
Unsurprisingly, medical terminology abounds
in EHRs. What makes this problem an even
greater obstacle to readability is that many medical
terms (and their inflections) originate from Latin
or Greek. Different languages have adapted these
terms differently (Bretschneider et al., 2013). The
Swedish medical terminology went through a
change during the 1990s due to a swedification
of diagnostic expressions performed in the 1987
update of the Swedish version of ICD, the Inter-
national Classification of Diseases
1
. For this ver-
sion, the Swedish National Board of Health and
Welfare decided to partly change the terminology
of traditional Latin- and Greek-rooted words to a
spelling compatible to Swedish spelling rules, as
well as abandoning the original rules for inflec-
tion (Smedby, 1991). In this spelling reform, c
and ch pronounced as k was changed to k, ph was
changed to f, th to t, and oe was changed to e.
For example, the technical term for cholecsystitis
(inflammation of the gall bladder) is spelled kole-
cystit in contemporary Swedish, thus following the
convention of changing ch to k and removing the
Latin ending of -is. The results
2
of exact match-
ing to kolecystit (English: cholecystitis) and some
presumed spelling variants clearly demonstrate the
slow progress (Table 1).
As medical literature is predominantly written
in English nowadays, physicians increasingly get
exposed to the English spelling of Latin and Greek
words rather than the Swedish one. This has re-
sulted in a multitude of alternate spellings of sev-
eral medical terms. For example, tachycardia
(rapid heart) is correctly spelled takykardi, but is
1
http://www.who.int/classifications/
icd/en/
2
Based on a subset of the Stockholm Electronic Pa-
tient Record Corpus (Dalianis et al., 2012) of 100,000 daily
notes (DAY) written by physicians of varying disciplines (4
mill. tokens) and 435,000 radiology reports (X-RAY) writ-
ten by radiologists (20 mill. tokens). KORP: http://
spraakbanken.gu.se/korp/
Term KORP DAY X-RAY
kolecystit 51 48 84
colecystit 0 1 8
cholecystit 4 88 1613
Table 1: Alternate spellings of the Swedish
medical term kolecystit (eng. cholecystitis) in
the Swedish corpus collection Korp, daily notes
(DAY) and radiology reports (X-RAY), respec-
tively. Correct spelling in bold.
also frequently found as tachycardi, tachykardi,
and takycardi (Kvist et al., 2011). A similar
French study found this kind of spelling variation
to be abundant as well (Ruch et al., 2003).
EHRs also contain neologisms. These are often
verbs, typically describing events relating to the
patient in active form, such as ?the patient is in-
farcting? (Swedish: patienten infarcerar) instead
of the unintentional ?the patient is having a my-
ocardial infarction?. Similar phenomena are de-
scribed by Josefsson (1999).
Abbreviations and acronyms in EHRs can fol-
low standardised writing rules or be ad hoc (Liu
et al., 2001). They are often domain-specific
and may be found in medical dictionaries such
as MeSH
3
and Snomed CT
4
. For instance, 18 of
the 100 most common words in Swedish radiol-
ogy reports were abbreviations, and 10 of them
were domain-specific (Kvist and Velupillai, 2013).
Because many medical terms are multiword ex-
pressions that are repeated frequently in a pa-
tient?s EHR, the use of acronyms is very common.
Skeppstedt et al. (2012) showed that 14% of di-
agnostic expressions were abbreviated in Swedish
clinical text.
Abbreviations are often ambiguous. As an
example, 33% of the short abbreviations in the
UMLS terminology are ambiguous (Liu et al.,
2001). Pakhomov et al. (2005) found that the ab-
breviation RA had more than 20 expansions in the
UMLS terminology alone. Furthermore, a certain
word or expression can be shortened in several dif-
ferent ways. For instance, in a Swedish intensive
care unit, the drug Noradrenalin was creatively
written in 60 different ways by the nurses (Allvin
et al., 2011).
It should be noted that speech recognition, al-
though common in many hospitals around the
3
www.ncbi.nlm.nih.gov
4
http://www.ihtsdo.org/
75
world, has not been introduced in Sweden, and
many physicians and all nurses type the notes
themselves. This is one explanation to the vari-
ation with respect to abbreviations.
User studies have shown that the greatest bar-
riers for patients lie mainly in the frequent use
of abbreviations, jargon and technical terminol-
ogy (Pyper et al., 2004; Keselman et al., 2007;
Adnan et al., 2010). The most common com-
prehension errors made by laymen concern clini-
cal concepts, medical terminology and medication
names. Furthermore, there are great challenges for
higher-level processing like syntax and semantics
(Meystre et al., 2008; Wu et al., 2013). The re-
search presented in this paper focuses on lexical
simplification of clinical text.
3 Related research
We are aware of several efforts to construct au-
tomated text simplification tools for clinical text
in English (Kandula et al., 2010; Patrick et al.,
2010). For Swedish, there are few studies on med-
ical language from a readability perspective. Borin
et al. (2009) present a thorough investigation on
Swedish (and English) medical language, but EHR
texts are explicitly not included. This section sum-
marizes research on Swedish (clinical) text with
respect to lexical simplification by handling of ab-
breviations, terminology and spelling correction.
3.1 Abbreviation detection
Abbreviation identification in English biomedical
and clinical texts has been studied extensively (e.g.
Xu et al. (2007), Liu et al. (2001)). For detec-
tion of Swedish medical abbreviations, there are
fewer studies. Dann?ells (2006) reports detection
of acronyms in medical journal text with 98% re-
call and 94% precision by using part of speech
information and heuristic rules. Clinical Swedish
presents greater problems than medical texts, be-
cause of ad hoc abbreviations and noisier text. By
using lexicons and a few heuristic rules, Isenius et
al. (2012) report the best F-score of 79% for ab-
breviation detection in clinical Swedish.
3.2 Compound splitting
Good compound analysis is critical especially for
languages whose orthographies concatenate com-
pound components. Swedish is among those lan-
guages, in which every such concatenation thus
corresponds to a word. The most common ap-
proach to compound splitting is to base it on a lex-
icon providing restrictions on how different word
forms can be used for generating compounds. For
example, Sj?obergh and Kann (2006) used a lex-
icon derived from SAOL (the Swedish Academy
word list), and
?
Ostling and Wir?en (2013) used the
SALDO lexicon of Swedish morphology (Borin
and Forsberg, 2009). With this kind of approach,
compound splitting is usually very reliable for
genres like newspaper text, with typical accuracies
for Swedish around 97%, but performs poorer in
domain specific genres.
3.3 Terminology detection
The detection of English medical terminology is
a widely researched area. An example of term
detection in English clinical texts is Wang and
Patrick (2009) work based on rule-based and ma-
chine learning methods, reporting 84% precision.
For Swedish clinical text, Kokkinakis and
Thurin (2007) have employed domain terminol-
ogy matching and reached 98% precision and 87%
recall in detecting terms of disorders. Using sim-
ilar approaches, Skeppstedt et al. (2012), reached
75% precision and 55% recall in detecting terms
of disorders. With a machine learning based ap-
proach, improved results were obtained: 80%
precision, 82% recall (Skeppstedt et al., 2014).
Skeppstedt et al. (2012) have also demonstrated
the negative influence of abbreviations and mul-
tiword expressions in their findings.
3.4 Spelling correction
A system for general spelling correction of
Swedish is described by Kann et al. (1998), but
we are not aware of any previous work related to
spelling correction of Swedish clinical text. An
example of spelling correction of clinical text for
other languages is Tolentino et al. (2007), who use
several algorithms for word similarity detection,
including phonological homonym lookup and n-
grams for contextual disambiguation. They report
a precision of 64% on English medical texts. An-
other example is Patrick et al. (2010) and Patrick
and Nguyen (2011), who combine a mixture of
generation of spelling candidates based on ortho-
graphic and phonological edit distance, and a 2-
word window of contextual information for rank-
ing the spelling candidates resulting in an accuracy
of 84% on English patient records. Sikl?oski et al.
(2013) use a statistical machine translation model
76
Figure 1: Distribution of 100 PR dataset sentences by length (number of sentences on the y-axis and
number of tokens on the x-axis).
(with 3-grams) for spelling correction, achieving
88% accuracy on Hungarian medical texts.
4 Experimental data
This study uses clinical notes
5
from the Stockholm
Electronic Patient Record corpus containing more
than 600,000 patients of all ages from more than
500 health units during 2006?2013 (Dalianis et al.,
2012).
A randomly selected subset of 100 daily notes
from different EHRs written by physicians be-
tween 2009?2010 was used as a gold standard
dataset for evaluating abbreviation detection, com-
pound splitting and spelling corrections. This 100
daily notes dataset contains 433 sentences and
3,888 tokens, as determined by Stagger (
?
Ostling,
2013), a Swedish tokenizer and POS tagger. The
majority of sentences contain between 4?11 to-
kens (see Figure 1.)
The text snippet in Figure 2 provides an illus-
trative example of the characteristics of a health
record. What is immediately striking is the num-
ber of misspellings, abbreviations, compounds and
words of foreign origin. But also the syntax is
peculiar, alternating between telegraphic clauses
with implicit arguments, and long sentences with
complex embeddings.
5
Approved by the Regional Ethical Review Board in
Stockholm (Etikpr?ovningsn?amnden i Stockholm), permis-
sion number 2012/2028-31/5
5 Lexical normalization of EHRs
Normalization of lexis in clinical text relies heav-
ily on the lookup in available lexicons, corpora and
domain terminologies. Although these resources
usually cover the majority of words (i.e. tokens)
in texts, however due to the ever evolving lan-
guage and knowledge inside the domain, medi-
cal texts, when analysed with the NLP tools, also
contain unknown
6
words. These remaining words
that are not covered by any lexicon, or corpora re-
source, can be misspellings, abbreviations, com-
pounds (new word formations), words in foreign
languages (Latin, Greek, English), or new terms.
Our approach to dealing with unknown words
combines a rule-based abbreviation detection and
Swedish statistical language model-based com-
pound analysis and misspelling resolution.
The following sections describe three methods
that are applied in a pipeline manner. That is, first,
all known abbreviations are detected and marked;
second the unknown words are checked whether
they are compounds; finally, for the remaining un-
known words, context dependent word corrections
are made.
5.1 Detecting abbreviations
This section describes the heuristics and lexi-
con lookup-based abbreviation detection method.
The Swedish Clinical Abbreviation and Medi-
cal Terminology Matcher (SCATM) is based on
6
By unknown words we mean words that cannot be
looked up in available lexical resources or linguistically ana-
lyzed by POS tokenizer.
77
Figure 2: Characteristics of a health record: misspellings (underline), abbreviations (bold), compounds
(italic) and words of foreign origin (red).
SCAN (Isenius et al., 2012). The SCATM method
uses domain-adapted Stagger (
?
Ostling, 2013)
for the tokenization and POS-tagging of text.
The adapted version of Stagger handles clinical-
specific
7
abbreviations from three domains, i.e. ra-
diology, emergency, and dietology. SCATM also
uses several lexicons to determine whether a word
is a common word (in total 122,847 in the lexi-
con), an abbreviation (in total 7,455 in the lexi-
con), a medical term (in total 17,380 in the lexi-
con), or a name (both first and last names, in total
404,899 in the lexicon). All words that are at most
6 characters long, or contains the characters ?-?
and/or ?.? are checked against these lexicons in a
specific order in order to determine whether it is
an abbreviation or not.
The SCATM method uses various lexicons
8
of
Swedish medical terms, Swedish abbreviations,
7
Abbreviations that do not follow conventional orthogra-
phy styles, e.g. a typical abbreviation p.g.a. (en. due to) can
have the following variants p g a, pga, p. G. A., p. gr. a.
8
the sources of lexicons are: anatomin.se,
neuro.ki.se smittskyddsinstitutet.se,
medicinskordbok.se, runeberg.org, g3.
spraakdata.gu.se/saob, sv.wikipedia.org/
wiki/Lista_ver_frkortningar, karolinska.
se/Karolinska-Universitetslaboratoriet/
Sidor-om-PTA/Analysindex-alla-enheter/
Forkortningar/ and the list of Swedish names (Carlsson
and Dalianis, 2010).
Swedish words and Swedish names (first and last).
5.2 Compound splitting
For compound splitting, we use a collection of lex-
ical resources, the core of which is a full-form
dictionary produced by Nordisk spr?akteknologi
holding AS (NST), comprising 927,000 en-
tries
9
. In addition, various resources from the
medical domain have been mined for vocab-
ulary: Swedish SNOMED
10
terminology, the
L?akartidningen medical journal
11
corpus, and
Swedish Web health-care guides/manuals
12
.
A refinement of the basic lexicon-driven tech-
nique described in the related research section is
that our compound splitting makes use of contex-
tual disambiguation. As the example of hj?arteko
illustrates, this compound can be hypothetically
split into
13
:
hj?art+eko (en. cardiac+echo)
9
Available at: www.nb.no/Tilbud/Forske/
Spraakbanken/Tilgjengelege-ressursar/
Leksikalske-ressursar
10
www.socialstyrelsen.se/
nationellehalsa/nationelltfacksprak/
11
http://spraakbanken.gu.se/eng/
research/infrastructure/korp
12
www.1177.se and www.vardguiden.se
13
Korp (http://spraakbanken.gu.se/korp) is a collection of
Swedish corpora, comprising 1,784,019,272 tokens, as of
January 2014.
78
KORP freq.: 642 + 5,669
hj?arte+ko (en. beloved+cow)
KORP freq.: 8 + 8,597
For choosing the most likely composition in the
given context, we use the Stockholm Language
Model with Entropy (SLME) (
?
Ostling, 2012)
which is a simple n-gram language model.
The max probability defines the correct word
formation constituents:
hj?art+eko 2.3e-04
hj?arte+ko 5.1e-07
The SMLE is described in the following section.
5.3 Misspelling detection
The unknown words that are not abbreviations or
compounds can very likely be misspellings. Mis-
spellings can be a result of typing errors or the lack
of knowledge of the correct spelling.
Our approach to clinical Swedish misspellings
is based on the best practices of spell checkers
for Indo-European languages, namely the phonetic
similarity key method combined with a method
to measure proximity between the strings. In
our spelling correction method, the Edit distance
(Levenshtein, 1966) algorithm is used to measure
the proximity of orthographically possible can-
didates. The Soundex algorithm (Knuth, 1973)
shortlists the spelling candidates which are phono-
logically closest to the misspelled word. Further,
the spelling correction candidates are analyzed in
a context by using the SLME n-gram model.
The SLME employs the Google Web 1T 5-
gram, 10 European Languages, Version 1, dataset
for Swedish, which is the largest publically avail-
able Swedish data resource. The SLME is a sim-
ple n-gram language model, based on the Stupid
Backoff Model (Brants et al., 2007). The n-gram
language model calculates the probability of a
word in a given context:
P (w
L
1
) =
L
?
i=1
P (w
i
|w
i?1
1
) ?
L
?
i=1
?
P (w
i
|w
i?1
i?n+1
)
(1)
The maximum-likelihood probability estimates
for the n-grams are calculated by their relative fre-
quencies:
r(w
i
|w
i?1
i?n+1
) =
f(w
i
i?n+1
)
f(w
i?1
i?n+1
)
(2)
The smoothing is used when the complete n-
gram is not found. If r(w
i?1
i?n+1
) is not found,
then the model looks for r(w
i?1
i?n+2
) , r(w
i?1
i?n+3
),
and so on. The Stupid backoff (Brants et al.,
2007) smoothing method uses relative frequencies
instead of normalized probabilities and context-
dependent discounting. Equation (3) shows how
score S is calculated:
S(w
i
|w
i?1
i?k+1
) =
=
?
?
?
?
?
f(w
i
i?k+1
)
f(w
i?1
i?k+1
)
iff(w
i
i?k+1
)) > 0
?S(w
i
|w
i?1
i?k+2
) otherwise
(3)
The backoff parameter ? is set to 0.4, which was
heuristically determined by (Brants et al., 2007).
The recursion stops when the score for the last
context word is calculated. N is the size of the
corpus.
S(w
i
) =
f(w
i
)
N
(4)
The SLME n-gram model calculates the
probability of a word in a given context:
p(word|context). The following example
14
shows the case of spelling correction:
Original:
Vpl p?a onsdag. UK tortdag.
(en. Vpl on wednesday. UK thsday.)
torgdag (en. marketday): 4.2e-10
torsdag (en. Thursday): 1.1e-06
Corrected:
Vpl p?a onsdag. UK torsdag.
6 Experiments and results
Our approach to lexical normalization was
tested against a gold standard, namely, the 100
EHR daily notes dataset. The dataset was anno-
tated for abbreviations, compounds including ab-
breviations and misspellings by a physician.
We carried out the following experiments (see
Table 2):
1. SCATM to mark abbreviations and terms;
14
Vpl stands for V?ardplanering (en. planning for care), UK
stands for utskrivningsklar (en. ready for discharge).
79
Method Lexical normalization task Gold-
standard,
occurences
Precision, % Recall, %
SCATM 1 Abbreviation detection 550 91.1 81.0
SCATM 1a Abbreviations included in
compounds only
78 89.74 46.15
NoCM 1 Out-of-dictionary compound
splitting
97 83.5 -
NoCM 1a Out-of-dictionary com-
pounds which include
abbreviations
44 59.1 -
NoCM 2 Spelling correction 41 54.8 63.12
SCATM+NoCM Spelling correction 41 83.87 76.2
Table 2: Results of lexical normalization.
2. NoCM (lexical normalization of compounds
and misspellings as described in sections
5.2 and 5.3) to resolve compounds and mis-
spellings;
3. The combined experiment SCATM+NoCM
to resolve misspellings.
The last experimental setting was designed as a
solution to deal with compounds that include ab-
breviations. Marking abbreviations prior to the
spelling correction can help to reduce the number
of false positives.
The 433 sentences contained a total of 550 ab-
breviations (78 of these were constituents of com-
pound words), and 41 misspellings of which 13
were misspelled words containing abbreviations.
Due to the tokenization errors, a few sentence
boundaries were detected incorrectly, e.g. inter-
rupted dates and abbreviations. Because of this
some abbreviations were separated into different
sentences and thus added to false negatives and
false positives.
The first experiment (SCATM 1 and 1a) of de-
tecting abbreviations achieved both high precision
and recall. As a special case of demonstrating the
source of errors (see SCATM 1a) is the evaluation
of detecting abbreviations which are part of com-
pounds only. The low recall is due to the design of
the SCATM which does not handle words longer
than 6 characters, thus resulting in compounded
abbreviations like k?arlkir or ?overvak to go unde-
tected.
The evaluation of the second experiment
(NoCM 1, 1a and 2) showed that the majority
of out-of-dictionary compounds was resolved cor-
rectly (NoCM 1) and reached 83.5% precision.
Errors mainly occurred due to spelling candi-
date ranking, e.g. even+tull instead of eventuell
and compounds containing abbreviations and mis-
spelled words. As a special case of demonstrating
the source of errors of the latter (see NoCM 1a) is
the evaluation of those compounds
15
only which
contain abbreviations. The task of spelling correc-
tion (NoCM 2) performed poorly, reaching only
54.8% precision. This can be explained by failing
to resolve misspellings in compounds where ab-
breviations are compounded together with a mis-
spelled words, e.g. aciklocvirkonc (aciklovir kon-
centrate).
The third experiment (SCATM+NoCM) com-
bined abbreviation detection followed by the out-
of-dictionary word normalization (spelling cor-
rection and compound splitting). This setting
helped to resolve the earlier source of errors, i.e.
words that contain both misspelling(s) and abbre-
viation(s). The overall precision of spelling cor-
rection is 83.87%.
7 Conclusions
Our attempt to address the problem of lexical sim-
plification, and, in the long run, improve readabil-
ity of Swedish EHRs, by automatically detecting
and resolving out of dictionary words, achieves
91.1% (abbreviations), 83.5% (compound split-
ting) and 83.87% (spelling correction) precision,
respectively. These results are comparable to those
15
This number of compounds is derived from the number
of abbreviations included in compounds (from SCATM 1a)
by selecting only those out-of -dictionary words which do not
contain punctuation.
80
reported in similar studies on English and Hungar-
ian patient records (Patrick et al., 2010; Sikl?osi et
al., 2013).
Furthermore, the analysis of the gold standard
data revealed that around 14% of all words in
Swedish EHRs are abbreviations. More specifi-
cally, 2% of all the words are compounds includ-
ing abbreviations. In contrast, and somewhat un-
expectedly, only 1% are misspellings. This dis-
tribution result is an important finding for future
studies in lexical simplification and readability
studies of EHRs, as it might be useful for inform-
ing automatic processing approaches.
We draw two conclusions from this study. First,
to advance research into the field of readability
of EHRs, and thus to develop suitable readability
measures it is necessary to begin by taking these
findings into account and by relating abbrevia-
tions, spelling variation, misspellings, compounds
and terminology to reading comprehension.
Second, as a future guideline for the overall
pipeline for detecting and resolving unknown, out-
of-dictionary words, we suggest handling abbrevi-
ations in a first step, and then taking care of mis-
spellings and potential compounds. The most ur-
gent area for future improvement of the method is
to handle compound words containing both abbre-
viations and misspellings.
Acknowledgements
The authors wish to thank the anonymous review-
ers for valuable feedback. Maria Kvist and Sum-
ithra Velupillai were in part funded by the V?ardal
Foundation, Sumithra also by the Swedish Re-
search Council and the Swedish Fulbright com-
mission. We thank Robert
?
Ostling who pro-
vided the POS tagger and the Stockholm Lan-
guage Model with Entropy.
References
M. Adnan, J. Warren, and M. Orr. 2010. Assess-
ing text characteristics of electronic discharge sum-
maries and their implications for patient readabil-
ity. In Proceedings of the Fourth Australasian Work-
shop on Health Informatics and Knowledge Man-
agement - Volume 108, HIKM ?10, pages 77?84,
Darlinghurst, Australia, Australia. Australian Com-
puter Society, Inc.
H. Allvin, E. Carlsson, H. Dalianis, R. Danielsson-
Ojala, V. Daudaravicius, M. Hassel, D. Kokki-
nakis, H. Lundgren-Laine, G.H. Nilsson, ?. Nytr?,
S. Salanter?a, M. Skeppstedt, H. Suominen, and
S. Velupillai. 2011. Characteristics of Finnish and
Swedish intensive care nursing narratives: a com-
parative analysis to support the development of clin-
ical language technologies. Journal of Biomedical
Semantics, 2(Suppl 3):S1, doi:10.1186/2041-1480-
2-S3-S1, July.
L. Borin and M. Forsberg. 2009. All in the family: A
comparison of SALDO and WordNet. In Proceed-
ings of the Nodalida 2009 Workshop on WordNets
and other Lexical Semantic Resources, pages 7?12.
NEALT.
L. Borin, N. Grabar, M. Gronostaj, C. Hallett, D. Hard-
castle, D. Kokkinakis, S. Williams, and A. Willis.
2009. Semantic Mining Deliverable D27.2: Em-
powering the patient with language technology.
Technical report, Semantic Mining (NOE 507505).
T. Brants, A. C. Popat, P. Xu, F. J. Och, and J. Dean.
2007. Large language models in machine transla-
tion. In In Proceedings of the 2007 Joint Conference
EMNLP-CoNLL, pages 858?867.
C. Bretschneider, S. Zillner, and M. Hammon. 2013.
Identifying pathological findings in German radiol-
ogy reports using a syntacto-semantic parsing ap-
proach. In Proceedings of the 2013 Workshop on
Biomedical Natural Language Processing (BioNLP
2013). ACL.
E. Carlsson and H. Dalianis. 2010. Influence of Mod-
ule Order on Rule-Based De-identification of Per-
sonal Names in Electronic Patient Records Writ-
ten in Swedish. In Proceedings of the Seventh In-
ternational Conference on Language Resources and
Evaluation, LREC 2010, pages 3071?3075, Valletta,
Malta, May 19?21.
H. Dalianis, M. Hassel, A. Henriksson, and M. Skepp-
stedt. 2012. Stockholm EPR Corpus: A Clinical
Database Used to Improve Health Care. In Pierre
Nugues, editor, Proc. 4th SLTC, 2012, pages 17?18,
Lund, October 25-26.
D. Dann?ells. 2006. Automatic acronym recognition.
In Proceedings of the 11th conference on European
chapter of the Association for Computational Lin-
guistics (EACL).
C. Friedman, P. Kra, and A. Rzhetsky. 2002. Two
biomedical sublanguages: a description based on the
theories of Zellig Harris. Journal of Biomedical In-
formatics, 35(4):222?235.
C. Hag`ege, P. Marchal, Q. Gicquel, S. Darmoni,
S. Pereira, and M. Metzger. 2011. Linguistic
and temporal processing for discovering hospital ac-
quired infection from patient records. In Proceed-
ings of the ECAI 2010 Conference on Knowledge
Representation for Health-care, KR4HC?10, pages
70?84, Berlin, Heidelberg. Springer-Verlag.
N. Isenius, S. Velupillai, and M. Kvist. 2012. Initial
results in the development of scan: a swedish clini-
cal abbreviation normalizer. In Proceedings of the
81
CLEF 2012 Workshop on Cross-Language Evalu-
ation of Methods, Applications, and Resources for
eHealth Document Analysis - CLEFeHealth2012,
Rome, Italy, September. CLEF.
G. Josefsson. 1999. F?a feber eller tempa? N?agra
tankar om agentivitet i medicinskt fackspr?ak.
S. Kandula, D. Curtis, and Q. Zeng-Treitler. 2010. A
Semantic and Syntactic Text Simplification Tool for
Health Content. In Proc AMIA 2010, pages 366?
370.
V. Kann, R. Domeij, J. Hollman, and M. Tillenius.
1998. Implementation Aspects and Applications of
a Spelling Correction Algorithm. . Technical Report
TRITA-NA-9813, NADA, KTH.
A. Keselman, L. Slaughter, CA. Smith, H. Kim, G. Di-
vita, A. Browne, and et al. 2007. Towards
consumer-friendly PHRs: patients experience with
reviewing their health records. In AMIA Annu Symp
Proc 2007, pages 399?403.
D. E. Knuth, 1973. The Art of Computer Program-
ming: Volume 3, Sorting and Searching, pages 391?
392. Addison-Wesley.
D. Kokkinakis and A. Thurin. 2007. Identifica-
tion of Entity References in Hospital Discharge Let-
ters. In Proceedings of the 16th Nordic Conference
of Computational Linguistics (NODALIDA) 2007,
pages 329?332, Tartu, Estonia.
M. Kvist and S. Velupillai. 2013. Professional
Language in Swedish Radiology Reports ? Char-
acterization for Patient-Adapted Text Simplifica-
tion. In Proceedings of the Scandinavian Con-
ference on Health Informatics 2013, Copenhagen,
Denmark, August. Link?oping University Electronic
Press, Link?opings universitet.
M. Kvist, M. Skeppstedt, S. Velupillai, and H. Dalianis.
2011. Modeling human comprehension of swedish
medical records for intelligent access and summa-
rization systems, a physician?s perspective. In Proc.
9th Scandinavian Conference on Health Informat-
ics, SHI, Oslo, August.
V. Laippala, F. Ginter, S. Pyysalo, and T. Salakoski.
2009. Towards automated processing of clinical
Finnish: Sublanguage analysis and a rule-based
parser. Int journal of medical informatics, 78:e7?
e12.
VI Levenshtein. 1966. Binary Codes Capable of Cor-
recting Deletions, Insertions and Reversals. Soviet
Physics Doklady, 10:707?710.
H. Liu, Y. A. Lussier, and C. Friedman. 2001. Disam-
biguating Ambiguous Biomedical Terms in Biomed-
ical Narrative Text: An Unsupervised Method.
Journal of Biomedical Informatics, 34:249?261.
S. M. Meystre, G. K. Savova, K. C. Kipper-Schuler,
and John E. Hurdle. 2008. Extracting informa-
tion from textual documents in the electronic health
record: a review of recent research. IMIA Yearbook
of Medical Informatics 2008. 47 Suppl 1:138-154.
R.
?
Ostling and M. Wir?en, 2013. Compounding in
a Swedish Blog Corpus, pages 45?63. Stockholm
Studies in Modern Philology. New series 16. Stock-
holm university.
R.
?
Ostling. 2012.
http://www.ling.su.se/english/nlp/tools/slme/stockholm-
language-model-with-entropy-slme-1.101098 .
R.
?
Ostling. 2013. Stagger: an Open-Source Part of
Speech Tagger for Swedish. Northern European
Journal of Language Technology, 3:1?18.
S. Pakhomov, T. Pedersen, and C. G. Chute. 2005. Ab-
breviation and Acronym Disambiguation in Clinical
Discourse. In Proc AMIA 2005, pages 589?593.
J. Patrick and D. Nguyen. 2011. Automated Proof
Reading of Clinical Notes. In Helena Hong Gao
and Minghui Dong, editors, PACLIC, pages 303?
312. Digital Enhancement of Cognitive Develop-
ment, Waseda University.
J. Patrick, M. Sabbagh, S. Jain, and H. Zheng. 2010.
Spelling correction in Clinical Notes with Emphasis
on First Suggestion Accuracy. In 2nd Workshop on
Building and Evaluating Resources for Biomedical
Text Mining, pages 2?8.
C. Pyper, J. Amery, M. Watson, and C. Crook. 2004.
Patients experiences when accessing their on-line
electronic patient records in primary care. The
British Journal of General Practice, 54:38?43.
P. Ruch, R. Baud, and A. Geissb?uhler. 2003. Using
lexical disambiguation and named-entity recogni-
tion to improve spelling correction in the electronic
patient record. Artificial Intelligence in Medicine,
29(1-2):169?184.
B. Sikl?osi, A. Nov?ak, and G. Pr?osz?eky, 2013. Context-
Aware Correction of Spelling Errors in Hungar-
ian Medical Documents, pages 248?259. Number
Lecture Notes in Computer Science 7978. Springer
Berlin Heidelberg.
J. Sj?obergh and V. Kann. 2006. Vad kan statistik
avsl?oja om svenska sammans?attningar? Spr?ak och
stil, 1:199?214.
M. Skeppstedt, M. Kvist, and H Dalianis. 2012.
Rule-based Entity Recognition and Coverage of
SNOMED CT in Swedish Clinical Text. In Pro-
ceedings of the Eighth International Conference on
Language Resources and Evaluation, LREC 2012,
pages 1250?1257, Istanbul, Turkey, May 23?25.
M. Skeppstedt, M. Kvist, G. H. Nilsson, and H. Dalia-
nis. 2014. Automatic recognition of disorders,
findings, pharmaceuticals and body structures from
82
clinical text: An annotation and machine learn-
ing study. Journal of Biomedical Informatics,
http://dx.doi.org/10.1016/j.jbi.2014.01.012.
B. Smedby. 1991. Medicinens Spr?ak: spr?aket
i sjukdomsklassifikationen ? mer konsekvent
f?orsvenskning efterstr?avas [Language of Medicine:
the language of diagnose classification - more
consequent Swedification sought]. L?akartidningen,
pages 1519?1520.
G. Surj?an and G. H?eja. 2003. About the language of
Hungarian discharge reports. Stud Health Technol
Inform, 95:869?873.
H. D. Tolentino, M. D. Matters, W. Walop, B. Law,
W. Tong, F. Liu, P. A. Fontelo, K. Kohl, and D. C.
Payne. 2007. A UMLS-based spell checker for nat-
ural language processing in vaccine safety. BMC
Med. Inf. & Decision Making, 7.
Y. Wang and J. Patrick. 2009. Cascading classifiers for
named entity recognition in clinical notes. In Pro-
ceedings of the Workshop on Biomedical Informa-
tion Extraction, WBIE ?09, pages 42?49, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
D. T. Y. Wu, D. A. Hanauer, Q. Mei, P. M. Clark,
L. C. An, J. Lei, J. Proulx, Q. Zeng-Treitler, and
K. Zheng. 2013. Applying Multiple Methods to As-
sess the Readability of a Large Corpus of Medical
Documents. Stud Health Technol Inform, 192:647?
651.
H. Xu, P. D. Stetson, and C. Friedman. 2007. A Study
of Abbreviations in Clinical Notes. In Proc AMIA
2007, pages 821?825.
83
Proceedings of the 3rd Workshop on Predicting and Improving Text Readability for Target Reader Populations (PITR) @ EACL 2014, pages 94?103,
Gothenburg, Sweden, April 26-30 2014.
c
?2014 Association for Computational Linguistics
EACL - Expansion of Abbreviations in CLinical text
Lisa Tengstrand*, Be
?
ata Megyesi*, Aron Henriksson
+
, Martin Duneld
+
and Maria Kvist
+
*Department of Linguistics and Philology,
Uppsala University, Sweden
tengstrand@ling.su.se, beata.megyesi@lingfil.uu.se
+
Department of Computer and System Sciences,
Stockholm University, Sweden
aronhen@dsv.su.se, xmartin@dsv.su.se, maria.kvist@karolinska.se
Abstract
In the medical domain, especially in clin-
ical texts, non-standard abbreviations are
prevalent, which impairs readability for
patients. To ease the understanding of the
physicians? notes, abbreviations need to be
identified and expanded to their original
forms. We present a distributional seman-
tic approach to find candidates of the origi-
nal form of the abbreviation, and combine
this with Levenshtein distance to choose
the correct candidate among the semanti-
cally related words. We apply the method
to radiology reports and medical journal
texts, and compare the results to general
Swedish. The results show that the cor-
rect expansion of the abbreviation can be
found in 40% of the cases, an improve-
ment by 24 percentage points compared to
the baseline (0.16), and an increase by 22
percentage points compared to using word
space models alone (0.18).
1 Introduction
Abbreviations are prevalent in text, especially in
certain text types where the author has either lim-
ited space or time to write the written message and
therefore shortens some words or phrases. This
might, however, make it difficult for the reader
to understand the meaning of the actual abbre-
viation. Although some abbreviations are well-
known, and frequently used by most of us (e.g.,
i.e., pm, etc.), most of the abbreviations used in
specialized domains are often less known to the
public. Interpreting them is not an easy task, as ab-
breviations are often ambiguous and their correct
meaning depends on the context in which they ap-
pear. For example, military and governmental staff
would naturally read EACL as Emergency Action
Checklist, people in the food and beverage busi-
ness might think of the company name EACL, lin-
guists would probably interpret it as the European
Chapter of Chinese Linguistics, while computa-
tional linguists would generally claim that EACL
stands for the European Chapter of the Associa-
tion for Computational Linguistics. However, the
readers of this particular article know, as the title
suggests, that the intended meaning here is the Ex-
pansion of Abbreviations in CLinical text.
It has been shown that abbreviations are fre-
quently occurring in various domains and genres,
such as in historical documents, messages in so-
cial media, as well as in different registers used
by specialists within a particular field of exper-
tise. Clinical texts produced by health care per-
sonnel is an example of the latter. The clinical
texts are communication artifacts, and the clini-
cal setting requires that information is expressed
in an efficient way, resulting in short telegraphic
messages. Physicians and nurses need to docu-
ment their work to describe findings, treatments
and procedures precisely and compactly, often un-
der time pressure.
In recent years, governments and health care ac-
tors have started making electronic health records
accessible, not only to other caretakers, but also
to patients in order to enable them to participate
actively in their own health care processes. How-
ever, several studies have shown that patients have
difficulties to comprehend their own health care
reports and other medical texts due to the different
linguistic features that characterize these, aswell
as to medical jargon and technical terminology
(Elhadad, 2006; Rudd et al., 1999; Keselman et
al., 2007). It has also been shown that physicians
rarely adapt their writing style in order to produce
documents that are accessible to lay readers (Al-
lvin, 2010). Besides the use of different termi-
nologies and technical terms, an important obsta-
cle for patients to comprehend medical texts is the
frequent use of ? for the patients unknown ? ab-
94
breviations (Keselman et al., 2007; Adnan et al.,
2010).
In health records, abbreviations, which consti-
tute linguistic units that are inherently difficult to
decode, are commonly used and often non stan-
dard (Skeppstedt, 2012). An important step in
order to increase readability for lay readers is to
translate abbreviated words into their correspond-
ing full length words.
The aim of this study is to explore a distri-
butional semantic approach combined with word
normalization, measured by Levenshtein distance,
to abbreviation expansion. Using distributional
semantic models, which can be applied to large
amounts of data, has been shown to be a viable
approach to extracting candidates for the underly-
ing, original word of an abbreviation. In order to
find the correct expansion among the semantically
related candidates, we apply the Levenshtein dis-
tance measure. We report on experiments on com-
parative studies of various text types in Swedish,
including radiology reports, medical journals and
texts taken from a corpus of general Swedish.
2 Background
An abbreviation is a shorter ? abbreviated ? form
of a word or phrase, often originating from a tech-
nical term or a named entity. Abbreviations are
typically formed in one of three ways: by (i) clip-
ping the last character sequence of the word (e.g.,
pat for patient or pathology), (ii) merging the ini-
tial letter(s) of the words to form an acronym (e.g.,
UU for Uppsala University), or (iii) merging some
of the letters ? often the initial letter of the sylla-
bles ? in the word (e.g., msg for message). Abbre-
viations can also be formed as a combination of
these three categories (e.g., EACL for Expansion
of Abbreviations in CLinical text).
Automatically expanding abbreviations to their
original form has been of interest to computational
linguists as a means to improve text-to-speech, in-
formation retrieval and information extraction sys-
tems. Rule-based systems as well as statistical and
machine learning methods have been proposed to
detect and expand abbreviations. A common com-
ponent of most solutions is their reliance on the as-
sumption that an abbreviation and its correspond-
ing definition will appear in the same text.
Taghva and Gilbreth (1999) present a method
for automatic acronym-definition extraction in
technical literature, where acronym detection is
based on case and token length constraints. The
surrounding text is subsequently searched for pos-
sible definitions corresponding to the detected
acronym using an inexact pattern-matching algo-
rithm. The resulting set of candidate definitions
is then narrowed down by applying the Longest
Common Subsequence (LCS) algorithm (Nakatsu
et al., 1982) to the candidate pairs. They report
98% precision and 93% recall when excluding
acronyms of two or fewer characters.
Park and Byrd (2001), along somewhat similar
lines, propose a hybrid text mining approach for
abbreviation expansion in technical literature. Or-
thographic constraints and stop lists are first used
to detect abbreviations; candidate definitions are
then extracted from the adjacent text based on a set
of pre-specified conditions. The abbreviations and
definitions are converted into patterns, for which
transformation rules are constructed. An initial
rule-base comprising the most frequent rules is
subsequently employed for automatic abbreviation
expansion. They report 98% precision and 94%
recall as an average over three document types.
In the medical domain, most approaches to
abbreviation resolution also rely on the co-
occurrence of abbreviations and definitions in a
text, typically by exploiting the fact that abbrevi-
ations are sometimes defined on their first men-
tion. These studies extract candidate abbreviation-
definition pairs by assuming that either the defi-
nition or the abbreviation is written in parenthe-
ses (Schwartz and Hearst, 2003). The process of
determining which of the extracted abbreviation-
definition pairs are likely to be correct is then
performed either by rule-based (Ao and Takagi,
2005) or machine learning (Chang et al., 2002;
Movshovitz-Attias and Cohen, 2012) methods.
Most of these studies have been conducted on
English corpora; however, there is one study on
Swedish medical text (Dann?ells, 2006). There are
problems with this popular approach to abbrevia-
tion expansion: Yu et al. (2002) found that around
75% of all abbreviations in the biomedical litera-
ture are never defined.
The application of this method to clinical text
is even more problematic, as it seems highly un-
likely that abbreviations would be defined in this
way. The telegraphic style of clinical narrative,
with its many non-standard abbreviations, is rea-
sonably explained by time constraints in the clin-
ical setting. There has been some work on iden-
95
tifying such undefined abbreviations in clinical
text (Isenius et al., 2012), as well as on finding
the intended abbreviation expansion among candi-
dates in an abbreviation dictionary (Gaudan et al.,
2005).
Henriksson et al. (2012; 2014) present a method
for expanding abbreviations in clinical text that
does not require abbreviations to be defined, or
even co-occur, in the text. The method is based
on distributional semantic models by effectively
treating abbreviations and their corresponding def-
inition as synonymous, at least in the sense of shar-
ing distributional properties. Distributional se-
mantics (see Cohen and Widdows (2009) for an
overview) is based on the observation that words
that occur in similar contexts tend to be semanti-
cally related (Harris, 1954). These relationships
are captured in a Random Indexing (RI) word
space model (Kanerva et al., 2000), where se-
mantic similarity between words is represented as
proximity in high-dimensional vector space. The
RI word space representation of a corpus is ob-
tained by assigning to each unique word an ini-
tially empty, n-dimensional context vector, as well
as a static, n-dimensional index vector, which con-
tains a small number of randomly distributed non-
zero elements (-1s and 1s), with the rest of the
elements set to zero
1
. For each occurrence of a
word in the corpus, the index vectors of the sur-
rounding words are added to the target word?s con-
text vector. The semantic similarity between two
words can then be estimated by calculating, for in-
stance, the cosine similarity between their context
vectors. A set of word space models are induced
from unstructured clinical data and subsequently
combined in various ways with different parame-
ter settings (i.e., sliding window size for extracting
word contexts). The models and their combina-
tions are evaluated for their ability to map a given
abbreviation to its corresponding definition. The
best model achieves 42% recall. Improvement of
the post-processing of candidate definitions is sug-
gested in order to obtain enhanced performance on
this task.
The estimate of word relatedness that is ob-
tained from a word space model is purely statis-
tical and has no linguistic knowledge. When word
pairs should not only share distributional proper-
ties, but also have similar orthographic represen-
1
Generating sparse vectors of a sufficiently high dimen-
sionality in this manner ensures that the index vectors will be
nearly orthogonal.
tations ? as is the case for abbreviation-definition
pairs ? normalization procedures could be ap-
plied. Given a set of candidate definitions for a
given abbreviation, the task of identifying plausi-
ble candidates can be viewed as a normalization
problem. Petterson et al. (2013) utilize a string
distance measure, Levenshtein distance (Leven-
shtein, 1966), in order to normalize historical
spelling of words into modern spelling. Adjusting
parameters, i.e., the maximum allowed distance
between source and target, according to observed
distances between known word pairs of historical
and modern spelling, gives a normalization accu-
racy of 77%. In addition to using a Levenshtein
distance weighting factor of 1, they experiment
with context free and context-sensitive weights for
frequently occurring edits between word pairs in a
training corpus. The context-free weights are cal-
culated on the basis of one-to-one standard edits
involving two characters; in this setting the nor-
malization accuracy is increased to 78.7%. Fre-
quently occurring edits that involve more than two
characters, e.g., substituting two characters for
one, serve as the basis for calculating context-
sensitive weights and gives a normalization accu-
racy of 79.1%. Similar ideas are here applied to
abbreviation expansion by utilizing a normaliza-
tion procedure for candidate expansion selection.
3 Method
The current study aims to replicate and extend
a subset of the experiments conducted by Hen-
riksson et al. (2012), namely those that concern
the abbreviation expansion task. This includes
the various word space combinations and the pa-
rameter optimization. The evaluation procedure
is similar to the one described in (Henriksson et
al., 2012). The current study, however, focuses on
post-processing of the semantically related words
by introducing a filter and a normalization proce-
dure in an attempt to improve performance. An
overview of the approach is depicted in Figure 1.
Abbreviation expansion can be viewed as a two-
step procedure, where the first step involves de-
tection, or extraction, of abbreviations, and the
second step involves identifying plausible expan-
sions. Here, the first step is achieved by extracting
abbreviations from a clinical corpus with clinical
abbreviation detection software and using a list of
known medical abbreviations. The second step is
performed by first extracting a set of semantically
96
clinical text
abbreviation
extraction
abbreviations
baseline corpus
word space
induction
expansion
word
extraction
clinical word space
expansion
word
filtering
Levenshtein
distance
normal-
ization
abbreviation-candidate expansions
evaluation
Figure 1: The abbreviation expansion process of
the current study.
similar words for each abbreviation and treating
these as initial expansions. More plausible expan-
sions of each abbreviation are then obtained by fil-
tering the expansion words and applying a normal-
ization procedure.
3.1 Data
3.1.1 Corpora
Four corpora are used in the experiments: two
clinical corpora, a medical (non-clinical) corpus
and a general Swedish corpus (Table 1).
The clinical corpora are subsets of the Stock-
holm EPR Corpus (Dalianis et al., 2009), com-
prising health records for over one million pa-
tients from 512 clinical units in the Stockholm re-
gion over a five-year period (2006-2010)
2
. One
of the clinical corpora contains records from vari-
ous clinical units, for the first five months of 2008,
henceforth referred to as SEPR, and the other con-
tains radiology examination reports, produced in
2009 and 2010, the Stockholm EPR X-ray Corpus
(Kvist and Velupillai, 2013) henceforth referred to
as SEPR-X. The clinical corpora were lemmatized
2
This research has been approved by the Regional Ethical
Review Board in Stockholm (Etikpr?ovningsnamnden i Stock-
holm), permission number 2012/2028-31/5
using Granska (Knutsson et al., 2003).
The experiments in the current study also in-
clude a medical corpus. The electronic editions of
L?akartidningen (Journal of the Swedish Medical
Association), with issues from 1996 to 2010, have
been compiled into a corpus (Kokkinakis, 2012),
here referred to as LTK.
To compare the medical texts to general
Swedish, the third version of the Stockholm Ume?a
Corpus (SUC 3.0) (K?allgren, 1998) is used. It is
a balanced corpus and consists of written Swedish
texts from the early 1990?s from various genres.
Corpus #Tokens #Types #Lemmas
SEPR 109,663,052 853,341 431,932
SEPR-X 20,290,064 200,703 162,387
LTK 24,406,549 551,456 498,811
SUC 1,166,593 97,124 65,268
Table 1: Statistical descriptions of the corpora
3.1.2 Reference standards
A list of medical abbreviation-definition pairs is
used as test data and treated as the reference stan-
dard in the evaluation. The list is derived from
Cederblom (2005) and comprises 6384 unique ab-
breviations from patient records, referrals and sci-
entific articles. To increase the size of the test
data, the 40 most frequent abbreviations are ex-
tracted by a heuristics-based clinical abbreviation
detection tool called SCAN (Isenius et al., 2012).
A domain expert validated these abbreviations and
manually provided the correct expansion(s).
An inherent property of word space models is
that they model semantic relationships between
unigrams. There are, however, abbreviations that
expand into multiword expressions. Ongoing re-
search on modeling semantic composition with
word space models exists, but, in the current study
abbreviations that expanded to multiword defini-
tions were simply removed from the test data set.
The two sets of abbreviation-expansion pairs were
merged into a single test set, containing 1231
unique entries in total.
In order to obtain statistically reliable seman-
tic relations in the word space, the terms of inter-
est must be sufficiently frequent in the data. As a
result, only abbreviation-expansion pairs with fre-
quencies over 50 in SEPR and SEPR-X, respec-
tively, were included in each test set. The SEPR
test set contains 328 entries and the SEPR-X test
97
set contains 211 entries. Each of the two test data
sets is split into a development set (80%) for model
selection, and a test set (20%) for final perfor-
mance estimation.
3.2 Expansion word extraction
For the experiments where semantically related
words were used for extraction of expansion
words, the top 100 most correlated words for each
of the abbreviations were retrieved from each of
the word space model configurations that achieved
the best results in the parameter optimization ex-
periments.
The optimal parameter settings of a word space
vary with the task and data at hand. It has been
shown that when modeling paradigmatic (e.g.,
synonymous) relations in word spaces, a fairly
small context window size is preferable (Sahlgren,
2006). Following the best results of Henriksson et
al. (2012), we experiment with window sizes of
1+1, 2+2, and 4+4.
Two word space algorithms are explored: Ran-
dom Indexing (RI), to retrieve the words that occur
in a similar context as the query term, and Random
Permutation (RP), which also incorporates word
order information when accumulating the context
vectors (Sahlgren et al., 2008). In order to exploit
the advantages of both algorithms, and to combine
models with different parameter settings, RI and
RP model combinations are also evaluated. The
models and their combinations are:
? Random Indexing (RI): words with a contextually high
similarity are returned; word order within the context
window is ignored.
? Random Permutation (RP): words that are contextu-
ally similar and used in the same relative positions are
returned; these are more likely to share grammatical
properties.
? RP-filtered RI candidates (RI RP): returns the top ten
terms in the RI model that are among the top thirty
terms in the RP model.
? RI-filtered RP candidates (RP RI): returns the top ten
terms in the RP model that are among the top thirty
terms in the RI model.
? RI and RP combination of similarity scores (RI+RP):
sums the cosine similarity scores from the two models
for each candidate term and returns the candidates with
the highest aggregate score.
All models are induced with three different con-
text window sizes for the two clinical corpora,
SEPR and SEPR-X. For each corpus, two variants
are used for word space induction, one where stop
words are removed and one where stop words are
retained. All word spaces are induced with a di-
mensionality of 1000.
For parameter optimization and model selec-
tion, the models and model combinations are
queried for semantically similar words. For each
of the abbreviations in the development set, the ten
most similar words are retrieved. Recall is com-
puted with regard to this list of candidate words,
whether the correct expansion is among these ten
candidates. Since the size of the test data is rather
limited, 3-fold cross validation is performed on
the development set for the parameter optimiza-
tion experiments. For both SEPR and SEPR-X de-
velopment sets, a combination of a RI model with
a context window size of 4+4 and a RP model with
4+4 context window size in the summing similar-
ity scores setting were among the most successful
with recall scores of 0.25 for SEPR and 0.17 for
SEPR-X.
3.3 Filtering expansion words
Given the expansion words, extracted from clini-
cal word spaces or baseline corpora (the baselines
are more thoroughly accounted for in 3.5), a filter
was applied in order to generate candidate expan-
sions. The filter was defined as a set of require-
ments, which had to be met in order for the expan-
sion word to be extracted as a candidate expansion.
The requirements were that the intitial letter of the
abbreviation and expansion word had to be iden-
tical. All the letters of the abbreviation also had
to be present in the expansion word in the same
order.
String length difference was also a part of the
requirements: the expansion word had to be at
least one character longer than the abbreviation.
In order to define an upper bound for expansion to-
ken length, string length differences of the SEPR
and SEPR-X development sets were obtained.
The distribution of string length differences for
abbreviation-expansion pairs in the SEPR devel-
opment set ranged from 1 to 21 characters. If a
maximum string length difference of 14 was al-
lowed, 95.2% of the abbreviation-expansion pairs
were covered. As for the string length differences
in the SEPR-X development set, the distribution
ranged from 1 to 21 characters. If a string length
difference of up to and including 14 characters
was allowed, 96.3% of the abbreviation-expansion
pairs were covered. Thus, a maximum difference
98
in string length of 14 was also required for the ex-
pansion word to be extracted as a candidate expan-
sion.
3.4 Levenshtein distance normalization
Given the set of filtered candidate expansions for
the abbreviations, choosing the correct one can be
seen as a normalization problem. The goal is to
map a source word to a target word, similarly to
for instance methods for spelling correction. The
target word is chosen from a list of words, and the
choice is based on the distance between the source
and the target where a small distance implies high
plausibility. However, we cannot adopt the same
assumptions as for the problem of spelling correc-
tion, where the most common distance between a
source word and the correct target word is 1 (Ku-
kich, 1992). Intuitively, we can expect that there
are abbreviations that expand to words within a
larger distance than 1. It would seem somewhat
useless to abbreviate words by one character only,
although it is not entirely improbable.
Similarly to measuring the string length differ-
ence in order to define an upper bound for filtering
candidate expansions, the Levenshtein distances
for abbreviation-expansion pairs in the develop-
ment sets were obtained.
For the SEPR and SEPR-X development sets,
allowing a Levenshtein distance up to and in-
cluding 14 covers 97.8% and 96.6% of the
abbreviation-expansion pairs, as shown in Table 2.
Given the filtered candidate expansions, the
Levenshtein distance for the abbreviation and each
of the candidate expansions were computed. For
each one of the candidate expansions, the Leven-
shtein distance beween the entry and the abbrevi-
ation was associated with the entry. The result-
ing list was sorted in ascending order according to
Levenshtein distance.
Going through the candidate expansion list, if
the Levenshtein distance was less than or identical
to the upper bound for Levenshtein distance (14),
the candidate expansion was added to the expan-
sion list that was subsequently used in the evalu-
ation. In the Levenshtein distance normalization
experiments, a combination of semantically re-
lated words and words from LTK was used. When
compiling the expansion list, semantically related
words were prioritized. This implied that word
space candidate expansion would occupy the top
positions in the expansion list, in ascending order
SEPR SEPR SEPR-X SEPR-X
LD Avg % SDev Avg % SDev
1 1 0.3 0.4 0.2
2 4.6 0.4 5 0.6
3 13 1.2 14.7 1.3
4 12.2 1 15.1 0.6
5 12.7 1.3 14.5 2.2
6 12.7 0.8 12.9 0.9
7 8.4 0.7 7.8 0.3
8 10.4 1.5 9.8 2
9 5.7 0.7 4.9 0.5
10 4.1 0.7 2.9 0.3
11 3 0.5 2.6 0.4
12 3 0.6 2.6 0.4
13 3.8 5.5 1.3 0.5
14 3.5 1.1 2.2 0.8
15 1.3 0.5 1.3 0.5
16 1.6 0.4 0.4 0.2
17 0.2 0.1
18 0.8 0.3 1 0.1
20 0.2 0.1
21 0.2 0.1 0.5 0
Table 2: Levenshtein distance distribution for
abbreviation-expansion pairs. Average proportion
over 5 folds at each Levensthein distance with
standard deviation (SDev) in SEPR and SEPR-X
development sets.
according to Levenshtein distance. The size of the
list was restricted to ten, and the remaining posi-
tions, if there were any, were populated by LTK
candidate expansions in ascending order accord-
ing to Levenshtein distance to the abbreviation. If
there were more than one candidate expansion at
a specific Levenshtein distance, ranking of these
was randomized.
3.5 Evaluation
The evaluation procedure of the abbreviation ex-
pansion implied assessing the ability of finding the
correct expansions for abbreviations. In order to
evaluate the performance gain of using semantic
similarity to produce the list of candidate expan-
sions over using the filtering and normalization
procedure alone, a baseline was created. For the
baseline, expansion words were instead extracted
from the baseline corpora, the corpus of general
Swedish SUC 3.0 and the medical corpus LTK.
A list of all the lemma forms from each baseline
99
corpus (separately) was provided for each abbre-
viation as initial expansion words. The filter and
normalization procedure was then applied to these
expansion words.
The reference standard contained abbreviation-
expansion pairs, as described in 3.1.2. If any of the
correct expansions (some of the abbreviations had
multiple correct expansions) was present in the ex-
pansion list provided for each abbreviation in the
test set, this was regarded as a true positive. Preci-
sion was computed with regard to the position of
the correct expansion in the list and the number of
expansions in the expansion list, as suggested in
Henriksson (2013). For an abbreviation that ex-
panded to one word only, this implied that the ex-
pansion list besides holding the correct expansion,
also contained nine incorrect expansions, which
was taken into account when computing precision.
The list size was static: ten expansions were pro-
vided for each abbreviation, and this resulted in
an overall low precision. Few of the abbreviations
in the development set expanded to more than one
word, giving a precision of 0.17-0.18 for all exper-
iments.
Results of baseline abbreviation expansion in
the development sets are given in table 3. Recall
is given as an average of 5 folds, as cross valida-
tion was performed. The baseline achieves over-
all low recall, with the lowest score of 0.08 for the
SEPR-X development set using SUC for candidate
expansion extraction. The rest of the recall results
are around 0.11.
Corpus SEPR SEPR SEPR-X SEPR-X
Recall SDev Recall SDev
SUC 0.10 0.05 0.08 0.06
LTK 0.11 0.06 0.11 0.11
Table 3: Baseline average recall for SEPR and
SEPR-X development sets.
Results from abbreviation expansion using se-
mantically related words with filtering and nor-
malization to refine the selection of expansions on
SEPR and SEPR-X development sets are shown in
Table 4. Recall is given as an average of 5 folds,
as cross validation was performed. The seman-
tically related words are extracted from the word
space model configuration that had the top recall
scores in the parameter optimization experiments
described in 3.2, namely the combination of an
RI model and an RP model both with 4+4 context
window sizes. Recall is increased by 14 percent-
age points for SEPR and 20 percentage points for
SEPR-X when applying filtering and normaliza-
tion to the semantically related words.
SEPR SEPR SEPR-X SEPR-X
Recall SDev Recall SDev
0.39 0.05 0.37 0.1
Table 4: Abbreviation expansion results for SEPR
and SEPR-X development sets using the best
model from parameter optimization experiments
(RI.4+4+RP.4+4).
4 Results
4.1 Expansion word extraction
The models and model combinations that had the
best recall scores in the word space parameter op-
timization were also evaluated on the test set. The
models that had top recall scores in 3.2 achieved
0.2 and 0.18 for SEPR and SEPR-X test sets re-
spectively, compared to 0.25 and 0.17 in the word
space parameter optimization.
4.2 Filtering expansion words and
Levenshtein normalization
Abbreviation expansion with filtering and normal-
ization was evaluated on the SEPR and SEPR-X
test sets. The results are summarized in Table 5.
SEPR SEPR-X
SUC 0.09 0.16
LTK 0.08 0.14
Expansion word extraction 0.20 0.18
Filtering and normalization 0.38 0.40
Table 5: SEPR and SEPR-X test set results in ab-
breviation expansion.
Baseline recall scores were 0.09 and 0.08 for
SUC and LTK respectively, showing a lower score
for LTK compared to the results on the SEPR de-
velopment set. For abbreviation expansion (with
filtering and normalization) using semantically re-
lated words in combination with LTK, the best re-
call score was 0.38 for the SEPR test set, com-
pared to 0.39 for the same model evaluated on the
SEPR development set. Compared to the results of
using semantically related words only (expansion
word extraction), recall increased by 18 percent-
100
age points for the same model when filtering and
normalization was applied.
Evaluation on the SEPR-X test set gave higher
recall scores for both baseline corpora compared
to the baseline results for the SEPR-X develop-
ment set: the SUC result increased by 8 percentage
points for recall. For LTK, there was an increase in
recall of 3 percentage points. For the SEPR-X test
set, recall increased by 22 percentage points when
filtering and normalization was applied to seman-
tically related words extracted from the best model
configuration.
In comparison to the results of Henriksson et
al (2012), where recall of the best model is 0.31
without and 0.42 with post-processing of the ex-
pansion words for word spaces induced from the
data set (i.e., an increase in recall by 11 percentage
points), the filtering and normalization procedure
for expansion words of the current study yielded
an increase by 18 percentage points.
5 Discussion
The filter combined with the Levenshtein normali-
sation procedure to refine candidate expansion se-
lection showed a slight improvement compared to
using post-processing, although the normalization
procedure should be elaborated in order to be able
to confidently claim that Levenshtein distance nor-
malization is a better approach to expansion candi-
date selection. A suggestion for future work is to
introduce weights based on frequently occurring
edits between abbreviations and expansions and to
apply these in abbreviation normalization.
The approach presented in this study is limited
to abbreviations that translate into one full length
word. Future research should include handling
multiword expressions, not only unigrams, in or-
der to process acronyms and initialisms.
Recall of the development sets in the word
space parameter optimization experiments showed
higher scores for SEPR (0.25) compared to SEPR-
X (0.17). An explanation to this could be that the
amount of data preprocessing done prior to word
space induction might have varied, in terms of ex-
cluding sentences with little or no clinical con-
tent. This will of course affect word space co-
occurrence information, as word context is accu-
mulated without taking sentence boundaries into
account.
The lemmatization of the clinical text used for
word space induction left some words in their
original form, causing test data and semantically
related words to be morphologically discrepant.
Lemmatization adapted to clinical text might have
improved results. Spelling errors were also fre-
quent in the clinical text, and abbreviations were
sometimes normalized into a misspelled variant of
the correct expansion. In the future, spelling cor-
rection could be added and combined with abbre-
viation expansion.
The impact that this apporach to abbreviation
expansion might have on readability of clinical
texts should also be assessed by means of an ex-
trinsic evaluation, a matter to be pursued in future
research.
6 Conclusions
We presented automatic expansion of abbrevia-
tions consisting of unigram full-length words in
clinical texts. We applied a distributional semantic
approach by using word space models and com-
bined this with Levenshtein distance measures to
choose the correct candidate among the semanti-
cally related words. The results show that the cor-
rect expansion of the abbreviation can be found
in 40% of the cases, an improvement by 24 per-
centage points compared to the baseline (0.16) and
an increase by 22 percentage points compared to
using word space models alone (0.18). Applying
Levenshtein distance to refine the selection of se-
mantically related candidate expansions yields a
total recall of 0.38 and 0.40 for radiology reports
and medical health records, respectively.
Acknowledgments
The study was partly funded by the V?ardal Fun-
dation and supported by the Swedish Foundation
for Strategic Research through the project High-
Performance Data Mining for Drug Effect Detec-
tion (ref. no. IIS11-0053) at Stockholm Univer-
sity, Sweden. The authors would also like to direct
thanks to the reviewers for valuable comments.
References
M. Adnan, J. Warren, and M. Orr. 2010. Assess-
ing text characteristics of electronic discharge sum-
maries and their implications for patient readability.
In Proceedings of the Fourth Australasian Workshop
on Health Informatics and Knowledge Management-
Volume 108, pages 77?84. Australian Computer So-
ciety, Inc.
101
H. Allvin. 2010. Patientjournalen som genre: En text-
och genreanalys om patientjournalers relation till pa-
tientdatalagen. Master?s thesis, Stockholm Univer-
sity.
H. Ao and T. Takagi. 2005. ALICE: an algorithm
to extract abbreviations from MEDLINE. Journal
of the American Medical Informatics Association,
12(5):576?586.
S. Cederblom. 2005. Medicinska f?orkortningar och
akronymer (In Swedish). Studentlitteratur.
J.T. Chang, H. Sch?utze, and R.B. Altman. 2002. Creat-
ing an online dictionary of abbreviations from med-
line. Journal of the American Medical Informatics
Association, 9:612?620.
T. Cohen and D. Widdows. 2009. Empirical dis-
tributional semantics: Methods and biomedical ap-
plications. Journal of Biomedical Informatics,
42(2):390?405.
H. Dalianis, M. Hassel, and S. Velupillai. 2009. The
Stockholm EPR Corpus ? Characteristics and some
initial findings. In Proceedings of the 14th Interna-
tional Symposium on Health Information Manage-
ment Research, pages 243?249.
D. Dann?ells. 2006. Automatic acronym recognition.
In Proceedings of the 11th conference on European
chapter of the Association for Computational Lin-
guistics (EACL), pages 167?170.
N. Elhadad. 2006. User-sensitive text summarization:
Application to the medical domain. Ph.D. thesis,
Columbia University.
S. Gaudan, H. Kirsch, and D. Rebholz-Schuhmann.
2005. Resolving abbreviations to their senses in
MEDLINE. Bioinformatics, 21(18):3658?3664,
September.
Z.S. Harris. 1954. Distributional structure. Word,
10:146?162.
A. Henriksson, H. Moen, M. Skeppstedt, A. Eklund,
V. Daudaravicius, and M. Hassel. 2012. Syn-
onym Extraction of Medical Terms from Clinical
Text Using Combinations of Word Space Models.
In Proceedings of Semantic Mining in Biomedicine
(SMBM 2012), pages 10?17.
A. Henriksson, H. Moen, M. Skeppstedt, V. Daudar-
avicius, and M. Duneld. 2014. Synonym extrac-
tion and abbreviation expansion with ensembles of
semantic spaces. Journal of Biomedical Semantics,
5(6).
A. Henriksson. 2013. Semantic Spaces of Clini-
cal Text: Leveraging Distributional Semantics for
Natural Language Processing of Electronic Health
Records. Licentiate thesis, Department of Computer
and Systems Sciences, Stockholm University.
N. Isenius, S. Velupillai, and M. Kvist. 2012.
Initial Results in the Development of SCAN: a
Swedish Clinical Abbreviation Normalizer. In Pro-
ceedings of the CLEF 2012 Workshop on Cross-
Language Evaluation of Methods, Applications, and
Resources for eHealth Document Analysis (CLEFe-
Health2012).
G. K?allgren. 1998. Documentation of the Stockholm-
Ume?a corpus. Department of Linguistics, Stockholm
University.
P. Kanerva, J. Kristoferson, and A. Holst. 2000. Ran-
dom indexing of text samples for latent semantic
analysis. In Proceedings of the 22nd annual con-
ference of the cognitive science society, page 1036.
A. Keselman, L. Slaughter, C. Arnott-Smith, H. Kim,
G. Divita, A. Browne, C. Tsai, and Q. Zeng-Treitler.
2007. Towards consumer-friendly PHRs: patients
experience with reviewing their health records.
In AMIA Annual Symposium Proceedings, volume
2007, pages 399?403.
O. Knutsson, J. Bigert, and V. Kann. 2003. A ro-
bust shallow parser for Swedish. In Proceedings of
Nodalida.
D. Kokkinakis. 2012. The Journal of the
Swedish Medical Association-a Corpus Resource
for Biomedical Text Mining in Swedish. In Pro-
ceedings of Third Workshop on Building and Eval-
uating Resources for Biomedical Text Mining Work-
shop Programme, page 40.
K. Kukich. 1992. Techniques for automatically cor-
recting words in text. ACM Computing Surveys
(CSUR), 24(4):377?439.
M. Kvist and S. Velupillai. 2013. Professional Lan-
guage in Swedish Radiology Reports ? Charac-
terization for Patient-Adapted Text Simplification.
In Scandinavian Conference on Health Informatics
2013, pages 55?59.
V.I. Levenshtein. 1966. Binary codes capable of cor-
recting deletions, insertions and reversals. In Soviet
physics doklady, volume 10, page 707.
D. Movshovitz-Attias and W.W. Cohen. 2012.
Alignment-HMM-based Extraction of Abbrevia-
tions from Biomedical Text. In Proceedings of the
2012 Workshop on Biomedical Natural Language
Processing (BioNLP 2012), pages 47?55.
N. Nakatsu, Y. Kambayashi, and S. Yajima. 1982. A
longest common subsequence algorithm suitable for
similar text strings. Acta Informatica, 18(2):171?
179.
Y. Park and R.J. Byrd. 2001. Hybrid text mining for
finding abbreviations and their definitions. In Pro-
ceedings of the 2001 conference on empirical meth-
ods in natural language processing, pages 126?133.
102
E. Pettersson, B. Megyesi, and J. Nivre. 2013. Nor-
malisation of historical text using context-sensitive
weighted levenshtein distance and compound split-
ting. In Proceedings of the 19th Nordic Conference
of Computational Linguistics (NODALIDA 2013),
pages 163?179.
R.E. Rudd, B.A. Moeykens, and T.C. Colton. 1999.
Health and literacy: a review of medical and pub-
lic health literature. Office of Educational Research
and Improvement.
M. Sahlgren, A. Holst, and P. Kanerva. 2008. Permu-
tations as a means to encode order in word space. In
Proceedings of the 30th Annual Meeting of the Cog-
nitive Science Society, pages 1300?1305.
M. Sahlgren. 2006. The Word-space model. Ph.D.
thesis, Stockholm University.
A.S. Schwartz and M.A. Hearst. 2003. A simple al-
gorithm for identifying abbreviation definitions in
biomedical text. In Proceedings of Pacific Sympo-
sium on Biocomputing, pages 451?462.
M. Skeppstedt. 2012. From Disorder to Order: Ex-
tracting clinical findings from unstructured text. Li-
centiate thesis, Department of Computer and Sys-
tems Sciences, Stockholm University.
K. Taghva and J. Gilbreth. 1999. Recogniz-
ing acronyms and their definitions. International
Journal on Document Analysis and Recognition,
1(4):191?198.
H. Yu, G. Hripcsak, and C. Friedman. 2002. Map-
ping abbreviations to full forms in biomedical arti-
cles. Journal of the American Medical Informatics
Association, 9(3):262?272.
103
