Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1476?1480,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Using crowdsourcing to get representations based on regular expressions
Anders S?gaard and Hector Martinez and Jakob Elming and Anders Johannsen
Center for Language Technology
University of Copenhagen
DK-2300 Copenhagen S
{soegaard|alonso|zmk867|ajohannsen}@hum.ku.dk
Abstract
Often the bottleneck in document classifica-
tion is finding good representations that zoom
in on the most important aspects of the doc-
uments. Most research uses n-gram repre-
sentations, but relevant features often occur
discontinuously, e.g., not. . . good in sentiment
analysis. In this paper we present experi-
ments getting experts to provide regular ex-
pressions, as well as crowdsourced annota-
tion tasks from which regular expressions can
be derived. Somewhat surprisingly, it turns
out that these crowdsourced feature combina-
tions outperform automatic feature combina-
tion methods, as well as expert features, by a
very large margin and reduce error by 24-41%
over n-gram representations.
1 Introduction
Finding good representations of classification prob-
lems is often glossed over in the literature. Sev-
eral authors have emphasized the need to pay more
attention to finding such representations (Wagstaff,
2012; Domingos, 2012), but in document classifica-
tion most research still uses n-gram representations.
This paper considers two document classification
problems where such representations seem inade-
quate. The problems are answer scoring (Burstein
et al, 1998), on data from stackoverflow.com, and
multi-attribute sentiment analysis (McAuley et al,
2012). We argue that in order to adequately repre-
sent such problems we need discontinuous features,
i.e., regular expressions.
The problem with using regular expressions as
features is of course that even with a finite vocab-
ulary we can generate infinitely many regular ex-
pressions that match our documents. We suggest to
use expert knowledge or crowdsourcing in the loop.
In particular we present experiments where standard
representations are augmented with features from a
few hours of manual work, by machine learning ex-
perts or by turkers.
Somewhat surprisingly, we find that features de-
rived from crowdsourced annotation tasks lead to the
best results across the three datasets. While crowd-
sourcing of annotation tasks has become increasing
popular in NLP, this is, to the best of our knowledge,
the first attempt to crowdsource the problem of find-
ing good representations.
1.1 Related work
Musat et al (2012) design a collaborative two-player
game for sentiment annotation and collecting a sen-
timent lexicon. One player guesses the sentiment of
a text and picks a word from it that is representative
of its sentiment. The other player also provides a
guess observing only this word. If the two guesses
agree, both players get a point. The idea of gam-
ifying the problem of finding good representations
goes beyond crowdsourcing, but is not considered
here. Boyd-Graber et al (2012) crowdsource the
feature weighting problem, but using standard rep-
resentations. The work most similar to ours is prob-
ably Tamuz et al (2011), who learn a ?crowd kernel?
by asking annotators to rate examples by similarity,
providing an embedding that promotes feature com-
binations deemed relative when measuring similar-
ity.
1476
BoW Exp AMT
n P (1) m ?x m ?x m ?x
STACKOVERFLOW 97,519 0.5013 30,716 0.00131 1,156 0.1380 172,691 0.00331
TASTE 152,390 0.5003 38,227 0.00095 666 0.10631 114,588 0.00285
APPEARANCE 152,331 0.5009 37,901 0.00097 650 0.14629 102,734 0.00289
Table 1: Characteristics of the n?m data sets
2 Experiments
Data The three datasets used in our experi-
ments come from two sources, namely stackover-
flow.com and ratebeer.com. The two beer review
datasets (TASTE and APPEARANCE) are described
in McAuley et al (2012) and available for down-
load.1 Each input example is an unstructured review
text, and the associated label is the score assigned to
taste or appearance by the reviewer. We randomly
sample about 152k data points, as well as 500 exam-
ples for experiments with experts and turks.
We extracted the STACKOVERFLOW dataset from
a publicly available data dump,2, and we briefly de-
scribe our sampling process here. We select pairs of
answers, where one is ranked higher than the other
by stackoverflow.com users. Obviously the answers
submitted first have a better chance of being ranked
highly, so we also require that the highest ranked
answer was submitted last. From this set of answer
pairs, we randomly sample 97,519 pairs, as well as
500 examples for our experiments with experts and
turks.
Our experiments are classification experiments
using the same learning algorithm in all experi-
ments, namely L1-regularized logistic regression.
We don?t set any parameters The only differences
between our systems are in the feature sets. Results
are from 5-fold cross-validation. The four feature
sets are described below: BoW, HI, Exp and AMT.
For motivating using regular expressions, con-
sider the following sentence from a review of John
Harvard?s Grand Cru:
(1) Could have been more flavorful.
The only word carrying direct sentiment in this
sentence is flavorful, which is positive, but the sen-
tence is a negative evaluation of the Grand Cru?s
1http://snap.stanford.edu/data/web-RateBeer.html
2http://www.clearbits.net/torrents/2076-aug-2012
taste. The trigram been more flavorful seems neg-
ative at first, but in the context of negation or in a
comparative, it can become positive again. How-
ever, note that this trigram may occur discontinu-
ously, e.g., in been less watery and more flavorful.
In order to match such occurrences, we need simple
regular expressions, e.g.,:
been.*more.*flavorful
This is exactly the kind of regular expressions we
asked experts to submit, and that we derived from
the crowdsourced annotation tasks. Note that the
sentence says nothing about the beer?s appearance,
so this feature is only relevant in TASTE, not in
APPEARANCE.
BoW and BoW+HI Our most simple baseline ap-
proach is a bag-of-words model of unigram features
(BoW). We lower-case our data, but leave in stop
words. We also introduce a semantically enriched
unigram model (BoW)+HI, where in addition to
representing what words occur in a text, we also
represent what Harvard Inquirer (HI)3 word classes
occur in it. The HI classes are used to generate
features from the crowdsourced annotation tasks,
so the semantically enriched unigram model is an
important baseline in our experiments below.
BoW+Exp In order to collect regular expressions
from experts, we set up a web interface for query-
ing held-out portions of the datasets with regular ex-
pressions that reports how occurrences of the sub-
mitted regular expressions correlate with class. We
used the Python re syntax for regular expressions
after augmenting word forms with POS and seman-
tic classes from the HI. Few of the experts made use
of the POS tags, but many regular expressions in-
cluded references to HI classes.
3http://www.wjh.harvard.edu/ inquirer/homecat.htm
1477
Regular expressions submitted by participants
were visible to other participants during the exper-
iment, and participants were allowed to work to-
gether. Participants had 15 minutes to familiarize
themselves with the syntax used in the experiments.
Each query was executed in 2-30 seconds.
Seven researchers and graduate students spent
five effective hours querying the datasets with
regular expressions. In particular, they spent three
hours on the Stack Exchange dataset, and one hour
on each of the two RateBeer datasets. One had to
leave an hour early. So, in total, we spent 20 person
hours on Stack Exchange, and seven person hours
on each of the RateBeer datasets. In the five hours,
we collected 1,156 regular expressions for the
STACKOVERFLOW dataset, and about 650 regular
expressions for each of the two RateBeer datasets.
Exp refers to these sets of regular expressions. In
our experiments below we concatenate these with
the BoW features to form BoW+Exp.
BoW+AMT For each dataset, we also had 500 held-
out examples annotated by three turkers each, using
Amazon Mechanical Turk,4 obtaining 1,500 HITs
for each dataset. The annotators were presented with
each text, a review or an answer, twice: once as run-
ning text, once word-by-word with bullets to tick off
words. The annotators were instructed to tick off
words or phrases that they found predictive of the
text?s sentiment or answer quality. They were not in-
formed about the class of the text. We chose this an-
notation task, because it is relatively easy for annota-
tors to mark spans of text with a particular attribute.
This set-up has been used in other applications, in-
cluding NER (Finin et al, 2010) and error detection
(Dahlmeier et al, 2013). The annotators were con-
strained to tick off at least three words, including
one closed class item (closed class items were col-
ored differently). Finally, we only used annotators
with a track record of providing high-quality anno-
tations in previous tasks. It was clear from the aver-
age time spent by annotators that annotating STACK-
OVERFLOW was harder than annotating the Rate-
beer datasets. The average time spent on a Rate-
beer HIT was 44s, while for STACKOVERFLOW it
was 3m:8s. The mean number of words ticked off
4www.mturk.com
BoW HI Exp AMT
STACKOVERF 0.655 0.654 0.683 0.739
TASTE 0.798 0.797 0.798 0.867
APPEARANCE 0.758 0.760 0.761 0.859
Table 2: Results using all features
was between 5.6 and 7, with more words ticked off
in STACKOVERFLOW. The maximum number of
words ticked off by an annotator was 41. We spent
$292.5 on the annotations, including a trial round.
This was supposed to match, roughly, the cost of the
experts consulted for BoW+Exp.
The features generated from the annotations were
constructed as follows: We use a sliding window of
size 3 to extract trigrams over the possibly discon-
tinuous words ticked off by the annotators. These
trigrams were converted into regular expressions by
placing Kleene stars between the words. This gives
us a manually selected subset of skip trigrams. For
each skip trigram, we add copies with one or more
words replaced by one of their HI classes.
Feature combinations This subsection introduces
some harder baselines for our experiments, consid-
ered in Experiment #2. The simplest possible way
of combining unigram features is by considering n-
gram models. An n-gram extracts features from a
sliding window (of size n) over the text. We call this
model BoW(N = n). Our BoW(N = 1) model
takes word forms as features, and there are obvi-
ously more advanced ways of automatically combin-
ing such features.
Kernel representations We experimented with ap-
plying an approximate feature map for the addi-
tive ?2-kernel. We used two sample steps, result-
ing in 4N + 1 features. See Vedaldi and Zimmer-
man (2011) for details.
Deep features We also ran denoising autoen-
coders (Pascal et al, 2008), previously applied
to a wide range of NLP tasks (Ranganath et al,
2009; Socher et al, 2011; Chen et al, 2012), with
2N nodes in the middle layer to obtain a deep
representation of our datasets from ?2-BoW input.
The network was trained for 15 epochs. We set the
drop-out rate to 0.0 and 0.3.
Summary of feature sets The feature sets ? BoW,
1478
10
2
10
3
N
60
62
64
66
68
70
72
74
A
c
c
u
r
a
c
y
StackOverflow
?
2
?BoW
?
2
?BoW+HI
?
2
?BoW+Exp
?
2
?BoW+AMT
10
2
10
3
N
70
75
80
85
90
A
c
c
u
r
a
c
y
Taste
?
2
?BoW
?
2
?BoW+HI
?
2
?BoW+Exp
?
2
?BoW+AMT
10
2
10
3
N
70
75
80
85
90
A
c
c
u
r
a
c
y
Appearance
?
2
?BoW
?
2
?BoW+HI
?
2
?BoW+Exp
?
2
?BoW+AMT
Figure 1: Results selecting N features using ?2 (left to right): STACKOVERFLOW, TASTE, and APPEARANCE. The
x-axis is logarithmic scale.
10
2
10
3
N
60
62
64
66
68
70
72
74
A
c
c
u
r
a
c
y
?
2
?BoW(N=1)
?
2
?BoW(N=2)
?
2
?kernel
?
2
?dauto
?
2
?BoW+AMT
10
2
10
3
N
70
75
80
85
90
A
c
c
u
r
a
c
y
?
2
?BoW(N=1)
?
2
?BoW(N=2)
?
2
?kernel
?
2
?dauto
?
2
?BoW+AMT
10
2
10
3
N
70
72
74
76
78
80
82
84
A
c
c
u
r
a
c
y
?
2
?BoW(N=1)
?
2
?BoW(N=2)
?
2
?kernel
?
2
?dauto
?
2
?BoW+AMT
Figure 2: Results using different feature combination techniques (left to right): STACKOVERFLOW, TASTE, and
APPEARANCE. The x-axis is logarithmic scale.
Exp and AMT ? are very different. Their character-
istics are presented in Table 1. P (1) is the class dis-
tribution, e.g., the prior probability of positive class.
n is the number of data points, m the number of
features. Finally, ?x is the average density of data
points. One observation is of course that the expert
feature set Exp is much smaller than BoW and AMT,
but note also that the expert features fire about 150
times more often on average than the BoW features.
HI is only a small set of additional features.
3 Results
Experiment #1: BoW vs. Exp and AMT We present
results using all features, as well as results obtained
after selecting k features as ranked by a simple ?2
test. The results using all collected features are pre-
sented in Table 2. The error reduction on STACK-
OVERFLOW when adding crowdsourced features to
our baseline model (BoW+AMT), is 24.3%. On
TASTE, it is 34.2%. On APPEARANCE, it is 41.0%.
The BoW+AMT feature set is bigger than those of
the other models. We therefore report results using
the top-k features as ranked by a simple ?2 test.
The result curves are presented in the three plots in
Fig. 1. With +500 features, BoW+AMT outperforms
the other models by a large margin.
Experiment #2: AMT vs. more baselines The
BoW baseline uses a standard representation that,
while widely used, is usually thought of as a weak
baseline. BoW+HIT did not provide a stronger base-
line. We also show that bigram features, kernel-
based decomposition and deep features do not pro-
vide much stronger baselines either. The result
curves are presented in the three plots in Fig. 2.
BoW+AMT is still significantly better than all other
models with +500 features. Since autoencoders
are consistently worse than denoising autoencoders
(drop-out 0.3), we only plot denoising autoencoders.
4 Conclusion
We presented a new method for deriving feature
representations from crowdsourced annotation tasks
and showed how it leads to 24%-41% error reduc-
tions on answer scoring and multi-aspect sentiment
analysis problems. We saw no significant improve-
ments using features contributed by experts, kernel
representations or learned deep representations.
1479
References
Jordan Boyd-Graber, Brianna Satinoff, He He, and Hal
Daume. 2012. Besting the quiz master: Crowdsourc-
ing incremental classification games. In NAACL.
Jill Burstein, Karen Kukich, Susanne Wolff, Chi Lu,
Martin Chodorow, Lisa Braden-Harder, and Mary Dee
Harris. 1998. Automated scoring using a hybrid fea-
ture identification technique. In ACL.
Minmin Chen, Zhixiang Xu, Kilian Weinberger, and Fei
Sha. 2012. Marginalized denoising autoencoders for
domain adaptation. In ICML.
Daniel Dahlmeier, Hwee Tou Ng, and Siew Mei Wu.
2013. Building a large annotated corpus of learner
English. In Workshop on Innovative Use of NLP for
Building Educational Applications, NAACL.
Pedro Domingos. 2012. A few useful things to know
about machine learning. In CACM.
Tim Finin, Will Murnane, Anand Karandikar, Nicholas
Keller, Justin Martineau, and Mark Dredze. 2010.
Annotating named entities in Twitter data with crowd-
sourcing. In NAACL Workshop on Creating Speech
and Language Data with Amazon?s Mechanical Turk.
Julian McAuley, Jure Leskovec, and Dan Jurafsky. 2012.
Learning attitudes and attributes from multi-aspect re-
views. In ICDM.
Claudiu-Christian Musat, Alireza Ghasemi, and Boi Falt-
ings. 2012. Sentiment analysis using a novel human
computation game. In Workshop on the People?s Web
Meets NLP, ACL.
Vincent Pascal, Hugo Larochelle, Yoshua Bengio, and
Pierre-Antoine Manzagol. 2008. Extracting and com-
posing robust features with denoising autoencoders. In
ICML.
Rajesh Ranganath, Dan Jurafsky, and Dan McFarland.
2009. It?s not you, it?s me: detecting flirting and its
misperception in speed-dates. In NAACL.
Richard Socher, Eric Huan, Jeffrey Pennington, Andrew
Ng, and Christopher Manning. 2011. Dynamic pool-
ing and unfolding recursive autoencoders for para-
phrase detection. In NIPS.
Omer Tamuz, Ce Liu, Serge Belongie, Ohad Shamir, and
Adam Tauman Kalai. 2011. Adaptively learning the
crowd kernel. In ICML.
Andrea Vedaldi and Andrew Zisserman. 2011. Efficient
additive kernels via explicit feature maps. In CVPR.
Kiri Wagstaff. 2012. Machine learning that matters. In
ICML.
1480
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 968?973,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Importance weighting and unsupervised domain adaptation
of POS taggers: a negative result
Barbara Plank, Anders Johannsen and Anders S?gaard
Center for Language Technology
University of Copenhagen, Denmark
Njalsgade 140, DK-2300 Copenhagen S
bplank@cst.dk,ajohannsen@hum.ku.dk,soegaard@hum.ku.dk
Abstract
Importance weighting is a generalization
of various statistical bias correction tech-
niques. While our labeled data in NLP is
heavily biased, importance weighting has
seen only few applications in NLP, most of
them relying on a small amount of labeled
target data. The publication bias toward
reporting positive results makes it hard to
say whether researchers have tried. This
paper presents a negative result on unsu-
pervised domain adaptation for POS tag-
ging. In this setup, we only have unlabeled
data and thus only indirect access to the
bias in emission and transition probabili-
ties. Moreover, most errors in POS tag-
ging are due to unseen words, and there,
importance weighting cannot help. We
present experiments with a wide variety of
weight functions, quantilizations, as well
as with randomly generated weights, to
support these claims.
1 Introduction
Many NLP tasks rely on the availability of anno-
tated data. The majority of annotated data, how-
ever, is sampled from newswire corpora. The
performance of NLP systems, e.g., part-of-speech
(POS) tagger, parsers, relation extraction sys-
tems, etc., drops significantly when they are ap-
plied to data that departs from newswire conven-
tions. So while we can extract information, trans-
late and summarize newswire in major languages
with some success, we are much less successful
processing microblogs, chat, weblogs, answers,
emails or literature in a robust way. The main rea-
sons for the drops in accuracy have been attributed
to factors such as previously unseen words and bi-
grams, missing punctuation and capitalization, as
well as differences in the marginal distribution of
data (Blitzer et al., 2006; McClosky et al., 2008;
S?gaard and Haulrich, 2011).
The move from one domain to another (from a
source to a new target domain), say from newspa-
per articles to weblogs, results in a sample selec-
tion bias. Our training data is now biased, since
it is sampled from a related, but nevertheless dif-
ferent distribution. The problem of automatically
adjusting the model induced from source to a dif-
ferent target is referred to as domain adaptation.
Some researchers have studied domain adap-
tation scenarios, where small samples of labeled
data have been assumed to be available for the
target domains. This is usually an unrealistic as-
sumption, since even for major languages, small
samples are only available from a limited number
of domains, and in this work we focus on unsuper-
vised domain adaptation, assuming only unlabeled
target data is available.
Jiang and Zhai (2007), Foster et al. (2010; Plank
and Moschitti (2013) and S?gaard and Haulrich
(2011) have previously tried to use importance
weighting to correct sample bias in NLP. Im-
portance weighting means assigning a weight
to each training instance, reflecting its impor-
tance for modeling the target distribution. Im-
portance weighting is a generalization over post-
stratification (Smith, 1991) and importance sam-
pling (Smith et al., 1997) and can be used to cor-
rect bias in the labeled data.
Out of the four papers mentioned, only S?gaard
and Haulrich (2011) and Plank and Moschitti
(2013) considered an unsupervised domain adap-
tation scenario, obtaining mixed results. These
two papers assume covariate shift (Shimodaira,
2000), i.e., that there is only a bias in the marginal
distribution of the training data. Under this as-
sumption, we can correct the bias by applying a
weight function
P
t
(x)
P
s
(x)
to our training data points
(labeled sentences) and learn from the weighted
data. Of course this weight function cannot be
968
computed in general, but we can approximate it
in different ways.
In POS tagging, we typically factorize se-
quences into emission and transition probabilities.
Importance weighting can change emission prob-
abilities and transition probabilities by assigning
weights to sentences. For instance, if our corpus
consisted of three sequences: 1) a/A b/A, 2) a/A
b/B, and 3) a/A b/B, then P (B|A) = 2/3. If se-
quences two and three were down-weighted to 0.5,
then P (B|A) = 1/2.
However, this paper argues that importance
weighting cannot help adapting POS taggers to
new domains using only unlabeled target data. We
present three sources of evidence: (a) negative
results with the most obvious weight functions
across various English datasets, (b) negative re-
sults with randomly sampled weights, as well as
(c) an analysis of annotated data indicating that
there is little variation in emission and transition
probabilities across the various domains.
2 Related work
Most prior work on importance weighting use a
domain classifier, i.e., train a classifier to discrimi-
nate between source and target instances (S?gaard
and Haulrich, 2011; Plank and Moschitti, 2013)
(y ? {s, t}). For instance, S?gaard and Haulrich
(2011) train a n-gram text classifier and Plank
and Moschitti (2013) a tree-kernel based clas-
sifier on relation extraction instances. In these
studies,
?
P (t|x) is used as an approximation of
P
t
(x)
P
s
(x)
, following Zadrozny (2004). In ?3, we fol-
low the approach of S?gaard and Haulrich (2011),
but consider a wider range of weight functions.
Others have proposed to use kernel mean match-
ing (Huang et al., 2007) or minimizing KL-
divergence (Sugiyama et al., 2007).
Jiang and Zhai (2007) use importance weight-
ing to select a subsample of the source data by
subsequently setting the weight of all selected data
points to 1, and 0 otherwise. However, they do
so by relying on a sequential model trained on
labeled target data. Our results indicate that the
covariate shift assumption fails to hold for cross-
domain POS tagging. While the marginal distri-
butions obviously do differ (since we can tell do-
mains apart without POS analysis), this is most
likely not the only difference. This might explain
the positive results obtained by Jiang and Zhai
(2007). We will come back to this in ?4.
Cortes et al. (2010) show that importance
weighting potentially leads to over-fitting, but pro-
pose to use quantiles to obtain more robust weight
functions. The idea is to rank all weights and ob-
tain q quantiles. If a data point x is weighted by
w, and w lies in the ith quantile of the ranking
(i ? q), x is weighted by the average weight of
data points in the ith quantile.
The weighted structured perceptron (?3) used in
the experiments below was recently used for a dif-
ferent problem, namely for correcting for bias in
annotations (Plank et al., 2014).
l l l l
l l l l l l l l l l l l l l l l
0 5 10 15 20
92
93
94
95
96
97
98
99 l wsjanswersreviews
emailsweblogsnewsgroups
Figure 1: Training epochs vs tagging accuracy for
the baseline model on the dev data.
3 Experiments
3.1 Data
We use the data made available in the SANCL
2012 Shared Task (Petrov and McDonald, 2012).
The training data is the OntoNotes 4.0 release
of the Wall Street Journal section of the Penn
Treebank, while the target domain evaluation data
comes from various sources, incl. Yahoo Answers,
user reviews, emails, weblogs and newsgroups.
For each target domain, we have both development
and test data.
3.2 Model
In the weighted perceptron (Cavallanti et al.,
2006), we make the learning rate dependent on the
current instance x
n
, using the following update:
w
i+1
? w
i
+ ?
n
?(y
n
? sign(w
i
? x
n
))x
n
(1)
where ?
n
is the weight associated with x
n
. See
Huang et al. (2007) for similar notation.
We extend this idea straightforwardly to the
structured perceptron (Collins, 2002), for which
969
System Answers Newsgroups Reviews Avg Emails Weblogs WSJ
Our system 91.08 91.57 91.59 91.41 87.97 92.19 97.32
SANCL12-2nd 90.99 92.32 90.65 91.32 ? ? 97.76
SANCL12-best 91.79 93.81 93.11 92.90 ? ? 97.29
SANCL12-last 88.24 89.70 88.15 88.70 ? ? 95.14
FLORS basic 91.17 92.41 92.25 88.67 91.37 97.11 91.94
Table 1: Tagging accuracies and comparison to prior work on the SANCL test sets (fine-grained POS).
we use an in-house implementation. We use
commonly used features, i.e., w,w
?1
, w
?2
,
w
+1
, w
+2
, digit, hyphen, capitalization, pre-
/suffix features, and Brown word clusters. The
model seems robust with respect to number
of training epochs, cf. Figure 1. Therefore
we fix the number of epochs to five and use
this setting in all our experiments. Our code
is available at: https://bitbucket.org/
bplank/importance-weighting-exp.
3.3 Importance weighting
In our first set of experiments, we follow S?gaard
and Haulrich (2011) in using document classifiers
to obtain weights for the source instances. We
train a text classifier that discriminates the two
domains (source and target). For each sentence
in the source and target domain (the unlabeled
text that comes with the SANCL data), we mark
whether it comes from the source or target do-
main and train a binary classifier (logistic regres-
sion) to discriminate between the two. For ev-
ery sentence in the source we obtain its probabil-
ity for the target domain by doing 5-fold cross-
validation. While S?gaard and Haulrich (2011)
use only token-based features (word n-grams ?
3), we here exploit a variety of features: word
token n-grams, and two generalizations: using
Brown clusters (estimated from the union of the
5 target domains), and Wiktionary tags (if a word
has multiple tags, we assign it the union of tags as
single tag; OOV words are marked as such).
The distributions of weights can be seen in the
upper half of Figure 2.
3.3.1 Results
Table 1 shows that our baseline model achieves
state-of-the-art performance compared to
SANCL (Petrov and McDonald, 2012)
1
and
FLORS (Schnabel and Sch?utze, 2014). Our
results align well with the second best POS
tagger in the SANCL 2012 Shared Task. Note
1
https://sites.google.com/site/sancl2012/home/
shared-task/results
Figure 2: Histogram of different weight functions.
that the best tagger in the shared task explicitly
used normalization and various other heuristics
to achieve better performance. In the rest of the
paper, we use the universal tag set part of the
SANCL data (Petrov et al., 2012).
Figure 3 presents our results on development
data for different importance weighting setups.
None of the above weight functions lead to signifi-
cant improvements on any of the datasets. We also
tried scaling and binning the weights, as suggested
by Cortes et al. (2010), but results kept fluctuating
around baseline performance, with no significant
improvements.
3.4 Random weighting
Obviously, weight functions based on document
classifiers may simply not characterize the rele-
vant properties of the instances and hence lead to
bad re-weighting of the data. We consider three
random sampling strategies, namely sampling ran-
dom uniforms, random exponentials, and random
970
Figure 3: Results on development data for different weight functions, i.e., document classifiers trained
on a) raw tokens; b) tokens replaced by Wiktionary tags; c) tokens replaced by Brown cluster ids. The
weight was the raw p
t
(y|x) value, no scaling, no quantiles. Replacing only open-class tokens for b) and
c) gave similar or lower performance.
Zipfians and ran 500 samples for each. For these
experiments, we estimate significance cut-off lev-
els of tagging accuracies using the approximate
randomization test. To find the cut-off levels,
we randomly replace labels with gold labels until
the achieved accuracy significantly improves over
the baseline for more than 50% of the samples.
For each accuracy level, 50 random samples were
taken.
llll
l
l
llll
lll
ll
llll
l
ll
l
l
llll
l
l
ll
lll
llllll
l
llll
llllll
llllll
l
lll
lllllll
l
ll
l
llllll
l
l
l
lll
llllll
l
l
lllll
ll
l
l
lllllll
llllll
l
l
l
llll
llll
l
lll
l
ll
ll
ll
ll
ll
l
llll
ll
l
ll
ll
ll
ll
l
ll
lllllll
l
l
l
lll
llllll
l
l
ll
ll
ll
ll
l
l
lllll
ll
l
llll
lllllll
llllll
l
ll
ll
ll
l
l
lll
lll
ll
lll
l
l
l
l
lllll
l
lll
l
l
lllll
l
ll
0 200 400
93.2
93.6
94.0
answers
Index
l randomexpzipf
ll
ll
l
lllll
ll
l
l
ll
l
l
l
l
l
l
ll
l
ll
ll
ll
ll
ll
llll
ll
l
ll
l
l
ll
ll
l
ll
ll
llll
lllll
l
ll
l
l
l
l
ll
l
ll
l
lllll
l
l
ll
l
ll
lll
ll
l
l
lll
lll
l
ll
ll
l
llll
ll
l
l
l
l
l
l
l
ll
l
l
l
l
lllll
lllllll
l
ll
ll
l
l
l
llll
ll
l
l
ll
ll
ll
l
ll
ll
l
l
l
lll
l
l
l
ll
l
ll
lll
lll
ll
lll
l
ll
ll
llll
ll
l
ll
l
l
llllll
lll
l
l
lll
llll
l
lll
ll
l
l
l
ll
l
l
lll
l
l
l
l
l
llll
lllll
0 200 4009
4.2
94.4
94.6
94.8
reviews
Index
TA
l
lll
l
llll
l
ll
llll
ll
llll
ll
llllll
ll
ll
llll
l
l
l
ll
llllll
lll
ll
lll
l
ll
l
llll
l
ll
l
ll
lll
ll
l
l
l
lll
ll
l
lll
l
l
l
ll
ll
ll
lll
llll
l
ll
lllll
l
llll
lll
l
l
lll
ll
l
l
l
llll
l
llll
l
l
l
lll
ll
ll
l
llllll
ll
l
llll
ll
lllll
lll
l
l
lllll
l
l
ll
l
l
l
l
lllll
ll
ll
l
lll
lll
ll
l
l
l
l
ll
lllll
lll
llll
l
l
llllllll
l
ll
llll
lll
l
l
l
l
lll
l
l
l
l
l
ll
ll
lll
l
lll
ll
l
ll
l
l
l
ll
ll
llll
llll
0 200 400
93.4
93.8
emails
Index
TA
l
ll
lll
ll
l
lll
ll
ll
l
l
ll
ll
llllllll
ll
lllll
l
l
l
ll
l
l
ll
l
lll
lll
llll
l
lll
l
l
lllll
ll
lll
lllllll
llll
ll
l
l
ll
ll
l
l
lllll
lllll
llllll
lllll
lllllll
l
ll
lllll
l
ll
l
l
ll
l
l
llllll
l
l
l
l
llll
llllll
ll
ll
lllll
llllllll
l
lll
l
ll
l
l
lllllll
l
ll
l
l
lllll
lll
ll
ll
llll
llllll
l
llll
ll
l
l
l
ll
lll
l
llll
l
lllllll
llll
l
l
lll
l
l
ll
l
l
l
lll
l
l
l
0 200 400
94.4
94.8
95.2
weblogs
Index
TA
lll
lll
lll
ll
ll
lll
lll
l
ll
ll
l
lllllllll
lllll
l
l
lll
l
ll
ll
ll
llll
l
llll
llll
l
l
lll
l
lllll
lllll
llllllll
llll
llll
l
ll
l
lll
ll
ll
lllll
lllllllllll
llll
llll
l
ll
lll
ll
lllllll
lllll
l
llllll
l
l
lll
ll
l
lllll
l
ll
ll
ll
lll
ll
llll
lll
l
llll
llllll
lll
lll
llll
lllll
ll
ll
l
llllllll
ll
ll
llll
lll
l
l
llll
l
ll
l
llll
llll
l
ll
l
lll
ll
l
l
lll
llllll
ll
l
0 200 400
94.2
94.6
95.0
newsgroups
Index
TA
0 200 400
93.0
93.4
93.8
answers
Index
0 200 400
94.2
94.6
reviews
Index
TA
0 200 400
93.2
93.6
94.0
emails
Index
TA
0 200 400
94.2
94.6
95.0
95.4
weblogs
Index
TA
0 200 400
94.0
94.4
94.8
newsgroups
Index
TA
0 50 100 150
92.5
93.0
93.5
94.0
answers
0 50 100 150
93.5
94.0
94.5
reviews
TA
0 50 100 150
92.5
93.0
93.5
94.0
emails
TA
0 50 100 150
94.0
94.5
95.0
weblogs
TA
0 50 100 1509
3.5
94.0
94.5
95.0
newsgroups
TA
Figure 4: Random weight functions (500 runs
each) on test sets. Solid line is the baseline per-
formance, while the dashed line is the p-value cut-
off. From top: random, exponential and Zipfian
weighting. All runs fall below the cut-off.
3.4.1 Results
The dashed lines in Figure 4 show the p-value cut-
offs for positive results. We see that most random
weightings of data lead to slight drops in perfor-
mance or are around baseline performance, and no
weightings lead to significant improvements. Ran-
dom uniforms seem slightly better than exponen-
tials and Zipfians.
domain (tokens) avg tag ambiguity OOV KL ?
type token
wsj (train/test: 731k/39k) 1.09 1.41 11.5 0.0006 0.99
answers (28k) 1.09 1.22 27.7 0.048 0.77
reviews (28k) 1.07 1.19 29.5 0.040 0.82
emails (28k) 1.07 1.19 29.9 0.027 0.92
weblogs (20k) 1.05 1.11 22.1 0.010 0.96
newsgroups (20k) 1.05 1.14 23.1 0.011 0.96
Table 2: Relevant statistics for our analysis (?4)
on the test sets: average tag ambiguity, out-of-
vocabulary rate, and KL-divergence and Pearson
correlation coefficient (?) on POS bigrams.
4 Analysis
Some differences between the gold-annotated
source domain data and the gold-annotated tar-
get data used for evaluation are presented in Ta-
ble 2. One important observation is the low ambi-
guity of word forms in the data. This makes the
room for improvement with importance weight-
ing smaller. Moreover, the KL divergencies over
POS bigrams are also very low. This tells us that
transition probabilities are also relatively constant
across domains, again suggesting limited room for
improvement for importance weighting.
Compared to this, we see much bigger differ-
ences in OOV rates. OOV rates do seem to explain
most of the performance drop across domains.
In order to verify this, we implemented a ver-
sion of our structured perceptron tagger with type-
constrained inference (T?ackstr?om et al., 2013).
This technique only improves performance on un-
seen words, but nevertheless we saw significant
improvements across all five domains (cf. Ta-
ble 3). This suggests that unseen words are a
more important problem than the marginal distri-
bution of data for unsupervised domain adaptation
of POS taggers.
971
ans rev email webl newsg
base 93.41 94.44 93.54 94.81 94.55
+type constr. 94.09? 94.85? 94.31? 95.99? 95.97?
p-val cut-off 93.90 94.85 94.10 95.3 95.10
Table 3: Results on the test sets by adding Wik-
tionary type constraints. ?=p-value < 0.001.
We also tried Jiang and Zhai?s subset selection
technique (?3.1 in Jiang and Zhai (2007)), which
assumes labeled training material for the target
domain. However, we did not see any improve-
ments. A possible explanation for these different
findings might be the following. Jiang and Zhai
(2007) use labeled target data to learn their weight-
ing model, i.e., in a supervised domain adaptation
scenario. This potentially leads to very different
weight functions. For example, let the source do-
main be 100 instances of a/A b/B and 100 in-
stances of b/B b/B, and the target domain be 100
instances of a/B a/B. Note that a domain classi-
fier would favor the first 100 sentences, but in an
HMM model induced from the labeled target data,
things look very different. If we apply Laplace
smoothing, the probability of a/A b/B accord-
ing to the target domain HMM model would be
? 8.9e
?7
, and the probability of b/B b/B would
be ? 9e
?5
. Note also that this set-up does not as-
sume covariate shift.
5 Conclusions and Future Work
Importance weighting, a generalization of various
statistical bias correction techniques, can poten-
tially correct bias in our labeled training data, but
this paper presented a negative result about impor-
tance weighting for unsupervised domain adapta-
tion of POS taggers. We first presented exper-
iments with a wide variety of weight functions,
quantilizations, as well as with randomly gener-
ated weights, none of which lead to significant im-
provements. Our analysis indicates that most er-
rors in POS tagging are due to unseen words, and
what remains seem to not be captured adequately
by unsupervised weight functions.
For future work we plan to extend this work to
further weight functions, data sets and NLP tasks.
Acknowledgements
This research is funded by the ERC Starting Grant
LOWLANDS No. 313695.
References
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In EMNLP.
Giovanni Cavallanti, Nicol`o Cesa-Bianchi, and Clau-
dio Gentile. 2006. Tracking the best hyperplane
with a simple budget perceptron. In COLT.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In EMNLP.
Corinna Cortes, Yishay Mansour, and Mehryar Mohri.
2010. Learning bounds for importance weighting.
In NIPS.
George Foster, Cyril Goutte, and Roland Kuhn. 2010.
Discriminative instance weighting for domain adap-
tation in statistical machine translation. In EMNLP.
Jiayuan Huang, Alexander Smola, Arthur Gretton,
Karsten Borgwardt, and Bernhard Sch?olkopf. 2007.
Correcting sample bias by unlabeled data. In NIPS.
Jing Jiang and ChengXiang Zhai. 2007. Instance
weighting for domain adaptation in NLP. In ACL.
David McClosky, Eugene Charniak, and Mark John-
son. 2008. When is self-training effective for pars-
ing? In COLING.
Slav Petrov and Ryan McDonald. 2012. Overview of
the 2012 Shared Task on Parsing the Web. In Notes
of the First Workshop on Syntactic Analysis of Non-
Canonical Language (SANCL).
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012.
A universal part-of-speech tagset. In LREC.
Barbara Plank and Alessandro Moschitti. 2013. Em-
bedding semantic similarity in tree kernels for do-
main adaptation of relation extraction. In ACL.
Barbara Plank, Dirk Hovy, and Anders S?gaard. 2014.
Learning part-of-speech taggers with inter-annotator
agreement loss. In EACL.
Tobias Schnabel and Hinrich Sch?utze. 2014. Flors:
Fast and simple domain adaptation for part-of-
speech tagging. TACL, 2:15?16.
Hidetoshi Shimodaira. 2000. Improving predictive in-
ference under covariate shift by weighting the log-
likelihood function. Journal of Statistical Planning
and Inference, 90:227?244.
Peter Smith, Mansoor Shafi, and Hongsheng Gao.
1997. Quick simulation: A review of importance
sampling techniques in communications systems.
IEEE Journal on Selected Areas in Communica-
tions, 15(4):597?613.
T.M.F. Smith. 1991. Post-stratification. The Statisti-
cian, 40:315?323.
972
Anders S?gaard and Martin Haulrich. 2011.
Sentence-level instance-weighting for graph-based
and transition-based dependency parsing. In IWPT.
Masashi Sugiyama, Shinichi Nakajima, Hisashi
Kashima, Paul von B?unau, and Motoaki Kawanabe.
2007. Direct importance estimation with model se-
lection and its application to covariate shift adapta-
tion. In NIPS.
Oscar T?ackstr?om, Dipanjan Das, Slav Petrov, Ryan
McDonald, and Joakim Nivre. 2013. Token and
type constraints for cross-lingual part-of-speech tag-
ging. TACL, 1:1?12.
Bianca Zadrozny. 2004. Learning and evaluating clas-
sifiers under sample selection bias. In ICML.
973
Proceedings of NAACL-HLT 2013, pages 617?626,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Down-stream effects of tree-to-dependency conversions
Jakob Elming, Anders Johannsen, Sigrid Klerke, Emanuele Lapponi?,
Hector Martinez, Anders S?gaard
Center for Language Technology, University of Copenhagen
?Institute for Informatics, University of Oslo
Abstract
Dependency analysis relies on morphosyntac-
tic evidence, as well as semantic evidence.
In some cases, however, morphosyntactic ev-
idence seems to be in conflict with seman-
tic evidence. For this reason dependency
grammar theories, annotation guidelines and
tree-to-dependency conversion schemes often
differ in how they analyze various syntactic
constructions. Most experiments for which
constituent-based treebanks such as the Penn
Treebank are converted into dependency tree-
banks rely blindly on one of four-five widely
used tree-to-dependency conversion schemes.
This paper evaluates the down-stream effect of
choice of conversion scheme, showing that it
has dramatic impact on end results.
1 Introduction
Annotation guidelines used in modern depen-
dency treebanks and tree-to-dependency conversion
schemes for converting constituent-based treebanks
into dependency treebanks are typically based on
a specific dependency grammar theory, such as the
Prague School?s Functional Generative Description,
Meaning-Text Theory, or Hudson?s Word Grammar.
In practice most parsers constrain dependency struc-
tures to be tree-like structures such that each word
has a single syntactic head, limiting diversity be-
tween annotation a bit; but while many dependency
treebanks taking this format agree on how to an-
alyze many syntactic constructions, there are still
many constructions these treebanks analyze differ-
ently. See Figure 1 for a standard overview of clear
and more difficult cases.
The difficult cases in Figure 1 are difficult for
the following reason. In the easy cases morphosyn-
tactic and semantic evidence cohere. Verbs gov-
ern subjects morpho-syntactically and seem seman-
tically more important. In the difficult cases, how-
ever, morpho-syntactic evidence is in conflict with
the semantic evidence. While auxiliary verbs have
the same distribution as finite verbs in head position
and share morpho-syntactic properties with them,
and govern the infinite main verbs, main verbs seem
semantically superior, expressing the main predi-
cate. There may be distributional evidence that com-
plementizers head verbs syntactically, but the verbs
seem more important from a semantic point of view.
Tree-to-dependency conversion schemes used
to convert constituent-based treebanks into
dependency-based ones also take different stands on
the difficult cases. In this paper we consider four dif-
ferent conversion schemes: the Yamada-Matsumoto
conversion scheme yamada,1 the CoNLL 2007
format conll07,2 the conversion scheme ewt used in
the English Web Treebank (Petrov and McDonald,
2012),3 and the lth conversion scheme (Johansson
1The Yamada-Matsumoto scheme can be
replicated by running penn2malt.jar available at
http://w3.msi.vxu.se/?nivre/research/Penn2Malt.html. We
used Malt dependency labels (see website). The Yamada-
Matsumoto scheme is an elaboration of the Collins scheme
(Collins, 1999), which is not included in our experiments.
2The CoNLL 2007 conversion scheme can be
obtained by running pennconverter.jar available at
http://nlp.cs.lth.se/software/treebank converter/with the
?conll07? flag set.
3The EWT conversion scheme can be repli-
cated using the Stanford converter available at
http://nlp.stanford.edu/software/stanford-dependencies.shtml
617
Clear cases Difficult cases
Head Dependent ? ?
Verb Subject Auxiliary Main verb
Verb Object Complementizer Verb
Noun Attribute Coordinator Conjuncts
Verb Adverbial Preposition Nominal
Punctuation
Figure 1: Clear and difficult cases in dependency annotation.
and Nugues, 2007).4 We list the differences in
Figure 2. An example of differences in analysis is
presented in Figure 3.
In order to access the impact of these conversion
schemes on down-stream performance, we need ex-
trinsic rather than intrinsic evaluation. In general
it is important to remember that while researchers
developing learning algorithms for part-of-speech
(POS) tagging and dependency parsing seem ob-
sessed with accuracies, POS sequences or depen-
dency structures have no interest on their own. The
accuracies reported in the literature are only inter-
esting insofar they correlate with the usefulness of
the structures predicted by our systems. Fortunately,
POS sequences and dependency structures are use-
ful in many applications. When we consider tree-to-
dependency conversion schemes, down-stream eval-
uation becomes particularly important since some
schemes are more fine-grained than others, leading
to lower performance as measured by intrinsic eval-
uation metrics.
Approach in this work
In our experiments below we apply a state-of-the-art
parser to five different natural language processing
(NLP) tasks where syntactic features are known to
be effective: negation resolution, semantic role la-
beling (SRL), statistical machine translation (SMT),
sentence compression and perspective classification.
In all five tasks we use the four tree-to-dependency
conversion schemes mentioned above and evaluate
them in terms of down-stream performance. We also
compare our systems to baseline systems not rely-
4The LTH conversion scheme can be ob-
tained by running pennconverter.jar available at
http://nlp.cs.lth.se/software/treebank converter/ with the
?oldLTH? flag set.
ing on syntactic features, when possible, and to re-
sults in the literature, when comparable results exist.
Note that negation resolution and SRL are not end
applications. It is not easy to generalize across five
very different tasks, but the tasks will serve to show
that the choice of conversion scheme has significant
impact on down-stream performance.
We used the most recent release of the Mate parser
first described in Bohnet (2010),5 trained on Sec-
tions 2?21 of the Wall Street Journal section of the
English Treebank (Marcus et al, 1993). The graph-
based parser is similar to, except much faster, and
performs slightly better than the MSTParser (Mc-
Donald et al, 2005), which is known to perform
well on long-distance dependencies often important
for down-stream applications (McDonald and Nivre,
2007; Galley and Manning, 2009; Bender et al,
2011). This choice may of course have an effect on
what conversion schemes seem superior (Johansson
and Nugues, 2007). Sentence splitting was done us-
ing splitta,6, and the sentences were then tokenized
using PTB-style tokenization7 and tagged using the
in-built Mate POS tagger.
Previous work
There has been considerable work on down-stream
evaluation of syntactic parsers in the literature, but
most previous work has focused on evaluating pars-
ing models rather than linguistic theories. No one
has, to the best of our knowledge, compared the
impact of choice of tree-to-dependency conversion
scheme across several NLP tasks.
Johansson and Nugues (2007) compare the im-
pact of yamada and lth on semantic role labeling
5http://code.google.com/p/mate-tools/
6http://code.google.com/p/splitta/
7http://www.cis.upenn.edu/?treebank/tokenizer.sed
618
FORM1 FORM2 yamada conll07 ewt lth
Auxiliary Main verb 1 1 2 2
Complementizer Verb 1 2 2 2
Coordinator Conjuncts 2 1 2 2
Preposition Nominal 1 1 1 2
Figure 2: Head decisions in conversions. Note: yamada also differ from CoNLL 2007 in proper names.
Figure 3: CoNLL 2007 (blue) and LTH (red) dependency conversions.
performance, showing that lth leads to superior per-
formance.
Miyao et al (2008) measure the impact of syntac-
tic parsers in an information extraction system iden-
tifying protein-protein interactions in biomedical re-
search articles. They evaluate dependency parsers,
constituent-based parsers and deep parsers.
Miwa et al (2010) evaluate down-stream per-
formance of linguistic representations and parsing
models in biomedical event extraction, but do not
evaluate linguistic representations directly, evaluat-
ing representations and models jointly.
Bender et al (2011) compare several parsers
across linguistic representations on a carefully de-
signed evaluation set of hard, but relatively frequent
syntactic constructions. They compare dependency
parsers, constituent-based parsers and deep parsers.
The authors argue in favor of evaluating parsers on
diverse and richly annotated data. Others have dis-
cussed various ways of evaluating across annotation
guidelines or translating structures to a common for-
mat (Schwartz et al, 2011; Tsarfaty et al, 2012).
Hall et al (2011) discuss optimizing parsers for
specific down-stream applications, but consider only
a single annotation scheme.
Yuret et al (2012) present an overview of the
SemEval-2010 Evaluation Exercises on Semantic
Evaluation track on recognition textual entailment
using dependency parsing. They also compare sev-
eral parsers using the heuristics of the winning sys-
tem for inference. While the shared task is an
example of down-stream evaluation of dependency
parsers, the evaluation examples only cover a subset
of the textual entailments relevant for practical ap-
plications, and the heuristics used in the experiments
assume a fixed set of dependency labels (ewt labels).
Finally, Schwartz et al (2012) compare the
above conversion schemes and several combinations
thereof in terms of learnability. This is very different
from what is done here. While learnability may be
a theoretically motivated parameter, our results indi-
cate that learnability and downstream performance
do not correlate well.
2 Applications
Dependency parsing has proven useful for a wide
range of NLP applications, including statistical ma-
chine translation (Galley and Manning, 2009; Xu et
al., 2009; Elming and Haulrich, 2011) and sentiment
analysis (Joshi and Penstein-Rose, 2009; Johansson
and Moschitti, 2010). This section describes the ap-
plications and experimental set-ups included in this
study.
In the five applications considered below we
619
use syntactic features in slightly different ways.
While our statistical machine translation and sen-
tence compression systems use dependency rela-
tions as additional information about words and on
a par with POS, our negation resolution system uses
dependency paths, conditioning decisions on both
dependency arcs and labels. In perspective classifi-
cation, we use dependency triples (e.g. SUBJ(John,
snore)) as features, while the semantic role labeling
system conditions on a lot of information, including
the word form of the head, the dependent and the ar-
gument candidates, the concatenation of the depen-
dency labels of the predicate, and the labeled depen-
dency relations between predicate and its head, its
arguments, dependents or siblings.
2.1 Negation resolution
Negation resolution (NR) is the task of finding nega-
tion cues, e.g. the word not, and determining their
scope, i.e. the tokens they affect. NR has recently
seen considerable interest in the NLP community
(Morante and Sporleder, 2012; Velldal et al, 2012)
and was the topic of the 2012 *SEM shared task
(Morante and Blanco, 2012).
The data set used in this work, the Conan Doyle
corpus (CD),8 was released in conjunction with the
*SEM shared task. The annotations in CD extend
on cues and scopes by introducing annotations for
in-scope events that are negated in factual contexts.
The following is an example from the corpus show-
ing the annotations for cues (bold), scopes (under-
lined) and negated events (italicized):
(1) Since we have been so
unfortunate as to miss him [. . . ]
CD-style scopes can be discontinuous and overlap-
ping. Events are a portion of the scope that is se-
mantically negated, with its truth value reversed by
the negation cue.
The NR system used in this work (Lapponi et al,
2012), one of the best performing systems in the
*SEM shared task, is a CRF model for scope resolu-
tion that relies heavily on features extracted from de-
pendency graphs. The feature model contains token
distance, direction, n-grams of word forms, lemmas,
POS and combinations thereof, as well as the syntac-
tic features presented in Figure 4. The results in our
8http://www.clips.ua.ac.be/sem2012-st-neg/data.html
Syntactic
constituent
dependency relation
parent head POS
grand parent head POS
word form+dependency relation
POS+dependency relation
Cue-dependent
directed dependency distance
bidirectional dependency distance
dependency path
lexicalized dependency path
Figure 4: Features used to train the conditional random
field models
experiments are obtained from configurations that
differ only in terms of tree-to-dependency conver-
sions, and are trained on the training set and tested
on the development set of CD. Since the negation
cue classification component of the system does not
rely on dependency features at all, the models are
tested using gold cues.
Table 1 shows F1 scores for scopes, events and
full negations, where a true positive correctly as-
signs both scope tokens and events to the rightful
cue. The scores are produced using the evaluation
script provided by the *SEM organizers.
2.2 Semantic role labeling
Semantic role labeling (SRL) is the attempt to de-
termine semantic predicates in running text and la-
bel their arguments with semantic roles. In our
experiments we have reproduced the second best-
performing system in the CoNLL 2008 shared task
in syntactic and semantic parsing (Johansson and
Nugues, 2008).9
The English training data for the CoNLL 2008
shared task were obtained from PropBank and
NomBank. For licensing reasons, we used
OntoNotes 4.0, which includes PropBank, but not
NomBank. This means that our system is only
trained to classify verbal predicates. We used
the Clearparser conversion tool10 to convert the
OntoNotes 4.0 and subsequently supplied syntac-
tic dependency trees using our different conversion
schemes. We rely on gold standard argument identi-
fication and focus solely on the performance metric
semantic labeled F1.
9http://nlp.cs.lth.se/software/semantic parsing: propbank
nombank frames
10http://code.google.com/p/clearparser/
620
2.3 Statistical machine translation
The effect of the different conversion schemes was
also evaluated on SMT. We used the reordering
by parsing framework described by Elming and
Haulrich (2011). This approach integrates a syn-
tactically informed reordering model into a phrase-
based SMT system. The model learns to predict the
word order of the translation based on source sen-
tence information such as syntactic dependency re-
lations. Syntax-informed SMT is known to be use-
ful for translating between languages with different
word orders (Galley and Manning, 2009; Xu et al,
2009), e.g. English and German.
The baseline SMT system is created as described
in the guidelines from the original shared task.11
Only modifications are that we use truecasing in-
stead of lowercasing and recasing, and allow train-
ing sentences of up to 80 words. We used data
from the English-German restricted task: ?3M par-
allel words of news, ?46M parallel words of Eu-
roparl, and ?309M words of monolingual Europarl
and news. We use newstest2008 for tuning, new-
stest2009 for development, and newstest2010 for
testing. Distortion limit was set to 10, which is
also where the baseline system performed best. The
phrase table and the lexical reordering model is
trained on the union of all parallel data with a max
phrase length of 7, and the 5-gram language model
is trained on the entire monolingual data set.
We test four different experimental systems that
only differ with the baseline in the addition of a syn-
tactically informed reordering model. The baseline
system was one of the tied best performing system
in the WMT 2011 shared task on this dataset. The
four experimental systems have reordering models
that are trained on the first 25,000 sentences of the
parallel news data that have been parsed with each
of the tree-to-dependency conversion schemes. The
reordering models condition reordering on the word
forms, POS, and syntactic dependency relations of
the words to be reordered, as described in Elming
and Haulrich (2011). The paper shows that while
reordering by parsing leads to significant improve-
ments in standard metrics such as BLEU (Papineni
et al, 2002) and METEOR (Lavie and Agarwal,
2007), improvements are more spelled out with hu-
11 http://www.statmt.org/wmt11/translation-task.html
man judgements. All SMT results reported below
are averages based on 5 MERT runs following Clark
et al (2011).
2.4 Sentence compression
Sentence compression is a restricted form of sen-
tence simplification with numerous usages, includ-
ing text simplification, summarization and recogniz-
ing textual entailment. The most commonly used
dataset in the literature is the Ziff-Davis corpus.12 A
widely used baseline for sentence compression ex-
periments is Knight and Marcu (2002), who intro-
duce two models: the noisy-channel model and a de-
cision tree-based model. Both are tree-based meth-
ods that find the most likely compressed syntactic
tree and outputs the yield of this tree. McDonald et
al. (2006) instead use syntactic features to directly
find the most likely compressed sentence.
Here we learn a discriminative HMM model
(Collins, 2002) of sentence compression using
MIRA (Crammer and Singer, 2003), comparable to
previously explored models of noun phrase chunk-
ing. Our model is thus neither tree-based nor
sentence-based. Instead we think of sentence com-
pression as a sequence labeling problem. We com-
pare a model informed by word forms and predicted
POS with models also informed by predicted depen-
dency labels. The baseline feature model conditions
emission probabilities on word forms and POS us-
ing a ?2 window and combinations thereoff. The
augmented syntactic feature model simply adds de-
pendency labels within the same window.
2.5 Perspective classification
Finally, we include a document classification dataset
from Lin and Hauptmann (2006).13 The dataset con-
sists of blog posts posted at bitterlemons.org by Is-
raelis and Palestinians. The bitterlemons.org web-
site is set up to ?contribute to mutual understanding
through the open exchange of ideas.? In the dataset,
each blog post is labeled as either Israeli or Pales-
tinian. Our baseline model is just a standard bag-
of-words model, and the system adds dependency
triplets to the bag-of-words model in a way similar
to Joshi and Penstein-Rose (2009). We do not re-
move stop words, since perspective classification is
12LDC Catalog No.: LDC93T3A.
13https://sites.google.com/site/weihaolinatcmu/data
621
bl yamada conll07 ewt lth
DEPRELS - 12 21 47 41
PTB-23 (LAS) - 88.99 88.52 81.36? 87.52
PTB-23 (UAS) - 90.21 90.12 84.22? 90.29
Neg: scope F1 - 81.27 80.43 78.70 79.57
Neg: event F1 - 76.19 72.90 73.15 76.24
Neg: full negation F1 - 67.94 63.24 61.60 64.31
SentComp F1 68.47 72.07 64.29 71.56 71.56
SMT-dev-Meteor 35.80 36.06 36.06 36.16 36.08
SMT-test-Meteor 37.25 37.48 37.50 37.58 37.51
SMT-dev-BLEU 13.66 14.14 14.09 14.04 14.06
SMT-test-BLEU 14.67 15.04 15.04 14.96 15.11
SRL-22-gold - 81.35 83.22 84.72 84.01
SRL-23-gold - 79.09 80.85 80.39 82.01
SRL-22-pred - 74.41 76.22 78.29 66.32
SRL-23-pred - 73.42 74.34 75.80 64.06
bitterlemons.org 96.08 97.06 95.58 96.08 96.57
Table 1: Results. ?: Low parsing results on PTB-23 using ewt are explained by changes between the PTB-III and the
Ontonotes 4.0 release of the English Treebank.
similar to authorship attribution, where stop words
are known to be informative. We evaluate perfor-
mance doing cross-validation over the official train-
ing data, setting the parameters of our learning algo-
rithm for each fold doing cross-validation over the
actual training data. We used soft-margin support
vector machine learning (Cortes and Vapnik, 1995),
tuning the kernel (linear or polynomial with degree
3) and C = {0.1, 1, 5, 10}.
3 Results and discussion
Our results are presented in Table 1. The parsing
results are obtained relying on predicted POS rather
than, as often done in the dependency parsing liter-
ature, relying on gold-standard POS. Note that they
comply with the result in Schwartz et al (2012) that
Yamada-Matsumoto-style annotation is more easily
learnable.
The negation resolution results are significantly
better using syntactic features in yamada annota-
tion. It is not surprising that a syntactically ori-
ented conversion scheme performs well in this task.
Since Lapponi et al (2012) used Maltparser (Nivre
et al, 2007) with the freely available pre-trained
parsing model for English,14 we decided to also
run that parser with the gold-standard cues, in ad-
14http://www.maltparser.org/mco/english parser/engmalt.html
dition to Mate. The pre-trained model was trained
on Sections 2?21 of the Wall Street Journal sec-
tion of the English Treebank (Marcus et al, 1993),
augmented with 4000 sentences from the Question-
Bank,15 which was converted using the Stanford
converter and thus similar to the ewt annotations
used here. The results were better than using ewt
with Mate trained on Sections 2?21 alone, but worse
than the results obtained here with yamada conver-
sion scheme. F1 score on full negation was 66.92%.
The case-sensitive BLEU evaluation of the
SMT systems indicates that choice of conversion
scheme has no significant impact on overall perfor-
mance. The difference to the baseline system is
significant (p < 0.01), showing that the reorder-
ing model leads to improvement using any of the
schemes. However, the conversion schemes lead to
very different translations. This can be seen, for
example, by the fact that the relative tree edit dis-
tance between translations of different syntactically
informed SMT systems is 12% higher than within
each system (across different MERT optimizations).
The reordering approach puts a lot of weight on
the syntactic dependency relations. As a conse-
quence, the number of relation types used in the
conversion schemes proves important. Consider the
15http://www.computing.dcu.ie/?jjudge/qtreebank/
622
REFERENCE: Zum Glu?ck kam ich beim Strassenbahnfahren an die richtige Stelle .
SOURCE: Luckily , on the way to the tram , I found the right place .
yamada: Glu?cklicherweise hat auf dem Weg zur S-Bahn , stellte ich fest , dass der richtige Ort .
conll07: Glu?cklicherweise hat auf dem Weg zur S-Bahn , stellte ich fest , dass der richtige Ort .
ewt: Zum Glu?ck fand ich auf dem Weg zur S-Bahn , am richtigen Platz .
lth: Zum Glu?ck fand ich auf dem Weg zur S-Bahn , am richtigen Platz .
BASELINE: Zum Glu?ck hat auf dem Weg zur S-Bahn , ich fand den richtigen Platz .
Figure 5: Examples of SMT output.
ORIGINAL: * 68000 sweden ab of uppsala , sweden , introduced the teleserve , an integrated answering
machine and voice-message handler that links a macintosh to touch-tone phones .
BASELINE: 68000 sweden ab introduced the teleserve an integrated answering
machine and voice-message handler .
yamada 68000 sweden ab introduced the teleserve integrated answering
machine and voice-message handler .
conll07 68000 sweden ab sweden introduced the teleserve integrated answering
machine and voice-message handler .
ewt 68000 sweden ab introduced the teleserve integrated answering
machine and voice-message handler .
lth 68000 sweden ab introduced the teleserve an integrated answering
machine and voice-message handler .
HUMAN: 68000 sweden ab introduced the teleserve integrated answering
machine and voice-message handler .
Figure 6: Examples of sentence compression output.
example in Figure 5. German requires the verb in
second position, which is obeyed in the much bet-
ter translations produced by the ewt and lth sys-
tems. Interestingly, the four schemes produce virtu-
ally identical structures for the source sentence, but
they differ in their labeling. Where conll07 and ya-
mada use the same relation for the first two con-
stituents (ADV and vMOD, respectively), ewt and
lth distinguish between them (ADVMOD/PREP and
ADV/LOC). This distinction may be what enables
the better translation, since the model may learn to
move the verb after the sentence adverbial. In the
other schemes, sentence adverbials are not distin-
guished from locational adverbials. Generally, ewt
and lth have more than twice as many relation types
as the other schemes.
The schemes ewt and lth lead to better SRL
performance than conll07 and yamada when re-
lying on gold-standard syntactic dependency trees.
This supports the claims put forward in Johansson
and Nugues (2007). These annotations also hap-
pen to use a larger set of dependency labels, how-
ever, and syntactic structures may be harder to re-
construct, as reflected by labeled attachment scores
(LAS) in syntactic parsing. The biggest drop in
SRL performance going from gold-standard to pre-
dicted syntactic trees is clearly for the lth scheme,
at an average 17.8% absolute loss (yamada 5.8%;
conll07 6.8%; ewt 5.5%; lth 17.8%).
The ewt scheme resembles lth in most respects,
but in preposition-noun dependencies it marks the
preposition as the head rather than the noun. This
is an important difference for SRL, because seman-
tic arguments are often nouns embedded in preposi-
tional phrases, like agents in passive constructions.
It may also be that the difference in performance is
simply explained by the syntactic analysis of prepo-
sitional phrases being easier to reconstruct.
The sentence compression results are generally
much better than the models proposed in Knight and
Marcu (2002). Their noisy channel model obtains
an F1 compression score of 14.58%, whereas the
decision tree-based model obtains an F1 compres-
sion score of 31.71%. While F1 scores should be
complemented by human judgements, as there are
typically many good sentence compressions of any
source sentence, we believe that error reductions of
more than 50% indicate that the models used here
623
ADV AMOD CC COORDDEP EXP GAP IOBJ LGS NMOD OBJ P PMOD PRD PRN PRT ROOT SBJ VC VMOD
0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35
0.40
L
a
b
e
l
s
srl
neg
Figure 7: Distributions of dependency labels in the
Yamada-Matsumoto scheme
(though previously unexplored in the literature) are
fully competitive with state-of-the-art models.
We also see that the models using syntactic fea-
tures perform better than our baseline model, except
for the model using conll07 dependency annotation.
This may be surprising to some, since distributional
information is often considered important in sen-
tence compression (Knight and Marcu, 2002). Some
output examples are presented in Figure 6. Un-
surprisingly, it is seen that the baseline model pro-
duces grammatically incorrect output, and that most
of our syntactic models correct the error leading to
ungrammaticality. The model using ewt annotation
is an exception. We also see that conll07 introduces
another error. We believe that this is due to the way
the conll07 tree-to-dependency conversion scheme
handles coordination. While the word Sweden is not
coordinated, it occurs in a context, surrounded by
commas, that is very similar to coordinated items.
In perspective classification we see that syntactic
features based on yamada and lth annotations lead
to improvements, with yamada leading to slightly
better results than lth. The fact that a syntactically
oriented conversion scheme leads to the best results
may reflect that perspective classification, like au-
thorship attribution, is less about content than stylis-
tics.
While lth seems to lead to the overall best re-
sults, we stress the fact that the five tasks considered
here are incommensurable. What is more interest-
ing is that, task to task, results are so different. The
semantically oriented conversion schemes, ewt and
lth, lead to the best results in SRL, but with a signif-
icant drop for lth when relying on predicted parses,
while the yamada scheme is competitive in the other
four tasks. This may be because distributional infor-
mation is more important in these tasks than in SRL.
The distribution of dependency labels seems rel-
atively stable across applications, but differences in
data may of course also affect the usefulness of dif-
ferent annotations. Note that conll07 leads to very
good results for negation resolution, but bad results
for SRL. See Figure 7 for the distribution of labels
in the conll07 conversion scheme on the SRL and
negation scope resolution data. Many differences
relate to differences in sentence length. The nega-
tion resolution data is literary text with shorter sen-
tences, which therefore uses more punctuation and
has more root dependencies than newspaper articles.
On the other hand we do see very few predicate de-
pendencies in the SRL data. This may affect down-
stream results when classifying verbal predicates in
SRL. We also note that the number of dependency
labels have less impact on results in general than we
would have expected. The number of dependency
labels and the lack of support for some of them may
explain the drop with predicted syntactic parses in
our SRL results, but generally we obtain our best re-
sults with yamada and lth annotations, which have
12 and 41 dependency labels, respectively.
4 Conclusions
We evaluated four different tree-to-dependency con-
version schemes, putting more or less emphasis on
syntactic or semantic evidence, in five down-stream
applications, including SMT and negation resolu-
tion. Our results show why it is important to be
precise about exactly what tree-to-dependency con-
version scheme is used. Tools like pennconverter.jar
gives us a wide range of options when converting
constituent-based treebanks, and even small differ-
ences may have significant impact on down-stream
performance. The small differences are also impor-
tant for more linguistic comparisons that also tend to
gloss over exactly what conversion scheme is used,
e.g. Ivanova et al (2012).
Acknowledgements
Hector Martinez is funded by the ERC grant
CLARA No. 238405, and Anders S?gaard is
funded by the ERC Starting Grant LOWLANDS
No. 313695.
624
References
Emily Bender, Dan Flickinger, Stephan Oepen, and
Yi Zhang. 2011. Parser evaluation over local and non-
local dependencies in a large corpus. In EMNLP.
Bernd Bohnet. 2010. Top accuracy and fast dependency
parsing is not a contradiction. In COLING.
Jonathan H. Clark, Chris Dyer, Alon Lavie, and Noah A.
Smith. 2011. Better hypothesis testing for statistical
machine translation: controlling for optimizer instabil-
ity. In ACL.
Mike Collins. 1999. Head-driven statistical models for
natural language parsing. Ph.D. thesis, University of
Pennsylvania.
Michael Collins. 2002. Discriminative training methods
for Hidden Markov Models. In EMNLP.
Corinna Cortes and Vladimir Vapnik. 1995. Support-
vector networks. Machine Learning, 20(3):273?297.
Koby Crammer and Yoram Singer. 2003. Ultraconserva-
tive algorithms for multiclass problems. In JMLR.
Jakob Elming and Martin Haulrich. 2011. Reordering
by parsing. In Proceedings of International Workshop
on Using Linguistic Information for Hybrid Machine
Translation (LIHMT-2011).
Michel Galley and Christopher Manning. 2009.
Quadratic-time dependency parsing for machine trans-
lation. In ACL.
Keith Hall, Ryan McDonald, Jason Katz-Brown, and
Michael Ringgaard. 2011. Training dependency
parsers by jointly optimizing multiple objectives. In
EMNLP.
Angelina Ivanova, Stephan Oepen, Lilja ?vrelid, and
Dan Flickinger. 2012. Who did what to whom? a con-
trastive study of syntactico-semantic dependencies. In
LAW.
Richard Johansson and Alessandro Moschitti. 2010.
Syntactic and semantic structure for opinion expres-
sion detection. In CoNLL.
Richard Johansson and Pierre Nugues. 2007. Extended
constituent-to-dependency conversion for English. In
NODALIDA.
Richard Johansson and Pierre Nugues. 2008.
Dependency-based syntactic-semantic analysis
with propbank and nombank. In CoNLL.
Mahesh Joshi and Carolyn Penstein-Rose. 2009. Gen-
eralizing dependency features for opinion mining. In
ACL.
Kevin Knight and Daniel Marcu. 2002. Summariza-
tion beyond sentence extraction: a probabilistic ap-
proach to sentence compression. Artificial Intelli-
gence, 139:91?107.
Emanuele Lapponi, Erik Velldal, Lilja ?vrelid, and
Jonathon Read. 2012. UiO2: Sequence-labeling nega-
tion using dependency features. In *SEM.
Alon Lavie and Abhaya Agarwal. 2007. Meteor: an au-
tomatic metric for mt evaluation with high levels of
correlation with human judgments. In WMT.
Wei-Hao Lin and Alexander Hauptmann. 2006. Are
these documents written from different perspectives?
In COLING-ACL.
Mitchell Marcus, Mary Marcinkiewicz, and Beatrice
Santorini. 1993. Building a large annotated corpus
of English: the Penn Treebank. Computational Lin-
guistics, 19(2):313?330.
Ryan McDonald and Joakim Nivre. 2007. Characteriz-
ing the errors of data-driven dependency parsers. In
EMNLP-CoNLL.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic?. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceedings of
the Conference on Human Language Technology and
Empirical Methods in Natural Language Processing
2005, pages 523?530, Vancouver, British Columbia.
Ryan McDonald. 2006. Discriminative sentence com-
pression with soft syntactic evidence. In EACL.
Makoto Miwa, Sampo Pyysalo, Tadayoshi Hara, and
Jun?ichi Tsujii. 2010. Evaluating dependency repre-
sentation for event extraction. In COLING.
Yusuke Miyao, Rune S? tre, Kenji Sagae, Takuya Mat-
suzaki, and Jun?ichi Tsujii. 2008. Task-oriented eval-
uation of syntactic parsers and their representations. In
ACL.
Roser Morante and Eduardo Blanco. 2012. *sem 2012
shared task: Resolving the scope and focus of nega-
tion. In *SEM.
Roser Morante and Caroline Sporleder. 2012. Modal-
ity and negation: An introduction to the special issue.
Computational linguistics, 38(2):223?260.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev,
Gu?lsen Eryigit, Sandra Ku?bler, Svetoslav Marinov,
and Erwin Marsi. 2007. MaltParser: a language-
independent system for data-driven dependency pars-
ing. Natural Language Engineering, 13(2):95?135.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalua-
tion of machine translation. In ACL.
Slav Petrov and Ryan McDonald. 2012. Overview of
the 2012 Shared Task on Parsing the Web. In Notes
of the First Workshop on Syntactic Analysis of Non-
Canonical Language (SANCL).
Roy Schwartz, and Omri Abend, Roi Reichart, and Ari
Rappoport. 2011. Neutralizing linguistically prob-
lematic annotations in unsupervised dependency pars-
ing evaluation. In ACL.
Roy Schwartz, Omri Abend, and Ari Rappoport. 2012.
Learnability-based syntactic annotation design. In
COLING.
625
Reut Tsarfaty, Joakim Nivre, and Evelina Andersson.
2012. Cross-framework evaluation for statistical pars-
ing. In EACL.
Erik Velldal, Lilja ?vrelid, Jonathon Read, and Stephan
Oepen. 2012. Speculation and negation: Rules,
rankers, and the role of synta. Computational linguis-
tics, 38(2):369?410.
Peng Xu, Jaeho Kang, Michael Ringgaard, and Franz
Och. 2009. Using a dependency parser to improve
SMT for subject-object-verb languages. In NAACL-
HLT, Boulder, Colorado.
Deniz Yuret, Laura Rimell, and Aydin Han. 2012. Parser
evaluation using textual entailments. Language Re-
sources and Evaluation, Published online 31 October
2012.
626
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 408?412,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
EMNLP@CPH: Is frequency all there is to simplicity?
Anders Johannsen, H?ctor Mart?nez, Sigrid Klerke?, Anders S?gaard
Centre for Language Technology
University of Copenhagen
{ajohannsen|alonso|soegaard}@hum.ku.dk
sigridklerke@gmail.com?
Abstract
Our system breaks down the problem of rank-
ing a list of lexical substitutions according to
how simple they are in a given context into a
series of pairwise comparisons between can-
didates. For this we learn a binary classifier.
As only very little training data is provided,
we describe a procedure for generating artifi-
cial unlabeled data from Wordnet and a corpus
and approach the classification task as a semi-
supervised machine learning problem. We use
a co-training procedure that lets each classi-
fier increase the other classifier?s training set
with selected instances from an unlabeled data
set. Our features include n-gram probabilities
of candidate and context in a web corpus, dis-
tributional differences of candidate in a cor-
pus of ?easy? sentences and a corpus of normal
sentences, syntactic complexity of documents
that are similar to the given context, candidate
length, and letter-wise recognizability of can-
didate as measured by a trigram character lan-
guage model.
1 Introduction
This paper describes a system for the SemEval 2012
English Lexical Simplification shared task. The
task description uses a loose definition of simplic-
ity, defining ?simple words? as ?words that can be
understood by a wide variety of people, including for
example people with low literacy levels or some cog-
nitive disability, children, and non-native speakers of
English? (Specia et al, 2012).
Feature r
N????sf 0.33
N????sf+1 0.27
N????sf?1 0.27
L??sf -0.26
L??max -0.26
RIproto(l) -0.18
S??cn -0.17
S??w -0.17
S??cp -0.17
Feature r
RIproto(f) -0.15
C???max -0.14
RIorig(l) -0.11
L??tokens -0.10
C???min 0.10
SWfreq 0.08
SWLLR 0.07
C???avg -0.04
Table 1: Pearson?s r correlations. The table shows
the three highest correlated features per group, all of
which are significant at the p < 0.01 level
2 Features
We model simplicity with a range of features divided
into six groups. Five of these groups make use of
the distributional hypothesis and rely on external cor-
pora. We measure a candidate?s distribution in terms
of its lexical associations (RI), participation in syn-
tactic structures (S??), or corpus presence in order to
assess its simplicity (N????, SW, C???). A single
group, L??, measures intrinsic aspects of the substi-
tution candidate, such as its length.
The substitution candidate is either an adjective,
an adverb, a noun, or a verb, and all candidates within
a list share the same part of speech. Because word
class might influence simplicity, we allow our model
to fit parameters specific to the candidate?s part of
speech by making a copy of the features for each part
of speech which is active only when the candidate is
in the given part of speech.
408
Simple Wikipedia (SW) These two features con-
tain relative frequency counts of the substitution
form in Simple English Wikipedia (SWfreq), and the
log likelihood ratio of finding the word in the simple
corpus to finding it in regular Wikipedia (SWLLR)1.
Word length (L??) This set of three features de-
scribes the length of the substitution form in char-
acters (L??sf ), the length of the longest token
(L??max), and the length of the substitution form in
tokens (L??tokens). Word length is an integral part
of common measures of text complexity, e.g in the
English Flesch?Kincaid (Kincaid et al, 1975) in the
form of syllable count, and in the Scandinavian LIX
(Bjornsson, 1983).
Character trigram model (C???) These three
features approximate the reading difficulty of a word
in terms of the probabilities of its forming character
trigrams, with special characters to mark word be-
ginning and end. A word with an unusual combi-
nation of characters takes longer to read and is per-
ceived as less simple (Ehri, 2005).
We calculate the minimum, average, and maxi-
mum trigram probability (C???min, C???avg, and
C???max).2
Web corpus N-gram (N????) These 12 features
were obtained from a pre-built web-scale language
model3. Features of the form N????sf?i, where
0 < i < 4, express the probability of seeing the
substitution form together with the following (or pre-
vious) unigram, bigram, or trigram. N????sf is
the probability of substitution form itself, a feature
which also is the backbone of our frequency base-
line.
Random Indexing (RI) These four features are
obtained from measures taken from a word-to-word
distributional semantic model. Random Indexing
(RI) was chosen for efficiency reasons (Sahlgren,
2005). We include features describing the seman-
tic distances between the candidate and the original
1Wikipedia dump obtained March 27, 2012. Date on the
Simple Wikipedia dump is March 22, 2012.
2Trigram probabilities derived from Google T1 unigram
counts.
3The ?jun09/body? trigram model from Microsoft Web N-
gram Services.
form (RIorig), and between the candidate and a proto-
type vector (RIproto). For the distance between can-
didate and original, we hypothesize that annotators
would prefer a synonym closer to the original form.
A prototype distributional vector of a set of words is
built by summing the individual word vectors, thus
obtaining a representation that approximates the be-
havior of that class overall (Turney and Pantel, 2010).
Longer distances indicate that the currently exam-
ined substitution is far from the shared meaning of
all the synonyms, making it a less likely candidate.
The features are included for both lemma and surface
forms of the words.
Syntactic complexity (S??) These 23 features
measure the syntactic complexity of documents
where the substitution candidate occurs. We used
measures from (Lu, 2010) in which they describe 14
automatic measures of syntactic complexity calcu-
lated from frequency counts of 9 types of syntactic
structures. This group of syntax-metric scores builds
on two ideas.
First, syntactic complexity and word difficulty go
together. A sentence with a complicated syntax is
more likely to be made up of difficult words, and
conversely, the probability that a word in a sentence
is simple goes up when we know that the syntax of
the sentence is uncomplicated. To model this we
search for instances of the substitution candidates in
the UKWAC corpus4 and measure the syntactic com-
plexity of the documents where they occur.
Second, the perceived simplicity of a word may
change depending on the context. Consider the ad-
jective ?frigid?, which may be judged to be sim-
pler than ?gelid? if referring to temperature, but per-
haps less simple than ?ice-cold? when characterizing
someone?s personality. These differences in word
sense are taken into account by measuring the sim-
ilarity between corpus documents and substitution
contexts and use these values to provide a weighted
average of the syntactic complexity measures.
3 Unlabeled data
The unlabeled data set was generated by a three-
step procedure involving synonyms extracted from
Wordnet5 and sentences from the UKWAC corpus.
4http://wacky.sslmit.unibo.it/
5http://wordnet.princeton.edu/
409
1) Collection: Find synsets for unambigious lem-
mas in Wordnet. The synsets must have more than
three synonyms. Search for the lemmas in the cor-
pus. Generate unlabeled instances by replacing the
lemma with each of its synonyms. 2) Sampling: In
the unlabeled corpus, reduce the number of ranking
problems per lemma to a maximum of 10. Sample
from this pool while maintaining a distribution of
part of speech similar to that of the trial and test set.
3) Filtering: Remove instances for which there are
missing values in our features.
The unlabeled part of our final data set contains
n = 1783 problems.
4 Ranking
We are given a number of ranking problems (n =
300 in the trial set and n = 1710 for the test data).
Each of these consists of a text extract with a posi-
tion marked for substitution, and a set of candidate
substitutions.
4.1 Linear order
Let X (i) be the substitution set for the i-th problem.
We can then formalize the ranking problem by as-
suming that we have access to a set of (weighted)
preference judgments, w(a ? b) for all a, b ? X (i)
such that w(a ? b) is the value of ranking item a
ahead of b. The values are the confidence-weighted
pair-wise decisions from our binary classifier. Our
goal is then to establish a total order on X (i) that
maximizes the value of the non-violated judgments.
This is an instance of the Linear Ordering Problem
(Mart? and Reinelt, 2011), which is known to be NP-
hard. However, with problems of our size (maximum
ten items in each ranking), we escape these complex-
ity issues by a very narrow margin?10! ? 3.6 mil-
lion means that the number of possible orderings is
small enough to make it feasible to find the optimal
one by exhaustive enumeration of all possibilities.
4.2 Binary classication
In order to turn our ranking problem into binary clas-
sification, we generate a new data set by enumerat-
ing all point-wise comparisons within a problem and
for each apply a transformation function ?(a,b) =
a ? b. Thus each data point in the new set is the
difference between the feature values of two candi-
dates. This enables us to learn a binary classifier for
the relation ?ranks ahead of?.
We use the trial set for labeled training data L and,
in a transductive manner, treat the test set as unla-
beled data Utest. Further, we supplement the pool of
unlabeled data with artificially generated instances
Ugen, such that U = Utest ? Ugen.
Using a co-training setup (Blum and Mitchell,
1998), we divide our features in two independent sets
and train a large margin classifier6 on each split. The
classifiers then provide labels for data in the unla-
beled set, adding the k most confidently labeled in-
stances to the training data for the other classifier, an
iterative process which continues until there is no un-
labeled data left. At the end of the training we have
two classifiers. The classification result is a mixture-
of-experts: the most confident prediction of the two
classifiers. Furthermore, as an upper-bound of the
co-training procedure, we define an oracle that re-
turns the correct answer whenever it is given by at
least one classifier.
4.3 Ties
In many cases we have items a and b that tie?in
which case both a ? b and b ? a are violated. We
deal with these instances by omitting them from the
training set and setting w(a ? b) = 0. For the fi-
nal ranking, our system makes no attempt to produce
ties.
5 Experiments
In our experiments we vary feature-split, size of un-
labeled data, and number of iterations. The first fea-
ture split, S???SW, pooled all syntactic complexity
features and Wikipedia-based features in one view,
with the remaining feature groups in another view.
Our second feature split, S???C????L??, combined
the syntactic complexity features with the character
trigram language model features and the basic word
length features. Both splits produced a pair of classi-
fiers with similar performance?each had an F-score
of around .73 and an oracle score of .87 on the trial
set on the binary decision problem, and both splits
performed equally on the ranking task.
6Liblinear with L1 penalty and L2 loss. Parameter settings
were default. http://www.csie.ntu.edu.tw/?cjlin/liblinear/
410
System All N V R A
M????????F??? 0.449 0.367 0.456 0.487 0.493
S???SWf 0.377 0.283 0.269 0.271 0.421
S???SWl 0.425 0.355 0.497 0.408 0.425
S???C????L??f 0.377 0.284 0.469 0.270 0.421
S???C????L??l 0.435 0.362 0.481 0.465 0.439
Table 2: Performance on part of speech. Unlabeled
set was Utest. Subscripts tell whether the scores are
from the first or last iteration
With a large unlabeled data set available, the clas-
sifiers can avoid picking and labeling data points
with a low certainty, at least initially. The assump-
tion is that this will give us a higher quality training
set. However, as can be seen in Figure 1, none of our
systems are benefitting from the additional data. In
fact, the systems learn more when the pool of unla-
beled data is restricted to the test set.
Our submitted systems, O??1 and O??2 scored
0.405 and 0.393 on the test set, and 0.494 and 0.500
on the trial set. Following submission we adjusted
a parameter7 and re-ran each split with both U and
Utest.
We analyzed the performance by part of speech
and compared them to the frequency baseline as
shown in Table 2. For the frequency baseline, per-
formance is better on adverbs and adjectives alone,
and somewhat worse on nouns. Both our sys-
tems benefit from co-training on all word classes.
S???C????L??, our best performing system, no-
tably has a score reduction (compared to the base-
line) of only 5% on adverbs, eliminates the score re-
duction on nouns, and effectively beats the baseline
score on verbs with a 6% increase.
6 Discussion
The frequency baseline has proven very strong, and,
as witnessed by the correlations in Table 1, frequency
is by far the most powerful signal for ?simplicity?.
But is that all there is to simplicity? Perhaps it is.
For a person with normal reading ability, a sim-
ple word may be just a word with which the per-
son is well-acquainted?one that he has seen be-
fore enough times to have a good idea about what
it means and in which contexts it is typically used.
7In particular, we selected a larger value for the C parameter
in the liblinear classifier.
0 5000 10000 15000 20000 25000
Unlabeled datapoints
0.38
0.40
0.42
0.44
0.46
0.48
Sc
ore
SYN-SW(Utest)
SYN-CHAR-LEN(Utest)
SYN-CHAR-LEN(U)
Figure 1: Test set kappa score vs. number of data
points labeled during co-training
And so an n-gram model might be a fair approxi-
mation. However, lexical simplicity in English may
still be something very different to readers with low
literacy. For instance, the highly complex letter-to-
sound mapping rules are likely to prevent such read-
ers from arriving at the correct pronunciation of un-
seen words and thus frequent words with exceptional
spelling patterns may not seem simple at all.
A source of misclassifications discovered in our
error analysis is the fact that substituting candidates
into the given contexts in a straight-forward manner
can introduce syntactic errors. Fixing these can re-
quire significant revisions of the sentence, and yet
the substitutions resulting in an ungrammatical sen-
tence are sometimes still preferred to grammatical al-
ternatives.8 Here, scoring the substitution and the
immediate context in a language model is of little
use. Moreover, while these odd grammatical errors
may be preferable to many non-native English speak-
ers with adequate reading skills, such errors can be
more obstructing to reading impaired users and be-
ginning language learners.
Acknowledgments
This research is partially funded by the European Commission?s
7th Framework Program under grant agreement n? 238405
(CLARA).
8For example sentence 1528: ?However, it appears they in-
tend to pull out all stops to get what they want.? Gold: {try ev-
erything} {do everything it takes} {pull} {stop at nothing} {go
to any length} {yank}.
411
References
C. H. Bjornsson. 1983. Readability of Newspa-
pers in 11 Languages. Reading Research Quarterly,
18(4):480?497.
A Blum and T Mitchell. 1998. Combining labeled and
unlabeled data with co-training. In Proceedings of the
eleventh annual conference on Computational learning
theory, pages 92?100. ACM.
Linnea C. Ehri. 2005. Learning to read words: The-
ory, findings, and issues. Scientific Studies of Reading,
9(2):167?188.
J P Kincaid, R P Fishburne, R L Rogers, and B S Chissom.
1975. Derivation of New Readability Formulas (Auto-
mated Readability Index, Fog Count and Flesch Read-
ing Ease Formula) for Navy Enlisted Personnel.
Xiaofei Lu. 2010. Automatic analysis of syntactic com-
plexity in second language writing. International Jour-
nal of Corpus Linguistics, 15(4):474?496.
Rafael Mart? and Gerhard Reinelt. 2011. The Lin-
ear Ordering Problem: Exact and Heuristic Methods
in Combinatorial Optimization (Applied Mathematical
Sciences). Springer.
Magnus Sahlgren. 2005. An introduction to random
indexing. In Methods and Applications of Seman-
tic Indexing Workshop at the 7th International Con-
ference on Terminology and Knowledge Engineering,
TKE, volume 5.
Lucia Specia, Sujay K. Jauhar, and Rada Mihalcea. 2012.
SemEval-2012 Task 1: English Lexical Simplifica-
tion. In Proceedings of the 6th International Work-
shop on Semantic Evaluation (SemEval 2012), Mon-
treal, Canada.
P. D Turney and P. Pantel. 2010. From frequency to
meaning: Vector space models of semantics. Journal
of Artificial Intelligence Research, 37:141?188.
412
Proceedings of the Third Joint Conference on Lexical and Computational Semantics (*SEM 2014), pages 1?11,
Dublin, Ireland, August 23-24 2014.
More or less supervised supersense tagging of Twitter
Anders Johannsen, Dirk Hovy, H
?
ector Mart??nez Alonso, Barbara Plank, Anders S?gaard
Center for Language Technology
University of Copenhagen, Denmark
Njalsgade 140
ajohannsen@hum.ku.dk, dirk@cst.dk, alonso@hum.ku.dk
plank@cst.dk, soegaard@hum.ku.dk
Abstract
We present two Twitter datasets annotated
with coarse-grained word senses (super-
senses), as well as a series of experiments
with three learning scenarios for super-
sense tagging: weakly supervised learn-
ing, as well as unsupervised and super-
vised domain adaptation. We show that
(a) off-the-shelf tools perform poorly on
Twitter, (b) models augmented with em-
beddings learned from Twitter data per-
form much better, and (c) errors can be
reduced using type-constrained inference
with distant supervision from WordNet.
1 Introduction
Supersense tagging (SST, Ciaramita and Altun,
2006) is the task of assigning high-level ontolog-
ical classes to open-class words (here, nouns and
verbs). It is thus a coarse-grained word sense dis-
ambiguation task. The labels are based on the lexi-
cographer file names for Princeton WordNet (Fell-
baum, 1998). They include 15 senses for verbs
and 26 for nouns (see Table 1). While WordNet
also provides catch-all supersenses for adjectives
and adverbs, these are grammatically, not seman-
tically motivated, and do not provide any higher-
level abstraction (recently, however, Tsvetkov et
al. (2014) proposed a semantic taxonomy for ad-
jectives). They will not be considered in this paper.
Coarse-grained categories such as supersenses
are useful for downstream tasks such as question-
answering (QA) and open relation extraction (RE).
SST is different from NER in that it has a larger set
of labels and in the absence of strong orthographic
cues (capitalization, quotation marks, etc.). More-
over, supersenses can be applied to any of the lex-
ical parts of speech and not only proper names.
Also, while high-coverage gazetteers can be found
for named entity recognition, the lexical resources
available for SST are very limited in coverage.
Twitter is a popular micro-blogging service,
which, among other things, is used for knowledge
sharing among friends and peers. Twitter posts
(tweets) announce local events, say talks or con-
certs, present facts about pop stars or program-
ming languages, or simply express the opinions of
the author on some subject matter.
Supersense tagging is relevant for Twitter, be-
cause it can aid e.g. QA and open RE. If someone
posts a message saying that some LaTeX module
now supports ?drawing trees?, it is important to
know whether the post is about drawing natural
objects such as oaks or pines, or about drawing
tree-shaped data representations.
This paper is, to the best of our knowledge, the
first work to address the problem of SST for Twit-
ter. While there exist corpora of newswire and
literary texts that are annotated with supersenses,
e.g., SEMCOR (Miller et al., 1994), no data is
available for microblogs or related domains. This
paper introduces two new data sets.
Furthermore, most, if not all, of previous work
on SST has relied on gold standard part-of-speech
(POS) tags as input. However, in a domain such
as Twitter, which has proven to be challenging
for POS tagging (Foster et al., 2011; Ritter et
al., 2011), results obtained under the assumption
of available perfect POS information are almost
meaningless for any real-life application.
In this paper, we instead use predicted POS tags
and investigate experimental settings in which one
or more of the following resources are available to
us:
? a large corpus of unlabeled Twitter data;
? Princeton WordNet (Fellbaum, 1998);
? SEMCOR (Miller et al., 1994); and
? a small corpus of Twitter data annotated with
supersenses.
We approach SST of Twitter using various de-
grees of supervision for both learning and domain
adaptation (here, from newswire to Twitter). In
1
weakly supervised learning, only unlabeled data
and the lexical resource WordNet are available to
us. While the quality of lexical resources varies,
this is the scenario for most languages. We present
an approach to weakly supervised SST based on
type-constrained EM-trained second-order HMMs
(HMM2s) with continuous word representations.
In contrast, when using supervised learning, we
can distinguish between two degrees of supervi-
sion for domain adaptation. For some languages,
e.g., Basque, English, Swedish, sense-annotated
resources exist, but these corpora are all limited
to newswire or similar domains. In such lan-
guages, unsupervised domain adaptation (DA)
techniques can be used to exploit these resources.
The setting does not presume labeled data from
the target domain. We use discriminative mod-
els for unsupervised domain adaptation, training
on SEMCOR and testing on Twitter.
Finally, we annotated data sets for Twitter, mak-
ing supervised domain adaptation (SU) exper-
iments possible. For supervised domain adapta-
tion, we use the annotated training data sets from
both the newswire and the Twitter domain, as well
as WordNet.
For both unsupervised domain adaptation and
supervised domain adaptation, we use structured
perceptron (Collins, 2002), i.e., a discriminative
HMM model, and search-based structured predic-
tion (SEARN) (Daume et al., 2009). We aug-
ment both the EM-trained HMM2, discrimina-
tive HMMs and SEARN with type constraints and
continuous word representations. We also exper-
imented with conditional random fields (Lafferty
et al., 2001), but obtained worse or similar results
than with the other models.
Contributions In this paper, we present two
Twitter data sets with manually annotated su-
persenses, as well as a series of experiments
with these data sets. These experiments cover
existing approaches to related tasks, as well as
some new methods. In particular, we present
type-constrained extensions of discriminative
HMMs and SEARN sequence models with con-
tinuous word representations that perform well.
We show that when no in-domain labeled data
is available, type constraints improve model
performance considerably. Our best models
achieve a weighted average F1 score of 57.1 over
nouns and verbs on our main evaluation data
set, i.e., a 20% error reduction over the most
frequent sense baseline. The two annotated Twit-
ter data sets are publicly released for download
at https://github.com/coastalcph/
supersense-data-twitter.
n.Tops n.object v.cognition
n.act n.person v.communication
n.animal n.phenomenon v.competition
n.artifact n.plant v.consumption
n.attribute n.possession v.contact
n.body n.process v.creation
n.cognition n.quantity v.emotion
n.communication n.relation v.motion
n.event n.shape v.perception
n.feeling n.state v.possession
n.food n.substance v.social
n.group n.time v.stative
n.location v.body v.weather
n.motive v.change
Table 1: The 41 noun and verb supersenses in
WordNet
2 More or less supervised models
This sections covers the varying degree of super-
vision of our systems as well as the usage of type
constraints as distant supervision.
2.1 Distant supervision
Distant supervision in these experiments was im-
plemented by only allowing a system to predict
a certain supersense for a given word if that su-
persense had either been observed in the training
data, or, for unobserved words, if the sense was
the most frequent sense in WordNet. If the word
did not appear in the training data nor in WordNet,
no filtering was applied. We refer to the distant-
supervision strategy as type constraints.
Distant supervision was implemented differ-
ently in SEARN and the HMM model. SEARN
decomposes sequential labelling into a series of
binary classifications. To constrain the labels we
simply pick the top-scoring sense for each token
from the allowed set. Structured perceptron uses
Viterbi decoding. Here we set the emission prob-
abilities for disallowed senses to negative infinity
and decode as usual.
2.2 Weakly supervised HMMs
The HMM2 model is a second-order hidden
Markov model (Mari et al., 1997; Thede and
Harper, 1999) using logistic regression to estimate
emission probabilities. In addition we constrain
2
w1
t
1
t
2
P(t
2
|t
1
)
P(w
1
|t
1
)
t
3
w
2
w
3
Figure 1: HMM2 with continuous word represen-
tations
the inference space of the HMM2 tagger using
type-level tag constraints derived from WordNet,
leading to roughly the model proposed by Li et
al. (2012), who used Wiktionary as a (part-of-
speech) tag dictionary. The basic feature model
of Li et al. (2012) is augmented with continuous
word representation features as shown in Figure 1,
and our logistic regression model thus works over
a combination of discrete and continuous variables
when estimating emission probabilities. We do 50
passes over the data as in Li et al. (2012).
We introduce two simplifications for the HMM2
model. First, we only use the most frequent senses
(k = 1) in WordNet as type constraints. The
most frequent senses seem to better direct the EM
search for a local optimum, and we see dramatic
drops in performance on held-out data when we
include more senses for the words covered by
WordNet. Second, motivated by computational
concerns, we only train and test on sequences of
(predicted) nouns and verbs, leaving out all other
word classes. Our supervised models performed
slightly worse on shortened sequences, and it is an
open question whether the HMM2 models would
perform better if we could train them on full sen-
tences.
2.3 Structured perceptron and SEARN
We use two approaches to supervised sequen-
tial labeling, structured perceptron (Collins, 2002)
and search-based structured prediction (SEARN)
(Daume et al., 2009). The structured perceptron
is a in-house reimplementation of Ciaramita and
Altun (2006).
1
SEARN performed slightly better
than structured perceptron, so we use it as our in-
house baseline in the experiments below. In this
section, we briefly explain the two approaches.
1
https://github.com/coastalcph/
rungsted
2.3.1 Structured perceptron (HMM)
Structured perceptron learning was introduced in
Collins (2002) and is an extension of the online
perceptron learning algorithm (Rosenblatt, 1958)
with averaging (Freund and Schapire, 1999) to
structured learning problems such as sequence la-
beling.
In structured perceptron for sequential labeling,
where we learn a function from sequences of data
points x
1
. . . x
n
to sequences of labels y
1
. . . y
n
,
we begin with a random weight vector w
0
initial-
ized to all zeros. This weight vector is used to
assign weights to transitions between labels, i.e.,
the discriminative counterpart of P (y
i+1
| y
i
), and
emissions of tokens given labels, i.e., the counter-
part of P (x
i
| y
i
). We use Viterbi decoding to de-
rive a best path
?
y through the correspondingm?n
lattice (with m the number of labels). Let the fea-
ture mapping ?(x,y) be a function from a pair
of sequences ?x,y? to all the features that fired
to make y the best path through the lattice for x.
Now the structured update for a sequence of data
points is simply ?(?(x,y)??(x,
?
y)), i.e., a fixed
positive update of features that fired to produce the
correct sequence of labels, and a fixed negative up-
date of features that fired to produce the best path
under the model. Note that if y =
?
y, no features
are updated.
2.3.2 SEARN
SEARN is a way of decomposing structured pre-
diction problems into search and history-based
classification. In sequential labeling, we decom-
pose the sequence of m tokens into m classifica-
tion problems, conditioning our labeling of the ith
token on the history of i ? 1 previous decisions.
The cost of a mislabeling at training time is de-
fined by a cost function over output structures. We
use Hamming loss rather than F
1
as our cost func-
tion, and we then use stochastic gradient descent
with quantile loss as a our cost-sensitive learning
algorithm. We use a publicly available implemen-
tation.
2
3 Experiments
We experiment with weakly supervised learning,
unsupervised domain adaptation, as well as su-
pervised domain adaptation, i.e., where our mod-
els are induced from hand-annotated newswire
and Twitter data. Note that in all our experiments,
2
http://hunch.net/
?
vw/
3
we use predicted POS tags as input to the system,
in order to produce a realistic estimate of SST per-
formance.
3.1 Data
Our experiments rely on combinations of available
resources and newly annotated Twitter data sets
made publicly available with this paper.
3.1.1 Available resources
Princeton WordNet (Fellbaum, 1998) is the main
resource for SST. The lexicographer file names
provide the label alphabet of the task, and the tax-
onomy defined therein is used not only in the base-
lines, but also as a feature in the discriminative
models. We use the WordNet 3.0 distribution.
SEMCOR (Miller et al., 1994) is a sense-
annotated corpus composed of 80% newswire and
20% literary text, using the sense inventory from
WordNet. SEMCOR comprises 23k distinct lem-
mas in 234k instances. We use the texts which
have full annotations, leaving aside the verb-only
texts (see Section 6).
We use a distributional semantic model in order
to incorporate distributional information as fea-
tures in our system. In particular, we use the
neural-network based models from (Mikolov et
al., 2013), also referred as word embeddings. This
model makes use of skip-grams (n-grams that do
not need to be consecutive) within a word window
to calculate continuous-valued vector representa-
tions from a recurrent neural network. These dis-
tributional models have been able to outperform
state of the art in the SemEval-2012 Task 2 (Mea-
suring degrees of relational similarity). We calcu-
late the embeddings from an in-house corpus of
57m English tweets using a window size 5 and
yielding vectors of 100 dimensions.
We also use the first 20k tweets of the 57m
tweets to train our HMM2 models.
3.1.2 Annotation
While an annotated newswire corpus and a high-
quality lexical resource already enable us to train,
we also need at least a small sample of anno-
tated tweets data to evaluate SST for Twitter. Fur-
thermore, if we want to experiment with super-
vised SST, we also need sufficient annotated Twit-
ter data to learn the distribution of sense tags.
This paper presents two data sets: (a) super-
sense annotations for the POS+NER-annotated
data set described in Ritter et al. (2011), which we
use for training, development and evaluation, us-
ing the splits proposed in Derczynski et al. (2013),
and (b) supersense annotations for a sample of 200
tweets, which we use for additional, out-of-sample
evaluation. We call these data sets RITTER-
{TRAIN,DEV,EVAL} and IN-HOUSE-EVAL, re-
spectively. The IN-HOUSE-EVAL dataset was
downloaded in 2013 and is a sample of tweets that
contain links to external homepages but are other-
wise unbiased. It was previously used (with part-
of-speech annotation) in (Plank et al., 2014). Both
data sets are made publicly available with this pa-
per.
Supersenses are annotated with in spans defined
by the BIO (Begin-Inside-Other) notation. To ob-
tain the Twitter data sets, we carried out an an-
notation task. We first pre-annotated all data sets
with WordNet?s most frequent senses. If the word
was not in WordNet and a noun, we assigned it the
sense n.person. All other words were labeled O.
Chains of nouns were altered to give every ele-
ment the sense of the head noun, and the BI tags
adjusted, i.e.:
Empire/B-n.loc State/B-n.loc Building/B-n.artifact
was changed to
Empire/B-n.artifact State/I-n.artifact Building/I-
n.artifact
For the RITTER data, three paid student an-
notators worked on different subsets of the pre-
annotated data. They were asked to correct mis-
takes in both the BIO notation and the assigned
supersenses. They were free to chose from the full
label set, regardless of the pre-annotation. While
the three annotators worked on separate parts, they
overlapped on a small part of RITTER-TRAIN (841
tokens). On this subset, we computed agreement
scores and annotation difficulties. The average
raw agreement was 0.86 and Cohen?s ? 0.77. The
majority of tokens received the O label by all an-
notators; this happended in 515 out of 841 cases.
Excluding these instances to evaluate the perfor-
mance on the more difficult content words, raw
agreement dropped to 0.69 and Cohen?s ? to 0.69.
The IN-HOUSE-EVAL data set was annotated
by two different annotators, namely two of the au-
thors of this article. Again, for efficiency reasons
they worked on different subsets of the data, with
an overlapping portion. Their average raw agree-
ment was 0.65 and their Cohen?s ? 0.62. For this
data set, we also compute F
1
, defined as usual as
the harmonic mean of recall and precision. To
4
compute this, we set one of the annotators as gold
data and the other as predicted data. However,
since F
1
is symmetrical, the order does not mat-
ter. The annotation F
1
gives us another estimate
of annotation difficulty. We present the figures in
Table 3.
3.2 Baselines
For most word sense disambiguation studies, pre-
dicting the most frequent sense (MFS) of a word
has been proven to be a strong baseline. Follow-
ing this, our MFS baseline simply predicts the su-
persense of the most frequent WordNet sense for
a tuple of a word and a part of speech. We use
the part of speech predicted by the LAPOS tagger
(Tsuruoka et al., 2011). Any word not in Word-
Net is labeled as noun.person, which is the most
frequent sense overall in the training data. After
tagging, we run a script to correct the BI tag pre-
fixes, as described above for the annotation ask.
We also compare to the performance of exist-
ing SST systems. In particular we use Sense-
Learner (Mihalcea and Csomai, 2005) as a base-
line, which produces estimates of the WordNet
sense for each word. For these predictions, we
retrieve the corresponding supersense. Finally,
we use a publicly available reimplementation of
Ciaramita and Altun (2006) by Michael Heilman,
which reaches comparable performance on gold-
tagged SEMCOR.
3
3.3 Model parameters
We use the feature model of Paa? and Reichartz
(2009) in all our models, except the weakly su-
pervised models. For the structured perceptron we
set the number of passes over the training data on
the held-out development data. The weakly super-
vised models use the default setting proposed in
Li et al. (2012). We have used the standard online
setup for SEARN, which only takes one pass over
the data.
The type of embedding is the same in all our
experiments. For a given word the embedding fea-
ture is a 100 dimensional vector, which combines
the embedding of the word with the embedding of
adjacent words. The feature combination f
e
for a
word w
t
is calculated as:
f
e
(w
t
) =
1
2
(e(w
t?1
) + e(w
t+1
))? 2e(w
t
),
3
http://www.ark.cs.cmu.edu/mheilman/
questions/SupersenseTagger-10-01-12.tar.
gz
where the factor of two is chosen heurestically to
give more weight to the current word.
We also set a parameter k on development data
for using the k-most frequent senses inWordNet
as type constraints. Our supervised models are
trained on SEMCOR+RITTER-TRAIN or simply
RITTER-TRAIN, depending on what gave us the
best performance on the held-out data.
4 Results
The results are presented in Table 2. We dis-
tinguish between three settings with various de-
grees of supervision: weakly supervised, which
uses no domain annotated information, but solely
relies on embeddings trained on unlabeled Twit-
ter data; unsupervised domain adaptation (DA),
which uses SemCor for supervised training; and
supervised domain adaptation (SU), which uses
annotated Twitter data in addition to the SemCor
data for training.
In each of the two domain adaptation settings,
SEARN and HMM are evaluated with type con-
straints as distant supervision, and without for
comparison. SEARN without embeddings or dis-
tant supervision serves as an in-house baseline.
In Table 3 we present the WordNet token cov-
erage of predicted nouns and verbs in the devel-
opment and evaluation data, as well as the inter-
annotator agreement F
1
scores.
All the results presented in Table 2 are
(weighted averaged) F
1
measures obtained on pre-
dicted POS tags. Note that these results are con-
siderably lower than results on supersense tagging
newswire (up to 80 F
1
) that assume gold standard
POS tags (Ciaramita and Altun, 2006; Paa? and
Reichartz, 2009).
The re-implementation of the state-of-the-art
system improves slightly upon the most frequent
sense baseline. SenseLearner does not seem to
capture the relevant information and does not
reach baseline performance. In other words, there
is no off-the-shelf tool for supersense tagging of
Twitter that does much better than assigning the
most frequent sense to predicted nouns and verbs.
Our weakly supervised model performs worse
than the most frequent sense baseline. This is a
negative result. It is, however, well-known from
the word sense disambiguation literature that the
MFS is a very strong baseline. Moreover, the EM
learning problem is hard because of the large la-
bel set and weak distributional evidence for super-
5
RITTER IN-HOUSE
DEV EVAL EVAL
Wordnet noun-verb
token coverage 83.72 70.22 41.18
Inter-annotator
agreement (F1) 81.01 69.15 61.57
Table 3: Properties of dataset.
senses.
The unsupervised domain adaptation and fully
supervised systems perform considerably better
than this baseline across the board. In the unsuper-
vised domain adaptation setup, we see huge im-
provements from using type constraints as distant
supervision. In the supervised setup, we only see
significant improvements adding type constraints
for the structured perceptron (HMM), but not for
search-based structured prediction (SEARN).
For all the data sets, there is still a gap between
model performance and human inter-annotator
agreement levels (see Table 3), leaving some room
for improvements. We hope that the release of the
data sets will help further research into this.
4.1 Coarse-grained evaluation
We also experimented with the more coarse-
grained classes proposed by Yuret and Yatbaz
(2010). Here our best model obtained an F
1
score
for mental concepts (nouns) of 72.3%, and 62.6%
for physical concepts, on RITTER-DEV. The over-
all F
1
score for verbs is 85.6%. The overall F
1
is
75.5%. Note that this result is not directly com-
parable to the figure (72.9%) reported in Yuret
and Yatbaz (2010), since they use different data
sets, exclude verbs and make different assump-
tions, e.g., relying on gold POS tags.
5 Error analysis
We have seen that inter-annotator agreements on
supersense annotation are reliable at above .60
but far from perfect. The Hinton diagram in Ta-
ble 2 presents the confusion matrix between our
annotators on IN-HOUSE-EVAL.
Errors in the prediction primarily stem from
two sources: out-of-vocabulary words and incor-
rect POS tags. Figure 3 shows the distribution of
senses over the words that were not contained in
either the training data, WordNet, or the Twitter
data used to learn the embeddings. The distribu-
tion follows a power law, with the most frequent
sense being noun.person, followed by noun.group,
and noun.artifact. The first two are related to NER
categories, namely PER and ORG, and can be ex-
pected, since Twitter users frequently talk about
new actors, musicians, and bands. Nouns of com-
munication are largely related to films, but also in-
clude Twitter, Facebook, and other forms of social
media. Note that verbs occur only towards the tail
end of the distribution, i.e., there are very few un-
known verbs, even in Twitter.
Overall, our models perform best on labels with
low lexical variability, such as quantities, states
and times for nouns, as well as consumption, pos-
session and stative for verbs. This is unsurprising,
since these classes have lower out-of-vocabulary
rates.
With regards to the differences between source
(SEMCOR) and target (Twitter) domains, we ob-
serve that the distribution of supersenses is al-
ways headed by the same noun categories like
noun.person or noun.group, but the frequency of
out-of-vocabulary stative verbs plummets in the
target domain, as some semantic types are more
closed class than others. There are for instance
fewer possibilities for creating new time units
(noun.time) or stative verbs like be than people or
company names (noun.person or noun.group, re-
spectively).
The weakly supervised model HMM2 has
higher precision (57% on RITTER-DEV) than re-
call (48.7%), which means that it often predicts
words to not belong to a semantic class. This
suggests an alternative strategy, which is to train
a model on sequences of purely non-O instances.
This would force the model to only predict O on
words that do not appear in the reduced sequences.
One important source of error seems to be un-
reliable part-of-speech tagging. In particular we
predict the wrong POS for 20-35% of the verbs
across the data sets, and for 4-6.5% of the nouns.
In the SEMCOR data, for comparability, we have
wrongly predicted tags for 6-8% of the anno-
tated tokens. Nevertheless, the error propaga-
tion of wrongly predicted nouns and verbs is par-
tially compensated by our systems, since they are
trained on imperfect input, and thus it becomes
possible for the systems to predict a noun super-
sense for a verb and viceversa. In our data we have
found e.g. that the noun Thanksgiving was incor-
rectly tagged as a verb, but its supersense was cor-
rectly predicted to be noun.time, and that the verb
guess had been mistagged as noun but the system
6
Resources Results
Token-level Type-level RITTER IN-HOUSE
SemCor Twitter Embeddings Type constraints DEV EVAL EVAL
General baselines
MFS - - - + 47.54 44.98 38.65
SENSELEARNER + - - - 14.61 26.24 22.81
HEILMAN + - - - 48.96 45.03 39.65
Weakly supervised systems
HMM2 - - - + 47.09 42.12 26.99
Unsupervised domain adaptation systems (DA)
SEARN (Baseline) + - - - 48.31 42.34 34.30
SEARN + - + - 52.45 48.30 40.22
SEARN + - + + 56.59 50.89 40.50
HMM + - + - 52.40 47.90 40.51
HMM + - + + 57.14 50.98 41.84
Supervised domain adaptation systems (SU)
SEARN (Baseline) + + - - 58.30 52.12 36.86
SEARN + + + - 63.05 57.09 42.37
SEARN + + + + 62.72 57.14 42.42
HMM + + + - 57.20 49.26 39.88
HMM + + + + 60.66 51.40 41.60
Table 2: Weighted F1 average over 41 supersenses.
7
Figure 2: Inter-annotator confusion matrix on TWITTER-EVAL.
0
0.1
0.2
0.3
0.4
noun.
person noun.
group
noun.
artifac
t
noun.
comm
unicat
ion
noun.
event
noun.
locatio
n
noun.
time noun.
act
noun.
food
noun.
attribu
te
noun.
relatio
n
verb.c
ogniti
on
verb.c
reatio
n
verb.e
motio
n
verb.m
otion
verb.p
ercept
ion
verb.s
tative
Figure 3: Sense distribution of OOV words.
8
still predicted the correct verb.cognition as super-
sense.
6 Related Work
There has been relatively little previous work on
supersense tagging, and to the best of our knowl-
edge, all of it has been limited to English newswire
and literature (SEMCOR and SENSEVAL).
The task of supersense tagging was first intro-
duced by Ciaramita and Altun (2006), who used
a structured perceptron trained and evaluated on
SEMCOR via 5-fold cross validation. Their eval-
uation included a held-out development set on
each fold that was used to estimate the number of
epochs. They used additional training data con-
taining only verbs. More importantly, they relied
on gold standard POS tags. Their overall F
1
score
on SEMCOR was 77.1. Reichartz and Paa? (Re-
ichartz and Paa?, 2008; Paa? and Reichartz, 2009)
extended this work, using a CRF model as well
as LDA topic features. They report an F
1
score
of 80.2, again relying on gold standard POS fea-
tures. Our implementation follows their setup and
feature model, but we rely on predicted POS fea-
tures, not gold standard features.
Supersenses provide information similar to
higher-level distributional clusters, but more in-
terpretable, and have thus been used as high-
level features in various tasks, such as preposi-
tion sense disambiguation, noun compound inter-
pretation, and metaphor detection (Ye and Bald-
win, 2007; Tratz and Hovy, 2010; Tsvetkov et al.,
2013). Princeton WordNet only provides a fully
developed taxonomy of supersenses for verbs and
nouns, but Tsvetkov et al. (2014) have recently
proposed an extension of the taxonomy to cover
adjectives. Outside of English, supersenses have
been annotated for Arabic Wikipedia articles by
Schneider et al. (2012).
In addition, a few researchers have tried to
solve coarse-grained word sense disambiguation
problems that are very similar to supersense tag-
ging. Kohomban and Lee (2005) and Kohom-
ban and Lee (2007) also propose to use lexicogra-
pher file identifers from Princeton WordNet senses
(supersenses) and, in addition, discuss how to re-
trieve fine-grained senses from those predictions.
They evaluate their model on all-words data from
SENSEEVAL-2 and SENSEEVAL-3. They use a
classification approach rather than structured pre-
diction.
Yuret and Yatbaz (2010) present a weakly unsu-
pervised approach to this problem, still evaluating
on SENSEVAL-2 and SENSEVAL-3. They focus
only on nouns, relying on gold part-of-speech, but
also experiment with a coarse-grained mapping,
using only three high level classes.
For Twitter, we are aware of little previous work
on word sense disambiguation. Gella et al. (2014)
present lexical sample word sense disambiguation
annotation of 20 target nouns on Twitter, but no
experimental results with this data. There has also
been related work on disambiguation to Wikipedia
for Twitter (Cassidy et al., 2012).
In sum, existing work on supersense tagging
and coarse-grained word sense disambiguation for
English has to the best of our knowledge all fo-
cused on newswire and literature. Moreover, they
all rely on gold standard POS information, making
previous performance estimates rather optimistic.
7 Conclusion
In this paper, we present two Twitter data sets with
manually annotated supersenses, as well as a se-
ries of experiments with these data sets. The data
is publicly available for download.
In this article we have provided, to the best
of our knowledge, the first supersense tagger for
Twitter. We have shown that off-the-shelf tools
perform poorly on Twitter, and we offer two
strategies?namely distant supervision and the us-
age of embeddings as features?that can be com-
bined to improve SST for Twitter.
We propose that distant supervision imple-
mented as type constraints during decoding is a
viable method to limit the mispredictions of su-
persenses by our systems, thereby enforcing pre-
dicted senses that a word has in WordNet. This ap-
proach compensates for the size limitations of the
training data and mitigates the out-of-vocabulary
effect, but is still subject to the coverage of Word-
Net; which is far from perfect for words coming
from high-variability sources such as Twitter.
Using distributional semantics as features in
form of word embeddings also improves the pre-
diction of supersenses, because it provides seman-
tic information for words, regardless of whether
they have been observed the training data. This
method does not require a hand-created knowl-
edge base like WordNet, and is a promising tech-
nique for domain adaptation of supersense tag-
ging.
9
References
Taylor Cassidy, Heng Ji, Lev-Arie Ratinov, Arkaitz Zu-
biaga, and Hongzhao Huang. 2012. Analysis and
enhancement of wikification for microblogs with
context expansion. In COLING, volume 12, pages
441?456.
Massimiliano Ciaramita and Yasemin Altun. 2006.
Broad-coverage sense disambiguation and informa-
tion extraction with a supersense sequence tagger. In
Proc. of EMNLP, pages 594?602, Sydney, Australia,
July.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In EMNLP.
Hal Daume, John Langford, and Daniel Marcu. 2009.
Search-based structured prediction. Machine Learn-
ing, pages 297?325.
Leon Derczynski, Alan Ritter, Sam Clark, and Kalina
Bontcheva. 2013. Twitter part-of-speech tagging
for all: overcoming sparse and noisy data. In
RANLP.
Christiane Fellbaum. 1998. WordNet: an electronic
lexical database. MIT Press USA.
Jennifer Foster, Ozlem Cetinoglu, Joachim Wagner,
Josef Le Roux, Joakim Nivre, Deirde Hogan, and
Josef van Genabith. 2011. From news to comments:
Resources and benchmarks for parsing the language
of Web 2.0. In IJCNLP.
Yoav Freund and Robert Schapire. 1999. Large margin
classification using the perceptron algorithm. Ma-
chine Learning, 37:277?296.
Spandana Gella, Paul Cook, and Timothy Baldwin.
2014. One sense per tweeter and other lexical se-
mantic tales of Twitter. In EACL.
Upali Kohomban and Wee Lee. 2005. Learning se-
mantic classes for word sense disambiguation. In
ACL.
Upali Kohomban and Wee Lee. 2007. Optimizing
classifier performance in word sense disambiguation
by redefining word sense classes. In IJCAI.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: prob-
abilistic models for segmenting and labeling se-
quence data. In ICML.
Shen Li, Jo?ao Grac?a, and Ben Taskar. 2012. Wiki-ly
supervised part-of-speech tagging. In EMNLP.
Jean-Francois Mari, Jean-Paul Haton, and Abdelaziz
Kriouile. 1997. Automatic word recognition based
on second-order hidden Markov models. IEEE
Transactions on Speech and Audio Processing,
5(1):22?25.
Rada Mihalcea and Andras Csomai. 2005. Sense-
learner: Word sense disambiguation for all words in
unrestricted text. In Proceedings of the ACL 2005
on Interactive poster and demonstration sessions,
pages 53?56. Association for Computational Lin-
guistics.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory
Corrado, and Jeffrey Dean. 2013. Distributed rep-
resentations of words and phrases and their compo-
sitionality. In NIPS.
George A. Miller, Martin Chodorow, Shari Landes,
Claudia Leacock, and Robert G. Thomas. 1994.
Using a semantic concordance for sense identifica-
tion. In Proceedings of the workshop on Human
Language Technology, pages 240?243. Association
for Computational Linguistics.
Gerhard Paa? and Frank Reichartz. 2009. Exploit-
ing semantic constraints for estimating supersenses
with CRFs. In Proc. of the Ninth SIAM Interna-
tional Conference on Data Mining, pages 485?496,
Sparks, Nevada, May.
Barbara Plank, Dirk Hovy, and Anders S?gaard. 2014.
Learning part-of-speech taggers with inter-annotator
agreement loss. In Proceedings of EACL.
Frank Reichartz and Gerhard Paa?. 2008. Estimating
Supersenses with Conditional Random Fields. In
Proceedings of ECMLPKDD.
Alan Ritter, Sam Clark, Mausam, and Oren Etzioni.
2011. Named entity recognition in tweets: an ex-
perimental study. In EMNLP.
Frank Rosenblatt. 1958. The perceptron: a probabilis-
tic model for information storage and organization
in the brain. Psychological Review, 65(6):386?408.
Nathan Schneider, Behrang Mohit, Kemal Oflazer, and
Noah A Smith. 2012. Coarse lexical semantic an-
notation with supersenses: an arabic case study. In
Proceedings of the 50th Annual Meeting of the As-
sociation for Computational Linguistics, pages 253?
258. Association for Computational Linguistics.
Scott Thede and Mary Harper. 1999. A second-order
hidden Markov model for part-of-speech tagging. In
ACL.
Stephen Tratz and Eduard Hovy. 2010. Isi: automatic
classification of relations between nominals using a
maximum entropy classifier. In Proceedings of the
5th International Workshop on Semantic Evaluation,
pages 222?225. Association for Computational Lin-
guistics.
Yoshimasa Tsuruoka, Yusuke Miyao, and Jun?ichi
Kazama. 2011. Learning with lookahead: can
history-based models rival globally optimized mod-
els? In CoNLL.
10
Yulia Tsvetkov, Elena Mukomel, and Anatole Gersh-
man. 2013. Cross-lingual metaphor detection us-
ing common semantic features. Meta4NLP 2013,
page 45.
Yulia Tsvetkov, Nathan Schneider, Dirk Hovy, Archna
Bhatia, Manaal Faruqui, and Chris Dyer. 2014.
Augmenting english adjective senses with super-
senses. In Proc. of LREC.
Patrick Ye and Timothy Baldwin. 2007. Melb-yb:
Preposition sense disambiguation using rich seman-
tic features. In Proceedings of the 4th International
Workshop on Semantic Evaluations, pages 241?244.
Association for Computational Linguistics.
Deniz Yuret and Mehmet Yatbaz. 2010. The noisy
channel model for unsupervised word sense disam-
biguation. Computational Linguistics, 36:111?127.
11
Proceedings of the Workshop on Distributional Semantics and Compositionality (DiSCo?2011), pages 29?32,
Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguistics
Shared task system description:
Frustratingly hard compositionality prediction
Anders Johannsen, Hector Martinez Alonso, Christian Rish?j and Anders S?gaard
Center for Language Technology
University of Copenhagen
{ajohannsen|alonso|crjensen|soegaard}@hum.ku.dk
Abstract
We considered a wide range of features for
the DiSCo 2011 shared task about composi-
tionality prediction for word pairs, including
COALS-based endocentricity scores, compo-
sitionality scores based on distributional clus-
ters, statistics about wordnet-induced para-
phrases, hyphenation, and the likelihood of
long translation equivalents in other lan-
guages. Many of the features we considered
correlated significantly with human compo-
sitionality scores, but in support vector re-
gression experiments we obtained the best re-
sults using only COALS-based endocentric-
ity scores. Our system was nevertheless the
best performing system in the shared task, and
average error reductions over a simple base-
line in cross-validation were 13.7% for En-
glish and 50.1% for German.
1 Introduction
The challenge in the DiSCo 2011 shared task is to
estimate and predict the semantic compositionality
of word pairs. Specifically, the data set consists of
adjective-noun, subject-verb and object-verb pairs
in English and German. The organizers also pro-
vided the Wacky corpora for English and German
with lowercased lemmas.1 In addition, we also ex-
perimented with wordnets and using Europarl cor-
pora for the two languages (Koehn, 2005), but none
of the features based on these resources were used
in the final submission.
Semantic compositionality is an ambiguous term
in the linguistics litterature. It may refer to the po-
sition that the meaning of sentences is built from
1http://wacky.sslmit.unibo.it/
the meaning of its parts through very general prin-
ciples of application, as for example in type-logical
grammars. It may also just refer to a typically not
very well defined measure of semantic transparency
of expressions or syntactic constructions, best illus-
trated by examples:
(1) pull the plug
(2) educate people
The verb-object word pair in example (1) is in
the training data rated as much less compositional
than example (2). The intuition is that the mean-
ing of the whole is less related to the meaning of
the parts. The compositionality relation is not de-
fined more precisely, however, and this may in part
explain why compositionality prediction seems frus-
tratingly hard.
2 Features
Many of our features were evaluated with different
amounts of slop. The slop parameter permits non-
exact matches without resorting to language-specific
shallow patterns. The words in the compounds are
allowed to move around in the sentence one position
at a time. The value of the parameter is the maxi-
mum number of steps. Set to zero, it is equivalent
to an exact match. Below are a couple of example
configurations. Note that in order for w1 and w2 to
swap positions, we must have slop > 1 since slop=1
would place them on top of each other.
x x w1 w2 x x (slop=0)
x x w1 x w2 x (slop=1)
x x w1 x x w2 (slop=2)
x x w2 w1 x x (slop=2)
29
2.1 LEFT-ENDOC, RIGHT-ENDOC and
DISTR-DIFF
These features measure the endocentricity of a word
pair w1 w2. The distribution of w1 is likely to be
similar to the distribution of ?w1 w2? if w1 is the
syntactic head of ?w1 w2?. The same is to be ex-
pected for w2, when w2 is the head.
Syntactic endocentricity is related to composi-
tionality, but the implication is one-way only. A
highly compositional compound is endocentric, but
an endocentric compound need not be highly com-
positional. For example, the distribution of ?olive
oil?, which is endocentric and highly compositional,
is very similar to the distribution of ?oil?, the head
word. On the other hand, ?golden age? which is
ranked as highly non-compositional in the training
data, is certainly endocentric. The distribution of
?golden age? is not very different from that of ?age?.
We used COALS (Rohde et al, 2009) to cal-
culate word distributions. The COALS algorithm
builds a word-to-word semantic space from a cor-
pus. We used the implementation by Jurgens and
Stevens (2010), generating the semantic space from
the Wacky corpora for English and German with du-
plicate sentences removed and low-frequency words
substituted by dummy symbols. The word pairs
have been fed to COALS as compounds that have to
be treated as single tokens, and the semantic space
has been generated and reduced using singular value
decompositon. The vectors for w1, w2 and ?w1 w2?
are calculated, and we compute the cosine distance
between the semantic space vectors for the word
pair and its parts, and between the parts themselves,
namely for ?w1 w2? and w1, for ?w1 w2? and w2,
and for w1 and w2, say for ?olive oil? and ?olive?,
for ?olive oil? and ?oil?, and for ?olive? and ?oil?.
LEFT-ENDOC is the cosine distance between the left
word and the compound. RIGHT-ENDOC is the co-
sine distance between the right word and the com-
pound. Finally, DISTR-DIFF is the cosine distance
between the two words, w1 and w2.
2.2 BR-COMP
To accommodate for the weaknesses of syntactic en-
docentricity features, we also tried introducing com-
positionality scores based on hierarchical distribu-
tional clusters that would model semantic composi-
tionality more directly. The scores are referred to
below as BR-COMP (compositionality scores based
on Brown clusters), and the intuition behind these
scores is that a word pair ?w1 w2?, e.g. ?hot dog?, is
non-compositional if w1 and w2 have high colloca-
tional strength, but if w1 is replaced with a different
word w?1 with similar distribution, e.g. ?warm?, then
?w?1 w2? is less collocational. Similarly, if w2 is re-
placed with a different word w?2 with similar distri-
bution, e.g. ?terrier?, then ?w1 w?2? is also much less
collocational than ?w1 w2?.
We first induce a hierarchical clustering of the
words in the Wacky corpora cl : W ? 2W with
W the set of words in our corpora, using publicly
available software.2 Let the collocational strength of
the two words w1 and w2 be G2(w1, w2). We then
compute the average collocational strength of distri-
butional clusters, BR-CS (collocational strength of
Brown clusters):
BR-CS(w1, w2) =
?Nx?cl(w1),x??cl(w2)G
2(x, x?)
N
with N = |cl(w1)| ? |cl(w2)|. We now let
BR-COMP(w1, w2) =
BR-CS(w1,w2)
G2(w1,w2)
.
The Brown clusters were built with C = 1000
and a cut-off frequency of 1000. With these settings
the number of word types per cluster is quite high,
which of course has a detrimental effect on the se-
mantic coherence of the cluster. To counter this we
choose to restrict cl(w) and cl(w?) to include only
the 50 most frequently occurring terms.
2.3 PARAPHR
These features have to do with alternative phrasings
using synonyms from Princeton WordNet 3 and Ger-
maNet4. One word in the compound is held con-
stant while the other is replaced with its synonyms.
The intuition is again that non-compositional com-
pounds are much more frequent than any compound
that results from replacing one of the constituent
words with one of its synonyms. For ?hot dog? we
thus generate ?hot terrier? and ?warm dog?, but not
?warm terrier?. Specifically, PARAPHR?100 means
2http://www.cs.berkeley.edu/?pliang/software/
3http://wordnet.princeton.edu/
4GermaNet Copyright c? 1996, 2008 by University of
Tu?bingen.
30
that at least one of the alternative compounds has
a document count of more than 100 in the cor-
pus. PARAPHRav is the average count for all para-
phrases, PARAPHRsum is the sum of these counts,
and PARAPHRrel is the average count for all para-
phrases over the count of the word pair in question.
2.4 HYPH
The HYPH features were inspired by Bergsma et
al. (2010). It was only used for English. Specif-
ically, we used the relative frequency of hyphen-
ated forms as features. For adjective-noun pairs
we counted the number of hyphenated occurrences,
e.g. ?front-page?, and divided that number by the
number of non-hyphenated occurrences, e.g. ?front
page?. For subject-verb and object-verb pairs, we
add -ing to the verb, e.g. ?information-collecting?,
and divided the number of such forms with non-
hyphenated equivalents, e.g. ?information collect-
ing?.
2.5 TRANS-LEN
The intuition behind our bilingual features is that
non-compositional words typically translate into a
single word or must be paraphrased using multiple
words (circumlocution or periphrasis). TRANS-LEN
is the probability that the phrase?s translation, possi-
bly with intervening articles and markers, is longer
than lmin and shorter than lmax , i.e.:
TRANS-LEN(w1, w2, lmin , lmax ) =
???trans(w1 w2),l1?|? |?l2P (?|w1 w2)
???trans(w1 w2)P (?|w1 w2)
We use English and German Europarl (Koehn,
2005) to train our translation models. In particular,
we use the phrase tables of the Moses PB-SMT sys-
tem5 trained on a lemmatized version of the WMT11
parallel corpora for English and German. Below
TRANS-LEN-n will be the probability of the trans-
lation of a word pair being n or more words. We
also experimented with average translation length as
a feature, but this did not correlate well with seman-
tic compositionality.
5http://statmt.org
feat ?
English German
rel-type = ADJ NN 0.0750 *0.1711
rel-type = V SUBJ 0.0151 **0.2883
rel-type = V OBJ 0.0880 0.0825
LEFT-ENDOC **0.3257 *0.1637
RIGHT-ENDOC **0.3896 0.1379
DISTR-DIFF *0.1885 0.1128
HYPH (5) 0.1367 -
HYPH (5) reversed *0.1829 -
G2 0.1155 0.0535
BR-CS *0.1592 0.0242
BR-COMP 0.0292 0.0024
Count (5) 0.0795 *0.1523
PARAPHR?|w1 w?2| 0.1123 0.1242
PARAPHRrel (5) 0.0906 0.0013
PARAPHRav (1) 0.1080 0.0743
PARAPHRav (5) 0.1313 0.0707
PARAPHRsum (1) 0.0496 0.0225
PARAPHR?100 (1) **0.2434 0.0050
PARAPHR?100 (5) **0.2277 0.0198
TRANS-LEN-1 0.0797 0.0509
TRANS-LEN-2 0.1109 0.0158
TRANS-LEN-3 0.0935 0.0489
TRANS-LEN-5 0.0240 0.0632
Figure 1: Correlations. Coefficients marked with * are
significant (p < 0.05), and coefficients marked with **
are highly significant (p < 0.01). We omit features with
different slop values if they perform significantly worse
than similar features.
3 Correlations
We have introduced five different kinds of features,
four of which are supposed to model semantic com-
positionality directly. For feature selection, we
therefore compute the correlation of features with
compositionality scores and select features that cor-
relate significantly with compositionality. The fea-
tures are then used for regression experiments.
4 Regression experiments
For our regression experiments, we use support vec-
tor regression with a high (7) degree kernel. Other-
wise we use default parameters of publicly available
software.6 In our experiments, however, we were
not able to produce substantially better results than
what can be obtained using only the features LEFT-
ENDOC and RIGHT-ENDOC. In fact, for German
using only LEFT-ENDOC gave slightly better results
than using both. These features are also those that
correlate best with human compositionality scores
according to Figure 1. Consequently, we only use
6http://www.csie.ntu.edu.tw/?cjlin/libsvm/
31
these features in our official runs. Our evaluations
below are cross-validation results on training and de-
velopment data using leave-one-out. We compare
using only LEFT-ENDOC and RIGHT-ENDOC (for
English) with using all significant features that seem
relatively independent. For English, we used LEFT-
ENDOC, RIGHT-ENDOC, DISTR-DIFF, HYPH (5)
reversed, BR-CS, PARAPHR?100 (1). For German,
we used rel-type = ADJ NN, rel-type=V SUBJ and
RIGHT-ENDOC. We only optimized on numeric
scores. The submitted coarse-grained scores were
obtained using average +/- average deviation.7
English German
dev test dev test
BL 18.395 47.123
all sign. indep. 19.22 23.02
L-END+R-END 15.89 16.19 23.51 24.03
err.red (L+R) 0.137 0.501
5 Discussion
Our experiments have shown that the DiSCo 2011
shared task about compositionality prediction was a
tough challenge. This may be because of the fine-
grained compositionality metric or because of in-
consistencies in annotation, but note also that the
syntactically oriented features seem to perform a
lot better than those trying to single out semantic
compositionality from syntactic endocentricity and
collocational strength. For example, LEFT-ENDOC,
RIGHT-ENDOC and BR-CS correlate with compo-
sitionality scores, whereas BR-COMP does not, al-
though it is supposed to model compositionality
more directly. Could it perhaps be that annotations
reflect syntactic endocentricity or distributional sim-
ilarity to a high degree, rather than what is typically
thought of as semantic compositionality?
Consider a couple of examples of adjective-noun
pairs in English in Figure 2 for illustration. These
examples are taken from the training data, but we
have added our subjective judgments about semantic
and syntactic markedness and collocational strength
(peaking at G2 scores). It seems that semantic
markedness is less important for scores than syntac-
7These thresholds were poorly chosen, by the way. Had we
chosen less balanced cut-offs, say 0 and 72, our improved accu-
racy on coarse-grained scores (59.4) would have been compara-
ble to and slightly better than the best submitted coarse-grained
scores (58.5).
sem syn coll score
floppy disk X 61
free kick X 77
happy birthday X X 47
large scale X X 55
old school X X X 37
open source X X 49
real life X 69
small group 91
Figure 2: Subjective judgments about semantic and syn-
tactic markedness and collocational strength.
tic markedness and collocational strength. In partic-
ular, the combination of syntactic markedness and
collocational strength makes annotators rank word
pairs such as happy birthday and open source as
non-compositional, although they seem to be fully
compositional from a semantic perspective. This
may explain why our COALS-features are so predic-
tive of human compositionality scores, and why G2
correlates better with these scores than BR-COMP.
6 Conclusions
In our experiments for the DiSCo 2011 shared task
we have considered a wide range of features and
showed that some of them correlate significantly and
sometimes highly significantly with human compo-
sitionality scores. In our regression experiments,
however, our best results were obtained with only
one or two COALS-based endocentricity features.
We report error reductions of 13.7% for English and
50.1% for German.
References
Shane Bergsma, Aditya Bhargava, Hua He, and Grzegorz
Kondrak. 2010. Predicting the semantic composition-
ality of prefix verbs. In EMNLP.
David Jurgens and Keith Stevens. 2010. The S-Space
package: an open source package for word space mod-
els. In ACL.
Philipp Koehn. 2005. Europarl: a parallel corpus for
statistical machine translation. In MT-Summit.
Douglas Rohde, Laura Gonnerman, and David Plaut.
2009. An improved model of semantic similarity
based on lexical co-occurrence. In Cognitive Science.
32
Proceedings of the Eighteenth Conference on Computational Language Learning, pages 1?10,
Baltimore, Maryland USA, June 26-27 2014.
c
?2014 Association for Computational Linguistics
What?s in a p-value in NLP?
Anders S?gaard, Anders Johannsen, Barbara Plank, Dirk Hovy and Hector Martinez
Center for Language Technology
University of Copenhagen
soegaard@hum.ku.dk
Abstract
In NLP, we need to document that our pro-
posed methods perform significantly bet-
ter with respect to standard metrics than
previous approaches, typically by re-
porting p-values obtained by rank- or
randomization-based tests. We show that
significance results following current re-
search standards are unreliable and, in ad-
dition, very sensitive to sample size, co-
variates such as sentence length, as well as
to the existence of multiple metrics. We
estimate that under the assumption of per-
fect metrics and unbiased data, we need a
significance cut-off at ?0.0025 to reduce
the risk of false positive results to <5%.
Since in practice we often have consider-
able selection bias and poor metrics, this,
however, will not do alone.
1 Introduction
In NLP, we try to improve upon state of the art
language technologies, guided by experience and
intuition, as well as error analysis from previous
experiments, and research findings often consist in
system comparisons showing that System A is bet-
ter than System B.
Effect size, i.e., one system?s improvements
over another, can be seen as a random variable.
If the random variable follows a known distribu-
tion, e.g., a normal distribution, we can use para-
metric tests to estimate whether System A is bet-
ter than System B. If it follows a normal dis-
tribution, we can use Student?s t-test, for exam-
ple. Effect sizes in NLP are generally not nor-
mally distributed or follow any of the other well-
studied distributions (Yeh, 2000; S?gaard, 2013).
The standard significance testing methods in NLP
are therefore rank- or randomization-based non-
parametric tests (Yeh, 2000; Riezler and Maxwell,
2005; Berg-Kirkpatrick et al., 2012). Specifi-
cally, most system comparisons across words, sen-
tences or documents use bootstrap tests (Efron and
Tibshirani, 1993) or approximate randomization
(Noreen, 1989), while studies that compare perfor-
mance across data sets use rank-based tests such as
Wilcoxon?s test.
The question we wish to address here is: how
likely is a research finding in NLP to be false?
Naively, we would expect all reported findings to
be true, but significance tests have their weak-
nesses, and sometimes researchers are forced
to violate test assumptions and basic statistical
methodology, e.g., when there is no one estab-
lished metric, when we can?t run our models on
full-length sentences, or when data is biased. For
example, one such well-known bias from the tag-
ging and parsing literature is what we may refer to
as the WSJ FALLACY. This is the false belief that
performance on the test section of the Wall Street
Journal (WSJ) part of the English Penn treebank
is representative for performance on other texts in
English. In other words, it is the belief that our
samples are always representative. However, (the
unawareness of) selection bias is not the only rea-
son research findings in NLP may be false.
In this paper, we critically examine significance
results in NLP by simulations, as well as running
a series of experiments comparing state-of-the-art
POS taggers, dependency parsers, and NER sys-
tems, focusing on the sensitivity of p-values to var-
ious factors.
Specifically, we address three important factors:
Sample size. When system A is reported to be
better than system B, this may not hold across do-
mains (cf. WSJ FALLACY). More importantly,
though, it may not even hold on a sub-sample of
the test data, or if we added more data points to
the test set. Below, we show that in 6/10 of our
POS tagger evaluations, significant effects become
insignificant by (randomly) adding more test data.
1
Covariates. Sometimes we may bin our results
by variables that are actually predictive of the out-
come (covariates) (Simmons et al., 2011). In some
subfields of NLP, such as machine translation or
(unsupervised) syntactic parsing, for example, it
is common to report results that only hold for sen-
tences up to some length. If a system A is reported
to be better than a system B on sentences up to
some length, A need not be better than B, neither
for a different length nor in general, since sentence
length may actually be predictive of A being better
than B.
Multiple metrics. In several subfields of NLP,
we have various evaluation metrics. However, if
a system A is reported to be better than a system
B with respect to some metric M
1
, it need not be
better with respect to some other metric M
2
. We
show that even in POS tagging it is sometimes the
case that results are significant with respect to one
metric, but not with respect to others.
While these caveats should ideally be avoided
by reporting significance over varying sample
sizes and multiple metrics, some of these effects
also stem from the p-value cut-off chosen in the
NLP literature. In some fields, p-values are re-
quired to be much smaller, e.g., in physics, where
the 5   criterion is used, and maybe we should also
be more conservative in NLP?
We address this question by a simulation of the
interaction of type 1 and type 2 error in NLP and
arrive at an estimate that more than half of research
findings in NLP with p < 0.05 are likely to be
false, even with a valid metric and in the absence
of selection bias. From the same simulations, we
propose a new cut-off level at 0.0025 or smaller
for cases where the metric can be assumed to be
valid, and where there is no selection bias.
1
We
briefly discuss what to do in case of selection bias
or imperfect metrics.
Note that we do not discuss false discovery rate
control or family wise error rate procedures here.
While testing with different sample sizes could
be be considered multiple hypothesis testing, as
pointed out by one of our anonymous reviewers,
NLP results should be robust across sample sizes.
Note that the p < 0.0025 cut-off level corresponds
1
In many fields, including NLP, it has become good prac-
tice to report actual p-values, but we still need to understand
how significance levels relate to the probability that research
findings are false, to interpret such values. The fact that we
propose a new cut-off level for the ideal case with perfect
metrics and no bias does not mean that we do not recommend
reporting actual p-values.
to a Bonferroni correction for a family of m = 20
hypotheses.
Our contributions
Several authors have discussed significance test-
ing in NLP before us (Yeh, 2000; Riezler and
Maxwell, 2005; Berg-Kirkpatrick et al., 2012), but
while our discussion touches on many of the same
topics, this paper is to the best of our knowledge
the first to:
a) show experimentally how sensitive p-values
are to sample size, i.e., that in standard NLP
experiments, significant effects may actually
disappear by adding more data.
b) show experimentally that multiple metrics
and the use of covariates in evaluation in-
crease the probability of positive test results.
c) show that even under the assumption of per-
fect metrics and unbiased data, as well as our
estimates of type 1 and 2 error in NLP, you
need at least p < 0.0025 to reduce the prob-
ability of a research finding being false to be
< 5%.
2 Significance testing in NLP
Most NLP metric for comparing system outputs
can be shown to be non-normally distributed
(S?gaard, 2013) and hence, we generally cannot
use statistical tests that rely on such an assump-
tion, e.g., Student?s t-test. One alternative to such
tests are non-parametric rank-based tests such as
Wilcoxon?s test. Rank-based tests are sometimes
used in NLP, and especially when the number of
observations is low, e.g., when evaluating perfor-
mance across data sets, such tests seem to be the
right choice (Demsar, 2006; S?gaard, 2013). The
draw-back of rank-based tests is their relatively
weak statistical power. When we reduce scores to
ranks, we throw away information, and rank-based
tests are therefore relatively conservative, poten-
tially leading to high type 2 error rate ( , i.e., the
number of false negatives over trials). An alterna-
tive, however, are randomization-based tests such
as the bootstrap test (Efron and Tibshirani, 1993)
and approximate randomization (Noreen, 1989),
which are the de facto standards in NLP. In this
paper, we follow Berg-Kirkpatrick et al. (2012) in
focusing on the bootstrap test. The bootstrap test is
non-parametric and stronger than rank-based test-
ing, i.e., introduces fewer type 2 errors. For small
samples, however, it does so at the expense of a
2
higher type 1 error (?, i.e., the number of false
positives). The reason for this is that for the boot-
strap test to work, the original sample has to cap-
ture most of the variation in the population. If the
sample is very small, though, this is likely not the
case. Consequently, with small sample sizes, there
is a risk that the calculated p-value will be arti-
ficially low?simply because the bootstrap sam-
ples are too similar. In our experiments below, we
make sure only to use bootstrap when sample size
is > 200, unless otherwise stated. In our experi-
ments, we average across 3 runs for POS and NER
and 10 runs for dependency parsing.
DOMAIN #WORDS TASKS
POS Dep. NER
CONLL 2007
Bio 4k ?
Chem 5k ?
SWITCHBOARD 4
Spoken 162k ?
ENGLISH WEB TREEBANK
Answers 29k ? ?
Emails 28k ? ?
Newsgrs 21k ? ?
Reviews 28k ? ?
Weblogs 20k ? ?
WSJ 40k ? ?
FOSTER
Twitter 3k ?
CONLL 2003
News 50k ?
Table 1: Evaluation data.
3 Experiments
Throughout the rest of the paper, we use four run-
ning examples: a synthetic toy example and three
standard experimental NLP tasks, namely POS
tagging, dependency parsing and NER. The toy
example is supposed to illustrate the logic behind
our reasoning and is not specific to NLP. It shows
how likely we are to obtain a low p-value for the
difference in means when sampling from exactly
the same (Gaussian) distributions. For the NLP
setups (2-4), we use off-the-shelf models or avail-
able runs, as described next.
3.1 Models and data
We use pre-trained models for POS tagging and
dependency parsing. For NER, we use the output
of the best performing systems from the CoNLL
2003 shared task. In all three NLP setups, we
compare the outcome of pairs of systems. The
data sets we use for each of the NLP tasks are
listed in Table 1 (Nivre et al., 2007a; Foster et
Figure 1: Accuracies of LAPOS VS. STANFORD
across 10 data sets.
al., 2011; Tjong Kim Sang and De Meulder, 2003,
LDC99T42; LDC2012T13).
POS tagging. We compare the performance
of two state-of-the-art newswire taggers across 10
evaluation data sets (see Table 1), namely the LA-
POS tagger (Tsuruoka et al., 2011) and the STAN-
FORD tagger (Toutanova et al., 2003), both trained
on WSJ00?18. We use the publicly available pre-
trained models from the associated websites.
2
Dependency parsing. Here we compare the
pre-trained linear SVM MaltParser model for En-
glish (Nivre et al., 2007b) to the compositional
vector grammar model for the Stanford parser
(Socher et al., 2013). For this task, we use the sub-
set of the POS data sets that comes with Stanford-
style syntactic dependencies (cf. Table 1), exclud-
ing the Twitter data set which we found too small
to produce reliable results.
NER. We use the publicly available runs of
the two best systems from the CoNLL 2003
shared task, namely FLORIAN (Florian et al.,
2003) and CHIEU-NG (Chieu and Ng, 2003).
3
3.2 Standard comparisons
POS tagging. Figure 1 shows that the LAPOS
tagger is marginally better than STANFORD on
macro-average, but it is also significantly better? If
we use the bootstrap test over tagging accuracies,
the difference between the two taggers is only sig-
nificant (p < 0.05) in 3/10 cases (see Table 2),
namely SPOKEN, ANSWERS and REVIEWS. In
two of these cases, LAPOS is significantly better
2
http://www.logos.ic.i.u-tokyo.ac.jp/
?
tsuruoka/lapos/ and http://nlp.stanford.
edu/software/tagger.shtml
3
http://www.cnts.ua.ac.be/conll2003/
ner/
3
TA (b) UA (b) SA (b) SA(w)
Bio 0.3445 0.0430 0.3788 0.9270
Chem 0.3569 0.2566 0.4515 0.9941
Spoken <0.001 <0.001 <0.001 <0.001
Answers <0.001 0.0143 <0.001 <0.001
Emails 0.2020 <0.001 0.1622 0.0324
Newsgrs 0.3965 0.0210 0.1238 0.6602
Reviews 0.0020 0.0543 0.0585 0.0562
Weblogs 0.2480 0.0024 0.2435 0.9390
WSJ 0.4497 0.0024 0.2435 0.9390
Twitter 0.4497 0.0924 0.1111 0.7853
Table 2: POS tagging p-values across tagging ac-
curacy (TA), accuracy for unseen words (UA) and
sentence-level accuracy (SA) with bootstrap (b)
and Wilcoxon (w) (p < 0.05 gray-shaded).
LAS UAS
Answers 0.020 <0.001
Emails 0.083 <0.001
Newsgroups 0.049 <0.001
Reviews <0.001 <0.001
Weblogs <0.001 <0.001
WSJ <0.001 <0.001
Table 3: Parsing p-values (MALT-LIN
VS. STANFORD-RNN) across LAS and UAS
(p < 0.05 gray-shaded).
than STANFORD, but in one case it is the other way
around. If we do a Wilcoxon test over the results
on the 10 data sets, following the methodology
in Demsar (2006) and S?gaard (2013), the differ-
ence, which is ?0.12% on macro-average, is not
significant (p ? 0.1394). LAPOS is thus not sig-
nificantly better than STANFORD across data sets,
but as we have already seen, it is significantly bet-
ter on some data sets. So if we allow ourselves
to cherry-pick our data sets and report significance
over word-level tagging accuracies, we can at least
report significant improvements across a few data
sets.
Dependency parsing. Using the bootstrap test
over sentences, we get the p-values in Table 3.
We see that differences are always significant
wrt. UAS, and in most cases wrt. LAS.
NER. Here we use the macro-f
1
as our stan-
dard metric. FLORIAN is not significantly bet-
ter than CHIEU-NG with p < 0.05 as our cut-
off (p ? 0.15). The two systems were also re-
ported to have overlapping confidence intervals in
the shared task.
3.3 p-values across metrics
In several NLP subfields, multiple metrics are in
use. This happens in dependency parsing where
multiple metrics (Schwartz et al., 2011; Tsarfaty
et al., 2012) have been proposed in addition to un-
labeled and labeled attachment scores, as well as
exact matches. Perhaps more famously, in ma-
chine translation and summarization it is com-
mon practice to use multiple metrics, and there
exists a considerable literature on that topic (Pa-
pineni et al., 2002; Lin, 2004; Banerjee and Lavie,
2005; Clark et al., 2011; Rankel et al., 2011).
Even in POS tagging, some report tagging ac-
curacies, tagging accuracies over unseen words,
macro-averages over sentence-level accuracies, or
number of exact matches.
The existence of several metrics is not in it-
self a problem, but if researchers can cherry-pick
their favorite metric when reporting results, this
increases the a priori chance of establishing sig-
nificance. In POS tagging, most papers report sig-
nificant improvements over tagging accuracy, but
some report significant improvements over tag-
ging accuracy of unknown words, e.g., Denis and
Sagot (2009) and Umansky-Pesin et al. (2010).
This corresponds to the situation in psychology
where researchers cherry-pick between several de-
pendent variables (Simmons et al., 2011), which
also increases the chance of finding a significant
correlation.
Toy example. We draw two times 100 val-
ues from identical (0, 1)-Gaussians 1000 times
and calculate a t-test for two independent sam-
ples. This corresponds to testing the effect size
between two systems on a 1000 randomly cho-
sen test sets with N = 100. Since we are sam-
pling from the same distribution, the chance of
p < ? should be smaller than ?. In our simula-
tion, the empirical chance of obtaining p < 0.01
is .8%, and the chance of obtaining p < 0.05 is
4.8%, as expected. If we simulate a free choice
between two metrics by introducing choice be-
tween a pair of samples and a distorted copy of
that pair (inducing random noise at 10%), simu-
lating the scenario where we have a perfect metric
and a suboptimal metric, the chance of obtaining
p < 0.05 is 10.0%. We see a significant correla-
tion (p < 0.0001) between Pearson?s ? between
the two metrics, and the p-value. The less the two
metrics are correlated, the more likely we are to
obtain p < 0.05. If we allow for a choice between
two metrics, the chance of finding a significant dif-
ference increases considerably. If the two metrics
are identical, but independent (introducing a free
choice between two pairs of samples), we have
4
P (A_B) = P (A) + P (B)  P (A)P (B), hence
the chance of obtaining p < 0.01 is 1.9%, and the
chance of obtaining p < 0.05 is 9.75%.
POS tagging. In our POS-tagging experiments,
we saw a significant improvement in 3/10 cases
following the standard evaluation methodology
(see Table 2). If we allow for a choice between
tagging accuracy and sentence-level accuracy, we
see a significant improvement in 4/10 cases, i.e.,
for 4/10 data sets the effect is significance wrt. at
least one metric. If we allow for a free choice be-
tween all three metrics (TA, UA, and SA), we ob-
serve significance in 9/10 cases. This way the ex-
istence of multiple metrics almost guarantees sig-
nificant differences. Note that there are only two
data sets (Answers and Spoken), where all metric
differences appear significant.
Dependency parsing. While there are multi-
ple metrics in dependency parsing (Schwartz et
al., 2011; Tsarfaty et al., 2012), we focus on
the two standard metrics: labeled (LAS) and un-
labeled attachment score (UAS) (Buchholz and
Marsi, 2006). If we just consider the results in
Table 3, i.e., only the comparison of MALT-LIN
VS. STANFORD-RNN, we observe significant im-
provements in all cases, if we allow for a free
choice between metrics. Bod (2000) provides a
good example of a parsing paper evaluating mod-
els using different metrics on different test sets.
Chen et al. (2008), similarly, only report UAS.
NER. While macro-f
1
is fairly standard in
NER, we do have several available multiple met-
rics, including the unlabeled f
1
score (collapsing
all entity types), as well as the f
1
scores for each
of the individual entity types (see Derczynski and
Bontcheva (2014) for an example of only report-
ing f
1
for one entity type). With macro-f
1
and
f
1
for the individual entity types, we observe that,
while the average p-value for bootstrap tests over
five runs is around 0.15, the average p-value with a
free choice of metrics is 0.02. Hence, if we allow
for a free choice of metrics, FLORIAN comes out
significantly better than CHIEU-NG.
3.4 p-values across sample size
We now show that p-values are sensitive to sam-
ple size. While it is well-known that studies with
low statistical power have a reduced chance of
detecting true effects, studies with low statistical
power are also more likely to introduce false pos-
itives (Button et al., 2013). This, combined with
the fact that free choice between different sample
Figure 2: The distribution of p-values with (above)
and without (below) multiple metrics.
Figure 3: POS tagging p-values varying sample
sizes (p < 0.05 shaded).
sizes also increases the chance of false positives
(Simmons et al., 2011), is a potential source of er-
ror in NLP.
Toy example. The plot in Figure 2 shows the
distribution of p-values across 1000 bootstrap tests
(above), compared to the distribution of p-values
with a free choice of four sample sizes. It is clear
that the existence of multiple metrics makes the
probability of a positive result much higher.
POS tagging. The same holds for POS tag-
ging. We plot the p-values across various sample
sizes in Figure 3. Note that even when we ignore
the smallest sample size (500 words), where re-
sults may be rather unreliable, it still holds that for
Twitter, Answers, Newsgrs, Reviews, Weblogs and
WSJ, i.e., more than half of the data sets, a sig-
nificant result (p < 0.05) becomes insignificant
by increasing the sample size. This shows how
unreliable significance results in NLP with cut-off
p < 0.05 are.
5
Figure 4: Parsing p-values varying sample sizes
(p < 0.05 shaded)
Figure 5: NER p-values varying sample sizes (p <
0.05 shaded)
Dependency parsing. We performed simi-
lar experiments with dependency parsers, seeing
much the same picture. Our plots are presented in
Figure 4. We see that while effect sizes are al-
ways significant wrt. UAS, LAS differences be-
come significant when adding more data in 4/6
cases. An alternative experiment is to see how
often a bootstrap test at a particular sample size
comes out significant. The idea is to sample, say,
10% of the test data 100 times and report the ra-
tio of positive results. We only present the results
for MALT-LIN VS. STANFORD-RNN in Table 4,
but the full set of results (including comparisons of
more MaltParser and Stanford parser models) are
made available at http://lowlands.ku.dk.
For MALT-LIN VS. STANFORD-RNN differ-
ences on the full Emails data set are consistently
insignificant, but on small sample sizes we do get
significant test results in more than 1/10 cases. We
see the same picture with Newsgrs and Reviews.
On Weblogs and WSJ, the differences on the full
data sets are consistently significant, but here we
see that the test is underpowered at small sam-
ple sizes. Note that we use bootstrap tests over
sentences, so results with small samples may be
somewhat unreliable. In sum, these experiments
show how small sample sizes not only increase the
chance of false negatives, but also the chance of
false positives (Button et al., 2013).
NER. Our plots for NER are presented in Fig-
ure 5. Here, we see significance at small sam-
ple sizes, but the effect disappears with more data.
This is an example of how underpowered studies
may introduce false positives (Button et al., 2013).
3.5 p-values across covariates
Toy example. If we allow for a choice between
two subsamples, using a covariate to single out a
subset of the data, the chance of finding a signifi-
cant difference increases. Even if we let the subset
be a random 50-50 split, the chance of obtaining
p < 0.01 becomes 2.7%, and the chance of obtain-
ing p < 0.05 is 9.5%. If we allow for both a choice
of dependent variables and a random covariate, the
chance of obtaining p < 0.01 is 3.7%, and the
chance of obtaining p < 0.05 is 16.2%. So iden-
tical Gaussian variables will appear significantly
different in 1/6 cases, if our sample size is 100,
and if we are allowed a choice between two iden-
tical, but independent dependent variables, and a
choice between two subsamples provided by a ran-
dom covariate.
POS We see from Figure 6 that p-values are
also very sensitive to sentence length cut-offs. For
instance, LAPOS is significantly (p < 0.05) bet-
ter than STANFORD on sentences shorter than 16
words in EMAILS, but not on sentences shorter
than 14 words. On the other hand, when longer
sentences are included, e.g., up to 22 words, the
effect no longer appears significant. On full sen-
tence length, four differences seem significant, but
if we allow ourselves to cherry-pick a maximum
sentence length, we can observe significant differ-
ences in 8/10 cases.
Figure 6: POS tagging p-values varying sentence
length (p < 0.05 shaded)
We observe similar results in Dependency
parsing and NER when varying sentence length,
but do not include them here for space rea-
sons. The results are available at http://
lowlands.ku.dk. We also found that other
covariates are used in evaluations of dependency
parsers and NER systems. In dependency pars-
ing, for example, parsers can either be evaluated
6
N Emails Newsgrs Reviews Weblogs WSJ
LAS UAS LAS UAS LAS UAS LAS UAS LAS UAS
10% 14 % 100 % 9 % 100 % 33% 100 % 42 % 99 % 28 % 75 %
25% 15 % 100 % 23 % 100 % 52% 100 % 68 % 100 % 27 % 98 %
50% 19 % 100 % 25 % 100 % 78% 100 % 100 % 100 % 60 % 100 %
75% 22 % 100 % 41 % 100 % 97% 100 % 100 % 100 % 80 % 100 %
100% 0 % 100 % 36 % 100 % 100% 100 % 100 % 100 % 100 % 100 %
Table 4: Ratio of positive results (p < 0.05) for MALT-LIN VS. STANFORD-RNN at sample sizes (N )
.
on naturally occurring text such as in our experi-
ments or at tailored test suites, typically focusing
on hard phenomena (Rimell et al., 2009). While
such test suites are valuable resources, cf. Man-
ning (2011), they do introduce free choices for re-
searchers, increasing the a priori chance of posi-
tive results. In NER, it is not uncommon to leave
out sentences without any entity types from eval-
uation data. This biases evaluation toward high
recall systems, and the choice between including
them or not increases chances of positive results.
4 How likely are NLP findings to be
false?
The previous sections have demonstrated how
many factors can contribute to reporting an erro-
neously significant result. Given those risks, it is
natural to wonder how likely we are as a field to
report false positives. This can be quantified by
the positive predictive value (PPV), or probability
that a research finding is true. PPV is defined as
(1  )R
R  R+?
(1)
The PPV depends on the type 1 and 2 error rates
(? and  ) and the ratio of true relations over null
relations in the field (R) (Ioannidis, 2005).
R. The likelihood that a research finding is true
depends on the ratio of true relations over null re-
lations in the field, usually denoted R (Ioannidis,
2005). Out of the systems that researchers in the
field would test out (not rejecting them a priori),
how many of them are better than the current state
of the art? The a priori likelihood of a relation be-
ing true, i.e., a new system being better than state
of the art, is R/(R+1). Note that while the space
of reasonably motivated methods may seem big to
researchers in the field, there is often more than
one method that is better than the current state of
the art. Obviously, as the state of the art improves,
R drops. On the other hand, if R becomes very
low, researchers are likely to move on to new ap-
plications where R is higher.
The type 1 error rate (?) is also known as the
false positive rate, or the likelihood to accept a
non-significant result. Since our experiments are
fully automated and deterministic, and precision
usually high, the type 1 error rate is low in NLP.
What is not always appreciated in the field is that
this should lead us to expect true effects to be
highly significant with very low p-values, much
like in physics. The type 2 error rate ( ) is the
false negative rate, i.e., the likelihood that a true
relation is never found. This factors into the recall
of our experimental set-ups.
So what values should we use to estimate PPV?
Our estimate for R (how often reasonable hy-
potheses lead to improvements over state of the
art) is around 0.1. This is based on a sociolog-
ical rather than an ontological argument. With
? = 0.05 and R = 0.1, researchers get positive
results inR+(1 R)? cases, i.e.,? 1/7 cases. If
researchers needed to test more than 7 approaches
to ?hit the nail?, they would never get to write pa-
pers. With ? = 0.05, and   set to 0.5, we find that
the probability of a research finding being true ?
given there is no selection bias and with perfectly
valid metrics ? is just 50%:
PPV =
(1  )R
R  R+?
=
0.5?0.1
0.1 0.05+0.05
=
0.05
0.1
= 0.5
(2)
In other words, if researchers do a perfect experi-
ment and report p < 0.05, the chance of that find-
ing being true is the chance of seeing tail when
flipping a coin. With p < 0.01, the chance is 5/6,
i.e., the chance of not getting a 3 when rolling a
die. Of course these parameters are somewhat ar-
bitrary. Figure 7 shows PPV for various values of
?.
In the experiments in Section 3, we consistently
used the standard p-value cut-off of 0.05. How-
ever, our experiments have shown that significance
results at this threshold are unreliable and very
sensitive to the choice of sample size, covariates,
or metrics. Based on the curves in Figure 7, we
7
Figure 7: PPV for different ? (horizontal line is PPV for p = 0.05, vertical line is ? for PPV=0.95).
could propose a p-value cut-off at p < 0.0025.
This is the cut-off that ? in the absence of bias and
with perfect metrics ? gives us the level of con-
fidence we expect as a research community, i.e.,
PPV = 0.95. Significance results would thus be
more reliable and reduce type 1 error.
5 Discussion
Incidentally, the p < 0.0025 cut-off also leads to
a 95% chance of seeing the same effect on held-
out test data in Berg-Kirkpatrick et al. (2012) (see
their Table 1, first row). The caveat is that this
holds only in the absence of bias and with perfect
metrics. In reality, though, our data sets are of-
ten severely biased (Berg-Kirkpatrick et al., 2012;
S?gaard, 2013), and our metrics are far from per-
fect (Papineni et al., 2002; Lin, 2004; Banerjee
and Lavie, 2005; Schwartz et al., 2011; Tsarfaty et
al., 2012). Here, we discuss how to address these
challenges.
Selection bias. The WSJ FALLACY (Section
1) has been widely discussed in the NLP litera-
ture (Blitzer et al., 2006; Daume III, 2007; Jiang
and Zhai, 2007; Plank and van Noord, 2011). But
if our test data is biased, how do we test whether
System A performs better than System B in gen-
eral? S?gaard (2013) suggests to predict signif-
icance across data sets. This only assumes that
data sets are randomly chosen, e.g., not all from
newswire corpora. This is also standard practice in
the machine learning community (Demsar, 2006).
Poor metrics. For tasks such as POS tagging
and dependency parsing, our metrics are subopti-
mal (Manning, 2011; Schwartz et al., 2011; Tsar-
faty et al., 2012). System A and System B may
perform equally well as measured by some met-
ric, but contribute very differently to downstream
tasks. Elming et al. (2013) show how parsers
trained on different annotation schemes lead to
very different downstream results. This suggests
that being wrong with respect to a gold standard,
e.g., choosing NP analysis over a ?correct? DP
analysis, may in some cases lead to better down-
stream performance. See the discussion in Man-
ning (2011) for POS tagging. One simple ap-
proach to this problem is to report results across
available metrics. If System A improves over Sys-
tem B wrt. most metrics, we obtain significance
against the odds. POS taggers and dependency
parsers should also be evaluated by their impact
on downstream performance, but of course down-
stream tasks may also introduce multiple metrics.
6 Conclusion
In sum, we have shown that significance results
with current research standards are unreliable, and
we have provided a more adequate p-value cut-off
under the assumption of perfect metrics and unbi-
8
ased data. In the cases where these assumptions
cannot be met, we suggest reporting significance
results across datasets wrt. all available metrics.
Acknowledgements
We would like to thank the anonymous review-
ers, as well as Jakob Elming, Matthias Gondan,
and Natalie Schluter for invaluable comments and
feedback. This research is funded by the ERC
Starting Grant LOWLANDS No. 313695.
References
Satanjeev Banerjee and Alon Lavie. 2005. ME-
TEOR: an automatic metric for MT evaluation with
improved correlation with human judgments. In
ACL Workshop on Intrinsic and Extrinsic Evalua-
tion Measures for MT and/or Summarization.
Taylor Berg-Kirkpatrick, David Burkett, and Dan
Klein. 2012. An empirical investigation of statis-
tical significance in nlp. In EMNLP.
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In EMNLP.
Rens Bod. 2000. Parsing with the shortest derivation.
In COLING.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
Shared Task on Multilingual Dependency Parsing.
In CoNLL.
Katherine Button, John Ioannidis, Claire Mokrysz,
Brian Nosek, Jonathan Flint, Emma Robinson, and
Marcus Munafo. 2013. Power failure: why small
sample size undermines the reliability of neuro-
science. Nature Reviews Neuroscience, 14:365?376.
Wenliang Chen, Youzheng Wu, and Hitoshi Isahara.
2008. Learning Reliable Information for Depen-
dency Parsing Adaptation. In COLING.
Hai Leong Chieu and Hwee Tou Ng. 2003. Named en-
tity recognition with a maximum entropy approach.
In CoNLL.
Jonathan H. Clark, Chris Dyer, Alon Lavie, and
Noah A. Smith. 2011. Better hypothesis testing for
statistical machine translation: controlling for opti-
mizer instability. In ACL.
Hal Daume III. 2007. Frustratingly easy domain adap-
tation. In ACL.
Janez Demsar. 2006. Statistical comparisons of clas-
sifiers over multiple data sets. Journal of Machine
Learning Research, 7:1?30.
Pascal Denis and Beno??t Sagot. 2009. Coupling an
annotated corpus and a morphosyntactic lexicon for
state-of-the-art pos tagging with less human effort.
In PACLIC.
Leon Derczynski and Kalina Bontcheva. 2014.
Passive-aggressive sequence labeling with discrim-
inative post-editing for recognising person entities
in tweets. In EACL.
Bradley Efron and Robert Tibshirani. 1993. An intro-
duction to the bootstrap. Chapman & Hall, Boca
Raton, FL.
Jakob Elming, Anders Johannsen, Sigrid Klerke,
Emanuele Lapponi, Hector Martinez Alonso, and
Anders S?gaard. 2013. Down-stream effects of
tree-to-dependency conversions. In NAACL.
Radu Florian, Abe Ittycheriah, Hongyan Jing, and
Tong Zhang. 2003. Named entity recognition
through classifier combination. In CoNLL.
Jennifer Foster, Ozlem Cetinoglu, Joachim Wagner,
Josef Le Roux, Joakim Nivre, Deirde Hogan, and
Josef van Genabith. 2011. From news to comments:
Resources and benchmarks for parsing the language
of Web 2.0. In IJCNLP.
John Ioannidis. 2005. Why most published research
findings are false. PLoS Medicine, 2(8):696?701.
Jing Jiang and ChengXiang Zhai. 2007. Instance
weighting for domain adaptation in NLP. In ACL.
Chin-Yew Lin. 2004. ROUGE: a package for auto-
matic evaluation of summaries. In WAS.
Chris Manning. 2011. Part-of-speech tagging from
97% to 100%: Is it time for some linguistics? In
CICLing.
Joakim Nivre, Johan Hall, Sandra K?ubler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007a. The CoNLL 2007 Shared Task on
Dependency Parsing. In EMNLP-CoNLL.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas
Chanev, G?ulsen Eryigit, Sandra K?ubler, Svetoslav
Marinov, and Erwin Marsi. 2007b. MaltParser:
a language-independent system for data-driven de-
pendency parsing. Natural Language Engineering,
13(2):95?135.
Eric Noreen. 1989. Computer intensive methods for
testing hypotheses. Wiley.
Kishore Papineni, Salim Roukus, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In ACL, pages
311?318, Philadelphia, Pennsylvania.
Barbara Plank and Gertjan van Noord. 2011. Effective
measures of domain similarity for parsing. In ACL.
Peter Rankel, John Conroy, Eric Slud, and Dianne
O?Leary. 2011. Ranking human and machine sum-
marization systems. In EMNLP.
9
Stefan Riezler and John Maxwell. 2005. On some pit-
falls in automatic evaluation and significance test-
ing for MT. In ACL Workshop on Intrinsic and Ex-
trinsic Evaluation Measures for Machine Transla-
tion and/or Summarization.
Laura Rimell, Stephen Clark, and Mark Steedman.
2009. Unbounded dependency recovery for parser
evaluation. In EMNLP.
Roy Schwartz, and Omri Abend, Roi Reichart, and
Ari Rappoport. 2011. Neutralizing linguisti-
cally problematic annotations in unsupervised de-
pendency parsing evaluation. In ACL.
Joseph Simmons, Leif Nelson, and Uri Simonsohn.
2011. False-positive psychology: undisclosed flexi-
bility in data collection and analysis allows present-
ing anything as significant. Psychological Science,
22(11):1359?1366.
Richard Socher, John Bauer, Chris Manning, and An-
drew Ng. 2013. Parsing with compositional vector
grammars. In ACL.
Anders S?gaard. 2013. Estimating effect size across
datasets. In NAACL.
Erik F Tjong Kim Sang and Fien De Meulder.
2003. Introduction to the conll-2003 shared task:
Language-independent named entity recognition. In
In CoNLL.
Kristina Toutanova, Dan Klein, Chris Manning, and
Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In
NAACL.
Reut Tsarfaty, Joakim Nivre, and Evelina Andersson.
2012. Cross-framework evaluation for statistical
parsing. In EACL.
Yoshimasa Tsuruoka, Yusuke Miyao, and Jun?ichi
Kazama. 2011. Learning with lookahead: can
history-based models rival globally optimized mod-
els? In CoNLL.
Shulamit Umansky-Pesin, Roi Reichart, and Ari Rap-
poport. 2010. A multi-domain web-based algorithm
for POS tagging of unknown words. In COLING.
Alexander Yeh. 2000. More accurate tests for the sta-
tistical significance of result differences. In ACL.
10
