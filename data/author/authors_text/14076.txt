Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 206?209,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
JU: A Supervised Approach to Identify Semantic Relations from Paired 
Nominals 
 
Santanu Pal         Partha Pakray             Dipankar Das          Sivaji Bandyopadhyay
Department of Computer Science & Engineering, Jadavpur University, Kolkata, India 
santanupersonal1@gmail.com,parthapakray@gmail.com,     
dipankar.dipnil2005@gmail.com,sivaji_cse_ju@yahoo.com 
Abstract 
This article presents the experiments carried 
out at Jadavpur University as part of the 
participation in Multi-Way Classification of 
Semantic Relations between Pairs of Nomi-
nals in the SemEval 2010 exercise. Separate 
rules for each type of the relations are iden-
tified in the baseline model based on the 
verbs and prepositions present in the seg-
ment between each pair of nominals. Inclu-
sion of WordNet features associated with 
the paired nominals play an important role 
in distinguishing the relations from each 
other. The Conditional Random Field (CRF) 
based machine-learning framework is 
adopted for classifying the pair of nominals.  
Application of dependency relations, 
Named Entities (NE) and various types of 
WordNet features along with several com-
binations of these features help to improve 
the performance of the system. Error analy-
sis suggests that the performance can be im-
proved by applying suitable strategies to 
differentiate each paired nominal in an al-
ready identified relation. Evaluation result 
gives an overall macro-averaged F1 score of 
52.16%.     
1 Introduction 
Semantic Relations describe the relations between 
concepts or meanings that are crucial but hard to 
identify. The present shared task aims to develop 
the systems for automatically recognizing semantic 
relations between pairs of nominals. Nine relations 
such as Cause-Effect, Instrument-Agency, Product-
Producer, Content-Container, Entity-Origin, En-
tity-Destination, Component-Whole, Member-
Collection and Message-Topic are given for Se-
mEval-2010 Task #8 (Hendrix et al, 2010). The 
relation that does not belong to any of the nine re-
lations is tagged as Other. The first five relations 
also featured in the previous SemEval-2007 Task 
#4.  
The present paper describes the approach of 
identifying semantic relations between pair of 
nominals. The baseline system is developed based 
on the verbs and prepositions present in the senten-
tial segment between the two nominals. Some 
WordNet (Miller, 1990) features are also used in 
the baseline for extracting the relation specific at-
tributes (e.g. Content type hypernym feature used 
for extracting the relation of Content-Container). 
The performance of the baseline system is limited 
due to the consideration of only the verb and 
preposition words in between the two nominals 
along with a small set of WordNet features. Hence, 
the Conditional Random Field (CRF) (McCallum 
et al, 2001) based framework is considered to ac-
complish the present task. The incorporation of 
different lexical features (e.g. WordNet hyponyms, 
Common-parents, distance), Named Entities (NE) 
and syntactic features (direct or transitive depend-
ency relations of parsing) has noticeably improved 
the performance of the system. It is observed that 
nominalization feature plays an effective role for 
identifying as well as distinguishing the relations. 
The test set containing 2717 sentences is evaluated 
against four different training sets. Some of the 
relations, e.g. Cause-Effect, Member-Collection 
perform well in comparison to other relations in all 
the four test results. Reviewing of the confusion 
matrices suggests that the system performance can 
be improved by reducing the errors that occur in 
distinguishing the two individual nominals in each 
relation. 
The rest of the paper is organized as follows. 
The pre-processing of resources and the baseline 
system are described in Section 2 and Section 3 
respectively. Development of CRF-based model is 
discussed in Section 4. Experimental results along 
206
with error analysis are specified in Section 5. Fi-
nally Section 6 concludes the paper. 
2 Resource Pre-Processing 
The annotated training corpus containing 8000 sen-
tences was made available by the respective task 
organizers. The objective is to evaluate the effec-
tiveness of the system in terms of identifying se-
mantic relations between pair of nominals. The 
rule-based baseline system is evaluated against the 
whole training corpus. But, for in-house experi-
ments regarding CRF based framework, the devel-
opment data is prepared by randomly selecting 500 
sentences from the 8000 training sentences. Rest 
7500 sentences are used for training of the CRF-
model. The format of one example entry in training 
file is as follows.  
"The system as described above has its greatest 
application in an arrayed <e1>configuration</e1> 
of antenna <e2>elements</e2>."  
Component-Whole (e2, e1)  
Comment: Not a collection: there is structure 
here, organisation. 
    Each of the training sentences is annotated by 
the paired nominals tagged as <e1> and <e2>. 
The relation of the paired nominals and a comment 
portion describing the detail of the input type fol-
lows the input sentence. 
The sentences are filtered and passed through 
Stanford Dependency Parser (Marneffe et al, 
2006) to identify direct as well as transitive de-
pendencies between the nominals. The direct de-
pendency is identified based on the simultaneous 
presence of both nominals, <e1> as well as <e2> 
in the same dependency relation whereas the tran-
sitive dependencies are verified if <e1> and <e2> 
are connected via one or more intermediate de-
pendency relations.  
Each of the sentences is passed through a Stan-
ford Named Entity Recognizer (NER)1 for identi-
fying the named entities. The named entities are 
the useful hints to separately identify the relations 
like Entity-Origin and Entity-Destination from 
other relations as the Origin and Destination enti-
ties are tagged by the NER frequently than other 
entities. 
Different seed lists are prepared for different 
types of verbs. For example, the lists for causal 
                                                          
1  http://nlp.stanford.edu/software/CRF-NER.shtml 
and motion verbs are developed by processing the 
XML files of English VerbNet (Kipper-Schuler, 
2005). The list of the causal and motion verbs are 
prepared by collecting the member verbs if their 
corresponding class contain the semantic type  
?CAUSE? or ?MOTION?. The other verb lists are 
prepared manually by reviewing the frequency of 
verbs in the training corpus. The WordNet stem-
mer is used to identify the root forms of the verbs.   
3 Baseline Model 
The baseline model is developed based on the 
similarity clues present in the phrasal pattern con-
taining verbs and prepositions. Different rules are 
identified separately for the nine different rela-
tions. A few WordNet features such as hypernym, 
meronym, distance and Common-Parents are 
added into the rule-based baseline model. Some of 
the relation specific rules are mentioned below. 
For example, if any of the nominals contain 
their meronym property as ?whole? and if the hy-
pernym tree for one of the nominals contains the 
word ?whole?, the relation is identified as a Com-
ponent-Whole relation.   But, the ordering of the 
nominals <e1> and <e2> is done based on the 
combination of ?has?, ?with? and ?of? with other 
word level components.  
The relations Cause-Effect, Entity-Destination 
are identified based on the causal verbs (cause, 
lead etc.) and motion verbs (go, run etc.) respec-
tively. One of the main criteria for extracting these 
relations is to verify the presence of causal and 
motion verbs in between the text segment of <e1> 
and <e2>. Different types of specific relaters (as, 
because etc.) are identified from the text segment 
as well. It is observed that such specific causal re-
laters help in distinguishing other relations from 
Cause-Effect.  
If one of the nominals is described as instrument 
type in its hypernym tree, the corresponding rela-
tion is identified as Instrument-Agency but the base 
level filtering criterion is applied if both the nomi-
nals belong to instrument type. On the other hand, 
if any of the nominals belong to the hypernym tree 
as content or container or hold type, it returns the 
relation Content-Container as a probable answer. 
Similarly, if both of them belong to the same type, 
the condition is fixed as false criterion for that par-
ticular category. The nominals identified as the 
part of collective nouns and associated with 
207
phrases like "of", "in", "from" between <e1> and 
<e2> contain the relation of Member-Collection. 
The relations e.g. Message-Topic uses seed list of 
verbs that satisfy the communication type in the 
hypernym tree and Product-Producer relation con-
cerns the hypernym feature as Product type. 
But, the identification of the proper ordering of 
the entities in the relation, i.e., whether the relation 
is valid between <e1, e2> or <e2, e1> is done by 
considering the passive sense of the sentence with 
the help of the keyword ?by? as well as by some 
passive dependency relations.  
The evaluation of the rule-based baseline sys-
tem on the 8000 training data gives an average F1-
score of 22.45%. The error analysis has shown that 
use of lexical features only is not sufficient to ana-
lyze the semantic relation between two nominals 
and the performance can be improved by adopting 
strategies for differentiating the nominals of a par-
ticular pair. 
4 CRF-based Model 
To improve the baseline system performance, 
CRF-based machine learning framework 
(McCallum et al, 2001) is considered for classify-
ing the semantic relations that exist among the or-
dered pair of nominals. Identification of appropri-
ate features plays a crucial role in any machine-
learning framework. The following features are 
identified heuristically by manually reviewing the 
corpus and based on the frequency of different 
verbs in different relations. 
? 11 WordNet features (Synset, Synonym, 
Gloss, Hyponym, Nominalization, Holo-
nym, Common-parents, WordNet distance, 
Sense ID, Sense count, Meronym) 
? Named Entities (NE) 
? Direct Dependency 
? Transitive Dependency 
? 9 separate verb list containing relation spe-
cific verbs, each for 9 different semantic 
relations  
Different singleton features and their combinations 
are generated from the training corpus. Instead of 
considering the whole sentence as an input to the 
CRF-based system, only the pairs of nominals are 
passed for classification. The previous and next 
token of the current token with respect to each of 
the relations are added in the template to identify 
their co-occurrence nature that in turn help in the 
classification process. Synsets containing synony-
mous verbs of the same and different senses are 
considered as individual features.   
4.1 Feature Analysis  
The importance of different features varies accord-
ing to the genre of the relations. For example, the 
Common-parents WordNet feature plays an effec-
tive role in identifying the Content-Container and 
Product-Producer relations. If the nominals in a 
pair share a common Sense ID and Sense Count  
then this is considered as a feature. The combina-
tion of multiple features in comparison with a sin-
gle feature generally shows a reasonable perform-
ance enhancement of the present classification sys-
tem. Evaluation on the development data for the 
various feature combinations has shown that the 
nominalization feature effectively performs for all 
the relations. WordNet distance feature is used for 
capturing the relations like Content-Container and 
Component-Whole. The direct and transitive de-
pendency syntactic features contribute in identify-
ing the relation as well as identify the ordering of 
the entities <e1> and <e2> in the relation. 
The Named-Entity (NE) relation plays an impor-
tant role in distinguishing the relations, e.g., Entity-
Origin and Entity-Destination from other relations. 
The person tagged NEs have been excluded from 
the present task as such NEs are not present in the  
Entity-Origin and Entity-Destination relations. It 
has been observed that the relation specific verbs 
supply useful clues to the training phrase for dif-
ferentiating relations among nominals.   
The system is trained on 7500 sentences and the 
evaluation is carried out on 500 development sen-
tences achieving an F1-Score of 57.56% F1-Score. 
The tuning on the development set has been carried 
out based on the performance produced by the 
individual features that effectively contains 
WordNet relations. In addition to that, the 
combination of dependency features with verb 
feature plays an contributory role on the system 
evaluation results. 
208
Table 1: Precision, Recall and F1-scores (in %) of semantic relations in (9+1) way directionality-based evaluation 
 
5 Experimental Results 
The active feature list is prepared after achieving 
the best possible F1-score of 61.82% on the devel-
opment set of 500 sentences. The final training of 
the CRF-based model is carried out on four differ-
ent sets containing 1000, 2000, 4000 and 8000 sen-
tences. These four training sets are prepared by 
extracting sentences from the beginning of the 
training corpus and the final evaluation is carried 
out on 2717 test sentences as provided by the or-
ganizers. The results on the four test sets termed as 
TD1, TD2, TD3 and TD4 are shown in Table 1. 
The error analysis is done based on the information 
present in the confusion matrices. The fewer occur-
rence of Entity-Destination (e2, e1) instance in the 
training corpus plays the negative role in identify-
ing the relation. Mainly, the strategy used for as-
signing the order among the entities, i.e., either 
<e1, e2> or <e2, e1> in the already identified re-
lations is the main cause of errors of the system. 
The Entity-Origin, Product-Producer and Mes-
sage-Topic relations suffer from overlapping prob-
lem with other relations. Each of the tested nomi-
nal pairs is tagged with more than one relation. 
But, selecting the first output tag produced by CRF 
is considered as the final relational tag for each of 
the nominal pairs. Hence, a distinguishing strategy 
needs to be adopted for fine-grained selection.  
6 Conclusion and Future Task 
In our approach to automatic classification of se-
mantic relations between nominals, the system 
achieves its best performance using the lexical fea-
ture such as nominalization of WordNet and syn-
tactic information such as dependency relations. 
These facts lead us to conclude that semantic fea-
tures from WordNet, in general, play a key role in 
the classification task. The present system aims for 
assigning class labels to discrete word level entities 
but the context feature is not taken into considera-
tion. The future task is to evaluate the performance 
of the system by capturing the context present be-
tween the pair of nominals.  
References  
Andrew McCallum, Fernando Pereira and John 
Lafferty. 2001. Conditional Random Fields: Prob-
abilistic Models for Segmenting and labeling Se-
quence Data. ICML-01, 282 ? 289. 
George A. Miller. 1990. WordNet: An on-line lexical 
database. International Journal of Lexicography, 
3(4): 235?312. 
Karin Kipper-Schuler. 2005. VerbNet. A broad-
coverage, comprehensive verb lexicon. Ph.D. thesis, 
University of Pennsylvania, Philadelphia, PA. 
Marie-Catherine de Marneffe, Bill MacCartney, and 
Christopher D. Manning. 2006. Generating Typed 
Dependency Parses from Phrase Structure Parses. 
(LREC 2006). 
Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva, 
Preslav Nakov, Diarmuid ?O S?eaghdha, Sebastian 
Padok , Marco Pennacchiotti, Lorenza Romano, Stan 
Szpakowicz. 2010. SemEval-2010 Task 8: Multi-
Way Classification of Semantic Relations Between 
Pairs of Nominals. 5th SIGLEX Workshop. 
TD1 TD2 TD3 TD4 Relations 
Prec. Recall F1 Prec. Recall F1 Prec. Recall F1 Prec. Recall F1 
Cause-Effect 76.33 65.85 70.70 78.55 65.85 71.64 79.86 68.90 73.98 79.26 72.26 75.60
Component-Whole 49.25 31.41 38.36 48.76 37.82 42.60 50.77 42.31 46.15 58.40 49.04 53.31
Content-Container 31.35 30.21 30.77 37.93 34.38 36.07 40.65 32.81 36.31 51.15 34.90 41.49
   Entity-Destination 37.58 62.67 46.98 43.43 63.36 51.53 43.09 63.01 51.18 
 
47.07 60.62 52.99
Entity-Origin 62.50 46.51 53.33 61.95 49.22 54.86 60.18 52.71 56.20 64.02 53.10 58.05
Instrument-Agency 19.46 23.08 21.11 21.18 27.56 23.96 26.43 23.72 25.00 32.48 24.36 27.84
Member-Collection 50.97 67.81 58.20 54.82 70.82 61.80 59.93 72.53 65.63 66.80 71.67 69.15
Message-Topic 41.70 41.38 41.54 50.23 42.15 45.83 52.81 46.74 49.59 57.78 49.81 53.50
Product-Producer 52.94 7.79 13.58 48.94 9.96 16.55 59.09 16.88 26.26 53.17 29.00 37.54
Other 21.10 27.09 23.72 24.48 33.70 28.36 26.28 37.44 30.88 26.64 42.07 32.62
Average F1 score 42.62 44.98 47.81 52.16 
209
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 571?574,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
JU_CSE_NLP: Multi-grade Classification of Semantic Similarity  
Between Text Pairs 
 
    Snehasis Neogi1, Partha Pakray2, Sivaji Bandyopadhyay1            Alexander Gelbukh 
                           1Computer Science & Engineering Department                        Center for Computing Research 
                           Jadavpur University, Kolkata, India                                 National Polytechnic Institute         
                  2Computer Science & Engineering Department                               Mexico City, Mexico 
                           Jadavpur University, Kolkata, India                                 gelbukh@gelbukh.com 
                        Intern at Xerox Research Centre Europe 
                                          Grenoble, France 
                  {snehasis1981,parthapakray}@gmail.com 
            sbandyopadhyay@cse.jdvu.ac.in 
 
Abstract 
This article presents the experiments car-
ried out at Jadavpur University as part of 
the participation in Semantic Textual Si-
milarity (STS) of Task 6 @ Semantic 
Evaluation Exercises (SemEval-2012). 
Task-6 of SemEval- 2012 focused on se-
mantic relations of text pair. Task-6 pro-
vides five different text pair files to 
compare different semantic relations and 
judge these relations through a similarity 
and confidence score. Similarity score is 
one kind of multi way classification in the 
form of grade between 0 to 5. We have 
submitted one run for the STS task. Our 
system has two basic modules - one deals 
with lexical relations and another deals 
with dependency based syntactic relations 
of the text pair. Similarity score given to a 
pair is the average of the scores of the 
above-mentioned modules. The scores 
from each module are identified using rule 
based techniques. The Pearson Correlation 
of our system in the task is 0.3880. 
1 Introduction 
Task-61 [1] of SemEval-2012 deals with seman-
tic similarity of text pairs. The task is to find the 
similarity between the sentences in the text pair 
(s1 and s2) and return a similarity score and an 
optional confidence score. There are five datasets 
                                                          
1 http://www.cs.york.ac.uk/semeval-2012/task6/ 
in the test data and with tab separated text pairs. 
The datasets are as follows: 
 
? MSR-Paraphrase, Microsoft Research Pa-
raphrase Corpus (750 pairs of sentences.) 
? MSR-Video, Microsoft Research Video De-
scription Corpus (750 pairs of sentences.) 
? SMTeuroparl: WMT2008 development data-
set (Europarl section) (459 pairs of sen-
tences.)  
? SMTnews: news conversation sentence pairs 
from WMT.(399 pairs of sentences.) 
? OnWN: pairs of sentences where the first 
comes from Ontonotes and the second from a 
WordNet definition. (750 pairs of sentences.) 
 
Similarity score ranges from 0 to 5 and confi-
dence score from 0 to 100. An s1-s2 pair gets a 
similarity score of 5 if they are completely 
equivalent. Similarity score 4 is allocated for 
mostly equivalent s1-s2 pair. Similarly, score 3 is 
allocated for roughly equivalent pair. Score 2, 1 
and 0 are allocated for non-equivalent details 
sharing, non-equivalent topic sharing and totally 
different pairs respectively. Major challenge of 
this task is to find the similarity score based simi-
larity for the text pair. Generally text entailment 
tasks refer whether sentence pairs are entailed or 
not: binary classification (YES, NO) [2] or multi-
classification (Forward, Backward, bidirectional 
or no entailment) [3][4]. But multi grade classifi-
cation of semantic similarity assigns a score to 
the sentence pair. Our system considers lexical 
and dependency based syntactic measures for 
semantic similarity. Similarity scores are the ba-
sic average of these module scores. A subsequent 
571
section describes the system architecture. Section 
2 describes JU_NLP_CSE system for STS task. 
Section 3 describes evaluation and experimental 
results. Conclusions are drawn in Section 4.  
2 System Architecture  
The system of Semantic textual similarity task 
has two main modules: one is lexical module and 
another one is dependency parsing based syntac-
tic module. Both these module have some pre-
processing tasks such as stop word removal, co-
reference resolution and dependency parsing etc. 
Figure 1 displays the architecture of the system.   
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1: System Architecture 
2.1 Pre-processing Module 
The system separates the s1-s2 sentence pairs 
contained in the different STS task datasets. 
These separated pairs are then passed through the 
following sub modules: 
i. Stop word Removal: Stop words are removed 
from s1 - s2 sentence pairs. 
ii. Co-reference: Co-reference resolutions are 
carried out on the datasets before passing through 
the TE module. The objective is to increase the 
score of the entailment percentage. A word or 
phrase in the sentence is used to refer to an entity 
introduced earlier or later in the discourse and 
both having same things then they have the same 
referent or co reference. When the reader must 
look back to the previous context, reference is 
called "Anaphoric Reference". When the reader 
must look forward, it is termed "Cataphoric Ref-
erence". To address this problem we used a tool 
called JavaRAP2 (A java based implementation 
of Anaphora Procedure (RAP) - an algorithm by 
Lappin and Leass (1994)). 
iii. Dependency Parsing: Separated s1 ? s2 sen-
tences are parsed using Stanford dependency 
parser3 to produce the dependency relations in 
the texts. These dependency relations are used 
for WordNet based syntactic matching.     
2.2 Lexical Matching Module 
In this module the TE system calculates different 
matching scores such as N ? Gram match, Text 
Similarity, Chunk match, Named Entity match 
and POS match.  
 
i. N-Gram Match module: The N-Gram match 
basically measures the percentage match of the 
unigram, bigram and trigram of hypothesis 
present in the corresponding text. These scores 
are simply combined to get an overall N ? Gram 
matching score for a particular pair.  
 
ii. Chunk Match module: In this sub module 
our system evaluates the key NP-chunks of both 
text (s1) and hypothesis (s2) using NP Chunker 
v1.13 (The University of Sheffield). The hypo-
thesis NP chunks are matched in the text NP 
chunks. System calculates an overall value for 
the chunk matching, i.e., number of text NP 
chunks that match the hypothesis NP chunks. If 
the chunks are not similar in their surface form 
then our system goes for wordnet synonyms 
matching for the words and if they match in 
wordnet synsets information, it will be encoun-
tered as a similar chunk. WordNet [5] is one of 
most important resource for lexical analysis. The 
WordNet 2.0 has been used for WordNet based 
chunk matching. The API for WordNet Search-
ing (JAWS)4 is an API that provides Java appli-
cations with the ability to retrieve data from the 
WordNet synsets. 
iii. Text Similarity Module: System takes into 
consideration several text similarities calculated 
                                                          
2 http://aye.comp.nus.edu.sg/~qiu/NLPTools/JavaRAP.html 
3 http://www.dcs.shef.ac.uk/~mark/phd/software/ 
4 http://lyle.smu.edu/~tspell/jaws/index.html 
 
 
572
over the s1-s2 pair. These text similarity values 
are summed up to produce a total score for a par-
ticular s1-s2 pair. Major Text similarity measures 
that our system considers are: 
 
? Cosine Similarity 
? Lavenstine Distance 
? Euclidean Distance 
? MongeElkan Distance 
? NeedlemanWunch Distance 
? SmithWaterman Distance 
? Block Distance 
? Jaro Similarity 
? MatchingCoefficient Distance 
? Dice Similarity 
? OverlapCoefficient 
? QGrams Distance 
 
iv. Named Entity Matching: It is based on the 
detection and matching of Named Entities in the 
s1-s2 pair. Stanford Named Entity Recognizer5 is 
used to tag the named entities in both s1 and s2. 
System simply maps the number of hypothesis 
(s2) NEs present in the text (s1). A score is allo-
cated for the matching. 
 
NE_match = (Number of common NEs in Text 
and Hypothesis) / (Number of NE in Hypothesis). 
 
v. Part ?of ? Speech (POS) Matching: This 
module basically deals with matching the com-
mon POS tags between s1 and s2 sentences. 
Stanford POS tagger6 is used to tag the part of 
speech in both s1 and s2. System matches the 
verb and noun POS words in the hypothesis that 
match in the text. A score is allocated based on 
the number of POS matching. 
 
POS_match = (Number of common verb and 
noun POS in Text and Hypothesis) / (Total num-
ber of verb and noun POS in hypothesis). 
 
System calculates the sum of the entire sub mod-
ule (modules described in section 2.2) scores and 
forms a single percentage score for the lexical 
matching. This score is then compared with some 
predetermined threshold value to assign a final 
lexical score for each pair. If percentage value is 
                                                          
5 http://nlp.stanford.edu/software/CRF-NER.shtml 
6 http://nlp.stanford.edu/software/tagger.shtml 
above 0.80 then lexical score 5 is allocated. If the 
value is between 0.60 to 0.80 then lexical score 4 
is allocated. Similarly, lexical score 3 is allocated 
for percentage score of 0.40 to 0.60 and so on. 
One lexical score is finally generated for each 
text pair.     
2.3. Syntactic Matching Module: 
TE system considers the preprocessed dependen-
cy parsed text pairs (s1 ? s2) and goes for word 
net based matching technique. After parsing the 
sentences, they have some attributes like subject, 
object, verb, auxiliaries and prepositions tagged 
by the dependency parser tag set. System uses 
these attributes for the matching procedure and 
depending on the nature of matching a score is 
allocated to the s1-s2 pair. Matching procedure is 
basically done through comparison of the follow-
ing features that are present in both the text and 
the hypothesis.    
? Subject ? Subject comparison. 
? Verb ? Verb Comparison. 
? Subject ? Verbs Comparison. 
? Object ? Object Comparison. 
? Cross Subject ? Object Comparison. 
? Object ? Verbs Comparison. 
? Prepositional phrase comparison. 
 
Each of these comparisons produces one match-
ing score for the s1-s2 pair that are finally com-
bined with previously generated lexical score to 
generate the final similarity score by taking sim-
ple average of lexical and syntactic matching 
scores. The basic heuristics are as follows: 
(i) If the feature of the text (s1) directly matches 
the same feature of the hypothesis (s2), matching 
score 5 is allocated for the text pair. 
(ii) If the feature of either text (s1) or hypothesis 
(s2) matches with the wordnet synsets of the cor-
responding text (s1) or hypothesis (s2), matching 
score 4 is allocated.     
(iii) If wordnet synsets of the feature of the text 
(s1) match with one of the synsets of the feature 
of the hypothesis (s2), matching score 3 is given 
to the pair. 
(iv) If wordnet synsets of the feature of either 
text (s1) or hypothesis (s2) match with the syn-
sets of the corresponding text (s1) or hypothesis 
(s2) then matching score 2 is allocated for the 
pair. 
573
(v) Similarly if in both the cases match occurs in 
the second level of wordnet synsets, matching 
score 1is allocated. 
(vi) Matching score 0 is allocated for the pair 
having no match in their features. 
After execution of the module, system generates 
some scores. Lexical module generates one lexi-
cal score and wordnet based syntactic matching 
module generates seven matching scores. At the 
final stage of the system all these scores are 
combined and the mean is evaluated on this 
combined score. This mean gives the similarity 
score for a particular s1-s2 pair of different data-
sets of STS task. Optional confidence score is 
also allocated which is basically the similarity 
score multiplied by 10, i.e., if the similarity score 
is 5.22, the confidence score will be 52.2.     
3. Experiments on Dataset and Result  
We have submitted one run in SemEval-2012 
Task 6. The results for Run on STS Test set are 
shown in Table 1. 
 
task6-JU_CSE_NLP-
Semantic_Syntactic_Approach 
Correlations 
ALL    0.3880 
ALLnrm 0.6706 
Mean 0.4111 
MSRpar  0.3427 
MSRvid 0.3549 
SMT-eur 0.4271 
On-WN 0.5298 
SMT-news 0.4034 
Table 1: Results of Test Set 
ALL: Pearson correlation with the gold standard 
for the five datasets and the corresponding rank 
82. 
ALLnrm: Pearson correlation after the system 
outputs for each dataset are fitted to the gold 
standard using least squares and the correspond-
ing rank 86. 
Mean: Weighted mean across the 5 datasets, 
where the weight depends on the number of pairs 
in the dataset and the corresponding rank 76. 
The subsequent rows show the pearson correla-
tion scores for each of the individual datasets. 
 
4. Conclusion 
Our JU_CSE_NLP system for the STS task 
mainly focus on lexical and syntactic approaches. 
There are some limitations in the lexical match-
ing module that shows a correlation that is not 
higher in the range. In case of simple sentences 
lexical matching is helpful for entailment but for 
complex and compound sentences the lexical 
matching module loses its accuracy. Semantic 
graph matching or conceptual graph implementa-
tion can improve the system. That is not consi-
dered in our present work. Machine learning 
tools can be used to learn the system based on the 
features. It can also improve the correlation. In 
future work our system will include semantic 
graph matching and a machine-learning module.  
Acknowledgments 
The work was done under support of the DST 
India-CONACYT Mexico project ?Answer Vali-
dation through Textual Entailment? funded by 
DST, Government of India.  
References  
[1] Eneko Agirre, Daniel Cer, Mona Diab and Aitor 
Gonzalez.  SemEval-2012 Task 6: A Pilot on Se-
mantic Textual Similarity. In Proceedings of the 
6th International Workshop on Semantic Evalua-
tion (SemEval 2012), in conjunction with the First 
Joint Conference on Lexical and Computational 
Semantics (*SEM 2012). (2012) 
[2] Dagan, I., Glickman, O., Magnini, B.: The 
PASCAL Recognising Textual Entailment Chal-
lenge. Proceedings of the First PASCAL Recogniz-
ing Textual Entailment Workshop. (2005). 
[3] H. Shima, H. Kanayama, C.-W. Lee, C.-J. Lin,T. 
Mitamura, S. S. Y. Miyao, and K. Takeda. Over-
view of ntcir-9 rite: Recognizing inference in text. 
In NTCIR-9 Proceedings,2011. 
[4] Pakray, P., Neogi, S., Bandyopadhyay, S., Gel-
bukh, A.: A Textual Entailment System using Web 
based Machine Translation System. NTCIR-9, Na-
tional Center of Sciences, Tokyo, Japan. December 
6-9, 2011. (2011). 
[5] Fellbaum, C.: WordNet: An Electronic Lexical 
Database. MIT Press (1998). 
574
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 689?695,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
JU_CSE_NLP: Language Independent Cross-lingual  
Textual Entailment System 
 
Snehasis Neogi1, Partha Pakray2, Sivaji Bandyopadhyay1, 
Alexander Gelbukh3 
1
Computer Science & Engineering Department 
Jadavpur University, Kolkata, India 
2
Computer Science & Engineering Department 
Jadavpur University, Kolkata, India 
Intern at Xerox Research Centre Europe 
Grenoble, France 
3
Center for Computing Research 
National Polytechnic Institute 
Mexico City, Mexico 
{snehasis1981,parthapakray}@gmail.com 
sbandyopadhyay@cse.jdvu.ac.in 
gelbukh@gelbukh.com 
 
Abstract 
This article presents the experiments car-
ried out at Jadavpur University as part of 
the participation in Cross-lingual Textual 
Entailment for Content Synchronization 
(CLTE) of task 8 @ Semantic Evaluation 
Exercises (SemEval-2012). The work ex-
plores cross-lingual textual entailment as a 
relation between two texts in different lan-
guages and proposes different measures 
for entailment decision in a four way clas-
sification tasks (forward, backward, bidi-
rectional and no-entailment). We set up 
different heuristics and measures for eva-
luating the entailment between two texts 
based on lexical relations. Experiments 
have been carried out with both the text 
and hypothesis converted to the same lan-
guage using the Microsoft Bing translation 
system. The entailment system considers 
Named Entity, Noun Chunks, Part of 
speech, N-Gram and some text similarity 
measures of the text pair to decide the en-
tailment judgments. Rules have been de-
veloped to encounter the multi way 
entailment issue. Our system decides on 
the entailment judgment after comparing 
the entailment scores for the text pairs. 
Four different rules have been developed 
for the four different classes of entailment. 
The best run is submitted for Italian ? 
English language with accuracy 0.326. 
1 Introduction 
Textual Entailment (TE) (Dagan and Glick-
man, 2004) is one of the recent challenges of 
Natural Language Processing (NLP). The Task 
8 of SemEval-20121 [1] defines a textual en-
tailment system that specifies two major as-
pects: the task is based on cross-lingual 
corpora and the entailment decision must be 
four ways. Given a pair of topically related text 
fragments (T1 and T2) in different languages, 
the CLTE task consists of automatically anno-
tating it with one of the following entailment 
judgments: 
i. Bidirectional (T1 ->T2 & T1 <- T2): the two 
fragments entail each other (semantic equiva-
lence)  
ii. Forward (T1 -> T2 & T1!<- T2): unidirec-
tional  entailment from T1 to T2 . 
iii. Backward (T1! -> T2 & T1 <- T2): unidirec-
tional entailment from T2 to T1.  
iv. No Entailment (T1! -> T2 & T1! <- T2): 
there is no entailment between T1 and T2. 
CLTE (Cross Lingual Textual Entailment) task 
consists of 1,000 CLTE dataset pairs (500 for 
                                                          
1http://www.cs.york.ac.uk/semeval2012/index.php?id=tasks 
689
training and 500 for test) available for the fol-
lowing language combinations: 
     - Spanish/English (spa-eng)  
     - German/English (deu-eng). 
     - Italian/English (ita-eng)  
     - French/English (fra-eng) 
 
Seven Recognizing Textual Entailment (RTE) 
evaluation tracks have already been held: RTE-1 
in 2005 [2], RTE-2 [3] in 2006, RTE-3 [4] in 
2007, RTE-4 [5] in 2008, RTE-5 [6] in 2009, 
RTE-6 [7] in 2010 and RTE-7 [8] in 2011. RTE 
task produces a generic framework for entail-
ment task across NLP applications. The RTE 
challenges have moved from 2 ? way entailment 
task (YES, NO) to 3 ? way task (YES, NO, 
UNKNOWN). EVALITA/IRTE [9] task is simi-
lar to the RTE challenge for the Italian language. 
So far, TE has been applied only in a monolin-
gual setting. Cross-lingual Textual Entailment 
(CLTE) has been proposed ([10], [11], [12]) as 
an extension of Textual Entailment. In 2010, 
Parser Training and Evaluation using Textual 
Entailment [13] was organized by SemEval-2. 
Recognizing Inference in Text (RITE)2 orga-
nized by NTCIR-9 in 2011 is the first to expand 
TE as a 5-way entailment task (forward, back-
ward, bi-directional, contradiction and indepen-
dent) in a monolingual scenario [14].  
We have participated in RTE-5 [15], RTE-6 
[16], RTE-7 [17], SemEval-2 Parser Training 
and Evaluation using Textual Entailment Task 
and RITE [18]. 
Section 2 describes our Cross-lingual Textual 
Entailment system. The various experiments 
carried out on the development and test data sets 
are described in Section 3 along with the results. 
The conclusions are drawn in Section 4. 
2 System Architecture  
Our system for CLTE task is based on a set of 
heuristics that assigns entailment scores to a text 
pair based on lexical relations. The text and the 
hypothesis in a text pair are translated to the 
same language using the Microsoft Bing ma-
chine translation system. The system separates 
the text pairs (T1 and T2) available in different 
languages and preprocesses them. After prepro-
                                                          
2 http://artigas.lti.cs.cmu.edu/rite/Main_Page 
cessing we have used several techniques such as 
Word Overlaps, Named Entity matching, Chunk 
matching, POS matching to evaluate the sepa-
rated text pairs. These modules return a set of 
score statistics, which helps the system to go for 
multi-class entailment decision based on the 
predefined rules. We have submitted 3 runs for 
each language pair for the CLTE task and there 
are some minor differences in the architectures 
that constitute the 3 runs. The three system ar-
chitectures are described in section 2.1, section 
2.2 and section 2.3. 
2.1 System Architecture 1: CLTE Task 
with  Translated English Text  
The system architecture of Cross-lingual textual 
entailment consists of various components such 
as Preprocessing Module, Lexical Similarity 
Module, Text Similarity Module. Lexical Simi-
larity module again is divided into subsequent 
modules like POS matching, Chunk matching 
and Named Entity matching. Our system calcu-
lates these measures twice once considering T1 
as text and T2 as hypothesis and once T2 as text 
and T1 as hypothesis. The mapping is done in 
both directions T1-to-T2 and T2-to-T1 to arrive 
at the appropriate four way entailment decision 
using a set of rules. Each of these modules is 
now being described in subsequent subsections. 
Figure 1 shows our system architecture where 
the text sentence is translated to English. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1: System Architecture 
 
CLTE Task Data 
(T1, T2) 
T1.txt 
T2.txt 
 
Translated in 
Eng.  Using Bing 
Translator 
Preprocessing 
(Stop word removal, 
Co referencing) 
 
N-Gram Module 
Preprocessing 
(Stop word removal, 
Co referencing) 
Chunking Module 
Text Similarity Module Named Entity POS Module 
? Lexical Score (S1) 
S1 
? Lexical Score (S2) 
S1 
If (S1>S2) Then Entailment = ?forward? 
If (S1<S2) Then Entailment = ?backward? 
If (S1=S2) or (abs (S1-S2) <Threshold) Then Entailment = ?bidirectional? 
(fra, ita, deu, 
spa language) 
T1- Text 
T2- Hypothesis 
 
T1 ? Hypothesis 
T2 - Text 
 
If (S1=S2 and (S1=S2) <Threshold) Then Entailment = ?no_entailment? 
(English 
language) 
 
690
2.1.1 Preprocessing Module 
The system separates the T1 and T2 pair from 
the CLTE task data. T1 sentences are in differ-
ent languages (In French, Italian, German and 
Spanish) where as T2 sentences are in English. 
Microsoft Bing translator3 API for Bing transla-
tor (microsoft-translator-java-api-0.4-jar-with-
dependencies.jar) is being used to translate the 
T1 text sentences into English. The translated 
T1 and T2 sentences are passed through the two 
sub modules. 
i. Stop word Removal: Stop words are removed 
from the T1 and T2 sentences. 
ii. Co-reference: Co?reference chains are eva-
luated for the datasets before passing them to the 
TE module. The objective is to increase the en-
tailment score after substituting the anaphors 
with their antecedents. A word or phrase in the 
sentence is used to refer to an entity introduced 
earlier or later in the discourse and both having 
same things then they have the same referent or 
co-reference. When the reader must look back to 
the previous context, co-reference is called 
"Anaphoric Reference". When the reader must 
look forward, it is termed "Cataphoric Refer-
ence". To address this problem we used a tool 
called JavaRAP4 (A java based implementation 
of Anaphora Procedure (RAP) - an algorithm by 
Lappin and Leass (1994)). It has been observed 
that the presence of co ? referential expressions 
are very small in sentence based paradigm.   
2.1.2 Lexical Based Textual Entailment 
(TE) Module 
T1 - T2 pairs are the inputs to the system. The 
TE module is executed once by considering T1 
as text and T2 as hypothesis and again by consi-
dering T2 as text and T1 as hypothesis. The 
overall TE module is a collection of several lex-
ical based sub modules.  
i. N-Gram Match module: The N-Gram match 
basically measures the percentage match of the 
unigram, bigram and trigram of hypothesis 
present in the corresponding text. These scores 
are simply combined to get an overall N ? Gram 
matching score for a particular pair. By running 
                                                          
3 http://code.google.com/p/microsoft-translator-java-api/ 
4 http://aye.comp.nus.edu.sg/~qiu/NLPTools/JavaRAP.html 
the module we get two scores, one for T1-T2 
pair and another for T2-T1 pair. 
       
ii. Chunk Similarity module: In this sub mod-
ule our system evaluates the key NP-chunks of 
both text and hypothesis identified using NP 
Chunker v1.15. Then our system checks the 
presence of NP-Chunks of hypothesis in the cor-
responding text. System calculates the overall 
value for the chunk matching, i.e., number of 
text NP-chunks that match with hypothesis NP-
chunks. If the chunks are not similar in their sur-
face form then our system goes for WordNet 
matching for the words and if they match in 
WordNet synsets information, the chunks are 
considered as similar. 
WordNet [19] is one of most important resource 
for lexical analysis. The WordNet 2.0 has been 
used for WordNet based chunk matching. The 
API for WordNet Searching (JAWS)6 is an API 
that provides Java applications with the ability 
to retrieve data from the WordNet database. Let 
us consider the following example taken from 
training data: 
 
T1: Due/JJ to/TO [an/DT error/NN of/IN com-
munication/NN] between/IN [the/DT police/NN] 
? 
T2: On/IN [Tuesday/NNP] [a/DT failed/VBN 
communication/NN] between/IN? 
 
The chunk in T1 [error communication] matches 
with T2 [failed communication] via WordNet 
based synsets information. A weight is assigned 
to the score depending upon the nature of chunk 
matching. 
 
 
 
                   M[i] = Wm[i] * ? / Wc[i] 
Where N= Total number of chunk containing 
hypothesis. 
M[i] = Match Score of the ith  Chunk. 
Wm[i] = Number of words matched in the i
th 
chunk. 
Wc[i] = Total words in the i
th chunk. 
                    1 if surface word matches. 
and ? = 
                ? if matche via WordNet 
                                                          
5 http://www.dcs.shef.ac.uk/~mark/phd/software/ 
6 http://lyle.smu.edu/~tspell/jaws/index.html 
691
System takes into consideration several text si-
milarity measures calculated over the T1-T2 
pair. These text similarity measures are summed 
up to produce a total score for a particular text 
pair. Similar to the Lexical module, text simi-
larity module is also executed for both T1-T2 
and T2-T1 pairs.   
iii. Text Distance Module: The following major 
text similarity measures have been considered 
by our system. The text similarity measure 
scores are added to generate the final text dis-
tance score. 
 
?   Cosine Similarity 
?   Levenshtein Distance 
?   Euclidean Distance 
?   MongeElkan Distance 
?   NeedlemanWunch Distance 
?   SmithWaterman Distance 
?   Block Distance 
?   Jaro Similarity 
?   MatchingCoefficient Similarity 
?   Dice Similarity 
?   OverlapCoefficient 
?   QGrams Distance 
 
iv. Named Entity Matching: It is based on the 
detection and matching of Named Entities in the 
T1-T2 pair. Stanford Named Entity Recognizer7 
(NER) is used to tag the Named Entities in both 
T1 and T2. System simply matches the number 
of hypothesis NEs present in the text. A score is 
allocated for the matching. 
NE_match = (Number of common NEs in Text 
and Hypothesis)/(Number of NEs in Hypothe-
sis). 
v. Part-of-Speech (POS) Matching: This mod-
ule basically deals with matching the common 
POS tags between T1 and T2 pair. Stanford POS 
tagger8 is used to tag the part of speech in both 
T1 and T2. System matches the verb and noun 
POS words in the hypothesis that match in the 
text. A score is allocated based on the number of 
POS matching.  
 
POS_match = (Number of verb and noun                            
POS in Text and Hypothesis)/(Total number of 
verb and noun POS in hypothesis).    
                                                          
7 http://nlp.stanford.edu/software/CRF-NER.shtml 
8 http://nlp.stanford.edu/software/tagger.shtml 
System adds all the lexical matching scores to 
evaluate the total score for a particular T1- T2 
pair, i.e.,  
    Pair1:  (T1 ? Text and T2 ? Hypothesis) 
    Pair2:   (T1 ? Hypothesis and T2 - Text). 
Total lexical score for each pair can be mathe-
matically represented by: 
 
 
where S1 represents the score for the pair with 
T1 as text and T2 as hypothesis while S2 
represents the score from T1 to T2. The figure 2 
shows the sample output values of the TE mod-
ule. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 2: output values of this module 
 
The system finally compares the above two val-
ues S1 and S2 as obtained from the lexical mod-
ule to go for four-class entailment decision. If 
score S1, i.e., the mapping score with T1 as text 
and T2 as hypothesis is greater than the score 
S2, i.e., mapping score with T2 as text and T1 as 
hypothesis, then the entailment class will be 
?forward?. Similarly if S1 is less than S2, i.e., 
T2 now acts as the text and T1 acts as the hypo-
thesis then the entailment class will be ?back-
ward?. Similarly if both the scores S1 and S2 are 
equal the entailment class will be ?bidirectional? 
(entails in both directions). Measuring ?bidirec-
tional? entailment is much more difficult than 
any other entailment decision due to combina-
tions of different lexical scores. As our system 
produces a final score (S1 and S2) that is basi-
cally the sum over different similarity measures, 
 
692
the tendency of identical S1 ? S2 will be quite 
small. As a result we establish another heuristic 
for ?bidirectional? class. If the absolute value 
difference between S1 and S2 is below the thre-
shold value, our system recognizes the pair as 
?bidirectional? (abs (S1 ? S2) < threshold). This 
threshold has been set as 5 based on observation 
from the training file. If the individual scores S1 
and S2 are below a certain threshold, again set 
based on the observation in the training file, then 
system concludes the entailment class as 
?no_entailment?. This threshold has been set as 
20 based on observation from the training file. 
2.2 System Architecture 2: CLTE Task 
with translated hypothesis  
System Architecture 2 is based on lexical match-
ing between the text pairs (T1, T2) and basically 
measures the same attributes as in the architec-
ture 1. In this architecture, the English hypothe-
sis sentences are translated to the language of 
the text sentence (French, Italian, Spanish and 
German) using the Microsoft Bing Translator. 
The CLTE dataset is preprocessed after separat-
ing the (T1, T2) pairs. Preprocessing module 
includes stop word removal and co-referencing. 
After preprocessing, the system executes the TE 
module for lexical matching between the text 
pairs. This module comprises N-Gram matching, 
Text Similarity, Named Entity Matching, POS 
matching and Chunking. The TE module is ex-
ecuted once with T1 as text and T2 as hypothe-
sis and again with T1 as hypothesis and T2 as 
text. But in this architecture N-Gram matching 
and text similarity modules differ from the pre-
vious architecture. In system architecture 1, the 
N-Gram matching and text similarity values are 
calculated on the English text translated from T1 
(i.e., Text in Spanish, German, French and Ital-
ian languages). In system architecture 2, the Mi-
crosoft Bing translator is used to translate T2 
texts (in English) to different languages (i.e. in 
Spanish, German, French and Italian) and calcu-
late N ? Gram matching and Text Similarity 
values on these (T1 ? newly translated T2) pairs. 
Other lexical sub modules are executed as be-
fore. These lexical matching scores are stored 
and compared according to the heuristic defined 
in section 2.1.    
2.3 System Architecture 3: CLTE task 
using Voting 
The system considers the output of the previous 
two systems (Run 1 from System architecture 1 
and Run 2 from System architecture 2) as input. 
If the entailment decision of both the runs agrees 
then this is output as the final entailment label. 
Otherwise, if they do not agree, the final entail-
ment label will be ?no_entailment?. The voting 
rule can be defined as the ANDing rule where 
logical AND operation of the two inputs are 
considered to arrive at the final evaluation class. 
3 Experiments on Datasets and Results   
Three runs (Run 1, Run 2 and Run 3) for each 
language were submitted for the SemEval-3 
Task 8. The descriptions of submissions for the 
CLTE task are as follows: 
 
? Run1: Lexical matching between text pairs 
(Based on system Architecture ? 1). 
? Run2: Lexical matching between text pairs  
    (Based on System Architecture ? 2). 
? Run3: ANDing Module between Run1 and  
          Run2. (Based on System Architecture ?3). 
 
The CLTE dataset consists of 500 training 
CLTE pairs and 500 test CLTE pairs. The re-
sults for Run 1, Run 2 and Run 3 for each lan-
guage on CLTE Development set are shown in 
Table 1.  
 
Run Name Accuracy 
JU-CSE-NLP_deu-eng_run1 0.284 
JU-CSE-NLP_deu-eng_run2 0.268 
JU-CSE-NLP_deu-eng_run3 0.270 
JU-CSE-NLP_fra-eng_run1 0.290 
JU-CSE-NLP_fra-eng_run2 0.320 
JU-CSE-NLP_fra-eng_run3 0.278 
JU-CSE-NLP_ita-eng_run1 0.302 
JU-CSE-NLP_ita-eng_run2 0.298 
JU-CSE-NLP_ita-eng_run3 0.298 
JU-CSE-NLP_spa-eng_run1 0.270 
JU-CSE-NLP_spa-eng_run2 0.262 
JU-CSE-NLP_spa-eng_run3 0.262 
 
Table 1: Results on Development set 
 
693
The comparison of the runs for different lan-
guages shows that in case of deu-eng language 
pair system architecture ? 1 is useful for devel-
opment data whereas system architecture ? 2 is 
more accurate for test data. For fra-eng language 
pair, system architecture - 2 is more accurate for 
development data whereas voting helps to get 
more accurate results for test data. Similar to the 
deu-eng language pair, ita-eng language pair 
shows same trends, i.e., system architecture ? 1 
is more helpful for development data and system 
architecture ? 2 is more accurate for test data. In 
case of spa-eng language pair system architec-
ture ? 1 is helpful for both development and test 
data. 
 
The results for Run 1, Run 2 and Run 3 for each 
language on CLTE Test set are shown in Table 
2. 
 
Run Name Accuracy 
JU-CSE-NLP_deu-eng_run1 0.262 
JU-CSE-NLP_deu-eng_run2 0.296 
JU-CSE-NLP_deu-eng_run3 0.264 
JU-CSE-NLP_fra-eng_run1 0.288 
JU-CSE-NLP_fra-eng_run2 0.294 
JU-CSE-NLP_fra-eng_run3 0.296 
JU-CSE-NLP_ita-eng_run1 0.316 
JU-CSE-NLP_ita-eng_run2 0.326 
JU-CSE-NLP_ita-eng_run3 0.314 
JU-CSE-NLP_spa-eng_run1 0.274 
JU-CSE-NLP_spa-eng_run2 0.266 
JU-CSE-NLP_spa-eng_run3 0.272 
 
Table 2: Results on Test Set 
4 Conclusions and Future Works 
We have participated in Task 8 of Semeval-2012 
named Cross Lingual Textual Entailment mainly 
based on lexical matching and translation of text 
and hypothesis sentences in the cross lingual 
corpora. Both lexical matching and translation 
have their limitations. Lexical matching is useful 
for simple sentences but fails to retain high ac-
curacy for complex sentences with number of 
clauses. Semantic graph matching or conceptual 
graph is a good substitution to overcome these 
limitations. Machine learning technique is 
another important tool for multi-class entailment 
task. Features can be trained by some machine 
learning tools (such as SVM, Na?ve Bayes or 
Decision tree etc.) with multi-way entailment 
(forward, backward, bi-directional, no-
entailment) as its class. Works have been started 
in these directions. 
Acknowledgments 
The work was carried out under partial support 
of the DST India-CONACYT Mexico project 
?Answer Validation through Textual Entail-
ment? funded by DST, Government of India and 
partial support of the project CLIA Phase II 
(Cross Lingual Information Access) funded by 
DIT, Government of India. 
References  
[1] Negri, M., Marchetti, A., Mehdad, Y., Bentivogli, 
L., and Giampiccolo, D.: Semeval-2012 Task 8: 
Cross-lingual Textual Entailment for Content Syn-
chronization. In Proceedings of the 6th International 
Workshop on Semantic Evaluation (SemEval 2012). 
[2]  Dagan, I., Glickman, O., Magnini, B.: The 
PASCAL Recognising Textual Entailment Chal-
lenge. Proceedings of the First PASCAL Recog-
nizing Textual Entailment Workshop. (2005). 
[3] Bar-Haim, R., Dagan, I., Dolan, B., Ferro, L., 
Giampiccolo, D., Magnini, B., Szpektor, I.: The-
Seond PASCAL Recognising Textual Entailment 
Challenge. Proceedings of the Second PASCAL 
Challenges Workshop on Recognising Textual En-
tailment, Venice, Italy (2006). 
[4] Giampiccolo, D., Magnini, B., Dagan, I., Dolan, 
B.: The Third PASCAL Recognizing Textual En-
tailment Challenge. In Proceedings of the ACL-
PASCAL Workshop on Textual Entailment and 
Paraphrasing, Prague, Czech Republic. (2007). 
[5] Giampiccolo, D., Dang, H. T., Magnini, B., Da-
gan, I., Cabrio, E.: The Fourth PASCAL Recogniz-
ing Textual Entailment Challenge. In TAC 2008 
Proceedings. (2008) 
[6] Bentivogli, L., Dagan, I., Dang. H.T., Giampicco-
lo, D., Magnini, B.: The Fifth PASCAL Recogniz-
ing Textual Entailment Challenge. In TAC 2009 
Workshop, National Institute of Standards and 
Technology Gaithersburg, Maryland USA. (2009). 
[7] Luisa Bentivogli, Peter Clark, Ido Dagan, Hoa 
Trang Dang,Danilo Giampiccolo: The Sixth 
PASCAL Recognizing Textual Entailment Chal-
694
lenge. In TAC 2010 Notebook Proceedings. 
(2010) 
[8] Bentivogli, L., Clark, P., Dagan, I., Dang, H., 
Giampiccolo, D.: The Seventh PASCAL Recogniz-
ing Textual Entailment Challenge. In TAC 2011 
Notebook Proceedings. (2011) 
[9] Bos, Johan, Fabio Massimo Zanzotto, and Marco 
Pennacchiotti. 2009. Textual Entailment at 
EVALITA 2009: In Proceedings of EVALITA 
2009. 
[10] Mehdad, Yashar, Matteo Negri, and Marcello 
Federico.2010. Towards Cross-Lingual Textual 
entailment. In Proceedings of the 11th Annual 
Conference of the North American Chapter of the 
Association for Computational Linguistics, 
NAACL-HLT 2010. LA, USA. 
[11] Negri, Matteo, and Yashar Mehdad. 2010. 
Creating a Bilingual Entailment Corpus through 
Translations with Mechanical Turk: $100 for a 
10-day Rush. In Proceedings of the NAACL-HLT 
2010, Creating Speech and Text Language Data 
With Amazon's Mechanical Turk Workshop. LA, 
USA. 
[12] Mehdad, Yashar, Matteo Negri, Marcello Fede-
rico. 2011. Using Bilingual Parallel Corpora for 
Cross-Lingual Textual Entailment. In Proceedings 
of ACL 2011. 
[13] Yuret, D., Han, A., Turgut, Z.: SemEval-2010 
Task 12: Parser Evaluation using Textual Entail-
ments. Proceedings of the SemEval-2010 Evalua-
tion Exercises on Semantic Evaluation. (2010).  
 
[14] H. Shima, H. Kanayama, C.-W. Lee, C.-J. Lin,T. 
Mitamura, S. S. Y. Miyao, and K. Takeda. Over-
view of ntcir-9 rite: Recognizing inference in text. 
In NTCIR-9 Proceedings,2011. 
[15]  Pakray, P., Bandyopadhyay, S., Gelbukh, A.: 
Lexical based two-way RTE System at RTE-5. Sys-
tem Report, TAC RTE Notebook. (2009) 
 
[16] Pakray, P., Pal, S., Poria, S., Bandyopadhyay, S., 
, Gelbukh, A.: JU_CSE_TAC: Textual Entailment 
Recognition System at TAC RTE-6. System Re-
port, Text Analysis Conference Recognizing Tex-
tual Entailment Track (TAC RTE) Notebook. 
(2010) 
 
[17] Pakray, P., Neogi, S., Bhaskar, P., Poria, S., 
Bandyopadhyay, S., Gelbukh, A.: A Textual En-
tailment System using Anaphora Resolution. Sys-
tem Report. Text Analysis Conference 
Recognizing Textual Entailment Track Notebook, 
November 14-15. (2011) 
 
[18] Pakray, P., Neogi, S., Bandyopadhyay, S., Gel-
bukh, A.: A Textual Entailment System using Web 
based Machine Translation System. NTCIR-9, Na-
tional Center of Sciences, Tokyo, Japan. Decem-
ber 6-9, 2011. (2011) 
 
[19]  Fellbaum, C.: WordNet: An Electronic Lexical 
Database. MIT Press (1998). 
695
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 448?453,
Dublin, Ireland, August 23-24, 2014.
NTNU: Measuring Semantic Similarity with
Sublexical Feature Representations and Soft Cardinality
Andr
?
e Lynum, Partha Pakray, Bj
?
orn Gamb
?
ack Sergio Jimenez
{andrely,parthap,gamback}@idi.ntnu.no sgjimenezv@unal.edu.co
Norwegian University of Science and Technology Universidad Nacional de Colombia
Trondheim, Norway Bogot?a, Colombia
Abstract
The paper describes the approaches taken
by the NTNU team to the SemEval 2014
Semantic Textual Similarity shared task.
The solutions combine measures based
on lexical soft cardinality and character
n-gram feature representations with lexi-
cal distance metrics from TakeLab?s base-
line system. The final NTNU system is
based on bagged support vector machine
regression over the datasets from previous
shared tasks and shows highly competi-
tive performance, being the best system on
three of the datasets and third best overall
(on weighted mean over all six datasets).
1 Introduction
The Semantic Textual Similarity (STS) shared task
aims at providing a unified framework for evaluat-
ing textual semantic similarity, ranging from ex-
act semantic equivalence to completely unrelated
texts. This is represented by the prediction of
a similarity score between two sentences, drawn
from a particular category of text, which ranges
from 0 (different topics) to 5 (exactly equivalent)
through six grades of semantic similarity (Agirre
et al., 2013). This paper describes the NTNU
submission to the SemEval 2014 STS shared task
(Task 10). The approach is based on the lexical
and distributional features of the baseline Take-
Lab system from the 2012 shared task (
?
Sari?c et al.,
2012), but improves on it in three ways: by adding
two new categories of features and by using a bag-
ging regression model to predict similarity scores.
The new feature categories added are based on
soft cardinality and character n-grams, described
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence de-
tails:http://creativecommons.org/licenses/by/4.0/
in Section 2. The parameters of the two cate-
gories are optimised over several corpora and the
features are combined through support vector re-
gression (Section 3) to create the actual systems
(Section 4). As Section 5 shows, the new mea-
sures give the baseline system a substantial boost,
leading to very competitive results in the shared
task evaluation.
2 Feature Generation Methods
The methods used for creating new features utilise
soft cardinality and character n-grams. Soft cardi-
nality (Jimenez et al., 2010) was used successfully
for the STS task in previous SemEval editions
(Jimenez et al., 2012a; Jimenez et al., 2013a).
The NTNU systems utilise an ensemble of such 18
measures, based only on surface text information,
which were extracted using soft cardinality with
different similarity functions, as further described
in Section 2.1.
Section 2.2 then introduces the similarity mea-
sures based on character n-gram feature represen-
tations, which proved themselves as the strongest
features in the STS 2013 task (Marsi et al., 2013).
The measures used here replace character n-gram
features with cluster frequencies or vector val-
ues based on the n-gram collocational structure
learned in an unsupervised manner from text data.
A variety of n-gram feature representations were
trained on subsets of Wikipedia and the best per-
forming ones were used for the new measures,
which are based on cosine similarity between the
document vectors derived from each sentence in a
given pair.
2.1 Soft Cardinality Measures
Soft cardinality resembles classical set cardinality
as it is a method for counting the number of ele-
ments in a set, but differs from it in that similarities
among elements are being considered for the ?soft
counting?. The soft cardinality of a set of words
448
A = {a
1
, a
2
, .., a
|A|
} (a sentence) is defined by:
|A|
sim
=
|A|
?
i=1
w
a
i
?
|A|
j=1
sim(a
i
, a
j
)
p
(1)
Where p is a parameter that controls the cardinal-
ity?s softness (p?s default value is 1) and w
a
i
are
weights for each word, obtained through inverse
document frequency (idf ) weighting. sim(a
i
, a
j
)
is a similarity function that compares two words
a
i
and a
j
using the symmetrized Tversky?s index
(Tversky, 1977; Jimenez et al., 2013a) represent-
ing them as sets of 3-grams of characters. That
is, a
i
= {a
i,1
, a
i,2
, ..., a
i,|a
i
|
} where a
i,n
is the n
th
character trigram in the word a
i
in A. Thus, the
proposed word-to-word similarity is given by:
sim(a
i
, a
j
)=
|c|
?(?|a
min
|+(1??)|a
max
|)+|c|
(2)
?
?
?
?
?
|c| = |a
i
? a
j
|+ bias
sim
|a
min
| = min {|a
i
\ a
j
|, |a
j
\ a
i
|}
|a
max
| = max {|a
i
\ a
j
|, |a
j
\ a
i
|}
The sim function is equivalent to the Dice?s co-
efficient if the three parameters are given their de-
fault values, namely ? = 0.5, ? = 1 and bias = 0.
The soft cardinalities of any pair of sentencesA,
B andA?B can be obtained using Eq. 1. The soft
cardinality of the intersection is approximated by
|A?B|
sim
= |A|
sim
+|B|
sim
?|A?B|
sim
. These
four basic soft cardinalities are algebraically re-
combined to produce an extended set of 18 fea-
tures as shown in Table 1. The featureSTS
sim
is a
parameterized similarity function built by reusing
at word level the symmetrized Tversky?s index
(Eq. 2), whose parameters are tuned from training
data (as further described in Subsection 3.2).
Although this method is based purely on string
matching, the soft cardinality has been shown to
be a very strong baseline for semantic textual com-
parison. The word-to-word similarity sim in Eq. 1
could be replaced by other similarity functions
based on semantic networks or any distributional
representation making this method able to capture
more complex semantic relations among words.
2.2 Sublexical Feature Representations
We have created a set of similarity measures based
on induced representations of character n-grams.
The measures are based on similarity between
STS
sim
(|A|?|A?B|)
/|A|
|A|
(|A|?|A?B|)
/|A?B|
|B|
|B|
/|A?B|
|A ?B|
(|B|?|A?B|)
/|B|
|A ?B|
(|B|?|A?B|)
/|A?B|
|A| ? |A ?B|
|A?B|
/|A|
|B| ? |A ?B|
|A?B|
/|B|
|A ?B| ? |A ?B|
|A?B|
/|A?B|
|A|
/|A?B|
(|A?B|?|A?B|)
/|A?B|
NB: in this table only, | ? | is short for | ? |
sim
Table 1: Soft cardinality features.
document vectors, here the centroid of the individ-
ual term vector representations, which are trained
on character n-grams rather than full words. The
vector representations are induced in an unsuper-
vised manner from large unannotated corpora us-
ing word clustering, topic learning and word rep-
resentation learning methods.
In this paper, three different methods have
been used for creating the character n-gram fea-
ture representations: Brown Clusters (Brown et
al., 1992), Latent Semantic Indexing (LSI) topics
(Deerwester et al., 1990), and log linear skip-gram
models (Mikolov et al., 2013). The Brown clusters
were trained using the implementation by Liang
(2005), while the LSI topic vectors and log linear
skip-gram representations were trained using the
Gensim topic modelling framework (
?
Reh?u?rek and
Sojka, 2010). In addition, tf-idf (Term-Frequency
Inverse Document Frequency) weighting was used
when training LSI topic models. We used a cosine
distance measure between document vectors con-
sisting of the centroid of the term representation
vectors. For Brown clusters, the normalized term
frequency vectors were used with the cluster IDs
instead of the terms themselves. For LSI topic rep-
resentations, the tf-idf weighted topic mixture for
each term was used as the term representation. For
the log linear skip-grams, the word representations
were extracted from the model weight matrix.
3 Feature and Parameter Optimisation
The extracted features and the parameters for the
two methods described in the previous section
were optimised over several sets of training data.
As no training data was explicitly provided for the
STS evaluation campaign this year, we used dif-
ferent training sets from past campaigns and from
Wikipedia for the new test sets.
449
Test set Training set
deft-forum
MSRvid 2012 train and test +
OnWN 2012 and 2013 test
deft-news MSRvid 2012 train + test
headlines headlines 2013 test
images MSRvid 2012 train + test
OnWN OnWN 2012 and 2013 test
tweet-news
SMTeuroparl 2012 test +
SMTnews 2012 test
Table 2: Training-test set pairs.
3.1 Training Data and Pre-processing
The training-test sets pairs used for optimising the
parameters of the soft cardinality methods were
selected from the STS 2012 and STS 2013 task,
as shown in Table 2. The character n-gram repre-
sentation vectors were trained in an unsupervised
manner on two subsets of Wikipedia consisting,
respectively, of the first 12 million words (10
8
characters, hence referred to as Wiki8) and of 125
million words (10
9
characters; Wiki9).
First, however, the training data had to be pre-
processed. Thus, before extracting the idf weights
and the soft cardinality features, all the texts
shown in Table 2 were passed through the follow-
ing four pre-processing steps:
(i) tokenization and stop-word removal (pro-
vided by NLTK, Bird et al. (2009)),
1
(ii) conversion to lowercase characters,
(iii) punctuation and special character removal
(e.g., ?.?, ?;?, ?$?, ?&?), and
(iv) Porter stemming.
Character n-grams including whitespace were
generated from the Wikipedia texts, which in con-
trast only were pre-processed in a 3-step chain:
(i) removal of punctuation and extra whites-
pace,
(ii) replacing numbers with their single digit
word (?one?, ?two?, etc.), and
(iii) lowercasing all text.
1
http://www.nltk.org/
Data ? ? bias p ?
?
?
?
bias
?
deft-forum 1.01 -1.01 0.24 0.93 -2.71 0.42 1.63
deft-news 3.36 -0.64 1.37 0.44 2.36 0.72 0.02
headlines 0.36 -0.29 4.17 0.85 -4.50 0.43 0.19
images 1.12 -1.11 0.93 0.64 -0.98 0.50 0.11
OnWN 0.53 -0.53 1.01 1.00 -4.89 0.52 0.46
tweet-news 0.13 0.14 2.80 0.01 2.66 1.74 0.45
Table 3: Optimal parameters used for each dataset.
3.2 Soft Cardinality Parameter Optimisation
The first feature in Table 1, STS
sim
, was used to
optimise the four parameters ?, ?, bias, and p in
the following way. First, we built a text similarity
function reusing Eq. 2 for comparing two sets of
words (instead of two sets of character 3-grams)
and replacing the classic cardinality |?| by the soft
cardinality | ? |
sim
from Eq. 1. This text similarity
function adds three parameters (?
?
, ?
?
, and bias
?
)
to the initial four parameter set (?, ?, bias, and p).
Second, these seven parameters were set to their
default values and the scores obtained from this
function for each pair of sentences were compared
to the gold standards in the training data using
Pearson?s correlation. The parameter search space
was then explored iteratively using hill-climbing
until reaching optimal Pearson?s correlation. The
criterion for assignment of training-test set pairs
was by closeness of average character length. The
optimal training parameters are shown in Table 3.
3.3 Parameters for N-gram Feature Training
The character n-gram feature representation vec-
tors were trained while varying the parameters of
n-gram size, cluster size, and term frequency cut-
offs for all models. For the log linear skip-gram
models, our intuition is that a larger skip-gram
context is needed than the 5 or 10 wide skip-grams
used to train word-based representations due to the
smaller term vocabulary and dependency between
adjacent n-grams, so instead we trained models us-
ing skip-gram widths of 25 or 50 terms. Term fre-
quency cut-offs were set to limit the model size,
but also potentially serve as a regularization on
the resulting measure. In detail, the following sub-
lexical representation measures are used:
? Log linear skip-gram representations of char-
acter 3- and 4-grams of size 1000 and 2000,
respectively. Trained on the Wiki8 corpus us-
ing a skip gram window of size 25 and 50,
and frequency cut-off of 5.
450
? Brown clusters with size 1024 of character 4-
grams using a frequency cut-off of 20.
? Brown clusters of character 3-, 4- and 5-
grams with cluster sizes of resp. 1024, 2048
and 1024. The representations are trained on
the Wiki9 corpus with successively increas-
ing frequency cut-offs of 20, 320 and 1200.
? LSI topic vectors based on character 4-grams
of size 2000. Trained on the Wiki8 corpus
using a frequency cut-off of 5.
? LSI topic vectors based on character 4-grams
of size 1000. Trained on the Wiki9 corpus
using a frequency cut-off of 80.
3.4 Similarity Score Regression
The final sentence pair similarity score is predicted
by a Support Vector Regression (SVR) model with
a Radial Basis (RBF) kernel (Vapnik et al., 1997).
The model is trained on all the test data for the
2013 STS shared task combined with all the trial
and test data of the 2012 STS shared task.
The combined dataset hence consists of about
7,500 sentence pairs from nine different text cat-
egories: five sets from the annotated data sup-
plied to STS 2012, based on Microsoft Research
Paraphrase and Video description corpora (MSR-
par and MSvid), statistical machine translation
system output (SMTeuroparl and SMTnews), and
sense mappings between OntoNotes and WordNet
(OnWN); and four sets from the STS 2013 test
data: headlines (news headlines), SMT, OnWN,
and FNWM (mappings of sense definitions from
FrameNet and WordNet).
The SVR model was trained as a bagged classi-
fier, that is, for each run, 100 regression models
were trained with 80% of the samples and fea-
tures of the original training set drawn with re-
placement. The outputs of all models were then
averaged into a final prediction. This bagged train-
ing procedure adds extra regularization, which can
reduce the instability of prediction accuracy be-
tween different test data categories.
The prediction pipeline was implemented with
the Scikit-learn software framework (Pedregosa et
al., 2011), and the SVR models were trained with
the implementation?s default parameters: cost
penalty (C) 1.0, margin () 0.1, and RBF precision
(?) 1/|featurecount|.
We were unable to improve the performance
over these defaults by cross validation parameter
search unless the models were trained for specific
text categories. Consequently no parameter opti-
mization was performed during training of the fi-
nal systems.
4 Submitted Systems
The three submitted systems consist of one us-
ing only the soft cardinality features described in
Section 3.2 (NTNU-run1), one system using a
baseline set of lexical measures and WordNet aug-
mented similarity in addition to the new sublexical
representation measures (NTNU-run2), and one
(NTNU-run3) which combines the output from
the other two systems by taking the mean of the
two sets of predictions. NTNU-run3 thus repre-
sents a combination of the measures and methods
introduced by NTNU-run1 and NTNU-run2.
In addition to the sublexical feature measures
described in Section 3.3, NTNU-run2 uses the fol-
lowing baseline features adapted from the Take-
Lab 2012 system submission (
?
Sari?c et al., 2012).
? Simple lexical features: Relative document
length differences, number overlap, case
overlap, and stock symbol named entity
recognition.
? Lemma and word n-gram overlap of orders 1-
3, frequency weighted lemma and word over-
lap, and WordNet augmented overlap.
? Cosine similarity between the summed word
representation vectors from each sentence us-
ing LSI models based on large corpora with
or without frequency weighting.
The specific measures used in the submitted
systems were found by training the regression
model on the STS 2012 shared task data and eval-
uating on the STS 2013 test data. We used a step-
wise forward feature selection method by compar-
ing mean (but unweighted) correlation on the four
test categories in order to identify the subset of
measures to include in the final system.
The system composes a feature set of similar-
ity scores from these 20 baseline measures and the
nine sublexical representation measures, and uses
these to train a bagged SVM regressor as described
in Section 3.4 in order to predict the final semantic
similarity score for new sentence pairs.
451
NTNU-run1 NTNU-run2 NTNU-run3 Best
Dataset r rank r rank r rank r
deft-forum 0.4369 16 0.5084 2 0.5305 1 0.5305
deft-news 0.7138 14 0.7656 6 0.7813 2 0.7850
headlines 0.7219 17 0.7525 13 0.7837 1 0.7837
images 0.8000 9 0.8129 4 0.8343 1 0.8343
OnWN 0.8348 7 0.7767 20 0.8502 4 0.8745
tweet-news 0.4109 33 0.7921 1 0.6755 13 0.7921
mean 0.6531 20 0.7347 4 0.7426 2 0.7429
weighted mean 0.6631 21 0.7491 4 0.7549 3 0.7610
Table 4: Final evaluation results for the submitted systems.
5 Results and Discussion
The final evaluation results for the three submit-
ted systems are shown in Table 4, where the right-
most column (?Best?) for comparison displays the
performance figures obtained by any of the 38 sys-
tems on each dataset.
The systems using sublexical representation
based measures show competitive performance,
ranking third and fourth among the submitted sys-
tems with a weighted mean correlation of ?0.75.
They also produced the best result in four out of
the six text categories in the evaluation dataset,
with NTNU-run3 being the #1 system on deft-
forum, headlines and images, #2 on deft-news, and
#4 on OnWN. It would thus have been the clear
winner if it had not been for its sub-par perfor-
mance on the tweet-news dataset, which on the
other hand is the category NTNU-run2 was the
best of all systems on.
The system based solely on soft cardinality fea-
tures, NTNU-run1, displays more modest perfor-
mance ranking at 21
st
place (of the in total 38 sub-
mitted systems) with ?0.66 correlation. This is a
bit surprising, since this method for obtaining fea-
tures from pairs of texts was used successfully in
other SemEval tasks such as cross-lingual textual
entailment (Jimenez et al., 2012b) and student re-
sponse analysis (Jimenez et al., 2013b). Similarly,
Croce et al. (2012) used soft cardinality represent-
ing text as a bag of dependencies (syntactic soft
cardinality) obtaining the best results in the typed-
similarity task (Croce et al., 2013).
From our results it can be noted that for most
categories the sublexical representation measures
show strong performance in NTNU-run2, with a
significantly better result for the combined sys-
tem NTNU-run3. This indicates that while the soft
cardinality features are weaker predictors overall,
they are complimentary to the sublexical and lex-
ical features of NTNU-run2. It is also indicative
that this is not the case for the tweet-news cate-
gory, where the text is more ?free form? and less
normative, so it would be expected that sublexical
approaches should have stronger performance.
Acknowledgements
This work was made possible with the support
from Department of Computer and Information
Science, Norwegian University of Science and
Technology.
Partha Pakray was 2013?2014 supported by an
ERCIM Alain Bensoussan Fellowship.
The NTNU systems are partly based on code
made available by the Text Analysis and Knowl-
edge Engineering Laboratory, Department of
Electronics, Microelectronics, Computer and In-
telligent Systems, Faculty of Electrical Engineer-
ing and Computing, University of Zagreb.
452
References
Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-
Agirre, and Weiwei Guo. 2013. *SEM 2013 shared
task: Semantic textual similarity. In Second Joint
Conference on Lexical and Computational Seman-
tics (*SEM), Volume 1: Proceedings of the Main
Conference and the Shared Task: Semantic Textual
Similarity, pages 32?43, Atlanta, Georgia, USA,
June.
Steven Bird, Ewan Klein, and Edward Loper.
2009. Natural Language Processing with Python.
O?Reilly Media, Inc.
Peter F Brown, Peter V Desouza, Robert L Mercer,
Vincent J Della Pietra, and Jenifer C Lai. 1992.
Class-based n-gram models of natural language.
Computational linguistics, 18(4):467?479.
Danilo Croce, Valerio Storch, P. Annesi, and Roberto
Basili. 2012. Distributional compositional seman-
tics and text similarity. In 2012 IEEE Sixth Interna-
tional Conference on Semantic Computing (ICSC),
pages 242?249, September.
Danilo Croce, Valerio Storch, and Roberto Basili.
2013. UNITOR-CORE TYPED: Combining text
similarity and semantic filters through SV regres-
sion. In Second Joint Conference on Lexical and
Computational Semantics (*SEM), Volume 1: Pro-
ceedings of the Main Conference and the Shared
Task: Semantic Textual Similarity, pages 59?65, At-
lanta, Georgia, USA, June.
Scott C. Deerwester, Susan T Dumais, Thomas K. Lan-
dauer, George W. Furnas, and Richard A. Harshman.
1990. Indexing by latent semantic analysis. JASIS,
41(6):391?407.
Sergio Jimenez, Fabio Gonzalez, and Alexander Gel-
bukh. 2010. Text comparison using soft cardi-
nality. In Edgar Chavez and Stefano Lonardi, ed-
itors, String Processing and Information Retrieval,
volume 6393 of LNCS, pages 297?302. Springer,
Berlin, Heidelberg.
Sergio Jimenez, Claudia Becerra, and Alexander Gel-
bukh. 2012a. Soft cardinality: A parameterized
similarity function for text comparison. In Proceed-
ings of the Sixth International Workshop on Seman-
tic Evaluation (SemEval 2012), Montr?eal, Canada,
7-8 June.
Sergio Jimenez, Claudia Becerra, and Alexander Gel-
bukh. 2012b. Soft cardinality+ ML: Learning adap-
tive similarity functions for cross-lingual textual en-
tailment. In Proceedings of the Sixth International
Workshop on Semantic Evaluation (SemEval 2012),
Montr?eal, Canada, 7-8 June.
Sergio Jimenez, Claudia Becerra, and Alexander Gel-
bukh. 2013a. SOFTCARDINALITY-CORE: Im-
proving text overlap with distributional measures for
semantic textual similarity. In Second Joint Con-
ference on Lexical and Computational Semantics
(*SEM), Volume 1: Proceedings of the Main Con-
ference and the Shared Task: Semantic Textual Sim-
ilarity, Atlanta, Georgia, USA, June.
Sergio Jimenez, Claudia Becerra, and Alexander Gel-
bukh. 2013b. SOFTCARDINALITY: Hierarchical
text overlap for student response analysis. In Second
Joint Conference on Lexical and Computational Se-
mantics (*SEM), Volume 1: Proceedings of the Main
Conference and the Shared Task: Semantic Textual
Similarity, Atlanta, Georgia, USA, June.
Percy Liang. 2005. Semi-supervised learning for nat-
ural language. Ph.D. thesis, Massachusetts Institute
of Technology.
Erwin Marsi, Hans Moen, Lars Bungum, Gleb Sizov,
Bj?orn Gamb?ack, and Andr?e Lynum. 2013. NTNU-
CORE: Combining strong features for semantic sim-
ilarity. In Second Joint Conference on Lexical and
Computational Semantics (*SEM), Volume 1: Pro-
ceedings of the Main Conference and the Shared
Task: Semantic Textual Similarity, pages 66?73, At-
lanta, Georgia, USA, June.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-
sos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. 2011. Scikit-learn: Machine learn-
ing in Python. Journal of Machine Learning Re-
search, 12:2825?2830.
Radim
?
Reh?u?rek and Petr Sojka. 2010. Software
Framework for Topic Modelling with Large Cor-
pora. In Proceedings of the LREC 2010 Workshop
on New Challenges for NLP Frameworks, pages 45?
50, Valletta, Malta, May. ELRA.
Amos Tversky. 1977. Features of similarity. Psycho-
logical Review, 84(4):327?352, July.
Vladimir Vapnik, Steven E. Golowich, and Alex
Smola. 1997. Support vector method for function
approximation, regression estimation, and signal
processing. In Michael C. Mozer, Michael I. Jordan,
and Thomas Petsche, editors, Advances in Neural
Information Processing Systems, volume 9, pages
281?287. MIT Press, Cambridge, Massachusetts.
Frane
?
Sari?c, Goran Glava?s, Mladen Karan, Jan
?
Snajder,
and Bojana Dalbelo Ba?si?c. 2012. TakeLab: Sys-
tems for measuring semantic text similarity. In Pro-
ceedings of the Sixth International Workshop on Se-
mantic Evaluation (SemEval 2012), pages 441?448,
Montr?eal, Canada, 7-8 June.
453
Proceedings of the 3rd Workshop on Hybrid Approaches to Translation (HyTra) @ EACL 2014, pages 48?57,
Gothenburg, Sweden, April 27, 2014. c?2014 Association for Computational Linguistics
Automatic Building and Using Parallel Resources for SMT from 
Comparable Corpora 
Santanu Pal1, Partha Pakray2, Sudip Kumar Naskar3 
1Universit?t Des Saarlandes, Saarbr?cken, Germany 
2Computer & Information Science,  
Norwegian University of Science and Technology, Trondheim, Norway 
3Department of Computer Science & Engineering,  
Jadavpur University, Kolkata, India 
1santanu.pal@uni-saarland.de, 
2partha.pakray@idi.ntnu.no, 
3sudip.naskar@cse.jdvu.ac.in 
 
 
Abstract 
Building parallel resources for corpus 
based machine translation, especially 
Statistical Machine Translation (SMT), 
from comparable corpora has recently 
received wide attention in the field 
Machine Translation research. In this 
paper, we propose an automatic approach 
for extraction of parallel fragments from 
comparable corpora. The comparable 
corpora are collected from Wikipedia 
documents and this approach exploits the 
multilingualism of Wikipedia. The 
automatic alignment process of parallel 
text fragments uses a textual entailment 
technique and Phrase Based SMT (PB-
SMT) system.  The parallel text 
fragments extracted thus are used as 
additional parallel translation examples 
to complement the training data for a PB-
SMT system. The additional training data 
extracted from comparable corpora 
provided significant improvements in 
terms of translation quality over the 
baseline as measured by BLEU. 
1 Introduction 
Comparable corpora have recently attracted huge 
interest in natural language processing research. 
Comparable corpora are now considered as a rich 
resource for acquiring parallel resources such as 
parallel corpus or parallel text fragments,. 
Parallel text extracted from comparable corpora 
can take an important role in improving the 
quality of machine translation (MT) (Smith et al. 
2010).  Parallel text extracted from comparable 
corpora are typically added with the training 
corpus as additional training material which is 
expected to facilitate better performance of SMT 
systems specifically for low density language 
pairs. 
In the present work, we try to extract 
English?Bengali parallel fragments of text from 
comparable corpora. We have collected 
document aligned corpus of English?Bengali 
document pairs from Wikipedia which provides a 
huge collection of documents in many different 
languages. For automatic alignment of parallel 
fragments we have used two-way textual 
entailment (TE) system and a baseline SMT 
system.  
Textual entailment (TE), introduced by 
(Dagan and Glickman, 2004), is defined as a 
directional relationship between pairs of text 
expressions, denoted by the entailing text (T) and 
the entailed hypothesis (H). T entails H if the 
meaning of H can be inferred from the meaning 
of T. Textual Entailment has many applications 
in NLP tasks, such as summarization, 
information extraction, question answering, 
48
information retrieval, machine translation, etc. In 
machine translation, textual entailment can be 
applied to MT evaluation (Pado et al., 2009). A 
number of research works have been carried out 
on cross-lingual Textual entailment using MT 
(Mehdad et al.,2010; Negri et al., 2010; Neogi et 
al., 2012). However, to the best of our 
knowledge, the work presented here is the first 
attempt towards employing textual entailment for 
the purpose of extracting parallel text fragments 
from comparable corpora which in turn are used 
to improve MT system.  
Munteanu and Marcu (2006) suggested that 
comparable corpora tend to have parallel data at 
sub-sentential level. Hence, instead of finding 
sentence level parallel resource from comparable 
corpora, in the present work we mainly focus on 
finding parallel fragments of text. 
We carried out the task of automatic alignment 
of parallel fragments using three steps: (i) mining 
comparable corpora form Wikipedia, (ii) 
sentence level alignment using two-way TE and 
a baseline Bengali?English SMT system, and 
finally (iii) clustering the parallel sentence 
aligned comparable corpora using textual 
entailment and then aligning parallel fragments 
of text by textual entailment and a baseline 
Bengali?English SMT system.  
Although, we have collected document 
aligned comparable corpora, the documents in 
the corpus do not belong to any particular 
domain. Even with such a corpus we have been 
able to improve the performance of an existing 
machine translation system which was built on 
tourism domain data. This also signifies the 
contribution of this work towards domain 
adaptation of MT systems. 
The rest of the paper is organized as follows. 
Section 2 describes the related work. Section 3 
describes the mining process of the comparable 
corpora. The two-way TE system architecture is 
described in section 4. Section 5 describes the 
automatic alignment technique of parallel 
fragment of texts. Section 6 describes the tools 
and resources used for this work. The 
experiments and evaluation results are presented 
in section 7. Section 8 concludes and presents 
avenues for future work. 
2 Related Work  
Comparable corpora have been used in many 
research areas in NLP, especially in machine 
translation. Several earlier works have studied 
the use of comparable corpora in machine 
translation. However, most of these approaches 
(Fung and McKeown, 1997; Fung and Yee, 1998; 
Rapp, 1999; Chiao and Zweigenbaum, 2002; 
Dejean et al., 2002; Kaji, 2005; Otero, 2007; 
Saralegui et al., 2008; Gupta et al., 2013) are 
specifically focused on extracting word 
translations from comparable corpora. Most of 
the strategies follow a standard method based on 
the context vector similarity measure such as 
finding the target words that have the most 
similar distributions with a given source word. In 
most of the cases, a starting list contains the 
?seed expressions? and this list is required to 
build the context vectors of the words in both the 
languages. A bilingual dictionary can be used as 
a starting list. The bilingual list can also be 
prepared form parallel corpus using bilingual 
correlation method (Otero, 2007). Instead of a 
bilingual list, multilingual thesaurus could also 
be used for this purpose (Dejean, 2002).  
Wikipedia is a multilingual encyclopedia 
available in different languages and it can be 
used as a source of comparable corpora. Otero et 
al. (2010) stored the entire Wikipedia for any 
two languages and transformed it into a new 
collection: CorpusPedia. Our work shows that 
only a small ad-hoc corpus containing Wikipedia 
articles could prove to be beneficial for existing 
MT systems. 
In the NIST shared task on Recognizing 
Textual Entailment Challenge (RTE), several 
methods have been proposed to tackle the textual 
entailment problem. Most of these systems use 
some form of lexical matching, e.g., n-gram, 
word similarity, etc. and even simple word 
overlap. A number of systems represent the texts 
as parse trees (e.g., syntactic or dependency trees) 
49
before the actual task. Some of the systems use 
semantic features (e.g., logical inference, 
Semantic Role Labelling) for solving the text and 
hypothesis entailment problem. MacCartney et al. 
(2006) proposed a new architecture for textual 
inference in which finding a good alignment is 
separated from evaluating entailment. Agichtein 
et al. (2008) presented a supervised machine 
learning approach to train a classifier over a 
variety of lexical, syntactic, and semantic metrics. 
Malakasiotis (2009) used string similarity 
measures applied to shallow abstractions of the 
input sentences and a Maximum Entropy 
classifier to learn how to combine the resulting 
features.  
In the present work, we used the textual 
entailment system of Pakray et al. (2011) which 
performed well on various RTE tasks and 
datasets, as well as other NLP tasks like question 
answering, summarization, etc. We integrated a 
new module to by using reVerb 1  tool and 
optimized all the features produced by different 
modules. 
The main objective of the present work is to 
investigate whether textual entailment can be 
used to establish alignments between text 
fragments in comparable corpora and whether 
the parallel text fragments extracted thus can 
improve MT system performance. 
3 Mining Comparable Corpora 
We collected comparable corpora from 
Wikipedia - online collaborative encyclopedia 
available in a wide variety of languages. English 
Wikipedia contains largest volume of data such 
as millions of articles; there are many language 
editions with at least 100,000 articles. Wikipedia 
links articles on the same topic in different 
languages using ?interwiki? linking facility. 
Wikipedia is an enormously useful re-source for 
extracting parallel resources as the documents in 
different languages are already aligned. We first 
collect an English document from Wikipedia and 
then find the same document in Bengali if there 
                                                        
1 http://reverb.cs.washington.edu/ 
exists any inter-language link. Extracted 
English?Bengali document pairs from Wikipedia 
are already comparable since they are written 
about the same entity. Although each 
English?Bengali document pairs are comparable 
and they discuss about the same topic, most of 
the times they are not exact translation of each 
other; as a result parallel fragments of text are 
rarely found in these document pairs. The bigger 
the size of the fragment may result less probable 
parallel version will be found in the target side. 
Nevertheless, there is always chance of getting 
parallel phrase, tokens or even sentences in 
comparable documents.   
We designed a crawler to collect comparable 
corpora for English?Bengali document pairs. 
Based on an initial seed keyword list, the crawler 
first visits each English page of Wikipedia, saves 
the raw text (in HTML format), and then follows 
the cross-lingual link for each English page and 
collects the corresponding Bengali document. In 
this way, we collect English?Bengali comparable 
documents in the tourism domain. We retain only 
the textual information and all the other details 
are discarded. We extract English and Bengali 
sentences from each document. The extracted 
sentences from each English document are not 
parallel with the corresponding Bengali 
document. Moreover, Bengali documents are 
contained limited information compare to the 
English document. We align sentences of 
English?Bengali from these comparable corpora 
through a baseline PB-SMT system. A Bengali-
English baseline PB-SMT system has been 
developed which was trained on 
English?Bengali tourism domain corpus. We 
translated Bengali sentences into English. The 
translated sentence is then examined for 
entailment in the English comparable document 
by using two-way TE system proposed in section 
4. If it is more than 50% entailed with the target 
document then the target sentence is directly 
fetched form the comparable English document 
and the source-target sentence pair are saved in a 
list. In this way, we extract parallel sentences 
from comparable corpora. These parallel 
sentences except those are 100% entailed may 
50
not be completely parallel but they are 
comparable. So, we created a parallel fragment 
list which is proposed in section 5.  
4 Two-way Textual Entailment System 
A two-way automatic textual entailment (TE) 
recognition system that uses lexical, syntactic 
and semantic features has been described in this 
section. The system architecture has been shown 
in Figure 1. The TE system has used the Support 
Vector Machine (SVM) technique that uses 
thirty-one features for training purpose. In lexical 
module there are eighteen features and eleven 
features from syntactic module, one feature by 
using reVerb and one feature from semantic 
module. 
 
Fig.1 Two way TE architecture 
4.1 Lexical Module 
In this module six lexical comparisons and 
seventeen lexical distance comparisons between 
text and hypothesis has used.  
Six lexical comparisons are WordNet 
(Fellbaum, 1998) based unigram match, bigram 
match, longest common sub-sequence, skip-gram, 
stemming and named entity matching.  We have 
calculated weight from each of these six 
comparisons in equation (1). 
weight =
number - of - common - tokens - between - text - and - hypothesis?
number - of - tokens - in - hypothesis?  
(1) 
The API for WordNet Searching (JAWS) 2 
provides Java applications with the ability to 
retrieve data from the WordNet 2.1 database. 
For Named entity detection we have used Text 
Tokenization Toolkit (LT-TTT2)3 (Grover et. al., 
1999). The LT-TTT2 named entity component 
has been used.  
For lexical distance measure, we have used 
features of Vector Space Measures (Euclidean 
distance, Block distance, Minkowsky distance, 
Cosine similarity, Matching Coefficient), Set-
based Similarities (Dice, Jaccard, Overlap, 
Harmonic), Edit Distance Measures (Levenshtein 
distance, Smith-Waterman distance, Jaro 
Distance). Lexical distance measurement has 
used the libraries SimMetrics 4 , SimPack 5  and 
SecondString6. SimMetrics is a Similarity Metric 
Library, e.g., from edit distance (Levenshtein, 
Gotoh, Jaro etc) to other metrics, (e.g Soundex, 
Chapman). 
4.2 Syntactic Module  
The syntactic module compares the dependency 
relations in both hypothesis and text. The system 
extracts syntactic structures from the text-
hypothesis pairs using Combinatory Categorial 
Grammar (C&C CCG) Parser 7  and Stanford 
Parser 8  and compares the corresponding 
structures to determine if the entailment relation 
is established. Two different systems have been 
implemented one system used Stanford Parser 
output and another system used C&C CCG 
Parser. The system accepts pairs of text snippets 
(text and hypothesis) at the input and gives score 
for each comparison. Some of the important 
comparisons on the dependency structures of the 
text and the hypothesis are Subject-subject 
comparison, WordNet Based Subject-Verb 
                                                        
2 http://lyle.smu.edu/~tspell/jaws/index.html 
3 http://www.ltg.ed.ac.uk/software/lt-ttt2 
4 http://sourceforge.net/projects/simmetrics/ 
5https://files.ifi.uzh.ch/ddis/oldweb/ddis/research/simpack/in
dex.html 
6 http://sourceforge.net/projects/secondstring/ 
7 http://svn.ask.it.usyd.edu.au/trac/candc/wiki 
8 http://nlp.stanford.edu/software/lex-parser.shtml 
51
Comparison, Subject-Subject Comparison, 
Object-Verb Comparison, WordNet Based 
Object-Verb Comparison, Cross Subject-Object 
Comparison Number Comparison, Noun 
Comparison, Prepositional Phrase Comparison, 
Determiner Comparison and other relation 
Comparison.  
4.3 reVerb Module  
ReVerb 9  is a tool, which extracts binary 
relationships from English sentences.  The 
extraction format is in Table 1. 
Extraction Format arg1 rel arg2 
Example A person is playing a guitar 
reVerb Extracts arg1= {A person}  rel = {is 
playing} arg2 = {a guitar} 
 
Table 1: Example by reVerb Tool 
The system parsed the text and the hypothesis 
by reverb tool. Each of the relations compares 
between text and hypothesis and calculates a 
score for each pair. 
4.4 Semantic Module 
The semantic module based on the Universal 
Networking Language (UNL) (Uchida and Zhu, 
2001). The UNL can express information or 
knowledge in semantic network form with hyper-
nodes. The UNL is like a natural language for 
computers to represent and process human 
knowledge. There are two modules in UNL 
system - En-converter and De-converter module. 
The process of representing natural language 
sentences in UNL graphs is called En-converting 
and the process of generating natural language 
sentences out of UNL graphs is called De-
converting. An En-Converter is a language 
independent parser, which provides a framework 
for morphological, syntactic, and semantic 
analysis synchronously. The En-Converter is 
based on a word dictionary and a set of 
enconversion grammar rules. It analyses 
sentences according to the en-conversion rules. 
A De-Converter is a language independent 
                                                        
9 http://reverb.cs.washington.edu/ 
generator, which provides a framework for 
syntactic and morphological generation 
synchronously. 
An example UNL relation for a sentence 
?Pfizer is accused of murdering 11 children? is 
shown in Table 2. 
[S:00] 
{org:en} Pfizer is accused of murdering 11 children 
{/org} 
{unl} 
obj(accuse(icl>do,equ>charge,cob>abstract_thing,agt>per
son,obj>person).@entry 
.@present,pfizer.@topic) 
qua:01(child(icl>juvenile>thing).@pl,11) 
obj:01(murder(icl>kill>do,agt>thing,obj>living_thing).@
entry,child(icl>juvenile 
>thing).@pl) 
cob(accuse(icl>do,equ>charge,cob>abstract_thing,agt>per
son,obj>person).@entr 
y.@present,:01) 
{/unl}  
[/S] 
 
Table 2: Example of UNL  
The system converts the text and the 
hypothesis into UNL relations by En-Converter. 
Then it compares the UNL relations in both the 
text and the hypothesis and gives a score for each 
comparison. 
4.5 Feature Extraction Module 
The features are listed in Table 3: 
Name of Features No of features 
Lexical Module 18 
Syntactic Module 11 
reVerb Module 1 
Semantic Module 1 
 
Table 3: Features for SVM 
4.6 Support Vector Machines (SVM) 
Support Vector Machines (SVMs) 10  are 
supervised learning models used for 
classification and regression analysis. The basic 
SVM takes a set of input data and predicts, for 
                                                        
10 http://en.wikipedia.org/wiki/Support_vector_machine 
52
each given input, which of two possible classes 
form the output, making it a non-probabilistic 
binary linear classifier.   
The SVM based our Textual Entailment 
system has used the following data sets: RTE-1 
development and RTE-1 annotated test set, RTE-
2 development set and RTE-2 annotated test set, 
RTE-3 development set and RTE-3 annotated 
test set to deal with the two-way classification 
task. The system has used the LIBSVM -- A 
Library for Support Vector Machines 11  for the 
classifier to learn from this data set. 
5 Alignment of Parallel fragments using 
proposed TE system 
We have extracted parallel fragment from the 
parallel sentence aligned comparable resource 
list as well as the training data. Initially, we 
make cluster on the English side of this list with 
the help of two-way TE method. More than 50% 
entailed sentences have been considered to take a 
part of the same cluster. The TE system divides 
the complete set of comparable resources list into 
some smaller sets of cluster. Each cluster 
contains at least two English sentences. Each 
English cluster is corresponding to the set 
comparable Bengali sentences. So in this way we 
have developed a number of English Bengali 
parallel clusters. We intersect between the both 
English and Bengali sentences which are 
belonging to the same clusters.     
We try to align the English and Bengali 
fragments extracted from a parallel sentence 
aligned comparable resource list. If both sides 
contain only one fragment then the alignment is 
trivial, and we add such fragment pairs to seed 
another parallel fragment corpus that contains 
examples having only one token in both side. 
Otherwise, we establish alignments between the 
English and Bengali fragments using translation. 
If both the English and Bengali side contains n 
number of fragments, and the alignments of n-1 
fragments can be established through translation 
                                                        
11 http://www.csie.ntu.edu.tw/~cjlin/libsvm/ 
or by means of already existing alignments, then 
the nth alignment is trivial.  
These parallel fragments of text, extracted 
from the comparable corpora are added with the 
tourism domain training corpus to enhance the 
performance of the baseline PB-SMT system. 
6 Tools and Resources 
A sentence-aligned English?Bengali parallel 
corpus contains 23,492 parallel sentences from 
the travel and tourism domain has been used in 
the present work. The corpus has been collected 
from the consortium-mode project ?Development 
of English to Indian Languages Machine 
Translation (EILMT) System 12 ?. The Stanford 
Parser 13  and CRF chunker 14  (Xuan-Hieu Phan, 
2006) have been used for parsing and chunking 
in the source side of the parallel corpus, 
respectively.  
The experiments were carried out using the 
standard log-linear PB-SMT model as our 
baseline system: GIZA++ implementation of 
IBM word alignment model 4, phrase-extraction 
heuristics described in (Koehn et al., 2003), 
minimum-error-rate training (Och, 2003) on a 
held-out development set, target language model 
trained using SRILM toolkit (Stolcke, 2002) with 
Kneser-Ney smoothing (Kneser and Ney, 1995) 
and the Moses decoder (Koehn et al., 2007) have 
been used in the present study. 
7 Experiments and Results 
We randomly identified 500 sentences each for 
the development set and the test set from the 
initial parallel corpus. The rest is considered as 
the training corpus. The training corpus was 
filtered with the maximum allowable sentence 
length of 100 words and sentence length ratio of 
1:2 (either way). Finally the training corpus 
                                                        
12 The EILMT project is funded by the Department of 
Electronics and Information Technology (DEITY), Ministry 
of Communications and Information Technology (MCIT), 
Government of India. 
13 http://nlp.stanford.edu/software/lex-parser.shtml 
14 http://crfchunker.sourceforge.net/ 
53
contained 22,492 sentences. In addition to the 
target side of the parallel corpus, we used a 
monolingual Bengali corpus containing 488,026 
words from the tourism domain for building the 
target language model. Experiments were carried 
out with different n-gram settings for the 
language model and the maximum phrase length 
and it was found that a 4-gram language model 
and a maximum phrase length of 7 produce the 
optimum baseline result on both the development 
and the test set. We carried out the rest of the 
experiments using these settings. 
The collected comparable corpus consisted of 
5582 English?Bengali document pairs. It is 
evident from Table 4 that English documents are 
more informative than the Bengali documents as 
the number of sentences in English documents is 
much higher than those in the Bengali documents. 
When the Bengali fragments of texts were passed 
to the Bengali?English translation module some 
of them could not be translated into English and 
also, some of them could be translated only 
partially. Therefore, some of the tokens were 
translated while some were not. Some of those 
partially translated text fragments were aligned 
through textual entailment; however, most of 
them were discarded. As can be seen from Table 
4, 9,117 sentences were entailed in the English 
side, of which the system was able to establish 
cross-lingual entailment for 2,361 
English?Bengali sentence pairs.  
 No. of 
English 
sentence 
No. of 
Bengali 
sentence 
Extraction from 
Comparable corpora 
579037 169978 
more than 50% Entailed 
English Sentences 
9117 - 
more than 50% Entailed 
(sentence aligned 
comparable) 
2361 2361 
parallel fragment of texts 
from sentence aligned 
comparable list 
3937 3937 
 
Table 4: Statistics of the sentence aligned comparable 
list and the aligned parallel text fragments.  
Finally, the textual entailment based alignment 
procedure was able to align 3937 parallel 
fragments as reported in Table 4. Manual 
inspection of the parallel list revealed that most 
of the aligned texts were of good quality. 
We carried out evaluation of the MT quality 
using four automatic MT evaluation metrics: 
BLEU (Papineni et al., 2002), METEOR 
(Banerjee and Lavie, 2005), NIST (Doddington, 
2002) and TER (Snover et al., 2006). Table 5 
shows the performance of the PB-SMT systems 
built on the initial training corpus and the larger 
training corpus containing parallel text fragments 
extracted from the comparable corpora. Treating 
the parallel text fragments extracted from the 
comparable corpora as additional training 
material results in significant improvement in 
terms of BLEU (1.73 points, 15.84% relative) 
over the baseline system. Similar improvements 
are also obtained for the other metrics.  The low 
evaluation scores could be attributed to the fact 
that Bengali is a morphologically rich language 
and has a relatively free phrase order; besides 
there were only one set of reference translations 
for the testset. 
Experiments BLEU NIST METEOR TER 
Baseline 10.92 4.16 0.3073 75.34 
Baseline  + 
parallel 
fragments of 
texts as 
additional 
training 
material 
12.65 4.32 0.3144 73.00 
 
Table 5: Evaluation results 
8 Conclusion and Future Work 
In this paper, we have successfully extracted 
English?Bengali parallel fragments of text from 
comparable corpora using textual entailment 
techniques. The parallel text fragments extracted 
thus were able to bring significant improvements 
in the performance of an existing machine 
translation system. For low density language 
pairs, this approach can help to improve the 
state-of-art machine translation quality. A 
manual inspection on a subset of the output 
revealed that the additional training material 
54
extracted from comparable corpora effectively 
resulted in better lexical choice and less OOV 
words than the baseline output.  As the collected 
parallel text does not belong to any particular 
domain, this work also signifies that out of 
domain data is also useful to enhance the 
performance of a domain specific MT system. 
This aspect of the work would be useful for 
domain adaptation in MT. As future work, we 
would like to carry out experiments on larger 
datasets.  
Acknowledgments 
The research leading to these results has received 
funding from the EU project EXPERT ?the 
People Programme (Marie Curie Actions) of the 
European Union's Seventh Framework 
Programme FP7/2007-2013<tel:2007-2013>/ 
under REA grant agreement no. [317471]. We 
acknowledge the support from Department of 
Computer and Information Science, Norwegian 
University of Science and Technology and also 
support from ABCDE fellowship programme 
2012-1013. 
References  
Banerjee, Satanjeev and Alon Lavie. 2005. METEOR: 
An Automatic Metric for MT Evaluation with 
Improved Correlation with Human Judgments. 
Proceedings of the ACL Workshop on Intrinsic and 
Extrinsic Evaluation Measures for Machine 
Translation and/or Summarization, Ann Arbor, 
Michigan, pages 65?72. 
Chiao, Yun-Chuang and Pierre Zweigenbaum. 2002. 
Looking for candidate translational equivalents in 
specialized, comparable corpora. In Proceedings of 
the 19th international conference on Computational 
linguistics, Volume 2, Association for 
Computational Linguistics, pages 1-5. 
Dagan, Ido and Oren Glickman. 2004. Probabilistic 
textual entailment: generic applied modeling of 
language variability, In PASCAL Workshop on 
Learning Methods for Text Understanding and 
Mining, Grenoble, France. 
De Marneffe, Marie-Catherine, Bill MacCartney, 
Trond Grenager, Daniel Cer, Anna Rafferty, and 
Christopher D. Manning. 2006. Learning to 
distinguish valid textual entailments. In B. Magnini 
and I. Dagan (eds.), Proceedings of the Second 
PASCAL Recognizing Textual Entailment 
Challenge. Venice: Springer, pages 74?79. 
D?jean, Herv?, ?ric Gaussier, and Fatia Sadat. 2002. 
Bilingual terminology extraction: an approach 
based on a multilingual thesaurus applicable to 
comparable corpora. In Proceedings of the 19th 
International Conference on Computational 
Linguistics COLING, Pages 218-224. 
 Doddington, George. 2002. Automatic evaluation of 
machine translation quality using n-gram co-
occurrence statistics. In Proceedings of the second 
international conference on Human Language 
Technology Research . Morgan Kaufmann 
Publishers Inc, pages. 138-145. 
Fung, Pascale and Kathleen McKeown. 1997. Finding 
terminology translations from non-parallel corpora. 
In Proceedings of the 5th Annual Workshop on 
Very Large Corpora, pages 192-202. 
Fung, Pascale and Lo Yuen Yee. 1998. An IR 
approach for translating new words from 
nonparallel, comparable texts. In Proceedings of 
the 17th international conference on Computational 
linguistics-Volume 1, Association for 
Computational Linguistics, pages 414-420. 
Gupta, Rajdeep, Santanu Pal, and Sivaji 
Bandyopadhyay. 2013. Improving MT System 
Using Extracted Parallel Fragments of Text from 
Comparable Corpora. In proceedings of 6th 
workshop of Building and Using Comparable 
Corpora (BUCC), ACL, Sofia, Bulgaria, Pages 69-
76. 
Kneser, Reinhard and Hermann Ney. 1995. Improved 
backing-off for n-gram language modeling. In 
Proceedings of the IEEE International Conference 
on Acoustics, Speech and Signal Processing, 
volume I. pages 181-184. 
Koehn, Philipp, Hieu Hoang, Alexandra Birch, Chris 
Callison-Burch, Marcello Federico,Nicola Bertoldi, 
Brooke Cowan, Wade Shen, Christine Moran, 
Richard Zens, Chris Dyer, Ond rej Bojar, 
Alexandra Constantin, and Evan Herbst. Moses: 
open source toolkit for statistical machine 
translation. In Proceedings of the 45th Annual 
Meeting of the ACL on Interactive Poster and 
Demonstration Sessions. Association for 
Computational Linguistics, pages 177-180. 
Koehn, Philipp, Franz Josef Och, and Daniel Marcu. 
2003. Statistical phrase-based translation. In 
55
Proceedings of the 2003 Conference of the North 
American Chapter of the Association for 
Computational Linguistics on Human Language 
Technology-Volume 1, Association for 
Computational Linguistics, pages 48-54. 
Mehdad, Yashar, Matteo Negri, and Marcello 
Federico. 2010. Towards Cross-Lingual Textual 
entailment. In Proceedings of the 11th Annual 
Conference of the North American Chapter of the 
Association for Computational Linguistics,  
NAACL-HLT 2010. LA, USA.  
Munteanu,  Dragos Stefan and Daniel Marcu. 2006. 
Extracting parallel sub-sentential fragments from 
non-parallel corpora. In Proceedings of the 21st 
International Conference on Computational 
Linguistics and the 44th annual meeting of the 
Association for Computational Linguistics, 
Association for Computational Linguistics, pages 
81-88. 
Negri, Matteo, and Yashar Mehdad. 2010. Creating a 
Bilingual Entailment Corpus through Translations 
with Mechanical Turk: $100 for a 10-day Rush. In 
Proceedings of the NAACL-HLT 2010, Creating 
Speech and Text Language Data With Amazon's 
Mechanical Turk Workshop. LA, USA.  
Neogi, Snehasis, Partha Pakray, Sivaji 
Bandyopadhyay, and Alexander Gelbukh. 2012. 
JU_CSE_NLP: Language Independent Cross-
lingual Textual Entailment System. (*SEM) First 
Joint Conference on Lexical and Computational 
Semantics, Collocated with NAACL-HLT 2012, 
Montreal, Canada.  
Och, F. Josef. 2003. Minimum error rate training in 
statistical machine translation. In Proceedings of 
the 41st Annual Meeting on Association for 
Computational Linguistics-Volume 1, Association 
for Computational Linguistics, pages 160-167. 
Och, F. Josef and Herman Ney. 2000. Giza++: 
Training of statistical translation models. 
Otero, P. Gamallo. 2007. Learning bilingual lexicons 
from comparable english and spanish corpora. 
Proceedings of MT Summit xI, pages 191-198. 
Otero, P. Gamallo and Isaac Gonz?lez L?pez. 2010. 
Wikipedia as multilingual source of comparable 
corpora. In Proceedings of the 3rd Workshop on 
Building and Using Comparable Corpora, LREC, 
pages 21-25. 
 Papineni, Kishore, Salim Roukos, Todd Ward, and 
Wei-Jing Zhu. 2002. BLEU: a method for 
automatic evaluation of machine translation. In 
Proceedings of the 40th annual meeting on 
association for computational linguistics, 
Association for Computational Linguistics, pages 
311-318. 
Prodromos Malakasiotis. 2009. "AUEB at TAC 2009", 
In TAC 2009 Workshop, National Institute of 
Standards and Technology Gaithersburg, Maryland 
USA. 
Rapp, Reinhard. 1999. Automatic identification of 
word translations from unrelated English and 
German corpora. In Proceedings of the 37th annual 
meeting of the Association for Computational 
Linguistics on Computational Linguistics, 
Association for Computational Linguistics, pages 
519-526. 
Saralegui, X., San Vicente, I., and Gurrutxaga, A. 
2008. Automatic generation of bilingual lexicons 
from comparable corpora in a popular science 
domain. In LREC 2008 workshop on building and 
using comparable corpora. 
Pado, Sebastian, Michel Galley, Dan Jurafsky, and 
Christopher D. Manning. 2009. Textual entailment 
features for machine translation evaluation. In 
Proceedings of the EACL Workshop on Statistical 
Machine Translation, Athens, Greece, pages 37?41. 
 Smith, R. Jason, Chris Quirk, and Kristina Toutanova. 
2010. Extracting parallel sentences from 
comparable corpora using document level 
alignment. In Human Language Technologies: The 
2010 Annual Conference of the North American 
Chapter of the Association for Computational 
Linguistics, Association for Computational 
Linguistics, pages 403-411. 
Snover, Matthew, Bonnie Dorr, Richard Schwartz, 
Linnea Micciulla, and John Makhoul. 2006. A 
study of translation edit rate with targeted human 
annotation. Proceedings of Association for 
Machine Translation in the Americas, Cambridge, 
Massachusetts, USA, pages 223?231. 
Pakray, Partha, Snehasis Neogi, Pinaki Bhaskar, 
Soujanya Poria, Sivaji Bandyopadhyay, and 
Alexander Gelbukh. 2011. A Textual Entailment 
System using Anaphora Resolution. System Report, 
Text Analysis Conference Recognizing Textual 
Entailment Track (TAC RTE) Notebook, 
November 14-15, 2011, National Institute of 
56
Standards and Technology, Gaithersburg, 
Maryland USA 
Stolcke, Andreas. 2002. SRILM-an extensible 
language modeling toolkit. In Proceedings of the 
international conference on spoken language 
processing, Volume 2, pages 901-904. 
Wang, Rui and G?nter Neumann. 2007. Recognizing 
Textual Entailment Using Sentence Similarity 
based on Dependency Tree Skeletons. In 
Proceedings of the third PASCAL Recognising 
Textual Entailment Challenge. 
Xuan-Hieu Phan. 2006. CRFChunker: CRF English 
Phrase Chunker , http://crfchunker.sourceforge.net/. 
57
