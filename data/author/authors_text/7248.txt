Granularity Effects in Tense Translation 
Michae l  Schiehlen* 
Inst i tute ibr Comi)utat ional  Linguistics, University of Stuttgart ,  
Azenbergstr.  12, 70174 Stut tgar t  
mike~adler, ims. uni-stuttgart, de 
1 In t roduct ion  
One of the daunting problems in machine trans- 
lation (MT) is the mapping of tense. The paper 
singles out tile problem of translating German 
present ense into English. This problem seems 
particularly instructive as its solution requires 
calculation of aspect; as well as determination 
of tile temporal location of events with respect 
to the time of speech. We present a disam- 
biguation algorithm which makes use of gram 
ularity calculations to establish the scopal order 
of temporal adverbial phrases. The described 
algorithm has been implemented and is running 
in the Verbmobil system. 
The paper is organized as follows. In sections 2 
through 4 we 1)resent he problem and discuss 
the linguistic factors involved, always keeping 
an eye on their exploitation for disambiguation. 
Sections 5 and 6 are devoted to an abstract; de f  
inition of temporal granularity and a discussion 
of granularity effects on scope resolution. In sec- 
tion 7 the actual disambiguation algorithm is 
presented, while section 8 describes its perfor- 
mance on the Verbmobil test data. A summary 
closes the paper. 
2 Present  or Future?  
In contrast o English, the German present ense 
is commonly used to describe both present and 
future happenings. One task in translation from 
German to English is therefore tile dismnbigua- 
tion of German/)resent tense to present ime or 
future time. 
(1) present tense ~ future time 
-+ present ime 
* This work was fimded by the German Federal Min- 
istry of Education, Science, Research and Technology 
(BMBF) in the framework of the Verbmobil Project un- 
der Grant 01 IV 101 U. Many thanks are  due to Profi 
H. Kamp attd K. Eberle. All errors are my own. 
2.1 Temporal  Orientation 
A prominent factor involved in the choice be- 
tween present and future time (Butt, 1995) is 
the temporal orientation of tile time adverbials 
that modi\[y the tensed verb. 
Only a limited set of time adverbials can refer to 
present ime. Indeed, the set is so small that it 
can be enumerated. The adverbials can be fhr- 
ther subclassified according to other times they 
may refer to. 
? only present (now, at the moment) 
? also past (just, German eben) 
? any time (toda35 this week, in the mean- 
time, tbr two weeks) 
All other time adverbials are incompatible with 
present ime. 
(2) * On 19th November 2000, I sleep late. 
? Some adverbials only refer to past time (e.g. 
~stm'da35 last week, formerIL recentl), two 
days ago). 
(3) * I will be here yesterday. 
? Others can only be used with filturc time 
(tomorrow, next week, soon, in ?our d~ys). 
2.2 Verbs Immune to Temporal 
Orientation Effects 
In some cases tile temporal orientation of adver- 
bials sounds a false alarm: Even though an ad- 
verbial requiring non-present syntactically mod- 
ifies the tensed verb, German present tense is 
translated as present (see examples (4) from 
the British National Corpus). The effect comes 
about because semantically the adverbial mod- 
ifies not the verb's eventuality but one of the 
verb's objects. 
712 
(4) a. 'lbmorrow i am ah'eady 1)lamfing a golf 
trip with the boss. (Verbmobi\] (:orlms) 
b. And w(" wish trim the very best of lu(:k 
tomorrow in Birminghmn. 
(British National Corpus, l INe) 
('. Another storm fl'om SE ... is expected 
here tomorrow. (BNC) 
The decisive factor seems to be the verb sense 
involved. Thus, for disambiguation a. list; of such 
'"il\[llll\[llte" vet 'b senses  ltlltSt be compiled. Such 
verbs can be modified by adverbials requiring 
1)r(~sent and adverbials requiring non-1)resent at 
the  Sallle time. 
(5) am Montag haben wit jo%zt noch etwas Zeit 
011 ~/\]011(t~l.} r have we i~ow still some time 
but now we still have some time on Monday. 
2.3 Temt)oral  Or ientat ion  and Scope 
Only the temporal orientation of wide-s(:ol)e 
adverbials is relevant for tense (lisambiguation 
(Butt, 1995). lrrequen(:y )tdverbials intercel)t 
the disambiguating etfe(:t (see (6)). It is (;here- 
fore imi)ortan(; to evaluate only those a(lvcr- 
bials that outs(:ope the f l 'eqltelt(;y adverbial with 
widest scope. 
(6) a. John will be here on Monday. 
b. John is here on Monday every week. 
(:. John will |)e here in April every week. 
3 S imple  or P rogress ive?  
A distinctive feature of the English tense system 
th',~t is missing in German is the differentiation 
be(,ween siml)le and t)rogressive aspect. We t'o- 
(:us here on 1;\]1(: usag(: of aspect in 1)r(:sent ime. 
(7) present ime -~ ,~'imple/)re,~'ent 
-+ l>rcs<mt t)rogres,vi~<'. 
3.1 L inguist ic  N~etors 
The factors involved have been thoroughly stud- 
ted and classitied in the linguistic literature 
(Greenbaum and Quirk, 1990). So we confine 
ourselves to a short re.view here. 
State Present .  Stative verb senses get simple 
aspect. 
(8) a. ? We are having a house on Oxtbrd 
Street;. 
b. We arc having dinner. 
Habi tua l  Present .  A habit is a set of <:vents 
of the same type. In semantic tern:s, a habit 
arises from quantiti('ation over events. If the 
events extend indefinitely into the past and fi:- 
ture., the lmbit is conceived as perutancu, t "and 
simple aspect is used; if the events occur over 
~ limited period of time, the habit is conceived 
as temporary and progressive aspect is appro- 
priate. A frequency adverbial can be used to 
speci\[y tit(: (relative) number of occurrences of 
the event. Ge.ncral .\[acts cml be viewed as a spe- 
cial type of a habit. They arc always expressed 
in simple asl)ect (see (9)). 
(9) Because water boils at 100?C, water is 
boiling at 100?C in the pot. 
Ins tantaneous  Present .  Dynamic verb 
senst~s that ret'er to a single event with little 
or no (lur~tion oceun'ing at the Sl)eech time 
are exl)ressed with simple aspect. This type 
of \])resent is used in commentm'ies (10a), 
self  commentaries (10b) and with performative 
verbs (10c) referring to speech acts. 
(10) a. Joe scores a goal. 
b. I enclose an apt)lication tbrm. 
(:. For I)ermission to tmblish this paper, 
the authors l;tm.nk the l)el):~rtment of 
Economi(: Develol)ment. 
Durat iona l  Present .  Dynamic verl) senses 
denoting an incomph:te vent with dm'ation get 
progressive aspect. 
(11.) a. We are looking at; March sixteenth. 
(Verbmobil corpus) 
b. This is looking interesting. 
(Verbmobil corpus) 
3.2 Disambiguat ion  
State  Present;. Disambiguation requires def- 
inition and classification of all relevant verb 
senses according to stativity. When in a first ap- 
1)roximation only the most fl'equent verb sense 
of each verb are considered, a list; of stative verbs 
can be extracted from a corpus. 
Habi tua l  l?resent. The presence of a fie- 
quency adverbial points to a reading of Habitual 
Present. Since every event can be construed as 
a general fact, general facts arc very difficult to 
identify and will be disregarded. 
713 
I ns tantaneous  Present .  For disambiguation 
achievement verbs used in selfcommentm'ies 
and perfbrmative verbs need to be listed. 
Durat iona l  P resent .  Present events are usu- 
ally regarded as having duration, so progressive 
is the default aspect for dynamic verb senses in 
the present. 
4 Per fec t  o r  Not?  
In a special case, Germm~ present tense can be 
rendered as English present perfect: In English, 
perfect is used to describe periods that begin in 
the past and lend up to the present; German 
uses a non-perf~ctive t nse in this situation. 
(1.2) Wir leben schon fiinf Jahre in Amsterdam. 
We live ah'eady five years in Amsterdam. 
We have lived in Amsterdam for five years. 
Whenever a period is described that begins be- 
fore and still holds at speedl time, a limitative 
time adverbial i is used. (This term is due to 
Bras (1.990).) This peculiarity makes disam- 
biguation very easy. 
(13) a. Er ist erst zwei Wochen hier. 
lie is onlyt two weeks here. 
He has only been here for two weeks. 
b. Er ist nur zwei Wochen hier. 
He is only two weeks hcxe. 
He is here for only two weeks. 
5 Def in i t ion  o f  Granu lar i ty  
For our purposes we model the time axis as 
the set of rational 2 numbers Q. An interval 
is then a pair of rational nmnbers <s, c), such 
that s < e. The d'uration of an interval is the 
distance between start and end of the interval 
(d~Lr({s, c)) = e - s). On the interval structure 
we define the relations of inclusion (143) and 
overlap (14t)). 
(1.4) a. (81,c1} C (82, c2) +-} 82 ~.~ s1 A ~1 ~ (22 
b. @1,C1) 0 (82,('2) ~ 81 ~ (32 A 82 ~__ e 1 
1Limigal;ive adverbials go with the prepositions since 
mid for plus temporal measure nouns in English; in Ger- 
man they occur with the preposition seit and as duration 
adverbials modified by schon and erst. 
'~Although natural mmlbers could have been used to(), 
rational mm~bers are convenient since they allow free 
choice of the unit. 
5.1 Tempora l  nouns  
A temporal noun denotes a set; of intervals. We 
define the granularity of a temporal noun for- 
mally as a pair of numbers specifying the mini- 
real and maximal duration of its intervals (e.g. 
or'an(day) = (1., 1), gT(t,/~.(conference) = (1, 28), 
ora'n(senfinar) = (0.00138889, 334.812) if" the 
unit is a dw) .  The following relation is used 
to compare granularity values. 
(15) (d,tr~,durl) > {d'ur'~,dur~) +~ 
{dur'~,d'ur~) 7~ <&,r~,dur~> A dur I >> dur~ 
If 7t 1 has coarser grmmlarity than rt2, then an 
interval of rt:l cannot be included in aD. interval 
of' ~7,2. 
(16) , ,2: > -+ 
Vii ~ r~q,i,e c n2 : il g i2) 
Consider the fbllowing definition. 
(17) A telnporal noun n has t;11o property of 
disjoh~tness iff Vi~, i2 E n : ~il O i9 
Every calendar measure noun defines a partition 
on the time axis and has therefore the t)roperty 
of disjointness (e.g. hour, day, week, etelniti)O. 
Nouns functionally dependent on calendar mea-  
Sln'e l/ouns inherit the property (e.g. Monda35 
ChristlnaS, holida:B 6:45, ...). Event and state 
nouns have the disjointness property, if the de- 
scribed intervals fimctiolmlly depend on one of 
the particil)ants and the pm'ticil)allt is definite. 
Thus, example (18) is &wiant. 
(18) * Tomorrow at 6:30 Jones will give a talk 
at every conference. 
\]if ~zl has finer granularity than a noml ~z~ which 
has the property of disjointness, then every in- 
terval in r~,l overlaps with at most one interval 
in n2 (we disregard the case where il joins two 
intervals in n2). 
(19) V/;1 E ')'1: I{ ?=: i', A ,:1 o ,:2 }1 -< 1 
5.2 Funct ions on Granular i ty  
Rela t iona l  Nouns .  Among the temporal re- 
lational nouns we distinguish nouns describing 
periods of definite length (e.g. quarter, t;hird, 
hall) f'ronl those that refer to periods of in- 
definite length (e.g. beginning, middle, end). 
In grmmlarity calculation, relational nouns con- 
tribute a factor. With relational nouns referring 
714 
l:o periods of indefinil;e, length we so.t; l;hc fact;or 
1 1;o ~-. 
Tota l )ora l  I ) repos i t ions .  .letup n'al t)r(~i)osi- 
l;ions are mapl)c'd l;o inl;erva\] re, lal;ions. 
1. Some 1)rcl)osil;ions do not alter gramfl~ril;y 
(e.g. /;Cml)oral ocation on, in, at, duration 
during, throughout,/br). 
2. So:::(: pr(;posiCions des(:ril)c an oi)en-(md('d 
int;(;rval (e.g. bcibrc, at'tot, ti'om-ml, raM1). 
3. S()ln(; preposil;ions requir(; a. d(;t;ailed del:cr- 
minai;ion of i;oml)or;fl rcf(',rcn(:c, if t;hcy are 
l;o yieht granu\]a.ril;y t)r(;(li('fio:~s ((;.g. Hil~ce,, 
I)~'t;wc(;J 0 . 
To ('al(:ulaI;(, /~ramflaril;y wil;h i;h(' lal;t('a" l;w() 
('lass(;s, \re l;ak(; the grmmlmiI;y of I;h(; pr(;posi- 
I;ions' NP ~trgmncni;s as a gu iddin< Th(; h('m'is- 
I:i(" asSmnl)l;ion is I;hat; if a, time adverbial is I;o 
d(,,signal;e a long period more (tel;nil is given in 
the NP a rguln(,nl;. TIHI,% l;h(', rcl(,.vanI; l;(;mpo- 
ral l)r('posil;ions map l;hc. n(mn gra.mflarii;y 1;o th('. 
n(',xl; higher level ()n a ccrI;~till Sca.lc (day nlonl;h 
year el;t,rnil;y). 
(':0) 1,efo,.(,, 6:; 0 
1)(Zore Monday (< inOlli;h or y('.a.r) 
b(~fore Monday, lsl: of ()(:l;ol)(,w (< year) 
1)('.for('. Monday,  lst; of ()(:l;ol)cr 1998 
(infinite) 
Detcrmin( t rs .  Two (:lass(;s of dci;(;rmin('xs (:alJ 
t)e disl;inguis\]md: Sl)(;(-ifi(: ((lolinil;(; or ind(:linil;(;) 
~md qua,nl;iti(:a.l;ional (Ka.m t) and l/(;yh',, 1993). 
Sp(x'ific d(;l;(;rmin(;rs (to noI: (:hang(: gra:mla.rit;y. 
Q tmnl;ifi(;rs, how('ver, ('.xt('.n(l gramila.ril;y if il; is 
('lem" l;\]mt, ovc.ry inl:(;rval (l(:noi;c.d 1)y a. tX'ml)oral 
noun occurs only on(:(' wil;hin a. (:(;rl;ain l)eriod. 
(21) Mo,,d., y (,v(;(,,k) 
(;v('a'y beginning of a (;olff(~'r(!nc(; 
((:onf:'renc('O 
O,V(:l'y bimont;hly m(;cl;ing (two monl;hs) 
BUT: (;very qum't;e,r of a y(;ax" (ttu'c(: 
months, nol; year) 
Appos i t ions .  If temporal  nouns form ~t (:on- 
sl;ituent ((;.g. yo.vtcrday nt'tcrl~oo.u), the gram:- 
\]a:dl;y of the. head noun is (:hosc, n (tyl)i(:ally l;his is 
l;h(, filw.r gra.nularity). Sill(:(; ini;(;rwds arc usually 
descril)oxt Oll S() l l rC(.  ~ }/,lid (;}/,l'g(;l; side, granularii;y 
iI~\[brInal;ion of \])oth source and l;arg(;t tOtal)oral 
nouns can 1)(, exl)loil;cd to achieve higher preci- 
sion (e.g. Vormittag. morning). 
6 Scope Reso lu t ion  
For tens(', disamtfigua,tion scope re,solul;ion of 
|;ime adverbials can l)e crucial (cf. st',ctfion 2.3). 
6.1. I ,Smct ional  Concepts  
Functional conc(;i)ts resta'icl; the t)ossibilit;ies of 
scope r('.solul;ion (Alshawi, 1992). 
A quant;ificr Q(x,/2., S) is iterative iff it requires 
l;h&t; con('o.t)i;u}fl klmv, q(~.dgc allows for al; h;asI; 
l;wo ol)j(,.cl;s in the interse.cl, ion of il;s rcsl;ric- 
t:i,,,, ,md(,a,'  s (1{ :'; : A 
S(x) }1 > \]). Ex~mqfl(;s t:br it;ea'~l;ivc qumlt, i- 
ti(;rs m'c  eve,35 most, scvcra\] and l;hc distrilml;ive 
re~(liltg of phn'al. 
A conccl)t; (_7 is funct;iomd on doma,in D and 
r',mgc \]eitl:'V:,: : 19(:~:) -+ I{Y: W(y)AO(x ,y )} \ [  < 
1 (e.g. every human has exactly one faA;hc.r). 
ll; can b(; shown thai; if an itc'ral;ive qua nl;ifier 
qua.ni;iii('.~ over i;h(' range of a funcl;iona\] concepi;, 
il; musl; oul;sCOl)e. I;he dom a.in quantifier (e.g. m .... 
c W t?~thcr oul;s('opt',s a. stude,ut; in r'veW ti~thc,': ()f 
a .'-,'tlldcut) (Moran and lk,r(;ira, 1992). 
'li;mporal ov(',rlap is a fllncl;ional conc(;pl; if the 
donmin inl;('.rva\] has Iin(,.r gramlla\]'il;y t;h;m l:he 
rm, ge inlx;,'va.l (see (19)). Ih'.ncc', if l;wo time 
a dvert,ials 'n,i (on Monday in (22)) and n2 (c'veW 
u'ec'k in (22)) modify the same ev(!nl;, nl has 
liner gramflm'il;y l;han '/~,2, and r~,2 is il;(;ral;iv('ly 
(tuanl;ificd, i;h(m ~*,2 Illll,ql; outs(:op(; hi.  
(22) .lotto visil;ed .lane every w(!('k on Monday. 
6.2 Tempora l  Quant i f i ca t ion  
Temporal quanl;ificatioll has a curious i)roi)(;rty. 
Prcl)o,~il;iona\] t)hrases are gonel'ally l;real;(',d as 
inWrsc('l;ive modifiers to the head insl;m:c('. (AI- 
sh~uvi, 1.992). II:' we analyse sent;on(:(; (23~) i .  
l;his v(:in, w(; gel; (231)) as h)gical r(;1)rescntat;ion: 
The des('ribcd evenl;s arc situal;ed in May and in 
()V(}l'y x, VO, Ok.  
(23) a. In May John visited ,Jmm (:very wed(. 
1,. ? , , , :  a v,,,, : ,,,(,(,k(,,,) 
visit(c, John, .lmm) A c C. 'm, A (' C 'u; 
On closer insl)ecl;ion wc s(;(; thal; I;h(; repr(;s(;ni;a- 
l;ion is (:Ollla'adictory. Take an arbil;rary week, 
say in April. Tlw.n I;he fornmla ass(;rts that 
t, her(; is a visit; in thi,q w(,ek (i.(;. in April) and 
715 
ill May, which is inconsistent. Thus, the for- 
mula should not quantify over weeks in general 
but over weeks in May: An inclusion restric- 
tion to the wide-scope adverbial is needed in 
the narrow-scope adverbial (Kamp and Reyle, 
1993). 
(24) ~m: May(rr~) A Vw: week(w) A w C rn, 
~e : visit(e, John, Jane) A e C m A e C_ w 
Since temporal quantification requires that a 
narrow-scope iterative adverbial be included 
in the wide-scope adverbial, configurations are 
excluded where by conceptual knowledge the 
narrow-scope adverbial al cannot be included in 
the wide-scope adverbial a2. By theorem (16) 
this is the case if a,i has coarser granularity 
than a2. 
6.3 Granularity and Scope 
We have now seen two nlotivations for the prin- 
ciple (25). 
(25) If ~,~ has finer granularity than r~2 and rt,2 
is iteratively quantified, ~,1 camot  have 
scope over ~t2. 
Let us now consider the tbllowing t)rinciple: 
(26) If 7/'1 has finer granulm'ity than 7~,,) and r~,l 
is iteratiwfly quantified, ~zl cmmot have 
scope  over  ?~,2. 
We are not in a position to tbrmally explain the 
principle. It, holds for at least all nouns with 
the disjointness property. Although in exam- 
ple (27a) the vq reading would make perfect 
sense (Jones is always on holiday), it is excluded. 
In contrast, example (27b) allows this reading 
(.Jones shuttles between conference sites). The 
principle is not restricted to temporal grmmlar- 
ity: In (27c) the V~ reading is excluded, too. 
(27) a. Every Monday, Jones was here in a 
month in which he was on holiday. 
b. Every afternoon, Jones gave a talk at a 
conference. 
c. On every page, 1%und something in- 
teresting in a paper I read. 
Taken together, the two principles assert that 
the grmmlarity ranking determines the scope or- 
der. In the dismnbiguation algorithm presented 
in section 7 we are mainly interested in the po- 
sition of the highest (iterative) quantifier. So if 
every pair of time adverbials can be compared 
in terms of granularity, we have a procedure to 
compute this position. Comparison of granular- 
ity (1.5) is not defined if the granularity values 
overlap or m'e equal. Equal granularity is only 
possible with specific time adverbials. 
(28) ? John came from every Wednesday to ev- 
ery weekend 
Other heuristics will have to come into play in 
case of overlapping ranularity (see section 8 for 
further discussion). 
6.4 Deictic Adverbials 
Another factor for determining scope order is 
deixis. Some adverbials ~tre connected in their 
interpretation to the time of speedl (now, at the 
moment, next week, last week). Since time of 
speech is deictic (it depends on the context of 
utterance), hence defilfite, every function on it 
will also be definite. Deictic adverbials always 
get wide-scope position. 
Sentences in which deixis conflicts with granu- 
larity m'e deviant. 
(29) ? Next month, I will be here every year. 
7 The D isambiguat ion  A lgor i thm 
In the implementation, an underspecified se- 
nlant;ie representation formalism is used to en- 
code the source analysis and the transfer esult 
for the target (the Verbmobil Interface Term 
(FIT) formalism (Bos el; al., 1998), which is 
based on the theory of Underspecified Discourse 
Representation Structures (Ileyle, 1995)). The 
disambiguation heuristics of the system com- 
pletely rely on local context. The most impor- 
tant features in local context are source tense, 
the predicate names of the tensed verb in source 
mid target, and the time adverbials inodifying 
the verb in source and target (Sdfiehlen, 1998). 
In a first step, the source and target representa- 
tions are converted into an abstract representa- 
tion, using the VIT transfer forlnalism (Dorna 
and Emele, 1996). All information irrelevant 
to tense resolution is removed. Since no full- 
fledged tense logic is implemented, information 
about temporal reference is discarded as well. 
Temporal adverbs are decomposed into prepo- 
sitional phrases (e.g. .yesterday -+ on a past, 
716 
day). In particular the following featm'es are 
extracted: 
(30) a. multiple classification of verb senses 
? stative or dynamic (for English 
only, cf. section 3.2) 
? 1)otentially pertbrmative/self- 
colnmentary or not (for English 
only, cf. section 3.2) 
? "immune" or not (cf. section 2.2) 
b. temporal relation expressed by preI)o- 
sitions 
e. temt)oral orientation of pret)ositions 
(on Monday), adjectives (the previ -  
ous Monda:v) and determiners (this 
Monday) (of. section 2.1) 
d. granularity of nouns and adverbs (e.g. 
Cerma,) g ,,, ti gig (,,,ho>-(l<,j)) 
e. classification of determiners (quantiti- 
cational/specitic/alnbiguous) 
f. classification of adverbs (frequency ad- 
verb or not) 
We now give an outline of the disambiguatixm 
algorithm for translating German t)resent. It is 
(:lear l;hat the algorithln is \]mm'isti(" in lllglJxy Hs- 
t)ects, but in the absence of Colltext alld (lel;aile, d 
discourse anMysis it does not seem possible to 
do much better. 
1. For every tensed verb, determine all time 
adverbials modii~ying it and collect them in 
the set TA. 
2. Order the tinle adverbials in TA according 
to scope (of. section 6). 
3. Let STA be the set; of all specitic time ad- 
verbials in 5/'A (i.e. adverbials denoting a 
specific interval) not in the scope of a quan- 
tificational adverbial or frequency adverb 
(cf. section 2.3). 
4. Perfect or Not? (cf. section 4) 
If one of the time adverbials in ETA is con- 
nected to the tensed verb over a linfitative 
relation, choose perfect; else choose non- 
t)erf'ective tense. 
5. Present or Future? (cf. section 2) 
Choose present if the verb is "immune" to 
temi)oral orientation (cf. section 2.2). Else 
unify the temporal orientation contributed 
by the time adverbials in ETA. In case the 
result excludes present ime, choose fltture; 
else choose present (cf. section 2.1). 
6. Simple or Progressive Aspect? (of. sec- 
tion 3) 
(a) If the tensed verb has a stative sense, 
choose simple (State Present). 
(b) If the tensed verb is used in a self- 
commentary or perfonnatively, choose 
simple (Ilxstantaneous Present). 
(c) If there is an adverbial ill STA, choose 
progressive (Temporary Habit and Du- 
rational Present). 
(d) If there is a quantificational dverbial 
or flequency adverb in TA, choose sim- 
ple (Permmmnt Habit). 
(e) Else choose progressive (Durational 
Present). 
8 Resu l ts  
We tested the system on a data base of 13,625 
pairs of Gernlan VI~I\]q with their English trans- 
lations, containing 12,036 tensed verbs. All 
the d~tl;a were in the appointment schedul- 
ing domain which is investigated in Verbmobil. 
They wore transliter~ted and syntactically an- 
notated 1)3; hand. rlk'anslation was perforined by 
the Verbmol)il transfer component (Dorna and 
Emele, 1996). 2,758 tensed verbs were modified 
by time. adverbiMs, 1,373 of these verbs were 
modified by time adverbials with known granu- 
larity. 
The algorithm made the tbllowing choices tbr 
these data. The second column shows the total 
number of tensed verbs, the third column only 
counts those modified by time adverbials with 
granularity. 
-Perfect 0 0 
Future 729 729 
State or Instax)taneou8 Presenl 8,782 516 
Permanent Habit 29 29 
Dynalnic Present 2,496 99 
The described algorithm only inspects linguistic 
factors. Doxnain-specific nformation could po- 
tentially improve results. In the Verbmobil do- 
main e.g. several event types do not hal)i)en in 
717 
the present but only in the fllture (tra.vc\], meet, 
eat, ...). 
1,1 93.6% of the cases a specific wide-scope time 
adverbial could be determined with gramflarity 
constraints. In 4.7% of the cases several time 
adverbials of equal granularity had wide scope. 
A good deal of these cases were alignment er- 
rors with the translations (e.g. iibel"morgen - 
the day after tomorrow). Other cases were due 
to the lack of a treatment for coordination (e.g. 
on Monday aud on Thmwday or ti'om June to 
August). Some cases were genuine double de- 
scriptions of days: 
(31) a. Is it possible for you tomorrow on the 
second? 
b. I would have time on Wednesdw on 
Wednesday the third of May. 
In 1.7% of the eases the wide-scope adverbial 
could not be determined because some a.dver- 
bials had overlapping ralmlarity values. Here 
the main culprit was the unspecified adverb 
when (see (32a)). Other cases were due to in- 
correct preposition attachment (see (321,)). 
(32) a. When shall we meet on Mond~y? Next 
week/6:30. 
b. Would you be available in the time pe- 
riod until June? 
9 Summary  
The pal/er has presented a disambiguation algo- 
rithm ibr translation of German present into En- 
glish. After a discussion of the factors involved, 
particular emphasis was placed on an account 
of scope resolution among time adverbials. It 
has been shown that grmmhu'ity calculations go 
a long way towards the goal of full scope resolu- 
tioii. The cross-commotions between granular- 
ity and scope have been analysed in detail, and 
some motivation for these connections has been 
given. 
One area of future work is to apply the model to 
larger corpora and extend it to cover the full set 
of tenses. If translations can be aligned with the 
training data, it would be interesting to investi- 
gate the extent to which the model can be used 
to extract (parts of) the pertinent granularity 
information on temporal nouns from the corpus 
(Schiehlen, 1998). For example, the occurrence 
of a configuration like (33) could be interpreted 
as evidence tbr NOUN having coarser granular- 
ity than week. 
(33) FUTUIT{E-EVENT every week in NOUN 
References  
Hiyan Alshawi. 1992. The Core Language En- 
gine. MIT Press, Cambridge, MA, USA. 
Johan Bos, Bianka Buschbeck-Wolf, Michael 
Dorna, and C.J. Rupp. 1998. Managing infor- 
mation at linguistic interfaces. In Proceedings 
of the 171h International Co~@rcncc on Com- 
putational Linguistics (COLING '98), Montreal, 
Canada. 
Myrimn Bras. 1990. Calcul des Structures ~/};m- 
porcllcs du \])iscour& Ph.D. thesis, Universit6 
Paul Sabatier (le Toulouse. 
Miriam t3utt. 1995. Transfer I: Tense and As- 
pect. Verbmobil Ret)ort 55, SfS, Universit'~tt 
T/ibingen, Germany, Jalmary. 
Michael Dorna and Martin C. Emcle. 1996. 
Semantic-Based Tl"ansfer. In Proceedings qf 
th, e 161h International Co~7:fl',rence on Computa- 
tional Linguistics (COLING '96"), Copenhagen, 
Demnmk. 
Sidney Creenbamn and 12,andolph Quirk. 1990. 
A Student~.s Grammar of th, c English La'u, guagc. 
Longmml, Harlow, England. 
Hans Kamp and Uwe Reyle. 1993. F'rom Dis- 
course to Logic: An Int~vd'uction to Modelthe- 
or(:tic Semantics of Nat'wral Language. l(\]uwe, r 
Academic l?ut)lishers, Dordreeht, Holland. 
Douglas B. Moran and Fernando C.N. Pel"eira. 
1992. Quantifier Scoping. In Hiyan Alshawi, ed- 
itor, Th, c Cor(: Language Engine, chapter 8. MIT 
Press, Cambridge, MA, USA. 
Uwe Reyle. 1.995. Oll Reasoning with Anti)i- 
guities. In Proceedings of the 7th Co',:fc'rcr~,cc 
of the European Ch, aptcr of th, c Association .for 
Computational Linguistics (EA CL '95), pages 1 
8, Dublin, Ireland. 
Michael Schiehlen. 1.998. Learning Tense Trans- 
lation from Bilingual Corpora. In Proceedings 
of the 17th, International Co~:fcrcncc on Com- 
putational Linguistics (COLING '98), Montreal, 
Canada. 
718 
Robust  Semant ic  Const ruct ion  
Michae l  Sch ieh len*  
Inst i tute tbr Comlmtat iona l  Linguistics, University of Stuttgart ,  
Azenbergstr.  12, 70174 Stut tgar t  
mike@adler, ims. uni-stuttgart, de 
1 Introduction 
Recent years have seen a surge ill interest f~r 
robust fiat analysis, i.e. NLP systems with fairly 
limited supl)ly of linguistic knowledge but with 
vast coverage. The paper describes a module 
that serves as a back-end to such fiat analysis 
methods and transforms their output into full 
semantic representations a  constructed by deep 
analysis methods. In particular, the module has 
been designed so as to process input fl'om 
? tree banks 
? a statistic context-free parser trained on 
these tree banks 
? a finite-state parser 
? a traditional feature-structure parser 
The semantic representations which the mod- 
ule constructs m'e so-called Verbmobil Inter- 
~&ce \[\[~Ol'lllS (\\[Irl~s) (BOS et al, 1998), (l)uilding 
on Reyle's Underspecified Discourse R,epresen- 
tation Structures (1993), see an example, in Fig- 
ure 1). Although in principle othe, r representa- 
tions could be constructed as well, VITs seem 
to be a particularly good choice: They Call 1oe 
implemented as sets of coustraints o that se- 
mantic construction (SC) reduces to collecting 
the constraints and unifying some variables in 
these constraints. Furthermore VITs are sup- 
ported by ml abstract data type (Dorlla, 2000). 
Several daunting prol)lems had to be t'aced in 
tile design of the module. 
* This work was fimded by the German Federal Min- 
istry of Education, Science, Research and Technology 
(BMBF) in the ti'amework of the Verbmobil Project un- 
der Grant 01 IV 1(}1 U. Many thanks are duc to M. Emele 
and the colleagues in Verbmobil. 
week( y ) 
next( y ) 
decl(~7\] ) 
\ 
~ 1  ~ maybe(\[~ ) / 
/ /  should(el ,~\])  1 
into( e ,  y ) 
move(e, x ) pron(x) 
Figure 1: VIT for So maybe, u~'e should move, 
into idle next u,e, ek. 
Context -F ree  Input .  The tree banks provid- 
ing tile input structures (which have been built 
in the Verbmobil project) only encode context- 
free trees to facilitate the training of a statisti- 
cal parser. This means that non-local depen- 
dencies are either left out (e.g. topicalization 
in English) or treated by flattening out sub- 
trees into rules (e.g. head-movenmnt ill Ger- 
man). The latter strategy can create a vast 
amount of rules: Sin(:e Gelunan head-nmvement 
connects a clause-initial and a clause-final posi- 
tion, every clause frame gives rise to a new rule. 
To thee this challenge some adjustments had to 
be made: 
(1) Predicate-argument structure is indispens- 
able for SC but presupposes reconstruction of 
long distance dependencies ("movement"). If 
syntax cmmot supl)ly it, SC has to retrieve it; 
on its own (see Section 5.2). 
(2) The sheer bulk of rules prohibits manual tag: 
ging of syntactic rules with semantic rules. In- 
1101 
stead, syntax has to provide pertinent informa- 
tion in its rules so that SC can determiim the 
semantic operations required. 
Robustness .  Since the tree banks have been 
constructed by hand, errors are prone to crop 
Ul). Likewise, flat analysis methods cannot be 
expected to deliver input of the same quality 
as deep traditional parsers. Finally, grammars 
and semantic formalism will often difl'er in their 
subcategorization assmnptions: The verb move 
e.g. subcategorizes for hito in the tree bank (see 
Figure 2) but not in tile VIT tbnnalism (see Fig- 
ure 3). 
To handle this problem, the syntax-semantics- 
interface should be dismantled as far as possible: 
Only the most indispensable information should 
be taken over fl'om syntax. By neglecting all tile 
rest the system stands a good chance of skipping 
syntax errors. Furthermore in many cases deci- 
sions made ill syntax need to be overturned in 
semantics (e.g. the complement/adjunct specifi- 
cations), hnportant semantic information is of- 
ten deterlnined only in SC or in subsequent dis- 
ambiguation lnodules that have access to larger 
stores of context. This approach eases the bur- 
den on syntactic analysis and potentially ields 
more reliable results. 
D iverse Input .  A SC module should be able 
to handle input from a variety of grammars and 
convert it into an independellt tbrmat of seman- 
tic representation. Thus, a common syntax- 
semantics-interface (or inore precisely an inter- 
face between syntax and SC) must be defined 
onto which every type of' input is mapped. 
2 Design Principles 
To cope with tile problems mentioned, tradi- 
tional SC techniques (Montague, 1973) (Pereira 
and Shieber, 1987) (Bos el; al., 1996) cannot 
be used. Instead, the fbllowing ideas were ex- 
ploited. 
Modularity and Underspeeification. A 
major problem in SC is the treatment of mn- 
biguity. Often the local rule context available in 
SC does not give enough ilffonnation to resolve 
such ambiguities. In these cases, underspecifi- 
cation should be used to defer the resolution of 
choices. Thus, the described module builds a 
lexically and scopally underspecified represen- 
tation. Subsequently the lexical ambiguities are 
resolved by disambiguation modules. 
COMIZ_ . . . .  ~ I tD 
AIkJ/~D- 
/ADJ/~"J tD 
/ ---~ ~Bj/ ~HD ~. 
/ / ~7 ?D2""C~ OMP 
/ / / '7  Ime'---<OMP \ 
/ / / / -7 m)~C-OMI_'__\ 
SPI,,/~ D 
RB RB PP MD VB IN DT JJ NN 
So maybe we should move into the next week 
Figure 2: Example for an application tree. 
Modularity and Syntax-Semantics- 
Interface. To facilitate modularity a syntax- 
semantics-interface is explicitly defined. The 
input of every parser is mapped onto an 
interface structure called application twe, see 
Figure 2. In this way input Dora various sources 
can be processed with a minimum of effort. 
Semantic Database. Great emphasis is laid 
on an external database of semantic predicates 
(Heinecke and Worm, 1996). This database as- 
sociates lemmas with predicate nmnes, seman- 
tic classes and subcategorization frames (see the 
entry in Figure 3). 
3 System Overv iew 
The process of SC can be split into two phases 
(see Figure 4). In the first, phase an application 
I;ree is traversed and simultaneously an under- 
specified semantic representt~tion is lmilt (com- 
positional semantic onstruction, see section 5). 
In the second phase the semantic representation 
is partially (lisambiguated (see section 6). The 
two phases are preceded by a step which me.- 
diates between the actual output of the syntax 
and the syntax-semantics-interface. 
4 Syntax-Semantics- Interface 
Traditionally, the content of the syntax- 
semantics-interface is somewhat contentious. 
While syntax-oriented approaches try to inte- 
grate a good part of SC already into the pars- 
ing process (cf. the construction of f-structure 
in LFG), other al)proaches put tile main focus 
on semantics (e.g. Montague Grammar). To 
achieve a high degree of flexibility, a modular 
SC system has to settle for the lowest, common 
denonfinator of all input sources. The follow- 
ing information seems to be minimally required 
from the syntax. 
1102 
Lemma PredName 
move move 
SemClass SyntFrame Sort 
vi3 argl:subj,arg3:obj move sit 
Figure 3: Ent ry in the  semantic database. 
ArgSorts 
agentive,entity 
contezt-frec trec 
; 
\[ preprocessing step \] 
; 
application tree 
4 
compositional \] 
semantic construction <- semant ic  lex icon 
semantic represcv, tation 
; 
noncolnpositional \] 
semantic COllstruction <-- i d iom lex icon  
4 
sc',nantic re.l)r(',~em, tation 
Figure 4: System overview. 
(l) The parser should d(~livcr a tree tbr the 
parsed string whi(:h the SC system then ('an con- 
vert into a hierarchical stmmian'e of senmnti(: o\])- 
(;rations (an applicatiml, tree). 
(2) Every word in the input string should 1)e 
syntactically classified, i.e. assigned a .~!lntac- 
tic cateflo'r!/ or \])art of Sl)(;e.(:h tag. \?e will as- 
Sllllle that the parser  assigns every word exactly 
one category. (Lexical underslmcification could 
conceivatfy b(; used to deal with multiple cate- 
gories.) Then morphological analysis (either in 
synt;ax or SC) maps l;he word cate~rory pair to a 
morphological lemma and a set of morphologi- 
e::fl features. SC records the feal;llres in the VIT 
while it uses l;he lemma as a key to the seman- 
tic lexicon. In (:ase the lemma is unknown in 
the semantic lexicon, the, syst, eln uses the syn~ 
tacti(: category to automati(:ally asso(:iate ~ new 
1)redicate and semantic (:lass with the lemma. 
(3) Every rule used in the tree shouhl speci~y for 
each of the categories on its right-hand side ex- 
actly one grammatical role (GR). It7 the grmmnar 
does not do this, ORs must be deternfined in the 
prel)roeessing step (e.g. determiners in NPs are 
specifiers). Gll.s are llsed to COlttro\]. the choice of 
s,.;nmntic el)orations. The set of CHs emlfloyed 
is; inspired by HPSG (Pollard and Sag, 1994:): 
Head, Complement, Adjunct, Om~:j'anct, Spcci- 
- Z - - _Z  Z -_Z  Z - - _Z  S - -Z  - - _7_Z  - - - - _Z  - - - - _ -  - 
Y _ _ _ / f f ' -  
\[ houk,(o, ) 7 " 
i ! 
Figure 5: ()peration of a(1.immtion. 
tier, Part of a Multi-Word Lezcme. The corm- 
spending semm~tic operations are Complemcn- 
ration, Adjunction, Coordination, Spec~i/ication, 
and Predicate Form, ation. Except for Coordina- 
tion and Predicate Formation all operations are 
l)ilmry. A nile without a head is considered el- 
lil)ti('al and an abstract 1)redicate for the missing 
head is inserted in semantics. 
5 Compos i t iona l  Semant ic  
Const ruct ion  Process  
Conll)ositional SC follows the at)plieation tree 
(the context-free backbone) and detc,:nfincs the 
in'edicate-argunmnt structure (th(', subcatego- 
rization paths). 
5.1 Senmnt ic  Const ruct ion  on the 
ConsGtuent  St ructure  
Figure 5 shows two adjun(:tion operations: In 
the firs/: one, the inl:erseetive a(kiunct into the 
next week is adjoined to move. In the second 
one, maybe is adjoined to the clause. The pic- 
ture makes clem' what the data structure for 
a partial result should look like: a set of con- 
straints and some pointers to variables in these 
constraints (e.g. the partial result for maybe 
would be { maybe(l~,lq),12 < hl, l l  C- la }~ 
and (12, la)). Since only finitely many pointers 
J l n  a VIT, every predicate is referenced over a base 
label (e.g. lj for maybe). The constraint 12 < ha says 
that; the box 12 is subordinated to box lq, while l: C la 
sl;a|;es t, hat; predicate l: is in box la. 
1103 
are involved, they can be collected in a record. 
All partial results are classified into six seman- 
tic types according to the pointers they allow 
for: nhead (nominal head, ibr nouns), vhead 
(verbal head, for verbs), adj (adjuncts ~, for ad- 
verbs, adjectives, subclanses, PPs, also preposi- 
tions and subordinating co~\junctions), ncomp 
(nonfinal complements, for pronouns, NPs, also 
determiners), vcomp (verbal complements, for 
sentences and complement clauses, also sentence 
moods and complementizers), cnj (tbr coordi- 
nating conjunctions). 
Semantic operations expect arguments of spe- 
cific semantic types: Complementation com- 
bines heads with complements, Adjunction com- 
bines heads with adjuncts. Specification con- 
verts an incoml)lete ncomp (i.e. a deternfiner) 
and a nhead into a complete ncomp. Coordi- 
nation combines a cnj with a series of partial 
results of equal type. 
If a type clash occurs, a "type raising" opera- 
tion is invoked. Such operations usually insert 
specific abstract predicates that represent pho- 
netically empty words or elided materiM still to 
be retrieved by ellipsis resolution in a later step. 
Figure 6 gives a concise description of these op- 
erations a. Consider some type-raising exmnples: 
(1) I will be here Monday. 
udef 
unspcc rood 
(nhead -+ ncomp) 
(ncomp -~ adj) 
(2) I will come if necessary. 
star (adj --+ vhead) 
(3) Afternoon might work or early morniT~,g. 
abstr_ tel (ncomp -+ vhead)  
5.2 Semantic Construct ion on the 
Predicate-Argument Structure 
While the application tree (Figure 2) states that 
the pronoun we is the subject of should, in the 
2VITs provide a lexical underspecification class for 
intersective ( .g. into the vext week and scopal adjuncts 
(e.g. maybe). Thus, SC has to handle intersecLive and 
seopal adjunction i  parallel. 
3In Figure 6 the following names are used for newly 
inserted predicates: udef (mill determiner), unspcc_ rood 
(mill preposition), stat (auxiliary verb be), abstr_nom 
(nominal ellipsis), abstr_ tel (verbal ellipsis), dccl (declar- 
ative sentence mood), poss (relation expressed by ge.ni- 
tive), de.f (definite quantifier). 
comp(abstrrcl,) a,..-?@ 
I1~\\% -. oo %.~'% ,,; _ / / ,o . ,  , 
~ . . _ 
Figure 6: Type-raising operations. 
semantic representation (Figure 1) we is the sub- 
ject of move. So in this case head and seman- 
tic subject m:e not in the same local rule. To 
retrace such non-local dependencies, a slash de- 
vice is used to store the pertinent information 
(the argument variable and the box label of the 
head) and propagate it through the application 
tree in search for a licenser. If a subcategorized 
element occurs without a subcategorizing head 
(as occurs often in fl'agmentary input), an ellip- 
tical element is assumed: 
(4) I mean if you --~ absh'_ 'tel with subject you 
6 Noncompos i t iona l  Semant ic  
Const ruct ion  
In noncompositional SC idioms are recognized 
m~d a higher level of abstraction is achieved. 
Technically, noncompositional SC is about 
transforming VITs. Thus, for implementation 
the VIT transfer package of Dorna and Emele 
((1996)) is used. Linguistically, the component 
performs the following tasks: 
? recognition of multi-word lexemes that 
arc not designated as such by syntax 
(e.g. greeting expressions good night, com- 
paratives more comfortable) 
? recognition and comput~tion of clock times 
(e.g. a qm~rter to ten) and (late expressions 
? recognition of titles (e.g. Frau Miiller) 
? partial dismnbiguation of sentence mood 
(e.g. who did it; is recognized as a question) 
? distribution of conjoined material, if re- 
quired by the level of abstraction aimed for 
1104 
(e.g. clock tilnes between a. quarter to and 
half l)aSt to, n, (late expressions Monday the 
third and tenth) 
? eoml)ositional morphology for German 
(e.g. Sl;it'tmuseum -: museum with the 
ualne Stilt) 
7 Summary 
The paper has presented a module d capable 
o:\[' tlandling inlmt from tlat, analysis lnetho(ls 
and transforming them into full-fledged seman- 
tic representations. The module works rolmstly 
and currently has a throughput of about 98% 
on Verbmobil tree bank input (i.e. it gener- 
al:es 21,222 English and 26,789 German VITx.) 
The remaining 2% are due to errors in the SC 
module, errors in the tree bank, or coordination 
1)tel)ictus 1)etween SC and tree 1)ank. 
Evaluation of the module is COnll)lieated 1) 3, the 
effort involved in mmmally constructing a siz- 
able set of input structures and correspondillg 
semantic rel)resentations, l?urthermore, t;t1(: VIT 
formalism has been in constant flux over the 
last; years with the correct outlmt representa- 
tions changing almost monthly. It is, however, 
envisaged to perform an evaluation on('e dust 
has settled. 
The approach described adds in two respects to 
the rot)ustnexs of the overall system. First, l;he 
flat analysis lmrxerx used are very rolmst as con- 
terns low-level inconsistencies such as agreement 
failures or missing function words (prepositions, 
determiners, COmlflementizers, etc.). Second, 
the data analysed in the Verbmobil tree banks 
m:e exclusively xpoken language. Hence, the tree 
banks encode analyxes tbr phenomena such as 
fl'agmentary inlmt , truncated or elliptical sell- 
tencex, etc. The described module, gives seman- 
tic analyses for all of these constructions. (Usu- 
ally an abstract predicate is incorl)orated which 
given a hint to subsequent modules that aim to 
piece together partial utterances.) 
Another perspective of this work is that it pro- 
vides a first step towards a real corlms seman- 
tics by converting large sets of data into seman- 
tic representations. Due to the abstraction they 
embody, semantic representations are a valuable 
too\] for content queries to the. processed corpora. 
More immediately, the semmltic representations 
4More inibrmation can be found in Schiehlen (1999). 
generated by the described module have been 
used as text and training data tbr applications 
requiring abstract input, such as transfer in ma- 
chine translation and generation. 
References 
3ohan Bos, Bjgrn Gambit(k, Cln'istiml Lieske, 
Yoshiki Mori, Manii'ed Pinkal, and Km'sten L. 
Worm. 1996. Coml)ositional Semantics in Verb- 
mobil. In Proceedings of the 16th, Interna- 
tional Cm~:ferencc. on ComFutational Linguistics 
(COLING '96'), Copenhagen, l)emnark. 
Johan Box, Bianka Buschbeck-Wolf, Michael 
Dorna, and C.3. I{upp. 1998. Managing intbr- 
ination at linguistic interfaces. In Proceedings 
of th, e IT(h, Intcrnatio'nal CoT~ference on Com,- 
p'u, tational Linguistics (COLING '98), Montreal, 
Ca.lmda. 
Michael l)orna and Martin C. Einele. 1996. 
Senmntic-Based Transfer. In Proceedings of 
the. 16th, I'lder'nationaI Co'n:fi'.rence on Computa- 
tional Linguistic,~ (COLING '96), Cot)enhagen , 
Denmark. 
Michael Dorna. 2000. A Library Package for 
the Verbmobil Interface Term. Verbmobil E,e- 
port 238, Institut fiir maschinelle Sprachverar- 
l)eitm~g. 
.\]ohannes Heinecke and Karsten Worm. 1996. 
A Lexical Semantic Database for Verbmol)il. \[51 
Proceedings d the/tth, internatio'na, l Cov:fere',,ce 
on Comp'u, tational Linguistics (COMPLEX '96), 
Bud~l)est , I{ungary. 
Eichard Montague. 1973. The, Proper Treat- 
ment; of Quantification. In Jaako Hintikka, 
Julius Moravcsik, and Patrick Suppes, editors, 
Approaches to Natural Lang,uage , pages 221 
242. Reidel, l)ordreeht. 
Ferlmndo C.N. Pereira and Stuart M. Shieber. 
1987. Prolog and Natural-Language Analysis. 
CSLI Lecture Notes. Center for the Study of 
Language and hfformation, Stanford, Calitbr- 
nia. 
Carl Pollard and Ivan Sag. 1994. Itead- 
Driven Phrase Structure Grammar. University 
of Chicago Press, Chicago. 
Uwe Reyle. 1993. Dealing with Ambiguities 
by Underspeeification: Construction, I{epresen- 
tation and Deduction. Journal of Semantics, 
10(2):123-179. 
Michael Sdfiehlen. 1999. Semantikkonstruktion. 
Ph.D. thesis, Universitgt Stuttgart. 
1105 
 
		 	
		Optimizing Algorithms for Pronoun Resolution
Michael Schiehlen  
Institute for Computational Linguistics
University of Stuttgart
Azenbergstra?e 12, D-70174 Stuttgart
mike@ims.uni-stuttgart.de
Abstract
The paper aims at a deeper understanding of sev-
eral well-known algorithms and proposes ways to
optimize them. It describes and discusses factors
and strategies of factor interaction used in the algo-
rithms. The factors used in the algorithms and the
algorithms themselves are evaluated on a German
corpus annotated with syntactic and coreference in-
formation (Negra) (Skut et al, 1997). A common
format for pronoun resolution algorithms with sev-
eral open parameters is proposed, and the parameter
settings optimal on the evaluation data are given.
1 Introduction
In recent years, a variety of approaches to pronoun
resolution have been proposed. Some of them are
based on centering theory (Strube, 1998; Strube and
Hahn, 1999; Tetreault, 2001), others on Machine
Learning (Aone and Bennett, 1995; Ge et al, 1998;
Soon et al, 2001; Ng and Cardie, 2002; Yang et al,
2003). They supplement older heuristic approaches
(Hobbs, 1978; Lappin and Leass, 1994). Unfortu-
nately, most of these approaches were evaluated on
different corpora making different assumptions so
that direct comparison is not possible. Appreciation
of the new insights is quite hard. Evaluation differs
not only with regard to size and genre of corpora but
also along the following lines.
Scope of application: Some approaches only deal
with personal and possessive pronouns (centering
and heuristic), while others consider coreference
links in general (Soon et al, 2001; Ng and Cardie,
2002; Yang et al, 2003). A drawback of this lat-
ter view is that it mixes problems on different lev-
els of difficulty. It remains unclear how much of
the success is due to the virtues of the approach and
how much is due to the distribution of hard and easy
problems in the corpus. In this paper, we will only
deal with coreferential pronouns (i.e. possessive,
demonstrative, and third person pronouns).

My thanks go to Melvin Wurster for help in annotation and
to Ciprian Gerstenberger for discussion.
Quality of linguistic input: Some proposals were
evaluated on hand annotated (Strube and Hahn,
1999) or tree bank input (Ge et al, 1998; Tetreault,
2001). Other proposals provide a more realistic
picture in that they work as a backend to a parser
(Lappin and Leass, 1994) or noun chunker (Mitkov,
1998; Soon et al, 2001; Ng and Cardie, 2002)). In
evaluation of applications presupposing parsing, it
is helpful to separate errors due to parsing from in-
trinsic errors. On the other hand, one would also
like to gauge the end-to-end performance of a sys-
tem. Thus we will provide performance figures for
both ideal (hand-annotated) input and realistic (au-
tomatically generated) input.
Language: Most approaches were evaluated on
English where large resources are available, both
in terms of pre-annotated data (MUC-6 and MUC-7
data) and lexical information (WordNet). This paper
deals with German. Arguably, the free word-order
of German arguably leads to a clearer distinction be-
tween grammatical function, surface order, and in-
formation status (Strube and Hahn, 1999).
The paper is organized as follows. Section 2 de-
scribes the evaluation corpus. Section 3 describes
several factors relevant to pronoun resolution. It as-
sesses these factors against the corpus, measuring
their precision and restrictiveness. Section 4 de-
scribes and evaluates six algorithms on the basis of
these factors. It also captures the algorithms as para-
metric systems and proposes parameter settings op-
timal on the evaluation data. Section 5 concludes.
2 Evaluation Corpus
We chose as an evaluation base the NEGRA tree
bank, which contains about 350,000 tokens of Ger-
man newspaper text. The same corpus was also pro-
cessed with a finite-state parser, performing at 80%
dependency f-score (Schiehlen, 2003).
All personal pronouns (PPER), possessive pro-
nouns (PPOSAT), and demonstrative pronouns
(PDS) in Negra were annotated in a format geared
to the MUC-7 guidelines (MUC-7, 1997). Proper
names were annotated automatically by a named
entity recognizer. In a small portion of the corpus
(6.7%), all coreference links were annotated. Thus
the size of the annotated data (3,115 personal pro-
nouns1 , 2,198 possessive pronouns, 928 demonstra-
tive pronouns) compares favourably with the size of
evaluation data in other proposals (619 German pro-
nouns in (Strube and Hahn, 1999), 2,477 English
pronouns in (Ge et al, 1998), about 5,400 English
coreferential expressions in (Ng and Cardie, 2002)).
In the experiments, systems only looked for sin-
gle NP antecedents. Hence, propositional or pred-
icative antecedents (8.4% of the pronouns anno-
tated) and split antecedents (0.2%) were inaccessi-
ble, which reduced optimal success rate to 91.4%.
3 Factors in Pronoun Resolution
Pronoun resolution is conditioned by a wide range
of factors. Two questions arise: Which factors are
the most effective? How is interaction of the factors
modelled? The present section deals with the first
question, while the second question is postponed to
section 4.
Many approaches distinguish two classes of res-
olution factors: filters and preferences. Filters ex-
press linguistic rules, while preferences are merely
tendencies in interpretation. Logically, filters are
monotonic inferences that select a certain subset
of possible antecedents, while preferences are non-
monotonic inferences that partition the set of an-
tecedents and impose an order on the cells.
In the sequel, factors proposed in the literature are
discussed and their value is appraised on evaluation
data. Every factor narrows the set of antecedents
and potentially discards correct antecedents. Ta-
ble 1 lists both the success rate maximally achiev-
able (broken down according to different types of
pronouns) and the average number of antecedents
remaining after applying each factor. Figures are
also given for parsed input. Preferences are evalu-
ated on filtered sets of antecedents.
3.1 Filters
Agreement. An important filter comes from mor-
phology: Agreement in gender and number is gener-
ally regarded as a prerequisite for coreference. Ex-
ceptions are existant but few (2.5%): abstract pro-
nouns (such as that in English) referring to non-
neuter or plural NPs, plural pronouns co-referring
with singular collective NPs (Ge et al, 1998), an-
tecedent and anaphor matching in natural gender
1Here, we only count anaphoric pronouns, i.e. third person
pronouns not used expletively.
rather than grammatical gender. All in all, a max-
imal performance of 88.9% is maintained. The fil-
ter is very restrictive, and cuts the set of possible
antecedents in half. See Table 1 for details.
Binding. Binding constraints have been in the
focus of linguistic research for more than thirty
years. They provide restrictions on co-indexation
of pronouns with clause siblings, and therefore can
only be applied with systems that determine clause
boundaries, i.e. parsers (Mitkov, 1998). Empiri-
cally, binding constraints are rules without excep-
tions, hence they do not lead to any loss in achiev-
able performance. The downside is that their restric-
tive power is quite bad as well (0.3% in our corpus,
cf. Table 1).
Sortal Constraints. More controversial are sor-
tal constraints. Intuitively, they also provide a hard
filter: The correct antecedent must fit into the en-
vironment of the pronoun (Carbonell and Brown,
1988). In general, however, the required knowledge
sources are lacking, so they must be hand-coded and
can only be applied in restricted domains (Strube
and Hahn, 1999). Selectional restrictions can also
be modelled by collocational data extracted by a
parser, which have, however, only a very small im-
pact on overall performance (Kehler et al, 2004).
We will neglect sortal constraints in this paper.
3.2 Preferences
Preferences can be classified according to their re-
quirements on linguistic processing. Sentence Re-
cency and Surface Order can be read directly off the
surface. NP Form presupposes at least tagging. A
range of preferences (Grammatical Roles, Role Par-
allelism, Depth of Embedding, Common Path), as
well as all filters, presuppose full syntactic analysis.
Mention Count and Information Status are based on
previous decisions of the anaphora resolution mod-
ule.
Sentence Recency (SR). The most important cri-
terion in pronoun resolution (Lappin and Leass,
1994) is the textual distance between anaphor and
antecedent measured in sentences. Lappin and Le-
ass (1994) motivate this preference as a dynamic ex-
pression of the attentional state of the human hearer:
Memory capability for storage of discourse refer-
ents degrades rapidly.
Several implementations are possible. Perhaps
most obvious is the strategy implicit in Lappin
and Leass (1994)?s algorithm: The antecedent is
searched in a sentence that is as recent as possi-
ble, beginning with the already uttered part of the
current sentence, continuing in the last sentence, in
the one but last sentence, and so forth. In case no
Constraint Upper Bound
 
number Parser
total PPER PPOSAT PDS of antec. UpperB antec.
no VP 91.6 98.4 100.0 48.5 123.2 85.5 128.4
no split 91.4 98.3 100.0 47.8 123.2
agreement 88.9 96.8 99.5 37.6 53.0 79.1 61.8
binding 88.9 52.7 78.7 61.4
sentence recency SR 78.8 84.6 90.2 32.3 2.4 66.2 2.7
grammatical role GR 74.0 82.32 87.9 13.0 14.5 51.2 9.0
role parallelism RP 64.3 77.4 ? 20.0 12.5 47.0 10.3
surface order  LR 53.5 62.8 56.6 15.3 1 42.6 1
surface order  RL 45.9 45.9 55.7 22.7 1 35.2 1
depth of embedding DE 51.6 51.3 67.7 14.1 2.4 41.7 4.0
common path CP 51.7 52.3 64.2 19.9 5.3 46.8 11.3
equivalence classes EQ 63.6 67.5 78.4 15.7 1.3 51.3 1.5
mention count MC 32.9 40.3 34.0 4.6 5.5 35.7 7.1
information status IS 65.3 71.1 77.4 16.7 16.6 49.7 16.3
NP form NF 42.4 49.9 44.4 12.8 7.4 20.6 8.3
NP form (pronoun) NP 73.7 82.4 79.8 30.2 29.7 59.7 36.6
Table 1: Effect of Factors
antecedent is found in the previous context, subse-
quent sentences are inspected (cataphora), also or-
dered by proximity to the pronoun.
1
10
100
1000
10000
-2-10 1 2 3 4 5 6 7 8 9 12 19
all
PPER
PPOSAT
PDS
Figure 1: Sentence Recency
Figure 1 shows the absolute frequencies of sen-
tence recency values when only the most recent an-
tecedent (in the order just stated) is considered. In
Negra, 55.3% of all pronominal anaphora can be re-
solved intrasententially, and 97.6% within the last
three sentences. Since only 1.6% of all pronouns
are cataphoric, it seems reasonable to neglect cat-
aphora, as is mostly done (Strube and Hahn, 1999;
Hobbs, 1978). Table 1 underscores the virtues of
Sentence Recency: In the most recent sentence with
antecedents satisfying the filters, there are on aver-
age only 2.4 such antecedents. However, the benefit
also comes at a cost: The upper ceiling of perfor-
mance is lowered to 82.0% in our corpus: In many
cases an incorrect antecedent is found in a more re-
cent sentence.
Similarly, we can assess other strategies of sen-
tence ordering that have been proposed in the litera-
ture. Hard-core centering approaches only deal with
the last sentence (Brennan et al, 1987). In Negra,
these approaches can consequently have at most a
success rate of 44.2%. Performance is particularly
low with possessive pronouns which often only have
antecedents in the current sentence. Strube (1998)?s
centering approach (whose sentence ordering is des-
ignated as SR2 in Table 2) also deals with and even
prefers intrasentential anaphora, which raises the
upper limit to a more acceptable 80.2%. Strube and
Hahn (1999) extend the context to more than the last
sentence, but switch preference order between the
last and the current sentence so that an antecedent
is determined in the last sentence, whenever possi-
ble. In Negra, this ordering imposes an upper limit
of 51.2%.
Grammatical Roles (GR). Another important
factor in pronoun resolution is the grammatical role
of the antecedent. The role hierarchy used in cen-
tering (Brennan et al, 1987; Grosz et al, 1995)
ranks subjects over direct objects over indirect ob-
jects over others. Lappin and Leass (1994) provide a
more elaborate model which ranks NP complements
and NP adjuncts lowest. Two other distinctions in
their model express a preference of rhematic2 over
thematic arguments: Existential subjects, which fol-
low the verb, rank very high, between subjects and
direct objects. Topic adjuncts in pre-subject posi-
tion separated by a comma rank very low, between
adjuncts and NP complements. Both positions are
not clearly demarcated in German. When the Lap-
pin&Leass hierarchy is adopted to German with-
out changes, a small drop in performance results
as compared with the obliqueness hierarchy used in
centering. So we will use the centering hierarchy.
Table 1 shows the effect of the role-based prefer-
ence on our data. The factor is both less restrictive
and less precise than sentence recency.
The definition of a grammatical role hierarchy is
more involved in case of automatically derived in-
put, as the parser cannot always decide on the gram-
matical role (determining grammatical roles in Ger-
man may require world knowledge). It proposes a
syntactically preferred role, however, which we will
adopt.
Role Parallelism (RP). Carbonell and Brown
(1988) argue that pronouns prefer antecedents in the
same grammatical roles. Lappin and Leass (1994)
also adopt such a principle. The factor is, however,
not applicable to possessive pronouns.
Again, role ambiguities make this factor slightly
problematic. Several approaches are conceivable:
Antecedent and pronoun are required to have a com-
mon role in one reading (weak match). Antecedent
and pronoun are required to have the same role
in the reading preferred by surface order (strong
match). Antecedent and pronoun must display
the same role ambiguity (strongest match). Weak
match restricted performance to 49.9% with 12.1
antecedents on average. Strong match gave an up-
per limit of 47.0% but with only 10.3 antecedents on
average. Strongest match lowered the upper limit to
43.1% but yielded only 9.3 antecedents. In interac-
tion, strong match performed best, so we adopt it.
Surface Order (LR, RL). Surface Order is usu-
ally used to bring down the number of available
antecedents to one, since it is the only factor that
produces a unique discourse referent. There is less
consensus on the preference order: (sentence-wise)
left-to-right (Hobbs, 1978; Strube, 1998; Strube
and Hahn, 1999; Tetreault, 1999) or right-to-left
(recency) (Lappin and Leass, 1994). Furthermore,
something has to be said about antecedents which
embed other antecedents (e.g. conjoined NPs and
their conjuncts). We registered performance gains
2Carbonell and Brown (1988) also argue that clefted or
fronted arguments should be preferred.
(of up to 3%) by ranking embedding antecedents
higher than embedded ones (Tetreault, 2001).
Left-to-right order is often used as a surrogate for
grammatical role hierarchy in English. The most
notable exception to this equivalence are fronting
constructions, where grammatical roles outperform
surface order (Tetreault, 2001). A comparison of the
lines for grammatical roles and for surface order in
Table 1 shows that the same is true in German.
Left-to-right order performs better (upper limit
56.8%) than right-to-left order (upper limit 49.2%).
The gain is largely due to personal pronouns;
demonstrative pronouns are better modelled by
right-to-left order. It is well-known that German
demonstrative pronouns contrast with personal pro-
nouns in that they function as topic-shifting devices.
Another effect of this phenomenon is the poor per-
formance of the role preferences in connection with
demonstrative pronouns.
Depth of Embedding (DE). A prominent factor
in Hobbs (1978)?s algorithm is the level of phrasal
embedding: Hobbs?s algorithm performs a breadth-
first search, so antecedents at higher levels of em-
bedding are preferred.
Common Path (CP). The syntactic version of
Hobbs (1978)?s algorithm also assumes maximiza-
tion of the common path between antecedents and
anaphors as measured in NP and S nodes. Accord-
ingly, intra-sentential antecedents that are syntacti-
cally nearer to the pronoun are preferred. The factor
only applies to intrasentential anaphora.
The anaphora resolution module itself generates
potentially useful information when processing a
text. Arguably, discourse entities that have been of-
ten referred to in the previous context are topical
and more likely to serve as antecedents again. This
principle can be captured in different ways.
Equivalence Classes (EQ). Lappin and Leass
(1994) make use of a mechanism based on equiva-
lence classes of discourse referents which manages
the attentional properties of the individual entities
referred to. The mechanism stores and provides in-
formation on how recently and in which grammat-
ical role the entities were realized in the discourse.
The net effect of the storage mechanism is that dis-
course entities are preferred as antecedents if they
recently came up in the discourse. But the mecha-
nism also integrates the preferences Role Hierarchy
and Role Parallelism. Hence, it is one of the best-
performing factors on our data. Since the equiva-
lence class scheme is tightly integrated in the parser,
the problem of ideal anaphora resolution data does
not arise.
Mention Count (MC). Ge et al (1998) try to fac-
torize the same principle by counting the number of
times a discourse entities has been mentioned in the
discourse already. However, they do not only train
but also test on the manually annotated counts, and
hence presuppose an optimal anaphora resolution
system. In our implementation, we did not bother
with intrasentential mention count, which depends
on the exact traversal. Rather, mention count was
computed only from previous sentences.
Information Status (IS). Strube (1998) and
Strube and Hahn (1999) argue that the informa-
tion status of an antecedent is more important than
the grammatical role in which it occurs. They dis-
tinguish three levels of information status: entities
known to the hearer (as expressed by coreferential
NPs, unmodified proper names, appositions, rela-
tive pronouns, and NPs in titles), entities related to
such hearer-old entities (either overtly via modifiers
or by bridging), and entities new to the hearer. Like
(Ge et al, 1998), Strube (1998) evaluates on ideal
hand annotated data.
NP Form (NF, NP). A cheap way to model in-
formation status is to consider the form of an an-
tecedent (Tetreault, 2001; Soon et al, 2001; Strube
and M?ller, 2003). Personal and demonstrative
pronouns are necessarily context-dependent, and
proper nouns are nearly always known to the hearer.
Definite NPs may be coreferential or interpreted by
bridging, while indefinite NPs are in their vast ma-
jority new to the hearer. We considered two propos-
als for orderings of form: preferring pronouns and
proper names over other NPs over indefinite NPs
(Tetreault, 2001) (NF) or preferring pronouns over
all other NPs (Tetreault, 2001) (NP).
4 Algorithms and Evaluation
In this section, we consider the individual ap-
proaches in more detail, in particular we will look
at their choice of factors and their strategy to model
factor interaction. According to interaction poten-
tial, we distinguish three classes of approaches: Se-
rialization, Weighting, and Machine Learning.
We re-implemented some of the algorithms de-
scribed in the literature and evaluated them on syn-
tactically ideal and realistic German3 input. Evalu-
ation results are listed in Table 2.
With the ideal treebank input, we also assumed
ideal input for the factors dependent on previous
3A reviewer points out that most of the algorithms were pro-
posed for English, where they most likely perform better. How-
ever, the algorithms also incorporate a theory of saliency, which
should be language-independent.
anaphora resolution results. With realistic parsed
input, we fed the results of the actual system back
into the computation of such factors.
4.1 Serialization Approaches
Algorithmical approaches first apply filters uncon-
ditionally; possible exceptions are deemed non-
existant or negligible. With regard to interaction
of preferences, many algorithms (Hobbs, 1978;
Strube, 1998; Tetreault, 2001) subscribe to a
scheme, which, though completely rigid, performs
surprisingly well: The chosen preferences are ap-
plied one after the other in a certain pre-defined or-
der. Application of a preference consists in select-
ing those of the antecedents still available that are
ranked highest in the preference order.
Hobbs (1978)?s algorithm essentially is a con-
catenation of the preferences Sentence Recency
(without cataphora), Common Path, Depth of Em-
bedding, and left-to-right Surface Order. It also im-
plements the binding constraints by disallowing sib-
ling to the anaphor in a clause or NP as antecedents.
Like Lappin and Leass (1994), we replaced this im-
plementation by our own mechanism to check bind-
ing constraints, which raised the success rate.
The Left-Right Centering algorithm of Tetreault
(1999) is similar to Hobbs?s algorithm, and is com-
posed of the preferences Sentence Recency (without
cataphora), Depth of Embedding, and left-to-right
Surface Order. Since it is a centering approach, it
only inspects the current and last sentence.
Strube (1998)?s S-list algorithm is also restricted
to the current and last sentence. Predicative com-
plements and NPs in direct speech are excluded
as antecedents. The primary ordering criterion is
Information Status, followed by Sentence Recency
(without cataphora) and left-to-right Surface Order.
Since serialization provides a quite rigid frame,
we conducted an experiment to find the best per-
forming combination of pronoun resolution factors
on the treebank and the best combination on the
parsed input. For this purpose, we checked all per-
mutations of preferences and subtracted preferences
from the best-performing combinations until perfor-
mance degraded (greedy descent). Greedy descent
outperformed hill-climbing. The completely anno-
tated 6.7% of the corpus were used as development
set, the rest as test set.
4.2 Weighting Approaches
Compared with the serialization approaches, the al-
gorithm of Lappin and Leass (1994) is more sophis-
ticated: It uses a system of hand-selected weights
to control interaction among preferences, so that
in principle the order of preference application can
Algorithm Definition F-Scores ? treebank F-Score
total PPER PPOSAT PDS Parser
(Hobbs, 1978) SR   CP   DE   LR 59.9 65.1 70.5 17.4 45.4
(Tetreault, 1999) SR2   DE   LR 57.0 64.1 61.9 17.2 43.3
(Strube, 1998) IS   SR2   LR 57.9 65.9 63.7 12.0 39.1
optimal algor. (treebank) SR   CP   IS   DE   MC   RP   GR   RL 70.4 75.6 82.0 22.7 43.7
optimal algor. (parsed) SR   CP   GR   IS   DE   LR 67.7 74.3 82.0 10.6 50.6
(Lappin and Leass, 1994) EQ   SR   RL 65.4 71.0 78.0 16.6 50.8
(Ge et al, 1998) Hobbs+MC 43.4 45.7 53.6 12.1 36.3
(Soon et al, 2001) (SR+NP)   RL 24.8 30.8 23.6 0.0 26.8
optimal algor. (C4.5) (SR/RL+GR+NF/IS)   RL 71.1 78.2 79.0 9.8 51.7
Table 2: Performance of Algorithms
switch under different input data. In the actual real-
ization, however, the weights of factors lie so much
apart that in the majority of cases interaction boils
down to serialization. The weighting scheme in-
cludes Sentence Recency, Grammatical Roles, Role
Parallelism, on the basis of the equivalence class ap-
proach described in section 3.2. Final choice of an-
tecedents is relegated to right-to-left Surface Order.
Interestingly, the Lappin&Leass algorithm out-
performs even the best serialization algorithm on
parsed input.
4.3 Machine Learning Approaches
Machine Learning approaches (Ge et al, 1998;
Soon et al, 2001; Ng and Cardie, 2002) do not dis-
tinguish between filters and preferences. They sub-
mit all factors as features to the learner. For every
combination of feature values the learner has the
freedom to choose different factors and to assign
different strength to them.
Thus the main problem is not choice and in-
teraction of factors, but rather the formulation of
anaphora resolution as a classification problem.
Two proposals emerge from the literature. (1) Given
an anaphor and an antecedent, decide if the an-
tecedent is the correct one (Ge et al, 1998; Soon
et al, 2001; Ng and Cardie, 2002). (2) Given
an anaphor and two antecedents, decide which an-
tecedent is more likely to be the correct one (Yang
et al, 2003). In case (1), the lopsidedness of the
distribution is problematic: There are much more
negative than positive training examples. Machine
Learning tools have to surpass a very high baseline:
The strategy of never proposing an antecedent typ-
ically already yields an f-score of over 90%. In
case (2), many more correct decisions have to be
made before a correct antecedent is found. Thus it is
important in this scenario, that the set of antecedents
is subjected to a strict filtering process in advance so
that the system only has to choose among the best
candidates and errors are less dangerous.
Ge et al (1998)?s probabilistic approach com-
bines three factors (aside from the agreement filter):
the result of the Hobbs algorithm, Mention Count
dependent on the position of the sentence in the ar-
ticle, and the probability of the antecedent occur-
ring in the local context of the pronoun. In our
re-implementation, we neglected the last factor (see
section 3.1). Evaluation was performed using 10-
fold cross validation.
Other Machine Learning approaches (Soon et al,
2001; Ng and Cardie, 2002; Yang et al, 2003) make
use of decision tree learning4 ; we used C4.5 (Quin-
lan, 1993). To construct the training set, Soon et al
(2001) take the nearest correct antecedent in the pre-
vious context as a positive example, while all pos-
sible antecedents between this antecedent and the
pronoun serve as negative examples. For testing,
potential antecedents are presented to the classifier
in Right-to-Left order; the first one classified posi-
tive is chosen. Apart from agreement, only two of
Soon et al (2001)?s features apply to pronominal
anaphora: Sentence Recency, and NP Form (with
personal pronouns only). We used every 10th sen-
tence in Negra for testing, all other sentences for
training. On parsed input, a very simple decision
tree is generated: For every personal and posses-
sive pronoun, the nearest agreeing pronoun is cho-
sen as antecedent; demonstrative pronouns never
get an antecedent. This tree performs better than the
more complicated tree generated from treebank in-
put, where also non-pronouns in previous sentences
can serve as antecedents to a personal pronoun.
Soon et al (2001)?s algorithm performs below its
potential. We modified it somewhat to get better re-
sults. For one, we used every possible antecedent
4On our data, Maximum Entropy (Kehler et al, 2004) had
problems with the high baseline, i.e. proposed no antecedents.
in the training set, which improved performance
on the treebank set (by 1.8%) but degraded perfor-
mance on the parsed data (by 2%). Furthermore, we
used additional features, viz. the grammatical role
of antecedent and pronoun, the NP form of the an-
tecedent, and its information status. The latter two
features were combined to a single feature with very
many values, so that they were always chosen first in
the decision tree. We also used fractional numbers
to express intrasentential word distance in addition
to Soon et al (2001)?s sentential distance. Role
Parallelism (Ng and Cardie, 2002) degraded perfor-
mance (by 0.3% F-value). Introducing agreement
as a feature had no effect, since the learner always
determined that mismatches in agreement preclude
coreference. Mention Count, Depth of Embedding,
and Common Path did not affect performance either.
5 Conclusion
The paper has presented a survey of pronoun reso-
lution factors and algorithms. Two questions were
investigated: Which factors should be chosen, and
how should they interact? Two types of factors,
?filters? and ?preferences?, were discussed in detail.
In particular, their restrictive potential and effect on
success rate were assessed on the evaluation corpus.
To address the second question, several well-known
algorithms were grouped into three classes accord-
ing to their solution to factor interaction: Serializa-
tion, Weighting, and Machine Learning. Six algo-
rithms were evaluated against a common evaluation
set so as to facilitate direct comparison. Different
algorithms have different strengths, in particular as
regards their robustness to parsing errors. Two of
the interaction strategies (Serialization and Machine
Learning) allow data-driven optimization. Optimal
algorithms could be proposed for these strategies.
References
Chinatsu Aone and Scott William Bennett. 1995. Evalu-
ating automated and manual acquisition of anaphora
resolution strategies. In ACL?95, pages 122?129,
Cambridge, MA.
Susan E. Brennan, Marilyn W. Friedman, and Carl J.
Pollard. 1987. A centering approach to pronouns. In
ACL?87, pages 155?162, Stanford, CA.
Jaime G. Carbonell and Ralph D. Brown. 1988.
Anaphora resolution: A multi-strategy approach. In
COLING ?88, pages 96?101.
Niyu Ge, John Hale, and Eugene Charniak. 1998. A sta-
tistical approach to anaphora resolution. In Proceed-
ings of the Sixth Workshop on Very Large Corpora,
pages 161?170.
Barbara J. Grosz, Aravind K. Joshi, and Scott Weinstein.
1995. Centering: A Framework for Modeling the Lo-
cal Coherence of Discourse. Computational Linguis-
tics, 21(2):203?225.
Jerry R. Hobbs. 1978. Resolving pronoun references.
Lingua, 44:311?338.
Andrew Kehler, Douglas Appelt, Lara Taylor, and Alek-
sandr Simma. 2004. The (Non)Utility of Predicate-
Argument Frequencies for Pronoun Interpretation. In
Proceedings of the 2nd HLT/NAACL, Boston, MA.
Shalom Lappin and Herbert J. Leass. 1994. An algo-
rithm for pronominal anaphora resolution. Computa-
tional Linguistics, 20(4):535?561.
Ruslan Mitkov. 1998. Robust pronoun resolution with
limited knowledge. In COLING ?98, pages 869?875,
Montreal, Canada.
MUC-7. 1997. Coreference task definition. In Proceed-
ings of the Seventh Message Understanding Confer-
ence (MUC-7).
Vincent Ng and Claire Cardie. 2002. Improving ma-
chine learning approaches to coreference resolution.
In ACL?02, pages 104?111, Philadelphia, PA.
J. Ross Quinlan. 1993. C4.5: Programs for Machine
Learning. Morgan Kaufmann Publishers, San Mateo,
CA.
Michael Schiehlen. 2003. Combining Deep and Shallow
Approaches in Parsing German. In ACL?03, pages
112?119, Sapporo, Japan.
Wojciech Skut, Brigitte Krenn, Thorsten Brants, and
Hans Uszkoreit. 1997. An Annotation Scheme for
Free Word Order Languages. In ANLP-97, Washing-
ton, DC.
Wee Meng Soon, Hwee Tou Ng, and Daniel Chung Yong
Lim. 2001. A Machine Learning Approach to Coref-
erence Resolution of Noun Phrases. Computational
Linguistics, 27(4):521?544.
Michael Strube and Udo Hahn. 1999. Functional
Centering ? Grounding Referential Coherence in
Information Structure. Computational Linguistics,
25(3):309?344.
Michael Strube and Christoph M?ller. 2003. A Machine
Learning Approach to Pronoun Resolution in Spo-
ken Dialogue. In ACL?03, pages 168?175, Sapporo,
Japan.
Michael Strube. 1998. Never look back: An alterna-
tive to Centering. In COLING ?98, pages 1251?1257,
Montreal, Canada.
Joel R. Tetreault. 1999. Analysis of Syntax-Based Pro-
noun Resolution Methods. In ACL?99, pages 602?
605, College Park, MA.
Joel R. Tetreault. 2001. A corpus-based evaluation
of centering and pronoun resolution. Computational
Linguistics, 27(4):507?520.
Xiaofeng Yang, Guodong Zhou, Jian Su, and Chew Lim
Tan. 2003. Coreference Resolution Using Competi-
tion Learning Approach. In ACL?03, pages 176?183,
Sapporo, Japan.
Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pp. 1156?1160,
Prague, June 2007. c?2007 Association for Computational Linguistics
Global Learning of Labelled Dependency Trees
Michael Schiehlen Kristina Spranger
Institute for Computational Linguistics
University of Stuttgart
D-70174 Stuttgart
Michael.Schiehlen@ims.uni-stuttgart.de
Kristina.Spranger@ims.uni-stuttgart.de
Abstract
In the paper we describe a dependency
parser that uses exact search and global
learning (Crammer et al, 2006) to produce
labelled dependency trees. Our system inte-
grates the task of learning tree structure and
learning labels in one step, using the same
set of features for both tasks. During la-
bel prediction, the system automatically se-
lects for each feature an appropriate level
of smoothing. We report on several exper-
iments that we conducted with our system.
In the shared task evaluation, it scored better
than average.
1 Introduction
Dependency parsing is a topic that has engendered
increasing interest in recent years. One promis-
ing approach is based on exact search and struc-
tural learning (McDonald et al, 2005; McDonald
and Pereira, 2006). In this work we also pursue
this approach. Our system makes no provisions for
non-projective edges. In contrast to previous work,
we aim to learn labelled dependency trees at one
fell swoop. This is done by maintaining several
copies of feature vectors that capture the features?
impact on predicting different dependency relations
(deprels). In order to preserve the strength of Mc-
Donald et al (2005)?s approach in terms of unla-
belled attachment score, we add feature vectors for
generalizations over deprels. We also employ vari-
ous reversible transformations to reach treebank for-
mats that better match our feature representation and
that reduce the complexity of the learning task. The
paper first presents the methodology used, goes on to
describing experiments and results and finally con-
cludes.
2 Methodology
2.1 Parsing Algorithm
In our approach, we adopt Eisner (1996)?s bottom-
up chart-parsing algorithm in McDonald et al
(2005)?s formulation, which finds the best pro-
jective dependency tree for an input string  
	



. We assume that every possible head?
dependent pair 

is described by a feature vec-
tor  with associated weights 163
164
165
166
Ellipsis Resolution with Underspecified Scope
Michael Schiehlen  
Institute of Natural Language Processing
Azenbergstr. 12
70174 Stuttgart
Fed. Rep. of Germany
mike@ims.uni-stuttgart.de
Abstract
The paper presents an approach to ellipsis
resolution in a framework of scope under-
specification (Underspecified Discourse
Representation Theory). It is argued that
the approach improves on previous pro-
posals to integrate ellipsis resolution and
scope underspecification (Crouch, 1995;
Egg et al, 2001) in that application pro-
cesses like anaphora resolution do not re-
quire full disambiguation but can work
directly on the underspecified representa-
tion. Furthermore it is shown that the ap-
proach presented can cope with the exam-
ples discussed by Dalrymple et al (1991)
as well as a problem noted recently by
Erk and Koller (2001).
1 Introduction
Explicit computation of all scope configurations is
apt to slow down an NLP system considerably.
Therefore, underspecification of scope ambiguities
is an important prerequisite for efficient processing.
Many tasks, like ellipsis resolution or anaphora res-
olution, are arguably best performed on a represen-
tation with fixed scope order. An underspecification
formalism should support execution of these tasks.
This paper aims to upgrade an existing underspec-
ification formalism for scope ambiguities, Under-
specified Discourse Representation Theory (UDRT)
(Reyle, 1993), so that both ellipsis and anaphora res-
olution can work on the underspecified structures.

Many thanks for discussion and motivation are due to the
colleagues in Saarbr?cken.
Several proposals have been made in the lit-
erature on how to integrate scope underspecifica-
tion and ellipsis resolution in a single formalism,
e.g. Quasi-Logical Forms (QLF) (Crouch, 1995)
and the Constraint Language for Lambda Structures
(CLLS) (Egg et al, 2001). That work has primar-
ily aimed at devising methods to untangle quanti-
fier scoping and ellipsis resolution which often in-
teract closely (see Section 6). To this end, descrip-
tion languages have been modelled in which the dis-
ambiguation steps of a derivation need not be exe-
cuted but rather can be explicitly recorded as con-
straints on the final structure. Constraints are only
evaluated when the underspecified representation is
finally interpreted. In contrast, UDRT aims at pro-
viding a representation formalism that supports in-
terpretation processes such as theorem proving and
anaphora resolution. Understood in this sense, un-
derspecification often obviates the need for com-
plete disambiguation. Another consequence is, how-
ever, that the strategy of postponing disambigua-
tion steps is in some cases insufficient. A case
in point is the phenomenon dubbed Missing An-
tecedents by Grinder and Postal (1971), illustrated
in sentence (1): One of the pronoun?s antecedents
is overt, the other is supplied by ellipsis resolution.
(1) Harry sank a destroyer  and so did Bill and
they   both went down with all hands. (Grinder
and Postal, 1971, 279)
Most approaches to ellipsis and anaphora resolution,
e.g. (Asher, 1993; Crouch, 1995; Egg et al, 2001),
can readily derive the reading. But consider:
(2) Harry sometimes reads a book about a sea-
battle and so does Bill. They borrow those
books from the library.
                  Computational Linguistics (ACL), Philadelphia, July 2002, pp. 72-79.
                         Proceedings of the 40th Annual Meeting of the Association for
Example (2) still retains five readings (Are there two
or even more books? are there one, two, or more
than two sea-battles?). An underspecified represen-
tation should not be committed to any of these read-
ings, but it should specify that ?a book? has narrow
scope with respect to the conjunction. Furthermore,
an approach to underspecification and ellipsis reso-
lution should make clear why this representation is
to be constructed for the discourse (2). While QLF
fails the first requirement (a single representation),
CLLS fails the second (triggers for construction).
(3) * A destroyer  went down in some battle and a
cruiser did too. Harry sank both destroyers   .
The discourse in (3) is not well-formed. But none
of the approaches mentioned can ascertain this fact
without complete scope resolution (or ad-hoc re-
strictions).
The paper is organized as follows. Section 2 gives
a short introduction to UDRT. Section 3 formulates
the general setup of ellipsis resolution assumed in
the rest of the paper. Section 4 presents a proposal
to deal with scope parallelism in an underspecified
representation. Section 5 shows how ellipsis can be
treated if it is contained in its antecedent. Section 6
describes a way to model the interaction of ellipsis
resolution and scope resolution in an underspecified
structure. In section 7 strict and sloppy identity is
discussed. Section 8 concludes.
2 Underspecified Discourse
Representation Structures
Reyle (1993) proposes a formalism for under-
specification of scope ambiguity. The under-
specified representations are called Underspeci-
fied Discourse Representation Structures (UDRSs).
Completely specified UDRSs correspond to the
Discourse Representation Structures (DRSs) of
Kamp and Reyle (1993). A UDRS is a triple con-
sisting of a top label, a set of labelled conditions
or discourse referents, and a set of subordination
constraints. A UDRS is (partially) disambiguated
by adding subordination constraints. A UDRS must,
however, always comply with the following well-
formedness conditions: (1) It does not contain cy-
cles (subordination is a partial order). (2) No label
is subordinated to two labels which are siblings, i.e.
part of the same complex condition (subordination
is a tree order).
Figure 1 shows the UDRS for sentence 4 in formal
and graph representation.
(4) Every professor found most solutions.
l0
l5: every( x, l1:    , l2:    )
l8: professor( x ) l9:  solution( y )
l7:  find( x, y )
x l6:  most( y, l3:    ,l4:    )y
  
,
{  every 
	    , {   ,
  
	

professor 
	

,


,

most 
	


Combining Deep and Shallow Approaches in Parsing German
Michael Schiehlen
Institute for Computational Linguistics, University of Stuttgart,
Azenbergstr. 12, D-70174 Stuttgart
mike@adler.ims.uni-stuttgart.de
Abstract
The paper describes two parsing schemes:
a shallow approach based on machine
learning and a cascaded finite-state parser
with a hand-crafted grammar. It dis-
cusses several ways to combine them and
presents evaluation results for the two in-
dividual approaches and their combina-
tion. An underspecification scheme for
the output of the finite-state parser is intro-
duced and shown to improve performance.
1 Introduction
In several areas of Natural Language Processing, a
combination of different approaches has been found
to give the best results. It is especially rewarding to
combine deep and shallow systems, where the for-
mer guarantees interpretability and high precision
and the latter provides robustness and high recall.
This paper investigates such a combination consist-
ing of an n-gram based shallow parser and a cas-
caded finite-state parser1 with hand-crafted gram-
mar and morphological checking. The respective
strengths and weaknesses of these approaches are
brought to light in an in-depth evaluation on a tree-
bank of German newspaper texts (Skut et al, 1997)
containing ca. 340,000 tokens in 19,546 sentences.
The evaluation format chosen (dependency tuples)
is used as the common denominator of the systems
1Although not everyone would agree that finite-state
parsers constitute a ?deep? approach to parsing, they still are
knowledge-based, require efforts of grammar-writing, a com-
plex linguistic lexicon, manage without training data, etc.
in building a hybrid parser with improved perfor-
mance. An underspecification scheme allows the
finite-state parser partially ambiguous output. It is
shown that the other parser can in most cases suc-
cessfully disambiguate such information.
Section 2 discusses the evaluation format adopted
(dependency structures), its advantages, but also
some of its controversial points. Section 3 formu-
lates a classification problem on the basis of the
evaluation format and applies a machine learner to
it. Section 4 describes the architecture of the cas-
caded finite-state parser and its output in a novel
underspecification format. Section 5 explores sev-
eral combination strategies and tests them on several
variants of the two base components. Section 6 pro-
vides an in-depth evaluation of the component sys-
tems and the hybrid parser. Section 7 concludes.
2 Parser Evaluation
The simplest method to evaluate a parser is to count
the parse trees it gets correct. This measure is, how-
ever, not very informative since most applications do
not require one hundred percent correct parse trees.
Thus, an important question in parser evaluation is
how to break down parsing results.
In the PARSEVAL evaluation scheme (Black et
al., 1991), partially correct parses are gauged by the
number of nodes they produce and have in com-
mon with the gold standard (measured in precision
and recall). Another figure (crossing brackets) only
counts those incorrect nodes that change the partial
order induced by the tree. A problematic aspect of
the PARSEVAL approach is that the weight given to
particular constructions is again grammar-specific,
since some grammars may need more nodes to de-
scribe them than others. Further, the approach does
not pay sufficient heed to the fact that parsing de-
cisions are often intricately twisted: One wrong de-
cision may produce a whole series of other wrong
decisions.
Both these problems are circumvented when
parsing results are evaluated on a more abstract
level, viz. dependency structure (Lin, 1995).
Dependency structure generally follows predicate-
argument structure, but departs from it in that the
basic building blocks are words rather than predi-
cates. In terms of parser evaluation, the first property
guarantees independence of decisions (every link is
relevant also for the interpretation level), while the
second property makes for a better empirical justifi-
cation. for evaluation units. Dependency structure
can be modelled by a directed acylic graph, with
word tokens at the nodes. In labelled dependency
structure, the links are furthermore classified into a
certain set of grammatical roles.
Dependency can be easily determined from con-
stituent structure if in every phrase structure rule
a constituent is singled out as the head (Gaifman,
1965). To derive a labelled dependency structure, all
non-head constituents in a rule must be labelled with
the grammatical role that links their head tokens to
the head token of the head constituent.
There are two cases where the divergence be-
tween predicates and word tokens makes trouble: (1)
predicates expressed by more than one token, and
(2) predicates expressed by no token (as they occur
in ellipsis). Case 1 frequently occurs within the verb
complex (of both English and German). The solu-
tion proposed in the literature (Black et al, 1991;
Lin, 1995; Carroll et al, 1998; K?bler and Telljo-
hann, 2002) is to define a normal form for depen-
dency structure, where every adjunct or argument
attaches to some distinguished part of the verb com-
plex. The underlying assumption is that those cases
where scope decisions in the verb complex are se-
mantically relevant (e.g. with modal verbs) are not
resolvable in syntax anyway. There is no generally
accepted solution for case 2 (ellipsis). Most authors
in the evaluation literature neglect it, perhaps due
to its infrequency (in the NEGRA corpus, ellipsis
only occurs in 1.2% of all dependency relations).
Robinson (1970, 280) proposes to promote one of
the dependents (preferably an obligatory one) (1a)
or even all dependents (1b) to head status.
(1) a. the very brave
b. John likes tea and Harry coffee.
A more sweeping solution to these problems is to
abandon dependency structure at all and directly
go for predicate-argument structure (Carroll et al,
1998). But as we argued above, moving to a
more theoretical level is detrimental to comparabil-
ity across grammatical frameworks.
3 A Direct Approach: Learning
Dependency Structure
According to the dependency structure approach to
evaluation, the task of the parser is to find the cor-
rect dependency structure for a string, i.e. to as-
sociate every word token with pairs of head token
and grammatical role or else to designate it as inde-
pendent. To make the learning task easier, the num-
ber of classes should be reduced as much as possi-
ble. For one, the task could be simplified by focus-
ing on unlabelled dependency structure (measured
in ?unlabelled? precision and recall (Eisner, 1996;
Lin, 1995)), which is, however, in general not suffi-
cient for further semantic processing.
3.1 Tree Property
Another possibility for reduction is to associate ev-
ery word with at most one pair of head token and
grammatical role, i.e. to only look at dependency
trees rather than graphs. There is one case where
the tree property cannot easily be maintained: co-
ordination. Conceptually, all the conjuncts are head
constituents in coordination, since the conjunction
could be missing, and selectional restrictions work
on the individual conjuncts (2).
(2) John ate (fish and chips|*wish and ships).
But if another word depends on the conjoined heads
(see (4a)), the tree property is violated. A way out
of the dilemma is to select a specific conjunct as
modification site (Lin, 1995; K?bler and Telljohann,
2002). But unless care is taken, semantically vi-
tal information is lost in the process: Example (4)
shows two readings which should be distinguished
in dependency structure. A comparison of the two
readings shows that if either the first conjunct or
the last conjunct is unconditionally selected certain
readings become undistinguishable. Rather, in or-
der to distinguish a maximum number of readings,
pre-modifiers must attach to the last conjunct and
post-modifiers and coordinating conjunctions to the
first conjunct2 . The fact that the modifier refers to
a conjunction rather than to the conjunct is recorded
in the grammatical role (by adding c to it).
(4) a. the [fans and supporters] of Arsenal
b. [the fans] and [supporters of Arsenal]
Other constructions contradicting the tree property
are arguably better treated in the lexicon anyway
(e.g. control verbs (Carroll et al, 1998)) or could
be solved by enriching the repertory of grammati-
cal roles (e.g. relative clauses with null relative pro-
nouns could be treated by adding the dependency re-
lation between head verb and missing element to the
one between head verb and modified noun).
In a number of linguistic phenomena, dependency
theorists disagree on which constituent should be
chosen as the head. A case in point are PPs. Few
grammars distinguish between adjunct and subcate-
gorized PPs at the level of prepositions. In predicate-
argument structure, however, the embedded NP is
in one case related to the preposition, in the other
to the subcategorizing verb. Accordingly, some ap-
proaches take the preposition to be the head of a PP
(Robinson, 1970; Lin, 1995), others the NP (K?bler
and Telljohann, 2002). Still other approaches (Tes-
ni?re, 1959; Carroll et al, 1998) conflate verb,
preposition and head noun into a triple, and thus
only count content words in the evaluation. For
learning, the matter can be resolved empirically:
2Even in this setting some readings cannot be distinguished
(see e.g. (3) where a conjunction of three modifiers would
be retrieved). Nevertheless, the proposed scheme fails in only
0.0017% of all dependency tuples.
(3) In New York, we never meet, but in Boston.
Note that by this move we favor interpretability over projectiv-
ity, but example (4a) is non-projective from the start.
Taking prepositions as the head somewhat improves
performance, so we took PPs to be headed by prepo-
sitions.
3.2 Encoding Head Tokens
Another question is how to encode the head to-
ken. The simplest method, encoding the word by its
string position, generates a large space of classes. A
more efficient approach uses the distance in string
position between dependent and head token. Finally,
Lin (1995) proposes a third type of representation:
In his work, a head is described by its word type, an
indication of the direction from the dependent (left
or right) and the number of tokens of the same type
that lie between head and dependent. An illustrative
representation would be ?paperwhich refers to the
second nearest token paper to the right of the cur-
rent token. Obviously there are far too many word
tokens, but we can use Part-Of-Speech tags instead.
Furthermore information on inflection and type of
noun (proper versus common nouns) is irrelevant,
which cuts down the size even more. We will call
this approach nth-tag. A further refinement of the
nth-tag approach makes use of the fact that depen-
dency structures are acylic. Hence, only those words
with the same POS tag as the head between depen-
dent and head must be counted that do not depend
directly or indirectly on the dependent. We will call
this approach covered-nth-tag.
pos dist nth-tag cover
labelled 1,924 1,349 982 921
unlabelled 97 119 162 157
Figure 1: Number of Classes in NEGRA Treebank
Figure 1 shows the number of classes the individ-
ual approaches generate on the NEGRA Treebank.
Note that the longest sentence has 115 tokens (with
punctuation marks) but that punctuation marks do
not enter dependency structure. The original tree-
bank exhibits 31 non-head syntactic3 grammatical
roles. We added three roles for marker comple-
ments (CMP), specifiers (SPR), and floating quanti-
fiers (NK+), and subtracted the roles for conjunction
markers (CP) and coreference with expletive (RE).
3i.e. grammatical roles not merely used for tokenization
22 roles were copied to mark reference to conjunc-
tion. Thus, all in all there was a stock of 54 gram-
matical roles.
3.3 Experiments
We used   -grams (3-grams and 5-grams) of POS
tags as context and C4.5 (Quinlan, 1993) for ma-
chine learning. All results were subjected to 10-fold
cross validation.
The learning algorithm always returns a result.
We counted a result as not assigned, however, if it
referred to a head token outside the sentence. See
Figure 2 for results4 of the learner. The left column
shows performance with POS tags from the treebank
(ideal tags, I-tags), the right column values obtained
with POS tags as generated automatically by a tag-
ger with an accuracy of 95% (tagger tags, T-tags).
I-tags T-tags
F-val prec rec F-val prec rec
dist, 3 .6071 .6222 .5928 .5902 .6045 .5765
dist, 5 .6798 .6973 .6632 .6587 .6758 .6426
nth-tag, 3 .7235 .7645 .6866 .6965 .7364 .6607
nth-tag, 5 .7716 .7961 .7486 .7440 .7682 .7213
cover, 3 .7271 .7679 .6905 .7009 .7406 .6652
cover, 5 .7753 .7992 .7528 .7487 .7724 .7264
Figure 2: Results for C4.5
The nth-tag head representation outperforms the
distance representation by 10%. Considering
acyclicity (cover) slightly improves performance,
but the gain is not statistically significant (t-test with
99%). The results are quite impressive as they stand,
in particular the nth-tag 5-gram version seems to
achieve quite good results. It should, however, be
stressed that most of the dependencies correctly de-
termined by the n-gram methods extend over no
more than 3 tokens. With the distance method, such
?short? dependencies make up 98.90% of all depen-
dencies correctly found, with the nth-tag method
still 82%, but only 79.63% with the finite-state
parser (see section 4) and 78.91% in the treebank.
4If the learner was given a chance to correct its errors, i.e.
if it could train on its training results in a second round, there
was a statistically significant gain in F-value with recall rising
and precision falling (e.g. F-value .7314, precision .7397, recall
.7232 for nth-tag trigrams, and F-value .7763, precision .7826,
recall .7700 for nth-tag 5-grams).
4 Cascaded Finite-State Parser
In addition to the learning approach, we used a cas-
caded finite-state parser (Schiehlen, 2003), to extract
dependency structures from the text. The layout
of this parser is similar to Abney?s parser (Abney,
1991): First, a series of transducers extracts noun
chunks on the basis of tokenized and POS-tagged
text. Since center-embedding is frequent in German
noun phrases, the same transducer is used several
times over. It also has access to inflectional informa-
tion which is vital for checking agreement and deter-
mining case for subsequent phases (see (Schiehlen,
2002) for a more thorough description). Second, a
series of transducers extracts verb-final, verb-first,
and verb-second clauses. In contrast to Abney, these
are full clauses, not just simplex clause chunks, so
that again recursion can occur. Third, the result-
ing parse tree is refined and decorated with gram-
matical roles, using non-deterministic ?interpreta-
tion? transducers (the same technique is used by
Abney (1991)). Fourth, verb complexes are exam-
ined to find the head verb and auxiliary passive or
raising verbs. Only then subcategorization frames
can be checked on the clause elements via a non-
deterministic transducer, giving them more specific
grammatical roles if successful. Fifth, dependency
tuples are extracted from the parse tree.
4.1 Underspecification
Some parsing decisions are known to be not resolv-
able by grammar. Such decisions are best handed
over to subsequent modules equipped with the rel-
evant knowledge. Thus, in chart parsing, an under-
specified representation is constructed, from which
all possible analyses can be easily and efficiently
read off. Elworthy et al (2001) describe a cascaded
parser which underspecifies PP attachment by allow-
ing modifiers to be linked to several heads in a de-
pendency tree. Example (5) illustrates this scheme.
(5) I saw a man in a car on the hill.
The main drawback of this scheme is its overgener-
ation. In fact, it allows six readings for example (5),
which only has five readings (the speaker could not
have been in the car, if the man was asserted to be
on the hill). A similar clause with 10 PPs at the
end would receive 39,916,800 readings rather than
58,786. So a more elaborate scheme is called for,
but one that is just as easy to generate.
A device that often comes in handy for under-
specification are context variables (Maxwell III and
Kaplan, 1989; D?rre, 1997). First let us give every
sequence of prepositional phrases in every clause a
specific name (e.g. 1B for the second sequence in
the first clause). Now we generate the ambiguous
dependency relations (like (Elworthy et al, 2001))
but label them with context variables. Such context
variables consist of the sequence name   , a num-
ber  designating the dependent in left-to-right or-
der (e.g. 0 for in, 1 for on in example (5)), and a
number  designating the head in left-to-right (e.g.
0 for saw, 1 for man, 2 for hill in (5)). If the links
are stored with the dependents, the number  can be
left implicit. Generation of such a representation is
straightforward and, in particular, does not lead to a
higher class of complexity of the full system. Ex-
ample (6) shows a tuple representation for the two
prepositions of sentence (5).
(6) in [1A00] saw ADJ, [1A01] man ADJ
on [1A10] saw ADJ, [1A11] man ADJ,
[1A12] car ADJ
In general, a dependent  can modify  heads,
viz. the heads numbered 	


 . Now we
put the following constraint on resolution: A depen-
dent  can only modify a head  if no previous
dependent  which could have attached to  (i.e.




 ) chose some head   to the left of  
rather than  . The condition is formally expressed
in (7). In example (6) there are only two dependents
( ffProceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X),
pages 231?235, New York City, June 2006. c?2006 Association for Computational Linguistics
Language Independent Probabilistic Context-Free Parsing
Bolstered by Machine Learning
Michael Schiehlen Kristina Spranger
Institute for Computational Linguistics
University of Stuttgart
D-70174 Stuttgart
Michael.Schiehlen@ims.uni-stuttgart.de
Kristina.Spranger@ims.uni-stuttgart.de
Abstract
Unlexicalized probabilistic context-free
parsing is a general and flexible approach
that sometimes reaches competitive re-
sults in multilingual dependency parsing
even if a minimum of language-specific
information is supplied. Furthermore, in-
tegrating parser results (good at long de-
pendencies) and tagger results (good at
short range dependencies, and more easily
adaptable to treebank peculiarities) gives
competitive results in all languages.
1 Introduction
Unlexicalized probabilistic context-free parsing is
a simple and flexible approach that nevertheless
has shown good performance (Klein and Manning,
2003). We applied this approach to the shared task
(Buchholz et al, 2006) for Arabic (Hajic? et al,
2004), Chinese (Chen et al, 2003), Czech (B?h-
mov? et al, 2003), Danish (Kromann, 2003), Dutch
(van der Beek et al, 2002), German (Brants et al,
2002), Japanese (Kawata and Bartels, 2000), Por-
tuguese (Afonso et al, 2002), Slovene (D?eroski et
al., 2006), Spanish (Civit Torruella and Mart? An-
ton?n, 2002), Swedish (Nilsson et al, 2005), Turk-
ish (Oflazer et al, 2003; Atalay et al, 2003), but
not Bulgarian (Simov et al, 2005). In our ap-
proach we put special emphasis on language inde-
pendence: We did not use any extraneous knowl-
edge; we did not do any transformations on the
treebanks; we restricted language-specific parame-
ters to a small, easily manageable set (a classifica-
tion of dependency relations into complements, ad-
juncts, and conjuncts/coordinators, and a switch for
Japanese to include coarse POS tag information, see
section 3.4). In a series of post-submission experi-
ments, we investigated how much the parse results
can help a machine learner.
2 Experimental Setup
For development, we chose the initial   sentences of
every treebank, where   is the number of the sen-
tences in the test set. In this way, the sizes were
realistic for the task. For parsing the test data, we
added the development set to the training set.
All the evaluations on the test sets were performed
with the evaluation script supplied by the conference
organizers. For development, we used labelled F-
score computed from all tokens except the ones em-
ployed for punctuation (cf. section 3.2).
3 Context Free Parsing
3.1 The Parser
Basically, we investigated the performance of a
straightforward unlexicalized statistical parser, viz.
BitPar (Schmid, 2004). BitPar is a CKY parser that
uses bit vectors for efficient representation of the
chart and its items. If frequencies for the grammat-
ical and lexical rules in a training set are available,
BitPar uses the Viterbi algorithm to extract the most
probable parse tree (according to PCFG) from the
chart.
231
3.2 Converting Dependency Structure to
Constituency Structure
In order to determine the grammar rules required by
the context-free parser, the dependency trees in the
CONLL format have to be converted to constituency
trees. Gaifman (1965) proved that projective de-
pendency grammars can be mapped to context-free
grammars. The main information that needs to be
added in going from dependency to constituency
structure is the category of non-terminals. The usage
of special knowledge bases to determine projections
of categories (Xia and Palmer, 2001) would have
presupposed language-dependent knowledge, so we
investigated two other options: Flat rules (Collins
et al, 1999) and binary rules. In the flat rules ap-
proach, each lexical category projects to exactly one
phrasal category, and every projection chain has a
length of at most one. The binary rules approach
makes use of the X-bar-scheme and thus introduces
along with the phrasal category an intermediate cate-
gory. The phrasal category must not occur more than
once in a projection chain, and a projection chain
must not end in an intermediate category. In both ap-
proaches, projection is only triggered if dependents
are present; in case a category occurs as a depen-
dent itself, no projection is required. In coordination
structures, the parent category is copied from that of
the last conjunct.
Non-projective relations can be treated as un-
bounded dependencies so that their surface posi-
tion (antecedent position) is related to the position
of their head (trace position) with an explicit co-
indexed trace (like in the Penn treebank). To find
the position of trace and antecedent we assume three
constraints: The antecedent should c-command its
trace. The antecedent is maximally near to the trace
in depth of embedding. The trace is maximally near
to the antecedent in surface order.
Finally the placement of punctuation signs has
a major impact on the performance of a parser
(Collins et al, 1999). In most of the treebanks, not
much effort is invested into the treatment of punc-
tuation. Sometimes, punctuation signs play a role
in predicate-argument structure (commas acting as
coordinators), but more often they do not, in which
case they are marked by special roles (e.g. ?pnct?,
?punct?, ?PUNC?, or ?PUNCT?). We used a general
mechanism to re-insert such signs, for all languages
but CH (no punctuation signs) and AR, CZ, SL (re-
liable annotation). Correct placement of punctua-
tion presupposes knowledge of the punctuation rules
valid in a language. In the interest of generality, we
opted for a suboptimal solution: Punctuation signs
are inserted in the highest possible position in a tree.
3.3 Subcategorization and Coordination
The most important language-specific information
that we made use of was a classification of de-
pendency relations into complements, coordina-
tors/conjuncts, and other relations (adjuncts).
Given knowledge about complement relations, it
is fairly easy to construct subcategorization frames
for word occurrences: A subcategorization frame is
simply the set of the complement relations by which
dependents are attached to the word. To give the
parser access to these lists, we annotated the cate-
gory of a subcategorizing word with its subcatego-
rization frame. In this way, the parser can learn to as-
sociate the subcategorization requirements of a word
with its local syntactic context (Schiehlen, 2004).
Coordination constructions are marked either in
the conjuncts (CH, CZ, DA, DU, GE, PO, SW) or
the coordinator (AR, SL). If conjuncts show coordi-
nation, a common representation of asyndetic coor-
dination has one conjunct point to another conjunct.
It is therefore important to distinguish coordinators
from conjuncts. Coordinators are either singled out
by special dependency relations (DA, PO, SW) or by
their POS tags (CH, DU). In German, the first con-
junct phrase is merged with the whole coordinated
phrase (due to a conversion error?) so that determin-
ing the coordinator as a head is not possible.
We also experimented with attaching the POS
tags of heads to the categories of their adjunct de-
pendents. In this way, the parser could differenti-
ate between e.g. verbal and nominal adjuncts. In
our experiments, the performance gains achieved by
this strategy were low, so we did not incorporate it
into the system. Possibly, better results could be
achieved by restricting annotation to special classes
of adjuncts or by generalizing the heads? POS tags.
3.4 Categories
As the treebanks provide a lot of information with
every word token, it is a delicate question to de-
232
Ch Da Du Ge Ja Po Sp Tu
coarse POS 72.99 69.38 69.27 ? 79.07 66.09
fine POS 61.21 69.78 67.72 7.40 73.44 71.75 54.96
POS + feat ? 42.67 40.40 ?
dep-rel 76.61 72.77 70.70 70.31 78.12 72.93 66.93 65.03
coarse + dep-rel 77.61 67.56 69.43 ? 81.36 64.03
fine + dep-rel 51.21 57.72 68.55 46.28 36.59 54.97
Figure 1: Types of Categories (Development Results)
cide on the type and granularity of the information to
use in the categories of the grammar. The treebanks
specify for every word a (fine-grained) POS tag, a
coarse-grained POS tag, a collection of morphosyn-
tactic features, and a dependency relation (dep-rel).
Only the dependency relation is really orthogonal;
the other slots contain various generalizations of the
same morphological information. We tested sev-
eral options: coarse-grained POS tag (if available),
fine-grained POS tag, fine-grained POS tag with
morphosyntactic features (if available), name of de-
pendency relation, and the combinations of coarse-
grained or fine-grained POS tags with the depen-
dency relation.
Figure 1 shows F-score results on the develop-
ment set for several languages and different com-
binations. The best overall performer is dep-rel;
this somewhat astonishing fact may be due to the
superior quality of the annotations in this slot (de-
pendency relations were annotated by hand, POS
tags automatically). Furthermore, being checked in
evaluation, dependency relations directly affect per-
formance. Since we wanted a general language-
independent strategy, we used always the dep-rel
tags but for Japanese. The Japanese treebank fea-
tures only 8 different dependency relations, so we
added coarse-grained POS tag information. In the
categories for Czech, we deleted the suffixes mark-
ing coordination, apposition and parenthesis (Co,
Ap, Pa), reducing the number of categories roughly
by a factor of four. In coordination, conjuncts inherit
the dep-rel category from the parent.
Whereas the dep-rel information is submitted to
the parser directly in terms of the categories, the
information in the lemma, POS tag and morpho-
syntactic features slot was used only for back-off
smoothing when associating lexical items with cate-
Cz Ge Sp Sw
dep-rel 52.66 70.31 66.93 72.91
new classific 58.92 74.32 66.09 61.59
new + dep-rel 56.94 78.40 64.03 66.32
Figure 4: Manual POS Tag Classes (Development)
gories. A grammar with this configuration was used
to produce the results submitted (cf. line labelled CF
in Figures 2 and 3).
Instead of using the category generalizations sup-
plied with the treebanks directly, manual labour can
be put into discovering classifications that behave
better for the purposes of statistical parsing. So,
Collins et al (1999) proposed a tag classification
for parsing the Czech treebank. We also investi-
gated a classification for German1, as well as one for
Swedish and one for Spanish, which were modelled
after the German classification. The results in Fig-
ure 4 show that new classifications may have a dra-
matic effect on performance if the treebank is suf-
ficiently large. In the interest of generality, we did
not make use of the language dependent tag classifi-
cations for the results submitted, but we will never-
theless report results that could have been achieved
with these classifications.
3.5 Markovization
Another strategy that is often used in statistical pars-
ing is Markovization (Collins, 1999): Treebanks
1punctuation {$( $? $, $.} adjectives {ADJA ADJD CARD}
adverbs {ADV PROAV PTKA PTKNEG PTKVZ PWAV}
prepositions {APPR APPO APZR APPRART KOKOM} nouns
{NN NE NNE PDS PIS PPER PPOSS PRELS PRF PWS
SYM} determiners {ART PDAT PIAT PRELAT PPOSAT
PWAT} verb forms {VAFIN VMFIN VVFIN} {VAIMP
VVIMP} {VAINF VMINF VVINF} {VAPP VMPP VVPP}
{VVIZU PTKZU} clause-like items {ITJ PTKANT KOUS}
233
Ar Ch Cz Da Du Ge Ja Po Sl Sp Sw Tu Bu
Best 66.91 89.96 80.18 84.79 79.19 87.34 91.65 87.60 73.44 82.25 84.58 65.68 87.57
Average 59.94 78.32 67.17 76.16 70.73 78.58 85.86 80.63 65.16 73.52 76.44 55.95 79.98
CF (submitted) 44.39 66.20 53.34 76.05 72.11 68.73 83.35 71.01 50.72 46.96 71.10 49.81 ?
MaxEnt 59.16 61.65 63.28 73.25 64.47 73.94 82.79 80.30 66.27 69.73 72.99 47.16 ?
combined 61.82 73.34 71.74 79.64 75.51 80.75 88.15 82.43 67.09 71.15 76.88 53.65 ?
CF+Markov 45.37 70.76 55.14 74.49 72.55 68.87 84.57 71.89 55.16 47.95 71.18 51.64 ?
CFM+newcl 73.84 62.10 77.76 49.61 ?
combined 76.84 72.76 82.59 69.38 72.57 ?
new rules (in %) 7.15 6.03 4.64 7.34 5.03 7.42 5.59 6.69 21.00 9.50 10.14 14.23
Figure 2: Labelled Accuracy Results on the Test Sets
Ar Ch Cz Da Du Ge Ja Po Sl Sp Sw Tu
CF 41.91 76.61 52.66 72.77 70.69 70.31 81.36 72.76 49.00 66.93 72.91 65.03
CF+Markov 63.00 80.25 52.80 73.31 70.70 70.51 82.59 74.37 52.43 67.81 73.56 82.80
CFM+newcl 83.07 59.03 80.42 69.30
Figure 3: F Score Results on the Development Sets
usually contain very many long rules of low fre-
quency (presumably because inserting nodes costs
annotators time). Such rules cannot have an impact
in a statistical system (the line new-rules in Figure 2
shows the percentage of rules in the test set that are
not in the training set); it is better to view them as
products of a Markov process that chooses first the
head, then the symbols left of the head and finally
the symbols right of the hand. In a bigram model, the
choice of left and right siblings is made dependent
not only on the parent and head category, but also on
the last sibling on the left or right, respectively. For-
mally the probability of a rule with left hand side
 
and right hand side 	
 is bro-
ken down to the product of the probability 
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 783?792,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Bootstrapping Coreference Resolution Using Word Associations
Hamidreza Kobdani, Hinrich Schu?tze, Michael Schiehlen and Hans Kamp
Institute for Natural Language Processing
University of Stuttgart
kobdani@ims.uni-stuttgart.de
Abstract
In this paper, we present an unsupervised
framework that bootstraps a complete corefer-
ence resolution (CoRe) system from word as-
sociations mined from a large unlabeled cor-
pus. We show that word associations are use-
ful for CoRe ? e.g., the strong association be-
tween Obama and President is an indicator
of likely coreference. Association information
has so far not been used in CoRe because it is
sparse and difficult to learn from small labeled
corpora. Since unlabeled text is readily avail-
able, our unsupervised approach addresses the
sparseness problem. In a self-training frame-
work, we train a decision tree on a corpus that
is automatically labeled using word associa-
tions. We show that this unsupervised system
has better CoRe performance than other learn-
ing approaches that do not use manually la-
beled data.
1 Introduction
Coreference resolution (CoRe) is the process of find-
ing markables (noun phrases) referring to the same
real world entity or concept. Until recently, most ap-
proaches tried to solve the problem by binary classi-
fication, where the probability of a pair of markables
being coreferent is estimated from labeled data. Al-
ternatively, a model that determines whether a mark-
able is coreferent with a preceding cluster can be
used. For both pair-based and cluster-based models,
a well established feature model plays an important
role. Typical systems use a rich feature space based
on lexical, syntactic and semantic knowledge. Most
commonly used features are described by Soon et al
(2001).
Most existing systems are supervised systems,
trained on human-labeled benchmark data sets for
English. These systems use linguistic features based
on number, gender, person etc. It is a challenge to
adapt these systems to new domains, genres and lan-
guages because a significant human labeling effort is
usually necessary to get good performance.
To address this challenge, we pursue an unsuper-
vised self-training approach. We train a classifier
on a corpus that is automatically labeled using asso-
ciation information. Self-training approaches usu-
ally include the use of some manually labeled data.
In contrast, our self-trained system is not trained on
any manually labeled data and is therefore a com-
pletely unsupervised system. Although training on
automatically labeled data can be viewed as a form
of supervision, we reserve the term supervised sys-
tem for systems that are trained on manually labeled
data.
The key novelty of our approach is that we boot-
strap a competitive CoRe system from association
information that is mined from an unlabeled cor-
pus in a completely unsupervised fashion. While
this method is shallow, it provides valuable informa-
tion for CoRe because it considers the actual iden-
tity of the words in question. Consider the pair of
markables (Obama, President). It is a likely coref-
erence pair, but this information is not accessible
to standard CoRe systems because they only use
string-based features (often called lexical features),
named entity features and semantic word class fea-
tures (e.g., from WordNet) that do not distinguish,
783
say, Obama from Hawking.
In our approach, word association information is
used for clustering markables in unsupervised learn-
ing. Association information is calculated as asso-
ciation scores between heads of markables as de-
scribed below. We view association information as
an example of a shallow feature space which con-
trasts with the rich feature space that is generally
used in CoRe.
Our experiments are conducted using the
MCORE system (?Modular COreference REso-
lution?).1 MCORE can operate in three different
settings: unsupervised (subsystem A-INF), super-
vised (subsystem SUCRE (Kobdani and Schu?tze,
2010)), and self-trained (subsystem UNSEL). The
unsupervised subsystem A-INF (?Association
INFormation?) uses the association scores between
heads as the distance measure when clustering
markables. SUCRE (?SUpervised Coreference
REsolution?) is trained on a labeled corpus
(manually or automatically labeled) similar to
standard CoRe systems. Finally, the unsupervised
self-trained subsystem UNSEL (?UNsupervised
SELf-trained?) uses the unsupervised subsystem
A-INF to automatically label an unlabeled corpus
that is then used as a training set for SUCRE.
Our main contributions in this paper are as fol-
lows:
1. We demonstrate that word association informa-
tion can be used to develop an unsupervised
model for shallow coreference resolution (sub-
system A-INF).
2. We introduce an unsupervised self-trained
method (UNSEL) that takes a two-learner two-
feature-space approach. The two learners are
A-INF and SUCRE. The feature spaces are the
shallow and rich feature spaces.
3. We show that the performance of UNSEL is
better than the performance of other unsuper-
vised systems when it is self-trained on the au-
tomatically labeled corpus and uses the lever-
aging effect of a rich feature space.
4. MCORE is a flexible and modular framework
that is able to learn from data with different
1MCORE can be downloaded from ifnlp.org/
?schuetze/mcore.
quality and domain. Not only is it able to deal
with shallow information spaces (A-INF), but
it can also deliver competitive results for rich
feature spaces (SUCRE and UNSEL).
This paper is organized as follows. Related work
is discussed in Section 2. In Section 3, we present
our system architecture. Section 4 describes the ex-
periments and Section 5 presents and discusses our
results. The final section presents our conclusions.
2 Related Work
There are three main approaches to CoRe: super-
vised, semi-supervised (or weakly supervised) and
unsupervised. We use the term semi-supervised for
approaches that use some amount of human-labeled
coreference pairs.
Mu?ller et al (2002) used co-training for coref-
erence resolution, a semi-supervised method. Co-
training puts features into disjoint subsets when
learning from labeled and unlabeled data and tries
to leverage this split for better performance. Ng and
Cardie (2003) use self-training in a multiple-learner
framework and report performance superior to co-
training. They argue that the multiple learner ap-
proach is a better choice for CoRe than the multi-
ple view approach of co-training. Our self-trained
model combines multiple learners (A-INF and SU-
CRE) and multiple views (shallow/rich informa-
tion). A key difference to the work by Mu?ller et al
(2002) and Ng and Cardie (2003) is that we do not
use any human-labeled coreference pairs.
Our basic idea of self-training without human la-
bels is similar to (Kehler et al, 2004), but we ad-
dress the general CoRe problem, not just pronoun
interpretation.
Turning to unsupervised CoRe, Haghighi and
Klein (2007) proposed a generative Bayesian model
with good performance. Poon and Domingos (2008)
introduced an unsupervised system in the framework
of Markov logic. Ng (2008) presented a generative
model that views coreference as an EM clustering
process. We will show that our system, which is
simpler than prior work, outperforms these systems.
Haghighi and Klein (2010) present an ?almost un-
supervised? CoRe system. In this paper, we only
compare with completely unsupervised approaches,
784
not with approaches that make some limited use of
labeled data.
Recent work by Haghighi and Klein (2009), Klen-
ner and Ailloud (2009) and Raghunathan et al
(2010) challenges the appropriateness of machine
learning methods for CoRe. These researchers show
that a ?deterministic? system (essentially a rule-
based system) that uses a rich feature space includ-
ing lexical, syntactic and semantic features can im-
prove CoRe performance. Almost all CoRe systems,
including ours, use a limited number of rules or fil-
ters, e.g., to implement binding condition A that re-
flexives must have a close antecedent in some sense
of ?close?. In our view, systems that use a few ba-
sic filters are fundamentally different from carefully
tuned systems with a large number of complex rules,
some of which use specific lexical information. A
limitation of complex rule-based systems is that they
require substantial effort to encode the large number
of deterministic constraints that guarantee good per-
formance. Moreover, these systems are not adapt-
able (since they are not machine-learned) and may
have to be rewritten for each new domain, genre
and language. Consequently, we do not compare our
performance with deterministic systems.
Ponzetto (2010) extracts metadata from
Wikipedia for supervised CoRe. Using such
additional resources in our unsupervised system
should further improve CoRe performance. Elsner
et al (2009) present an unsupervised algorithm
for identifying clusters of entities that belong to
the same named entity (NE) class. Determining
common membership in an NE class like person is
an easier task than determining coreference of two
NEs.
3 System Architecture
Figure 1 illustrates the system architecture of our
unsupervised self-trained CoRe system (UNSEL).
Oval nodes are data, box nodes are processes. We
take a self-training approach to coreference resolu-
tion: We first label the corpus using the unsuper-
vised model A-INF and then train the supervised
model SUCRE on this automatically labeled train-
ing corpus. Even though we train on a labeled cor-
pus, the labeling of the corpus is produced in a com-
pletely automatic fashion, without recourse to hu-
Unlabeled Data
Unsupervised Model (A-INF)
Automatically Labeled Data
Supervised Model (SUCRE)
Figure 1: System Architecture of UNSEL (Unsupervised
Self-Trained Model).
man labeling. Thus, it is an unsupervised approach.
The MCORE architecture is very flexible; in par-
ticular, as will be explained presently, it can be eas-
ily adapted for supervised as well as unsupervised
settings.
The unsupervised and supervised models have an
identical top level architecture; we illustrate this in
Figure 2. In preprocessing, tokens (words), mark-
ables and their attributes are extracted from the input
text. The key difference between the unsupervised
and supervised approaches is in how pair estimation
is accomplished ? see Sections 3.1 & 3.2 for de-
tails.
The main task in chain estimation is clustering.
Figure 3 presents our clustering method, which is
used for both supervised and unsupervised CoRe.
We search for the best predicted antecedent (with
coreference probability p ? 0.5) from right to left
starting from the end of the document. McEnery et
al. (1997) showed that in 98.68% of cases the an-
tecedent is within a 10-sentence window; hence we
use a window of 10 sentences for search. We have
found that limiting the search to a window increases
both efficiency and effectiveness.
Filtering. We use a feature definition language
to define the templates according to which the fil-
ters and features are calculated. These templates
are hard constraints that filter out all cases that are
clearly disreferent, e.g., (he, she) or (he, they). We
use the following filters: (i) the antecedent of a re-
flexive pronoun must be in the same sentence; (ii)
the antecedent of a pronoun must occur at a distance
of at most 3 sentences; (iii) a coreferent pair of a
noun and a pronoun or of two pronouns must not
785
Input Text Preprocessing Markables Pair Estimation
Markable Chains Chain Estimation Markable Pairs
Figure 2: Common architecture of unsupervised (A-INF) and supervised (SUCRE) models.
Chain Estimation (M1, M2, . . . , Mn)
1. t? 1
2. For each markable Mi: Ci ? {Mi}
3. Proceed through the markables from the end
of the document. For each Mj , consider each
preceding Mi within 10 sentences:
If Pair Estimation(Mi, Mj)>=t: Ci ? Ci?Cj
4. t? t? 0.01
5. If t >= 0.5: go to step 3
Pair Estimation (Mi, Mj):
If Filtering(Mi, Mj)==FALSE then return 0;
else return the probability p (or association
score N ) of markable pair (Mi, Mj) being
coreferent.
Filtering (Mi, Mj):
return TRUE if all filters for (Mi, Mj) are
TRUE else FALSE
Figure 3: MCORE chain estimation (clustering) algo-
rithm (test). t is the clustering threshold. Ci refers to
the cluster that Mi is a member of.
disagree in number; (iv) a coreferent pair of two pro-
nouns must not disagree in gender. These four filters
are used in supervised and unsupervised modes of
MCORE.
3.1 Unsupervised Model (A-INF)
Figure 4 (top) shows how A-INF performs pair esti-
mation. First, in the pair generation step, all possible
pairs inside 10 sentences are generated. Other steps
are separately explained for train and test as follows.
Train. In addition to the filters (i)?(iv) described
above, we use the following filter: (v) If the head
of markable M2 matches the head of the preceding
markable M1, then we ignore all other pairs for M2
in the calculation of association scores.
This additional filter is necessary because an ap-
proach without some kind of string matching con-
straint yields poor results, given the importance of
string matching for CoRe. As we will show below,
even the simple filters (i)?(v) are sufficient to learn
high-quality association scores; this means that we
do not need the complex features of ?determinis-
tic? systems. However, if such complex features are
available, then we can use them to improve perfor-
mance in our self-trained setting.
To learn word association information from an
unlabeled corpus (see Section 4), we compute mu-
tual information (MI) scores between heads of mark-
ables. We defineMI as follows: (Cover and Thomas,
1991)
MI(a, b) =
?
i?{a?,a}
?
j?{b?,b}
P (i, j) log2
P (i, j)
P (i)P (j)
E.g., P (a, b?) is the probability of a pair whose two
elements are a and a word not equal to b.
Test. A key virtue of our approach is that in the
classification of pairs as coreferent/disreferent, the
coreference probability p estimated in supervised
learning plays exactly the same role as the associ-
ation information score N (defined below). For p, it
is important that we only consider pairs with p ? 0.5
as potentially coreferent (see Figure 3). To be able to
impose the same constraint on N , we normalize the
MI scores by the maximum values of the two words
and take the average:
N(a, b) =
1
2
(
MI(a, b)
argmaxxMI(a, x)
+
MI(a, b)
argmaxxMI(x, b)
)
In the above equation, the value of N indicates how
strongly two words are associated. N is normalized
to ensure 0 ? N ? 1. If a or b did not occur, then
we set N =0.
In filtering for test, we use filters (i)?(iv). We then
fetch the MI values and calculate N values. The
clustering algorithm described in Figure 3 uses these
N values in exactly the same way as p: we search for
the antecedent with the maximum association score
786
N greater than 0.5 from right to left starting from
the end of the document.
As we will see below, using N scores acquired
from an unlabeled corpus as the only source of in-
formation for CoRe performs surprising well. How-
ever, the weaknesses of this approach are (i) the fail-
ure to cover pairs that do not occur in the unlabeled
corpus (negatively affecting recall) and (ii) the gen-
eration of pairs that are not plausible candidates for
coreference (negatively affecting precision). To ad-
dress these problems, we train a model on a corpus
labeled by A-INF in a self-training approach.
3.2 Supervised Model (SUCRE)
Figure 4 (bottom) presents the architecture of pair
estimation for the supervised approach (SUCRE).
In the pair generation step for train, we take each
coreferent markable pair (Mi, Mj) without inter-
vening coreferent markables and use (Mi, Mj) as a
positive training instance and (Mi, Mk), i < k < j,
as negative training instances. For test, we generate
all possible pairs within 10 sentences. After filter-
ing, we then calculate a feature vector for each gen-
erated pair that survived filters (i)?(iv).
Our basic features are similar to those described
by Soon et al (2001): string-based features, dis-
tance features, span features, part-of-speech fea-
tures, grammatical features, semantic features, and
agreement features. These basic features are engi-
neered with the goal of creating a feature set that
will result in good performance. For this purpose
we used the relational feature engineering frame-
work which has been presented in (Kobdani et al,
2010). It includes powerful and flexible methods for
implementing and extracting new features. It allows
systematic and fast search of the space of features
and thereby reduces the time and effort needed for
defining optimal features. We believe that the good
performance of our supervised system SUCRE (ta-
bles 1 and 2) is the result of our feature engineering
approach.2
As our classification method, we use a decision
2While this is not the focus of this paper, SUCRE has per-
formance comparable to other state-of-the-art supervised sys-
tems. E.g., B3/MUC F1 are 75.6/72.4 on ACE-2 and 69.4/70.6
on MUC-6 compared to 78.3/66.0 on ACE-2 and 70.9/68.5 on
MUC-6 for Reconcile (Stoyanov et al, 2010)
tree3 (Quinlan, 1993) that is trained on the training
set to estimate the coreference probability p for a
pair and then applied to the test set. Note that, as
is standard in CoRe, filtering and feature calculation
are exactly the same for training and test, but that
pair generation is different as described above.
4 Experimental Setup
4.1 Data Sets
For computing word association, we used a cor-
pus of about 63,000 documents from the 2009 En-
glish Wikipedia (the articles that were larger than
200 bytes). This corpus consists of more than 33.8
million tokens; the average document length is 500
tokens. The corpus was parsed using the Berkeley
parser (Petrov and Klein, 2007). We ignored all sen-
tences that had no parse output. The number of de-
tected markables (all noun phrases extracted from
parse trees) is about 9 million.
We evaluate unsupervised, supervised and self-
trained models on ACE (Phase 2) (Mitchell et al,
2003).4 This data set is one of the most widely
used CoRe benchmarks and was used by the sys-
tems that are most comparable to our approach; in
particular, it was used in most prior work on unsu-
pervised CoRe. The corpus is composed of three
data sets from three different news sources. We give
the number of test documents for each: (i) Broadcast
News (BNEWS): 51. (ii) Newspaper (NPAPER):
17. (iii) Newswire (NWIRE): 29. We report re-
sults for true markables (markables extracted from
the answer keys) to be able to compare with other
systems that use true markables.
In addition, we use the recently published
OntoNotes benchmark (Recasens et al, 2010).
OntoNotes is an excerpt of news from the OntoNotes
Corpus Release 2.0 (Pradhan et al, 2007). The ad-
vantage of OntoNotes is that it contains two parallel
annotations: (i) a gold setting, gold standard manual
annotations of the preprocessing information and (ii)
an automatic setting, automatically predicted anno-
tations of the preprocessing information. The au-
tomatic setting reflects the situation a CoRe system
3We also tried support vector machines and maximum en-
tropy models, but they did not perform better.
4We used two variants of ACE (Phase 2): ACE-2 and
ACE2003
787
Markable Pairs Filtering Association Calculation
Pair Generation Filter Templates Association Information Train/Test
Markable Pairs Filtering Feature Calculation Feature Vectors
Pair Generation Filter Templates Feature Templates Train/Test
Figure 4: Pair estimation in the unsupervised model A-INF (top) and in the supervised model SUCRE (bottom).
faces in reality; in contrast, the gold setting should
be considered less realistic.
The issue of gold vs. automatic setting is directly
related to a second important evaluation issue: the
influence of markable detection on CoRe evaluation
measures. In a real application, we do not have ac-
cess to true markables, so an evaluation on system
markables (markables automatically detected by the
system) reflects actual expected performance better.
However, reporting only CoRe numbers (even for
system markables) is not sufficient either since ac-
curacy of markable detection is necessary to inter-
pret CoRe scores. Thus, we need (i) measures of
the quality of system markables (i.e., an evaluation
of the markable detection subtask) and CoRe per-
formance on system markables as well as (ii) a mea-
sure of CoRe performance on true markables. We
use OntoNotes in this paper to perform such a, in
our view, complete and realistic evaluation of CoRe.
The two evaluations correspond to the two evalua-
tions performed at SemEval-2010 (Recasens et al,
2010): the automatic setting with system markables
and the gold setting with true markables. Test set
size is 85 documents.
In the experiments with A-INF we use Wikipedia
to compute association information and then evalu-
ate the model on the test sets of ACE and OntoNotes.
For the experiments with UNSEL, we use its unsu-
pervised subsystem A-INF (which uses Wikipedia
association scores) to automatically label the train-
ing sets of ACE and OntoNotes. Then for each data
set, the supervised subsystem of UNSEL (i.e., SU-
CRE) is trained on its automatically labeled training
set and then evaluated on its test set. Finally, for
the supervised experiments, we use the manually la-
beled training sets and evaluate on the corresponding
test sets.
4.2 Evaluation Metrics
We report recall, precision, and F1 for MUC (Vilain
et al, 1995), B3 (Bagga and Baldwin, 1998), and
CEAF (Luo, 2005). We selected these three met-
rics because a single metric is often misleading and
because we need to use metrics that were used in
previous unsupervised work.
It is well known that MUC by itself is insuffi-
cient because it gives misleadingly high scores to the
?single-chain? system that puts all markables into
one chain (Luo et al, 2004; Finkel and Manning,
2008). However, B3 and CEAF have a different
bias: they give high scores to the ?all-singletons?
system that puts each markable in a separate chain.
On OntoNotes test, we get B3 = 83.2 and CEAF
= 71.2 for all-singletons, which incorrectly sug-
gests that performance is good; but MUC F1 is 0 in
this case, demonstrating that all-singletons performs
poorly. With the goal of performing a complete eval-
uation, one that punishes all-singletons as well as
single-chain, we use one of the following two com-
binations: (i) MUC and B3 or (ii) MUC and CEAF.
Recasens et al (2010) showed that B3 and CEAF
are highly correlated (Pearson?s r = 0.91). There-
fore, either combination (i) or combination (ii) fairly
characterizes CoRe performance.
5 Results and Discussion
Table 1 compares our unsupervised self-trained
model UNSEL and unsupervised model A-INF to
788
MUC B3 CEAF
BNEWS-ACE-2 Prec. Rec. F1 Prec. Rec. F1 Prec. Rec. F1
1 P&D 68.3 66.6 67.4 70.3 65.3 67.7 ? ? ?
2 A-INF 60.8 61.4 61.1 55.5 69.0 61.5 52.6 52.0 52.3
3 UNSEL 72.5 65.6 68.9 72.5 66.4 69.3 56.7 64.8 60.5
4 SUCRE 86.6 60.3 71.0 87.6 64.6 74.4 56.1 81.6 66.5
NWIRE-ACE-2 Prec. Rec. F1 Prec. Rec. F1 Prec. Rec. F1
5 P&D 67.7 67.3 67.4 74.7 68.8 71.6 ? ? ?
6 A-INF 62.4 57.4 59.8 59.2 62.4 60.7 46.8 52.5 49.5
7 UNSEL 76.2 61.5 68.1 81.5 67.6 73.9 61.5 77.1 68.4
8 SUCRE 82.5 65.7 73.1 85.4 72.3 78.3 63.5 80.6 71.0
NPAPER-ACE-2 Prec. Rec. F1 Prec. Rec. F1 Prec. Rec. F1
9 P&D 69.2 71.7 70.4 70.0 66.5 68.2 ? ? ?
10 A-INF 60.6 56.0 58.2 52.4 60.3 56.0 38.9 44.0 41.3
11 UNSEL 78.6 65.7 71.6 74.0 68.0 70.9 57.6 73.2 64.5
12 SUCRE 82.5 67.0 73.9 80.7 69.5 74.6 58.8 77.1 66.7
BNEWS-ACE2003 Prec. Rec. F1 Prec. Rec. F1 Prec. Rec. F1
13 H&K 68.3 56.8 62.0 ? ? ? 59.9 53.9 56.7
14 Ng 71.4 56.1 62.8 ? ? ? 60.5 53.3 56.7
15 A-INF 60.9 64.9 62.8 50.9 72.5 59.8 53.8 49.4 51.5
16 UNSEL 69.5 65.0 67.1 70.2 65.9 68.0 58.5 64.2 61.2
17 SUCRE 73.9 68.5 71.1 75.4 69.6 72.4 60.1 66.6 63.2
NWIRE-ACE2003 Prec. Rec. F1 Prec. Rec. F1 Prec. Rec. F1
18 H&K 66.2 46.8 54.8 ? ? ? 62.8 49.6 55.4
19 Ng 68.3 47.0 55.7 ? ? ? 60.7 49.2 54.4
20 A-INF 62.7 60.5 61.6 54.8 66.1 59.9 47.7 50.2 49.0
21 UNSEL 64.8 68.6 66.6 61.5 73.6 67.0 59.8 55.1 57.3
22 SUCRE 77.6 69.3 73.2 78.8 75.2 76.9 65.1 74.4 69.5
Table 1: Scores for MCORE (A-INF, SUCRE and UNSEL) and three comparable systems on ACE-2 and ACE2003.
P&D (Poon and Domingos, 2008) on ACE-2; and
to Ng (Ng, 2008) and H&K5 (Haghighi and Klein,
2007) on ACE2003. To our knowledge, these three
papers are the best and most recent evaluation results
for unsupervised learning and they all report results
on ACE-2 and ACE-2003. Results on SUCRE will
be discussed later in this section.
A-INF scores are below some of the earlier unsu-
pervised work reported in the literature (lines 2, 6,
10) although they are close to competitive on two
of the datasets (lines 15 and 20: MUC scores are
equal or better, CEAF scores are worse). Given the
simplicity of A-INF, which uses nothing but asso-
5We report numbers for the better performing Pronoun-only
Salience variant of H&K proposed by Ng (2008).
ciations mined from a large unannotated corpus, its
performance is surprisingly good.
Turning to UNSEL, we see that F1 is always bet-
ter for UNSEL than for A-INF, for all three mea-
sures (lines 3 vs 2, 7 vs 6, 11 vs 10, 16 vs 15, 21
vs 20). This demonstrates that the self-training step
of UNSEL is able to correct many of the errors that
A-INF commits. Both precision and recall are im-
proved with two exceptions: recall of B3 decreases
from line 2 to 3 and from 15 to 16.
When comparing the unsupervised system UN-
SEL to previous unsupervised results, we find that
UNSEL?s F1 is higher in all runs (lines 3 vs 1, 7 vs
5, 11 vs 9, 16 vs 13&14, 21 vs 18&19). The differ-
ences are large (up to 11%) compared to H&K and
789
Ng. The difference to P&D is smaller, ranging from
2.7% (B3, lines 11 vs 9) to 0.7% (MUC, lines 7 vs
5). Given that MCORE is a simpler and more ef-
ficient system than this prior work on unsupervised
CoRe, these results are promising.
In contrast to F1, there is no consistent trend for
precision and recall. For example, P&D is better
than UNSEL on MUC recall for BNEWS-ACE-2
(lines 1 vs 3) and H&K is better than UNSEL on
CEAF precision for NWIRE-ACE2003 (lines 18 vs
21). But this higher variability for precision and re-
call is to be expected since every system trades the
two measures off differently.
These results show that the application of self-
training significantly improves performance. As dis-
cussed in Section 3.1, self-training has positive ef-
fects on both recall and precision. We now present
two simplified examples that illustrate this point.
Example for recall. Consider the markable pair
(Novoselov6,he) in the test set. Its N score is 0 be-
cause our subset of 2009 Wikipedia sentences has
no occurrence of Novoselov. However, A-INF finds
many similar pairs like (Einstein,he) and (Hawk-
ing,he), pairs that have high N scores. Suppose
we represent pairs using the following five fea-
tures: <sentence distance, string match, type of
first markable, type of second markable, number
agreement>. Then (Einstein,he), (Hawking,he) and
(Novoselov,he) will all be assigned the feature vector
<1, No, Proper Noun, Personal Pronoun, Yes>. We
can now automatically label Wikipedia using A-INF
? this will label (Einstein,he) and (Hawking,he) as
coreferent ? and train SUCRE on the resulting train-
ing set. SUCRE can then resolve the coreference
(Novoselov,he) correctly. We call this the better re-
call effect.
Example for precision. Using the same repre-
sentation of pairs, suppose that for the sequence of
markables Biden, Obama, President the markable
pairs (Biden,President) and (Obama,President) are
assigned the feature vectors <8, No, Proper Noun,
Proper Noun, Yes> and <1, No, Proper Noun,
Proper Noun, Yes>, respectively. Since both pairs
have N scores > 0.5, A-INF incorrectly puts the
three markables into one cluster. But as we would
expect, A-INF labels many more markable pairs
6The 2010 physics Nobel laureate.
 10
 20
 30
 40
 50
 60
 70
 80
 100  20000  40000  60000
Pr
ec
., 
R
ec
. a
nd
 F
1
Number of input Wikipedia articles
MUC-Prec.
MUC-Rec.
MUC-F1
Figure 5: MUC learning curve for A-INF.
with the second feature vector (distance=1) as coref-
erent than with the first one (distance=8) in the en-
tire automatically labeled training set. If we now
train SUCRE on this training set, it can resolve such
cases in the test set correctly even though they are
so similar: (Biden,President) is classified as disref-
erent and (Obama,President) as coreferent. We call
this the better precision effect.
Recall that UNSEL has better recall and precision
than A-INF in almost all cases (discussion of Ta-
ble 1). This result shows that better precision and
better recall effects do indeed benefit UNSEL.
To summarize, the advantages of our self-training
approach are: (i) We cover cases that do not occur
in the unlabeled corpus (better recall effect); and (ii)
we use the leveraging effect of a rich feature space
including distance, person, number, gender etc. to
improve precision (better precision effect).
Learning curve. Figure 5 presents MUC scores
of A-INF as a function of the number of Wikipedia
articles used in unsupervised learning. We can see
that a small number of input articles (e.g., 100) re-
sults in low recall and high precision. When we in-
crease the number of input articles, recall rapidly in-
creases and precision rapidly decreases up to about
10,000 articles. Increase and decrease continue
more slowly after that. F1 increases throughout be-
cause lower precision is compensated by higher re-
call. This learning curve demonstrates the impor-
tance of the size of the corpus for A-INF.
Comparison of UNSEL with SUCRE
Table 2 compares our unsupervised self-trained
(UNSEL) and supervised (SUCRE) models with
the recently published SemEval-2010 OntoNotes re-
790
Gold setting + True markables
System MD MUC B3 CEAF
Relax 100 33.7 84.5 75.6
SUCRE2010 100 60.8 82.4 74.3
SUCRE 100 64.3 87.0 80.1
UNSEL 100 63.0 86.9 79.7
Automatic setting + System markables
System MD MUC B3 CEAF
SUCRE2010 80.7 52.5 67.1 62.7
Tanl-1 73.9 24.6 61.3 57.3
SUCRE 80.9 55.7 69.7 66.6
UNSEL 80.9 55.0 69.8 66.3
Table 2: F1 scores for MCORE (SUCRE and UNSEL)
and the best comparable systems in SemEval-2010. MD:
Markable Detection F1 (Recasens et al, 2010).
sults (gold and automatic settings). We compare
with the scores of the two best systems, Relax and
SUCRE20107 (for the gold setting with true mark-
ables) and SUCRE2010 and Tanl-1 (for the automatic
setting with system markables, 89.9% markable de-
tection (MD) F1). It is apparent from this table that
our supervised and unsupervised self-trained mod-
els outperform Relax, SUCRE2010 and Tanl-1. We
should make clear that we did not use the test set for
development to ensure a fair comparison with the
participant systems at SemEval-2010.
Table 1 shows that the unsupervised self-trained
system (UNSEL) does a lot worse than the su-
pervised system (SUCRE) on ACE.8 In contrast,
UNSEL performs almost as well as SUCRE on
OntoNotes (Table 2), for both gold and automatic
settings: F1 differences range from +.1 (Auto-
matic, B3) to ?1.3 (Gold, MUC). We suspect that
this is partly due to the much higher proportion
of singletons in OntoNotes than in ACE-2: 85.2%
(OntoNotes) vs. 60.2% (ACE-2). The low recall of
the automatic labeling by A-INF introduces a bias
for singletons when UNSEL is self-trained. Another
reason is that the OntoNotes training set is about
4 times larger than each of BNEWS, NWIRE and
7It is the first version of our supervised system that took part
in SemEval-2010. We call it SUCRE2010.
8A reviewer observes that SUCRE?s performance is better
than the supervised system of Ng (2008). This may indicate
that part of our improved unsupervised performance in Table 1
is due to better feature engineering implemented in SUCRE.
NPAPER training sets. With more training data,
UNSEL can correct more of its precision and re-
call errors. For an unsupervised approach, which
only needs unlabeled data, there is little cost to cre-
ating large training sets. Thus, this comparison of
ACE-2/Ontonotes results is evidence that in a realis-
tic scenario using association information in an un-
supervised self-trained system is almost as good as
a system trained on manually labeled data.
It is important to note that the comparison of
SUCRE to UNSEL is the most direct comparison
of supervised and unsupervised CoRe learning we
are aware of. The two systems are identical with the
single exception that they are trained on manual vs.
automatic coreference labels.
6 Conclusion
In this paper, we have demonstrated the utility of
association information for coreference resolution.
We first developed a simple unsupervised model for
shallow CoRe that only uses association information
for finding coreference chains. We then introduced
an unsupervised self-trained approach where a su-
pervised model is trained on a corpus that was auto-
matically labeled by the unsupervised model based
on the association information. The results of the ex-
periments indicate that the performance of the unsu-
pervised self-trained approach is better than the per-
formance of other unsupervised learning systems. In
addition, we showed that our system is a flexible and
modular framework that is able to learn from data
with different quality (perfect vs noisy markable de-
tection) and domain; and is able to deliver good re-
sults for shallow information spaces and competitive
results for rich feature spaces. Finally, our frame-
work is the first CoRe system that is designed to sup-
port three major modes of machine learning equally
well: supervised, self-trained and unsupervised.
Acknowledgments
This research was funded by DFG (grant SCHU
2246/4).
We thank Aoife Cahill, Alexander Fraser, Thomas
Mu?ller and the anonymous reviewers for their help-
ful comments.
791
References
Amit Bagga and Breck Baldwin. 1998. Algorithms for
scoring coreference chains. In LREC Workshop on
Linguistics Coreference ?98, pages 563?566.
Thomas M. Cover and Joy A. Thomas. 1991. Elements
of Information Theory. Wiley.
Micha Elsner, Eugene Charniak, and Mark Johnson.
2009. Structured generative models for unsupervised
named-entity clustering. In HLT-NAACL ?09, pages
164?172.
Jenny Rose Finkel and Christopher D. Manning. 2008.
Enforcing transitivity in coreference resolution. In
HLT ?08, pages 45?48.
Aria Haghighi and Dan Klein. 2007. Unsupervised
coreference resolution in a nonparametric bayesian
model. In ACL ?07, pages 848?855.
Aria Haghighi and Dan Klein. 2009. Simple coreference
resolution with rich syntactic and semantic features. In
EMNLP ?09, pages 1152?1161.
Aria Haghighi and Dan Klein. 2010. Coreference resolu-
tion in a modular, entity-centered model. In NAACL-
HLT ?10, pages 385?393.
Andrew Kehler, Douglas E. Appelt, Lara Taylor, and
Aleksandr Simma. 2004. Competitive Self-Trained
Pronoun Interpretation. In HLT-NAACL ?04, pages
33?36.
Manfred Klenner and ?Etienne Ailloud. 2009. Opti-
mization in coreference resolution is not needed: A
nearly-optimal algorithm with intensional constraints.
In EACL, pages 442?450.
Hamidreza Kobdani and Hinrich Schu?tze. 2010. Sucre:
A modular system for coreference resolution. In Se-
mEval ?10, pages 92?95.
Hamidreza Kobdani, Hinrich Schu?tze, Andre Burkovski,
Wiltrud Kessler, and Gunther Heidemann. 2010. Re-
lational feature engineering of natural language pro-
cessing. In CIKM ?10. ACM.
Xiaoqiang Luo, Abe Ittycheriah, Hongyan Jing, Nanda
Kambhatla, and Salim Roukos. 2004. A
Mention-Synchronous Coreference Resolution Algo-
rithm Based on the Bell Tree. In ACL ?04, pages 135?
142.
Xiaoqiang Luo. 2005. On coreference resolution perfor-
mance metrics. In HLT ?05, pages 25?32.
A. McEnery, I. Tanaka, and S. Botley. 1997. Corpus
annotation and reference resolution. In ANARESOLU-
TION ?97, pages 67?74.
Alexis Mitchell, Stephanie Strassel, Mark Przybocki,
JK Davis, George Doddington, Ralph Grishman,
Adam Meyers, Ada Brunstein, Lisa Ferro, and Beth
Sundheim. 2003. ACE-2 version 1.0. Linguistic Data
Consortium, Philadelphia.
Christoph Mu?ller, Stefan Rapp, and Michael Strube.
2002. Applying co-training to reference resolution. In
ACL ?02, pages 352?359.
Vincent Ng and Claire Cardie. 2003. Bootstrapping
coreference classifiers with multiple machine learning
algorithms. In EMNLP ?03, pages 113?120.
Vincent Ng. 2008. Unsupervised models for coreference
resolution. In EMNLP ?08, pages 640?649.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In HLT-NAACL ?07, pages
404?411.
Simone Paolo Ponzetto. 2010. Knowledge Acquisi-
tion from a Collaboratively Generated Encyclopedia,
volume 327 of Dissertations in Artificial Intelligence.
Amsterdam, The Netherlands: IOS Press & Heidel-
berg, Germany: AKA Verlag.
Hoifung Poon and Pedro Domingos. 2008. Joint unsu-
pervised coreference resolution with markov logic. In
EMNLP ?08, pages 650?659.
Sameer S. Pradhan, Eduard Hovy, Mitch Marcus, Martha
Palmer, Lance Ramshaw, and Ralph Weischedel.
2007. Ontonotes: A unified relational semantic rep-
resentation. In ICSC ?07, pages 517?526.
J. Ross Quinlan. 1993. C4.5: Programs for machine
learning. Morgan Kaufmann Publishers Inc., San
Francisco, CA, USA.
Karthik Raghunathan, Heeyoung Lee, Sudarshan Ran-
garajan, Nathanael Chambers, Mihai Surdeanu, Dan
Jurafsky, and Christopher Manning. 2010. A multi-
pass sieve for coreference resolution. In EMNLP ?10,
pages 492?501.
Marta Recasens, Llu??s Ma`rquez, Emili Sapena,
M.Anto`nia Mart??, Mariona Taule?, Ve?ronique Hoste,
Massimo Poesio, and Yannick Versley. 2010.
SemEval-2010 Task 1: Coreference resolution in
multiple languages. In SemEval ?10, pages 70?75.
Wee Meng Soon, Hwee Tou Ng, and Daniel Chung Yong
Lim. 2001. A machine learning approach to coref-
erence resolution of noun phrases. In CL ?01, pages
521?544.
Veselin Stoyanov, Claire Cardie, Nathan Gilbert, Ellen
Riloff, David Buttler, and David Hysom. 2010. Coref-
erence resolution with reconcile. In ACL ?10, pages
156?161.
Marc Vilain, John Burger, John Aberdeen, Dennis Con-
nolly, and Lynette Hirschman. 1995. A model-
theoretic coreference scoring scheme. In MUC6 ?95,
pages 45?52.
792
