Combining Multiple Models for Speech Information Retrieval 
Muath Alzghool and Diana Inkpen  
School of Information Technology and Engineering 
University of Ottawa 
{alzghool,diana}@ site.uottawa.ca  
Abstract 
In this article we present a method for combining different information retrieval models in order to increase the retrieval performance 
in a Speech Information Retrieval task. The formulas for combining the models are tuned on training data. Then the system is evaluated 
on test data. The task is particularly difficult because the text collection is automatically transcribed spontaneous speech, with many 
recognition errors. Also, the topics are real information needs, difficult to satisfy. Information Retrieval systems are not able to obtain 
good results on this data set, except for the case when manual summaries are included. 
 
1. Introduction  
Conversational speech such as recordings of interviews or 
teleconferences is difficult to search through. The 
transcripts produced with Automatic Speech Recognition 
(ASR) systems tend to contain many recognition errors, 
leading to low Information Retrieval (IR) performance 
(Oard et al, 2007). 
Previous research has explored the idea of combining 
the results of different retrieval strategies; the motivation is 
that each technique will retrieve different sets of relevant 
documents; therefore combining the results could produce 
a better result than any of the individual techniques. We 
propose new data fusion techniques for combining the 
results of different IR models. We applied our data fusion 
techniques to the Mallach collection (Oard et al, 2007) 
used in the Cross-Language Speech Retrieval (CLSR) task 
at Cross-Language Evaluation Forum (CLEF) 2007. The 
Mallach collection comprises 8104 ?documents? which are 
manually-determined topically-coherent segments taken 
from 272 interviews with Holocaust survivors, witnesses 
and rescuers, totalling 589 hours of speech. Figure 1 shows 
the document structure in CLSR test collection, two ASR 
transcripts are available for this data, in this work we use 
the ASRTEXT2004A field provided by IBM research with 
a word error rate of 38%. Additionally, metadata fields for 
each document include: two sets of 20 automatically 
assigned keywords determined using two different kNN 
classifiers (AK1 and AK2), a set of a varying number of 
manually-assigned keywords (MK), and a manual 
3-sentence summary written by an expert in the field.  A set 
of 63 training topics and 33 test topics were generated for 
this task. The topics provided with the collection were 
created in English from actual user requests. Topics were 
structured using the standard TREC format of Title, 
Description and Narrative fields. To enable CL-SR 
experiments the topics were translated into Czech, German, 
French, and Spanish by native speakers; Figure 2 and 3 
show two examples for English and its translation in 
French respectively. Relevance judgments were generated 
using a search-guided procedure and standard pooling 
methods. See (Oard et al, 2004) for full details of the 
collection design.  
We present results on the automatic transcripts for 
English queries and translated queries (cross-language) 
for two combination methods; we also present results 
when manual summaries and manual keywords are 
indexed. 
 
<DOC> 
<DOCNO>VHF[IntCode]-[SegId].[SequenceNum]</DOCNO\> 
<INTERVIEWDATA>Interviewee name(s) and 
birthdate</INTERVIEWDATA> 
<NAME>Full name of every person mentioned</NAME> 
<MANUALKEYWORD>Thesaurus keywords assigned to the 
segment</MANUALKEYWORD> 
<SUMMARY>3-sentence segment summary</SUMMARY> 
<ASRTEXT2004A>ASR transcript produced in 
2004</ASRTEXT2004A> 
<ASRTEXT2006A>ASR transcript produced in 
2006</ASRTEXT2006A> 
<AUTOKEYWORD2004A1>Thesaurus keywords from a kNN 
classifier</AUTOKEYWORD2004A1> 
<AUTOKEYWORD2004A2>Thesaurus keywords from a second 
kNN classifier</AUTOKEYWORD2004A2> 
</DOC> 
Figure 1. Document structure in CL-SR test collection. 
 
<top>  
<num>1159  
<title>Child survivors in Sweden  
<desc>Describe survival mechanisms of children born 
in 1930-1933 who spend the war in concentration 
camps or in hiding and who presently live in Sweden. 
 <narr>The relevant material should describe the 
circumstances and inner resources of the surviving 
children. The relevant material also describes how 
the wartime experience affected their post-war 
adult life. </top> 
Figure 2. Example for English topic in CL-SR test collection. 
 
<top>  
<num>1159  
<title>Les enfants survivants en Su?de  
<desc>Descriptions des m?canismes de survie des 
enfants n?s entre 1930 et 1933 qui ont pass? la 
guerre en camps de concentration ou cach?s et qui 
vivent actuellement en Su?de.  
<narr>? 
</top>  
Figure 3. Example for French topic in CL-SR test collection. 
2. System Description  
Our Cross-Language Information Retrieval systems 
were built with off-the-shelf components. For the retrieval 
part, the SMART (Buckley, Salton, &Allan, 1992; Salton 
&Buckley, 1988) IR system and the Terrier (Amati &Van 
Rijsbergen, 2002; Ounis et al, 2005) IR system were 
tested with many different weighting schemes for 
indexing the collection and the queries.  
SMART was originally developed at Cornell 
University in the 1960s. SMART is based on the vector 
space model of information retrieval. We use the standard 
notation: weighting scheme for the documents, followed 
by dot, followed by the weighting scheme for the queries, 
each term-weighting scheme is described as a 
combination of term frequency, collection frequency, and 
length normalization components where the schemes are 
abbreviated according to its components variations (n no 
normalization, c cosine, t idf, l log, etc.) We used nnn.ntn, 
ntn.ntn, lnn.ntn, ann.ntn, ltn.ntn, atn.ntn, ntn.nnn , 
nnc.ntc, ntc.ntc, ntc.nnc, lnc.ntc, anc.ntc, ltc.ntc, atc.ntc 
weighting schemes (Buckley, Salton, &Allan, 1992; 
Salton &Buckley, 1988);  lnn.ntn performs very well in 
CLEF-CLSR 2005 and 2006 (Alzghool &Inkpen, 2007; 
Inkpen, Alzghool, &Islam, 2006); lnn.ntn means that lnn 
was used for documents and ntn for queries according to 
the following formulas:  
0.1)ln(nln += tfweight        (1) 
tn
Ntfweight logntn ?=     (2)      
where tf denotes the term frequency of a term t in the 
document or query, N denotes the number of documents 
in the collection, and nt denotes the number of documents 
in which the term t occurs.  
Terrier was originally developed at the University of 
Glasgow. It is based on Divergence from Randomness 
models (DFR) where IR is seen as a probabilistic process 
(Amati &Van Rijsbergen, 2002; Ounis et al, 2005). We 
experimented with the In_expC2 (Inverse Expected 
Document Frequency model with Bernoulli after-effect 
and normalization) weighting model, one of Terrier?s 
DFR-based document weighting models.  
Using the In_expC2 model, the relevance score of a 
document d for a query q is given by the formula: 
                  (3) ?
?
=
qt
dtwqtfqdsim ),(.),(
where qtf is the frequency of term t in the query q, and w(t,d) 
is the relevance score of a document d for the query term t, 
given by: 
)
5.0
1log()
)1(
1(),( 2 +
+??+?
+=
e
e
et n
Ntfn
tfnn
Fdtw   (4) 
where 
-F is the term frequency of t in the whole collection. 
-N is the number of document in the whole collection.  
-nt is the document frequency of t. 
-ne is given by ))
1
(1( Fte N
n
Nn
???=  (5) 
- tfne is the normalized within-document frequency of the 
term t in the document d. It is given by the normalization 2 
(Amati &Van Rijsbergen, 2002; Ounis et al, 2005): 
)_1(log
l
lavgctftfn ee ?+?=     (6) 
where c is a parameter, tf is the within-document 
frequency of the term t in the document d, l is the 
document length, and avg_l is the average document 
length in the whole collection. 
We estimated the parameter c of the Terrier's 
normalization 2 formula by running some experiments on 
the training data, to get the best values for c depending on 
the topic fields used. We obtained the following values: 
c=0.75 for queries using the Title only, c=1 for queries 
using the Title and Description fields, and c=1 for queries 
using the Title, Description, and Narrative fields. We select 
the c value that has a best MAP score according to the 
training data. 
For translating the queries from French and Spanish 
into English, several free online machine translation tools 
were used. The idea behind using multiple translations is 
that they might provide more variety of words and 
phrases, therefore improving the retrieval performance. 
Seven online MT systems (Inkpen, Alzghool, &Islam, 
2006) were used for translating from Spanish and from 
French into English. We combined the outputs of the MT 
systems by simply concatenating all the translations. All 
seven translations of a title made the title of the translated 
query; the same was done for the description and narrative 
fields.  
We propose two methods for combining IR models. We 
use the sum of normalized weighted similarity scores of 15 
different IR schemes as shown in the following formulas: 
 
 ?
?
?+=
schemsIRi
iMAPr NormSimiWiWFusion )]()([1
34      (7) 
?
?
?=
schemsIRi
iMAPr NormSimiWiWFusion )(*)(2
34      (8)                         
where Wr(i) and WMAP(i) are experimentally determined 
weights based on the recall (the number of relevant 
documents retrieved) and precision (MAP score) values for 
each IR scheme computed on the training data. For 
example, suppose that two retrieval runs r1 and r2 give 0.3 
and 0.2 (respectively) as  MAP scores on training data; we 
normalize these scores by dividing them by the maximum 
MAP value: then WMAP(r1) is 1 and WMAP(r2) is 0.66 (then 
we compute the power 3 of these weights, so that one 
weight stays 1 and the other one decreases; we chose power 
3 for MAP score and power 4 for recall, because the MAP 
is more important than the recall). We hope that when we 
multiply the similarity values with the weights and take the 
summation over all the runs, the performance of the 
combined run will improve. NormSimi is the normalized 
similarity for each IR scheme. We did the normalization by 
dividing the similarity by the maximum similarity in the 
run. The normalization is necessary because different 
weighting schemes will generate different range of 
similarity values, so a normalization method should 
applied to each run.  Our method is differed than the work 
done by Fox and Shaw in (1994), and Lee in ( 1995); they 
combined the results by taking the summation of the 
similarity scores without giving any weight to each run. In 
our work we weight each run according to the precision 
and recall on the training data.  
3. Experimental Results 
We applied the data fusion methods described in section 2 
to 14 runs produced by SMART and one run produced by 
Terrier.  Performance results for each single run and fused 
runs are presented in Table 1, in which % change is given 
with respect to the run providing better effectiveness in 
each combination on the training data. The Manual 
English column represents the results when only the 
manual keywords and the manual summaries were used 
for indexing the documents using English topics, the 
Auto-English column represents the results when 
automatic transcripts are indexed from the documents, for 
English topics. For cross-languages experiments the 
results are represented in the columns Auto-French, and 
Auto-Spanish, when using the combined translations 
produced by the seven online MT tools, from French and 
Spanish into English. Since the result of combined 
translation for each language was better than when using 
individual translations from each MT tool on the training 
data (Inkpen, Alzghool, &Islam, 2006), we used only the 
combined translations in our experiments. 
Data fusion helps to improve the performance (MAP 
score) on the test data. The best improvement using data 
fusion (Fusion1) was on the French cross-language 
experiments with 21.7%, which is statistically significant 
while on monolingual the improvement was only 6.5% 
which is not significant. We computed these 
improvements relative to the results of the best 
single-model run, as measured on the training data. This 
supports our claim that data fusion improves the recall by 
bringing some new documents that were not retrieved by 
all the runs. On the training data, the Fusion2 method 
gives better results than Fusion1 for all cases except on 
Manual English, but on the test data Fusion1 is better than 
Fusion2. In general, the data fusion seems to help, 
because the performance on the test data in not always 
good for weighting schemes that obtain good results on 
the training data, but combining models allows the 
best-performing weighting schemes to be taken into 
consideration. 
The retrieval results for the translations from French 
were very close to the monolingual English results, 
especially on the training data, but on the test data the 
difference was significantly worse. For Spanish, the 
difference was significantly worse on the training data, 
but not on the test data.  
Experiments on manual keywords and manual 
summaries available in the test collection showed high 
improvements, the MAP score jumped from 0.0855 to 
0.2761 on the test data. 
4. Conclusion 
We experimented with two different systems: Terrier 
and SMART, with combining the various weighting 
schemes for indexing the document and query terms. We 
proposed two methods to combine different weighting 
scheme from different systems, based on weighted 
summation of normalized similarity measures; the weight 
for each scheme was based on the relative precision and 
recall on the training data. Data fusion helps to improve 
the retrieval significantly for some experiments 
(Auto-French) and for other not significantly (Manual 
English). Our result on automatic transcripts for English 
queries (the required run for the CLSR task at CLEF 
2007), obtained a MAP score of 0.0855. This result was 
significantly better than the other 4 systems that 
participated in the CLSR task at CLEF 2007(Pecina et al, 
2007). 
In future work we plan to investigate more methods of 
data fusion (to apply a normalization scheme scalable to 
unseen data), removing or correcting some of the speech 
recognition errors in the ASR content words, and to use 
speech lattices for indexing.  
5. References 
 
Alzghool, M. & Inkpen, D. (2007). Experiments for the 
cross language speech retrieval task at CLEF 2006. In 
C. Peters, (Ed.), Evaluation of multilingual and 
multi-modal information retrieval (Vol. 4730/2007, 
pp. 778-785). Springer. 
Amati, G. & Van Rijsbergen, C. J. (2002). Probabilistic 
models of information retrieval based on measuring 
the divergence from randomness (Vol. 20). ACM,  
New York. 
Buckley, C., Salton, G., & Allan, J. (1992). Automatic 
retrieval with locality information using smart. In 
Text retrieval conferenc (TREC-1) (pp. 59-72). 
Inkpen, D., Alzghool, M., & Islam, A. (2006). Using 
various indexing schemes and multiple translations in 
the CL-SR task at CLEF 2005. In C. Peters, (Ed.), 
Accessing multilingual information repositories 
(Vol. 4022/2006, pp. 760-768). Springer,  London. 
Lee, J. H. (1995). Combining multiple evidence from 
different properties of weighting schemes, 
Proceedings of the 18th annual international ACM 
SIGIR conference on Research and development in 
information retrieval. ACM, Seattle, Washington, 
United States. 
Oard, D. W., Soergel, D., Doermann, D., Huang, X., 
Murray, G. C., Wang, J., Ramabhadran, B., Franz, 
M., & Gustman, S. (2004). Building an information 
retrieval test collection for spontaneous 
conversational speech, Proceedings of the 27th 
annual international ACM SIGIR conference on 
Research and development in information retrieval. 
ACM, Sheffield, United Kingdom. 
Oard, D. W., Wang, J., Jones, G. J. F., White, R. W., 
Pecina, P., Soergel, D., Huang, X., & Shafran, I. 
(2007). Overview of the CLEF-2006 cross-language 
speech retrieval track. In C. Peters, (Ed.), Evaluation 
of multilingual and multi-modal information 
retrieval (Vol. 4730/2007, pp. 744-758). Springer,  
Heidelberg. 
Ounis, I., Amati, G., Plachouras, V., He, B., Macdonald, 
C., & Johnson, D. (2005). Terrier information 
retrieval platform In Advances in information 
retrieval (Vol. 3408/2005, pp. 517-519). Springer,  
Heidelberg. 
Pecina, P., Hoffmannov?a, P., Jones, G. J. F., Zhang, Y., 
& Oard, D. W. (2007). Overview of the CLEF-2007 
cross language speech retrieval track, Working Notes 
of the CLEF- 2007 Evaluation, . CLEF2007, 
Budapest-Hungary. 
Salton, G. & Buckley, C. (1988). Term weighting 
approaches in automatic text retrieval. Information 
Processing and Management, 24(5): 513-523. 
Shaw, J. A. & Fox, E. A. (1994). Combination of multiple 
searches. In Third text retrieval conference (trec-3) 
(pp. 105-108). National Institute of Standards and 
Technology Special Publication. 
 
 
Manual English Auto-English Auto-French Auto-Spanish Weighting 
scheme Training Test Training Test Training Test Training Test 
nnc.ntc 0.2546 0.2293 0.0888 0.0819 0.0792 0.055 0.0593 0.0614 
ntc.ntc 0.2592 0.2332 0.0892 0.0794 0.0841 0.0519 0.0663 0.0545 
lnc.ntc 0.2710 0.2363 0.0898 0.0791 0.0858 0.0576 0.0652 0.0604 
ntc.nnc 0.2344 0.2172 0.0858 0.0769 0.0745 0.0466 0.0585 0.062 
anc.ntc 0.2759 0.2343 0.0723 0.0623 0.0664 0.0376 0.0518 0.0398 
ltc.ntc 0.2639 0.2273 0.0794 0.0623 0.0754 0.0449 0.0596 0.0428 
atc.ntc 0.2606 0.2184 0.0592 0.0477 0.0525 0.0287 0.0437 0.0304 
nnn.ntn 0.2476 0.2228 0.0900 0.0852 0.0799 0.0503 0.0599 0.061 
ntn.ntn 0.2738 0.2369 0.0933 0.0795 0.0843 0.0507 0.0691 0.0578 
lnn.ntn 0.2858 0.245 0.0969 0.0799 0.0905 0.0566 0.0701 0.0589 
ntn.nnn 0.2476 0.2228 0.0900 0.0852 0.0799 0.0503 0.0599 0.061 
ann.ntn 0.2903 0.2441 0.0750 0.0670 0.0743 0.038 0.057 0.0383 
ltn.ntn 0.2870 0.2435 0.0799 0.0655 0.0871 0.0522 0.0701 0.0501 
atn.ntn 0.2843 0.2364 0.0620 0.0546 0.0722 0.0347 0.0586 0.0355 
In_expC2 0.3177 0.2737 0.0885 0.0744 0.0908 0.0487 0.0747 0.0614 
Fusion 1 0.3208 0.2761 0.0969 0.0855 0.0912 0.0622 0.0731 0.0682 
% change 1.0% 0.9% 0.0% 6.5% 0.4% 21.7% -2.2% 10.0% 
Fusion 2 0.3182 0.2741 0.0975 0.0842 0.0942 0.0602 0.0752 0.0619 
% change 0.2% 0.1% 0.6% 5.1% 3.6% 19.1% 0.7% 0.8% 
Table 1. Results (MAP scores) for 15 weighting schemes using Smart and Terrier (the In_expC2 model), and the results 
for the two Fusions Methods. In bold are the best scores for the 15 single runs on the training data and the corresponding 
results on the test data.  
 
Weighting 
scheme 
Manual English Auto-English Auto- French Auto- Spanish 
 Train. Test Train. Test Train. Test Train. Test 
nnc. ntc 2371 1827 1726 1306 1687 1122 1562 1178 
ntc.ntc 2402 1857 1675 1278 1589 1074 1466 1155 
lnc.ntc 2402 1840 1649 1301 1628 1111 1532 1196 
ntc.nnc 2354 1810 1709 1287 1662 1121 1564 1182 
anc.ntc 2405 1858 1567 1192 1482 1036 1360 1074 
ltc.ntc 2401 1864 1571 1211 1455 1046 1384 1097 
atc.ntc 2387 1858 1435 1081 1361 945 1255 1011 
nnn.ntn 2370 1823 1740 1321 1748 1158 1643 1190 
ntn.ntn 2432 1863 1709 1314 1627 1093 1502 1174 
lnn.ntn 2414 1846 1681 1325 1652 1130 1546 1194 
ntn.nnn 2370 1823 1740 1321 1748 1158 1643 1190 
ann.ntn 2427 1859 1577 1198 1473 1027 1365 1060 
ltn.ntn 2433 1876 1582 1215 1478 1070 1408 1134 
atn.ntn 2442 1859 1455 1101 1390 975 1297 1037 
In_expC2 2638 1823 1624 1286 1676 1061 1631 1172 
Fusion 1 2645 1832 1745 1334 1759 1147 1645 1219 
% change 0.3% 0.5 % 0.3% 1.0% 0.6% -1.0% 0.1% 2.4% 
Fusion 2 2647 1823 1727 1337 1736 1098 1631 1172 
% change 0.3% 0.0% 0.8% 1.2% -0.7% -5.5% -0.7% -1.5% 
Table 2. Results (number of relevant documents retrieved) for 15 weighting schemes using Terrier and SMART, and the 
results for the Fusions Methods. In bold are the best scores for the 15 single runs on training data and the corresponding 
test data. 
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 183?186,
Prague, June 2007. c?2007 Association for Computational Linguistics
ILK2: Semantic Role Labelling for Catalan and Spanish using TiMBL
Roser Morante, Bertjan Busser
ILK, Dept. of Language and Information Sciences
Tilburg University, P.O.Box 90153
NL-5000 LE Tilburg, The Netherlands
{R.Morante,G.J.Busser}@uvt.nl
Abstract
In this paper we present a semantic role la-
beling system submitted to the task Multi-
level Semantic Annotation of Catalan and
Spanish in the context of SemEval?2007.
The core of the system is a memory?based
classifier that makes use of full syntactic in-
formation. Building on standard features,
we train two classifiers to predict separately
the semantic class of the verb and the seman-
tic roles.
1 Introduction
Semantic role labelling (SRL) has been addressed
in the CoNLL?2004 and CoNLL?2005 Shared
Tasks (Carreras and Ma`rquez, 2004; Carreras and
Ma`rquez, 2005) for English. In the task Multilevel
Semantic Annotation of Catalan and Spanish of the
SemEval competition 2007, the target are two differ-
ent languages. The general SRL task consists of two
tasks: prediction of semantic roles (SR) and predic-
tion of the semantic class of the verb (SC).
The data provided in the task (Ma`rquez et al,
2007) are sentences annotated with lemma, POS
tags, syntactic information, semantic roles, and the
semantic classes of the verb. A training corpus for
Catalan (ca.3LB) and another for Spanish (sp.3LB)
are provided. Although the setting is similar to
the CoNLL?Shared Task 2005, three relevant differ-
ences are that the corpora are significantly smaller,
that the syntactic information is based on a manu-
ally corrected treebank, which contains also syntac-
tic functions (i.e. direct object, indirect object, etc.),
and that the set of semantic roles is larger, especially
for core arguments.
Our goal is to check whether simple individual
systems could produce competitive results in both
subtasks, and whether they would be robust enough
when applied to two languages and to the held?out
test sets provided.
2 System description
We approach the SRL task as two classification
problems: prediction of SR and prediction of SC.
We hypothesize that the two problems can be solved
in the same way for both languages. We build two
very similar systems that differ only in some of the
features used, as we explain below.
The task is solved in three phases: 1) A pre?
processing phase that is very similar to the sequen-
tialization in (Ma`rquez et al, 2005). We call it focus
selection. It consists of identifying the potential can-
didates to be assigned a semantic role or a semantic
verb class. 2) The classification. 3) Some limited
postprocessing.
2.1 Focus selection
The system starts by finding the target verb (which
is marked in the corpus as such). Then, it finds
the complete form of the verb (that in the corpus is
tagged as verb group, infinitive, gerund, etc.) and
the clause boundaries in order to look for the siblings
of the verb that are under the same clause. Our as-
sumption is that all siblings of the verb are potential
candidates for semantic roles. The focus selection
process produces two groups of focus tokens: on the
one hand, the verbs and, on the other, the siblings of
183
the verbs. These tokens will be the instances in each
training set. Table 1 shows the number of training
and test instances for each subtask.
Training 3LB Test 3LB Test CESS
Ca. Sp. Ca. Sp. Ca. Sp.
SR 23202 24668 1335 1451 1241 1186
SC 8932 9707 510 615 463 465
Table 1: Number of instances per corpus for each task (?Ca?
stands for Catalan, ?Sp? stands for Spanish).
2.2 Classification
In both systems we approach the classification task
in one step, predicting directly the SR and the SC
class. This means that in the SR task we do not
perform a previous classification to select the tokens
that might be assigned a role. We assume that all
verbs belong to a class. As for the SR, we assume
that most siblings of the verb will have a class, ex-
cept for those that have syntactic functions AO, ET,
MOD, NEG, IMPERS, PASS, and VOC. The sib-
lings that do not have a semantic role are assigned
the NONE tag. Because the corpus is small and be-
cause the amount of instances with a NONE class is
proportionally low, we do not consider it necessary
to filter these cases.
Regarding the learning algorithm, we use the
IB1 classifier as implemented in TiMBL (version
5.1) (Daelemans et al, 2004), a supervised induc-
tive algorithm for learning classification tasks based
on the k nearest neighbor (k-nn) algorithm. In IB1,
similarity is defined by a feature?level distance met-
ric between a test instance and a memorized training
instance. The metric combines a per?feature value?
based distance metric with global feature weights
that account for relative differences in importance
of the features.
The TiMBL parameters used in the systems are
the IB1 algorithm, the Jeffrey Divergence as feature
metric, MVDM threshold at level 1, weighting us-
ing GainRatio, k=11, and weighting neighbors as
function of their Inverse Linear Distance (for details
we refer the reader to the TiMBL reference guide
(Daelemans et al, 2004)).
As for the features, we started by using the same
feature set for both classifiers and then, after some
experimentation, we decided to use slightly differ-
ent feature sets for the two sub-tasks. Most of the
features we designed are features that have become
standard for the SRL task (Gildea and Jurafsky,
2002; Xue and Palmer, 2004; Carreras and Ma`rquez,
2004; Carreras and Ma`rquez, 2005). In our system,
the features relate to the verb, the verb siblings, what
we take to be the content word of the siblings, the
clause, and the relation verb?arguments. Addition-
ally, we added lexical features extracted from the
verb lexicon provided for the task, and from Word-
Net.
After experimenting with 323 features, we se-
lected 98 for the SR task and 77 for the SC subclass.
In order to select the features, we started with a basic
system, the results of which were used as a baseline.
Every new feature that was added to the basic system
was evaluated in terms of average accuracy in 10-
fold cross-validation experiments; if it improved the
performance on held-out data, it was added to the se-
lection. One problem with this hill-climbing method
is that the selection of features is determined by the
order in which the features have been introduced.
We also performed experiments applying the feature
selection process reported in (Tjong Kim Sang et al,
2005), a bi-directional hill climbing process. How-
ever, experiments with this advanced method did not
produce a better selection of features.
The features for the SR prediction subtask are the
following:
? Features on the verb (6). They are shared by all
the instances that represent phrases belonging to the
same clause:
VForm; VLemma; VCau: binary feature that indicate if the
verb is in a causative construction with hacer, fer or if the main
verb is causar; VPron, VImp, VPass: binary features that indi-
cate if the verb is pronominal, impersonal, and in passive form
respectively.
? Features on the sibling in focus (12):
SibSynCat: syntactic category; SibSynFunc: syntactic
function; SibPrep: preposition; SibLemW1, SibPOSW1,
SibLemW2, SibPOSW2, SibLemW3, SibPOSW3: lemma
and POS tag of the first, second and third words of the sibling;
SibRelPos: position of the sibling in relation to the verb (PRE
or POST); Sib+1RelPos: position of the sibling next to the cur-
rent phrase in relation to the verb (PRE or POST); SibAbsPos:
absolute position of the sibling in the clause.
? Features that describe the properties of the content
word (CW) of the focus sibling (13): in the case of
prepositional phrases the CW is the head of the first
noun phrase; in cases of coordination, we only take
the first element of the coordination.
184
CWord; CWLemma; CWPOS: we take only the first char-
acter of the POS tags provided; CWPOSType: the type of
POS, second character of the POS tags provided; CWGender;
CWne: binary feature that indicates if the CW is a named en-
tity; CWtmp, CWloc: binary features that indicate if the CW
is a temporal or a locative adverb respectively; CW+2POS,
CW+3POS: POS of the second and third words after CW.
CWwnsc1, CWwnsc2, CWwnsc3: additionally, if the CW
is a noun, we extract information from WordNet (Fellbaum,
1998) about the first, second, and third more frequent seman-
tic classes of the CW in WordNet. We cannot decide on a sin-
gle one because the corpus is not disambiguated. The seman-
tic class corresponds to the lexicographer files in WN3.0. For
nouns there are 25 file numbers.
? Features on the clause (24):
CCtot: total number of siblings with function CC (cir-
cumstancial complement); SUJRelPos, CAGRelPos, CDRel-
Pos, CIRelPos, ATRRelPos, CPREDRelPos, CREGRelPos:
relative positions of siblings with functions SUJ, CAG, CD,
CI,ATR, CPRED, and CREG in relation to verb (PRE or
POST); SEsib: binary feature that indicates if the clause con-
tains a verbal se; SIBtot: total number of verb siblings in
the clause; SynFuncSib8, SynCatSib8, PrepSib8,W1Sib8,
W2Sib8, W3Sib8, W4Sib8, SynFuncSib9, SynCatSib9,
PrepSib9, W1Sib9, W2Sib9, W3Sib9, W4Sib9: syntactic
function, syntactic category, preposition, and first to fourth
word of siblings 8 and 9.
? Features extracted from the lexicon of verbal
frames (43) that the task organizers provided. We
access the lexicon to check if it is possible for a verb
to have a certain semantic role. We check it for all
semantic role classes, except for ArgX-Ag, ArgX-
Cau, ArgX-Pat, ArgX-Tem because they proved not
to be informative. The features are binary.
For the SC prediction task the features are similar,
but not exactly the same. Both systems contain some
features about all candidate arguments. We point out
the differences:
? Features that are in the SR system and that are not
in the SC system:
Verb form (VForm), verb lemma (VLemma), absolute po-
sition of the sibling in the clause (SibAbsPos), function of
the sibling (SibSynFunc), preposition of the sibling (SibPrep),
POS tag of the second and third words after CW (CW+2POS,
CW+3POS), information about the WN classes of the CW
(CWwnsc1, CWwnsc2, CWwnsc3), feature about the CW be-
ing a named entity (CWne, SIBtot), syntactic function, syn-
tactic category, preposition and first to fourth word of sib-
lings 8 and 9 (SynFuncSib8, SynCatSib8, PrepSib8,W1Sib8,
W2Sib8, W3Sib8, W4Sib8, SynFuncSib9, SynCatSib9,
PrepSib9, W1Sib9, W2Sib9, W3Sib9, W4Sib9).
? Features that are only in the SC system:
AllCats: vector of the syntactic categories of the siblings in
the order that they appear in the clause; AllFuncs: vector of the
functions of the siblings in the order that they appear; AllFuncs-
Bin vector with eight binary values that represent if a sibling
with that function is present or not; Sib+1Prep, Sib+2Prep:
prepositions of the two siblings after the verb.
2.3 Postprocessing
As for the postprocessing phase, it consists of six
simple rules to correct some basic errors in predict-
ing some types of ArgM arguments. It only applies
to the SR task. The rules are the following ones:
1. If prediction = ArgM?LOC, ArgM?MNR or ArgM?ADV,
and either {SibPrep = ?durante? or ?durant?}, or {SibSynCat =
sn and one of the WN semantic classes = 28}, then prediction =
ArgM-TMP.
2. If prediction = ArgM?LOC, ArgM?MNR or ArgM?ADV,
and CWLemma is a temporal adverb, then prediction = ArgM?
TMP.
3. If prediction = ArgM?TMP and one of the WN classes = 15,
then prediction = ArgM?LOC.
4. If prediction = ArgM?TMP, ArgM-MNR or ArgM-ADV, and
CWLemma = locative adverb, then prediction = ArgM-LOC.
5. If prediction = ArgM-TMP or ArgM-ADV, and CWwnsc1 =
15, and SibPrep = ?en? or ?desde? or ?hacia? or ?a? or ?des de?
or ?cap a?, then prediction = ArgM?LOC.
6. If prediction = ArgM?ADV and CWLemma = causal con-
junction, then prediction = ArgM?CAU.
We are aware of the fact that these are very simple
rules and that more elaborate postprocessing tech-
niques can be applied, like the ones used in (Tjong
Kim Sang et al, 2005) in order to make sure that the
same role was not predicted more than once in the
same clause.
SR TASK Perf.Props Precision Recall F?=1
Test ca.3LB 73.35% 86.59% 85.91% 86.25
Test ca.CESS 60.55% 82.60% 78.03% 80.25
Overall ca 67.24% 84.72% 82.12% 83.40
Test sp.3LB 68.07% 83.05% 82.54% 82.80
Test sp.CESS 73.76% 85.88% 85.80% 85.84
Overall sp 70.52% 84.30% 83.98% 84.14
Overall SR 68.96% 84.50% 83.07% 83.78
SC TASK Perf.Props Precision Recall F?=1
Test ca.3LB 90.86% 90.30% 88.72% 89.50
Test ca.CESS 90.41% 90.20% 88.27% 89.22
Overall ca 90.64% 90.25% 88.50% 89.37
Test sp.3LB 84.12% 80.00% 78.44% 79.21
Test sp.CESS 90.54% 89.89% 89.89% 89.89
Overall sp 86.88% 84.30% 83.36% 83.83
Overall SC 88.67% 87.12% 85.81% 86.46
SRL TASK Perf.Props Precision Recall F?=1
Overall ca ? 86.44% 84.08 % 85.24
Overall sp ? 84.30% 83.78 % 84.04
Overall SRL ? 85.32% 83.93 % 84.62
Table 2: Overall results in the SR (above), SC (middle),
and general SRL tasks (?Perf.Props?: perfect propositions; ?ca?:
Catalan; ?sp?: Spanish).
185
3 Results
The overall official results of the system are shown
in Table 2. The SC system performs better (over-
all F1 = 86.46) than the SR system (overall F1 =
83.78). In global, the systems perform better for
Catalan (overall F1 = 85.24) than for Spanish (over-
all F1 = 84.04), although the SC system performs
better for Catalan (89.37 vs. 86.46), and the SR sys-
tem performs better for Spanish (84.14 vs 83.40).
Striking results are that the SR system gets signif-
icantly better results with the held?out test for Span-
ish, and that both of the complete SRL systems get
significantly better results with the held?out test for
Spanish. This might be due to differences in the pro-
cess of gathering and annotation of the corpus.
SP?CESS F Precision Recall F?=1
Overall 85.88% 85.80% 85.84
Arg0?AGT 16.19% 92.83% 92.41% 92.62
Arg0?CAU 1.23% 100% 50% 66.67
Arg1 1.79% 88.46% 82.14% 85.19
Arg1?LOC 0.11% 0.00% 0.00% 0.00
Arg1?PAT 20.09% 93.82% 94.19% 94.00
Arg1?TEM 14.08% 86.54% 91.84% 89.11
Arg2 2.05% 68.00% 77.27% 72.34
Arg2?ATR 9.88% 91.67% 90.41% 91.03
Arg2?BEN 2.40% 96.30% 100.00% 98.11
Arg2?EFI 0.19% 0.00% 0.00% 0.00
Arg2?EXT 0.19% 0.00% 0.00% 0.00
Arg2?LOC 1.13% 0.00% 0.00% 0.00
Arg2?PAT 0.01% 0.00% 0.00% 0.00
Arg3?ATR 0.05% 0.00% 0.00% 0.00
Arg3?BEN 0.16% 100.00% 100.00% 100.00
Arg3?EIN 0.08% 0.00% 0.00% 0.00
Arg3?FIN 0.04% 100.00% 33.33% 50.00
Arg3?ORI 0.29% 0.00% 0.00% 0.00
Arg4?DES 0.60% 83.33% 83.33% 83.33
ArgL 0.71% 16.67% 20.00% 18.18
ArgM?ADV 10.67% 68.12% 68.12% 68.12
ArgM?CAU 1.50% 55.56% 45.45% 50.00
ArgM?FIN 1.30% 64.71% 84.62% 73.33
ArgM?LOC 4.94% 78.21% 77.22% 77.71
ArgM?MNR 2.28% 36.36% 57.14% 44.44
ArgM?TMP 7.19% 88.75% 81.61% 85.03
V ? 100.00% 100.00% 100.00
Table 3: Detailed results on the Spanish CESS?ECE test cor-
pus for the SR subtask. F: frequency of the semantic roles in
the training corpus, without counting V.
Table 3 shows detailed results on the Spanish
CESS?ECE corpus for the SR task. Low scores are
generally related to low frequency of the SR in the
training corpus, and high scores are related to high
frequency or to overt marking of the SR.
4 Conclusions
We have presented two memory?based SRL systems
that make use of full syntactic information and ap-
proach the tasks in three steps. Results show that
rather simple individual systems can produce com-
petitive results in both tasks, and that they are ro-
bust enough to be applied to two languages and to
the held?out test sets provided. Improvements of the
systems would consist in improving the focus selec-
tion step, and applying more elaborate techniques
for feature selection and postprocessing.
Acknowledgements
This research has been funded by the postdoctoral grant
EX2005?1145 awarded by the Ministerio de Educacio?n y Cien-
cia of Spain to the project Te?cnicas semiautoma?ticas para el eti-
quetado de roles sema?nticos en corpus del espan?ol. We would
like to thank Martin Reynaert, Caroline Sporleder, Antal van
den Bosch, and the anonymous reviewers for their comments
and suggestions.
References
X. Carreras and Ll. Ma`rquez. 2004. Introduction to the
CoNLL-2004 shared task: Semantic role labeling. In Pro-
ceedings of CoNLL?2004, Boston MA, USA.
X. Carreras and Ll. Ma`rquez. 2005. Introduction to the
CoNLL-2005 shared task: Semantic role labeling. In Pro-
ceedings of CoNLL?2005, Ann Arbor, Michigan, June.
W. Daelemans, J. Zavrel, K. Van der Sloot, and A. Van den
Bosch. 2004. TiMBL: Tilburg memory based learner, ver-
sion 5.1, reference guide. Technical Report Series 04-02,
ILK, Tilburg, The Netherlands.
Christiane Fellbaum, editor. 1998. WordNet: An Electronic
Database. MIT Press, Cambridge, MA.
D. Gildea and D. Jurafsky. 2002. Automatic labeling of seman-
tic roles. Computational Linguistics, 28(3):245?288.
LL. Ma`rquez, P. Comas, J. Gime?nez, and N. Catala`. 2005. Se-
mantic role labeling as sequential tagging. In Proceedings of
CoNLL?2005, Ann Arbor, Michigan.
Ll. Ma`rquez, M.A. Mart??, M. Taule?, and L. Villarejo. 2007.
SemEval-2007 Task 09: Multilevel semantic annotation of
catalan and spanish. In Proceedings of SemEval-2007, the
4th Workshop on Semantic Evaluations, Prague, Czech Re-
public.
E. Tjong Kim Sang, S. Canisius, A. van den Bosch, and
T. Bogers. 2005. Applying spelling error correction tech-
niques for improving semantic role labelling. In Proceed-
ings of CoNLL-2005, pages 229?232, Ann Arbor, Michigan.
N. Xue and M. Palmer. 2004. Calibrating features for semantic
role labeling. In Proceedings of 2004 Conference on Empir-
ical Methods in Natural Language Processing, Barcelona,
Spain.
186
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 187?190,
Prague, June 2007. c?2007 Association for Computational Linguistics
ILK: Machine learning of semantic relations with shallow features
and almost no data
Iris Hendrickx
CNTS / Language Technology Group
Uversity of Antwerp,
Universiteitsplein 1
2610 Wilrijk, Belgium
iris.hendrickx@ua.ac.be
Roser Morante, Caroline Sporleder,
Antal van den Bosch
ILK / Communication and Information Sciences
Tilburg University, P.O. Box 90153,
5000 LE Tilburg, The Netherlands
{R.Morante,C.Sporleder,
Antal.vdnBosch}@uvt.nl
Abstract
This paper summarizes our approach to the
Semeval 2007 shared task on ?Classifica-
tion of Semantic Relations between Nom-
inals?. Our overall strategy is to develop
machine-learning classifiers making use of
a few easily computable and effective fea-
tures, selected independently for each clas-
sifier in wrapper experiments. We train two
types of classifiers for each of the seven re-
lations: with and without WordNet informa-
tion.
1 Introduction
We interpret the task of determining semantic rela-
tions between nominals as a classification problem
that can be solved, per relation, by machine learning
algorithms. We aim at using straightforward features
that are easy to compute and relevant to preferably
all of the seven relations central to the task.
The starting conditions of the task provide us with
a very small amount of training data, which further
stresses the need for robust, generalizable features,
that generalize beyond surface words. We there-
fore hypothesize that generic information on the lex-
ical semantics of the entities involved in the rela-
tion is crucial. We developed two systems, based
on two sources of semantic information. Since the
entities in the provided data were word-sense dis-
ambiguated, an obvious way to model their lexical
semantics was by utilizing WordNet3.0 (Fellbaum,
1998) (WN). One of the systems followed this route.
We also entered a second system, which did not
rely on WN but instead made use of automatically
generated semantic clusters (Decadt and Daelemans,
2004) to model the semantic classes of the entities.
For both systems we trained seven binary clas-
sifiers; one for each relation. From a pool of eas-
ily computable features, we selected feature subsets
for each classifier in a number of wrapper exper-
iments, i.e. repeated cross-validation experiments
on the training set to test out subset selections sys-
tematically. Along with feature subsets we also
chose the machine-learning method independently
for each classifier.
Section 2 presents the system description, Sec-
tion 3, the results, and Section 4, the conclusions.
2 System Description
The development of the system consists of a prepro-
cessing phase to extract the features, and the classi-
fication phase.
2.1 Preprocessing
Each sentence is preprocessed automatically in the
following steps. First, the sentence is tokenized with
a rule-based tokenizer. Next a part-of-speech tag-
ger and text chunker that use the memory-based tag-
ger MBT (Daelemans et al, 1996) produces part-
of-speech tags and NP chunk labels for each token.
Then a memory-based shallow parser predicts gram-
matical relations between verbs and NP chunks such
as subject, object or modifier (Buchholz, 2002). The
tagger, chunker and parser were all trained on the
WSJ Corpus (Marcus et al, 1993). We also use
a memory-based lemmatizer (Van den Bosch et al,
1996) trained on Celex (Baayen et al, 1993) to pre-
dict the lemma of each word.
187
The features extracted are of three types: seman-
tic, lexical, and morpho-syntactic. The features that
apply to the entities in a relation (e1,e2) are extracted
for term 1 (t1) and term 2 (t2) of the relation, where
t1 is the first term in the relation name, and t2 is the
second term. For example, in the relation CAUSE?
EFFECT, t1 is CAUSE and t2 is EFFECT.
The semantic features are the following:
WN semantic class of t1 and t2. The WN seman-
tic class of each entity in the relation. For the WN-
based system, we determined the semantic class of
the entities on the basis of the lexicographer file
numbers (LFN) in WN3.0. The LFN are encoded in
the synset number provided in the annotation of the
data. For nouns there are 25 file numbers that corre-
spond to suitably abstract semantic classes, namely:
noun.Tops(top concepts for nouns), act, animal, artifact, at-
tribute, body, cognition, communication event, feeling, food,
group, location, motive, object, person, phenomenon, plant,
possession, process, quantity, relation, shape, state, substance,
time.
Is container (is C). Exclusively for the
CONTENT?CONTAINER relation we furthermore
included two binary features that test whether the
two entities in the relation are hyponyms of the
synset container in WN. For the PART?WHOLE
relation we also experimented with binary features
expressing whether the two entities in the relation
have some type of meronym and holonym relation,
but these features did not prove to be predictive.
Cluster class of t1 and t2. A cluster class iden-
tifier for each entity in the relation. This informa-
tion is drawn from automatically generated clusters
of semantically similar nouns (Decadt and Daele-
mans, 2004) generated on the British National Cor-
pus (Clear, 1993). The corpus was first prepro-
cessed by a lemmatizer and the memory-based shal-
low parser, and the found verb?object relations were
used to cluster nouns in groups. We used the top-
5000 lemmatized nouns, that are clustered into 250
groups. This is an example of two of these clusters:
? {can pot basin tray glass container bottle tin pan mug cup
jar bowl bucket plate jug vase kettle}
? {booth restaurant bath kitchen hallway toilet bedroom
hall suite bathroom interior lounge shower compartment
oven lavatory room}
The lexical features are the following:
Lemma of t1 and t2 (lem1, lem2). The lemmas of
the entities involved in the relation. In case an entity
consisted of multiple words (e.g. storage room) we
use the lemma of the head noun (i.e. room).
Main verb (verb). The main verb of the sentence
in which the entities involved in the relation appear,
as predicted by the shallow parser.
The morpho-syntactic features are:
GramRel (gr1, gr2). The grammatical relation
tags of the entities.
Suffixes of t1 and t2 (suf1, suf2). The suffixes of
the entity lemmas. We implemented a rule-based
suffix guesser, which determines whether the nouns
involved in the relation end in a derivational suffix,
such as -ee, -ment etc. Suffixes often provide cues
for semantic properties of the entities. For exam-
ple, the suffix -ee usually indicates animate (and typ-
ically human) referents (e.g. detainee etc.), whereas
(-ment) points at abstract entities (e.g. statement).
While the features were selected independently
for all relations, the seven classifiers in the WN-
based system all make use of the WN semantic class
features; in the system that did not use WN, the
seven classifiers make use of the cluster class fea-
tures instead.
2.2 Classification
We experimented with several machine learning
frameworks and different feature (sub-)sets. For
rapid testing of different learners and feature sets,
and given the size of the training data (140 exam-
ples for each relation), we made use of the Weka ma-
chine learning software1 (Witten and Frank, 1999).
We systematically tested the following algorithms:
NaiveBayes (NB) (Langley et al, 1992), BayesNet
(BN) (Cooper and Herskovits, 1992), J48 (Quinlan,
1993), Jrip (Cohen, 1995), IB1 and IBk (Aha et al,
1991), LWL (Atkeson et al, 1997), and Decision-
Stumps (DS) (Iba and Langley, 1992), all with de-
fault algorithm settings.
The classifiers for all seven relations were opti-
mized independently in a number of 10-fold cross-
validation (CV) experiments on the provided train-
1http://www.cs.waikato.ac.nz/ml/weka/
188
ing sets. The feature sets and learning algorithms
which were found to obtain the highest accuracies
for each relation were then used when applying the
classifiers to the unseen test data.
The classifiers of the cluster-based system (A) all
use the two cluster class features. The other se-
lected features and the chosen algorithms (CL) are
displayed in Table 1. Knowledge of the identity of
the lemmas was found to be beneficial for all clas-
sifiers. With respect to the machine learning frame-
work, Naive Bayes was selected most frequently.
Relation CL lem1 lem2 verb gr1 gr2 suf1 suf2
Cause-Effect DS + + + + + + +
Instr-Agency LWL + + + + +
Product-Producer NB + + + + + + +
Origin-Entity IBk + + + + + +
Theme-Tool NB + + + + +
Part-Whole NB + + + + + +
Content-Container NB + + + + + +
Table 1: The final selected algorithms and features
for each relation by the cluster-based system (A).
The classifiers of the WN-based system (B) all
use at least the WN semantic class features. Ta-
ble 2 shows the other selected features and algorithm
for each relation. None of the classifiers use all the
features. For the part-whole relation no extra fea-
tures besides the WN class are selected. Also the
classifiers for the relations cause-effect and content-
container only use two additional features. The list
of best found algorithms shows that ?like with the
cluster-based system? a Bayesian approach is fa-
vorable, as it is selected in four of seven cases.
Relation CL lem1 lem2 verb gr1 gr2 suf1 suf2 is C
Cause-Effect BN + +
Instr-Agency NB + + +
Product-Producer IB1 + + + +
Origin-Entity IBk + + + + +
Theme-Tool NB + + + + + +
Part-Whole J48
Content-Container BN + +
Table 2: The final selected algorithms and features
for each relation by the WN-based system (B). (is C
is the CONTENT-CONTAINER specific feature.)
3 Results
In Table 3 we first present the best results computed
on the training set using 10-fold CV for the cluster-
based system (A) and the WN-based system (B).
These results are generally higher than the official
test set results, shown in Tables 4 and 5, possibly
showing a certain amount of overfitting on the train-
ing sets.
Relation A B
Cause-Effect 56.4 72.9
Instrument-Agency 71.4 75.7
Product-Producer 65.0 67.9
Origin-Entity 70.7 78.6
Theme-Tool 75.7 79.3
Part-Whole 65.7 73.6
Content-Container 70.0 75.4
Avg 67.9 74.8
Table 3: Average accuracy on the training set com-
puted in 10-fold CV experiments of the cluster-
based system (A) and the WN-based system (B).
The official scores on the test set are computed
by the task organizers: accuracy, precision, recall
and F1 score. Table 4 presents the results of the
cluster-based system. Table 5 presents the results
of the WN-based system. (The column Total shows
the number of instances in the test set.) Markable is
the high accuracy for the PART-WHOLE relation as
the classifier was only trained on two features cod-
ing the WN classes.
A4 Pre Rec F Acc Total
Cause?Effect 53.3 97.6 69.0 55.0 80
Instrument?Agency 56.1 60.5 58.2 57.7 78
Product?Producer 69.1 75.8 72.3 61.3 93
Origin?Entity 60.7 47.2 53.1 63.0 81
Theme?Tool 64.5 69.0 66.7 71.8 71
Part?Whole 48.4 57.7 52.6 62.5 72
Content?Container 71.4 78.9 75.0 73.0 74
Avg 60.5 69.5 63.8 63.5 78.4
Table 4: Test scores for the seven relations of the
cluster-based system trained on 140 examples (A4).
The system using all training data with WordNet
features, B4 (Table 5), performs better in terms of F-
score on six out of the seven subtasks as compared
to the system that does not use the WordNet features
but the semantic cluster information instead, A4 (Ta-
ble 4). This is largely due to a lower precision of the
A4 system. The WordNet features appear to be di-
rectly responsible for a relatively higher precision.
In contrast, the semantic cluster features of sys-
tem A sometimes boost recall. A4?s recall on the
189
B4 Pre Rec F Acc Total
Cause?Effect 69.0 70.7 69.9 68.8 80
Instrument?Agency 69.8 78.9 74.1 73.1 78
Product?Producer 79.7 75.8 77.7 71.0 93
Origin?Entity 71.0 61.1 65.7 71.6 81
Theme?Tool 69.0 69.0 69.0 74.6 71
Part?Whole 73.1 73.1 73.1 80.6 72
Content?Container 78.1 65.8 71.4 73.0 74
Avg 72.8 70.6 71.5 73.2 78.4
Table 5: Test scores for the seven relations of the
WN-based system trained on 140 examples (B4).
CAUSE?EFFECT relation is 97.6% (the classifier pre-
dicts the class ?true? for 75 of the 80 examples),
and on CONTENT?CONTAINER the system attains
78.9%, markedly better than B4.
4 Conclusion
We have shown that a machine learning approach us-
ing shallow and easily computable features performs
quite well on this task. The system using Word-
Net features based on the provided disambiguated
word senses outperforms the cluster-based system.
It would be interesting to compare both systems to a
more realistic WN-based system that uses predicted
word senses by a Word Sense Disambiguation sys-
tem.
However we end by noting that the amount of
training and test data in this shared task should be
considered too small to base any reliable conclu-
sions on. In a realistic scenario (e.g. when high-
precision relation classification would be needed as
a component of a question-answering system), more
training material would have been gathered, and the
examples would not have been seeded by a limited
number of queries ? especially the negative exam-
ples are very artificial now due to their similarity to
the positive cases, and the fact that they are down-
sampled very unrealistically. Rather, the focus of the
task should be on detecting positive instances of the
relations in vast amounts of text (i.e. vast amounts of
implicit negative examples). Positive training exam-
ples should be as randomly sampled from raw text
as possible. The seven relations are common enough
to warrant a focused effort to annotate a reasonable
amount of randomly selected text, gathering several
hundreds of positive cases of each relation.
References
D. W. Aha, D. Kibler, M. Albert. 1991. Instance-based
learning algorithms. Machine Learning, 6:37?66.
C. Atkeson, A. Moore, S. Schaal. 1997. Locally
weighted learning. Artificial Intelligence Review,
11(1?5):11?73.
R. H. Baayen, R. Piepenbrock, H. van Rijn. 1993. The
CELEX lexical data base on CD-ROM. Linguistic
Data Consortium, Philadelphia, PA.
S. Buchholz. 2002. Memory-Based Grammatical Rela-
tion Finding. PhD thesis, University of Tilburg.
J. H. Clear. 1993. The British national corpus. MIT
Press, Cambridge, MA, USA.
W. Cohen. 1995. Fast effective rule induction. In Pro-
ceedings of the 12th International Conference on Ma-
chine Learning, 115?123. Morgan Kaufmann.
G. F. Cooper, E. Herskovits. 1992. A bayesian method
for the induction of probabilistic networks from data.
Machine Learning, 9(4):309?347.
W. Daelemans, J. Zavrel, P. Berck, S. Gillis. 1996.
Mbt: A memory-based part of speech tagger genera-
tor. In Proceedings of the 4th ACL/SIGDAT Workshop
on Very Large Corpora, 14?27.
B. Decadt, W. Daelemans. 2004. Verb classification -
machine learning experiments in classifying verbs into
semantic classes. In Proceedings of the LREC 2004
Workshop Beyond Named Entity Recognition: Seman-
tic Labeling for NLP Tasks, 25?30, Lisbon, Portugal.
C. Fellbaum, ed. 1998. WordNet: An Electronic
Database. MIT Press, Cambridge, MA.
W. Iba, P. Langley. 1992. Induction of one-level decision
trees. Proceedings of the Ninth International Confer-
ence on Machine Learning, 233?240.
P. Langley, W. Iba, K. Thompson. 1992. An analysis of
Bayesian classifiers. In Proceedings of the Tenth An-
nual Conference on Artificial Intelligence, 223?228.
AAAI Press and MIT Press.
M. Marcus, S. Santorini, M. Marcinkiewicz. 1993.
Building a Large Annotated Corpus of English:
the Penn Treebank. Computational Linguistics,
19(2):313?330.
J. Quinlan. 1993. C4.5: Programs for Machine Learn-
ing. Morgan Kaufmann, San Mateo, CA.
A. Van den Bosch, W. Daelemans, A. Weijters. 1996.
Morphological analysis as classification: an inductive-
learning approach. In K. Oflazer, H. Somers, eds.,
Proceedings of the Second International Conference
on New Methods in Natural Language Processing,
NeMLaP-2, Ankara, Turkey, 79?89.
I. H. Witten, E. Frank. 1999. Data Mining: Practical
Machine Learning Tools and Techniques with Java Im-
plementations. Morgan Kaufman, San Francisco, CA.
190
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 715?724,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Learning the Scope of Negation in Biomedical Texts
Roser Morante?, Anthony Liekens?, Walter Daelemans?
CNTS - Language Technology Group?, Applied Molecular Genomics Group?
University of Antwerp
Prinsstraat 13, B-2000 Antwerpen, Belgium
{Roser.Morante,Anthony.Liekens,Walter.Daelemans}@ua.ac.be
Abstract
In this paper we present a machine learning
system that finds the scope of negation in
biomedical texts. The system consists of two
memory-based engines, one that decides if the
tokens in a sentence are negation signals, and
another that finds the full scope of these nega-
tion signals. Our approach to negation detec-
tion differs in two main aspects from existing
research on negation. First, we focus on find-
ing the scope of negation signals, instead of
determining whether a term is negated or not.
Second, we apply supervised machine learn-
ing techniques, whereas most existing systems
apply rule-based algorithms. As far as we
know, this way of approaching the negation
scope finding task is novel.
1 Introduction
In this paper we present a machine learning system
that finds the scope of negation in biomedical texts.
The system consists of two classifiers, one that de-
cides if the tokens in a sentence are negation sig-
nals (i.e., words indicating negation), and another
that finds the full scope of these negation signals.
Finding the scope of a negation signal means deter-
mining at sentence level which words in the sentence
are affected by the negation. Our approach differs in
two main aspects from existing research. First, we
focus on finding the scope of negation signals, in-
stead of determining whether a term is negated or
not. Second, we apply supervised machine learn-
ing techniques, whereas most existing systems apply
rule-based algorithms.
Predicting the scope of negation is important in
information extraction from text for obvious rea-
sons; instead of simply flagging the sentences con-
taining negation as not suited for extraction (which
is currently the best that can be done), correct se-
mantic relations can be extracted when the scope of
negation is known, providing a better recall.
Not being able to recognize negation can also
hinder automated indexing systems (Mutalik et al,
2001; Rokach et al, 2008). As Mutalik et al (2001)
put it, ?to increase the utility of concept indexing of
medical documents, it is necessary to record whether
the concept has been negated or not?. They highlight
the need to detect negations in examples like ?no ev-
idence of fracture?, so that an information retrieval
system does not return irrelevant reports.
Szarvas et al (2008) report that 13.45% of the
sentences in the abstracts section of the BioScope
corpus and 13.76% of the sentences in the full papers
section contain negations. A system that does not
deal with negation would treat these cases as false
positives.
The goals of this research are to model the scope
finding task as a classification task similar to the se-
mantic role labeling task, and to test the performance
of a memory?based system that finds the scope of
negation signals. Memory-based language process-
ing (Daelemans and van den Bosch, 2005) is based
on the idea that NLP problems can be solved by
reuse of solved examples of the problem in mem-
ory, applying similarity-based reasoning on these
examples in order to solve new problems. As lan-
guage processing tasks typically involve many sub-
regularities and (pockets of) exceptions, it has been
715
argued that lazy learning is at an advantage in solv-
ing these highly disjunctive learning problems com-
pared to eager learning, as the latter eliminates not
only noise but also potentially useful exceptions
(Daelemans et al, 1999). Memory-based algorithms
have been successfully applied in language process-
ing to a wide range of linguistic tasks, from phonol-
ogy to semantic analysis, such as semantic role la-
beling (Morante et al, 2008).
The paper is organised as follows. In Section 2,
we summarise related work. In Section 3, we de-
scribe the corpus with which the system has been
trained. In Section 4, we introduce the task to be
performed by the system, which is described in Sec-
tion 5. The results are presented and discussed in
Section 6. Finally, Section 7 puts forward some con-
clusions.
2 Related work
Negation has been a neglected area in open-domain
natural language processing. Most research has been
performed in the biomedical domain and has fo-
cused on detecting if a medical term is negated or
not, whereas in this paper we focus on detecting the
full scope of negation signals.
Chapman et al (2001) developed NegEx, a reg-
ular expression based algorithm for determining
whether a finding or disease mentioned within nar-
rative medical reports is present or absent. The re-
ported results are 94.51 precision and 77.84 recall.
Mutalik et al (2001) developed Negfinder, a rule-
based system that recognises negated patterns in
medical documents. It consists of two tools: a lexi-
cal scanner called lexer that uses regular expressions
to generate a finite state machine, and a parser. The
reported results are 95.70 recall and 91.80 precision.
Sanchez-Graillet and Poesio (2007) present an
analysis of negated interactions in biological texts
and a heuristics-based system that extracts such in-
formation. They treat all types of negation: (i) Af-
fixal negation, which is expressed by an affix. (ii)
Noun phrase or emphatic negation, expressed syn-
tactically by using a negative determiner (e.g. no,
nothing). (iii) Inherent negation, expressed by words
with an inherently negative meaning (e.g. absent).
(iv) Negation with explicit negative particles (e.g.
no, not). The texts are 50 journal articles. The pre-
liminary results reported range from 54.32 F-score
to 76.68, depending on the method applied.
Elkin et al (2005) describe a rule-based system
that assigns to concepts a level of certainty as part of
the generation of a dyadic parse tree in two phases:
First a preprocessor breaks each sentence into text
and operators. Then, a rule based system is used to
decide if a concept has been positively, negatively,
or uncertainly asserted. The system achieves 97.20
recall and 98.80 precision.
The systems mentioned above are essentially
based on lexical information. Huang and
Lowe (2007) propose a classification scheme of
negations based on syntactic categories and patterns
in order to locate negated concepts, regardless of
their distance from the negation signal. Their hybrid
system that combines regular expression matching
with grammatical parsing achieves 92.60 recall and
99.80 precision.
Additionally, Boytcheva et al (2005) incorporate
the treatment of negation in a system, MEHR, that
extracts from electronic health records all the in-
formation required to generate automatically patient
chronicles. According to the authors ?the nega-
tion treatment module inserts markers in the text for
negated phrases and determines scope of negation by
using negation rules?. However, in the paper there
is no description of the rules that are used and it is
not explained how the results presented for negation
recognition (57% of negations correctly recognised)
are evaluated.
The above-mentioned research applies rule-based
algorithms to negation finding. Machine learning
techniques have been used in some cases. Averbuch
et al (2004) developed an algorithm that uses infor-
mation gain to learn negative context patterns.
Golding and Chapman (2003) experiment with
machine learning techniques to distinguish whether
a medical observation is negated by the word not.
Their corpus contains 207 selected sentences from
hospital reports, in which a negation appears. They
use Naive Bayes and Decision Trees and achieve a
maximum of 90 F-score. According to the authors,
their main finding is that ?when negation of a UMLS
term is triggered with the negation phrase not, if the
term is preceded by the then do not negate?.
Goryachev et al (2006) compare the perfor-
mance of four different methods of negation de-
716
tection, two regular expression-based methods and
two classification-based methods trained on 1745
discharge reports. They show that the regular
expression-based methods have better agreement
with humans and better accuracy than the classifica-
tion methods. Like in most of the mentioned work,
the task consists in determining if a medical term is
negated.
Rokach et al (2008) present a new pattern-based
algorithm for identifying context in free-text med-
ical narratives.The originality of the algorithm lies
in that it automatically learns patterns similar to the
manually written patterns for negation detection.
Apart from work on determining whether a term is
negated or not, we are not aware of research that has
focused on learning the full scope of negation sig-
nals inside or outside biomedical natural language
processing. The research presented in this paper pro-
vides a new approach to the treatment of negation
scope in natural language processing.
3 Corpus
The corpus used is a part of the BioScope cor-
pus (Szarvas et al, 2008)1, a freely available re-
source that consists of medical and biological texts.
Every sentence is annotated with information about
negation and speculation that indicates the bound-
aries of the scope and the keywords, as shown in (1).
(1) PMA treatment, and <xcope id=?X1.4.1?><cue
type=?negation? ref=?X1.4.1?>not<cue> retinoic
acid treatment of the U937 cells</xcope> acts in
inducing NF-KB expression in the nuclei.
A first characteristic of the annotation of scope in
the BioScope corpus is that all sentences that assert
the non-existence or uncertainty of something are
annotated, in contrast to other corpora where only
sentences of interest in the domain are annotated.
A second characteristic is that the annotation is ex-
tended to the biggest syntactic unit possible so that
scopes have the maximal length. In (2) below, nega-
tion signal no scopes over primary impairment of
glucocorticoid metabolism instead of scoping only
over primary.
(2) There is [no] primary impairment of glucocorticoid
metabolism in the asthmatics.
1Web page: www.inf.u-szeged.hu/rgai/bioscope.
The part used in our experiments are the biologi-
cal paper abstracts from the GENIA corpus (Collier
et al, 1999). This part consists of 11,872 sentences
in 1,273 abstracts. We automatically discarded five
sentences due to annotation errors. The total num-
ber of words used is 313,222, 1,739 of which are
negation signals that belong to the different types
described in (Sanchez-Graillet and Poesio, 2007).
We processed the texts with the GENIA tag-
ger (Tsuruoka and Tsujii, 2005; Tsuruoka et al,
2005), a bidirectional inference based tagger that an-
alyzes English sentences and outputs the base forms,
part-of-speech tags, chunk tags, and named entity
tags in a tab-separated format2. Additionally, we
converted the annotation about scope of negation
into a token-per-token representation.
Table 1 shows an example sentence of the corpus
that results from converting and processing the Bio-
Scope representation. Following the standard for-
mat of the CoNLL Shared Task 2006 (Buchholz and
Marsi, 2006), sentences are separated by a blank line
and fields are separated by a single tab character. A
sentence consists of tokens, each one starting on a
new line. A token consists of the following 10 fields:
1. ABSTRACT ID: number of the GENIA ab-
stract.
2. SENTENCE ID: sentence counter starting at 1
for each new abstract.
3. TOKEN ID: token counter, starting at 1 for
each new sentence.
4. FORM: word form or punctuation symbol.
5. LEMMA: lemma of word form.
6. POS TAG: Penn Treebank part-of-speech tags
described in (Santorini, 1990).
7. CHUNK TAG: IOB (Inside, Outside, Begin)
tags produced by the GENIA tagger that indi-
cate if a token is inside a certain chunk, outside,
or at the beginning.
8. NE TAG: IOB named entity tags produced by
the GENIA tagger that indicate if a token is in-
2The accuracy of the tagger might be inflated due to the fact
that it was trained on the GENIA corpus.
717
ABSTR SNT TOK FORM LEMMA POS CHUNK NE NEG NEG SCOPE
ID ID ID TAG TAG TAG SGN
10415075 07 1 NF-kappa NF-kappa NN B-NP B-protein I-NEG O-NEG
10415075 07 2 B B NN I-NP I-protein I-NEG O-NEG
10415075 07 3 binding binding NN I-NP O I-NEG O-NEG
10415075 07 4 activity activity NN I-NP O I-NEG O-NEG
10415075 07 5 was be VBD B-VP O I-NEG O-NEG
10415075 07 6 absent absent JJ B-ADJP O NEG I-NEG O-NEG
10415075 07 7 in in IN B-PP O I-NEG O-NEG
10415075 07 8 several several JJ B-NP O I-NEG O-NEG
10415075 07 9 SLE SLE NN I-NP O I-NEG O-NEG
10415075 07 10 patients patient NNS I-NP O I-NEG O-NEG
10415075 07 11 who who WP B-NP O I-NEG O-NEG
10415075 07 12 were be VBD B-VP O I-NEG O-NEG
10415075 07 13 not not RB I-VP O NEG I-NEG I-NEG
10415075 07 14 receiving receive VBG I-VP O I-NEG I-NEG
10415075 07 15 any any DT B-NP O I-NEG I-NEG
10415075 07 16 medication medication NN I-NP O I-NEG I-NEG
10415075 07 17 , , , O O I-NEG I-NEG
10415075 07 18 including include VBG B-PP O I-NEG I-NEG
10415075 07 19 corticosteroidscorticosteroid NNS B-NP O I-NEG I-NEG
10415075 07 20 . . . O O O-NEG O-NEG
Table 1: Example sentence of the BioScope corpus converted into columns format.
side a certain named entity, outside, or at the
beginning.
9. NEG SIGNAL: tokens that are negation signals
are marked as NEG. Negation signals in the
BioScope corpus are not always single words,
like the signal could not. After the tagging pro-
cess the signal cannot becomes also multiword
because the tagger splits it in two words. In
these cases we assign the NEG mark to not.
10. NEG SCOPE: IO tags that indicate if a token
is inside the negation scope (I-NEG), or out-
side (O-NEG). These tags have been obtained
by converting the xml files of BioScope. Each
token can have one or more NEG SCOPE tags,
depending on the number of negation signals in
the sentence.
4 Task description
We approach the scope finding task as a classifica-
tion task that consists of classifying the tokens of
a sentence as being a negation signal or not, and
as being inside or outside the scope of the negation
signal(s). This happens as many times as there are
negation signals in the sentence. Our conception of
the task is inspired by Ramshaw and Marcus? rep-
resentation of text chunking as a tagging problem
(Ramshaw and Marcus, 1995) .
The information that can be used to train the sys-
tem appears in columns 1 to 8 of Table 1. The infor-
mation to be predicted by the system is contained in
columns 9 and 10.
As far as we know, approaching the negation
scope finding task as a token per token classifica-
tion task is novel, whereas at the same time it con-
forms to the well established standards of the re-
cent CoNLL Shared Tasks3 on dependency parsing
(Buchholz and Marsi, 2006; Nivre et al, 2007) and
semantic role labeling (Surdeanu et al, 2008). By
setting up the task in this way we show that the nega-
tion scope finding task can be modelled in a way
similar to semantic role labeling, and by conform-
ing to existing standards we show that learning the
scope of negation can be integrated in a joint learn-
ing task with dependency parsing and semantic role
labeling.
3Web page of CoNLL:
http://www.ifarm.nl/signll/conll/.
718
5 System description
In order to solve the task, we apply supervised ma-
chine learning techniques. We build a memory-
based scope finder, that tackles the task in two
phases. In the first phase a classifier predicts if a to-
ken is a negation signal, and in the second phase an-
other classifier predicts if a token is inside the scope
of each of the negation signals. Additionally, the
output of the second classifier is postprocessed with
an algorithm that converts non-consecutive blocks of
scope into consecutive, as explained in Section 5.3.
As for the first and second phases, we use a
memory?based classifier as implemented in TiMBL
(version 6.1.2) (Daelemans et al, 2007), a super-
vised inductive algorithm for learning classification
tasks based on the k-nearest neighbor classification
rule (Cover and Hart, 1967). Similarity is defined by
computing (weighted) overlap of the feature values
of a test instance and training instances. The metric
combines a per-feature value distance metric (Cost
and Salzberg, 1993) with gain ratio (Quinlan, 1993)
based global feature weights that account for relative
differences in discriminative power of the features.
5.1 Negation signal finding
In this phase, a classifier predicts whether a token is
a negation signal or not. The memory-based classi-
fier was parameterised by using overlap as the sim-
ilarity metric, gain ratio for feature weighting, and
using 7 k-nearest neighbors. All neighbors have
equal weight when voting for a class. The instances
represent all tokens in the corpus and they have the
following features:
? Of the token: Form, lemma, part of speech, and
chunk IOB tag.
? Of the token context: Form, POS, and IOB tag
of the three previous and three next tokens.
5.2 Scope finding
In the first step of this phase, a classifier predicts
whether a token is in the scope of each of the nega-
tion signals of a sentence. A pair of a negation signal
and a token from the sentence represents an instance.
This means that all tokens in a sentence are paired
with all negation signals that occur in the sentence.
For example, token NF-kappa in Table 1 will be rep-
resented in two instances as shown in (3). An in-
stance represents the pair [NF?KAPPA, absent] and
another one represents the pair [NF?KAPPA, not].
(3) NF-kappa absent [features] I-NEG
NF-kappa not [features] O-NEG
Negation signals are those that have been classi-
fied as such in the previous phase. Only sentences
that have negation signals are selected for this phase.
The memory?based algorithm was parameterised
in this case by using overlap as the similarity metric,
gain ratio for feature weighting, using 7 k-nearest
neighbors, and weighting the class vote of neighbors
as a function of their inverse linear distance.
The features of the scope finding classifier are:
? Of the negation signal: Form, POS, chunk IOB
tag, type of chunk (NP, VP, ...), and form, POS,
chunk IOB tag, type of chunk, and named en-
tity of the 3 previous and 3 next tokens.
? Of the paired token: form, POS, chunk IOB
tag, type of chunk, named entity, and form,
POS, chunk IOB tag, type of chunk, and named
entity type of the 3 previous and 3 next tokens.
? Of the tokens between the negation signal and
the token in focus: Chain of POS types, dis-
tance in number of tokens, and chain of chunk
IOB tags.
? Others: A binary feature indicating whether the
token and the negation signal are in the same
chunk, and location of the token relative to the
negation signal (pre, post, same).
5.3 Post-processing
Negation signals in the BioScope corpus always
have one consecutive block of scope tokens, includ-
ing the signal token itself. However, the scope find-
ing classifier can make predictions that result in non-
consecutive blocks of scope tokens: we observed
that 54% of scope blocks predicted by the sys-
tem given gold standard negation signals are non?
consecutive. This is why in the second step of the
scope finding phase, we apply a post-processing al-
gorithm in order to increase the number of fully cor-
rect scopes. A scope is fully correct if all tokens in a
719
sentence have been assigned their correct class label
for a given negation signal. Post-processing ensures
that the resulting scope is one consecutive block of
tokens.
In the BioScope corpus negation signals are inside
of their scope. The post-processing algorithm that
we apply first checks if the negation signal is in its
scope. If the signal is out, the algorithm overwrites
the predicted scope in order to include the signal in
its scope.
Given the position of the signal in the sentence,
the algorithm locates the starting and ending tokens
of the consecutive block of predicted scope tokens
that surrounds the signal. Other blocks of predicted
scope tokens may have been predicted outside of this
block, but they are separated from the current block,
which contains the signal, by tokens that have been
predicted not to be in the scope of the negation, as in
Figure 1.
k
signal
ml
Figure 1: Non-consecutive blocks of scope tokens. For
a signal, two blocks of k = 6 and m = 3 tokens are
predicted to be the scope of the signal token, but they are
separated by l = 2 tokens that are predicted to be out of
scope.
The post-processing algorithm decides whether
the detached blocks should be connected as one con-
secutive block of scope tokens, or whether the de-
tached block of scope tokens should be discarded
from the scope. Dependent on this decision, ei-
ther the classification of the separated blocks, or the
separating non-scope tokens are considered noisy,
and their classification is updated to produce one
consecutive block of scope tokens for each signal.
This check is performed iteratively for all detached
blocks of scope tokens.
As in Figure 1, consider a sentence where the
negation signal is in one block K of predicted scope
of length k tokens and another block M of m con-
secutive tokens that is predicted as scope but is sep-
arated from the latter scope block by l out-of-scope
tokens.
If non-consecutive blocks are near each other, i.e.,
if l is sufficiently small in comparison with k and
m, then the intermediate tokens that have been pre-
dicted out of scope could be considered as noise and
converted into scope tokens. In contrast, if there are
too many intermediate tokens that separate the two
blocks of scope tokens, then the additional block of
scope is probably wrongly classified.
Following this logic, if l < ?(k + m), with a
specifically chosen ?, the intermediate out-of-scope
tokens are re-classified as scope tokens, and the
separated blocks are connected to form one bigger
block containing the negation signal. Otherwise,
the loose block of scope is re-classified to be out of
scope. When the main scope is extended, and more
blocks are found that are separated from the main
scope block, the algorithm reiterates this procedure
until one consecutive block of scope tokens has been
found.
Our implementation first looks for separated
blocks from right to left, and then from left to right.
Dependent on whether blocks need to be added be-
fore or after the main scope block, we have observed
in preliminary tests that ? = 0.2 for extending the
main scope block from right to left, and ? = 0.3 for
extending the block from left to right into the sen-
tence provide the best results. Algorithm 1 details
the above procedure in pseudo code.
Algorithm 1 Post-processing
K ? scope block that contains signal
while M ? nearest separated scope block do
L? non-scope block between K and M
if |L| < ?(|K|+ |M |) then
include L in scope
else
exclude M from scope
end if
K ? scope block that contains signal
end while
6 Results
The results have been obtained by performing 10-
fold cross validation experiments. The evaluation
is made using the precision and recall measures
(Van Rijsbergen, 1979), and their harmonic mean,
F-Measure. We calculate micro F1.
720
In the negation finding task, a negation token is
correctly classified if it has been assigned a NEG
class. In the scope finding task, a token is correctly
classified if all the IO tag(s) that it has been assigned
are correct. This means that when there is more than
one negation signal in the sentence, the token has to
be correctly assigned an IO tag for as many negation
signals as there are. For example, token NF-kappa
from Table 1 reproduced in (4) will not be correct
if it is assigned classes I-NEG I-NEG or O-NEG I-
NEG.
(4) 10415075 07 1 NF-kappa NF-kappa NN B-NP
B-protein I-NEG O-NEG
Additionally, we evaluated the percentage of fully
correct scopes (PCS).
6.1 Negation signal finding
We calculate two baselines for negation signal find-
ing. Baseline 1 (B1) is calculated by assigning the
NEG class to all the tokens that had no or not as
lemma, which account for 72.80% of the negation
signals. The F1 of the baseline is 80.66. Baseline
2 (B2) is calculated by assigning the NEG class to
all the tokens that had no, not, lack, neither, unable,
without, fail, absence, or nor as lemma. These lem-
mas account for 85.85 % of the negation signals.
Baseline Total Prec. Recall F1
B1 1739 90.42 72.80 80.66
B2 1739 89.77 93.38 91.54
Table 2: Baselines of the negation finding system.
Table 3 shows the overall results of the negation
signal finding system and the results per negation
signal. With F1 94.40, it outperforms Baseline 2
by 2.86 points. Precision and recall are very simi-
lar. Scores show a clear unbalance between different
negation signals. Those with the lowest frequencies
get lower scores than those with the highest frequen-
cies. Probably, this could be avoided by training the
system with a bigger corpus.
However, a bigger corpus would not help solve all
the errors because some of them are caused by in-
consistency in the annotation. For example, absence
is annotated as a negation signal in 57 cases, whereas
in 22 cases it is not annotated as such, although in all
cases it is used as a negation signal. Example 5 (a)
Neg signals Total Prec. Recall F1
lack (v) 55 100.00 100.00 100.00
neither (con) 34 100.00 100.00 100.00
lack (n) 33 100.00 100.00 100.00
unable 30 100.00 100.00 100.00
neither (det) 8 100.00 100.00 100.00
no (adv) 5 100.00 100.00 100.00
without 83 100.00 98.79 99.39
nor 44 100.00 100.00 98.89
rather 19 95.00 100.00 97.43
not 1057 96.15 96.97 96.56
no (det) 204 95.63 96.56 96.09
none 7 85.71 85.71 85.71
fail 57 79.36 87.71 83.33
miss 2 66.66 100.00 80.00
absence 57 67.64 80.70 73.60
failure 8 45.54 62.50 52.63
could 6 66.66 33.33 44.44
absent 13 42.85 23.07 30.00
with 6 0.00 0.00 0.00
either 2 0.00 0.00 0.00
instead 2 0.00 0.00 0.00
never 2 0.00 0.00 0.00
impossible 1 0.00 0.00 0.00
lacking 1 0.00 0.00 0.00
loss 1 0.00 0.00 0.00
negative 1 0.00 0.00 0.00
or 1 0.00 0.00 0.00
Overall 1739 94.21 94.59 94.40
Table 3: F scores of the negation finding classifier.
shows one of the 22 cases of absence that has not
been annotated, and Example 5 (b) shows one of the
57 cases of absence annotated as a negation signal.
Also fail is not annotated as a negation signal in 13
cases where it should.
(5) (a) Retroviral induction of TIMP-1 not only
resulted in cell survival but also in continued DNA
synthesis for up to 5 d in the absence of serum,
while controls underwent apoptosis.
(b) A significant proportion of transcripts appear to
terminate prematurely in the <xcope id= X654.8.1
><cue type= negation ref= X654.8.1 > absence
</cue> of transactivators </xcope>.
Other negation signals are arbitrarily annotated.
Failure is annotated as a negation signal in 8 cases
where it is followed by a preposition, like in Exam-
ple 6 (a), and it is not annotated as such in 26 cases,
like Example 6 (b), where it is modified by an adjec-
tive.
721
(6) (a) ... the <xcope id= X970.8.2><cue type=
negation ref= X970.8.2>failure</cue> of eTh1
cells to produce IL-4 in response to an antigen
</xcope> is due, at least partially, to a <xcope id=
X970.8.1>< cue type= negation ref= X970.8.1>
failure</cue> to induce high-level transcription
of the IL-4 gene by NFAT </xcope></xcope>.
(b) Positive-pressure mechanical ventilation
supports gas exchange in patients with respiratory
failure but is also responsible for significant lung
injury.
The errors in detecting with as a negation signal
are caused by the fact that it is embedded in the ex-
pression with the exception of, which occurs 6 times
in contrast with the 5265 occurrences of with. Could
appears as a negation signal because the tagger does
not assign to it the lemma can, but could, causing
the wrong assignment of the tag NEG to not, instead
of could when the negation cue in BioScope is could
not.
6.2 Scope finding
We provide the results of the classifier and the re-
sults of applying the postprocessing algorithm to the
output of the classifier.
Table 4 shows results for two versions of the
scope finding classifier, one based on gold standard
negation signals (GS NEG), and another (PR NEG)
based on negation signals predicted by the classifier
described in the previous section.
Prec. Recall F1 PCS
GS NEG 86.03 85.53 85.78 39.39
PR NEG 79.83 77.42 78.60 36.31
Table 4: Results of the scope finding classifier with gold-
standard (GS NEG) and with predicted negation signals
(PR NEG).
The F1 of PR NEG is 7.18 points lower than the
F1 of GS NEG, which is an expected effect due to
the performance of classifier that finds negation sig-
nals. Precision and recall of GS NEG are very bal-
anced, whereas PR NEG has a lower recall than pre-
cision. These measures are the result of a token per
token evaluation, which does not guarantee that the
complete sequence of scope is correct. This is re-
flected in the low percentage of fully correct scopes
of both versions of the classifier.
In Table 5, we present the results of the system af-
ter applying the postprocessing algorithm. The most
remarkable result is the 29.60 and 21.58 error reduc-
tion in the percentage of fully correct scopes of GS
NEG and PR NEG respectively, which shows that
the algorithm is efficient. Also interesting is the in-
crease in F1 of GS NEG and PR NEG.
Prec. Recall F1 PCS
GS NEG 88.63 88.17 88.40 57.33
PR NEG 80.70 81.29 80.99 50.05
Table 5: Results of the system with gold-standard (GS
NEG) and with predicted negation signals (PR NEG) af-
ter applying the postprocessing algorithm.
Table 6 shows detailed results of the system based
on predicted negation signals after applying the
postprocessing algorithm. Classes O-NEG and I-
NEG are among the most frequent and get high
scores. Classes composed only of O-NEG tags are
easier to predict.
Scope tags Total Prec. Recall F1
O-NEG 29590 86.78 84.75 85.75
O-NEG O-NEG O-NEG 46 100.00 63.04 77.33
I-NEG 12990 73.41 80.72 76.89
O-NEG O-NEG 2848 84.11 68.43 75.46
I-NEG I-NEG O-NEG 69 62.92 81.15 70.88
I-NEG I-NEG 684 57.30 65.93 61.31
I-NEG O-NEG O-NEG 20 50.00 75.00 60.00
O-NEG I-NEG 791 72.13 50.06 59.10
I-NEG O-NEG 992 45.32 67.94 54.37
O-NEG I-NEG I-NEG 39 100.00 20.51 34.04
I-NEG I-NEG I-NEG 22 26.66 36.36 30.76
O-NEG O-NEG I-NEG 14 0.00 0.00 0.00
Overall 48105 80.70 81.29 80.99
Table 6: F scores of the system per scope class after ap-
plying the postprocessing algorithm.
Table 7 shows information about the percentage
of correct scopes per negation signal after applying
the algorithm to PR-NEG. A clear example of an
incorrect prediction is the occurrence of box in the
list. The signal with the highest percentage of PCS
is without, followed by no (determiner), rather and
not, which are above 50%. It would be interesting to
investigate how the syntactic properties of the nega-
tion signals are related to the percentage of correct
scopes, and how does the algorithm perform depend-
ing on the type of signal.
722
Neg signals Total Correct PCS
without 82 56 68.29
no (det) 206 133 64.56
rather 20 11 55.00
not 1066 556 52.15
neither (det) 8 4 50.00
none 7 3 42.85
neither (conj) 34 16 47.05
no (adv) 5 2 40.00
fail 63 23 36.50
missing 3 1 33.33
absence 68 22 32.35
lack (v.) 54 17 31.48
absent 7 2 28.57
lack (n.) 33 9 27.27
nor 43 11 25.58
unable 30 8 26.66
failure 11 0 0.00
could 3 0 0.00
negative 1 0 0.00
never 1 0 0.00
box 1 0 0.00
Overall 1746 874 50.05
Table 7: Information about Percentage of Correct Scopes
(PCS) per negation signal in PR-NEG.
7 Conclusions
Given the fact that a significant portion of biomed-
ical text is negated, recognising negated instances
is important in NLP applications. In this paper we
have presented a machine learning system that finds
the scope of negation in biomedical texts. The sys-
tem consists of two memory-based classifiers, one
that decides if the tokens in a sentence are negation
signals, and another that finds the full scope of the
negation signals.
The first classifier achieves 94.40 F1, and the sec-
ond 80.99. However, the evaluation in terms of cor-
rect scopes shows the weakness of the system. This
is why a postprocessing algorithm is applied. The
algorithm achieves an error reduction of 21.58, with
50.05 % of fully correct scopes in the system based
on predicted negation signals.
These results suggest that unsupervised machine
learning algorithms are suited for tackling the task,
as it was expected from results obtained in other
natural language processing tasks. However, results
also suggest that there is room for improvement. A
first improvement would consist in predicting the
scope chunk per chunk instead of token per token,
because most negation scope boundaries coincide
with boundaries of chunks.
We have highlighted the fact that our approach
to negation detection focuses on finding the scope
of negation signals, instead of determining whether
a term is negated or not, and on applying super-
vised machine learning techniques. As far as we
know, this approach is novel. Unfortunately, there
are no previous comparable approaches to measure
the quality of our results.
Additionally, we have shown that negation find-
ing can be modelled as a classification task in a way
similar to other linguistic tasks like semantic role la-
beling. In our model, tokens of a sentence are clas-
sified as being a negation signal or not, and as being
inside or outside the scope of the negation signal(s).
This representation would allow to integrate the task
with other semantic tasks and exploring the interac-
tion between different types of knowledge in a joint
learning setting.
Further research is possible in several directions.
In the first place, other machine learning algorithms
could be integrated in the system in order to opti-
mise performance. Secondly, the system should be
tested in different types of biomedical texts, like full
papers or medical reports to check its robustness.
Finally, the postprocessing algorithm could be im-
proved by using more sophisticated sequence classi-
fication techniques (Dietterich, 2002) .
Acknowledgments
Our work was made possible through financial sup-
port from the University of Antwerp (GOA project
BIOGRAPH). We are thankful to three anonymous
reviewers for their valuable comments and sugges-
tions.
References
M. Averbuch, T. Karson, B. Ben-Ami, O. Maimon, and
L. Rokach. 2004. Context-sensitive medical informa-
tion retrieval. In Proc. of the 11th World Congress
on Medical Informatics (MEDINFO-2004), pages 1?
8, San Francisco, CA. IOS Press.
723
S. Boytcheva, A. Strupchanska, E. Paskaleva, and
D. Tcharaktchiev. 2005. Some aspects of negation
processing in electronic health records. In Proc. of
International Workshop Language and Speech Infras-
tructure for Information Access in the Balkan Coun-
tries, pages 1?8, Borovets, Bulgaria.
S. Buchholz and E. Marsi. 2006. CoNLL-X shared task
on multilingual dependency parsing. In Proc. of the X
CoNLL Shared Task, New York. SIGNLL.
W. W. Chapman, W. Bridewell, P. Hanbury, G. F. Cooper,
and B.G. Buchanan. 2001. A simple algorithm for
identifying negated findings and diseases in discharge
summaries. J Biomed Inform, 34:301?310.
N. Collier, H.S. Park, N. Ogata, Y. Tateisi, C. Nobata,
T. Sekimizu, H. Imai, and J. Tsujii. 1999. The GE-
NIA project: corpus-based knowledge acquisition and
information extraction from genome research papers.
In Proceedings of EACL-99.
S. Cost and S. Salzberg. 1993. A weighted nearest neigh-
bour algorithm for learning with symbolic features.
Machine learning, 10:57?78.
T. M. Cover and P. E. Hart. 1967. Nearest neighbor
pattern classification. Institute of Electrical and Elec-
tronics Engineers Transactions on Information The-
ory, 13:21?27.
W. Daelemans and A. van den Bosch. 2005. Memory-
based language processing. Cambridge University
Press, Cambridge, UK.
W. Daelemans, A. Van den Bosch, and J. Zavrel. 1999.
Forgetting exceptions is harmful in language learning.
Machine Learning, Special issue on Natural Language
Learning, 34:11?41.
W. Daelemans, J. Zavrel, K. Van der Sloot, and A. Van
den Bosch. 2007. TiMBL: Tilburg memory based
learner, version 6.1, reference guide. Technical Report
Series 07-07, ILK, Tilburg, The Netherlands.
T. G. Dietterich. 2002. Machine learning for sequential
data: A review. In Lecture Notes in Computer Science
2396, pages 15?30, London. Springer Verlag.
P. L. Elkin, S. H. Brown, B. A. Bauer, C.S. Husser,
W. Carruth, L.R. Bergstrom, and D. L. Wahner-
Roedler. 2005. A controlled trial of automated classi-
fication of negation from clinical notes. BMC Medical
Informatics and Decision Making, 5(13).
l. M. Goldin and W.W. Chapman. 2003. Learning to
detect negation with ?Not? in medical texts. In Pro-
ceedings of ACM-SIGIR 2003.
S. Goryachev, M. Sordo, Q.T. Zeng, and L. Ngo. 2006.
Implementation and evaluation of four different meth-
ods of negation detection. Technical report, DSG.
Y. Huang and H.J. Lowe. 2007. A novel hybrid approach
to automated negation detection in clinical radiology
reports. J Am Med Inform Assoc, 14(3):304?311.
R. Morante, W. Daelemans, and V. Van Asch. 2008. A
combined memory-based semantic role labeler of en-
glish. In Proc. of the CoNLL 2008, pages 208?212,
Manchester, UK.
A.G. Mutalik, A. Deshpande, and P.M. Nadkarni. 2001.
Use of general-purpose negation detection to augment
concept indexing of medical documents. a quantita-
tive study using the UMLS. J Am Med Inform Assoc,
8(6):598?609.
J. Nivre, J. Hall, S. Ku?bler, R. McDonald, J. Nilsson,
S. Riedel, and D. Yuret. 2007. The CoNLL-2007
shared task on dependency parsing. In Proc. of the
CoNLL Shared Task Session of EMNLP-CoNLL 2007,
pages 915?932, Prague.
J.R. Quinlan. 1993. C4.5: Programs for Machine Learn-
ing. Morgan Kaufmann, San Mateo, CA.
L. Ramshaw and M. Marcus. 1995. Text chunking using
transformation-based learning. In Proc. of ACL Third
Workshop on Very Large Corpora, pages 82?94, Cam-
bridge, MA. ACL.
L. Rokach, R.Romano, and O. Maimon. 2008. Negation
recognition in medical narrative reports. Information
Retrieval Online.
O. Sanchez-Graillet and M. Poesio. 2007. Negation of
protein-protein interactions: analysis and extraction.
Bioinformatics, 23(13):424?432.
B. Santorini. 1990. Part-of-speech tagging guidelines
for the penn treebank project. Technical report MS-
CIS-90-47, Department of Computer and Information
Science, University of Pennsylvania.
M. Surdeanu, R. Johansson, A. Meyers, Ll. Ma`rquez,
and J. Nivre. 2008. The CoNLL-2008 shared task on
joint parsing of syntactic and semantic dependencies.
In Proc. of CoNLL 2008, pages 159?177, Manchester,
UK.
G. Szarvas, V. Vincze, R. Farkas, and J. Csirik. 2008.
The BioScope corpus: annotation for negation, uncer-
tainty and their scopein biomedical texts. In Proc. of
BioNLP 2008, pages 38?45, Columbus, Ohio, USA.
ACL.
Y. Tsuruoka and J. Tsujii. 2005. Bidirectional in-
ference with the easiest-first strategy for tagging se-
quence data. In Proc. of HLT/EMNLP 2005, pages
467?474.
Y. Tsuruoka, Y. Tateishi, J. Kim, T. Ohta, J. McNaught,
S. Ananiadou, and J. Tsujii, 2005. Advances in Infor-
matics - 10th Panhellenic Conference on Informatics,
volume 3746 of Lecture Notes in Computer Science,
chapter Part-of-Speech Tagger for Biomedical Text,
Advances in Informatics, pages 382?392. Springer,
Berlin/Heidelberg.
C.J. Van Rijsbergen. 1979. Information Retrieval. But-
terworths, London.
724
Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), pages 21?29,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
A metalearning approach to processing the scope of negation
Roser Morante, Walter Daelemans
CNTS - Language Technology Group
University of Antwerp
Prinsstraat 13, B-2000 Antwerpen, Belgium
{Roser.Morante,Walter.Daelemans}@ua.ac.be
Abstract
Finding negation signals and their scope in
text is an important subtask in information ex-
traction. In this paper we present a machine
learning system that finds the scope of nega-
tion in biomedical texts. The system combines
several classifiers and works in two phases.
To investigate the robustness of the approach,
the system is tested on the three subcorpora
of the BioScope corpus representing different
text types. It achieves the best results to date
for this task, with an error reduction of 32.07%
compared to current state of the art results.
1 Introduction
In this paper we present a machine learning system
that finds the scope of negation in biomedical texts.
The system works in two phases: in the first phase,
negation signals are identified (i.e., words indicating
negation), and in the second phase the full scope of
these negation signals is determined. Although the
system was developed and tested on biomedical text,
the same approach can also be used for text from
other domains.
Finding the scope of a negation signal means de-
termining at sentence level the sequence of words in
the sentence that is affected by the negation. This
task is different from determining whether a word is
negated or not. For a sentence like the one in Exam-
ple (1) taken from the BioScope corpus (Szarvas et
al., 2008), the system detects that lack, neither, and
nor are negation signals; that lack has as its scope
lack of CD5 expression, and that the discontinuous
negation signal neither ... nor has as its scope nei-
ther to segregation of human autosome 11, on which
the CD5 gene has been mapped, nor to deletion of
the CD5 structural gene.
(1) <sentence id=?S334.5?>Analysis at the phenotype and
genetic level showed that <xcope id?X334.5.3?><cue
type=?negation? ref=?X334.5.3?>lack</cue> of CD5
expression</xcope> was due <xcope id=?X334.5.1?>
<cue type=?negation? ref=?X334.5.1?>neither</cue>
to segregation of human autosome 11, on which the CD5
gene has been mapped, <cue type=?negation?
ref=?X334.5.1?>nor</cue> to deletion of the CD5
structural gene</xcope>.</sentence>
Predicting the scope of negation is relevant for
text mining and information extraction purposes. As
Vincze et al (2008) put it, extracted information that
falls in the scope of negation signals cannot be pre-
sented as factual information. It should be discarded
or presented separately. Szarvas et al (2008) report
that 13.45% of the sentences in the abstracts section
of the BioScope corpus and 12.70% of the sentences
in the full papers section contain negations. A sys-
tem that does not deal with negation would treat the
facts in these cases incorrectly as positives. Addi-
tionally, information about the scope of negation is
useful for entailment recognition purposes.
The approach to the treatment of negation in NLP
presented in this paper was introduced in Morante et
al. (2008). This system achieved a 50.05 percent-
age of correct scopes but had a number of impor-
tant shortcomings. The system presented here uses
a different architecture and different classification
task definitions, it can deal with multiword negation
signals, and it is tested on three subcorpora of the
BioScope corpus. It achieves an error reduction of
21
32.07% compared to the previous system.
The paper is organised as follows. In Section 2,
we summarise related work. In Section 3, we de-
scribe the corpus on which the system has been de-
veloped. In Section 4, we introduce the task to be
performed by the system, which is described in Sec-
tion 5. Results are presented and discussed in Sec-
tion 6. Finally, Section 7 puts forward some conclu-
sions.
2 Related work
Negation has been a neglected area in open-domain
natural language processing. Most research has
been performed in the biomedical domain and has
focused on detecting whether a medical term is
negated or not, whereas in our approach we focus
on detecting the full scope of negation signals.
Chapman et al (2001) developed NegEx, a reg-
ular expression based algorithm for determining
whether a finding or disease mentioned within nar-
rative medical reports is present or absent. The re-
ported results are 94.51% precision and 77.84% re-
call. Mutalik et al (2001) developed Negfinder, a
rule-based system that recognises negated patterns
in medical documents. It consists of two tools: a lex-
ical scanner that uses regular expressions to generate
a finite state machine, and a parser. The reported re-
sults are 95.70% recall and 91.80% precision.
Sanchez-Graillet and Poesio (2007) present an
analysis of negated interactions in 50 biomedical
articles and a heuristics-based system that extracts
such information. The preliminary results reported
range from 54.32% F-score to 76.68%, depending
on the method applied. Elkin et al (2005) describe a
rule-based system that assigns to concepts a level of
certainty as part of the generation of a dyadic parse
tree in two phases: First a preprocessor breaks each
sentence into text and operators. Then, a rule based
system is used to decide if a concept has been pos-
itively, negatively, or uncertainly asserted. The sys-
tem achieves 97.20% recall and 98.80% precision.
The systems mentioned above are essentially
based on lexical information. Huang and
Lowe (2007) propose a classification scheme of
negations based on syntactic categories and patterns
in order to locate negated concepts, regardless of
their distance from the negation signal. Their hy-
brid system that combines regular expression match-
ing with grammatical parsing achieves 92.60% re-
call and 99.80% precision. Additionally, Boytcheva
et al (2005) incorporate the treatment of negation
in a system, MEHR, that extracts from electronic
health records all the information required to gen-
erate automatically patient chronicles. They report
57% of negations correctly recognised.
The above-mentioned research applies rule-based
algorithms to negation finding. Machine learning
techniques have been used in some cases. Averbuch
et al (2004) developed an algorithm that uses infor-
mation gain to learn negative context patterns. Gold-
ing and Chapman (2003) experiment with Naive
Bayes and Decision Trees to distinguish whether a
medical observation is negated by the word not in a
corpus of hospital reports. They report a maximum
of 90% F-score.
Goryachev et al (2006) compare the perfor-
mance of four different methods of negation de-
tection, two regular expression-based methods and
two classification-based methods trained on 1745
discharge reports. They show that the regular
expression-based methods show better agreement
with humans and better accuracy than the classifica-
tion methods. Like in most of the work mentioned,
the task consists in determining whether a medi-
cal term is negated. Rokach et al (2008) present a
new pattern-based algorithm for identifying context
in free-text medical narratives.The originality of the
algorithm lies in that it automatically learns patterns
similar to the manually written patterns for negation
detection.
We are not aware of any research that has focused
on learning the full scope of negation signals outside
biomedical natural language processing.
3 Negation in the BioScope Corpus
The system has been developed using the BioScope
corpus (Szarvas et al, 2008; Vincze et al, 2008)1,
a freely available resource that consists of medical
and biological texts. In the corpus, every sentence
is annotated with information about negation and
speculation. The annotation indicates the bound-
aries of the scope and the keywords, as shown in (1)
above. In the annotation, scopes are extended to the
1Web page: www.inf.u-szeged.hu/rgai/bioscope.
22
biggest syntactic unit possible, so that scopes have
the maximal length, and the negation signal is al-
ways included in the scope. The annotation guide-
lines and the inter-annotator agreement information
can be found on the web page.
Clinical Papers Abstracts
#Documents 1954 9 1273
#Sentences 6383 2670 11871
#Words 41985 60935 282243
#Lemmas 2320 5566 14506
Av. length sentences 7.73 26.24 26.43
% Sent. 1-10 tokens 75.85 11.27 3.17
% Sent. 11-20 tokens 20.99 27.67 30.49
% Sent. 21-30 tokens 2.94 29.55 35.93
% Sent. 31-40 tokens 0.15 17.00 19.76
% Sent. > 40 tokens 0.01 0.03 10.63
%Negation sentences 13.55 12.70 13.45
#Negation signals 877 389 1848
Av. length scopes 4.98 8.81 9.43
Av. length scopes 4.84 7.61 8.06
to the right
Av. length scopes 6.33 5.69 8.55
to the left
% Scopes to the right 97.64 81.77 85.70
% Scopes to the left 2.35 18.22 14.29
Table 1: Statistics about the subcorpora in the BioScope
corpus and the negation scopes (?Av?. stands for aver-
age).
The BioScope corpus consists of three parts: clin-
ical free-texts (radiology reports), biological full pa-
pers and biological paper abstracts from the GENIA
corpus (Collier et al, 1999). Table 1 shows statistics
about the corpora. Negation signals are represented
by one or more tokens.
Only one negation signal (exclude) that occurs in
the papers subcorpus does not occur in the abstracts
subcorpus, and six negation signals (absence of, ex-
clude, favor, favor over, may, rule out that appear in
the clinical subcorpus do not appear in the abstracts
subcorpus. The negation signal no (determiner) ac-
counts for 11.74 % of the negation signals in the ab-
stracts subcorpus, 12.88 % in the papers subcorpus,
and 76.65 % in the clinical subcorpus. The nega-
tion signal not (adverb) accounts for 58.89 % of the
negation signals in the abstracts subcorpus, 53.22 %
in the papers subcorpus, and 6.72 % in the clinical
subcorpus.
The texts have been processed with the GENIA
tagger (Tsuruoka and Tsujii, 2005; Tsuruoka et al,
2005), a bidirectional inference based tagger that an-
alyzes English sentences and outputs the base forms,
part-of-speech tags, chunk tags, and named entity
tags in a tab-separated format. Additionally, we con-
verted the annotation about scope of negation into a
token-per-token representation, following the stan-
dard format of the 2006 CoNLL Shared Task (Buch-
holz and Marsi, 2006), where sentences are sepa-
rated by a blank line and fields are separated by a
single tab character. A sentence consists of a se-
quence of tokens, each one starting on a new line.
4 Finding the scope of negation
We model the scope finding task as two consecutive
classification tasks: a first one that consists of classi-
fying the tokens of a sentence as being at the begin-
ning of a negation signal, inside or outside. This al-
lows the system to find multiword negation signals.
The second classification task consists of classi-
fying the tokens of a sentence as being the first ele-
ment of the scope, the last, or neither. This happens
as many times as there are negation signals in the
sentence. We have chosen this classification model
after experimenting with two additional models that
produced worse results: in one case we classifed to-
kens as being inside or outside of the scope. In an-
other case we classified chunks, instead of tokens, as
being inside or outside of the scope.
5 System description
The two classification tasks (identifying negation
signals and finding the scope) are implemented us-
ing supervised machine learning methods trained on
part of the annotated corpus.
5.1 Identifying negation signals
In this phase, a classifier predicts whether a token is
the first token of a negation signal, inside a nega-
tion signal, or outside of it. We use IGTREE as
implemented in TiMBL (version 6.1.2) (Daelemans
et al, 2007). TiMBL2 is a software package that
contains implementations of memory-based learn-
ing algorithms like IB1 and IGTREE. We also ex-
perimented with IB1, but it produced lower results.
2TiMBL can be downloaded from the web page
http://ilk.uvt.nl/timbl/.
23
The classifier was parameterised by using gain ra-
tio for feature weighting. The instances represent all
tokens in the corpus and they have features of the
token (lemma) and of the token context: word form,
POS, and chunk IOB tag3 of one token to the left
and to the right; word form of the second token to
the left and to the right. According to the gain ratio
scores, the most informative feature is the lemma of
the token, followed by the chunk IOB tag of the to-
ken to the right, and the features relative to the token
to the left.
The test file is preprocessed using a list of nega-
tion signals extracted from the training corpus, that
are unambiguous in the training corpus. The list
comprises the following negation signals: absence,
absent, fail, failure, impossible, lack, loss, miss, neg-
ative, neither, never, no, none, nor, not, unable, with-
out. Instances with this negation signals are directly
assigned their class. The classifier predicts the class
of the rest of tokens.
5.2 Scope finding
In this phase three classifiers predict whether a token
is the first token in the scope sequence, the last, or
neither. A fourth classifier is a metalearner that uses
the predictions of the three classifiers to predict the
scope classes. The three object classifiers that pro-
vide input to the metalearner were trained using the
following machine learning methods:
? Memory-based learning as implemented in TiMBL
(version 6.1.2) (Daelemans et al, 2007), a super-
vised inductive algorithm for learning classification
tasks based on the k-nearest neighbor classification
rule (Cover and Hart, 1967). In this lazy learning
approach, all training data is kept in memory and
classification of a new item is achieved by extrap-
olation from the most similar remembered training
items.
? Support vector machines (SVM) as implemented in
SVMlightV6.01 (Joachims, 1999). SVMs are de-
fined on a vector space and try to find a decision
surface that best separates the data points into two
classes. This is achieved by using quadratic pro-
gramming techniques. Kernel functions can be used
to map the original vectors to a higher-dimensional
space that is linearly separable.
3Tags produced by the GENIA tagger that indicate if a token
is inside a certain chunk, outside, or at the beginning.
? Conditional random fields (CRFs) as implemented
in CRF++-0.51 (Lafferty et al, 2001). CRFs de-
fine a conditional probability distribution over label
sequences given a particular observation sequence
rather than a joint distribution over label and ob-
servation sequences, and are reported to avoid the
label bias problem of HMMs and other learning ap-
proaches.
The memory-based learning algorithm was pa-
rameterised by using overlap as the similarity met-
ric, gain ratio for feature weighting, using 7 k-
nearest neighbors, and weighting the class vote of
neighbors as a function of their inverse linear dis-
tance. The SVM was parameterised in the learning
phase for classification, cost factor of 1 and biased
hyperplane, and it used a linear kernel function. The
CRFs classifier used regularization algorithm L2 for
training, the hyper-parameter and the cut-off thresh-
old of features were set to 1.
An instance represents a pair of a negation signal
and a token from the sentence. This means that all
tokens in a sentence are paired with all negation sig-
nals that occur in the sentence. Negation signals are
those that have been classified as such in the previ-
ous phase. Only sentences that have negation signals
are selected for this phase.
We started with a larger, extensive pool of 131
features which encoded information about the nega-
tion signal, the paired token, their contexts, and the
tokens in between. Feature selection experiments
were carried out with the memory-based learning
classifier. Features were selected based on their
gain ratio, starting with all the features and elimi-
nating the least informative features. We also per-
formed experiments applying the feature selection
process reported in Tjong Kim Sang et al (2005),
a bi-directional hill climbing process. However, ex-
periments with this method did not produce a better
selection of features.
The features of the first three classifers are:
? Of the negation signal: Chain of words.
? Of the paired token: Lemma, POS, chunk IOB tag,
type of chunk; lemma of the second and third tokens
to the left; lemma, POS, chunk IOB tag, and type of
chunk of the first token to the left and three tokens
to the right; first word, last word, chain of words,
and chain of POSs of the chunk of the paired token
and of two chunks to the left and two chunks to the
24
right.
? Of the tokens between the negation signal and the
token in focus: Chain of POS types, distance in
number of tokens, and chain of chunk IOB tags.
? Others: A feature indicating the location of the to-
ken relative to the negation signal (pre, post, same).
The fourth classifier, a metalearner, is also a CRF
as implemented in CRF++. The features of this clas-
sifier are:
? Of the negation signal: Chain of words, chain of
POS, word of the two tokens to the right and two
tokens to the left, token number divided by the total
number of tokens in the sentence.
? Of the paired token: Lemma, POS, word of two to-
kens to the right and two tokens to the left, token
number divided by the total number of tokens in the
sentence.
? Of the tokens between the negation signal and the
token in focus: Binary features indicating if there
are commas, colons, semicolons, verbal phrases or
one of the following words between the negation
signal and the token in focus:
Whereas, but, although, nevertheless, notwith-
standing, however, consequently, hence, therefore,
thus, instead, otherwise, alternatively, furthermore,
moreover.
? About the predictions of the three classifiers: pre-
diction, previous and next predictions of each of
the classifiers, full sequence of previous and full se-
quence of next predictions of each of the classifiers.
? Others: A feature indicating the location of the to-
ken relative to the negation signal (pre, post, same).
Negation signals in the BioScope corpus always
have one consecutive block of scope tokens, includ-
ing the signal token itself. However, the classifiers
only predict the first and last element of the scope.
We need to process the output of the classifers in
order to build the complete sequence of tokens that
constitute the scope. We apply the following post-
processing:
(2) - If one token has been predicted as FIRST and one
as LAST, the sequence is formed by the tokens
between first and last.
- If one token has been predicted as FIRST and
none has been predicted as LAST, the sequence is
formed by the token predicted as FIRST.
- If one token has been predicted as LAST and
none as FIRST, the sequence will start at the
negation signal and it will finish at the token
predicted as LAST.
- If one token has been predicted as FIRST and
more than one as LAST, the sequence will end with
the first token predicted as LAST after the token
predicted as FIRST, if there is one.
- If one token has been predicted as LAST and
more than one as FIRST, the sequence will start at
the negation signal.
- If no token has been predicted as FIRST and
more than one as LAST, the sequence will start at
the negation signal and will end at the first token
predicted as LAST after the negation signal.
6 Results
The results provided for the abstracts part of the cor-
pus have been obtained by performing 10-fold cross
validation experiments, whereas the results provided
for papers and clinical reports have been obtained by
training on the full abstracts subcorpus and testing
on the papers and clinical reports subcorpus. The
latter experiment is therefore a test of the robustness
of the system when applied to different text types
within the same domain.
The evaluation is made using the precision and
recall measures (Van Rijsbergen, 1979), and their
harmonic mean, F-score. In the negation finding
task, a negation token is correctly classified if it has
been classified as being at the beginning or inside the
negation signal. We also evaluate the percentage of
negation signals that have been correctly identified.
In the scope finding task, a token is correctly classi-
fied if it has been correctly classified as being inside
or outside of the scope of all the negation signals that
there are in the sentence. This means that when there
is more than one negation signal in the sentence, the
token has to be correctly assigned a class for as many
negation signals as there are. Additionally, we eval-
uate the percentage of correct scopes (PCS). A scope
is correct if all the tokens in the sentence have been
assigned the correct scope class for a specific nega-
tion signal. The evaluation in terms of precision and
recall measures takes as unit a token, whereas the
evaluation in terms of PCS takes as unit a scope.
25
6.1 Negation signal finding
An informed baseline system has been created by
tagging as negation signals the tokens with the
words: absence, absent, fail, failure, impossible, in-
stead of, lack, loss, miss, negative, neither, never, no,
none, nor, not, rather than, unable, with the excep-
tion of, without. The list has been extracted from the
training corpus. Baseline results and inter-annotator
agreement scores are shown in Table 2.
Corpus Prec. Recall F1 Correct IAA
Abstracts 100.00 95.17 97.52 95.09 91.46
Papers 100.00 92.46 96.08 92.15 79.42
Clinical 100.00 97.53 98.75 97.72 90.70
Table 2: Baseline results of the negation finding system
and inter-annotator agreement (IAA) in %.
Table 3 shows the results of the system, which are
significantly higher than the results of the baseline
system. With a more comprehensive list of negation
signals it would be possible to identify all of them in
a text.
Corpus Prec. Recall F1 Correct
Abstracts 100.00 98.75 99.37 98.68
Papers 100.00 95.72 97.81 95.80
Clinical 100.00 98.09 99.03 98.29
Table 3: Results of the negation finding system in %.
The lower result of the papers subcorpus is caused
by the high frequency of the negation signal not in
this corpus (53.22 %), that is correct in 93.68 % of
the cases. The same negation signal is also frequent
in the abstracts subcorpus (58.89 %), but in this case
it is correct in 98.25 % of the cases. In the clinical
subcorpus not has low frequency (6.72 %), which
means that the performance of the classifer for this
negation signal (91.22 % correct) does not affect so
much the global results of the classifier. Most errors
in the classification of not are caused by the system
predicting it as a negation signal in cases not marked
as such in the corpus. The following sentences are
some examples:
(3) However, programs for tRNA identification [...] do not
necessarily perform well on unknown ones.
The evaluation of this ratio is difficult because not all
true interactions are known.
However, the Disorder module does not contribute
significantly to the prediction.
6.2 Scope finding
An informed baseline system has been created by
calculating the average length of the scope to the
right of the negation signal in each corpus and tag-
ging that number of tokens as scope tokens. We take
the scope to the right for the baseline because it is
much more frequent than the scope to the left, as is
shown by the statistics contained in Table 1 of Sec-
tion 3.
Corpus Prec. Recall F1 PCS PCS-2 IAA
Abstracts 76.68 78.26 77.46 7.11 37.45 92.46
Papers 69.34 66.92 68.11 4.76 24.86 70.86
Clinical 86.85 74.96 80.47 12.95 62.27 76.29
Table 4: Baseline results of the scope finding system and
inter-annotator agreement (IAA) in %.
Baseline results and inter-annotator agreement
scores are presented in Table 4. The percentage
of correct scopes has been measured in two ways:
PCS measures the proportion of correctly classified
tokens in the scope sequence, whereas PCS-2 mea-
sures the proportion of nouns and verbs that are cor-
rectly classifed in the scope sequence. This less
strict way of computing correctness is motivated by
the fact that being able to determine the concepts
and relations that are negated (indicated by content
words) is the most important use of the negation
scope finder. The low PCS for the three subcorpora
indicates that finding the scope of negations is not a
trivial task. The higher PCS for the clinical subcor-
pus follows a trend that applies also to the results of
the system. The fact that, despite a very low PCS,
precision, recall and F1 are relatively high indicates
that these measures are in themselves not reliable to
evaluate the performance of the system.
The upper-bound results of the metalearner sys-
tem assuming gold standard identification of nega-
tion signals are shown in Table 5.
Corpus Prec. Recall F1 PCS PCS-2
Abstracts 90.68 90.68 90.67 73,36 74.10
Papers 84.47 84.95 84.71 50.26 54.23
Clinical 91.65 92.50 92.07 87.27 87.95
Table 5: Results of the scope finding system with gold-
standard negation signals.
The results of the metalearner system are pre-
sented in Table 6. Results with gold-standard nega-
26
tion signals are especially better for the clinical sub-
corpus because except for lack, negative and not,
all negation signals score a PCS higher than 90 %.
Thus, in the clinical subcorpus, if the negation sig-
nals are identified, their scope will be correctly
found. This does not apply to the abstracts and pa-
pers subcorpus.
Corpus Prec. Recall F1 PCS PCS-2
Abstracts 81.76 83.45 82.60 66.07 66.93
Papers 72.21 69.72 70.94 41.00 44.44
Clinical 86.38 82.14 84.20 70.75 71.21
Table 6: Results of the scope finding system with pre-
dicted negation signals.
In terms of PCS, results are considerably higher
than baseline results, whereas in terms of precision,
recall and F1, results are slightly higher. Com-
pared to state of the art results (50.05 % PCS in
(anonymous reference) for the abstracts subcorpus),
the system achieves an error reduction of 32.07 %,
which shows that the system architecture presented
in this paper leads to more accurate results.
Evaluating the system in terms of a more relaxed
measure (PCS-2) does not reflect a significant in-
crease in its performance. This suggests that when
a scope is incorrectly predicted, main content to-
kens are also incorrectly left out of the scope or
added. An alternative to the PCS-2 measure would
be to mark in the corpus the relevant negated content
words and evaluate if they are under the scope.
Results also show that the system is portable to
different types of documents, although performance
varies depending on the characteristics of the corpus.
Clinical reports are easier to process than papers and
abstracts, which can be explained by several factors.
One factor is the length of sentences: 75.85 % of
the sentences in the clinical reports have 10 or less
words, whereas this rate is 3.17 % for abstracts and
11.27 % for papers. The average length of a sen-
tence for clinical reports is 7.73 tokens, whereas for
abstracts it is 26.43 and for papers 26.24. Shorter
sentences imply shorter scopes. In the scope finding
phase, when we process the output of the classifiers
to build the complete sequence of tokens that con-
stitute the scope, we give preference to short scopes
by choosing as LAST the token classified as LAST
that is the closest to the negation signal. A way to
make the system better portable to texts with longer
sentences would be to optimise the choice of the last
token in the scope.
Abstracts Papers Clinical
# PCS # PCS # PCS
absence 57 56.14 - - - -
absent 13 15.38 - - - -
can not 28 42.85 16 50.00 - -
could not 14 57.14 - - - -
fail 57 63.15 13 38.46 - -
lack 85 57.64 20 45.00 - -
negative - - - - 17 0.00
neither 33 51.51 - - - -
no 207 73.42 44 50.00 673 73.10
nor 43 44.18 - - - -
none 7 57.14 10 0.00 - -
not 1036 69.40 200 39.50 57 50.87
rather than 20 65.00 12 41.66 - -
unable 30 40.00 - - - -
without 82 89.02 24 58.33 - -
Table 7: PCS per negation signal for negation signals that
occur more than 10 times in one of the subcorpus.
Another factor that causes a higher performance
on the clinical subcorpus is the frequency of the
negation signal no (76.65 %), which has also a high
PCS in abstracts, as shown in Table 7. Typical ex-
ample sentences with this negation signal are shown
in (4). Its main characteristics are that the scope is
very short (5 tokens average in clinical reports) and
that it scopes to the right over a noun phrase.
(4) No findings to account for symptoms.
No signs of tuberculosis.
The lower performance of the system on the pa-
pers subcorpus compared to the abstracts subcorpus
is due to the high proportion of the negation signal
not (53.22 %), which scores a low PCS (39.50), as
shown in Table 7. Table 7 also shows that, except
for can not, all negation signals score a lower PCS
on the papers subcorpus. This difference can not
be caused by the sentence length, since the average
sentence length in the abstracts subcorpus (26.43 to-
kens) is similar to the average sentence length in the
papers subcorpus (26.24). The difference may be
related to the difference in the length of the scopes
and their direction. For example, the average length
of the scope of not is 8.85 in the abstracts subcorpus
and 6.45 in the papers subcorpus. The scopes to the
27
left for not amount to 23.28 % in the papers subcor-
pus and to 16.41 % in the abstracts subcorpus, and
the average scope to the left is 5.6 tokens in the pa-
pers subcorpus and 8.82 in the abstracts subcorpus.
As for the results per negation signal on the ab-
stracts corpus, the negation signals that score higher
PCS have a low (none) or null (absence, fail, lack,
neither, no, rather than, without) percentage of
scopes to the left. An exception is not with a high
score and 16.41% of scopes to the left. The negation
signals with lower PCS have a higher percentage of
scopes to the left (absent, can not, nor, unable). A
typical error for the negation signal unable is exem-
plified by the sentence VDR DNA-binding mutants
were unable to either bind to this element in vitro
or repress in vivo, in which the gold scope starts at
the beginning of the sentence, where the predicted
scopes starts at the negation signal.
6.2.1 Results of the metalearner versus results
of the first three classifiers
The choice of a metalearner approach has been
motivated by the significantly higher results that the
metalearner produces compared to the results of the
first three classifiers. The results of each of the clas-
sifiers independently are presented in Table 8.
Algor. Ev. Abstracts Papers Clinical
TiMBL Prec. 78.85 68.66 82.25
Rec. 80.54 66.29 78.56
F1 79.69 67.46 80.36
PCS 56.80 33.59 70.87
PCS-2 57.99 37.30 71.21
CRF Prec. 78.49 68.94 93.42
Rec. 80.16 66.57 80.24
F1 79.31 67.73 86.33
PCS 59.90 36.50 59.51
PCS-2 60.04 38.88 59.74
SVM Prec. 77.74 68.01 93.80
Rec. 79.35 65.66 85.16
F1 78.54 66.82 89.27
PCS 56.80 33.33 82.45
PCS-2 57.59 35.18 82.68
Table 8: Results for the first three classifiers of the scope
finding system.
PCS results show that the metalearner system per-
forms significantly better than the three classifiers
for the abstracts and papers subcorpora, but not for
the clinical subcorpus, in which case TiMBL and
SVM produce higher scores, although only the SVM
results are significantly better with a difference of
11.7 PCS. An analysis in detail of the SVM scores
per negation signal shows that the main difference
between the scores of the metalearner and SVM is
that the SVM is good at predicting the scopes of the
negation signal no when it occurs as the first token
in the sentence, like in (4) above. When no occurs
in other positions, SVM scores 1.17 PCS better.
We plan to perform experiments with the three
classifiers using the features of the metalearner that
are not related to the predictions, in order to check if
the three classifiers would perform better.
7 Conclusions
In this paper we have presented a metalearning ap-
proach to processing the scope of negation signals.
Its performance is evaluated in terms of percent-
age of correct scopes on three test sets. With 66.07
% PCS on the abstracts corpus the system achieves
32.07 % of error reduction over current state of the
art results. The architecture of the system is new for
this problem, with three classifiers and a metalearner
that takes as input the output of the first classifiers.
The classification task definition is also original.
We have shown that the system is portable to dif-
ferent corpora, although performance fluctuates de-
pending on the characteristics of the corpora. The
results per corpus are determined to a certain extent
by the scores of the negation signals no and not, that
are very frequent and difficult to process in some text
types. Shorter scopes are easier to learn as reflected
in the results of the clinical corpus, where no is the
most frequent negation signal. We have also shown
that the metalearner performs better than the three
first classifiers, except for the negation signal no in
clinical reports, for which the SVM classifier pro-
duces the highest scores.
Future research will deal with a more detailed
analysis of the errors by each of the three initial clas-
sifiers compared to the errors of the metalearner in
order to better understand why the results of the met-
alearner are higher. We also would like to perform
feature analysis, and test the system on general do-
main corpora.
28
Acknowledgments
Our work was made possible through financial sup-
port from the University of Antwerp (GOA project
BIOGRAPH). We are grateful to four anonymous
reviewers for their valuable comments and sugges-
tions.
References
M. Averbuch, T. Karson, B. Ben-Ami, O. Maimon, and
L. Rokach. 2004. Context-sensitive medical informa-
tion retrieval. In Proc. of the 11th World Congress
on Medical Informatics (MEDINFO-2004), pages 1?
8, San Francisco, CA. IOS Press.
S. Boytcheva, A. Strupchanska, E. Paskaleva, and
D. Tcharaktchiev. 2005. Some aspects of negation
processing in electronic health records. In Proc. of
International Workshop Language and Speech Infras-
tructure for Information Access in the Balkan Coun-
tries, pages 1?8, Borovets, Bulgaria.
S. Buchholz and E. Marsi. 2006. CoNLL-X shared task
on multilingual dependency parsing. In Proc. of the X
CoNLL Shared Task, New York. SIGNLL.
W. W. Chapman, W. Bridewell, P. Hanbury, G. F. Cooper,
and B.G. Buchanan. 2001. A simple algorithm for
identifying negated findings and diseases in discharge
summaries. J Biomed Inform, 34:301?310.
N. Collier, H.S. Park, N. Ogata, Y. Tateisi, C. Nobata,
T. Sekimizu, H. Imai, and J. Tsujii. 1999. The GE-
NIA project: corpus-based knowledge acquisition and
information extraction from genome research papers.
In Proceedings of EACL-99.
T. M. Cover and P. E. Hart. 1967. Nearest neighbor
pattern classification. Institute of Electrical and Elec-
tronics Engineers Transactions on Information The-
ory, 13:21?27.
W. Daelemans, J. Zavrel, K. Van der Sloot, and A. Van
den Bosch. 2007. TiMBL: Tilburg memory based
learner, version 6.1, reference guide. Technical Report
Series 07-07, ILK, Tilburg, The Netherlands.
P. L. Elkin, S. H. Brown, B. A. Bauer, C.S. Husser,
W. Carruth, L.R. Bergstrom, and D. L. Wahner-
Roedler. 2005. A controlled trial of automated classi-
fication of negation from clinical notes. BMC Medical
Informatics and Decision Making, 5(13).
l. M. Goldin and W.W. Chapman. 2003. Learning to
detect negation with ?Not? in medical texts. In Pro-
ceedings of ACM-SIGIR 2003.
S. Goryachev, M. Sordo, Q.T. Zeng, and L. Ngo. 2006.
Implementation and evaluation of four different meth-
ods of negation detection. Technical report, DSG.
Y. Huang and H.J. Lowe. 2007. A novel hybrid approach
to automated negation detection in clinical radiology
reports. J Am Med Inform Assoc, 14(3):304?311.
T. Joachims, 1999. Advances in Kernel Methods -
Support Vector Learning, chapter Making large-Scale
SVM Learning Practical, pages 169?184. MIT-Press,
Cambridge, MA.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proc. of ICML
2001, pages 282?289.
R. Morante, A. Liekens, and W. Daelemans. 2008. A
combined memory-based semantic role labeler of en-
glish. In Proc. of the EMNLP 2008, pages 715?724,
Honolulu, Hawaii.
A.G. Mutalik, A. Deshpande, and P.M. Nadkarni. 2001.
Use of general-purpose negation detection to augment
concept indexing of medical documents. a quantita-
tive study using the UMLS. J Am Med Inform Assoc,
8(6):598?609.
L. Rokach, R.Romano, and O. Maimon. 2008. Negation
recognition in medical narrative reports. Information
Retrieval Online.
O. Sanchez-Graillet and M. Poesio. 2007. Negation of
protein-protein interactions: analysis and extraction.
Bioinformatics, 23(13):424?432.
G. Szarvas, V. Vincze, R. Farkas, and J. Csirik. 2008.
The BioScope corpus: annotation for negation, uncer-
tainty and their scopein biomedical texts. In Proc. of
BioNLP 2008, pages 38?45, Columbus, Ohio, USA.
ACL.
E. Tjong Kim Sang, S. Canisius, A.van den Bosch, and
T. Bogers. 2005. Applying spelling error correction
techniques for improving semantic role labelling. In
Proc. of CoNLL 2005, pages 229?232.
Y. Tsuruoka and J. Tsujii. 2005. Bidirectional in-
ference with the easiest-first strategy for tagging se-
quence data. In Proc. of HLT/EMNLP 2005, pages
467?474.
Y. Tsuruoka, Y. Tateishi, J. Kim, T. Ohta, J. McNaught,
S. Ananiadou, and J. Tsujii, 2005. Advances in Infor-
matics - 10th Panhellenic Conference on Informatics,
volume 3746 of Lecture Notes in Computer Science,
chapter Part-of-Speech Tagger for Biomedical Text,
Advances in Informatics, pages 382?392. Springer,
Berlin/Heidelberg.
C.J. Van Rijsbergen. 1979. Information Retrieval. But-
terworths, London.
V. Vincze, G. Szarvas, R. Farkas, G. Mo?ra, and J. Csirik.
2008. The BioScope corpus: biomedical texts anno-
tated for uncertainty, negation and their scopes. BMC
Bioinformatics, 9((Suppl 11)):S9.
29
Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL): Shared Task, pages 25?30,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Joint memory-based learning of syntactic and semantic dependencies
in multiple languages
Roser Morante, Vincent Van Asch
CNTS - Language Technology Group
University of Antwerp
Prinsstraat 13
B-2000 Antwerpen, Belgium
{Roser.Morante,Vincent.VanAsch}@ua.ac.be
Antal van den Bosch
Tilburg University
Tilburg centre for Creative Computing
P.O. Box 90153
NL-5000 LE Tilburg, The Netherlands
Antal.vdnBosch@uvt.nl
Abstract
In this paper we present a system submitted to the
CoNLL Shared Task 2009 performing the identifi-
cation and labeling of syntactic and semantic depen-
dencies in multiple languages. Dependencies are
truly jointly learned, i.e. as if they were a single
task. The system works in two phases: a classifica-
tion phase in which three classifiers predict different
types of information, and a ranking phase in which
the output of the classifiers is combined.
1 Introduction
In this paper we present the machine learning system
submitted to the CoNLL Shared Task 2009 (Hajic?
et al, 2009). The task is an extension to multi-
ple languages (Burchardt et al, 2006; Hajic? et al,
2006; Kawahara et al, 2002; Palmer and Xue, 2009;
Surdeanu et al, 2008; Taule? et al, 2008) of the
CoNLL Shared Task 2008, combining the identifica-
tion and labeling of syntactic dependencies and se-
mantic roles. Our system is a joint-learning system
tested in the ?closed? challenge, i.e. without making
use of external resources.
Our system operates in two phases: a classifica-
tion phase in which three memory-based classifiers
predict different types of information, and a rank-
ing phase in which the output of the classifiers is
combined by ranking the predictions. Semantic and
syntactic dependencies are jointly learned and pro-
cessed. In the task description no precise defini-
tion is given of joint learning. We consider that a
joint-learning system is one in which semantic and
syntactic dependencies are learned and processed
jointly as a single task. In our system this is achieved
by fully merging semantic and syntactic dependen-
cies at the word level as the first step.
One direct consequence of merging the two tasks,
is that the class space becomes more complex;
the number of classes increases. Many machine-
learning approaches do not scale well to larger class
spaces in terms of efficiency and computer resource
requirements. Memory-based learning is a noted ex-
ception, as it is largely insensitive to the number of
classes in terms of efficiency. This is the primary
reason for using memory-based learning. Memory-
based language processing (Daelemans and van den
Bosch, 2005) is based on the idea that NLP prob-
lems can be solved by storing solved examples of the
problem in their literal form in memory, and apply-
ing similarity-based reasoning on these examples in
order to solve new ones. Memory-based algorithms
have been previously applied to semantic role la-
beling and parsing separately (Morante et al, 2008;
Canisius and Tjong Kim Sang, 2007).
We briefly discuss the issue of true joint learning
of two tasks in Section 2. The system is described
in Section 3, Section 4 presents and discusses the
results, and in Section 5 we put forward some con-
clusions and future research.
2 Joint learning
When two tasks share the same feature space, there
is the natural option to merge them and consider the
merge as a single task. The merging of two tasks
will typically lead to an increase in the number of
classes, and generally a more complex class space.
In practice, if two combined tasks are to some ex-
25
tent related, the increase will tend to be less than the
product of the number of classes in the two original
tasks, as classes from both tasks will tend to cor-
relate. Yet, even a mild increase of the number of
classes leads to a further fragmentation of the class
space, and thus to less training examples per class
label. Joint learning can therefore only lead to posi-
tive results if the data sparsity effect of the fragmen-
tation of the class space is counter-balanced by an
improved learnability.
Here, we treat the syntactic and semantic tasks as
one and the same task. At the word level, we merge
the class labels of the two tasks into single labels,
and present the classifiers with these labels. Further
on in our system, as we describe in the next sec-
tion, we do make use of the compositionality of the
labels, as the semantic and syntactic output spaces
represented two different types of structure.
3 System description
The joint system that we submitted works in
two phases: a classification phase in which three
memory-based classifiers predict different aspects of
joint syntactic and semantic labeling, and a ranking
phase in which the output of the classifiers is com-
bined. Additionally, a memory-based classifier is
used for predicate sense disambiguation. As a first
step, before generating the instances of the classi-
fiers we merge the semantic and syntactic dependen-
cies into single labels. The merged version of the
dependencies from an example sentence is shown
in Table 1, where column MERGED DEPs contains
all the dependencies of a token separated by a blank
space expressed in labels with the following format:
PHEAD::PDEPREL:APRED.
3.1 Phase 1: Classification
In the classification phase, three classifiers predict
different local aspects of the global output structure.
The classifiers have been optimized for English, by
training on the full training set and testing on the
development set; these optimized settings were then
used for the other six languages. We experimented
with manually selected parameters and with param-
eters selected by a genetic algorithm, but the param-
eters found by the genetic algorithm did not yield
better results than the manually selected parameters.
N Token Merged Dependencies
1 Housing 2::NMOD:A1
2 starts 2:: :A2 3::SBJ: 4:: :A1 6:: :A1 13:: :A0
3 are 0::ROOT:
4 expected 3::VC:
5 to 4::OPRD:C-A1
6 quicken 5::IM:
7 a 8::NMOD:
8 bit 6::OBJ:A2
9 from 6::ADV:A3
10 August 13::NMOD:AM-TMP
11 ?s 10::SUFFIX:
12 annual 13::NMOD:AM-TMP
13 pace 9::PMOD:
14 of 13::NMOD:A2
15 1,350,000 16::NMOD:
16 units 14::PMOD:
17 . 3::P:
Table 1: Example sentence with merged depen-
dency labels.
3.1.1 Classifier 1: Pairwise semantic and
syntact dependencies
Classifier 1 predicts the merged semantic and syn-
tactic dependencies that hold between two tokens.
Instances represent combinations of pairs of tokens
within a sentence. Each token is combined with all
other tokens in the sentence. The class predicted is
the PDEPREL:APRED label. The amount of classes
per language is shown in Table 2 (?Classifier 1?).
Number of classes
Lang. Classifier 1 Classifier 2
Cat 111 111
Chi 309 1209
Cze 395 1221
Eng 351 1957
Ger 152 300
Jap 103 505
Spa 124 124
Table 2: Number of classes per language predicted
by Classifiers 1 and 2.
We use an IB1 memory?based algorithm as im-
plemented in TiMBL (version 6.1.2) 1, a memory-
based classifier based on the k-nearest neighbor
1TiMBL: http://ilk.uvt.nl/timbl
26
rule. The IB1 algorithm was parameterised by us-
ing modified value difference as the similarity met-
ric, gain ratio for feature weighting, using 11 k-
nearest neighbors, and weighting the class vote of
neighbors as a function of their inverse linear dis-
tance. Because of time limitations we used TRIBL
for Czech and Chinese to produce the official results,
although we also provide postevaluation results pro-
duced with IB1. TRIBL is a hybrid combination
of IB1 and IGTREE, a fast decision-tree approxi-
mation of k-NN (Daelemans and van den Bosch,
2005), trading off fast decision-tree lookup on the
most important features (in our experiments, five)
with slower k-NN classification on the remaining
features.
The features2 used by this classifier are:
? The word, lemma, POS and FILLPRED3 of the to-
ken, the combined token and of two tokens before
and after token and combined token.
? POS and FILLPRED of the third token before and
after token and combined token.
? Distance between token and combined token, loca-
tion of token in relation to combined token.
Because data are skewed towards the NONE
class, we downsampled the training instances so that
there would be a negative instance for every positive
instance. Instances with the NONE class to be kept
were randomly selected.
3.1.2 Classifier 2: Per-token relations
Classifier 2 predicts the labels of the dependency
relations of a token with its syntactic and/or seman-
tic head(s). Instances represent a token. As an ex-
ample, the instance that represents token 2 in Table 1
would have as class: :A2-SBJ: - :A1- :A1- :A0.
The amount of classes per language is shown in Ta-
ble 2 under ?Classifier 2?. The number of classes
exceeds 1,000 for Chinese, Czech, and English.
The features used by the classifier are the word,
lemma, POS and FILLPRED of the token and two
tokens before and after the token. We use the IB1
memory?based algorithm parameterised in the same
way as Classifier 1.
2POS refers to predicted part-of-speech and lemma to pre-
dicted lemma in the description of features for all classifiers.
3The FILLPRED column has value Y if a token is a predi-
cate.
3.1.3 Classifier 3: Pairwise detection of a
relation
Classifier 3 is a binary classifier that predicts
whether two tokens have a dependency relation. In-
stance representation follows the same scheme as
with Classifier 1. We use the IGTREE algorithm as
implemented in TiMBL. The data are also skewed
towards the NONE class, so we downsampled the
training instances so that there would be a negative
instance for every four positive instances.
The features used by this classifier are:
? The word, lemma, POS and FILLPRED of the to-
ken, of the combined token, and of two tokens be-
fore and after the token.
? Word and lemma of two tokens before and after
combined token.
? Distance between token and combined token.
3.1.4 Results
The results of the Classifiers are presented in Ta-
ble 3. The performance of Classifiers 1 and 3 is sim-
ilar across languages, whereas the scores for Clas-
sifier 2 are lower for Chinese, Czech and English.
This can be explained by the fact that the number of
classes that Classifier 2 predicts for these languages
is significantly higher.
Lang. C1 C2 C3
Cat 94.77 86.30 97.96
Chi 92.10 70.11 95.47
Cze 87.33 67.87 93.88
Eng 94.17 76.16 95.37
Ger 92.76 83.23 93.77
Jap 91.55 81.22 96.75
Spa 94.76 84.40 96.39
Table 3: Micro F1 scores per classifier (C) and per
language.
Training times for the three classifiers were rea-
sonably short, as is to be expected with memory-
based classification. With English, C2 takes just
over two minutes to train, and C3 half a minute. C1
takes 8 hours and 18 minutes, due to the much larger
amount of examples and features.
3.2 Phase 2: Ranking
The classifier that is at the root of generating the
desired output (dependency graphs and semantic
27
role assignments) is Classifier 1, which predicts the
merged semantic and syntactic dependencies that
hold between two tokens (PDEPREL:APRED la-
bels). If this classifier would be able to predict the
dependencies with 100% accuracy, no further pro-
cessing would be necessary. Naturally, however, the
classifier predicts incorrect dependencies to a certain
degree, and does not provide a graph in wich all to-
kens have at least a syntactic head. It achieves 51.3%
labeled macro F1. The ranking phase improves this
performance. This is done in three steps: (i) ranking
the predictions of Classifier 1; (ii) constructing an
intermediate dependency tree, and (iii) adding extra
semantic dependencies to the tree.
3.2.1 Ranking predictions of Classifier 1
In order to disambiguate between all possible de-
pendencies predicted by this classifier, the system
applies ranking rules. It analyses the dependency
relations that have been predicted for a token with
its potential parents in the sentence and ranks them.
For example, for a sentence with 10 tokens, the sys-
tem would make 10 predictions per token. The pre-
dictions are first ranked by entropy of the class dis-
tribution for that prediction, then using the output of
Classifier 2, and next using the output of Classifier 3.
Ranking by entropy In order to compute entropy
we use the (inverse-linear) distance-weighted class
label distributions among the nearest neighbors that
Classifier 1 was able to find. For example, the pre-
diction for an instance can be: { NONE (2.74),
NMOD: (0.48) }. We can compute the entropy for
this instance using the formula in (1):
?
n?
i=1
P (labeli)log2(P (labeli)) (1)
with
- n: the total number of different labels in the distri-
bution, and
- P (labeli): the weight of label ithe total sum of the weights in the distribution
The system ranks the prediction with the lowest
entropy in position 1, while the prediction with the
highest entropy is ranked in the last position. The
rationale behind this is that the lower the entropy,
the more certain the classifier is about the predicted
dependency. Table 4 lists the first six heads for the
predicate word ?starts? ranked by entropy (cf. Ta-
ble 1).
Head Predicted label Distribution Entropy
Housing NONE { NONE (8.51) } 0.0
expected :A1 { :A1 (5.64) } 0.0
to NONE { NONE (4.74) } 0.0
quicken :A0 { :A0 (4.13), :A1 (0.18), :A2 (0.31) } 0.56
are NONE { NONE (2.56), SBJ: (0.52) } 0.65
starts :A0 { :A0 (7.90), :A1 (0.61), :A2 (1.50) } 0.93
Table 4: Output of Classifier 1 for the first six heads
of ?starts?, ranked by entropy.
On the development data for English, applying
this rule causes a marked error reduction of 26.5%
on labeled macro F1: from 51.3% to 64.2%.
Ranking by Classifier 2 The next ranking step is
performed by using the predictions of Classifier 2,
i.e. the estimated labels of the dependency rela-
tions of a token with its syntactic and/or semantic
head(s). The system ranks the predictions that are
not in the set of possible dependencies predicted by
Classifier 2 at the bottom of the ranked list.
Head Predicted label Distribution Entropy
expected :A1 { :A1 (5.64) } 0.0
Housing NONE { NONE (8.51) } 0.0
to NONE { NONE (4.74) } 0.0
quicken :A0 { :A0 (4.13), :A1 (0.18), :A2 (0.31) } 0.56
are NONE { NONE (2.56), SBJ: (0.52) } 0.65
starts :A0 { :A0 (7.90), :A1 (0.61), :A2 (1.50) } 0.93
Table 5: Output of Classifier 1 for the first six heads
of ?starts?. Ranked by entropy and Classifier 2.
Because this is done after ranking by entropy, the
instances with the lowest entropy are still at the top
of the list. Table 5 displays the re-ranked six heads
of ?starts?, given that Classifier 2 has predicted that
possible relations to heads are SBJ:A1 and :A1, and
given that only ?expected? is associated with one of
these two relations.
On the development data for English, applying
this rule induces a 9.0% error reduction on labeled
macro F1: from 64.2% to 67.4%.
Ranking by Classifier 3 The final ranking step
makes use of Classifier 3, which predicts the rela-
tion that holds between two tokens. The dependency
relations predicted by Classifier 1 that are not con-
firmed by Classifier 3 predicting that a relation exists
are moved to the end of the ranked list. Table 6 lists
the resulting ranked list. On the development data
for English, applying this rule yields another 5.2%
28
error reduction on labeled macro F1: from 67.4% to
69.1%.
Head Predicted label Distribution Entropy
expected :A1 { :A1 (5.64) } 0.0
quicken :A0 { :A0 (4.13), :A1 (0.18), :A2 (0.31) } 0.56
starts :A0 { :A0 (7.90), :A1 (0.61), :A2 (1.50) } 0.93
Housing NONE { NONE (8.51) } 0.0
to NONE { NONE (4.74) } 0.0
are NONE { NONE (2.56), SBJ: (0.52) } 0.65
Table 6: Output of Classifier 1 for the first six heads
of ?starts?. Ranked by entropy, Classifier 2, and
Classifier 3.
3.2.2 Construction of the intermediate
dependency tree
After ranking the predictions of Classifier 1, the
system selects a syntactic head for every token. This
is motivated by the fact that every token has one and
only one syntactic head. The system selects the pre-
diction with the best ranking that has in the PDE-
PREL part a value different than ? ?.
The intermediate tree can have more than one root
or no root at all. To make sure that every sentence
has one and only one root we apply some extra rules.
If the sentence does not have a token with a root la-
bel, the system checks the distributions of Classi-
fier 1. The token with the rootlabel in its distribution
that is the head of the biggest number of tokens is
taken as root. If the intermediate tree has more than
one root, the last root is taken as root. The other root
tokens get the label with a syntax part (PDEPREL)
that has the highest score in the distribution of Clas-
sifier 1.
The product of this step is a tree in which ev-
ery token is uniquely linked to a syntactic head.
Because syntactic and semantic dependencies have
been linked, the tree contains also semantic depen-
dencies. However, the tree is missing the purely se-
mantic dependencies. The next step adds these rela-
tions to the dependency tree.
3.2.3 Adding extra semantic dependencies
In order to find the tokens that have only a seman-
tic relation with a predicate, the system analyses for
each predicate (i.e. tokens marked with Y in FILL-
PRED) the list of predictions made by Classifier 1
and selects the predictions in which the PDEPREL
part of the label is ? ? and the APRED part of the
label is different than ? ?. On the development data
for English, applying this rule produces a 6.7% er-
ror reduction on labeled macro F1: from 69.1% to
71.1%.
3.3 Predicate sense disambiguation
Predicate sense disambiguation is performed by a
classifier per language that predicts the sense of the
predicate, except for Japanese, as with that language
the lemma is taken as the sense. We use the IGTREE
algorithm. Instances represent predicates and the
features used are the word, lemma and POS of the
predicate, and the lemma and POS of two tokens be-
fore and after the predicate. The results per language
are presented in Table 7.
Lang. Cat Chi Cze Eng Ger Spa
F1 82.40 94.85 87.84 93.64 73.57 81.13
Table 7: Micro F1 for the predicate sense disam-
biguation.
4 Overall results
The system was developed by training on the train-
ing set provided by the task organisers and testing
on the development set. The final results were ob-
tained by testing on the testing set. Table 8 shows
the global results of the system for syntactic and se-
mantic dependencies.
Lang. F1 Precision Recall
Cat 73.75 74.91 72.63
Chi 67.16 68.09 66.26
Chi* 67.79 68.70 66.89
Cze 60.50 62.55 58.58
Cze* 68.68 70.38 67.07
Eng 78.19 79.69 76.74
Ger 67.51 69.52 65.62
Jap 77.75 81.91 73.98
Spa 70.78 71.34 70.22
Av. 70.81 72.57 69.15
Table 8: Macro F1, precision and recall for all de-
pendencies per language. Postevaluation results are
marked with *.
Table 9 shows the scores of syntactic and seman-
tic dependencies in isolation.
29
Syntax Semantics
Lang. LA F1 Precision Recall
Cat 77.33 70.14 72.49 67.94
Chi 67.58 66.71 68.59 64.93
Chi* 67.92 67.63 69.48 65.86
Cze 49.41 71.49 75.68 67.75
Cze* 60.03 77.28 80.73 74.11
Eng 80.35 75.97 79.04 73.13
Ger 73.88 61.01 65.15 57.36
Jap 86.17 68.82 77.66 61.80
Spa 73.07 68.48 69.62 67.38
Av. 72.54 68.95 72.60 65.76
Table 9: Labeled attachment (LA) score for syntac-
tic dependencies and Macro F1, precision and recall
of semantic dependencies per language. Postevalua-
tion results are marked with *.
5 Conclusions
In this paper we presented the system that we sub-
mitted to the ?closed? challenge of the CoNLL
Shared Task 2009. We observe fairly low scores,
which can be possibly improved for all languages by
making use of the available morpho-syntactic fea-
tures, which we did not use in the present system,
by optimising the classifiers per language, and by
improving the reranking algorithm. We also ob-
serve a relatively low recall on the semantic task as
compared to overall recall, indicating that syntactic
dependencies are identified with a better precision-
recall balance. A logical continuation of this study
is to compare joint learning to learning syntactic and
semantic dependencies in isolation, using the same
architecture. Only then will we be able to put for-
ward conclusions about the performance of a joint
learning system versus the performance of a system
that learns syntax and semantics independently.
Acknowledgments
This study was made possible through financial sup-
port from the University of Antwerp (GOA project
BIOGRAPH), and from the Netherlands Organisa-
tion for Scientific Research.
References
Aljoscha Burchardt, Katrin Erk, Anette Frank, Andrea
Kowalski, Sebastian Pado?, and Manfred Pinkal. 2006.
The SALSA corpus: a German corpus resource for
lexical semantics. In Proceedings of the 5th Interna-
tional Conference on Language Resources and Evalu-
ation (LREC-2006), Genoa, Italy.
S. Canisius and E. Tjong Kim Sang. 2007. A con-
straint satisfaction approach to dependency parsing.
In Proceedings of the CoNLL Shared Task Session of
EMNLP-CoNLL 2007, pages 1124?1128.
W. Daelemans and A. van den Bosch. 2005. Memory-
based language processing. Cambridge University
Press, Cambridge, UK.
Jan Hajic?, Jarmila Panevova?, Eva Hajic?ova?, Petr
Sgall, Petr Pajas, Jan S?te?pa?nek, Jir??? Havelka, Marie
Mikulova?, and Zdene?k Z?abokrtsky?. 2006. Prague De-
pendency Treebank 2.0.
Jan Hajic?, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Anto`nia Mart??, Llu??s
Ma`rquez, Adam Meyers, Joakim Nivre, Sebastian
Pado?, Jan S?te?pa?nek, Pavel Stran?a?k, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The CoNLL-
2009 shared task: Syntactic and semantic depen-
dencies in multiple languages. In Proceedings of
the 13th Conference on Computational Natural Lan-
guage Learning (CoNLL-2009), June 4-5, Boulder,
Colorado, USA.
Daisuke Kawahara, Sadao Kurohashi, and Ko?iti Hasida.
2002. Construction of a Japanese relevance-tagged
corpus. In Proceedings of the 3rd International
Conference on Language Resources and Evaluation
(LREC-2002), pages 2008?2013, Las Palmas, Canary
Islands.
R. Morante, W. Daelemans, and V. Van Asch. 2008. A
combined memory-based semantic role labeler of en-
glish. In Proc. of the CoNLL 2008, pages 208?212,
Manchester, UK.
Martha Palmer and Nianwen Xue. 2009. Adding seman-
tic roles to the Chinese Treebank. Natural Language
Engineering, 15(1):143?172.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu??s Ma`rquez, and Joakim Nivre. 2008. The CoNLL-
2008 shared task on joint parsing of syntactic and se-
mantic dependencies. In Proceedings of the 12th Con-
ference on Computational Natural Language Learning
(CoNLL-2008).
Mariona Taule?, Maria Anto`nia Mart??, and Marta Re-
casens. 2008. AnCora: Multilevel Annotated Corpora
for Catalan and Spanish. In Proceedings of the 6th
International Conference on Language Resources and
Evaluation (LREC-2008), Marrakesh, Morroco.
30
Proceedings of the Workshop on BioNLP, pages 28?36,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Learning the scope of hedge cues in biomedical texts
Roser Morante, Walter Daelemans
CNTS - Language Technology Group
University of Antwerp
Prinsstraat 13, B-2000 Antwerpen, Belgium
{Roser.Morante,Walter.Daelemans}@ua.ac.be
Abstract
Identifying hedged information in biomedical
literature is an important subtask in informa-
tion extraction because it would be mislead-
ing to extract speculative information as fac-
tual information. In this paper we present a
machine learning system that finds the scope
of hedge cues in biomedical texts. The sys-
tem is based on a similar system that finds the
scope of negation cues. We show that the same
scope finding approach can be applied to both
negation and hedging. To investigate the ro-
bustness of the approach, the system is tested
on the three subcorpora of the BioScope cor-
pus that represent different text types.
1 Introduction
Research on information extraction of biomedical
texts has grown in the recent years. Most work
concentrates on finding relations between biologi-
cal entities, like genes and proteins (Krauthammer
et al, 2002; Mitsumori et al, 2006; Krallinger et
al., 2008a; Krallinger et al, 2008b). Determining
which information has been hedged in biomedical
literature is an important subtask of information ex-
traction because extracted information that falls in
the scope of hedge cues cannot be presented as fac-
tual information. It should be discarded or presented
separately with lower confidence. The amount of
hedged information present in texts cannot be un-
derstimated. Vincze et al (2008) report that 17.70%
of the sentences in the abstracts section of the Bio-
Scope corpus and 19.44% of the sentences in the
full papers section contain hedge cues. Light et al
(2004) estimate that 11% of sentences in MEDLINE
abstracts contain speculative fragments. Szarvas
(2008) reports that 32.41% of gene names men-
tioned in the hedge classification dataset described
in Medlock and Briscoe (2007) appears in a specu-
lative sentence.
In this paper we present a machine learning sys-
tem that finds the scope of hedge cues in biomedical
texts. Finding the scope of a hedge cue means deter-
mining at sentence level which words in the sentence
are affected by the hedge cue. The system combines
several classifiers and works in two phases: in the
first phase hedge cues (i.e., words indicating spec-
ulative language) are identified, and in the second
phase the full scope of these hedge cues is found.
This means that for a sentence like the one in Ex-
ample (1) taken from the BioScope corpus (Szarvas
et al, 2008), the system performs two actions: first,
it detects that suggest, might, and or are hedge sig-
nals; second, it detects that suggest has as its scope
expression of c-jun, jun B and jun D genes might be
involved in terminal granulocyte differentiation or in
regulating granulocyte functionality, that might has
as its scope be involved in terminal granulocyte dif-
ferentiation or in regulating granulocyte functional-
ity, and that or has as its scope in regulating granu-
locyte functionality.
(1) These results <xcope id=?X7.5.3? ><cue type= ?spec
ulation? ref=?X7.5.3?> suggest </cue> that <xcope
id= ?X7.5.2?>expression of c-jun, jun B and jun D
genes <cue type= ?speculation? ref= ?X7.5.2?> might
</cue> be involved <xcope id=?X7.5.1?>in terminal
granulocyte differentiation <cue type= ?speculation?
ref=?X7.5.1? >or</cue> in regulating granulocyte
functionality </xcope></xcope></xcope>.
28
Contrary to current practice to only detect modal-
ity, our system also determines the part of the sen-
tence that is hedged. We are not aware of other sys-
tems that perform this task. The system is based on a
similar system that finds the scope of negation cues
(Morante and Daelemans, 2009). We show that the
system performs well for this task and that the same
scope finding approach can be applied to both nega-
tion and hedging. To investigate the robustness of
the approach, the system is tested on three subcor-
pora of the BioScope corpus that represent different
text types. Although the system was developed and
tested on biomedical text, the same approach can
also be applied to text from other domains.
The paper is organised as follows. In Section 2,
we summarise related work. In Section 3, we de-
scribe the corpus on which the system has been de-
veloped. In Section 4, we introduce the task to be
performed by the system, which is described in Sec-
tion 5. Results are presented and discussed in Sec-
tion 6. Finally, Section 7 puts forward some conclu-
sions.
2 Related work
Hedging has been broadly treated from a theoretical
perspective. The term hedging is originally due to
Lakoff (1972), who introduces it in relation to pro-
totype theory. Palmer (1986) defines a term related
to hedging, epistemic modality, which expresses the
speaker?s degree of commitment to the truth of a
proposition. Saur?? et al (2006) research the modal-
ity of events, which ?expresses the speaker?s degree
of of commitment to the events being referred to in
a text?. They treat a wide spectrum of modal types
and present the codification of modality information
with the specification language TimeML, which al-
lows to mark modality cues at a lexical level and at
a syntactic level.
As for research that focuses specifically on scien-
tific texts with descriptive purposes, Hyland (1998)
describes hedging in scientific research articles,
proposing a pragmatic classification of hedge ex-
pressions based on an exhaustive analysis of a cor-
pus. The catalogue of hedging cues includes modal
auxiliaries, epistemic lexical verbs, epistemic ad-
jectives, adverbs, and nouns. Additionally, it in-
cludes also a variety of non?lexical cues. Light et
al. (2004) analyse the use of speculative language
in MEDLINE abstacts. They studied the expression
of levels of belief (hypothesis, tentative conclusions,
hedges, and speculations) and annotated a corpus
of abstracts in order to check if the distinction be-
tween high speculative, low speculative and definite
sentences could be made reliably. They found that
the speculative vs. definite distinction was reliable,
but the distinction between low and high speculative
was not. Thompson et al (2008) report on a list of
words and phrases that express modality in biomed-
ical texts and put forward a categorisation scheme.
The list and the scheme are validated by annotating
202 MEDLINE abstracts.
Some NLP applications incorporate modality in-
formation. Friedman et al (1994) develop a med-
ical text processor ?that translates clinical informa-
tion in patient documents into controlled vocabulary
terms?. The system uses a semantic grammar that
consists of rules that specify well-formed semantic
patterns. The extracted findings are assigned one
of five types of modality information: no, low cer-
tainty, moderate certainty, high certainty and cannot
evaluate. Di Marco and Mercer (2005) use hedging
information to classify citations. They observe that
citations appear to occur in sentences marked with
hedging cues.
Work on hedging in the machine learning field
has as a goal to classify sentences into speculative
or definite (non speculative). Medlock and Briscoe
(2007) provide a definition of what they consider to
be hedge instances and define hedge classification
as a weakly supervised machine learning task. The
method they use to derive a learning model from
a seed corpus is based on iteratively predicting la-
bels for unlabeled training samples. They report ex-
periments with SVMs on a dataset that they make
publicly available1. The experiments achieve a re-
call/precision break even point (BEP) of 0.76. They
apply a bag-of-words (BOG) approach to sample
representation. Medlock (2008) presents an exten-
sion of this work by experimenting with more fea-
tures (part-of-speech (PoS), lemmas, and bigrams).
Experiments show that the PoS representation does
not yield significant improvement over the results in
1Available at
http://www.benmedlock.co.uk/hedgeclassif.html.
29
Medlock and Briscoe (2007), whereas with a lemma
representation the system achieves a peak perfor-
mance of 0.8 BEP, and with bigrams of 0.82 BEP.
Szarvas (2008) follows Medlock and Briscoe (2007)
in classifying sentences as being speculative or non-
speculative. Szarvas develops a MaxEnt system that
incorporates bigrams and trigrams in the feature rep-
resentation and performs a complex feature selection
procedure in order to reduce the number of keyword
candidates. It achieves up to 0.85 BEP and 85.08
F1 by using an external dictionary. Kilicoglu and
Bergler (2008) apply a linguistically motivated ap-
proach to the same clasification task by using knowl-
edge from existing lexical resources and incorpo-
rating syntactic patterns. Additionally, hedge cues
are weighted by automatically assigning an informa-
tion gain measure and by assigning weights semi?
automatically depending on their types and central-
ity to hedging. The system achieves results of 0.85
BEP.
As mentioned earlier, we are not aware of re-
search that has focused on learning the scope of
hedge signals inside or outside of the biomedical do-
main, which makes a direct comparison with the ap-
proaches described here impossible.
3 Hedge cues in the BioScope Corpus
The system has been developed using the BioScope
corpus (Szarvas et al, 2008; Vincze et al, 2008)2,
a freely available resource that consists of medical
and biological texts. In the corpus, every sentence is
annotated with information about negation and spec-
ulation. The annotation indicates the boundaries of
the scope and the keywords, as shown in (1) above.
In the annotation, scopes are extended to the biggest
syntactic unit possible, so that scopes have the max-
imal length, and the speculation cue is always in-
cluded in the scope.
The BioScope corpus consists of three parts: clin-
ical free-texts (radiology reports), biological full pa-
pers and biological paper abstracts from the GENIA
corpus (Collier et al, 1999). Table 1 shows statistics
about the corpora. Hedge cues are represented by
one or more tokens, as (2) shows, where the hedge
cues that appear in the three corpora are listed. The
complete list of all hedge cues comprises 176 cues.
2Web page: www.inf.u-szeged.hu/rgai/bioscope.
In the same corpora the number of negation cues is
lower, 38.
(2) apparent, apparently, appear, assume, can, consider,
consistent with, could, either, indicate, likely, may, no
evidence, not, or, perhaps, possible, possibly,
presumably, probable, probably, should, suggestion,
support, think, unclear, whether, would
35 hedge cues that occur in the clinical reports
subcorpus do not occur in the abstracts subcorpus,
and 34 hedge cues that appear in the papers subcor-
pus do not appear in the abstracts subcorpus. Only
15.90% of the total of hedge cues appear in the three
subcorpora. The most frequent hedge cues in the ab-
stracts subcorpus are may (19.15 %), appear (5.30
%), and or (4.45 %); in the papers subcorpus, sug-
gest (10.26 %), may (9.97 %), and might (5.86 %);
and in the clinical subcorpus, or (24.27 %), suggest
(5.62 %), and evaluate for (5.27 %).
Clinical Papers Abstracts
#Documents 1954 9 1273
#Sentences 6383 2670 11871
#Words 41985 60935 282243
#Lemmas 2320 5566 14506
Av. length sentences 7.73 26.24 26.43
%Hedge sentences 13.39 19.44 17.70
# Hedge cues 1189 714 2769
Av. length scopes 5.92 14.37 16.27
Av. length scopes 5.15 13.00 15.44
to the right
Av. length scopes 2.46 5.94 5.60
to the left
% Scopes to the right 73.28 76.55 82.45
% Scopes to the left 26.71 23.44 17.54
Table 1: Statistics about the subcorpora in the BioScope
corpus and the hedge scopes (?Av?. stands for average).
The texts have been processed with the GENIA
tagger (Tsuruoka and Tsujii, 2005; Tsuruoka et al,
2005), a bidirectional inference based tagger that an-
alyzes English sentences and outputs the base forms,
part-of-speech tags, chunk tags, and named entity
tags in a tab-separated format. Additionally, we con-
verted the annotation about scope of negation into a
token-per-token representation, following the stan-
dard format of the 2006 CoNLL Shared Task (Buch-
holz and Marsi, 2006), where sentences are sepa-
rated by a blank line and fields are separated by a
single tab character. A sentence consists of a se-
quence of tokens, each one starting on a new line.
30
4 Finding the scope of hedge cues
We model this task in the same way that we mod-
elled the task for finding the scope of negation
(Morante and Daelemans, 2009), i.e., as two con-
secutive classification tasks: a first one that consists
of classifying the tokens of a sentence as being at the
beginning of a hedge signal, inside or outside. This
allows the system to find multiword hedge cues. The
second classification task consists of classifying the
tokens of a sentence as being the first element of the
scope, the last, or neither. This happens as many
times as there are hedge cues in the sentence.
5 System description
The two classification tasks (identifying hedge cues
and finding the scope) are implemented using super-
vised machine learning methods trained on part of
the annotated corpus.
5.1 Identifying hedge cues
In this phase, a classifier predicts for all tokens in a
sentence whether a token is the first token of a hedge
cue (B-cue), inside a hedge cue (I-cue), or outside of
it (O-cue). For sentence (3) the system assigns the
B-cue class to indicate, the I-cue class to that and
the O-cue class to the rest of tokens.
(3) These results indicate that a component or
components of NF?AT have the potential to
reconstitute NF(P)
The instances represent all tokens in the corpus
and they have features about the token: lemma,
word, part-of-speech (POS) and IOB3 chunk tag;
and features about the token context: Word, POS
and IOB chunk tag of 3 tokens to the right and 3 to
the left.
We use IGTREE as implemented in TiMBL (ver-
sion 6.1.2) (Daelemans et al, 2007). We also ex-
perimented with IB1, but it produced lower results.
The classifier was parameterised by using gain ratio
for feature weighting. According to the gain ratio
scores, the most informative features are the lemma
and word of the token in focus, followed by the word
of the token to the right and of the token to the left.
We performed two experiments. In one, the test
file is preprocessed using a list of hedge cues ex-
3I stands for ?inside?, B for ?beginning?, and O for ?outside?.
tracted from the training corpus. The list comprises
the following hedge cues listed in (4). Instances with
these hedge cues are directly assigned their class.
The classifier predicts the class of the rest of tokens.
In the other experiment we don?t preprocess the test
file.
(4) appear, apparent, apparently, believe, either, estimate,
hypothesis, hypothesize, if, imply, likely, may, might, or,
perhaps, possible, possibly, postulate, potential,
potentially, presumably, probably, propose, putative,
should, seem, speculate, suggest, support, suppose,
suspect, think, uncertain, unclear, unkwown, unlikely,
whether, would
5.2 Scope finding
In this phase three classifiers predict for all tokens
in the sentence whether a token is the first token in
the scope sequence (F-scope), the last (L-scope), or
neither (NONE). For the sentence in 3, the classi-
fiers assign the class F-scope to indicate, L-scope to
NF(P), and NONE to the rest of tokens. A fourth
classifier is a metalearner that uses the predictions
of the three classifiers to predict the scope classes.
An instance represents a pair of a hedge cue and a
token from the sentence. This means that all tokens
in a sentence are paired with all hedge cues that oc-
cur in the sentence. Hedge cues are those that have
been classified as such in the previous phase. Only
sentences that have hedge cues are selected for this
phase. The three object classifiers that provide input
to the metalearner were trained using the following
machine learning methods:
? Memory-based learning as implemented in
TiMBL (Daelemans et al, 2007), a supervised
inductive algorithm for learning classification tasks
based on the k-nearest neighbor classification
rule (Cover and Hart, 1967). In this lazy learning
approach, all training data is kept in memory
and classification of a new item is achieved by
extrapolation from the most similar remembered
training items.
? Support vector machines (SVM) as implemented in
SVMlightV6.01 (Joachims, 1999). SVMs are de-
fined on a vector space and try to find a decision
surface that best separates the data points into two
classes. This is achieved by using quadratic pro-
gramming techniques. Kernel functions can be used
to map the original vectors to a higher-dimensional
space that is linearly separable.
31
? Conditional random fileds (CRFs) as implemented
in CRF++-0.51 (Lafferty et al, 2001). CRFs de-
fine a conditional probability distribution over label
sequences given a particular observation sequence
rather than a joint distribution over label and ob-
servation sequences, and are reported to avoid the
label bias problem of HMMs and other learning ap-
proaches.
The memory-based learning algorithm was pa-
rameterised in this case by using overlap as the sim-
ilarity metric, gain ratio for feature weighting, using
7 k-nearest neighbors, and weighting the class vote
of neighbors as a function of their inverse linear dis-
tance. The SVM was parameterised in the learning
phase for classification, cost factor of 1 and biased
hyperplane, and it used a linear kernel function. The
CRFs classifier used regularization algorithm L2 for
training, the hyper-parameter and the cut-off thresh-
old of features were set to 1.
We have used the same features used for the sys-
tem that finds the scope of negation. The features of
the first three classifers are:
? Of the hedge signal: Chain of words.
? Of the paired token: Lemma, POS, chunk IOB tag,
type of chunk; lemma of the second and third tokens
to the left; lemma, POS, chunk IOB tag, and type of
chunk of the first token to the left and three tokens
to the right; first word, last word, chain of words,
and chain of POSs of the chunk of the paired token
and of two chunks to the left and two chunks to the
right.
? Of the tokens between the hedge cue and the token
in focus: Chain of POS types, distance in number
of tokens, and chain of chunk IOB tags.
? Others: A feature indicating the location of the to-
ken relative to the hedge cue (pre, post, same).
The fourth classifier, a metalearner, is also a CRFs
as implemented in CRF++. The features of this clas-
sifier are:
? Of the hedge signal: Chain of words, chain of POS,
word of the two tokens to the right and two tokens to
the left, token number divided by the total of tokens
in the sentence.
? Of the paired token: Lemma, POS, word of two to-
kens to the right and two tokens to the left, token
number divided by the total of tokens in the sen-
tence.
? Of the tokens between the hedge cue and the to-
ken in focus: Binary features indicating if there are
commas, colons, semicolons, verbal phrases or one
of the following words between the hedge cue and
the token in focus: Whereas, but, although, nev-
ertheless, notwithstanding, however, consequently,
hence, therefore, thus, instead, otherwise, alterna-
tively, furthermore, moreover.
? About the predictions of the three classifiers: pre-
diction, previous and next predictions of each of
the classifiers, full sequence of previous and full se-
quence of next predictions of each of the classifiers.
? Others: A feature indicating the location of the to-
ken relative to the hedge cue (pre, post, same).
Hedge cues in the BioScope corpus always scope
over a consecutive block of tokens, including the cue
token itself. However, the classifiers only predict
the first and last element of the scope. We need to
process the output of the classifers in order to build
the complete sequence of tokens that constitute the
scope. We apply the following postprocessing:
(5) - If one token has been predicted as FIRST and one
as LAST, the sequence is formed by the tokens
between first and last.
- If one token has been predicted as FIRST and
none has been predicted as LAST, the sequence is
formed by the token predicted as FIRST.
- If one token has been predicted as LAST and
none as FIRST, the sequence will start at the hedge
cue and it will finish at the token predicted as
LAST.
- If one token has been predicted as FIRST and
more than one as LAST, the sequence will end with
the first token predicted as LAST after the token
predicted as FIRST, if there is one.
- If one token has been predicted as LAST and
more than one as FIRST, the sequence will start at
the hedge signal.
- If no token has been predicted as FIRST and
more than one as LAST, the sequence will start at
the hedge cue and will end at the first token
predicted as LAST after the hedge signal.
6 Results
The results provided for the abstracts part of the cor-
pus have been obtained by performing 10-fold cross
validation experiments, whereas the results provided
32
for papers and clinical reports have been obtained by
training on the full abstracts subcorpus and testing
on the papers and clinical reports subcorpus. The
latter experiment is therefore a test of the robustness
of the system when applied to different text types
within the same domain. The evaluation is made us-
ing the precision and recall measures (Van Rijsber-
gen, 1979), and their harmonic mean, F-score. We
report micro F1.
In the hedge finding task, a hedge token is cor-
rectly classified if it has been classified as being at
the beginning or inside the hedge signal. We also
evaluate the percentage of hedge cues that have been
correctly identified. In the scope finding task, a to-
ken is correctly classified if it has been correctly
classified as being inside or outside of the scope of
all the hedge cues that there are in the sentence. This
means that when there is more than one hedge cue
in the sentence, the token has to be correctly as-
signed a class for as many hedge signals as there
are. Additionally, we evaluate the percentage of cor-
rect scopes (PCS). A scope is correct if all the tokens
in the sentence have been assigned the correct scope
class for a specific hedge signal. The evaluation in
terms of precision and recall measures takes as unit a
token, whereas the evaluation in terms of PCS takes
as unit a scope.
6.1 Hedge cue finding
An informed baseline system has been created by
tagging as hedge cues the tokens with the words
listed in (4) above. The list has been extracted from
the training corpus. The results are shown in Table 2.
Corpus Prec. Recall F1 % Correct
Abstracts 55.62 71.77 62.67 70.91
Papers 54.39 61.21 57.60 64.46
Clinical 66.55 40.78 50.57 51.38
Table 2: Baseline results of the hedge finding system.
The fact that the results are lower for the papers
and clinical subcorpora can be explained by the fact
that the list of cues has been extracted from the train-
ing corpus.
Table 3 shows the results of the system. The
results of the system for abstracts and papers are
higher than baseline, but for clinical they are lower.
This is due to the fact that in the baseline system the
hedge cue or that accounts for 24.53 % of the hedge
cues is 100 % correct, whereas the system achieves
only 0.72 % of correct predictions. The score ob-
tained by or is also the reason why the system pro-
duces lower results for the clinical subcorpus.
Corpus Prec. Recall F1 % Correct
Abstracts 90.81 79.84 84.77 78.67
Papers 75.35 68.18 71.59 69.86
Clinical 88.10 27.51 41.92 33.36
Table 3: Results of the hedge finding system without pre-
processing.
Table 4 shows the results of the system with pre-
processing. In terms of % of correct cues, the system
that uses a preprocessed test set gets higher scores,
but in terms of F1 it gets lower results, except for the
clinical subcorpus. The drop in F1 of this system is
caused by a drop in precision due to the excess of
false positives.
Corpus Prec. Recall F1 % Correct
Abstracts 60.74 94.83 74.05 96.03
Papers 56.56 84.03 67.61 88.60
Clinical 71.25 52.33 60.34 64.49
Table 4: Results of the hedge finding system with prepro-
cessing.
In the abstracts subcorpus the hedge cue that has
the biggest proportion of false positives is or. Of the
1062 accurrences of or, in 88.32% of the cases or is
not a hedge cue. The system that uses preprocessing
produces 938 false positives and 4 false negatives,
whereas the other system produces 21 false positives
and 108 false negatives. In the papers subcorpus, the
hedge cues if, or, can, indicate and estimate cause
67.38% of the false positives. In the clinical subcor-
pus the hedge cues evidence, evidence of, no and ap-
pear cause 88.27% of the false positives. In contrast
with the abstracts subcorpus, the hedge cue or has
only 5 false positives and scores an F1 of 99.10. So,
in the clinical corpus or is not ambiguous, whereas
in the abstracts subcorpus it is very ambiguous. An
example of or as hedge cue in the clinical subcorpus
is shown in (6). An example of or as hedge cue in
the abstracts subcorpus is shown in (7), and as a non
cue in (8).
33
(6) Findings compatible with reactive airway disease
or viral lower respiratory tract infection.
(7) Nucleotide sequence and PCR analyses
demonstrated the presence of novel duplications or
deletions involving the NF-kappa B motif.
(8) In nuclear extracts from monocytes or
macrophages, induction of NF-KB occurred only if
the cells were previously infected with HIV-1.
Compared to negation cues, hedge cues are more
varied and more ambiguous. Both the system with-
out and with preprocessing for negation finding per-
formed better than the hedge finding system.
6.2 Scope finding
An informed baseline system has been created by
calculating the average length of the scope to the
right of the hedge cue in each corpus and tagging
that number of tokens as scope tokens. We take the
scope to the right for the baseline because it is much
more frequent than the scope to the left, as is shown
by the statistics contained in Table 1 of Section 3.
Baseline results are presented in Table 5. The low
PCS for the three subcorpora indicates that finding
the scope of hedge cues is not a trivial task. The fact
that, despite a very low PCS, precision, recall and
F1 are relatively high indicates that these measures
are in themselves not reliable to evaluate the perfor-
mance of the system.
Corpus Prec. Recall F1 PCS
Abstracts 78.92 62.19 69.56 3.15
Papers 72.03 50.43 59.33 2.19
Clinical 64.92 25.10 36.20 2.72
Table 5: Baseline results of the scope finding system.
The upper-bound results of the metalearner sys-
tem assuming gold standard identification of hedge
cues are shown in Table 6.
Corpus Prec. Recall F1 PCS PCS-2
Abstracts 89.71 89.09 89.40 77.13 78.21
Papers 77.78 77.10 77.44 47.94 58.21
Clinical 79.16 78.13 78.64 60.59 63.94
Table 6: Results of the scope finding system with gold-
standard hedge signals.
The percentage of correct scopes has been mea-
sured in two ways: PCS measures the proportion
of correctly classified tokens in the scope sequence,
whereas PCS-2 measures the proportion of nouns
and verbs that are correctly classifed in the scope
sequence. This less strict way of computing correct-
ness is motivated by the fact that being able to deter-
mine the concepts and relations that are speculated
(indicated by content words) is the most important
use of the hedge scope finder.
Results show that the system achieves a high per-
centage of fully correct scopes, and that, although
performance is lower for the papers and clinical cor-
pora, the system is portable. Table 7 shows the re-
sults of the negation scope finding system also with
gold standard negation cues. The comparison of re-
sults shows that for abstracts and papers the scores
are higher for the hedge system, which means that
the system can be used for finding both types of
scope.
Corpus Prec. Recall F1 PCS PCS-2
Abstracts 90.68 90.68 90.67 73,36 74.10
Papers 84.47 84.95 84.71 50.26 54.23
Clinical 91.65 92.50 92.07 87.27 87.95
Table 7: Results of the negation scope finding system
with gold-standard negation signals.
The results of the hedge system with predicted
hedge cues are presented in Table 8. The hedge cues
have been predicted by the system without the pre-
processing step presented in Subsection 6.1.
Corpus Prec. Recall F1 PCS PCS-2
Abstracts 85.77 72.44 78.54 65.55 66.10
Papers 67.97 53.16 59.66 35.92 42.37
Clinical 68.21 26.49 38.16 26.21 27.44
Table 8: Results of the scope finding system with pre-
dicted hedge signals.
In terms of PCS, which is a scope based measure,
results are considerably higher than baseline results,
whereas in terms of precision, recall and F1, which
are token based measures, results are lower. Eval-
uating the system in terms of a more relaxed mea-
sure (PCS-2) does not reflect a significant increase
in its performance. This suggests that when a scope
is incorrectly predicted, main content tokens are also
incorrectly left out of the scope or added.
Results also show that the system based on pre-
dicted hedge cues performs lower for all corpora,
34
which is also a trend observed for the negation scope
finding system. The difference in performance for
abstracts and papers follows the same trends as in
the negation system, whereas the drop in perfor-
mance for the clinical subcorpus is bigger. This
can be explained by the results obtained in the cues
finding phase, where the clinical subcorpus obtained
only 41.92% F1. However, gold standard results
show that if the hedge cues are identified, then the
system is portable.
Abstracts Papers Clinical
# PCS # PCS # PCS
appear 143 58.04 39 28.20 - -
can 48 12.5 25 0.00 22 0.00
consistent with - - - - 67 0.00
could 67 11.94 28 14.28 36 22.22
either 28 0.00 - - - -
evaluate for - - - - 86 3.84
imply 21 90.47 - - - -
indicate 23 73.91 - - - -
indicate that 276 89.49 - - - -
likely 59 59.32 36 30.55 63 66.66
may 516 81.39 68 54.41 107 80.37
might 72 73.61 40 35.00 - -
or 120 0.00 - - 276 0.00
possible 50 66.00 24 54.16 26 80.76
possibly 25 52.00 - - - -
potential 45 28.88 - - - -
potentially 21 52.38 - - - -
propose 38 63.15 - - - -
putatitve 39 17.94 - - - -
rule out - - - - 61 0.00
suggest 613 92.33 70 62.85 64 90.62
think 35 31.42 - - - -
unknown 26 15.38 - - - -
whether 96 72.91 - - - -
would - - 21 28.57 - -
Table 9: PCS per hedge cue for hedge cues that occur
more than 20 times in one of the subcorpus.
Table 9 shows the PCS results per hedge cue. The
cues that get better scores in the clinical and papers
subcorpora are cues that appear in the abstracts sub-
corpus and get a good score. Cues that occur in the
clinical subcorpus and do not occur in the abstracts
(training) subcorpus, get 0.00 score or close to 0.00,
whereas cues that appear in both subcorpora tend to
get a similar or better score in the clinical subcor-
pus. This is a trend that we also observed in the
negation scope finding system. As with that system,
we also observed that the papers subcorpus tends to
get lower scores than the abstracts subcorpus.
The results of the system based on gold standard
hedge cues showed that the system can be applied
to negation scope finding and hedge scope finding,
but these results show that the results of the second
phase of the system depend on the results of the first
phase of the system, and that finding hedge cues
is a domain dependent task. The cues that are not
present in the training data cannot be learned in the
test data and the same applies to their scope. This
observation is consistent with the observation that
the portability of hedge classifiers is limited, made
by Szarvas (Szarvas, 2008).
7 Conclusions
In this paper we have presented a metalearning ap-
proach to processing the scope of hedge cues, based
on a system that finds the scope of negation cues. We
have shown that the same system can find both the
scope of negation and hedge cues. The performance
of the system is evaluated in terms of percentage of
correct scopes on three text types.
In the hedge finding phase, the system achieves
an F1 of 84.77% in the abstracts subcorpus. Ex-
isting systems that classify sentences as speculative
or not reach an 85.00 BEP. Although the tasks are
different, we consider that the results of our system
are competitive. In the scope finding phase, the sys-
tem that uses predicted hedge cues achieves 65.55%
PCS in the abstracts corpus, which is very similar
to the result obtained by the negation scope finding
system with predicted negation cues (66.07% PCS).
However, the results for the papers and clinical sub-
corpora are considerably lower than the results for
the abstracts subcorpus in the two phases. In the
case of the negation scope finding system, the evalu-
ation on the clinical subcorpus yielded a 4.23% PCS
higher result, whereas in the case of the hedge scope
finding system the results are almost 30.00% PCS
lower, confirming the observation that the portabil-
ity of hedge classifers is limited. Future research
will focus on trying to improve the first phase of the
system and anlysing errors in depth in order to get
insights into how to get a better performance.
Acknowledgments
Our work was made possible through financial sup-
port from the University of Antwerp (GOA project
BIOGRAPH). We are thankful to three anonymous
reviewers for their valuable comments.
35
References
S. Buchholz and E. Marsi. 2006. CoNLL-X shared task
on multilingual dependency parsing. In Proc. of the X
CoNLL Shared Task, New York. SIGNLL.
N. Collier, H.S. Park, N. Ogata, Y. Tateisi, C. Nobata,
T. Sekimizu, H. Imai, and J. Tsujii. 1999. The GE-
NIA project: corpus-based knowledge acquisition and
information extraction from genome research papers.
In Proc. of EACL 1999.
T. M. Cover and P. E. Hart. 1967. Nearest neighbor
pattern classification. Institute of Electrical and Elec-
tronics Engineers Transactions on Information The-
ory, 13:21?27.
W. Daelemans, J. Zavrel, K. Van der Sloot, and A. Van
den Bosch. 2007. TiMBL: Tilburg memory based
learner, version 6.1, reference guide. Technical Report
Series 07-07, ILK, Tilburg, The Netherlands.
C. Di Marco and R.E. Mercer, 2005. Computing attitude
and affect in text: Theory and applications, chapter
Hedging in scientific articles as a means of classifying
citations. Springer-Verlag, Dordrecht.
C. Friedman, P. Alderson, J. Austin, J.J. Cimino, and S.B.
Johnson. 1994. A general natural?language text pro-
cessor for clinical radiology. JAMIA, 1(2):161?174.
K. Hyland. 1998. Hedging in scientific research articles.
John Benjamins B.V, Amsterdam.
T. Joachims, 1999. Advances in Kernel Methods -
Support Vector Learning, chapter Making large-Scale
SVM Learning Practical, pages 169?184. MIT-Press,
Cambridge, MA.
H. Kilicoglu and S. Bergler. 2008. Recognizing specu-
lative language in biomedical research articles: a lin-
guistically motivated perspective. BMC Bioinformat-
ics, 9(Suppl 11):S10.
M. Krallinger, F. Leitner, C. Rodriguez-Penagos, and
A. Valencia. 2008a. Overview of the protein?protein
interaction annotation extraction task of BioCreative
II. Genome Biology, 9(Suppl 2):S4.
M. Krallinger, A. Valencia, and L. Hirschman. 2008b.
Linking genes to literature: text mining, informa-
tion extraction, and retrieval applications for biology.
Genome Biology, 9(Suppl 2):S8.
M. Krauthammer, P. Kra, I. Iossifov, S.M. Gomez,
G. Hripcsak, V. Hatzivassiloglou, C. Friedman, and
A.Rzhetsky. 2002. Of truth and pathways: chasing
bits of information through myriads of articles. Bioin-
formatics, 18(Suppl 1):S249?57.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proc. of ICML
2001, pages 282?289.
G. Lakoff. 1972. Hedges: a study in meaning criteria
and the logic of fuzzy concepts. Chicago Linguistics
Society Papers, 8:183?228.
M. Light, X.Y.Qiu, and P. Srinivasan. 2004. The lan-
guage of bioscience: facts, speculations, and state-
ments in between. In Proc. of the BioLINK 2004,
pages 17?24.
B. Medlock and T. Briscoe. 2007. Weakly supervised
learning for hedge classification in scientific literature.
In Proc. of ACL 2007, pages 992?999.
B. Medlock. 2008. Exploring hedge identification in
biomedical literature. JBI, 41:636?654.
T. Mitsumori, M. Murata, Y. Fukuda, K Doi, and H. Doi.
2006. Extracting protein-protein interaction informa-
tion from biomedical text with svm. IEICE - Trans.
Inf. Syst., E89-D(8):2464?2466.
R. Morante and W. Daelemans. 2009. A metalearning
approach to processing the scope of negation. In Proc.
of CoNLL 2009, Boulder, Colorado.
F.R. Palmer. 1986. Mood and modality. CUP, Cam-
bridge, UK.
R. Saur??, M. Verhagen, and J. Pustejovsky. 2006. Anno-
tating and recognizing event modality in text. In Proc.
of FLAIRS 2006, pages 333?339.
G. Szarvas, V. Vincze, R. Farkas, and J. Csirik. 2008.
The BioScope corpus: annotation for negation, uncer-
tainty and their scope in biomedical texts. In Proc. of
BioNLP 2008, pages 38?45, Columbus, Ohio. ACL.
G. Szarvas. 2008. Hedge classification in biomedi-
cal texts with a weakly supervised selection of key-
words. In Proc. of ACL 2008, pages 281?289, Colum-
bus, Ohio, USA. ACL.
P. Thompson, G. Venturi, J. McNaught, S. Montemagni,
and S. Ananiadou. 2008. Categorising modality in
biomedical texts. In Proc. of the LREC 2008 Workshop
on Building and Evaluating Resources for Biomedical
Text Mining 2008, pages 27?34, Marrakech. LREC.
Y. Tsuruoka and J. Tsujii. 2005. Bidirectional in-
ference with the easiest-first strategy for tagging se-
quence data. In Proc. of HLT/EMNLP 2005, pages
467?474.
Y. Tsuruoka, Y. Tateishi, J. Kim, T. Ohta, J. McNaught,
S. Ananiadou, and J. Tsujii, 2005. Advances in Infor-
matics - 10th Panhellenic Conference on Informatics,
volume 3746 of LNCS, chapter Part-of-Speech Tagger
for Biomedical Text, Advances in Informatics, pages
382?392. Springer, Berlin/Heidelberg.
C.J. Van Rijsbergen. 1979. Information Retrieval. But-
terworths, London.
V. Vincze, G. Szarvas, R. Farkas, G. Mo?ra, and J. Csirik.
2008. The BioScope corpus: biomedical texts anno-
tated for uncertainty, negation and their scopes. BMC
Bioinformatics, 9(Suppl 11):S9.
36
Proceedings of the Workshop on BioNLP: Shared Task, pages 59?67,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
A memory?based learning approach to event extraction in biomedical texts
Roser Morante, Vincent Van Asch, Walter Daelemans
CNTS - Language Technology Group
University of Antwerp
Prinsstraat 13
B-2000 Antwerpen, Belgium
{Roser.Morante,Walter.Daelemans,Vincent.VanAsch}@ua.ac.be
Abstract
In this paper we describe the memory-based ma-
chine learning system that we submitted to the
BioNLP Shared Task on Event Extraction. We mod-
eled the event extraction task using an approach that
has been previously applied to other natural lan-
guage processing tasks like semantic role labeling
or negation scope finding. The results obtained by
our system (30.58 F-score in Task 1 and 29.27 in
Task 2) suggest that the approach and the system
need further adaptation to the complexity involved
in extracting biomedical events.
1 Introduction
In this paper we describe the memory-based ma-
chine learning system that we submitted to the
BioNLP shared task on event extraction1. The sys-
tem operates in three phases. In the first phase, event
triggers and entities other than proteins are detected.
In the second phase, event participants and argu-
ments are identified. In the third phase, postprocess-
ing heuristics select the best frame for each event.
Memory-based language processing (Daelemans
and van den Bosch, 2005) is based on the idea that
NLP problems can be solved by reuse of solved ex-
amples of the problem stored in memory. Given
a new problem, the most similar examples are re-
trieved, and a solution is extrapolated from them.
As language processing tasks typically involve many
1Web page: http://www-tsujii.is.s.u-tokyo.
ac.jp/GENIA/SharedTask/index.html
subregularities and (pockets of) exceptions, it has
been argued that memory-based learning is at an
advantage in solving these highly disjunctive learn-
ing problems compared to more eager learning that
abstract from the examples, as the latter eliminates
not only noise but also potentially useful exceptions
(Daelemans et al, 1999).
The BioNLP Shared Task 2009 takes a
linguistically-motivated approach, which is re-
flected in the properties of the shared task definition:
rich semantics, a text-bound approach, and decom-
position of linguistic phenomena. Memory-based
algorithms have been successfully applied in lan-
guage processing to a wide range of linguistic tasks,
from phonology to semantic analysis. Our goal was
to investigate the performance of a memory?based
approach to the event extraction task, using only
the information available in the training corpus and
modelling the task applying an approach similar to
the one that has been applied to tasks like semantic
role labeling (Morante et al, 2008) or negation
scope detection (Morante and Daelemans, 2009).
In Section 2 we briefly describe the task. Section
3 reviews some related work. Section 4 presents the
system, and Section 5 the results. Finally, some con-
clusions are put forward in Section 6.
2 Task description
The BioNLP Shared Task 2009 on event extrac-
tion consists of recognising bio-molecular events in
biomedical texts, focusing on molecular events in-
volving proteins and genes. An event is defined as a
relation that holds between multiple entities that ful-
fil different roles. Events can participate in one type
59
of events: regulation events.
The task is divided into the three subtasks listed
below. We participated in subtasks 1 and 2.
? Task 1: event detection and characterization. This
task involves event trigger detection, event typing,
and event participant recognition.
? Task 2: event argument recognition. Recognition
of entities other than proteins and the assignment of
these entities as event arguments.
? Task 3: recognition of negations and speculations.
The task did not include a named entity recogni-
tion subtask. A gold standard set of named entity
annotations for proteins was provided by the organ-
isation. A dataset based on the publicly available
portion of the GENIA (Collier et al, 1999) corpus
annotated with events (Kim et al, 2008) and of the
BioInfer (Pyysalo et al, 2007) corpus was provided
for training, and held-out parts of the same corpora
were provided for development and testing.
The inter-annotator agreement reported for the
Genia Event corpus is 56% strict match2, which
means that the event type is the same, the clue ex-
pressions are overlapping and the themes are the
same. This low inter-annotator agreement is an in-
dicator of the complexity of the task. Similar low
inter-annotator agreement rates (49.00 %) in identi-
fication of events have been reported by Sasaki et al
(2008).
3 Related work
In recent years, research on text mining in the
biomedical domain has experienced substantial
progress, as shown in reviews of work done in this
field (Krallinger and Valencia, 2005; Ananiadou and
McNaught, 2006; Krallinger et al, 2008b). Some
corpora have been annotated with event level infor-
mation of different types: PropBank-style frames
(Wattarujeekrit et al, 2004; Chou et al, 2006),
frame independent roles (Kim et al, 2008), and
specific roles for certain event types (Sasaki et al,
2008). The focus on extraction of event frames us-
ing machine learning techniques is relatively new
because there were no corpora available.
2We did not find inter-annotator agreement measures in
the paper that describes the corpus (Kim et al, 2008), but in
www-tsujii.is.s.u-tokyo.ac.jp/T-FaNT/T-FaNT
.files/Slides/Kim.pdf.
Most work focuses on extracting biological rela-
tions from corpora, which consists of finding asso-
ciations between entities within a text phrase. For
example, Bundschus et al (2008) develop a Condi-
tional Random Fields (CRF) system to identify re-
lations between genes and diseases from a set of
GeneRIF (Gene Reference Into Function) phrases.
A shared task was organised in the framework of
the Language Learning in Logic Workshop 2005 de-
voted to the extraction of relations from biomedical
texts (Ne?dellec, 2005). Extracting protein-protein
interactions has also produced a lot of research, and
has been the focus of the BioCreative II competi-
tion (Krallinger et al, 2008a).
As for event extraction, Yakushiji et al (2001)
present work on event extraction based on full-
parsing and a large-scale, general-purpose grammar.
They implement an Argument Structure Extractor.
The parser is used to convert sentences that describe
the same event into an argument structure for this
event. The argument structure contains arguments
such as semantic subject and object. Information
extraction itself is performed using pattern matching
on the argument structure. The system extracts 23 %
of the argument structures uniquely, and 24% with
ambiguity. Sasaki et al (2008) present a supervised
machine learning system that extracts event frames
from a corpus in which the biological process E. coli
gene regulation was linguistically annotated by do-
main experts. The frames being extracted specify
all potential arguments of gene regulation events.
Arguments are assigned domain-independent roles
(Agent, Theme, Location) and domain-dependent
roles (Condition, Manner). Their system works in
three steps: (i) CRF-based named entity recogni-
tion to assign named entities to word sequences; (ii)
CRF-based semantic role labeling to assign seman-
tic roles to word sequences with named entity labels;
(iii) Comparison of word sequences with event pat-
terns derived from the corpus. The system achieves
50% recall and 20% precision.
We are not aware of work that has been carried
out on the data set of the BioNLP Shared Task 2009
before the task took place.
60
4 System description
We developed a supervised machine learning sys-
tem. The system operates in three phases. In the first
phase, event triggers and entities other than proteins
are detected. In the second phase, event participants
and arguments are identified. In the third phase,
postprocessing heuristics select the best frame for
each event. Parameterisation of the classifiers used
in Phases 1 and 2 was performed by experiment-
ing with sets of parameters on the development set.
We experimented with manually selected parame-
ters and with parameters selected by a genetic algo-
rithm, but the parameters found by the genetic algo-
rithm did not yield better results than the manually
selected parameters
As a first step, we preprocess the corpora with the
GDep dependency parser (Sagae and Tsujii, 2007)
so that we can use part-of-speech tags and syntac-
tic information as features for the machine learner.
GDep is a a dependency parser for biomedical text
trained on the Tsujii Lab?s GENIA treebank. The
dependency parser predicts for every word the part-
of-speech tag, the lemma, the syntactic head, and
the dependency relation. In addition to these regular
dependency tags it also provides information about
the IOB-style chunks and named entities. The clas-
sifiers use the output of GDep in addition to some
frequency measures as features.
We represent the data into a columns format, fol-
lowing the standard format of the CoNLL Shared
Task 2006 (Buchholz and Marsi, 2006), in which
sentences are separated by a blank line and fields
are separated by a single tab character. A sentence
consists of tokens, each one starting on a new line.
4.1 Phase 1: Entity Detection
In the first phase, a memory based classifier pre-
dicts for every word in the corpus whether it is an
entity or not and the type of entity. In this set-
ting, entity refers to what in the shared task def-
inition are events and entities other than proteins.
Classes are defined in the IOB-style3 in order to
find entities that span over multiple words. Figure
1 shows a simplified version of a sentence in which
high level is a Positive Regulation event that spans
over multiple tokens and proenkephalin is a Pro-
3I stands for ?inside?, B for ?beginning?, and O for ?outside?.
tein. The Protein class does not need to be predicted
by the classifier because this information is pro-
vided by the Task organisers. The classes predicted
are: O, {B,I}-Entity, {B,I}-Binding, {B,I}-Gene Ex-
pression, {B,I}-Localization, {B,I}-Negative Regula-
tion, {B,I}-Positive Regulation, {B,I}-Phosphorylation,
{B,I}-Protein Catabolism, {B,I}-Transcription.
Token Class Token Class
Upon O which O
activation O correlate O
, O with O
T O high B-Positive regulation
lymphocyte O level I-Positive regulation
accumulate O of O
high O proenkephalin B-Protein
level O mRNA O
of O in O
the O the O
neuropeptide O cell O
enkephalin O . O
Figure 1: Instance representation for the entity de-
tection classifier.
We use the IB1 memory?based classifier as im-
plemented in TiMBL (version 6.1.2) (Daelemans
et al, 2007), a supervised inductive algorithm for
learning classification tasks based on the k-nearest
neighbor classification rule (Cover and Hart, 1967).
The memory-based learning algorithm was param-
eterised in this case by using modified value differ-
ence as the similarity metric, gain ratio for feature
weighting, using 7 k-nearest neighbors, and weight-
ing the class vote of neighbors as a function of their
inverse linear distance. For training we did not use
the entire set of instances from the training data. We
downsampled the instances keeping 5 negative in-
stances (class label O) for every positive instance.
Instances to be kept were randomly selected. The
features used by this classifier are the following:
? About the token in focus: word, chunk tag, named
entity tag as provided by the dependency parser,
and, for every entity type, a number indicating how
many times the focus word triggered this type of en-
tity in the training corpus.
? About the context of the token in focus: lemmas
ranging from the lemma at position -4 until the
lemma at position +3 (relative to the focus word);
part-of-speech ranging from position -1 until posi-
tion +1; chunk ranging from position -1 until posi-
tion +1 relative to the focus word; the chunk be-
61
fore the chunk to which the focus word belongs;
a boolean indicating if a word is a protein or not
for the words ranging from position -2 until posi-
tion +3.
Class label Precision Recall F-score
B-Gene expression 59.32 60.23 59.77
B-Regulation 30.41 33.58 31.91
B-Entity 40.21 41.49 40.84
B-Positive regulation 41.16 46.25 43.56
B-Binding 57.76 53.14 55.36
B-Negative regulation 42.94 48.67 45.63
I-Negative regulation 7.69 3.33 4.65
I-Positive regulation 14.29 13.24 13.74
B-Phosphorylation 75.68 71.80 73.68
I-Regulation 14.29 10.00 11.77
B-Transcription 48.78 59.70 53.69
I-Entity 20.00 16.13 17.86
B-Localization 75.00 60.00 66.67
B-Protein catabolism 73.08 100.00 84.44
O 97.66 97.62 97.64
Table 1: Results of the entity detection classifier.
Entities that are not in the table have a precision and
recall of 0.
Table 1 shows the results4 of this first step. All
class labels with a precision and recall of 0 are left
out. The overall accuracy is 95.4%. This high ac-
curacy is caused by the skewness of the data in the
training corpus, which contains a higher proportion
of instances with class label O. Instances with this
class are correctly classified in the development test.
B-Protein catabolism and B-Phosphorylation get the
highest scores. The reason why these classes get
higher scores can be that the words that trigger these
events are less diverse.
4.2 Phase 2: predicting the arguments and
participants of events
In the second phase, another memory-based clas-
sifier predicts the participants and arguments of an
event. Participants have the main role in the event
and arguments are entities that further specify the
event. In (1), for the event phosphorylation the sys-
tem has to find that STAT1, STAT3, STAT4, STAT5a,
and STAT5b are participants with the role Theme and
that tyrosine is an argument with the role Site.
4In this section we provide results on development data be-
cause the gold test data have not been made available.
(1) IFN-alpha enhanced tyrosine phosphorylation
of STAT1, STAT3, STAT4, STAT5a, and
STAT5b.
We use the IB1 algorithm as implemented in
TiMBL (version 6.1.2) (Daelemans et al, 2007).
The classifier was parameterised by using gain ratio
for feature weighting, overlap as distance metrics,
11 nearest neighbors for extrapolation, and normal
majority voting for class voting weights.
For this classifier, instances represent combina-
tions of an event with all the entities in a sentence,
for as many events as there are in a sentence. Entities
include entities and events. We use as input the out-
put of the classifier in Phase 1, so only events and
entities classified as such in Phase 1, and the gold
proteins will be combined. Events can have partici-
pants and arguments in a sentence different that their
sentence. We calculated that in the training corpus
these cases account for 5.54% of the relations, and
decided to restrict the combinations at the sentence
level. For the sentence in (1) above, where tyrosine,
phosphorylation, STAT1, STAT3, STAT4, STAT5a,
and STAT5b are entities and of those only phospho-
rylation is an event, the instances would be produced
by combining phosphorylation with the seven enti-
ties.
The features used by this classifier are the follow-
ing:
? Of the event and of the combined entity: first word,
last word, type, named entity provided by GDep,
chain of lemmas, chain of part-of-speech (POS)
tags, chain of chunk tags, dependency label of the
first word, dependency label of the last word.
? Of the event context and of the combined entity con-
text: word, lemma, POS, chunk, and GDep named
entity of the five previous and next words.
? Of the context between event and combined entity:
the chain of chunks in between, number of tokens in
between, a binary feature indicating whether event
is located before or after entity.
? Others: four features indicating the parental rela-
tion between the first and last words of the event
and the first and last words of the entity. The values
for this feature are: event father, event ancestor, en-
tity father, entity ancestor, none. Five binary fea-
tures indicating if the event accepts certain roles
(Theme, Site, ToLoc, AtLoc, Cause).
62
Table 2 shows the results of this classifier per type
of participant (Cause, Site, Theme) and type of ar-
gument (AtLoc, ToLoc). Arguments are very infre-
quent, and the participants are skewed towards the
class Theme. Classes Site and Theme score high F1,
and in both cases recall is higher than precision. The
fact that the classifier overpredicts Sites and Themes
will have a negative influence in the final scores of
the full system. Further research will focus on im-
proving precision.
Part/Arg Total Precision Recall F1
Cause 61 28.88 21.31 24.52
Site 20 54.83 85.00 66.66
Theme 683 55.50 72.32 62.80
AtLoc 1 25.00 100.00 40.00
ToLoc 4 75.00 75.00 75.00
Table 2: Results of finding the event participants and
arguments.
Table 3 shows the results of finding the event par-
ticipants and arguments per event type, expressed in
terms of accuracy on the development corpus. Cause
is easier to predict for Positive Regulation events,
Site is the easiest class to predict, taking into ac-
count that AtLoc and ToLoc occur only 5 times in
total, and Theme can be predicted successfully for
Transcription and Gene Expression events, whereas
it gets lower scores for Regulation, Binding, and
Positive Regulation events.
Event Arguments/Participants
Type Cause Site Theme AtLoc ToLoc
Binding - 100.00 56.00 - -
Gene Expr. - - 89.95 - -
Localization - - 73.07 100.00 75.00
- Regulation 11.11 0.00 75.00 - -
Phosphorylation 0.00 100.00 70.83 - -
+ Regulation 27.77 90.90 56.77 - -
Protein Catab. - - 60.00 - -
Regulation 13.33 0.00 46.87 - -
Transcription - - 94.44 - -
Table 3: Results of finding the event participants and
arguments per event type (accuracy).
Table 4 shows the results of finding the event par-
ticipants that are Entity and Protein per type of event
for events that are not regulations. Entity scores high
in all cases, whereas Protein scores high for Tran-
scription and Gene Expression events and low for
Binding events.
Event Arg./Part. Type
Type Entity Protein
Binding 100.00 56.00
Gene Expr. - 89.90
Localization 80.00 73.07
Phosphorylation 100.00 68.00
Protein Catab. - 60.00
Transcription - 94.44
Table 4: Results of finding the event participants and
arguments that are Entity and Protein per event type
(accuracy).
Table 5 shows the results of finding the partic-
ipants and arguments of regulation events. In the
case of regulation events, Entity is easier to classify
with Positive Regulation events, and Protein with
Negative Regulation events. In the cases in which
events are participants of regulation events, Bind-
ing, Gene Expression and Phosphorylation are easier
to classify with Positive Regulation events, Local-
ization with Regulation events, Protein Catabolism
with Negative Regulation events, and Transcription
is easy to classify in all cases.
Arg./Part. Event Type
Type Regulation + Regulation -Regulation
Entity 0.00 90.90 0.00
Protein 17.85 38.88 45.45
Binding - 75.00 66.66
Gene Expr. 66.66 90.47 75.00
Localization 100.00 80.00 75.00
Phosphorylation 0.00 44.44 0.00
Protein Catab. 0.00 40.00 100.00
Transcription 100.00 92.85 100.00
Table 5: Results of finding event arguments and par-
ticipants for regulation events (accuracy).
From the results of the system in this phase we can
extract some conclusions: data are skewed towards
the Theme class; Themes are not equally predictable
for the different types of events, they are better
predictable for Gene Expression and Transcription;
Proteins are more difficult to classify when they are
Themes of regulation events; and Transcription and
Localization events are easier to predict as Themes
of regulation events, compared to the other types of
events that are Themes of regulation events. This
63
suggests that it could be worth experimenting with
a classifier per entity type and with a classifier per
role, instead of using the same classifier for all types
of entities.
4.3 Phase 3: heuristics to select the best frame
per event
Phases 1 and 2 aimed at identifying events and can-
didates to event participants. However, the purpose
of the task is to extract full frames of events. For a
sentence like the one in (1) above, the system has to
extract the event frames in (2).
(2) 1. Phosphorylation (phosphorylation): Theme
(STAT1) Site (tyrosine)
2. Phosphorylation (phosphorylation): Theme
(STAT3) Site (tyrosine)
3. Phosphorylation (phosphorylation): Theme
(STAT5a) Site (tyrosine)
4. Phosphorylation (phosphorylation): Theme
(STAT4) Site (tyrosine)
5. Phosphorylation (phosphorylation): Theme
(STAT5b) Site (tyrosine)
It is necessary to apply heuristics in order to build
the event frames from the output of the second clas-
sifier, which for the sentence in (1) above should
contain the predictions in (3).
(3) 1. phosphorylation STAT1 : Theme
2. phosphorylation STAT3 : Theme
3. phosphorylation STAT5a : Theme
4. phosphorylation STAT4 : Theme
5. phosphorylation STAT5b : Theme
6. phosphorylation tyrosine : Site
Thus, in the third phase, postprocessing heuristics
determine which is the frame of each event.
4.3.1 Specific heuristics for each type of event
The system contains different rules for each of the
5 types of participants (Cause, Site, Theme, AtLoc,
ToLoc). The text entities are the entities defined dur-
ing Phase 2. An event is created for every text entity
for which the system predicted at least one partic-
ipant or argument. To illustrate this we can take a
look at the predictions for the Gene Expression event
in (4) where the identifiers starting by T refer to en-
tities in the text. The prediction would results in the
events listed in (5).
(4) Gene expression=
Theme:T11=Theme:T12=Theme:T13
(5) E1 Gene expression:T23 Theme:T11
E2 Gene expression:T23 Theme:T12
E3 Gene expression:T23 Theme:T13
Gene expression, Transcription, and Protein
catabolism. These type of events have only a
Theme. Therefore, an event frame is created for ev-
ery Theme predicted for events that belong to these
types.
Localization. A Localization event can have one
Theme and 2 arguments: AtLoc and ToLoc. A
Localization event with more than one predicted
Theme will result in as many frames as predicted
Themes. The arguments are passed on to every
frame.
Binding. A Binding event can have multiple
Themes and multiple Site arguments. If the system
predicts more than one Theme for a Binding event,
the heuristics first check if these Themes are in a co-
ordination structure. Coordination checking consists
of checking whether the word ?and? can be found
between the Themes. Coordinated Themes will give
rise to separate frames. Every participant and loose
Theme is added to all created event lines. This case
applies to the sentence in (6)
(6) When we analyzed the nature of STAT
proteins capable of binding to IL-2Ralpha,
pim-1, and IRF-1 GAS elements after cytokine
stimulation, we observed IFN-alpha-induced
binding of STAT1, STAT3, and STAT4, but not
STAT5 to all of these elements.
The frames that should be created for this sen-
tence listed in (7).
(7) 1. Binding (binding): Theme(STAT4)
Theme2(IRF-1) Site2(GAS elements)
2. Binding (binding): Theme(STAT3)
Theme2:(IL-2Ralpha) Site2(GAS elements)
3. Binding (binding): Theme(STAT3)
Theme2(IRF-1) Site2(GAS elements)
4. Binding (binding): Theme(STAT4)
Theme2(pim-1) Site2(GAS elements)
5. Binding (binding): Theme(STAT1)
Theme2(IL-2Ralpha) Site2(GAS elements)
64
6. Binding (binding): Theme(STAT4)
Theme2(IL-2Ralpha) Site2(GAS elements)
7. Binding (binding): Theme(IL-2Ralpha)
Site(GAS elements)
8. Binding (binding): Theme(pim-1) Site(GAS
elements)
9. Binding (binding): Theme(STAT1)
Theme2(IRF-1) Site2(GAS elements)
10. Binding (binding): Theme(STAT3)
Theme2(pim-1) Site2(GAS elements)
11. Binding (binding): Theme(IRF-1) Site(GAS
elements)
12. Binding (binding): Theme(STAT1)
Theme2(pim-1) Site2(GAS elements)
Phosphorylation. A Phosphorylation event can
have one Theme and one Site. Multiple Themes for
the same event will result in multiple frames. The
Site argument will be added to every frame.
Regulation, Positive regulation, and Negative
regulation. A Regulation event can have a Theme,
a Cause, a Site, and a CSite. For Regulation events
the system uses a different approach when creating
new frames. It first checks which of the participants
and arguments occurs the most frequent in a predic-
tion and it creates as many separate frames as are
needed to give every participant/argument its own
frame. The remaining participants/arguments are
added to the nearest frame. For this type of event
a new frame can be created not only for multiple
Themes but also for e.g. multiple Sites. The purpose
of this strategy is to increase the recall of Regulation
events.
4.3.2 Postprocessing
After translating predictions into frames some
corrections are made.
1. Every Theme and Cause that is not a Protein is
thrown away.
2. Every frame that has no Theme is provided
with a default Theme. If no Protein is found
before the focus word, the closest Protein after
the word is taken as the default Theme.
3. Duplicates are removed.
5 Results
The official results of our system for Task 1 are pre-
sented in Table 6. The best F1 score are for Gene Ex-
pression and Protein Catabolism events. The lowest
results are for all the types of regulation events and
for Binding events. Binding events are more diffi-
cult to predict correctly because they can have more
than one Theme.
Total Precision Recall F1
Binding 347 12.97 31.03 18.29
Gene Expr. 722 51.39 68.96 58.89
Localization 174 20.69 78.26 32.73
Phosphorylation 135 28.15 67.86 39.79
Protein Catab. 14 64.29 42.86 51.43
Transcription 137 24.82 41.46 31.05
Regulation 291 8.93 23.64 12.97
+Regulation 983 11.70 31.68 17.09
-Regulation 379 11.08 29.85 16.15
TOTAL 3182 22.50 47.70 30.58
Table 6: Official results of Task 1. Approximate
Span Matching/Approximate Recursive Matching.
The official results of our system for Task 2 are
presented in Table 7. Results are similar to the re-
sults of Task 1 because there are not many more ar-
guments than participants. Recognising arguments
was the additional goal of Task 2 in relation to
Task 1.
Total Precision Recall F1
Binding 349 11.75 28.28 16.60
Gene Expr. 722 51.39 68.96 58.89
Localization 174 17.82 67.39 28.18
Phosphorylation 139 15.83 39.29 22.56
Protein Catab. 14 64.29 42.86 51.43
Transcription 137 24.82 41.46 31.05
Regulation 292 8.56 22.73 12.44
+Regulation 987 11.35 30.85 16.59
-Regulation 379 11.08 29.20 15.76
TOTAL 3193 21.52 45.77 29.27
Table 7: Official results of Task 2. Approximate
Span Matching/Approximate Recursive Matching.
Results obtained on the development set are a lit-
tle bit higher. For Task1 an overall F1 of 34.78 and
for Task 2 33.54.
For most event types precision and recall are un-
balanced, the system scores higher in recall. Fur-
ther research should focus on increasing precision
because the system is predicting false positives. It
would be possible to add a step in order to fil-
ter out the false positives by comparing word se-
quences with event patterns derived from the cor-
pus, which is an approach taken in the system by
Sasaki et al (2008) .
65
In the case of Binding events, both precision and
recall are low. There are two explanations for this.
In the first place, the first classifier misses almost
half of the binding events. As an example, for
the sentence in (8.1), the gold standard identifies as
binding event the multiwords binds as a homodimer
and form heterodimers, whereas the system identi-
fies two binding events for the same sentence, binds
and homodimer, none of which is correct because
the correct one is the multiword unit. For the sen-
tence in (8.2), the gold standard identifies as binding
events bind, form homo-, and heterodimers, whereas
the system identifies only binds.
(8) 1. The KBF1/p50 factor binds as a homodimer but can
also form heterodimers with the products of other
members of the same family, like the c-rel and v-rel
(proto)oncogenes.
2. A mutant of KBF1/p50 (delta SP), unable to bind to
DNA but able to form homo- or heterodimers, has been
constructed.
From the sentence in (8.1) above the eight frames
in (9) should be extracted, whereas the system ex-
tracts only the frames in (10), which are incorrect
because the events have not been correctly identi-
fied.
(9) 1. Binding(binds as a homodimer) : Theme(KBF1)
2. Binding(binds as a homodimer) : Theme(p50)
3. Binding(form heterodimers) : Theme(KBF1)
Theme2(c-rel)
4. Binding(form heterodimers) : Theme(p50)
Theme2(v-rel)
5. Binding(form heterodimers) : Theme(p50)
Theme2(c-rel)
6. Binding(form heterodimers) : Theme(KBF1)
Theme2(v-rel)
7. Binding(bind) : Theme(p50)
8. Binding(bind) : Theme(KBF1)
(10) 1. Binding(binds) : Theme(v-rel)
2. Binding(homodimer) : Theme(c-rel)
The complexity of frame extraction of Binding
events contrasts with the less complex extraction of
frames for Gene Expression events, like the one in
sentence (11), where expression has been identified
correctly by the system as an event and the frame in
(12) has been correctly extracted.
(11) Thus, c-Fos/c-Jun heterodimers might contribute to the
repression of DRA gene expression.
(12) Gene Expression(expression) : Theme(DRA)
6 Conclusions
In this paper we presented a supervised machine
learning system that extracts event frames from
biomedical texts in three phases. The system partic-
ipated in the BioNLP Shared Task 2009, achieving
an F-score of 30.58 in Task 1, and 29.27 in Task 2.
The frame extraction task was modeled applying the
same approach that has been applied to tasks like se-
mantic role labeling or negation scope detection, in
order to check whether such an approach would be
suitable for a frame extraction task. The results ob-
tained for the present task do not compare to results
obtained in the mentioned tasks, where state of the
art F-scores are above 80.
Extracting biomedical event frames is more com-
plex than labeling semantic roles because of several
reasons. Semantic roles are mostly assigned to syn-
tactic constituents, predicates have only one frame
and all the arguments belong to the same frame. In
contrast, in the biomedical domain one event can
have several frames, each frame having different
participants, the boundaries of which do not coin-
cide with syntactic constituents.
The system presented here can be improved in
several directions. Future research will concentrate
on increasing precision in general, and precision and
recall of binding events in particular. Analysing in
depth the errors made by the system at each phase
will allow us to find the weaker aspects of the sys-
tem. From the results of the system in the second
phase we could draw some conclusions: data are
skewed towards the Theme class; Themes are not
equally predictable for the different types of events;
Proteins are more difficult to classify when they are
Themes of regulation events; and Transcription and
Localization events are easier to predict as Themes
of regulation events, compared to the other types of
events that are Themes of regulation events. We plan
to experiment with a classifier per entity type and
with a classifier per role, instead of using the same
classifier for all types of entities. Additionally, the
effects of the postprocessing rules in Phase 3 will be
evaluated.
66
Acknowledgments
Our work was made possible through financial sup-
port from the University of Antwerp (GOA project
BIOGRAPH). We are grateful to two anonymous re-
viewers for their valuable comments.
References
S. Ananiadou and J. McNaught. 2006. Text Mining for
Biology and Biomedicine. Artech House Books, Lon-
don.
S. Buchholz and E. Marsi. 2006. CoNLL-X shared task
on multilingual dependency parsing. In Proc. of the X
CoNLL Shared Task, New York. SIGNLL.
M. Bundschus, M. Dejori, M. Stetter, V. Tresp, and H-
P Kriegel. 2008. Extraction of semantic biomedi-
cal relations from text using conditional random fields.
BMC Bioinformatics, 9.
W.C. Chou, R.T.H. Tsai, Y-S. Su, W. Ku, T-Y Sung, and
W-L Hsu. 2006. A semi-automatic method for an-
notating a biomedical proposition bank. In Proc. of
ACL Workshop on Frontiers in Linguistically Anno-
tated Corpora 2006, pages 5?12.
N. Collier, H.S. Park, N. Ogata, Y. Tateisi, C. Nobata,
T. Sekimizu, H. Imai, and J. Tsujii. 1999. The GE-
NIA project: corpus-based knowledge acquisition and
information extraction from genome research papers.
In Proc. of EACL 1999.
T. M. Cover and P. E. Hart. 1967. Nearest neighbor
pattern classification. Institute of Electrical and Elec-
tronics Engineers Transactions on Information The-
ory, 13:21?27.
W. Daelemans and A. van den Bosch. 2005. Memory-
based language processing. Cambridge University
Press, Cambridge, UK.
W. Daelemans, A. Van den Bosch, and J. Zavrel. 1999.
Forgetting exceptions is harmful in language learning.
Machine Learning, Special issue on Natural Language
Learning, 34:11?41.
W. Daelemans, J. Zavrel, K. Van der Sloot, and A. Van
den Bosch. 2007. TiMBL: Tilburg memory based
learner, version 6.1, reference guide. Technical Report
Series 07-07, ILK, Tilburg, The Netherlands.
J.D. Kim, T. Ohta, and J. Tsujii. 2008. Corpus annotation
for mining biomedical events from literature. BMC
Bioinformatics, 9:10.
M. Krallinger and A. Valencia. 2005. Text-mining and
information-retrieval services for molecular biology.
Genome Biology, 6:224.
M. Krallinger, F. Leitner, C. Rodriguez-Penagos, and
A. Valencia. 2008a. Overview of the protein?protein
interaction annotation extraction task of BioCreative
II. Genome Biology, 9(Suppl 2):S4.
M. Krallinger, A. Valencia, and L. Hirschman. 2008b.
Linking genes to literature: text mining, informa-
tion extraction, and retrieval applications for biology.
Genome Biology, 9(Suppl 2):S8.
R. Morante and W. Daelemans. 2009. A metalearning
approach to processing the scope of negation. In Pro-
ceedings of CoNLL 2009, Boulder, Colorado.
R. Morante, W. Daelemans, and V. Van Asch. 2008. A
combined memory-based semantic role labeler of En-
glish. In Proc. of the CoNLL 2008, pages 208?212,
Manchester, UK.
C. Ne?dellec. 2005. Learning language in logic ? genic
interaction extraction challenge. In Proc. of Learn-
ing Language in Logic Workshop 2005, pages 31?37,
Bonn.
S. Pyysalo, F. Ginter, J. Heimonen, J. Bjo?rne, J. Boberg,
J. Ja?rvinen, and T. Salakoski. 2007. BioInfer: a corpus
for information extraction in the biomedical domain.
BMC Bioinformatics, 8(50).
K. Sagae and J. Tsujii. 2007. Dependency parsing and
domain adaptation with lr models and parser ensem-
bles. In Proc. of CoNLL 2007 Shared Task, EMNLP-
CoNLL, pages 82?94, Prague. ACL.
Y. Sasaki, P. Thompson, P. Cotter, J. McNaught, and
S. Ananiadou. 2008. Event frame extraction based
on a gene regulation corpus. In Proc. of Coling 2008,
pages 761?768.
T. Wattarujeekrit, P.K. Shah, and N. Collier. 2004.
PASBio: predicate-argument structures for event ex-
traction in molecular biology. BMC Bioinformatics,
5:155.
A. Yakushiji, Y. Tateisi, Y. Miyao, and J. Tsujii. 2001.
Event extraction from biomedical papers using a full
parser. In Pac Symp Biocomput.
67
Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 106?111,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
SemEval-2010 Task 10:
Linking Events and Their Participants in Discourse
Josef Ruppenhofer and Caroline Sporleder
Computational Linguistics
Saarland University
{josefr,csporled}@coli.uni-sb.de
Roser Morante
CNTS
University of Antwerp
Roser.Morante@ua.ac.be
Collin Baker
ICSI
Berkeley, CA 94704
collin@icsi.berkeley.edu
Martha Palmer
Department of Linguistics
University of Colorado at Boulder
martha.palmer@colorado.edu
Abstract
In this paper, we describe the SemEval-2010
shared task on ?Linking Events and Their Par-
ticipants in Discourse?. This task is a variant
of the classical semantic role labelling task.
The novel aspect is that we focus on linking
local semantic argument structures across sen-
tence boundaries. Specifically, the task aims at
linking locally uninstantiated roles to their co-
referents in the wider discourse context (if such
co-referents exist). This task is potentially ben-
eficial for a number of NLP applications and
we hope that it will not only attract researchers
from the semantic role labelling community
but also from co-reference resolution and infor-
mation extraction.
1 Introduction
Semantic role labelling (SRL) has been defined as
a sentence-level natural-language processing task in
which semantic roles are assigned to the syntactic
arguments of a predicate (Gildea and Jurafsky, 2002).
Semantic roles describe the function of the partici-
pants in an event. Identifying the semantic roles of
the predicates in a text allows knowing who did what
to whom when where how, etc.
SRL has attracted much attention in recent years,
as witnessed by several shared tasks in Sense-
val/SemEval (Ma`rquez et al, 2007; Litkowski, 2004;
Baker et al, 2007; Diab et al, 2007), and CoNLL
(Carreras and Ma`rquez, 2004; Carreras and Ma`rquez,
2005; Surdeanu et al, 2008). The state-of-the-art
in semantic role labelling has now advanced so
much that a number of studies have shown that au-
tomatically inferred semantic argument structures
can lead to tangible performance gains in NLP ap-
plications such as information extraction (Surdeanu
et al, 2003), question answering (Shen and Lapata,
2007) or recognising textual entailment (Burchardt
and Frank, 2006).
However, semantic role labelling as it is currently
defined also misses a lot of information that would
be beneficial for NLP applications that deal with
text understanding (in the broadest sense), such as
information extraction, summarisation, or question
answering. The reason for this is that SRL has tra-
ditionally been viewed as a sentence-internal task.
Hence, relations between different local semantic ar-
gument structures are disregarded and this leads to a
loss of important semantic information.
This view of SRL as a sentence-internal task is
partly due to the fact that large-scale manual anno-
tation projects such as FrameNet1 and PropBank2
typically present their annotations lexicographically
by lemma rather than by source text. Furthermore,
in the case of FrameNet, the annotation effort did
not start out with the goal of exhaustive corpus an-
notation but instead focused on isolated instances of
the target words sampled from a very large corpus,
which did not allow for a view of the data as ?full-text
annotation?.
It is clear that there is an interplay between local
argument structure and the surrounding discourse
(Fillmore, 1977). In early work, Palmer et al (1986)
discussed filling null complements from context by
using knowledge about individual predicates and ten-
1http://framenet.icsi.berkeley.edu/
2http://verbs.colorado.edu/?mpalmer/
projects/ace.html
106
dencies of referential chaining across sentences. But
so far there have been few attempts to find links
between argument structures across clause and sen-
tence boundaries explicitly on the basis of semantic
relations between the predicates involved. Two no-
table exceptions are Fillmore and Baker (2001) and
Burchardt et al (2005). Fillmore and Baker (2001)
analyse a short newspaper article and discuss how
frame semantics could benefit discourse processing
but without making concrete suggestions of how to
model this. Burchardt et al (2005) provide a detailed
analysis of the links between the local semantic argu-
ment structures in a short text; however their system
is not fully implemented either.
In the shared task, we intend to make a first step
towards taking SRL beyond the domain of individual
sentences by linking local semantic argument struc-
tures to the wider discourse context. In particular, we
address the problem of finding fillers for roles which
are neither instantiated as direct dependents of our
target predicates nor displaced through long-distance
dependency or coinstantatiation constructions. Of-
ten a referent for an uninstantiated role can be found
in the wider context, i.e. in preceding or following
sentences. An example is given in (1), where the
CHARGES role (ARG2 in PropBank) of cleared is left
empty but can be linked to murder in the previous
sentence.
(1) In a lengthy court case the defendant was
tried for murder. In the end, he was cleared.
Another very rich example is provided by (2),
where, for instance, the experiencer and the object of
jealousy are not overtly expressed as syntactic depen-
dents of the noun jealousy but can be inferred to be
Watson and the speaker, Holmes, respectively.
(2) Watson won?t allow that I know anything
of art but that is mere jealousy because our
views upon the subject differ.
NIs are also very frequent in clinical reports.
For example, in (3) the EXPERIENCER role of
?cough?, ?tachypnea?, and ?breathing? can be linked
to ?twenty-two month old?. Text mining systems in
the biomedical domain focus on extracting relations
between biomedical entities and information about
patients. It is important that these systems extract
information as accurately as possible. Thus, finding
co-referents for NIs is also very relevant for improv-
ing results on mining relations in biomedical texts.
(3) Twenty-two month old with history of recur-
rent right middle lobe infiltrate. Increased
cough, tachypnea, and work of breathing.
In the following sections we describe the task in
more detail. We start by providing some background
on null instantiations (Section 2). Section 3 gives an
overview of the task, followed by a description of
how we intend to create the data (Section 4). Sec-
tion 5 provides a short description of how null in-
stantiations could be resolved automatically given
the provided data. Finally, Section 6 discusses the
evaluation measures and we wrap up in Section 7.
2 Background on Null Instantiation
The theory of null complementation used here is the
one adopted by FrameNet, which derives from the
work of Fillmore (1986).3 Briefly, omissions of core
arguments of predicates are categorised along two
dimensions, the licensor and the interpretation they
receive. The idea of a licensor refers to the fact that
either a particular lexical item or a particular gram-
matical construction must be present for the omission
of a frame element (FE) to occur. For instance, the
omission of the agent in (4) is licensed by the passive
construction.
(4) No doubt, mistakes were made 0Protagonist.
The omission is a constructional omission because
it can apply to any predicate with an appropriate
semantics that allows it to combine with the passive
construction. On the other hand, the omission in (5)
is lexically specific: the verb arrive allows the Goal
to be unspecified but the verb reach, also a member
of the Arriving frame, does not.
(5) We arrived 0Goal at 8pm.
The above two examples also illustrate the second
major dimension of variation. Whereas, in (4) the
protagonist making the mistake is only existentially
bound within the discourse (instance of indefinite null
3Palmer et al?s (1986) treatment of uninstantiated ?essential
roles? is very similar (see also Palmer (1990)).
107
instantiation, INI), the Goal location in (5) is an entity
that must be accessible to speaker and hearer from
the discourse or its context (definite null instantiation,
DNI). Finally note that the licensing construction or
lexical item fully and reliably determines the interpre-
tation. Missing by-phrases always have an indefinite
interpretation and whenever arrive omits the Goal
lexically, the Goal has to be interpreted as definite,
as it is in (5).
The import of this classification to the task here
is that we will concentrate on cases of DNI whether
they are licensed lexically or constructionally.
3 Task Description
We plan to run the task in the following two modes:
Full Task For the full task we supply a test set in
which the target words are marked and labelled with
the correct sense (i.e. frame).4 The participants then
have to:
1. find the overt semantic arguments of the target
(role recognition)
2. label them with the correct role (role labelling)
3. recognize definite null instantiations and find
links to antecedents in the wider context (NI
linking)
NIs only In the second mode, participants will be
supplied with a test set which is annotated with gold
standard local semantic argument structure.5 The
task is then restricted to recognizing that a core role
is missing, ascertaining that it must have a definite
interpretation and finding a filler for it (i.e., sub-task
3 from the full task).
The full task and the null instantiation linking task
will be evaluated separately. By setting up a SRL
task, we expect to attract participants from the es-
tablished SRL community. Furthermore, by allow-
ing participants to only address the second task, we
4We supply the correct sense to ensure that all systems use
the same role inventory for each target (i.e., the role inventory
associated with the gold standard sense). This makes it easier
to evaluate the systems consistently with respect to role assign-
ments and null instantiation linking, which is our main focus.
5The training set is identical for both set-ups and will contain
the full annotation, i.e., frames, semantic roles and their fillers,
and referents of null instantiations in the wider context (see
Section 4 for details).
hope to also attract researchers from areas such as co-
reference resolution or information extraction who do
not want to implement a complete SRL system. We
also plan to provide the data with both FrameNet and
PropBank style annotations to encourage researchers
from both areas to take part.
4 Data
The data will come from one of Arthur Conan
Doyle?s fiction works. We chose fiction rather than
news because we believe that fiction texts with
a linear narrative generally contain more context-
resolvable null instantiations. They also tend to be
longer and have a simpler structure than news texts
which typically revisit the same facts repeatedly at
different levels of detail (in the so-called ?inverted
pyramid? structure) and which mix event reports with
commentary and evaluation, thus sequencing mate-
rial that is understood as running in parallel. Fiction
texts should lend themselves more readily to a first at-
tempt at integrating discourse structure into semantic
role labeling. We chose Conan Doyle?s work because
most of his books are not subject to copyright restric-
tions anymore, which allows us to freely release the
annotated data.
We plan to make the data sets available with both
FrameNet and PropBank semantic argument anno-
tation, so that participants can choose which frame-
work they want to work in. The annotations will
originally be made using FrameNet-style and will
later be mapped semi-automatically to PropBank an-
notations. The data set for the FrameNet version of
the task will be built at Saarland University, in close
co-operation with the FrameNet team in Berkeley.
We aim for the same density of annotation as is ex-
hibited by FrameNet?s existing full-text annotation6
and are currently investigating whether the semantic
argument annotation can be done semi-automatically,
e.g., by starting the annotation with a run of the Shal-
maneser role labeller (Erk and Pado?, 2006), whose
output is then corrected and expanded manually. To
ensure a high annotation quality, at least part of the
data will be annotated by two annotators and then
manually adjudicated. We also provide detailed an-
notation guidelines (largely following the FrameNet
6http://framenet.icsi.berkeley.edu/
index.php?option=com_wrapper&Itemid=84
108
guidelines) and any open questions are discussed in
a weekly annotation meeting.
For the annotation of null instantiations and their
links to the surrounding discourse we have to create
new guidelines as this is a novel annotation task. We
will adopt ideas from the annotation of co-reference
information, linking locally unrealised roles to all
mentions of the referents in the surrounding dis-
course, where available. We will mark only identity
relations but not part-whole or bridging relations be-
tween referents. The set of unrealised roles under
consideration includes only the core arguments but
not adjuncts (peripheral or extra-thematic roles in
FrameNet?s terminology). Possible antecedents are
not restricted to noun phrases but include all con-
stituents that can be (local) role fillers for some pred-
icate plus complete sentences (which can sometimes
fill roles such as MESSAGE).
The data-set for PropBank will be created by map-
ping the FrameNet annotations onto PropBank and
NomBank labels. For verbal targets, we use the Sem-
link7 mappings. For nominal targets, there is no
existing hand-checked mapping between FrameNet
and NomBank but we will explore a way of build-
ing a FrameNet - NomBank mapping at least for
eventive nouns indirectly with the help of Semlink.
This would take advantage of the fact that PropBank
verbs and eventive NomBank nouns both have a map-
ping to VerbNet classes, which are referenced also by
Semlink. Time permitting, non-eventive nouns could
be mapped manually. For FrameNet targets of other
parts of speech, in particular adjectives and prepo-
sitions, no equivalent PropBank-style counterparts
will be available. The result of the automatic map-
pings will be partly hand-checked. The annotations
resolving null instantiations need no adjustment.
We intend to annotate at least two data sets of
around 4,000 words. One set for testing and one for
training. Because we realise that the training set will
not be large enough to train a semantic role labelling
system on it, we permit the participants to boost the
training data for the SRL task by making use of the
existing FrameNet and PropBank corpora.8
7http://verbs.colorado.edu/semlink/
8This may require some genre adaption but we believe this is
feasible.
5 Resolving Null Instantiations
We conceive of null instantiation resolution as a three
step problem. First, one needs to determine whether a
core role is missing. This involves looking up which
core roles are overtly expressed and which are not.
In the second step, one needs to determine what
licenses an omission and what its interpretation is.
To do this, one can use rules and heuristics based on
various syntactic and lexical facts of English. As an
example of a relevant syntactic fact, consider that sub-
jects in English can only be omitted when licensed by
a construction. One such construction is the impera-
tive (e.g. Please, sit down). Since this construction
also specifies that the missing referent must be the
addressee of the speaker of the imperative, it is clear
what referent one has to try to find.
As for using lexical knowledge, consider omis-
sions of the Goods FE of the verb steal in the Theft
frame. FrameNet annotation shows that whenever
the Goods FE of steal is missing it is interpreted in-
definitely, suggesting that a new instance of the FE
being missing should have the same interpretation.
More evidence to the same effect can be derived us-
ing Ruppenhofer?s (2004) observation that the inter-
pretation of a lexically licensed omission is definite
if the overt instances of the FE have mostly definite
form (i.e. have definite determiners such as that, the ,
this), and indefinite if they are mostly indefinite (i.e.
have bare or indefinite determiners such as a(n) or
some). The morphology of overt instances of an FE
could be inspected in the FrameNet data, or if the
predicate has only one sense or a very dominant one,
then the frequencies could even be estimated from
unannotated corpora.
The third step is linking definite omissions to ref-
erents in the context. This linking problem could be
modelled as a co-reference resolution task. While
the work of Palmer et al (1986) relied on special
lexicons, one might instead want to learn information
about the semantic content of different role fillers
and then assess for each of the potential referents in
the discourse context whether their semantic content
is close enough to the expected content of the null
instantiated role.
Information about the likely fillers of a role can
be obtained from annotated data sets (e.g., FrameNet
or PropBank). For instance, typical fillers of the
109
CHARGES role of clear might be murder, accusa-
tions, allegations, fraud etc. The semantic content of
the role could then be represented in a vector space
model, using additional unannotated data to build
meaning vectors for the attested role fillers. Meaning
vectors for potential role fillers in the context of the
null instantiation could be built in a similar fashion.
The likelihood of a potential filler filling the target
role can then be modelled as the distance between the
meaning vector of the filler and the role in the vec-
tor space model (see Pado? et al (2008) for a similar
approach for semi-automatic SRL).
We envisage that the manually annotated null in-
stantiated data can be used to learn additionally
heuristics for the filler resolution task, such as in-
formation about the average distance between a null
instantiation and its most recent co-referent.
6 Evaluation
As mentioned above we allow participants to address
either the full role recognition and labelling task plus
the linking of null instantiations or to make use of
the gold standard semantic argument structure and
look only at the null instantiations. We also permit
systems to perform either FrameNet or PropBank
style SRL. Hence, systems can be entered for four
subtasks which will be evaluated separately:
? full task, FrameNet
? null instantiations, FrameNet
? full task, PropBank
? null instantiations, PropBank
The focus for the proposed task is on the null in-
stantiation linking, however, for completeness, we
also evaluate the standard SRL task. For role recogni-
tion and labelling we use a standard evaluation set-up,
i.e., for role recognition we will evaluate the accuracy
with respect to the manually created gold standard,
for role labelling we will evaluate precision, recall,
and F-Score.
The null instantiation linkings are evaluated
slightly differently. In the gold standard, we will iden-
tify referents for null instantiations in the discourse
context. In some cases, more than one referent might
be appropriate, e.g., because the omitted argument
refers to an entity that is mentioned multiple times
in the context. In this case, a system should be given
credit if the null instantiation is linked to any of these
expressions. To achieve this we create equivalence
sets for the referents of null instantiations. If the null
instantiation is linked to any item in the equivalence
set, the link is counted as a true positive. We can then
define NI linking precision as the number of all true
positive links divided by the number of links made by
a system, and NI linking recall as the number of true
positive links divided by the number of links between
a null instantiation and its equivalence set in the gold
standard. NI linking F-Score is then the harmonic
mean between NI linking precision and recall.
Since it may sometimes be difficult to determine
the correct extend of the filler of an NI, we score
an automatic annotation as correct if it includes the
head of the gold standard filler in the predicted filler.
However, in order to not favour systems which link
NIs to excessively large spans of text to maximise the
likelihood of linking to a correct referent, we intro-
duce a second evaluation measure, which computes
the overlap (Dice coefficient) between the words in
the predicted filler (P) of a null instantiation and the
words in the gold standard one (G):
NI linking overlap = 2|P ?G||P |+ |G| (6)
Example (7) illustrates this point. The verb won in
the second sentence evokes the Finish competition
frame whose COMPETITION role is null instantiated.
From the context it is clear that the competition role
is semantically filled by their first TV debate (head:
debate) and last night?s debate (head: debate) in
the previous sentences. These two expressions make
up the equivalence set for the COMPETITION role in
the last sentence. Any system that would predict a
linkage to a filler that covers the head of either of
these two expressions would score a true positive for
this NI. However, a system that linked to last night?s
debate would have an NI linking overlap of 1 (i.e.,
2*3/(3+3)) while a system linking the whole second
sentence Last night?s debate was eagerly anticipated
to the NI would have an NI linking overlap of 0.67
(i.e., 2*3/(6+3))
(7) US presidential rivals Republican John
McCain and Democrat Barack Obama have
yesterday evening attacked each other over
110
foreign policy and the economy, in [their
first TV debate]Competition. [Last night?s
debate]Competition was eagerly anticipated.
Two national flash polls suggest that
[Obama]Competitor wonFinish competition
0Competition.
7 Conclusion
In this paper, we described the SemEval-2010 shared
task on ?Linking Events and Their Participants in
Discourse?. With this task, we intend to take a first
step towards viewing semantic role labelling not as a
sentence internal problem but as a task which should
really take the discourse context into account. Specif-
ically, we focus on finding referents for roles which
are null instantiated in the local context. This is po-
tentially useful for various NLP applications. We
believe that the task is timely and interesting for a
number of researchers not only from the semantic
role labelling community but also from fields such as
co-reference resolution or information extraction.
While our task focuses specifically on finding links
between null instantiated roles and the discourse con-
text, we hope that in setting it up, we can stimulate re-
search on the interaction between discourse structure
and semantic argument structure in general. Possible
future editions of the task could then focus on addi-
tional connections between local semantic argument
structures (e.g., linking argument structures that refer
to the same event).
8 Acknowledgements
Josef Ruppenhofer and Caroline Sporleder are supported
by the German Research Foundation DFG (under grant
PI 154/9-3 and the Cluster of Excellence Multimodal
Computing and Interaction (MMCI), respectively). Roser
Morante?s research is funded by the GOA project BIO-
GRAPH of the University of Antwerp.
References
C. Baker, M. Ellsworth, and K. Erk. 2007. SemEval-
2007 Task 19: Frame semantic structure extraction. In
Proceedings of SemEval-07.
A. Burchardt and A. Frank. 2006. Approximating textual
entailment with LFG and framenet frames. In Pro-
ceedings of the Second Recognising Textual Entailment
Workshop.
A. Burchardt, A. Frank, and M. Pinkal. 2005. Building
text meaning representations from contextually related
frames ? A case study. In Proceedings of IWCS-6.
X. Carreras and Ll. Ma`rquez. 2004. Introduction to the
CoNLL-2004 shared task: Semantic role labeling. In
Proceedings of CoNLL-04, pages 89?97.
X. Carreras and Ll. Ma`rquez. 2005. Introduction to the
CoNLL-2005 shared task: Semantic role labeling. In
Proceedings of CoNLL-05, pages 152?164.
M. Diab, M. Alkhalifa, S. ElKateb, C. Fellbaum, A. Man-
souri, and M. Palmer. 2007. SemEval-2007 Task 18:
Arabic semantic labeling. In Proc. of SemEval-07.
K. Erk and S. Pado?. 2006. Shalmaneser - a flexible
toolbox for semantic role assignment. In Proceedings
of LREC-06.
C.J. Fillmore and C.F. Baker. 2001. Frame semantics for
text understanding. In Proc. of the NAACL-01 Work-
shop on WordNet and Other Lexical Resources.
C.J. Fillmore. 1977. Scenes-and-frames semantics, lin-
guistic structures processing. In Antonio Zampolli,
editor, Fundamental Studies in Computer Science, No.
59, pages 55?88. North Holland Publishing.
C.J. Fillmore. 1986. Pragmatically controlled zero
anaphora. In Proceedings of the Twelfth Annual Meet-
ing of the Berkeley Liguistics Society.
D. Gildea and D. Jurafsky. 2002. Automatic labeling of
semantic roles. Computational Linguistics, 28(3):245?
288.
K. Litkowski. 2004. SENSEVAL-3 Task: Automatic
labeling of semantic roles. In Proc. of SENSEVAL-3.
L. Ma`rquez, L. Villarejo, M. A. Mart?`, and M. Taule`. 2007.
SemEval-2007 Task 09: Multilevel semantic annotation
of Catalan and Spanish. In Proceedings of SemEval-07.
S. Pado?, M. Pennacchiotti, and C. Sporleder. 2008. Se-
mantic role assignment for event nominalisations by
leveraging verbal data. In Proceedings of Coling-2008.
M. Palmer, D. Dahl, R. Passonneau, L. Hirschman,
M. Linebarger, and J. Dowding. 1986. Recovering
implicit information. In Proceedings of ACL-1986.
M. Palmer. 1990. Semantic Processing for Finite Do-
mains. CUP, Cambridge, England.
J. Ruppenhofer. 2004. The interaction of valence and
information structure. Ph.d., University of California,
Berkeley, CA.
D. Shen and M. Lapata. 2007. Using semantic roles to
improve question answering. In Proc. of EMNLP-07.
M. Surdeanu, S. Harabagiu, J. Williams, and P. Aarseth.
2003. Using predicate arguments structures for infor-
mation extraction. In Proceedings of ACL-2003.
M. Surdeanu, R. Johansson, A. Meyers, Ll. Ma`rquez, and
J. Nivre. 2008. The CoNLL-2008 shared task on joint
parsing of syntactic and semantic dependencies. In
Proceedings of CoNLL-2008, pages 159?177.
111
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 579?589, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
A Statistical Relational Learning Approach to Identifying
Evidence Based Medicine Categories
Mathias Verbeke? Vincent Van Asch? Roser Morante?
Paolo Frasconi? Walter Daelemans? Luc De Raedt?
? Department of Computer Science, Katholieke Universiteit Leuven, Belgium
{mathias.verbeke, luc.deraedt}@cs.kuleuven.be
? Department of Linguistics, Universiteit Antwerpen, Belgium
{roser.morante, vincent.vanasch, walter.daelemans}@ua.ac.be
? Dipartimento di Sistemi e Informatica, Universita` degli Studi di Firenze, Italy
p-f@dsi.unifi.it
Abstract
Evidence-based medicine is an approach
whereby clinical decisions are supported by
the best available findings gained from scien-
tific research. This requires efficient access
to such evidence. To this end, abstracts in
evidence-based medicine can be labeled using
a set of predefined medical categories, the so-
called PICO criteria. This paper presents an
approach to automatically annotate sentences
in medical abstracts with these labels. Since
both structural and sequential information are
important for this classification task, we use
kLog, a new language for statistical relational
learning with kernels. Our results show a clear
improvement with respect to state-of-the-art
systems.
1 Introduction
Evidence-based medicine (EBM) or evidence-based
practice (EBP) combines clinical expertise, the pref-
erences and values of the patient and the best
available evidence to make good patient care deci-
sions. Clinical research findings are systematically
reviewed, appraised and used to improve the patient
care, for which efficient access to such evidence is
required. In order to facilitate the search process,
medical documents are labeled using a set of prede-
fined medical categories, the PICO criteria. PICO is
an acronym for the mnemonic concepts that are used
to construct queries when searching for scientific ev-
idence in the EBM process. The need to automatize
the annotation process has initiated research into au-
tomatic approaches to annotate sentences in medical
documents with the PICO labels.
As indicated by Kim et al2011), both the struc-
tural information of the words in the sentence, and
that of the sentences in the document are important
features for this task. Furthermore, sequential infor-
mation can leverage the dependencies between dif-
ferent sentences in the text. Therefore we propose an
approach using kLog (Frasconi et al2012) to tackle
this problem. kLog is a new language for statistical
relational learning with kernels, that is embedded in
Prolog, and builds upon and links together concepts
from database theory, logic programming and learn-
ing from interpretations. Learning from interpreta-
tions is a logical and relational learning setting (De
Raedt et al2008) in which the examples are inter-
pretations, that is, sets of tuples that are true in the
examples. In a sense, each example can be viewed
as a small relational database. kLog is able to trans-
form relational into graph-based representations and
apply kernel methods to extract an extended high-
dimensional feature space.
The choice for kLog was motivated by previous
results (Verbeke et al2012), where we showed that
a statistical relational learning approach using kLog
is able to process the contextual aspects of language
improving on state-of-the-art results for hedge cue
detection. However, the current task adds two levels
of complexity. First, next to the relations between
the words in the sentence, now also the relations be-
tween the sentences in the document become impor-
tant. In the proposed approach, we first generate a
feature space with kLog that captures the intrasen-
tential properties and relations. Hereafter, these fea-
tures serve as input for a structured output support
vector machine that can handle sequence tagging
579
(Tsochantaridis et al2004), in order to take the
intersentential features into account. Second, since
there are more than two categories, and each sen-
tence can have multiple labels, the problem is now a
multiclass multilabel classification task.
The main contribution of this paper is that we
show that kLog?s relational nature and its ability
to declaratively specify and use background knowl-
edge is beneficial for natural language learning prob-
lems. This is shown on the NICTA-PIBOSO corpus,
for which we present results that indicate a clear im-
provement on the state-of-the-art.
The remainder of this paper is organized as fol-
lows. In Section 2, we outline earlier work that is
related to the research presented here. Section 3 de-
scribes the methodology of our method. We present
a thorough evaluation of our method in Section
4. The last section draws conclusions and presents
some ideas for future work.
2 Related Work
EBM is an approach to clinical problem-solving
based on ?systematically finding, appraising, and us-
ing contemporaneous research findings as the ba-
sis for clinical decisions? (Rosenberg and Donald,
1995). The evidence-based process consists of four
steps: (1) Formulating a question from a patient?s
problem; (2) Searching the literature for relevant
clinical articles; (3) Evaluating the evidence; And
(4) implementing useful findings in clinical prac-
tice. Given the amounts of medical publications
available in databases such as PubMed, automating
step 2 is crucial to help doctors in their practice.
Efforts in this direction from the NLP community
have so far focused on corpus annotation (Demner-
Fushman and Lin, 2007; Kim et al2011), text cate-
gorization (Davis-Desmond and Molla?, 2012), sum-
marization (Molla? and Santiago-Mart??nez, 2011),
and question-anwering (Niuet al2003; Demner-
Fushman and Lin, 2007).
The existing corpora are usually annotated with
the PICO mnemonic (Armstrong, 1999) concepts,
that are used to build queries when searching for
literature for EBM purposes. The PICO concepts
are: primary Problem (P) or population, main Inter-
vention (I), main intervention Comparison (C), and
Outcome of intervention (O). PICO helps determin-
ing what terms are important in a query and there-
fore it helps building the query, which is sent to the
search repositories. Once the documents are found,
they need to be read by a person who eliminates ir-
relevant documents.
The first attempt to classify PICO concepts is pre-
sented in Demner-Fushman and Lin (2007), who
apply a rule-based approach to identify sentences
where PICO concepts occur and a supervised ap-
proach to classify sentences that contain an Out-
come. The features used by this classifier are n-
grams, position, and semantic information from the
parser used to process the data. The system is trained
on 275 abstracts manually annotated. The accura-
cies reported range from 80% for Population, 86%
for Problem, 80% for Intervention, and, from 64%
to 95% for Outcome depending on the test set of ab-
stracts.
Kim et al2011) perform a similar classification
task in two steps. First a classifier identifies the sen-
tences that contain PICO concepts, and then another
classifier assigns PICO tags to the sentences found
to be relevant by the previous classifier. The sys-
tem is based on a CRF algorithm and is trained on
the NICTA-PIBOSO corpus. This dataset contains
1,000 medical abstracts manually annotated with an
extension of the PICO tagset, for which the defini-
tions are listed in Table 1. The annotation is per-
formed at sentence level and one sentence may have
more than one tag. An example of an annotated
abstract from the corpus can be found in the sup-
plementary material. The features used by the al-
gorithm include features derived from the context,
semantic relations, structure and sequencing of the
text. The system is evaluated for 5-way and 6-way
classification and results are provided apart from
structured and unstructured abstracts. The F-scores
for structured abstracts is 89.32% for 5-way classifi-
cation and 80.88% for 6-way classification, whereas
for unstructured abstracts it is 71.54% for 5-way
classification and 64.66% for 6-way classification.
Chung (2009) uses CRF to classify PICO con-
cepts by combining them with general categories as-
sociated with rhetorical roles: Aim, Method, Results
and Conclusion. Her system is tested on corpora of
abstracts of randomized control trials. First struc-
tured abstracts with headings labeled with PICO
580
Background Material that informs and may place the current study in perspective, e.g. work that preceded the
current; information about disease prevalence; etc.
Population The group of individual persons, objects or items comprising the study?s sample, or from which
the sample was taken for statistical measurement
Intervention The act of interfering with a condition to modify it or with a process to change its course (includes
prevention)
Outcome The sentence(s) that best summarizes the consequences of an intervention
Study Design The type of study that is described in the abstract
Other Any sentence not falling into one of the other categories and presumed to provide little help with
clinical decision making, i.e. non-key or irrelevant sentences
Table 1: Definitions of the semantic tags used as annotation categories (taken from Kim et al2011)).
concepts are used. A sentence level classification
task is performed, assigning only one rhetorical role
per sentence. The F-scores obtained range from 0.93
to 0.98. Then another sentence level classification
task is performed to automatically assign the labels
Intervention, Participant and Outcome Measures to
sentences in unstructured and structured abstracts
without headings. F-scores of up to 0.83 and 0.84
are obtained for Intervention and Outcome Measure
sentences.
Other work aimed at identifying rhetorical zones
in biomedical articles. In this case areas of text are
classified in terms of the rhetorical categories In-
troduction, Methods, Results and Discussion (IM-
RAD) (Agarwal and Yu, 2009) or richer categories,
such as problem-setting or insight (Mizuta et al
2006).
There exists a wide range of statistical relational
learning systems (Getoor and Taskar, 2007; De
Raedt et al2008), and many of these systems
are in principle useful for natural language process-
ing. The most popular formalism today is Markov
Logic, which has already been used for natural lan-
guage processing tasks such as semantic role label-
ing (Riedel and Meza-Ruiz, 2008) and coreference
resolution (Poon and Domingos, 2008). With re-
spect to Markov Logic, two distinguishing features
of kLog are that 1) it employs kernel based meth-
ods grounded in statistical learning theory, and 2) it
employs a Prolog like language for defining and us-
ing background knowledge. As Prolog is a program-
ming language, this is more flexible that the formal-
ism used by Markov Logic.
3 Methodology
In learning from examples, or interpretations (De
Raedt et al2008), the instances are sampled iden-
tically and independently from some unknown but
fixed distribution. They can be represented as pairs
z = (x, y), in which x represents the inputs and y
the outputs. An example interpretation can be found
in Figure 3, where the hasCategory relation repre-
sents y in this case, since it is the target relation we
want to predict. The inputs x are formed by all other
facts. The task is now to learn a function h : X ? Y
that maps the inputs to the outputs. Sentences may
have multiple labels. Hence this is a structured out-
put task where the output is a sequence of sets of
labels attached to the sentences in a given document.
kLog is the new statistical relational language for
learning with kernels that we use to tackle the PICO
categories classification task. The novelty of kLog
is that, based on the regular, linguistic features, it
allows to define an extended high-dimensional fea-
ture space that is also able to take relational features
into account in a principled manner. Furthermore,
its declarative approach offers a flexible and inter-
pretable way to construct features.
The choice of kLog is motivated by our previous
results (Verbeke et al2012), where we showed that
the relational representation of the domain as used
by kLog is able to take the contextual aspects of lan-
guage into account. Whereas there we only used
the relations at the sentence level, the current task
adds a new level of complexity, since the identifica-
tion of PICO categories in abstracts also requires to
take into account various relations between the sen-
tences of an abstract. The general workflow of our
approach is depicted in Figure 1, which will be de-
581
Database
(Fig. 3)
Extensionalized 
database
Graph
(Fig. 4)
Kernel matrix/
feature vectors
Statistical 
learner
Raw data
(sentence)
Feature extraction
(lemma, POS,?)
Declarative feature 
construction
Graphicalization
Feature 
generation
Graph kernel 
(NSPDK)
kLog
Figure 1: General kLog workflow.
scribed step by step in the following paragraphs.
Preprocessing The sentences have been prepro-
cessed with a named entity tagger and a dependency
parser.
Named entity tagging has been performed with
the BiogaphTA named entity module, which
matches token sequences with entries in the UMLS
database1. UMLS integrates over 2 million names
for some 900,000 concepts from more than 60 fami-
lies of biomedical vocabularies (Bodenreider, 2004).
The tagger matches sequences with a length of max-
imum 4 tokens. This covers 66.2% of the UMLS
entries. By using UMLS, different token sequences
referring to the same concept can be mapped to
the same concept identifier (CID). The BiographTA
named entity tagger has been evaluated on the
BioInfer corpus (Pyysalo et al2007) obtaining a
72.02 F1 score.
Dependency parsing has been performed with the
GENIA dependency parser GDep (Sagae and Tsu-
jii, 2007), which uses a best-first probabilistic shift-
reduce algorithm based on the LR algorithm (Knuth,
1965) and extended by the pseudo-projective pars-
ing technique. This parser is a version of the KSDep
dependency parser trained on the GENIA Treebank
for parsing biomedical text. KSDep was evaluated
in the CoNLL Shared Task 2007 obtaining a La-
beled Attachment Score of 89.01% for the English
dataset. GDEP outputs the lemmas, chunks, Genia
named entities and dependency relations of the to-
kens in a sentence.
This information can be represented as an
Entity/Relationship (E/R) diagram, a modeling
paradigm that is frequently used in database theory
(Garcia-Molina et al2008). The E/R-model for the
1From UMLS only the MRCONSO.RRF and MRSTY.RRF
files are used.
problem under consideration is shown in Figure 2,
which provides an abstract representation of the ex-
amples, i.e. medical abstracts in this case. We will
show later how this abstract representation can be
unrolled for each example, resulting in a graph; cf.
also Figure 4 for our example sentence. This rela-
tional database representation will serve as the input
for kLog.
w
depHead
next
wordID
depRel
lemma
POS-tag
chunktag
wordString
NEGenia
NEUMLS
sentence
hasWord
class
sentID
hasCategory
nextS
Figure 2: E/R-diagram modeling the sentence identifica-
tion task.
The entities are the words and sentences in the
abstract. They are represented by the rectangles in
the E/R-model. Each entity can have a number of
properties attached to it, depicted by the ovals and
has a unique identifier (underlined properties). As
in database theory, each entity corresponds with a
tuple, or fact, in the database.
Figure 3 shows a part of an example interpretation
z. For example, w(w4 1,?Surgical?,?Surgical?,b-
np,jj,?O?,?O?) specifies a word entity, with w4 1 as
identifier and the other arguments as properties. As
indicated before, as lexical information we take the
token string itself, its lemma, the part-of-speech tag
and the chunk tag into account. We also include
some semantic information, namely two binary val-
ues indicating if the word is a (biological) named
entity. sentence(s4,4) represents a sentence entity,
with its index in the abstract as a property.
Furthermore, the E/R-diagram also contains a
number of relationships, which are represented by
582
sentence(s4,4)
hasCategory(s4,?background?)
w(w4_1,?Surgical?,?Surgical?,b-np,
jj,?O?,?O?) hasWord(s4,w4_1)
dh(w4_1,w4_2,nmod)
nextW(w4_2,w4_1)
w(w4_2,?excision?,?excision?,i-np,
nn,?O?,?O?) hasWord(s4,w4_2)
dh(w4_2,w4_5,sub)
nextW(w4_3,w4_2)
w(w4_3,?of?,?of?,b-pp,in,?O?,?O?)
hasWord(s4,w4_3)
dh(w4_3,w4_2,nmod)
nextW(w4_4,w4_3)
w(w4_4,?CNV?,?CNV?,b-np,nn,
?B-protein?,?O?) hasWord(s4,w4_4)
dh(w4_4,w4_3,pmod)
nextW(w4_5,w4_4)
...
Figure 3: Part of an example interpretation z, represent-
ing the example sentence in Figure 4.
the diamonds. They are linked to the entities that
participate in the relationship, or stand alone if they
characterize general properties of the interpretation.
An example relation is nextW(w4 2,w4 1), which
indicates the sequence of the words in the sentence.
dh(w4 1,w4 2,nmod) specifies that word w4 1 is
a noun modifier of word w4 2, and thus serves to
incorporate the dependency relationships between
the words. hasCategory(s4,?background?) signi-
fies that sentence s4 is a sentence describing back-
ground information. This relation is the target re-
lation that we want to predict for this task and will
not be taken into account as a feature, but is listed in
the database and only used during the training of the
model.
Since the previously described entities and rela-
tionships are listed explicitly in the database, these
are called extensional relations, in contrast to the in-
tensional relations, as we will describe next.
Declarative feature construction A strength of
kLog is that it is also capable of constructing fea-
tures declaratively, by using intensional relations.
This enables one to encode additional background
knowledge based on a small set of preprocessed fea-
tures, which renders experimentation very flexible
and makes the results more interpretable. It further-
more allows one to limit the required features to the
core discriminative ones. These intensional features
are defined through definite clauses, and is done us-
ing an extension of the declarative programming lan-
guage Prolog. The following features were used.
We make a distinction between the features used for
structured and unstructured abstracts.
For structured abstracts, four intensional relations
were defined. The relation lemmaRoot(S,L) is
specified as:
lemmaRoot(S,L) ?
hasWord(S, I),
w(I,_,L,_,_,_,_),
dh(I,_,root).
For each sentence, it only selects the lemmas
of the root word in the dependency tree, which
markedly limits the number of word features used.
The following relations are related to, and try to
capture the document structure imposed by the sec-
tion headers present in the structured abstracts.
hasHeaderWord(S,X) identifies whether a sen-
tence is a header of a section. In order to realize this,
it selects the words of a sentence that count more
than four characters (to discard short names of bio-
logical entities), which all need to be uppercase.
hasHeaderWord(S,X) ?
w(W,X,_,_,_,_,_),
hasWord(S,W),
(atom(X) -> name(X,C) ; C = X),
length(C,Len),
Len > 4,
all_upper(C).
Also the sentences below a certain section header
need to be marked as belonging to this sec-
tion, which is done by the relation hasSection-
Header(S,X).
hasSectionHeader(S,X) ?
nextS(S1,S),
hasHeaderWord(S1,X).
hasSectionHeader(S,X) ?
nextS(S1,S),
not isHeaderSentence(S),
once(hasSectionHeader(S1,X)).
583
For the unstructured abstracts, also the lemma-
Root relation is used, but next to the lemma, now
also the part-of-speech tag of the root word is taken
into account. Since the unstructured abstracts lack
section headers, other features were needed to dis-
tinguish between the different sections, for which
the relation prevLemmaRoot proved to be very in-
formative. It adds the lemma of the root word in the
previous sentence as a property to the current sen-
tence under consideration.
prevLemmaRoot(S,L) ?
nextS(S1,S),
lemmaRoot(S1,L,_).
The intensional predicates are grounded. This is
a proces similar to materialization in databases, that
is, the atoms implied by the background knowledge
and the facts in the example are all computed using
Prolog?s deduction mechanism. This leads to the
extensionalized database, in which both the exten-
sional as well as the grounded intensional predicates
are listed.
Graphicalization and feature generation In the
third step, the interpretations are graphicalized, i.e.
transformed into graphs. Since the facts that form
the interpretation still conform to the E/R-diagram,
this can be interpreted as unfolding the E/R-diagram
over the data. An example illustrating this process
is given in Figure 4. Each interpretation is converted
into a bipartite graph, for which there is a vertex for
every ground atom of every E-relation, one for every
ground atom of every R-relation, and an undirected
edge {e, r} if an entity e participates in relationship
r.
The obtained graphs can then be used in the next
step for feature generation. This is done by means
of a graph kernel ?, which calculates the similar-
ity between two graphicalized interpretations. Any
graph kernel that allows fast computations on large
graphs and has a flexible bias to enable heteroge-
neous features can in theory be applied. In the cur-
rent implementation, an extension of the Neighbor-
hood Subgraph Pairwise Distance Kernel (NSPDK)
(Costa and De Grave, 2010) is used.
NSPDK is a decomposition kernel (Haussler,
1999), in which pairs of subgraphs are compared
                                         
      
       
s0
s1
s2
s3
nextS
nextS
next
s4
s5
s6
 
 
 
s7
 
s8
 
s9
 
title
title
Surgical  excision  of  CNV  may  allow  stabilisation  or  improvement  of  vision.
background
next next                         
dh(nmod)
dh(sub)
dh(pmod)
  
hasWord
  
  
  
  
  
Figure 4: Graphicalization Gz of interpretation z.
to each other in order to calculate the similarity be-
tween two graphs. These subgraphs can be seen as
circles in the graph, and are defined by three hyper-
parameters. First of all, there is the center of the
subgraph, the kernel point, which can be any entity
or relation in the graph. The entities and relations
to be taken into account as kernel points are marked
beforehand as a subset of the intensional and exten-
sional domain relations. The radius r determines
the size of the subgraphs and defines which entities
or relations around the kernel point are taken into ac-
count. Each entity or relation that is within a number
of r edges away from the kernel point is considered
to be part of the subgraph. The third hyperparam-
eter, the distance d, determines how far apart from
each other the kernel points can be. Each subgraph
around a kernel point that is within a distance d or
less from the current kernel point will be considered.
This is captured by the relation Rr,d(Av, Bu, G) be-
tween two rooted subgraphs Av, Bu and a graph G,
which selects all pairs of neighborhood graphs of ra-
dius r whose roots are at distance d in a given graph
G.
The kernel ?r,d(G,G?) between graphs G and G?
on the relation Rr,d is then defined as:
?r,d(G,G
?) =
?
Av , Bu ? R
?1
r,d(G)
A?v? , B
?
u? ? R
?1
r,d(G
?)
?(Av, A
?
v?)?(Bu, B
?
u?)
(1)
584
For efficiency reasons, an upper bound is imposed
on the radius and distance parameters, which leads
to the following kernel definition:
Kr?,d?(G,G
?) =
r??
r=0
d??
d=0
?r,d(G,G
?) (2)
We hereby limit the sum of the ?r,d kernels for all
increasing values of the radius and distance parame-
ter up to a maximum given value of r?, respectively
d?.
The result of this graphicalization and feature
generation process is an extended, high-dimensional
feature space, which serves as input for the statisti-
cal learner in the next step.
Learning The constructed feature space contains
one feature vector per sentence. This implies that
the sequence information of the sentences at the doc-
ument level is not taken into account yet. Since the
order of the sentences in the abstract is a valuable
feature for this prediction problem, a learner that
reflects this in the learning process is needed, al-
though in principle any statistical learner can be used
on the feature space constructed by kLog. There-
fore we opted for SVM-HMM2 (Tsochantaridis et
al., 2004), which is an implementation of structural
support vector machines for sequence tagging. In
contrast to a conventional Hidden Markov Model,
SVM-HMM is able to take these entire feature vec-
tors as observations, and not just atomic tokens.
In our case, the instances to be tagged are formed
by the sentences for which feature vectors were cre-
ated in the previous step. The qid is a special fea-
ture that is used in the structured SVM to restrict
the generation of constraints. Since every document
needs to be represented as a sequence of sentences,
in SVM-HMM, the qid?s are used to obtain the doc-
ument structure. The order of the HMM was set
to 2, which means that the two previous sentences
were considered for collective classification. The
cost value was set to 500, and was determined via
cross-validation. For epsilon, the default value, 0.5,
was kept, since this mainly only influences the run-
ning time and memory consumption during training.
2http://www.cs.cornell.edu/people/tj/
svm_light/svm_hmm.html
All S U
Nb. Abstracts 1000 376 624
Nb. Sentences 10379 4774 5605
- Background 2557 669 1888
- Intervention 690 313 377
- Outcome 4523 2240 2283
- Population 812 369 443
- Study Design 233 149 84
- Other 1564 1034 530
Table 2: Number of abstracts and sentences for Struc-
tured (S) and Unstructured (U) abstract sets, including
number of sentences per class (taken from (Kim et al
2011)).
4 Evaluation
We evaluate the performance of kLog against a base-
line system and a memory-based tagger (Daelemans
and van den Bosch, 2005). The results are also com-
pared against those from Kim et al2011), which is
the state-of-the-art system for this task.
4.1 Datasets
We perform our experiments on the NICTA-
PIBOSO dataset from Kim et al2011) (kindly pro-
vided by the authors). It contains 1,000 abstracts of
which 500 were retrieved from MEDLINE by query-
ing for diverse aspects in the traumatic brain injury
and spinal cord injury domain. The dataset consists
of two types of abstracts. If the abstract contains
section headings (e.g. Background, Methodology,
Results, etc.), it is considered to be structured. This
information can be used as a feature in the model.
The other abstracts are regarded unstructured.
The definitions of the semantic tags used as an-
notations categories are a variation on the PICO tag
set, with the addition of two additional categories
(see Table 1 in Section 2). Each sentence can be an-
notated with multiple classes. This renders the task
a multiclass multilabel classification problem. The
statistics on this dataset can be found in Table 2.
In order to apply the same evaluation setting as
Kim et al2011), we used the dataset from Demner-
Fushman et al2005) as external dataset. It con-
sists of 100 sentences of which 51 are structured.
Because the semantic tag set used for annotation
slightly differs from the one presented in Table 1,
and to make our results comparable, we will use the
same mapping as used in Kim et al2011).
585
4.2 Baseline and benchmarks
We compare the kLog system to three other systems:
a baseline system, a memory-based system, and the
scores reported by Kim et al2011).
The memory-based system that we use is based
on the memory-based tagger MBT3 (Daelemans and
van den Bosch, 2005). This machine learner is orig-
inally designed for part-of-speech tagging. It pro-
cesses data on a sentence basis by carrying out se-
quential tagging, viz. the class label or other features
from previously tagged tokens can be used when
classifying a new token. In our setup, the sentences
of an abstract are taken as the processing unit and
the collection of all sentences in an abstract is taken
as one sequence.
The features that are used to label a sentence are
the class labels of four previous sentences, the am-
bitags of the following two sentences, the lemma of
the dependency root of the sentence, the position of
the sentence in the abstract, the lemma of the root
of the previous sentence, and section information.
For each root lemma, all possible class labels, as ob-
served in the training data, are concatenated into one
ambitag. These tags are stored in a list. An am-
bitag for a sentence is retrieved by looking up the
root lemma in this list. The position of the sentence
is expressed by a number. Section information is ob-
tained by looking for a previous sentence that con-
sists of only one token in uppercase. Finally, basic
lemmatization is carried out by removing a final S.
All other settings of MBT are the default settings
and no feature optimization nor feature selection has
been carried out to prevent overfitting.
When a class label contains multiple labels, like
e.g. population and study design, these labels are
concatenated in an alphabetically sorted manner.
This method of working reduces the multilabel prob-
lem to a problem with many different labels, i.e. the
label powerset method of Tsoumakas et al2010).
The baseline system is exactly the same as
the memory-based system except that no machine
learner is included. The most frequent class label in
the training data, i.e. Outcome, is assigned to each
instance. The memory-based system enables us to
compare kLog against a basic machine learning ap-
proach, using few features. The majority baseline
3http://ilk.uvt.nl/mbt [16 March 2012]
system enables us to compare the memory-based
system and kLog against a baseline in which no in-
formation about the observations is used.
4.3 Parametrization
From the kernel definition it might be clear that
the kLog hyperparameters, namely the distance d
and radius r, can have a strong influence on the
results. This requires a deliberate choice during
parametrization. From a linguistic perspective, the
use of unigrams and bigrams is justifiable, since
most phrases that reveal clues on the structure of the
abstract (e.g. evaluation measures, methodolody, fu-
ture work) can be expressed with single or pairs of
words. This is reflected by a distance and radius both
set to 1, which enables to take all possible combina-
tions of consecutive words into account and captures
the relational information attached to the word in fo-
cus, i.e. the current kernel point. This is confirmed
by cross-validation on other settings for the hyper-
parameters.
Since kLog generates a feature vector, only the
sequence information at word level is taken into ac-
count by kLog. Since we use a sequence labeling
approach as statistical learner, i.e. SVM-HMM, at
the level of the abstract this information is however
implicitly taken into account during learning. For
SVM-HMM, only the cost parameter C, which reg-
ulates the trade-off between the slack and the mag-
nitude of the weight-vector, and , that specifies the
precision to which constraints are required to be sat-
isfied by the solution, were optimized by means of
cross-validation. For the other parameters, the de-
fault values were used.
4.4 Results
Experiments are run on structured and unstructured
abstracts separately. On the NICTA-PIBOSO cor-
pus, we performed 10-fold cross-validation. Over
all folds, all labels, i.e. the parts of the multilabels,
are compared in a binary way between gold standard
and prediction. Summing all true positives, false
positives, and false negatives over all folds leads to
micro-averaged F-scores. This was done for two dif-
ferent settings. In one setting, CV/6-way, we com-
bined the labeling of the sentences with the identifi-
cation of irrelevant information, by adding the Other
586
label as an extra class in the classification. The re-
sults are listed in Table 3.
CV/6-way MBT Kim et alLog
Label S U S U S U
Background 71.0 61.3 81.84 68.46 86.19 76.90
Intervention 24.3 6.4 20.25 12.68 26.05 16.14
Outcome 87.9 70.4 92.32 72.94 92.99 77.69
Population 50.6 15.9 56.25 39.80 35.62 21.58
Study Design 45.9 13.10 43.95 4.40 45.5 6.67
Other 86.1 20.9 69.98 24.28 87.98 24.42
Table 3: F-scores per class for structured (S) and unstruc-
tured (U) abstracts.
For this setting, kLog is able to outperform both
MBT and the system of Kim et al2011), for both
structured and unstructured abstracts on all classes
except Population. From Table 4, where the micro-
average F-scores over all classes and for all settings
are listed, it can be observed that kLog performs up
to 3.73% better than MBT over structured abstracts,
and 9.67% better over unstructured ones.
Although to a lesser extent for the structured ab-
stracts, the same pattern can be observed for the
CV/5-way setting, where we tried to classify the sen-
tences only, without considering the irrelevant ones.
The per-class results for this setting are shown in Ta-
ble 5. Now the scores for Population are comparable
to the other systems, due to which we assume these
sentences are similar in structure to the ones labeled
with Other.
For the external corpus, the results are listed in Ta-
ble 6. Although kLog performs comparably for the
individual classes Background and Intervention, its
overall performance is worse on the structured ab-
stracts. In case of the unstructured abstracts, kLog
performs better on the majority of the individual
classes and in overall performance for the 5-way set-
ting, and comparable for the 4-way setting.
Baseline MBT kLog
Method S U S U S U
CV/6-way 43.90 41.87 80.56 57.47 84.29 67.14
CV/5-way 61.79 46.66 86.96 64.37 87.67 72.95
Ext/5-way 66.18 6.76 36.34 11.56 20.50 14.00
Ext/4-way 30.11 27.23 67.29 55.96 50.40 50.50
Table 4: Micro-averaged F1-score obtained for structured
(S) and unstructured (U) abstracts, both for 10-fold cross-
validation (CV) and on the external corpus (Ext).
CV/5-way MBT Kim et alLog
Label S U S U S U
Background 87.1 64.9 87.92 70.67 91.45 80.06
Intervention 48.0 6.9 48.08 21.39 45.58 22.65
Outcome 95.8 75.9 96.03 80.51 96.21 83.04
Population 70.9 21.4 63.88 43.15 63.96 23.32
Study Design 50.0 7.4 47.44 8.6 48.08 4.50
Table 5: F-scores per class for 5-way classification over
structured (S) and unstructured (U) abstracts.
MBT Kim et alLog
Label S U S U S U
Ext/5-way
Background 58.9 15.7 56.18 15.67 58.30 29.10
Intervention 21.5 13.8 15.38 28.57 40.00 34.30
Outcome 29.3 17.8 81.34 60.45 27.80 24.10
Population 10.7 17.8 35.62 28.07 5.60 28.60
Other 40.7 3.5 46.32 15.77 11.40 8.50
Ext/4-way
Background 90.4 67.5 77.27 37.5 65 68.6
Intervention 29 23.1 28.17 8.33 28.1 32.3
Outcome 74.1 74.6 90.5 78.77 72.4 72.7
Population 48.7 23.8 42.86 28.57 11.8 15.4
Table 6: F-scores per class for 5-way and 4-way classifi-
cation over structured (S) and unstructured (U) abstracts
on the external corpus.
As a general observation, it is important to note
that there is a high variability between the different
labels. Due to kLog?s ability to take the structured
input into account, we assume a correlation between
the sentence structure of the label and the predic-
tion quality. We intend to perform an extensive error
analysis, in order to detect patterns which may allow
us to incorporate additional declarative background
knowledge into our model.
5 Conclusions
We presented a statistical relational learning ap-
proach for the automatic identification of PICO cat-
egories in medical abstracts. To this extent, we used
kLog, a new framework for logical and relational
learning with kernels. Due to its graphical approach,
it is able to exploit the full relational representation,
that is often inherent in language structure. Since
contextual features are often essential and relations
are prevalent, the aim of this paper was to show
that statistical relational learning in general, and the
graph kernel-based approach of kLog in particular,
is specifically suited for problems in natural lan-
587
guage learning.
In future work, we intend to explore additional
ways to incorporate background knowledge in a
declarative way, since it renders the language learn-
ing problem more intuitive and gives a better under-
standing of feature contribution. Furthermore, we
also want to investigate the use of SRL approaches
for high-relational domains, and make a clear com-
parison with related techniques.
6 Acknowledgements
This research is funded by the Research Foundation
Flanders (FWO project G.0478.10 - Statistical Re-
lational Learning of Natural Language), and made
possible through financial support from the KU Leu-
ven Research Fund (GOA project 2008/08 Proba-
bilistic Logic Learning), the University of Antwerp
(GOA project BIOGRAPH) and the Italian Min-
istry of Education, University, and Research (PRIN
project 2009LNP494 - Statistical Relational Learn-
ing: Algorithms and Applications). The authors
would like to thank Fabrizio Costa, Kurt De Grave
and the anonymous reviewers for their valuable
feedback.
References
Shashank Agarwal and Hong Yu. 2009. Automatically
Classifying Sentences in Full-text Biomedical Articles
into Introduction, Methods, Results and Discussion.
Bioinformatics, 25(23):3174?3180.
E. C. Armstrong. 1999. The Well-built Clinical Ques-
tion: the Key to Finding the Best Evidence Efficiently.
WMJ, 98(2):25?28.
Olivier Bodenreider. 2004. The Unified Medical
Language System (UMLS): Integrating Biomed-
ical Terminology. Nucleic Acids Research,
32(Suppl.1):D267?D270.
Grace Y Chung. 2009. Sentence Retrieval for Abstracts
of Randomized Controlled Trials. BMC Medical In-
formatics and Decision Making, 9(10).
Fabrizio Costa and Kurt De Grave. 2010. Fast Neighbor-
hood Subgraph Pairwise Distance Kernel. Proceed-
ings of the 26th International Conference on Machine
Learning, 255?262, Haifa, Israel. Omnipress.
Walter Daelemans and Antal van den Bosch. 2005.
Memory-Based Language Processing.. Studies in
Natural Language Processing. Cambridge University
Press, Cambridge, UK.
P. Davis-Desmond and Diego Molla?. 2012. Detection of
Evidence in Clinical Research Papers. Proceedings of
the Australasian Workshop On Health Informatics and
Knowledge Management (HIKM 2012), Melbourne,
Australia, 129:13?20. Australian Computer Society,
Inc.
Dina Demner-Fushman, Barbara Few, Susan E. Hauser,
and George Thoma. 2005. Automatically Identifying
Health Outcome Information in MEDLINE Records.
Journal of the American Medical Informatics Associa-
tion (JAMIA), 13:52?60.
Dina Demner-Fushman and Jimmy Lin. 2007. An-
swering Clinical Questions with Knowledge Based
and Statistical Techniques. Computational Linguis-
tics, 33(1):63?103.
Luc De Raedt, Paolo Frasconi, Kristian Kersting, and
Stephen Muggleton, editors. 2008. Probabilistic In-
ductive Logic Programming. In: Lecture Notes in
Computer Science (LNCS), 4911. Springer-Verlag,
Heidelberg, Germany.
Paolo Frasconi, Fabrizio Costa, Luc De Raedt, and
Kurt De Grave. 2012. kLog - a Language
for Logical and Relational Learning with Kernels.
arXiv:1205.3981v2.
Hector Garcia-Molina, Jeff Ullman, and Jennifer Widom.
2008. Database Systems: The Complete Book. Pren-
tice Hall Press, Englewood Cliffs, NJ, USA.
Lise Getoor and Ben Taskar. 2007. Introduction to Sta-
tistical Relational Learning (Adaptive Computation
and Machine Learning). The MIT Press, Cambridge,
MA, USA.
David Haussler. 1999. Convolution kernels on discrete
structures. Technical report (UCSC-CRL-99-10), Uni-
versity of California at Santa Cruz.
Su Nam Kim, David Martinez, Lawrence Cavedon, and
Lars Yencken. 2011. Automatic Classification of Sen-
tences to Support Evidence Based Medicine. BMC
Bioinformatics, 12(2):S5.
Donald E. Knuth. 1965. On the Translation of Lan-
guages from Left to Right. Information and Control,
8: 607?639.
Y. Mizuta, A. Korhonen, T. Mullen, and N. Collier.
2006. Zone Analysis in Biology Articles as a Basis
for Information Extraction. International Journal of
Medical Informatics, 75(6):468?487.
Diego Molla? and Mara Elena Santiago-Mart??nez. 2011.
Development of a Corpus for Evidence Medicine
Summarisation. Proceedings of the 2011 Australasian
Language Technology Workshop (ALTA 2011), Can-
berra, Australia, 86?94. Association for Computa-
tional Linguistics.
Yun Niu, Graeme Hirst, Gregory McArthur, and Patri-
cia Rodriguez-Gianolli. 2003. Answering Clinical
588
Questions with Role Identification. Proceedings of the
ACL, Workshop on Natural Language Processing in
Biomedicine. Sapporo, Japan, 73?80. Association for
Computational Linguistics.
Hoifung Poon and Pedro Domingos. 2008. Joint un-
supervised coreference resolution with Markov logic.
Proceedings of the Conference on Empirical Methods
in Natural Language Processing (EMNLP 2008. Hon-
olulu, Hawaii, 650?659. Association for Computa-
tional Linguistics.
Sampo Pyysalo, Filip Ginter, Juho Heimonen, Jari
Bjo?rne, Jorma Boberg, Jouni Ja?rvinen, and Tapio
Salakoski. 2007. BioInfer: a Corpus for Information
Extraction in the Biomedical Domain. BMC Bioinfor-
matics, 8:50.
Sebastian Riedel and Ivan Meza-Ruiz. 2008. Collective
semantic role labelling with Markov logic. Proceed-
ings of the Twelfth Conference on Computational Nat-
ural Language Learning (CoNLL 2008. Manchester,
United Kingdom, 193?197. Association for Computa-
tional Linguistics.
William Rosenberg and Anna Donald. 1995. Evidence
Based Medicine: an Approach to Clinical Problem
Solving. British Medical Journal, 310(6987):1122?
1126.
Kenji Sagae and Jun?ichi Tsujii. 2007. Dependency
Parsing and Domain Adaptation with LR Models and
Parser Ensembles. Proceedings of the CoNLL Shared
Task Session of EMNLP-CoNLL 2007, Prague, Czech
Republic, 1044?1050. Association for Computational
Linguistics.
Ioannis Tsochantaridis, Thomas Hofmann, Thorsten
Joachims, and Yasemin Altun. 2004. Support Vector
Machine Learning for Interdependent and Structured
Output Spaces. Proceedings of the twenty-first inter-
national conference on Machine learning (ICML), Al-
berta, Canada, 104?111. ACM.
Grigorios Tsoumakas and Ioannis Katakis and Ioannis P.
Vlahavas. Oded Maimon and Lior Rokach, editors.
2010. Mining Multi-label Data. In: Data Mining and
Knowledge Discovery Handbook, 2nd ed., 667?685.
Springer-Verlag, Heidelberg, Germany.
Mathias Verbeke, Paolo Frasconi, Vincent Van Asch,
Roser Morante, Walter Daelemans, and Luc De Raedt.
2012. Kernel-based Logical and Relational Learning
with kLog for Hedge Cue Detection. Proceedings of
the 21th International Conference on Inductive Logic
Programming, in press.
589
Modality and Negation:
An Introduction to the
Special Issue
Roser Morante?
University of Antwerp
Caroline Sporleder??
Saarland University
Traditionally, most research in NLP has focused on propositional aspects of meaning. To truly
understand language, however, extra-propositional aspects are equally important. Modality
and negation typically contribute significantly to these extra-propositional meaning aspects.
Although modality and negation have often been neglected by mainstream computational lin-
guistics, interest has grown in recent years, as evidenced by several annotation projects dedicated
to these phenomena. Researchers have started to work on modeling factuality, belief and certainty,
detecting speculative sentences and hedging, identifying contradictions, and determining the
scope of expressions of modality and negation. In this article, we will provide an overview of how
modality and negation have been modeled in computational linguistics.
1. Introduction
Modality and negation are two grammatical phenomena that have been studied for a
long time. Aristotle was the initial main contributor to the analysis of negation from
a philosophical perspective. Since then, thousands of studies have been performed, as
illustrated by the Basic Bibliography of Negation in Natural Language (Seifert and Welte
1987). One of the first categorizations of modality is proposed by Otto Jespersen (1924)
in the chapter about Mood, where the grammarian distinguishes between ?categories
containing an element of will? and categories ?containing no element of will.? His
grammar devotes also a chapter to negation.
In contrast to the substantial number of theoretical studies, the computational treat-
ment of modality and negation is a newly emerging area of research. The emergence
of this area is a natural consequence of the consolidation of areas that focus on the
computational treatment of propositional aspects of meaning, like semantic role label-
ing, and a response to the need for processing extra-propositional aspects of meaning
as a further step towards text understanding. That there is more to meaning than
just propositional content is a long-held view. Prabhakaran, Rambow, and Diab (2010)
? CLiPS, University of Antwerp, Prinsstraat 13, B-2000 Antwerpen, Belgium.
E-mail: roser.morante@ua.ac.be.
?? Computational Linguistics, Saarland University, Postfach 15 11 50, D-66041 Saarbru?cken, Germany.
E-mail: csporled@coli.uni-sb.de.
Submission received: 5 April 2011; revised submission received: 18 January 2012; accepted for publication:
24 January 2012.
? 2012 Association for Computational Linguistics
Computational Linguistics Volume 38, Number 2
illustrate this statement with the following examples, where the event LAY OFF(GM,
workers) is presented with different extra-propositional meanings:
(1) a. GM will lay off workers.
b. A spokesman for GM said GM will lay off workers.
c. GM may lay off workers.
d. The politician claimed that GM will lay off workers.
e. Some wish GM would lay of workers.
f. Will GM lay off workers?
g. Many wonder whether GM will lay off workers.
Generally speaking, modality is a grammatical category that allows the expression
of aspects related to the attitude of the speaker towards her statements in terms of
degree of certainty, reliability, subjectivity, sources of information, and perspective. We
understand modality in a broad sense, which involves related concepts like ?subjec-
tivity?, ?hedging?, ?evidentiality?, ?uncertainty?, ?committed belief,? and ?factuality?.
Negation is a grammatical category that allows the changing of the truth value of
a proposition. A more detailed definition of these concepts with examples will be
presented in Sections 2 and 3.
Modality and negation are challenging phenomena not only from a theoretical
perspective, but also from a computational point of view. So far two main tasks have
been addressed in the computational linguistics community: (i) the detection of various
forms of negation and modality and (ii) the resolution of the scope of modality and
negation cues. Whereas modality and negation tend to be lexically marked, the class
of markers is heterogeneous, especially in the case of modality. Determining whether
a sentence is speculative or whether it contains negated concepts cannot be achieved
by simple lexical look-up of words potentially indicating modality or negation. Modal
verbs like might are prototypical modality markers, but they can be used in multiple
senses. Multiword expressions can also express modality (e.g., this brings us to the largest
of all mysteries or little was known). Modality and negation interact with mood and tense
markers, and also with each other. Finally, discourse factors also add to the complexity
of these phenomena.
Incorporating information about modality and negation has been shown to be
useful for a number of applications such as recognizing textual entailment (de Marneffe
et al 2006; Snow, Vanderwende, and Menezes 2006; Hickl and Bensley 2007), machine
translation (Baker et al 2010), trustworthiness detection (Su, Huang, and Chen 2010),
classification of citations (Di Marco, Kroon, and Mercer 2006), clinical and biomedical
text processing (Friedman et al 1994; Szarvas 2008), and identification of text struc-
ture (Grabar and Hamon 2009).
This overview is organized as follows: Sections 2 and 3 define modality and nega-
tion, respectively. Section 4 gives details of linguistic resources annotated with various
aspects of negation and modality. We also discuss properties of the different annotation
schemes that have been proposed. Having discussed the linguistic basis as well as the
available resources, the remainder of the article then provides an overview of automated
methods for dealing with modality and negation. Most of the work in this area has
been carried out at the sentence or predicate level. Section 5 discusses various methods
for detecting speculative sentences. This is only a first step, however. For a more fine-
224
Morante and Sporleder Modality and Negation
grained analysis, it is necessary to deal with modality and negation on a sub-sentential
(i.e., predicate) level.
This is addressed in Section 6, which also discusses various methods for the impor-
tant task of scope detection. Section 7 then moves on to work on detecting negation and
modality at a discourse level, that is, in the context of recognizing contrasts and con-
tradictions. Section 8 takes a closer look at dealing with positive and negative opinions
and summarizes studies in the field of sentiment analysis that have explicitly modeled
modality and negation. Section 9 provides an overview of the articles in this special
issue. Finally, Section 10 concludes this article by outlining some of the remaining
challenges.
Some notational conventions should be clarified. In the literature, the affixes, words
or multiword expressions that express modality and negation have been referred to as
triggers, signals, markers, and cues. Here, we will refer to them as cues and we will
mark them in bold in the examples. The boundaries of their scope will be marked with
square brackets.
2. Modality
From a theoretical perspective, modality can be defined as a philosophical concept,
as a subject of the study of logic, or as a grammatical category. There are many def-
initions and classifications of modal phenomena. Even if we compiled an exhaustive
and precise set of existing definitions, we would still be providing a limited view
on what modality is, because, as Salkie, Busuttil, and van der Auwera (2009, page 7)
put it:
. . .modality is a big intrigue. Questions erstwhile considered solved become open
questions again. New observations and hypotheses come to light, not least because
the subject matter is changing.
Defining modality from a computational linguistics perspective for this special
issue becomes even more difficult because several concepts are used to refer to phe-
nomena that are related to modality, depending on the task at hand and the specific
phenomena that the authors address. To mention some examples, research focuses on
categorizing modality, on committed belief tagging, on resolving the scope of hedge
cues, on detecting speculative language, and on computing factuality. These concepts
are related to the attitude of the speaker towards her statements in terms of degree of
certainty, reliability, subjectivity, sources of information, and perspective. Because this
special issue focuses on the computational treatment of modality, we will provide a
general theoretical description of modality and the related concepts mentioned in the
computational linguistics literature at the cost of offering a simplified view of these
concepts.
Jespersen (1924, page 329) attempts to place all moods in a logically consistent
system, distinguishing between ?categories containing an element of will? and ?cat-
egories containing no element of will??later named as propositional modality and
event modality by Palmer (1986). Lyons (1977, page 793) describes epistemic modality
as concerned with matters of knowledge and belief, ?the speaker?s opinion or attitude
towards the proposition that the sentence expresses or the situation that the proposition
describes.? Palmer (1986, page 8) distinguishes propositional modality, which is ?con-
cernedwith the speaker?s attitude to the truth-value or factual status of the proposition?
225
Computational Linguistics Volume 38, Number 2
as in Example (2a), and event modality, which ?refers to events that are not actualized,
events that have not taken place but are merely potential? as in Example (2b):
(2) a. Kate must be at home now.
b. Kate must come in now.
Within propositional modality, Palmer defines two types: epistemic, used by speakers
?to express their judgement about the factual status of the proposition,? and evidential,
used ?to indicate the evidence that they have for its factual status? (Palmer 1986, 8?
9). He also defines two types of event modality: deontic, which relates to obligation
or permission and to conditional factors ?that are external to the relevant individual,?
and dynamic, where the factors are internal to the individual (Palmer 1986, pages 9?
13). Additionally, Palmer indicates other categories that may be marked as irrealis and
may be found in the mood system: future, negative, interrogative, imperative-jussive,
presupposed, conditional, purposive, resultative, wishes, and fears. Palmer explains
how modality relates to tense and aspect: The three categories are concerned with the
event reported by the utterance, whereas tense is concerned with the time of the event
and aspect is ?concerned with the nature of the event in terms of its internal temporal
constituency? (Palmer 1986, pages 13?16).
From a philosophical standpoint, von Fintel (2006) defines modality as ?a category
of linguistic meaning having to do with the expression of possibility and necessity.?
In this sense ?a modalized sentence locates an underlying or prejacent proposition
in the space of possibilities.? Von Fintel describes several types of modal meaning
(alethic, epistemic, deontic, bouletic, circumstantial, and teleological), some of which
are introduced by von Wright (1951), and shows that modal meaning can be expressed
by means of several types of expressions, such as modal auxiliaries, semimodal verbs,
adverbs, nouns, adjectives, and conditionals.
Within the modal logic framework several authors provide a more technical ap-
proach to modality. Modal logic (von Wright 1951; Kripke 1963) attempts to represent
formally the reasoning involved in expressions of the type it is necessary that ... and
it is possible that ... starting from a weak logic called K (Garson 2009). Taken in a
broader sense, modal logic also aims at providing an analysis for expressions of deontic,
temporal, and doxastic logic. Within the modal logic framework, modality is analyzed
in terms of possible worlds semantics (Kratzer 1981). The initial idea is that modal
expressions are considered to express quantification over possible worlds.
Kratzer (1981, 1991), however, argues that modal expressions are more complex
than quantifiers and that their meaning is context-dependent. Recent work on modality
in the framework of modal logic is presented by Portner (2009, pages 2?8), who groups
modal forms into three categories: sentential modality (?the expression of modal mean-
ing at the level of the whole sentence?); sub-sentential modality (?the expression of
modal meaningwithin constituents smaller than a full clause?); and discoursemodality
(?any contribution to meaning in discourse which cannot be accounted for in terms of a
traditional semantic framework?).
From a typological perspective, the study of modality seeks to describe how the
languages of the world express different types of modality (Palmer 1986; van der
Auwera and Plungian 1998). Knowing how modality is expressed across languages is
relevant for the computational linguistics community, not only because it is essential
for developing automated systems for languages other than English, but also because
226
Morante and Sporleder Modality and Negation
it throws some light on the underlying phenomena that might be beneficial for the
development of novel methods for dealing with modality.
Concepts related to modality that have been studied in computational linguistics
are: hedging, evidentiality, uncertainty, factuality, and subjectivity. The term hedging is
originally due to Lakoff (1972, page 195), who describes hedges as ?words whose job is
to make things more or less fuzzy.? Lakoff starts from the observation that ?natural
language concepts have vague boundaries and fuzzy edges and that, consequently,
natural language sentences will very often be neither true, nor false, nor nonsensical,
but rather true to a certain extent and false to a certain extent, true in certain aspects
and false in certain aspects? (Lakoff 1972, page 183). In order to deal with this aspect of
language, he extends the classical propositional and predicate logic to fuzzy logic and
focuses on the study of hedges. Hyland (1998) studies hedging in scientific texts. He
proposes a pragmatic classification of hedge expressions based on an exhaustive analy-
sis of a corpus. The catalogue of hedging cues includes modal auxiliaries, epistemic
lexical verbs, epistemic adjectives, adverbs, nouns, and a variety of non-lexical cues.
Evidentiality is related to the expression of the information source of a statement.
As Aikhenvald (2004, page 1) puts it:
In about a quarter of the world?s languages, every statement must specify the type of
source on which it is based [...]. This grammatical category, whose primary meaning is
information source, is called ?evidentiality?.
This grammatical category was already introduced by Boas (1938), and has been studied
afterwards, although less than modality. There is no agreement on whether it should
be a subcategory of modality (Palmer 1986; de Haan 1995) or a category by itself (de
Haan 1999; Aikhenvald 2004). A broader definition relates evidentiality to the expres-
sion of the speaker?s attitude towards the information being presented (Chafe 1986).
Ifantidou (2001, page 5) considers that the function of evidentials is to indicate the
source of knowledge (observation, hearsay, inference, memory) on which a statement
is based and the speaker?s degree of certainty about the proposition expressed.
Certainty is a type of subjective information that can be conceived of as a variety
of epistemic modality (Rubin, Liddy, and Kando 2005). Here we take their definition
(page 65):
. . . certainty is viewed as a type of subjective information available in texts and a
form of epistemic modality expressed through explicitly-coded linguistic means.
Such devices [...] explicitly signal presence of certainty information that covers a
full continuum of writer?s confidence, ranging from uncertain possibility and
withholding full commitment to statements.
Factuality involves polarity, epistemic modality, evidentiality, and mood. It is
defined by Saur?? (2008, page 1) as:
. . . the level of information expressing the commitment of relevant sources towards
the factual nature of eventualities in text. That is, it is in charge of conveying whether
eventualities are characterized as corresponding to a fact, to a possibility, or to a
situation that does not hold in the world.
Factuality can be expressed by several linguistic means: negative polarity particles,
modality particles, event-selecting predicates which project factuality information on
the events denoted by their arguments (claim, suggest, promise, etc.), and syntactic
227
Computational Linguistics Volume 38, Number 2
constructions involving subordination. The factuality of a specific event can change
during the unfolding of the text. As described in Saur?? and Pustejovsky (2009), depend-
ing on the polarity, events are depicted as either facts or counterfacts. Depending on
the level of uncertainty combined with polarity, events will be presented as possibly
factual (3a) or possibly counterfactual (3b).
(3) a. The United States may extend its naval quarantine to Jordan?s Red Sea
port of Aqaba.
b. They may not have enthused him for their particular brand of political
idealism.
The term subjectivity is introduced by Banfield (1982). Work on subjectivity in
computational linguistics is initially due to Wiebe, Wilson, and collaborators (Wiebe
1994; Wiebe et al 2004; Wiebe, Wilson, and Cardie 2005; Wilson 2008; Wilson et al 2005;
Wilson, Wiebe, and Hwa 2006) and focuses on learning subjectivity from corpora. As
Wiebe et al (2004, page 279) put it:
Subjective language is language used to express private states in the context of a text or
conversation. Private state is a general covering term for opinions, evaluations,
emotions, and speculations.
Subjectivity is expressed by means of linguistic expressions of various types from
words to syntactic devices that are called subjective elements. Subjective statements
are presented from the point of view of someone, who is called the source. As Wiebe
et al (2004) highlight, subjective does not mean not true. For example, in Example (4a),
criticized expresses subjectivity, but the events CRITICIZE and SMOKE are presented as
being true. Not all events contained in subjective statements need to be true, however.
Modal expressions can be used to express subjective language, as in Example (4b),
where the modal cue perhaps combined with the future tense is used to present the event
FORGIVE as non-factual.
(4) a. John criticized Mary for smoking.
b. Perhaps you?ll forgive me for reposting his response.
Modality and evidentiality are grammatical categories, whereas certainty, hedging,
and subjectivity are pragmatic positions, and event factuality is a level of information. In
this special issue we will use the term modality in a broad sense, similar to the extended
modality of Matsuyoshi et al (2010), which they use to refer to ?modality, polarity, and
other associated information of an event mention.? Subjectivity in the general sense
and opinion are beyond the scope of this special issue, however, because research in
these areas focuses on different topics and already has a well defined framework of
reference.
Modality-related phenomena are not rare. According to Light, Qiu, and Srinivasan
(2004), 11% of sentences in MEDLINE contain speculative language. Vincze et al (2008)
report that around 18% of sentences occurring in biomedical abstracts are specula-
tive. Nawaz, Thompson, and Ananiadou (2010) find that around 20% of the events
in a biomedical corpus belong to speculative sentences and that 7% of the events are
expressed with some degree of speculation. Szarvas (2008) notes that a significant
proportion of the gene names mentioned in a corpus of biomedical articles appear in
228
Morante and Sporleder Modality and Negation
speculative sentence (638 occurences out of a total of 1,968). This means that approx-
imately 1 in every 3 genes should be excluded from the interaction detection process.
Rubin (2006) reports that 59% of the sentences in a corpus of 80 articles from The New
York Times were identified as epistemically modalized.
3. Negation
Negation is a complex phenomenon that has been studied from many perspectives, in-
cluding cognition, philosophy, and linguistics. As described by Lawler (2010, page 554),
cognitively, negation ?involves some comparison between a ?real? situation lacking
some particular element and an ?imaginal? situation that does not lack it.? In the logic
formalisms, ?negation is the only significant monadic functor,? whose behavior is de-
scribed by the Law of Contradiction that asserts that no proposition can be both true
and not true. In natural language, negation functions as an operator, like quantifiers and
modals. A main characteristic of operators is that they have a scope, which means that
their meaning affects other elements in the text. The affected elements can be located in
the same clause (5a) or in a previous clause (5b).
(5) a. We didn?t find the book.
b. We thought we would find the book. This was not the case.
The study of negation in philosophy started with Aristotle, but nowadays is still a
topic that generates a considerable number of publications in the field of philosophy,
logic, psycholinguistics, and linguistics. Horn (1989) provides an extensive description
of negation from a historic perspective and an analysis of negation in relation to seman-
tic and pragmatic phenomena. Tottie (1991) studies negation as a grammatical category
from a descriptive and quantitative point of view, based on the analysis of empirical
material. She defines two main types of negation in natural language: rejections of
suggestions and denials of assertions. Denials can be explicit and implicit.
Languages have devices for negating entire propositions (clausal negation) or con-
stituents of clauses (constituent negation). Most languages have several grammatical
devices to express clausal negation, which are used with different purposes like negat-
ing existence, negating facts, or negating different aspects, modes, or speech acts (Payne
1997). As described by Payne (page 282):
. . . a negative clause is one that asserts that some event, situation, or state of affairs
does not hold. Negative clauses usually occur in the context of some presupposition,
functioning to negate or counter-assert that presupposition.
van der Wouden (1997) defines what a negative context is, showing that negation
can be expressed by a variety of grammatical categories. We reproduce some of his
examples in Example (6).
(6) a. Verbs: We want to avoid doing any look-up, if possible.
b. Nouns: The positive degree is expressed by the absence of any
phonic sequence.
c. Adjectives: It is pointless to teach any of the vernacular languages
as a subject in schools.
229
Computational Linguistics Volume 38, Number 2
d. Adverbs: I?ve never come across anyone quite as brainwashed as
your student.
e. Prepositions: You can exchange without any problem.
f. Determiners: This fact has no direct implications for any of the two
methods of font representation.
g. Pronouns: Nobody walks anywhere in Tucson.
h. Complementizers: Leave the door ajar, lest any latecomers should
find themselves shut out.
i. Conjunctions: But neither this article nor any other similar review I
have seen then had the methodological discipline to take the opposite
point of view.
Negation can also be expressed by affixes, as in motionless or unhappy, and by chang-
ing the intonation or facial expression, and it can occur in a variety of syntactic
constructions.
Typical negation problems that persist in the study of negation are determining
the scope when negation occurs with quantifiers (7a), neg-raising (7b), the use of
polarity items (7c) (any, the faintest idea), double or multiple negation (7d), and affixal
negation (Tottie 1991).
(7) a. All the boys didn?t leave.
b. I don?t think he is coming.
c. I didn?t see anything.
d. I don?t know nothing no more.
Like modality, negation is a frequent phenomenon in texts. Tottie reports that nega-
tion is twice as frequent in spoken text (27.6 per 1,000 words) as in written text (12.8 per
1,000 words). Elkin et al (2005) find that 1,823 out of 14,792 concepts in 41 Health
Records from JohnsHopkins University are identified as negated by annotators. Nawaz,
Thompson, and Ananiadou (2010) report that more than 3% of the biomedical events
in 70 abstracts of the GENIA corpus are negated. Councill, McDonald, and Velikovich
(2010) annotate a corpus of product reviews with negation information and they find
that 19% of the sentences contain negations (216 out of 1,135).
3.1 Negation versus Negative Polarity
Negation and negative polarity are interrelated concepts, but it is important to notice
that they are different. Negation has been defined as a grammatical phenomenon used
to state that some event, situation, or state of affairs does not hold, whereas polarity
is a relation between semantic opposites. As Israel (2004, page 701) puts it, ?as such
polarity encompasses not just the logical relation between negative and affirmative
propositions, but also the conceptual relations defining contrary pairs like hot?cold,
long?short, and good?bad.? Israel defines three types of polar oppositions: contradiction,
a relation in which one term must be true and the other false; contrariety, a relation
in which only one term may be true, although both can be false; and reversal, which
230
Morante and Sporleder Modality and Negation
involves an opposition between scales (?necessary, likely, possible? ?impossible, un-
likely, uncertain?.). The relation between negation and polarity lies in the fact that
negation can reverse the polarity of an expression.
In this context, negative polarity items (NPIs) ?are expressions with a limited distri-
bution, part of which includes negative sentences? (Hoeksema 2000, page 115), like any
in Example (8a) or ever in Example (8b). Lawler (2010, page 554) defines NPI as ?a term
applied to lexical items, fixed phrases, or syntactic construction types that demonstrate
unusual behavior around negation.? NPIs felicitously occur only in the scope of some
negative element, such as didn?t in Example (8b). If this element is removed, the sentence
becomes agrammatical, as shown in Example (8c). The presence of an NPI in a context
does not guarantee that something is being negated, however, because NPIs can also
occur in certain grammatical circumstances, like interrogatives as in Example (8d).
(8) a. I didn?t read any book.
b. He didn?t ever read the book.
c. * He ever read the book.
d. Do you think I could ever read this book?
Polarity is a discrete category that can take two values: positive and negative. De-
termining the polarity of words, and phrases is a central task in sentiment analysis,
in particular, disambiguating the contextual polarity of words (Wilson, Wiebe, and
Hoffman 2009). Thus, in the context of sentiment analysis positive and negative polarity
refers to positive and negative opinions, emotions, and evaluations.
Negation is a topic of study in sentiment analysis because it is what Wilson,
Wiebe, and Hoffman (2009, page 402) call a polarity influencer, an element that can
change the polarity of an expression. As they put it, however, ?many things besides
negation can influence contextual polarity, and even negation is not always straight-
forward.? We discuss different ways of modeling negation in sentiment analysis in
Section 8. The study of negative polarity is beyond the scope of this special issue,
however.
4. Categorizing and Annotating Modality and Negation
Over the last few years, several corpora of texts from various domains have been
annotated at different levels (expression, event, relation, sentence) with information
related to modality and negation. Compared to other phenomena like semantic ar-
gument structure, dialogue acts, or discourse relations, however, no comprehensive
annotation standard has been defined for modality and negation. In this section, we
describe the categorization schemes that have been proposed and the corpora that have
been annotated.
In the framework of the OntoSem project (Nirenburg and Raskin 2004) a corpus
has been annotated with modality categories and an analyzer has been developed
that takes as input unrestricted raw text and carries out several levels of linguistic
analysis, including modality at the semantic level (Nirenburg and McShane 2008).
The output of the semantic analysis is represented as formal text-meaning represen-
tations. Modality information is encoded as part of the semantic module in the lexical
entries of the modality cues. Four modality attributes are encoded: MODALITY TYPE,
231
Computational Linguistics Volume 38, Number 2
VALUE, SCOPE, and ATTRIBUTED-TO. The MODALITY TYPES are: polarity, whether a
proposition is positive or negated; volition, the extent to which someone wants or
does not want the event/state to occur; obligation, the extent to which someone
considers the event/state to be necessary; belief, the extent to which someone be-
lieves the content of the proposition; potential, the extent to which someone believes
that the event/state is possible; permission, the extent to which someone believes that
the event/state is permitted; and evaluative, the extent to which someone believes
the event/state is a good thing. The SCALAR VALUE ranges from zero to one. The
SCOPE attribute is the predicate that is affected by the modality and the ATTRIBUTED-
TO attribute indicates to whom the modality is assigned, the default value being
the speaker. In Example (9), should is identified as a modality cue and character-
ized with the type obligative, value 0.8, scope camouflage, and is attributed to the
speaker.
(9) Entrance to the tower should be totally camouflaged
The publicly available MPQA Opinion Corpus1 (Wiebe, Wilson, and Cardie 2005)
contains 10,657 sentences in 535 documents of English newswire annotated with in-
formation about private states at the word and phrase level. For every expression of
private state a private state frame is defined indicating the SOURCE of the private state,
whose private state is being expressed; the TARGET, what the private state is about;
and properties like INTENSITY, SIGNIFICANCE, and TYPE OF ATTITUDE. Three types of
private state expressions are considered for the annotation: explicit mentions like fears
in Example (10a), speech events like said in Example (10b), and expressive subjective
elements, like full of absurdities in Example (10b). Apart from representing private states
in private state frames, Wiebe, Wilson, and Cardie also define objective speech event
frames that represent ?material that is attributed to some source, but is presented as
an objective fact? (page 171). Having two types of frames allows a distinction between
opinion-oriented material (10a, 10b) and factual material (10c).
(10) a. ?The U.S. fears a spill-over,? said Xirao-Nima.
b. ?The report is full of absurdities,? Xirao-Nima said.
c. Sergeant O?Leary said the incident took place at 2:00 pm.
Rubin, Liddy, and Kando (2005) define a model for categorizing certainty. The
model distinguishes four dimensions: LEVEL, which encodes the degree of certainty;
PERSPECTIVE, which encodes whose certainty is involved; FOCUS, the object of certainty;
and TIME, which encodes at what time the certainty is expressed. Each dimension is
further subdivided into categories, resulting in 72 possible dimension?category combi-
nations. The four certainty LEVELS are absolute (Example (11a)), high (Example (11b)),
moderate (Example (11c)), and low (Example (11d)). PERSPECTIVE separates the writer?s
point of view and the reported point of view. FOCUS is divided into abstract and fac-
tual information. TIME can be past, present, or future. The model is used to annotate
certainty markers in 32 articles from The New York Times along these dimensions. Rubin
1 The MPQA corpus is available from http://www.cs.pitt.edu/mpqa/mpqa corpus.html. Last accessed
on 8 December 2011.
232
Morante and Sporleder Modality and Negation
et al find that editorials have a higher frequency of modality markers per sentence than
news stories.
(11) a. An enduring lesson of the Reagan years, of course, is that it really does
take smoke and mirrors to produce tax cuts, spending initiatives and a
balanced budget at the same time.
b. ... but clearly an opportunity is at hand for the rest of the world to
pressure both sides to devise a lasting peace based on democratic values
and respect for human rights.
c. That fear now seems exaggerated, but it was not entirely fanciful.
d. So far the presidential candidates are more interested in talking about
what a surplusmight buy than in the painful choices that lie ahead.
The model is adapted in Rubin (2006, 2007) by adding a category uncertainty for
certainty LEVEL, changing the FOCUS categories into facts and events and opin-
ions, emotions, or judgements, and adding the irrelevant category for TIME. Inter-
annotator agreement measures are reported for 20 articles of the 80 annotated articles
randomly selected from The New York Times (Rubin 2006). For the task of deciding
whether a statement wasmodalized by an explicit certaintymarker or not, an agreement
of 0.33 ?cohen is reached. The agreement measures per dimension were 0.15 for level,
0.13 for focus, 0.44 for perspective, and 0.41 for time.
The Automatic Content Extraction 2008 corpus (Linguistic Data Consortium 2008)
for relation detection and recognition collects English and Arabic texts from a variety
of resources including radio and TV broadcast news, talk shows, newswire articles,
Internet news groups, Web logs, and conversational telephone speech. Relations are
ordered pairs of entities and are annotated with modality and tense attributes. The two
modality attributes are asserted and other. Asserted relations pertain to situations in
the real world, whereas other relations pertain to situations in ?some other world de-
fined by counterfactual constraints elsewhere in the context.? If the entities constituting
the arguments of a relation are hypothetical, then the relation can still be understood
as asserted. In Example (12), the ORG-Aff.Membership relation between terrorists and
Al-Qaeda is annotated as asserted and the Physical.Located relation between terrorists
and Baghdad is annotated as other. The attributes for TENSE are past, future, present,
and unspecified.
(12) We are afraid Al-Qaeda terrorists will be in Baghdad.
The Penn Discourse TreeBank (Prasad et al 2008) is a corpus annotated with in-
formation related to discourse structure. Discourse connectives are considered to be
the anchors of discourse relations and to act as predicates taking two abstract objects.
Abstract objects can be assertions, beliefs, facts, or eventualities. Discourse connectives
and their arguments are assigned attribution-related features (Prasad et al 2006) such as
SOURCE (writer, other, arbitrary), TYPE (reflecting the nature of the relation between
the agent and the abstract object), SCOPAL POLARITY of attribution, and DETERMINACY
(indicating the presence of contexts canceling the entailment of attribution). The text
spans signaling the attribution are also marked. Prasad et al (2006) report that 34% of
the discourse relations have some non-writer agent. SCOPAL POLARITY is annotated
to identify cases when verbs of attribution (say, think, ...) are negated syntactically
233
Computational Linguistics Volume 38, Number 2
(didn?t say) or lexically (denied). An argument of a connective is marked Neg for SCOPAL
POLARITY when the interpretation of the connective requires the surface negation to
take semantic scope over the lower argument. As stated by Prasad et al, in Example (13),
the but clause entails an interpretation such as ?I think it?s not a main consideration,?
for which the negation must take narrow scope over the embedded clause rather than
the higher clause.
(13) ?Having the dividend increases is a supportive element in the market
outlook, but I don?t think it?s a main consideration,? he says.
TimeML (Pustejovsky et al 2005) is a specification language for events and temporal
expressions in natural language that has been applied to the annotation of corpora like
TimeBank (Pustejovsky et al 2006). As described in Saur??, Verhagen, and Pustejovsky
(2006), TimeML encodes different types of modality at the lexical and syntactic level
with different tags. At the lexical level, Situation Selecting Predicates (SSPs) are en-
coded by means of the attribute CLASS within the EVENT tag, which allows to encode
the difference between SSPs that are actions (Example (14a)2) and SSPs that are states
(Example (14b)). SSPs of perception (Example (14c)) and reporting (Example (14d)) are
encoded with more specific values due to their role in providing evidentiality. Informa-
tion about modal auxiliaries and negative polarity, which are also lexically expressed,
is encoded in the attributes MODALITY and POLARITY. Modality at the syntactic level
is encoded as an attribute of the tag SLINK (Subordination Link), which can have
several values: factive, counterfactive, evidential, negative evidential, modal,
and conditional.
(14) a. Companies such as Microsoft or a combined worldcom MCI are trying
to monopolize Internet access.
b. Analysts also suspect suppliers have fallen victim to their own success.
c. Some neighbors told Birmingham police they saw a man running.
d. No injuries were reported over the weekend.
FactBank (Saur?? and Pustejovsky 2009) is a corpus of events annotated with factu-
ality information, which adds to the TimeBank corpus an additional level of semantic
information. Events are annotated with a discrete set of factuality values using a bat-
tery of criteria that allow annotators to differentiate among these values. It consists
of 208 documents that contain 9,488 annotated events. The categorization model is
based on Horn?s (1989) analysis of epistemic modality in terms of scalar predication.
For epistemic modality Horn proposes the scale ?certain, {probable/likely}, possible?.
For the negative counterpart he proposes the scale ?uncertain, {unlikely/improbable},
impossible?. Saur?? and Pustejovsky map this system into the traditional Square of
Opposition (Parsons 2008), which originated with Aristotle. The resulting degrees
of factuality defined in FactBank are the following: fact, counterfact, probable,
not probable, possible, not certain, certain but unknown output, and unknown
or uncommitted. An example of the certain but unknown output value is shown in
Example (15) for the event COME, and examples of the unknown or uncommitted value
2 The event affected by the SSP is underlined.
234
Morante and Sporleder Modality and Negation
for the same event are found in Example (16). Discriminatory co-predication tests are
provided for the annotators to determine the factuality of events. The interannotator
agreement reported for assigning factuality values is ?cohen 0.81.
(15) John knows whether Mary came.
(16) a. John does not know whether Mary came.
b. John does not know that Mary came.
c. John knows that Paul said that Mary came.
A corpus of 50,108 event mentions in blogs and Web posts in Japanese has been
annotated with information about extended modality (Matsuyoshi et al 2010). The an-
notation scheme of extended modality is based on four desiderata: information should
be assigned to the event mention; the modality system has to be language independent;
polarity should be divided into two classes: POLARITY ON THE ACTUALITY of the
event and SUBJECTIVE POLARITY from the perspective of the source?s evaluation; and
the annotation labels should not be too fine-grained. In Example (17) the polarity on
actuality is negative for the events STUDY and PASS because they did not occur, but the
subjective polarity for the PASS event is positive. Extended modality is characterized
along seven components: SOURCE, indicating who expresses an attitude towards the
event; TIME, future or non future; CONDITIONAL, whether a target event mention is a
proposition with a condition; PRIMARY MODALITY TYPE, determining the fundamental
meaning of the event mention (assertion, volition, wish, imperative, permission, in-
terrogative); ACTUALITY, degree of certainty; EVALUATION, subjective polarity, which
can be positive, negative, or neutral; and FOCUS, what aspect of the event is the focus
of negation, inference, or interrogation. Reported inter-annotator agreement for two
annotators on 300 event mentions ranges from 0.69 to 0.76 ?cohen depending on the
category.
(17) If I had studied mathematics harder, I could have passed the examination.
A publicly available modality lexicon3 has been developed by Baker et al (2010)
in order to automatically annotate a corpus with modality information. This lexicon
contains modal cues related to factivity. The lexicon entries consist of five components:
the cue sequence of words, part-of-speech (PoS) for each word, a modality type, a head
word, and one or more subcategorization codes. Three components are identified in
sentences that contain a modality cue: the TRIGGER is the word or sequence of words
that expresses modality; the TARGET is the event, state, or relation that the modality
scopes over; and the HOLDER is the experiencer or cognizer of themodality. This scheme
distinguishes eight modalities: requirement (does H require P?), permissive (does H
allow P?), success (does H succeed in P?), effort (does H try to do P?), intention (does
H intend P?), ability (can H do P?), want (does H want P?), and belief (with what
strength does H believe P?). The annotation guidelines to annotate the modalities are
defined in Baker et al (2009).
3 Web site of the modality lexicon: http://www.umiacs.umd.edu/?bonnie/ModalityLexicon.txt.
Last accessed on 8 December 2011.
235
Computational Linguistics Volume 38, Number 2
The scope of negation has been annotated on a corpus of Conan Doyle stories
(Morante, Schrauwen, and Daelemans 2011)4 (The Hound of the Baskervilles and The
Adventure of Wisteria Lodge), which have also been annotated with coreference and
semantic roles for the SemEval Task Linking Events and Their Participants in Discourse
(Ruppenhofer et al 2010). As for negation, the corpus is annotated with negation cues
and their scope in a way similar to the BioScope corpus (Vincze et al 2008) described
subsequently, and in addition negated events are also marked, if they occur in fac-
tual statements. Blanco and Moldovan (2011) take a different approach by annotating
the focus, ?that part of the scope that is most prominently or explicitly negated,? in
the 3,993 verbal negations signaled with MNEG in the PropBank corpus. According to the
authors, the annotation of the focus allows the derivation of the implicit positive mean-
ing of negated statements. For example, in Example (18) the focus of the negation is on
until 2008, and the implicit positive meaning is ?They released the UFO files in 2008.?
(18) They didn?t release the UFO files until 2008.
The corpora and categorization schemes described here reflect research focusing on
general-domain texts. With the growth of research on biomedical text mining, annota-
tion of modality phenomena in biomedical texts has become central. Scientific language
makes use of speculation and hedging to express lack of definite belief. Light, Qiu, and
Srinivasan (2004) are pioneers in analyzing the use of speculative language in scientific
texts. They study the expression of levels of belief in MEDLINE abstracts by means
of hypotheses, tentative conclusions, hedges, and speculations, and annotate a corpus
of abstracts in order to check whether the distinctions between high speculative, low
speculative, and definite sentences could be made reliably. Their findings suggest that
the speculative versus definite distinction is reliable while the distinction between low
and high speculative is not.
The annotation work by Wilbur, Rzhetsky, and Shatkay (2006) is motivated by the
need to identify and characterize parts of scientific documents where reliable infor-
mation can be found. They define five dimensions to characterize scientific sentences:
FOCUS (scientific versus general), POLARITY (positive versus negative statement),
LEVEL OF CERTAINTY in the range 0?3, STRENGTH of evidence, and DIRECTION/TREND
(increase or decrease in certain measurement).
A corpus5 of six articles from the functional genomics literature has been anno-
tated at the sentence level for speculation (Medlock and Briscoe 2007). Sentences are
annotated as being speculative or not. Of the 1,157 sentences, 380 were found to be
speculative. An inter-annotator agreement of 0.93 ?cohen is reported.
BioInfer (Pyysalo et al 2007) is a corpus of 1,100 sentences from abstracts of
biomedical research articles annotated with protein, gene, and RNA relationships. The
annotation scheme captures information about the absence of a relation. Statements
expressing absence of a relation such as not affected by or independent of are annotated
using a predicate NOT, as in this example: not:NOT(affect:AFFECT(deletion of SIR3,
silencing)).
The Genia Event corpus (Kim, Ohta, and Tsujii 2008) contains 9,372 sentences where
biological events are annotated with negation and uncertainty. In the case of negation,
4 Web site of the Conan Doyle corpus: http://www.clips.ua.ac.be/BiographTA/corpora.html.
Last accessed on 8 December 2011.
5 The Medlock and Briscoe corpus is available from http://www.benmedlock.co.uk/hedgeclassif.html.
Last accessed on 8 December 2011.
236
Morante and Sporleder Modality and Negation
events are marked with the label exists or non-exists. In the case of uncertainty,
events are labeled into three categories: certain, which is chosen by default; probable,
if the event existence cannot be stated with certainty; and doubtful, if the event is under
investigation or forms part of a hypothesis. Linguistic cues are not annotated.
The BioScope corpus (Vincze et al 2008) is a freely available resource6 that gathers
medical and biological texts. It consists of three parts: clinical free-texts (radiology
reports), full-text biological articles, and biological article abstracts from the GENIA
corpus (Collier et al 1999). In total it contains 20,000 sentences. Instances of negative
and speculative language are annotated with information about the linguistic cues that
express them and their scope. Negation is understood as the implication of the non-
existence of something as in Example (19a). Speculative statements express the possible
existence of something as in Example (19b). The scope of a keyword is determined by
syntax and it is extended to the largest syntactic unit to the right of the cue, including all
the complements and adjuncts of verbs and auxiliaries. The inter-annotator agreement
rate for scopes is defined as the F-measure of one annotation, treating the second
one as the gold standard. It ranges from 62.50 for speculation in full articles to 92.46
for negation in abstracts. All agreement measures are lower for speculation than for
negation. The BioScope corpus has been provided as a training corpus for the biological
track of the 2010 edition of the CoNLL Shared Task on Learning to Detect Hedges and their
Scope in Natural Language Text (Farkas et al 2010b). The additional test files provided in
the Shared Task are annotated in the same way.
(19) a. Mildly hyperinflated lungs [without focal opacity].
b. This result [suggests that the valency of Bi in the material is smaller
than +3].
Because the Genia Event and BioScope corpus share 958 abstracts, it is possible to
compare their annotations, as it is done by Vincze et al (2010). Their study shows that
the scopes of BioScope are not directly useful to detect the certainty status of the events
in Genia, and that the BioScope annotation is more easily adaptable to non-biomedical
applications. A description of negation cues and their scope in biomedical texts, based
on the cues that occur in the BioScope corpus, can be found in Morante (2010), where
information is provided relative to the ambiguity of the negation cue and to the type of
scope, as well as examples. The description shows that the scope depends mostly on the
PoS of the cue and on the syntactic features of the clause.
The NaCTeM team has annotated events in biomedical texts with meta-knowledge
that includes polarity andmodality (Thompson et al 2008). Themodality categorization
scheme covers epistemic modality and speculation and contains information about the
following dimensions: KNOWLEDGE TYPE, LEVEL OF CERTAINTY, and POINT OF VIEW.
Four types of knowledge are defined, three of which are based on Palmer?s (1986) clas-
sification of epistemic modality: speculative, deductive, sensory, and experimental
results or findings. The levels of certainty are four: absolute, high, moderate, and
low. The possible values for POINT OF VIEW are writer and other. An updated ver-
sion of the meta-knowledge annotation scheme is presented by Nawaz, Thompson,
and Ananiadou (2010). The scheme consists of six dimensions: KNOWLEDGE TYPE,
certainty level, source, lexical polarity, manner, and LOGICAL TYPE. Three
6 The BioScope corpus is available from http://www.inf.u-szeged.hu/rgai/bioscope. Last accessed on
8 December 2011.
237
Computational Linguistics Volume 38, Number 2
levels of certainty are defined: low confidence or considerable speculation, high
confidence or slight speculation, and no expression of uncertainty or speculation.
Information about negation is encoded in the LEXICAL POLARITY dimension, which
identifies negated events. Negation is defined here as ?the absence or non-existence of
an entity or a process.?
For languages other than English there are much fewer resources. A corpus of 6,740
sentences from the Stockholm Electronic Patient Record Corpus (Dalianis and Velupillai
2010) has been annotated with certain and uncertain expressions as well as speculative
and negation cues, with the purpose of creating a resource for the development of au-
tomatic detection of speculative language in Swedish clinical text. The categories used
are: certain, uncertain, and undefined at sentence level, and negation, speculative
words, and undefined speculative words at token level. Inter-annotator agreement
for certain sentences and negation are high, but for the rest of the classes results are
lower.
5. Detection of Speculative Sentences
Initial work on processing speculation focuses on classifying sentences as specula-
tive or definite (non-speculative), depending on whether they contain speculation
cues.
Light, Qiu, and Srinivasan (2004) explore the ability of a Support Vector Machine
(SVM) classifier to perform this task on a corpus of biomedical abstracts using a stem-
ming representation. The results of the system are compared to a majority decision
baseline and to a substring matching baseline produced by classifying as speculative
sentences which contain the following strings: suggest, potential, likely, may, at least,
in part, possible, potential, further investigation, unlikely, putative, insights, point toward,
promise, and propose. The precision results are higher for the SVM classifier (84% com-
pared with 55% for the substring matching method), but the recall results are higher for
the substring matching method (79% compared with 39% for the SVM classifier).
Medlock and Briscoe (2007) model hedge classification as a weakly supervised
machine learning task performed on articles from the functional genomics literature.
They develop a probabilistic learner to acquire training data, which returns a labeled
data set from which a probabilistic classifier is trained. The training corpus consists of
300,000 randomly selected sentences; the manually annotated test corpus consists of
six full articles.7 Their classifier obtains 0.76 BEP (Break Even Point), outperforming
baseline results obtained with a substring matching technique. Error analysis shows
that the system has problems distinguishing between a speculative assertion and one
relating to a pattern of observed non-universal behavior, like Example (20), which is
wrongly classified as speculative.
(20) Each component consists of a set of subcomponents that can be localized
within a larger distributed neural system.
Medlock (2008) presents an extension of this work by experimenting with more fea-
tures (PoS, stems, and bigrams). Experiments show that although the PoS representation
7 The Drosophila melanogaster corpus is available at http://www.benmedlock.co.uk/hedgeclassif.html.
Last accessed on 8 December 2011.
238
Morante and Sporleder Modality and Negation
does not yield significant improvement over the results in Medlock and Briscoe (2007),
the system achieves a weakly significant improvement with a stemming representation.
The best results are obtained with a combination of stems and adjacent stem bigrams
representation (0.82 BEP).
Following Medlock and Briscoe (2007), Szarvas (2008) develops a Maximum En-
tropy classifier that incorporates bigrams and trigrams in the feature representation
and performs a reranking based feature selection procedure that allows a reduction
of the number of keyword candidates from 2,407 to 253. The system is trained on the
data set of Medlock and Briscoe and evaluated on four newly annotated biomedical
full articles8 and on radiology reports. The best results of the system are achieved
by performing automatic and manual feature selection consecutively and by adding
external dictionaries. The final results on biomedical articles are 85.29 BEP and 85.08
F1 score. The results for the external corpus of radiology reports are lower, at 82.07
F1 score.
A different type of system is presented by Kilicoglu and Bergler (2008), who apply
a linguistically motivated approach to the same classification task by using knowledge
from existing lexical resources and incorporating syntactic patterns, including un-
hedgers, lexical cues, and patterns that strongly suggest non-speculation. Additionally,
hedge cues are weighted by automatically assigning an information gain measure to
them and by assigning weights semi-automatically based on their types and centrality
to hedging. The hypothesis behind this approach is that ?a more linguistically oriented
approach can enhance recognition of speculative language.? The results are evaluated
on the Drosophila data set from Medlock and Briscoe (2007) and the four annotated
BMC Bioinformatics articles from Szarvas (2008). The best results on the Drosophila data
set are obtained with the semi-automatic weighting scheme, which achieves a compet-
itive BEP of 0.85. The best results on the BMC Bioinformatics articles are obtained also
with semi-automatic weighting yielding a BEP of 0.82 improving over previous results.
According to Kilicoglu and Bergler, the best results of the semi-automatic weighting
scheme are due to the fact that the scheme relies on the particular semantic properties
of the hedging indicators. The relatively stable results of the semi-automatic weighting
scheme across data sets could indicate that this scheme is more generalizable than one
based on machine learning techniques. The false negatives are due to missing syntactic
patterns and to certain derivational forms of epistemic words (suggest?suggestive)
that are not identified. False positives are due to word sense ambiguity of hedging
cues like could and appear, and to weak hedging cues like epistemic deductive verbs
(conclude, estimate), some adverbs (essentially, usually), and nominalizations (implication,
assumption).
A different task is introduced by Shatkay et al (2008). The task consists of classifying
sentence fragments from biomedical texts along five dimensions, two of which are
CERTAINTY (four levels) and POLARITY (negated or not). Fragments are individual
statements in the sentences as exemplified in Example (21). For certainty level, the
feature vector represents single words, bigrams, and trigrams; for polarity detection,
it represents single words and syntactic phrases. They perform a binary classifica-
tion per class using SVMs. Results on polarity classification are 1.0 F-measure for the
positive class and 0.95 for the negative class, and results on level of certainty vary
8 The four annotated BMC Bioinformatics articles are available at http://www.inf.u-szeged.hu/?szarvas/
homepage/hedge.html. Last accessed on 8 December 2011.
239
Computational Linguistics Volume 38, Number 2
from 0.99 F-measure for level 3, which is the majority class, to 0.46 F-measure for
level 2.
(21) ?fragment 1We demonstrate that ICG-001 binds specifically to CBP?
?fragment 2 but not the related transcriptional coactivator p3000?
Ganter and Strube (2009) introduce a new domain of analysis. They develop a
system for automatic detection of Wikipedia sentences that contain weasel words, as
in Example (22). Weasel words are ?words and phrases aimed at creating an impression
that something specific and meaningful has been said, when in fact only a vague or
ambiguous claim has been communicated.?9 As Ganter and Strube indicate, weasel
words are closely related to hedges and private states. Wikipedia editors are advised
to avoid weasel words because they ?help to obscure the meaning of biased expressions
and are therefore dishonest.?10
(22) a. Others argue {{weasel-inline}} that the news media are simply catering
to public demand.
b. ... therefore America is viewed by some {{weasel-inline}} technology
planners as falling further behind Europe.
Ganter and Strube experiment with two classifiers, one based on words preceding
the weasel and another one based on syntactic patterns. The similar results (around
0.70 BEP) of the two classifiers show that word frequency and distance to the weasel tag
provide sufficient information. The classifier that uses syntactic patterns outperforms
the classifier based on words on data manually re-annotated by the authors, however,
suggesting that the syntactic patterns detect weasel words that have not yet been
tagged.
Classification of uncertain sentences was consolidated as a task with the 2010
edition of the CoNLL Shared Task on Learning to Detect Hedges and their Scope in Nat-
ural Language Text (Farkas et al 2010b), where Task 1 consisted in detecting uncertain
sentences. Systems were required to perform a binary classification task on two types
of data: biological abstracts and full articles, and paragraphs fromWikipedia. As Farkas
et al describe, the approaches to solving the task follow two major directions: Some
systems handle the task as a classical sentence disambiguation problem and apply a
bag-of-words approach, and other systems focus on identifying speculation cues, so
that sentences containing cues would be classified as uncertain. In this second group
some systems apply a token-based classification approach and others use sequential
labeling. The typical feature set for Task 1 includes the wordform, lemma or stem, PoS
and chunk codes; and some systems incorporate features from the dependency and/or
constituent parse tree of the sentences. The evaluation of Task 1 is performed at the
sentence level using the F1 score of the uncertain class. The scores for precision are
higher than for recall, and systems are ranked in different positions for each of the data
sets, which suggests that the systems are optimized for one of the data types. The top-
ranked systems for biological data follow a sequence labeling approach, whereas the
9 Definition of weasel words in Wikipedia: http://en.wikipedia.org/wiki/Weasel word. Last accessed
on 8 December 2011.
10 Wikipedia instructions about weasel words are available at http://simple.wikipedia.org/wiki/
Wikipedia:Avoid weasel words. Last accessed on 8 December 2011.
240
Morante and Sporleder Modality and Negation
top-ranked systems for Wikipedia data follow a bag-of-words approach. None of the
top-ranked systems uses features derived from syntactic parsing. The best system for
Wikipedia data (Georgescul 2010) implements an SVM and obtains an F1 score of 60.2,
whereas the best system for biological data (Tang et al 2010) incorporates conditional
random fields (CRF) and obtains an F1 score of 86.4.
As a follow-up of the CoNLL Shared Task, Velldal (2011) proposes to handle the
hedge detection task as a simple disambiguation problem, restricted to the words that
have previously been observed as hedge cues. This reduces the number of examples
that need to be considered and the relevant feature space. Velldal develops a large-
margin SVM classifier based on simple sequence-oriented n-gram features collected for
PoS-tags, lemmas, and surface forms. This system produces better results (86.64 F1) than
the best system of the CoNLL Shared Task (Tang et al 2010).
From the research presented in this section it seems that classifying sentences as
to whether they are speculative or not can be performed by using knowledge-poor
machine learning approaches as well as by linguistically motivated methods. It would
be interesting to determine whether a combination of both approaches would yield
better results. In the machine learning approaches the features used to solve this task
are mainly shallow features such as words, bigrams, and trigrams. Syntax features do
not seem to add new information, although a linguistically informed method based on
syntactic patterns can produce similar results to machine-learning approaches based
on shallow features. Hedge cues are ambiguous and domain dependent, reducing the
portability of hedge classifiers. It has also been shown that it is feasible to build a hedge
classifier in an unsupervised manner.
6. Event-Level Detection of Modality and Negation
Althoughmodality and negation detection at the sentence level can be useful for certain
purposes, it is often the case that not all the information contained in a sentence is
affected by the presence of modality and negation cues. Modality and negation cues are
operators that have a scope and only the part of the sentence within the scope will be
affected by them. For example, the sentence in Example (23a)11 would be classified as
speculative in a sentence-level classification task, despite the fact that the cue unlikely
scopes only over the clause headed by the event PRODUCE. In Example (23b) the
negation cue scopes over the subject of led, assigning negative polarity to the event
COPE WITH, but not to the rest of the events.
(23) a. He is now an enthusiastic proponent of austerity and reform but this
has lost him voters and [was unlikely to produce sufficient growth, or
jobs, to win him new ones by next spring].
b. Its [inability to cope with file-sharing] led to the collapse of
recorded-music sales and the growing dependence on live music.
Research focusing on determining the scope of cues has revolved around two types
of tasks: finding the events and concepts that are negated or speculated, and resolving
the full scope of cues. Sections 6.1 and 6.2 describe them in detail.
11 The two examples are sentences from articles in The Economist.
241
Computational Linguistics Volume 38, Number 2
6.1 Finding Speculated and Negated Events and Entities
Research on finding negated concepts originated in the medical domain motivated by
the need to index, extract, and encode clinical information that can be useful for patient
care, education, and biomedical studies. In order to automatically process information
contained in clinical reports it is of great importance to determine whether symptoms,
signs, treatments, outcomes, or any other clinical relevant factors are present or not. As
Elkin et al (2005) state, ?erroneous assignment of negation can lead to missing allergies
and other important health data that can negatively impact patient safety.? Chapman
et al (2001a) point out that accurate indexing of reports requires differentiating perti-
nent negatives from positive conditions. Pertinent negatives are ?findings and diseases
explicitly or implicitly described as absent in a patient.?
The first systems developed to find negated concepts in clinical reports are rule-
based and use lexical information. NegExpander (Aronow, Fangfang, and Croft 1999)
is a module of a health record classification system. It adds a negation prefix to the
negated tokens in order to differentiate between a concept and its negated variant.
Negfinder (Mutalik, Deshpande, and Nadkarni 2001) finds negated patterns in dictated
medical documents. It is a pipeline system that works in three steps: concept finding
to identify UMLS concepts; input transformation to replace every instance of a concept
with a coded representation; and a lexing/parsing step to identify negations, negation
patterns, and negation terminators. In this system negation is defined as ?words imply-
ing the total absence of a concept or thing in the current situation.? Some phenomena are
identified as difficulties for the system: The fact that negation cues can be single words
or complex verb phrases like could not be currently identified; verbs that when preceded
by not negate their subject, as in X is not seen; and the fact that a single negation cue can
scope over several concepts (A, B, and C are absent) or over some but not all of them (there
is no A, B and C, but D seemed normal). Elkin et al (2005) describe a rule-based system that
assigns a level of certainty to concepts in electronic health records. Negation assignment
is performed by the automated negation assignment grammar as part of the rule based
system that decides whether a concept has been positively, negatively, or uncertainly
asserted.
Chapman et al (2001a, 2001b) developed NegEx,12 a regular expression based algo-
rithm for determining whether a finding or disease mentioned within narrative medical
reports is present or absent. The system uses information about negation phrases that
are divided in two groups: pseudo-negation phrases that seem to indicate negation, but
instead identify double negatives (not ruled out), and phrases that are used as negations
when they occur before or after Unified Medical Language System terms. The precision
of the system is 84% and the recall 78%. Among the system?s weaknesses, the authors
report detecting the scope of not and no. In the three examples in (24a?c) the system
would find that infection is negated. In Example (24d) edemawould be found as negated,
and in Example (24e) cva also. NegEx has been also adapted to process Swedish clinical
text (Skeppstedt 2010).
(24) a. This is not the source of the infection.
b. We did not treat the infection.
c. We did not detect an infection.
12 Web site of NegEx: http://code.google.com/p/negex/. Last accessed on 8 December 2011.
242
Morante and Sporleder Modality and Negation
d. No cyanosis and positive edema.
e.No history of previous cva.
ConText (Harkema et al 2009) is an extension of NegEx. This system uses also
regular expressions and contextual information in order to determine whether clin-
ical conditions mentioned in clinical reports are negated, hypothetical, historical, or
experienced by someone other than the patient. As for negation, a term is negated if
it falls within the scope of a negation cue. In this approach, the scope of a cue extends
to the right of the cue and ends in a termination term or at the end of the sentence. The
system is evaluated on six different types of reports obtaining an average precision of
94% and average recall of 92%. Harkema et al find that negation cues have the same
interpretation across report types.
The systems described here cannot determine correctly the scope of negation cues
when the concept is separated by multiple words from the cue. This motivated Huang
and Lowe (2007) to build a system based on syntax information. Negated phrases are lo-
cated within a parse tree by combining regular expression matching and a grammatical
approach. To construct the negation grammar, the authors manually identify sentences
with negations in 30 radiology reports andmark up negation cues, negated phrases, and
negation patterns. The system achieves a precision of 98.6% and a recall of 92.6%. The
limitations of this system are related to the comprehensiveness of a manually derived
grammar and to the performance of the parser.
Apart from rule-based systems, machine learning techniques have also been ap-
plied to find negated and speculated concepts. Goldin and Chapman (2003) experiment
with Na??ve Bayes and decision trees to determine whether a medical observation is
negated by theword not in a corpus of hospital reports. The F-measure of both classifiers
is similar, 89% and 90%, but Na??ve Bayes gets a higher precision and the decision tree a
higher recall. Averbuch et al (2004) develop an Information Gain algorithm for learning
negative context patterns in discharge summaries and measure the effect of context
identification on the performance of medical information retrieval. 4,129 documents
are annotated with appearances of certain terms, which are annotated as positive or
negative, as in Example (25).
(25) a. The patient presented with episodes of nausea and vomiting associated
with epigastric pain for the past 2 weeks. POSITIVE
b. The patient was able to tolerate food without nausea or vomiting.
NEGATIVE
Their algorithm scores 97.47 F1. It selects certain items as indicators of negative context
(any, changes in, changes, denies, had no, negative for, of systems, was no, without), but it
does not select no and not. As Averbuch et al (2004, page 284) put it, ?Apparently, the
mere presence of the word ?no? or ?not? is not sufficient to indicate negation.? The
authors point out five sources of errors: coordinate clauses with but, as in Example (26a)
where weight loss is predicted as negative; future reference, as in Example (26b), where
the symptoms were predicted as positive; negation indicating existence, as in Exam-
ple (26c), where nausea is predicted as negative; positive adjectives, as in Example (26d),
where appetite andweight loss are predicted as negative; andwrong sentence boundaries.
(26) a. There were no acute changes, but she did have a 50 pound weight loss.
243
Computational Linguistics Volume 38, Number 2
b. The patient was given clear instructions to call for any worsening pain,
fever, chills, bleeding.
c. The patient could not tolerate the nausea and vomiting associated with
Carboplatininal Pain.
d. There were no fevers, headache, or dizziness at home and no diffuse
abdominal pain, fair appetite with significant weight loss.
Rokach, Romano, and Maimon (2008) present a pattern-based algorithm for iden-
tifying context in free-text medical narratives. The algorithm automatically learns
patterns similar to the manually written patterns for negation detection using two
algorithms: longest common sequence and Teiresias (Rigoutsos and Floratos 1998),
an algorithm designed to discover motifs in biological sequences. A non-ranker filter
feature selection algorithm is applied to select the informative patterns (35 out of 2,225).
In the classification phase three classifiers are combined sequentially, each learning
different types of patterns. Experimental results show that the sequential combination
of decision tree classifiers obtains 95.9 F-measure, outperforming the results of single
hiddenMankovmodels and CRF classifiers based on several versions of a bag-of-words
representation.
Goryachev et al (2006) compare the performance of four different methods of nega-
tion detection, two regular expression basedmethods that are adaptations of NegEx and
NegExpander, and two classification-based methods, Na??ve Bayes and SVM, trained
on 1,745 discharge reports. They find that the regular expression-based methods show
better agreement with humans and better accuracy than the classification methods.
Goryachev et al indicate that the reason why the classifiers do not perform as well
as NegEx and NegExpander may be related to the fact that the classifiers are trained on
discharge summaries and tested on outpatient notes.
Another comparison of approaches to assertion classification is made by Uzuner,
Zhang, and Sibanda (2009), who develop a statistical assertion classifier, StAC, to
classify medical problems in patient records into four categories: positive, negative,
uncertain, and alter-association13 assertions. The StAC approach makes use of
lexical and syntactic context in conjunction with SVM. It is evaluated on discharge
summaries and on radiology reports. The comparison with an extended version of the
NegEx algorithm (ENegEx), adapted to capture alter-association in addition to positive,
negative, and uncertain assertions, shows a better performance of the statistical classi-
fier for all categories, even when it is trained and tested on different corpora. Results
also show that the StAC classifier can solve the task by using the words that occur in a
four-word window around the target problem and that it performs well across corpora.
This work focuses mostly on negation in clinical documents, but processing nega-
tion and speculation also plays a role in extracting relations and events from the abun-
dant literature on molecular biology. Finding negative cases is useful for filtering out
false positives in relation extraction, as support for automatic database curation, or for
refining pathways.
Sanchez-Graillet and Poesio (2007) develop a heuristics-based system that extracts
negated protein?protein interactions using a full dependency parser from articles about
chemistry. The system uses cue words and information from the syntax tree to find
potential constructions that express negation. If a negation construction is found, the
13 Alter-association assertions state that the problem is not associated with the patient.
244
Morante and Sporleder Modality and Negation
system extracts the arguments of the predicate that is negated based on the dependency
tree. The maximum F1 score that the system achieves is 62.96%, whereas the upper-
bound of the system with gold-standard protein recognition is 76.68% F1 score.
The BioNLP?09 Shared Task on Event Extraction (Kim et al 2009) addressed bio-
molecular event extraction. It consisted of three subtasks each aiming at different levels
of specificity, one of which was dedicated to finding whether the recognized biological
events are negated or speculated. Six teams submitted systems with results varying
from 2.64 to 23.13 F-measure for negation and 8.95 to 25.27 for speculation. To partic-
ipate in this subtask the systems had to first perform Task 1 in order to detect events,
which explains the low results. The best scores were obtained by a system that applies
syntax-based heuristics (Kilicoglu and Bergler 2009). Once events are identified, the
system analyzes the dependency path between the event trigger and speculation or
negation cues in order to determine whether the event is within the scope of the cues.
Sarafraz and Nenadic (2010a) further explore the potential of machine learning
techniques to detect negated events in the BioNLP?09 Shared Task data. They train an
SVM with a model that represents lexical, semantic, and syntax features. The system
works with gold-standard event detection and results are obtained by performing
10-fold cross-validation experiments. Evaluation is performed only on gene regula-
tion events, which means that the results are not comparable with the Shared Task
results. The best results are obtained when all features are combined, achieving a 53.85
F1 score. Error analysis shows that contrastive patterns like that in Example (27) with
the cue unlike are recurrent as a source of errors. Sarafraz and Nenadic (2010b) have
also compared a machine learning approach with a rule-based approach based on
command relations, finding that the machine learning approach produces better re-
sults. Optimal results are obtained when individual classifiers are trained for each
event class.
(27) Unlike TNFR1, LMP1 can interact directly with receptor-interacting
protein (RIP) and stably associates with RIP in EBV-transformed
lymphoblastoid cell lines.
Modality and negation processing at the event level has also been performed on
texts from a domain outside the biomedical domain. Here we describe systems that
process the factuality of events and a modality tagger.
EvITA and SlinkET (Saur??, Verhagen, and Pustejovsky 2006a, 2006b) are two sys-
tems for automatically identifying and tagging events in text and assigning to them
contextual modality features. EvITA assigns modality and polarity values to events
using pattern-matching techniques over chunks. SlinkET is a rule-based system that
identifies contexts of subordination that involve some types of modality, referred to as
SLINKs in TimeML (Pustejovsky et al 2005), and assigns one of the following types
to them: factive, counterfactive, evidential, negative evidential or modal. The
reported performance for SlinkET is 92% precision and 56% recall (Saur??, Verhagen,
and Pustejovsky 2006a). DeFacto (Saur?? 2008) is a factuality profiler. As Saur?? puts it,
the algorithm assumes a conceptual model where factuality is a property that speakers
(sources) attribute to events. Two relevant aspects of the algorithm are that it processes
the interaction of different factuality markers scoping over the same event and that it
identifies the relevant sources of the event. The system is described in detail in the article
by Saur?? and Pustejovsky included in this special issue.
Baker et al (2010) take a different approach. Instead of focusing on an event in
order to find its factuality, they focus on modality cues in order to find the predicate
245
Computational Linguistics Volume 38, Number 2
that is within their scope (target). They describe two modality taggers that identify
modality cues and modality targets, a string-based tagger and a structure-based tagger,
and compare their performances. The string-based tagger takes as input text tagged
with PoS and marks as modality cues words or phrases that match exactly cues from a
modality lexicon. More information about the modality taggers and their application in
machine translation can be found in the article by Baker et al included in this special
issue.
Finally, Diab et al (2009) model belief categorization as a sequence labeling task,
which allows them to treat cue detection and scope recognition in a unified fashion.
Diab et al distinguish three belief categories. For committed belief the writer indicates
clearly that he or she believes a proposition. In the case of non-committed belief the
writer identifies the proposition as something in which he or she could believe but
about which the belief is not strong. This category is further subdivided into weak
belief, which is often indicated by modals, such as may, and reported speech. The
final category, not applicable, refers to cases which typically do not have a belief value
associated with them, for example because the proposition does not have a truth value.
This category covers questions and wishes. Diab et al manually annotated a data set
consisting of 10,000 words with these categories and then used it to train and test an
automatic system for belief identification. The system makes use of a variety of lexical,
contextual, and syntactic features. Diab et al found that relatively simple features such
as the tokens in a window around the target word and the PoS tags lead to the best
performance, possibly due to the fact that some of the higher level features, such as the
verb type, are noisy.
6.2 Full Scope Resolution
The scope resolution task consists of determining at a sentence level which tokens
are affected by modality and negation cues. Thanks to the existence of the BioScope
corpus several full scope resolvers have been developed. The task was first modeled
as a classification problem with the purpose of finding the scope of negation cues in
biomedical texts (Morante, Liekens, and Daelemans 2008). It was further developed
for modality and negation cues by recent work on the same corpus (Morante and
Daelemans 2009a, 2009b; O?zgu?r and Radev 2009), and it was consolidated with the
edition of the 2010 CoNLL Shared Task on Learning to Detect Hedges and their Scope in
Natural Language Text (Farkas et al 2010a).
Morante, Liekens, and Daelemans (2008) approach the scope resolution task as a
classification task. Their conception of the task is inspired by Ramshaw and Marcus?s
(1995) representation of text chunking as a tagging problem and by the standard CoNLL
representation format (Buchholz andMarsi 2006). By setting up the task in this way they
show that the task can be modeled as a sequence labeling problem, and by conforming
to the existing CoNLL standards they show that scope resolution could be integrated
in a joint learning setting with dependency parsing and semantic role labeling. Their
system is a memory-based scope finder that tackles the task in two phases: cue identifi-
cation and scope resolution, which are modeled as consecutive token level classification
tasks. Morante and Daelemans (2009b) present another scope resolution system that
uses a different architecture, can deal with multiword negation cues, and is tested on
the three subcorpora of the BioScope corpus. For resolving the scope, three classifiers
(kNN, SVM, CRF++) predict whether a token is the first token in the scope sequence,
the last, or neither. A fourth classifier is a metalearner that uses the predictions of the
246
Morante and Sporleder Modality and Negation
three classifiers to predict the scope classes. The system is evaluated on three corpora
using as measure the percentage of fully correct scopes (PCS), which is 66.07 for the
corpus of abstracts on which the classifiers are trained, 41.00 for the full articles and
70.75 for the clinical reports. They show that the system is portable to different corpora,
although performance fluctuates.
Full scope resolution of negation cues has been performed as a support task to
determine the polarity of sentiments. In this context, negation is conceived as a con-
textual valence shifter (Kennedy and Inkpen 2006). If a sentiment is found within
the scope of a negation cue, its polarity should be reversed. Several proposals de-
fine the scope of a negation cue in terms of a certain number of words to the right
of the cue (Pang, Lee, and Vaithyanathan 2002; Hu and Liu 2004), but this solution is
not accurate enough. This is why research has been performed on integrating scope
resolver into sentiment analysis systems (Jia, Yu, and Meng 2009; Councill, McDonald,
and Velikovich 2010).
Jia, Yu, and Meng (2009) describe a rule-based system that uses information from
a parse tree. The algorithm first detects a candidate scope and then prunes the words
within the candidate scope that do not belong to the scope. The candidate scope of a
negation term t is formed by the descendant leaf nodes of the least common-ancestor of
the node representing t and the node representing the word t? immediately to the right
of t, that are found to the right of t?. Heuristic rules are applied in order to determine
the boundaries of the candidate scope. The rules involve the use of delimiters (elements
that mark the end of the scope), and conditional delimiters (elements that mark the end
of the scope under certain conditions). Additionally, situations are defined in which a
negation cue does not have a scope: phrases like not only, not just, not to mention, no
wonder, negative rhetorical questions, and restricted comparative sentences. Jia, Yu, and
Meng report that incorporating their scope resolution algorithm into two systems that
determine the polarity of sentiment words in reviews and in the TREC blogosphere
collection produces better accuracy results than incorporating other algorithms that are
described in the literature.
Councill, McDonald, and Velikovich (2010) present a system in some aspects similar
to the system described by Morante and Daelemans (2009b). The main differences with
Morante et al?s system are that in the first phase, the cues are detected by means of a
dictionary of 35 cues instead of being machine learned; in the second phase only a CRF
classifier is used, and this classifier incorporates features from dependency syntax. The
system is trained and evaluated on the abstracts and clinical reports of the BioScope
corpus and on a corpus of product reviews. The PCS reported for the BioScope corpus
is 53.7 and 39.8 for the Product Reviews corpus. Cross training results are also reported
showing that the system obtains better results for the Product Reviews corpus when
trained on BioScope, which, according to the authors, would indicate that the scope
boundaries are more difficult to predict in the Product Reviews corpus. Councill et al
also report that the scores of their sentiment analysis systemwith negation incorporated
improve by 29.5% and 11.4% for positive and negative sentiment, respectively. For
negative sentiment precision improves 46.8% and recall 6.6%.
It is worth mentioning that the systems trained on the BioScope corpus cannot deal
with intersentential, implicit, and affixal negation. Further research could focus on these
aspects of negation. Apart from scope resolvers for negation, several full scope resolvers
have been developed for modality.
Morante and Daelemans (2009a) test whether the scope resolver for negation
(Morante and Daelemans 2009b) is portable to resolve the scope of hedge cues, showing
that the same scope resolution approach can be applied to both negation and hedging.
247
Computational Linguistics Volume 38, Number 2
In the scope resolution phase, the system achieves 65.55% PCS in the abstracts corpus,
which is very similar to the result obtained by the negation resolver (66.07% PCS).
The system is also evaluated on the three types of text of the BioScope corpus. The
difference in performance for abstracts and full articles follows the same trends as in
the negation system, whereas the drop in performance for the clinical subcorpus is
higher, which indicates that there is more variation of modality cues across corpora
than there is of negation cues.
The modality scope resolver described by O?zgur and Radev (2009) solves the task
in two phases also, but differently from Morante and Daelemans (2009a); in the second
phase the scope boundaries are found with a rule-based module that uses information
from the syntax tree. This system is evaluated on the abstracts and full articles of the
BioScope corpus. The scope resolution is evaluated in terms of accuracy, achieving
79.89% in abstracts and 61.13% in full articles.
Task 2 of the 2010 edition of the CoNLL Shared Task (Farkas et al 2010b) consisted
of resolving the scope of hedge cues on biomedical texts. A scope-level F1 measure was
used as the main evaluation metric where true positives were scopes which exactly
matched the gold-standard cues and gold-standard scope boundaries assigned to the
cue word. The best system (Morante, van Asch, and Daelemans 2010) achieved a
F1 score of 57.3. As Farkas et al (2010b) describe, each Task 2 system was built upon
a Task 1 system, attempting to recognize the scopes for the predicted cue phrases. Most
systems regarded multiple cues in a sentence to be independent from each other and
formed different classification instances from them. The scope resolution for a certain
cue was typically carried out by a token based classification. Systems differ in the
number of class labels used as a target and in the machine learning approaches ap-
plied. Most systems, following Morante and Daelemans (2009a), used three class labels:
first, last, and none, and two systems used four classes by adding inside, whereas
three systems followed a binary classification approach. Most systems included a post-
processing mechanism to produce continuous scopes, according to the BioScope anno-
tation. Sequence labeling and token-based classification machine learning approaches
were applied, and information from the dependency path between the cue and the
token in question was generally encoded in the feature space.
The system that scored the best results for Task 2 (Morante, van Asch, and
Daelemans 2010) follows the same approach as Morante and Daelemans (2009a), al-
though it introduces substantial differences: This system uses only one classifier to
solve Task 2, whereas the system described in Morante and Daelemans (2009a) used
three classifiers and a metalearner; this system uses features from both shallow and
dependency syntax, instead of only shallow syntax features; and it incorporates in the
feature representation information from a lexicon of hedge cues generated from the
training data.
As a follow-up of the CoNLL Shared Task, ?vrelid, Velldal, and Oepen (2010)
investigate the contribution of syntax to scope resolution. They apply a hybrid, two-
stage approach to the scope resolution task. In the first stage, a Maximum Entropy
classifier, combining surface-oriented and syntax features, identifies cue words, and
multiword cues are identified in a postprocessing step. In the second stage a small set
of hand-crafted rules operating over dependency representations are applied to resolve
the scope. This system is evaluated following exactly the same settings as the CoNLL
Shared Task. The results do not improve over the best shared task results but show that
handcrafted syntax-based rules achieve a very competitive performance. ?vrelid et al
report that the errors of their system are mostly of two classes: (i) failing to recognize
phrase and clause boundaries, as in Example (28a), and (ii) not dealing successfully with
248
Morante and Sporleder Modality and Negation
relatively superficial properties of the text as in Example (28b). The scope boundaries
produced by the system are marked with ???.
(28) a. ... [the reverse complement ?mR of m will be considered to be ...?].
b. This ?[might affect the results] if there is a systematic bias on the
composition of a protein interaction set?.
Finally, Zhu et al (2010) approach the scope learning problem via simplified shallow
semantic parsing. The cue is regarded as the predicate and its scope is mapped into
several constituents as the arguments of the cue. The system resolves the scope of nega-
tion and modality cues in the standard two phase approach. For cue identification they
apply an SVM that uses features from the surrounding words and from the structure of
the syntax tree. The scope resolution task is different than in previous systems. The task
is addressed in three consecutive phases: (1) argument pruning, consisting of collecting
as argument candidates any constituent in the parse tree whose parent covers the given
cue except the cue node itself and its ancestral constituents; (2) argument identification
where a binary classifier is applied to determine the argument candidates as either
valid arguments or non-arguments; and (3) postprocessing to guarantee that the scope
is a continuous sequence of arguments. The system is trained on the abstracts part of
the BioScope corpus and tested on the three parts of the BioScope corpus. Evaluating
the system following the CoNLL Shared Task setting would shed more light on the
advantages of the semantic parsing approach as compared to other approaches.
From the systems and results described in this section, we can conclude that al-
though there has been substantial research on the scope resolution task, there is still
room for improvement. The performance of scope resolvers is still far from having
reached the level of well established tasks like semantic role labeling or parsing. Prob-
ably, better results can be obtained by a combination of more experimental work with
algorithms and a deeper analysis of the task from a linguistic perspective so that the
representation models can be improved. The article by Velldal et al in this special issue
provides new insights into the task.
7. Processing Contradiction and Contrast
The concept of negation is closely related to the discourse-level concepts of ?contradic-
tion? and ?contrast,? which typically require an explicit or implicit negation.
Contradiction is a relation that holds between two documents with contradictory
content. Detecting contradiction is important for tasks which extract information from
multi-document collections, such as question-answering and multi-document summa-
rization. Since 2007 contradiction detection has also been included as a subtask in the
Textual Entailment Challenge (Giampiccolo et al 2007), spurring an increased interest
in the development of systems which can automatically detect contradictions. The two
contradictory sentence pairs in Examples (29) and (30) (both from Harabagiu et al
[2006]) illustrate the relation between contradiction and negation. In Example (29) the
contradiction is signaled by the explicit negation marker never, whereas in Example (30)
the negation is implicit and signaled by the use of call off in the second sentence (which
is an antonym of begin in the first sentence).
(29) a. Joachim Johansson held off a dramatic fightback from defending
champion Andy Roddick, to reach the semi-finals of the US Open
on Thursday night.
249
Computational Linguistics Volume 38, Number 2
b. Defending champion Andy Roddick never took on Joachim Johansson.
(30) a. In California, one hundred twenty Central Americans, due to be
deported, began a hunger strike when their deportation was delayed.
b. A hunger strike was called off.
Although contradiction typically occurs across documents, contrast is a discourse
relation within documents. At least some types of contrast involve negation, notably
those that involve a denial of expectation. The negation can be explicit as in Exam-
ple (31a), implicit (31b), or entailed (31c) (see Umbach [2004]).
(31) a. John cleaned his room, but he didn?t wash the dishes.
b. John cleaned his room, but he skipped the washing up.
c. John cleaned up the room, but Bill did the dishes.
Given this interrelation between negation and contradiction on the one hand and
negation and contrast on the other, it it not surprising that negation detection has been
studied in the context of discourse relation classification and contradiction detection.
Most studies in this area use fairly standard?i.e., sentence-based?methods for nega-
tion detection. Once the negation has been detected it is then used as a feature for the
higher-level tasks of contradiction or contrast detection.
For instance, Harabagiu, Hickl, and Lacatusu (2006) discuss a system which first
detects negated expressions and then finds contradictions on the basis of the de-
tected negations. To detect explicit negation Harabagiu et al use a lexicon of explicit
cues. To determine the scope they use a set of heuristics, which varies depending on
whether the negated object is an event, an entity, or a state. For events, the negation is
assumed to scope over the whole predicate?argument structure. For entities and for
states realized by nominalizations the negation is assumed to scope over the whole
NP. Implicit negations are detected by searching for antonymy chains in WordNet. de
Marneffe, Rafferty, and Manning (2008) also make use of negation detection to discover
contradictions. They do so rather implicitly, however, by using a number of features
which check for explicit negation, polarity, and antonymy. Ritter et al (2008) present a
contradiction detection system that uses the TEXTRUNNER system (Banko et al 2007)
to extract relations of the form R(x,y) (e.g., was born in(Mozart,Salzburg)). They then
inspect potential contradictions (i.e., relations which overlap in one variable but not in
the other) and filter out non-contradictions by looking, for example, for synonyms and
meronyms.
In the context of contrast detection in discourse processing, negation detection is
rarely used as an explicit step. An exception is Kim et al (2006), who are concerned
with discovering contrastive information about protein interaction in biomedical texts.
They only deal with explicitly marked negation which occurs in the context of a contrast
relation marked by a contrast signaling connective such as but. Unlike Kim et al, Pitler,
Louis, and Nenkova (2009) are concerned with detecting implicit discourse relations?
namely, relations which are not explicitly signalled by a connective such as but. To detect
such relations, they define a number of features, including polarity features. Hence they
make implicit use of negation information but do not aim to detect it as a separate
subtask.
250
Morante and Sporleder Modality and Negation
8. Positive and Negative Opinions
Much work in the NLP community has been carried out in the area of identifying
positive and negative opinions, also known as opinion mining, sentiment analysis, or
subjectivity analysis.14 Sentiment analysis touches on the topic of this special issue as
both negation and modality cues can help determine the opinion of an opinion holder
on a subject. Negation in particular has received attention in the sentiment analysis
community as negation can affect the polarity of an expression. Negation and polarity
are two different concepts, however (see Section 3.1). The relation between negation and
polarity is also not always entirely straightforward. For example, whereas negation can
change the polarity of an expression from positive to negative (e.g., good vs. not good in
Examples (32a) vs. (32b)) it can also shift negative polarity to neutral or even positive
polarity (32c).
(32) a. This is a good camera.
b. This is not a good camera.
c. This is by no means a bad camera.
In this section, we discuss some approaches that make explicit use of negation in
the context of sentiment analysis. For a recent general overview of work on sentiment
analysis, we refer the reader to Pang and Lee (2008).
Wiegand et al (2010) present a survey of the role of negation in sentiment analysis.
They indicate that it is necessary to perform fine-grained linguistic analysis in order
to extract features for machine learning or rule-based opinion analysis systems. The
features allow the incorporation of information about linguistic phenomena such as
negation (Wiegand et al 2010, page 60). Early approaches made use of negation in
a bag-of-words model by prefixing a word x with a negation marker if a negation
word was detected immediately preceding x (Pang, Lee, and Vaithyanathan 2002).
Thus x and NOT x were treated as two completely separate features. Although this
model is relatively simple to compute and leads to an improvement over a bag-of-
words model without negation, Pang, Lee, and Vaithyanathan (2002) found that the
effect of adding negation was relatively small, possibly because the introduction
of additional features corresponding to negated words increases the feature space
and thereby also data sparseness. Later work introduced more sophisticated use of
negation, for example, by explicitly modeling negation expressions as polarity shifters,
which change the polarity of an expression (Kennedy and Inkpen 2006; Polanyi
and Zaenen 2006), or by introducing specific negation features (Wilson, Wiebe, and
Hoffman 2005; Wilson, Wiebe, and Hwa 2006; Wilson 2008). It was found that these
more sophisticated models typically lead to a significant improvement over a simple
bag-of-words model with negation prefixes. This improvement can to a large extent
be directly attributed to the better modeling of negation (Wilson, Wiebe, and Hoffman
2009). Whereas modeling negation in opinion mining frequently involves determining
the polarity of opinions (Hu and Liu 2004; Kim and Hovy 2004; Wilson, Wiebe, and
Hoffman 2005; Wilson 2008), some researchers have also used negation models to
14 The three terms are used sometimes interchangeably and sometimes reserved for somewhat different
contexts. We follow here the definitions of Pang and Lee (2008) who use ?opinion mining? and
?sentiment analysis? as largely synonymous terms and ?subjectivity analysis? as a cover term for both.
251
Computational Linguistics Volume 38, Number 2
determine the strength of opinions (Popescu and Etzioni 2005; Wilson, Wiebe, and Hwa
2006). Choi and Cardie (2010) found that performing both tasks jointly can lead to a
significant improvement over a pipeline model in which the two tasks are performed
separately. Councill, McDonald, and Velikovich (2010) also show that explicit modeling
of negation has a positive effect on polarity detection.
9. Overview of the Articles in this Special Issue
For this special issue we invited articles on all aspects of the computational modeling
and processing of modality and negation. Given that this area is both theoretically
complex?with several competing linguistic theories having been put forward for
various aspects of negation and modality?and computationally challenging, we
particularly encouraged submissions with a substantial analysis component, either
in the form of a data or task analysis or in the form of a detailed error analysis. We
received 25 submissions overall, reflecting a significant interest in these phenomena
in the computational linguistics community. After a rigorous review process, we
selected five articles, covering various aspects of the topic. Three of the articles (Saur??
and Pustejovsky; de Marneffe et al; and Szarvas et al) deal with one specific aspect
of modality, namely, certainty (in the widest sense) from both a theoretical and a
computational perspective. The remaining two articles (Velldal et al and Baker et al)
deal with both negation and modality detection in a more application-focused setting.
The following paragraphs provide a detailed overview of the articles.
In the first article, Saur?? and Pustejovsky introduce their model of factuality. They
distinguish the dimensions of polarity and certainty and use a four-point scale for the
latter. They also explicitly model different sources and embedding of factuality across
several levels. They then present a linguistically motivated, symbolic system, DeFacto,
for computing factuality and attributing it to the correct sources. The model operates
on dependency parses and exploits a number of lexical cues together with hard-coded
rules to process factuality within a sentence in a top?down fashion.
Whereas Saur?? and Pustejowsky focus on lexical and intra-sentential aspects of
factuality, the article by de Marneffe et al looks specifically at the pragmatic component
of factuality (called veridicality in their article). They argue that although individual
lexemes might be associated with discrete veridicality categories out of context, spe-
cific usages are better viewed as evoking probability distributions over veridicality
categories, where world knowledge and discourse context can shift the probabilities
in one or the other direction. To support this hypothesis, de Marneffe et al carried
out an annotation study with linguistically naive subjects, which provides evidence for
considerable variation between subjects, especially with respect to neighboring veridi-
cality categories. In a second step, de Marneffe et al show how this type of pragmatic
veridicality can be modeled in a supervised machine learning setting.
In the following article, Szarvas et al provide a cross-domain and cross-genre
view of (un-)certainty. They propose a novel categorization scheme for uncertainty that
unifies existing schemes, which, they argue, are to some extent domain- and genre-
dependent. They provide a detailed analysis of different linguistic manifestations of
uncertainty in several types of text and then propose a method for adapting uncertainty
detection systems to novel domains. They show that instead of simply boosting the
available training data from the target domain with randomly selected data from the
source domain, it is often more beneficial to select those instances from the source
domain that contain uncertainty cues that are also observed in the target domain. In
252
Morante and Sporleder Modality and Negation
this scenario, the additional data from the source domain is exploited to fine-tune the
disambiguation of target domain cues rather than to learn novel cues.
Moving from certainty to negation and speculation in a more general sense, Velldal
et al show how deep and shallow approaches can be combined for cue detection and
scope resolution. They assume a closed class of speculation and negation cues and cast
cue detection as a disambiguation rather than a classification task, using supervised
machine learning based on n-gram features.
In a second step, they tackle scope resolution, for which they propose two models.
The first implements a number of syntax-driven rules over dependency structures, and
the second model is data-driven and ranks candidate scopes on the basis of constituent
trees.
The final article, by Baker et al, also addresses modality and negation processing,
but within a particular application scenario, namely, machine translation. The authors
propose a novel annotation scheme for modality and negation and two rule-based
taggers for identifying cues and scopes. The first tagger employs string matching in
combination with a semi-automatically developed cue lexicon; the second goes beyond
the surface string and utilizes heuristics based on syntax. In the machine translation
process, syntax trees in the source language are then automatically enriched with
modality and negation information before being translated.
10. Final Remarks
In this article, we have given an overview of the treatment of negation and modality
in computational linguistics. Although much work has been done in recent years and
many models for dealing with various aspects of these two phenomena have been
proposed, it is clear that much still remains to be done.
The first challenge is a theoretical one and pertains to the categorization and anno-
tation of negation and, especially, modality. Currently, many annotation schemes exist
in parallel (see Section 4). As a consequence, the existing annotated corpora are all rel-
atively small. Significant progress in this area depends on the availability of annotated
resources, however, both for training and testing automated systems and for (corpus)
linguistic studies that can support the development of linguistically informed systems.
Ideally, any larger scale resource creation project should be preceded by a discussion in
the computational linguistics community about which aspects of negation andmodality
should be annotated and how this should be done (see, e.g., Nirenburg and McShane
[2008]). To some extent this is already happening and the public release of annotated
resources such as the MPQA (Wiebe, Wilson, and Cardie 2005) or the BioScope (Vincze
et al 2008) corpus, as well as the organization of shared tasks (Farkas et al 2010a), are
steps in the right direction. Related to this challenge is the question of which aspects
of extra-propositional meaning need to be modeled for which applications. Outside
sentiment analysis, relatively little research has been carried out in this area so far.
A second challenge involves the adequate modeling of modality and negation.
For example, although we can detect extra-propositional content, few researchers so
far have investigated how interactions between extra-propositional meaning aspects
can be adequately modeled. Also, most approaches have addressed the detection of
negation at a sentence or predicate level. Discourse-level interdependencies between
different aspects of extra-propositional content have been largely ignored. To address
this challenge, we believe that more research into linguistically motivated approaches
is necessary.
253
Computational Linguistics Volume 38, Number 2
Finally, most research so far has been carried out on English and on selected do-
mains and genres (biomedical, reviews, newswire). It would be interesting to also look
at different languages and devisemethods for cross-lingual bootstrapping. It would also
be good to broaden the set of domains and genres (including fiction, scientific texts,
weblogs, etc.) since extra-propositional meaning is particularly susceptible to domain
and genre effects.
Acknowledgments
Roser Morante?s research is funded by
the GOA project BioGraph: Text Mining
on Heterogeneous Databases: An Application
to Optimized Discovery of Disease Relevant
Genetic Variants of the University of
Antwerp, Belgium. Caroline Sporleder
is supported by the German Research
Foundation DFG (Cluster of Excellence
Multimodal Computing and Interaction
[MMCI]).
References
Aikhenvald, Alexandra Y. 2004. Evidentiality.
Oxford University Press, New York.
Aronow, David B., Feng Fangfang, and W.
Bruce Croft. 1999. Ad hoc classification of
radiology reports. JAMIA, 6(5):393?411.
Averbuch, Mordechai, Tom H. Karson,
Benjamin Ben-Ami, Oded Maimon, and
Lior Rokach. 2004. Context-sensitive
medical information retrieval. In
Proceedings of the 11th World Congress on
Medical Informatics (MEDINFO-2004),
pages 1?8, San Francisco, CA.
Baker, Kathrin, Michael Bloodgood, Mona
Diab, Bonnie Dorr, Ed Hovy, Lori Levin,
Marjorie McShane, Teruko Mitamura,
Sergei Nirenburg, Christine Piatko, Owen
Rambow, and Gramm Richardson. 2009.
SIMT SCALE 2009 modality annotation
guidelines. Technical report 4, Human
Language Technology Center of
Excellence, Baltimore, MD.
Baker, Kathrin, Michael Bloodgood, Bonnie
Dorr, Nathaniel W. Filardo, Lori Levin,
and Christine Piatko. 2010. A modality
lexicon and its use in automatic tagging.
In Proceedings of the Seventh Conference on
International Language Resources and
Evaluation (LREC?10), pages 1402?1407,
Valetta.
Banfield, Ann. 1982. Unspeakable Sentences.
Routledge and Kegan Paul, Boston, MA.
Banko, Michele, Michael Cafarella, Stephen
Soderland, Matt Broadhead, and Oren
Etzioni. 2007. Open information extraction
from the Web. In Proceedings of International
Joint Conferences on Artificial Intelligence
2007, pages 2670?2676, Hyderabad.
Blanco, Eduardo and Dan Moldovan. 2011.
Semantic representation of negation using
focus detection. In Proceedings of 49th
Annual Meeting of the Association for
Computational Linguistics, pages 19?24,
Portland, OR.
Boas, Franz, 1938. Language. In Franz Boas,
General Anthropology. D.C. Heath and
Company, Boston, MA, pages 124?145.
Buchholz, Sabine and Erwin Marsi. 2006.
CoNLL-X shared task on multilingual
dependency parsing. In Proceedings of
the X CoNLL Shared Task, pages 149?164,
New York.
Chafe, Wallace. 1986. Evidentiality in
English conversation and academic
writing. In W. Chafe and J. Nichols,
editors. Evidentiality: The Linguistic Coding
of Epistemology. Ablex, Norwood, NJ.
Chapman, Wendy W., Will Bridewell, Paul
Hanbury, Gregory F. Cooper, and Bruce G.
Buchanan. 2001a. A simple algorithm for
identifying negated findings and diseases
in discharge summaries. J Biomed Inform,
34:301?310.
Chapman, Wendy W., Paul Hanbury,
Gregory F. Cooper, and Bruce G.
Buchanan. 2001b. Evaluation of negation
phrases in narrative clinical reports.
In Proceedings of the American Medical
Informatics Association Symposium. 2001,
pages 105?109, Washington, DC.
Choi, Yejin and Claire Cardie. 2010.
Hierarchical sequential learning for
extracting opinions and their attributes.
In Proceedings of the ACL 2010 Conference
Short Papers, pages 269?274, Uppsala.
Collier, Nigel, Hyun S. Park, Norihiro
Ogata, Yuka Tateisi, Chikashi Nobata,
Tomoko Ohta, Tateshi Sekimizu, Hisao
Imai, Katsutoshi Ibushi, and Jun?ichi Tsujii.
1999. The GENIA project: Corpus-based
knowledge acquisition and information
extraction from genome research papers.
In Proceedings of EACL-99, pages 271?272,
Bergen.
Councill, Isaac, Ryan McDonald, and Leonid
Velikovich. 2010. What?s great and what?s
not: Learning to classify the scope of
negation for improved sentiment analysis.
In Proceedings of the Workshop on Negation
254
Morante and Sporleder Modality and Negation
and Speculation in Natural Language
Processing, pages 51?59, Uppsala.
Dalianis, Hercules and Sumithra Velupillai.
2010. How certain are clinical assessments?
Annotating Swedish clinical text for
(un)certainties, speculations and
negations. In Proceedings of the Seventh
Conference on International Language
Resources and Evaluation (LREC?10),
Valletta.
de Haan, Frederik. 1995. The Interaction of
Modality and Negation: A Typological Study.
Garland Publishing, Inc., New York.
de Haan, Frederik. 1999. Evidentiality
and epistemic modality: Setting the
boundaries. Journal of Linguistics,
18:83?102.
de Marneffe, Marie-Catherine, Bill
Maccartney, Trond Grenager, Daniel Cer,
Anna Rafferty, and Christopher D.
Manning. 2006. Learning to distinguish
valid textual entailments. In Proceedings of
the Second PASCAL Challenges Workshop
on Recognising Textual Entailment,
pages 74?79, Venice.
de Marneffe, Marie-Catherine, Anna N.
Rafferty, and Christopher D. Manning.
2008. Finding contradictions in text. In
Proceedings of ACL 2008, pages 1039?1047,
Columbus, OH.
Di Marco, Chrysanne, Frederick Kroon, and
Robert Mercer. 2006. Using hedges to
classify citations in scientific articles. In
W. Bruce Croft, James Shanahan, Yan Qu,
and Janyce Wiebe, editors, Computing
Attitude and Affect in Text: Theory and
Applications, volume 20 of The Information
Retrieval Series. Springer, Amsterdam,
pages 247?263.
Diab, Mona T., Lori Levin, Teruko Mitamura,
Owen Rambow, Vinodkumar
Prabhakaran, and Weiwei Guo. 2009.
Committed belief annotation and tagging.
In ACL-IJNLP 09: Proceedings of the
Third Linguistic Annotation Workshop,
pages 68?73, Singapore.
Elkin, Peter L., Steven H. Brown, Brent A.
Bauer, Casey S. Husser, William Carruth,
Larry R. Bergstrom, and Dietlind L.
Wahner-Roedler. 2005. A controlled
trial of automated classification of
negation from clinical notes. BMC Medical
Informatics and Decision Making, 5(13).
doi: 10.1186/1472-6947-5-13.
Farkas, Richa?rd, Veronika Vincze, Gyo?rgy
Mo?ra, Ja?nos Csirik, and Gyo?rgy Szarvas.
2010a. The CoNLL 2010 shared task:
Learning to detect hedges and their scope
in natural language text. In Proceedings of
the CoNLL2010 Shared Task, pages 1?12,
Uppsala.
Farkas, Richa?rd, Veronika Vincze, Gyo?rgy
Szarvas, Gyo?rgy Mo?ra, and Ja?nos Csirik,
editors. 2010b. Proceedings of the Fourteenth
Conference on Computational Natural
Language Learning, Uppsala.
Friedman, Carol, Philip Alderson, John H. M.
Austin, James J. Cimino, and Stephen B.
Johnson. 1994. A general natural?language
text processor for clinical radiology.
JAMIA, 1(2):161?174.
Ganter, Viola and Michael Strube. 2009.
Finding hedges by chasing weasels:
Hedge detection using wikipedia tags and
shallow linguistic features. In Proceedings
of the ACL-IJCNLP 2009 Conference Short
Papers, pages 173?176, Suntec.
Garson, James. 2009. Modal logic.
In Edward N. Zalta, editor, The
Stanford Encyclopedia of Philosophy.
Standord University, CA. Available
at http://plato.stanford.edu/
archives/win2009/entries/
logic-modal/.
Georgescul, Maria. 2010. A hedgehop over a
max-margin framework using hedge cues.
In Proceedings of the Fourteenth Conference
on Computational Natural Language
Learning, pages 26?31, Uppsala.
Giampiccolo, Danilo, Bernardo Magnini,
Ido Dagan, and Bill Dolan. 2007.
The third pascal recognizing textual
entailment challenge. In Proceedings of
the ACL-PASCAL Workshop on Textual
Entailment and Paraphrasing, pages 1?9,
Prague.
Goldin, Ilya M. and Wendy W. Chapman.
2003. Learning to detect negation with
?Not? in medical texts. In Proceedings of
ACM-SIGIR 2003, Toronto.
Goryachev, Sergey, Margarita Sordo,
Qing T. Zeng, and Long Ngo. 2006.
Implementation and evaluation of four
different methods of negation detection.
Technical report DSG. Harvard Medical
School, Boston, MA.
Grabar, Natalia and Thierry Hamon.
2009. Exploitation of speculation
markers to identify the structure of
biomedical scientifc writing. In
AMIA 2009 Symposium Proceedings,
pages 203?207, San Francisco, CA.
Harabagiu, Sanda, Andrew Hickl, and
Finley Lacatusu. 2006. Negation, contrast
and contradiction in text processing.
In Proceedings of the 21st International
Conference on Artificial Intelligence,
pages 755?762, Las Vegas, NV.
255
Computational Linguistics Volume 38, Number 2
Harkema, Henk, John N. Dowling, Tyler
Thornblade, and Wendy W. Chapman.
2009. ConText: An algorithm for
determining negation, experiencer,
and temporal status from clinical
reports. Journal of Biomedical Informatics,
42:839?851.
Hickl, Andrew and Jeremy Bensley. 2007.
A discourse commitment-based
framework for recognizing textual
entailment. In Proceedings of the
ACL-PASCAL Workshop on Textual
Entailment and Paraphrasing,
pages 171?176, Stroudsburg, PA.
Hoeksema, Jack. 2000. Negative polarity
items: triggering, scope, and c-command.
In L. Horn and Y. Kato, editors, Negation
and Polarity. Oxford University Press,
Oxford, pages 115?146.
Horn, Laurence R. 1989. A Natural History
of Negation. Chicago University Press,
Chicago, IL.
Hu, Minqing and Bing Liu. 2004. Mining and
summarizing customer reviews. In KDD
?04: Proceedings of the Tenth ACM SIGKDD
International Conference on Knowledge
Discovery and Data Mining, pages 168?177,
New York.
Huang, Yang and Henry J. Lowe. 2007. A
novel hybrid approach to automated
negation detection in clinical radiology
reports. Journal of the American Medical
Informatics Association, 14(3):304?311.
Hyland, Ken. 1998. Hedging in Scientific
Research Articles. John Benjamins B.V.,
Amsterdam.
Ifantidou, Elly. 2001. Evidentials and
Relevance. John Benjamins, Amsterdam.
Israel, Michael. 2004. The pragmatics of
polarity. In L. Horn and G. Ward, editors,
The Handbook of Pragmatics. Blackwell,
Oxford, pages 701?723.
Jespersen, Otto. 1924. The Philosophy of
Grammar. Allen and Unwin, London.
Jia, Lifeng, Clement Yu, and Weiyi Meng.
2009. The effect of negation on sentiment
analysis and retrieval effectivenesss. In
Proceedings of the 18th ACM Conference on
Information and Knowledge Management,
pages 1827?1830, New York City.
Kennedy, Alistair and Diana Inkpen.
2006. Sentiment classification of movie
reviews using contextual valence shifters.
Computational Intelligence, 22(2):110?125.
Kilicoglu, Halil and Sabine Bergler. 2008.
Recognizing speculative language
in biomedical research articles: a
linguistically motivated perspective.
BMC Bioinformatics, 9(Suppl 11):S10.
Kilicoglu, Halil and Sabine Bergler. 2009.
Syntactic dependency based heuristics for
biological event extraction. In Proceedings
of the Workshop on Current Trends in
Biomedical Natural Language Processing:
Shared Task, BioNLP ?09, pages 119?127,
Stroudsburg, PA.
Kim, Jin-Dong, Tomoko Ohta, Sampo
Pyysalo, Yoshinobu Kano, and Jun?ichi
Tsujii. 2009. Overview of BioNLP shared
task on event extraction. In Proceedings of
the BioNLP 2009 Workshop Companion
Volume for Shared Task 2009, pages 1?9,
Boulder, CO.
Kim, Jin-Dong, Tomoko Ohta, and Jun?ichi
Tsujii. 2008. Corpus annotation for mining
biomedical events from literature. BMC
Bioinformatics, 9(10). doi:
10.1186/1471-2105-9-10.
Kim, Jung-Jae, Zhuo Zhang, Jong C. Park,
and See-Kiong Ng. 2006. BioContrasts:
Extracting and exploiting protein?protein
contrastive relations from biomedical
literature. Bioinformatics, 22:597?605.
Kim, Soo-Min and Ed Hovy. 2004.
Determining the sentiment of opinions.
In COLING ?04: Proceedings of the 20th
International Conference on Computational
Linguistics, page 1367, Morristown, NJ.
Kratzer, Angelika. 1981. The notional
category of modality. In H. J. Eikmeyer
and H. Rieser, editors,Words, Worlds,
and Contexts. New Approaches in Word
Semantics. De Gruyter, Berlin, pages 38?74.
Kratzer, Angelika. 1991. Modality. In
A. von Stechow and D. Wunderlich,
editors, Semantics: An International
Handbook of Contemporary Research.
De Gruyter, Berlin, pages 639?650.
Kripke, Saul. 1963. Semantic considerations
on modal logic. Acta Philosophica Fennica,
16:83?94.
Lakoff, George. 1972. Hedges: a study in
meaning criteria and the logic of fuzzy
concepts. Chicago Linguistics Society Papers,
8:183?228.
Lawler, John. 2010. Negation and negative
polarity. In P. C. Hogan, editor, Cambridge
Encyclopedia of the Language Sciences.
Cambridge University Press, Cambridge,
UK, pages 554?555.
Light, Mark, Xin Y. Qiu, and Padmini
Srinivasan. 2004. The language of
bioscience: Facts, speculations, and
statements in between. In Proceedings of
BioLINK 2004, pages 17?24, Boston, MA.
Linguistic Data Consortium. 2008. ACE
(Automatic Content Extraction) English
annotation guidelines for relations.
256
Morante and Sporleder Modality and Negation
Technical Report Version 6.2 2008.04.28.
LDC, Philadelphia, PA.
Lyons, John. 1977. Semantics. Cambridge
University Press, Cambridge, UK.
Matsuyoshi, Suguru, Megumi Eguchi,
Chitose Sao, Koji Murakami, Kentaro Inui,
and Yuji Matsumoto. 2010. Annotating
event mentions in text with modality,
focus, and source information. In
Proceedings of the Seventh Conference on
International Language Resources and
Evaluation (LREC?10), pages 1456?1463,
Valletta.
Medlock, Ben. 2008. Exploring hedge
identification in biomedical literature.
JBI, 41:636?654.
Medlock, Ben and Ted Briscoe. 2007. Weakly
supervised learning for hedge classification
in scientific literature. In Proceedings of
ACL 2007, pages 992?999, Prague.
Morante, Roser. 2010. Descriptive analysis
of negation cues in biomedical texts.
In Proceedings of the Seventh Conference
on International Language Resources and
Evaluation (LREC?10), pages 1429?1436,
Valletta.
Morante, Roser and Walter Daelemans.
2009a. Learning the scope of hedge cues in
biomedical texts. In Proceedings of BioNLP
2009, pages 28?36, Boulder, CO.
Morante, Roser and Walter Daelemans.
2009b. A metalearning approach to
processing the scope of negation.
In Proceedings of CoNLL 2009,
pages 28?36, Boulder, CO.
Morante, Roser, Anthony Liekens, and
Walter Daelemans. 2008. Learning the
scope of negation in biomedical texts.
In Proceedings of EMNLP 2008,
pages 715?724, Honolulu, HI.
Morante, Roser, Sarah Schrauwen, and
Walter Daelemans. 2011. Annotation of
negation cues and their scope. Guidelines
v1.0. Technical Report 3, CLiPS, Antwerp,
Belgium.
Morante, Roser, Vincent van Asch, and
Walter Daelemans. 2010. Memory-based
resolution of in-sentence scopes of
hedge cues. In Proceedings of CoNLL,
pages 40?47, Uppsala.
Mutalik, Pradeep G., Aniruddha Deshpande,
and Prakash M. Nadkarni. 2001. Use of
general-purpose negation detection to
augment concept indexing of medical
documents. A quantitative study using
the UMLS. Journal of the American Medical
Informatics Association, 8(6):598?609.
Nawaz, Raheel, Paul Thompson, and
Sophia Ananiadou. 2010. Evaluating a
meta-knowledge annotation scheme for
bio-events. In Proceedings of the Workshop
on Negation and Speculation in Natural
Language Processing, pages 69?77, Uppsala.
Nirenburg, Sergei and Marjorie McShane.
2008. Annotating modality. OntoSem final
project report. University of Maryland,
Baltimore County.
Nirenburg, Sergei and Victor Raskin.
2004. Ontological Semantics. MIT Press,
Cambridge, MA.
?vrelid, Lilja, Erik Velldal, and Stephan
Oepen. 2010. Syntactic scope resolution
in uncertainty analysis. In Proceedings
of the 23rd International Conference on
Computational Linguistics, COLING ?10,
pages 1379?1387, Stroudsburg, PA.
O?zgu?r, Arzucan and Dragomir R. Radev.
2009. Detecting speculations and their
scopes in scientific text. In Proceedings of
EMNLP 2009, pages 1398?1407, Singapore.
Palmer, Frank R. 1986.Mood and
Modality. Cambridge University Press,
Cambridge, UK.
Pang, Bo and Lillian Lee. 2008. Opinion
mining and sentiment analysis.
Foundations and Trends in Information
Retrieval, 2(1?2):1?135.
Pang, Bo, Lillian Lee, and Shivakumar
Vaithyanathan. 2002. Thumbs up?
Sentiment classification using machine
learning techniques. In Proceedings of the
2002 Conference on Empirical Methods in
Natural Language Processing, pages 79?86.
Philadelphia, PA.
Parsons, Terence. 2008. The traditional
square of opposition. In E. N. Zalta,
editor, The Stanford Encyclopedia of
Philosophy. Stanford University. Available
at http://plato.stanford.edu/
archives/fall2008/entries/square/.
Payne, Thomas E. 1997. Describing
Morphosyntax. Cambridge University
Press, Cambridge, UK.
Pitler, Emily, Annie Louis, and Ani Nenkova.
2009. Automatic sense prediction for
implicit discourse relations in text. In
Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the
4th International Joint Conference on
Natural Language Processing of the AFNLP,
pages 683?691, Suntec.
Polanyi, Livia and Annie Zaenen. 2006.
Contextual valence shifters. In W. Bruce
Croft, James Shanahan, Yan Qu, and
Janyce Wiebe, editors, Computing Attitude
and Affect in Text: Theory and Applications,
volume 20 of The Information Retrieval
Series. Springer, Netherlands, pages 1?10.
257
Computational Linguistics Volume 38, Number 2
Popescu, Ana-Maria and Oren Etzioni. 2005.
Extracting product features and opinions
from reviews. In HLT ?05: Proceedings of the
conference on Human Language Technology
and Empirical Methods in Natural Language
Processing, pages 339?346, Morristown, NJ.
Portner, Paul. 2009.Modality. Oxford
University Press, Oxford, UK.
Prabhakaran, Vinodkumar, Owen Rambow,
and Mona Diab. 2010. Automatic
committed belief tagging. In Proceedings of
COLING 2010, pages 1014?1022, Beijing.
Prasad, Rashmi, Nikhil Dinesh, Alan Lee,
Aravind Joshi, and Bonnie Webber. 2006.
Annotating attribution in the Penn
Discourse TreeBank. In SST ?06: Proceedings
of the Workshop on Sentiment and Subjectivity
in Text, pages 31?38, Morristown, NJ.
Prasad, Rashmi, Nikhil Dinesh, Alan Lee,
Eleni Miltsakaki, Livio Robaldo, Aravind
Joshi, and Bonnie Webber. 2008. The Penn
Discourse TreeBank 2.0. In Proceedings of
the Sixth International Language Resources
and Evaluation (LREC?08), pages 2961?2968,
Marrakech.
Pustejovsky, James, Robert Knippen, Jessica
Littman, and Roser Saur??. 2005. Temporal
and event information in natural language
text. Language Resources and Evaluation,
39(2?3):123?164.
Pustejovsky, James, Mark Verhagen, Roser
Saur??, Jessica Littman, Robert Gaizauskas,
Graham Katz, Inderjeet Mani, Robert
Knippen, and Andrea Setzer. 2006.
Timebank 1.2. Linguistic Data
Consortium, Philadelphia, PA.
Pyysalo, Sampo, Filip Ginter, Juho
Heimonen, Jari Bjo?rne, Jorma Boberg,
Jouni Ja?rvinen, and Tapio Salakoski.
2007. BioInfer: A corpus for information
extraction in the biomedical domain.
BMC Bioinformatics, 8(50). doi:
10.1186/1471-2105-8-50.
Ramshaw, Lance and Mitch Marcus. 1995.
Text chunking using transformation-based
learning. In Proceedings of ACL Third
Workshop on Very Large Corpora,
pages 82?94, Cambridge, MA.
Rigoutsos, Isidore and Aros Floratos. 1998.
Combinatorial pattern discovery in
biological sequences: The TEIRESIAS
algorithm. Bioinformatics, 14(1):55?67.
Ritter, Alan, Stephen Soderland, Doug
Downey, and Oren Etzioni. 2008. It?s a
contradiction?no, it?s not: A case study
using functional relations. In Proceedings of
EMNLP 2008, pages 11?20, Honolulu, HI.
Rokach, Lior, Roni Romano, and Oded
Maimon. 2008. Negation recognition in
medical narrative reports. Information
Retrieval Online, 11(6):499?538.
Rubin, Victoria L. 2006. Identifying Certainty
in Texts. Ph.D. thesis, Syracuse University,
Syracuse, NY.
Rubin, Victoria L. 2007. Stating with
certainty or stating with doubt:
intercoder reliability results for manual
annotation of epistemically modalized
statements. In NAACL ?07: Human
Language Technologies 2007: The Conference
of the North American Chapter of the
Association for Computational Linguistics;
Companion Volume, Short Papers,
pages 141?144, Morristown, NJ.
Rubin, Victoria L., Elizabeth Liddy,
and Noriko Kando. 2005. Certainty
identification in texts: Categorization
model and manual tagging results. In
Computing Attitude and Affect in Text:
Theory and Applications, volume 20 of
Information Retrieval Series. Springer-
Verlag, New York, pages 61?76.
Ruppenhofer, Joseph, Caroline Sporleder,
Roser Morante, Colin Baker, and
Martha Palmer. 2010. Semeval-2010 task
10: Linking events and their participants
in discourse. In Proceedings of the 5th
International Workshop on Semantic
Evaluation, pages 45?50, Uppsala.
Salkie, Raphael, Pierre Busuttil, and
Johan van der Auwera. 2009. Introduction.
Modality in English. Theory and Description,
Mouton de Gruyter, Berlin, pages 1?8.
Sanchez-Graillet, Olivia and Massimo
Poesio. 2007. Negation of protein?protein
interactions: analysis and extraction.
Bioinformatics, 23(13):424?432.
Sarafraz, Farzaneh and Goran Nenadic.
2010a. Identification of negated
regulation events in the literature:
Exploring the feature space. In Proceedings
of the Symposium for Semantic Mining in
Biomedicine (SMBM 2010), pages 137?141,
Hinxton.
Sarafraz, Farzaneh and Goran Nenadic.
2010b. Using SVMs with the command
relation features to identify negated events
in biomedical literature. In Proceedings of
the Workshop on Negation and Speculation in
Natural Language Processing, pages 78?85,
Uppsala.
Saur??, Roser. 2008. A Factuality Profiler for
Eventualities in Text. Ph.D. thesis, Brandeis
University, Waltham, MA.
Saur??, Roser and James Pustejovsky. 2009.
FactBank: A corpus annotated with
event factuality. Language Resources and
Evaluation, 43(3):227?268.
258
Morante and Sporleder Modality and Negation
Saur??, Roser, Mark Verhagen, and James
Pustejovsky. 2006a. Annotating and
recognizing event modality in text.
In Proceedings of FLAIRS 2006,
pages 333?339, Melbourne.
Saur??, Roser, Mark Verhagen, and James
Pustejovsky. 2006b. SlinkET: A partial
modality parser for events. In Proceedings
of LREC 2006, pages 1332?1337, Genoa.
Seifert, Stephan and Werner Welte. 1987.
A Basic Bibliography of Negation in Natural
Language. Gu?nter Narr, Tu?bingen.
Shatkay, Hagit, Fengxia Pan, Andrey
Rzhetsky, and W. John Wilbur. 2008.
Multi-dimensional classification of
biomedical texts: Toward automated
practical provision of high-utility
text to diverse users. Bioinformatics,
24(18):2086?2093.
Skeppstedt, Maria. 2010. Negation detection
in Swedish clinical text. In Proceedings
of the NAACL HLT 2010 Second Louhi
Workshop on Text and Data Mining of Health
Documents, pages 15?21, Los Angeles, CA.
Snow, Rion, Lucy Vanderwende, and Arul
Menezes. 2006. Effectively using syntax for
recognizing false entailment. In Proceedings
of the Main Conference on Human Language
Technology Conference of the North American
Chapter of the Association of Computational
Linguistics, pages 33?40, Morristown, NJ.
Su, Qi, Chu-Ren Huang, and Helen Kai-yun
Chen. 2010. Evidentiality for text
trustworthiness detection. In Proceedings of
the 2010 Workshop on NLP and Linguistics:
Finding the Common Ground, pages 10?17,
Uppsala.
Szarvas, Gyo?rgy. 2008. Hedge classification
in biomedical texts with a weakly
supervised selection of keywords. In
Proceedings of ACL 2008, pages 281?289,
Columbus, OH.
Tang, Buzhou, Xiaolong Wang, Xuan Wang,
Bo Yuan, and Shixi Fan. 2010. A cascade
method for detecting hedges and their
scope in natural language text. In
Proceedings of the Fourteenth Conference
on Computational Natural Language
Learning, pages 13?17, Uppsala.
Thompson, Paul, Gilua Venturi, John
McNaught, Simonetta Montemagni, and
Sophia Ananiadou. 2008. Categorising
modality in biomedical texts. In Proceedings
of the LREC 2008 Workshop on Building and
Evaluating Resources for Biomedical Text
Mining 2008, pages 27?34, Marrakech.
Tottie, Gunnel. 1991. Negation in English
Speech and Writing: A Study in Variation.
Academic Press, New York.
Umbach, Carla. 2004. On the notion of
contrast in information structure and
discourse structure. Journal of Semantics,
21:155?175.
Uzuner, O?zlem, Xiaoran Zhang, and
Tawanda Sibanda. 2009. Machine learning
and rule-based approaches to assertion
classification. JAMIA, 16(1):109?115.
van der Auwera, Johan and Vladimir A.
Plungian. 1998. Modality?s semantic map.
Linguistic Typology, 2:79?124.
van der Wouden, Ton. 1997. Negative
Contexts: Collocation, Polarity, and Multiple
Negation. Routledge, London.
Velldal, Erik. 2011. Predicting speculation:
A simple disambiguation approach
to hedge detection in biomedical
literature. Journal of Biomedical Semantics,
2(Suppl 5):S7.
Vincze, Veronika, Gyo?rgy Szarvas, Richard
Farkas, Gyo?rgy Mo?ra, and Ja?nos Csirik.
2008. The BioScope corpus: biomedical
texts annotated for uncertainty, negation
and their scopes. BMC Bioinformatics,
9(Suppl 11):S9.
Vincze, Veronika, Gyo?rgy Szarvas, Gyo?rgy
Mo?ra, Tomoko Ohta, and Richard Farkas.
2010. Linguistic scope-based and
biological event-based speculation and
negation annotations in the genia event
and BioScope corpora. In Proceedings
of the Symposium for Semantic Mining in
Biomedicine (SMBM 2010), pages 84?92,
Hinxton.
von Fintel, Kai. 2006. Modality and
language. In D. M. Borchert, editor,
Encyclopedia of Philosophy. MacMillan
Reference, Detroit, MI, second edition.
Available at http://mit.edu/fintel/
fintel-2006-modality.pdf.
von Wright, Georg H. 1951. An Essay in
Modal Logic. North Holland, Amsterdam.
Wiebe, Janyce. 1994. Tracking point of view
in narrative. Computational Linguistics,
20(2):233?287.
Wiebe, Janyce, Theresa Wilson, Rebecca
Bruce, Matthew Bell, and Melanie Martin.
2004. Learning subjective language.
Computational Linguistics, 30(3):277?308.
Wiebe, Janyce, Theresa Wilson, and Claire
Cardie. 2005. Annotating expressions of
opinions and emotions in language.
Language Resources and Evaluation,
38:165?210.
Wiegand, Michael, Alexandra Balahur,
Benjamin Roth, Dietrich Klakow, and
Andre?s Montoyo. 2010. A survey on the
role of negation in sentiment analysis. In
Proceedings of the Workshop on Negation and
259
Computational Linguistics Volume 38, Number 2
Speculation in Natural Language Processing,
pages 60?68, Uppsala.
Wilbur, W. John, Andrey Rzhetsky, and
Hagit Shatkay. 2006. New directions in
biomedical text annotations: Definitions,
guidelines and corpus construction.
BMC Bioinformatics, 7:356.
Wilson, Theresa. 2008. Fine-grained
Subjectivity and Sentiment Analysis:
Recognizing the Intensity, Polarity, and
Attitudes of Private States. Ph.D. thesis,
University of Pittsburgh, Pittsburgh, PA.
Wilson, Theresa, Paul Hoffmann, Swapna
Somasundaran, Jason Kessler, Janyce
Wiebe, Yejin Choi, Claire Cardie, Ellen
Riloff, and Siddharth Patwardhan. 2005.
OpinionFinder: A system for subjectivity
analysis. In Proceedings of HLT/EMNLP on
Interactive Demonstrations, pages 34?35,
Morristown, NJ.
Wilson, Theresa, Janyce Wiebe, and Paul
Hoffman. 2005. Recognizing contextual
polarity in phrase-level sentiment
analysis. In Proceedings of HLT-EMNLP,
pages 347?354, Vancouver.
Wilson, Theresa, Janyce Wiebe, and Paul
Hoffman. 2009. Recognizing contextual
polarity: An exploration of features for
phrase-level analysis. Computational
Linguistics, 35(3):399?433.
Wilson, Theresa, Janyce Wiebe, and Rebecca
Hwa. 2006. Recognizing strong and weak
opinion clauses. Computational Intelligence,
22(2):73?99.
Zhu, Qiaoming, Junhui Li, Hongling Wang,
and Guodong Zhou. 2010. A unified
framework for scope learning via
simplified shallow semantic parsing.
In Proceedings of EMNLP 2010,
pages 714?724, Cambridge, MA.
260
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 45?50,
Uppsala, Sweden, 15-16 July 2010.
c?2010 Association for Computational Linguistics
SemEval-2010 Task 10:
Linking Events and Their Participants in Discourse
Josef Ruppenhofer and Caroline Sporleder
Computational Linguistics
Saarland University
{josefr,csporled}@coli.uni-sb.de
Roser Morante
CNTS
University of Antwerp
Roser.Morante@ua.ac.be
Collin Baker
ICSI
Berkeley, CA 94704
collin@icsi.berkeley.edu
Martha Palmer
Department of Linguistics
University of Colorado at Boulder
martha.palmer@colorado.edu
Abstract
We describe the SemEval-2010 shared
task on ?Linking Events and Their Partic-
ipants in Discourse?. This task is an ex-
tension to the classical semantic role label-
ing task. While semantic role labeling is
traditionally viewed as a sentence-internal
task, local semantic argument structures
clearly interact with each other in a larger
context, e.g., by sharing references to spe-
cific discourse entities or events. In the
shared task we looked at one particular as-
pect of cross-sentence links between ar-
gument structures, namely linking locally
uninstantiated roles to their co-referents
in the wider discourse context (if such
co-referents exist). This task is poten-
tially beneficial for a number of NLP ap-
plications, such as information extraction,
question answering or text summarization.
1 Introduction
Semantic role labeling (SRL) has been defined as
a sentence-level natural-language processing task
in which semantic roles are assigned to the syntac-
tic arguments of a predicate (Gildea and Jurafsky,
2002). Semantic roles describe the function of the
participants in an event. Identifying the seman-
tic roles of the predicates in a text allows knowing
who did what to whom when where how, etc.
However, semantic role labeling as it is cur-
rently defined misses a lot of information due to
the fact that it is viewed as a sentence-internal
task. Hence, relations between different local se-
mantic argument structures are disregarded. This
view of SRL as a sentence-internal task is partly
due to the fact that large-scale manual annotation
projects such as FrameNet
1
and PropBank
2
typ-
ically present their annotations lexicographically
by lemma rather than by source text.
It is clear that there is an interplay between lo-
cal argument structure and the surrounding dis-
course (Fillmore, 1977). In early work, Palmer et
al. (1986) discussed filling null complements from
context by using knowledge about individual pred-
icates and tendencies of referential chaining across
sentences. But so far there have been few attempts
to find links between argument structures across
clause and sentence boundaries explicitly on the
basis of semantic relations between the predicates
involved. Two notable exceptions are Fillmore and
Baker (2001) and Burchardt et al (2005). Fillmore
and Baker (2001) analyse a short newspaper arti-
cle and discuss how frame semantics could benefit
discourse processing but without making concrete
suggestions of how to model this. Burchardt et al
(2005) provide a detailed analysis of the links be-
tween the local semantic argument structures in a
short text; however their system is not fully imple-
mented either.
With the shared task, we aimed to make a first
step towards taking SRL beyond the domain of
individual sentences by linking local semantic ar-
gument structures to the wider discourse context.
The task addresses the problem of finding fillers
for roles which are neither instantiated as direct
dependents of our target predicates nor displaced
through long-distance dependency or coinstantia-
tion constructions. Often a referent for an unin-
stantiated role can be found in the wider context,
i.e. in preceding or following sentences. An ex-
ample is given in (1), where the CHARGES role
1
http://framenet.icsi.berkeley.edu/
2
http://verbs.colorado.edu/
?
mpalmer/
projects/ace.html
45
(ARG2 in PropBank) of cleared is left empty but
can be linked to murder in the previous sentence.
(1) In a lengthy court case the defendant was
tried for murder. In the end, he was
cleared.
Another very rich example is provided by (2),
where, for instance, the experiencer and the ob-
ject of jealousy are not overtly expressed as depen-
dents of the noun jealousy but can be inferred to be
Watson and the speaker, Holmes, respectively.
(2) Watson won?t allow that I know anything
of art but that is mere jealousy because our
views upon the subject differ.
This paper is organized as follows. In Section 2
we define how the concept of Null Instantiation
is understood in the task. Section 3 describes the
tasks to be performed, and Section 4, how they
are evaluated. Section 5 presents the participant
systems, and Section 6, their results. Finally, in
Section 7, we put forward some conclusions.
2 Null Instantiations
The theory of null complementation used here is
the one adopted by FrameNet, which derives from
the work of Fillmore (1986).
3
Briefly, omissions
of core arguments of predicates are categorized
along two dimensions, the licensor and the in-
terpretation they receive. The idea of a licensor
refers to the fact that either a particular lexical item
or a particular grammatical construction must be
present for the omission of a frame element (FE)
to occur. For instance, the omission of the agent in
(3) is licensed by the passive construction.
(3) No doubt, mistakes were made
0
Protagonist
.
The omission is a constructional omission be-
cause it can apply to any predicate with an appro-
priate semantics that allows it to combine with the
passive construction. On the other hand, the omis-
sion in (4) is lexically specific: the verb arrive al-
lows the Goal to be unspecified but the verb reach,
also a member of the Arriving frame, does not.
(4) We arrived 0
Goal
at 8pm.
3
Palmer et al?s (1986) treatment of uninstantiated ?essen-
tial roles? is very similar (see also Palmer (1990)).
The above two examples also illustrate the sec-
ond major dimension of variation. Whereas, in (3)
the protagonist making the mistake is only existen-
tially bound within the discourse (instance of in-
definite null instantiation, INI), the Goal location
in (4) is an entity that must be accessible to speaker
and hearer from the discourse or its context (def-
inite null instantiation, DNI). Finally, note that
the licensing construction or lexical item fully and
reliably determines the interpretation. Whereas
missing by-phrases have always an indefinite in-
terpretation, whenever arrive omits the Goal lexi-
cally, the Goal has to be interpreted as definite, as
it is in (4).
The import of this classification to the task here
is that we will concentrate on cases of DNI, be
they licensed lexically or constructionally.
3 Description of the Task
3.1 Tasks
We originally intended to offer the participants a
choice of two different tasks: a full task, in which
the test set was only annotated with gold stan-
dard word senses (i.e., frames) for the target words
and the participants had to perform role recogni-
tion/labeling and null instantiation linking, and a
NI only task, in which the test set was already
annotated with gold standard semantic argument
structures and the participants only had to recog-
nize definite null instantiations and find links to
antecedents in the wider context (NI linking).
However, it turned out that the basic semantic
role labeling task was already quite challenging
for our data set. Previous shared tasks have shown
that frame-semantic SRL of running text is a hard
problem (Baker et al, 2007), partly due to the fact
that running text is bound to contain many frames
for which no or little annotated training data are
available. In our case the difficulty was increased
because our data came from a new genre and do-
main (i.e., crime fiction, see Section 3.2). Hence,
we decided to add standard SRL, i.e., role recogni-
tion and labeling, as a third task (SRL only). This
task did not involve NI linking.
3.2 Data
The participants were allowed to make use of a va-
riety of data sources. We provided a training set
annotated with semantic argument structure and
null instantiation information. The annotations
were originally made using FrameNet-style and
46
later mapped semi-automatically to PropBank an-
notations, so that participants could choose which
framework they wanted to work in. The data for-
mats we used were TIGER/SALSA XML (Erk
and Pad?o, 2004) (FrameNet-style) and a modified
CoNLL-format (PropBank-style). As it turned
out, all participants chose to work on FrameNet-
style annotations, so we will not describe the Prop-
Bank annotation in this paper (see Ruppenhofer et
al. (2009) for more details).
FrameNet-style annotation of full text is ex-
tremely time-consuming. Since we also had to an-
notate null instantiations and co-reference chains
(for evaluation purposes, see Section 4), we could
only make available a limited amount of data.
Hence, we allowed participants to make use of ad-
ditional data, in particular the FrameNet and Prop-
Bank releases.
4
We envisaged that the participants
would want to use these additional data sets to
train SRL systems for the full task and to learn
something about typical fillers for different roles
in order to solve the NI linking task. The anno-
tated data sets we made available were meant to
provide additional information, e.g., about the typ-
ical distance between an NI and its filler and about
how to distinguish DNIs and INIs.
We annotated texts from two of Arthur Conan
Doyle?s fiction works. The text that served as
training data was taken from ?The Adventure of
Wisteria Lodge?. Of this lengthy, two-part story
we annotated the second part, titled ?The Tiger of
San Pedro?. The test set was made up of the last
two chapters of ?The Hound of the Baskervilles?.
We chose fiction rather than news because we be-
lieve that fiction texts with a linear narrative gen-
erally contain more context-resolvable NIs. They
also tend to be longer and have a simpler structure
than news texts, which typically revisit the same
facts repeatedly at different levels of detail (in the
so-called ?inverted pyramid? structure) and which
mix event reports with commentary and evalua-
tion, thus sequencing material that is understood
as running in parallel. Fiction texts should lend
themselves more readily to a first attempt at inte-
grating discourse structure into semantic role la-
beling. We chose Conan Doyle?s work because
most of his books are not subject to copyright any-
more, which allows us to freely release the anno-
tated data. Note, however, that this choice of data
4
For FrameNet we provided an intermediate release,
FrameNet 1.4 alpha, which contained more frames and lexi-
cal units than release 1.3.
means that our texts come from a different domain
and genre than many of the examples in FrameNet
and PropBank as well as making use of a some-
what older variety of English.
5
Table 1 provides basic statistics of the data sets.
The training data had 3.1 frames per sentence and
the test data 3.2, which is lower than the 8.8 frames
per sentence in the test data of the 2007 SemEval
task on Frame Semantic Structure Extraction.
6
We
think this is mainly the result of switching to a do-
main different from the bulk of what FrameNet
has made available in the way of full-text anno-
tation. In doing so, we encountered many new
frames and lexical units for which we could not
ourselves create the necessary frames and pro-
vide lexicographic annotations. The statistics also
show that null-instantiation is relatively common:
in the training data, about 18.7% of all FEs are
omitted, and in the test set, about 18.4%. Of the
DNIs, 80.9% had an antecedent in the training
data, and 74.2% in the test data.
To ensure a high quality of the annotations, both
data sets were annotated by more than one person
and then adjudicated. The training set was an-
notated independently by two experienced anno-
tators and then adjudicated by the same two peo-
ple. The test set was annotated by three annota-
tors and then adjudicated by the two experienced
annotators. Throughout the annotation and adju-
dication process, we discussed difficult cases and
also maintained a wiki. Additionally, we created a
software tool that checked the consistency of our
annotations against the frame, frame element and
FE-relation specifications of FrameNet and alerted
annotators to problems with their annotations. The
average agreement (F-score) for frame assignment
for pairs of annotators on the two chapters in the
test set ranges from 0.7385 to 0.7870. The agree-
ment of individual annotators with the adjudicated
gold standard ranges from 0.666 to 0.798. Given
that the gold standard for the two chapters features
228 and 229 different frame types, respectively,
this level of agreement seems quite good.
5
While PropBank provides annotations for the Penn Tree-
bank and is thus news-based, the lexicographic annotations
in FrameNet are extracted from the BNC, a balanced cor-
pus. The FrameNet full-text annotations, however, only cover
three domains: news, travel guides, and nuclear proliferation
reports.
6
The statistics in Table 1 and all our discussion of the
data includes only instances of semantic frames and ignores
the instances of the Coreference, Support, and Relativization
frames, which we labeled on the data as auxiliary informa-
tion.
47
data set sentences tokens frame inst. frame types overt FEs DNIs (resolved) INIs
train 438 7,941 1,370 317 2,526 303 (245) 277
test 525 9,131 1,703 452 3,141 349 (259) 361
Table 1: Statistics for the provided data sets
For the annotation of NIs and their links to the
surrounding discourse we created new guidelines
as this was a novel annotation task. We adopted
ideas from the annotation of co-reference informa-
tion, linking locally unrealized roles to all men-
tions of the referents in the surrounding discourse,
where available. We marked only identity rela-
tions but not part-whole or bridging relations be-
tween referents. The set of unrealized roles un-
der consideration includes only the core arguments
but not adjuncts (peripheral or extra-thematic roles
in FrameNet?s terminology). Possible antecedents
are not restricted to noun phrases but include all
constituents that can be (local) role fillers for
some predicate plus complete sentences (which
can sometimes fill roles such as MESSAGE).
4 Evaluation
As noted above, we allowed participants to ad-
dress three different tasks: SRL only, NI only,
full task. For role recognition and labeling we
used a standard evaluation set-up, i.e., accuracy for
role labeling and precision, recall, F-Score for role
recognition.
The NI linkings were evaluated slightly differ-
ently. In the gold standard, we identified refer-
ents for null instantiations in the discourse con-
text. In some cases, more than one referent might
be appropriate, e.g., because the omitted argument
refers to an entity that is mentioned multiple times
in the context. In this case, a system is given credit
if the NI is linked to any of these expressions. To
achieve this we create equivalence sets for the ref-
erents of NIs (by annotating coreference chains).
If the NI is linked to any item in the equivalence
set, the link is counted as a true positive. We can
then define NI linking precision as the number
of all true positive links divided by the number of
links made by a system, and NI linking recall as
the number of true positive links divided by the
number of links between an NI and its equivalence
set in the gold standard. NI linking F-Score is
then the harmonic mean between NI linking preci-
sion and recall.
Since it may sometimes be difficult to deter-
mine the correct extent of the filler of an NI, we
score an automatic annotation as correct if it in-
cludes the head of the gold standard filler in the
predicted filler. However, in order to not favor sys-
tems which link NIs to very large spans of text to
maximize the likelihood of linking to a correct ref-
erent, we introduce a second evaluation measure,
which computes the overlap (Dice coefficient) be-
tween the words in the predicted filler (P) of an NI
and the words in the gold standard one (G):
NI linking overlap =
2|P ?G|
|P | + |G|
(5)
Example (6) illustrates this point. The verb
won in the second sentence evokes the Fin-
ish competition frame whose COMPETITION role
is omitted. From the context it is clear that the
competition role is semantically filled by their first
TV debate (head: debate) and last night?s debate
(head: debate) in the previous sentences. These
two expressions form the equivalence set for the
COMPETITION role in the last sentence. Any sys-
tem that would predict a linkage to a filler that
covers the head of either of these two expressions
would score a true positive for this NI. However,
a system that linked to last night?s debate would
have an NI linking overlap of 1 (i.e., 2*3/(3+3))
while a system linking the whole second sentence
Last night?s debate was eagerly anticipated to the
NI would have an overlap of 0.67 (i.e., 2*3/(6+3))
(6) US presidential rivals Republican John
McCain and Democrat Barack Obama
have yesterday evening attacked each
other over foreign policy and the econ-
omy, in [their first TV debate]
Competition
.
[Last night?s debate]
Competition
was ea-
gerly anticipated. Two national flash
polls suggest that [Obama]
Competitor
won
Finish competition
0
Competition
.
5 Participating Systems
While a fair number of people expressed an inter-
est in the task and 26 groups or individuals down-
loaded the data sets, only three groups submitted
48
results for evaluation. Feedback from the teams
that downloaded the data suggests that this was
due to coinciding deadlines and to the difficulty
and novelty of the task. Only the SEMAFOR
group addressed the full task, using a pipeline of
argument recognition followed by NI identifica-
tion and resolution. Two groups (GETARUNS++
and SEMAFOR) tackled the NI only task, and
also two groups, the SRL only task (CLR and SE-
MAFOR
7
).
All participating systems were built upon ex-
isting systems for semantic processing which
were modified for the task. Two of the groups,
GETARUNS++ and CLR, employed relatively
deep semantic processing, while the third, SE-
MAFOR, employed a shallower probabilistic sys-
tem. Different approaches were taken for NI link-
ing. The SEMAFOR group modeled NI linking as
a variant of role recognition and labeling by ex-
tending the set of potential arguments beyond the
locally available arguments to also include noun
phrases from the previous sentence. The system
then uses, among other information, distributional
semantic similarity between the heads of potential
arguments and role fillers in the training data. The
GETARUNS++ group applied an existing system
for deep semantic processing, anaphora resolution
and recognition of textual entailment, to the task.
The system analyzes the sentences and assigns its
own set of labels, which are subsequently mapped
to frame semantic categories. For more details of
the participating systems please consult the sepa-
rate system papers.
6 Results and Analysis
6.1 SRL Task
Argument Recognition Label
Prec. Rec. F1 Acc.
SHA 0.6332 0.3884 0.4812 0.3471
SEM 0.6528 0.4674 0.5448 0.4184
CLR 0.6702 0.1121 0.1921 0.1093
Table 2: Shalmaneser (SHA), SEMAFOR (SEM)
and CLR performance on the SRL task (across
both chapters)
The results on the SRL task are shown in Table
2. To get a better sense of how good the perfor-
mance of the submitted systems was on this task,
7
For SEMAFOR, this was the first step of their pipeline.
we applied the Shalmaneser statistical semantic
parser (Erk and Pad?o, 2006) to our test data and
report the results. Note, however, that we used a
Shalmaneser trained only on FrameNet version 1.3
which is different from the version 1.4 alpha that
was used in the task, so its results are lower than
what can be expected with release 1.4 alpha.
We observe that although the SEMAFOR and
the CLR systems score a higher precision than
Shalmaneser for argument recognition, the SE-
MAFOR system scores considerably higher recall
than Shalmaneser, whereas the CLR system scores
a much lower recall.
6.2 NI Task
Tackling the resolution of NIs proved to be a dif-
ficult problem due to a variety of factors. First,
the NI sub-task was completely new and involves
several steps of linguistic processing. It also is
inherently difficult in that a given FE is not al-
ways omitted with the same interpretation. For
instance, the Content FE of the Awareness frame
evoked by know is interpreted as indefinite in
the blog headline More babbling about what it
means to know but as definite in a discourse
like Don?t tell me you didn?t know!. Second,
prior to this SemEval task there was no full-text
training data available that contained annotations
with all the kinds of information that is relevant
to the task, namely overt FEs, null-instantiated
FEs, resolutions of null-instantiations, and coref-
erence. Third, the data we used also represented
a switch to a new domain compared to existing
FrameNet full-text annotation, which comes from
newspapers, travel guides, and the nuclear pro-
liferation domain. Our most frequent frame was
Observable bodyparts, whereas it is Weapons in
FrameNet full-text. Fourth, it was not well un-
derstood at the beginning of the task that, in cer-
tain cases, FrameNet?s null-instantiation annota-
tions for a given FE cannot be treated in isolation
of the annotations of other FEs. Specifically, null-
instantiation annotations interact with the set of re-
lations between core FEs that FrameNet uses in its
analyses. As an example, consider the CoreSet re-
lation, which specifies that from a set of core FEs
at least one must be instantiated overtly, though
more of them can be. As long as one of the FEs
in the set is expressed overtly, null-instantiation is
not annotated for the other FEs in the set. For
instance, in the Statement frame, the two FEs
49
Topic and Message are in one CoreSet and the
two FEs Speaker and Medium are in another. If
a frame instance occurs with an overt Speaker and
an overt Topic, the Medium and Message FEs are
not marked as null-instantiated. Automatic sys-
tems that treat each core FE separately, may pro-
pose DNI annotations for Medium and Message,
resulting in false positives.
Therefore, we think that the evaluation that we
initially defined was too demanding for a novel
task. It would have been better to give sepa-
rate scores for 1) ability to recognize when a core
FE has to be treated as null-instantiated; 2) abil-
ity to distinguish INI and DNI; and 3) ability to
find antecedents. The systems did have to tackle
these steps anyway and an analysis of the sys-
tem output shows that they did so with different
success. The two chapters of our test data con-
tained a total of 710 null instantiations, of which
349 were DNI and 361 INI. The SEMAFOR sys-
tem recognized 63.4% (450/710) of the cases of
NI, while the GETARUNS++ system found only
8.0% (57/710). The distinction between DNI and
INI proved very difficult, too. Of the NIs that
the SEMAFOR system correctly identified, 54.7%
(246/450) received the correct interpretation type
(DNI or INI). For GETARUNS++, the percentage
is higher at 64.2% (35/57), but also based on fewer
proposed classifications. A simple majority-class
baseline gives a 50.8% accuracy. Interestingly, the
SEMAFOR system labeled many more INIs than
DNIs, thus often misclassifying DNIs as INI. The
GETARUNS++ system applied both labels about
equally often.
7 Conclusion
In this paper we described the SemEval-2010
shared task on ?Linking Events and Their Partic-
ipants in Discourse?. The task is novel, in that it
tackles a semantic cross-clausal phenomenon that
has not been treated before in a task, namely, link-
ing locally uninstantiated roles to their coreferents
at the text level. In that sense the task represents
a first step towards taking SRL beyond the sen-
tence level. A new corpus of fiction texts has been
annotated for the task with several types of seman-
tic information: semantic argument structure, co-
reference chains and NIs. The results scored by
the systems in the NI task and the feedback from
participant teams shows that the task was more dif-
ficult than initially estimated and that the evalua-
tion should have focused on more specific aspects
of the NI phenomenon, rather than on the com-
pleteness of the task. Future work will focus on
modeling the task taking this into account.
Acknowledgements
Josef Ruppenhofer and Caroline Sporleder are supported
by the German Research Foundation DFG (under grant PI
154/9-3 and the Cluster of Excellence Multimodal Comput-
ing and Interaction (MMCI), respectively). Roser Morante?s
research is funded by the GOA project BIOGRAPH of the
University of Antwerp. We would like to thank Jinho Choi,
Markus Dr?ager, Lisa Fuchs, Philip John Gorinski, Russell
Lee-Goldman, Ines Rehbein, and Corinna Schorr for their
help with preparing the data and/or implementing software
for the task. Thanks also to the SemEval-2010 Chairs Katrin
Erk and Carlo Strapparava for their support during the task
organization period.
References
C. Baker, M. Ellsworth, K. Erk. 2007. SemEval-2007
Task 19: Frame semantic structure extraction. In
Proceedings of SemEval-07.
A. Burchardt, A. Frank, M. Pinkal. 2005. Building text
meaning representations from contextually related
frames ? A case study. In Proceedings of IWCS-6.
K. Erk, S. Pad?o. 2004. A powerful and versatile XML
format for representing role-semantic annotation. In
Proceedings of LREC-2004.
K. Erk, S. Pad?o. 2006. Shalmaneser - a flexible tool-
box for semantic role assignment. In Proceedings of
LREC-06.
C. Fillmore, C. Baker. 2001. Frame semantics for text
understanding. In Proc. of the NAACL-01 Workshop
on WordNet and Other Lexical Resources.
C. Fillmore. 1977. Scenes-and-frames semantics, lin-
guistic structures processing. In A. Zampolli, ed.,
Fundamental Studies in Computer Science, No. 59,
55?88. North Holland Publishing.
C. Fillmore. 1986. Pragmatically controlled zero
anaphora. In Proceedings of the Twelfth Annual
Meeting of the Berkeley Liguistics Society.
D. Gildea, D. Jurafsky. 2002. Automatic label-
ing of semantic roles. Computational Linguistics,
28(3):245?288.
M. Palmer, D. Dahl, R. Passonneau, L. Hirschman,
M. Linebarger, J. Dowding. 1986. Recovering im-
plicit information. In Proceedings of ACL-1986.
M. Palmer. 1990. Semantic Processing for Finite Do-
mains. CUP, Cambridge, England.
J. Ruppenhofer, C. Sporleder, R. Morante, C. Baker,
M. Palmer. 2009. Semeval-2010 task 10: Linking
events and their participants in discourse. In The
NAACL-HLT 2009 Workshop on Semantic Evalua-
tions: Recent Achievements and Future Directions
(SEW-09).
50
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 265?274,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
*SEM 2012 Shared Task: Resolving the Scope and Focus of Negation
Roser Morante
CLiPS - University of Antwerp
Prinsstraat 13, B-2000 Antwerp, Belgium
Roser.Morante@ua.ac.be
Eduardo Blanco
Lymba Corporation
Richardson, TX 75080 USA
eduardo@lymba.com
Abstract
The Joint Conference on Lexical and Compu-
tational Semantics (*SEM) each year hosts a
shared task on semantic related topics. In its
first edition held in 2012, the shared task was
dedicated to resolving the scope and focus of
negation. This paper presents the specifica-
tions, datasets and evaluation criteria of the
task. An overview of participating systems is
provided and their results are summarized.
1 Introduction
Semantic representation of text has received consid-
erable attention these past years. While early shal-
low approaches have been proven useful for several
natural language processing applications (Wu and
Fung, 2009; Surdeanu et al, 2003; Shen and La-
pata, 2007), the field is moving towards analyzing
and processing complex linguistic phenomena, such
as metaphor (Shutova, 2010) or modality and nega-
tion (Morante and Sporleder, 2012).
The *SEM 2012 Shared Task is devoted to nega-
tion, specifically, to resolving its scope and focus.
Negation is a grammatical category that comprises
devices used to reverse the truth value of proposi-
tions. Broadly speaking, scope is the part of the
meaning that is negated and focus the part of the
scope that is most prominently or explicitly negated
(Huddleston and Pullum, 2002). Although negation
is a very relevant and complex semantic aspect of
language, current proposals to annotate meaning ei-
ther dismiss negation or only treat it in a partial man-
ner.
The interest in automatically processing nega-
tion originated in the medical domain (Chapman
et al, 2001), since clinical reports and discharge
summaries must be reliably interpreted and indexed.
The annotation of negation and hedge cues and their
scope in the BioScope corpus (Vincze et al, 2008)
represented a pioneering effort. This corpus boosted
research on scope resolution, especially since it was
used in the CoNLL 2010 Shared Task (CoNLL
ST 2010) on hedge detection (Farkas et al, 2010).
Negation has also been studied in sentiment analy-
sis (Wiegand et al, 2010) as a means to determine
the polarity of sentiments and opinions.
Whereas several scope detectors have been de-
veloped using BioScope (Morante and Daelemans,
2009; Velldal et al, 2012), there is a lack of cor-
pora and tools to process negation in general domain
texts. This is why we have prepared new corpora
for scope and focus detection. Scope is annotated
in Conan Doyle stories (CD-SCO corpus). For each
negation, the cue, its scope and the negated event, if
any, are marked as shown in example (1a). Focus is
annotated on top of PropBank, which uses the WSJ
section of the Penn TreeBank (PB-FOC corpus). Fo-
cus annotation is restricted to verbal negations an-
notated with MNEG in PropBank, and all the words
belonging to a semantic role are selected as focus.
An annotated example is shown in (1b)1.
(1) a. [John had] never [said as much before]
b. John had never said {as much} before
The rest of this paper is organized as follows.
The two proposed tasks are described in Section 2,
and the corpora in Section 3. Participating systems
and their results are summarized in Section 4. The
approaches used by participating systems are de-
scribed in Section 5, as well as the analysis of re-
sults. Finally, Section 6 concludes the paper.
1Throughout this paper, negation cues are marked in bold
letters, scopes are enclosed in square brackets and negated
events are underlined; focus is enclosed in curly brackets.
265
2 Task description
The *SEM 2012 Shared Task2 was dedicated to re-
solving the scope and focus of negation (Task 1 and
2 respectively). Participants were allowed to engage
in any combination of tasks and submit at most two
runs per task. A pilot task combining scope and
focus detection was initially planned, but was can-
celled due to lack of participation. We received a
total of 14 runs, 12 for scope detection (7 closed, 5
open) and 2 for focus detection (0 closed, 2 open).
Submissions fall into two tracks:
? Closed track. Systems are built using exclusively
the annotations provided in the training set and are
tuned with the development set. Systems that do
not use external tools to process the input text or
that modify the annotations provided (e.g., simplify
parse tree, concatenate lists of POS tags, ) fall under
this track.
? Open track. Systems can make use of any external
resource or tool. For example, if a team uses an ex-
ternal semantic parser, named entity recognizer or
obtains the lemma for each token by querying ex-
ternal resources, it falls under the open track. The
tools used cannot have been developed or tuned us-
ing the annotations of the test set.
Regardless of the track, teams were allowed to
submit their final results on the test set using a sys-
tem trained on both the training and development
sets. The data format is the same as in several pre-
vious CoNLL Shared Tasks (Surdeanu et al, 2008).
Sentences are separated by a blank line. Each sen-
tence consists of a sequence of tokens, and a new
line is used for each token.
2.1 Task 1: Scope Resolution
Task 1 aimed at resolving the scope of negation cues
and detecting negated events. The task is divided
into 3 subtasks:
1. Identifying negation cues, i.e., words that express
negation. Cues can be single words (e.g., never),
multiwords (e.g., no longer, by no means), or affixes
(e.g.l im-, -less). Note that negation cues can be
discontinuous, e.g., neither [. . . ] nor.
2. Resolving the scope of negation. This subtask ad-
dresses the problem of determining which tokens
within a sentence are affected by the negation cue.
A scope is a sequence of tokens that can be discon-
tinuous.
2www.clips.ua.ac.be/sem2012-st-neg/
3. Identifying the negated event or property, if any.
The negated event or property is always within the
scope of a cue. Only factual events can be negated.
For the sentence in (2), systems have to identify
no and nothing as negation cues, after his habit he
said and after mine I asked questions as scopes, and
said and asked as negated events.
(2) [After his habit he said] nothing, and after mine I
asked no questions.
After his habit he said nothing, and [after mine I
asked] no [questions].
2.1.1 Evaluation measures
Previously, scope resolvers have been evaluated at
either the token or scope level. The token level eval-
uation checks whether each token is correctly la-
beled (inside or outside the scope), while the scope
level evaluation checks whether the full scope is cor-
rectly labeled. The CoNLL 2010 ST introduced pre-
cision and recall at scope level as performance mea-
sures and established the following requirements: A
true positive (TP) requires an exact match for both
the negation cue and the scope. False positives (FP)
occur when a system predicts a non-existing scope
in gold, or when it incorrectly predicts a scope exist-
ing in gold because: (1) the negation cue is correct
but the scope is incorrect; (2) the cue is incorrect
but the scope is correct; (3) both cue and scope are
incorrect. These three scenarios also trigger a false
negative (FN). Finally, FN also occur when the gold
annotations specify a scope but the system makes no
such prediction (Farkas et al, 2010).
As we see it, the CONLL 2010 ST evaluation
requirements were somewhat strict because for a
scope to be counted as TP, the negation cue had
to be correctly identified (strict match) as well as
the punctuation tokens within the scope. Addi-
tionally, this evaluation penalizes partially correct
scopes more than fully missed scopes, since partially
correct scopes count as FP and FN, whereas missed
scopes count only as FN. This is a standard prob-
lem when applying the F measures to the evaluation
of sequences. For this shared task we have adopted
a slightly different approach based on the following
criteria:
? Punctuation tokens are ignored.
? We provide a scope level measure that does not re-
quire strict cue match. To count a scope as TP this
266
measure requires that only one cue token is cor-
rectly identified, instead of all cue tokens.
? To count a negated event as TP we do not require
correct identification of the cue.
? To evaluate cues, scopes and negated events, partial
matches are not counted as FP, only as FN. This is to
avoid penalizing partial matches more than missed
matches.
The following evaluation measures have been
used to evaluate the systems:
? Cue-level F1-measures (Cue).
? Scope-level F1-measures that require only partial
cue match (Scope NCM).
? Scope-level F1-measures that require strict cue
match (Scope CM). In this case, all tokens of the
cue have to be correctly identified.
? F1-measure over negated events (Negated), com-
puted independently from cues and from scopes.
? Global F1-measure of negation (Global): the three
elements of the negation ? cue, scope and negated
event ? all have to be correctly identified (strict
match).
? F1-measure over scope tokens (Scope tokens). The
total of scope tokens in a sentence is the sum of to-
kens of all scopes. For example, if a sentence has
two scopes, one of five tokens and another of seven
tokens, then the total of scope tokens is twelve.
? Percentage of correct negation sentences (CNS).
A second version of the measures (Cue/Scope
CM/Scope NCM/Negated/Global-B) was calculated
and provided to participants, but was not used to
rank the systems, because it was introduced in the
last period of the development phase following the
request of a participant team. In the B version of the
measures, precision is not counted as (TP/(TP+FP)),
but as (TP / total of system predictions), counting in
this way the percentage of perfect matches among
all the system predictions. Providing this version of
the measures also allowed us to compare the results
of the two versions and to check if systems would
be ranked in a different position depending on the
version.
Even though we believe that relaxing scope eval-
uation by ignoring punctuation marks and relaxing
the strict cue match requirement is a positive feature
of our evaluation, we need to explore further in order
to define a scope evaluation measure that captures
the impact of partial matches in the scores.
2.2 Task 2: Focus Detection
This task tackles focus of negation detection. Both
scope and focus are tightly connected. Scope is the
part of the meaning that is negated and focus is that
part of the scope that is most prominently or explic-
itly negated (Huddleston and Pullum, 2002). Focus
can also be defined as the element of the scope that is
intended to be interpreted as false to make the over-
all negative true.
Detecting focus of negation is useful for retriev-
ing the numerous words that contribute to implicit
positive meanings within a negation. Consider the
statement The government didn?t release the UFO
files {until 2008}. The focus is until 2008, yielding
the interpretation The government released the UFO
files, but not until 1998. Once the focus is resolved,
the verb release, its AGENT The government and its
THEME the UFO files are positive; only the TEMPO-
RAL information until 2008 remains negated.
We only target verbal negations and focus is al-
ways the full text of a semantic role. Some examples
of annotation and their interpretation (Int) using fo-
cus detection are provided in (3?5).
(3) Even if that deal isn?t {revived}, NBC hopes to
find another.
Int: Even if that deal is suppressed, NBC hopes to
find another.
(4) A decision isn?t expected {until some time next
year}.
Int: A decision is expected at some time next year.
(5) . . . it told the SEC it couldn?t provide financial
statements by the end of its first extension
?{without unreasonable burden or expense}?.
Int: It could provide them by that time with a huge
overhead.
2.2.1 Evaluation measures
Task 2 is evaluated using precision, recall and F1.
Submissions are ranked by F1. For each negation,
the predicted focus is considered correct if it is a per-
fect match with the gold annotations.
3 Data Sets
We have released two datasets, which will be avail-
able from the web site of the task: CD-SCO for
scope detection and PB-FOC for focus detection.
The next two sections introduce the datasets.
267
WL2 108 0 After After IN (S(S(PP* After
WL2 108 1 his his PRP$ (NP* his
WL2 108 2 habit habit NN *)) habit
WL2 108 3 he he PRP (NP*) he
WL2 108 4 said say VBD (VP* said said
WL2 108 5 nothing nothing NN (NP*))) nothing
WL2 108 6 , , , *
WL2 108 7 and and CC *
WL2 108 8 after after IN (S(PP* after
WL2 108 9 mine mine NN (NP*)) mine
WL2 108 10 I I PRP (NP*) I
WL2 108 11 asked ask VBD (VP* asked asked
WL2 108 12 no no DT (NP* no
WL2 108 13 questions question NNS *))) questions
WL2 108 14 . . . *)
Figure 1: Example sentence from CD-SCO.
3.1 CD-SCO: Scope Annotation
The corpus for Task 1 is CD-SCO, a corpus of Co-
nan Doyle stories. The training corpus contains The
Hound of the Baskervilles, the development corpus,
The Adventure of Wisteria Lodge, and the test corpus
The Adventure of the Red Circle and The Adventure
of the Cardboard Box. The original texts are freely
available from the Gutenberg Project.3
CD-SCO is annotated with negation cues and
their scope, as well as the event or property that is
negated. The cues are the words that express nega-
tion and the scope is the part of a sentence that is
affected by the negation cues. The negated event
or property is the main event or property actually
negated by the negation cue. An event can be a pro-
cess, an action, or a state.
Figure 1 shows an example sentence. Column 1
contains the name of the file, column 2 the sentence
#, column 3 the token #, column 4 the word, column
5 the lemma, column 6 the PoS, column 7 the parse
tree information and columns 8 to end the negation
information. If a sentence does not contain a nega-
tion, column 8 contains ?***? and there are no more
columns. If it does contain negations, the informa-
tion for each one is encoded in three columns: nega-
tion cue, scope, and negated event respectively.
The annotation of cues and scopes is inspired by
the BioScope corpus, but there are several differ-
ences. First and foremost, BioScope does not an-
notate the negated event or property. Another im-
3http://www.gutenberg.org/browse/
authors/d\#a37238
Training Dev. Test
# tokens 65,450 13,566 19,216
# sentences 3644 787 1089
# negation sent. 848 144 235
% negation sent. 23.27 18.29 21.57
# cues 984 173 264
# unique cues 30 20 20
# scopes 887 168 249
# negated 616 122 173
Table 1: CD-SCO Corpus statistics.
portant difference concerns the scope model itself:
in CD-SCO, the cue is not considered to be part of
the scope. Furthermore, scopes can be discontinu-
ous and all arguments of the negated event are con-
sidered to be part of the scope, including the subject,
which is kept out of the scope in BioScope. A final
difference is that affixal negation is annotated in CD-
SCO, as in (6).
(6) [He] declares that he heard cries but [is] un[{able}
to state from what direction they came].
Statistics for the corpus is presented in Table 1.
More information about the annotation guidelines is
provided by Morante et al (2011) and Morante and
Daelemans (2012), including inter-annotator agree-
ment.
The corpus was preprocessed at the University
of Oslo. Tokenization was obtained by the PTB-
compliant tokenizer that is part of the LinGO En-
glish Resource Grammar. 4
4http://moin.delph-in.net/
268
Apart from the gold annotations, the corpus was
provided to participants with additional annotations:
? Lemmatization using the GENIA tagger (Tsuruoka
and Tsujii, 2005), version 3.0.1, with the ?-nt? com-
mand line option. GENIA PoS tags are comple-
mented with TnT PoS tags for increased compati-
bility with the original PTB.
? Parsing with the Charniak and Johnson (2005) re-
ranking parser.5 For compatibility with PTB con-
ventions, the top-level nodes in parse trees (?S1?),
were removed. The conversion of PTB-style syntax
trees into CoNLL-style format was performed using
the CoNLL 2005 Shared Task software.6
3.2 PB-FOC: Focus Annotation
We have adapted the only previous annotation effort
targeting focus of negation for PB-FOC (Blanco and
Moldovan, 2011). This corpus provides focus an-
notation on top of PropBank. It targets exclusively
verbal negations marked with MNEG in PropBank
and selects as focus the semantic role containing the
most likely focus. The motivation behind their ap-
proach, annotation guidelines and examples can be
found in the aforementioned paper.
We gathered all negations from sections 02?21,
23 and 24 and discarded negations for which the fo-
cus or PropBank annotations were not sound, leav-
ing 3,544 instances.7 For each verbal negation, PB-
FOC provides the current sentence, and the previous
and next sentences as context. For each sentence,
along with the gold focus annotations, PB-FOC con-
tains the following additional annotations:
? Token number;
? POS tags using the Brill tagger (Brill, 1992);
? Named Entities using the Stanford named en-
tity recognizer recognizer (Finkel et al, 2005);
? Chunks using the chunker by Phan (2006);
? Syntactic tree using the Charniak parser (Char-
niak, 2000);
? Dependency tree derived from the syntactic
tree (de Marneffe et al, 2006);
ErgTokenization, http://moin.delph-in.net/
ReppTop
5November 2009 release available from Brown University.
6http://www.lsi.upc.edu/?srlconll/
srlconll-1.1.tgz
7The original focus annotation targeted the 3,993 negations
marked with MNEG in the whole PropBank.
Train Devel Test
1 role 2,210 515 672
2 roles 89 15 38
3 roles 3 0 2
All 2,302 530 712
S
em
an
ti
c
ro
le
s
fo
cu
s
be
lo
ng
s
to
A1 980 222 309
AM-NEG 592 138 172
AM-TMP 161 35 46
AM-MNR 127 27 38
A2 112 28 36
A0 94 23 31
None 88 19 35
AM-ADV 78 23 26
C-A1 46 6 16
AM-PNC 33 8 12
AM-LOC 25 4 10
A4 11 2 5
R-A1 10 2 2
Other 40 8 16
Table 2: Basic numeric analysis for PB-FOC. The first 4
rows indicate the number of unique roles each negation
belongs to, the rest indicate the counts for each role.
? Semantic roles using the labeler described by
(Punyakanok et al, 2008); and
? Verbal negation, indicates with ?N? if that token
correspond to a verbal negation for which focus
must be predicted.
Figure 2 provides a sample of PB-FOC. Know-
ing that the original focus annotations were done on
top of PropBank and that focus corresponds to a sin-
gle role, semantic role information is key to predict
the focus. In Table 2, we show some basic numeric
analysis regarding focus annotation and the automat-
ically obtained semantic role labels. Most instances
of focus belong to a single role in the three splits
and the most common role focus belongs to is A1,
followed by AM-NEG, M-TMP and M-MNR. Note
that some instances have at least one word that does
not belong to any role (88 in training, 19 in develop-
ment and 35 in test).
4 Submissions and results
A total of 14 runs were submitted: 12 for scope de-
tection and 2 for focus detection. The unbalanced
number of submissions might be due to the fact that
both tasks are relatively new and the tight timeline
(six weeks) under which systems were developed.
269
Marketers 1 NNS O B-NP (S1(S(NP*) 2 nsubj (A0*) * - *
believe 2 VBP O B-VP (VP* 0 root (V*) * - *
most 3 RBS O B-NP (SBAR(S(NP* 4 amod (A1* (A0* - FOCUS
Americans 4 NNPS O I-NP *) 7 nsubj * *) - FOCUS
wo 5 MD O B-VP (VP* 7 aux * (AM-MOD*) - *
n?t 6 RB O I-VP * 7 neg * (AM-NEG*) - *
make 7 VB O I-VP (VP* 2 ccomp * (V*) N *
the 8 DT O B-NP (NP* 10 det * (A1* - *
convenience 9 NN O I-NP * 10 nn * * - *
trade-off 10 NN O I-NP *)))))) 7 dobj *) *) - *
... 11 : O O * 2 punct * * - *
. 12 . O O *)) 2 punct * * - *
Figure 2: Example sentence from PB-FOC.
Team Prec. Rec. F1
O
pe
n UConcordia, run 1 60.00 56.88 58.40
UConcordia, run 2 59.85 56.74 58.26
Table 3: Official results for Task 2.
Some participants showed interest in the second task
and expressed that they did not participate because
of lack of time. In this section, we present the results
for each task.
4.1 Task 1
Six teams (UiO1, UiO2, FBK, UWashington,
UMichigan, UABCoRAL) submitted results for the
closed track with a total of seven runs, and four
teams (UiO2, UGroningen, UCM-1, UCM-2) sub-
mitted results for the open track with a total of five
runs. The evaluation results are provided in Ta-
ble 4, which contains the official results, and Table 5,
which contains the results for evaluation measures
B.
The best Global score in the closed track was ob-
tained by UiO1 (57.63 F1). The best score for Cues
was obtained by FBK (92.34 F1), for Scopes CM
by UiO2 (73.39 F1), for Scopes NCM by UWash-
ington (72.40 F1), and for Negated by UiO1 (67.02
F1). The best Global score in the open track was ob-
tained by UiO2 (54.82 F1), as well as the best scores
for Cues (91.31 F1), Scopes CM (72.39 F1), Scopes
NCM (72.39 F1), and Negated (61.79 F1).
4.2 Task 2
Only one team participated in Task 2, UConcordia
from CLaC Lab at Concordia University. They sub-
mitted two runs and the official results are summa-
rized in Table 3. Their best run scored 58.40 F1.
5 Approaches and analysis
In this section we summarize the methodologies ap-
plied by participants to solve the tasks and we ana-
lyze the results.
5.1 Task 1
To solve Task 1 most teams develop a three module
pipeline with a module per subtask. Scope resolu-
tion and negated event detection are independent of
each other and both depend on cue detection. An
exception is the UiO1 system, which incorporates a
module for factuality detection. Most systems ap-
ply machine learning algorithms, either Conditional
Random Fields (CRFs) or Support Vector Machines
(SVMs), while less systems implement a rule-based
approach. Syntax information is widely employed,
either in the form of rules or incorporated in the
learning model. Multi-word and affixal negation
cues receive a special treatment in most cases, and
scopes are generally postprocessed.
The systems that participate in the closed track
are machine learning based. The UiO1 system is an
adaptation of another system (Velldal et al, 2012),
which combines SVM cue classification with SVM-
based ranking of syntactic constituents for scope
resolution. The approach is extended to identify
negated events by first classifying negations as fac-
tual or non-factual, and then applying an SVM
ranker over candidate events. The original treat-
ment of factuality in this system results in the high-
est score for both the negated event subtask and the
global task.
The UiO2 system combines SVM cue classifica-
tion with CRF-based sequence labeling. An original
aspect of the UiO2 approach is the model represen-
270
O
ffi
ci
al
re
su
lt
s
fo
r
Ta
sk
1 Cu
es
Sc
op
es
C
M
Sc
op
es
N
C
M
Sc
op
e
To
ke
ns
N
eg
at
ed
G
lo
ba
l
%
C
N
S
P
re
c.
R
ec
.
F
1
P
re
c.
R
ec
.
F
1
P
re
c.
R
ec
.
F
1
P
re
c.
R
ec
.
F
1
P
re
c.
R
ec
.
F
1
P
re
c.
R
ec
.
F
1
Closedtrack
U
iO
1
r2
89
.1
7
93
.5
6
91
.3
1
83
.8
9
60
.6
4
70
.3
9
83
.8
9
60
.6
4
70
.3
9
75
.8
7
90
.0
8
82
.3
7
60
.5
8
75
.0
0
67
.0
2
79
.8
7
45
.0
8
57
.6
3
43
.8
3
U
iO
1
r1
91
.4
2
92
.8
0
92
.1
0
87
.4
3
61
.4
5
72
.1
7
87
.4
3
61
.4
5
72
.1
7
81
.9
9
88
.8
1
85
.2
6
60
.5
0
72
.8
9
66
.1
2
83
.4
5
43
.9
4
57
.5
7
42
.1
3
U
iO
2
89
.1
7
93
.5
6
91
.3
1
85
.7
1
62
.6
5
72
.3
9
85
.7
1
62
.6
5
72
.3
9
86
.0
3
81
.5
5
83
.7
3
68
.1
8
52
.6
3
59
.4
0
78
.2
6
40
.9
1
53
.7
3
40
.0
0
F
B
K
93
.4
1
91
.2
9
92
.3
4
88
.9
6
58
.2
3
70
.3
9
88
.9
6
58
.2
3
70
.3
9
81
.5
3
82
.4
4
81
.9
8
64
.1
4
56
.7
1
60
.2
0
84
.9
6
36
.3
6
50
.9
3
35
.7
4
U
W
as
hi
ng
to
n
88
.0
4
92
.0
5
90
.0
0
82
.7
2
63
.4
5
71
.8
1
82
.9
0
64
.2
6
72
.4
0
83
.2
6
83
.7
7
83
.5
1
58
.0
4
50
.9
2
54
.2
5
74
.0
2
35
.6
1
48
.0
9
34
.0
4
U
M
ic
hi
ga
n
94
.3
1
87
.8
8
90
.9
8
90
.0
0
50
.6
0
64
.7
8
90
.0
0
50
.6
0
64
.7
8
84
.8
5
80
.6
6
82
.7
0
50
.0
0
52
.2
4
51
.1
0
84
.2
7
28
.4
1
42
.4
9
27
.2
3
U
A
B
C
oR
A
L
85
.9
3
85
.6
1
85
.7
7
79
.0
4
53
.0
1
63
.4
6
79
.5
3
54
.6
2
64
.7
6
85
.3
7
68
.8
6
76
.2
3
65
.0
0
38
.4
6
48
.3
3
66
.3
6
27
.6
5
39
.0
4
26
.8
1
Opentrack
U
iO
2
89
.1
7
93
.5
6
91
.3
1
85
.7
1
62
.6
5
72
.3
9
85
.7
1
62
.6
5
72
.3
9
82
.2
5
82
.1
6
82
.2
0
66
.9
0
57
.4
0
61
.7
9
78
.7
2
42
.0
5
54
.8
2
41
.2
8
U
G
ro
ni
ng
en
r2
88
.8
9
84
.8
5
86
.8
2
76
.1
2
40
.9
6
53
.2
6
76
.1
2
40
.9
6
53
.2
6
69
.2
0
82
.2
7
75
.1
7
56
.6
3
65
.2
9
60
.6
5
72
.0
0
27
.2
7
39
.5
6
27
.2
3
U
C
M
-1
89
.2
6
91
.2
9
90
.2
6
82
.8
6
46
.5
9
59
.6
4
82
.8
6
46
.5
9
59
.6
4
85
.3
7
68
.5
3
76
.0
3
66
.6
7
12
.7
2
21
.3
6
66
.2
8
21
.5
9
32
.5
7
18
.7
2
U
C
M
-2
81
.3
4
64
.3
9
71
.8
8
67
.1
3
38
.5
5
48
.9
8
66
.9
0
38
.9
6
49
.2
4
58
.3
0
67
.7
0
62
.6
5
46
.1
5
21
.1
8
29
.0
3
42
.6
5
10
.9
8
17
.4
6
11
.9
1
U
G
ro
ni
ng
en
r1
86
.9
0
82
.9
5
84
.8
8
46
.3
8
12
.8
5
20
.1
2
46
.3
8
12
.8
5
20
.1
2
69
.6
9
70
.3
0
69
.9
9
53
.9
4
52
.0
5
52
.9
8
37
.7
4
7.
58
12
.6
2
7.
66
Ta
bl
e
4:
O
ffi
ci
al
re
su
lt
s.
?r
1?
st
an
ds
fo
r
ru
n
1
nd
?r
2?
fo
r
ru
n
2.
C
N
S
st
an
ds
fo
r
C
or
re
ct
N
eg
at
io
n
S
en
te
nc
es
.
?C
M
?
st
an
ds
fo
r
C
ue
M
at
ch
an
d
?N
C
M
?
st
an
ds
fo
r
N
o
C
ue
M
at
ch
.
C
ue
s
B
Sc
op
es
B
C
M
Sc
op
es
B
N
C
M
N
eg
at
ed
B
G
lo
ba
lB
P
re
c.
R
ec
.
F
1
P
re
c.
R
ec
.
F
1
P
re
c.
R
ec
.
F
1
P
re
c.
R
ec
.
F
1
P
re
c.
R
ec
.
F
1
Closedtrack
U
iO
1
r2
86
.9
7
93
.5
6
90
.1
4
56
.5
5
60
.6
4
58
.5
2
56
.5
5
60
.6
4
58
.5
2
58
.6
0
75
.0
0
65
.7
9
41
.9
0
45
.0
8
43
.4
3
U
iO
1
r1
89
.0
9
92
.8
0
90
.9
1
59
.3
0
61
.4
5
60
.3
6
59
.3
0
61
.4
5
60
.3
6
57
.6
2
72
.8
9
64
.3
6
42
.1
8
43
.9
4
43
.0
4
U
iO
2
86
.9
7
93
.5
6
90
.1
4
59
.3
2
62
.6
5
60
.9
4
59
.3
2
62
.6
5
60
.9
4
67
.1
6
52
.6
3
59
.0
1
38
.0
3
40
.9
1
39
.4
2
F
B
K
91
.6
3
91
.2
9
91
.4
6
58
.2
3
58
.2
3
58
.2
3
58
.2
3
58
.2
3
58
.2
3
60
.3
9
56
.7
1
58
.4
9
38
.0
3
40
.9
1
39
.4
2
U
W
as
hi
ng
to
n
85
.2
6
92
.0
5
88
.5
2
58
.5
2
63
.4
5
60
.8
9
59
.2
6
64
.2
6
61
.6
6
53
.9
0
50
.9
2
52
.3
7
32
.9
8
35
.6
1
34
.2
4
U
M
ic
hi
ga
n
92
.8
0
87
.8
8
90
.2
7
55
.5
1
50
.6
0
52
.9
4
55
.5
1
50
.6
0
52
.9
4
38
.2
5
52
.2
4
44
.1
6
30
.0
0
28
.4
1
29
.1
8
U
A
B
C
oR
A
L
79
.5
8
85
.6
1
82
.4
8
55
.2
3
53
.0
1
54
.1
0
56
.9
0
54
.6
2
55
.7
4
62
.5
0
38
.4
6
47
.6
2
25
.7
0
27
.6
5
26
.6
4
Opentrack
U
iO
2
86
.9
7
93
.5
6
90
.1
4
59
.5
4
62
.6
5
61
.0
6
59
.5
4
62
.6
5
61
.0
6
63
.8
2
57
.4
0
60
.4
4
39
.0
8
42
.0
5
40
.5
1
U
G
ro
ni
ng
en
r2
85
.8
2
84
.8
5
85
.3
3
39
.8
4
40
.9
6
40
.3
9
39
.8
4
40
.9
6
40
.3
9
55
.2
2
65
.2
9
59
.8
3
27
.5
9
27
.2
7
27
.4
3
U
C
M
-1
86
.6
9
91
.2
9
88
.9
3
45
.6
7
46
.5
9
46
.1
3
45
.6
7
46
.5
9
46
.1
3
66
.6
7
12
.7
2
21
.3
6
20
.5
0
21
.5
9
21
.0
3
U
C
M
-2
72
.3
4
64
.3
9
68
.1
3
41
.2
0
38
.5
5
39
.8
3
41
.6
3
38
.9
6
40
.2
5
44
.4
4
21
.1
8
28
.6
9
12
.3
4
10
.9
8
11
.6
2
U
G
ro
ni
ng
en
r1
83
.9
1
82
.9
5
83
.4
3
12
.2
6
12
.8
5
12
.5
5
12
.2
6
12
.8
5
12
.5
5
52
.6
6
52
.0
5
52
.3
5
7.
66
7.
58
7.
62
Ta
bl
e
5:
R
es
ul
ts
w
it
he
va
lu
at
io
n
m
ea
su
re
s
B
.P
re
ci
si
on
is
ca
lc
ul
at
ed
as
:
tr
ue
po
si
tiv
es
/
to
ta
l
of
sy
st
em
pr
ed
ic
ti
on
s.
?r
1?
st
an
ds
fo
r
ru
n
1
nd
?r
2?
fo
r
ru
n
2.
?C
M
?
st
an
ds
fo
r
C
ue
M
at
ch
an
d
?N
C
M
?
st
an
ds
fo
r
N
o
C
ue
M
at
ch
.
Pa
rt
ic
ip
at
in
g
in
st
it
ut
io
ns
:
U
iO
:
U
ni
ve
rs
it
y
of
O
sl
o;
F
B
K
:
Fo
nd
az
io
ne
B
ru
no
K
es
sl
er
&
U
ni
ve
rs
it
y
of
T
re
nt
o;
U
W
as
hi
ng
to
n:
U
ni
ve
rs
it
y
of
W
as
hi
ng
to
n;
U
M
ic
hi
ga
n:
U
ni
ve
rs
it
y
of
M
ic
hi
ga
n;
U
A
B
C
oR
A
L
:
C
oR
A
L
L
ab
U
ni
ve
rs
it
y
of
A
la
ba
m
a;
U
G
ro
ni
ng
en
:
U
ni
ve
rs
it
y
of
G
ro
ni
ng
en
;U
C
M
:C
om
pl
ut
en
se
U
ni
ve
rs
it
y
of
M
ad
ri
d.
271
tation for scopes and negated events, where tokens
are assigned a set of labels that attempts to de-
scribe their behavior within the mechanics of nega-
tion. After unseen sequences are labeled, in-scope
and negated tokens are assigned to their respective
cues using simple post-processing heuristics.
The FBK system consists of three different CRF
classifiers, as well as the UMichigan. A character-
istic of the cue model of the UMichigan system is
that tokens are assigned five labels in order to rep-
resent the different types of negation. Similarly, the
UWashington system has a CRF sequence tagger for
scope and negated event detection, while the cue de-
tector learns regular expression matching rules from
the training set. The UABCoRAL system follows
the same strategy, but instead of CRFs it employs
SVM Light.
The resources utilized by participants in the open
track are diverse. UiO2 reparsed the data with Malt-
Parser in order to obtain dependency graphs. For the
rest, the system is the same as in the closed track.
The global results obtained by this system in the
closed track are higher than the results obtained in
the open track, which is mostly due to a higher per-
formance of the scope resolution module. This is the
only machine learning system in the open track and
the highest performing one.
The UGroningen system is based on tools that
produce complex semantic representations. The sys-
tem employs the C&C tools8 for parsing and Boxer9
to produce semantic representations in the form of
Discourse Representation Structures (DRSs). For
cue detection, the DRSs are converted to flat, non-
recursive structures, called Discourse Representa-
tion Graphs (DRGs). These DRGs allow for cue de-
tection by means of labelled tuples. Scope detection
is done by gathering the tokens that occur within the
scope of the negated DRSs. For negated event detec-
tion, a basic algorithm takes the detected scope and
returns the negated event based on information from
the syntax tree within the scope.
UCM-1 and UCM-2 are rule-based systems that
rely heavily on information from the syntax tree.
The UCM-1 system was initially designed for pro-
8http://svn.ask.it.usyd.edu.au/trac/
candc/wiki/Documentation
9http://svn.ask.it.usyd.edu.au/trac/
candc/wiki/boxer
cessing opinionated texts. It applies a dictionary ap-
proach to cue detection, with the detection of affixal
cues being performed using WordNet. Non-affixal
cue detection is performed by consulting a prede-
fined list of cues. It then uses information from the
syntax tree in order to get a first approximation to
the scope, which is later refined using a set of post-
processing rules. In the case of the UCM-2 system
an algorithm detects negation cues and their scope
by traversing Minipar dependency structures. Fi-
nally, the scope is refined with post-processing rules
that take into account the information provided by
the first algorithm and linguistic clause boundaries.
If we compare tracks, the Global best results ob-
tained in the closed track (57.63 F1) are higher than
the Global best results obtained in the open track
(54.82 F1). If we compare approaches, the best re-
sults in the two tracks are obtained with machine
learning-based systems. The rule-based systems
participating in the open track clearly score lower
(39.56 F1 the best) than the machine learning-based
system (54.82 F1).
Regarding subtasks, systems achieve higher re-
sults in the cue detection task (92.34 F1 the best) and
lower results in the scope resolution (72.40 F1 the
best) and negated event detection (67.02 F1 the best)
tasks. This is not surprising, not only because of
the error propagation effect, but also because the set
of negation cues is closed and comprises mostly sin-
gle tokens, whereas scope sequences are longer. The
best results in cue detection are obtained by the FBK
system that uses CRFs and applies a special proce-
dure to detect the negation cues that are subtokens.
The best scores for scope resolution (72.40, 72.39
F1) are obtained by two machine learning compo-
nents. UWashington uses CRFs with features de-
rived from the syntax tree. UiO2 uses CRFs mod-
els with syntactic and lexical features for scopes, to-
gether with a set of labels aimed at capturing the
behavior of certain tokens within the mechanics of
negation. The best scores for negated events (67.02
F1) are obtained by the UiO1 system that first clas-
sifies negations as factual or non-factual, and then
applies an SVM ranker over candidate events.
Finally, we would like to draw the attention to the
different scores obtained depending on the evalua-
tion measure used. When scope resolution is evalu-
ated with the Scope (NCM, CM) measure, results
272
are much lower than when using the Scope To-
kens measure, which does not reflect the ability of
systems to deal with sequences. Another observa-
tion is related to the difference in precision scores
between the two versions of the evaluation mea-
sures. Whereas for Cues and Negated the differ-
ences are not so big because most cues and negated
events span over a single token, for Scopes they are.
The best Scope NCM precision score is 90.00 %,
whereas the best Scope NCM B precision score is
59.54 %. This shows that the scores can change
considerably depending on how partial matches are
counted (as FP and FN, or only as FN). As a final
remark it is worth noting that the ranking of systems
does not change when using the B measures.
5.2 Task 2
UConcordia submitted two runs in the open track.
Both of them follow the same three component ap-
proach. First, negation cues are detected. Second,
the scope of negation is extracted based on depen-
dency relations and heuristics defined by Kilicoglu
and Bergler (2011). Third, the focus of negation
is determined within the elements belonging to the
scope following three heuristics.
6 Conclusions
In this paper we presented the description of the first
*SEM Shared Task on Resolving the Scope and Fo-
cus of Negation, which consisted of two different
tasks related to different aspects of negation: Task 1
on resolving the scope of negation, and Task 2 on
detecting the focus of negation. Task 1 was di-
vided into three subtasks: identifying negation cues,
resolving their scope, and identifying the negated
event. Two new datasets have been produced for this
Shared Task: the CD-SCO corpus of Conan Doyle
stories annotated with scopes, and the PB-FOC cor-
pus, which provides focus annotation on top of Prop-
Bank. New evaluation software was also developed
for this task. The datasets and the evaluation soft-
ware will be available on the web site of the Shared
Task. As far as we know, this is the first task that fo-
cuses on resolving the focus and scope of negation.
A total of 14 runs were submitted, 12 for scope
detection and 2 for focus detection. Of these, four
runs are from systems that take a rule-based ap-
proach, two runs from hybrid systems, and the rest
from systems that take a machine learning approach
using SVMs or CRFs. Most participants designed a
three component architecture.
For a future edition of the shared task we would
like to unify the annotation schemes of the two cor-
pora, namely the annotation of focus in PB-FOC and
negated events in CD-SCO. The annotation of more
data with both scope and focus would allow us to
study the two aspects jointly. We would also like to
provide better evaluation measures for scope reso-
lution. Currently, scopes are evaluated in terms of
F1, which demands a division of errors into the cat-
egories TP/FP/TN/FN borrowed from the evaluation
of information retrieval systems. These categories
are not completely appropriate to be assigned to se-
quence tasks, such as scope resolution.
Acknowledgements
We are very grateful to Vivek Srikumar for pre-
processing the PB-FOC corpus with the Illinois se-
mantic role labeler, and to Stephan Oepen for pre-
processing the CD-SCO corpus. We also thank the
*SEM organisers and the ST participants. Roser
Morante?s research was funded by the University of
Antwerp (GOA project BIOGRAPH).
References
Eduardo Blanco and Dan Moldovan. 2011. Semantic
Representation of Negation Using Focus Detection. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for C omputational Linguistics: Human Lan-
guage Technologies, pages 581?589, Portland, Ore-
gon, USA. Association for Computational Linguistics.
Eric Brill. 1992. A simple rule-based part of speech tag-
ger. In Proceedings of the third conference on Applied
natural language processing, ANLC ?92, pages 152?
155, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Wendy W. Chapman, Will Bridewell, Paul Hanbury, Gre-
gory F. Cooper, and Bruce G. Buchanan. 2001. A
simple algorithm for identifying negated findings and
diseases in discharge summaries. J Biomed Inform,
34:301?310.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In Proceedings of the 43rd Annual Meeting of
the Association for Computational Linguistics, pages
173?180, Ann Arbor.
273
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of the 1st North American
chapter of the Association for Computational Lin-
guistics conference, NAACL 2000, pages 132?139,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating Typed
Dependency Parses from Phrase Structure Parses. In
Proceedings of the IEEE / ACL 2006 Workshop on
Spoken Language Technology. The Stanford Natural
Language Processing Group.
Richa?rd Farkas, Veronika Vincze, Gyo?rgy Mo?ra, Ja?nos
Csirik, and Gyo?rgy Szarvas. 2010. The CoNLL-2010
Shared Task: Learning to Detect Hedges and their
Scope in Natural Language Text. In Proceedings of
the Fourteenth Conference on Computational Natural
Language Learning, pages 1?12, Uppsala, Sweden.
Association for Computational Linguistics.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs sam-
pling. In Proceedings of the 43rd Annual Meeting on
Association for Computational Linguistics, ACL ?05,
pages 363?370, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Rodney D. Huddleston and Geoffrey K. Pullum. 2002.
The Cambridge Grammar of the English Language.
Cambridge University Press.
Halil Kilicoglu and Sabine Bergler. 2011. Effective bio-
event extraction using trigger words and syntactic de-
pendencies. Computational Intelligence, 27(4):583?
609.
Roser Morante and Walter Daelemans. 2009. A met-
alearning approach to processing the scope of nega-
tion. In Proceedings of the 13th Conference on Natu-
ral Language Learning, pages 21?29, Boulder, CO.
Roser Morante and Walter Daelemans. 2012.
ConanDoyle-neg: Annotation of negation cues and
their scope in Conan Doyle stories. In Proceedings
of LREC 2012, Istambul.
Roser Morante and Caroline Sporleder. 2012. Special is-
sue on modality and negation: An introduction. Com-
putational Linguistics.
Roser Morante, Sarah Schrauwen, and Walter Daele-
mans. 2011. Annotation of negation cues and their
scope. guidelines v1.0. Technical Report Series CTR-
003, CLiPS, University of Antwerp, Antwerp, April.
Xuan-Hieu Phan. 2006. Crfchunker: Crf english phrase
chunker.
Vasin Punyakanok, Dan Roth, and Wen-tau Yih. 2008.
The importance of syntactic parsing and inference in
semantic role labeling. Computational Linguistics,
34(2):257?287, June.
Dan Shen and Mirella Lapata. 2007. Using Semantic
Roles to Improve Question Answering. In Proceed-
ings of the 2007 Joint Conference on Empirical Meth-
ods in Natural Language Processing and Computa-
tional Natural Language Learning (EM NLP-CoNLL),
pages 12?21.
Ekaterina Shutova. 2010. Models of Metaphor in NLP.
In Proceedings of the 48th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 688?697,
Uppsala, Sweden. ACL.
Mihai Surdeanu, Sanda Harabagiu, John Williams, and
Paul Aarseth. 2003. Using Predicate-Argument Struc-
tures for Information Extraction. In Proceedings of the
41st Annual Meeting of the Association for Computa-
tional Linguistics, pages 8?15, Sapporo, Japan. Asso-
ciation for Computational Linguistics.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu??s Ma`rquez, and Joakim Nivre. 2008. The conll-
2008 shared task on joint parsing of syntactic and se-
mantic dependencies. In CoNLL 2008: Proceedings of
the 12th Conference on Computational Natural Lan-
guage Learning, page 159177, Manchester.
Yoshimasa Tsuruoka and Jun?ichi Tsujii. 2005. Bidi-
rectional inference with the easiest-first strategy for
tagging sequence data. In Proceedings of of Human
Language Technology Conference and Conference on
Empirical Methods in Natural Language Processing,
pages 467?474, Vancouver.
Erik Velldal, Lilja ?vrelid, Jonathon Read, and Stephan
Oepen. 2012. Speculation and negation: Rules,
rankers, and the role of syntax. Computational Lin-
guistics.
Veronika Vincze, Gyorgy Szarvas, Richard Farkas, Gy-
orgy Mora, and Janos Csirik. 2008. The Bio-
Scope corpus: biomedical texts annotated for uncer-
tainty, negation and their scopes. BMC Bioinformat-
ics, 9(Suppl 11):S9+.
Michael Wiegand, Alexandra Balahur, Benjamin Roth,
Dietrich Klakow, and Andre?s Montoyo. 2010. A sur-
vey on the role of negation in sentiment analysis. In
Proceedings of the Workshop on Negation and Specu-
lation in Natural Language Processing, pages 60?68,
Uppsala, Sweden. University of Antwerp.
Dekai Wu and Pascale Fung. 2009. Semantic Roles
for SMT: A Hybrid Two-Pass Model. In Proceedings
of Human Language Technologies: The 2009 Annual
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, Companion
Volume: Short Papers, pages 13?16, Boulder, Col-
orado. Association for Computational Linguistics.
274
CNTS: Memory-Based Learning of Generating Repeated References
Iris Hendrickx, Walter Daelemans, Kim Luyckx, Roser Morante, Vincent Van Asch
CNTS, Department of Linguistics
University of Antwerp
Prinsstraat 13, 2000, Antwerp, Belgium
firstname.lastname@ua.ac.be
Abstract
In this paper we describe our machine learning
approach to the generation of referring expres-
sions. As our algorithm we use memory-based
learning. Our results show that in case of pre-
dicting the TYPE of the expression, having one
general classifier gives the best results. On the
contrary, when predicting the full set of prop-
erties of an expression, a combined set of spe-
cialized classifiers for each subdomain gives
the best performance.
1 Introduction
In this paper we describe the systems with which
we participated in the GREC task of the REG 2008
challenge (Belz and Varges, 2007). The GREC task
concerns predicting which expression is appropriate
to refer to a particular discourse referent in a certain
position in a text, given a set of alternative referring
expressions for selection. The organizers provided
the GREC corpus that consists of 2000 texts col-
lected from Wikipedia, from 5 different subdomains
(people, cities, countries, mountains and rivers) .
One of the main goals of the task is to discover
what kind of information is useful in the input to
make the decision between candidate referring ex-
pressions. We experimented with a pool of features
and several machine learning algorithms in order to
achieve this goal.
2 Method
We apply a standard machine learning approach
to the task. We train a classifier to predict the
correct label for each mention. As our machine
learning algorithm we use memory-based learn-
ing as implemented in the Timbl package (Daele-
mans et al, 2007). To select the optimal algorith-
mic parameter setting for each classifier we used
a heuristic optimization method called paramsearch
(Van den Bosch, 2004). We also tried several other
machine learning algorithms implemented in the
Weka package (Witten and Frank, 2005), but these
experiments did not lead to better results and are not
further discussed here.
We developed four systems: a system that only
predicts the TYPE of each expression (Type), so it
predicts four class labels; and a system that pre-
dicts the four properties (TYPE, EMPATHIC, HEAD,
CASE) of each expression simultaneously (Prop).
The class labels predicted by this system are con-
catenated strings: ?common no nominal plain?, and
these concatenations lead to 14 classes, which
means that not all combinations appear in the train-
ing set. For both Type an Prop we created two vari-
ants: one general classifer (g) that is trained on all
subdomains, and a set of combined specialized clas-
sifiers (s) that are optimized for each domain sepa-
rately.
3 System description
To build the feature representations, we first prepro-
cessed the texts performing the following actions:
rule-based tokenization, memory-based part-of-
speech tagging, NP-chunking, Named entity recog-
nition, and grammatical relation finding (Daelemans
and van den Bosch, 2005). We create an instance for
each mention, using the following features to repre-
194
sent each instance:
? Positional features: the sentence number, the
NP number, a boolean feature that indicates if
the mention appears in the first sentence.
? Syntactic and semantic category given of the
entity (SEMCAT, SYNCAT).
? Local context of 3 words and POS tags left and
right of the entity.
? Distance to the previous mention measured in
sentences and in NPs.
? Trigram pattern of the given syntactic cate-
gories of 3 previous mentions.
? Boolean feature indicating if the previous sen-
tence contains another named entity than the
entity in focus.
? the main verb of the sentence.
We do not use any information about the given set
of alternative expressions except for post process-
ing. In a few cases our classifier predicts a label that
is not present in the set of alternatives. For those
cases we choose the most frequent class label (as es-
timated on the training set).
We experimented with predicting all subdomains
with the same classifier and with creating separate
classifiers for each subdomains. We expected that
semantically different domains would have different
preferences for expressions.
4 Results
We provide results for the four systems Type-g,
Type-s, Prop-g and Prop-s in Table 1. The evalua-
tion script was provided by the organisers. The vari-
ant Type-g performs best with a score of 76.52% on
the development set.
5 Conclusions
In this paper we described our machine learning ap-
proach to the generation of referring expressions.
We reported results of four memory-based systems.
Predicting all subdomains with the same classifier
is more efficient when predicting the coarse-grained
TYPE class. On the contrary, training specialized
classifiers for each subdomain works better for the
Data Type-g Type-s
Cities 64.65 60.61
Countries 75.00 71.74
Mountains 75.42 77.07
People 85.37 72.50
Rivers 65.00 80.00
All 76.52 72.26
Data Prop-g Prop-s
Cities 63.64 65.66
Countries 72.83 69.57
Mountains 72.08 74.58
People 79.51 79.51
Rivers 65.00 70.00
All 73.02 73.93
Table 1: Accuracy on GREC development set.
more fine-grained prediction of all properties simu-
laneously. For the test set we will present results the
two best systems: CNTS-Type-g and CNTS-Prop-s.
Acknowledgments
This research is funded by FWO, IWT, GOA BOF UA,
and the STEVIN programme funded by the Dutch and
Flemish Governments.
References
A. Belz and S. Varges. 2007. Generation of repeated ref-
erences to discourse entities. In In Proceedings of the
11th European Workshop on Natural Language Gen-
eration (ENLG?07), pages 9?16.
W. Daelemans and A. van den Bosch. 2005. Memory-
based language processing. Cambridge University
Press, Cambridge, UK.
W. Daelemans, J. Zavrel, K. Van der Sloot, and
A. Van den Bosch. 2007. TiMBL: Tilburg Memory
Based Learner, version 6.1, reference manual. Techni-
cal Report 07-07, ILK, Tilburg University.
A. Van den Bosch. 2004. Wrapped progressive sampling
search for optimizing learning algorithm parameters.
In Proceedings of the 16th Belgian-Dutch Conference
on Artificial Intelligence, pages 219?226.
I. H. Witten and E. Frank. 2005. Data Mining: Prac-
tical machine learning tools and techniques, second
edition. Morgan Kaufmann, San Francisco.
195
CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 208?212
Manchester, August 2008
A Combined Memory-Based Semantic Role Labeler of English
Roser Morante, Walter Daelemans, Vincent Van Asch
CNTS - Language Technology Group
University of Antwerp
Prinsstraat 13, B-2000 Antwerpen, Belgium
{Roser.Morante,Walter.Daelemans,Vincent.VanAsch}@ua.ac.be
Abstract
We describe the system submitted to
the closed challenge of the CoNLL-2008
shared task on joint parsing of syntactic
and semantic dependencies. Syntactic de-
pendencies are processed with the Malt-
Parser 0.4. Semantic dependencies are
processed with a combination of memory-
based classifiers. The system achieves
78.43 labeled macro F1 for the complete
problem, 86.07 labeled attachment score
for syntactic dependencies, and 70.51 la-
beled F1 for semantic dependencies.
1 Introduction
In this paper we describe the system submitted to
the closed challenge of the CoNLL-2008 shared
task on joint parsing of syntactic and semantic de-
pendencies (Surdeanu et al, 2008). Compared to
the previous shared tasks on semantic role label-
ing, the innovative feature of this one is that it
consists of extracting both syntactic and seman-
tic dependencies. The semantic dependencies task
comprises labeling the semantic roles of nouns and
verbs and disambiguating the frame of predicates.
The system that we present extracts syntactic
and semantic dependencies independently. Syn-
tactic dependencies are processed with the Malt-
Parser 0.4 (Nivre, 2006; Nivre et al, 2007). Se-
mantic dependencies are processed with a combi-
nation of memory-based classifiers.
Memory-based language processing (Daele-
mans and van den Bosch, 2005) is based on the
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
idea that NLP problems can be solved by stor-
ing solved examples of the problem in their literal
form in memory, and applying similarity-based
reasoning on these examples in order to solve new
ones. Keeping literal forms in memory has been
argued to provide a key advantage over abstracting
methods in NLP that ignore exceptions and sub-
regularities (Daelemans et al, 1999).
Memory-based algorithms have been previously
applied to semantic role labeling. Van den
Bosch et al (2004) participated in the CoNLL-
2004 shared task with a system that extended
the basic memory-based learning method with
class n-grams, iterative classifier stacking, and
automatic output post-processing. Tjong Kim
Sang et al (2005) participated in the CoNLL-
2005 shared task with a system that incorporates
spelling error correction techniques. Morante and
Busser (2007) participated in the SemEval-2007
competition with a semantic role labeler for Span-
ish based on gold standard constituent syntax.
These systems use different types of constituent
syntax (shallow parsing, full parsing). We are
aware of two systems that perform semantic role
labeling based on dependency syntax previous to
the CoNLL-2008 shared task. Hacioglu (2004)
converts the data from the CoNLL-2004 shared
task into dependency trees and uses support vector
machines. Morante (2008) describes a memory-
based semantic role labeling system for Spanish
based on gold standard dependency syntax.
We developed a memory-based system for the
CoNLL-2008 shared task in order to evaluate the
performance of this methodology in a completely
new semantic role labeling setting.
The paper is organised as follows. In Section 2
the system is described, Section 3 contains an anal-
ysis of the results, and Section 4 puts forward some
208
conclusions.
2 System description
The system processes syntactic and semantic de-
pendencies independently. The syntactic depen-
dencies are processed with the MaltParser 0.4. The
semantic dependencies are processed with a cas-
cade of memory-based classifiers. We use the
IB1 classifier as implemented in TiMBL (version
6.1.2) (Daelemans et al, 2007), a supervised in-
ductive algorithm for learning classification tasks
based on the k-nearest neighbor classification rule
(Cover and Hart, 1967). In IB1, similarity is de-
fined by computing (weighted) overlap of the fea-
ture values of a test instance and a memorized ex-
ample. The metric combines a per-feature value
distance metric with global feature weights that
account for relative differences in discriminative
power of the features.
2.1 Syntactic dependencies
The MaltParser 0.4
1
(Nivre, 2006; Nivre et al,
2007) is an inductive dependency parser that uses
four essential components: a deterministic algo-
rithm for building labeled projective dependency
graphs; history-based feature models for predict-
ing the next parser action; support vector ma-
chines for mapping histories to parser actions;
and graph transformations for recovering non-
projective structures.
The learner type used was support vector ma-
chines, with the same parameter options re-
ported by (Nivre et al, 2006). The parser
algorithm used was Nivre, with the options
and model (eng.par) for English as specified
on http://w3.msi.vxu.se/users/jha/conll07/. The
tagset.pos, tagset.cpos and tagset.dep were ex-
tracted from the training corpus.
2.2 Semantic dependencies
The semantics task consists of finding the predi-
cates, assigning a PropBank or a NomBank frame
to them and extracting their semantic role depen-
dencies. Because of lack of resources, we did not
have time to develop a word sense disambiguation
system. So, predicates were assigned the frame
?.01? by default.
The system handles the semantic role labeling
task in three steps: predicate identification, seman-
1
Web page of MaltParser 0.4:
http://w3.msi.vxu.se/?nivre/research/MaltParser.html.
tic dependency classification, and combination of
classifiers.
2.2.1 Predicate identification
In this phase, a classifier predicts if a word is a
predicate or not. The IB1 algorithm was param-
eterised by using overlap as the similarity metric,
information gain for feature weighting, using 7 k-
nearest neighbors, and weighting the class vote of
neighbors as a function of their inverse linear dis-
tance. The instances represent all nouns and verbs
in the corpus and they have the following features:
? Word form, lemma, part of speech (POS), the three last
letters of the word, and the lemma and POS of the five
previous and five next words. To obtain the previous
word we perform a linear left-to-right search. This is
how previous has to be interpreted further on when fea-
tures are described.
The accuracy of the classifier on the develop-
ment test is 0.9599 (4240/4417) for verbs and
0.8981 (9226/10272) for nouns.
2.2.2 Semantic dependency classification
In this phase, three groups of multi-class clas-
sifiers predict in one step if there is a dependency
between a word and a predicate, and the type of
dependency, i.e. semantic role.
Group 1 (G1) consists of two classifiers: one
for predicates that are nouns and another for pred-
icates that are verbs. The instances represent a
predicate-word combination. The predicates are
those that have been classified as such in the previ-
ous phase. As for the combining words, determin-
ers and certain combinations are excluded based
on the fact that they never have a role in the train-
ing corpus.
The IB1 algorithm was parameterised by using
overlap as the similarity metric, information gain
for feature weighting, using 11 k-nearest neigh-
bors, and weighting the class vote of neighbors as
a function of their inverse linear distance. The fea-
tures of the noun classifier are:
? About the predicate: word form. About the combining
word: word form, POS, dependency type, word form
of the two previous and two next words. Chain of POS
types between the word and the predicate. Distance be-
tween the word and the predicate. Binary feature indi-
cating if the word depends on the predicate. Six chains
of POS tags between the word and its three previous and
three next predicates in relation to the current predicate.
209
The features of the verb classifier are:
? The same as for the noun classifier and additionally:
POS of the word next to the current combining word,
binary feature indicating if the combining word de-
pends on the predicate previous to the current predicate,
binary feature indicating if the predicate previous to the
combining word is located before or after the current
predicate.
The verb classifier achieves an overall accuracy
of 0.9244 (80805/87412), and the noun classifier,
0.9173 (69836/76132) in the development set.
Group 2 (G2) consists also of two classifiers:
one for predicates that are nouns and another for
predicates that are verbs. The instances represent
combinations of word-predicate, but the test cor-
pus contains only those instances that G1 has clas-
sified as having a role.
The IB1 algorithm was parameterised in the
same way as for G1, except that it computes 7 k-
nearest neighbors instead of 11. The two classifiers
use the same features:
? About the predicate: word form, chain of lemmas of the
syntactic siblings, chain of lemmas of the syntactic chil-
dren. About the combining word: word form, POS, de-
pendency type, word form of the two previous and the
two next words, POS+type of dependency and lemma
of the syntactic father, chain of dependency types and
chain of lemmas of the syntactic children. Chain of
POS types between word and predicate, distance and
syntactic dependency type between word and predicate.
The verb classifier achieves an overall accuracy
of 0.5656 (4160/7355), and the noun classifier,
0.5017 (2234/4452) in the development set.
Group 3 (G3) consists of one classifier. Like
G2, instances represent combinations of word-
predicate, but the test corpus contains only those
instances that G1 has classified as having a role..
The IB1 algorithm was parameterised in the same
way as for G2. It uses the following features:
About the predicate: lemma, POS, POS of the 3 previous
and 3 next predicates. About the combining word: lemma,
POS, and dependency type, POS of the 3 previous and 3 next
words. Distance between the predicate and the word. A bi-
nary feature indicating if the combining word is located be-
fore or after the predicate.
The classifier achieves an overall accuracy of
0.5527 (6526/11807).
2.2.3 Combination of classifiers
In this phase the three groups of classifiers are
combined in a simple way: if G2 and G3 agree
in classifying a semantic dependency, their solu-
tion is chosen, else the solution of G1 is chosen.
This system combination choice is explained by
the fact that G1 has a higher accuracy than G2 and
G3 when the three classifiers are applied to the de-
velopment set. G2 and G3 are used to eliminate
overgeneration of roles by G1.
The performance of the system in the develop-
ment corpus with only the G1 classifiers is 66.07
labeled F1. The combined system achieves a
10.8% error reduction, with 69.75 labeled F1.
3 Results
The results of the system are shown in Table 1.
We will focus on commenting on the semantic
scores. The system scores 71.88 labeled F1 in the
in-domain corpus (WSJ) and 59.23 in the out-of-
domain corpus (Brown). Unlabeled F1 in the WSJ
corpus is almost 10% higher than labeled F1. La-
beled precision is 12.40% higher than labeled re-
call.
WSJ BROWN
SYNTACTIC SCORES
Labeled attachment score 86.88 79.58
Unlabeled attachment score 89.37 84.85
Label accuracy score 91.48 86.00
SEMANTIC SCORES
Labeled precision 78.61 65.25
Labeled recall 66.21 54.23
Labeled F1 71.88 59.23
Unlabeled precision 89.13 83.61
Unlabeled recall 75.08 69.48
Unlabeled F1 81.50 75.89
OVERALL MACRO SCORES
Labeled macro precision 82.74 72.41
Labeled macro recall 76.54 66.90
Labeled macro F1 79.52 69.55
Unlabeled macro precision 89.25 84.23
Unlabeled macro recall 82.22 77.16
Unlabeled macro F1 85.59 80.54
Table 1: Results of the system in the WSJ and
BROWN corpora expressed in %.
3.1 Discussion
The performance of the semantic role labeler is af-
fected considerably by the performance of the first
classifier for predicate detection. The system can-
not recover from the predicates that are missed in
this phase. Experiments without the first classifier
and with gold standard predicates (detection and
classification) result in 80.89 labeled F1, 9.01 %
210
higher than the results of the system with predi-
cate detection. We opted for identifying predicates
as a first step in order to reduce the number of
training instances for the second phase, classifica-
tion of semantic dependencies. For the same rea-
son, we opted for selecting only nouns and verbs
as instances, aware of the fact that we would miss
a very low number of predicates with other cate-
gories. The results of predicate identification can
be improved by setting up a combined system, in-
stead of a single classifier, and by incorporating a
system for frame disambiguation.
Equally important would be to find better fea-
tures for the identification of noun predicates,
since the features used generalise better for verbs
than for nouns. Table 2 shows that the system is
better at identifying verbs than it is at identifying
nouns.
Total F1 Pred. F1 Pred.
Id.&Cl. Id.
CC 3 - -
CD 1 - -
IN 3 - -
JJ 16 - -
NN 3635 77.57 85.59
NNP 10 30.77 38.46
NNS 1648 75.47 83.65
PDT 2 - -
RP 4 - -
VB 1278 79.28 98.87
VBD 1320 85.44 99.24
VBG 742 77.05 94.41
VBN 985 76.43 92.08
VBP 343 78.60 97.81
VBZ 504 80.94 97.36
WP 2 - -
WRB 2 - -
Table 2: Predicate (Pred.) identification (Id.) and
classification (Cl.) in the WSJ corpus expressed in
%.
A characteristic of the semantic role labeler is
that recall is considerably lower than precision
(12.40 %). This can be further analysed with the
data shown in Table 3.
Except for the dependency VB*+AM-NEG,
precision is higher than recall for all semantic de-
pendencies. We run the semantic role labeler with
gold standard predicates and with gold standard
syntax and predicates. The difference between pre-
cision and recall is around 10 % in both cases,
which confirms that low recall is a characteristic
of the semantic role labeler, probably caused by
the fact that the features do not generalise good
enough. The semantic role labeler with gold stan-
Dependency Total Recall Prec. F1
NN*+A0 2339 42.41 77.80 54.90
NN*+A1 3757 61.17 78.32 68.69
NN*+A2 1537 45.48 82.24 58.57
NN*+A3 349 50.14 88.38 63.98
NN*+AM-ADV 32 9.38 37.50 15.01
NN*+AM-EXT 33 18.18 85.71 30.00
NN*+AM-LOC 232 30.60 63.96 41.40
NN*+AM-MNR 344 34.59 79.87 48.27
NN*+AM-NEG 35 2.86 100.00 5.56
NN*+AM-TMP 492 54.88 83.33 66.18
VB*+A0 3509 68.99 82.63 75.20
VB*+A1 4844 74.24 83.28 78.50
VB*+A2 1085 55.94 69.21 61.87
VB*+A3 169 41.42 79.55 54.48
VB*+A4 99 74.75 88.10 80.88
VB*+AM-ADV 488 38.93 59.19 46.97
VB*+AM-CAU 70 50.00 70.00 58.33
VB*+AM-DIR 81 29.63 57.14 39.02
VB*+AM-DIS 315 52.70 74.11 61.60
VB*+AM-EXT 32 50.00 59.26 54.24
VB*+AM-LOC 355 52.11 57.10 54.49
VB*+AM-MNR 335 46.57 61.18 52.88
VB*+AM-MOD 539 92.21 95.95 94.04
VB*+AM-NEG 227 94.71 90.34 92.47
VB*+AM-PNC 113 33.63 54.29 41.53
VB*+AM-TMP 1068 64.51 80.40 71.58
VB*+C-A1 192 65.10 74.85 69.64
VB*+R-A0 222 65.77 87.43 75.07
VB*+R-A1 155 49.68 73.33 59.23
VB*+R-AM-LOC 21 23.81 71.43 35.71
VB*+R-AM-TMP 52 46.15 66.67 54.54
Table 3: Semantic dependencies identification and
classification in the WSJ corpus for dependencies
with more than 20 occurences expressed in %.
dard predicates scores 86.06 % labeled precision
and 76.32 % labeled recall. The semantic role
labeler with gold standard predicates and syntax
scores 89.20 % precision and 79.47 % recall.
Table 3 also shows that the unbalance between
precision and recall is higher for dependencies of
nouns than for dependencies of verbs, and that
both recall and precision are higher for dependen-
cies from verbs. Thus, the system performs better
for verbs than for nouns. This is in part caused
by the fact that more noun predicates than verb
predicates are missed in the predicate identifica-
tion phase. The scores of the the semantic role
labeler with gold standard predicates show lower
differences in F1 between verbs and nouns.
The fact that the semantic role labeler performs
3.16 % labeled F1 better with gold standard syntax
(compared to the system with gold standard syntax
and predicates) confirms that gold standard syntax
provides useful information to the system.
Additionally, the difference in performance be-
tween the semantic role labeler presented to the
211
competition and the semantic role labeler with
gold standard predicates (9.01 % labeled F1) sug-
gests that, although the results of the system are
encouraging, there is room for improvement, and
improvement should focus on increasing the recall
scores.
4 Conclusions
In this paper we have presented a system submitted
to the closed challenge of the CoNLL-2008 shared
task on joint parsing of syntactic and semantic de-
pendencies. We have focused on describing the
part of the system that extracts semantic dependen-
cies, a combination of memory-based classifiers.
The system achieves a semantic score of 71,88 la-
beled F1. Results show that the system is con-
siderably affected by the first phase of predicate
identification, that the system is better at extract-
ing the semantic dependencies of verbs than those
of nouns, and that recall is substantially lower than
precision. These facts suggest that, although the
results are encouraging, there is room for improve-
ment.
5 Acknowledgements
This work was made possible through financial
support from the University of Antwerp (GOA
project BIOGRAPH), and from the Flemish Insti-
tute for the Promotion of Innovation by Science
and Technology Flanders (IWT) (TETRA project
GRAVITAL). The experiments were carried out in
the CalcUA computing facilities. We are grateful
to Stefan Becuwe for his support.
References
Cover, T. M. and P. E. Hart. 1967. Nearest neigh-
bor pattern classification. Institute of Electrical and
Electronics Engineers Transactions on Information
Theory, 13:21?27.
Daelemans, W. and A. van den Bosch. 2005. Memory-
based language processing. Cambridge University
Press, Cambridge, UK.
Daelemans, W., A. Van den Bosch, and J. Zavrel. 1999.
Forgetting exceptions is harmful in language learn-
ing. Machine Learning, Special issue on Natural
Language Learning, 34:11?41.
Daelemans, W., J. Zavrel, K. Van der Sloot, and A. Van
den Bosch. 2007. TiMBL: Tilburg memory based
learner, version 6.1, reference guide. Technical Re-
port Series 07-07, ILK, Tilburg, The Netherlands.
Hacioglu, K. 2004. Semantic role labeling using de-
pendency trees. In COLING ?04: Proceedings of
the 20th international conference on Computational
Linguistics, Morristown, NJ, USA. ACL.
Morante, R. and B. Busser. 2007. ILK2: Semantic
role labelling for Catalan and Spanish using TiMBL.
In Proceedings of the 4th International Workshop on
Semantic Evaluations (SemEval-2007), pages 183?
186.
Morante, R. 2008. Semantic role labeling tools trained
on the Cast3LB-CoNLL-SemRol corpus. In Pro-
ceedings of the LREC 2008, Marrakech, Morocco.
Nivre, J., J. Hall, J. Nilsson, G. Eryigit, and S. Marinov.
2006. Labeled pseudo?projective dependency pars-
ing with support vector machines. In Proceedings
of the Tenth Conference on Computational Natural
Language Learning, CoNLL-X, New York City, NY,
June.
Nivre, J., J. Hall, J. Nilsson, A. Chanev, G. Eryigit,
S. K?ubler, S. Marinov, and E. Marsi. 2007. Malt-
Parser: a language-independent system for data-
driven dependency parsing. Natural Language En-
gineering, 13(2):95?135.
Nivre, J. 2006. Inductive Dependency Parsing.
Springer.
Surdeanu, Mihai, Richard Johansson, Adam Meyers,
Llu??s M`arquez, and Joakim Nivre. 2008. The
CoNLL-2008 shared task on joint parsing of syntac-
tic and semantic dependencies. In Proceedings of
the 12th Conference on Computational Natural Lan-
guage Learning (CoNLL-2008).
Tjong Kim Sang, E., S. Canisius, A. van den Bosch,
and T. Bogers. 2005. Applying spelling error cor-
rection techniques for improving semantic role la-
belling. In Proceedings of the Ninth Conference
on Natural Language Learning (CoNLL-2005), Ann
Arbor, MI.
van den Bosch, A., S. Canisius, W. Daelemans, I. Hen-
drickx, and E. Tjong Kim Sang. 2004. Memory-
based semantic role labeling: Optimizing features,
algorithm, and output. In Ng, H.T. and E. Riloff, ed-
itors, Proceedings of the Eighth Conference on Com-
putational Natural Language Learning (CoNLL-
2004), Boston, MA, USA.
212
Proceedings of the 2010 Workshop on Biomedical Natural Language Processing, ACL 2010, pages 126?127,
Uppsala, Sweden, 15 July 2010. c?2010 Association for Computational Linguistics
Semantic role labeling of gene regulation events: preliminary results
Roser Morante
CLiPS - University of Antwerp
Prinsstraat 13, B-2000 Antwerpen, Belgium
Roser.Morante@ua.ac.be
Abstract
This abstract describes work in progress
on semantic role labeling of gene regula-
tion events. We present preliminary results
of a supervised semantic role labeler that
has been trained and tested on the GREC
corpus.
1 Introduction
Semantic role labeling (SRL) is a natural language
processing task that consists of identifying the ar-
guments of predicates within a sentence and as-
signing a semantic role to them. This task can
support the extraction of relations from biomedi-
cal texts. Recent research has produced a rich va-
riety of SRL systems to process general domain
corpora. However, only a few systems have been
developed to process biomedical corpora (Tzong-
Han Tsai et al 2007; Bethard et al, 2008). In
this abstract, we present preliminary results of
a new system that is trained on the GREC cor-
pus (Thompson et al, 2009).
The GREC corpus consists of 240 MEDLINE
abstracts, in which gene regulation events have
been annotated with different types of informa-
tion, like the span of the event and of its argu-
ments, and the semantic role of the arguments.
Events can be verbs (58%) and nominalised verbs
(42%). The corpus is divided into two species-
specific subcorpora: E. coli (167 abstracts, 2394
events) and human (73 abstracts, 673 events).
2 System description
We perform two preprocessing steps. First, we
extract the text and parse it with the GDep
parser (Sagae and Tsujii, 2007) and then we con-
vert the corpus from xml into CoNLL format. Ta-
ble 1 shows a preprocessed sentence. The sys-
tem performs argument identification and seman-
tic role assignment in a single step, assuming gold
standard event identification. It consists of one
classifier that classifies an instance into one of the
semantic role classes or the NONE class. An in-
stance represents a combination of an event and a
potential argument (PA). In order to generate the
PAs, the system relies on information from the
dependency syntax tree, which means that errors
in the syntactic tree influence directly the perfor-
mance of the system. We consider that the fol-
lowing tokens or combinations of tokens can be
PAs: main verbs, nouns, adjectives, pronouns and
adverbs; main verbs, nouns, adjectives, pronouns
and adverbs with their modifiers to the left in the
string of words; main verbs, nouns, adjectives,
pronouns, adverbs, prepositions and relative pro-
nouns with their modifiers to the left and to the
right in the string of words.
The features extracted to perform the classifica-
tion task are the following:
? About the event and the PA: chain of words, lemmas,
POS, and dependency labels of all the tokens; lemma, POS
and dependency label of head token, first token and last token;
lemma and POS of syntactic father of head; lemma, POS,
and dependency label of previous and next three tokens in
the string of words; even type.
? About the dependency tree: feature indicating who is the
ancestor (event, PA, other); lemma, POS, and dependency la-
bel of the first common ancestor of event and PA, if there
is one; chain of dependency labels and chain of POS from
event to common ancestor, and from PA to common ances-
tor, if there is one; chain of dependency labels and chain of
POS from PA to event, if event is ancestor of PA; chain of de-
pendency labels and chain of POS from event to PA, if PA is
ancestor of event; chain of dependency labels and POS from
event to ROOT and from PA to ROOT.
? Normalised distance in number of tokens between event
an potential argument in the string of words.
We use an IB1 memory?based algorithm as im-
plemented in TiMBL (version 6.1.2) 1(Daelemans
et al, 2009), a memory-based classifier based on
the k-nearest neighbor rule. The IB1 algorithm
was parameterised by using Jeffrey divergence as
the similarity metric, gain ratio for feature weight-
ing, using 5 k-nearest neighbors, and weighting
1TiMBL: http://ilk.uvt.nl/timbl
126
# WORD LEMMA CHUNK POS DEP LABEL #E TYPE ROLES
1 Lrp Lrp B-NP NN 2 SUB B-Agent B-Agent B-Agent
2 binds bind B-VP VBZ 0 ROOT E1 GRE
3 to to B-PP TO 2 VMOD
4 two two B-NP CD 5 NMOD
5 regions region I-NP NNS 3 PMOD
6 in in B-PP IN 5 NMOD
7 the the B-NP DT 10 NMOD B-Destination
8 dadAX dadAX I-NP NN 10 NMOD I-Destination
9 promoter promoter I-NP NN 10 NMOD I-Destination
10 region region I-NP NN 6 PMOD I-Destination
11 of of B-PP IN 10 NMOD
12 Escherichia Escherichia B-NP FW 13 NMOD
13 coli coli I-NP FW 11 PMOD
14 to to B-VP TO 15 VMOD
15 repress repress I-VP VB 13 NMOD E2 Gene Repression
16 and and I-VP CC 15 VMOD
17 activate activate I-VP VB 15 VMOD E3 Gene Activation
18 transcription transcription B-NP NN 17 OBJ B-Theme B-Theme
19 directly directly B-ADVP RB 17 VMOD B-Manner B-Manner
20 . . O . 2 P
Table 1: Sentence 1 from abstract 10216857 in E. coli corpus. Column # contains the token number;
WORD, the word; LEMMA to LABEL contain information provided by the GDEP parser; #E, the event
number; TYPE, the type of event, and ROLES contains columns with argument labels for each event
following textual order, i.e., the first column corresponds to the first event in #E, the second column to
the second event, etc.
the class vote of neighbors as a function of their
inverse distance.
3 Preliminary results
We provide 5 fold cross-validation (CV) and
cross-domain (CD) results in Table 2. The CV re-
sults are obtained by training and testing on dif-
ferent partitions of the same corpus. The CD re-
sults are obtained by training on one corpus and
testing on the other. Although we cannot directly
compare this results with results of other systems
on exactly the same corpus, Sasaki et al (2008)
report CV results on a corpus of 677 MEDLINE
abstracts on E. Coli gene regulation events. The
precision achieved by their system is 49.00 and
the recall 18.60. We consider that the results of
our system are encouraging to proceed with fur-
ther research.
Corpus Precision Recall F1
E coli CV 59.72 32.29 41.92
E coli CD 49.87 18.07 26.53
Human CV 47.98 22.43 30.57
Human CD 56.57 25.90 35.53
Table 2: F1, precision and recall for argument
identification and labeling.
4 Future work
Future work will deal with incorporating domain
specific knowledge and with improving the ma-
chine learning techniques. We will experiment
with other algorithms, like Conditional Random
Fields, which are well known sequence labelers.
Additionally, we will implement also a constraint
satisfaction algorithm.
Acknowledgments
This preliminary study was made possible through
financial support from the University of Antwerp
(GOA project BIOGRAPH).
References
S. Bethard, Z. Lu, J.H. Martin, and L. Hunter. 2008. Se-
mantic role labeling for protein transport predicates. BMC
Bioinformatics, 9:277.
W. Daelemans, J. Zavrel, K. Van der Sloot, and A. Van den
Bosch. 2009. TiMBL: Tilburg memory based learner, ver-
sion 6.2, reference guide. Technical Report Series 09-01,
ILK, Tilburg, The Netherlands.
K. Sagae and J. Tsujii. 2007. Dependency parsing and do-
main adaptation with LR models and parser ensembles. In
Proc. of CoNLL 2007: Shared Task, pages 82?94, Prague,
Czech Republic.
Y. Sasaki, P. Thompson, Ph. Cotter, J. McNaught, and S. Ana-
niadou. 2008. Event frame extraction based on a gene
regulation corpus. In Proc. of Coling 2008, pages 761?
768, Manchester, UK.
P. Thompson, S. A Igbal, J. McNaught, and S. Ananiadou.
2009. Construction of an annotated corpus to support
biomedical information extraction. BMC Bioinformatics,
10:349.
R. Tzong-Han Tsai et al 2007. BIOSMILE: A semantic role
labeling system for biomedical verbs using a maximum-
entropy model with automatically generated template fea-
tures. BMC Bioinformatics, 8:325.
127
Proceedings of the Fourteenth Conference on Computational Natural Language Learning: Shared Task, pages 40?47,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Memory-Based Resolution of In-Sentence Scopes of Hedge Cues
Roser Morante, Vincent Van Asch, Walter Daelemans
CLiPS - University of Antwerp
Prinsstraat 13
B-2000 Antwerpen, Belgium
{Roser.Morante,Walter.Daelemans,Vincent.VanAsch}@ua.ac.be
Abstract
In this paper we describe the machine
learning systems that we submitted to the
CoNLL-2010 Shared Task on Learning to
Detect Hedges and Their Scope in Nat-
ural Language Text. Task 1 on detect-
ing uncertain information was performed
by an SVM-based system to process the
Wikipedia data and by a memory-based
system to process the biological data.
Task 2 on resolving in-sentence scopes of
hedge cues, was performed by a memory-
based system that relies on information
from syntactic dependencies. This system
scored the highest F1 (57.32) of Task 2.
1 Introduction
In this paper we describe the machine learning
systems that CLiPS1 submitted to the closed track
of the CoNLL-2010 Shared Task on Learning to
Detect Hedges and Their Scope in Natural Lan-
guage Text (Farkas et al, 2010).2 The task con-
sists of two subtasks: detecting whether a sentence
contains uncertain information (Task 1), and re-
solving in-sentence scopes of hedge cues (Task 2).
To solve Task 1, systems are required to classify
sentences into two classes, ?Certain? or ?Uncer-
tain?, depending on whether the sentence contains
factual or uncertain information. Three annotated
training sets are provided: Wikipedia paragraphs
(WIKI), biological abstracts (BIO-ABS) and bio-
logical full articles (BIO-ART). The two test sets
consist of WIKI and BIO-ART data.
Task 2 requires identifying hedge cues and find-
ing their scope in biomedical texts. Finding the
scope of a hedge cue means determining at sen-
tence level which words in the sentence are af-
fected by the hedge cue. For a sentence like the
1Web page: http://www.clips.ua.ac.be
2Web page: http://www.inf.u-szeged.hu/rgai
/conll2010st
one in (1) extracted from the BIO-ART training
corpus, systems have to identify likely and sug-
gested as hedge cues, and they have to find that
likely scopes over the full sentence, and that sug-
gested scopes over by the role of murine MIB in
TNF? signaling. A scope will be correctly re-
solved only if both the cue and the scope are cor-
rectly identified.
(1) <xcope id=2> The conservation from Drosophila to
mammals of these two structurally distinct but
functionally similar E3 ubiquitin ligases is <cue
ref=2>likely</cue> to reflect a combination of
evolutionary advantages associated with: (i)
specialized expression pattern, as evidenced by the
cell-specific expression of the neur gene in sensory
organ precursor cells [52]; (ii) specialized function, as
<xcope id=1> <cue ref=1>suggested</cue> by the
role of murine MIB in TNF? signaling</xcope> [32];
(iii) regulation of protein stability, localization, and/or
activity</xcope>.
Systems are to be trained on BIO-ABS and
BIO-ART and tested on BIO-ART. Example (1)
shows that sentences in the BIO-ART dataset can
be quite complex because of their length, because
of their structure - very often they contain enu-
merations, and because they contain bibliographic
references and references to tables and figures.
Handling these phenomena is necessary to detect
scopes correctly in the setting of this task. Note
that the scope of suggested above does not include
the bibliographic reference [32], whereas the scope
of likely includes all the bibliographic references,
and that the scope of likely does not include the
final punctuation mark.
In the case of the BIO data, we approach Task
1 as a prerequisite for Task 2. Therefore we treat
them as two consecutive classification tasks: a first
one that consists of classifying the tokens of a sen-
tence as being at the beginning of a hedge sig-
nal, inside or outside. This allows the system to
find multiword hedge cues. We tag a sentence as
uncertain if at least a hedge cue is found in the
sentence. The second classification task consists
40
of classifying the tokens of a sentence as being
the first element of the scope, the last, or nei-
ther. This happens as many times as there are
hedge cues in the sentence. The two classification
tasks are implemented using memory-based learn-
ers. Memory-based language processing (Daele-
mans and van den Bosch, 2005) is based on the
idea that NLP problems can be solved by reuse of
solved examples of the problem stored in memory.
Given a new problem, the most similar examples
are retrieved, and a solution is extrapolated from
them.
Section 2 is devoted to related work. In Sec-
tion 3 we describe how the data have been prepro-
cessed. In Section 4 and Section 5 we present the
systems that perform Task 1 and Task 2. Finally,
Section 6 puts forward some conclusions.
2 Related work
Hedging has been broadly treated from a theoret-
ical perspective. The term hedging is originally
due to Lakoff (1972). Palmer (1986) defines a
term related to hedging, epistemic modality, which
expresses the speaker?s degree of commitment to
the truth of a proposition. Hyland (1998) focuses
specifically on scientific texts. He proposes a prag-
matic classification of hedge expressions based on
an exhaustive analysis of a corpus. The catalogue
of hedging cues includes modal auxiliaries, epis-
temic lexical verbs, epistemic adjectives, adverbs,
nouns, and a variety of non?lexical cues. Light
et al (2004) analyse the use of speculative lan-
guage in MEDLINE abstracts. Some NLP appli-
cations incorporate modality information (Fried-
man et al, 1994; Di Marco and Mercer, 2005).
As for annotated corpora, Thompson et al (2008)
report on a list of words and phrases that express
modality in biomedical texts and put forward a cat-
egorisation scheme. Additionally, the BioScope
corpus (Vincze et al, 2008) consists of a collec-
tion of clinical free-texts, biological full papers,
and biological abstracts annotated with negation
and speculation cues and their scope.
Although only a few pieces of research have fo-
cused on processing negation, the two tasks of the
CoNLL-2010 Shared Task have been addressed
previously. As for Task 1, Medlock and Briscoe
(2007) provide a definition of what they consider
to be hedge instances and define hedge classifi-
cation as a weakly supervised machine learning
task. The method they use to derive a learning
model from a seed corpus is based on iteratively
predicting labels for unlabeled training samples.
They report experiments with SVMs on a dataset
that they make publicly available3. The experi-
ments achieve a recall/precision break even point
(BEP) of 0.76. They apply a bag-of-words ap-
proach to sample representation. Medlock (2008)
presents an extension of this work by experiment-
ing with more features (part-of-speech, lemmas,
and bigrams). With a lemma representation the
system achieves a peak performance of 0.80 BEP,
and with bigrams of 0.82 BEP. Szarvas (2008) fol-
lows Medlock and Briscoe (2007) in classifying
sentences as being speculative or non-speculative.
Szarvas develops a MaxEnt system that incor-
porates bigrams and trigrams in the feature rep-
resentation and performs a complex feature se-
lection procedure in order to reduce the number
of keyword candidates. It achieves up to 0.85
BEP and 85.08 F1 by using an external dictio-
nary. Kilicoglu and Bergler (2008) apply a lin-
guistically motivated approach to the same clas-
sification task by using knowledge from existing
lexical resources and incorporating syntactic pat-
terns. Additionally, hedge cues are weighted by
automatically assigning an information gain mea-
sure and by assigning weights semi?automatically
depending on their types and centrality to hedging.
The system achieves results of 0.85 BEP.
As for Task 2, previous work (Morante and
Daelemans, 2009; O?zgu?r and Radev, 2009) has
focused on finding the scope of hedge cues in
the BioScope corpus (Vincze et al, 2008). Both
systems approach the task in two steps, identify-
ing the hedge cues and finding their scope. The
main difference between the two systems is that
Morante and Daelemans (2009) perform the sec-
ond phase with a machine learner, whereas O?zgur
and Radev (2009) perform the second phase with
a rule-based system that exploits syntactic infor-
mation.
The approach to resolving the scopes of hedge
cues that we present in this paper is similar to
the approach followed in Morante and Daelemans
(2009) in that the task is modelled in the same
way. A difference between the two systems is that
this system uses only one classifier to solve Task
2, whereas the system described in Morante and
Daelemans (2009) used three classifiers and a met-
3Available at
http://www.benmedlock.co.uk/hedgeclassif.html.
41
alearner. Another difference is that the system in
Morante and Daelemans (2009) used shallow syn-
tactic features, whereas this system uses features
from both shallow and dependency syntax. A third
difference is that that system did not use a lexicon
of cues, whereas this system uses a lexicon gener-
ated from the training data.
3 Preprocessing
As a first step, we preprocess the data in order
to extract features for the machine learners. We
convert the xml files into a token-per-token rep-
resentation, following the standard CoNLL for-
mat (Buchholz and Marsi, 2006), where sentences
are separated by a blank line and fields are sepa-
rated by a single tab character. A sentence consists
of a sequence of tokens, each one starting on a new
line.
The WIKI data are processed with the Memory
Based Shallow Parser (MBSP) (Daelemans and
van den Bosch, 2005) in order to obtain lemmas,
part-of-speech (PoS) tags, and syntactic chunks,
and with the MaltParser (Nivre, 2006) in order to
obtain dependency trees. The BIO data are pro-
cessed with the GDep parser (Sagae and Tsujii,
2007) in order to get the same information.
# WORD LEMMA PoS CHUNK NE D LABEL C S
1 The The DT B-NP O 3 NMOD O O O
2 structural structural JJ I-NP O 3 NMOD O O O
3 evidence evidence NN I-NP O 4 SUB O O O
4 lends lend VBZ B-VP O 0 ROOT B F O
5 strong strong JJ B-NP O 6 NMOD I O O
6 support support NN I-NP O 4 OBJ I O O
7 to to TO B-PP O 6 NMOD O O O
8 the the DT B-NP O 11 NMOD O O O
9 inferred inferred JJ I-NP O 11 NMOD B O F
10 domain domain NN I-NP O 11 NMOD O O O
11 pair pair NN I-NP O 7 PMOD O L L
12 , , , O O 4 P O O O
13 resulting result VBG B-VP O 4 VMOD O O O
14 in in IN B-PP O 13 VMOD O O O
15 a a DT B-NP O 18 NMOD O O O
16 high high JJ I-NP O 18 NMOD O O O
17 confidence confidence NN I-NP O 18 NMOD O O O
18 set set NN I-NP O 14 PMOD O O O
19 of of IN B-PP O 18 NMOD O O O
20 domain domain NN B-NP O 21 NMOD O O O
21 pairs pair NNS I-NP O 19 PMOD O O O
22 . . . O O 4 P O O O
Table 1: Preprocessed sentence.
Table 1 shows a preprocessed sentence with the
following information per token: the token num-
ber in the sentence, word, lemma, PoS tag, chunk
tag, named entity tag, head of token in the depen-
dency tree, dependency label, cue tag, and scope
tags separated by a space, for as many cues as
there are in the sentence.
In order to check whether the conversion from
the xml format to the CoNLL format is a source
of error propagation, we convert the gold CoNLL
files into xml format and we run the scorer pro-
vided by the task organisers. The results obtained
are listed in Table 2.
Task 1 Task 2
WIKI BIO-ART BIO-ABS BIO-ART BIO-ABS
F1 100.00 100.00 100.00 99.10 99.66
Table 2: Evaluation of the conversion from xml to
CoNLL format.
4 Task 1: Detecting uncertain
information
In Task 1 sentences have to be classified as con-
taining uncertain or unreliable information or not.
The task is performed differently for theWIKI and
for the BIO data, since we are interested in finding
the hedge cues in the BIO data, as a first step to-
wards Task 2.
4.1 Wikipedia system (WIKI)
In the WIKI data a sentence is marked as uncertain
if it contains at least one weasel, or cue for uncer-
tainty. The list of weasels is quite extensive and
contains a high number of unique occurrences. For
example, the training data contain 3133 weasels
and 1984 weasel types, of which 63% are unique.
This means that a machine learner will have diffi-
culties in performing the classification task. Even
so, some generic structures can be discovered
in the list of weasels. For example, the differ-
ent weasels A few people and A few sprawling
grounds follow a pattern. We manually select the
42 most frequent informative tokens4 from the list
of weasels in the training partition. In the remain-
der of this section we will refer to these tokens as
weasel cues.
Because of the wide range of weasels, we opt
for predicting the (un)certainty of a sentence, in-
stead of identifying the weasels. The sentence
classification is done in three steps: instance cre-
ation, SVM classification and sentence labeling.
4Weasel cues: few, number, variety, bit, great, majority,
range, variety, all, almost, arguably, certain, commonly, gen-
erally, largely, little, many, may, most, much, numerous, of-
ten, one, other, others, perhaps, plenty of, popular, possibly,
probably, quite, relatively, reportedly, several, some, suggest,
there be, the well-known, various, very, wide, widely.
42
4.1.1 Instance creation
Although we only want to predict the (un)certainty
of a sentence as a whole, we classify every token
in the sentence separately. After parsing the data
we create one instance per token, with the excep-
tion of tokens that have a part-of-speech from the
list: #, $, :, LS, RP, UH, WP$, or WRB. The ex-
clusion of these tokens is meant to simplify the
classification task.
The features used by the system during classifi-
cation are the following:
? About the token: word, lemma, PoS tag, chunk tag,
dependency head, and dependency label.
? About the token context: lemma, PoS tag, chunk tag
and dependency label of the two tokens to the left and
right of the token in focus in the string of words of the
sentence.
? About the weasel cues: a binary marker that indicates
whether the token in focus is a weasel cue or not, and a
number defining the number of weasel cues that there
are in the entire sentence.
These instances with 24 non-binary features
carry the positive class label if the sentence is un-
certain. We use a binarization script that rewrites
the instance to a format that can be used with a
support vector machine and during this process,
feature values that occur less than 2 times are
omitted.
4.1.2 SVM classification
To label the instances of the unseen data we use
SVMlight (Joachims, 2002). We performed some
experiments with different settings and decided
to only change the type of kernel from the de-
fault linear kernel to a polynomial kernel. For
the Wikipedia training data, the training of the
246,876 instances with 68417 features took ap-
proximately 22.5 hours on a 32 bit, 2.2GHz, 2GB
RAM Mac OS X machine.
4.1.3 Sentence labeling
In this last step, we collect all instances from the
same sentence and inspect the predicted labels for
every token. If more than 5% of the instances are
marked as uncertain, the whole sentence is marked
as uncertain. The idea behind the setup is that
many tokens are very ambiguous in respect to un-
certainty because they do not carry any informa-
tion. Fewer tokens are still ambiguous, but contain
some information, and a small set of tokens are al-
most unambiguous. This small set of informative
tokens does not have to coincide with weasels nor
weasels cues. The result is that we cannot predict
the actual weasels in a sentence, but we get an in-
dication of the presence of tokens that are common
in uncertain sentences.
4.2 Biological system (BIO)
The system that processes the BIO data is different
from the system that processes theWIKI data. The
BIO system uses a classifier that predicts whether
a token is at the beginning of a hedge signal, inside
or outside. So, instances represent tokens. The in-
stance features encode the following information:
? About the token: word, lemma, PoS tag, chunk tag, and
dependency label.
? About the context to the left and right in the string of
words of the sentence: word of the two previous and
three next tokens, lemma and dependency label of pre-
vious and next tokens, deplabel, and chunk tag and PoS
of next token. A binary feature indicating whether the
next token has an SBAR chunk tag.
? About the context in the syntactic dependency tree:
chain of PoS tags, chunk tags and dependency label
of children of token; word, lemma, PoS tag, chunk tag,
and dependency label of father; combined tag with the
lemma of the token and the lemma of its father; chain
of dependency labels from token to ROOT. Lemma of
next token, if next token is syntactic child of token. If
token is a verb, lemma of the head of the token that is
its subject.
? Dictionary features. We extract a list of hedge cues
from the training corpus. Based on this list, two binary
features indicate whether token and next token are po-
tential cues.
? Lemmas of the first noun, first verb and first adjective
in the sentence.
The classifier is the decision tree IGTree as im-
plemented in TiMBL (version 6.2) 5(Daelemans
et al, 2009), a fast heuristic approximation of k-
nn, that makes a heuristic approximation of near-
est neighbor search by a top down traversal of the
tree. It was parameterised by using overlap as the
similarity metric and information gain for feature
weighting. Running the system on the test data
takes 10.44 seconds in a 64 bit 2.8GHz 8GB RAM
Intel Xeon machine with 4 cores.
4.3 Results
All the results published in the paper are calcu-
lated with the official scorer provided by the task
organisers. We provide precision (P), recall (R)
and F1. The official results of Task 1 are pre-
sented in Table 3. We produce in-domain and
5TiMBL: http://ilk.uvt.nl/timbl
43
cross-domain results. The BIO in-domain re-
sults have been produced with the BIO system,
by training on the training data BIO-ABS+BIO-
ART, and testing on the test data BIO-ART. The
WIKI in-domain results have been produced by
the WIKI system by training on WIKI and test-
ing on WIKI. The BIO cross-domain results have
been produced with the BIO system, by train-
ing on BIO-ABS+BIO-ART+WIKI and testing on
BIO-ART. The WIKI cross-domain results have
been produced with the WIKI system by train-
ing on BIO-ABS+BIO-ART+WIKI and testing on
WIKI. Training the SVM with BIO-ABS+BIO-
ART+WIKI augmented the training time exponen-
tially and the system did not finish on time for sub-
mission. We report post-evaluation results.
In-domain Cross-domain
P R F1 P R F1
WIKI 80.55 44.49 57.32 80.64* 44.94* 57.71*
BIO 81.15 82.28 81.71 80.54 83.29 81.89
Table 3: Uncertainty detection results (Task 1 -
closed track). Post-evaluation results are marked
with *.
In-domain results confirm that uncertain sen-
tences inWikipedia text are more difficult to detect
than uncertain sentences in biological text. This
is caused by a loss in recall of the WIKI system.
Compared to results obtained by other systems
participating in the CoNLL-2010 Shared Task, the
BIO system performs 4.47 F1 lower than the best
system, and the WIKI system performs 2.85 F1
lower. This indicates that there is room for im-
provement. As for cross-domain results, we can-
not conclude that the cross-domain data harm the
performance of the system, but we cannot state
either that the cross-domain data improve the re-
sults. Since we performed Task 1 as a step towards
Task 2, it is interesting to know what is the per-
formance of the system in identifying hedge cues.
Results are shown in Table 4. One of the main
sources of errors in detecting the cues are due to
the cue or. Of the 52 occurrences in the test corpus
BIO-ART, the system produces 3 true positives, 8
false positives and 49 false negatives.
In-domain Cross-domain
P R F1 P R F1
Bio 78.75 74.69 76.67 78.14 75.45 76.77
Table 4: Cue matching results (Task 1 - closed
track).
5 Task 2: Resolution of in-sentence
scopes of hedge cues
Task 2 consists of resolving in-sentence scopes of
hedge cues in biological texts. The system per-
forms this task in two steps, classification and
postprocessing, taking as input the output of the
system that finds cues.
5.1 Classification
In the classification step a memory-based classi-
fier classifies tokens as being the first token in the
scope sequence, the last, or neither, for as many
cues as there are in the sentence. An instance rep-
resents a pair of a predicted hedge cue and a token.
All tokens in a sentence are paired with all hedge
cues that occur in the sentence.
The classifier used is an IB1 memory?based al-
gorithm as implemented in TiMBL (version 6.2)6
(Daelemans et al, 2009), a memory-based classi-
fier based on the k-nearest neighbor rule (Cover
and Hart, 1967). The IB1 algorithm is parame-
terised by using overlap as the similarity metric,
gain ratio for feature weighting, using 7 k-nearest
neighbors, and weighting the class vote of neigh-
bors as a function of their inverse linear distance.
Running the system on the test data takes 53 min-
utes in a 64 bit 2.8GHz 8GB RAM Intel Xeon ma-
chine with 4 cores.
The features extracted to perform the classifi-
cation task are listed below. Because, as noted
by O?zgu?r and Radev (2009) and stated in the an-
notation guidelines of the BioScope corpus7, the
scope of a cue can be determined from its lemma,
PoS tag, and from the syntactic construction of the
clause (passive voice vs. active, coordination, sub-
ordination), we use, among others, features that
encode information from the dependency tree.
? About the cue: chain of words, PoS label, dependency
label, chunk label, chunk type; word, PoS tag, chunk
tag, and chunk type of the three previous and next to-
kens in the string of words in the sentence; first and
last word, chain of PoS tags, and chain of words of the
chunk where cue is embedded, and the same features
for the two previous and two next chunks; binary fea-
ture indicating whether cue is the first, last or other to-
ken in sentence; binary feature indicating whether cue
is in a clause with a copulative construction; PoS tag
and dependency label of the head of cue in the depen-
dency tree; binary feature indicating whether cue is lo-
cated before or after its syntactic head in the string of
6TiMBL: http://ilk.uvt.nl/timbl.
7Available at: http://www.inf.u-szeged.hu/
rgai/project/nlp/bioscope/Annotation%20
guidelines2.1.pdf.
44
words of the sentence; feature indicating whether cue
is followed by an S-BAR or a coordinate construction.
? About the token: word, PoS tag, dependency label,
chunk tag, chunk type; word, PoS tag, chunk tag, and
chunk type of the three previous and three next tokens
in the string of words of the sentence; chain of PoS
tag and lemmas of two and three tokens to the right of
token in the string of words of the sentence; first and
last word, chain of PoS tags, and chain of words of the
chunk where token is embedded, and the same features
for the two previous and two next chunks; PoS tag and
deplabel of head of token in the dependency tree; bi-
nary feature indicating whether token is part of a cue.
? About the token in relation to cue: binary features indi-
cating whether token is located before or after cue and
before or after the syntactic head of cue in the string
of words of the sentence; chain of PoS tags between
cue and token in the string of words of the sentence;
normalised distance between cue and token (number of
tokens in between divided by total number of tokens);
chain of chunks between cue and token; feature indi-
cating whether token is located before cue, after cue or
wihtin cue.
? About the dependency tree: feature indicating who is
ancestor (cue, token, other); chain of dependency la-
bels and chain of PoS tags from cue to common an-
cestor, and from token to common ancestor, if there is
a common ancestor; chain of dependency labels and
chain of PoS from token to cue, if cue is ancestor of to-
ken; chain of dependency labels and chain of PoS from
cue to token, if token is ancestor of cue; chain of de-
pendency labels and PoS from cue to ROOT and from
token to ROOT.
Features indicating whether token is a candidate to be
the first token of scope (FEAT-FIRST), and whether
token is a candidate to be the last token of the scope
(FEAT-LAST). These features are calculated by a
heuristics that takes into account detailed information
of the dependency tree. The value of FEAT-FIRST de-
pends on whether the clause is in active or in passive
voice, on the PoS of the cue, and on the lemma in some
cases (for example, verbs appear, seem). The value of
FEAT-LAST depends on the PoS of the cue.
5.2 Postprocessing
In the corpora provided for this task, scopes are
annotated as continuous sequences of tokens that
include the cue. However, the classifiers only pre-
dict the first and last element of the scope. In or-
der to guarantee that all scopes are continuous se-
quences of tokens we apply a first postprocessing
step (P-SCOPE) that builds the sequence of scope
based on the following rules:
1. If one token has been predicted as FIRST and one as
LAST, the sequence is formed by the tokens between
FIRST and LAST.
2. If one token has been predicted as FIRST and none has
been predicted as LAST, the sequence is formed by the
tokens between FIRST and the first token that has value
1 for FEAT-LAST.
3. If one token has been predicted as FIRST and more
than one as LAST, the sequence is formed by the tokens
between FIRST and the first token predicted as LAST
that is located after cue.
4. If one token has been predicted as LAST and none as
FIRST, the sequence will start at the hedge cue and it
will finish at the token predicted as LAST.
5. If no token has been predicted as FIRST and more than
one as LAST, the sequence will start at the hedge cue
and will end at the first token predicted as LAST after
the hedge signal.
6. If one token has been predicted as LAST and more than
one as FIRST, the sequence will start at the cue.
7. If no tokens have been predicted as FIRST and no to-
kens have been predicted as LAST, the sequence will
start at the hedge cue and will end at the first token that
has value 1 for FEAT-LAST.
The system predicts 987 scopes in total. Of
these, 1 FIRST and 1 LAST are predicted in 762
cases; a different number of predictions is made
for FIRST and for LAST in 217 cases; no FIRST
and no LAST are predicted in 5 cases, and 2
FIRST and 2 LAST are predicted in 3 cases. In 52
cases no FIRST is predicted, in 93 cases no LAST
is predicted.
Additionally, as exemplified in Example 1 in
Section 1, bibliographic references and references
to tables and figures do not always fall under the
scope of cues, when the references appear at the
end of the scope sequence. If references that ap-
pear at the end of the sentence have been predicted
by the classifier within the scope of the cue, these
references are set out of the scope in a second post-
processing step (P-REF).
5.3 Results
The official results of Task 2 are presented in Ta-
ble 5. The system scores 57.32 F1, which is the
highest score of the systems that participated in
this task.
In-domain
P R F1
BIO 59.62 55.18 57.32
Table 5: Scope resolution official results (Task 2 -
closed track).
In order to know what is the effect of the post-
processing steps, we evaluate the output of the
system before performing step P-REF and before
performing step P-SCOPE. Table 6 shows the re-
sults of the evaluation. Without P-REF, the perfor-
mance decreases in 7.30 F1. This is caused by the
45
fact that a considerable proportion of scopes end
in a reference to bibliography, tables, or figures.
Without P-SCOPE it decreases 4.50 F1 more. This
is caused, mostly, by the cases in which the classi-
fier does not predict the LAST class.
In-domain
P R F1
BIO before P-REF 51.98 48.20 50.02
BIO before P-SCOPE 48.82 44.43 46.52
Table 6: Scope resolution results before postpro-
cessing steps.
It is not really possible to compare the scores
obtained in this task to existing research previous
to the CoNLL-2010 Shared Task, namely the re-
sults obtained by O?zgu?r and Radev (2009) on the
BioScope corpus with a rule-based system and by
Morante and Daelemans (2009) on the same cor-
pus with a combination of classifiers. O?zgu?r and
Radev (2009) report accuracy scores (61.13 on full
text), but no F measures are reported. Morante and
Daelemans (2009) report percentage of correct
scopes for the full text data set (42.37), obtained
by training on the abstracts data set, whereas the
results presented in Table 5 are reported in F mea-
sures and obtained in by training and testing on
other corpora. Additionally, the system has been
trained on a corpus that contains abstracts and full
text articles, instead of only abstracts. However,
it is possible to confirm that, even with informa-
tion on dependency syntax, resolving the scopes of
hedge cues in biological texts is not a trivial task.
The scores obtained in this task are much lower
than the scores obtained in other tasks that involve
semantic processing, like semantic role labeling.
The errors of the system in Task 2 are caused
by different factors. First, there is error propaga-
tion from the system that finds cues. Second, the
system heavily relies on information from the syn-
tactic dependency tree. The parser used to prepro-
cess the data (GDep) has been trained on abstracts,
instead of full articles, which means that the per-
formance on full articles will be lower, since sen-
tence are longer and more complex. Third, en-
coding the information of the dependency tree
in features for the learner is not a straightfor-
ward process. In particular, some errors in resolv-
ing the scope are caused by keeping subordinate
clauses within the scope, as in sentence (2), where,
apart from not identifying speculated as a cue, the
system wrongly includes resulting in fewer high-
confidence sequence assignments within the scope
of may. This error is caused in the instance con-
struction phase, because token assignments gets
value 1 for feature FEAT-LAST and token algo-
rithm gets value 0, whereas it should have been
otherwise.
(2) We speculated that the presence of multiple isotope
peaks per fragment ion in the high resolution Orbitrap
MS/MS scans <xcope id=1><cue ref=1>may
</cue> degrade the sensitivity of the search
algorithm, resulting in fewer high-confidence
sequence assignments</xcope>.
Additionally, the test corpus contains an article
about the annotation of a corpus of hedge cues,
thus, an article that contains metalanguage. Our
system can not deal with sentences like the one in
(3), in which all cues with their scopes are false
positives.
(3) For example, the word <xcope id=1><cue ref=1>
may</cue> in sentence 1</xcope>) <xcope id=2>
<cue ref=2>indicates that</cue> there is some
uncertainty about the truth of the event, whilst the
phrase Our results show that in 2) <xcope id=3>
<cue ref=3>indicates that</cue> there is
experimental evidence to back up the event described
by encodes</xcope></xcope>.
6 Conclusions and future research
In this paper we presented the machine learning
systems that we submitted to the CoNLL-2010
Shared Task on Learning to Detect Hedges and
Their Scope in Natural Language Text. The BIO
data were processed by memory-based systems in
Task 1 and Task 2. The system that performs Task
2 relies on information from syntactic dependen-
cies. This system scored the highest F1 (57.32) of
Task 2.
As for Task 1, in-domain results confirm that
uncertain sentences inWikipedia text are more dif-
ficult to detect than uncertain sentences in biolog-
ical text. One of the reasons is that the number of
weasels is much higher and diverse than the num-
ber of hedge cues. BIO cross-domain results show
that adding WIKI data to the training set causes a
slight decrease in precision and a slight increase
in recall. The errors of the BIO system show that
some cues, like or are difficult to identify, because
they are ambiguous. As for Task 2, results indi-
cate that resolving the scopes of hedge cues in bi-
ological texts is not a trivial task. The scores ob-
tained in this task are much lower than the scores
obtained in other tasks that involve semantic pro-
cessing, like semantic role labeling. The results
46
are influenced by propagation of errors from iden-
tifying cues, errors in the dependency tree, the ex-
traction process of syntactic information from the
dependency tree to encode it in the features, and
the presence of metalanguage on hedge cues in the
test corpus. Future research will focus on improv-
ing the identification of hedge cues and on using
different machine learning techniques to resolve
the scope of cues.
Acknowledgements
The research reported in this paper was made pos-
sible through financial support from the University
of Antwerp (GOA project BIOGRAPH).
References
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-
X shared task on multilingual dependency parsing.
In Proceedings of the CoNLL-X Shared Task, New
York. SIGNLL.
Thomas M. Cover and Peter E. Hart. 1967. Nearest
neighbor pattern classification. Institute of Electri-
cal and Electronics Engineers Transactions on In-
formation Theory, 13:21?27.
Walter Daelemans and Antal van den Bosch. 2005.
Memory-based language processing. Cambridge
University Press, Cambridge, UK.
Walter Daelemans, Jakub Zavrel, Ko Van der Sloot, and
Antal Van den Bosch. 2009. TiMBL: Tilburg Mem-
ory Based Learner, version 6.2, Reference Guide.
Number 09-01 in Technical Report Series. Tilburg,
The Netherlands.
Chrysanne Di Marco and Robert E. Mercer, 2005.
Computing attitude and affect in text: Theory and
applications, chapter Hedging in scientific articles
as a means of classifying citations. Springer-Verlag,
Dordrecht.
Richa?rd Farkas, Veronika Vincze, Gyo?rgy Mo?ra, Ja?nos
Csirik, and Gyo?rgy Szarvas. 2010. The CoNLL-
2010 Shared Task: Learning to Detect Hedges and
their Scope in Natural Language Text. In Proceed-
ings of the Fourteenth Conference on Computational
Natural Language Learning (CoNLL-2010): Shared
Task, pages 1?12, Uppsala, Sweden, July. Associa-
tion for Computational Linguistics.
Carol Friedman, Philip Alderson, John Austin, James J.
Cimino, and Stephen B. Johnson. 1994. A general
natural?language text processor for clinical radiol-
ogy. Journal of the American Medical Informatics
Association, 1(2):161?174.
Ken Hyland. 1998. Hedging in scientific research ar-
ticles. John Benjamins B.V, Amsterdam.
Thorsten Joachims. 2002. Learning to Classify Text
Using Support Vector Machines, volume 668 of The
Springer International Series in Engineering and
Computer Science. Springer.
Halil Kilicoglu and Sabine Bergler. 2008. Recogniz-
ing speculative language in biomedical research ar-
ticles: a linguistically motivated perspective. BMC
Bioinformatics, 9(Suppl 11):S10.
George Lakoff. 1972. Hedges: a study in meaning
criteria and the logic of fuzzy concepts. Chicago
Linguistics Society Papers, 8:183?228.
Marc Light, Xin Y.Qiu, and Padmini Srinivasan. 2004.
The language of bioscience: facts, speculations, and
statements in between. In Proceedings of the Bi-
oLINK 2004, pages 17?24.
Ben Medlock and Ted Briscoe. 2007. Weakly super-
vised learning for hedge classification in scientific
literature. In Proceedings of ACL 2007, pages 992?
999.
Ben Medlock. 2008. Exploring hedge identification in
biomedical literature. Journal of Biomedical Infor-
matics, 41:636?654.
Roser Morante and Walter Daelemans. 2009. Learn-
ing the scope of hedge cues in biomedical texts. In
Proceedings of BioNLP 2009, pages 28?36, Boul-
der, Colorado.
Joakim Nivre. 2006. Inductive Dependency Parsing,
volume 34 of Text, Speech and Language Technol-
ogy. Springer.
Arzucan O?zgu?r and Dragomir R. Radev. 2009. Detect-
ing speculations and their scopes in scientific text.
In Proceedings of EMNLP 2009, pages 1398?1407,
Singapore.
Frank R. Palmer. 1986. Mood and modality. CUP,
Cambridge, UK.
Kenji Sagae and Jun?ichi Tsujii. 2007. Dependency
parsing and domain adaptation with LR models and
parser ensembles. In Proceedings of CoNLL 2007:
Shared Task, pages 82?94, Prague, Czech Republic.
Gyo?rgy Szarvas. 2008. Hedge classification in
biomedical texts with a weakly supervised selection
of keywords. In Proceedings of ACL 2008, pages
281?289, Columbus, Ohio, USA. ACL.
Paul Thompson, Giulia Venturi, John McNaught,
Simonetta Montemagni, and Sophia Ananiadou.
2008. Categorising modality in biomedical texts. In
Proceedings of the LREC 2008 Workshop on Build-
ing and Evaluating Resources for Biomedical Text
Mining 2008, pages 27?34, Marrakech. LREC.
Veronika Vincze, Gyo?rgy Szarvas, Richa?rd Farkas,
Gyo?rgy Mo?ra, and Ja?nos Csirik. 2008. The Bio-
Scope corpus: biomedical texts annotated for uncer-
tainty, negation and their scopes. BMC Bioinformat-
ics, 9(Suppl 11):S9.
47
Corpus-based approaches to processing the scope of negation
cues: an evaluation of the state of the art
Roser Morante, Sarah Schrauwen and Walter Daelemans
CLiPS-CL University of Antwerp
{Roser.Morante,Walter.Daelemans}@ua.ac.be
Sarah.Schrauwen@student.ua.ac.be
Abstract
In this paper we summarize existing work on the recently introduced task of processing the scope
of negation and modality cues; we analyse the scope model that existing systems can process, which
is mainly the model reflected in the annotations of the biomedical corpus on which the systems have
been trained; and we point out aspects of the scope finding task that would be different based on
observations from a corpus from a different domain and nature.
1 Introduction
Negation and modality are complex aspects of the semantics of language. Modality was introduced
by Jespersen (1924), who distinguishes between two categories of mood that later have been named
as deontic modality and epistemic modality. Lyons (1996) describes epistemic modality as concerned
with matters of knowledge and belief, ?the speaker?s opinion or attitude towards the proposition that the
sentence expresses or the situation that the proposition describes?. Palmer (1986) defines it as expressing
the speaker?s degree of commitment to the truth of a proposition. Polarity is a discrete category that can
take two values: positive and negative. Positive polarity is used by speakers to put information as a fact
in the world, whereas negative polarity is used to put information as a counterfact, a fact that does not
hold in the world. Negation is a linguistic resource used to express negative polarity.
Although the treatment of these topics in computational linguistics is relatively new compared to
other areas like machine translation, parsing or semantic role labeling, incorporating information about
modality and polarity has been shown to be useful for a number of applications, such as biomedical
text processing (Di Marco and Mercer, 2005; Chapman et al, 2001), opinion mining and sentiment
analysis (Wilson et al, 2005), recognizing textual entailment (Snow et al, 2006), and automatic style
checking (Ganter and Strube, 2009). In general, the treatment of modality and negation is very relevant
for computational applications that process factuality (Saur??, 2008). For example, information extraction
systems may be confronted with fragments of texts like the one presented in (1)1, which contains two
negation cues2 (not, un-) and one speculation cue (likely) that affect the factuality of the events being
expressed:
(1) The atovaquone/proguanil combination has not been widely used yet in West Africa so it is unlikely that the patient
was initially infected with an atovaquone-resistant strain.
So far two main tasks have been addressed within the natural language processing (NLP) community:
(i) the detection of various forms of polarity and modality and (ii) the identification of the scope of
negation and modality cues. In this paper we reflect on the achievements of the recently introduced
scope finding task (Section 2), we analyse the scope model that existing systems can process (Section 3),
and we point out aspects of the scope finding task that would be different based on observations from a
corpus from a different domain (Section 4).
1Example to be found in http://www.biomedcentral.com/content/pdf/1475-2875-1-1.pdf [last consulted 8-10-2010]
2A cue is the lexical marker that expresses negation or modality.
350
2 Achievements in scope processing
In the last years, several corpora have been annotated with information related to modality and polarity,
which have made it possible to develop machine learning systems. Annotation has been performed at
different levels: word (Hassan and Radev, 2010), expression (Baker et al, 2010; Toprak et al, 2010),
sentence (Medlock and Briscoe, 2007), event (Saur?? and Pustejovsky, 2009), discourse relation (Prasad
et al, 2006), text (Amancio et al, 2010), and scope of negation and modality cues (Vincze et al, 2008).
Thanks to the existence of the BioScope corpus, the scope processing task was recently introduced. Bio-
Scope is a freely available resource, that consists of three parts of medical and biological texts annotated
with negation and hedge cues and their scope.
The scope processing task is concerned with determining at a sentence level which tokens are affected
by modality and negation cues. It was first modelled as a classification problem by Morante et al (2008).
Later on several systems have been trained on the same corpus (Morante and Daelemans, 2009; O?zgu?r
and Radev, 2009; Agarwal and Yu, 2010; Li et al, 2010). Councill et al (2010) process scopes of
negation cues in a different corpus of product reviews, but this corpus is not publicly available.
The CoNLL Shared Task 2010 on Learning to detect hedges and their scope in natural language
text (Farkas et al, 2010) boosted research on the topic. It consisted of identifying sentences containing
uncertainty and recognizing speculative text spans inside sentences. Participating systems would, for
example, produce the tagged sentence in (2)3, in which propose, suggest and possible are identified as
hedge cues and their scope is marked in agreement with the gold standard.
(2) We [propose propose that the existence of the alternative alignments, specific to distinct groups of genes, [suggest
suggests presence of different synchronization modes between the two organisms and [possible possible functional
decoupling of particular physiological gene networks in the course of evolution possible]suggest]propose] .
The best system (Morante et al, 2010) for hedge scope finding in the CoNLL ST 2010 scores 57.32
F-score. Although the results are lower than the scores obtained in other well established tasks (i.e.
semantic role labeling, dependency parsing), we can say that setting the first step towards automatic scope
processing is an achievement. However, it can be useful to revise the characteristics of the scopes that
the systems learn to process, not from a technical machine learning perspective, but from the linguistic
annotation perspective, since the annotation model that systems learn determines the quality of the system
output and the knowledge that can be inferred from the scopes.
3 Scope model based on the BioScope corpus
Most existing scope labelers have been trained on the BioScope corpus. Thus, the model of scope that
these systems learn is determined by the characteristics of scope as they have been annotated in BioScope.
Additionally, the systems have been trained for a specific domain, biomedical texts, but it might be the
case that negation and speculation cues require different annotation specifications for texts from other
domains. In this section we analyze the characteristics of the scope model in the BioScope corpus based
on the guidelines (BioScope, 2008) and we propose some changes for further annotation work that we
are carrying out. We mark in italics the statements from the BioScope guidelines and we comment on
them.
? The scope is always a continuous sequence of tokens and the cue is included in the scope. Although
most scopes in the corpus are continuous, examples such as (3), in which sentence adverbs do not belong
to the scope, suggest that the scopes should be annotated as discontinuous if necessary:
(3) [not The number of glucocorticoid receptors per cell (Ro) and the binding affinity (Kd) for dexamethasone werenot] ,
however, [notnot significantly different not]
? Scopes can be determined on the basis of syntax and they extend to the biggest unit possible. If
necessary, complements and adjuncts are included in the scope. It would be useful to furhter specify
how different syntactic constructions (coordination, subordination, etc.) should be annotated.
3In the examples below, cues will be marked in bold and their scope between brackets.
351
? The scope of negative auxiliaries, adjectives and adverbs usually starts with the cue and ends at the
end of the phrase, clause or sentence. In (4) the scope extends to the right of not. In our view, the scope
should include the subject because the subject contributes to the meaning of the event being negated. If,
as Lyons (1996) suggest, we paraphrase the negative connective in (4) with the formula it is not the case
that, we obtain (5), where the subject is under the scope of the formula.
(4) Once again, the Disorder module does [not not contribute positively to the prediction not]
(5) Once again, it is not the case that the Disorder module does contribute positively to the prediction
? Passive voice changes the scope of the cue because the object of the active construction is the subject
of the passive construction. According to the BioScope guidelines, the scope of not in 6 and 7 would
be annotated differently. As indicated above, we consider that the subject of the active sentence is also
under the scope of the negation, so in our view both sentences should be analyzed equally.
(6) [not Levels of RNA coding for the receptor were not modulated by exposure to high levels of ligand not]
(7) Exposure to high levels of ligand does [not not modulate levels of RNA coding for the receptor not]
? Negative conjunctions generally scope over the syntactic unit whose members it coordinates. However,
if the complex negative keyword occurs within the subject of the sentence, its scope is extended to the
whole sentence. (8) is the example provided in the guidelines, but paraphrasing the sentence with the it
is not the case formula as in (9) shows that the subject should also be included in the scope.
(8) In contrast, sodium salicylate (1 mM) inhibited [neither?norneither adhesion nor expression of these adhesion
molecules neither?nor]
(9) In contrast, it is not the case that sodium salicylate (1 mM) inhibits either adhesion or expression of these adhesion
molecules
? Prepositions scope over the following (noun) phrase. (10) is the example provided in the guidelines,
where without scopes over a noun phrase. Nevertheless, without can be followed by a verb phrase, as in
(11). In this case, one could argue that the logical subject of the verb should be included in the scope of
the preposition, since the negation can be paraphrased as in (12).
(10) [without Mildly hyperinflated lungs without focal opacity without]
(11) [without CD28 costimulation without] augments IL-2 secretion of activated lamina propria T cells by increasing
mRNA stability [without without enhancing IL-2 gene transactivation without]
(12) It is not the case that CD28 costimulation enhances IL-2 gene transactivation
Possible improvements in the BioScope annotation model are pointed out in Vincze (2010), namely
the treatment of elliptic constructions, and discontinuous and intersecting scopes. An additional im-
provement would be to annotate affixal negation. We consider that (13) is equivalent to (14) and should
receive the same analysis, since they can be paraphrased as in (15):
(13) Actually, [un tRNASec and tRNAPyl have unusual secondary structures 515 un]
(14) Actually, [not tRNASec and tRNAPyl do not have usual secondary structures 515 not]
(15) Actually, it is not the case that tRNASec and tRNAPyl have usual secondary structures 515
4 Annotating scopes in a different domain
The existing scope labelers have been trained on biomedical texts. However, it is reasonable to expect
that texts from other domains contain different phenomena that would affect the systems performance.
We are currently analysing negations and their scopes in a complete different corpus, The Hound of the
Baskervilles (HB) by Conan Doyle. This corpus has been annotated with coreference and semantic roles
for the SemEval Task Linking Events and Their Participants in Discourse (Ruppenhofer et al, 2010), and
will be further annotated with negation and modality cues. Phenomena in this corpus show that whereas
the scope of cues can be determined in a similar way as it is determined in biomedical texts, identifying
352
negation cues in certain contexts, which is the first part of the scope finding task, is not only a matter of
lexical lookup:
? Not all negative affixes are negation cues. For example the affix un- in unspoken does not negate its
root morpheme. Unspoken does not mean ?not spoken?, but ?understood without the need for words?.
Consequently, in (16) unspoken is not a negation cue.
(16) All my unspoken instincts, my vague suspicions, suddenly took shape and centred upon the naturalist
? Fixed expressions like could not help in the sentence below do not negate the modified event.
(17) Why about Sir Henry in particular? I could not help asking
? Negation words in tag questions do not have a negation function, but a pragmatic function, since the
speaker seeks confirmation from the addressee. A similar case are negation words in dialogue checks
like don?t you think in (19).
(18) You have been inside the house, have you not, Watson?
(19) Don?t you think, Watson, that you are away from your charge rather long?
? Negation words in exclamative particles do not have a negation function. In (20), don?t tell me does
not express a negated event. This is a multiword construction used to express surprise.
(20) ?Don?t tell me that it is our friend Sir Henry!?
? Some modality cues, such as no doubt, contain false negation cues. In (21) no doubt is a fixed
expression that expresses certainty, no event is negated. It is an expression that acts at the discourse level
conveying information about the attitude of the speaker towards his statement.
(21) Partly it came no doubt from his own masterful nature, which loved to dominate and surprise those who were around
him
? The context influences the effect of the negation cue. The volitive verb wish in (22) and the conditional
construction in (23) cancel the negative effect of not.
(22) Your mission to-day has justified itself, and yet I could almost wish that you had not left his side
(23) In fact, if you had not gone to-day it is exceedingly probable that I should have gone to-morrow
5 Conclusions and future work
In this paper we have briefly presented the achievements in processing the scope of negation and modality
cues. There are currently several systems that can process scopes in biomedical texts, however there is a
lack of annotated resources, since there is only one publicly available corpus. We have also pointed out
that the quality of the systems output depends not only on the technical aspects of the systems, but also
on the linguistic model contained in the annotations. Based on annotation work on a literary corpus, we
have pointed out some difficulties that existing systems could face in detecting cues.
We are currently annotating texts by Conan Doyle with negation cues and their scopes. For defining
the guidelines we take the model of the BioScope corpus as a starting point and we include modifica-
tions based on the observations made above. The annotated corpus and the guidelines will be publicly
available.
Apart from annotating more data, further work will focus on computing the factuality of statements
based on the scopes of negation and modality cues and other contextual features, and studying the inter-
action between negation and modality.
Acknowledgements
This study was made possible through financial support from the University of Antwerp (GOA project
BIOGRAPH). We would like to thank four anonymous reviewers for their suggestions.
353
References
Agarwal, S. and H. Yu (2010). Detecting hedge cues and their scope in biomedical literature. Journal of Biomedical Informat-
ics 710.016/j.jbi.2010.08.003.
Amancio, D. R., R. Fabbri, O. N. Oliveira Jr., M. Nunes, and L. Costa (2010, July). Distinguishing between positive and
negative opinions with complex network features. In Proc. of TextGraphs-5 - 2010 Workshop on Graph-based Methods for
Natural Language Processing, Uppsala, Sweden, pp. 83?87. ACL.
Baker, K., M. Bloodgood, B. Dorr, N. Filardo, L. Levin, and C. Piatko (2010). A modality lexicon and its use in automatic
tagging. In Proceedings of the Seventh conference on International Language Resources and Evaluation (LREC?10), Valetta,
Malta, pp. 1402?1407. European Language Resources Association (ELRA).
BioScope (2008). Annotation guidelines. http://www.inf.u-szeged.hu/rgai/project/nlp/bioscope/Annotation guidelines2.1.pdf.
Chapman, W., W. Bridewell, P. Hanbury, G. Cooper, and B. Buchanan (2001). A simple algorithm for identifying negated
findings and diseases in discharge summaries. J Biomed Inform 34, 301?310.
Councill, I., R. McDonald, and L. Velikovich (2010, July). What?s great and what?s not: learning to classify the scope of
negation for improved sentiment analysis. In Proc. of the Workshop on Negation and Speculation in Natural Language
Processing, Uppsala, Sweden, pp. 51?59. University of Antwerp.
Di Marco, C. and R. Mercer (2005). Computing attitude and affect in text: Theory and applications, Chapter Hedging in
scientific articles as a means of classifying citations. Dordrecht: Springer-Verlag.
Farkas, R., V. Vincze, G. Szarvas, G. Mo?ra, and J. Csirik (Eds.) (2010, July). Proc. of the Fourteenth Conference on Computa-
tional Natural Language Learning. Uppsala, Sweden: ACL.
Ganter, V. and M. Strube (2009). Finding hedges by chasing weasels: Hedge detection using wikipedia tags and shallow
linguistic features. In Proc. of the ACL-IJCNLP 2009 Conference Short Papers, Suntec, Singapore, pp. 173?176.
Hassan, A. and D. Radev (2010, July). Identifying text polarity using random walks. In Proc. of the 48th Annual Meeting of
the ACL, Uppsala, Sweden, pp. 395?403. ACL.
Jespersen, O. (1924). The philosophy of grammar. London: Allen and Unwin.
Li, J., Q. Zhu, and G. Zhou (2010). A unified framework for scope learning via simplified shallow semantic parsing. In Proc.
of EMNLP 2010.
Lyons, J. (1996). Semantics. Cambridge: CUP.
Medlock, B. and T. Briscoe (2007). Weakly supervised learning for hedge classification in scientific literature. In Proc. of ACL
2007, pp. 992?999.
Morante, R. and W. Daelemans (2009). Learning the scope of hedge cues in biomedical texts. In Proc. of BioNLP 2009,
Boulder, Colorado, pp. 28?36.
Morante, R., A. Liekens, and W. Daelemans (2008). Learning the scope of negation in biomedical texts. In Proc. of the EMNLP
2008, Honolulu, Hawaii, pp. 715?724.
Morante, R., V. Van Asch, and W. Daelemans (2010, July). Memory-based resolution of in-sentence scopes of hedge cues. In
Proc. of the Fourteenth Conference on Computational Natural Language Learning, Uppsala, Sweden, pp. 40?47. ACL.
O?zgu?r, A. and D. Radev (2009). Detecting speculations and their scopes in scientific text. In Proc. of EMNLP 2009, Singapore,
pp. 1398?1407.
Palmer, F. (1986). Mood and modality. Cambridge, UK: CUP.
Prasad, R., N. Dinesh, A. Lee, A. Joshi, and B. Webber (2006). Annotating attribution in the penn discourse treebank. In SST
?06: Proc. of the Workshop on Sentiment and Subjectivity in Text, Morristown, NJ, USA, pp. 31?38. ACL.
Ruppenhofer, J., C. Sporleder, R. Morante, C. Baker, and M. Palmer (2010, July). Semeval-2010 task 10: Linking events and
their participants in discourse. In Proc. of the 5th International Workshop on Semantic Evaluation, Uppsala, Sweden, pp.
45?50. ACL.
Saur??, R. (2008). A factuality profiler for eventualities in text. Ph. D. thesis, Waltham, MA, USA.
Saur??, R. and J. Pustejovsky (2009). FactBank: A corpus annotated with event factuality. Language Resources and Evalua-
tion 43(3), 227?268.
Snow, R., L. Vanderwende, and A. Menezes (2006). Effectively using syntax for recognizing false entailment. In Proc. of HLT
NAACL, Morristown, NJ, USA, pp. 33?40. ACL.
Toprak, C., N. Jakob, and I. Gurevych (2010, July). Sentence and expression level annotation of opinions in user-generated
discourse. In Proc. of the 48th Annual Meeting of the ACL, Uppsala, Sweden, pp. 575?584. ACL.
Vincze, V. (2010, July). Speculation and negation annotation in natural language texts: what the case of bioscope might (not)
reveal. In Proc. of the Workshop on Negation and Speculation in Natural Language Processing, Uppsala, Sweden, pp.
28?31. University of Antwerp.
Vincze, V., G. Szarvas, R. Farkas, G. Mo?ra, and J. Csirik (2008). The BioScope corpus: biomedical texts annotated for
uncertainty, negation and their scopes. BMC Bioinformatics 9((Suppl 11)), S9.
Wilson, T., P. Hoffmann, S. Somasundaran, J. Kessler, J. Wiebe, Y. Choi, C. Cardie, E. Riloff, and S. Patwardhan (2005).
Opinionfinder: a system for subjectivity analysis. In Proc. of HLT/EMNLP on Interactive Demonstrations, Morristown, NJ,
USA, pp. 34?35. ACL.
354
