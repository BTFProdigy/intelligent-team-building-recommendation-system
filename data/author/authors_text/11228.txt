Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 2291?2302, Dublin, Ireland, August 23-29 2014.
Separating Brands from Types: an Investigation of Different Features for
the Food Domain
Michael Wiegand and Dietrich Klakow
Spoken Language Systems
Saarland University
D-66123 Saarbru?cken, Germany
{Michael.Wiegand|Dietrich.Klakow}@lsv.uni-saarland.de
Abstract
We examine the task of separating types from brands in the food domain. Framing the problem
as a ranking task, we convert simple textual features extracted from a domain-specific corpus into
a ranker without the need of labeled training data. Such method should rank brands (e.g. sprite)
higher than types (e.g. lemonade). Apart from that, we also exploit knowledge induced by semi-
supervised graph-based clustering for two different purposes. On the one hand, we produce an
auxiliary categorization of food items according to the Food Guide Pyramid, and assume that a
food item is a type when it belongs to a category unlikely to contain brands. On the other hand,
we directly model the task of brand detection using seeds provided by the output of the textual
ranking features. We also harness Wikipedia articles as an additional knowledge source.
1 Introduction
Brands play a significant role in social life. They are the subject matter of many discussions in social me-
dia. Their automatic detection for information extraction tasks is a pressing problem since, despite their
unique property to refer to commercial products of specific companies, in everyday language they often
occur in similar contexts as common nouns. A typical domain where such behaviour can be observed is
the food domain, where food brands (e.g. nutella or sprite) are often used synonymously with the food
type1 of which the brand is a prototypical instance (e.g. chocolate spread or lemonade). Such usage is
illustrated in (1) and (2).
(1) In the evening, I eat a slice of bread with either nutella or marmalade.
(2) I prepare my pancakes with baking soda, water and a lacing of sprite instead of sugar.
This particular phenomenon of metonymy (Lakoff and Johnson, 1980), commonly referred to as generi-
cized trademarks, of course, has consequences on automatic lexicon induction methods. If one automat-
ically extracts food types, one also obtains food brands.
In this paper, we examine features to detect brands automatically. Solving the issue with the help of
a manually-compiled list of brands neglects parts of the nature of brands. Brands come and go. Some
products may be discontinued after a certain amount of time (e.g. due to limited popularity) while, on the
other hand, new products constantly enter the market. For instance, popular food brands, such as sierra
mist or kazoozles, did not exist a decade ago. Therefore, a list of brands that is manually created today
may not reflect the predominant food brands that will be available in a decade.
The features we introduce to detect brands consider both the intrinsic properties of brands and their
contextual environment. Even though in many contexts, brands are used as ordinary type expressions
(1), there might be specific contexts that are only observed with brands. We also consider distributional
properties: brands may co-occur with other brands. Moreover, they may be biased towards certain
categories, e.g. sweets, beverages etc. For the latter, we actually exploit the usage of food brands to be
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
1We define food type as common nouns that denote a particular type of food, e.g. apple, chocolate, cheese etc.
2291
Method Corpus Corpus Type P@10 P@100 P@500
ranking by frequency chefkoch.de domain specific 0.00 22.00 25.60
induction based on coordination Wikipedia open domain 90.00 60.00 47.80
induction based on coordination chefkoch.de domain specific 100.00 98.00 92.00
Table 1: Precision at rank n (P@n) of different food induction methods.
Label Items Examples
Food Types 1745 apple, baguette, beer, corn flakes, crisps, basmati rice, broccoli, choco-
late spread, gouda, orange juice, pork, potato, steak, sugar
Food Brands 221 activia, babybel, becel, butterfinger, kit kat, nutella, pepsi, philadelphia,
smacks, smarties, sprite, ramazzotti, tuborg, volvic
Table 2: Gold standard of the food vocabulary.
used as genericized trademarks, allowing food categorization methods for types to be easily extended to
brands. Moreover, we examine how external knowledge resources, such as Wikipedia, can be harnessed
as a means to separate brands from types. Our task is lexicon construction rather than contextual entity
classification, that is, we are interested in what a food item generally conveys and not what it conveys in
a specific context.
We consider the food domain as a target domain since there are large, unlabeled domain-specific
corpora available gathered from social media which are vital for the methods we explore. It is also a
domain for which there has already been done research in the area of natural language processing (NLP),
and there are common applications, such as virtual customer advice or product recommendation, that
may exploit such NLP technology.
The methods we consider require no, or hardly any human supervision. Thus, we imagine that they
can also be applied to other domains at a low cost. In particular, other life-style domains, such as fashion,
cosmetics or electronics show parallels, since comparable textual web data from which to extract domain-
specific knowledge are available.
Our experiments are carried out on German data, but our findings should carry over to other languages
since the issues we address are (mostly) language universal. All examples are given as English transla-
tions. We use the term food item to refer to the union of food brands and food types. All food items will
be written in lowercase reflecting the identical case spelling in German, i.e. types and brands are both
written uppercase. In English, both types and brands can be written uppercase or lowercase2, however,
there is a tendency in user-generated content/social media to write mostly lowercase.
2 Motivation & Data
Previous research on lexicon induction proposed a widely applicable method based on coordination
(Hatzivassiloglou and McKeown, 1997; Riloff and Shepherd, 1997; Roark and Charniak, 1998): First,
a set of seed expressions that are typical of the categories one wants to induce are defined. Then, addi-
tional instances of those categories are obtained by extracting conjuncts of the seed expressions (i.e. all
expressions that match <seed> and/or <expression> are extracted as new instances). A detailed study
of such lexicon induction has recently been published by Ziering et al. (2013), who also point out the
great semantic coherence of conjuncts.
This method can also be applied to the food domain. As a domain-specific dataset for all our experi-
ments, we use a crawl of chefkoch.de3 (Wiegand et al., 2012) consisting of 418, 558 webpages of forum
entries. chefkoch.de is the largest German web portal for food-related issues. Table 1 shows the effec-
tiveness of coordination as a means of extracting food items from our domain-specific corpus. Given a
seed set of 10 frequent food items (we use: water, salt, sugar, salad, bread, meat, cake, flour, butter and
2There are plenty of food types that are written uppercase, e.g. Jaffa Cakes, Beef Wellington, BLT, Hoppin? John etc.
3www.chefkoch.de
2292
Properties Type of Property Example Brands Types
nonwords general ebly, sprite, twix 41.63 -NA-
derived from proper noun general cheddar, evian, jim beam 31.22 2.29
foreign words general camembert, merci, wasabi 27.15 12.37
length general average no. of characters 7.97 10.53
word initial plosives stylistic p,t,k,b,d,g (attract attention) 31.22 35.81
assonance stylistic fanta, kiwi (fruit), papaya 11.76 11.06
alliteration stylistic babybel, blueberry, tic tac 6.79 3.78
onomatopoeia stylistic crunchips, popcorn 2.71 0.52
rhyme stylistic jelly belly, hubba bubba 1.35 0.06
Table 3: Comparison of intrinsic properties between brands and types; brands are always underlined; all
numbers (except for length) are the proportion with the respective property.
potato), we compute all conjuncts and rank them according to frequency. We do this on our domain-
specific corpus and on Wikipedia. As a baseline, we simply sort all nouns according to frequency in our
domain-specific corpus. The table shows that ranking by frequency is no effective method. Conjuncts
produce good results provided that they are extracted from a domain-specific corpus.
Even though coordination is a very reliable method to induce food items, it fails to distinguish between
food types and food brands. We produced a labeled food vocabulary to be used for all our subsequent
experiments consisting of food types and food brands (see Table 2). The food types exclusively comprise
the food vocabulary from Wiegand et al. (2014). The food brands were manually selected with the help
of the web. We only include food items that occur at least 5 times in our corpus. In our food vocabulary,
87% of our food brands occur as a conjunct of a food type. Therefore, the problem of confusing brands
with types is inherent to induction based on coordination.
3 Intrinsic Properties
Table 3 provides some statistics on intrinsic properties of our food items giving some indication which
feature types might be used for this task. We also include some stylistic properties of brands that have
been addressed in previous marketing research and applied psychology. We focus on fairly straightfor-
ward features from desirable brand name characteristics (Robertson, 1989), since we assume that there
is more general agreement on the underlying concepts than there is on the concepts underlying complex
sound symbolism (Klink, 2000; Yorkston and Menon, 2004). For the statistics in Table 3, most proper-
ties (i.e. all except length and word-initial plosives) have been detected manually. The reason for this is
that their automatic detection is not trivial (e.g. there is no established algorithm to detect onomatopoeia;
even the detection of rhyme or assonance is not straightforward given the low grapheme-phoneme corre-
spondence of English). We did not want the statistics for this exploratory experiment to be distorted by
error-proneness of the detection methods.
Table 3 shows that a large part of brands are nonwords indicating that this task is hard to be solved with
intrinsic features only. Since there is a high number of brands that are derived from some existing proper
noun being either a person or a location, named-entity recognition might be applied to this task. Many
brands are also foreign words. Unfortunately, applying language checking software on our food items
turned out to perform poorly. (These tools are only effective on longer texts, e.g. sentences or entire
documents, and do not work on isolated words, as in our problem setting.) We also noticed a difference
in average word length between brands and types which is consistent with Robertson (1989) who claims
that brand names should be simple. Most stylistic features seem to be less relevant to our task as they
are either too infrequent or not discriminative. Therefore, we do not consider them as features for the
detection of brands in our forthcoming experiments.
2293
Figure 1: Processing Pipeline for ranking.
Feature Type Features
ranking feature LENGTH, COMMERCE, NER
target
, NER
context
, DIVERS, PAT
mod
, PAT
pp
reset feature GRAPH
pyramid
bootstrapping feature GRAPH
brand
, WIKI, VSM
Table 4: Feature classification.
4 Method
Our aim is to determine predictive features for the detection of brands. Rather than employing some
supervised learner that requires manually labeled training data, we want to convert these features directly
into a classifier without costly labeled data. We conceive this task as a ranking task. The reason for using
a ranking is that our features can be translated into a ranking score in a very straightforward manner.
For the evaluation, we do not have to determine some empirical threshold separating the category brand
from the category type. Instead, the evaluation measures we employ for ranking implicitly assume highly
ranked instances as brands and instances ranked at the bottom as types.
For the ranking task, we employ the processing pipeline as illustrated in Figure 1. Most of our features
are designed in such a way that they assign a ranking score to each of our food items by counting how
often a feature is observed with a food item; that is why we call these features ranking features. The
resulting ranking should assign high scores to food brands and low scores to food types. If we want to
combine several features into one ranking, we simply average for each food item the different ranking
scores of the individual ranking features. This is possible since they have the same range [0; 1]. We
obtain such range by normalizing the number of occurrences of a feature with a particular food item by
the total number of occurrences of that food item. The combination by averaging is unbiased as it treats
all features equally.
We also introduce a reset feature which is applied on top of an existing ranking provided by ranking
features. A reset feature is a negative feature in the sense that it is usually a reliable cue that a food item
is not a brand. If it fires for a particular food item, then its ranking score is reset to 0.
Finally, we add bootstrapping features. These features produce an output similar to the ranking fea-
tures (i.e. another ranking). However, unlike the ranking features, the bootstrapping features produce
their output based on a weakly-supervised method which requires some labeled input. Rather than manu-
ally providing that input, we derive it from the combined output that is provided by the ranking and reset
features. We restrict ourselves to instances with a high-confidence prediction, which translates to the top
and bottom end of a ranking. (Since the instances are not manually labeled, of course, not every label
assignment will be correct. We hope, however, that by restricting to instances with a high-confidence
prediction, we can reduce the amount of errors to a minimum.) The output of a bootstrapping feature is
combined with the set of ranking features to a new ranking onto which again a reset feature is applied.
Table 4 shows which feature (each will be discussed below) belongs to which of the above feature
types (i.e. ranking, reset or bootstrapping features). Most features (i.e. all except WIKI) are extracted
from our domain-specific corpus introduced in ?2.
2294
4.1 Length
Since we established that brands tend to be shorter than types (?3), we add one feature that ranks each
food item according to its number of characters.
4.2 Target Named-Entity Recognition (NER
target
)
Brands can be considered a special kind of named entities. We apply a part-of-speech tagger to count
how often a food item has been tagged as a proper noun. We decided against a named-entity recognizer
as it usually only recognizes persons, locations and organizations, while part-of-speech taggers employ
a general tag for all proper nouns (that may go well beyond the three afore-mentioned common types).
We use a statistical tagger, i.e. TreeTagger (Schmid, 1994), that also employs features below the word
level. As many of our food items will be unknown words, a character-level analysis may still be able to
make useful predictions.
4.3 Contextual Named-Entity Recognition (NER
context
)
We also count the number of other named entities that co-occur with the target food brand within the
same sentence. We are only interested in organizations; an organization co-occurring with a brand is
likely to be the company producing that brand (e.g. He loves Kellogg?s
company
frosties
brand
.) For this
feature, we rely on the output of a named-entity recognizer for German (Chrupa?a and Klakow, 2010).
4.4 Diversification (DIVERS)
Once a product has established itself on the market for a substantial amount of time, many companies
introduce variants of their brand to further consolidate their market position. The purpose of this diversi-
fication is to appeal to customers with special needs. A typical variant of food brands are light products.
In many cases, the names of variants consist of the name of the original brand with some prefix or suffix
indicating the particular type of variant (e.g. mini babybel or philadelphia light). We manually compiled
11 affixes and check for each food item how often it is accompanied by one of them.
4.5 Commerce Cues (COMMERCE)
Presumably, brands are more likely to be mentioned in the context of commercial transaction events than
types. Therefore, we created a list of words that indicate these types of events. The list was created
ad hoc. We used external resources, such as FrameNet (Baker et al., 1998) or GermaNet (Hamp and
Feldweg, 1997) (the German version of WordNet (Miller et al., 1990)), and made no attempt to tune that
list to our domain-specific food corpus. The final list (85 cues in total) comprises: verbs (and deverbal
nouns) that convey the event of a commercial transaction (e.g. buy, purchase or sell), persons involved in
a commercial transaction (e.g. customer or shop assistant), means of purchase (e.g. money, credit card
or bill), places of purchase (e.g. supermarket or shop) and judgment of price (e.g. cheap or expensive).
4.6 Food Modifier (PAT
mod
)
Even though many mentions of brands are similar to those of types, there exist some particular contexts
that are mostly observed with brands. If the food item to be classified often occurs as a modifier of
another food item, then the target item is likely to be some brand. This is due to the fact that many
brands are often mentioned in combination with the food type that they represent, e.g. volvic mineral
water, nutella chocolate spread.
4.7 Prepositional Phrase Embedding (PAT
pp
)
Instead of appearing as a modifier (?4.6), a brand may also be embedded in some prepositional phrase
that has a similar meaning, e.g. We only buy the chocolate spread [by nutella]
PP
.
4.8 Graph-based Methods (GRAPH)
We also employ some semi-supervised graph clustering method in order to assign semantic types to food
items as introduced in Wiegand et al. (2014). The underlying data structure is a food graph that is gener-
ated automatically from our domain-specific corpus where nodes represent food items and edge weights
2295
Category Description General Brands
MEAT meat and fish (products) 19.48 1.31
BEVERAGE beverages (incl. alcoholic drinks) 17.19 23.96
SWEET sweets, pastries and snack mixes 14.90 25.60
SPICE spices and sauces 10.53 2.42
VEGE vegetables (incl. salads) 10.38 0.00
STARCH starch-based side dishes 9.21 4.42
MILK milk products 6.71 23.48
FRUIT fruits 4.48 1.14
GRAIN grains, nuts and seeds 3.41 0.00
FAT fat 2.54 20.00
EGG eggs 0.92 0.00
Table 5: Proportion of categories in the entire food vocabulary (General) and among brands (Brands).
represent the similarity between different items. The weights are computed based on the frequency of
co-occurrence within a similarity pattern (e.g. X instead of Y). Food items that cluster with each other
in such a graph (i.e. food items that often co-occur in a similarity pattern) are most likely to belong to
the same class. For the detection of brands, we examine two different types of food categorization. We
always use the same clustering method (Wiegand et al., 2014) and the same graph. Depending on the
specific type of categorization, we only change the seeds to fit the categories to be induced.
4.8.1 Categories of the Food Guide Pyramid (GRAPH
pyramid
)
The first categorization we consider is the categorization of food items according to the Food Guide
Pyramid (U.S. Department of Agriculture, 1992) as examined in Wiegand et al. (2014). We observed
that food brands are not equally distributed throughout the entire range of food items. There is a notable
bias of food brands towards beverages (mostly soft drinks and alcoholic drinks), sweets, snack mixes,
dairy products and fat. Other categories, e.g. nuts, vegetables or meat, hardly contain brands.4 The
category inventory and the proportion among types and brands are displayed in Table 5.
We use the category information as a negative feature, that is, we re-set the ranking score to 0 if the
category of the food item is either MEAT, SPICE, VEGE, STARCH, FRUIT, GRAIN or EGG. In order
to obtain a category assignment to our food vocabulary, we re-run the best configuration from Wiegand et
al. (2014) including the choice of category seeds. We just extend the graph that formerly only contained
food types by nodes representing brands. We use no manually-compiled knowledge regarding food
brands. Even though the seed food items are exclusively food types, we hope to be also able to make
inferences regarding food brands. This is illustrated in Figure 2(a): The brand mars can be grouped with
food types that are sweets, therefore, we conclude that mars is also some sweet. (Brands can be grouped
with food types of their food category, since food brands are often used as if they were types (?1)). Since
sweets are plausible candidates for brands (Table 5), mars is likely to be some brand.
We think that such bias of brands towards certain subcategories is also present in other domains. For
example, in the electronic domain laptops will have a much larger variety of brands than network cables.
Similarly, in the fashion domain there exist much more shoe brands than sock brands.
4.8.2 Direct Graph Clustering Separating Brands from Types (GRAPH
brand
)
We also apply graph clustering directly for the separation of brands from types, i.e. we assign some
brand and type seeds and then run graph-based clustering (Figure 2(b)). In order to combine the output
of this clustering with that of the previous methods, we interpret the confidence of the output as a ranking
score. As we pursue an unsupervised approach, we do not manually label the seeds but rely on the output
of a ranker using a combination of above features (Figure 1). Instances at the top of the ranking are
considered brand seeds, while instances at the bottom are considered type seeds.
4There may be companies which, among other things, also sell these food types, but we do not want to extract the names of
organizations (as in traditional named-entity recognition), e.g. Kraft Foods, but specific product names, e.g. philadelphia.
2296
(a) food type categorization (b) brand detection
Figure 2: Similarity graphs; bold items are seeds; line width of edges represents strength of similarity.
4.9 Wikipedia Bootstrapping (WIKI)
For many information extraction tasks, the usage of collaboratively-edited resources is increasingly be-
coming popular. One of the largest resources of that type is Wikipedia. For our vocabulary of food items,
we could match 57% of the food brands and 53% of the food types with a Wikipedia article.
Even though Wikipedia may hold some useful information for the detection of brands, this information
is not readily available in a structured format, such as infoboxes. This is illustrated by (3)-(5) which
display the first sentence of three Wikipedia articles, where (3) and (4) are food brands and (5) is a food
type. There is some thematic overlap across the two categories (e.g. (4) and (5) describe the ingredients
of the food item). However, if one also considers the entire articles, some notable topical differences
between brands and types become obvious. The articles of food brands typically focus on commercial
aspects (i.e. market situation and product history) while articles of food types describe the actual food
item (e.g. by distinguishing it from other food items or naming its origin). Therefore, a binary topic
classification based on the entire document should be a suitable approach. In the light of the diversified
language employed for articles on brands (cp. (3)-(4)), we consider a bag-of-words classifier more
effective than applying some textual patterns on those texts.
(3) BRAND: Twix is a chocolate bar made by Mars, Inc.
(4) BRAND: Smarties is a brand under which Nestle? produces colour-varied sugar-coated chocolate
lentils.
(5) TYPE: Milk chocolate is a type of chocolate made from cocoa produce (cocoa bean, cocoa butter),
sugar, milk or dairy products.
Similar to GRAPH
brand
(?4.8.2), we harness Wikipedia via a bootstrapping method. We generate a
labeled training set of Wikipedia articles representing brands and types using the combined output of
the ranking features (+ reset feature). We then train a supervised classifier on these data and classify all
articles representing food items of our food vocabulary. We use the output score of the classifier for the
article of each food item (which amounts to some confidence score) and thus obtain a ranking score. For
those food items for which no Wikipedia entry exists, we produce a score of 0.
4.10 Vector Space Model (VSM)
While GRAPH
brand
(?4.8.2) determines similar food items by means of highly weighted edges in a sim-
ilarity graph (that represent the frequency of co-occurrences with a similarity pattern), we also examine
whether distributional similarity can be harnessed for the same purpose. We represent each food item
as a vector, where the vector components encode the frequency of words that co-occur with mentions of
the food item in a fixed window of 5 words (in our domain-specific corpus). Similar to GRAPH
brand
(?4.8.2) and WIKI (?4.9), we consider the n highest and m lowest ranked food items provided by rank-
ing features (+ reset feature) as labeled brand and type instances for a supervised classifier. For testing,
we apply this classifier on each food item in our vocabulary, or more precisely, its vector representation.
Thus we obtain another ranking score (again, the output amounts to some confidence score).
2297
Plain +Graph
pyramid
(reset feature)
Feature P@10 P@50 P@100 P@200 AP P@10 P@50 P@100 P@200 AP
RANDOM 10.00 18.00 14.00 14.00 0.119 20.00 22.00 22.00 21.50 0.167
LENGTH 10.00 20.00 22.00 21.50 0.163 10.00 32.00 41.00 40.00 0.230
DIVERS 60.00 46.00 37.00 25.00 0.207 60.00 50.00 39.00 30.50 0.240
COMMERCE 30.00 28.00 31.00 27.00 0.220 40.00 38.00 39.00 35.00 0.294
NER
context
70.00 72.00 52.00 43.50 0.401 80.00 72.00 51.00 46.50 0.425
PAT
pp
90.00 78.00 64.00 50.00 0.439 100.00 78.00 69.00 53.00 0.476
PAT
mod
60.00 68.00 69.00 58.00 0.460 90.00 76.00 76.00 58.00 0.507
NER
target
80.00 70.00 60.00 52.50 0.479 80.00 78.00 72.00 61.50 0.525
combined 100.00 88.00 66.00 59.00 0.612 100.00 86.00 76.00 62.50 0.626
Table 6: Precision at rank n (P@n) and average precision (AP) of the different ranking features.
Partition Prec Rec F
Food Types 70.49 72.82 71.04
Food Brands 69.09 66.21 64.93
Table 7: Performance of food categorization according to the Food Guide Pyramid (auxiliary classifica-
tion).
5 Experiments
In the following experiments, we mostly evaluate rankings. For that we employ precision at rank n and
average precision. The former computes precision at a predefined rank n, whereas the latter provides
an average of the precisions measured at every possible rank. While average precision provides a score
that evaluates the ranking as a whole, precision at rank n typically focuses on the correctness of higher
ranks.5
5.1 Evaluation of Ranking Features
Table 6 (left half) displays the results of the individual and combined ranking features. As a trivial base-
line, we also include RANDOM which is randomized ranking of the food items. The table shows that all
features except LENGTH produce a notably better ranking than RANDOM. Following the inspection of
intrinsic properties of brands in ?3, it does not come as a surprise that NER
target
is the strongest feature.
However, also the contextual features NER
context
, PAT
pp
and PAT
mod
produce reasonable results. If we
combine all features (except the poorly performing LENGTH), we obtain a notable improvement over
NER
target
which proves that those different features are complementary to a certain extent.
5.2 Evaluation of the Reset Feature
In Table 7, we examine the food categorization according to the Food Guide Pyramid as such. For
this evaluation, we partition the output of automatic categorization into (actual) types and brands. Thus
we can compare the performance between those two different types of food items, and can quantify
the loss on the categorization on brands against the categorization on types. (Due to the fact that the
seeds exclusively comprise types, we must assume that performance on brands will be lower.)6 Even
though there is a slight loss on brands (mostly recall), we still consider this categorization useful for our
purposes.
5The manually labeled food vocabulary is available at:
www.lsv.uni-saarland.de/personalPages/michael/relFood.html
6Since the categories to indicate unlikely brands (?4.8.1) are extremely sparse (Table 5), we conflate them for this evaluation
as one large category NEGATIVE. Because of this and due to the fact that the food type vocabulary is slightly smaller than
the one used in Wiegand et al. (2014) (since we only consider food items mentioned at least 5 times in our corpus (?2)), the
performance scores of food categorization in Table 7 and the one reported in Wiegand et al. (2014) differ accordingly.
2298
Classifier Acc Prec Rec F
Baselines
Majority-Class Classifier 88.76 44.38 50.00 47.02
seeds only: 50 top+150 bottom 9.51 91.00 13.85 23.47
seeds only: 100 top+300 bottom 18.57 86.17 25.48 37.81
Bootstrap. Features
WIKI (seeds: 50 top+150 bottom) 43.95 87.68 43.33 57.91
VSM (seeds: 100 top+300 bottom) 77.87 64.93 81.61 66.39
GRAPH
brand
(seeds: 100 top+300 bottom) 82.91 81.36 67.27 73.53
Table 8: Bootstrapping features in isolation compared with baselines (i.e. reference classifiers).
Table 6 (right half) shows the performance of the corresponding reset feature on the brand detection
task. We observe a systematic increase in performance when added on top of the ranking features.
5.3 Evaluation of Bootstrapping Features
Table 9 displays the performance of the bootstrapping features. For the labeled training data, we empir-
ically determined the optimal class ratio (1:3) and the optimal number of seeds (the top 100 and bottom
300 items for VSM and GRAPH
brand
, and top 50 and bottom 150 items for WIKI). As a supervised
classifier for VSM and WIKI, we chose Support Vector Machines using SVMlight (Joachims, 1999).
The table shows that only GRAPH
brand
and WIKI improve the ranking, whereas WIKI is notably
stronger. These results suggest that Wikipedia is a good resource from which to learn whether a food
item is a brand or not. However, this task could not be completely solved byWIKI since not all food items
are covered by Wikipedia (?4.9). To further prove this, we also evaluate an upper bound of Wikipedia,
WIKI
oracle
(exclusively using that resource), in which we pretend to correctly interpret every Wikipedia
page as an article for either a food brand or a food type. We rank all brands having a Wikipedia article
highest. They are followed by those food items having no article (ordered randomly) and, finally, by the
food types having a Wikipedia article. Table 9 shows that we are able to outperform WIKI
oracle
.
Our pipeline (Figure 1) applies the reset feature at two stages. We also examine whether it is necessary
to apply that feature for a second time. Presumably, the bootstrapping feature is so effective that we do
not have to apply further type filtering. After all, the reset feature will also downweight some correct
food items (Table 5). Table 9 confirms that when the reset feature is applied only once, we obtain a better
performance (according to average precision) for all bootstrapping features (even for VSM).
Finally, Table 8 evaluates the bootstrapping features in isolation. Since, unlike the ranking features,
the bootstrapping features provide a definite classification for each food item (in addition to a prediction
score evaluated as a ranking score), we consider the output for a binary classification task. In this setting,
we make use of the four evaluation measures accuracy, precision, recall and F-score. For the last three
measures, we always compute the macro average score.
As a baseline, we also include a majority-class classifier that always predicts the class food type.
Interestingly, in terms of F-score, GRAPH
brand
is the best method rather than WIKI, i.e. the best method
from the previous evaluation in Table 9. The reason for this is that we evaluate in isolation rather than in
combination with other features (i.e. parts of the additional benefit included in GRAPH
brand
may already
be contained in ranking and reset features). Secondly, in a ranking task (Table 9), good performance is
usually achieved by classifiers biased towards a high precision. Indeed, the best ranker in Table 9, i.e.
WIKI, achieves the highest precision in Table 8.
6 Related Work
Ling and Weld (2012) examine named-entity recognition on data that also include brands, however, the
class of brands is not explicitly discussed. Putthividhya and Hu (2011) explore brands in the context
of product attribute extraction. Entities are extracted from eBay?s clothing and shoe category. Nadeau
et al. (2006) explicitly generate gazetteers of car brands obtained from corresponding websites. Those
textual data are very restrictive in that they do not represent sentences but category listings or tables. In
this paper, we consider as textual source a more general text type, i.e. forum entries, that comprise full
2299
-2nd reset
Feature P@200 AP P@200 AP
WIKI
oracle
66.00 0.429 -N/A- -N/A-
ranking+GRAPH
pyramid
62.50 0.626 -N/A- -N/A-
ranking+GRAPH
pyramid
+VSM 60.00 0.619 63.00 0.661
ranking+GRAPH
pyramid
+GRAPH
brand
67.50 0.638 65.50 0.662
ranking+GRAPH
pyramid
+WIKI 70.00 0.688 73.00 0.718
Table 9: Impact of bootstrapping; -2nd reset: does not apply reset feature for a second time (Figure 1).
sentences. Previous work also focuses on traditional (semi-)supervised algorithms. Hence, there are only
few additional insights as to the specific properties of brand names. Min and Park (2012) examine the
aspect of product instance distinction on the use case of product reviews on jeans from Amazon. Their
work focuses on temporal features to identify distinct product instances (these may also include brand
names).
The food domain has also recently received some attention. Different types of classification have
been explored including ontology mapping (van Hage et al., 2005), part-whole relations (van Hage et
al., 2006), recipe attributes (Druck, 2013), dish detection and the categorization of food types according
to the Food Guide Pyramid (Wiegand et al., 2014). Relation extraction tasks have also been examined.
While a strong focus is on food-health relations (Yang et al., 2011; Miao et al., 2012; Kang et al., 2013;
Wiegand and Klakow, 2013), relations relevant to customer advice have also been addressed (Wiegand
et al., 2012; Wiegand et al., 2014). Beyond that, Chahuneau et al. (2012) relate sentiment information to
food prices with the help of a large corpus consisting of restaurant menus and reviews. Druck and Pang
(2012) extract actionable recipe refinements. To the best of our knowledge, we present the first work that
explicitly addresses the detection of brands in the food domain. While brands as such present an addi-
tional dimension to previously examined types of categorization, we also show that the categorization
according to the Food Guide Pyramid helps to decide whether a food item is a brand or not.
7 Conclusion
We examined the task of separating types from brands in the food domain. Framing the problem as a
ranking task, we directly converted predictive features extracted from a domain-specific corpus into a
ranker without the need of labeled training data. Apart from those ranking features, we also exploited
knowledge induced by semi-supervised graph-based clustering for two different purposes. On the one
hand, we produced an auxiliary categorization of food items according to the Food Guide Pyramid, and
assumed that a food item is a type when it belongs to a category that is unlikely to contain brands. On
the other hand, we directly modelled the task of brand detection by using seeds provided by the output
of the textual ranking features. We also learned additional high-precision knowledge from Wikipedia
webpages using a similar bootstrapping scheme.
Acknowledgements
This work was performed in the context of the Software-Cluster project SINNODIUM. Michael Wie-
gand was funded by the German Federal Ministry of Education and Research (BMBF) under grant no.
01IC10S01. The authors would like to thank Melanie Reiplinger for proofreading the paper.
References
Collin F. Baker, Charles J. Fillmore, and John B. Lowe. 1998. The Berkeley FrameNet Project. In Proceed-
ings of the International Conference on Computational Linguistics and Annual Meeting of the Association for
Computational Linguistics (COLING/ACL), pages 86?90, Montre?al, Quebec, Canada.
Victor Chahuneau, Kevin Gimpel, Bryan R. Routledge, Lily Scherlis, and Noah A. Smith. 2012. Word Salad:
Relating Food Prices and Descriptions. In Proceedings of the Joint Conference on Empirical Methods in Natural
2300
Language Processing and Computational Natural Language Learning (EMNLP/CoNLL), pages 1357?1367,
Jeju Island, Korea.
Grzegorz Chrupa?a and Dietrich Klakow. 2010. A Named Entity Labeler for German: Exploiting Wikipedia and
Distributional Clusters. In Proceedings of the Conference on Language Resources and Evaluation (LREC),
pages 552?556, La Valletta, Malta.
Gregory Druck and Bo Pang. 2012. Spice it up? Mining Refinements to Online Instructions from User Generated
Content. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL), pages
545?553, Jeju, Republic of Korea.
Gregory Druck. 2013. Recipe Attribute Detection Using Review Text as Supervision. In Proceedings of the
IJCAI-Workshop on Cooking with Computers (CWC), Beijing, China.
Birgit Hamp and Helmut Feldweg. 1997. GermaNet - a Lexical-Semantic Net for German. In Proceedings of ACL
workshop Automatic Information Extraction and Building of Lexical Semantic Resources for NLP Applications,
pages 9?15, Madrid, Spain.
Vasileios Hatzivassiloglou and Kathleen R. McKeown. 1997. Predicting the Semantic Orientation of Adjec-
tives. In Proceedings of the Conference on European Chapter of the Association for Computational Linguistics
(EACL), pages 174?181, Madrid, Spain.
Thorsten Joachims. 1999. Making Large-Scale SVM Learning Practical. In B. Scho?lkopf, C. Burges, and
A. Smola, editors, Advances in Kernel Methods - Support Vector Learning, pages 169?184. MIT Press.
Jun Seok Kang, Polina Kuznetsova, Michael Luca, and Yejin Choi. 2013. Where Not to Eat? Improving Public
Policy by Predicting Hygiene Inspections Using Online Reviews. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing (EMNLP), pages 1443?1448, Seattle, WA, USA.
Richard R. Klink. 2000. Creating Brand Names with Meaning: The Use of Sound Symbolism. Marketing Letters,
11(1):5?20.
George Lakoff and Mark Johnson. 1980. Metaphors We Live By. University of Chicago Press.
Xiao Ling and Daniel S. Weld. 2012. Fine-Grained Entity Recognition. In Proceedings of the National Conference
on Artificial Intelligence (AAAI), pages 94?100, Toronto, Canada.
Qingliang Miao, Shu Zhang, Bo Zhang, Yao Meng, and Hao Yu. 2012. Extracting and Visualizing Semantic
Relationships from Chinese Biomedical Text. In Proceedings of the Pacific Asia Conference on Language,
Information and Compuation (PACLIC), pages 99?107, Bali, Indonesia.
George Miller, Richard Beckwith, Christiane Fellbaum, Derek Gross, and Katherine Miller. 1990. Introduction to
WordNet: An On-line Lexical Database. International Journal of Lexicography, 3:235?244.
Hye-Jin Min and Jong C. Park. 2012. Product Name Classification for Product Instance Distinction. In Proceed-
ings of the Pacific Asia Conference on Language, Information and Compuation (PACLIC), pages 289?298, Bali,
Indonesia.
David Nadeau, Peter D. Turney, and Stan Matwin. 2006. Unsupervised Named-Entity Recognition: Generating
Gazetteers and Resolving Ambiguity. In Proceedings of the Canadian Conference on Artificial Intelligence,
pages 266?277, Que?bec City, Que?bec, Canada.
Duangmanee Putthividhya and Junling Hu. 2011. Bootstrapped Named Entity Recognition for Product Attribute
Extraction. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),
pages 1557?1567, Edinburgh, Scotland, UK.
Ellen Riloff and Jessica Shepherd. 1997. A Corpus-Based Approach for Building Semantic Lexicons. In Pro-
ceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 117?124,
Providence, RI, USA.
Brian Roark and Eugene Charniak. 1998. Noun-phrase co-occurrence statistics for semi-automatic semantic
lexicon construction. In Proceedings of the International Conference on Computational Linguistics (COLING),
pages 1110?1116, Montreal, Quebec, Canada.
Kim Robertson. 1989. Strategically Desirable Brand Name Characteristics. Journal of Comsumer Marketing,
6(4):61?71.
2301
Helmut Schmid. 1994. Probabilistic part-of-speech tagging using decision trees. In Proceedings of the Interna-
tional Conference on New Methods in Language Processing, pages 44?49, Manchester, United Kingdom.
Human Nutrition Information Service U.S. Department of Agriculture. 1992. The Food Guide Pyramid. Home
and Garden Bulletin 252, Washington, D.C., USA.
Willem Robert van Hage, Sophia Katrenko, and Guus Schreiber. 2005. A Method to Combine Linguistic
Ontology-Mapping Techniques. In Proceedings of International Semantic Web Conference (ISWC), pages 732
? 744, Galway, Ireland. Springer.
Willem Robert van Hage, Hap Kolb, and Guus Schreiber. 2006. A Method for Learning Part-Whole Relations. In
Proceedings of International Semantic Web Conference (ISWC), pages 723 ? 735, Athens, GA, USA. Springer.
Michael Wiegand and Dietrich Klakow. 2013. Towards Contextual Healthiness Classification of Food Items ? A
Linguistic Approach. In Proceedings of the International Joint Conference on Natural Language Processing
(IJCNLP), pages 19?27, Nagoya, Japan.
Michael Wiegand, Benjamin Roth, and Dietrich Klakow. 2012. Web-based Relation Extraction for the Food
Domain. In Proceedings of the International Conference on Applications of Natural Language Processing to
Information Systems (NLDB), pages 222?227, Groningen, the Netherlands. Springer.
Michael Wiegand, Benjamin Roth, and Dietrich Klakow. 2014. Automatic Food Categorization from Large
Unlabeled Corpora and Its Impact on Relation Extraction. In Proceedings of the Conference on European
Chapter of the Association for Computational Linguistics (EACL), pages 673?682, Gothenburg, Sweden.
Hui Yang, Rajesh Swaminathan, Abhishek Sharma, Vilas Ketkar, and Jason D?Silva, 2011. Learning Structure and
Schemas from Documents, volume 375 of Studies in Computational Intelligence, chapter Mining Biomedical
Text Towards Building a Quantitative Food-disease-geneNetwork, pages 205?225. Springer Berlin Heidelberg.
Eric Yorkston and Geeta Menon. 2004. A Sound Idea: Phonetic Effects of Brand Names on Consumer Judgments.
Journal of Consumer Research, 31:43?51.
Patrick Ziering, Lonneke van der Plas, and Hinrich Schuetze. 2013. Bootstrapping Semantic Lexicons for Techni-
cal Domains. In Proceedings of the International Joint Conference on Natural Language Processing (IJCNLP),
Nagoya, Japan, 1321?1329.
2302
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 325?335,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Generalization Methods for In-Domain and Cross-Domain Opinion
Holder Extraction
Michael Wiegand and Dietrich Klakow
Spoken Language Systems
Saarland University
D-66123 Saarbru?cken, Germany
{Michael.Wiegand|Dietrich.Klakow}@lsv.uni-saarland.de
Abstract
In this paper, we compare three different
generalization methods for in-domain and
cross-domain opinion holder extraction be-
ing simple unsupervised word clustering,
an induction method inspired by distant
supervision and the usage of lexical re-
sources. The generalization methods are
incorporated into diverse classifiers. We
show that generalization causes significant
improvements and that the impact of im-
provement depends on the type of classifier
and on how much training and test data dif-
fer from each other. We also address the
less common case of opinion holders being
realized in patient position and suggest ap-
proaches including a novel (linguistically-
informed) extraction method how to detect
those opinion holders without labeled train-
ing data as standard datasets contain too
few instances of this type.
1 Introduction
Opinion holder extraction is one of the most im-
portant subtasks in sentiment analysis. The ex-
traction of sources of opinions is an essential com-
ponent for complex real-life applications, such
as opinion question answering systems or opin-
ion summarization systems (Stoyanov and Cardie,
2011). Common approaches designed to extract
opinion holders are based on data-driven methods,
in particular supervised learning.
In this paper, we examine the role of general-
ization for opinion holder extraction in both in-
domain and cross-domain classification. General-
ization may not only help to compensate the avail-
ability of labeled training data but also conciliate
domain mismatches.
In order to illustrate this, compare for instance
(1) and (2).
(1) Malaysia did not agree to such treatment of Al-Qaeda sol-
diers as they were prisoners-of-war and should be accorded
treatment as provided for under the Geneva Convention.
(2) Japan wishes to build a $21 billion per year aerospace indus-
try centered on commercial satellite development.
Though both sentences contain an opinion
holder, the lexical items vary considerably. How-
ever, if the two sentences are compared on the ba-
sis of some higher level patterns, some similari-
ties become obvious. In both cases the opinion
holder is an entity denoting a person and this en-
tity is an agent1 of some predictive predicate (i.e.
agree in (1) and wishes in (2)), more specifically,
an expression that indicates that the agent utters a
subjective statement. Generalization methods ide-
ally capture these patterns, for instance, they may
provide a domain-independent lexicon for those
predicates. In some cases, even higher order fea-
tures, such as certain syntactic constructions may
vary throughout the different domains. In (1) and
(2), the opinion holders are agents of a predictive
predicate, whereas the opinion holder her daugh-
ters in (3) is a patient2 of embarrasses.
(3) Mrs. Bennet does what she can to get Jane and Bingley to-
gether and embarrasses her daughters by doing so.
If only sentences, such as (1) and (2), occur in
the training data, a classifier will not correctly ex-
tract the opinion holder in (3), unless it obtains
additional knowledge as to which predicates take
opinion holders as patients.
1By agent we always mean constituents being labeled as
A0 in PropBank (Kingsbury and Palmer, 2002).
2By patient we always mean constituents being labeled
as A1 in PropBank.
325
In this work, we will consider three differ-
ent generalization methods being simple unsuper-
vised word clustering, an induction method and
the usage of lexical resources. We show that gen-
eralization causes significant improvements and
that the impact of improvement depends on how
much training and test data differ from each other.
We also address the issue of opinion holders in
patient position and present methods including a
novel extraction method to detect these opinion
holders without any labeled training data as stan-
dard datasets contain too few instances of them.
In the context of generalization it is also impor-
tant to consider different classification methods
as the incorporation of generalization may have a
varying impact depending on how robust the clas-
sifier is by itself, i.e. how well it generalizes even
with a standard feature set. We compare two state-
of-the-art learning methods, conditional random
fields and convolution kernels, and a rule-based
method.
2 Data
As a labeled dataset we mainly use the MPQA
2.0 corpus (Wiebe et al 2005). We adhere to
the definition of opinion holders from previous
work (Wiegand and Klakow, 2010; Wiegand and
Klakow, 2011a; Wiegand and Klakow, 2011b),
i.e. every source of a private state or a subjective
speech event (Wiebe et al 2005) is considered an
opinion holder.
This corpus contains almost exclusively news
texts. In order to divide it into different domains,
we use the topic labels from (Stoyanov et al
2004). By inspecting those topics, we found that
many of them can grouped to a cluster of news
items discussing human rights issues mostly in
the context of combating global terrorism. This
means that there is little point in considering every
single topic as a distinct (sub)domain and, there-
fore, we consider this cluster as one single domain
ETHICS.3 For our cross-domain evaluation, we
want to have another topic that is fairly different
from this set of documents. By visual inspection,
we found that the topic discussing issues regard-
ing the International Space Station would suit our
purpose. It is henceforth called SPACE.
3The cluster is the union of documents with the following
MPQA-topic labels: axisofevil, guantanamo, humanrights,
mugabe and settlements.
Domain # Sentences # Holders in sentence (average)
ETHICS 5700 0.79
SPACE 628 0.28
FICTION 614 1.49
Table 1: Statistics of the different domain corpora.
In addition to these two (sub)domains, we
chose some text type that is not even news text
in order to have a very distant domain. There-
fore, we had to use some text not included in the
MPQA corpus. Existing text collections contain-
ing product reviews (Kessler et al 2010; Toprak
et al 2010), which are generally a popular re-
source for sentiment analysis, were not found
suitable as they only contain few distinct opinion
holders. We finally used a few summaries of fic-
tional work (two Shakespeare plays and one novel
by Jane Austen4) since their language is notably
different from that of news texts and they con-
tain a large number of different opinion holders
(therefore opinion holder extraction is a meaning-
ful task on this text type). These texts make up
our third domain FICTION. We manually labeled
it with opinion holder information by applying the
annotation scheme of the MPQA corpus.
Table 1 lists the properties of the different do-
main corpora. Note that ETHICS is the largest do-
main. We consider it our primary (source) domain
as it serves both as a training and (in-domain) test
set. Due to their size, the other domains only
serve as test sets (target domains).
For some of our generalization methods, we
also need a large unlabeled corpus. We use the
North American News Text Corpus (LDC95T21).
3 The Different Types of Generalization
3.1 Word Clustering (Clus)
The simplest generalization method that is con-
sidered in this paper is word clustering. By that,
we understand the automatic grouping of words
occurring in similar contexts. Such clusters are
usually computed on a large unlabeled corpus.
Unlike lexical features, features based on clusters
are less sparse and have been proven to signif-
icantly improve data-driven classifiers in related
tasks, such as named-entity recognition (Turian et
4available at: www.absoluteshakespeare.com/
guides/{othello|twelfth night}/summary/
{othello|twelfth night} summary.htm
www.wikisummaries.org/Pride and Prejudice
326
I. Madrid, Dresden, Bordeaux, Istanbul, Caracas, Manila, ...
II. Toby, Betsy, Michele, Tim, Jean-Marie, Rory, Andrew, ...
III. detest, resent, imply, liken, indicate, suggest, owe, expect, ...
IV. disappointment, unease, nervousness, dismay, optimism, ...
V. remark, baby, book, saint, manhole, maxim, coin, batter, ...
Table 2: Some automatically induced clusters.
ETHICS SPACE FICTION
1.47 2.70 11.59
Table 3: Percentage of opinion holders as patients.
al., 2010). Such a generalization is, in particular,
attractive as it is cheaply produced. As a state-
of-the-art clustering method, we consider Brown
clustering (Brown et al 1992) as implemented in
the SRILM-toolkit (Stolcke, 2002). We induced
1000 clusters which is also the configuration used
in (Turian et al 2010).5
Table 2 illustrates a few of the clusters induced
from our unlabeled dataset introduced in Section
(?) 2. Some of these clusters represent location
or person names (e.g. I. & II.). This exempli-
fies why clustering is effective for named-entity
recognition. We also find clusters that intuitively
seem to be meaningful for our task (e.g. III. &
IV.) but, on the other hand, there are clusters that
contain words that with the exception of their part
of speech do not have anything in common (e.g.
V.).
3.2 Manually Compiled Lexicons (Lex)
The major shortcoming of word clustering is that
it lacks any task-specific knowledge. The oppo-
site type of generalization is the usage of manu-
ally compiled lexicons comprising predicates that
indicate the presence of opinion holders, such as
supported, worries or disappointed in (4)-(6).
(4) I always supported this idea. holder:agent.
(5) This worries me. holder:patient
(6) He disappointed me. holder:patient
We follow Wiegand and Klakow (2011b) who
found that those predicates can be best obtained
by using a subset of Levin?s verb classes (Levin,
1993) and the strong subjective expressions of the
Subjectivity Lexicon (Wilson et al 2005). For
those predicates it is also important to consider
in which argument position they usually take an
opinion holder. Bethard et al(2004) found the
5We also experimented with other sizes but they did not
produce a better overall performance.
majority of holders are agents (4). A certain
number of predicates, however, also have opinion
holders in patient position, e.g. (5) and (6).
Wiegand and Klakow (2011b) found that many
of those latter predicates are listed in one of
Levin?s verb classes called amuse verbs. While
on the evaluation on the entire MPQA corpus,
opinion holders in patient position are fairly rare
(Wiegand and Klakow, 2011b), we may wonder
whether the same applies to the individual do-
mains that we consider in this work. Table 3
lists the proportion of those opinion holders (com-
puted manually) based on a random sample of 100
opinion holder mentions from those corpora. The
table shows indeed that on the domains from the
MPQA corpus, i.e. ETHICS and SPACE, those
opinion holders play a minor role but there is a no-
tably higher proportion on the FICTION-domain.
3.3 Task-Specific Lexicon Induction (Induc)
3.3.1 Distant Supervision with Prototypical
Opinion Holders
Lexical resources are potentially much more
expressive than word clustering. This knowledge,
however, is usually manually compiled, which
makes this solution much more expensive. Wie-
gand and Klakow (2011a) present an intermedi-
ate solution for opinion holder extraction inspired
by distant supervision (Mintz et al 2009). The
output of that method is also a lexicon of predi-
cates but it is automatically extracted from a large
unlabeled corpus. This is achieved by collecting
predicates that frequently co-occur with prototyp-
ical opinion holders, i.e. common nouns such as
opponents (7) or critics (8), if they are an agent
of that predicate. The rationale behind this is
that those nouns act very much like actual opin-
ion holders and therefore can be seen as a proxy.
(7) Opponents say these arguments miss the point.
(8) Critics argued that the proposed limits were unconstitutional.
This method reduces the human effort to specify-
ing a small set of such prototypes.
Following the best configuration reported
in (Wiegand and Klakow, 2011a), we extract 250
verbs, 100 nouns and 100 adjectives from our un-
labeled corpus (?2).
3.3.2 Extension for Opinion Holders in
Patient Position
The downside of using prototypical opinion
holders as a proxy for opinion holders is that it
327
anguish?, astonish, astound, concern, convince, daze, delight,
disenchant?, disappoint, displease, disgust, disillusion, dissat-
isfy, distress, embitter?, enamor?, engross, enrage, entangle?,
excite, fatigue?, flatter, fluster, flummox?, frazzle?, hook?, hu-
miliate, incapacitate?, incense, interest, irritate, obsess, outrage,
perturb, petrify?, sadden, sedate?, shock, stun, tether?, trouble
Table 4: Examples of the automatically extracted verbs
taking opinion holders as patients (?: not listed as
amuse verb).
is limited to agentive opinion holders. Opinion
holders in patient position, such as the ones taken
by amuse verbs in (5) and (6), are not covered.
Wiegand and Klakow (2011a) show that consid-
ering less restrictive contexts significantly drops
classification performance. So the natural exten-
sion of looking for predicates having prototypical
opinion holders in patient position is not effective.
Sentences, such as (9), would mar the result.
(9) They criticized their opponents.
In (9) the prototypical opinion holder opponents
(in the patient position) is not a true opinion
holder.
Our novel method to extract those predicates
rests on the observation that the past participle of
those verbs, such as shocked in (10), is very often
identical to some predicate adjective (11) having
a similar if not identical meaning. For the predi-
cate adjective, the opinion holder is, however, its
subject/agent and not its patient.
(10) He had shockedverb me. holder:patient
(11) I was shockedadj . holder:agent
Instead of extracting those verbs directly (10),
we take the detour via their corresponding pred-
icate adjectives (11). This means that we collect
all those verbs (from our large unlabeled corpus
(?2)) for which there is a predicate adjective that
coincides with the past participle of the verb.
To increase the likelihood that our extracted
predicates are meaningful for opinion holder ex-
traction, we also need to check the semantic type
in the relevant argument position, i.e. make sure
that the agent of the predicate adjective (which
would be the patient of the corresponding verb)
is an entity likely to be an opinion holder. Our
initial attempts with prototypical opinion holders
were too restrictive, i.e. the number of prototyp-
ical opinion holders co-occurring with those ad-
jectives was too small. Therefore, we widen the
semantic type of this position from prototypical
opinion holders to persons. This means that we
allow personal pronouns (i.e. I, you, he, she and
we) to appear in this position. We believe that this
relaxation can be done in that particular case, as
adjectives are much more likely to convey opin-
ions a priori than verbs (Wiebe et al 2004).
An intrinsic evaluation of the predicates that we
thus extracted from our unlabeled corpus is dif-
ficult. The 250 most frequent verbs exhibiting
this special property of coinciding with adjectives
(this will be the list that we use in our experi-
ments) contains 42% entries of the amuse verbs
(?3.2). However, we also found many other po-
tentially useful predicates on this list that are not
listed as amuse verbs (Table 4). As amuse verbs
cannot be considered a complete golden standard
for all predicates taking opinion holders as pa-
tients, we will focus on a task-based evaluation
of our automatically extracted list (?6).
4 Data-driven Methods
In the following, we present the two supervised
classifiers we use in our experiments. Both clas-
sifiers incorporate the same levels of representa-
tions, including the same generalization methods.
4.1 Conditional Random Fields (CRF)
The supervised classifier most frequently used
for information extraction tasks, in general, are
conditional random fields (CRF) (Lafferty et al
2001). Using CRF, the task of opinion holder ex-
traction is framed as a tagging problem in which
given a sequence of observations x = x1x2 . . . xn
(words in a sentence) a sequence of output tags
y = y1y2 . . . yn indicating the boundaries of opin-
ion holders is computed by modeling the condi-
tional probability P (x|y).
The features we use (Table 5) are mostly in-
spired by Choi et al(2005) and by the ones
used for plain support vector machines (SVMs)
in (Wiegand and Klakow, 2010). They are orga-
nized into groups. The basic group Plain does not
contain any generalization method. Each other
group is dedicated to one specific generalization
method that we want to examine (Clus, Induc
and Lex). Apart from considering generalization
features indicating the presence of generalization
types, we also consider those types in conjunction
with semantic roles. As already indicated above,
semantic roles are especially important for the de-
tection of opinion holders. Unfortunately, the cor-
328
Group Features
Plain
Token features: unigrams and bigrams
POS/chunk/named-entity features: unigrams, bi-
grams and trigrams
Constituency tree path to nearest predicate
Nearest predicate
Semantic role to predicate+lexical form of predicate
Clus
Cluster features: unigrams, bigrams and trigrams
Semantic role to predicate+cluster-id of predicate
Cluster-id of nearest predicate
Induc
Is there predicate from induced lexicon within win-
dow of 5 tokens?
Semantic role to predicate, if predicate is contained in
induced lexicon
Is nearest predicate contained in induced lexicon?
Lex
Is there predicate from manually compiled lexicons
within window of 5 tokens?
Semantic role to predicate, if predicate is contained in
manually compiled lexicons
Is nearest predicate contained in manually compiled
lexicons?
Table 5: Feature set for CRF.
responding feature from the Plain feature group
that also includes the lexical form of the predicate
is most likely a sparse feature. For the opinion
holder me in (10), for example, it would corre-
spond to A1 shock. Therefore, we introduce for
each generalization method an additional feature
replacing the sparse lexical item by a generaliza-
tion label, i.e. Clus: A1 CLUSTER-35265, Induc:
A1 INDUC-PRED and Lex: A1 LEX-PRED.6
For this learning method, we use CRF++.7 We
choose a configuration that provides good perfor-
mance on our source domain (i.e. ETHICS).8
For semantic role labeling we use SWIRL9, for
chunk parsing CASS (Abney, 1991) and for con-
stituency parsing Stanford Parser (Klein and Man-
ning, 2003). Named-entity information is pro-
vided by Stanford Tagger (Finkel et al 2005).
4.2 Convolution Kernels (CK)
Convolution kernels (CK) are special kernel func-
tions. A kernel function K : X ? X ? R com-
putes the similarity of two data instances xi and
xj (xi ? xj ? X). It is mostly used in SVMs that
estimate a hyperplane to separate data instances
from different classes H(~x) = ~w ? ~x + b = 0
where w ? Rn and b ? R (Joachims, 1999). In
6Predicates in patient position are given the same gener-
alization label as the predicates in agent position. Specially
marking them did not result in a notable improvement.
7http://crfpp.sourceforge.net
8The soft margin parameter ?c is set to 1.0 and all fea-
tures occurring less than 3 times are removed.
9http://www.surdeanu.name/mihai/swirl
convolution kernels, the structures to be compared
within the kernel function are not vectors com-
prising manually designed features but the under-
lying discrete structures, such as syntactic parse
trees or part-of-speech sequences. Since they are
directly provided to the learning algorithm, a clas-
sifier can be built without taking the effort of im-
plementing an explicit feature extraction.
We take the best configuration from (Wiegand
and Klakow, 2010) that comprises a combination
of three different tree kernels being two tree ker-
nels based on constituency parse trees (one with
predicate and another with semantic scope) and
a tree kernel encoding predicate-argument struc-
tures based on semantic role information. These
representations are illustrated in Figure 1. The re-
sulting kernels are combined by plain summation.
In order to integrate our generalization meth-
ods into the convolution kernels, the input struc-
tures, i.e. the linguistic tree structures, have to be
augmented. For that we just add additional nodes
whose labels correspond to the respective gener-
alization types (i.e. Clus: CLUSTER-ID, Induc:
INDUC-PRED and Lex: LEX-PRED). The nodes
are added in such a way that they (directly) domi-
nate the leaf node for which they provide a gener-
alization.10 If several generalization methods are
used and several of them apply for the same lex-
ical unit, then the (vertical) order of the general-
ization nodes is LEX-PRED  INDUC-PRED 
CLUSTER-ID.11 Figure 2 illustrates the predi-
cate argument structure from Figure 1 augmented
with INDUC-PRED and CLUSTER-IDs.
For this learning method, we use the
SVMLight-TK toolkit.12 Again, we tune the
parameters to our source domain (ETHICS).13
5 Rule-based Classifiers (RB)
Finally, we also consider rule-based classifiers
(RB). The main difference towards CRF and CK
is that it is an unsupervised approach not requiring
training data. We re-use the framework by Wie-
gand and Klakow (2011b). The candidate set are
all noun phrases in a test set. A candidate is clas-
sified as an opinion holder if all of the following
10Note that even for the configuration Plain the trees are
already augmented with named-entity information.
11We chose this order as it roughly corresponds to the
specificity of those generalization types.
12disi.unitn.it/moschitti
13The cost parameter?j (Morik et al 1999) was set to 5.
329
Figure 1: The different structures (left: constituency trees, right: predicate argument structure) derived from
Sentence (1) for the opinion holder candidate Malaysia used as input for convolution kernels (CK).
Figure 2: Predicate argument structure augmented
with generalization nodes.
conditions hold:
? The candidate denotes a person or group of persons.
? There is a predictive predicate in the same sentence.
? The candidate has a pre-specified semantic role in the event
that the predictive predicate evokes (default: agent-role).
The set of predicates is obtained from a given lex-
icon. For predicates that take opinion holders as
patients, the default agent-role is overruled.
We consider several classifiers that differ in the
lexicon they use. RB-Lex uses the combination of
the manually compiled lexicons presented in ?3.2.
RB-Induc uses the predicates that have been au-
tomatically extracted from a large unlabeled cor-
pus using the methods presented in ?3.3. RB-
Induc+Lex considers the union of those lexicons.
In order to examine the impact of modeling opin-
ion holders in patient position, we also introduce
two versions of each lexicon. AG just consid-
ers predicates in agentive position while AG+PT
also considers predicates that take opinion hold-
ers as patients. For example, RB-InducAG+PT
is a classifier that uses automatically extracted
predicates in order to detect opinion holders in
both agent and patient argument position, i.e.
RB-InducAG+PT also covers our novel extraction
method for patients (?3.3.2).
The output of clustering will exclusively be
evaluated in the context of learning-based meth-
Features
Induc Lex Induc+Lex
Domains AG AG+PT AG AG+PT AG+PT
ETHICS 50.77 50.99 52.22 52.27 53.07
SPACE 45.81 46.55 47.60 48.47 45.20
FICTION 46.59 49.97 54.84 59.35 63.11
Table 6: F-score of the different rule-based classifiers.
ods, since there is no straightforward way of in-
corporating this output into a rule-based classifier.
6 Experiments
CK and RB have an instance space that is differ-
ent from the one of CRF. While CRF produces
a prediction for every word token in a sentence,
CK and RB only produce a prediction for every
noun phrase. For evaluation, we project the pre-
dictions from RB and CK to word token level in
order to ensure comparability. We evaluate the se-
quential output with precision, recall and F-score
as defined in (Johansson and Moschitti, 2010; Jo-
hansson and Moschitti, 2011).
6.1 Rule-based Classifier
Table 6 shows the cross-domain performance of
the different rule-based classifiers. RB-Lex per-
forms better than RB-Induc. In comparison to the
domains ETHICS and SPACE the difference is
larger on FICTION. Presumably, this is due to the
fact that the predicates in Induc are extracted from
a news corpus (?2). Thus, Induc may slightly suf-
fer from a domain mismatch. A combination of
the two classifiers, i.e. RB-Lex+Induc, results in
a notable improvement in the FICTION-domain.
The approaches that also detect opinion holders as
patients (AG+PT) including our novel approach
(?3.3.2) are effective. A notable improvement can
330
Training Size (%)
Features Alg. 5 10 20 50 100
Plain
CRF 32.14 35.24 41.03 51.05 55.13
CK 42.15 46.34 51.14 56.39 59.52
+Clus
CRF 33.06 37.11 43.47 52.05 56.18
CK 42.02 45.86 51.11 56.59 59.77
+Induc
CRF 37.28 42.31 46.54 54.27 56.71
CK 46.26 49.35 53.26 57.28 60.42
+Lex
CRF 40.69 43.91 48.43 55.37 58.46
CK 46.45 50.59 53.93 58.63 61.50
+Clus+Induc
CRF 37.27 42.19 47.35 54.95 57.14
CK 45.14 48.20 52.39 57.37 59.97
+Clus+Lex
CRF 40.52 44.29 49.32 55.44 58.80
CK 45.89 49.35 53.56 58.74 61.43
+Lex+Induc
CRF 42.23 45.92 49.96 55.61 58.40
CK 47.46 51.44 54.80 58.74 61.58
All
CRF 41.56 45.75 50.39 56.24 59.08
CK 46.18 50.10 54.04 58.92 61.44
Table 7: F-score of in-domain (ETHICS) learning-
based classifiers.
only be measured on the FICTION-domain since
this is the only domain with a significant propor-
tion of those opinion holders (Table 3).
6.2 In-Domain Evaluation of
Learning-based Methods
Table 7 shows the performance of the learning-
based methods CRF and CK on an in-domain
evaluation (ETHICS-domain) using different
amounts of labeled training data. We carry out
a 5-fold cross-validation and use n% of the train-
ing data in the training folds. The table shows that
CK is more robust than CRF. The fewer training
data are used the more important generalization
becomes. CRF benefits much more from gener-
alization than CK. Interestingly, the CRF config-
uration with the best generalization is usually as
good as plain CK. This proves the effectiveness
of CK. In principle, Lex is the strongest general-
ization method while Clus is by far the weakest.
For Clus, systematic improvements towards no
generalization (even though they are minor) can
only be observed with CRF. As far as combina-
tions are concerned, either Lex+Induc or All per-
forms best. This in-domain evaluation proves that
opinion holder extraction is different from named-
entity recognition. Simple unsupervised general-
ization, such as word clustering, is not effective
and popular sequential classifiers are less robust
than margin-based tree-kernels.
Table 8 complements Table 7 in that it com-
pares the learning-based methods with the best
rule-based classifier and also displays precision
and recall. RB achieves a high recall, whereas the
learning-based methods always excel RB in pre-
cision.14 Applying generalization to the learning-
based methods results in an improvement of both
recall and precision if few training data are used.
The impact on precision decreases, however, the
more training data are added. There is always a
significant increase in recall but learning-based
methods may not reach the level of RB even
though they use the same resources. This is a
side-effect of preserving a much higher precision.
It also explains why learning-based methods with
generalization may have a lower F-score than RB.
6.3 Out-of-Domain Evaluation of
Learning-based Methods
Table 9 presents the results of out-of-domain clas-
sifiers. The complete ETHICS-dataset is used for
training. Some properties are similar to the pre-
vious experiments: CK always outperforms CRF.
RB provides a high recall whereas the learning-
based methods maintain a higher precision. Sim-
ilar to the in-domain setting using few labeled
training data, the incorporation of generalization
increases both precision and recall. Moreover, a
combination of generalization methods is better
than just using one method on average, although
Lex is again a fairly robust individual generaliza-
tion method. Generalization is more effective in
this setting than on the in-domain evaluation us-
ing all training data, in particular for CK, since
the training and test data are much more different
from each other and suitable generalization meth-
ods partly close that gap.
There is a notable difference in precision be-
tween the SPACE- and FICTION-domain (and
also the source domain ETHICS (Table 8)). We
strongly assume that this is due to the distribu-
tion of opinion holders in those datasets (Table 1).
The FICTION-domain contains much more opin-
ion holders, therefore the chance that a predicted
opinion holder is correct is much higher.
With regard to recall, a similar level of per-
formance as in the ETHICS-domain can only be
achieved in the SPACE-domain, i.e. CK achieves
a recall of 60%. In the FICTION-domain, how-
ever, the recall is much lower (best recall of CK
is below 47%). This is no surprise as the SPACE-
domain is more similar to the source domain than
14The reason for RB having a high recall is extensively
discussed in (Wiegand and Klakow, 2011b).
331
the FICTION-domain since ETHICS and SPACE
are news texts. FICTION contains more out-of-
domain language. Therefore, RB (which exclu-
sively uses domain-independent knowledge) out-
performs both learning-based methods including
the ones incorporating generalization. Similar re-
sults have been observed for rule-based classifiers
from other tasks in cross-domain sentiment anal-
ysis, such as subjectivity detection and polarity
classification. High-level information as it is en-
coded in a rule-based classifier generalizes better
than learning-based methods (Andreevskaia and
Bergler, 2008; Lambov et al 2009).
We set up another experiment exclusively for
the FICTION-domain in which we combine the
output of our best learning-based method, i.e. CK,
with the prediction of a rule-based classifier. The
combined classifier will predict an opinion holder,
if either classifier predicts one. The motivation for
this is the following: The FICTION-domain is the
only domain to have a significant proportion of
opinion holders appearing as patients. We want
to know how much of them can be recognized
with the best out-of-domain classifier using train-
ing data with only very few instances of this type
and what benefit the addition of using various RBs
which have a clearer notion of these constructions
brings about. Moreover, we already observed that
the learning-based methods have a bias towards
preserving a high precision and this may have as
a consequence that the generalization features in-
corporated into CK will not receive sufficiently
large weights. Unlike the SPACE-domain where
a sufficiently high recall is already achieved with
CK (presumably due to its stronger similarity to-
wards the source domain) the FICTION-domain
may be more severely affected by this bias and
evidence from RB may compensate for this.
Table 10 shows the performance of those com-
bined classifiers. For all generalization types
considered, there is, indeed, an improvement by
adding information from RB resulting in a large
boost in recall. Already the application of our in-
duction approach Induc results in an increase of
more than 8% points compared to plain CK. The
table also shows that there is always some im-
provement if RB considers opinion holders as pa-
tients (AG+PT). This can be considered as some
evidence that (given the available data we use)
opinion holders in patient position can only be ef-
fectively extracted with the help of RBs. It is also
CRF CK
Size Feat. Prec Rec F1 Prec Rec F1
10
Plain 52.17 26.61 35.24 58.26 38.47 46.34
All 62.85 35.96 45.75 63.18 41.50 50.10
50
Plain 59.85 44.50 51.05 59.60 53.50 56.39
All 62.99 50.80 56.24 61.91 56.20 58.92
100
Plain 64.14 48.33 55.13 62.38 56.91 59.52
All 64.75 54.32 59.08 63.81 59.24 61.44
RB 47.38 60.32 53.07 47.38 60.32 53.07
Table 8: Comparison of best RB with learning-based
approaches on in-domain classification.
Algorithms Generalization Prec Rec F
CK (Plain) 66.90 41.48 51.21
CK Induc 67.06 45.15 53.97
CK+RBAG Induc 60.22 54.52 57.23
CK+RBAG+PT Induc 61.09 58.14 59.58
CK Lex 69.45 46.65 55.81
CK+RBAG Lex 67.36 59.02 62.91
CK+RBAG+PT Lex 68.25 63.28 65.67
CK Induc+Lex 69.73 46.17 55.55
CK+RBAG Induc+Lex 61.41 65.56 63.42
CK+RBAG+PT Induc+Lex 62.26 70.56 66.15
Table 10: Combination of out-of-domain CK and rule-
based classifiers on FICTION (i.e. distant domain).
further evidence that our novel approach to extract
those predicates (?3.3.2) is effective.
The combined approach in Table 10 not only
outperforms CK (discussed above) but also RB
(Table 6). We manually inspected the output of
the classifiers to find also cases in which CK de-
tect opinion holders that RB misses. CK has the
advantage that it is not only bound to the relation-
ship between candidate holder and predicate. It
learns further heuristics, e.g. that sentence-initial
mentions of persons are likely opinion holders. In
(12), for example, this heuristics fires while RB
overlooks this instance as to give someone a share
of advice is not part of the lexicon.
(12) She later gives Charlotte her share of advice on running a
household.
7 Related Work
The research on opinion holder extraction has
been focusing on applying different data-driven
approaches. Choi et al(2005) and Choi et al
(2006) explore conditional random fields, Wie-
gand and Klakow (2010) examine different com-
binations of convolution kernels, while Johans-
son and Moschitti (2010) present a re-ranking ap-
proach modeling complex relations between mul-
tiple opinions in a sentence. A comparison of
332
SPACE (similar target domain) FICTION (distant target domain)
CRF CK CRF CK
Features Prec Rec F1 Prec Rec F1 Prec Rec F1 Prec Rec F1
Plain 47.32 48.62 47.96 45.89 57.07 50.87 68.58 28.96 40.73 66.90 41.48 51.21
+Clus 49.00 48.62 48.81 49.23 57.64 53.10 71.85 32.21 44.48 67.54 41.21 51.19
+Induc 42.92 49.15 45.82 46.66 60.45 52.67 71.59 34.77 46.80 67.06 45.15 53.97
+Lex 49.65 49.07 49.36 49.60 59.88 54.26 71.91 35.83 47.83 69.45 46.65 55.81
+Clus+Induc 46.61 48.78 47.67 48.65 58.20 53.00 71.32 35.88 47.74 67.46 42.17 51.90
+Lex+Induc 48.75 50.87 49.78 49.92 58.76 53.98 74.02 37.37 49.67 69.73 46.17 55.55
+Clus+Lex 49.72 50.87 50.29 53.70 59.32 56.37 73.41 37.15 49.33 70.59 43.98 54.20
All 49.87 51.03 50.44 51.68 58.76 54.99 72.00 37.44 49.26 70.61 44.83 54.84
best RB 41.72 57.80 48.47 41.72 57.80 48.47 63.26 62.96 63.11 63.26 62.96 63.11
Table 9: Comparison of best RB with learning-based approaches on out-of-domain classification.
those methods has not yet been attempted. In
this work, we compare the popular state-of-the-art
learning algorithms conditional random fields and
convolution kernels for the first time. All these
data-driven methods have been evaluated on the
MPQA corpus. Some generalization methods are
incorporated but unlike this paper they are neither
systematically compared nor combined. The role
of resources that provide the knowledge of argu-
ment positions of opinion holders is not covered
in any of these works. This kind of knowledge
should be directly learnt from the labeled train-
ing data. In this work, we found, however, that
the distribution of argument positions of opinion
holders varies throughout the different domains
and, therefore, cannot be learnt from any arbitrary
out-of-domain training set.
Bethard et al(2004) and Kim and Hovy (2006)
explore the usefulness of semantic roles provided
by FrameNet (Fillmore et al 2003). Bethard
et al(2004) use this resource to acquire labeled
training data while in (Kim and Hovy, 2006)
FrameNet is used within a rule-based classifier
mapping frame-elements of frames to opinion
holders. Bethard et al(2004) only evaluate on an
artificial dataset (i.e. a subset of sentences from
FrameNet and PropBank (Kingsbury and Palmer,
2002)). The only realistic test set on which Kim
and Hovy (2006) evaluate their approach are news
texts. Their method is compared against a sim-
ple rule-based baseline and, unlike this work, not
against a robust data-driven algorithm.
(Wiegand and Klakow, 2011b) is similar to
(Kim and Hovy, 2006) in that a rule-based ap-
proach is used relying on the relationship towards
predictive predicates. Diverse resources are con-
sidered for obtaining such words, however, they
are only evaluated on the entire MPQA corpus.
The only cross-domain evaluation of opinion
holder extraction is reported in (Li et al 2007) us-
ing the MPQA corpus as a training set and the NT-
CIR collection as a test set. A low cross-domain
performance is obtained and the authors conclude
that this is due to the very different annotation
schemes of those corpora.
8 Conclusion
We examined different generalization methods for
opinion holder extraction. We found that for in-
domain classification, the more labeled training
data are used, the smaller is the impact of gener-
alization. Robust learning methods, such as con-
volution kernels, benefit less from generalization
than weaker classifiers, such as conditional ran-
dom fields. For cross-domain classification, gen-
eralization is always helpful. Distant domains
are problematic for learning-based methods, how-
ever, rule-based methods provide a reasonable re-
call and can be effectively combined with the
learning-based methods. The types of generaliza-
tion that help best are manually compiled lexicons
followed by an induction method inspired by dis-
tant supervision. Finally, we examined the case
of opinion holders as patients and also presented
a novel automatic extraction method that proved
effective. Such dedicated extraction methods are
important as common labeled datasets (from the
news domain) do not provide sufficient training
data for these constructions.
Acknowledgements
This work was funded by the German Federal Ministry
of Education and Research (Software-Cluster) under
grant no. ?01IC10S01?. The authors thank Alessandro
Moschitti, Benjamin Roth and Josef Ruppenhofer for
their technical support and interesting discussions.
333
References
Steven Abney. 1991. Parsing By Chunks. In Robert
Berwick, Steven Abney, and Carol Tenny, editors,
Principle-Based Parsing. Kluwer Academic Pub-
lishers, Dordrecht.
Alina Andreevskaia and Sabine Bergler. 2008. When
Specialists and Generalists Work Together: Over-
coming Domain Dependence in Sentiment Tagging.
In Proceedings of the Annual Meeting of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies (ACL/HLT), Columbus, OH,
USA.
Steven Bethard, Hong Yu, Ashley Thornton, Vasileios
Hatzivassiloglou, and Dan Jurafsky. 2004. Extract-
ing Opinion Propositions and Opinion Holders us-
ing Syntactic and Lexical Cues. In Computing At-
titude and Affect in Text: Theory and Applications.
Springer-Verlag.
Peter F. Brown, Peter V. deSouza, Robert L. Mer-
cer, Vincent J. Della Pietra, and Jenifer C. Lai.
1992. Class-based n-gram models of natural lan-
guage. Computational Linguistics, 18:467?479.
Yejin Choi, Claire Cardie, Ellen Riloff, and Sid-
dharth Patwardhan. 2005. Identifying Sources
of Opinions with Conditional Random Fields and
Extraction Patterns. In Proceedings of the Con-
ference on Human Language Technology and Em-
pirical Methods in Natural Language Processing
(HLT/EMNLP), Vancouver, BC, Canada.
Yejin Choi, Eric Breck, and Claire Cardie. 2006. Joint
Extraction of Entities and Relations for Opinion
Recognition. In Proceedings of the Conference on
Empirical Methods in Natural Language Process-
ing (EMNLP), Sydney, Australia.
Charles. J. Fillmore, Christopher R. Johnson, and
Miriam R. Petruck. 2003. Background to
FrameNet. International Journal of Lexicography,
16:235 ? 250.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating Non-local Informa-
tion into Information Extraction Systems by Gibbs
Sampling. In Proceedings of the Annual Meeting
of the Association for Computational Linguistics
(ACL), Ann Arbor, MI, USA.
Thorsten Joachims. 1999. Making Large-Scale SVM
Learning Practical. In B. Scho?lkopf, C. Burges, and
A. Smola, editors, Advances in Kernel Methods -
Support Vector Learning. MIT Press.
Richard Johansson and Alessandro Moschitti. 2010.
Reranking Models in Fine-grained Opinion Anal-
ysis. In Proceedings of the International Confer-
ence on Computational Linguistics (COLING), Be-
jing, China.
Richard Johansson and Alessandro Moschitti. 2011.
Extracting Opinion Expressions and Their Polari-
ties ? Exploration of Pipelines and Joint Models. In
Proceedings of the Annual Meeting of the Associa-
tion for Computational Linguistics (ACL), Portland,
OR, USA.
Jason S. Kessler, Miriam Eckert, Lyndsay Clarke,
and Nicolas Nicolov. 2010. The ICWSM JDPA
2010 Sentiment Corpus for the Automotive Do-
main. In Proceedings of the International AAAI
Conference on Weblogs and Social Media Data
Challange Workshop (ICWSM-DCW), Washington,
DC, USA.
Soo-Min Kim and Eduard Hovy. 2006. Extracting
Opinions, Opinion Holders, and Topics Expressed
in Online News Media Text. In Proceedings of
the ACL Workshop on Sentiment and Subjectivity in
Text, Sydney, Australia.
Paul Kingsbury and Martha Palmer. 2002. From
TreeBank to PropBank. In Proceedings of the
Conference on Language Resources and Evaluation
(LREC), Las Palmas, Spain.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate Unlexicalized Parsing. In Proceedings of the
Annual Meeting of the Association for Computa-
tional Linguistics (ACL), Sapporo, Japan.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional Random Fields: Prob-
abilistic Models for Segmenting and Labeling Se-
quence Data. In Proceedings of the International
Conference on Machine Learning (ICML).
Dinko Lambov, Gae?l Dias, and Veska Noncheva.
2009. Sentiment Classification across Domains. In
Proceedings of the Portuguese Conference on Artifi-
cial Intelligence (EPIA), Aveiro, Portugal. Springer-
Verlag.
Beth Levin. 1993. English Verb Classes and Alter-
nations: A Preliminary Investigation. University of
Chicago Press.
Yangyong Li, Kalina Bontcheva, and Hamish Cun-
ningham. 2007. Experiments of Opinion Analy-
sis on the Corpora MPQA and NTCIR-6. In Pro-
ceedings of the NTCIR-6 Workshop Meeting, Tokyo,
Japan.
Mike Mintz, Steven Bills, Rion Snow, and Dan Juraf-
sky. 2009. Distant Supervision for Relation Extrac-
tion without Labeled Data. In Proceedings of the
Joint Conference of the Annual Meeting of the As-
sociation for Computational Linguistics and the In-
ternational Joint Conference on Natural Language
Processing of the Asian Federation of Natural Lan-
guage Processing (ACL/IJCNLP), Singapore.
Katharina Morik, Peter Brockhausen, and Thorsten
Joachims. 1999. Combining Statistical Learn-
ing with a Knowledge-based Approach - A Case
Study in Intensive Care Monitoring. In Proceedings
the International Conference on Machine Learning
(ICML).
Andreas Stolcke. 2002. SRILM - An Extensible Lan-
guage Modeling Toolkit. In Proceedings of the In-
334
ternational Conference on Spoken Language Pro-
cessing (ICSLP), Denver, CO, USA.
Veselin Stoyanov and Claire Cardie. 2011. Auto-
matically Creating General-Purpose Opinion Sum-
maries from Text. In Proceedings of Recent Ad-
vances in Natural Language Processing (RANLP),
Hissar, Bulgaria.
Veselin Stoyanov, Claire Cardie, Diane Litman, and
Janyce Wiebe. 2004. Evaluating an Opinion An-
notation Scheme Using a New Multi-Perspective
Question and Answer Corpus. In Proceedings of
the AAAI Spring Symposium on Exploring Attitude
and Affect in Text, Menlo Park, CA, USA.
Cigdem Toprak, Niklas Jakob, and Iryna Gurevych.
2010. Sentence and Expression Level Annotation
of Opinions in User-Generated Discourse. In Pro-
ceedings of the Annual Meeting of the Associa-
tion for Computational Linguistics (ACL), Uppsala,
Sweden.
Joseph Turian, Lev Ratinov, and Yoshua Bengio.
2010. Word Representations: A Simple and Gen-
eral Method for Semi-supervised Learning. In Pro-
ceedings of the Annual Meeting of the Associa-
tion for Computational Linguistics (ACL), Uppsala,
Sweden.
Janyce Wiebe, Theresa Wilson, Rebecca Bruce,
Matthew Bell, and Melanie Martin. 2004. Learn-
ing Subjective Language. Computational Linguis-
tics, 30(3).
Janyce Wiebe, Theresa Wilson, and Claire Cardie.
2005. Annotating Expressions of Opinions and
Emotions in Language. Language Resources and
Evaluation, 39(2/3):164?210.
Michael Wiegand and Dietrich Klakow. 2010. Convo-
lution Kernels for Opinion Holder Extraction. In
Proceedings of the Human Language Technology
Conference of the North American Chapter of the
ACL (HLT/NAACL), Los Angeles, CA, USA.
Michael Wiegand and Dietrich Klakow. 2011a. Proto-
typical Opinion Holders: What We can Learn from
Experts and Analysts. In Proceedings of Recent Ad-
vances in Natural Language Processing (RANLP),
Hissar, Bulgaria.
Michael Wiegand and Dietrich Klakow. 2011b. The
Role of Predicates in Opinion Holder Extraction. In
Proceedings of the RANLP Workshop on Informa-
tion Extraction and Knowledge Acquisition (IEKA),
Hissar, Bulgaria.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing Contextual Polarity in Phrase-
level Sentiment Analysis. In Proceedings of the
Conference on Human Language Technology and
Empirical Methods in Natural Language Process-
ing (HLT/EMNLP), Vancouver, BC, Canada.
335
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 673?682,
Gothenburg, Sweden, April 26-30 2014. c?2014 Association for Computational Linguistics
Automatic Food Categorization from Large Unlabeled Corpora and Its
Impact on Relation Extraction
Michael Wiegand and Benjamin Roth and Dietrich Klakow
Spoken Language Systems
Saarland University
D-66123 Saarbru?cken, Germany
{Michael.Wiegand|Benjamin.Roth|Dietrich.Klakow}@lsv.uni-saarland.de
Abstract
We present a weakly-supervised induc-
tion method to assign semantic informa-
tion to food items. We consider two tasks
of categorizations being food-type classi-
fication and the distinction of whether a
food item is composite or not. The cate-
gorizations are induced by a graph-based
algorithm applied on a large unlabeled
domain-specific corpus. We show that the
usage of a domain-specific corpus is vi-
tal. We do not only outperform a manually
designed open-domain ontology but also
prove the usefulness of these categoriza-
tions in relation extraction, outperforming
state-of-the-art features that include syn-
tactic information and Brown clustering.
1 Introduction
In view of the large interest in food in many parts
of the population and the ever increasing amount
of new dishes/food items, there is a need of au-
tomatic knowledge acquisition. We approach this
task with the help of natural language processing.
We investigate different methods to assign cate-
gories to food items. We focus on two categoriza-
tions, being a classification of food items to cat-
egories of the Food Guide Pyramid (U.S. Depart-
ment of Agriculture, 1992) and a categorization of
whether a food item is composite or not.
We present a semi-supervised graph-based ap-
proach to induce these food categorizations from
an unlabeled domain-specific text corpus crawled
from the Web. The method only requires mini-
mal manual guidance for the initialization of the
algorithm with seed terms. It depends, however,
on an automatically constructed high-quality sim-
ilarity graph. For that we choose a pattern-based
representation that outperforms a distributional-
based representation. For initialization, we ex-
amine some manually compiled seed words and
a very few simple surface patterns to automati-
cally induce such expressions. As a hard baseline,
we compare the effectiveness of using a general-
purpose ontology for the same types of categoriza-
tions. Apart from an intrinsic evaluation, we also
examine the categories in relation extraction.
The contributions of this paper are a method re-
quiring minimal supervision for a comprehensive
classification of food items and a proof of con-
cept that the knowledge that can thus be gained is
beneficial for relation extraction. Even though we
focus on a specific domain, the induction method
can be easily translated to other domains. In par-
ticular, other life-style domains, such as fashion,
cosmetics or home & gardening, show parallels
since comparable textual web data are available
and similar relation types (e.g. that two items fit
together or can be substituted by each other) exist.
Our experiments are carried out on German data
but our findings should carry over to other lan-
guages since the issues we address are (mostly)
language universal. For general accessibility, all
examples are given as English translations.
2 Data & Annotation
2.1 Domain-Specific Text Corpus
In order to generate a dataset for our experiments,
we used a crawl of chefkoch.de1 (Wiegand et al.,
2012b) consisting of 418, 558 webpages of food-
related forum entries. chefkoch.de is the largest
German web portal for food-related issues.
2.2 Food Categorization
As a food vocabulary, we employ a list of 1888
food items: 1104 items were directly extracted
from GermaNet (Hamp and Feldweg, 1997), the
German version of WordNet (Miller et al., 1990).
The items were identified by extracting all hy-
ponyms of the synset Nahrung (English: food). By
1www.chefkoch.de
673
Class Description Size Perc.
MEAT meat and fish (products) 394 20.87
BEVERAGE beverages (incl. alcoholic drinks) 298 15.78
VEGE vegetables (incl. salads) 231 12.24
SWEET sweets, pastries and snack mixes 228 12.08
SPICE spices and sauces 216 11.44
STARCH starch-based side dishes 185 9.80
MILK milk products 104 5.51
FRUIT fruits 94 4.98
GRAIN grains, nuts and seeds 77 4.08
FAT fat 41 2.18
EGG eggs 20 1.06
Table 1: The different food types (gold standard).
consulting the relation tuples from Wiegand et al.
(2012c) a further 784 items were added. We man-
ually annotated this vocabulary w.r.t. two tasks:
2.2.1 Task I: Food Types
The food type categories we chose are mainly in-
spired by the Food Guide Pyramid (U.S. Depart-
ment of Agriculture, 1992) that divides food items
into categories with similar nutritional properties.
This categorization scheme not only divides the
set of food items in many intuitive homogeneous
classes but it is also the scheme that is most com-
monly agreed upon. Table 1 lists the specific cat-
egories we use. For category assignment of com-
plex dishes comprising different food items we ap-
plied a heuristics: we always assign the category
that dominates the dish. A meat sauce, for exam-
ple, would thus be assigned MEAT (even though
there may be other ingredients than meat).
2.2.2 Task II: Dishes vs. Atomic Food Items
In addition to Task I, we include another catego-
rization that divides food items into dishes and
atomic food items (Table 2). By dish, we mainly
understand food items that are composite food
items made of other (atomic) food items. This
categorization is orthogonal to the previous clas-
sification of food items. We refrained from adding
dishes as a further category of food types in ?2.2.1,
as we would have ended up with a very heteroge-
neous class in the set of homogeneous food type
categories. Thus, dishes that differ greatly in nu-
trient content, such as Waldorf salad and chocolate
cake, would have been subsumed by one class.
3 Method
3.1 Graph-based Induction
We propose a semi-supervised graph-based ap-
proach to label food items with their respective
Class Description Examples Perc.
DISH composite food items cake, falafel, meat loaf 32.10
ATOM non-composite food items apple, steak, potato 67.90
Table 2: Distribution of dishes and atomic food
items among the food vocabulary (gold standard).
food categories. The underlying data structure
is a similarity graph connecting different food
items. Food items that belong to the same category
should be connected by highly weighted edges. In
order to infer the labels for each respective food
item, one first needs to specify a small set of seeds
for each category and then apply a graph-based
clustering method that divides the graph into clus-
ters that represent distinct food categories. Our
method is a low-resource approach that can also
be easily adapted to other domains. The only
domain-specific information required are an unla-
beled corpus and a set of seeds.
3.1.1 Construction of the Similarity Graph
To enable a graph-based induction, we generate a
similarity graph that connects similar food items.
For that purpose, a list of domain-independent
similarity-patterns was compiled. Each pattern is a
lexical sequence that connects the mention of two
food items (Table 3). Each pair of food items ob-
served with any of those patterns is connected via
a weighted edge (the different patterns are treated
equally). The weight is the total frequency of all
patterns co-occurring with a particular food pair.
Due to the high precision of our patterns, with
one or a few prototypical seeds we cannot expect
to find all items of a food category within the set
of items to which the seeds are directly connected.
Instead, one also needs to consider transitive con-
nectedness within the graph. For example, in Fig-
ure 1 banana and redberry are not directly con-
nected but they can be reached via pear or rasp-
berry. However, by considering mediate relation-
ships it becomes more difficult to determine the
most appropriate category for each food item since
most food items are connected to food items of dif-
ferent categories (in Figure 1, there are not only
edges between banana and other types of fruits
but there is also some edge to some sweet, i.e.
chocolate). For a unique class assignment, we ap-
ply a robust graph-based clustering algorithm. (It
will figure out that banana, pear, raspberry and
redberry belong to the same category and choco-
late belongs to another category, since it is mostly
674
Patterns food item
1
(or|or rather|instead of|?(?) food item
2
Example {apple: pineapple, pear, fruit, strawberry, kiwi} {steak:
schnitzel, sausage, roast, meat loaf, cutlet}
Table 3: Domain-independent patterns for build-
ing the similarity graph.
Figure 1: Illustration of the similarity graph.
linked to many other food items not being fruits.)
3.1.2 Semi-Supervised Graph Optimization
Our semi-supervised graph optimization (Belkin
and Niyogi, 2004) is a robust algorithm that was
primarily chosen since it only contains few free
parameters to adjust. It is based on two principles:
First, similar data points should be assigned simi-
lar labels, as expressed by a similarity graph of la-
beled and unlabeled data. Second, for labeled data
points the prediction of the learnt classifier should
be consistent with the (actual) gold labels.
We construct a weighted transition matrix W
of the graph by normalization of the matrix with
co-occurrence counts C which we obtain from the
similarity graph (?3.1.1). We use the common
normalization by a power of the degree function
d
i
=
?
j
C
ij
: it defines W
ij
=
C
ij
d
?
i
d
?
j
if i 6= j,
and W
ii
= 0. The normalization weight ? is the
first of two parameters used in our experiments for
semi-supervised graph optimization. For learning
the semi-supervised classifier, we use the method
of Zhou et al. (2004) to find a classifying function
which is sufficiently smooth with respect to both
the structure of unlabeled and labeled points.
Given a set of data points X = {x
1
, . . . , x
n
}
and label set L = {1, . . . , c}, with x
i:1?i?l
labeled
as y
i
? L and x
i:l+1?i?n
unlabeled. For predic-
tion, a vectorial function F : X ? Rc is estimated
assigning a vector F
i
of label scores to every x
i
.
The predicted labeling follows from these scores
as y?
i
= argmax
j?c
F
ij
. Conversely, the gold la-
beling matrix Y is a n ? c matrix with Y
ij
= 1 if
x
i
is labeled as y
i
= j and Y
ij
= 0 otherwise.
Minimizing the cost function Q aims at a trade-
off between information from neighbours and ini-
tial labeling information, controlled by parameter
Patterns Categorization Examples
patt
hearst
Food Types food item is some food type,
food type such as food item, . . .
patt
dishes
Dishes recipe for food item
patt
atom
Atomic Food Items made of|contains food item
Table 4: List of patterns to extract seeds.
? (the second parameter used in our experiments):
Q =
1
2
(
?
n
i,j=1
W
ij
?
?
?
?
?
1
?
?
i
F
i
?
1
?
?
j
F
j
?
?
?
?
?
+ ?
?
n
i=1
?F
i
? Y
i
?
)
where ?
i
is the degree function of W .
The first term in Q is the smoothness constraint,
its minimization leads to adjacent edges having
similar labels. The second term is the fitting con-
straint, its minimization leads to consistency of the
function F with the labeling of the data. The solu-
tion to the above cost function is found by solving
a system of linear equations (Zhou et al., 2004).
As we do not possess development data for this
work, we set the two free parameters ? = 0.5 and
? = 0.01. This setting is used for both induction
tasks and all configurations. It is a setting that pro-
vided reasonable results without any notable bias
for any particular configuration we examine.
3.1.3 Manually vs. Automatically Extracted
Seeds
We explore two types of seed initializations: (a)
a manually compiled list of seed food items and
(b) a small set of patterns (Table 4) by the help of
which such seeds are automatically extracted.
In order to extract seeds for Task I with the
pattern-based approach, we apply the patterns
from Hearst (1992). These patterns have been de-
signed for the acquisition of hyponyms. Task I can
also be regarded as some type of hyponym extrac-
tion. The food types (fruit, meat, sweets) repre-
sent the hypernyms for which we extract seed hy-
ponyms (banana, beef, chocolate).
In order to extract seeds for Task II, we apply
two domain-specific sets of patterns (patt
dish
and
patt
atom
). We rank the food items according to the
frequency of occurring with the respective pattern
set. Since food items may occur in both rankings,
we merge the two rankings in the following way:
score(food item) = #patt
dish
(food it.)?#patt
atom
(food it.)
The top end of this ranking represents dishes
while the bottom end represents atoms.
3.2 Using a General-Purpose Ontology
As a hard baseline, we also make use of the seman-
tic relationships encoded in GermaNet. Our two
675
types of food categorization schemes can be ap-
proximated with the hypernymy graph in that on-
tology: We manually identify nodes that resemble
our food categories (e.g. fruit, meat or dish) and
label any food item that is an immediate or a me-
diate hyponym of these nodes (e.g. apple for fruit)
with the respective category label. The downside
of this method is that a large amount of food items
is missing from the GermaNet-database (?2.2).
3.3 Other Baselines & Post-Processing
In addition to the previous methods we imple-
ment a heuristic baseline (HEUR) that rests on the
observation that German food items of the same
food category often share the same suffix, e.g.
Schokoladenkuchen (English: chocolate cake) and
Apfelkuchen (English: apple pie). For HEUR, we
manually compiled a set of few typical suffixes for
each food type/dish category (ranging from 3 to 8
suffixes per category). For classification of a food
item, we assign the food item the category label
whose suffix matched with the food item.2
We also examine an unsupervised baseline
(UNSUP) that applies spectral clustering on the
similarity graph following von Luxburg (2007):
? Input: a similarity matrix W and the number of categories to detect k.
? The laplacian L is constructed from W . It is the symmetric laplacian
L = I ?D
1/2
WD
1/2
, where D is a diagonal degree matrix.3
? A matrix U ? Rn?k is constructed that contains as columns the first
k eigenvectors u
1
, . . . , u
k
of L.
? The rows of U are interpreted as the new data points. The final cluster-
ing is obtained by k-means clustering of the rows of U .
UNSUP (which is completely parameter-free)
gives some indication about the intrinsic expres-
siveness of the similarity graph as it lacks any
guidance towards the categories to be predicted.
In graph-based food categorization, one can
only make predictions for food items that are con-
nected (be it directly or indirectly) to seed food
items within the similarity graph. To expand labels
to unconnected food items, we apply some post-
processing (POSTP). Similarly to HEUR, it ex-
ploits the suffix-similarity of food items. It assigns
each unconnected food item the label of the food
item (that could be labeled by the graph optimiza-
tion) that shares the longest suffix. Due to their
similar nature, we refrain from applying POSTP
on HEUR as it would produce no changes.
2Unlike German food items, English food items are of-
ten multi-word expressions. Therefore, we assume that for
English, instead of analyzing suffixes the usage of the head
of a multiword expression (i.e. chocolate cake) would be an
appropriate basis for a similar heuristic.
3That is, D
ii
equals to the sum of the ith row.
PLAIN +POSTP
Configuration graph Acc Prec Rec F1 Acc Prec Rec F1
UNSUP X 46.2 43.1 35.7 36.0 56.1 41.0 42.5 38.4
HEUR (plain) 25.5 87.9 32.2 42.9 N/A N/A N/A N/A
HEUR X 56.4 73.6 52.1 54.7 68.7 72.3 64.3 60.7
PAT-Top1 X 52.4 60.2 51.2 52.5 64.5 58.2 62.9 57.4
PAT-Top5 X 61.1 70.7 61.9 64.4 74.5 67.9 76.0 69.7
PAT-Top10 X 60.2 69.6 60.5 62.2 73.4 66.7 74.2 67.3
1-PROTO X 58.0 68.0 58.0 59.5 70.2 64.1 71.0 63.8
5-PROTO X 64.5 76.6 63.7 68.6 78.6 73.8 78.5 75.2
10-PROTO X 65.8 79.0 65.5 71.0 80.2 75.9 80.6 77.7
GermaNet (plain) 52.1 94.0 52.0 65.7 75.4 73.2 75.0 72.4
GermaNet X 68.3 84.7 63.4 71.6 82.7 81.8 77.7 79.1
Table 5: Comparison of different food-type classi-
fiers (graph indicates graph-based optimization).
4 Experiments
We report precision, recall and F-score and accu-
racy.4 For precision, recall and F-score, we list the
macro-averaged score.
4.1 Evaluation of Food Categorization
4.1.1 Detection of Food Types
Table 5 compares different classifiers and configu-
rations for the prediction of food types (against the
gold standard from Table 1). Apart from the pre-
viously described baselines, we consider n man-
ually selected prototypes (n-PROTO) and the top
n food items produced by Hearst-patterns (PAT-
Topn) as seeds for graph-based optimization. The
table shows that the semi-supervised graph-based
approach with these seeds outperforms the base-
lines UNSUP and HEUR. Only as few as 5
prototypical seeds (per category) are required to
obtain performance that is even better than us-
ing plain GermaNet. The table also shows that
post-processing (with our suffix-heuristics) con-
sistently improves performance. Manually choos-
ing prototypes is more effective than instantiating
seeds via Hearst-patterns. The quality of the out-
put of Hearst-patterns degrades from top 10 on-
wards. However, considering that PAT-Topn does
not include any manual intervention, it already
produces decent results. Finally, even GermaNet
can be effectively used as seeds.
4.1.2 Detection of Dishes
Table 6 compares different classifiers for the de-
tection of dishes (against the gold standard from
Table 2). Dishes and atomic food items are very
4All manually labeled resources are available at:
www.lsv.uni-saarland.de/personalPages/
michael/relFood.html
676
PLAIN +POSTP
Configuration graph Acc Prec Rec F1 Acc Prec Rec F1
UNSUP X 54.5 59.6 40.2 37.3 67.9 59.0 50.0 40.6
HEUR (plain) 74.1 84.3 59.9 58.6 N/A N/A N/A N/A
PAT-Top25 X 59.7 72.2 54.6 61.9 74.1 70.1 67.6 68.4
PAT-Top50 X 60.9 74.4 55.6 63.1 75.9 72.7 69.2 70.3
PAT-Top100 X 62.7 77.6 57.2 65.2 78.4 76.5 71.5 73.0
PAT-Top250 X 59.6 71.8 55.1 62.2 74.2 70.3 68.7 69.3
RAND-25 X 61.4 77.1 54.3 61.8 76.1 74.4 67.1 68.4
RAND-50 X 62.6 76.3 60.1 67.2 77.2 74.0 76.8 74.4
RAND-100 X 66.5 82.7 63.0 71.3 83.0 80.8 79.5 80.1
GermaNet (plain) 49.5 81.3 46.5 59.3 79.0 75.9 75.5 75.7
GermaNet X 60.8 79.4 51.3 57.6 75.9 78.2 64.4 65.4
Table 6: Comparison of different classifiers dis-
tinguishing between dishes and atomic food items
(graph indicates graph-based optimization).
PLAIN +POSTP
Configuration graph Acc Prec Rec F1 Acc Prec Rec F1
PAT-Top100 (plain) 9.5 89.5 10.5 18.6 63.6 61.5 63.5 61.3
PAT-Top100 X 62.7 77.6 57.2 65.2 78.4 76.5 71.5 73.0
RAND-100 (plain) 10.6 100.0 12.2 21.4 70.2 69.7 69.0 69.0
RAND-100 X 66.5 82.7 63.0 71.3 83.0 80.8 79.5 80.1
Table 7: Impact of graph-based optimization
(graph) for the detection of dishes.
heterogeneous classes which is why more seeds
are required for initialization. This means that
we cannot look for prototypes. For simplicity,
we resorted to randomly sample seeds from our
gold standard (RAND-n). For HEUR, we could
not find a small and intuitive set of suffixes that
are shared by many atomic food types, therefore
we considered all food types from our vocabulary
whose suffix did not match a typical dish suffix as
atomic. As this leaves no unspecified food items in
our vocabulary, we cannot use the output of HEUR
as seeds for graph-based optimization.
In contrast to the previous experiment, HEUR is
a more robust baseline. But again, post-processing
mostly improves performance, and patterns are not
as good as manual (random) seeds yet the former
are notably better than HEUR w.r.t. F-Score. Un-
like in the food-type classification, graph-based
optimization applied on GermaNet does not result
in some improvement. We assume that the preci-
sion of plain GermaNet with 81.3% is too low.5
Since GermaNet cannot effectively be used as
seeds for the graph-based optimization and post-
processing has already a strong positive effect, we
may wonder how effective the actual graph-based
5For other seeds for which it worked, we usually mea-
sured a precision of 90% or higher.
optimization is for this classification task. Af-
ter all, significantly more seeds are required for
this classification task than for the previous task,
so we need to show that it is not the mere seeds
(+post-processing) that are required for a reason-
able categorization. Table 7 examines two key
configurations with and without graph-based op-
timization. It shows that also for this classification
task, graph-based optimization produces a catego-
rization superior to the mere seeds. Moreover, the
suffix-based post-processing is complementary to
the improvement by the graph-based optimization.
4.1.3 Comparison of Initialization Methods
Table 8 compares for each food type 5 manually
selected prototypical seeds (i.e. 5-PROTO) and
the 5 food items most frequently been observed
with patt
hearst
(Table 4). While the manually cho-
sen seeds represent the spectrum of food items
within each particular class (e.g. for STARCH,
some type of pasta, rice and potato was chosen),
it is not possible to enforce such diversity with
the automatically extracted seeds. However, most
food items are correct. Table 9 displays the 10
most highly ranked dishes and atomic food items
extracted with patt
dish
and patt
atom
(Table 4). Un-
like the previous task (Table 8), we obtain more
heterogeneous seeds within the same class.
4.1.4 Distributional Similarity
Since many recent methods for related tasks, such
as noun classification, are based on so-called dis-
tributional similarity (Riloff and Shepherd, 1997;
Lin, 1998; Snow et al., 2004; Weeds et al., 2004;
Yamada et al., 2009; Huang and Riloff, 2010;
Lenci and Benotto, 2012), we also examine this as
an alternative representation to the pattern-based
similarity graph (Table 3). We represent each food
item as a vector which itself is an aggregate of
the contexts of all mentions of a particular food
item. We weighted the individual (context) words
co-occurring with the food item at a fixed window
size of 5 words with tf-idf. We can now apply
graph-based optimization on the similarity matrix
encoding the cosine similarities between any pos-
sible pair of vectors representing two food items.
As seeds, we use the best configuration (not em-
ploying GermaNet), i.e. 10-PROTO for food type
classification and RAND-100 for the dish classi-
fication. Since, however, the graph clustering is
not actually necessary, as we have a full similar-
ity matrix (rather than a sparse graph) that also al-
677
Class 5 Manually Chosen Seeds (5-PROTO) 5 Hearst-Pattern Seeds (PAT-Top5)
MEAT schnitzel, rissole, bologna, redfish, trout salmon, beef, chicken, turkey hen, poultry
BEVERAGE coffee, tea, water, beer, coke coffee, beer, mineral water, lemonade, tea
VEGE peas, green salad, tomato, cauliflower, carrot zucchini, lamb?s salad, broccoli, leek, cauliflower
SWEET chocolate, torte, popcorn, apple pie, potato crisps wine gum, marzipan, custard, pancake, biscuits
SPICE pepper, cinnamon, salt, gravy, remoulade cinnamon, laurel, clove, tomato sauce, basil
STARCH spaghetti, basmati rice, white bread, potato, french fries au gratin potatoes, jacket potato, potato, pita, jam
MILK yoghurt, gouda, cream cheese, cream, butter milk butter milk, bovine milk, soured milk, goat cheese, sour cream
FRUIT banana, apple, strawberries, apricot, orange banana, strawberries, pear, melon, kiwi
GRAIN hazelnut, pumpkin seed, rye flour, semolina, wheat sesame, spelt, wheat, millet, barley
FAT margarine, lard, colza oil, spread, butter margarine, lard, resolidified butter, coconut oil, tartar
EGG scrambled eggs, fried eggs, chicken egg, omelette, pickled egg yolk, fried eggs, albumen, offal, easter egg
Table 8: Comparison of different seed initializations for the food type categorization task (underlined
food items represent erroneously extracted food items).
lows us to compare any arbitrary pair of food items
directly, we also employ a second classifier (for
comparison) based on the nearest neighbour prin-
ciple. We assign each food item the label of the
most similar seed food item.
Table 10 compares these two classifiers with the
best previous result. It shows that the pattern-
based representation consistently outperforms the
distributional representation. The former may be
sparse but it produces high-precision similarity
links.6 The vector representation, on the other
hand, may not be sparse but it contains a high
degree of noise. The major problem is that not
only vectors of similar food items, such as chips
(fries), potatoes and rice, are similar to each other,
but also vectors of different food items that are
typically consumed with each other (e.g. fish
and chips). This is because of their frequent co-
occurrence (as in collocations like fish & chips).
Unfortunately, these pairs belong to different food
types. For the dish classification, however, the
vector representation is less of a problem.7
The distributional representation works better
with the simple nearest neighbour classifier. We
assume that graph-based optimization adds further
noise to the classification since, unlike the nearest
neighbour which only calculates the direct similar-
ity between two vectors, it also incorporates indi-
rect relationships (which may be more error-prone
than the direct relationships) between food items.
4.1.5 Do we need a domain-specific corpus?
In this section, we want to provide evidence that
apart from the similarity graph and seeds the tex-
tual source for the graph, i.e. our domain-specific
6By the label propagation within the graph-based opti-
mization, the sparsity problem is also mitigated.
7Fish and chips are both atoms, so in the dish classifica-
tion, it is no mistake to consider them similar food items.
Class 10 Seeds Extracted with Patterns (PAT-Top10)
DISH cookies, cake, praline, bread dumpling, jam, biscuit, cheese
cake, black-and-whites, onion tart, pasta salad
ATOM marzipan, flour, potato, olive oil, water, sugar, cream, choco-
late, milk, tomato
Table 9: Illustration of seed initialization for the
distinction between dishes and atomic food items.
Task Similarity Classifier Acc F1
Food Type distributional nearest neighbour 53.4 51.1
distributional graph 25.6 25.6
pattern-based graph 80.2 77.7
Dish distributional nearest neighbour 76.8 75.2
distributional graph 71.5 71.2
pattern-based graph 83.0 80.1
Table 10: Impact of the similarity representation.
corpus (chefkoch.de), is also important. For that
purpose, we compare our current corpus against
an open-domain corpus. We consider the German
version of Wikipedia since this resource also con-
tains encyclopedic knowledge about food items.
Table 11 compares the graph-based induction. As
in the previous section, we only consider the best
previous configuration. The table clearly shows
that our domain-specific text corpus is a more ef-
fective resource for our purpose than Wikipedia.
4.2 Evaluation for Relation Extraction
We now examine whether automatic food cate-
gorization can be harnessed for relation extrac-
tion. The task is to detect instances of the relation
types SuitsTo, SubstitutedBy and IngredientOf in-
troduced Wiegand et al. (2012b) (repeated in Ta-
ble 12) and motivated in Wiegand et al. (2012a).
These relation types are highly relevant for cus-
tomer advice/product recommendation. In partic-
ular, SuitsTo and SubstitutedBy are fairly domain-
independent relation types. Customers want to
678
know which items can be used together (SuitsTo),
be it two food items that can be used as a meal
or two fashion items that can be worn together.
Substitutes are also relevant for situations in which
item A is out of stock but item B can be offered as
an alternative. Therefore, insights from this work
should carry over to other domains.
We randomly extracted 1500 sentences from
our text corpus (?2.1) in which (at least) two food
items co-occur. Each food pair mention was man-
ually assigned one label. In addition to the three
relation types from above, we introduce the la-
bel Other for cases in which either another rela-
tion between the target food items is expressed or
the co-occurrence is co-incidental. On a subset of
200 sentences, we measured a substantial inter-
annotation agreement of Cohen?s ? = 0.67 (Lan-
dis and Koch, 1977).
We train a supervised classifier and incorporate
the knowledge induced from our domain-specific
corpus as features. We chose Support Vector Ma-
chines with 5-fold cross-validation using SVMlight-
multi-class (Joachims, 1999).
Table 13 displays all features that we examine
for supervised classification. Most features are
widely used throughout different NLP tasks. One
special feature brown takes into consideration the
output of Brown clustering (Brown et al., 1992)
which like our graph-based optimization produces
a corpus-driven categorization of words. Simi-
lar to UNSUP, this method is unsupervised but it
considers the entire vocabulary of our text corpus
rather than only food items. Therefore, this in-
formation can be considered as a generalization
of all contextual words. Such type of informa-
tion has been shown to be useful for named-entity
recognition (Turian et al., 2010) and relation ex-
traction (Plank and Moschitti, 2013).
For syntactic parsing, Stanford Parser (Rafferty
and Manning, 2008) was used. For Brown cluster-
ing, the SRILM-toolkit (Stolcke, 2002) was used.
Following Turian et al. (2010), we induced 1000
clusters (from our domain-specific corpus ?2.1).
4.2.1 Why should food categories be helpful
for relation extraction?
All relation types we consider comprise pairs of
two food items which makes these relation types
likely to be confused. Contextual information may
be used for disambiguation but there may also be
frequent contexts that are not sufficiently informa-
tive. For example, 25% of the instances of Ingre-
PLAIN +POSTP
Task Corpus graph Acc F1 Acc F1
Food Type
Wikipedia X 40.3 49.4 61.4 59.8
chefkoch.de X 65.8 71.0 80.2 77.7
Dish
Wikipedia X 50.4 53.1 75.4 71.1
chefkoch.de X 66.5 71.3 83.0 80.1
Table 11: Comparison of Wikipedia and domain-
specific corpus as a source for the similarity graph.
dientOf follow the lexical pattern food item
1
with
food item
2
(1). However, the same pattern also
covers 15% of the instances of SuitsTo (2).
(1) We had a stew with red lentils. (Relation: IngredientOf)
(2) We had salmon with broccoli. (Relation: SuitsTo)
The food type information we learned from our
text corpus might tell us which of the food items
are dishes. Only in (1), there is a dish, i.e. stew.
So, one may infer that the presence of dishes is
indicative of IngredientOf rather than SuitsTo.
food item
1
and food item
2
is another ambigu-
ous context. It cannot only be observed with the
relation SuitsTo, as in (3) (66% of all instantia-
tions of that pattern), but also SubstitutedBy (20%
of all mentions of that relation match that pattern),
as in (4). For SuitsTo, two food items that belong
to two different classes (e.g. MEAT and STARCH
or MEAT and VEGE) are quite characteristic. For
SubstitutedBy, the two food items are very often of
the same category of the Food Guide Pyramid.
(3) I very often eat fish and chips. (Relation: SuitsTo)
(4) For these types of dishes you can offer both Burgundy wine and
Champagne. (Relation: SubstitutedBy)
Since the second ambiguous context involves
the two general relation types SuitsTo and Substi-
tutedBy, resolving this ambiguity with automati-
cally induced type information has some signifi-
cance for other domains. In particular, for other
life-style domains, domain-specific type informa-
tion could be obtained following our method from
?3.1. The disambiguation rule that two entities of
the same type imply SubstitutedBy otherwise they
imply SuitsTo should also be widely applicable.
4.2.2 Results
Table 14 displays the performance of the different
feature sets for relation extraction. The features
designed from graph-based induction (i.e. graph)
work slightly better than GermaNet. The perfor-
mance of patt is not impressively high. However,
one should consider that patt can be used directly
without a supervised classifier (as each pattern is
679
Relation Description Example Freq. Perc.
SuitsTo food items that are typically consumed together My kids love the simple combination of fish fingers
with mashed potatoes.
633 42.20
SubstitutedBy similar food items commonly consumed in the same situations We usually buy margarine instead of butter. 336 22.40
IngredientOf ingredient of a particular dish Falafel is made of chickpeas. 246 16.40
Other other relation or co-occurrence of food items are co-incidental On my shopping list, I?ve got bread, cauliflower, ... 285 19.00
Table 12: The different relation types and their respective frequency on our dataset.
Features Description
patt lexical surface patterns used in Wiegand et al. (2012b)
word bag-of-words features: all words within the sentence
brown features using Brown clustering: all features from word but
words are replaced by induced clusters
pos part-of-speech sequence between target food items and tags
of the words immediately preceding and following them
synt path from syntactic parse tree from first target food item to
second target food item
conj conjunctive features: patt with brown classes of target food
items; pos sequence with brown classes of target food items;
synt with brown classes of target food items
graph semantic food information induced by graph optimization
(config.: 10-PROTO(+POSTP) and RAND-100(+POSTP))
germanet semantic food information derived from (plain) GermaNet
Table 13: Description of the feature set.
designed for a particular relation type, one can
read off from the matching pattern which class is
predicted). word is slightly better but, unlike patt,
it is dependent on supervised learning.
The only feature that individually manages to
significantly outperform word is graph. The tra-
ditional features (i.e. pos, synt and brown) only
produce some mild improvement when added
jointly to word along some conjunctive fea-
tures. When graph is added to this feature set
(i.e. word+patt+pos+synt+brown+conj), we ob-
tain another significant improvement. In con-
clusion, the information we induced from our
domain-specific corpus cannot be obtained by
other NLP-features, including other state-of-the-
art induction methods such as Brown clustering.
5 Related Work
While many of the previous works on noun catego-
rization also address the task of hypernym classifi-
cation (Hearst, 1992; Caraballo, 1999; Widdows,
2003; Kozareva et al., 2008; Huang and Riloff,
2010; Lenci and Benotto, 2012) and some include
examples involving food items (Widdows and
Dorow, 2002; Cederberg and Widdows, 2003),
only van Hage et al. (2005) and van Hage et al.
(2006) specifically focus on the classification of
food items. van Hage et al. (2005) deal with on-
tology mapping whereas van Hage et al. (2006)
explore part-whole relations.
Features Acc Prec Rec F1
germanet 45.3 41.3 37.2 37.3
graph 46.0 39.4 39.7 38.6
patt 59.8 49.8 41.1 38.7
word 60.1 56.9 54.5 55.1
word+patt 60.3 57.3 54.9 55.5
word+brown 59.5 56.1 54.6 54.9
word+synt 60.3 57.7 55.4 56.0
word+pos 59.8 56.6 54.6 55.1
word+germanet 61.3 58.6 56.0 56.7
word+graph 62.9 59.2 57.6 58.1?
word+patt+brown+synt+pos 60.4 57.3 56.2 56.5
word+patt+brown+synt+pos+conj 61.7 59.0 57.8 58.2?
word+patt+brown+synt+pos+conj+germanet 63.1 60.2 58.6 59.1?
word+patt+brown+synt+pos+conj+graph 64.7 62.1 60.3 60.9??
statistical significance testing (paired t-test): better than word ? at p < 0.1/
? at p < 0.05; ? better than word+patt+brown+synt+pos+conj at p < 0.05
Table 14: Comparison of various features (Ta-
ble 13) for (unrestricted) relation extraction.
The task of data-driven lexicon expansion has
also been explored before (Kanayama and Na-
sukawa, 2006; Das and Smith, 2012), however,
our paper presents the first attempt to carry out
a comprehensive categorization for the food do-
main. For the first time, we also show that type
information can effectively improve the extraction
of very common relations. For the twitter domain,
the usage of type information based on cluster-
ing has already been found effective for supervised
learning (Bergsma et al., 2013).
6 Conclusion
We presented an induction method to assign se-
mantic information to food items. We considered
two types of categorizations being food-type infor-
mation and information about whether a food item
is composite or not. The categorization is induced
by graph-based optimization applied on a large
unlabeled domain-specific text corpus. We pro-
duce categorizations that outperform a manually
compiled resource. The usage of such a domain-
specific corpus based on a pattern-based represen-
tation is vital and largely outperforms other text
corpora or a distributional representation. The in-
duced knowledge improves relation extraction.
680
Acknowledgements
This work was performed in the context of the Software-
Cluster project SINNODIUM. Michael Wiegand was funded
by the German Federal Ministry of Education and Research
(BMBF) under grant no. 01IC12SO1X. Benjamin Roth is
a recipient of the Google Europe Fellowship in Natural Lan-
guage Processing, and this research is supported in part by
this Google Fellowship. The authors would like to thank
Stephanie Ko?ser for annotating the dataset presented in this
paper.
References
Mikhail Belkin and Partha Niyogi. 2004. Semi-
supervised learning on Riemannian manifolds. Ma-
chine Learning, 56(1-3):209?239.
Shane Bergsma, Mark Dredze, Benjamin Van
Durme, Theresa Wilson, and David Yarowsky.
2013. Broadly Improving User Classification
via Communication-Based Name and Location
Clustering on Twitter. In Proceedings of the Human
Language Technology Conference of the North
American Chapter of the ACL (HLT/NAACL), pages
1010?1019, Atlanta, GA, USA.
Peter F. Brown, Peter V. deSouza, Robert L. Mer-
cer, Vincent J. Della Pietra, and Jenifer C. Lai.
1992. Class-based n-gram models of natural lan-
guage. Computational Linguistics, 18:467?479.
Sharon A. Caraballo. 1999. Automatic construction
of a hypernym-labeled noun hierarchy from text. In
Proceedings of the Annual Meeting of the Associ-
ation for Computational Linguistics (ACL), pages
120?126, College Park, MD, USA.
Scott Cederberg and Dominic Widdows. 2003. Us-
ing LSA and Noun Coordination Information to Im-
prove the Precision and Recall of Automatic Hy-
ponymy Extraction. In Proceedings of the Confer-
ence on Computational Natural Language Learn-
ing (CoNLL), pages 111?118, Edmonton, Alberta,
Canada.
Dipanjan Das and Noah A. Smith. 2012. Graph-
Based Lexicon Expansion with Sparsity-Inducing
Penalties. In Proceedings of the Human Lan-
guage Technology Conference of the North Ameri-
can Chapter of the ACL (HLT/NAACL), pages 677?
687, Montre?al, Quebec, Canada.
Birgit Hamp and Helmut Feldweg. 1997. GermaNet -
a Lexical-Semantic Net for German. In Proceedings
of ACL workshop Automatic Information Extraction
and Building of Lexical Semantic Resources for NLP
Applications, pages 9?15, Madrid, Spain.
Marti A. Hearst. 1992. Automatic Acquisition of
Hyponyms from Large Text Corpora. In Pro-
ceedings of the International Conference on Com-
putational Linguistics (COLING), pages 539?545,
Nantes, France.
Ruihong Huang and Ellen Riloff. 2010. Inducing
Domain-specific Semantic Class Taggers from (al-
most) Nothing. In Proceedings of the Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 275?285, Uppsala, Sweden.
Thorsten Joachims. 1999. Making Large-Scale SVM
Learning Practical. In B. Scho?lkopf, C. Burges,
and A. Smola, editors, Advances in Kernel Meth-
ods - Support Vector Learning, pages 169?184. MIT
Press.
Hiroshi Kanayama and Tetsuya Nasukawa. 2006.
Fully Automatic Lexicon Expansion for Domain-
oriented Sentiment Analysis. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 355?363, Syd-
ney, Australia.
Zornitsa Kozareva, Ellen Riloff, and Eduard Hovy.
2008. Semantic Class Learning from the Web
with Hyponym Pattern Linkage Graphs. In Pro-
ceedings of the Annual Meeting of the Association
for Computational Linguistics (ACL), pages 1048?
1056, Columbus, OH, USA.
J. Richard Landis and Gary G. Koch. 1977. The
Measurement of Observer Agreement for Categor-
ical Data. Biometrics, 33(1):159?174.
Alessandro Lenci and Guilia Benotto. 2012. Identi-
fying hypernyms in distributional semantic spaces.
In Proceedings of the Joint Conference on Lexical
and Computational Semantics (*SEM), pages 75?
79, Montre?al, Quebec, Canada.
Dekang Lin. 1998. Automatic Retrieval and Cluster-
ing of Similar Words. In Proceedings of the Annual
Meeting of the Association for Computational Lin-
guistics and International Conference on Computa-
tional Linguistics (ACL/COLING), pages 768?774,
Montreal, Quebec, Canada.
George Miller, Richard Beckwith, Christiane Fell-
baum, Derek Gross, and Katherine Miller. 1990.
Introduction to WordNet: An On-line Lexical
Database. International Journal of Lexicography,
3:235?244.
Barbara Plank and Alessandro Moschitti. 2013. Em-
bedding Semantic Similarity in Tree Kernels for Do-
main Adapation of Relation Extraction. In Pro-
ceedings of the Annual Meeting of the Association
for Computational Linguistics (ACL), pages 1498?
1507, Sofia, Bulgaria.
Anna Rafferty and Christopher D. Manning. 2008.
Parsing Three German Treebanks: Lexicalized and
Unlexicalized Baselines. In Proceedings of the ACL
Workshop on Parsing German (PaGe), pages 40?46,
Columbus, OH, USA.
681
Ellen Riloff and Jessica Shepherd. 1997. A
Corpus-Based Approach for Building Semantic
Lexicons. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 117?124, Providence, RI, USA.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2004.
Learning Syntactic Patterns for Automatic Hyper-
nym Discovery. In Advances in Neural Informa-
tion Processing Systems (NIPS), Vancouver, British
Columbia, Canada.
Andreas Stolcke. 2002. SRILM - An Extensible Lan-
guage Modeling Toolkit. In Proceedings of the In-
ternational Conference on Spoken Language Pro-
cessing (ICSLP), pages 901?904, Denver, CO, USA.
Joseph Turian, Lev Ratinov, and Yoshua Bengio.
2010. Word Representations: A Simple and General
Method for Semi-supervised Learning. In Proceed-
ings of the Annual Meeting of the Association for
Computational Linguistics (ACL), pages 384?394,
Uppsala, Sweden.
Human Nutrition Information Service U.S. Depart-
ment of Agriculture. 1992. The Food Guide Pyra-
mid. Home and Garden Bulletin 252, Washington,
D.C., USA.
Willem Robert van Hage, Sophia Katrenko, and Guus
Schreiber. 2005. A Method to Combine Linguis-
tic Ontology-Mapping Techniques. In Proceedings
of International Semantic Web Conference (ISWC),
pages 732 ? 744, Galway, Ireland. Springer.
Willem Robert van Hage, Hap Kolb, and Guus
Schreiber. 2006. A Method for Learning Part-
Whole Relations. In Proceedings of International
Semantic Web Conference (ISWC), pages 723 ? 735,
Athens, GA, USA. Springer.
Ulrike von Luxburg. 2007. A Tutorial on Spectral
Clustering. Statistics and Computing, 17:395?416.
Julie Weeds, David Weir, and Diana McCarthy. 2004.
Characterising Measures of Lexical Distributional
Similarity. In Proceedings of the International Con-
ference on Computational Linguistics (COLING),
pages 1015?1021, Geneva, Switzerland.
Dominic Widdows and Beate Dorow. 2002. A
Graph Model for Unsupervised Lexical Acquisition.
In Proceedings of the International Conference on
Computational Linguistics (COLING), pages 1093?
1099, Taipei, Taiwan.
Dominic Widdows. 2003. Unsupervised methods for
developing taxonomies by combining syntactic and
statistical information. In Proceedings of the Hu-
man Language Technology Conference of the North
American Chapter of the ACL (HLT/NAACL), pages
197?204, Edmonton, Alberta, Canada.
Michael Wiegand, Benjamin Roth, and Dietrich
Klakow. 2012a. Knowledge Acquisition with Nat-
ural Language Processing in the Food Domain: Po-
tential and Challenges. In Proceedings of the ECAI-
Workshop on Cooking with Computers (CWC),
pages 46?51, Montpellier, France.
Michael Wiegand, Benjamin Roth, and Dietrich
Klakow. 2012b. Web-based Relation Extraction
for the Food Domain. In Proceedings of the In-
ternational Conference on Applications of Natu-
ral Language Processing to Information Systems
(NLDB), pages 222?227, Groningen, the Nether-
lands. Springer.
Michael Wiegand, Benjamin Roth, Eva Lasarcyk,
Stephanie Ko?ser, and Dietrich Klakow. 2012c. A
Gold Standard for Relation Extraction in the Food
Domain. In Proceedings of the Conference on
Language Resources and Evaluation (LREC), pages
507?514, Istanbul, Turkey.
Ichiro Yamada, Kentaro Torisawa, Jun?ichi Kazama,
Kow Kuroda, Masaki Murata, Stijn De Saeger, Fran-
cis Bond, and Asuka Sumida. 2009. Hypernym Dis-
covery Based on Distributional Similarity and Hi-
erarchical Structures. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 929?927, Singapore.
Dengyong Zhou, Olivier Bousquet, Thomas Navin Lal,
Jason Weston, and Bernhard Scho?lkopf. 2004.
Learning with Local and Global Consistency. In
Advances in Neural Information Processing Systems
(NIPS), Vancouver and Whistler, British Columbia,
Canada.
682
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 117?122,
Gothenburg, Sweden, April 26-30 2014. c?2014 Association for Computational Linguistics
Comparing methods for deriving intensity scores for adjectives
Josef Ruppenhofer?, Michael Wiegand?, Jasper Brandes?
?Hildesheim University
Hildesheim, Germany
{ruppenho|brandesj}@uni-hildesheim.de
?Saarland University
Saarbru?cken, Germany
michael.wiegand@lsv.uni-saarland.de
Abstract
We compare several different corpus-
based and lexicon-based methods for the
scalar ordering of adjectives. Among
them, we examine for the first time a low-
resource approach based on distinctive-
collexeme analysis that just requires a
small predefined set of adverbial modi-
fiers. While previous work on adjective in-
tensity mostly assumes one single scale for
all adjectives, we group adjectives into dif-
ferent scales which is more faithful to hu-
man perception. We also apply the meth-
ods to both polar and non-polar adjectives,
showing that not all methods are equally
suitable for both types of adjectives.
1 Introduction
Ordering adjectives by strength (e.g. good < great
< excellent) is a task that has recently received
much attention due to the central role of intensity
classification in sentiment analysis. However, the
need to assess the relative strength of adjectives
also applies to non-polar adjectives. We are thus
interested in establishing prior or lexical intensity
scores and rankings for arbitrary sets of adjectives
that evoke the same scale.1 We do not address con-
textualized intensity, i.e. the fact that e.g. negation
and adverbs such as very or slightly impact the per-
ceived intensity of adjectives.
We work with four scales of adjectives (cf. Ta-
ble 1). Our polar adjectives include 29 adjectives
referring to quality and 18 adjectives relating to
intelligence. Our non-polar adjectives include 8
dimensional adjectives denoting size and 22 de-
noting duration. The adjectives are taken, in part,
from FrameNet?s (Baker et al., 1998) frames for
1As there has been previous work on how to group adjec-
tives into scales (Hatzivassiloglou and McKeown, 1993), we
consider this grouping as given.
DESIRABILITY, MENTAL PROPERTY, SIZE and
DURATION DESCRIPTION. These scales are used
because they are prototypical and have multiple
members on the positive and negative half-scales.
We evaluate several corpus- and resource-based
methods that have been used to assign intensity
scores to adjectives. We compare them to a new
corpus-based method that is robust and of low
complexity, and which directly uses information
related to degree modification of the adjectives to
be orderered. It rests on the observation that ad-
jectives with different types of intensities co-occur
with different types of adverbial modifiers.2
POLAR ADJECTIVES
Intelligence Adjs. Intensity Level
brilliant very high positive
ingenious high positive
brainy, intelligent medium positive
smart low positive
bright very low positive
daft very low negative
foolish low negative
inane lower medium negative
dim upper medium negative
dim-witted, dumb, mindless high negative
brainless, idiotic, imbecillic, moronic, stupid very high negative
Quality Adjs. Intensity Level
excellent, extraordinary, first-rate, great, outstand-
ing, super, superb, superlative, tip-top, top-notch
very high positive
good high positive
decent upper medium positive
fine, fair lower medium positive
okay, average low positive
so-so very low positive
mediocre very low negative
second-rate, substandard low negative
inferior lower medium negative
bad, crappy, lousy, poor, third-rate medium negative
rotten upper medium negative
awful high negative
shitty very high negative
DIMENSIONAL ADJECTIVES
Size Adjs. Intensity Level
colossal, enormous, gargantuan, giant, gigantic, gi-
normous, humongous
high positive
big, huge, immense, large, oversize, oversized, vast medium positive
outsize, outsized low positive
diminutive, little, puny, small low negative
tiny medium negative
microscopic high negative
Duration Adjs. Intensity Level
long high positive
lengthy medium positive
extended low positive
momentaneous low negative
brief, fleeting, momentary medium negative
short high negative
Table 1: Adjectives used grouped by human gold
standard intensity classes
2The ratings we collected and our scripts are avail-
able at www.uni-hildesheim.de/ruppenhofer/
data/DISA_data.zip.
117
2 Data and resources
Table 2 gives an overview of the different corpora
and resources that we use to produce the different
scores and rankings that we want to compare. The
corpora and ratings will be discussed alongside the
associated experimental methods in ?4.1 and ?4.2.
Corpora Tokens Reference
BNC ?112 M (Burnard, 2007)
LIU reviews ?1.06 B (Jindal and Liu, 2008)
ukWaC ?2.25 B (Baroni et al., 2009)
Resources Entries Reference
Affective norms ?14 K (Warriner et al., 2013)
SoCAL ? 6.5 K (Taboada et al., 2011)
SentiStrength ? 2.5 K (Thelwall et al., 2010)
Table 2: Corpora and resources used
3 Gold standard
We collected human ratings for our four sets of ad-
jectives. All items were rated individually, in ran-
domized order, under conditions that minimized
bias. Participants were asked to use a horizontal
slider, dragging it in the desired direction, repre-
senting polarity, and releasing the mouse at the de-
sired intensity, ranging from ?100 to +100 .
Through Amazon Mechanical Turk (AMT), we
recruited subjects with the following qualifica-
tions: US residency, a HIT-approval rate of at least
96% (following Akkaya et al. (2010)), and 500
prior completed HITs. We collected 20 ratings for
each item but had to exclude some participants?
answers as unusable, which reduced our sample to
17 subjects for some items. In the raw data, all ad-
jectives had different mean ratings and their stan-
dard deviations overlapped. We therefore trans-
formed the data into sets of equally strong adjec-
tives as follows. For a given pair of adjectives of
identical polarity, we counted how many partici-
pants rated adjective A more intense than adjective
B; B more intense than A; or A as intense as B.
Whenever a simple majority existed for one of the
two unequal relations, we adopted that as our rela-
tive ranking for the two adjectives.3 The resulting
rankings (intensity levels) are shown in Table 1.
4 Methods
Our methods to determine the intensity of adjec-
tives are either corpus- or lexicon-based.
3In our data, there was no need to break circular rankings,
so we do not consider this issue here.
4.1 Corpus-based methods
Our first method, distinctive-collexeme analysis
(Collex) (Gries and Stefanowitsch, 2004) assumes
that adjectives with different types of intensities
co-occur with different types of adverbial modi-
fiers (Table 3). End-of-scale modifiers such as ex-
tremely or absolutely target adjectives with a par-
tially or fully closed scale, such as brilliant or out-
standing, which occupy extreme positions on the
intensity scale. ?Normal? degree modifiers such
as very or rather target adjectives with an open
scale structure (in the sense of Kennedy and Mc-
Nally (2005)), such as good or decent, which oc-
cupy non-extreme positions.
To determine an adjective?s preference for one
of the two constructions, the Fisher exact test
(Pedersen, 1996) is used. It makes no distribu-
tional assumptions and does not require a min-
imum sample size. The direction in which ob-
served values differ from expected ones indicates a
preference for one construction over the other and
the p-values are taken as a measure of the prefer-
ence strength. Our hypothesis is that e.g. an adjec-
tive A with greater preference for the end-of-scale
construction than adjective B has a greater inher-
ent intensity than B. We ran distinctive-collexeme
analysis on both the ukWaC and the BNC. We re-
fer to the output as Collex
ukWaC
and Collex
BNC
.
Note that this kind of method has not yet been ex-
amined for automatic intensity classification.
end-of-scale ?normal?
100%, fully, totally, absolutely,
completely, perfectly, entirely,
utterly, almost, partially, half,
mostly
all, as, awfully, enough, extremely,
fairly, highly, how, least, less, much,
pretty, quite, rather, so, somewhat,
sort of, terribly, too, very, well
Table 3: Domain independent degree modifiers (3
most freq. terms in the BNC; 3 most freq. terms
in the ukWaC)
Another corpus-based method we consider em-
ploys Mean star ratings (MeanStar) from prod-
uct reviews as described by Rill et al. (2012). Un-
like Collex, this method uses no linguistic prop-
erties of the adjectives themselves. Instead, it de-
rives intensity from the star rating scores that re-
viewers (manually) assign to reviews. We count
how many instances of each adjective i (of the set
of adjectives to classify) occur in review titles with
a given star rating (score) S
j
within a review cor-
pus. The intensity score is defined as the weighted
mean of the star ratings SR
i
=
?
n
j=1
S
i
j
n
.
Horn (1976) proposes pattern-based diagnos-
118
Pattern Any Int. Qual. Size Dur.
X or even Y 4118 1 34 9 3
X if not Y 3115 1 0 29 0
be X but not Y 2815 0 74 3 1
not only X but Y 1114 0 3 0 0
X and in fact Y 45 0 0 0 0
not X, let alone Y 4 0 0 0 0
not Y, not even X 4 0 1 0 0
Table 4: Phrasal patterns in the ukWaC
tics for acquiring information about the scalar
structure of adjectives. This was validated on ac-
tual data by Sheinman and Tokunaga (2009). A
pattern such as not just/only X but Y implies that
[Y] must always be stronger than [X] (as in It?s
not just good but great.).
The pattern-based approach has a severe cover-
age problem. Table 4 shows the results for 7 com-
mon phrasal patterns in the larger of our two cor-
pora, the ukWaC. The slots in the patterns are typ-
ically not filled by adjectives from the same scale.
For example, the most frequent pattern X or even
Y has 4118 instances in the ukWaC. Only 34 of
these have quality adjectives in both slots. Though
de Melo and Bansal (2013) have shown that the
coverage problems can be overcome and state-of-
the-art results obtained using web scale data in the
form of Google n-grams, we still set aside this
method here because of its great resource need.
4.2 Manually compiled lexical resources
In addition to the corpus methods, we also con-
sider some manually compiled resources. We want
to know if the polarity and intensity information in
them can be used for ordering polar adjectives.
One resource we consider are the affective rat-
ings (elicited with AMT) for almost 14,000 En-
glish words collected by Warriner et al. (2013).
They include scores of valence (unhappy to
happy), arousal (calm to aroused) and dominance
(in control to controlled) for each word in the list.
This scoring system follows the dimensional the-
ory of emotion by Osgood et al. (1957). We will
interpret each of these dimensions as a separate in-
tensity score, i.e. War
V al
, War
Aro
and War
Dom
.
Beyond Warriner?s ratings, we consider the two
polarity lexicons SentiStrength (Thelwall et al.,
2010) and SoCAL (Taboada et al., 2011) which
also assign intensity scores to polar expressions.
5 Experiments
For our evaluation, we compute the similarity be-
tween the gold standard and every other ranking
we are interested in in terms of Spearman?s rank
correlation coefficient (Spearman?s ?).
Polar Dimensional
Data set Intelligence Quality Duration Size
MeanStar 0.886 0.935 0.148 -0.058
SoCAL 0.848 0.953 NA 0.776
SentiStrength 0.874 0.880 NA NA
Collex
ukWaC
0.837 0.806 0.732 0.808
Collex
ukWaC
? 0.845 0.753 0.732 0.940
Collex
BNC
0.834 0.790 0.732 0.733
Collex
BNC
? 0.705 0.643 0.834 0.700
War
V al
0.779 0.916 -0.632 -0.031
War
Aro
0.504 -0.452 0.316 0.717
War
Dom
0.790 0.891 0.632 0.285
Table 5: Spearman rank correlations with the hu-
man gold standard (?: only the 3 most frequent
modifiers are used (see Table 3))
5.1 Data transformation
For the word lists with numeric scores (MeanStar
(?4.1); SentiStrength, SoCAL, War
V al
, War
Aro
and War
Dom
(?4.2)) we did as follows: Adjectives
not covered by the word lists were ignored. Ad-
jectives with equal scores were given tied ranks.
For the experiments involving distinctive
collexeme analysis in our two corpora (?4.1) we
proceeded as follows: The adjectives classified
as distinctive for the end-of-scale modification
constructions were put at the top and bottom of
the ranking according to polarity; the greater the
collostructional strength for the adjective as de-
noted by the p-value, the nearer it is placed to the
top or bottom of the ranking. The adjectives that
are distinctive for the normal degree modification
construction are placed between those adjectives
distinctive for the end-of-scale modification
construction, again taking polarity and collostruc-
tional strength into account. This time, the least
distinctive lemmas for the normal modification
construction come to directly join up with the
least distinctive lemmas for the end-of-scale
construction. In between the normal modifiers,
we place adjectives that have no preference for
one or the other construction, which may result
from non-occurrence in small data sets (see ?5.2).
5.2 Results
The results of the pairwise correlations between
the human-elicited gold standard and the rankings
derived from various methods and resources are
shown in Table 5. For polar adjectives, most rank-
ings correlate fairly well with human judgments.
Warriner?s arousal list, however, performs poorly
on quality adjectives, whereas MeanStar and War-
riner?s dominance and valence lists perform bet-
ter on quality than on intelligence adjectives. For
MeanStar, this does not come as a surprise as qual-
ity adjectives are much more frequent in prod-
119
uct reviews than intelligence adjectives. Overall,
it seems that MeanStar most closely matches the
human judgments that we elicited for the intel-
ligence adjectives. SentiStrength also produces
high scores. However, we do not have full confi-
dence in that result since SentiStrength lacks many
of our adjectives, thus leading to a possibly higher
correlation than would have been achieved if ranks
(scores) had been available for all adjectives.
The picture is very different for the dimensional
(non-polar) adjectives. While Collex still gives
very good results, especially on the ukWaC, the
MeanStar method and most Warriner lists produce
very low positive or even negative correlations.
This shows that estimating the intensity of non-
polar adjectives from metadata or ratings elicited
in terms of affect is not useful. It is much better to
consider their actual linguistic behavior in degree
constructions, which Collex does. SentiStrength
has no coverage for size or duration adjectives.
SoCAL covers 14 of the 22 size adjectives.
Although it never gives the best result, Collex
produces stable results across both corpora and
the four scales. It also requires the least human
effort by far. While all other rankings are pro-
duced with the help of heavy human annotation
(even MeanStar is completely dependent on manu-
ally assigned review scores), one has only to spec-
ify some domain-independent degree and end-of-
scale modifiers. Table 5 also shows that normally
a larger set of modifiers is necessary: only consid-
ering the 3 most frequent terms (Table 3) results in
a notably reduced correlation. As there is no con-
sistent significant difference between Collex
BNC
and Collex
ukWaC
even though the ukWaC is 20
times larger than the BNC (Table 2), we may
conclude that the smaller size of the BNC is al-
ready sufficient. This, however, raises the question
whether even smaller amounts of data than the full
BNC could already produce a reasonable intensity
ranking. Figure 1 plots the Spearman correlation
for our adjectives using various sizes of the BNC
corpus.4 It shows that further reducing the size of
the corpus causes some deterioration, most signifi-
cantly on the intelligence adjectives. The counter-
intuitive curve for duration adjectives is explained
as follows. Collex produces ties in the middle of
the scale when data is lacking (see ?5.1). Because
the smallest corpus slices contain no or very few
instances and because the gold standard does in-
4For each size, we average across 10 samples.
 0.6
 0.65
 0.7
 0.75
 0.8
 0.85
 0.9
 0  20  40  60  80  100
Sp
ea
rm
an
?s 
rh
o
% Size of BNC
Intelligence
Quality
Size
Duration
Figure 1: Reducing the size of the BNC
clude several ties, the results for duration adjec-
tives are inflated initially, when data is lacking.
6 Related work
Sentiment analysis on adjectives has been exten-
sively explored in previous work, however, most
work focussed on the extraction of subjective ad-
jectives (Wiebe, 2000; Vegnaduzzo, 2004; Wie-
gand et al., 2013) or on the detection of polar ori-
entation (Hatzivassiloglou and McKeown, 1997;
Kamps et al., 2004; Fahrni and Klenner, 2008).
Intensity can be considered in two ways, as a
contextual strength analysis (Wilson et al., 2004)
or as an out-of-context analysis, as in this paper.
Our main contribution is that we compare sev-
eral classification methods that include a new
effective method based on distinctive-collexeme
analysis requiring hardly any human guidance and
which moreover can solve the problem of intensity
assignment for all, not only polar adjectives.
7 Conclusion
We compared diverse corpus-based and lexicon-
based methods for the intensity classification of
adjectives. Among them, we examined for the first
time an approach based on distinctive-collexeme
analysis. It requires only a small predefined set
of adverbial modifiers and relies only on infor-
mation about individual adjectives rather than co-
occurrences of adjectives within patterns. As a re-
sult, it can be used with far less data than e.g. the
Google n-grams provide. Unlike the mean star ap-
proach, it needs no extrinsic meta-data and it can
handle both polar and non-polar adjectives. Ac-
cordingly, it appears to be very promising for cases
where only few resources are available and as a
source of evidence to be used in hybrid methods.
120
Acknowledgments
Michael Wiegand was funded by the German Fed-
eral Ministry of Education and Research (BMBF)
under grant no. 01IC12SO1X. The authors would
like to thank Maite Taboada for providing her sen-
timent lexicon (SoCAL) to be used for the experi-
ments presented in this paper.
References
Cem Akkaya, Alexander Conrad, Janyce Wiebe, and
Rada Mihalcea. 2010. Amazon Mechanical Turk
for Subjectivity Word Sense Disambiguation. In
NAACL-HLT 2010 Workshop on Creating Speech
and Language Data With Amazon?s Mechanical
Turk, pages 195?203, Los Angeles, CA, USA.
Collin F. Baker, Charles J. Fillmore, and John B.
Lowe. 1998. The Berkeley Framenet Project.
In Proceedings of the International Conference
on Computational Linguistics and Annual Meeting
of the Association for Computational Linguistics
(COLING/ACL), pages 86?90, Montre?al, Quebec,
Canada.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi,
and Eros Zanchetti. 2009. The WaCky Wide Web:
A Collection of Very Large Linguistically Processed
Web-Crawled Corpora. Language Resources and
Evaluation, 43(3):209?226.
Lou Burnard, 2007. Reference Guide for the British
National Corpus. Research Technologies Service
at Oxford University Computing Services, Oxford,
UK.
Gerard de Melo and Mohit Bansal. 2013. Good, Great,
Excellent: Global Inference of Semantic Intensities.
Transactions of the Association for Computational
Linguistics, 1:279?290.
Angela Fahrni and Manfred Klenner. 2008. Old Wine
or Warm Beer: Target Specific Sentiment Analysis
of Adjectives. In Proceedings of the Symposium on
Affective Language in Human and Machine, pages
60?63, Aberdeen, Scotland, UK.
Stefan Th. Gries and Anatol Stefanowitsch. 2004.
Extending collostructional analysis: a corpus-based
perspective on ?alternations?. International Journal
of Corpus Linguistics, 9(1):97?129.
Vasileios Hatzivassiloglou and Kathleen McKeown.
1993. Towards the Automatic Identification of Ad-
jectival Scales: Clustering Adjectives According to
Meaning. In Proceedings of the Annual Meeting
of the Association for Computational Linguistics
(ACL), pages 172?182, Columbus, OH, USA.
Vasileios Hatzivassiloglou and Kathleen R. McKeown.
1997. Predicting the Semantic Orientation of Ad-
jectives. In Proceedings of the Conference on Euro-
pean Chapter of the Association for Computational
Linguistics (EACL), pages 174?181, Madrid, Spain.
Laurence Robert Horn. 1976. On the Semantic Prop-
erties of Logical Operators in English. Indiana Uni-
versity Linguistics Club.
Nitin Jindal and Bing Liu. 2008. Opinion Spam
and Analysis. In Proceedings of the international
conference on Web search and web data mining
(WSDM), pages 219?230, Palo Alto, USA.
Jaap Kamps, M.J. Marx, Robert J. Mokken, and
Maarten De Rijke. 2004. Using Wordnet to Mea-
sure Semantic Orientations of Adjectives. In Pro-
ceedings of the Conference on Language Resources
and Evaluation (LREC), pages 1115?1118, Lisbon,
Portugal.
Christopher Kennedy and Louise McNally. 2005.
Scale Structure, Degree Modification, and the
Semantics of Gradable Predicates. Language,
81(2):345?338.
Charles E. Osgood, George Suci, and Percy Tannen-
baum. 1957. The Measurement of Meaning. Uni-
versity of Illinois Press.
Ted Pedersen. 1996. Fishing for exactness. In
Proceedings of the South-Central SAS Users Group
Conference, Austin, TX, USA.
Sven Rill, Johannes Drescher, Dirk Reinel, Joerg
Scheidt, Oliver Schuetz, Florian Wogenstein, and
Daniel Simon. 2012. A Generic Approach to Gen-
erate Opinion Lists of Phrases for Opinion Mining
Applications. In Proceedings of the KDD-Workshop
on Issues of Sentiment Discovery and Opinion Min-
ing (WISDOM), Beijing, China.
Vera Sheinman and Takenobu Tokunaga. 2009. Ad-
jScales: Differentiating between Similar Adjectives
for Language Learners. CSEDU, 1:229?235.
Maite Taboada, Julian Brooke, Milan Tofiloski, Kim-
berly Voll, and Manfred Stede. 2011. Lexicon-
Based Methods for Sentiment Analysis. Computa-
tional Linguistics, 37(2):267 ? 307.
Mike Thelwall, Kevan Buckley, Georgios Paltoglou,
and Di Cai. 2010. Sentiment Strength Detec-
tion in Short Informal Text. Journal of the Ameri-
can Society for Information Science and Technology,
61(12):2544?2558.
Stefano Vegnaduzzo. 2004. Acquisition of Subjective
Adjectives with Limited Resources. In Proceedings
of the AAAI Spring Symposium on Exploring Atti-
tude and Affect in Text: Theories and Applications,
Stanford, CA, USA.
Amy Warriner, Victor Kuperman, and Marc Brysbaert.
2013. Norms of valence, arousal, and dominance for
13,915 english lemmas. Behavior Research Meth-
ods, Online First:1?17.
Janyce M. Wiebe. 2000. Learning Subjective Adjec-
tives from Corpora. In Proceedings of the National
Conference on Artificial Intelligence (AAAI), pages
735?740, Austin, TX, USA.
121
Michael Wiegand, Josef Ruppenhofer, and Dietrich
Klakow. 2013. Predicative Adjectives: An Unsu-
pervised Criterion to Extract Subjective Adjectives.
In Proceedings of the Human Language Technology
Conference of the North American Chapter of the
ACL (HLT/NAACL), pages 534?539, Atlanta, GA,
USA.
Theresa Wilson, Janyce Wiebe, and Rebecca Hwa.
2004. Just how mad are you? Finding strong and
weak opinion clauses. In Proceedings of the Na-
tional Conference on Artificial Intelligence (AAAI),
pages 761?767, San Jose, CA, USA.
122
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 795?803,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Convolution Kernels for Opinion Holder Extraction
Michael Wiegand and Dietrich Klakow
Spoken Language Systems
Saarland University
D-66123 Saarbru?cken, Germany
{Michael.Wiegand|Dietrich.Klakow}@lsv.uni-saarland.de
Abstract
Opinion holder extraction is one of the impor-
tant subtasks in sentiment analysis. The ef-
fective detection of an opinion holder depends
on the consideration of various cues on vari-
ous levels of representation, though they are
hard to formulate explicitly as features. In this
work, we propose to use convolution kernels
for that task which identify meaningful frag-
ments of sequences or trees by themselves.
We not only investigate how different levels
of information can be effectively combined
in different kernels but also examine how the
scope of these kernels should be chosen. In
general relation extraction, the two candidate
entities thought to be involved in a relation are
commonly chosen to be the boundaries of se-
quences and trees. The definition of bound-
aries in opinion holder extraction, however, is
less straightforward since there might be sev-
eral expressions beside the candidate opinion
holder to be eligible for being a boundary.
1 Introduction
In recent years, there has been a growing interest
in the automatic detection of opinionated content
in natural language text. One of the more impor-
tant tasks in sentiment analysis is the extraction of
opinion holders. Opinion holder extraction is one
of the critical components of an opinion question-
answering system (i.e. systems which automatically
answer opinion questions, such as ?What does [X]
like about [Y]??). Such systems need to be able to
distinguish which entities in a candidate answer sen-
tence are the sources of opinions (= opinion holder)
and which are the targets.
On other NLP tasks, in particular, on relation extrac-
tion, there has been much work on convolution ker-
nels, i.e. kernel functions exploiting huge amounts
of features without an explicit feature representa-
tion. Previous research on that task has shown that
convolution kernels, such as sequence and tree ker-
nels, are quite effective when compared to manual
feature engineering (Moschitti, 2008; Bunescu and
Mooney, 2005; Nguyen et al, 2009). In order to
effectively use convolution kernels, it is often nec-
essary to choose appropriate substructures of a sen-
tence rather than represent the sentence as a whole
structure (Bunescu and Mooney, 2005; Zhang et al,
2006; Moschitti, 2008). As for tree kernels, for ex-
ample, one typically chooses the syntactic subtree
immediately enclosing two entities potentially ex-
pressing a specific relation in a given sentence. The
opinion holder detection task is different from this
scenario. There can be several cues within a sen-
tence to indicate the presence of a genuine opinion
holder and these cues need not be member of a par-
ticular word group, e.g. they can be opinion words
(see Sentences 1-3), communication words, such as
maintained in Sentence 2, or other lexical cues, such
as according in Sentence 3.
1. The U.S. commanders consideropinion the prisoners to be un-
lawful combatantsopinion as opposed to prisoners of war.
2. During the summit, Koizumi maintainedcommunication a
clear-cut collaborative stanceopinion towards the U.S. and em-
phasized that the President was objectiveopinion and circum-
spect.
3. Accordingcue to Fernandez, it was the worst mistakeopinion in
the history of the Argentine economy.
795
Thus, the definition of boundaries of the structures
for the convolution kernels is less straightforward in
opinion holder extraction.
The aim of this paper is to explore in how far convo-
lution kernels can be beneficial for effective opinion
holder detection. We are not only interested in how
far different kernel types contribute to this extraction
task but we also contrast the performance of these
kernels with a manually designed feature set used
as a standard vector kernel. Finally, we also exam-
ine the effectiveness of expanding word sequences
or syntactic trees by additional prior knowledge.
2 Related Work
Choi et al (2005) examine opinion holder extraction
using CRFs with various manually defined linguis-
tic features and patterns automatically learnt by the
AutoSlog system (Riloff, 1996). The linguistic fea-
tures focus on named-entity information and syntac-
tic relations to opinion words. In this paper, we use
very similar settings. The features presented in Kim
and Hovy (2005) and Bloom et al (2007) resemble
very much Choi et al (2005). Bloom et al (2007)
also consider communication words to be predictive
cues for opinion holders.
Kim and Hovy (2006) and Bethard et al (2005) ex-
plore the usefulness of semantic roles provided by
FrameNet (Fillmore et al, 2003) for both opinion
holder and opinion target extraction. Due to data
sparseness, Kim and Hovy (2006) expand FrameNet
data by using an unsupervised clustering algorithm.
Choi et al (2006) is an extension of Choi et al
(2005) in that opinion holder extraction is learnt
jointly with opinion detection. This requires that
opinion expressions and their relations to opinion
holders are annotated in the training data. Seman-
tic roles are also taken as a potential source of in-
formation. In our work, we deliberately work with
minimal annotation and, thus, do not consider any
labeled opinion expressions and relations to opinion
holders in the training data. We exclusively rely on
entities marked as opinion holders. In many practi-
cal situations, the annotation beyond opinion holder
labeling is too expensive.
Complex convolution kernels have been success-
fully applied to various NLP tasks, such as rela-
tion extraction (Bunescu and Mooney, 2005; Zhang
et al, 2006; Nguyen et al, 2009), question an-
swering (Zhang and Lee, 2003; Moschitti, 2008),
and semantic role labeling (Moschitti et al, 2008).
In all these tasks, they offer competitive perfor-
mance to manually designed feature sets. Bunescu
and Mooney (2005) combine different sequence ker-
nels encoding different contexts of candidate en-
tities in a sentence. They argue that several ker-
nels encoding different contexts are more effective
than just using one kernel with one specific context.
We build on that idea and compare various scopes
eligible for opinion holder extraction. Moschitti
(2008) and Nguyen et al (2009) suggest that differ-
ent kinds of information, such as word sequences,
part-of-speech tags, syntactic and semantic informa-
tion should be contained in separate convolution ker-
nels. We also adhere to this notion.
3 Data
As labeled data, we use the sentiment annotation of
the MPQA 2.0 corpus1. Opinion holders are not ex-
plicitly labeled as such. However sources of pri-
vate states and subjective speech events (Wiebe et
al., 2003) are a fairly good approximation of the
task. Previous work (Choi et al, 2005; Kim and
Hovy, 2005; Choi et al, 2006) uses similar approxi-
mations.
4 Method
In this work, we consider all noun phrases (NPs)
as possible candidate opinion holders. Therefore,
the set of all data instances is the set of the NPs
within the MPQA 2.0 corpus. Each NP is labeled
as to whether it is a genuine opinion holder or not.
Throughout this section, we will use Sentence 2
from Section 1 as an example.
4.1 The Different Levels of Representation
Several levels of representation are important for
opinion holder extraction. Table 1 lists all the dif-
ferent levels that are used in this work. Generalized
sequences employ named-entity tags, an OPINION
tag for opinion words and a COMM tag for com-
munication words2. Thus, in a generalized word se-
1www.cs.pitt.edu/mpqa/databaserelease
2Note that all candidate tokens are reduced to one generic
CAND token. Thus, we hope to account for data sparseness in
796
quence (WRDGN ) a word is replaced by a general-
ized token whereas in a generalized part-of-speech
sequence (POSGN ) a part-of-speech tag is replaced.
For augmented constituent trees (CONSTAUG), the
same sources of information are used. The differ-
ence to generalizing sequences is that instead of re-
placing words by generalized tokens, we add a node
in the syntax tree with a generalized token so that it
dominates the pertaining leaf node (see also nodes
marked with AUG in Figure 2). All sources used for
this type of generalization are known to be predictive
for opinion holder classification (Choi et al, 2005;
Kim and Hovy, 2005; Choi et al, 2006; Kim and
Hovy, 2006; Bloom et al, 2007).
Note that the grammatical relation paths, i.e.
GRAMWRD and GRAMPOS , can only be applied
in case there is another expression in the focus in
addition to the candidate of the data instance itself,
e.g. the nearest opinion expression to the candidate.
Section 4.4 explains in detail how this is done.
Predicate-argument structures (PAS) are repre-
sented by PropBank trees (Kingsbury and Palmer,
2002).
4.2 Support Vector Machines and Kernel
Methods
Support Vector Machines (SVMs) are one of the
most robust supervised machine learning techniques
in which training data instances ~x are separated by a
hyperplane H(~x) = ~w ? ~x + b = 0 where w ? Rn
and b ? R. One advantage of SVMs is that ker-
nel methods can be applied which map the data to
other feature spaces in which they can be separated
more easily. Given a feature function ? : O ? R,
where O is the set of the objects, the kernel trick
allows the decision hyperplane to be rewritten as:
H(~x) =
(
?
i=1...l
yi?i~xi
)
? ~x + b =
?
i=1...l
yi?i~xi ? ~x+ b =
?
i=1...l
yi?i? (oi) ? ? (o) + b
where yi is equal to 1 for positive and ?1 for
negative examples, ?i ? R with ?i ? 0, oi?i ?
{1, . . . , l} are the training instances and the product
K(oi, o) = ??(oi) ? ?(o)? is the kernel function as-
sociated with the mapping ?.
case there are several tokens making up the candidate.
4.3 Sequence and Tree Kernels
A sequence kernel (SK) measures the similarity
of two sequences by counting the number of com-
mon subsequences. We use the kernel by Taylor
and Christianini (2004) which has the advantage that
it also considers subsequences of the original se-
quence with some elements missing. The extent of
these gaps in a sequence is suitably reflected by a
weighting function incorporated into the kernel.
Tree kernels (TKs) represent trees by their sub-
structures. The feature space of these substructures,
or fragments, is mapped onto a vector space. The
kernel function computes the similarity of pairs of
trees by counting the number of common fragments.
In this work, we evaluate two tree kernels: Subset
Tree Kernel (STK) (Collins and Duffy, 2002) and
Partial Tree Kernel (PTKbasic) (Moschitti, 2006).
In STK , a tree fragment can be any set of nodes
and edges of the original tree provided that every
node has either all or none of its children. This con-
straint makes that kind of kernel well-suited for con-
stituency trees which have been generated by con-
text free grammars since the constraint corresponds
to the restriction that no grammatical rule must be
broken. For example, STK enforces that a subtree,
such as [VP [VBZ, NP]], cannot be matched with
[VP [VBZ]] since the latter VP node only possesses
one of the children of the former.
PTKbasic is more flexible since the constraint
of STK on nodes is relaxed. This makes this
type of tree kernel less suitable for constituency
trees. We, therefore, apply it only to trees
representing predicate-argument structures (PAS)
(see Figure 1). Note that a data instance is
represented by a set of those structures3 rather
than a single structure. Thus, the actual partial
tree kernel function we use for this task, PTK ,
sums over all possible pairs PASl and PASm of
two data instances xi and xj: PTK(xi, xj) =
?
PASl?xi
?
PASm?xj
PTKbasic(PASl, PASm).
To summarize, Table 2 lists the different kernel
types we use coupled with the suitable levels of rep-
resentation. This choice of pairing has already been
motivated and empirically proven suitable on other
3i.e. all predicate-argument structures of a sentence in which
the head of the candidate opinion holder occurs
797
Type Description Example
WRD sequence of words During the summit , KoizumiCAND maintained a clear-cut
collaborative stance . . .
WRDGN sequence of generalized words During the summit , CAND COMM OPINION . . .
POS part-of-speech sequence IN DET NN PUNC CAND VBD DET JJ JJ NN . . .
POSGN generalized part-of-speech sequence IN DET NN PUNC CAND COMM OPINION . . .
CONST constituency tree see Figure 2 without nodes marked AUG
CONSTAUG augmented constituency tree see Figure 2
GRAMWRD grammatical relation path labels with words KoizumiCAND NSUBJ? maintained DOBJ? stance
GRAMPOS grammatical relation path labels with part-of-speech tags CAND NSUBJ? VBD DOBJ? NN
PAS predicate argument structures see Figure 1(a)
PASAUG augmented predicate argument structures see Figure 1(b)
Table 1: The different levels of representation.
(a) plain
(b) augmented
Figure 1: Predicate-argument structures (PAS).
tasks (Moschitti, 2008; Nguyen et al, 2009).
Type Description Levels of Representation
SK Sequential Kernel WRD(GN) , POS(GN),
GRAMWRD , GRAMPOS
STK Subset Tree Kernel CONST(AUG)
PTK Partial Tree Kernel PAS
V K Vector Kernel not restricted
Table 2: The different types of kernels.
4.4 The Different Scopes
We argue that using the entire word sequence or syn-
tax tree of the sentence in which a candidate opinion
holder is situated to represent a data instance pro-
duces too large structures for a convolution kernel.
Since a classifier based on convolution kernels has
to derive meaningful features by itself, the larger
these structures are, the more likely noise is included
in the model. Previous work in relation extraction
has also shown that the usage of more focused sub-
structures, e.g. the smallest subtree containing the
two candidate entities of a relation, is more effec-
tive (Zhang et al, 2006). Unfortunately, in our task
there is only one explicit entity we know of for each
data instance which is the candidate opinion holder.
However, there are several indicative cues within the
context of the candidate which might be considered
important. We identify three different cues being the
nearest predicate, i.e. full verb or nominalization,
opinion word and communication word4. For each
of these expressions, we define a scope where the
boundaries are the candidate opinion holder and the
pertaining cue. Given these scopes, we can define
resulting subsequences/subtrees and combine them.
We further add two background scopes, one being
the semantic scope of the candidate opinion holder
and the entire sentence. As semantic scope we con-
sider the subclause in which a candidate opinion
holder is situated5 .
Figure 2 illustrates the different scopes. Abbre-
viations are explained in Table 3. As already men-
tioned in Section 4.1 for grammatical relation paths,
a second expression in addition to the candidate
opinion holder is required. These expressions can be
derived from the different scopes, i.e. for PRED it
4These three expressions may coincide but do not have to.
5Typically, the subtree representing a subclause has the clos-
est S node dominating the candidate opinion holder as the root
node and it contains only those nodes from the original sentence
parse which are also dominated by that S node and whose path
to that node does not contain another S node.
798
is the nearest predicate to the candidate, for OP it is
the nearest opinion word and for COMM it is the
nearest communication word. For the background
scopes SEM and SENT , however, there is no sec-
ond expression in focus. Therefore, grammatical re-
lation paths cannot be defined for these scopes.
Type Description
PRED scope with the boundaries being the candidate opinion
holder and the nearest predicate
OP scope with the boundaries being the candidate opinion
holder and nearest opinion word
COMM scope with the boundaries being the candidate opinion
holder and the nearest communication word
SEM semantic scope of the candidate opinion holder, i.e.
subclause containing the candidate
SENT entire sentence in which in the opinion holder occurs
Table 3: The different types of scope.
4.5 Manually Designed Feature Set for a
Standard Vector Kernel
In addition to the different types of convolution ker-
nels, we also define an explicit feature set for a vec-
tor kernel (V K). Many of these features mainly de-
scribe properties of the relation between the candi-
date and the nearest predicate6 since in our initial
experiments the nearest predicate has always been
the strongest cue. Adding these types of features
for other cues, e.g. the nearest opinion or commu-
nication word, only resulted in a decrease in perfor-
mance. Table 4 lists all the features we use. Note
that this manual feature set employs all those sources
of information which are also exploited by the con-
volution kernels. Some of the information contained
in the convolution kernels can, however, only be rep-
resented in a more simplified fashion when using
a manual feature set. For example, the first PAS
in Figure 1(a) is converted to just the pair of pred-
icate and argument representing the candidate (i.e.
REL:maintain A0:Koizumi). The entire PAS is not
used since it would create too sparse features. Con-
volution kernels can cope with fairly complex struc-
tures as input since they internally match substruc-
tures. Manual features are less flexible since they do
not account for partial matches.
6We select the nearest predicate by using the syntactic parse
tree. Thus, we hope to select the predicate which syntactically
headword/governing category of CAND
is CAND capitalized/a person?
is CAND subj|dobj|iobj|pobj of OPINION/COMM?
is CAND preceded by according to? (Choi et al, 2005)
does CAND contain possessive and is followed by OPIN-
ION/COMM? (Choi et al, 2005)
is CAND preceded by by which is attached to OPINION/COMM?
(Choi et al, 2005)
predicate-argument pairs in which CAND occurs
lemma/part-of-speech tag/subcategorization frame/voice of nearest
predicate
is nearest predicate OPINION/COMM?
does CAND precede/follow nearest predicate?
words between nearest predicate and CAND (bag of words)
part-of-speech sequence between nearest predicate and CAND
constituency path/grammatical relation path from predicate to
CAND
Table 4: Manually designed feature set.
5 Experiments
We used 400 documents of the MPQA corpus for
five-fold crossvalidation and 133 documents as a de-
velopment set. We report statistical significance on
the basis of a paired t-test using 0.05 as the signif-
icance level. All experiments were done with the
SVM-Light-TK toolkit7. We evaluated on the basis
of exact phrase matching. We set the trade-off pa-
rameter j = 5 for all feature sets. For the manual
feature set we used a polynomial kernel of third de-
gree. These two critical parameters were tuned on
the development set. As far as the sequence and
tree kernels are concerned, we used the parameter
settings from Moschitti (2008), i.e. ? = 0.4 and
? = 0.4. Kernels were combined using plain sum-
mation. The documents were parsed using the Stan-
ford Parser (Klein and Manning, 2003). Named-
entity information was obtained by the Stanford tag-
ger (Finkel et al, 2005). Semantic roles were ob-
tained by using the parser by Zhang et al (2008).
Opinion expressions were identified using the Sub-
jectivity Lexicon from the MPQA project (Wil-
son et al, 2005). Communication words were ob-
tained by using the Appraisal Lexicon (Bloom et al,
2007). Nominalizations were recognized by looking
relates to the candidate opinion holder.
7available at disi.unitn.it/moschitti
799
Figure 2: Illustration of the different scopes on a CONSTAUG; nodes belonging to the candidate opinion holder are
marked with CAND.
up nouns in NOMLEX (Macleod et al, 1998).
5.1 Notation
Each kernel is represented as a triple
?levelOfRepresentation (Table 1), Scope (Table 3), typeOfKernel
(Table 2)?, e.g. ?CONST, SENT, STK? is a Subset
Tree Kernel of a constituency parse having the
scope of the entire sentence. Note that not all com-
binations of these three parameters are meaningful.
In the following, we will just focus on important
and effective combinations. The kernel composed
of manually designed features is denoted by just
V K . The kernel composed of predicate-argument
structures is denoted by ?PAS, SENT,PTK?.
5.2 Vector Kernel (VK)
The first line in Table 7 displays the result of the
vector kernel using a manually designed feature set.
It should be interpreted as a baseline. Due to the
high class imbalance we will focus on the compari-
son of F(1)-Score throughout this paper rather than
accuracy which is fairly biased on this data set. The
F-Score of this classifier is at 56.16%.
5.3 Sequence Kernels (SKs)
For both sequence and tree kernels we need to find
out what the best scope is, whether it is worthwhile
to combine different scopes and what different lay-
ers of representation can be usefully combined.
The upper part of Table 5 lists the results of simple
word kernels using the different scopes. The perfor-
mance of the kernels using individual scopes varies
greatly. The best scope is PRED (1), the second
best is SEM (2). The good performance of PRED
does not come as a surprise since the sequence is the
smallest among the different scopes, so this scope is
least affected by data sparseness. Moreover, this re-
sult is consistent with our initial experiments on the
manual feature set (see Section 4.5).
Using different combinations of the word se-
quence kernels shows that PRED and SEM (6)
are a good combination, whereas OP , COMM ,
and SENT (7;8;9) do not positively contribute to
the overall performance which is consistent which
the individual scope evaluation. Apparently, these
scopes capture less linguistically relevant structure.
The next part of Table 5 shows the contribution of
POS kernels when added to WRD kernels. Adding
the corresponding POS kernel to the WRD kernel
with PRED scope (10) results in an improvement
by more than 5% in F-Score. We get another im-
provement by approx. 3% when the corresponding
SEM kernels (11) are added. This suggests that
POS is an effective generalization and that the two
scopes PRED and SEM are complementary.
For the GRAMWRD kernel, the PRED scope
(12) is again most effective. We assume that this ker-
nel most likely expresses meaningful syntactic rela-
tionships for our task. Adding the GRAMPOS ker-
nel (14) gives another boost by almost 4%.
Generalized sequence kernels are important.
800
Adding the corresponding WRDGN kernels to the
WRD kernel with PRED and SEM scope results
in an improvement from 47.77% (1) to 53.00% (15)
which is a bit less than the combination of WRD
and POS(GN) kernels (16). However, these types of
kernels seem to be complementary since their com-
bination provides an F-Score of 56.06% (17). This
kernel combination already performs on a par with
the manually designed vector kernel though less in-
formation is taken into consideration.
Finally, the best combination of sequence ker-
nels (18) comprises WRD, WRDGN , POS, and
POSGN kernels with PRED and SEM scope
combined with a GRAMWRD and a GRAMPOS
kernel with PRED scope. The performance of
58.70% significantly outperforms the vector kernel.
5.4 Tree Kernels (TKs)
Table 6 shows the results of the different tree ker-
nels. The table is divided into two halves. The
left half (A) are plain tree kernels, whereas the right
half (B) are the augmented tree kernels. As far as
CONST kernels are concerned, there is a system-
atic improvement by approximately 2% using tree
augmentation. This proves that further non-syntactic
knowledge added to the tree itself results in an im-
proved F-Score. However, tree augmentation does
not have any impact on the PAS kernels.
The overall performance of the tree kernels shows
that they are much more expressive than sequence
kernels. For instance, in order to obtain the same
performance as of ?CONSTAUG, PRED,STK?
(19B), i.e. a single kernel with an F-Score 56.52, it
requires several sequence kernels, hence much more
effort. The performance of the different CONST
kernels relative to each other resembles the results
of the WRD kernels. The best scope is PRED
(19). By far the worst performance is obtained by
the SENT scope (23). The combination of PRED
and SEM scope achieves an F-Score of 59.67%
(25B) which is already slightly better than the best
configuration of sequence kernels (18).
The performance of the PAS kernel (28A) with
an F-Score of 53.51% is slightly worse than the best
single plain CONST kernel (19A). The PAS ker-
nel and the CONST kernels are complementary,
since their best combination (29B) achieves an F-
Score of 61.67% which is significantly better than
Combination Acc. Prec. Rec. F1
VK 93.63 53.28 59.37 56.16
best SKs 94.21 57.64 59.81 58.70
best TKs 94.16 56.18 68.36 61.67?
VK + best SKs 94.34 58.44 61.27 59.82?
VK + best TKs 94.33 57.41 68.03 62.27?
best SKs + best TKs 94.49 59.22 63.96 61.49?
VK + best SKs + best TKs 94.53 59.10 66.57 62.61??
Table 7: Results of kernel combinations (?: significantly
better than best SKs; ?: significantly better than best TKs;
all convolution kernels are significantly better than VK).
the best combination of CONST kernels (25B) or
sequence kernels (18).
5.5 Combinations
Table 7 lists the results of the different kernel type
combinations. If VK is added to the best TKs, the
best SKs, or both, a slight increase in F-Score is
achieved. The best performance with an F-Score of
62.61% is obtained by combining all kernels.
6 Conclusion
In this paper, we compared convolution kernels for
opinion holder extraction. We showed that, in gen-
eral, a combination of two scopes, namely the scope
immediately encompassing the candidate opinion
holder and its nearest predicate and the subclause
containing the candidate opinion holder provide best
performance. Tree kernels containing constituency
parse information and semantic roles achieve better
performance than sequence kernels or vector kernels
using a manually designed feature set. Best perfor-
mance is achieved if all kernels are combined.
Acknowledgements
Michael Wiegand was funded by the German research
council DFG through the International Research Training
Group ?IRTG? between Saarland University and Univer-
sity of Edinburgh.
The authors would like to thank Yi Zhang for pro-
cessing the MPQA corpus with his semantic-role label-
ing system, the researchers from the MPQA project for
helping to create an opinion holder corpus, and, in partic-
ular, Alessandro Moschitti for insightful comments and
suggestions.
801
ID Kernel Acc. Prec. Rec. F1
1 ?WRD, PRED, SK? 93.25 51.08 42.29 46.26
2 ?WRD, OP, SK? 92.77 46.38 32.52 38.21
3 ?WRD, COMM, SK? 92.42 43.70 35.99 39.46
4 ?WRD, SEM,SK? 93.16 50.32 34.65 41.04
5 ?WRD, SENT, SK? 90.60 29.90 27.29 28.53
6 ?WRD, PRED, SK? + ?WRD, SEM,SK? 93.78 56.55 41.36 47.77
7
P
j?{PRED,OP,COMM}?WRD, j,SK? 93.55 54.26 39.50 45.71
8
P
j?Scopes\SENT ?WRD, j, SK? 93.82 57.21 40.28 47.26
9
P
j?Scopes?WRD, j, SK? 93.63 55.15 39.52 46.03
10 ?WRD, PRED, SK? + ?POS, PRED, SK? 93.03 49.39 53.53 51.37
11
P
i?{PRED,SEM} (?WRD, i, SK? + ?POS, i, SK?) 93.86 55.60 53.22 54.38
12
P
i?{PRED,SEM}?WRD, i, SK? + ?GRAMWRD , PRED, SK? 94.01 58.19 45.88 51.29
13
P
i?{PRED,SEM}?WRD, i, SK? +
P
j?{PRED,OP,COMM}?GRAMWRD , j, SK? 93.83 56.28 45.64 50.40
14
X
i?{PRED,SEM}
?WRD, i, SK?+?GRAMWRD, PRED, SK?+?GRAMPOS, PRED, SK? 93.98 56.59 53.92 55.21
15
P
i?{PRED,SEM} (?WRD, i, SK? + ?WRDGN , i, SK?) 93.97 57.08 49.46 53.00
16
P
i?{PRED,SEM} (?WRD, i, SK? + ?POSGN , i, SK?) 93.97 56.60 52.42 54.42
17
X
i?{PRED,SEM}
(?WRD, i, SK? + ?WRDGN , i, SK? + ?POS, i, SK? + ?POSGN , i, SK?) 93.85 55.16 57.00 56.06
18
X
i?{PRED,SEM}
(?WRD, i, SK? + ?WRDGN , i, SK? + ?POS, i, SK? + ?POSGN , i, SK?) 94.21 57.64 59.81 58.70
+?GRAMWRD , PRED, SK? + ?GRAMPOS , PRED, SK?
Table 5: Results of the different sequence kernels.
A B
i = CONST, j = PAS i = CONSTAUG, j = PASAUG
ID Kernel Acc. Prec. Rec. F1 Acc. Prec. Rec. F1
19 ?i, PRED, STK? 92.89 48.68 62.34 54.67 93.12 49.99 65.04 56.52
20 ?i, OP,STK? 93.04 49.49 54.71 51.96 93.27 50.93 59.06 54.68
21 ?i, COMM,STK? 92.76 47.79 55.89 51.50 92.96 49.03 58.85 53.47
22 ?i, SEM,STK? 93.70 54.40 52.13 53.23 93.90 55.47 56.59 56.03
23 ?i, SENT,STK? 92.42 44.34 39.92 41.99 92.50 45.20 42.40 43.74
24
P
k?{PRED,OP,COMM}?i, k, STK? 93.62 53.26 60.05 56.44 93.77 54.06 63.21 58.26
25
P
k?{PRED,SEM}?i, k, STK? 93.90 55.26 59.50 57.30 94.13 56.57 63.12 59.67
26
P
k?Scopes\SENT ?i, k, STK? 94.09 56.65 59.68 58.11 94.21 57.21 62.61 59.80
27
P
k?Scopes?i, k, STK? 94.14 57.41 57.88 57.63 94.29 58.11 61.10 59.56
28 ?j, SENT, PTK? 92.11 45.02 69.96 53.51 91.92 44.27 67.39 53.43
29
X
k?{PRED,SEM}
?i, k, STK?+?PAS,SENT, PTK? 94.05 55.68 66.01 60.40 94.16 56.18 68.36 61.67
30
X
k?Scopes\SENT
?i, k, STK? + ?PAS,SENT, PTK? 94.30 57.95 62.62 60.19 94.36 58.07 64.94 61.31
Table 6: Results of the different tree kernels.
802
References
Steven Bethard, Hong Yu, Ashley Thornton, Vasileios
Hatzivassiloglou, and Dan Jurafsky. 2005. Extracting
Opinion Propositions and Opinion Holders using Syn-
tactic and Lexical Cues. In Computing Attitude and
Affect in Text: Theory and Applications. Springer.
Kenneth Bloom, Sterling Stein, and Shlomo Argamon.
2007. Appraisal Extraction for News Opinion Analy-
sis at NTCIR-6. In Proceedings of NTCIR-6 Workshop
Meeting, Tokyo, Japan.
Razvan C. Bunescu and Raymond J. Mooney. 2005.
Subsequence Kernels for Relation Extraction. In Pro-
ceedings of the Conference on Neural Information
Processing Systems (NIPS), Vancouver, Canada.
Yejin Choi, Claire Cardie, Ellen Riloff, and Siddharth
Patwardhan. 2005. Identifying Sources of Opinions
with Conditional Random Fields and Extraction Pat-
terns. In Proceedings of the Conference on Human
Language Technology and Empirical Methods in Nat-
ural Language Processing (HLT/EMNLP), Vancouver,
Canada.
Yejin Choi, Eric Breck, and Claire Cardie. 2006.
Joint Extraction of Entities and Relations for Opin-
ion Recognition. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP), Sydney, Australia.
Michael Collins and Nigel Duffy. 2002. New Ranking
Algorithms for Parsing and Tagging. In Proceedings
of the Annual Meeting of the Association for Compu-
tational Linguistics (ACL), Philadelphia, USA.
Charles. J. Fillmore, Christopher R. Johnson, and
Miriam R. Petruck. 2003. Background to FrameNet.
International Journal of Lexicography, 16:235 ? 250.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating Non-local Information
into Information Extraction Systems by Gibbs Sam-
pling. In Proceedings of the Annual Meeting of the
Association for Computational Linguistics (ACL), Ann
Arbor, USA.
Soo-Min Kim and Eduard Hovy. 2005. Identifying
Opinion Holders for Question Answering in Opin-
ion Texts. In Proceedings of AAAI-05 Workshop
on Question Answering in Restricted Domains, Pitts-
burgh, USA.
Soo-Min Kim and Eduard Hovy. 2006. Extracting Opin-
ions, Opinion Holders, and Topics Expressed in On-
line News Media Text. In Proceedings of the ACL
Workshop on Sentiment and Subjectivity in Text, Syd-
ney, Australia.
Paul Kingsbury and Martha Palmer. 2002. From Tree-
Bank to PropBank. In Proceedings of the 3rd Confer-
ence on Language Resources and Evaluation (LREC),
Las Palmas, Spain.
Dan Klein and Christopher D. Manning. 2003. Accurate
Unlexicalized Parsing. In Proceedings of the Annual
Meeting of the Association for Computational Linguis-
tics (ACL), Sapporo, Japan.
Catherine Macleod, Ralph Grishman, Adam Meyers,
Leslie Barrett, and Ruth Reeves. 1998. NOMLEX:
A Lexicon of Nominalizations. In Proceedings of EU-
RALEX, Lie`ge, Belgium.
Alessandro Moschitti, Daniele Pighin, and Roberto
Basili. 2008. Tree Kernels for Semantic Role Label-
ing. Computational Linguistics, 34(2):193 ? 224.
Alessandro Moschitti. 2006. Efficient Convolution Ker-
nels for Dependency and Constituent Syntactic Trees.
In Proceedings of the 17th European Conference on
Machine Learning (ECML), Berlin, Germany.
Alessandro Moschitti. 2008. Kernel Methods, Syn-
tax and Semantics for Relational Text Categorization.
In Proceedings of the Conference on Information and
Knowledge Management (CIKM), Napa Valley, USA.
Truc-Vien T. Nguyen, Alessandro Moschitti, and
Giuseppe Riccardi. 2009. Convolution Kernels on
Constituent, Dependency and Sequential Structures
for Relation Extraction. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language Pro-
cessing (EMNLP), Singapore.
Ellen Riloff. 1996. An Empirical Study of Automated
Dictionary Construction for Information Extraction.
Artificial Intelligence, 85.
John Taylor and Nello Christianini. 2004. Kernel Meth-
ods for Pattern Analysis. Cambridge University Press.
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2003.
Annotating Expressions of Opinions and Emotions in
Language. Language Resources and Evaluation, 1:2.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing Contextual Polarity in Phrase-
level Sentiment Analysis. In Proceedings of Hu-
man Language Technologies Conference/Conference
on Empirical Methods in Natural Language Process-
ing (HLT/EMNLP), Vancouver, Canada.
Dell Zhang and Wee Sun Lee. 2003. Question Classifi-
cation using Support Vector Machines. In Proceedings
of the ACM Special Interest Group on Information Re-
trieval (SIGIR), Toronto, Canada.
Min Zhang, Jie Zhang, and Jian Su. 2006. Explor-
ing Syntactic Features for Relation Extraction using a
Convolution Tree Kernel. In Proceedings of the Hu-
man Language Technology Conference of the North
American Chapter of the ACL (HLT/NAACL), New
York City, USA.
Yi Zhang, Rui Wang, and Hans Uszkoreit. 2008. Hy-
brid Learning of Dependency Structures from Het-
erogeneous Linguistic Resources. In Proceedings of
the Conference on Computational Natural Language
Learning (CoNLL), Manchester, United Kingdom.
803
Proceedings of NAACL-HLT 2013, pages 534?539,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Predicative Adjectives: An Unsupervised Criterion to Extract Subjective
Adjectives
Michael Wiegand
Spoken Language Systems
Saarland University
michael.wiegand@lsv.uni-saarland.de
Josef Ruppenhofer
Dept. of Information Science
and Language Technology
Hildesheim University
ruppenho@uni-hildesheim.de
Dietrich Klakow
Spoken Language Systems
Saarland University
dietrich.klakow@lsv.uni-saarland.de
Abstract
We examine predicative adjectives as an unsu-
pervised criterion to extract subjective adjec-
tives. We do not only compare this criterion
with a weakly supervised extraction method
but also with gradable adjectives, i.e. another
highly subjective subset of adjectives that can
be extracted in an unsupervised fashion. In or-
der to prove the robustness of this extraction
method, we will evaluate the extraction with
the help of two different state-of-the-art senti-
ment lexicons (as a gold standard).
1 Introduction
Since the early work on sentiment analysis, it has
been established that the part of speech with the
highest proportion of subjective words are adjec-
tives (Wiebe et al, 2004) (see Sentence (1)). How-
ever, not all adjectives are subjective (2).
(1) A grumpy guest made some impolite remarks
to the insecure and inexperienced waitress.
(2) The old man wearing a yellow pullover sat on a
plastic chair.
This justifies the exploration of criteria to automati-
cally separate the subjective adjectives from the non-
subjective adjectives.
In this work, we are interested in an out-of-
context assessment of adjectives and therefore eval-
uate them with the help of sentiment lexicons. We
examine the property of being a predicative adjec-
tive as an extraction criterion. Predicative adjectives
are adjectives that do not modify the head of a noun
phrase, but which predicate a property of the refer-
ent of a noun phrase to which they are linked via a
copula or a control predicate (3).
We show that adjectives that frequently occur as
predicative adjectives are more likely to convey sub-
jectivity (in general) than adjectives that occur non-
predicatively, such as the pre-nominal (attributive)
adjectives (4). A subjective adjective may occur
both as a predicative (3) and a non-predicative (5)
adjective and also convey subjectivity in both con-
texts. However, a large fraction of non-subjective
adjectives do not occur as predicative adjectives (6).
(3) Her idea was brilliant.
(4) This is a financial problem.
(5) She came up with a brilliant idea.
(6) ?The problem is financial.
2 Related Work
The extraction of subjective adjectives has already
attracted some considerable attention in previous re-
search. Hatzivassiloglou and McKeown (1997) ex-
tract polar adjectives by a weakly supervised method
in which subjective adjectives are found by search-
ing for adjectives that are conjuncts of a pre-defined
set of polar seed adjectives. Wiebe (2000) in-
duces subjective adjectives with the help of distribu-
tional similarity. Hatzivassiloglou and Wiebe (2000)
examine the properties of dynamic, gradable and
polar adjectives as a means to detect subjectivity.
Vegnaduzzo (2004) presents another bootstrapping
method of extracting subjective adjectives with the
help of head nouns of the subjective candidates and
distributional similarity. Baroni and Vegnaduzzo
534
(2004) employ Web-based Mutual information for
this task and largely outperform the results produced
by Vegnaduzzo (2004).
3 Method
In the following, we present different features with
the help of which subjective adjectives can be ex-
tracted. For all resulting lists, the adjectives will be
ranked according to their frequency of co-occurring
with a particular feature.
3.1 Extracting Predicative Adjectives (PRD)
For the extraction of predicative adjectives, we ex-
clusively rely on the output of a dependency parser.
Predicative adjectives are usually connected to the
subject of the sentence via the dependency label
nsubj (Example (7) would correspond to Sen-
tence (3)).
(7) nsubj(brilliant, idea)
3.2 Extracting Gradable Adjectives (GRD)
As an alternative extraction method, we consider
morpho-syntactically gradable adjectives. Gradable
adjectives, such as nice or small, are adjectives ?that
can be inflected to specify the degree or grade of
something? (Wiktionary1). It has been stated in pre-
vious work that if some adjective can build a com-
parative (e.g. nicer) or a superlative (e.g. nicest),
then this adjective tends to be subjective (Hatzivas-
siloglou and Wiebe, 2000).
We employ the property of gradability, since,
firstly, it is very predictive towards subjectivity and,
secondly, it is the only other unsupervised criterion
currently known to extract subjective adjectives. For
the extraction of gradable adjectives, we rely, on the
one hand, on the part-of-speech labels JJR (com-
parative) and JJS (superlative). On the other hand,
we also consider adjectives being modified by ei-
ther more or most. For the former case, we need
to normalize the comparative (e.g. nicer) or superla-
tive (e.g. nicest) word form to the canonical positive
word form (e.g. nice) that is commonly used in sen-
timent lexicons.
1http://en.wiktionary.org/wiki/gradable
3.3 Weakly-Supervised Extraction (WKS)
We also consider a weakly supervised extraction
method in this paper, even though it is not strictly
fair to compare such a method with our two pre-
vious extraction methods which are completely un-
supervised. WKS considers an adjective subjective,
if it co-occurs as a conjunct of a previously defined
highly subjective (seed) adjective (8). In order to de-
tect such conjunctions, we employ the dependency
relation conj. By just relying on surface patterns,
we would not be able to exclude spurious conjunc-
tions in which other constituents than the two adjec-
tives are coordinated, such as Sentence (10).
(8) This approach is ill-conceived and ineffective.
(9) conj(ill-conceived,
ineffective)
(10) [Evil witches are stereotypically dressed in
black] and [good fairies in white].
We also experimented with other related weakly-
supervised extraction methods, such as mutual in-
formation of two adjectives at the sentence level (or
even smaller window sizes). However, using con-
junctions largely outperformed these alternative ap-
proaches so we only pursue conjunctions here.
4 Experiments
As a large unlabeled (training) corpus, we chose the
North American News Text Corpus (LDC95T21)
comprising approximately 350 million words of
news text. For syntactic analysis we use the Stan-
ford Parser (Finkel et al, 2005). In order to decide
whether an extracted adjective is subjective or not,
we employ two sentiment lexicons, namely the Sub-
jectivity Lexicon (SUB) (Wilson et al, 2005) and
SO-CAL (SOC) (Taboada et al, 2011). According to
the recent in-depth evaluation presented in Taboada
et al (2011), these two sentiment lexicons are the
most effective resources for English sentiment anal-
ysis. By taking into account two different lexicons,
which have also been built independently of each
other, we want to provide evidence that our pro-
posed criterion to extract subjective adjectives is not
sensitive towards a particular gold standard (which
would challenge the general validity of the proposed
method).
535
ALL other new last many first such next political federal own sev-
eral few good? former same economic public major recent
American second big? foreign high small local military fi-
nancial little? national
PRD able? likely available clear? difficult? important? ready?
willing? hard? good? due possible? sure? interested un-
likely necessary? high responsible? easy? strong? unable?
different enough open aware happy impossible? right?
wrong? confident?
Table 2: The 30 most frequent adjectives (ALL) and pred-
icative adjectives (PRD); ? marks matches with both sen-
timent lexicons SUB and SOC.
In order to produce the subjective seed adjec-
tives for the weakly supervised extraction, we col-
lect from the sentiment lexicon that we evaluate the
n most frequent subjective adjectives according to
our corpus. In order to further improve the quality
of the seed set, we only consider strong subjective
expressions from SUB and expressions with the in-
tensity strength ?5 from SOC.
Table 1 lists the size of the different sentiment lex-
icons and the rankings produced by the different ex-
traction methods. Of course, the list of all adjectives
from the corpus (ALL) is the largest list2 while PRD
is the second largest and GRD the third largest. The
rankings produced by WKS are fairly sparse, in par-
ticular the ones induced with the help of SOC; ap-
parently there are more frequently occurring strong
subjective adjectives in SUB than there are high in-
tensity adjectives in SOC.
4.1 Frequent Adjectives vs. Frequent
Predicative Adjectives
Table 2 compares the 30 most frequent adjectives
(ALL) and predicative adjectives (PRD). Not only
does this table show that the proportion of subjective
adjectives is much larger among the predicative ad-
jectives but we may also gain some insight into what
non-subjective adjectives are excluded. Among the
high frequent adjectives are many quantifiers (many,
few and several) and ordinal expressions (first, next
and last). In principle, most of these expressions
are not subjective. One may argue that these adjec-
tives behave like function words. Since they occur
2It will also contain many words erroneously tagged as ad-
jectives, however, this is unlikely to affect our experiments since
we only focus on the highly ranked (i.e. most frequent) words.
The misclassifications rather concern infrequent words.
very frequently, one might exclude some of them
by just ignoring the most frequent adjectives. How-
ever, there are also other types of adjectives, espe-
cially pertainyms (political, federal, economic, pub-
lic, American, foreign, local, military, financial and
national) that appear on this list which could not be
excluded by that heuristic. We found that these non-
subjective content adjectives are present throughout
the entire ranking and they are fairly frequent (on
the ranking). On the list of predicative adjectives all
these previous types of adjectives are much less fre-
quent. Many of them only occur on lower ranks (and
we assume that several of them only got on the list
due to parsing errors).
4.2 Comparison of the Different Extraction
Methods
Table 3 compares the precision of the different ex-
traction methods at different cut-off values. It is in-
teresting to see that for ALL in particular the higher
ranks are worse than the lower ranks (e.g. rank
1000). We assume that this is due to the high-
frequency adjectives which are similar to function
words (see Section 4.1). At all cut-off values, how-
ever, this baseline is beaten by every other method,
including our proposed method PRD. The two unsu-
pervised methods PRD and GRD perform on a par
with each other. On SUB, PRD even mostly out-
performs GRD. The precision achieved by WKS is
quite good. However, the coverage of this method
is low. It would require more seed expressions to
increase it, however, this would also mean consider-
ably more manual guidance.
Table 3 also shows that the precision of all ex-
traction methods largely drops on the lower ranks.
However, one should not conclude from that the ex-
traction methods proposed only work well for highly
frequent words. The drop can be mostly explained
by the fact that the two sentiment lexicons we use
for evaluation are finite (i.e. SUB: 4396 words/SOC:
2827 words (Table 1)), and that neither of these lexi-
cons (nor their union) represents the complete set of
all English subjective adjectives. Both lexicons will
have a bias towards frequently occurring subjective
expressions.
Inspecting the ranks 3001-3020 produced by PRD
as displayed in Table 4, for example, actually reveals
that there are still many more subjective adjectives
536
Lexicons Extraction Methods
WKS-5 WKS-10 WKS-25 WKS-50
SUB SOC ALL PRD GRD SUB SOC SUB SOC SUB SOC SUB SOC
4396 2827 212287 20793 7942 292 81 440 131 772 319 1035 385
Table 1: Statistics regarding the size (i.e. number of adjectives) of the different sentiment lexicons and rankings.
artistic? appealable airtight adjustable? activist? accommodat-
ing acclimated well-meaning weakest upsetting? unsurpassed
unsatisfying? unopposed unobtrusive? unobjectionable unem-
ployable understanding? uncharacteristic submerged speechless
Table 4: A set of entries PRD produces on lower ranks
(ranks 3001-3020); ? marks matches with either of the
sentiment lexicons SUB or SOC.
than the matches with our sentiment lexicons sug-
gest (e.g. appealable, accomodating, well-meaning,
weakest, unsurpassed, unopposed, unobjectionable,
unemployable, uncharacteristic or speechless). In
other words, these are less frequent words; many
of them are actually subjective even though they are
not listed in the sentiment lexicons. Moreover, irre-
spective of the drop in precision on the lower ranks,
PRD and GRD still outperform ALL on both senti-
ment lexicons (Table 3). Despite the sparseness of
our two gold standards on the lower ranks, we thus
have some indication that PRD and GRD are more
effective than ALL.
The problem of the evaluation of less-frequent
words could not be solved by an extrinsic evaluation,
either, e.g. by using the extracted lists for some text
classification task (at the sentence/document level).
The evaluation on contextual classification on cor-
pora would also be biased towards high-frequency
words (as the word distribution is typically Zipfian).
For instance, on the MPQA-corpus (Wiebe et al,
2005), i.e. the standard dataset for (fine-grained)
sentiment analysis, there is not a single mention of
the subjective words appealable, accommodating,
unsurpassed, unopposed, unobtrusive or speechless,
which were found among the lower ranks 3001-
3020.
4.3 How Different Are Gradable and
Predicative Adjectives?
Since in the previous experiments the proportion of
subjective adjectives was similar among the grad-
able adjectives and the predicative adjectives, we
may wonder whether these two extraction methods
produce the same adjectives. In principle, the set of
gradable adjectives extracted is much smaller than
the list of extracted predicative adjectives (see Ta-
ble 1). We found that the gradable adjectives are
a proper subset of predicative adjectives, which is
in line with the observation by (Bolinger, 1972,
21) that gradable adjectives (which he calls degree
words) readily occur predicatively whereas non-
gradable ones tend not to.
However, while gradability implies compatibility
with predicative use, the reverse is not true. Ac-
cordingly, we found adjectives that are definitely not
gradable among the predicative adjectives that are
subjective, for instance endless, insolvent, nonexis-
tent, stagnant, unavailable or untrue. This means
that with the criterion of predicative adjectives one
is able to extract relevant subjective adjectives that
cannot be caught by the gradability criterion alone,
namely complementary adjectives that refer to a sim-
ple binary opposition (Cruse, 1986, 198-99).
4.4 Intersecting the Different Unsupervised
Criteria
In this section, we want to find out whether we can
increase the precision by considering intersections
of the two different unsupervised extraction crite-
ria. (Due to the sparsity of WKS, it does not make
sense to include that method in this experiment.) In
our previous experiments it turned out that as far as
precision is concerned, our new proposed extraction
criterion was similar to the gradability criterion. If,
however, the intersection of these two criteria pro-
duces better results, then we have provided some
further proof of the effectiveness of our proposed
criterion (even though we may sacrifice some exclu-
sive subjective adjectives in PRD as pointed out in
Section 4.3). It would mean that this criterion is also
beneficial in the presence of the gradability criterion.
Figure 1 shows the corresponding results. We
computed the intersection of PRD and GRD at var-
537
ALL PRD GRD WKS-5 WKS-10 WKS-25 WKS-50
Rank n SUB SOC SUB SOC SUB SOC SUB SOC SUB SOC SUB SOC SUB SOC
10 10.00 30.00 90.00 90.00 80.00 60.00 80.00 90.00 80.00 90.00 90.00 70.00 90.00 70.00
25 20.00 32.00 88.00 60.00 64.00 60.00 92.00 80.00 91.00 80.00 92.00 80.00 92.00 84.00
50 30.00 34.00 88.00 64.00 70.00 68.00 82.00 78.00 92.00 78.00 92.00 84.00 90.00 86.00
100 37.00 38.00 81.00 68.00 79.00 75.00 80.00 N/A 82.00 72.00 89.00 78.00 92.00 77.00
250 45.60 43.20 79.60 75.60 84.80 76.00 70.80 N/A 74.40 N/A 80.40 67.50 82.04 67.20
500 48.00 49.20 77.20 70.00 82.20 74.00 N/A N/A N/A N/A 72.60 N/A 75.20 N/A
1000 48.70 48.10 75.50 65.60 72.60 65.00 N/A N/A N/A N/A N/A N/A 64.30 N/A
1500 49.07 46.53 68.60 59.07 66.27 58.60 N/A N/A N/A N/A N/A N/A N/A N/A
2000 48.00 43.85 64.55 55.40 61.55 54.25 N/A N/A N/A N/A N/A N/A N/A N/A
2500 46.08 40.96 59.52 51.28 56.36 50.00 N/A N/A N/A N/A N/A N/A N/A N/A
3000 44.20 39.17 54.63 47.13 51.47 46.03 N/A N/A N/A N/A N/A N/A N/A N/A
Table 3: Precision at rank n of the different extraction methods; WKS-m denotes that for the extraction the m most
frequent subjective adjectives from the respective sentiment lexicon were considered as seed expressions.
ious cut-off values of n. The resulting intersection
comprises m ranks with m < n. The precision of
the intersection was consequently compared against
the precision of PRD and GRD at rank m. The figure
shows that with the exception of the higher ranks on
SUB (< 200) there is indeed a systematic increase
in precision when the intersection of PRD and GRD
is considered.
5 Conclusion
We examined predicative adjectives as a criterion
to extract subjective adjectives. As this extraction
method is completely unsupervised, it is preferable
to weakly supervised extraction methods since we
are not dependent on a manually designed high qual-
ity seed set and we obtain a much larger set of ad-
jectives. This extraction method is competitive if
not slightly better than gradable adjectives. In ad-
dition, combining these two unsupervised methods
by assessing their intersection results mostly in an
increase in precision.
Acknowledgements
This work was performed in the context of the Software-
Cluster project EMERGENT. Michael Wiegand was
funded by the German Federal Ministry of Education and
Research (BMBF) under grant no. ?01IC10S01?. The
authors would like to thank Maite Taboada for providing
her sentiment lexicon (SO-CAL) to be used for the exper-
iments presented in this paper.
 55
 60
 65
 70
 75
 80
 85
 500  1000  1500  2000  2500
Pr
ec
is
io
n
Top N Ranked Adjectives
Predicative Adjectives (PRD)
Gradable Adjectives (GRD)
Intersection of PRD and GRD
(a) Evaluation on SUB lexicon
 50
 55
 60
 65
 70
 75
 80
 500  1000  1500  2000  2500
Pr
ec
is
io
n
Top N Ranked Adjectives
Predicative Adjectives (PRD)
Gradable Adjectives (GRD)
Intersection of PRD and GRD
(b) Evaluation on SOC lexicon
Figure 1: Comparison of the individual rankings of GRD
and PRD with their intersection.
538
References
Marco Baroni and Stefano Vegnaduzzo. 2004. Identify-
ing Subjective Adjectives through Web-based Mutual
Information. In Proceedings of KONVENS, pages 17?
24, Vienna, Austria.
Dwight Bolinger. 1972. Degree words. Mouton, The
Hague.
David Alan Cruse. 1986. Lexical Semantics. Cambridge
University Press, Cambridge, UK.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating Non-local Information
into Information Extraction Systems by Gibbs Sam-
pling. In Proceedings of the Annual Meeting of the As-
sociation for Computational Linguistics (ACL), pages
363?370, Ann Arbor, MI, USA.
Vasileios Hatzivassiloglou and Kathleen R. McKeown.
1997. Predicting the Semantic Orientation of Adjec-
tives. In Proceedings of the Conference on European
Chapter of the Association for Computational Linguis-
tics (EACL), pages 174?181, Madrid, Spain.
Vasileios Hatzivassiloglou and Janyce Wiebe. 2000. Ef-
fects of Adjective Orientation and Gradability on Sen-
tence Subjectivity. In Proceedings of the International
Conference on Computational Linguistics (COLING),
pages 299?305, Saarbru?cken, Germany.
Maite Taboada, Julian Brooke, Milan Tofiloski, Kimberly
Voll, and Manfred Stede. 2011. Lexicon-Based Meth-
ods for Sentiment Analysis. Computational Linguis-
tics, 37(2):267 ? 307.
Stefano Vegnaduzzo. 2004. Acquisition of Subjective
Adjectives with Limited Resources. In Proceedings of
the AAAI Spring Symposium on Exploring Attitude and
Affect in Text: Theories and Applications, Stanford,
CA, USA.
Janyce Wiebe, Theresa Wilson, Rebecca Bruce, Matthew
Bell, and Melanie Martin. 2004. Learning Subjective
Language. Computational Linguistics, 30(3).
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005.
Annotating Expressions of Opinions and Emotions
in Language. Language Resources and Evaluation,
39(2/3):164?210.
Janyce M. Wiebe. 2000. Learning Subjective Adjectives
from Corpora. In Proceedings of the National Confer-
ence on Artificial Intelligence (AAAI), pages 735?740,
Austin, TX, USA.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing Contextual Polarity in Phrase-level
Sentiment Analysis. In Proceedings of the Conference
on Human Language Technology and Empirical Meth-
ods in Natural Language Processing (HLT/EMNLP),
pages 347?354, Vancouver, BC, Canada.
539
Proceedings of the Workshop on Negation and Speculation in Natural Language Processing, pages 60?68,
Uppsala, July 2010.
A Survey on the Role of Negation in Sentiment Analysis
Michael Wiegand
Saarland University
Saarbru?cken, Germany
michael.wiegand@lsv.uni-saarland.de
Alexandra Balahur
University of Alicante
Alicante, Spain
abalahur@dlsi.ua.es
Benjamin Roth and Dietrich Klakow
Saarland University
Saarbru?cken, Germany
benjamin.roth@lsv.uni-saarland.de
dietrich.klakow@lsv.uni-saarland.de
Andre?s Montoyo
University of Alicante
Alicante, Spain
montoyo@dlsi.ua.es
Abstract
This paper presents a survey on the role of
negation in sentiment analysis. Negation
is a very common linguistic construction
that affects polarity and, therefore, needs
to be taken into consideration in sentiment
analysis.
We will present various computational ap-
proaches modeling negation in sentiment
analysis. We will, in particular, focus
on aspects, such as level of representation
used for sentiment analysis, negation word
detection and scope of negation. We will
also discuss limits and challenges of nega-
tion modeling on that task.
1 Introduction
Sentiment analysis is the task dealing with the
automatic detection and classification of opinions
expressed in text written in natural language.
Subjectivity is defined as the linguistic expression
of somebody?s opinions, sentiments, emotions,
evaluations, beliefs and speculations (Wiebe,
1994). Subjectivity is opposed to objectivity,
which is the expression of facts. It is important to
make the distinction between subjectivity detec-
tion and sentiment analysis, as they are two sep-
arate tasks in natural language processing. Sen-
timent analysis can be dependently or indepen-
dently done from subjectivity detection, although
Pang and Lee (2004) state that subjectivity de-
tection performed prior to the sentiment analysis
leads to better results in the latter.
Although research in this area has started only re-
cently, the substantial growth in subjective infor-
mation on the world wide web in the past years
has made sentiment analysis a task on which con-
stantly growing efforts have been concentrated.
The body of research published on sentiment anal-
ysis has shown that the task is difficult, not only
due to the syntactic and semantic variability of
language, but also because it involves the extrac-
tion of indirect or implicit assessments of objects,
by means of emotions or attitudes. Being a part
of subjective language, the expression of opinions
involves the use of nuances and intricate surface
realizations. That is why the automatic study of
opinions requires fine-grained linguistic analysis
techniques and substantial efforts to extract fea-
tures for machine learning or rule-based systems,
in which subtle phenomena as negation can be ap-
propriately incorporated.
Sentiment analysis is considered as a subsequent
task to subjectivity detection, which should ideally
be performed to extract content that is not factual
in nature. Subsequently, sentiment analysis aims
at classifying the sentiment of the opinions into
polarity types (the common types are positive and
negative). This text classification task is also re-
ferred to as polarity classification.
This paper presents a survey on the role of nega-
tion in sentiment analysis. Negation is a very com-
mon linguistic construction that affects polarity
and, therefore, needs to be taken into considera-
tion in sentiment analysis. Before we describe the
computational approaches that have been devised
to account for this phenomenon in sentiment anal-
ysis, we will motivate the problem.
2 Motivation
Since subjectivity and sentiment are related to ex-
pressions of personal attitudes, the way in which
this is realized at the surface level influences the
manner in which an opinion is extracted and its
polarity is computed. As we have seen, sentiment
analysis goes a step beyond subjectivity detection,
60
including polarity classification. So, in this task,
correctly determining the valence of a text span
(whether it conveys a positive or negative opinion)
is equivalent to the success or failure of the auto-
matic processing.
It is easy to see that Sentence 1 expresses a posi-
tive opinion.
1. I like+ this new Nokia model.
The polarity is conveyed by like which is a polar
expression. Polar expressions, such as like or hor-
rible, are words containing a prior polarity. The
negation of Sentence 1, i.e. Sentence 2, using the
negation word not, expresses a negative opinion.
2. I do [not like+]? this new Nokia model.
In this example, it is straightforward to notice the
impact of negation on the polarity of the opinion
expressed. However, it is not always that easy
to spot positive and negative opinions in text. A
negation word can also be used in other expres-
sions without constituting a negation of the propo-
sition expressed as exemplified in Sentence 3.
3. Not only is this phone expensive but it is also heavy and
difficult to use.
In this context, not does not invert the polarity of
the opinion expressed which remains negative.
Moreover, the presence of an actual negation word
in a sentence does not mean that all its polar opin-
ions are inverted. In Sentence 4, for example, the
negation does not modify the second polar expres-
sion intriguing since the negation and intriguing
are in separate clauses.
4. [I do [not like+]? the design of new Nokia model] but
[it contains some intriguing+ new functions].
Therefore, when treating negation, one must be
able to correctly determine the scope that it has
(i.e. determine what part of the meaning expressed
is modified by the presence of the negation).
Finally, the surface realization of a negation is
highly variable, depending on various factors,
such as the impact the author wants to make on
the general text meaning, the context, the textual
genre etc. Most of the times, its expression is far
from being simple (as in the first two examples),
and does not only contain obvious negation words,
such as not, neither or nor. Research in the field
has shown that there are many other words that in-
vert the polarity of an opinion expressed, such as
diminishers/valence shifters (Sentence 5), connec-
tives (Sentence 6), or even modals (Sentence 7).
5. I find the functionality of the new phone less practical.
6. Perhaps it is a great phone, but I fail to see why.
7. In theory, the phone should have worked even under
water.
As can be seen from these examples, modeling
negation is a difficult yet important aspect of sen-
timent analysis.
3 The Survey
In this survey, we focus on work that has presented
novel aspects for negation modeling in sentiment
analysis and we describe them chronologically.
3.1 Negation and Bag of Words in Supervised
Machine Learning
Several research efforts in polarity classification
employ supervised machine-learning algorithms,
like Support Vector Machines, Na??ve Bayes Clas-
sifiers or Maximum Entropy Classifiers. For these
algorithms, already a low-level representation us-
ing bag of words is fairly effective (Pang et al,
2002). Using a bag-of-words representation, the
supervised classifier has to figure out by itself
which words in the dataset, or more precisely fea-
ture set, are polar and which are not. One either
considers all words occurring in a dataset or, as
in the case of Pang et al (2002), one carries out
a simple feature selection, such as removing infre-
quent words. Thus, the standard bag-of-words rep-
resentation does not contain any explicit knowl-
edge of polar expressions. As a consequence of
this simple level of representation, the reversal
of the polarity type of polar expressions as it is
caused by a negation cannot be explicitly modeled.
The usual way to incorporate negation modeling
into this representation is to add artificial words:
i.e. if a word x is preceded by a negation word,
then rather than considering this as an occurrence
of the feature x, a new feature NOT x is created.
The scope of negation cannot be properly modeled
with this representation either. Pang et al (2002),
for example, consider every word until the next
punctuation mark. Sentence 2 would, therefore,
result in the following representation:
8. I do not NOT like NOT this NOT new NOT Nokia
NOT model.
The advantage of this feature design is that a plain
occurrence and a negated occurrence of a word are
61
reflected by two separate features. The disadvan-
tage, however, is that these two contexts treat the
same word as two completely different entities.
Since the words to be considered are unrestricted,
any word ? no matter whether it is an actual po-
lar expression or not ? is subjected to this nega-
tion modification. This is not only linguistically
inaccurate but also increases the feature space with
more sparse features (since the majority of words
will only be negated once or twice in a corpus).
Considering these shortcomings, it comes to no
surprise that the impact of negation modeling on
this level of representation is limited. Pang et al
(2002) report only a negligible improvement by
adding the artificial features compared to plain bag
of words in which negation is not considered.
Despite the lack of linguistic plausibility, super-
vised polarity classifiers using bag of words (in
particular, if training and testing are done on the
same domain) offer fairly good performance. This
is, in particular, the case on coarse-grained clas-
sification, such as on document level. The suc-
cess of these methods can be explained by the
fact that larger texts contain redundant informa-
tion, e.g. it does not matter whether a classifier
cannot model a negation if the text to be classi-
fied contains twenty polar opinions and only one
or two contain a negation. Another advantage
of these machine learning approaches on coarse-
grained classification is their usage of higher order
n-grams. Imagine a labeled training set of docu-
ments contains frequent bigrams, such as not ap-
pealing or less entertaining. Then a feature set us-
ing higher order n-grams implicitly contains nega-
tion modeling. This also partially explains the ef-
fectiveness of bigrams and trigrams for this task as
stated in (Ng et al, 2006).
The dataset used for the experiments in (Pang et
al., 2002; Ng et al, 2006) has been established as
a popular benchmark dataset for sentiment analy-
sis and is publicly available1.
3.2 Incorporating Negation in Models that
Include Knowledge of Polar Expressions
- Early Works
The previous subsection suggested that appropri-
ate negation modeling for sentiment analysis re-
quires the awareness of polar expressions. One
way of obtaining such expressions is by using a
1http://www.cs.cornell.edu/people/
pabo/movie-review-data
polarity lexicon which contains a list of polar ex-
pressions and for each expression the correspond-
ing polarity type. A simple rule-based polarity
classifier derived from this knowledge typically
counts the number of positive and negative polar
expressions in a text and assigns it the polarity
type with the majority of polar expressions. The
counts of polar expressions can also be used as
features in a supervised classifier. Negation is typ-
ically incorporated in those features, e.g. by con-
sidering negated polar expressions as unnegated
polar expressions with the opposite polarity type.
3.2.1 Contextual Valence Shifters
The first computational model that accounts for
negation in a model that includes knowledge of
polar expressions is (Polanyi and Zaenen, 2004).
The different types of negations are modeled via
contextual valence shifting. The model assigns
scores to polar expressions, i.e. positive scores to
positive polar expressions and negative scores to
negative polar expressions, respectively. If a polar
expression is negated, its polarity score is simply
inverted (see Example 1).
clever (+2) ? not clever (?2) (1)
In a similar fashion, diminishers are taken into
consideration. The difference is, however, that
the score is only reduced rather than shifted to the
other polarity type (see Example 2).
efficient (+2)? rather efficient (+1) (2)
Beyond that the model also accounts for modals,
presuppositional items and even discourse-based
valence shifting. Unfortunately, this model is
not implemented and, therefore, one can only
speculate about its real effectiveness.
Kennedy and Inkpen (2005) evaluate a nega-
tion model which is fairly identical to the one pro-
posed by Polanyi and Zaenen (2004) (as far as
simple negation words and diminishers are con-
cerned) in document-level polarity classification.
A simple scope for negation is chosen. A polar
expression is thought to be negated if the negation
word immediately precedes it. In an extension of
this work (Kennedy and Inkpen, 2006) a parser is
considered for scope computation. Unfortunately,
no precise description of how the parse is used
for scope modeling is given in that work. Neither
is there a comparison of these two scope models
measuring their respective impacts.
62
Final results show that modeling negation is im-
portant and relevant, even in the case of such sim-
ple methods. The consideration of negation words
is more important than that of diminishers.
3.2.2 Features for Negation Modeling
Wilson et al (2005) carry out more advanced
negation modeling on expression-level polarity
classification. The work uses supervised machine
learning where negation modeling is mostly en-
coded as features using polar expressions. The
features for negation modeling are organized in
three groups:
? negation features
? shifter features
? polarity modification features
Negation features directly relate to negation ex-
pressions negating a polar expression. One feature
checks whether a negation expression occurs in a
fixed window of four words preceding the polar
expression. The other feature accounts for a polar
predicate having a negated subject. This frequent
long-range relationship is illustrated in Sentence 9.
9. [No politically prudent Israeli]
subject
could
support
polar pred
either of them.
All negation expressions are additionally disam-
biguated as some negation words do not function
as a negation word in certain contexts, e.g. not to
mention or not just.
Shifter features are binary features checking the
presence of different types of polarity shifters. Po-
larity shifters, such as little, are weaker than ordi-
nary negation expressions. They can be grouped
into three categories, general polarity shifters,
positive polarity shifters, and negative polarity
shifters. General polarity shifters reverse polarity
like negations. The latter two types only reverse
a particular polarity type, e.g. the positive shifter
abate only modifies negative polar expressions as
in abate the damage. Thus, the presence of a pos-
itive shifter may indicate positive polarity. The set
of words that are denoted by these three features
can be approximately equated with diminishers.
Finally, polarity modification features describe
polar expressions of a particular type modify-
ing or being modified by other polar expressions.
Though these features do not explicitly contain
negations, language constructions which are sim-
ilar to negation may be captured. In the phrase
[disappointed? hope+]?, for instance, a negative
polar expression modifies a positive polar expres-
sion which results in an overall negative phrase.
Adding these three feature groups to a feature
set comprising bag of words and features count-
ing polar expressions results in a significant im-
provement. In (Wilson et al, 2009), the experi-
ments of Wilson et al (2005) are extended by a
detailed analysis on the individual effectiveness of
the three feature groups mentioned above. The re-
sults averaged over four different supervised learn-
ing algorithms suggest that the actual negation fea-
tures are most effective whereas the binary polar-
ity shifters have the smallest impact. This is con-
sistent with Kennedy and Inkpen (2005) given the
similarity of polarity shifters and diminishers.
Considering the amount of improvement that is
achieved by negation modeling, the improvement
seems to be larger in (Wilson et al, 2005). There
might be two explanations for this. Firstly, the
negation modeling in (Wilson et al, 2005) is con-
siderably more complex and, secondly, Wilson et
al. (2005) evaluate on a more fine-grained level
(i.e. expression level) than Kennedy and Inkpen
(2005) (they evaluate on document level). As al-
ready pointed out in ?3.1, document-level polar-
ity classification contains more redundant infor-
mation than sentence-level or expression-level po-
larity classification, therefore complex negation
modeling on these levels might be more effective
since the correct contextual interpretation of an in-
dividual polar expression is far more important2.
The fine-grained opinion corpus used in (Wilson
et al, 2005; Wilson et al, 2009) and all the re-
sources necessary to replicate the features used in
these experiments are also publicly available3.
3.3 Other Approaches
The approaches presented in the previous sec-
tion (Polanyi and Zaenen, 2004; Kennedy and
Inkpen, 2005; Wilson et al, 2005) can be consid-
ered as the works pioneering negation modeling
in sentiment analysis. We now present some more
recent work on that topic. All these approaches,
however, are heavily related to these early works.
2This should also explain why most subsequent works
(see ?3.3) have been evaluated on fine-grained levels.
3The corpus is available under:
http://www.cs.pitt.edu/mpqa/
databaserelease and the resources
for the features are part of OpinionFinder:
http://www.cs.pitt.edu/mpqa/
opinionfinderrelease
63
3.3.1 Semantic Composition
In (Moilanen and Pulman, 2007), a method to
compute the polarity of headlines and complex
noun phrases using compositional semantics is
presented. The paper argues that the principles of
this linguistic modeling paradigm can be success-
fully applied to determine the subsentential polar-
ity of the sentiment expressed, demonstrating it
through its application to contexts involving senti-
ment propagation, polarity reversal (e.g. through
the use of negation following Polanyi and Zae-
nen (2004) and Kennedy and Inkpen (2005)) or
polarity conflict resolution. The goal is achieved
through the use of syntactic representations of sen-
tences, on which rules for composition are de-
fined, accounting for negation (incrementally ap-
plied to constituents depending on the scope) us-
ing negation words, shifters and negative polar ex-
pressions. The latter are subdivided into differ-
ent categories, such that special words are defined,
whose negative intensity is strong enough that they
have the power to change the polarity of the entire
text spans or constituents they are part of.
A similar approach is presented by Shaikh et al
(2007). The main difference to Moilanen and
Pulman (2007) lies in the representation format
on which the compositional model is applied.
While Moilanen and Pulman (2007) use syntac-
tic phrase structure trees, Shaikh et al (2007) con-
sider a more abstract level of representation be-
ing verb frames. The advantage of a more abstract
level of representation is that it more accurately
represents the meaning of the text it describes.
Apart from that, Shaikh et al (2007) design a
model for sentence-level classification rather than
for headlines or complex noun phrases.
The approach by Moilanen and Pulman (2007) is
not compared against another established classifi-
cation method whereas the approach by Shaikh et
al. (2007) is evaluated against a non-compositional
rule-based system which it outperforms.
3.3.2 Shallow Semantic Composition
Choi and Cardie (2008) present a more lightweight
approach using compositional semantics towards
classifying the polarity of expressions. Their
working assumption is that the polarity of a phrase
can be computed in two steps:
? the assessment of polarity of the constituents
? the subsequent application of a set of previously-
defined inference rules
An example rule, such as:
Polarity([NP1]? [IN] [NP2]?) = + (3)
may be applied to expressions, such as
[lack]?NP1 [of]IN [crime]?NP2 in rural areas.
The advantage of these rules is that they restrict
the scope of negation to specific constituents
rather than using the scope of the entire target
expression.
Such inference rules are very reminiscent of
polarity modification features (Wilson et al,
2005), as a negative polar expression is modified
by positive polar expression. The rules presented
by Choi and Cardie (2008) are, however, much
more specific, as they define syntactic contexts of
the polar expressions. Moreover, from each con-
text a direct polarity for the entire expression can
be derived. In (Wilson et al, 2005), this decision
is left to the classifier. The rules are also similar
to the syntactic rules from Moilanen and Pulman
(2007). However, they involve less linguistic
processing and are easier to comprehend4 . The
effectiveness of these rules are both evaluated in
rule-based methods and a machine learning based
method where they are anchored as constraints
in the objective function. The results of their
evaluation show that the compositional methods
outperform methods using simpler scopes for
negation, such as considering the scope of the
entire target expression. The learning method
incorporating the rules also slightly outperforms
the (plain) rule-based method.
3.3.3 Scope Modeling
In sentiment analysis, the most prominent work
examining the impact of different scope models
for negation is (Jia et al, 2009). The scope de-
tection method that is proposed considers:
? static delimiters
? dynamic delimiters
? heuristic rules focused on polar expressions
Static delimiters are unambiguous words, such as
because or unless marking the beginning of an-
other clause. Dynamic delimiters are, however,
4It is probably due to the latter, that these rules have
been successfully re-used in subsequent works, most promi-
nently Klenner et al (2009).
64
ambiguous, e.g. like and for, and require disam-
biguation rules, using contextual information such
as their pertaining part-of-speech tag. These de-
limiters suitably account for various complex sen-
tence types so that only the clause containing the
negation is considered.
The heuristic rules focus on cases in which po-
lar expressions in specific syntactic configurations
are directly preceded by negation words which re-
sults in the polar expression becoming a delimiter
itself. Unlike Choi and Cardie (2008), these rules
require a proper parse and reflect grammatical re-
lationships between different constituents.
The complexity of the scope model proposed
by Jia et al (2009) is similar to the ones of
the compositional models (Moilanen and Pulman,
2007; Shaikh et al, 2007; Choi and Cardie, 2008)
where scope modeling is exclusively incorporated
in the compositional rules.
Apart from scope modeling, Jia et al (2009) also
employ a complex negation term disambiguation
considering not only phrases in which potential
negation expressions do not have an actual negat-
ing function (as already used in (Wilson et al,
2005)), but also negative rhetorical questions and
restricted comparative sentences.
On sentence-level polarity classification, their
scope model is compared with
? a simple negation scope using a fixed window size
(similar to the negation feature in (Wilson et al, 2005))
? the text span until the first occurrence of a polar expres-
sion following the negation word
? the entire sentence
The proposed method consistently outperforms
the simpler methods proving that the incorpora-
tion of linguistic insights into negation modeling
is meaningful. Even on polarity document re-
trieval, i.e. a more coarse-grained classification
task where contextual disambiguation usually
results in a less significant improvement, the
proposed method also outperforms the other
scopes examined.
There have only been few research efforts in
sentiment analysis examining the impact of scope
modeling for negation in contrast to other research
areas, such as the biomedical domain (Huang and
Lowe, 2007; Morante et al, 2008; Morante and
Daelemans, 2009). This is presumably due to the
fact that only for the biomedical domain, publicly
available corpora containing annotation for the
scope of negation exist (Szarvas et al, 2008). The
usability of those corpora for sentiment analysis
has not been tested.
3.4 Negation within Words
So far, negation has only be considered as a phe-
nomenon that affects entire words or phrases.
The word expressing a negation and the words
or phrases being negated are disjoint. There are,
however, cases in which both negation and the
negated content which can also be opinionated
are part of the same word. In case, these words
are lexicalized, such as flaw-less, and are conse-
quently to be found a polarity lexicon, this phe-
nomenon does not need to be accounted for in sen-
timent analysis. However, since this process is (at
least theoretically) productive, fairly uncommon
words, such as not-so-nice, anti-war or offensive-
less which are not necessarily contained in lexical
resources, may emerge as a result of this process.
Therefore, a polarity classifier should also be able
to decompose words and carry out negation mod-
eling within words.
There are only few works addressing this particu-
lar aspect (Moilanen and Pulman, 2008; Ku et al,
2009) so it is not clear how much impact this type
of negation has on an overall polarity classification
and what complexity of morphological analysis is
really necessary. We argue, however, that in syn-
thetic languages where negation may regularly be
realized as an affix rather than an individual word,
such an analysis is much more important.
3.5 Negation in Various Languages
Current research in sentiment analysis mainly fo-
cuses on English texts. Since there are signifi-
cant structural differences among the different lan-
guages, some particular methods may only cap-
ture the idiosyncratic properties of the English lan-
guage. This may also affect negation modeling.
The previous section already stated that the need
for morphological analyses may differ across the
different languages.
Moreover, the complexity of scope modeling may
also be language dependent. In English, for ex-
ample, modeling the scope of a negation as a
fixed window size of words following the oc-
currence of a negation expression already yields
a reasonable performance (Kennedy and Inkpen,
2005). However, in other languages, for example
German, more complex processing is required as
the negated expression may either precede (Sen-
65
tence 10) or follow (Sentence 11) the negation ex-
pression. Syntactic properties of the negated noun
phrase (i.e. the fact whether the negated polar ex-
pression is a verb or an adjective) determine the
particular negation construction.
10. Peter mag den Kuchen nicht.
Peter likes the cake not.
?Peter does not like the cake.?
11. Der Kuchen ist nicht ko?stlich.
The cake is not delicious.
?The cake is not delicious.?
These items show that, clearly, some more ex-
tensive cross-lingual examination is required in or-
der to be able to make statements of the general
applicability of specific negation models.
3.6 Bad and Not Good are Not the Same
The standard approach of negation modeling sug-
gests to consider a negated polar expression, such
as not bad, as an unnegated polar expression with
the opposite polarity, such as good. Liu and Seneff
(2009) claim, however, that this is an oversimpli-
fication of language. Not bad and good may have
the same polarity but they differ in their respec-
tive polar strength, i.e. not bad is less positive
than good. That is why, Liu and Seneff (2009)
suggest a compositional model in which for indi-
vidual adjectives and adverbs (the latter include
negations) a prior rating score encoding their in-
tensity and polarity is estimated from pros and
cons of on-line reviews. Moreover, compositional
rules for polar phrases, such as adverb-adjective or
negation-adverb-adjective are defined exclusively
using the scores of the individual words. Thus,
adverbs function like universal quantifiers scaling
either up or down the polar strength of the specific
polar adjectives they modify. The model indepen-
dently learns what negations are, i.e. a subset of
adverbs having stronger negative scores than other
adverbs. In short, the proposed model provides
a unifying account for intensifiers (e.g. very), di-
minishers, polarity shifters and negation words. Its
advantage is that polarity is treated composition-
ally and is interpreted as a continuum rather than
a binary classification. This approach reflects its
meaning in a more suitable manner.
3.7 Using Negations in Lexicon Induction
Many classification approaches illustrated above
depend on the knowledge of which natural lan-
guage expressions are polar. The process of ac-
quiring such lexical resources is called lexicon in-
duction. The observation that negations co-occur
with polar expressions has been used for inducing
polarity lexicons on Chinese in an unsupervised
manner (Zagibalov and Carroll, 2008). One ad-
vantage of negation is that though the induction
starts with just positive polar seeds, the method
also accomplishes to extract negative polar expres-
sions since negated mentions of the positive po-
lar seeds co-occur with negative polar expressions.
Moreover, and more importantly, the distribution
of the co-occurrence between polar expressions
and negations can be exploited for the selection of
those seed lexical items. The model presented by
Zagibalov and Carroll (2008) relies on the obser-
vation that a polar expression can be negated but it
occurs more frequently without the negation. The
distributional behaviour of an expression, i.e. sig-
nificantly often co-occurring with a negation word
but significantly more often occurring without a
negation word makes up a property of a polar ex-
pression. The data used for these experiments are
publicly available5 .
3.8 Irony ? The Big Challenge
Irony is a rhetorical process of intentionally using
words or expressions for uttering meaning that is
different from the one they have when used liter-
ally (Carvalho et al, 2009). Thus, we consider
that the use of irony can reflect an implicit nega-
tion of what is conveyed through the literal use of
the words. Moreover, due to its nature irony is
mostly used to express a polar opinion.
Carvalho et al (2009) confirm the relevance of
(verbal) irony for sentiment analysis by an error
analysis of their present classifier stating that a
large proportion of misclassifications derive from
their system?s inability to account for irony.
They present predictive features for detecting
irony in positive sentences (which are actually
meant to have a negative meaning). Their find-
ings are that the use of emoticons or expressions
of gestures and the use of quotation marks within
a context in which no reported speech is included
are a good signal of irony in written text. Although
the use of these clues in the defined patterns helps
to detect some situations in which irony is present,
they do not fully represent the phenomenon.
5http://www.informatics.sussex.ac.uk/
users/tz21/coling08.zip
66
A data-driven approach for irony detection on
product-reviews is presented in (Tsur et al, 2010).
In the first stage, a considerably large list of simple
surface patterns of ironic expressions are induced
from a small set of labeled seed sentences. A pat-
tern is a generalized word sequence in which con-
tent words are replaced by a generic CW symbol.
In the second stage, the seed sentences are used to
collect more examples from the web, relying on
the assumption that sentences next to ironic ones
are also ironic. In addition to these patterns, some
punctuation-based features are derived from the
labeled sentences. The acquired patterns are used
as features along the punctuation-based features
within a k nearest neighbour classifier. On an in-
domain test set the classifier achieves a reasonable
performance. Unfortunately, these experiments
only elicit few additional insights into the general
nature of irony. As there is no cross-domain eval-
uation of the system, it is unclear in how far this
approach generalizes to other domains.
4 Limits of Negation Modeling in
Sentiment Analysis
So far, this paper has not only outlined the impor-
tance of negation modeling in sentiment analysis
but it has also shown different ways to account for
this linguistic phenomenon. In this section, we
present the limits of negation modeling in senti-
ment analysis.
Earlier in this paper, we stated that negation mod-
eling depends on the knowledge of polar expres-
sions. However, the recognition of genuine polar
expressions is still fairly brittle. Many polar ex-
pressions, such as disease are ambiguous, i.e. they
have a polar meaning in one context (Sentence 12)
but do not have one in another (Sentence 13).
12. He is a disease to every team he has gone to.
13. Early symptoms of the disease are headaches, fevers,
cold chills and body pain.
In a pilot study (Akkaya et al, 2009), it has al-
ready been shown that applying subjectivity word
sense disambiguation in addition to the feature-
based negation modeling approach of Wilson et al
(2005) results in an improvement of performance
in polarity classification.
Another problem is that some polar opinions are
not lexicalized. Sentence 14 is a negative prag-
matic opinion (Somasundaran and Wiebe, 2009)
which can only be detected with the help of exter-
nal world knowledge.
14. The next time I hear this song on the radio, I?ll throw
my radio out of the window.
Moreover, the effectiveness of specific negation
models can only be proven with the help of cor-
pora containing those constructions or the type of
language behaviour that is reflected in the mod-
els to be evaluated. This presumably explains why
rare constructions, such as negations using con-
nectives (Sentence 6 in ?2), modals (Sentence 7
in ?2) or other phenomena presented in the con-
ceptual model of Polanyi and Zaenen (2004), have
not yet been dealt with.
5 Conclusion
In this paper, we have presented a survey on
the role of negation in sentiment analysis. The
plethora of work presented on the topic proves that
this common linguistic construction is highly rel-
evant for sentiment analysis.
An effective negation model for sentiment analy-
sis usually requires the knowledge of polar expres-
sions. Negation is not only conveyed by common
negation words but also other lexical units, such as
diminishers. Negation expressions are ambiguous,
i.e. in some contexts do not function as a nega-
tion and, therefore, need to be disambiguated. A
negation does not negate every word in a sentence,
therefore, using syntactic knowledge to model the
scope of negation expressions is useful.
Despite the existence of several approaches to
negation modeling for sentiment analysis, in or-
der to make general statements about the effective-
ness of specific methods systematic comparative
analyses examining the impact of different nega-
tion models (varying in complexity) with regard to
classification type, text granularity, target domain,
language etc. still need to be carried out.
Finally, negation modeling is only one aspect that
needs to be taken into consideration in sentiment
analysis. In order to fully master this task, other
aspects, such as a more reliable identification of
genuine polar expressions in specific contexts, are
at least as important as negation modeling.
Acknowledgements
Michael Wiegand was funded by the BMBF project NL-
Search under contract number 01IS08020B. Alexandra Bal-
ahur was funded by Ministerio de Ciencia e Innovacio?n -
Spanish Government (grant no. TIN2009-13391-C04-01),
and Conselleria d?Educacio?n-Generalitat Valenciana (grant
no. PROMETEO/2009/119 and ACOMP/2010/286).
67
References
C. Akkaya, J. Wiebe, and R. Mihalcea. 2009. Subjec-
tivity Word Sense Disambiguation. In Proceedings
of EMNLP.
P. Carvalho, L. Sarmento, M. J. Silva, and
E. de Oliveira. 2009. Clues for Detecting
Irony in User-Generated Contents: Oh...!! It?s ?so
easy? ;-). In Proceedings of CIKM-Workshop TSA.
Y. Choi and C. Cardie. 2008. Learning with Compo-
sitional Semantics as Structural Inference for Sub-
sentential Sentiment Analysis. In Proceedings of
EMNLP.
Y. Huang and H. J. Lowe. 2007. A Novel Hybrid Ap-
proach to Automated Negation Detection in Clinical
Radiology Reports. JAMIA, 14.
L. Jia, C. Yu, and W. Meng. 2009. The Effect of Nega-
tion on Sentiment Analysis and Retrieval Effective-
ness. In Proceedings of CIKM.
A. Kennedy and D. Inkpen. 2005. Sentiment Classifi-
cation of Movie Reviews Using Contextual Valence
Shifters. In Proceedings of FINEXIN.
A. Kennedy and D. Inkpen. 2006. Sentiment Classifi-
cation of Movie Reviews Using Contextual Valence
Shifters. Computational Intelligence, 22.
M. Klenner, S. Petrakis, and A. Fahrni. 2009. Robust
Compositional Polarity Classification. In Proceed-
ings of RANLP.
L. Ku, T. Huang, and H. Chen. 2009. Using Morpho-
logical and Syntactic Structures for Chinese Opinion
Analysis. In Proceedings ACL/IJCNLP.
J. Liu and S. Seneff. 2009. Review Sentiment Scoring
via a Parse-and-Paraphrase Paradigm. In Proceed-
ings of EMNLP.
K. Moilanen and S. Pulman. 2007. Sentiment Con-
struction. In Proceedings of RANLP.
K. Moilanen and S. Pulman. 2008. The Good, the Bad,
and the Unknown. In Proceedings of ACL/HLT.
R. Morante and W. Daelemans. 2009. A Metalearning
Approach to Processing the Scope of Negation. In
Proceedings of CoNLL.
R. Morante, A. Liekens, and W. Daelemans. 2008.
Learning the Scope of Negation in Biomedical
Texts. In Proceedings of EMNLP.
V. Ng, S. Dasgupta, and S. M. Niaz Arifin. 2006. Ex-
amining the Role of Linguistic Knowledge Sources
in the Automatic Identification and Classification of
Reviews. In Proceedings of COLING/ACL.
B. Pang and L. Lee. 2004. A Sentimental Education:
Sentiment Analysis Using Subjectivity Summariza-
tion Based on Minimum Cuts. In Proceedings of
ACL.
B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs
up? Sentiment Classification Using Machine Learn-
ing Techniques. In Proceedings of EMNLP.
L. Polanyi and A. Zaenen. 2004. Context Valence
Shifters. In Proceedings of the AAAI Spring Sym-
posium on Exploring Attitude and Affect in Text.
M. A. M. Shaikh, H. Prendinger, and M. Ishizuka.
2007. Assessing Sentiment of Text by Semantic De-
pendency and Contextual Valence Analysis. In Pro-
ceedings of ACII.
S. Somasundaran and J. Wiebe. 2009. Recogniz-
ing Stances in Online Debates. In Proceedings of
ACL/IJCNLP.
G. Szarvas, V. Vincze, R. Farkas, and J. Csirik. 2008.
The BioScope Corpus: Annotation for Negation,
Uncertainty and Their Scope in Biomedical Texts.
In Proceedings of BioNLP.
O. Tsur, D. Davidov, and A. Rappoport. 2010.
ICWSM - A Great Catchy Name: Semi-Supervised
Recognition of Sarcastic Sentences in Online Prod-
uct Reviews. In Proceeding of ICWSM.
J. Wiebe. 1994. Tracking Point of View in Narrative.
Computational Linguistics, 20.
T. Wilson, J. Wiebe, and P. Hoffmann. 2005. Recog-
nizing Contextual Polarity in Phrase-level Sentiment
Analysis. In Proceedings of HLT/EMNLP.
T. Wilson, J. Wiebe, and P. Hoffmann. 2009. Rec-
ognizing Contextual Polarity: An Exploration for
Phrase-level Analysis. Computational Linguistics,
35:3.
T. Zagibalov and J. Carroll. 2008. Automatic Seed
Word Selection for Unsupervised Sentiment Classi-
fication of Chinese Text. In Proceedings of COL-
ING.
68
Proceedings of the Workshop on Language in Social Media (LASM 2013), pages 69?79,
Atlanta, Georgia, June 13 2013. c?2013 Association for Computational Linguistics
Towards the Detection of Reliable Food-Health Relationships
Michael Wiegand and Dietrich Klakow
Spoken Language Systems
Saarland University
D-66123 Saarbru?cken, Germany
{Michael.Wiegand|Dietrich.Klakow}@lsv.uni-saarland.de
Abstract
We investigate the task of detecting reli-
able statements about food-health relation-
ships from natural language texts. For that
purpose, we created a specially annotated
web corpus from forum entries discussing the
healthiness of certain food items. We ex-
amine a set of task-specific features (mostly)
based on linguistic insights that are instrumen-
tal in finding utterances that are commonly
perceived as reliable. These features are in-
corporated in a supervised classifier and com-
pared against standard features that are widely
used for various tasks in natural language pro-
cessing, such as bag of words, part-of speech
and syntactic parse information.
1 Introduction
In this paper, we explore some linguistic high-level
features to detect food-health relationships in natural
language texts that are perceived reliable. By food-
health relationships we mean relations that claim
that a food item is suitable (1) or unsuitable (2) for
some particular health condition.
(1) Baking soda is an approved remedy against
heartburn.
(2) During pregnancy women should not consume
any alcohol.
The same health claim may be uttered in differ-
ent ways (3)-(5) and, as a consequence, may be per-
ceived and judged differently. For the automatic ex-
traction of health claims, we believe that statements
that are perceived as reliable (4)-(5) are the most im-
portant to retrieve.
(3) Eggs do not have a negative impact on people
suffering from heart diseases.
(4) According to a leading medical scientist, the
consumption of eggs does not have a negative
impact on people suffering from heart diseases.
(5) I?m suffering from a heart disease and all my
life I?ve been eating many eggs; it never had
any impact on my well-being.
In this work, we will mine a web corpus of fo-
rum entries for such relations. Social media are a
promising source of such knowledge as, firstly, the
language employed is not very technical and thus,
unlike medical texts, accessible to the general pub-
lic. Secondly, social media can be considered as an
exclusive repository of popular wisdom. With re-
gard to the health conditions, we can find, for ex-
ample, home remedies. Despite the fact that many
of them are not scientifically proven, there is still a
great interest in that type of knowledge. However,
even though such content is usually not subject to
any scientific review, users would appreciate an au-
tomatic assessment of the quality of each relation ex-
pressed. In this work, we attempt a first step towards
this endeavour by automatically classifying these ut-
terances with regard to reliability.
The features we examine will be (mostly) based
on linguistic insights that are instrumental in finding
utterances that are commonly perceived as reliable.
These features are incorporated in a supervised clas-
sifier and compared against standard features that
are widely used for various tasks in natural language
processing, such as bag of words, part-of speech and
syntactic parse information.
69
Our experiments are carried out on German data.
We believe, however, that our findings carry over
to other languages since the linguistic aspects that
we address are (mostly) language universal. For the
sake of general accessibility, all examples will be
given as English translations.
2 Related Work
As far as the extraction of health relations from
social media are concerned, the prediction of epi-
demics (Fisichella et al, 2011; Torii et al, 2011;
Diaz-Aviles et al, 2012; Munro et al, 2012) has re-
cently attracted the attention of the research commu-
nity.
Relation extraction involving food items has also
been explored in the context of ontology align-
ment (van Hage et al, 2005; van Hage et al, 2006;
van Hage et al, 2010) and also as a means of knowl-
edge acquisition for virtual customer advice in a su-
permarket (Wiegand et al, 2012a).
The works most closely related to this paper are
Yang et al (2011) and Miao et al (2012). Both
of these works address the extraction of food-health
relationships. Unlike this work, they extract rela-
tions from scientific biomedical texts rather than so-
cial media. Yang et al (2011) also cover the task
of strength analysis which bears some resemblance
to the task of finding reliable utterances to some ex-
tent. However, the features applied to that classifica-
tion task are only standard features, such as bag of
words.
3 Data & Annotation
As a corpus for our experiments, we used a crawl
of chefkoch.de1 (Wiegand et al, 2012a) consisting
of 418, 558 webpages of food-related forum entries.
chefkoch.de is the largest web portal for food-related
issues in the German language. From this dataset,
sentences in which some food item co-occurred with
some health condition (e.g. pregnancy, diarrhoea
or flu) were extracted. (In the following, we will
also refer to these entities as target food item and
target health condition.) The food items were iden-
tified with the help of GermaNet (Hamp and Feld-
weg, 1997), the German version of WordNet (Miller
et al, 1990), and the health conditions were used
1www.chefkoch.de
from Wiegand et al (2012b). In total, 2604 sen-
tences were thus obtained.
For the manual annotation, each target sentence
(i.e. a sentence with a co-occurrence of target food
item and health condition) was presented in combi-
nation with the two sentences immediately preced-
ing and following it. Each target sentence was man-
ually assigned two labels, one specifying the type
of suitability (?3.1) and another specifying whether
the relation expressed is considered reliable or not
(?3.2).
3.1 Types of Suitability
The suitability-label indicates whether a polar rela-
tionship holds between the target food item and the
target health condition, and if so, which. Rather than
just focusing on positive polarity, i.e. suitability,
and negative polarity, i.e. unsuitability, we consider
more fine-grained classes. As such, the suitability-
label does not provide any explicit information about
the reliability of the utterance. In principle, ev-
ery polar relationship between target food item and
health condition expressed in a text could also be
formulated in such a way that it is perceived reliable.
In this work, we will consider the suitability-label as
given. We use it as a feature in order to measure the
correlation between suitability and reliability. The
usage of fine-grained labels is to investigate whether
subclasses of suitability or unsuitability have a ten-
dency to co-occur with reliability. (In other words:
we may assume differences among labels with the
same polarity type.) We define the following set of
fine-grained suitability-labels:
3.1.1 Suitable (SUIT)
SUIT encompasses all those statements in which
the consumption of the target food item is claimed to
be suitable for people affected by a particular health
condition (6). By suitable, we mean that there will
not be a negative effect on the health of a person
once he or she consumes the target food item. How-
ever, this relation type does not state that the con-
sumption is likely to improve the condition of the
person either.
(6) I also got dermatitis which is why my mother
used spelt flour [instead of wheat flour]; you
don?t taste a difference.
70
positive labels BENEF, SUIT, PREVENT
negative labels UNSUIT, CAUSE
Table 1: Categorization of suitability-labels.
3.1.2 Beneficial (BENEF)
While SUIT only states that the consumption of
the target food item is suitable for people with a par-
ticular health condition, BENEF actually states that
the consumption alleviates the symptoms of the con-
dition or even cures it (7). While both SUIT and
BENEF have a positive polarity, SUIT is much more
neutral than BENEF.
(7) Usually, a glass of milk helps me when I got a
sore throat.
3.1.3 Prevention (PREVENT)
An even stronger positive effect than the relation
type BENEF presents PREVENT which claims that
the consumption of the target food item can prevent
the outbreak of a particular disease (8).
(8) Citric acid largely reduces the chances of
kidney stones to develop.
3.1.4 Unsuitable (UNSUIT)
UNSUIT describes cases in which the consump-
tion of the target food item is deemed unsuitable (9).
Unsuitability means that one expects a negative ef-
fect (but it need not be mentioned explicitly), that
is, a deterioration of the health situation on the part
of the person who is affected by a particular health
condition.
(9) Raw milk cheese should not be eaten during
pregnancy.
3.1.5 Causation (CAUSE)
CAUSE is the negative counterpart of PREVENT.
It states that the consumption of the target food item
can actually cause a particular health condition (10).
(10) It?s a common fact that the regular consumption
of coke causes caries.
The suitability-labels can also be further sepa-
rated into two polar classes (i.e. positive and neg-
ative labels) as displayed in Table 1.
3.2 Reliability
Each utterance was additionally labeled as to
whether it was considered reliable (4)-(5) or not (3).
It is this label that we try to predict in this work. By
reliable, we understand utterances in which the re-
lations expressed are convincing in the sense that a
reputable source is cited, some explanation or empir-
ical evidence for the relation is given, or the relation
itself is emphasized by the speaker. In this work,
we are exclusively interested in detecting utterances
which are perceived reliable by the reader. We leave
aside whether the statements from our text corpus
are actually correct. Our aim is to identify linguis-
tic cues that evoke the impression of reliability on
behalf of the reader.
3.3 Class Distributions and Annotation
Agreement
Table 2 depicts the distribution of the reliability-
labels on our corpus while Table 3 lists the class dis-
tribution of the suitability-labels including the pro-
portion of the reliable instances among each cate-
gory. The proportion of reliable instances varies
quite a lot among the different suitability-labels,
which indicates that the suitability may be some ef-
fective feature.
Note that the class OTHER in Table 3 comprises
all instances in which the co-occurrence of a health
condition and a food item was co-incidental (11) or
there was some embedding that discarded the valid-
ity of the respective suitability-relation, as it is the
case, for example, in questions (12).
(11) It?s not his diabetes I?m concerned about but
the enormous amounts of fat that he consumes.
(12) Does anyone know whether I can eat tofu dur-
ing my pregnancy?
In order to measure interannotation agreement,
we collected for three health conditions their co-
occurrences with any food item. For the suitability-
labels we computed Cohen?s ? = 0.76 and for the
reliability-labels ? = 0.61. The agreement for reli-
ability is lower than for suitability. We assume that
the reason for that lies in the highly subjective no-
tion of reliability. Still, both agreements can be in-
terpreted as substantial (Landis and Koch, 1977) and
should be sufficiently high for our experiments.
71
Type Frequency Percentage
Reliable 480 18.43
Not Reliable 2124 81.57
Table 2: Distribution of the reliability-labels.
Type Frequency Perc. Perc. Reliable
BENEF 502 19.28 33.39
CAUSE 482 18.51 22.57
SUIT 428 16.44 17.91
UNSUIT 277 10.64 34.05
PREVENT 74 2.84 14.04
OTHER 841 32.30 0.00
Table 3: Distribution of the suitability-labels.
4 Feature Design
4.1 Task-specific High-level Feature Types
We now describe the different task-specific high-
level feature types. We call them high-level feature
types since they model concepts that typically gen-
eralize over sets of individual words (i.e. low-level
features).
4.1.1 Explanatory Statements (EXPL)
The most obvious type of reliability is a
suitability-relation that is also accompanied by some
explanatory statement. That is, some reason for the
relation expressed is given (13). We detect reasons
by scanning a sentence for typical discourse cues
(more precisely: conjunctions) that anchor such re-
marks, e.g. which is why or because.
(13) Honey has an antiseptic effect which is why it
is an ideal additive to milk in order to cure a
sore throat.
4.1.2 Frequent Observation (FREQ)
If a speaker claims to have witnessed a certain
relation very frequently or even at all times, then
there is a high likelihood that this relation actually
holds (14). We use a set of adverbs (18 expressions)
that express high frequency (e.g. often, frequently
etc.) or constancy (e.g. always, at all times etc.).
(14) What always helps me when I have the flu is a
hot chicken broth.
4.1.3 Intensifiers (INTENS)
Some utterances may also be perceived reliable if
their speaker adds some emphasis to them. One way
of doing so is by adding intensifiers to a remark (15).
(15) You can treat nausea with ginger very effec-
tively.
The intensifiers we use are a translation of the lex-
icon introduced in Wilson et al (2005). For the de-
tection, we divide that list into two groups:
The first group INTENSsimple are unambiguous
adverbs that always function as intensifiers no mat-
ter in which context they appear (e.g. very or ex-
tremely).
The second group includes more ambiguous ex-
pressions, such as adjectives that only function as
an intensifier if they modify a polar expression
(e.g. horrible pain or terribly nice) otherwise they
function as typical polar expressions (e.g. you
are horrible? or he sang terribly?). We employ
two methods to detect these ambiguous expressions.
INTENSpolar requires a polar expression of a polar-
ity lexicon to be modified by the intensifier, while
INTENSadj requires an adjective to be modified. In
order to identify polar expressions we use the polar-
ity lexicon underlying the PolArt system (Klenner
et al, 2009). We also consider adjectives since we
must assume that our polarity lexicon does not cover
all possible polar expressions. We chose adjectives
as a complement criterion as this part of speech is
known to contain most polar expressions (Hatzivas-
siloglou and McKeown, 1997; Hatzivassiloglou and
Wiebe, 2000).
4.1.4 Strong Polar Expressions (STROPO)
Instead of adding intensifiers in order to put more
emphasis to a remark (?4.1.3), one may also use
polar expressions that convey a high polar inten-
sity (16). For instance, nice and excellent refer to
the same scale and convey positive polarity but ex-
cellent has a much higher polar intensity than nice.
Taboada et al (2011) introduced an English polar-
ity lexicon SO-CAL in which polar expressions were
also assigned an intensity label. As our German
polarity lexicon (?4.1.3) does not contain compara-
ble intensity labels, we used a German translation
of SO-CAL. We identified polar expressions with a
high intensity score (i.e. ?4 or ?5) as strong po-
lar expressions. It includes 221 highly positive and
344 highly negative polar expressions. We also dis-
tinguish the polarity type (i.e. STROPO+ refers to
positive and STROPO? refers to negative polarity).
72
(16) Baking soda is an excellent remedy against
heartburn.
4.1.5 Superlatives (SUPER)
Another way of expressing high polar intensity is
by applying superlatives (17). Superlatives can only
be formed from gradable adjectives. At the same
time, the greatest amount of such adjectives are also
subjective expressions (Hatzivassiloglou and Wiebe,
2000). As a consequence, the detection of this
grammatical category does not depend on a subjec-
tivity/polarity lexicon but on simple morphological
suffixes (e.g. -est in strongest)2 or combinations
with certain modifiers (e.g. most in most terrific).
(17) Baking soda is the most effective remedy
against heartburn.
4.1.6 Statements Made by Authorities (AUTH)
If a statement is quoted from an authority, then it
is usually perceived more reliable than other state-
ments (4). Authorities in our domain are mostly sci-
entists and medical doctors. Not only does a men-
tion of those types of professions indicate an author-
ity but also the citation of their work. Therefore,
for this feature we also scan for expressions, such as
journal, report, survey etc. Our final look-up list of
cues comprises 53 expressions.
We also considered using the knowledge of user
profiles in order to identify speakers whose profes-
sion fall under our defined set of authorities. Unfor-
tunately, the overwhelming majority of users who
actually specified their profession cannot be consid-
ered as authorities (for the relations that we are inter-
ested in) by mere consideration of their profession.
Most users of chefkoch.de are either office employ-
ees, housewifes, students or chefs. Less than 1% are
authorities according to our definition. Due to the
severe sparsity of authorities, we refrained from us-
ing the professions as they are specified in the user
profiles.
2We could not use part-of-speech tagging for the detec-
tion of superlatives since, unlike the standard English part-of-
speech tag set (i.e. the Penn Treebank Tag Set (Marcus et al,
1993)), information regarding gradation (i.e. comparative and
superlative) is not reflected in the standard German tag set (i.e.
Stuttgart Tu?binger Tag Set (Schiller et al, 1995)).
4.1.7 Doctors? Prescriptions (PRESC)
Some of our food-health relations are also men-
tioned in the context of doctors? prescriptions (5).
That is, a doctor may prescribe a patient to con-
sume a particular food item since it is considered
suitable for their health condition, or he/she may
forbid a food item in case it is considered unsuit-
able. As already pointed out in ?4.1.6, doctors usu-
ally present an authority with regard to food-health
relations. That is why, their remarks should be con-
sidered reliable.
In order to detect doctors? prescriptions, we
mainly look for (modal) verbs in a sentence that ex-
press obligations or prohibitions. We found that, on
our dataset, people rarely mention their doctor ex-
plicitly if they refer to a particular prescription. In-
stead, they just mention that they must or must not
consume a particular food item. From the context,
however, it is obvious that they refer to their doc-
tor?s prescription (18).
(18) Due to my diabetes I must not eat any sweets.
4.1.8 Hedge Cues (HEDGE)
While all previous features were designed to iden-
tify cases of reliable statements, we also include fea-
tures that indicate the opposite. The most obvious
type of utterances that are commonly considered un-
reliable are so-called hedges (Lakoff, 1973) or spec-
ulations (19).
(19) Coke may cause cancer.
For this feature, we use a German translation of En-
glish cue words that have been found useful in pre-
vious work (Morante and Daelemans, 2009) which
results in 47 different expressions.
4.1.9 Types of Suitability-Relations (REL)
Finally, we also incorporate the information
about what type of suitability-relation a statement
was labeled with. The suitability-labels were al-
ready presented and motivated in ?3.1. The con-
crete features are: RELSUIT (?3.1.1), RELBENEF
(?3.1.2), RELPREVENT (?3.1.3), RELUNSUIT
(?3.1.4), RELCAUSE (?3.1.5).
73
Suffix Description
-WNDfood context window around food item
-WNDcond context window around health condition
-TS target sentence only
-EC entire (instance) context
Table 4: Variants for the individual feature types.
4.2 Variants of Feature Types
For our feature types we examine several variants
that differ in the size of context/scope. We distin-
guish between the target sentence and the entire con-
text of an instance, i.e. the target sentence plus the
two preceding and following sentences (?3). If only
the target sentence is considered, we can also con-
fine the occurrence of a cue word to a fixed window
(comprising 5 words) either around the target food
item or the target health condition rather than con-
sidering the entire sentence.
Small contexts usually offer a good precision. For
example, if a feature type occurs nearby a mention
of the target food item or health condition, the fea-
ture type and the target expression are likely to be
related to each other. The downside of such narrow
contexts is that they may be too sparse. Wide con-
texts may be better suited to situations in which a
high recall is desirable. However, ambiguous fea-
ture types may perform poorly with these contexts
as their co-occurrence with a target expression at a
large distance is likely to be co-incidental.
Table 4 lists all the variants that we use. These
variants are applied to all feature types except the
types of suitability (?4.1.9) as this label has only
been assigned to an entire target sentence.
4.3 Other Features
Table 5 lists the entire set of features that we ex-
amine in this work. The simplest classifier that we
can construct for our task is a trivial classifier that
predicts all statements as reliable statements. The
remaining features comprise bag of words, part-of-
speech and syntactic parse information. For the
latter two features, we employ the output of the
Stanford Parser for German (Rafferty and Manning,
2008).
Features Description
all trivial classifier that always predicts a reliable state-
ment
bow bag-of-words features: all words between the target
food item and target health condition and the words
immediately preceding and following each of them
pos part-of-speech features: part-of-speech sequence be-
tween target food item and health condition and tags
of the words immediately preceding and following
each of the target expressions
synt path from syntactic parse tree from target food item
to target health condition
task all task-specific high-level feature types from ?4.1
with their respective variants (?4.2)
Table 5: Description of all feature sets.
5 Experiments
Each instance to be classified is a sentence in which
there is a co-occurrence of a target food item and
a target health condition along its respective con-
text sentences (?3). We only consider sentences in
which the co-occurrence expresses an actual suit-
ability relationship between the target food item and
the target health condition, that is, we ignore in-
stances labeled with the suitability-label OTHER
(?3.3). We make this restriction as the instances
labeled as OTHER are not eligible for being reli-
able statements (Table 3). In this work, we take the
suitability-labels for granted (this allows us to easily
exclude the instances labeled as OTHER). The au-
tomatic detection of suitability-labels would require
a different classifier with a different set of features
whose appropriate discussion would be beyond the
scope of this paper.
5.1 Comparison of the Different Task-specific
High-level Features
In our first experiment, we want to find out how
the different task-specific high-level features that we
have proposed in this work compare to each other.
More specifically, we want to find out how the in-
dividual features correlate with the utterances that
have been manually marked as reliable. For that
purpose, Table 6 shows the top 20 features accord-
ing to Chi-square feature selection computed with
WEKA (Witten and Frank, 2005). More informa-
tion regarding the computation of Chi-square statis-
tics in the context of text classification can be found
in Yang and Pederson (1997). Note that we apply
feature selection only as a means of feature compar-
74
Rank Feature Score
1 FREQ-WNDfood 105.1
2 FREQ-TS 102.8
3 FREQ-WNDcond 75.9
4 FREQ-EC 29.2
5 AUTH-EC 23.7
6 STROPO+-WNDcond 20.5
7 RELBENEF 20.2
8 RELSUIT 16.8
9 INTENSsimple-WNDcond 16.4
10 AUTH-TS 15.4
11 STROPO+-TS 15.0
12 INTENSsimple-EC 14.1
13 STROPO+-WNDfood 13.7
14 INTENSadj-WNDfood 13.2
15 INTENSsimple-WNDfood 12.1
16 INTENSsimple-TS 11.6
17 PRESC-WNDfood 11.0
18 INTENSadj-WNDcond 9.7
19 INTENSpolar-EC 9.0
20 AUTH-WNDfood 7.9
Table 6: Top 20 features according to Chi-square fea-
ture ranking (for each feature type the most highly ranked
variant is highlighted).
ison. For classification (?5.2), we will use the entire
feature set.
5.1.1 What are the most effective features?
There are basically five feature types that dom-
inate the highest ranks. They are FREQ, AUTH,
STROPO, REL and INTENS. This already indicates
that several features presented in this work are ef-
fective. It is interesting to see that two types of
suitability-labels, i.e. RELBENEF and RELSUIT ,
are among the highest ranked features which sug-
gests that suitability and reliability are somehow
connected.
Table 7 shows both precision and recall for each
of the most highly ranked variant of the feature types
that appear on the top 20 ranks according to Chi-
square ranking (Table 6). Thus, we can have an idea
in how far the high performing feature types differ.
We only display one feature per feature type due to
the limited space. The table shows that for most of
these features precision largely outperforms recall.
RELBENEF is the only notable exception (its recall
actually outperforms precision).
5.1.2 Positive Orientation and Reliability
By closer inspection of the highly ranked features,
we found quite a few features with positive ori-
Feature Prec Rec
FREQ-WNDfood 71.13 14.38
AUTH-EC 41.81 15.42
STROPO+-WNDcond 63.38 3.54
RELBENEF 33.39 39.17
INTENSsimple-WNDcond 41.73 11.04
PRESC-WNDfood 45.00 5.63
Table 7: Precision and recall of different features (we list
the most highly ranked variants of the feature types from
Table 6).
entation, i.e. STROPO+-WNDcond, RELBENEF ,
RELSUIT , STROPO+-WNDcond, while their nega-
tive counterparts are absent. This raises the question
whether there is a bias for positive orientation for the
detection of reliability.
We assume that there are different reasons why
the positive suitability-labels (RELBENEF and
RELSUIT ) and strong positive polarity (STROPO+)
are highly ranked features:
As far as polarity features are concerned, it is
known from sentiment analysis that positive polar-
ity is usually easier to detect than negative polar-
ity (Wiegand et al, 2013). This can largely be as-
cribed to social conventions to be less blunt with
communicating negative sentiment. For that reason,
for example, one often applies negated positive polar
expressions (e.g. not okay) or irony to express a neg-
ative sentiment rather than using an explicit negative
polar expression. Of course, such implicit types of
negative polarity are much more difficult to detect
automatically.
The highly ranked suitability-labels may be labels
with the same orientation (i.e. they both describe
relationships that a food item is suitable rather than
unsuitable for a particular health condition), yet they
have quite different properties.3 While RELBENEF
is a feature positively correlating with reliable ut-
terances, the opposite is true of RELSUIT , that is,
there is a correlation but this correlation is nega-
tive. Table 8 compares their respective precision
and also includes the trivial (reference) classifier all
that always predicts a reliable statement. The ta-
ble clearly shows that RELBENEF is above the triv-
3It is not the case that the proportion of reliable utterances
is larger among the entire set of instances tagged with positive
suitability-labels than among the instances tagged with negative
suitability-labels (Table 1). In both cases, they are at approx.
26%.
75
ial feature while RELSUIT is clearly below. (One
may wonder why the gap in precision between those
different features is not larger. These features are
also high-recall features ? we have shown this for
RELBENEF in Table 7 ? so the smaller gaps may
already have a significant impact.) In plain, this
result means that a statement conveying that some
food item alleviates the symptoms of a particular
disease or even cures it (RELBENEF ) is more likely
to be an utterance that is perceived reliable rather
than statements in which the speaker merely states
that the food item is suitable given a particular health
condition (RELSUIT ). Presumably, the latter type
of suitability-relations are mostly uttered parenthet-
ically (not emphatically), or they are remarks in
which the relation is inferred, so that they are un-
likely to provide further background information. In
Sentence (20), for example, the suitability of whole-
meal products is inferred as the speaker?s father eats
these types of food due to his diabetes. The focus
of this remark, however, is the psychic well-being of
the speaker?s father. That entire utterance does not
present any especially reliable or otherwise helpful
information regarding the relationship between dia-
betes and wholemeal products.
(20) My father suffers from diabetes and is fed up
with eating all these wholemeal products. We
are worried that he is going to fall into a de-
pression.
Having explained that the two (frequently occur-
ring) positive suitability-labels are highly ranked
features because they separate reliable from less re-
liable statements, one may wonder why we do not
find a similar behaviour on the negative suitability-
labels. The answer to this lies in the fact that there
is no similar distinction between RELBENEF and
RELSUIT among utterances expressing unsuitabil-
ity. There is no neutral negative suitability-label
similar to RELSUIT . The relation RELUNSUIT
expresses unsuitability which is usually connected
with some deterioration in health.
5.1.3 How important are explanatory
statements for this task?
We were very surprised that the feature type to
indicate explanatory statements EXPL (?4.1.1) per-
formed very poorly (none of its variants is listed in
Feature RELSUIT all RELBENEF
Prec 17.81 26.46 33.39
Table 8: The precision of different REL-features com-
pared to the trivial classifier all that always predicts a re-
liable utterance.
Type EXPLall EXPLcue
Percentage 22.59 8.30
Table 9: Proportion of explanatory statements among re-
liable utterances (EXPLall: all reliable instances that are
explanatory statements; EXPLcue: subset of explanatory
statements that also contain a lexical cue).
Table 6) since we assumed explanatory statements
to be one of the most relevant types of utterances.
In order to find a reason for this, we manually an-
notated all reliable utterances as to whether they can
be regarded as an explanatory statement (EXPLall)
and, if so, whether (in principle) there are lexical
cues (such as our set of conjunctions) to identify
them (EXPLcue). Table 9 shows the proportion of
these two categories among the reliable utterances.
With more than 20% being labeled as this subtype,
explanatory statements are clearly not a fringe phe-
nomenon. However, lexical cues could only be ob-
served in approximately 1/3 of those instances. The
majority of cases, such as Sentence (21), do not con-
tain any lexical cues and are thus extremely difficult
to detect.
(21) Citrus fruits are bad for dermatitis. They in-
crease the itch. Such fruits are rich in acids that
irritate your skin.
In addition, all variants of our feature type EXPL
have a poor precision (between 20 ? 25%). This
means that the underlying lexical cues are too am-
biguous.
5.1.4 How important are the different
contextual scopes?
Table 6 clearly shows that the contextual scope
of a feature type matters. For example, for the fea-
ture type FREQ, the most effective scope achieves
a Chi-square score of 105.1 while the worst vari-
ant only achieves a score of 29.2. However, there
is no unique contextual scope which always outper-
forms the other variants. This is mostly due to the
76
Feature Set Prec Rec F1
all 26.46 100.00 41.85
bow 37.14 62.44 46.45
bow+pos 36.85 57.64 44.88
bow+synt 39.05 58.01 46.58
task 35.16 72.89 47.21
bow+task 42.54 66.01 51.56?
Table 10: Comparison of different feature sets (summary
of features is displayed in Table 5); ? significantly better
than bow at p < 0.05 (based on paired t-test).
fact the different feature types have different proper-
ties. On the one hand, there are unambiguous fea-
ture types, such as AUTH, which work fine with
a wide scope. But we also have ambiguous fea-
ture types that require a fairly narrow context. A
typical example are strong (positive) polar expres-
sions (STROPO+). (Polar expressions are known
to be very ambiguous (Wiebe and Mihalcea, 2006;
Akkaya et al, 2009).)
5.2 Classification
Table 10 compares the different feature sets with
regard to extraction performance. We carry out
a 5-fold cross-validation on our manually labeled
dataset. As a classifier, we chose Support Vector
Machines (Joachims, 1999). As a toolkit, we use
SVMLight4 with a linear kernel.
Table 10 clearly shows the strength of the high-
level features that we proposed. They do not only
represent a strong feature set on their own but they
can also usefully be combined with bag-of-words
features. Apparently, neither part-of-speech nor
parse information are predictive for this task.
5.3 Impact of Training Data
Figure 1 compares bag-of-words features and our
task-specific high-level features on a learning curve.
The curve shows that the inclusion of our task-
specific features improves performance. Interest-
ingly, with task alone we obtain a good performance
on smaller amounts of data. However, this classifier
is already saturated with 40% of the training data.
From then onwards, it is more effective to use the
combination bow+task. Our high-level features gen-
eralize well which is particularly important for situ-
ations in which only few training data are available.
4http://svmlight.joachims.org
 25
 30
 35
 40
 45
 50
 55
 10  20  30  40  50  60  70  80  90  100
F-
sc
or
e
Percentage of training data
bag of words (bow)
task-specific high-level features (task)
combination (bow+task)
Figure 1: Learning curve of the different feature sets.
However, in situations in which large training sets
are available, we additionally need bag of words that
are able to harness more sparse but specific informa-
tion.
6 Conclusion
In this paper, we examined a set of task-specific
high-level features in order to detect food-health re-
lations that are perceived reliable. We found that,
in principle, a subset of these features that include
adverbials expressing frequent observations, state-
ments made by authorities, strong polar expressions
and intensifiers are fairly predictive and complement
bag-of-words information. We also observed a cor-
relation between some suitability-labels and relia-
bility. Moreover, the effectiveness of the different
features depends very much on the context to which
they are applied.
Acknowledgements
This work was performed in the context of the Software-
Cluster project EMERGENT. Michael Wiegand was
funded by the German Federal Ministry of Education and
Research (BMBF) under grant no. ?01IC10S01?. The
authors would like to thank Stephanie Ko?ser for annotat-
ing the dataset presented in the paper. The authors would
also like to thank Prof. Dr. Wolfgang Menzel for provid-
ing the German version of the SO-CAL polarity lexicon
that has been developed at his department.
77
References
Cem Akkaya, Janyce Wiebe, and Rada Mihalcea. 2009.
Subjectivity Word Sense Disambiguation. In Proceed-
ings of the Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP), pages 190?199,
Singapore.
Ernesto Diaz-Aviles, Avar Stewart, Edward Velasco, Ker-
stin Denecke, and Wolfgang Nejdl. 2012. Epidemic
Intelligence for the Crowd, by the Crowd. In Proceed-
ings of the International AAAI Conference on Weblogs
and Social Media (ICWSM), Dublin, Ireland.
Marco Fisichella, Avar Stewart, Alfredo Cuzzocrea, and
Kerstin Denecke. 2011. Detecting Health Events on
the Social Web to Enable Epidemic Intelligence. In
Proceedings of the International Symposium on String
Processing and Information Retrieval (SPIRE), pages
87?103, Pisa, Italy.
Birgit Hamp and Helmut Feldweg. 1997. GermaNet - a
Lexical-Semantic Net for German. In Proceedings of
ACL workshop Automatic Information Extraction and
Building of Lexical Semantic Resources for NLP Ap-
plications, pages 9?15, Madrid, Spain.
Vasileios Hatzivassiloglou and Kathleen R. McKeown.
1997. Predicting the Semantic Orientation of Adjec-
tives. In Proceedings of the Conference on European
Chapter of the Association for Computational Linguis-
tics (EACL), pages 174?181, Madrid, Spain.
Vasileios Hatzivassiloglou and Janyce Wiebe. 2000. Ef-
fects of Adjective Orientation and Gradability on Sen-
tence Subjectivity. In Proceedings of the International
Conference on Computational Linguistics (COLING),
pages 299?305, Saarbru?cken, Germany.
Thorsten Joachims. 1999. Making Large-Scale SVM
Learning Practical. In B. Scho?lkopf, C. Burges, and
A. Smola, editors, Advances in Kernel Methods - Sup-
port Vector Learning. MIT Press.
Manfred Klenner, Stefanos Petrakis, and Angela Fahrni.
2009. Robust Compositional Polarity Classification.
In Proceedings of Recent Advances in Natural Lan-
guage Processing (RANLP), pages 180?184, Borovets,
Bulgaria.
George Lakoff. 1973. Hedging: A Study in Media Crite-
ria and the Logic of Fuzzy Concepts. Journal of Philo-
sophical Logic, 2:458 ? 508.
J. Richard Landis and Gary G. Koch. 1977. The Mea-
surement of Observer Agreement for Categorical Data.
Biometrics, 33(1):159?174.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a Large Annotated
Corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19(2):313?330, June. Special Issue
on Using Large Corpora.
Qingliang Miao, Shu Zhang, Bo Zhang, Yao Meng, and
Hao Yu. 2012. Extracting and Visualizing Seman-
tic Relationships from Chinese Biomedical Text. In
Proceedings of the Pacific Asia Conference on Lan-
guage, Information and Compuation (PACLIC), pages
99?107, Bali, Indonesia.
George Miller, Richard Beckwith, Christiane Fellbaum,
Derek Gross, and Katherine Miller. 1990. Introduc-
tion to WordNet: An On-line Lexical Database. Inter-
national Journal of Lexicography, 3:235?244.
Roser Morante and Walter Daelemans. 2009. Learning
the Scope of Hedge Cues in Biomedical Texts. In Pro-
ceedings of the BioNLP Workshop, pages 28?36, Boul-
der, CO, USA.
Robert Munro, Lucky Gunasekara, Stephanie Nevins,
Lalith Polepeddi, and Evan Rosen. 2012. Track-
ing Epidemics with Natural Language Processing and
Crowdsourcing. In Proceedings of the Spring Sympo-
sium for Association for the Advancement of Artificial
Intelligence (AAAI), pages 52?58, Toronto, Canada.
Anna Rafferty and Christopher D. Manning. 2008. Pars-
ing Three German Treebanks: Lexicalized and Un-
lexicalized Baselines. In Proceedings of the ACL
Workshop on Parsing German (PaGe), pages 40?46,
Columbus, OH, USA.
Anne Schiller, Simone Teufel, Christine Sto?ckert, and
Christine Thielen. 1995. Vorla?ufige Guidelines
fu?r das Tagging deutscher Textcorpora mit STTS.
Technical report, Universita?t Stuttgart, Insitut fu?r
maschinelle Sprachverarbeitung, and Seminar fu?r
Sprachwissenschaft, Universita?t Tu?bingen.
Maite Taboada, Julian Brooke, Milan Tofiloski, Kimberly
Voll, and Manfred Stede. 2011. Lexicon-Based Meth-
ods for Sentiment Analysis. Computational Linguis-
tics, 37(2):267 ? 307.
Manabu Torii, Lanlan Yin, Thang Nguyen, Chand T.
Mazumdar, Hongfang Liu, David M. Hartley, and
Noele P. Nelson. 2011. An exploratory study of a text
classification framework for internet-based surveil-
lance of emerging epidemics. Internal Journal of
Medical Informatics, 80(1):56?66.
Willem Robert van Hage, Sophia Katrenko, and Guus
Schreiber. 2005. A Method to Combine Linguistic
Ontology-Mapping Techniques. In Proceedings of In-
ternational Semantic Web Conference (ISWC), pages
732 ? 744, Galway, Ireland. Springer.
Willem Robert van Hage, Hap Kolb, and Guus Schreiber.
2006. A Method for Learning Part-Whole Relations.
In Proceedings of International Semantic Web Con-
ference (ISWC), pages 723 ? 735, Athens, GA, USA.
Springer.
Willem Robert van Hage, Margherita Sini, Lori Finch,
Hap Kolb, and Guus Schreiber. 2010. The OAEI food
78
task: an analysis of a thesaurus alignment task. Ap-
plied Ontology, 5(1):1 ? 28.
Janyce Wiebe and Rada Mihalcea. 2006. Word Sense
and Subjectivity. In Proceedings of the International
Conference on Computational Linguistics and Annual
Meeting of the Association for Computational Linguis-
tics (COLING/ACL), pages 1065?1072, Syndney, Aus-
tralia.
Michael Wiegand, Benjamin Roth, and Dietrich Klakow.
2012a. Web-based Relation Extraction for the Food
Domain. In Proceeding of the International Confer-
ence on Applications of Natural Language Process-
ing to Information Systems (NLDB), pages 222?227,
Groningen, the Netherlands. Springer.
Michael Wiegand, Benjamin Roth, Eva Lasarcyk,
Stephanie Ko?ser, and Dietrich Klakow. 2012b. A
Gold Standard for Relation Extraction in the Food Do-
main. In Proceedings of the Conference on Language
Resources and Evaluation (LREC), pages 507?514, Is-
tanbul, Turkey.
Michael Wiegand, Manfred Klenner, and Dietrich
Klakow. 2013. Bootstrapping polarity classifiers with
rule-based classification. Language Resources and
Evaluation, Online First:1?40.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing Contextual Polarity in Phrase-level
Sentiment Analysis. In Proceedings of the Conference
on Human Language Technology and Empirical Meth-
ods in Natural Language Processing (HLT/EMNLP),
pages 347?354, Vancouver, BC, Canada.
Ian Witten and Eibe Frank. 2005. Data Mining: Practi-
cal Machine Learning Tools and Techniques. Morgan
Kaufmann Publishers, San Francisco, US.
Yiming Yang and Jan Pederson. 1997. A Comparative
Study on Feature Selection in Text Categorization. In
Proceedings the International Conference on Machine
Learning (ICML), pages 412?420, Nashville, US.
Hui Yang, Rajesh Swaminathan, Abhishek Sharma, Vi-
las Ketkar, and Jason D?Silva, 2011. Learning Struc-
ture and Schemas from Documents, volume 375 of
Studies in Computational Intelligence, chapter Min-
ing Biomedical Text Towards Building a Quantitative
Food-disease-gene Network, pages 205?225. Springer
Berlin Heidelberg.
79
