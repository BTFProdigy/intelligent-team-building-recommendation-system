Vaakkriti: Sanskrit Tokenizer
Aasish Pappu and Ratna Sanyal
Indian Institute of Information Technology, Allahabad (U.P.), India
{ akpappu b03, rsanyal}@iiita.ac.in
Abstract
Machine Translation has evolved tremen-
dously in the recent time and stood as center
of research interest for many computer
scientists. Developing a Machine Transla-
tion system for ancient languages is much
more fascinating and challenging task. A
detailed study of Sanskrit language reveals
that its well-structured and finely orga-
nized grammar has affinity for automated
translation systems. This paper provides
necessary analysis of Sanskrit Grammar in
the perspective of Machine Translation and
also provides one of the possible solution for
Samaas Vigraha(Compound Dissolution).
Keywords: Machine Translation, Sanskrit,
Natural Language Parser, Samaas Vigraha,
Tokenization
1 Introduction
Sanskrit language and its grammar had exterted an
emphatic impact on Computer Science and related
research areas. It has resulted to put in extensive ef-
forts in the field ofMachine Translation(hereafter re-
ferred as MT). MT of Sanskrit is never an easy task,
because of structural vastness of its Grammar. Be-
sides, its strutural vastness Sanskrit Grammar is well
organized and least ambigious compared to other
natural languages, illustrated by the fact of increas-
ing fascination for this ancient Aryan language. Its
grammar possesses well organized rules and meta
rules to infer those rules, thus proving to be a pow-
erful analogy to context free grammar of a computer
language.
Subsequently, it supports the idea of developing a
parser for Sanskrit language, that would be helpful
in developing a full-fledged MT system. As a part of
development of parser, there are other important as-
pects to be taken care off. A morphological analyser
and a tokenizer are two of the important components
that play a vital role in the parser. A morpholog-
ical analyser is used for identification of the base
words from their morphonemes, further to under-
stand the semantics of the original text. A tokenizer
also plays its significant part in a parser, by identi-
fying the group or collection of words, existing as a
single and complex word in a sentence. Later on, it
breaks up the complex word into its constituents in
their appropriate forms. In Sanskrit, mainly we have
two categories of complex words. They are
? Sandhi
? Samaas
1.1 Sandhi and Samaas
Sandhi: When two words combine to produce a new
word whose point of combination is result of anni-
hilation of case-end of former word and case-begin
of latter. In short, the resulted new character that
has been created at the point of combination is ex-
actly equivalent to the sound produced when those
two words are uttered without a pause. The inverse
procedure to Sandhi-formation is known as Sandhi
Wicched.
On the other hand, when two or more words are
combined, based on their semantics then the result-
ing word is known as Samaas or Compound. Unlike
577
Sandhi, the point of combination in Samaas may or
may not be a deformed in the resulting word. The in-
verse procedure of break-up of a Samaas is known as
Samaas Vigraha. Considering the complexity of this
problem, we restricted our focus to Samaas Vigraha
or Compound Dissolution(hereafter Compound Dis-
solution is referred as CD for convenience).
1.2 Organization of the Paper
Initially, we would discuss about the problem of fo-
cus and the main objective of this paper in detail.
Further, a little overview about the Sanskrit grammar
and Knowledge Representation, that are required to
understand the underlying concepts of the system.
Then, we would brief about the existing systems in
this areas and the related areas of interest. Later on,
we would give a detailed description of the architec-
ture of Vaakkriti. We would give a detailed analysis
of the results of our system and finally, throw some
light over our contribution to this research area.We
shall conclude with some of drawbacks of our sys-
tem and the challenges we have faced.
2 The Problem
Semantics being the prime focus, we need to learn
the factors that effect the formation of a compound
from the set of atomic words. The basic problem
is identification of factors, by thorough analysis of
language structure or with the help of a linguist. Es-
pecially various examples of Samaas must be exten-
sively observed. After identification of factors, we
need to find out the appropriate form of Knowledge
Representation for the rule-base. Here, knowledge
being the rules, based on which a particular com-
pound is formed. The importance of CD can be
clearly understood, during the process of tokeniza-
tion. A well-defined set of rules in Sanskrit can
be found in ?Ashtadyayi?, authored by 3rd century
grammarian and linguist Panini. Ashtadyayi con-
tains rules of Grammar in a concise form, distributed
over eight chapters. Our rule-base system would be
based on the work of Kale et. al, that has detailed
description of Paninian Grammar.
3 Sanskrit Grammar
As we have already mentioned that, it is necessary
to know some of the basic concepts of the Sanskrit
grammar. First, we would give some important def-
initions of terms that are frequently used in this pa-
per.
3.1 Important Definitions
3.1.1 Vibhakti(Declension)
Sanskrit is a highly inflected language with three
grammatical genders (masculine, feminine, neuter)
and three numbers (singular, plural, dual). It has
eight cases: nominative, vocative, accusative, instru-
mental, dative, ablative, genitive, and locative.
3.1.2 Dhatupata(Verbal Conjugation)
The verbs tenses (a very inexact application of the
word, since more distinctions than simply tense are
expressed) are organized into four ?systems? (as well
as gerunds and infinitives, and such creatures as in-
tensives or frequentatives, desideratives, causatives,
and benedictives derived from more basic forms)
based on the different stem forms (derived from ver-
bal roots) used in conjugation. There are four tense
systems:
? Present (Present, Imperfect, Imperative, Opta-
tive)
? Perfect
? Aorist
? Future (Future, Conditional)
3.2 Factors that effect
The list of factors that are involved in a rule are
? Part of Speech(hereafter referred as POS)
? List of Words(a token must be among a set of
words to satisfy a rule)
? Case-End
? Case-Begin
? Declension
? Sense(a token with a particular sense is only
qualified)
? Meaning
? Affix
578
? Affix Type(Taddita and Kriti)
? Number(sng, two, mny)(hereafter we refer
number as num)
? Gender(mas, fem, neu)
The list of actions that act as functions in the con-
sequent of a rule are:-
? setDecl(set the declension case for a specified
token)
? addBefore(add a string before a specified to-
ken)
? addAfter(add a string after a specified token)
? setNumber(set the number of a to-
ken(sng,two,mny))
? replace(replace a token with a string related to
it)
3.3 Compounds
Nominal compounds occur with various structures,
however morphologically speaking they are essen-
tially the same. Each noun (or adjective) is in its
(weak) stem form, with only the final element re-
ceiving case inflection. Some examples of nominal
compounds include:
Itaretara
Example: rAml#mZBrtf/`?A,(RAmaLakshmaNaBaratAH)
to rAm, c l#mZ, c Brt, c f/`?,(RAma ca, LakshmaNa
ca, Barata ca)
Rule: ?token POS(token,noun) ? setDecl(token,nom)?
addAfter(token,c)
Samahaara
Example: pAZFpAdO(pANIpAdau)to
pAZF c pAddm^ c(pANI ca pADam)
Rule: ?token,?sense POS(token,noun) ?
SenseOf(token, sense) ? setDecl(token,nom)?
addAfter(token,c)
Dvitiya(Accusative) Tatpurusha
Example: d`,?AtFt,(dukhatItaH)to
d`,?m^ atFt,(dukham atItaH)
Rule: POS(token1,noun) ? WordList(token2,E?t ,
atFt , pEtt , gt , a(y-t , ?A? , aAp? , gmF , b`B`"`)
?setDecl(token1,acc)
Trutiya(Instrumental) Tatpurusha
Example: d`,?AtFt,to
d`,?m^ atFt,
Rule: POS(token1,noun) ? (POS(token2,verb) ?
WordList(token2,p?v? ,sd? `f ,Un))? setDecl(token1,ins)
Chaturthi(Dative) Tatpurusha
Example: y?pdAz(yupadaru)to y?py dAz(yupaya daru)
Rule: POS(token1,noun) ? (Sense(token2,?material?)
? WordList(token2,aT? ,bEl , Eht , s`? ,rE"t))?
setDecl(token1,dat)
Panchami(Ablative) Tatpurusha
Example: cOrBym^(cOrabayam)to cOrAd^ Bym^(cOraad
bayam)
Rule: POS(token1,noun) ? (WordList(token2,
By ,BFt ,BFEt ,BF, ,ap?t ,apoY , m`? ,pEtt ,apv-t))?
setDecl(token1,abl)
Shashti(Genitive) Tatpurusha
Example: rAjp`zq,(rAjapurushaH)to rAj?
p`zq,(rAjangya PurushaH)
Rule: POS(token1,noun) ? (POS(token2,noun)??
POS(token2,verb)?? NumeralType(token2,ordinal)??
SenseOf(token2,?quality?))? setDecl(token1,gen)
Saptami(Locative) Tatpurusha
Example: ngrkAk,(nagarAkAkaH)to ngr? kAk,
iv(nagare kAkaH iva)
Rule: POS(token1,noun)
? (MeaningOf(token2,?crow?)?
SenseOf(token2,?contempt?))? setDecl(token1,loc)?
addAfter(token2, iv)
4 Knowledge Representation
We have already learnt that the process of CD is sup-
ported by a rule-base system. A production system
is a good illustration to understand a rule-base sys-
tem. To represent a complex rule, it would be bet-
ter to use First Order Predicate Logic(FOPL). Un-
der FOPL a rule can be written as of the form P (a)?
Q(a)?Q(b)?R(c) ? Action1(a)?Action2(b)?Action1(c)
where P,Q and R are predicates
a, b and c are constant symbols
Action is a function symbol
The rule-base system of Vaakkriti is de-
veloped considering the factors as pred-
icates and the tokens as constant sym-
bols. A sample rule would look like this
579
POS(tok1, noun) ? (POS(tok2, verb) decl(tok2, acc)) ?
setDecl(token1, acc).
5 Related Work
In the recent times many efforts have been made to
develop various utilities for Sanskrit. The tools de-
veloped includes Sanskrit to other Indian Language
transliteration tools, simple and primitive transla-
tion tools, many grammar analysing tools and many
more learning tools. Some of the important works
includes Anusaraka, a primitive machine translation
tool developed by Akshar et. al. Anusaraka tries
to take advantage of the relative strengths of the
computer and the human reader, where the com-
puter takes the language load and leaves the world
knowledge load on the reader. Besides, these tools,
there are some beautiful theory-based research work
was also done. The concept of Indian Network Lan-
guage(INL) is one of such concepts that was pro-
posed by Anupam et. al. It gives a hypothesis to
consider Sanskrit as INL because of its important
properties like free word order and inherent seman-
tic net structure. There are few other interesting re-
search concepts that have been analysed in the con-
text of Sanskrit language. Rick Braggs et. al have
shown in his article how Knowledge Representation
in the language of Sanskrit is one of those wonderful
concept to show that Semantic Nets. Semantic Nets
are concept respresenting structures, that show how
a concept is related to other concepts semantically, a
semantic net would like in the figure below. Another
beautiful research work was comparison of Paninian
Grammar and Computer language Grammar. Bhate
et al has analysed to show that how well organized
and structured is Sanskrit Grammar and its forgot-
ten valuable contributions to the field of Computer
Science.
6 Architecture
An Itrans standard formatted devanagiri text is given
as input to the system and the output of the system
is the set of tokens produced after CD. The list of
components in the system are listed below:
? Input Processor
? Symbol Table
? Knowledge Base
? Inference Engine
? Database
? Rule-Base Editor
The architecture of Vaakkriti can be seen in the fig-
ure
Figure 1: Architecture of Vaakriti
The algorithm of Vakkriti is given below:- A de-
Algorithm 1 Algorithm of Vaakkriti
1: input? Itrans-Devanagiri Text
2: input? ? breakUp(input)
3: tokenList? tentativeTokenize(input?)
4: tokenInfoList? tokenList
5: for tokeni in tokenInfoList do
6: token(i)? extractInfo(tokeni
7: update token(i) in tokenInfoList
8: end for
9: for each rule(r) in Knowledge-Base(KB) do
10: result? infer(r,tokenInfoList)
11: if result is true then
return r
12: end if
13: end for
tailed description of each component is as follows.
580
6.1 Input Processor
The unstemmed compound taken as input to the sys-
tem is a string in itrans format. First, Input Processor
breaks the itrans string into chunks of characters on
the basis of Devanagiri Character set. The heuristic
for break up procedure is given below:-
The reason behind the breakup procedure is to
ease the process of breaking the string into words
in their tentative forms. If a string is considered as
it is without breakup into devanagiri characters, then
there is a high chance of ambiguity while lookup in
the dictionary. For example:-
Without breakup of input string
aja
ajagaraH-- Found this word
With breakup of string into character sequences
a,ja
a,ja,ga,raH
Later on the chunks of characters are processed as
in the procedure below:-
The words lying in input string are tentatively
guessed by maintaining a stack of character se-
quences, thus checking with the dictionary for the
right word. But, in most of the cases, the word in
the input string do not have an exact match in the
dictionary. This is because of the matra appended to
Case-End of a word. Therefore, we have generated
tokens for each matra and tried to find it in the dic-
tionary. If the word is found, then the word along
with its meaning is stored in the Symbol Table.
6.2 Symbol Table
Now, we shall discuss more about how a Symbol
Table fetches those subtle information of a token.
Symbol table extracts token information in the fol-
lowing manner:-
6.2.1 Part of Speech
Part of Speech is identified with the help of stan-
dard Monier Williams Dictionary, List of Adverbs,
List of Prepositions, List of Numerals.
6.2.2 Sense and Meaning
First, meaning of the token is known from the dic-
tionary and the sense of the token is fetched through
a special kind of procedure. The technique has fol-
lowing steps:-
1. Identify the nouns in the meaning phrase.
2. Find sense for each noun with the help of En-
glish Wordnet.
3. Find a list of ?common? senses for all the
nouns.
4. That list of senses is assumed to the sense of a
token.
6.2.3 Gender and Number
These are fetched from the XML database.
6.3 Knowledge Base
The Knowledge Base(KB) contains facts and rules
that supports the system, for identifying a given in-
put. The KB has been classified well, according to
the Rule Sets. A Rule Set is a set of rules that are
meant for a particular type of compound. Infact, a
new rule set can be created whenever there is a new
part of speech to be dealt with. It has been assumed
that, a rule has clauses(both unit and definite) on an-
tescendent side, whose number is equal to tentative
number of tokens in the input parsed string. On the
other hand, the consequent or conclusion contains
the list of actions that has to be operated over the to-
kens(in the input string) by the system. More about
the rule structure in the next section.
The KB is well integrated with the Rule Base Ed-
itor(RBE) and the Inference Engine. Currently, it
contains limited number of rules this makes the KB
non-monotonic, yet it can be made monotonic, by
addition of new rules.
6.4 Database
There is a large database that supports the whole sys-
tem of Vaakriti. The database is contained in the
form of XML files. There are following tables in the
database:-
? Nouns, Adjectives, Numerals Declensions.
? Adverbs, Conjunctions and Prepositions.
? Dictionary Database.
? Preverbs database.
? Other Morphonemes.
581
6.5 Inference Engine
Whenever premises of a particular are satisified by
the input parse string, then it is said that a rule is
fired. A fired rule applies its consequent part over
the parsed string to result in actual goal. This proce-
dure is known as Rule Inference.
6.6 Rule Base Editor
The sole motive of Rule-Base Editor is to free the
Knowledge Engineer free from rule entry. A Lin-
guist with little training to operate the GUI can be
provided, would suffice this task.
7 Results
The system has been tested with many examples that
have been taken from the book written by Kale et al
The set of examples have been chosen from differ-
ent set of Compounds. In most of the cases system
has given correct results with a precision of 90%,
but in some of the cases that involve sense, it be-
came quite difficult to produce the result. Lack of
linguistic tools like Wordnet for Sanskrit language
imposes limitations on word sense disambiguation.
We have developed a sense list for a limited set of
words by observing some of the important sanskrit
texts, based on the knowledge we have acquired.
8 Our Contribution
We have proposed a utility called Rule-Base Editor,
besides our actual work on CD. The motive behind
Rule-Base Editor is to induce the property of flexi-
bility into the system. It always avails a linguist to
enter new rules with the help of Rule-Base Editor
without any support from knowledge engineer.
We have already learnt that Samaas Vigraha(CD)
is the most important aspect of the tokenization
phase in a parser. Implicitly, the acquisition of fac-
tors and rules also gather equal importance. Signify-
ing this fact, we have done rigorous survey over the
grammar to identify these factors. Hence, we assert
that our system will be a significant contribution in
this area of research.
9 Future Scope and Conclusion
We assert that Vaakkriti would be a preliminary con-
tribution to the realm of NLP. Adding to the major
works that have been done already, Vaakkriti is an
attempt to enhance the existing works. We would
extend the current system and develop a full-fledged
parser that will suffice most of the requirements of
MTsystem.
Although, it looks the way that the problem has
been solved, but the actual problems arouses when
a Sanskrit poem is given as input to a MT system.
Usually, a sanskrit poem conveys more than one
meaning and sometimes figure of speech is used,
that adds fuel to the fire. This becomes a herculean
task for a MT system and it will remain as a myth
forever.
Acknowledgements
The authors would like to specially thank Gerard
Huet for providing linguistic database of declensions
and verbal roots, that was quite helpful in making
our system fine and complete. The authors grate-
fully acknowledge financial support from the Uni-
versal Digital Library project, funded by the Min-
istry of Communication and Information Technol-
ogy (MCIT) India and also Indian Institute of Infor-
mation Technology, Allahabad.
References
Higher Sanskrit Grammar, M. R. Kale, Motilal Banarasi-
Dass Publishers.
?Paninis Grammar and Computer Science?, Saroja Bhate
and Subhash Kak, Annals of the Bhandarkar Oriental
Research Institute, vol. 72, 1993, pp. 79-94.
?Knowledge Representation in Sanskrit and Artificial In-
telligence?, Rick Briggs
?Artificial Intelligence?, Elain Rich and Kevin Knight,
2nd Edition, Tata McGrawHill, 1991.
?Artificial Intelligence, AModern Approach? Stuart Rus-
sell and Peter Norvig, 2nd Edition, Pearson Education,
2003.
?Sanskrit as Indian Networking Language: A Sanskrit
Parser?, Anupam, 2004.
?Natural Language Processing, A Paninian Perspective?,
Akshar Bharti, Vineet Chaitanya and Rajeev Sangal,
Pearson Education.
?Natural Language Processing using PROLOG? by M.
Gerald, M Chris, Addison and Wisley, 1989.
?Cognitive Science Learning Resource?,
http://www.comp.leeds.ac.uk/ugadmit/cogsci/knowled
582
Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 99?107,
Seoul, South Korea, 5-6 July 2012. c?2012 Association for Computational Linguistics
The Structure and Generality of Spoken Route Instructions
Aasish Pappu and Alexander Rudnicky
Language Technologies Institute, Carnegie Mellon University
{aasish, air}@cs.cmu.edu
Abstract
A robust system that understands route instructions
should be able to process instructions generated nat-
urally by humans. Also desirable would be the abil-
ity to handle repairs and other modifications to exist-
ing instructions. To this end, we collected a corpus
of spoken instructions (and modified instructions)
produced by subjects provided with an origin and
a destination. We found that instructions could be
classified into four categories, depending on their
intent such as imperative, feedback, or meta com-
ment. We asked a different set of subjects to fol-
low these instructions to determine the usefulness
and comprehensibility of individual instructions. Fi-
nally, we constructed a semantic grammar and evalu-
ated its coverage. To determine whether instruction-
giving forms a predictable sub-language, we tested
the grammar on three corpora collected by others
and determined that this was largely the case. Our
work suggests that predictable sub-languages may
exist for well-defined tasks.
Index Terms: Robot Navigation, Spoken Instructions
1 Introduction
Generating and interpreting instructions is a topic of en-
during interest. Cognitive psychologists have examined
how people perceive spatial entities and structure route
instructions (Daniel and Denis, 1998; Allen, 1997). Lin-
guists and others have investigated how people articulate
route instructions in conversation with people or agents
(Eberhard et al, 2010; Gargett et al, 2010; Stoia et al,
2008; Marge and Rudnicky, 2010). Artificial intelligence
researchers have shown that under supervised conditions
autonomous agents can learn to interpret route instruc-
tions (Kollar et al, 2010; MacMahon et al, 2006; Ma-
tuszek et al, 2010; Bugmann et al, 2004; Chen and
Mooney, 2010).
While the subject has been approached from different
perspectives, it has been generally held that the language
of directions is mostly limited and only parts of the vo-
cabulary (such as location names) will vary from case to
case. We are interested in being able to interpret natural
directions, as might be given to a robot, and generating
corresponding trajectory. But natural directions contain
different types of information, some (more-or-less) eas-
ily interpreted (e.g., "go to the end of the hall") while
others seem daunting (e.g., "walk past the abstract mural
with birds"). So the question might actually be "is there
enough interpretable data in human directions to support
planning a usable trajectory?".
The language of instructions contains a variety of rel-
evant propositions: a preface to a route, an imperative
statement, or a description of a landmark. Previous work
has proposed both coarse and fine-grained instruction
taxonomies. (Bugmann et al, 2004) proposed a taxon-
omy of 15 primitive categories in a concrete ?action?
framework. In contrast, (Daniel and Denis, 1998) sug-
gested a five-way categorization based on cognitive prop-
erties of instructions.
Instructions vary greatly and can include superfluous
detail. (Denis et al, 1999) found that when people were
asked to read and assess a set of instructions some of the
instructions were deemed unnecessary and could be dis-
carded. There is some evidence (Lovelace et al, 1999;
Caduff and Timpf, 2008) that only the mention of sig-
nificant landmarks along the route leads to better-quality
instructions. Computational (rather than descriptive) ap-
proaches to this problem include: using sequence label-
ing approach to capture spatial relations, landmarks, and
action verbs (Kollar et al, 2010), generating a frame
structure for an instruction (MacMahon et al, 2006), or
using statistical machine translation techniques to trans-
late instructions into actions (Matuszek et al, 2010).
We describe a new instructions corpus, its analysis in
terms of a taxonomy suitable for automated understand-
ing and a verification that the instructions are in fact us-
able by humans. With a view to automating understand-
ing, we also constructed a grammar capable of processing
this language, and show that it provides good coverage
99
for both our corpus and three other corpora (Kollar et al,
2010; Marge and Rudnicky, 2010; Bugmann et al, 2004)
This paper is organized as following: Section 2 de-
scribes the corpus collection study. Then in Section 3,
we discuss the taxonomy of route instructions. Section 4
focuses on which categories are important for navigation.
In Section 5, we report our results and error analysis on
parsing instructions from our corpus and three other cor-
pora containing route instructions, followed by lessons
learned and future work.
2 The Navagati1 Corpus
We collected a corpus of spoken instructions describing
how to get from one part of a large building complex
to another. To ensure consistency we recruited individ-
uals who were familiar with the environment and conse-
quently could formulate such instructions without refer-
ence to maps or other materials. Since we are ultimately
interested in how such instructions are edited, we also in-
cluded conditions in which subjects were asked to modify
their instructions in several ways. The corpus is publicly
available2.
2.1 Participants and Procedure
We recruited subjects who were both fluent English
speakers and were also familiar with the environment (a
university building complex). Subjects were told to imag-
ine that they had encountered a visitor, not familiar with
the campus, at a specific location (in front of elevators on
a particular floor) who needed instructions to a specific
location, a caf? two buildings away.
For each set of instructions, subjects were asked to
think about the route and their instructions, then record
them as a single monologue. Subjects sat in front of
a computer and wore a close-talking microphone. Ini-
tially no map was provided and they were expected to
rely on their memory. In subsequent tasks they were
shown a floor-plan indicating a specific location of the
visitor and asked to modify their instructions. Speech
was transcribed using Amazon Mechanical Turk, shown
to be a reliable resource for spoken language transcription
(Marge et al, 2010). Transcriptions were normalized to
standardize spellings (e.g., building names).
2.2 Design
Previous works have focused on eliciting route instruc-
tions between multiple pairs of locations. There is a gen-
eral agreement that the structure of instructions did not
vary with the increase in number of start-end location
pairs. However previous works have not looked at how
instructions would be modified under different situations.
1Sanskrit root for Navigation meaning "to travel by boat"
2http://tts.speech.cs.cmu.edu/apappu/navagati/
We were interested in two general cases: normal in-
structions (Simple scenario) and repairing existing in-
structions (Repair scenario). Each scenario included
three tasks, as described below.
We selected two locations that could be walked be-
tween without necessarily going outside. However the
subjects were free to to give instructions for a route of
their choice between a location pair. The first location (A)
was in front of an elevator on the seventh floor of Gates
Hillman Center, the second location (B) was a cafe on the
fifth floor of Wean Hall. The expected pathway included
changes in floor, direction and passing through a different
building. It required reasonably detailed instructions.
In the Simple scenario, subjects were asked to generate
three variants, as follows: (1) instructions for A? B; (2)
for B ? A; and (3) a simplified version of (2).
The motivation behind (2) is to learn whether people
would make references about the parts of the route that
were previously traversed in the opposite direction. In
the case of (3), we were interested in the degree of in-
struction reuse and the condensation strategy. We explic-
itly told the subject ?Imagine that the visitor found your
instructions confusing. They asked you to simplify the
instructions. How would you do that??
The Repair scenario was designed to probe how a sub-
ject would alter their instructions in response to compli-
cations. Subjects were asked to modify their intial Simple
instructions (A ? B) to cope with: (1) visitor missing a
landmark and takes a wrong turn; (2) an obstruction (con-
struction) blocking the original path; and (3) the visitor
getting lost and ends up in an unknown part of the (mid-
dle) building. For each case, the subject was given a map
(as in figure 1) that marked the visitor?s location and had
to get the visitor back on track.
Figure 1: Map of the construction area (marked as star)
The tasks in this scenario were designed to see whether
people modify directions differently when three different
situations are presented. Precisely, we want to know if
100
there is any difference in the discourse structure and ver-
bosity of the directions.
2.3 Analysis
Nine subjects performed 6 tasks each, producing 54 sets
of instructions, for a total of 65 minutes of speech. Please
note that other corpora in the route instructions domain
have similiar scale (see Figure 5(a)). The transcriptions
were segmented semi-automatically into atomic units
corresponding to instruction steps. For example, the in-
struction ?Go left, then turn right? was segmented into:
?go left?, and ?then turn right? based on bigram heuris-
tics. We compiled a list of most frequent bigrams and
trigrams in the corpus e.g., ?and then?, ?after that? etc.
The transcriptions were segmented at the bigram/trigram
boundaries and were manually verified for the correctness
of a segment. The Simple scenario generated 552 instruc-
tions, the Repair part contained 382 instructions, a total
of 934. The vocabulary has 508 types and 7937 tokens.
Table 1 summarizes the factors measured in both the sce-
narios. Only two (marked by *) differed between scenar-
ios (t-test at p < 0.05). We examined acoustic properties
(for example mean pitch) but did not find any significant
differences across scenario type.
Table 1: Simple vs Repair Scenario
Factors Simple Repair
# Tokens 4461 3476
# Types 351 375
# Instructions 552 382
# Words-per-Instruction* 7.5 8.0
# Landmarks 450 314
# Motion Verbs* 775 506
# Spatial Prepositions 61 60
# Filler Phrases 414 380
We can compare language similarity across scenar-
ios by comparing the perplexity of text in the two sce-
narios. If the instructions and repairs are similar, we
would expect that a model built from one scenario should
be able to capture data from the other scenario. We
randomly divided data from each scenario into training
(70%) and testing data (30%). We built a trigram lan-
guage model (LM) smoothed with absolute discounting
using the CMU-SLM toolkit (Rosenfield, 1995). Then,
we computed the perplexity on testing data from each
scenario against each model. From Table 2, Simple-
LM has lower perplexity compared to Repair-LM on the
test sets. The perplexity of Simple-LM on Repair-Test
is slightly higher when compared to Simple-Test. This
could be due to the lexical diversity of the Repair scenario
or simply to the smaller sample size. Table 1 (row 1) indi-
cates that the data in Repair scenario is smaller than data
in Simple scenario. To explore the lexical diversity of
these two scenarios we conducted a qualitative analysis
of the instructions from both the scenarios.
In Task 1 of the Simple scenario, we only observed
a sequence of instructions. However in Task 2 of Simple
Scenario, we noticed references to instructions from Task
1 via words like ?remember?, ?same route?, etc. This
suggests that instructions may be considered in context of
previous exchanges and that this history should normally
be available for interpretation purposes. In Task 3 of the
Simple scenario, 7 out of 9 subjects simply repeated the
instructions from Task 2 while the rest provided a differ-
ent version of the same instructions. We did not observe
any other qualitative differences across three tasks in the
Simple scenario.
In Task 1 of the Repair scenario, all but one subject
gave instructions that returned the visitor to the missed
landmark, instead of bypassing the landmark. In Task 2,
the obstruction on the path could be negotiated through
a shorter or longer detour. But only 4 out of 9 partici-
pants suggested the shorter detour. In Task 3, we did not
observe anything different from Task 2. Despite the dif-
ference in the situations, the language of repair was found
to be quite similar. The structure of the delivery was orga-
nized as follows: (1) Subjects introduced the situation of
the visitor; (2) then modified the instructions according to
the situation. Introduction of the situation was different
in each task, (e.g., ?you are facing the workers? vs ?looks
like you are near office spaces? vs ?if you have missed
the atrium you took a wrong turn?). But the modification
or repair of the instructions was similar across the situa-
tions. The repaired instructions are sequences of instruc-
tions with a few cautionary statements inserted between
instructions. We believe that subjects added cautionary
statements in order to warn the visitor from going off-the-
route. We observed that 6.3% of the repaired instructions
were cautionary statements; we did not observe caution-
ary statements in the original Simple scenario. In order
to see the effect of these cautionary statements we re-
moved them from both training and testing sets of the
Repair scenario, then built a trigram LM using this con-
densed training data (Repair?w/o-cautionLM). Table 2
shows that perplexity drops when cautionary statements
are excluded from the repair scenario, indicating that
Simple and Repair scenarios are similar except for these
cautionary statements.
3 Taxonomy of Route Instructions
Taxonomies have been proposed in the past. Daniel
and Denis (1998) proposed a taxonomy that reflected at-
tributes of spatial cognition and included 5 classes: (1)
Imperatives; (2) Imperatives referring a landmark; (3)
Introduction of a landmark without an action; (4) Non-
spatial description of landmarks and (5) Meta comments.
101
Table 2: Perplexity of Simple/Repair Language Models
LM/Test Simple-Test Repair-Test Repair
-w/o-
cautionSimple-LM 29.6 36.5 30.3
Repair-LM 37.4 37.3 35.6
Repair
-w/o-
cautionLM
31.9 37.6 26.8
Bugmann et al (2004) suggested 15 primitive (robot-
executable) actions. We present a hierarchical instruction
taxonomy that takes into account both cognitive proper-
ties and the needs of robot navigation. This taxonomy is
based on 934 route instruction monologues. It should be
noted that this taxonomy is not based on dialog acts but
rather takes the intent of the instruction into the account.
3.1 Categories
We segmented the spoken instructions using a criterion
that split individual actions and observations. Our taxon-
omy is roughly comparable to that of (Daniel and Denis,
1998) but differs in the treatment of landmarks because
the mention of the landmarks in an instruction can be of
two types: contextual mention and positional mention.
Contextual Mention means when a landmark in the sur-
roundings but it is not on the path. On the other hand, po-
sitional mention requires the landmark to be on the path.
In our taxonomy, contextual mention becomes Advisory
instruction and positional mention is called Grounding
instruction. The taxonomy has four major categories that
subsume 18 sub-categories; these are given in Table 3.
For instance, ?You want to take a right? belongs to the
Imperative category. ?You will see a black door? is an
Advisory instruction about the surroundings. ?You are on
the first floor? denotes Grounding. ?Your destination is
located in another building and you will walk across three
buildings in this route? gives an overview of the route, a
Meta Comment. From Figure 2, we see that majority of
the route instructions are Imperative.
0 20 40 60
Grounding
Meta Comments
Advisory
Imperative 56.2%
18.6%
17.6%
7.6%
% distribution
Figure 2: First Tier Instruction Categories
3.1.1 Imperative Instructions
Imperative instructions are executable and can result
in physical displacement. We identified seven subcate-
gories of Imperatives that distinguish different contexts
(e.g., going along a corridor, changing floors via elevator
or stairs, or going to a specific location).
Imperative instructions can also include preconditions
or postconditions. The order of their execution varies
based on the directionality of the condition between two
instructions. Continue is interesting because it can
have travel-distance and travel-direction arguments, or
even no arguments. In the latter case the follower contin-
ues an action (e.g., ?keep walking?), until some unspeci-
fied condition ends it.
3.1.2 Advisory Instructions
While giving route instructions people mention land-
marks along the route as feedback to the direction-
follower. Some of these landmarks are not part of the path
but do serve as waypoints for the follower (e.g., ?you will
see a hallway right there?). We observe that landmarks
are distinct either functionally and/or physically. For ex-
ample, a hallway is both functionally and physically dif-
ferent from an elevator but only physically different from
a door because both function as an instrument (or path) to
get from one place to another. Based on this distinction,
we divided advisory instructions into five sub-categories
depending on the type of landmark mentioned in the in-
struction (see Table 3).
Compound locations (see Table 3) are closely located
but physically distinct. They may constitute part-whole
relationships e.g., ?TV screen with a motion sensor?.
We observed that compound locations are used to disam-
biguate when multiple instances of a landmark type are
present e.g., ?chair near the elevator vs ?chair near the
hallway?.
3.1.3 Grounding Instructions
Grounding instructions report absolute position. These
instructions indicate current view or location as opposed
to future view or location (indicated through advisory
instructions). These instructions constitute a landmark
name similar to advisory instructions and also follow the
distinction between the type of landmark mentioned in
the instruction (see Table 3).
3.1.4 Meta Comments
Meta comments are non-executable instructions added
to route instructions. People often make these comments
at the beginning of instructions and sometimes in be-
tween two imperative statements e.g., a precautionary
statement. In our corpus we found meta-comments in
two situations: (1) Preface or introduction of the route;
(2) Caution against a (metaphorical) pitfall in the route.
102
Category SubCategory Distribution Example
Imperative
Leave-Location 2.3% Exit the building; Come out of the room
Follow-Path 7.0% Walk along the corridor; go across the bridge
Floor-Transition 11.2% Take the elevator to fourth floor; Take the stairs to the fifth
Turn 24.2% Turn left
Go-To 27.2% Walk to the elevators
Continue 28.0% Keep going straight for few steps
Advisory
Floor-Level 5.4% You will see fourth floor of other building
Floor-Transition 12.2% You will see elevators
Compound-Location 13.4% You will see a hallway to the right of elevators
End-of-Pathway 21.5% You will see end of the hallway
Landmark 47.5% You will see a TV screen
Grounding
Compound-Location 5.9% You are on a hallway right next to the elevators
End-of-Pathway 8.2% You are on the bridge leading to other building
Floor-Level 42.4% You are on fourth floor of the building
Landmark 43.5% You are on standing near TV screen
Meta Comments
Caution 14.7% You can find it immediately; Don?t go that side
Miscellaneous 36.0% Let me guide you through it; I guess a simpler way would be
Preface 49.3% I will guide you to the cafe in that building
Table 3: Taxonomy of Categories with Examples
Both the example instructions and the distribution of the
subcategories are given in Table 3.
The language of meta comments is more diverse than
that of the other three categories. If we build trigram
language models for each category and measure the per-
plexity on a held-out set from same category the perplex-
ity is relatively high for Meta (49.6) compared to other
categories (Advisory: 19.5; Imperative: 18.5; Ground-
ing: 11.4). This suggests that automatic understanding
of meta comments might be problematic, consequently it
would be useful to determine the ralative utility of differ-
ent instruction categories. The next section describes at
attempt to do this.
4 Which Instructions are Relevant?
Given a variety of information present in a set of route
instructions, we wanted to investigate whether all that in-
formation is relevant for navigation. In order to find that
out we devised a user study asking people to follow in-
structions collected in our previous study. (Daniel and
Denis, 1998) conducted a similar study where they asked
subjects to read a set of instructions and strike-off in-
structions with too much or too little information. How-
ever, people may or may not feel the same when they fol-
low (physically navigate) these instructions. Therefore,
in our study the experimenter read instructions (of vary-
ing amount of detail) to the subjects while they physically
navigated through the environment.
4.1 Participants and Procedure
We chose 5 out of the 9 instruction sets, spoken by differ-
ent subjects (of average length 26.8 instructions per set)
from Task 1 of the Simple scenario discussed above. We
did not use the others because they contained few instruc-
tions (average of 13.5) and provided fewer instances of
instructions in different categories. Also, we did not use
instructions from Repair Scenario because those instruc-
tions dependent on a scenario and a set of instructions
that were already provided to the direction follower.
Our set of instructions included the full set, a set with
only imperatives and additional sets adding only one of
the remaining categories to the imperative set (see Ta-
ble 4), producing 25 distinct sets of instructions. Addi-
tionally, building names and the destination name (tran-
scribed in the instructions) were anonymized to avoid re-
vealing the destination or the ?heading? at the early stage
of the route.
We recruited 25 subjects, each doing one variant of the
instructions. In the session, the experimenter read one in-
struction at a time to the subject and walked behind the
subject as they proceeded. Subjects were asked to say
?done? when ready for the next instruction; they were
allowed to ask the experimenter to repeat instructions but
otherwise were on their own. The experimenter kept track
of how and where a subject got lost on their way to des-
tination. (No systematic effects were observed, but see
below.) At the end subjects were handed the entire set of
instructions and were asked to mark which instructions
were difficult to follow and which were redundant. Re-
maining instructions were deemed to be useful and inter-
pretable.
Table 4: Variants of an Instruction Set
Variant Imperative Advisory Grounding Meta
Imp X
Imp+Adv X X
Imp+Grnd X X
Imp+Meta X X
Entire Set X X X X
103
Category/Variant Imp Imp+Grnd Imp+Meta Imp+Adv Entire Set Category/Variant Imp Imp+Grnd Imp+Meta Imp+Adv Entire Set
Diff-Imp 11 10 12 9 12 Redun-Imp 5 8 12 11 8
Diff-Adv 0 10 5 10 10 Redun-Adv 5 10 19 10 29
Diff-Grnd 0 0 13 0 0 Redun-Grnd 20 13 47%47 53%53 27
Diff-Meta 4 15 12 4 4 Redun-Meta 19 31 65%65 23 50%50
Diff-All 6 9 11 7 9 Redun-All 9 13 26 17 21
Figure 3: What percent of instructions are Difficult (Diff) or Redundant (Redun)? On the left: Darker is Difficult right:
Darker is More Redundant Instructions
4.2 Analysis
Except for one subject, everybody reached the destina-
tion. Subjects found Imperative and Advisory instruc-
tions more useful compared to Grounding instructions
and Meta comments, irrespective of the instruction-set
they followed (see Figure 3). Figure 3(a) shows percent-
age of category-wise difficult instructions in each vari-
ant of an instruction set and 3(b) shows percentage of
category-wise redundant instructions in each variant of an
instruction set. For e.g., Diff-Imp/Imp+Meta means that
12% of imperative-instructions are difficult in the Imper-
ative+Meta variant.
16 out 25 Subjects got lost at least once i.e., they misin-
terpreted an instruction, followed along wrong path, then
they realized inconsistencies with spatial information and
the following instruction, and finally recovered from the
misinterpreted instruction. A subject lost thrice in the en-
tire experiment who misunderstood one instruction twice
and another instruction once. The subject was lost at an
intersection of three hallways and only one of them leads
towards the destination. This instruction did not have
sufficient information about the next heading. All sub-
jects who recovered from misinterpretation informed that
landmark?s attributes such as number of floors in a build-
ing (if building is the landmark) and the spatial orienta-
tion of the landmark helped them in recovery.
Instructions that lacked spatial orientation were found
to be particularly difficult to follow. Subjects found a few
of the imperative and advisory instructions difficult to fol-
low. While following these difficult instructions, people
realized that they got lost and asked the experimenter to
repeat the instructions. Examples of difficult instructions
and the people?s complaint on that instruction are as fol-
lows:
? So you kind of cross the atrium Complaint: partic-
ipants reported that they were not sure how far they
had to walk across the atrium.
? Go beside the handrails till the other end of this
building Complaint: no absolute destination, mul-
tiple hallways at the end of handrails
? Just walk down the hallway exit the building Com-
plaint: multiple exits to the building
? After you get off the elevator, take a left and then left
again Complaint: more than one left confused the
subjects
? You can see the building just in front of you Com-
plaint: there were three buildings standing in front
and the target building was slightly to the left.
? You will see the corridor that you want to take Com-
plaint: there were two corridors and the orientation
was unspecified in the instruction
5 Understanding Experiments
The Navagati (NAV) corpus instructions were divided
into training set (henceforth abbreviated as NAV-train)
and testing set (abbreviated as NAV-test) of size 654 (of 6
subjects) and 280 (of 3 subjects). The training set was
used to create a grammar based on the taxonomy de-
scribed in Section 3.
5.1 Grammar
A domain-specific grammar was written to cover most
frequent phrases from the training set using the Phoenix
(Ward, 1991) format. Phoenix grammars specify a hier-
archy of target concepts and is suited to parsing spon-
taneous speech. The resulting grammar produced cor-
rect and complete parses on 78% of the training data
(NAV-train). The remaining training instances were not
included due to unusual phrasing and disfluencies. The
concepts in the grammar are listed in the Table 5.
5.1.1 Managing Variable Vocabulary
Concepts such as Locations, Pathways and Adjectives-
of-Location use vocabulary that is specific to an environ-
ment, and the vocabulary of these concepts will change
104
Corpus #Instr Words/Instr Environmnt Modality H/R-H/R LiftingDevice PathWays Landmarks Adjectives
NAV 934 9 UnivCampus Speech Human-Human 0.029 0.046 0.169 0.13
MIT 684 15 UnivCampus Written Human-Human 0.045 0.016 0.163 0.062
IBL 769 8 ModelCity Speech Human-Robot n.a. 0.039 0.076 0.13
TTALK 1619 7 OpenSpace Speech Human-Robot n.a. 0.027 0.01 0.039
Figure 4: (a) Nature of the Corpora (b) Type-Token Ratio of Concepts across Corpora
Table 5: Higher level and Leaf node Concepts in Grammar
Category Concepts Examples
Imperative GoToPlace, Turn, etc
Conditional Imperative Move_Until_X where X is a condition
Advisory Instructions You_Will_See_Location
Grounding Instructions You_are_at_Location
Auxillary Concepts Examples
Locations buildings, other landmarks on the route
Adjectives-of-Locations large, open, black, small etc.
Pathways hallway, corridor, bridge, doors, etc.
LiftingDevice elevator, staircase, stairwell, etc.
Spatial Relations behind, above, on right, on left, etc.
Numbers turn-angles, distance, etc.
Ordinals first, second as in floor numbers
Filler phrases you may want to; you are gonna; etc.
with surroundings. We used an off-the-shelf part-of-
speech tagger (Toutanova et al, 2003) on NAV-train to
identify ?location-based? nouns and adjectives. These
were added to the grammar as instances of their respec-
tive concepts.
5.2 Parsing NAV Instructions
A parse can fall into one of the following categories: 1)
Complete: clean and correct parse with all concepts and
actions mentioned in the instruction. 2) Incomplete: If
some arguments for an action are missing. 3) Misparse:
no usable parse produced for an instruction.
Table 6 shows that 87% of the instructions from the
NAV corpus (excluding meta comments) are parsed cor-
rectly. Correct parses were produced for 89% of Imper-
atives, 87% of Advisory and 73% of Grounding instruc-
tions. Meta comments were excluded because they do
not constitute any valid actions and can be ignored. Nev-
ertheless 20% of the meta comments produced a valid
parse (i.e. unintended action).
5.3 Grammar Generality
The results for the NAV corpus seem encouraging but it
would be useful to know whether the NAV grammar gen-
eralizes to other directions scenarios. We selected three
corpora to examine this question: MIT (Kollar et al,
2010), IBL3 (Bugmann et al, 2004) and TTALK4 (Marge
and Rudnicky, 2010). All were navigation scenarios but
were collected in a variety of settings (see Figure 4(a)).
Corpus vocabularies were normalized using the process
described in 5.1.1 and location specific nouns and adjec-
tives added to the grammar. Punctuation was removed.
Figure 4(b) shows the type-token ratios for ?variable?
concepts. There are more landmarks and adjectives (that
tag along landmarks) in NAV and MIT compared to IBL
and fewest in TTALK corpus (a closed space with two
robots). Since, IBL and TTALK do not involve exten-
sive navigation inside the buildings there are no instances
of the elevator concept. However, IBL corpus has ?ex-
its, roads, streets? in the city environment which were
included in the PathWay concept.
5.4 Performance across Corpora
We randomly sampled 300 instructions from each of the
three corpora (MIT, IBL and TTALK) and evaluated their
parses against manually-created parses. Table 6) shows
results for each type of parse (Complete, Incomplete, or
Misparse). Meta comments were excluded, as discussed
earlier. The NAV grammar appears portable to three other
corpora. As shown in Category-Accuracy of Table 6 Im-
peratives and Advisory instructions are well-parsed by
the grammar. In TTALK corpus, there are very few land-
mark names but there are certain unusual sentences e.g.,
?she to the rear left hand wall of the room? causing lower
accuracy in Advisory instructions. We noticed that MIT
corpus had longer description of the landmarks, leading
to lower accuracy for Grounding. From Table 6 11% to
16% of Imperative instructions fail to get parsed across
the corpora. We consider these failures/errors below.
5.5 Error Analysis
We found six situations that produced incomplete and
misparsed instructions: (1) Underspecified arguments;
(2) Unusual or unobserved phrases; (2) False-starts and
ungrammatical language; (3) Uncovered words; (4) Pro-
longed description of landmarks within an instruction;
3http://www.tech.plym.ac.uk/soc/staff/guidbugm/ibl/readme1.html
4http://www.cs.cmu.edu/?robotnavcps/
105
Table 6: Parse Results
Parse Results NAV MIT IBL TTALK
# Instructions 280 300 300 300
% Complete 87% 78.8% 83.8% 83.4%
% Incomplete 3.1% 17% 6.6% 3.7%
% Misparse 9.8% 4.1% 9.5% 13%
Category Accuracy
Imperative 89% 89.4% 86.5% 84.7%
Advisory 87% 93.4% 87.4% 60%
Grounding 73% 62% 100% 100%
(5) Coreferences; 6) Non-specific instructions (eg. either
take the right hallway or the left hallway).
5.5.1 Incomplete and Misparsed Instructions
Out-of-Vocabulary (OOV) words were responsible for
the majority of incomplete parses across all the corpora;
many were singletons. Unusual phrases such as ?as if you
are doubling back on yourself? caused incomplete parses.
We also observed lengthy descriptions in instructions in
the MIT corpus, leading to incomplete parses. This cor-
pus was unusual in that it is composed of written, as op-
posed to spoken, instructions.
Misparsed instructions were caused due to both un-
grammatical phrases and OOV words. Ungrammatical
instructions contained either missed key content words
like verbs or false starts. These instructions did contain
meaningful fragments but they did not form a coherent
utterance e.g., ?onto a roundabout?.
We note that incomplete or otherwise non-
understandable utterancess can in principle be recovered
through clarification dialog (see e.g., (Bohus and Rud-
nicky, 2005). Direction giving should perhaps not be
limited to monologue delivery.
Table 7: Error Analysis for Incomplete and Misparsed instruc-
tions
Incomplete NAV MIT IBL TTALK
# Incomplete Instructions 8 49 19 10
MissingArgs 50% 8% 0% 0%
UnusualPhrases 0% 28% 35% 60%
Lengthy Descriptions 0% 20.4% 0% 0%
Coreferences 0% 0% 20.2% 0%
Non-concrete phrases 3% 2% 5% 0%
OOVs 47% 41.6% 39.8% 40%
Misparse
# Misparse Instructions 25 12 27 39
Ungrammatical phrases 24% 44% 16% 10%
OOVs 76% 66% 84% 90%
6 Conclusion
To better understand the structure of instructions and to
investigate how these might be automatically processed,
we collected a corpus of spoken instructions. We found
that instructions can be organized in terms of a straighfor-
ward two-level taxonomy. We examined the information
contents of different components and found that that the
Imperative and Advisory categories appear to be the most
relevant, though our subjects had little difficulty dealing
with instructions composed of only Imperatives; physical
context would seem to matter.
We found that it was possible to design a grammar that
reasonably covered the information-carrying instructions
in a set of instructions. And that a grammar built from our
corpus generalized quite well to corpora collected under
different circumstances.
Our study suggests that robust instruction-
understanding systems can be implemented and,
other than the challenge of dealing with location-specific
data, can be deployed in different environments. We
believe that this study also highlights the importance
of dialog-based clarification and the need for strate-
gies that can recognize and capture out-of-vocabulary
words. These capabilities are being incorporated into a
robot navigation system that can take instructions from
humans.
References
G. Allen. 1997. From knowledge to words to wayfinding:
Issues in the production and comprehension of route direc-
tions. Spatial Information Theory A Theoretical Basis for
GIS, pages 363?372.
D. Bohus and A.I. Rudnicky. 2005. Sorry, i didn?t catch that!-
an investigation of non-understanding errors and recovery
strategies. In 6th SIGdial Workshop on Discourse and Di-
alogue.
G. Bugmann, E. Klein, S. Lauria, and T. Kyriacou. 2004.
Corpus-based robotics: A route instruction example. Intelli-
gent Autonomous Systems 8.
D. Caduff and S. Timpf. 2008. On the assessment of land-
mark salience for human navigation. Cognitive processing,
9(4):249?267.
D.L. Chen and R.J. Mooney. 2010. Learning to interpret
natural language navigation instructions from observations.
Journal of Artificial Intelligence Research, 37:397?435.
M.P. Daniel and M. Denis. 1998. Spatial descriptions as navi-
gational aids: A cognitive analysis of route directions. Kog-
nitionswissenschaft, 7(1):45?52.
M. Denis, F. Pazzaglia, C. Cornoldi, and L. Bertolo. 1999.
Spatial discourse and navigation: An analysis of route di-
rections in the city of venice. Applied Cognitive Psychology,
13(2):145?174.
K. Eberhard, H. Nicholson, S. Kubler, S. Gundersen, and
M. Scheutz. 2010. The indiana .cooperative remote search
task.(crest) corpus. In Proc. of LREC, volume 10.
A. Gargett, K. Garoufi, A. Koller, and K. Striegnitz. 2010. The
give-2 corpus of giving instructions in virtual environments.
In Proc. of LREC.
106
T. Kollar, S. Tellex, D. Roy, and N. Roy. 2010. Toward under-
standing natural language directions. In Proceeding of the
5th ACM/IEEE HRI. ACM.
K. Lovelace, M. Hegarty, and D. Montello. 1999. Elements
of good route directions in familiar and unfamiliar environ-
ments. Spatial information theory. Cognitive and computa-
tional foundations of geographic information science, pages
751?751.
M. MacMahon, B. Stankiewicz, and B. Kuipers. 2006. Walk
the talk: Connecting language, knowledge, and action in
route instructions. Def, 2(6):4.
M. Marge and A.I. Rudnicky. 2010. Comparing spoken lan-
guage route instructions for robots across environment rep-
resentations. In SIGDIAL.
M. Marge, S. Banerjee, and A.I. Rudnicky. 2010. Us-
ing the amazon mechanical turk for transcription of spo-
ken language. In Acoustics Speech and Signal Processing
(ICASSP), 2010 IEEE International Conference on, pages
5270?5273. IEEE.
C. Matuszek, D. Fox, and K. Koscher. 2010. Following direc-
tions using statistical machine translation. In Proceeding of
the 5th ACM/IEEE international conference on Human-robot
interaction, pages 251?258. ACM.
R. Rosenfield. 1995. The cmu statistical language modeling
toolkit and its use in the 1994 arpa csr evaluation.
L. Stoia, D.M. Shockley, D.K. Byron, and E. Fosler-Lussier.
2008. Scare: A situated corpus with annotated referring ex-
pressions. In LREC 2008.
K. Toutanova, D. Klein, C.D. Manning, and Y. Singer. 2003.
Feature-rich part-of-speech tagging with a cyclic dependency
network. In Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computational Lin-
guistics on Human Language Technology-Volume 1, pages
173?180. Association for Computational Linguistics.
W. Ward. 1991. Understanding spontaneous speech: the
phoenix system. In ICASSP. IEEE.
107
Proceedings of the SIGDIAL 2013 Conference, pages 242?250,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
Predicting Tasks in Goal-Oriented Spoken Dialog Systems
using Semantic Knowledge Bases
Aasish Pappu and Alexander I. Rudnicky
Language Technologies Institute
Carnegie Mellon University
{aasish, air}@cs.cmu.edu
Abstract
Goal-oriented dialog agents are expected
to recognize user-intentions from an utter-
ance and execute appropriate tasks. Typi-
cally, such systems use a semantic parser
to solve this problem. However, semantic
parsers could fail if user utterances contain
out-of-grammar words/phrases or if the se-
mantics of uttered phrases did not match
the parser?s expectations. In this work,
we have explored a more robust method
of task prediction. We define task predic-
tion as a classification problem, rather than
?parsing? and use semantic contexts to im-
prove classification accuracy. Our classi-
fier uses semantic smoothing kernels that
can encode information from knowledge
bases such as Wordnet, NELL and Free-
base.com. Our experiments on two spoken
language corpora show that augmenting
semantic information from these knowl-
edge bases gives about 30% absolute im-
provement in task prediction over a parser-
based method. Our approach thus helps
make a dialog agent more robust to user
input and helps reduce number of turns re-
quired to detected intended tasks.
1 Introduction
Spoken dialog agents are designed with particular
tasks in mind. These agents could provide infor-
mation or make reservations, or other such tasks.
Many dialog agents often can perform multiple
tasks: think of a customer service kiosk system
at a bank. The system has to decide which task it
has to perform by talking to its user. This problem
of identifying what to do based on what a user has
said is called task prediction.
Task prediction is typically framed as a parsing
problem: A grammar is written to semantically
parse the input utterance from users, and these se-
mantic labels in combination are used to decide
what the intended task is. However, this method
is less robust to errors in user-input. A dialog sys-
tem consists of a pipeline of cascaded modules,
such as speech recognition, parsing, dialog man-
agement. Any errors made by these modules pro-
pogate and accumulate through the pipeline. Bo-
hus and Rudnicky (2005) have shown that this
cascade of errors, coupled with users employ-
ing out-of-grammar phrases results in many ?non-
understanding? and ?misunderstanding? errors.
There have been other approaches to perform
dialog task prediction. Gorin et al (1997) has pro-
posed a salience-phrase detection technique that
maps phrases to their corresponding tasks. Chu-
Carroll and Carpenter (1999) casted the task de-
tection as an information retrieval ? detect tasks
by measuring the distance between the query vec-
tor and representative text for each task. Bui
(2003) and Blaylock and Allen (2006) have cast it
as a hierarchical sequence labeling problem using
Hidden Markov Models (HMM). More recently,
(Bangalore and Stent, 2009) built an incremen-
tal parser that gradually determines the task based
on the incoming dialog utterances. (Chen and
Mooney, 2010) have developed a route instruc-
tions frame parser to determine the task in the con-
text of a mobile dialog robot. These approaches
mainly use local features such as dialog context,
speech features and grammar-based-semantic fea-
tures to determine the task. However grammar-
based-semantic features would be insufficient if
an utterance uses semantically similar phrases that
are not in the system?s domain or semantics. If
the system could explore semantic information be-
yond the scope of its local knowledge and use ex-
ternal knowledge sources then they will help im-
prove the task prediction.
(Cristianini et al, 2002) (Wang and Domeni-
coni, 2008) (Moschitti, 2009) found that open-
242
domain semantic knowledge resources are use-
ful for text classification problems. Their success
in limited data scenario is an attractive prospect,
since most dialog agents operate in scarce train-
ing data scenarios. (Bloehdorn et al, 2006) has
proposed a semantic smoothing kernel based ap-
proach for text classification. The intuition be-
hind their approach is that terms (particularly con-
tent words) of two similar sentences or documents
share superconcepts (e.g., hypernyms) in a knowl-
edge base. Semantic Similarity between two terms
can be computed using different metrics (Pedersen
et al, 2004) based on resources like WordNet.
Open domain resources such as world-wide-
web, had been used in the context of speech recog-
nition. (Misu and Kawahara, 2006) and (Creutz
et al, 2009) used web-texts to improve the lan-
guage models for speech recognition in a target
domain. They have used a dialog corpus in or-
der to query relevant web-texts to build the target
domain models. Although (Araki, 2012) did not
conduct empirical experiments, yet they have pre-
sented an interesting architecture that exploits an
open-domain resource like Freebase.com to build
spoken dialog systems.
In this work, we have framed the task prediction
problem as a classification problem. We use the
user?s utterances to extract lexical semantic fea-
tures and classify it into being one of the many
tasks the system was designed to perform. We
harness the power of semantic knowledge bases
by bootstraping an utterance with semantic con-
cepts related to the tokens in the utterance. The se-
mantic distance/similarity between concepts in the
knowledge base is incorporated into the model us-
ing a kernel. We show that our approach improves
the task prediction accuracy over a grammar-based
approach on two spoken corpora (1) Navagati
(Pappu and Rudnicky, 2012): a corpus of spo-
ken route instructions, and (2) Roomline (Bohus,
2003): a corpus of spoken dialog sessions in room-
reservation domain.
This paper is organized as following: Section
2 describes the problem of dialog task predic-
tion and the standard grammar based approach to
predict the dialog task. Then in Section 3, we
describe the open-domain knowledge resources
that were used in our approach and their advan-
tages/disadvantages. We will discuss our semantic
kernel based approach in the Section 4. We report
our experiment results on task prediction in Sec-
tion 5. In Section 6, we will analyze the errors that
occur in our approach, followed by concluding re-
marks and possible directions to this work.
2 Parser based Dialog Task Prediction
In a dialog system, there are two functions of a
semantic grammar ? encode linguistic constructs
used during the interactions and represent the do-
main knowledge in-terms of concepts and their in-
stances. Table 1 illustrates the tasks and the con-
cepts used in a navigation domain grammar. The
linguistic constructions help the parser to segment
an utterance into meaningful chunks. The domain
knowledge helps in labeling the tokens/phrases
with concepts. The parser uses the labeled tokens
and the chunked form of the utterance, to classify
the utterance into one of the tasks.
Table 1: Tasks and Concepts in Grammar
Tasks Examples
Imperative GoToPlace, Turn, etc
Advisory Instructions You_Will_See_Location
Grounding Instructions You_are_at_Location
Concepts Examples
Locations buildings, other landmarks
Adjectives-of-Locations large, open, black, small etc.
Pathways hallway, corridor, bridge, etc.
LiftingDevice elevator, staircase, etc.
Spatial Relations behind, above, on left, etc.
Numbers turn-angles, distance, etc.
Ordinals first, second, etc. floor numbers
The dialog agent uses the root node of a parser
output as the task. Figure 1 illustrates a semantic
parser output for a fictitious utterance in the nav-
igation domain. The dialog manager would con-
sider the utterance as an ?Imperative? for this ex-
ample.
Imperative
go direction
forward
distance
number
five
units
meters
Figure 1: Illustration of Semantic Parse Tree used
in a Dialog System
243
2.1 Grammar: A Knowledge Resource
Grammar is a very useful resource for a dialog sys-
tem because it could potentially represent an ex-
pert?s view of the domain. Since knowledge en-
gineering requires time and effort, very few di-
alog systems can afford to have grammars that
are expert-crafted and robust to various artefacts
of spoken language. This becomes a major chal-
lenge for real world dialog systems. If the sys-
tem?s grammar or the domain knowledge does not
conform to its users and their utterances, the parser
will fail to produce a correct parse, if the parse
is incorrect and/or the concept labeling is incor-
rect. Lack of comprehensive semantic knowledge
is the cause of this problem. An open-domain
knowledge base like Wordnet (Miller, 1995), Free-
base (Bollacker et al, 2008) or NELL (Carlson
et al, 2010) contains comprehensive information
about concepts and their relationships present in
the world. If used appropriately, open-domain
knowledge resources can help compensate for in-
complete semantic knowledge of the system.
3 Open-Domain Semantic Knowledge
Bases
Like grammars, open-domain knowledge re-
sources contain concepts, instances and relations.
The purpose of these resources is to organize
common sense and factoid information known to
the mankind in a machine-understandable form.
These resources, if filtered appropriately, contain
valuable domain-specific information for a dialog
agent. To this end, we propose to use three knowl-
edge resources along with the domain grammar for
the task prediction. A brief overview of each of the
knowledge resources is given below:
3.1 Wordnet: Expert Knowledge Base
Wordnet (Miller, 1995) is an online lexical
database of words and their semantics curated
by language experts. It organizes the words and
their morphological variants in a hierarchical fash-
ion. Every word has at least one synset i.e.,
sense and a synset has definite meaning and a
gloss to illustrate the usage. Synsets are con-
nected through relationships such as hypernyms,
hyponyms, meronyms, antonyms etc. Each synset
can be considered as an instance and their par-
ent synsets as concepts. Although Wordnet con-
tains several ( 120,000) word forms, some of our
domain-specific word forms (e.g., locations in a
navigation domain) will not be present. Therefore,
we would like to use other open-domain knowl-
edge bases to augment the agent?s knowledge.
3.2 Freebase: Community Knowledge Base
Freebase.com (Bollacker et al, 2008) is a col-
laboratively evolving knowledge base with the
effort of volunteers. It organizes the facts
based on types/concepts along with several predi-
cates/properties and their values for each fact. The
types are arranged in a hierarchy and the hierar-
chy is rooted at ?domain?. Freebase facts are con-
stantly updated by the volunteers. Therefore, it is a
good resource to help bootstrap the domain knowl-
edge of a dialog agent.
3.3 NELL: Automated Knowledge Base
Never-Ending Language Learner(NELL) (Carlson
et al, 2010) is a program that learns and organizes
the facts from the web in an unsupervised fashion.
NELL is on the other end of the knowledge base
spectrum which is not curated either by experts or
by volunteers. NELL uses a two-step approach to
learn new facts: (1) extract information from the
text using pattern-based, semi-structured relation
extractors (2) improve the learning for next itera-
tion based on the evidence from previous iteration.
Every belief/fact in its knowledge base has con-
cepts, source urls, extraction patterns, predicate,
the surface forms of the facts and a confidence
score for the belief. Although the facts could be
noisy in comparison to ones in other knowledge
bases, NELL continually adds and improves the
facts without much human effort.
4 Semantic Kernel based Dialog Task
Prediction
We would like to use this apriori knowledge about
the world and the domain to help us predict the
dialog task. The task prediction problem can be
treated as a classification problem. Classification
algorithms typically use bag-of-words representa-
tion that converts a document or sentence into a
vector with terms as components of the vector.
This representation produces very good results in
scenarios with sufficient training data. However
in a limited training data or extreme sparseness
scenario such as ours, (Siolas and d?Alch? Buc,
2000) has shown that Semantic Smoothing Ker-
nel technique is a promising approach. The major
advantage of this approach is that they can incor-
244
porate apriori knowledge from existing knowledge
bases. The semantic dependencies between terms,
dependencies between concepts and instances, can
be encoded in these kernels. The semantic kernels
can be easily plugged into a kernel based classi-
fier help us predict the task from the goal-oriented
dialog utterances.
In our experiments, we used an implementation
of Semantic Kernel from (Bloehdorn et al, 2006)
and plugged it into a Support Vector Machine
(SVM) classifier (SVMlight) (Joachims, 1999). As
a part of experimental setup, we will describe the
details of how did we extract the semantic depen-
dencies from each knowledge base and encoded
them into the kernel.
5 Experiments
Our goal is to improve the task prediction for a
given spoken dialog utterance by providing addi-
tional semantic context to the utterance with the
help of relevant semantic concepts from the se-
mantic knowledge bases. The baseline approach
would use the Phoenix parser?s output to deter-
mine the intended task for an utterance. From our
experiments, we show that our knowledge-driven
approach will improve upon the baseline perfor-
mance on two corpora (1) Navagati Corpus: a nav-
igation directions corpus (2) Roomline Corpus: a
room reservation dialog corpus.
5.1 Setup
We have divided each corpus into training and test-
ing datasets. We train our task classification mod-
els on the manual transcriptions of the training
data and evaluated the models on the ASR output
of the testing data. Both Navagati and Roomline
corpora came with manually annotated task labels
and manual transcriptions for the utterances. We
filtered out the non-task utterances such as ?yes?,
?no? and other clarifications from the Roomline
corpus. We obtained the ASR output for the Nava-
gati corpus by running the test utterances through
PocketSphinx (Huggins-Daines et al, 2006). The
Roomline corpus already had the ASR output for
the utterances. Table 2 illustrates some of the
statistics for each corpus.
Our baseline model for the task detection is the
Phoenix (Ward, 1991) parser output, which is the
default method used in the Ravenclaw/Olympus
dialog systems (Bohus et al, 2007). For the Nava-
gati Corpus we have obtained the parser output us-
ing the grammar and method described in (Pappu
and Rudnicky, 2012). For the Roomline corpus,
we extracted the parser output from the session
logs from the the corpus distribution.
Corpus-Stats Navagati RoomLine
Tasks 4 7
Words 503 498
Word-Error-rate 46.3% 25.6%
Task Utts 934 18911
Task Training-Utts 654 1324
Task Testing-Utts 280 567
Tasks
N1. Meta R1. NeedRoom
N2. Advisory R2. ChooseRoom
N3. Imperative R3. QueryFeatures
N4. Grounding R4. ListRooms
R5. Identification
R6. CancelReservation
R7. RejectRooms
Table 2: Corpus Statistics
5.1.1 Semantic Facts to Semantic Kernel
The semantic kernel takes a term proximity ma-
trix as an input, then produces a positive semidef-
inite matrix which can be used inside the kernel
function. In our case, we build a term proxim-
ity matrix for the words in the recognition vocabu-
lary. (Bloehdorn et al, 2006) found that using the
term-concept pairs in the proximity matrix is more
meaningful following the intuition that terms that
share more number of concepts are similar as op-
posed to terms that share fewer concepts. We have
used following measures to compute the proximity
value P and some of them are specific to respec-
tive knowledge bases:
? gra: No weighting for term-concept pairs in
the Grammar, i.e.,
P = 1, for all concepts ci of t, P = 0 other-
wise.
? fb: Weighting using normalized Free-
base.com relevance score, i.e.,
P = fbscore(t, ci)? fbscore(t, cmin)fbscore(t, cmax)? fbscore(t, cmin)(1)
? nell: Weighting for the NELL term-concept
pairs using the probability for a belief i.e.,
P = nellprob(t, ci) (2)
, for all concepts ci of t, P = 0 otherwise.
1Originally has 10356 utts; filtered out non-task utts.
245
? wnpath: Weighting for the term-concept
pairs in the Wordnet based on the shortest
path, i.e.,
P = wnPATH(t, ci) (3)
for all concepts ci of t, P = 0 otherwise.
? wnlch: Weighting for the term-concept
pairs in the Wordnet based on the Leacock-
Chodorow Similiarity, i.e.,
P = wnLCH(t, ci) (4)
for all concepts ci of t, P = 0 otherwise.
? wnwup: Weighting for the term-concept
pairs in the Wordnet based on the Wu-Palmer
Similarity, i.e.,
P = wnWUP (t, ci) (5)
for all concepts ci of t, P = 0 otherwise.
? wnres: Weighting for the term-concept
pairs in the Wordnet based on the Resnik
Similarity using Information Content, i.e.,
P = wnRES(t, ci) (6)
for all concepts ci of t, P = 0 otherwise.
To create a grammar-based proximity matrix,
we extracted the concept-token pairs from the
parser output on the reference transcriptions in
both corpora. In order to create a wordnet-based
proximity matrix, we retrieve the hypernyms for
the corresponding from Wordnet using the Word-
net 3.0 database 2. For the freebase concept-token
pairs, we query tokens for a list of types with the
help of the MQL query interface3 to the freebase.
To retrieve beliefs from NELL we downloaded a
tsv formatted database called every-belief-in-the-
KB4 and then queried for facts using unix grep
command.
5.2 Results
Our objective is to evalute the effect of augmented
semantic features on the task detection. As noted
earlier, we divided both corpora into training and
testing datasets. We build our models on the man-
ual transcriptions from the training data and eval-
uate on the ASR hypotheses of the testing data.
2http://www.princeton.edu/wordnet/download/
3https://www.googleapis.com/freebase/v1/search
4http://rtw.ml.cmu.edu/rtw/resources
For the Navagati corpus, we use the same training-
testing split that we used in our previous work be-
cause the grammar was developed based on the
training data. For the Roomline corpus, we ran-
domly sample 30% of the testing data from the
entire corpus.
Our first semantic-kernel based model SEM-
GRA uses the domain grammar as a ?knowledge
base?. This is a two step process: (1) we extract
the concept-token pairs from the parse output of
the training data. (2) Then, assign a uniform prox-
imity score (1.0) for all pairs of words that ap-
pear under a particular concept otherwise 0.0 (gra
as mentioned in the previous section). We aug-
ment the grammar concepts to the utterances in
the datasets, learn SEM-GRA model and classify
the test-hypotheses. For all our models we use
a fixed C = 0.07 value (soft-margin parameter)
for the SVM classifiers. This model achieved high-
est performance at this value during a parameter-
sweep. SEM-GRA model outperformed the parser-
baseline on both datasets (see Table 3). It clearly
takes advantage of the domain knowledge encoded
in the form of semantic-relatedness between con-
cepts and token pairs.
What if a dialog system does not have gram-
mar to begin with? We use the same two step pro-
cess to build semantic-kernel based models using
one open-domain knowledge base at a time. We
built Wordnet based models (SEM-WNWUP, SEM-
WNPATH, SEM-WNLCH, SEM-WNRES) using dif-
ferent proximity measures described in the previ-
ous section. From Table 3 SEM-WNRES model,
one that uses information content performs the
best among all wordnet based models. In order
to compute the information content we used the
pair-wise mutual information scores available for
brown-corpus.dat in the NLTK (Bird et al, 2009)
distribution. Other path based scores were also
computed using NLTK API for Wordnet.
We observed that our wordnet-based models
capture relatedness between most-common nouns
(e.g., room numbers) and their concepts but not
for some of the less-common ones (e.g., loca-
tion names). To compensate this imbalance, we
use larger knowledge resources freebase.com and
NELL. First we searched for the facts in each of
these knowledge bases using every token in the vo-
cabulary of both corpora. We pick the top concept
for each token based on the score provided by the
respective search interfaces. In freebase we have
246
Table 3: F1 (in %) comparison of parse baseline against semantic-kernel models with their corresponding
similarity metrics
Corpus baseline SEMGRA SEMWNWUP SEMWNPATH SEMWNLCH SEMWNRES SEMFBASE SEMNELL
Navagati 40.1 65.8 67.1 67.7 66.4 69 68.7 66.2
Roomline 54.3 79.7 77.3 79.5 79.6 80.6 83.3 81.1
about 100 concepts that are relevant to the vocab-
ulary and in the NELL model we have about 250
concepts that are relevant to the vocabulary in each
of the corpora. The models based on NELL (SEM-
NELL) and Freebase (SEM-FBASE) capture relat-
edness between less-common nouns and their con-
cepts. We can see that both of these models per-
form comparable to the domain grammar model
SEM-GRA which also captures the relatedness be-
tween less-common nouns and their concepts. We
believe that both freebase and NELL has a supe-
rior performance because of wider-range of con-
cept coverage and non-uniform proximity mea-
sures used in the semantic kernel, which gives
a better judgement of relatedness than a uniform
measure used in the SEM-GRA model.
Since we observed that an individual model is
good at capturing a particular aspect of an utter-
ance, we extended the individual semantic models
by combining the proximity matrices from each
of them and augmenting their semantic concepts
to the training and testing datasets. We built four
combined models as shown in Table 4 by varying
the wordnet?s proximity metric to identify which
one of them works best in combination with other
semantic metrics. The wnres metric performs the
best both in standalone and combination settings.
(Bloehdorn et al, 2006) also found that wnres
particularly performs well for lower values of the
soft-margin parameter in their experiments.
Table 4: F1-Score (in %): Models with semantics
combined from different KBs (ALL-KB)
Model Navagati Roomline
GRA+WNWUP+FBASE+NELL 70.8 82.2
GRA+WNPATH+FBASE+NELL 70.1 81.4
GRA+WNLCH+FBASE+NELL 70.8 81.3
GRA+WNRES+FBASE+NELL 73.4 83.7
6 Discussion
We have built a model that exploits different se-
mantic knowledge bases and predicts the task on
both corpora with high accuracy. But how is it af-
fected by factors like misrecognition and context
ambiguity?
6.1 Influence of Recognition Errors
When the recognition is bad, it is obvious that the
accuracy would go down. We would like to know
which of these knowledge resources can augment
useful semantics despite misrecognitions. Table 2
shows that WER on the Navagati corpus is about
46% and the Roomline corpus is about 25%. We
compared the F1-score of different models on ut-
terances for different ranges of WER as shown in
the Figure 2 on the Navagati Corpus. We notice
that the model built using all knowledge bases is
more robust even at higher WER. We did similar
analysis on the Roomline corpus and did not no-
tice any differences across models due to relatively
lower WER (25.6%).
0 20 40 60
50
60
70
80
90
Word Error Rate
F1
Sc
or
e
all-kb
wnres
nell
fbase
Figure 2: Word Error Rate vs F1-Score for KB-
based Models on Navagati Corpus
6.2 Confusion among Tasks
We found that a particular pair of tasks are more
confusing than others. Here we present an analysis
of such confusion pairs for both corpora for dif-
ferent classification models. Table 5 and Table 6
show the pairs of tasks that are most confused in
the experiments. The ALL-KB model (a combina-
tion of all knowledge bases) has least number of
247
Table 5: Most confusable pairs of tasks in Navagati Corpus for KB based classification models
(See Table 2 for task labels)
KBType ALL-KB SEM-WNRES SEM-NELL SEM-FBASE
ActualTask N2 N4 N2 N4 N2 N4 N1 N2 N4
Predicted N3 N1 N3 N3 N3 N3 N3 N3 N3
ConfusionPerTask 10.5% 27.7% 26.3% 33.3% 26.3% 38.8% 22.2% 28.9% 44.4%
Table 6: Most confusable pairs of tasks in Roomline Corpus for KB based classification models
(See Table 2 for task labels)
KBType ALL-KB SEM-WNRES SEM-NELL SEM-FBASE
ActualTask R4 R4 R6 R4 R6 R3 R4 R5 R6
Predicted R3 R5 R5 R1 R1 R1 R3 R1 R1
ConfusionPerTask 36.6% 48.7% 44.4% 25.6% 44.5% 32.5% 23% 53.4% 55.5%
confusion pairs among all the models. This is due
to more relevant concepts are augmented to an ut-
terance compared to fewer relevant concepts that
augmented while using individual models.
We inspected the confused tasks by examin-
ing the feature vectors of misclassified examples.
While using the ALL-KB model 10% of the utter-
ances from N2 (Advisory) were confused for N3
(Imperative) because of phrases like ?your left?,
?your right?. These phrases were often associated
with N3 utterances. To recovery from such ambi-
guities, the agent could ask a clarification question
e.g., ?are we talking about going there or find it
on the way?? to resolve the differences between
these tasks. The system could not only get clar-
ification but also bootstrap the original utterance
of the user with the clarification to gather addi-
tional context to retrain the task detection models.
The individual models were also confused about
N2 and N3 tasks, where we could use similar clar-
ification strategies to improve the task prediction.
27% of the N4 (grounding about current robot?s
position) utterances were confused for N1 (meta
comments about the robot?s rounavigation route)
because these utterances shared more number of
freebase concepts with the N1 model. The system
could resolve such utterances by asking a clarifi-
cation question ?are we talking about the current
position??. Individual models i.e., SEM-WNRES,
SEM-FBASE and SEM-NELL suffered mostly from
the lack of concepts capturing semantics related
to all types of entities (e.g., most common nouns,
less common entities etc.,) found in an utterance.
We examined the confusion pairs in the Room-
line corpus and observed that R4 (ListRooms) and
R3 (Queries) tasks were most confused in the
ALL-KB model. On closer inspection, we found
that R4 utterances are about listing the rooms that
are retrieved by the system. Whereas, R3 utter-
ances are about asking whether a room has a facil-
ity (e.g., projector availability). In the ambiguous
utterances, often the R4 utterances were about fil-
tering the list of rooms by a facility type.
7 Conclusion
We proposed framing the dialog task prediction
problem as a classification problem. We used an
SVM classifier with semantic smoothing kernels
that incorporate information from external knowl-
edge bases such as Wordnet, NELL, Freebase. Our
method shows good improvements over a parser-
based baseline. Our analysis also shows that our
proposed method makes task prediction be more
robust to moderate recognition errors.
We presented an analysis on task ambiguity and
found that these models can confuse one task for
another. We believe that this analysis highlights
the need for dialog based clarification strategies
that cannot only help the system for that instance
but also help the system improve its task predic-
tion accuracy in future dialog sessions.
8 Future Work
This work stands as a platform to make a spoken
dialog system learn relevant semantic information
from external knowledge sources. We would like
to extend this paradigm to let the system acquire
more information through dialog with a user. The
system could elicit new references to a known se-
mantic concept. For example, a navigation agent
knows a task called ?GoToRestaurant? but the
user-utterance had the word ?diner? and it was
248
not seen in the context of ?restaurant?. The agent
somewhat predicts this utterance is related to ?Go-
ToRestaurant? using the approach described in this
paper. It could ask the user an elicitation question:
?You used diner in the context of a restaurant, is
diner really a restaurant??. The answer to this
question will help the system gradually understand
what parts of an open-domain knowledge base can
be added into its own domain knowledge base. We
believe that the holistic approach of learning from
automated processes and learning through dialog,
will help the dialog systems get better interaction
by interaction.
References
Masahiro Araki. 2012. Rapid development process of
spoken dialogue systems using collaboratively con-
structed semantic resources. In Proceedings of the
13th Annual Meeting of the Special Interest Group
on Discourse and Dialogue, pages 70?73, Seoul,
South Korea, July. Association for Computational
Linguistics.
Srinivas Bangalore and Amanda J Stent. 2009. In-
cremental parsing models for dialog task structure.
In Proceedings of the 12th Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics, pages 94?102. Association for Compu-
tational Linguistics.
Steven Bird, Ewan Klein, and Edward Loper. 2009.
Natural language processing with Python. O?Reilly
Media.
Nate Blaylock and James Allen. 2006. Hierarchical
instantiated goal recognition. In Proceedings of the
AAAI Workshop on Modeling Others from Observa-
tions.
Stephan Bloehdorn, Roberto Basili, Marco Cammisa,
and Alessandro Moschitti. 2006. Semantic ker-
nels for text classification based on topological mea-
sures of feature similarity. In Data Mining, 2006.
ICDM?06. Sixth International Conference on, pages
808?812. IEEE.
Dan Bohus and Alexander I Rudnicky. 2005.
Sorry, I didn?t catch that!-an investigation of non-
understanding errors and recovery strategies. In 6th
SIGdial Workshop on Discourse and Dialogue.
Dan Bohus, Antoine Raux, Thomas K Harris, Maxine
Eskenazi, and Alexander I Rudnicky. 2007. Olym-
pus: an open-source framework for conversational
spoken language interface research. In Proceedings
of the workshop on bridging the gap Academic and
industrial research in dialog technologies, number
April, pages 32?39. Association for Computational
Linguistics.
Dan Bohus. 2003. Roomline. http://www.cs.
cmu.edu/~dbohus/RoomLine.
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: a col-
laboratively created graph database for structuring
human knowledge. SIGMOD 08 Proceedings of the
2008 ACM SIGMOD international conference on
Management of data, pages 1247?1249.
Hung H Bui. 2003. A general model for online proba-
bilistic plan recognition. In International Joint Con-
ference on Artificial Intelligence, volume 18, pages
1309?1318. Citeseer.
Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr
Settles, Estevam R Hruschka Jr., and Tom M
Mitchell. 2010. Toward an Architecture for Never-
Ending Language Learning. Artificial Intelligence,
2(4):1306?1313.
D.L. Chen and R.J. Mooney. 2010. Learning to in-
terpret natural language navigation instructions from
observations. Journal of Artificial Intelligence Re-
search, 37:397?435.
Jennifer Chu-Carroll and Bob Carpenter. 1999.
Vector-based natural language call routing. Compu-
tational linguistics, 25(3):361?388.
Mathias Creutz, Sami Virpioja, and Anna Kovaleva.
2009. Web augmentation of language models for
continuous speech recognition of sms text messages.
In Proceedings of the 12th Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics, pages 157?165. Association for Com-
putational Linguistics.
Nello Cristianini, John Shawe-Taylor, and Huma
Lodhi. 2002. Latent semantic kernels. Journal of
Intelligent Information Systems, 18(2):127?152.
Allen L Gorin, Giuseppe Riccardi, and Jeremy H
Wright. 1997. How may i help you? Speech com-
munication, 23(1-2):113?127.
D. Huggins-Daines, M. Kumar, A. Chan, A.W. Black,
M. Ravishankar, and A.I. Rudnicky. 2006. Pocket-
sphinx: A free, real-time continuous speech recogni-
tion system for hand-held devices. In ICASSP, vol-
ume 1. IEEE.
Thorsten Joachims. 1999. Making large-scale support
vector machine learning practical. In Advances in
kernel methods, pages 169?184. MIT Press.
George A Miller. 1995. WordNet: a lexical
database for English. Communications of the ACM,
38(11):39?41.
Teruhisa Misu and Tatsuya Kawahara. 2006. A boot-
strapping approach for developing language model
of new spoken dialogue systems by selecting web
texts. In Proc. Interspeech, pages 9?12.
249
Alessandro Moschitti. 2009. Syntactic and semantic
kernels for short text pair categorization. In Pro-
ceedings of the 12th Conference of the European
Chapter of the ACL (EACL 2009), pages 576?584.
Aasish Pappu and Alexander I Rudnicky. 2012. The
Structure and Generality of Spoken Route Instruc-
tions. Proceedings of the 13th Annual Meeting of
the Special Interest Group on Discourse and Dia-
logue, pages 99?107.
Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004. WordNet:: Similarity: measuring the
relatedness of concepts. In Demonstration Papers
at HLT-NAACL 2004, pages 38?41. Association for
Computational Linguistics.
Georges Siolas and Florence d?Alch? Buc. 2000. Sup-
port vector machines based on a semantic kernel
for text categorization. In Neural Networks, 2000.
IJCNN 2000, Proceedings of the IEEE-INNS-ENNS
International Joint Conference, volume 5, pages
205?209. IEEE.
Pu Wang and Carlotta Domeniconi. 2008. Build-
ing semantic kernels for text classification using
wikipedia. In Proceeding of the 14th ACM SIGKDD
international conference on Knowledge discovery
and data mining, pages 713?721. ACM.
W. Ward. 1991. Understanding spontaneous speech:
the phoenix system. In ICASSP. IEEE.
250
Proceedings of the of the EACL 2014 Workshop on Dialogue in Motion (DM), pages 63?67,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Conversational Strategies for Robustly Managing Dialog in Public Spaces
Aasish Pappu Ming Sun
Language Technologies Institute
Carnegie Mellon University
Pittsburgh PA, USA
{aasish,mings,seshadrs,air}@cs.cmu.edu
Seshadri Sridharan Alexander I. Rudnicky
Abstract
Open environments present an attention
management challenge for conversational
systems. We describe a kiosk system
(based on Ravenclaw?Olympus) that uses
simple auditory and visual information to
interpret human presence and manage the
system?s attention. The system robustly
differentiates intended interactions from
unintended ones at an accuracy of 93%
and provides similar task completion rates
in both a quiet room and a public space.
1 Introduction
Dialog systems designers try to minimize disrup-
tive influences by introducing physical and be-
havioral constraints to create predictable environ-
ments. This includes using a closed-talking mi-
crophone or limiting interaction to one user at a
time. But such constraints are difficult to apply
in public environments such as kiosks (Bohus and
Horvitz, 2010; Foster et al., 2012; Nakashima et
al., 2014), in-car assistants (Kun et al., 2007; Hof-
mann et al., 2013; Misu et al., 2013) or on mo-
bile robots (Haasch et al., 2004; Sabanovic et al.,
2006; Kollar et al., 2012). To implement dialog
systems that operate in public spaces, we have to
relax some of these constraints and deal with addi-
tional challenges. For example, the system needs
to select the correct interlocutor, who may be only
one of several possible ones in the vicinity, then
determine whether they are initiating the process
of engaging with the system.
In this paper we focus on the problems of
identifying a potential interlocutor in the environ-
ment, engaging them in conversation and provid-
ing suitable channel-maintenance cues (Bruce et
al., 2002; Fukuda et al., 2002; Al Moubayed and
Skantze, 2011). We address these problems in the
context of a simple application, a kiosk agent that
Figure 1: Ravenclaw?Olympus augmented with
multimodal input and output functions.
accepts tasks such as taking a message to a named
recipient. To evaluate the effectiveness of our ap-
proach we compared the system?s ability to man-
age conversations in a quiet room and in a public
area.
The remainder of this paper is organized as fol-
lows: we first describe the system architecture,
then present the evaluation setup and the results,
then review related work and finally conclude with
an analysis of the study.
2 System Architecture
Figure 1 shows the architecture; it incorporates
Ravenclaw/Olympus (Bohus et al., 2007) stan-
dard components (in white), new components (in
black) and modified ones (shaded). In the system
pipeline, the Audio Server receives audio from a
microphone, endpoints it and sends it to the ASR
63
Figure 2: Face states; some are animations.
engine (PocketSphinx); the decoding is passed to
NLU (Phoenix parser). ICE (Input Confidence Es-
timation) (Helios) assigns confidence scores for
the input concepts. Based on user?s input and
the context, the Dialog Manager (DM) determines
what to do next, perhaps using data from the
Domain Reasoner (DR). An Interaction Manager
(IM) initiates a spoken response using Natural
Language Generation (NLG) and Text-to-Speech
(TTS) component.
Three components were added: (1) Multimodal
Capture acquires audio and human position data
using a Kinect device
1
. (2) Awareness deter-
mines whether there is a potential interlocutor in
the vicinity and their current position, using skele-
tal and azimuth information. (3) Talking Head
that conveys the system?s state (as shown in Fig-
ure 2): whether it?s active (conversing and hint-
ing) or idle (asleep and doze) and whether fo-
cused concepts are grounded (conversing and non-
understanding); certain state representations (e.g.,
conversing) are coordinated with the TTS compo-
nent.
3 Evaluation
A robust system should be able to function as well
in a difficult situation as in a controlled one. We
compare the system?s performance in two environ-
ments, public and quiet, and evaluate the (a) sys-
tem?s awareness of intended users, and its (b) end-
to-end performance.
The same twenty subjects participated in both
1
See http://www.microsoft.com/en-us/
Kinectforwindows/develop/. Three sources are
tapped: the beam-formed audio, the sound source azimuth
and skeleton coordinates. Video data are not used.
experiments: a mix of American, Indian, Chinese
and Hispanic with different fluency levels of En-
glish. None of them had previously interacted with
this system prior to this study.
The subjects were told that they would interact
with a virtual agent displayed on a screen. Their
task for the awareness experiment was to make the
agent aware that they wished to interact. For the
end-to-end system performance, the task was to
instruct the agent to send a message to a named
recipient.
3.1 Situated Awareness
We define situated awareness as correctly engag-
ing the intended interlocutor (i.e., verbally ac-
knowledge the user?s presence) under two con-
ditions. When the user is positioned (i) inside
the visual range of the Kinect at LOC-0 in Fig-
ure 3(a); and (ii) outside the visual range of the
Kinect at LOC-1 in Figure 3(a). We used the effec-
tive range of the camera?s documented horizontal
field of view (57
?
); hereafter referred as its cone-
of-awareness.
We conducted the awareness experiment in a
public space, a lounge at a hub connecting mul-
tiple corridors. The area has tables and seating,
self-serve coffee, a microwave oven, etc. The ex-
periment was conducted during regular hours, be-
tween 10am to 6pm on weekdays. During these
times we observed occupants discussing projects,
preparing food, making coffee, etc. No direct at-
tempt was made to influence their behavior and we
believe that they made no attempt to accommo-
date our activities. Accordingly, the natural sound
level in the room varied in unpredictable ways. To
supplement naturally-occurring sounds, we played
audio of a conversation between two humans, an
extract from the SwitchBoard corpus (Graff et al.,
2001). It was played using a loudspeaker placed at
LOC-2 in Figure 3(a). The locations (0, 1, and 2)
are all 1.5m from the Kinect, which we deemed to
be a comfortable distance for the subjects. LOC-
1 and LOC-2 are 70
?
to the left and right of the
Kinect, outside its cone.
To detect the presence of an intended user, we
build an awareness model that uses three sensory
streams viz., voice activity, skeleton, and sound
source azimuth. This model relies on the co-
incidence of azimuth angle and the skeleton angle
(along with voice activity) to determine the pres-
ence of an intended user. We compare the pro-
64
Figure 3: (a) Plan of Public Space (lounge);(b) Plan of Quiet Room (lab). Dark circled markers indicate
locations (LOC-0, LOC-1, LOC-2), discussed in the text.
Condition Voice +Skeleton +Azimuth
Outside
the cone 28% ? 93%
Inside
the cone ? 25% 93%
Table 1: Accuracy for the Awareness Detection
posed model with two baselines: (1) conventional
voice-activity-detection (VAD): once speech is de-
tected the system responds as if a conversation is
initiated and (2) based on skeleton plus VAD: once
the skeleton appears in front of the Kinect and a
voice is heard, the system engages in conversation.
Table 1 shows the combination of sensory
streams we used under two conditions. For the
outside-the-cone condition, the participants stand
in LOC-1 as shown in Figure 3(a) and follow the
instructions from the agent. Initially, the sub-
ject?s skeleton is invisible to the agent; however
the subject is audible to the agent. Therefore, in
certain combinations of sensors (e.g., voice +
skeleton model and voice + skeleton
+ azimuth model) the system attempts to guide
them to move in front of it, i.e. to LOC-0, an
ideal position for interacting with the system. For
inside-the-cone condition, subjects stand at LOC-
0 where the agent can sense their skeleton.
When user stands at LOC-1 i.e., outside-
the-cone voice + skeleton model and
voice + skeleton + azimuth models
are functionally the same since the source of
distraction has no skeleton in the cone. When
user stands at LOC-0, i.e., inside-the-cone voice
alone is the same as voice + skeleton
model since the agent always sees a skeleton in
front of it. Therefore, this variant was not used.
We treated awareness detection as a binary de-
cision. An utterance is classified either as ?in-
tended? or ?unintended?. We manually labeled the
utterances whether they were directed at the sys-
tem (?intended?), ?unintended? otherwise. Accu-
racy on ?intended? speech is reported in the Ta-
ble 1. Within each condition, the order of the ex-
periments with different awareness strategies was
randomized.
We observe that the voice + skeleton +
azimuth model proves to be robust in the pub-
lic space. Its performance is significantly better,
t(38) = 8.1, p ? 0.001, compared to the other
baselines in both conditions. This result agrees
with previous research (Haasch et al., 2004; Bohus
and Horvitz, 2009) showing that a fusion of multi-
modal features improves performance over a uni-
modal approach. Our result indicates that a sim-
ple heuristic approach, using minimal visual and
audio features, provides usable attention manage-
ment in open environments. This approach helped
the system handle a complex interaction scenario
such as out-of-cone speech directed to the sys-
tem. If the speaker is out of range but is producing
possibly system-directed utterances, system urges
them to step to the front. We believe it can be ex-
tended to other complex cases by introducing ad-
ditional logic.
3.2 End-to-End System Performance
To investigate the effect of the environment, we
compare the system?s performance in public space
and quiet room. The average noise level in the
quiet room is about 47dB(A) with computers as
65
Metric Public
Space
Quiet
Room
Success Ratio 15/20 16/20
Avg # Turns 14.2 16.4
Concept Acc 67% 68%
Table 2: Public Space vs Quiet Room Performance
the primary source of noise. The background
sound level in the public space was 46dB; other
natural sources ranged up to 57dB. The audio dis-
tractor measured 57dB. The same ASR acoustic
models and processing parameters were used in
both environments. The participant stood at LOC-
0 in Figure 3(a) during the public space experi-
ment and Figure 3(b) during the quiet room ex-
periment. In both experiments, LOC-0 is 1.5m
away from the system. We used the voice +
skeleton + azimuth model to discriminate
user speech from distractions in the environment.
We gave each participant a randomized series
of message-sending tasks, e.g. ?send a message
to ?person? who is in room ?number??. Subjects
had a maximum of 3 minutes to complete; each
task required 7 turns. The number of tasks com-
pleted (over the group) is reported in terms of
task ?success-ratio?. Table 2 shows the success-
ratio of the task, the average number of turns
needed to complete the task, and the system?s per-
utterance concept accuracy (Boros et al., 1996).
There were no statistically significant differences
between quiet room and public space, (t(38) <
2, p > 0.5, on any metric). We conclude that
the channel maintenance technique we tested was
equally effective in both environments.
4 Related Work
The problem of deploying social agents in public
spaces has been of enduring interest; (Bohus and
Horvitz, 2010) list engagement as a challenge for
a physically situated agent in open-world interac-
tions. But the problem was noted earlier and solu-
tions were proposed; e.g a ?push-to-talk? protocol
to signal the onset of intended user speech (Stent
et al., 1999). (Sharp et al., 1997; Hieronymus et
al., 2006) described the use of attention phrase as
a required prefix to each user input. Although ex-
plicit actions are effective, they need to be learned
by users. This may not be practical for systems in
public areas engaged by casual users.
A more robust approach involves fusing sev-
eral sources of information such as audio, gaze
and pose(Horvitz et al., 2003; Bohus and Horvitz,
2009) (Hosoya et al., 2009; Nakano and Ishii,
2010). Previous works have shown that fusion
of different sensory information can improve at-
tention management. The drawback of such ap-
proaches is in the complexity of the sensor equip-
ment. Our work attempts to create the rele-
vant capabilities using a simple sensing device
and relying on explicitly modeled conversational
strategies. Others are also using the Microsoft
Kinect device for research in dialog. For example,
(Skantze and Al Moubayed, 2012) and (Foster et
al., 2012) presented a multiparty interaction sys-
tems that use Kinect for face tracking and skeleton
tracking combined with speech recognition.
In our current work, we show that situational
awareness can be integrated into an existing dia-
log framework, Ravenclaw?Olympus, that was not
originally designed with this functionality in mind.
The source code of the framework presented in
this work is publicly available for download
1
and
the acoustic models that have been adapted to the
Kinect audio channel
2
5 Conclusion
We found that a conventional spoken dialog sys-
tem can be adapted to a public space with mini-
mal modifications to accommodate additional in-
formation sources. Investigating the effectiveness
of different awareness strategies, we found that a
simple heuristic approach that uses a combination
of sensory streams viz., voice, skeleton and az-
imuth, can reliably identify the likely interlocutor.
End-to-end system performance in a public space
is similar to that observed in a quiet room, indi-
cating that, at least under the conditions we cre-
ated, usable performance can be achieved. This
is a useful finding. We believe that on this level,
channel maintenance is a matter of articulating a
model that specifies appropriate behavior in dif-
ferent states defined by a small number of dis-
crete features (presence, absence, coincidence).
We conjecture that such a framework is likely to
be extensible to more complex situations, for ex-
ample ones involving multiple humans in the en-
vironment.
1
http://trac.speech.cs.cmu.edu/repos/
olympus/tags/KinectOly2.0/
2
http://trac.speech.cs.cmu.edu/repos/
olympus/tags/KinectOly2.0/Resources/
DecoderConfig/AcousticModels/Semi_
Kinect.cd_semi_5000/
66
References
[Al Moubayed and Skantze2011] S. Al Moubayed and
G. Skantze. 2011. Turn-taking control using gaze in
multiparty human-computer dialogue: Effects of 2d and
3d displays. In Proceedings of AVSP, Florence, Italy,
pages 99?102.
[Bohus and Horvitz2009] D. Bohus and E. Horvitz. 2009.
Dialog in the open world: platform and applications. In
Proceedings of the 2009 international conference on Mul-
timodal interfaces, pages 31?38. ACM.
[Bohus and Horvitz2010] D. Bohus and E. Horvitz. 2010. On
the challenges and opportunities of physically situated dia-
log. In 2010 AAAI Fall Symposium on Dialog with Robots.
AAAI.
[Bohus et al.2007] D. Bohus, A. Raux, T.K. Harris, M. Eske-
nazi, and A.I. Rudnicky. 2007. Olympus: an open-source
framework for conversational spoken language interface
research. In Proceedings of the workshop on bridging the
gap: Academic and industrial research in dialog technolo-
gies, pages 32?39. Association for Computational Lin-
guistics.
[Boros et al.1996] M. Boros, W. Eckert, F. Gallwitz, G. Gorz,
G. Hanrieder, and H. Niemann. 1996. Towards under-
standing spontaneous speech: Word accuracy vs. concept
accuracy. In Spoken Language, 1996. ICSLP 96. Pro-
ceedings., Fourth International Conference on, volume 2,
pages 1009?1012. IEEE.
[Bruce et al.2002] A. Bruce, I. Nourbakhsh, and R. Simmons.
2002. The role of expressiveness and attention in human-
robot interaction. In Proceedings of 2002 IEEE Interna-
tional Conference on Robotics and Automation, volume 4,
pages 4138?4142. IEEE.
[Foster et al.2012] M.E. Foster, A. Gaschler, M. Giuliani,
A. Isard, M. Pateraki, and R.P.A. Petrick. 2012. ?two
people walk into a bar?: Dynamic multi-party social inter-
action with a robot agent. In Proc. of the 14th ACM Inter-
national Conference on Multimodal Interaction ICMI.
[Fukuda et al.2002] T. Fukuda, J. Taguri, F. Arai,
M. Nakashima, D. Tachibana, and Y. Hasegawa.
2002. Facial expression of robot face for human-robot
mutual communication. In Proceedings of 2002 IEEE
International Conference on Robotics and Automation,
volume 1, pages 46?51. IEEE.
[Graff et al.2001] D. Graff, K. Walker, and D. Miller. 2001.
Switchboard cellular part 1 transcribed audio. In Linguis-
tic Data Consortium, Philadelphia.
[Haasch et al.2004] A. Haasch, S. Hohenner, S. H?uwel,
M. Kleinehagenbrock, S. Lang, I. Toptsis, GA Fink,
J. Fritsch, B. Wrede, and G. Sagerer. 2004. Biron?the
bielefeld robot companion. In Proc. Int. Workshop on Ad-
vances in Service Robotics, pages 27?32. Stuttgart, Ger-
many: Fraunhofer IRB Verlag.
[Hieronymus et al.2006] J. Hieronymus, G. Aist, and
J. Dowding. 2006. Open microphone speech under-
standing: correct discrimination of in domain speech. In
Proceedings of 2006 IEEE international conference on
acoustics, speech, and signal processing, volume 1. IEEE.
[Hofmann et al.2013] H. Hofmann, U. Ehrlich, A. Berton,
A. Mahr, R. Math, and C. M?uller. 2013. Evaluation of
speech dialog strategies for internet applications in the car.
In Proceedings of the SIGDIAL 2013 Conference, pages
233?241, Metz, France, August. Association for Compu-
tational Linguistics.
[Horvitz et al.2003] E. Horvitz, C. Kadie, T. Paek, and
D. Hovel. 2003. Models of attention in computing and
communication: from principles to application. In Com-
munications of the ACM, volume 46, pages 52?59.
[Hosoya et al.2009] K. Hosoya, T. Ogawa, and T. Kobayashi.
2009. Robot auditory system using head-mounted square
microphone array. In Intelligent Robots and Systems,
2009. IROS 2009. IEEE/RSJ International Conference on,
pages 2736?2741. IEEE.
[Kollar et al.2012] T. Kollar, A. Vedantham, C. Sobel,
C. Chang, V. Perera, and M. Veloso. 2012. A multi-modal
approach for natural human-robot interaction. In Proceed-
ings of 2012 International Conference on Social Robots.
[Kun et al.2007] A. Kun, T. Paek, and Z. Medenica. 2007.
The effect of speech interface accuracy on driving perfor-
mance. In INTERSPEECH, pages 1326?1329.
[Misu et al.2013] T. Misu, A. Raux, I. Lane, J. Devassy, and
R. Gupta. 2013. Situated multi-modal dialog system in
vehicles. In Proceedings of the 6th Workshop on Eye Gaze
in Intelligent Human Machine Interaction: Gaze in Multi-
modal Interaction, pages 25?28. ACM.
[Nakano and Ishii2010] Y. Nakano and R. Ishii. 2010. Es-
timating user?s engagement from eye-gaze behaviors in
human-agent conversations. In Proceedings of the 15th
international conference on Intelligent user interfaces,
pages 139?148. ACM.
[Nakashima et al.2014] Taichi Nakashima, Kazunori Ko-
matani, and Satoshi Sato. 2014. Integration of multiple
sound source localization results for speaker identifica-
tion in multiparty dialogue system. In Natural Interaction
with Robots, Knowbots and Smartphones, pages 153?165.
Springer New York.
[Sabanovic et al.2006] S. Sabanovic, M.P. Michalowski, and
R. Simmons. 2006. Robots in the wild: Observing
human-robot social interaction outside the lab. In Ad-
vanced Motion Control, 2006. 9th IEEE International
Workshop on, pages 596?601. IEEE.
[Sharp et al.1997] R.D. Sharp, E. Bocchieri, C. Castillo,
S. Parthasarathy, C. Rath, M. Riley, and J. Rowland. 1997.
The watson speech recognition engine. In Proceedings of
1997 IEEE international conference on acoustics, speech,
and signal processing, volume 5, pages 4065?4068. IEEE.
[Skantze and Al Moubayed2012] G. Skantze and
S. Al Moubayed. 2012. Iristk: a statechart-based
toolkit for multi-party face-to-face interaction. In Proc.
of the 14th ACM International Conference on Multimodal
Interaction ICMI.
[Stent et al.1999] A. Stent, J. Dowding, J. Gawron, E. Bratt,
and R. Moore. 1999. The commandtalk spoken dialogue
system. In Proceedings of the 37th annual meeting of the
Association for Computational Linguistics on Computa-
tional Linguistics, pages 183?190. ACL.
67
Proceedings of the SIGDIAL 2014 Conference, pages 194?198,
Philadelphia, U.S.A., 18-20 June 2014.
c?2014 Association for Computational Linguistics
Knowledge Acquisition Strategies for Goal-Oriented Dialog Systems
Aasish Pappu Alexander I. Rudnicky
School of Computer Science
Carnegie Mellon University
{aasish, air}@cs.cmu.edu
Abstract
Many goal-oriented dialog agents are ex-
pected to identify slot-value pairs in a
spoken query, then perform lookup in
a knowledge base to complete the task.
When the agent encounters unknown slot-
values, it may ask the user to repeat or re-
formulate the query. But a robust agent
can proactively seek new knowledge from
a user, to help reduce subsequent task fail-
ures. In this paper, we propose knowledge
acquisition strategies for a dialog agent
and show their effectiveness. The acquired
knowledge can be shown to subsequently
contribute to task completion.
1 Introduction
Many spoken dialog agents are designed to per-
form specific tasks in a specified domain e.g., in-
formation about public events in a city. To carry
out its task, an agent parses an input utterance, fills
in slot-value pairs, then completes the task. Some-
times, information on these slot-value pairs may
not be available in its knowledge base. In such
cases, typically the agent categorizes utterances as
non-understanding errors. Ideally the incident is
recorded and the missing knowledge is incorpo-
rated into the system with a developer?s assistance
? a slow offline process.
There are other sources of knowledge: automat-
ically crawling the web, as done by NELL [Carl-
son et al., 2010], and community knowledge
bases such as Freebase [Bollacker et al., 2008].
These approaches provide globally popular slot-
values [Araki, 2012] and high-level semantic con-
texts [Pappu and Rudnicky, 2013]. Despite their
size, these knowledge bases may not contain in-
formation about the entities in a specific target
domain. However, users in the agent?s domain
can potentially provide specific information on
slot/values that are unavailable on the web, e.g.,
regarding a recent interest/hobby of the user?s
friend. Lasecki et al. [2013] have elicited natu-
ral language dialogs from humans to build NLU
models for the agent and Bigham et al. [2010]
have elicited answers to visual questions by in-
tegrating users into the system. One observation
from this work is that both users and non-users
can impart useful knowledge to system. In this
paper we propose spoken language strategies that
allow an agent to elicit new slot-value pairs from
its own user population to extend its knowledge
base. Open-domain knowledge may be elicited
through text-based questionnaires from non-users
of the system, but in a situated interaction scenario
spoken strategies may be more effective. We ad-
dress the following research questions:
1. Can an agent elicit reliable knowledge about
its domain from users? Particularly knowl-
edge it cannot locate elsewhere (e.g., on-line
knowledge bases). Is the collective knowl-
edge of the users sufficient to allow the agent
to augment its knowledge through interactive
means?
2. What strategies elicit useful knowledge from
users? Based on previous work in com-
mon sense knowledge acquisition [Von Ahn,
2006, Singh et al., 2002, Witbrock et al.,
2003], we devise spoken language strategies
that allow the system to solicit information by
presenting concrete situations and by asking
user-centric questions.
We address these questions in the context of the
EVENTSPEAK dialog system, an agent that provides
information about seminars and talks in an aca-
demic environment. This paper is organized as
follows. In Section 2, we discuss knowledge ac-
quisition strategies. In Section 3, we describe a
user study on these strategies. Then, we present
an evaluation on system acquired knowledge and
finally we make concluding remarks.
194
Table 1: System initiated strategies used by the agent for knowledge acquisition in the EVENTSPEAK system.
StrategyType Strategy Example Prompt
QUERYDRIVEN
QUERYEVENT I know events on campus. What do you want to know?
QUERYPERSON I know some of the researchers on campus.Whom do you want to know about?
PERSONAL
BUZZWORDS What are some of the popular phrases in your research?
FAMOUSPEOPLE Tell me some well-known people in your research area
SHOW&ASK
TWEET How would you describe this talk in a sentence, say a tweet.
KEYWORDS Give keywords for this talk in your own words.
PEOPLE Do you know anyone who might be interested in this talk?
2 Knowledge Acquisition Strategies
We posit three different circumstances that can
trigger knowledge acquisition behavior: (1) initi-
ated by expert users of the system [Holzapfel et al.,
2008, Spexard et al., 2006, L?utkebohle et al., 2009,
Rudnicky et al., 2010], (2) triggered by ?misun-
derstanding? of the user?s input [Chung et al.,
2003, Filisko and Seneff, 2005, Prasad et al., 2012,
Pappu et al., 2014], or (3) triggered by the system.
They are described below:
QUERYDRIVEN. The system prompts a user
with an open-ended question akin to ?how-may-I-
help-you? to learn what ?values? of a slot are of
interest to the user. This strategy does not ground
user about system?s knowledge limitations. How-
ever, it allows the system to acquire information
(slot-value pairs) from user?s input. The system
can choose to respond to the input or ignore the
input depending on its knowledge about the slot-
value pairs in the input. Table 1 shows strategies
of this kind i.e., QUERYEVENT and QUERYPERSON.
PERSONAL. The system asks a user about their
own interests and people who may share those in-
terests. This is an open-ended request as well, but
the system expects the response to be confined to
the user?s knowledge about specific entities in the
environment. BUZZWORDS and FAMOUSPEOPLE ex-
pects the user to provide values for the slots.
SHOW&ASK. The system provides a descrip-
tion of an event and asks questions to ground
user?s responses in relation to that event. E.g.,
given the title and abstract of a technical talk,
the system asks the user questions about the talk.
TWEET strategy is expected to elicit a concise de-
scription of the event, which eventually may help
the agent to both summarize events for other users
and identify keywords for an event. KEYWORDS
strategy expects the user to explicitly supply key-
words for an event. PEOPLE strategy expects the
user to provide names of likely event participants.
We hypothesized that these strategies may allow
the agent to learn new slot-value pairs that may
help towards better task performance.
3 Knowledge Acquisition Study
We conducted a user study to determine reliability
of the information acquired by the system. We per-
formed this study using the EVENTSPEAK
1
dialog
system, which provides information about upcom-
ing talks and other events that might be of inter-
est, and about ongoing research on campus. The
system presents material on a screen and accepts
spoken input, in a context similar to a kiosk.
The study evaluated performance of the seven
strategies described above. For SHOW&ASK strate-
gies, we had users respond regarding a specific
event. We used descriptions of research talks col-
lected from the university?s website. We used a
web-based interface for data collection; the inter-
face presented the prompt material and recorded
the subject?s voice response. Testvox
2
was used
to setup the experiments and Wami
3
for audio
recording.
3.1 User Study Design
We recruited 40 researchers (graduate students)
from the School of Computer Science, at Carnegie
Mellon, representative of the user population for
the EVENTSPEAK dialog system. Each subject re-
sponded to prompts from the QUERYDRIVEN, PER-
SONAL and SHOW&ASK strategies.
In the QUERYDRIVEN tasks, the QUERYEVENT
strategy, the system responds to the user?s query
with a list of talks. The user?s response is
recorded, then sent to an open-vocabulary speech
recognizer; the result is used as a query to a
database of talks. The results are then displayed on
the screen. The system applies the QUERYPERSON
strategy in a similar way. In the PERSONAL tasks,
the system applies the BUZZWORDS strategy to ask
the user about popular keyphrases in their research
1
http://www.speech.cs.cmu.edu/apappu/kacq
2
https://bitbucket.org/happyalu/testvox/wiki/Home
3
https://code.google.com/p/wami-recorder/
195
Figure 1: Time per Task for all strategies
Q
u
e
r
y
E
v
e
n
t
Q
u
e
r
y
P
e
r
s
o
n
B
u
z
z
w
o
r
d
s
F
a
m
o
u
s
P
e
o
p
l
e
T
w
e
e
t
P
e
o
p
l
e
K
e
y
w
o
r
d
s
0
1
2
3
4
1.51
2.23
0.91
0.71
2.51
0.69
0.97
T
i
m
e
i
n
m
i
n
u
t
e
s
Figure 2: Time per Task vs Expertise
E
x
p
e
r
t
L
e
v
e
l
1
E
x
p
e
r
t
L
e
v
e
l
2
E
x
p
e
r
t
L
e
v
e
l
3
E
x
p
e
r
t
L
e
v
e
l
4
0
1
2
3
4
T
i
m
e
i
n
m
i
n
u
t
e
s
tweet
people
keywords
area. The system then asks about well-known re-
searchers (FAMOUSPEOPLE) in the user?s area.
In the SHOW&ASK tasks, we use two seminar
descriptions per subject (in our pilot study, we
found that people provide more diverse responses
(in term of entities) in the SHOW&ASK based on
the event abstract, compared to PERSONAL, QUERY-
DRIVEN). We used a set of 80 research talk an-
nouncements (consisting of a title, abstract and
other information). For each talk, the system used
all three strategies viz., TWEET, KEYWORDS and PEO-
PLE. For the TWEET tasks, subjects were asked to
provide a one sentence description. They were al-
lowed to give a non-technical/high-level descrip-
tion if they were unfamiliar with the topic. For
the PEOPLE task, subjects had to give names of col-
leagues who might be interested in the talk. For
the KEYWORDS task, subjects provided keywords,
either their own words or ones selected from the
abstract.
Since the material is highly technical, we were
interested whether the tasks are cognitively de-
manding for people who are less familiar with the
subject of a talk. Therefore, users were asked to
indicate their familiarity with a particular talk (re-
search area in general) using a scale of 1?4: 4 be-
ing more familiar and 1 being less familiar.
3.2 Corpus Description
This user study produced 64 minutes of audio data,
on average 1.6 minutes per subject. We tran-
scribed the speech then annotated the corpus for
people names, and for research interests. Table 2
shows the number of unique slot-values found in
the corpus. We observe that the number of unique
research interests produced during SHOW&ASK is
higher than for other strategies. This confirms
our initial observations that this strategy elicits
diverse responses. The PERSONAL task produced
a relatively higher number of researcher names
(FAMOUSPEOPLE strategy) than other tasks. One ex-
planation might be that people may find it easier
to recall names in their own research area, as com-
pared to other areas. Overall, we identified 139
unique researcher names and 485 interests.
Table 2: Corpus Statistics
StrategyType
Unique
Researcher
Names
Unique
Research
Interests
QUERYDRIVEN 21 30
PERSONAL 77 107
SHOW&ASK 76 390
Overall 139 485
3.3 Corpus Analysis
One of the objectives of this work is to determine
What strategies can the agent use to elicit knowl-
edge from users? Although, time-cost will vary
with task and domain, a usable strategy should, in
general, be less demanding. We analyzed the time-
per-task for each strategy, shown in Figure 1. We
found that the TWEET strategy is not only more de-
manding, it has higher variance than other tasks.
One explanation is that people would attempt to
summarize the entire abstract including technical
details, despite the instructions indicated that a
non-technical description was acceptable. We can
see a similar trend in Figure 2 that irrespective
of expertise-level, subjects take more time to give
one sentence descriptions. We also observe high
variance and higher time-per-task for QUERYPER-
SON; this is due to the system deliberately not re-
turning any results for this task. This was done to
196
Table 3: Mean Precision for 200 researchers, broken down by the ?source? strategy used to acquire their name
Note: Only 85 of 200 researchers had Google Scholar pages, GScholar Accuracy is computed for only those 85.
Metric Description Text SHOW&ASK PERSONAL QUERYDRIVEN mean
Mean Precision 89.5% 86.9% 93.6% 86.2% 90.5%
GScholar Acc. 78.3% 82.3% 86.1% 100% 80.0%
find out whether subjects would repeat the task on
failure. Ideally the system needs to only rarely use
this strategy to not lose user?s trust and solicit mul-
tiple values for a given slot (e.g., person name) as
opposed to requesting list of values as in FAMOUS-
PEOPLE and PEOPLE strategies. We find that PEOPLE,
KEYWORDS, FAMOUSPEOPLE and BUZZWORDS strate-
gies are efficient with a time-per-task of less than
one minute. As shown in Figure 2, subjects do not
take much time to speak a list of names or key-
words.
4 Evaluation of Acquired Knowledge
To answer Can an agent elicit reliable knowl-
edge about its domain from users? we analyzed
the relevance of acquired knowledge. We have
two disjoint list of entities, (a) researchers and
(b) research interests; in addition we have speaker
names from the talk descriptions. Our goal is
to implicitly infer a list of interests for each re-
searcher without soliciting the user for the inter-
ests of every researcher exhaustively. To each re-
searcher in the list, we attribute list of interests that
were mentioned in the same context as researcher
was mentioned. We tag list of names acquired
from the FAMOUSPEOPLE strategy with list of key-
words acquired from the BUZZWORDS strategy ?
both lists acquired from same user. We repeat this
process for each name mentioned in relation to a
talk in the SHOW&ASK strategy. We tag keywords
mentioned in the KEYWORDS strategy to researchers
mentioned in the PEOPLE strategy.
4.1 Analysis
We produced 200 entries for researchers and their
set of interests. We then had two annotators (se-
nior graduate students) mark whether the system-
predicted interests were relevant/accurate. The an-
notators were allowed to use information found on
researchers? home pages and Google Scholar
4
to
evaluate the system-predicted interests.
This can be seen as an information retrieval (IR)
problem, where researcher is ?query? and interests
are ?documents?. So, we use Mean Precision, a
4
scholar.google.com
common metric in IR, to evaluate retrieval. In our
case, the ground truth for relevant interests comes
from the annotators. The results are shown in Ta-
ble 3. Our approach has high precision, 90.5%,
for all 200 researchers. We see that irrespective
of the strategy used to acquire entities, precision
is good. We also compared our predicted inter-
ests with interests listed by researchers themselves
on Google Scholar. There are only 85 researchers
from our list with a Google Scholar page; for these
our accuracy is 80%, again good. Moreover, sig-
nificant knowledge is absent from the web (at least
in our domain) yet can be elicited from users fa-
miliar with the domain.
5 Conclusion
We describe a set of knowledge acquisition strate-
gies that allow a system to solicit novel informa-
tion from users in a situated environment. To in-
vestigate the usability of these strategies, we con-
ducted a user study in the domain of research talks.
We analyzed a corpus of system-acquired knowl-
edge and have made the material available
5
. Our
data show that users on average take less than a
minute to provide new information using the pro-
posed elicitation strategies. The reliability of ac-
quired knowledge in predicting relationships be-
tween researchers and interests is quite good, with
a mean precision of 90.5%. We note that the PER-
SONAL strategy, which tries to tap personal knowl-
edge, appears to be particularly effective. More
generally, automated elicitation appears to be a
promising technique for continuous learning in
spoken dialog systems.
6 Appendix
System Predicted Researcher-Interests 1
rich stern deep neural networks, speech recog-
nition, signal processing, neural networks, machine
learning, speech synthesis
5
www.speech.cs.cmu.edu/apappu/pubdl/eventspeak corpus.zip
197
System Predicted Researcher-Interests 2
kishore prahallad dialogue systems, prosody,
speech synthesis, text to speech, pronunciation mod-
eling, low resource languages
System Predicted Researcher-Interests 3
carolyn rose crowdsourcing, meta discourse clas-
sification, statistical analysis, presentation skills in-
struction, man made system, education models, human
learning
System Predicted Researcher-Interests 4
florian metze dialogue systems, speech recogni-
tion, nlp, prosody, speech synthesis, text to speech,
pronunciation modeling, low resource languages, au-
tomatic accent identification
System Predicted Researcher-Interests 5
madhavi ganapathiraju protein structure, contin-
uous graphical models, generative models, structural
biology, protein structure dynamics, molecular dy-
namics
System Predicted Researcher-Interests 6
alexander hauptmann discriminatively trained
models, deep learning, computer vision, big data
System Predicted Researcher-Interests 7
jamie callan learning to rank, search, large scale
search, web search, click prediction, information re-
trieval, web mining, user activity, recommendation,
relevance, machine learning, web crawling, distributed
systems, structural similarity
System Predicted Researcher-Interests 8
lori levin natural language understanding, knowl-
edge reasoning, construction grammar, knowledge
bases, natural language processing
References
Masahiro Araki. Rapid development process of spoken dia-
logue systems using collaboratively constructed semantic
resources. In Proceedings of the SIGDIAL 2012 Confer-
ence, pages 70?73. ACL, 2012.
Jeffrey P Bigham, Chandrika Jayant, Hanjie Ji, Greg Little,
Andrew Miller, Robert C Miller, Robin Miller, Aubrey
Tatarowicz, Brandyn White, Samual White, et al. Vizwiz:
nearly real-time answers to visual questions. In Proceed-
ings of the 23rd ACM Symposium on User Interface soft-
ware and technology, pages 333?342. ACM, 2010.
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge,
and Jamie Taylor. Freebase: a collaboratively created
graph database for structuring human knowledge. Pro-
ceedings of the SIGMOD, pages 1247?1249, 2008.
Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr Set-
tles, Estevam R Hruschka Jr., and Tom M Mitchell. To-
ward an Architecture for Never-Ending Language Learn-
ing. Artificial Intelligence, 2(4):1306?1313, 2010.
Grace Chung, Stephanie Seneff, and Chao Wang. Automatic
acquisition of names using speak and spell mode in spo-
ken dialogue systems. In Proceedings of the NAACL-HLT,
pages 32?39. ACL, 2003.
Edward Filisko and Stephanie Seneff. Developing city name
acquisition strategies in spoken dialogue systems via user
simulation. In 6th SIGdial Workshop on Discourse and
Dialogue, 2005.
Hartwig Holzapfel, Daniel Neubig, and Alex Waibel. A dia-
logue approach to learning object descriptions and seman-
tic categories. Robotics and Autonomous Systems, 56(11):
1004?1013, November 2008.
Walter Stephen Lasecki, Ece Kamar, and Dan Bohus. Con-
versations in the crowd: Collecting data for task-oriented
dialog learning. In First AAAI Conference on Human
Computation and Crowdsourcing, 2013.
Ingo L?utkebohle, Julia Peltason, Lars Schillingmann,
Christof Elbrechter, Britta Wrede, Sven Wachsmuth, and
Robert Haschke. The Curious Robot: Structuring Inter-
active Robot Learning. In ICRA?09, pages 4156?4162.
IEEE, 2009.
Aasish Pappu and Alexander Rudnicky. Predicting tasks
in goal-oriented spoken dialog systems using semantic
knowledge bases. In Proceedings of the SIGDIAL, pages
242?250. ACL, 2013.
Aasish Pappu, Teruhisa Misu, and Rakesh Gupta. Investi-
gating critical speech recognition errors in spoken short
messages. In Proceedings of IWSDS, pages 39?49, 2014.
Rohit Prasad, Rohit Kumar, Sankaranarayanan Ananthakr-
ishnan, Wei Chen, Sanjika Hewavitharana, Matthew Roy,
Frederick Choi, Aaron Challenner, Enoch Kan, Arvind
Neelakantan, et al. Active error detection and resolu-
tion for speech-to-speech translation. In Proceedings of
IWSLT, 2012.
Alexander I Rudnicky, Aasish Pappu, Peng Li, and Matthew
Marge. Instruction Taking in the TeamTalk System. In
Proceedings of the AAAI Fall Symposium on Dialog with
Robots, pages 173?174, 2010.
Push Singh, Thomas Lin, Erik T Mueller, Grace Lim, Trav-
ell Perkins, and Wan Li Zhu. Open mind common
sense: Knowledge acquisition from the general public. In
CoopIS, DOA, and ODBASE, pages 1223?1237. Springer,
2002.
Thorsten Spexard, Shuyin Li, Britta Wrede, Jannik Fritsch,
Gerhard Sagerer, Olaf Booij, Zoran Zivkovic, Bas Ter-
wijn, and Ben Krose. BIRON, where are you? Enabling
a robot to learn new places in a real home environment by
integrating spoken dialog and visual localization. Integra-
tion The VLSI Journal, (section II):934?940, 2006.
Luis Von Ahn. Games with a purpose. Computer, 39(6):
92?94, 2006.
Michael Witbrock, David Baxter, Jon Curtis, Dave Schneider,
Robert Kahlert, Pierluigi Miraglia, Peter Wagner, Kathy
Panton, Gavin Matthews, and Amanda Vizedom. An inter-
active dialogue system for knowledge acquisition in cyc.
In Proceedings of the 18th IJCAI, pages 138?145, 2003.
198
