Squibs and Discussions 
On Coreferring: Coreference in MUC and 
Related Annotation Schemes 
Kees  van  Deemter*  
University of Brighton 
Rodger Kibble* 
Goldsmiths College 
In this paper, it is argued that "coreference" annotations, as performed in the MUC community 
for example, go well beyond annotation of the relation of coreference proper. As a result, it is not 
always clear what semantic relation these annotations are encoding. The paper discusses a number 
of problems with these annotations and concludes that rethinking of the coreference task is needed 
before the task is expanded. In particular, it suggests a division of labor whereby annotation of the 
coreference r lation proper is separated from other tasks such as annotation of bound anaphora 
and of the relation between asubject and a predicative NP. 
1. Introduction: Coreference Annotation 
Various practical tasks requiring language technology including, for example, infor- 
mation extraction and text summarization, can be performed more reliably if it is 
possible to automatically find parts of the text containing information about a given 
topic. For example, if a text summarizer has to select the most important informa- 
tion, in a given text, about the 1984 Wall Street crash, then the summarization task 
is greatly helped if a program can automatically spot all the clauses in the text that 
contain information about this crash. To evaluate a program of this kind, extensive 
language corpora have been prepared in which human readers have annotated what 
has been called the coreference relation. These annotated corpora are then used as a 
gold standard against which the program's achievements can be compared. The re- 
lation of coreference has been defined as holding between two noun phrases if they 
"refer to the same entity" (Hirschman et al 1997). More precisely, let us assume that 
al and a2 are occurrences of noun phrases (NPs) and let us assume that both have 
a unique referent in the context in which they occur (i.e., their context in the corpus 
makes them unambiguous). Under these assumptions we can use a functional nota- 
tion, e.g. Referent(a), as short for "the entity referred to by a"  and define (suppressing 
the role of context): 
Definition 
al and a2 corefer if and only if Referent(a1) =Referent(a2). 
Putting it simply: to determine whether al and a2 corefer, first determine Referent(a1) 
and Referent(a2), then see if they are equal. 
Ideally, of course, one would like to annotate many other semantic relations that 
hold between parts of a text, because they are also relevant for text interpretation. One 
candidate is the relation of anaphora. Loosely speaking--and glossing over some dif- 
ficulties regarding the precise delimitation of anaphora (Sidner 1979; Partee 1989; van 
* Information Technology Research Institute, University of Brighton, Lewes Road, Brighton BN2 4GJ, 
UK. E-mail: Kees.van.Deemter@itri.bton.ac.uk 
t Mathematical and Computing Science, Goldsmiths College, University of London, London SE14 6NW, 
UK. E-mail: R.Kibble@gold.ac.uk 
(~) 2001 Association for Computational Linguistics 
Computational Linguistics Volume 26, Number 4 
Deemter 1992)--an NP O~ 1 is said to take an NP a2 as its anaphoric antecedent if and 
only if al depends on Oz2 for its interpretation (e.g., Kamp and Reyle 1993). It follows 
that anaphora nd coreference are different hings. Coreference, for example, is an 
equivalence r lation; anaphora, by contrast, is irreflexive, nonsymmetrical, nd non- 
transitive. Secondly, anaphora, as it has just been defined, implies context-sensitivity 
of interpretation, and this is not true for coreference. For example, a name (President 
W. I. Clinton) and a description (Hillary Rodham's husband) can corefer without either 
of the two depending on the other for its interpretation. Anaphoric and coreferential 
relations can coincide, of course, but not all coreferential relations are anaphoric, nor 
are all anaphoric relations coreferential. (An example of the latter is bound anaphora, 
see Section 2.1.) 
Coreference annotation has been a focus of the Sixth and Seventh Message Under- 
standing Conferences (MUC-6, MUC-7) and various other annotation exercises (e.g., 
Passoneau 1997; Garside, Leech, and McEnery 1997; Davies et al 1998; Poesio 2000). In 
this squib, we intend to point at some fundamental problems with many of these an- 
notation exercises, which are caused by a failure to distinguish properly between coref- 
erence, anaphora, and other, related phenomena. Because the MUC project is the best- 
known example of coreference annotation, on which much subsequent work is based, 
and because of the public availability of the MUC Task Definition (TD, Hirschman 
and Chinchor \[1997\]), we will focus on MUC. 
Four criteria are listed for the MUC TD, in order of their priority (Hirschman and 
Chinchor 1997): 
. 
2. 
3. 
4. 
The MUC information extraction tasks should be supported by the 
annotations 
Good (defined as ca. 95%) interannotator agreement should be achievable 
It should be possible to annotate texts quickly and cheaply 
A corpus should be created that can be used as a tool for linguists 
outside MUC. 
The TD makes it clear that the annotation task has been simplified in a number of 
ways. For example, only NPs were annotated. Such eminently sensible simplifica- 
tions notwithstanding, wewill argue that the above-mentioned criteria have not been 
achieved and that a rethinking of the coreference annotation enterprise is in order be- 
fore it ventures into new domains involving speech, noisy data, etc. (see for example, 
Bagga, Baldwin, and Shelton \[1999\]), or before it extends the relation of coreference 
to cover whole/part and class/instance r lations (e.g. Popescu-Belis 1998; Hirschman 
and Chinchor 1997). 
2. Problems with Coreference Annotation 
In this section, some unclarities and inconsistencies will be discussed that we found 
in the literature on coreference annotation, and which appear to stem from confusion 
about what reference and coreference are. In Section 2.1, we will explore the tendency 
to apply coreference annotation to nonreferring NPs and bound anaphora, and we 
will argue that this tendency is problematic. In Section 2.2, we will argue that existing 
annotation enterprises still fail to respond properly to the well-known problem of how 
to annotate NPs that are used intensionally. In Section 2.3, we turn to a suggestion 
for the improvement of the actual process of annotation that has been made in the 
630 
van Deemter and Kibble On Coreferring 
literature, namely to separate the task of determining the "markables" from that of 
establishing coreference r lations between them, showing that this separation is hard 
to maintain. At the end of each subsection, some suggestions (Remedies) will be made 
on how the problems may be tackled. These suggestions will be elaborated in Section 3. 
2.1 Annotating Nonreferring NPs and Bound Anaphora 
The notion of reference is common to a broad variety of semantic theories (see Gamut 
\[1991\], Chapter 1, for discussion). When speakers/writers use an NP to refer to an 
object or a set of objects, they try to single out the entity uniquely. Thus, when someone 
utters the NP the tenant of the house, the speaker may aim to single out a unique person, 
say Mr. X. Even when this is the case (i.e., the NP is used referentially rather than 
attributively1), the notion of referring has its problems. For example, the speaker may 
be mistaken in her belief that Mr. X is the tenant of the house (Donnellan 1966). In 
such cases it is unclear who is being referred to. Such problems notwithstanding, work 
on coreference annotation has usually taken the notion of reference for granted, on the 
assumption that clear cases, where the referent of an NP is clearly defined, outnumber 
the problematic ones, at least in some important ypes of discourse. 
Let us, for now, buy into the assumption that reference is a straightforward notion. 
Then, following Bach (1987) (especially Sections 3.2 and 12.2), for example, one thing 
that is clear about reference is that some NPs do not refer. When someone says 
(1) a. No solution emerged from our discussions, or 
b. Whenever a solution emerged, we embraced it, 
the solution NPs do not refer to any single solution, nor to any definite set of solutions. 
Most theorists would agree that they do not have a referent. Nonreferring NPs can 
enter anaphoric relations. (For example, the NP a solution is the (bound) anaphoric 
antecedent to it in (lb).) But if they do not refer, the c0reference r lation as defined in 
Section i (which presupposes that Referent(olD and Referent(o~2) are defined) is not ap- 
plicable to them. Even so, the MUC TD asks annotators to treat them as if it was appli- 
cable. It acknowledges (page 10) that "one may argue that \[a bound anaphor and its an- 
tecedent\] are not coreferential in the usual sense," but falls short of explaining explicitly 
what types of anaphora re to be annotated and how (Hirschman and Chinchor 1997). 2
The annotation of bound anaphora merits some further elaboration. Consider, for 
example, quantifying NPs such as Every TV network (or, even more problematic, Most 
computational linguists \[Hirschman and Chinchor 1997\], see also Section 3). If Every TV 
network refers at all, then presumably it refers to the set of all TV networks (relevant to a 
certain domain). The TD, however, asks annotators to let Every TV network corefer with 
its in (lc). According to the definition of coreference, this means that Referent(Every 
TV network) = Referent(its) so that Referent(its) is the set of all TV networks, predicting 
incorrectly that (lc) means (ld): 
(1) c. Every TV network reported its proyqts 
c p. Every TV network reported every TV network's proyqts, 
1 See Donnellan (1966). For an interesting class of attributively used NPs, see van der Sandt (1992); 
examples include hypotheticals like If this house has a tenant then the tenant isprobably Dutch, where one 
might ask whether atenant and the tenant corefer. 
2 Sometimes the term "cospecification" has been employed to replace coreference (e.g. Sidner 1983; 
Davies et al 1998). It is unclear, however, whether abound anaphor and its antecedent cospecify, or
how the notion should be applied to intensional constructions (Section 2.2). 
631 
Computational Linguistics Volume 26, Number 4 
(Incidentally, coreference and anaphora re not only different; they are also extremely 
difficult o combine into a proper equivalence r lationship that allows us to recognize 
different clauses as being "about he same thing." Consider, for example, the relation, 
say R, which holds between NP1 and NP2 if and only if 
either NP1 and NP2 corefer (in the sense of the definition in Section 1) 
or NP1 is an anaphoric antecedent of NP2 
or NP2 is an anaphoric antecedent of NP1 
Note that R is not an equivalence r lation. The subject of (lc), for example, can corefer 
with a plural pronoun in the next sentence, .g., (...) They are now required to do this, 
but They and it (in (lc)) do not stand in the relation R.) 
Predicative NPs are another category of NPs whose referentiality is problematic 
and yet the MUC TD instructs annotators to let them corefer with other NPs. In (2a) 
and (2b), for example, the predicative NP the/a president off DD cannot be replaced 
by the proper name Higgins without changing the meaning of the sentence beyond 
recognition, indicating that the relation between the two NPs must be something other 
than coreference: 
(2) a. Higgins was~became th /a president of DD 
b. Higgins, once the president of DD, is now a humble university lecturer 
We will have more to say about predicative NPs in the next section. 
To sum up, MUC's annotators have been instructed to let NPs of all major classes 
(definite, quantificational, nd indefinite) "corefer" liberally with other NPs, even when 
it is far from clear that the NPs in question have been used referentially. As a result, 
the relation actually annotated in MUC--henceforth called the IDENT relation, fol- 
lowing Hirschman and Chinchor (1997)--must be distinguished from the coreference 
relation. The TD admits that certain instructions may be incompatible with the defini- 
tion of coreference but no reason is given for these incompatibilities and no intuitive 
motivation for the relation IDENT is offered. As a result, the annotator is left with a 
long series of instructions that fail to be held together by a common rationale. 
Remedy. Go back to basics: start from a definition of coreference and write a TD that 
implements he definition. We suggest that it is not until this has been done success- 
fully that extensions into the area of bound anaphora become a risk worth taking. 
2.2 Problems of Intensionality and Predication 
Problems posed to coreference annotation by intensionality (Hirschman et al 1997) 
have motivated considerable complications in the TD. Consider Section 6.4, which 
discusses the implications of "change over time." The TD says that "two markables 
should be recorded as coreferential if the text asserts them to be coreferential tANY 
TIME" (Hirschman and Chinchor 1997, page 11). Thus, for example, the TD points out 
that in a case like 
(3) Henry Higgins, who was formerly sales director off Sudsy Soaps, became president 
of Dreamy Detergents 
annotators are expected to mark (1) Henry Higgins, (2) sales director of Sudsy Soaps, 
and (3) president of Dreamy Detergents as coreferential. (Similar strategies seem to be 
632 
van Deemter and Kibble On Coreferring 
adopted by most practitioners of coreference annotation, e.g., Cristea et al \[1999\]). But 
since coreference is generally agreed to be a equivalence relation (e.g. Hirschman and 
Chinchor 1997, Section 1.3), this implies that the sales director of Sudsy Soaps and the 
president of Dreamy Detergents are the same person. Clearly, this cannot be right. 
Luckily, there are other parts of the same TD that do a better job of applying the 
notion of coreference to sentences involving change over time. Consider, for example, 
Section 1.3, where the sentence the stock price fell from $4.02 to $3.85 is discussed. Here 
annotators are asked to consider the stock price as standing in the IDENT relation with 
$3.85 but not with $4.02, because $3.85 is "the more recent value" (p. 3). (If both 
coreferred with the stock price, it would have followed that $4.02 and $3.85 are equal.) 
This solution, however, is still problematic. What, for instance, if the price continues 
to fall? 
(4) a. The stock price fell from $4.02 to $3.85; 
b. Later that day, it fell to an even lower value, at $3.82. 
Does the annotator have to go back to (4a), deciding that $3.82 is an even more recent 
value and the stock price does not stand in the IDENT relation with $3.85 after all? 
Remedy. At least three different strategies are conceivable. Perhaps most obviously, 
one might decide that coreference between a functional description like those in (3) or 
(4) and an NP denoting a value requires this value to be the present (rather than the 
most recent) value of the function. But, the text does not always say what the present 
value is. Moreover, functional descriptions do not always pertain to the present. In 
Last year, the president resigned, for example, the subject refers to last year's president, and 
consequently, it does not corefer with NPs referring to the present president. A second 
strategy, consistent with Dowty, Wall, and Peters (1981, Appendix iii) might be to say 
that The stock price refers only to a Montague-type individual concept, that is, a function 
from times to numbers. It would follow that The stock price does not corefer with either 
$4.02 or $3.85 and no problem would arise. Analogously, president of Dreamy Detergents, 
in (3) above, where it is used predicatively, might denote an individual concept rather 
than an individual. If the next sentence goes on to say He died within a week, then he 
is coreferential with Henry Higgins; if, instead, the text proceeds This is an influential 
position, but the pay is lousy, then This is coreferential with president of Dreamy Detergents. 
If both these analyses prove to be too complex to be used in large-scale annotation 
exercises, one might have to take the point of view that such descriptions simply do 
not refer. This would amount to a third strategy, which excludes these descriptions 
from entering coreference r lations altogether and leaving their analysis to the other 
tasks. 
2.3 What's Markable? 
It has been proposed that annotation can profitably be broken down into two more 
manageable steps: annotation of markables (step 1) is to be carried out before (step 2) 
partitioning the set of markables into equivalence classes of coreferring elements (e.g., 
Hirschman and Chinchor 1997). It turns out, however, that a strict distinction between 
the two steps is difficult to maintain, because, in principle, almost anything is mark- 
able. In the MUC-7 TD, this is sensibly acknowledged by letting annotators mark up 
certain elements only if they corefer with an existing markable: these include conjuncts 
and prenominal modifiers. In the following example, the first occurrence of aluminum 
is only considered to be markable because it corefers with the occurrence of this noun 
633 
Computational Linguistics Volume 26, Number 4 
as a bare NP in the second clause. 
(5) The price of aluminum siding has steadily increased, as the market for 
aluminum reacts to the strike in Chile. (Hirschman and Chinchor 1997) 
In other words: coreference (step 2) helps to determine what the markables are (step 1). 
Finding all the NPs that might participate in coreference becomes even harder if the 
annotation scheme is extended to cover event coreference (noted in the "wish list" in 
Section 1.4 of the TD) since it is often extremely difficult to determine which events 
can serve as antecedents (Hirschman and Chinchor 1997): 
(6) Be careful not to get the gel in your eyes. If this happens, rinse your eyes with 
clean water and tell your doctor. (ABPI, 1997) 
Examples of this kind suggest that one markable (e.g., an event) can give rise to 
another (e.g., the negation of the event). A complication of a similarly algebraic flavor 
arises if "discontinuous elements, including conjoined elements" are covered, as when 
a plural pronoun corefers with a combination ofpreviously occurring NPs (Hirschman 
and Chinchor 1997, section 1.4; see Garside, Leech, and McEnery \[1997\] for a proposal). 
Note especially that annotators would have to be on guard for the possibility of different 
combinations of markables coreferring to each other. A corpus, for example, can easily 
contain NPs A,B,C, and D for which Referent(A) U Referent(B) = Referent(C) U 
Referent(D). Even assuming that each of A, B, C, and D has been properly identified 
as a markable during step 1, this is little guarantee that annotators of step 2 will realize 
the complex coreference r lation between the combination of A and B and that of C 
and D. (Recall that coreference r lations are to be annotated even in the absence of an 
anaphoric relationship.) The number of possible combinations of markables (some 2 n 
when there are n markables) will often be too large to handle. 
Remedy. One alternative is to have a first pass where only referring expressions that 
look like anaphors are marked up, such as pronouns and definite NPs. Subsequent 
passes would look for antecedents for these expressions and link coreferring elements. 
An intermediate approach would be to mark up a core set of referring expressions on 
the first pass, allowing for further eferring expressions to be identified on subsequent 
passes if this is necessary to resolve coreference. The extent o which each strategy 
would contribute to accuracy and speed of annotation remains to be determined. 
3. Conclus ion 
Current "coreference" annotation practice, as exemplified by MUC, has overextended 
itself, mixing elements of genuine coreference with elements of anaphora nd predica- 
tion in unclear and sometimes contradictory ways. As a result, the annotated corpus 
emerging from MUC is unlikely to be as useful for the computational linguistics re- 
search community as one might hope (Criterion 4, see Section 1), the more so because 
generalization to other domains is bound to make problems worse. In many domains, 
for example, other sources of intensionality han change over time occur prominently. 
An example is epistemic modality: 
(7) Henry Higgins might be the man you have talked to. 
634 
van Deemter and Kibble On Coreferring 
The relation between Henry Higgins and the man you have talked to is analogous to that 
between Henry Higgins and sales director of Sudsy Soaps in (3), with possible worlds 
taking the place of points in time: the two NPs refer to the same individual in some 
possible worlds only (see Groenendijk, StokhoL and Veltman \[1996\] for relevant theo- 
retical work). Modality, of course, interacts with tense (as in Henry Higgins might become 
the president of Dreamy Detergents), leading to further complications. 
The MUC TD has addressed many of the difficult problems in the area of reference 
and coreference, but if its success is judged by the criteria in Hirschman and Chinchor 
(1997) (see Introduction), the results are mixed at best. Criterion 4 has been discussed 
above. Concerning Criterion 3, it appears doubtful that the present task definition can 
be applied "quickly and cheaply." Hirschman et al (1997), when discussing this issue, 
note that interannotator agreement, at the time of writing, was in the low eighties. 
This figure, which falls markedly short of the 95% required by Criterion 2, does not 
seem to have improved substantially since (Breck Baldwin, personal communication). 
Concerning Criterion 1, finally, it has been observed that the figures for recall in the 
MUC information extraction algorithm are rather discouraging (Appelt 1998). The 
material in Section 2 suggests that this relative lack of success is no accident and that 
unclarities in the TD are to blame. Repairs are not always easy to find. Given this 
situation, we suggest that a rethinking of the coreference task is required. 
Firstly, one needs a consistent story of what reference and coreference are taken 
to be. Theoretical work on reference does not show a consensus on some crucial ques- 
tions in this area (Bach 1987; Kronfeld and Roberts 1998). Different answers have been 
suggested, each with its own advantages and disadvantages. For example, one might 
identify the notion of a referring NP with that of a semantically definite NP in the sense 
of Barwise and Cooper (1981). 3This would include proper names, extensional definite 
descriptions, universally quantified NPs, and specifically used indefinites (e.g., a com- 
pany whose name is withheld), but it would exclude nonspecifically used indefinites such 
as at least n companies, most computational linguists. A more liberal approach along the 
lines of Kamp and Reyle (1993, Chapter 4), would predict hat a quantifying NP such 
as the subject of Most computational linguists use a parser refers to the set of those com- 
putational linguists who use a parser: the VP helps to determine the referent of the NP. 
The first approach would make annotation easier to perform and the results would be 
likely to be more reliable as a result, but it would feed less information i to the infor- 
mation extraction task. Trade-offs of this kind are unavoidable, and experimentation 
will be required to determine which option provides the best results. 
Secondly, we suggest a further division of labor whereby those phenomena that 
are no longer accounted for in the new TD are covered by other tasks (Kibble and 
van Deemter 2000). For example, the two NPs Henry Higgins and president of Sudsy 
Soaps (example (3)) do not corefer, and the relation between them should be irrelevant 
to coreference annotation. If it is imperative that information about Henry's previous 
jobs be saved for posterity then some other annotation task has to be defined, with 
its own very different TD, involving the notion of individuals having properties at 
certain times or intervals only. Something analogous is true for the annotation of 
bound anaphora. 
The issue under discussion illustrates a more general point. It is now widely 
agreed that linguistic theorizing is sometimes insufficiently informed by observational 
data. Conversely, we would like to submit that corpus-based research is sometimes 
3 A semantically definite NP c~ is one whose set-theoretic denotation takes the form of a principal filter 
(Partee, ter Meulen, and Wall 1990), i.e., a set of the form {X: Y C X} for some set of individuals Y. 
635 
Computational Linguistics Volume 26, Number 4 
insufficiently informed by theory. It follows, in our opinion, that there is scope for 
more collaboration between theoretical and corpus-based linguists in this area. This 
squib attempts to be a small step in this direction. 
Acknowledgments 
The authors wish to thank Christy Doran, 
Renate Henschel, Adam Kilgarriff, Paul 
Piwek, Massimo Poesio, Richard Power, and 
four anonymous referees for their 
comments on an earlier draft of this paper. 
We are grateful to Lynette Hirschman and 
Breck Baldwin for their very constructive 
responses to a predecessor f this paper 
(van Deemter and Kibble 1999). Kibble's 
work on this paper was funded by the UK's 
EPSRC as part of the GNOME (GR/L51126) 
and RAGS (GR/L77102) projects. 
References 
ABPI. 1997. 1996-1997 ABPI Compendium of 
Patient Information Leaflets. Association of 
the British Pharmaceutical Industry. 
Appelt, Douglas. 1998. An overview of 
information extraction technology and its 
application to information retrieval. In 
Proceedings ofTWLT14, Language Technology 
in Multimedia Information Retrieval, 
pages 49-58, Twente. 
Bach, Kent. 1987. Thought and Reference. 
Clarendon Press, Oxford. 
Bagga, Amit, Breck Baldwin, and Sara 
Shelton. 1998. Coreference and its 
applications. Call for papers for workshop 
associated with the 37th Annual Meeting 
of the Association for Computational 
Linguistics, University of Maryland, 1999. 
See www.cs.duke.edu/,-~amit / acc99- 
wkshp.html. 
Barwise, Jon and Robin Cooper. 1981. 
Generalized quantifiers and natural 
language. Linguistics and Philosophy, 
4:159-219. 
Cristea, Dan, Nancy Ide, Daniel Marcu, and 
Valentin Tablan. 1999. Discourse structure 
and coreference: An empirical study. In 
Dan Cristea, Nancy Ide, and Daniel 
Marcu, editors, Proceedings ofACL'99 Ws: 
The Relation of Discourse~Dialogue Structure 
and Reference, pages 46-53. 
Davies, Sarah, Massimo Poesio, Florence 
Bruneseaux, and Laurent Romary. 1998. 
Annotating coreference in dialogues: 
Proposal for a scheme for MATE. See 
www.cogsci.ed.ac.uk/~poesio/MATE/anno- 
manual.html. 
Donnellan, Keith. 1966. Reference and 
definite descriptions. Philosophical Review, 
75:281-304. 
Dowty, David, Robert Wall, and Stanley 
Peters. 1981. Introduction to Montague 
Semantics. Kluwer, Dordrecht. 
Gamut, L. T. F. 1991. Logic, Language and 
Meaning, Volume 2. University of Chicago 
Press, Chicago. 
Garside, Roger, Geoffrey Leech, and Tony 
McEnery. 1997. Corpus Annotation. 
Longman, London. 
Groenendijk, Jeroen, Martin Stokhof, and 
Frank Veltman. 1996. Coreference and 
modality. In Shalom Lappin, editor, The 
Handbook of Contemporary Semantic Theory. 
Blackwell, Cambridge, MA, pages 
179-214. 
Hirschman, Lynette and Nancy Chinchor. 
1997. MUC-7 coreference task definition. 
In MUC-7 Proceedings. Science 
Applications International Corporation. 
See www.muc.saic.com. 
Hirschman, Lynette, Patricia Robinson, John 
Burger, and Marc Vilain. 1997. 
Automating coreference: The role of 
annotated training data. In Proceedings of
AAAI Spring Symposium on Applying 
Machine Learning to Discourse Processing. 
Kamp, Hans and Uwe Reyle. 1993. From 
Discourse to Logic. Kluwer, Dordrecht. 
Kibble, Rodger and Kees van Deemter. 2000. 
Coreference annotation: Whither? In 
Maria Gavrilidou et al, editors, 
Proceedings ofthe 2nd International 
Conference on Language Resources and 
Evaluation, pages 1,281-1,286, Athens. 
Kronfeld, Ami and Lawrence Roberts. 1998. 
Special Issue on Reference, Pragmatics and 
Cognition 6. John Benjamins, Amsterdam 
and Philadelphia. 
Partee, Barbara. 1989. Binding implicit 
variables in quantified contexts. 
Proceedings ofthe Chicago Linguistic Society, 
25:342-365. 
Partee, Barbara, Alice ter Meulen, and 
Robert Wall. 1990. Mathematical Methods in 
Linguistics. Kluwer, Dordrecht. 
Passoneau, Rebecca. 1997. Instructions for 
applying discourse reference annotation 
for multiple applications (DRAMA). 
Unpublished manuscript. 
Poesio, Massimo. 2000. Annotating a corpus 
to develop and evaluate discourse ntity 
realization algorithms: Issues and 
preliminary results. In Maria Gavrilidou 
et al, editors, Proceedings ofthe 2nd 
International Conference on Language 
636 
van Deemter and Kibble On Coreferring 
Resources and Evaluations, pages 211-218, 
Athens. 
Popescu-Belis, Andrei. 1998. How corpora 
with annotated coreference links improve 
reference resolution. In Antonio Rubio et 
al., editors, First International Conference on 
Language Resources and Evaluation, 
pages 567-572, Granada. European 
Language Resources Association. 
Sidner, Candace. 1979. Towards 
a Computational Theory ofDeJinite Anaphora 
Comprehension in English Discourse. Ph.D. 
dissertation, AI Lab, MIT, Cambridge, MA. 
Sidner, Candace. 1983. Focusing in the 
comprehension f definite anaphora. In 
Michael Brady and Robert Berwick, 
editors, Computational Models of Discourse. 
MIT Press, Cambridge, MA, 
pages 267-330. 
van Deemter, Kees. 1992. Towards a 
generalization of anaphora. Journal of 
Semantics, 9:27-51. 
van Deemter, Kees and Rodger Kibble. 1999. 
What is coreference and what should 
coreference annotation be? In Amit 
Bagga, Breck Baldwin, and Sara Shelton, 
editors, Proceedings ofACL workshop on 
Coreference and Its Applications, 
pages 90-96, Maryland. 
van der Sandt, Rob. 1992. Presupposition 
projection as anaphora resolution. Journal 
of Semantics, 9:333-37. 
637 
A Reformulation of Rule 2 of Centering 
Theory 
Rodger Kibble* 
Goldsmiths College 
The standard preference ordering on the well-known centering transitions Continue, Retain, Shift 
is argued to be unmotivated: a partial, context-dependent ordering emerges from the interaction 
between principles dubbed cohesion (maintaining the same center of attention) and salience 
(realizing the center of attention as the most prominent NP). A new formulation of Rule 2 of 
centering theory is proposed that incorporates these principles as well as a streamlined version 
of Strube and Hahn's (1999) notion 0fcheapness. It is argued that this formulation provides a 
natural way to handle "topic switches" that appear to violate the canonical preference ordering. 
1. What Is Centering? 
Centering theory (henceforth CT) is a theory of local discourse structure that models 
the interaction of referential continuity and salience of discourse ntities in the internal 
organization of a text. The main assumptions of the theory as presented by Grosz, Joshi, 
and Weinstein (1995) (GJW) and Walker, Joshi, and Prince (1998) can be summarized 
as follows: 
. 
. 
. 
For each utterance in a discourse there is precisely one entity that is the 
center of attention. 
There is a preference, formalized as Rule 2, (1) for consecutive utterances 
within a discourse segment o keep the same entity as the center of 
attention, and (2) for the entity most prominently realized in an 
utterance to be identified as the center of attention. 
The center of attention is the entity that is most likely to be 
pronominalized: this preference is formalized as Rule 1. 
These principles are more precisely explicated in Section 2. 
An entity may become prominent as a referential link between successive utter- 
ances, or it may deliberately be made prominent, for example, through a less oblique 
grammatical role or by being mentioned early in a sentence. An attraction for nat- 
ural language processing practitioners has been that CT's predictions are based on 
easily computable structural properties of utterances rather than costly content-based 
reasoning. 
Standard expositions of CT hold that there is a total preference ordering over types 
of transitions between utterances defined in terms of the tests mentioned in Point 2 
above and formulated as Rule 2 in Table 1. I argue in what follows that the standard 
* Department ofMathematical and Computing Sciences, Goldsmiths College, University of London, 
London SE14 6NW, UK. E-maih R.Kibble@gold.ac.uk 
@ 2001 Association for Computational Linguistics 
Computational Linguistics Volume 27, Number 4 
Table 1 
Centering constraints and rules (adapted from Walker, Joshi, and Prince \[1998, 
pages 3-4\]). 
Constraints 
C1. There is precisely one Cb. 
C2. Every element of Cf(Un) must be realized in U,. 
C3. Cb(U,) is the highest-ranked lement of Cf(Un-1) that is realized in Un. 
Rules 
R1. If some element of Cf(Un_l) is realized as a pronoun in U,, then so is Cb(Un). 
R2. Continue is preferred over Retain, which is preferred over Smooth Shift, 
which is preferred over Rough Shift. 
account of CT both over- and undergenerates. On the one hand, the stipulated prefer- 
ence for Retain over Smooth Shift has not been confirmed by empirical evidence, and 
cannot be naturally incorporated in standard generation architectures. On the other 
hand, there is no mechanism within the theory to predict specific ases where a Retain 
or a Shift may be preferred over a Continue transition, as in the Retain-Shift pattern 
that has been argued to signal the introduction of a new discourse topic (Brennan, 
Friedman, and Pollard 1987 \[BFP\]; Strube and Hahn 1999). I aim to overcome these 
difficulties under an analysis that gives a partial ordering of the classic transitions, 
incorporating a "streamlined" version of Strube and Hahn's notion of "cheapness" to
handle the Retain-Shift pattern. I do not claim to offer new empirical results; the aim 
is rather to consolidate existing results in a more economic and principled formulation 
of Rule 2 itself. 
Finally, given that CT addresses local rather than global coherence, we need to 
consider the question, How local is "local"? Two possible notions of local coherence 
are (1) overall coherence of a multi-utterance discourse segment (as in the original 
GJW model, which stipulates preferences for coherent sequences of transitions)--this 
has been called "not psychologically plausible from a speaker's perspective" (Brennan 
1998, page 231); (2) coherence between immediately adjacent utterances, as in the BFP 
algorithm, which replaces the original preference for sequences of transitions with a 
preference ordering on transitions. In this paper, I explore an intermediate position 
put forward by Strube and Hahn (1999), which is a preference over pairs of transitions 
or triples of utterances, which may or may not cross segment boundaries. 
2. Transition Rules 
The main claims of CT are formalized in terms of Cb, the backward-looking center; 
Cf, a list of forward-looking centers for each utterance Un; and Cp or preferred center, 
the most salient candidate for subsequent utterances. Cf(Un) is a partial ordering on 
the entities mentioned (or "realized") in Un, ranked by grammatical role; for example, 
SUBJ > DIR-OBJ > INDIR-OBJ > COMP(S) > ADJUNCT(S). Cb(Un) is defined as the 
highest-ranked member of Cf(Un-1) that is realized in U,. Cp(U,) is the highest-ranked 
member of Cf(Un), and is predicted to be Cb(Un+l). 
The ranking of Cf by grammatical role has been widely adopted in the litera- 
ture following BFE though it is questioned by some researchers including Strube and 
580 
Kibble Rule 2 of Centering Theory 
Table 2 
Centering transitions (Walker, Joshi, and Prince 1998, page 6). 
Cb(Un) = Cb(Un_l) Cb(Un) 76 Cb(Un_l) 
or Cb(Un-1) undefined 
Cb(Un) = Cp(Un) Continue Smooth Shift 
Cb(Un) # Cp(Un) Retain Rough Shift 
Hahn (1999), who propose a ranking based on "functional information structure," a 
combination of degrees of "givenness" and left-to-right order. They note that the BFP 
ranking is not appropriate for German, which they say is a free-word-order language 
(page 310); more accurately, relative order of NPs within a clause is not determined 
by grammatical role to the extent hat it is in English. For the purposes of this pa- 
per, there is no need to commit to either BFP's or Strube and Hahn's rankings, or 
to go into the details of the latter's "functional centering" model, as both make the 
same predictions for the examples considered. Strube and Hahn themselves (page 334) 
state that the grammatical nd functional analyses achieved consistent results for all 
examples in GJW. I adopt the ranking by grammatical role for purposes of exposi- 
tion. 
2.1 "Salience" and "Cohesion" 
Transitions are defined in terms of two tests: whether the Cb stays the same (Cb(Un) = 
Cb(U,~-I)), and whether the Cb is realized as the most prominent NP (grammatically 
or otherwise): Cb(Un) = Cp(Un). For the sake of convenience and concision, I refer to 
the first of these tests as cohesion and the second as salience; it is important to keep 
in mind that I use the terms in these defined and limited ways, and to disregard 
(for the time being) other uses of the terms in the literature. There are four possible 
combinations, which are displayed in Table 2, resulting in the named transitions Con- 
tinue, Retain, Smooth Shift, and Rough Shift. The optimal case, where both salience 
and cohesion obtain, is Continue; the least preferred is Rough Shift. Walker, Joshi, 
and Prince (1998), following BFP, stipulate that Retain is preferred over Smooth Shift, 
which implies that cohesion is a stronger requirement than salience. However, corpus 
analyses reported by di Eugenio (1998, page 127), Hurewitz (1998, pages 280ff.), and 
Passoneau (1998, pages 338ff.) do not support his claim. In fact, all these researchers 
found a higher percentage of Smooth Shifts than Retains. In a spoken corpus, Pas- 
soneau found more Shifts than Continues. 
A preponderance of Shifts over Continues may reflect he domain and content of a 
text rather than the author's organizational goals. In fact, it can be seen that sequences 
of Smooth Shifts are rather natural in certain kinds of narrative or descriptive texts; 
see Example 1 (adapted from a pharmaceutical leaflet). 
Example 1 
a. The name of your medicine / is Compound X. 
b. Iti contains budesonideJ. 
(Continue) 
581 
Computational Linguistics Volume 27, Number 4 
c. Thisj is one of a group of medicines called corticosteroids k. 
(Smooth Shift) 
d. Thesek can help to relieve the symptoms of hay fever or rhinitis. 
(Smooth Shift) 
This does not appear to be an incoherent text, but there is no way that the content 
could be rearranged to turn the Shifts into Continues. However, we can see that the 
author has maintained centering coherence as far as the content allows. 
We may conclude that not only does corpus evidence fail to confirm the canonical 
ordering, but in fact corpus analysis itself is not sufficient o evaluate the claims of 
CT without taking into account he underlying semantic ontent of a text. That is, 
statistics about the relative frequency of occurrences of different ransition types do 
not in themselves tell us much about which transitions are preferred in particular 
situations ince they do not take account of the choices available to an author. 1 A 
more promising approach is that of Brennan (1998), who gave subjects the controlled 
narrative task of providing a running commentary on a video recording of a basketball 
game, and used the videotape itself to construct a "propositional script" listing the 
sequence of events and their participants, and identifying players who were likely to 
continue as the center of attention over a sequence of incidents. 
2.2 Rule 2 Applied to Generation 
Reiter (1994) claimed that existing generation systems converged on a "consensus," 
generic natural anguage generation (NLG) architecture consisting of the following 
tasks: 
? Content determination/text planning: deciding the content of a message 
and organizing the component propositions into a text tree; 
? Sentence planning: aggregating propositions into clausal units and 
choosing lexical items corresponding to concepts in the knowledge base, 
including referring expressions; 
? Linguistic realization: taking care of surface details such as agreement 
and orthography. 
I have argued elsewhere (Kibble 1999) that if CT is to be implemented in an NLG 
system, the principles I call "salience" and "cohesion" belong to different tasks within 
this scheme: "salience" is a matter for sentence planning, choosing a verb form or 
some other construction that makes the Cb prominent within a clause or sentence, 
while "cohesion"---ordering propositions in a text to maintain referential continuity-- 
is a matter for text planning. So there may be no single point in the generation process 
where the system has a choice between Retain and Shift, for instance: rather, the terms 
retain and shift describe the composite results of choices made at different stages of the 
generation task. This point is discussed in more detail in the cited paper. Referential 
continuity as determined by CT is only one of a number of factors determining the 
fluency and acceptability of generated text; see Kibble and Power (2000) for further 
discussion. 
1 Corpus tudies have also tended to be flawed by imprecise notions of "coreference'; s evan Deemter 
and Kibble (2000). 
582 
Kibble Rule 2 of Centering Theory 
3. The "Cheapness" Principle 
The Cp or preferred center has a dual role in CT: in optimal transitions, where Cp = 
Cb, it highlights the center of attention of the current utterance, and it is also intended 
to signal the center of attention of the following utterance: 
The preferred center epresents a prediction about the Cb of the follow- 
ing utterance. (Walker, Joshi, and Prince 1998, page 3) 
It turns out that this informally stated aspect of the Cp is not actually made explicit in 
the rules and constraints of CT: transitions /Un, U~+I) are defined in terms of the Cp 
of U~+I and the Cbs of Un and Un+l, but no definition mentions the Cp of U~. Strube 
and Hahn's "cheapness" principle can be seen as the "missing link" of CT, making 
explicit the prediction represented by the Cp. They question the canonical ordering of 
transitions, partly on the grounds that this ordering fails to predict the Retain-Shift 
pattern that has been claimed by some researchers to signal the introduction of a new 
"discourse topic." The principle of "cheapness" is intended to capture the intuition 
that a Retain is naturally followed by a Smooth Shift and is defined as follows: 
A transition pair is cheap if the backward-looking center of the current 
utterance is correctly predicted by the preferred center of the immedi- 
ately preceding utterance, i.e., Cb(Ui) = Cp(Ui_l)... (Strube and Hahn 
1999, page 332) 
Cheapness is claimed to minimize the inferential costs of processing sequences of 
utterances, and is proposed as a constraint on pairs of successive transitions as a 
replacement for the canonical orderings in Rule 2, which is restated as follows: 
Rule 2 ~ 
Cheap transition pairs are preferred over expensive ones. (Strube and 
Hahn 1999, page 334) 
This claim is supported by analysis of a variety of German texts. It turns out that 
although cheapness appears to be a sensible principle, it does not neatly partition the 
types of transition pairs; in particular, this principle does not necessarily hold of all 
Retain-Smooth Shift sequences. Strube and Hahn propose to rectify this by redefining 
the transitions, with an additional test Cp(Ui) = Cp(Ui-1) to subdivide Continue and 
Smooth Shift, resulting in new "expensive" transitions Exp-Continue and Exp-Smooth 
Shift. Strube and Hahn (1999, page 333) provide a table (not reproduced here) of 36 
transition pairs, labeled as "cheap," expensive," or "-" (not occurring). 
In fact, it seems that the way this principle is presented is unnecessarily compli- 
cated, and on closer examination it appears to be rather weak. First, if cheapness i the 
only criterion considered, CT would have nothing to say about texts such as Example 
1 that have no cheap transition pairs. So it appears unwise to simply abandon the 
canonical ordering altogether. Second, the constraint on transition pairs can be stated 
more economically in terms of triples of utterances. If it is the preferred case that for 
every transition pair in a discourse IIUn_l, Unl, IUn, Unq-lll, Cb(Un+l) = Cp(U,z), then 
this equation also holds for each triple IUn_l, Un, U~+I/and vice versa. Note also that 
if Cp(Un) is mentioned at all in Un+l, it is by definition the Cb of Un+l; so the require- 
ment can be stated more simply as Cp(Un) E Cf(Un+l). I propose that the cheapness 
principle should supplement rather than replace the principles of salience and cohesion. 
583 
Computational Linguistics Volume 27, Number 4 
A consequence of this is that the choice of Cp for an utterance Un has to look backward 
to/-/,,_1 (to identify the current Cb) and forward to Un+l. In general, the question of 
which principles take precedence in cases of conflict cannot be settled in this short 
paper, but I adopt the following working hypothesis: the optimal case will be the one 
where both cheapness and salience obtain; the normal case will be the one where at 
least one of them is satisfied, which may be at the expense of cohesion between the 
current and the subsequent u terance. If the Cp is part of a "cheap" sequence, correctly 
predicting the upcoming Cb, but does not identify the current Cb, this will normally 
be signaled by a nonanaphoric nominal form. 
Example 2 (adapted from GJW, page 217) illustrates the Retain-Shift pattern, 
though it does not provide unambiguous support for the proposal sketched above. 
The sequence (c-d-e) seems preferable to (c-d'-e') even though the latter apparently 
scores better according to the canonical ranking. In both sequences, cheapness i  satis- 
fied wherever it is applicable, but the apparently ess preferred sequence scores higher 
on salience and cohesion. 
Example 2 
a. John has had trouble arranging his vacation. 
b. He cannot find anyone to take over his responsibilities. 
c. He called up Mike yesterday to work out a plan. 
(Continue) 
d. Mike has annoyed him a lot recently. 
(Retain) 
e. He called John at 5 A.M. on Friday last week. 
(Smooth Shift) 
d'. He has been pretty annoyed with Mike recently. 
(Continue) 
ft. He got a call from him at 5 A.M. on Friday last week. 
(Continue) 
From an interpretation perspective, we can address this apparent discrepancy by 
looking again at the interaction between Rule 1 and Rule 2. Rule 1 states that if 
anything is pronominalized, the Cb will be; so in (d), for example, Mike cannot be 
interpreted as the Cb because the sentence contains a pronoun in addition. So in fact 
the Retain transition (c-d) is maximally coherent given the options available to the 
reader: salience is not tested for because the Cp is not a pronoun, but both cohesion 
and cheapness obtain. This choice means that Mike is predicted to be the new Cb of (e) 
so cohesion will be unavailable for (d-e). In general, it appears that cheapness i most 
likely to be an available option--the xpectation that the current Cp will be the next 
Cb is generally plausible--but salience and cohesion are not always available. Thus, 
if we take account of the options available to a reader at each stage, both versions of 
the discourse conform as far as possible to the principles of cohesion, salience, and 
cheapness. 
From a production perspective the question remains, Why has the author chosen 
the "lumpy" sequence (c-d-e) rather than a "smooth" sequence of Continues? Some 
possible answers are these: the preferred sequence uses simple, canonical active verb 
forms, which may be easier to process; the sequence of clause-initial pronouns He... 
He..., and so on, in the variant sequence makes it appear "flat" and uninteresting; 
584 
Kibble Rule 2 of Centering Theory 
Table 3 
Reanalysis of Example 2 (Cb in bold). 
Co Sal Ch 
a. John has had trouble arranging his vacation. - -  - -  Y 
b. He cannot find anyone to take over his responsibilities. Y Y Y 
c. He called up Mike yesterday to work out a plan. Y Y Y 
d. Mike has annoyed him a lot recently. Y N Y 
e. He called John at 5 A.M. on Friday last week. N Y - -  
d'. He has been pretty annoyed with Mike recently. Y Y Y 
e'. He got a call from him at 5 A.M. on Friday last week. Y y m 
the author is realizing a communicative goal to say something about John in (a-b-c) 
and something about Mike in (d-e). The bottom line is that from a generation point of 
view, centering is not enough. Maximizing coherent transitions will not in itself produce 
optimally fluent and readable text; instead, a number of other factors have to be taken 
into consideration i  order to minimize the inferential load on the reader, hold the 
reader's interest, and reflect communicative intentions. 
Both versions of the text are preferable to one where the last two sentences have 
different subjects: for example, (d) followed by (e")John . . . .  The intuition is that once 
the topic has changed, the discourse must stay with the new topic rather than "flip- 
flop" between two entities. This intuition can be sharpened by noting that (d-e) form an 
identifiable mbedded iscourse segment, whose subject matter is not directly related 
to the main issue of John's vacation plans. I conjecture that absence of salience is not 
penalized in segment-initial utterances as long as cheapness holds. 
The following restatement of Rule 2 is intended to bring out the Janus-like nature 
of CT, simultaneously ooking backward and forward: 
Rule 2" 
Cohesion: Prefer transitions (U,-1, U,} where: 
Cb(U,) is defined and 
if Cb(LIn_l) is defined, Cb(Un) = Cb(Un-1). 
Salience: Prefer transitions (Un-1, Un} where Cp(Un) = Cb(Un) 
\[if Cp(U~) is pronominal\] 
Cheapness: Prefer sequences (Un-1, U~, L/~+I} where Cp(Un) ? 
Cf(U~+I) 
Conditions: 
1. In case of conflicts, the following ordering is 
hypothesized: (cheapness \]salience} >
cohesion 
2. If U, is segment-initial, salience is not required 
if cheapness holds. 
Table 3 illustrates an analysis of Example 2 in terms of the interacting constraints. 
Note that the absence of salience against (d) is not penalized for reasons explained 
above. 
585 
Computational Linguistics Volume 27, Number 4 
This example suggests a need for optimization over sequences of more than two 
utterances. In a sequence Continue-Retain-Smooth Shift, the Shift is predicted in its 
local context but the Retain is not; although Retain is a cheap transition following 
Continue, another Continue would be cheap as well. The Retain is motivated as it 
allows a new topic to be introduced with a "cheap" Smooth Shift, and so we need 
to evaluate the whole sequence Continue-Retain-Smooth Shift. This illustrates that 
while a sequence that conforms to the cheapness principle may reduce the cognitive 
load on the hearer, it can actually increase the load on the speaker owing to the need 
to plan ahead beyond the current utterance. In fact, the proposals outlined here do 
not entail that speakers must plan the entire content and structure of sentences so far 
in advance. Rather, they entail that a speaker knows when uttering Un that he or she 
intends to express a particular fact about a particular entity E in utterance U,,+2; and 
it entails that the speaker would do well to prepare the hearer for this by making E 
prominent in utterance Un+l. The hypothesis i that speakers will make a degree of 
effort o help hearers to process their utterances smoothly, rather than opportunistically 
planning and realizing sentences one by one, but not to the extent of planning all the 
transitions in a discourse segment in advance of uttering anything. 
4. Conclusion 
Comparison of the standard preference ordering for centering transitions and Strube 
and Hahn's (1999) variant has established the following points: 
1. The strict ordering of canonical transitions assumed by GJW and others has not 
(so far) been confirmed by corpus evidence and does not naturally fit into generation 
architectures. There is no mechanism topredict he Retain-Shift sequence to introduce 
a new discourse topic. 
2. By reducing Rule 2 to a requirement for cheap transition pairs, Strube and Hahn 
weaken the predictive power of the theory while complicating the apparatus with two 
additional transitions and a 36-position table of cheap versus expensive pairs. 
I have argued that in fact we can dispense with not only Strube and Hahn's two 
new transition types but the four old ones as well, retaining them only for descriptive 
convenience. The various different transitions can be seen to emerge in a partial, context- 
dependent ordering as a result of the interaction of cohesion, salience, and cheapness. 
Following established practice in empirical work such as that discussed in Section 
2.1, centering coherence is applied to inter- as well as intrasegmental ransitions. The 
modified proposal is still weaker than GJW's original formulation as a wider variety 
of texts is tolerated. This underscores the fact that referential continuity as specified 
by CT may play an essential part in computing the overall coherence of utterance 
transitions but it is only one of the determinants of discourse structure. 
Acknowledgments 
Thanks to Richard Power for helpful 
discussions, and to the anonymous 
reviewers for their perceptive comments. 
This work was funded in part by the UK 
EPSRC under grant references GR/L51126 
and GR/L77102. 
References 
Brennan, Susan E. 1998. Centering as a 
psychological resource for achieving joint 
reference in spontaneous discourse. In 
Marilyn Walker, Aravind K. Joshi, and 
Ellen Prince, editors, Centering Theory in 
Discourse. Clarendon Press, Oxford, pages 
227-249. 
Brennan, Susan E., Marilyn Walker 
Friedman, and Carl Pollard. 1987. A 
centering approach to pronouns. In 
Proceedings ofthe 25th Annual Meeting of the 
Association for Computational Linguistics, 
pages 155-162. 
di Eugenio, Barbara. 1998. Centering in 
Italian. In Marilyn Walker and Aravind K. 
586 
Kibble Rule 2 of Centering Theory 
Joshi, and Ellen Prince, editors, Centering 
Theory in Discourse. Clarendon Press, 
Oxford, pages 115-137. 
Grosz, Barbara J., Aravind K. Joshi, and 
Scott Weinstein. 1995. Centering: A
framework for modeling the local 
coherence of discourse. Computational 
Linguistics, 21(2):203-225. 
Hurewitz, Felicia. 1998. A quantitative look 
at discourse coherence. In Marilyn Walker, 
Aravind K. Joshi, and Ellen Prince, 
editors, Centering Theory in Discourse. 
Clarendon Press, Oxford, pages 273-291. 
Kibble, Rodger J. 1999. Cb or not Cb? 
Centering theory applied to NLG. In 
Proceedings ofACL Workshop on the Relation 
of Discourse~Dialogue Structure and 
Reference, pages 72-81. 
Kibble, Rodger J. and Richard D. J. Power. 
2000. An integrated framework for text 
planning and pronominalisation. I  
Proceedings ofthe 1st International Conference 
on Natural Language Generation, pages 
77-84. 
Passoneau, Rebecca. 1998. Interaction of 
discourse structure with explicitness of 
discourse anaphoric noun phrases. In 
Marilyn Walker, Aravind K. Joshi, and 
Ellen Prince, editors, Centering Theory in 
Discourse. Clarendon Press, Oxford, pages 
327-358. 
Reiter, Ehud. 1994. Has a consensus NL 
generation architecture appeared, and is it 
psycholinguistically plausible? In 
Proceedings ofthe 7th International Workshop 
on Natural Language Generation, pages 
163-170. 
Strube, Michael and Udo Hahn. 1999. 
Functional centering: Grounding 
referential coherence in information 
structure. Computational Linguistics 
25(3):309-344. 
van Deemter, Kees and Rodger Kibble. 2000. 
On coreferring: Coreference annotation in 
MUC and related schemes. Computational 
Linguistics 26(4):615-623. 
Walker, Marilyn, Aravind K. Joshi, and 
Ellen Prince. 1998. Centering in naturally 
occurring discourse: An overview. In 
Marilyn Walker, Aravind K. Joshi, and 
Ellen Prince, editors, Centering Theory in 
Discourse. Clarendon Press, Oxford, pages 
1-28. 
587 
c? 2004 Association for Computational Linguistics
Optimizing Referential Coherence
in Text Generation
Rodger Kibble? Richard Power?
University of London University of Brighton
This article describes an implemented system which uses centering theory for planning of coherent
texts and choice of referring expressions. We argue that text and sentence planning need to be
driven in part by the goal of maintaining referential continuity and thereby facilitating pronoun
resolution: Obtaining a favorable ordering of clauses, and of arguments within clauses, is likely
to increase opportunities for nonambiguous pronoun use. Centering theory provides the basis for
such an integrated approach. Generating coherent texts according to centering theory is treated
as a constraint satisfaction problem. The well-known Rule 2 of centering theory is reformulated in
terms of a set of constraints?cohesion, salience, cheapness, and continuity?and we show sample
outputs obtained under a particular weighting of these constraints. This framework facilitates
detailed research into evaluation metrics and will therefore provide a productive research tool in
addition to the immediate practical benefit of improving the fluency and readability of generated
texts. The technique is generally applicable to natural language generation systems, which perform
hierarchical text structuring based on a theory of coherence relations with certain additional
assumptions.
1. Overview
A central task for natural language generation (NLG) systems is to produce text that
is coherent, in the sense in which (1a) is noticeably more coherent than (1b):
1. a. Elixir is a white cream.
It is used in the treatment of cold sores.
It contains aliprosan.
Aliprosan relieves viral skin disorders.
b. Elixir contains aliprosan.
Viral skin disorders are relieved by aliprosan.
Elixir is used in the treatment of cold sores.
It is a white cream.
We can observe various ways in which text organization influences coherence: the
sequence in which certain facts are presented, the order in which entities are mentioned
in a clause, and the possibilities available for identifying the intended reference of
pronouns. Generally, (1a) seems to conform better to a reader?s expectations of what
will be referred to next and of how to resolve underspecified referring expressions,
? Department of Computing, Goldsmiths College, University of London, London SE14 6NW, U. K.
E-mail: r.kibble@gold.ac.uk
? Information Technology Research Institute, University of Brighton, Brighton BN2 4GJ, U. K. E-mail:
Richard.Power@itri.brighton.ac.uk
Submission received: 17 October 2002; Revised submission received: 22 May 2004; Accepted for
publication: 6 August 2004
402
Computational Linguistics Volume 30, Number 4
in particular pronouns. These are issues which the well-known centering theory (CT)
of Grosz, Joshi, and Weinstein (1995; henceforth GJW) is concerned with. Previous
algorithms for pronominalization such as those of McCoy and Strube (1999), Henschel,
Cheng, and Poesio (2000), and Callaway and Lester (2002) have addressed the task of
deciding whether to realize an entity as a pronoun on the basis of given factors such as
its syntactic role and discourse history within a given text structure; what is essentially
novel in our approach is that we treat referential coherence as a planning problem, on
the assumption that obtaining a favorable ordering of clauses, and of arguments within
clauses, is likely to increase opportunities for nonambiguous pronoun use. Centering
theory provides the basis for such an integrated approach.1
Of course coherence of a text depends on the realization of rhetorical relations
(Mann and Thompson 1987) as well as referential continuity, and the latter is to an
extent a byproduct of the former, as clauses that are rhetorically related also tend
to mention the same entities. However, even when a set of facts is arranged in a
hierarchical RST structure, there are still many possible linear orderings with notice-
able differences in referential coherence. This article concentrates on the influence of
referential continuity on overall coherence and describes a method for applying CT
to problems in text planning and pronominalization in order to improve the fluency
and readability of generated texts. This method is applicable in principle to any sys-
tem which produces hierarchically structured text plans using a theory of coherence
relations, with the following additional assumptions:
? There is a one-to-one correspondence between predicates and verbs, so
that the options for syntactic realization can be predicted from the
argument structure of predicates. Such ?shallow? lexicalization appears
to be standard in applied NLG systems (Cahill 1999).
? Pronominalization is deferred until grammatical relations and word
order have been determined.
Our exposition will refer to an implemented document generation system, Icon-
oclast, which uses the technique of constraint satisfaction (van Hentenryck 1989;
Power 2000; Power, Scott, and Bouayad-Agha 2003) with CT principles implemented
among a set of soft constraints. The Iconoclast system allows the user to specify
content and rhetorical structure through an interactive knowledge-base editor and
supports fine-grained control over stylistic and layout features. The user-determined
rhetorical structure is transformed into a text structure or a set of candidate text struc-
tures which respect various text formation rules encoded as hard constraints. Not all
of the resulting text structures will give rise to stylistically acceptable documents, and
of those which may be judged acceptable, some will be noticeably preferable to others.
The text-structuring phase is followed by an evaluation of the candidate structures in
which they are ranked according to a set of preferences encoded as soft constraints.
Centering preferences are weighted along with other stylistic constraints to fix the
preferred final ordering both of propositions in the text and of arguments within a
clause.
It is not our primary aim in this short article to provide an empirical assessment
of the claims of CT, for which we refer the reader to the relevant papers, such as
1 Callaway and Lester (2002) note that CT-based pronominalization algorithms ?assume that the
discourse tree was constructed with Centering theory in mind? (page 91); in our case this assumption
is justified.
403
Kibble and Power Optimizing Referential Coherence
those collected in Walker, Joshi, and Prince (1998a) as well as Poesio et al (2002)
and other works cited there. We report elsewhere (Kibble and Power 2004) on two
ongoing empirical studies: A paired-comparison study of judgments by naive subjects
indicates that centering constraints make an appreciable difference to the acceptability
of texts, and a corpus study using what we believe to be a novel technique involving
perturbations provides clear evidence of preferences between the different constraints.
One of the strengths of our framework is that it can be used as a research tool for
the evaluation of variants of CT, as different realizations of an input sequence can be
generated by varying control parameters, and one can very quickly see the results of
alternative choices.
1.1 Related Work
Other researchers have applied CT to generation, though to our knowledge none have
applied it to text planning, sentence planning, and pronominalization in the integrated
way that we present in this article. This general approach is anticipated by McKeown?s
(1985) text-planning system, in which referential coherence is taken to be one of the
factors determining fluency, though McKeown?s work predates RST and centering.
Mittal et al (1998) apply what we term salience to sentence planning, with the goal of
realizing the Cb as subject, though the text planner does not have a goal of attempting
to maintain the same Cb. We regard Cheng?s (2000) work on the interaction of centering
preferences and aggregation in text planning as complementary to our enterprise.
Karamanis (2001), Kibble (2001), and Beaver (2004), have argued for a ranking of the
centering principles as opposed to weighting, and indeed Beaver provides a unified
formulation of the centering rules and constraints as a ranked set of OT constraints.
However, we believe that such a ranking stands in need of empirical justification,
and Beaver?s data actually provide little evidence for strict ranking as opposed to
weighting of constraints (see Kibble 2003). Constraint satisfaction search was applied
by Marcu (1996, 1997) to the far harder task of constructing RST trees given a set
of facts and a repertoire of rhetorical relations; Mellish et al (1998) argue that this
approach may not scale up to the generation of larger texts and propose an alternative
using stochastic search. We address the issue of computational complexity in section
4; however we do not face the same problems as Marcu, since the task for our text
planner is to convert a given RST tree into a (possibly singleton) set of text structures
rather than to build the RST tree from scratch.
2. Centering Parameters
We assume some familiarity with the basic concepts of CT. In this section we briefly
and informally summarize the main assumptions of the theory and explain how we
have interpreted and applied these assumptions:
1. For each utterance in a discourse there is said to be at most one entity that is the
center of attention or center (Constraint 1). The center in an utterance Un is the most
highly ranked entity realized in Un?1, which is also realized in Un (Constraint 3). This
is also referred to as the backward-looking center or Cb. (The set of entities mentioned
in an utterance Un is defined by Constraint 2 as the set of forward-looking centers
or Cfs.) It is not entirely clear whether Constraint 1 is to be taken as an empirical
claim or as a stipulation that some entity must be designated as Cb, if necessary by
constructing an indirect anaphoric link.
2. There is a preference for consecutive utterances within a discourse segment to
keep the same entity as the center and for the center to be realized as the highest-
ranked entity or preferred center (Cp). Kibble (1999) dubbed these principles cohe-
404
Computational Linguistics Volume 30, Number 4
Table 1
Centering transitions.
Continue Cohesion and Salience both hold; same center (or Cb(Un) undefined),
realized as Cp in Un+1
Retain Cohesion only; that is, center remains the same but is not realized
as Cp in Un+1
Smooth Shift Salience only; center of Un+1 realized as Cp but not equal to Cb(Un)
Rough Shift Neither cohesion nor salience holds
sion and salience, respectively. Combinations of these preferences provide the familiar
canonical set of transitions shown in Table 1, ranked in the stipulated order of pref-
erence first set out as Rule 2 by Brennan, Friedman, and Pollard (1987) and adopted
by Walker, Joshi, and Prince (1998b).
3. The center is the entity which is most likely to be pronominalized: GJW?s Rule 1
in its weakest form states that if any entity is referred to by a pronoun, the Cb must be.
As Poesio et al (2002) point out, CT can be viewed as a ?parametric? theory in
that key notions such as utterance and previous utterance, realization of entities, and
ranking are not given precise definitions by GJW, and subsequent applied studies
have had to begin by fixing particular instantiations of these notions.
2.1 Ranking
Since Brennan, Friedman, and Pollard (1987), a ranking in terms of grammatical roles
(or obliqueness) has become standard; for example: subject > direct object >
indirect object > others.
We have simplified matters somewhat for the purposes of this implementation. First,
we assume that syntactic realization serves only to distinguish the Cp from all other
referents, which are ranked on the same level: Thus effectively subject > others.
Secondly, we assume that the system already knows, from the argument structure
of the proposition, which entities can occur in subject position: Thus in realizing a
proposition ban(fda, elixir), both arguments are potential Cps because active and pas-
sive realizations are both allowed; for contain(elixir, gestodene), only elixir is a potential
Cp because we disallow Gestodene is contained by Elixir.
2.2 Realization
GJW?s original formulation distinguished between ?direct? realization, or coreference,
and ?indirect? realization, which corresponds to bridging reference. As an example,
in (1a) the terms cold sores and viral skin disorders are not strictly coreferential and so do
not count as direct realizations of the same entity, but if we allow indirect realization,
then there is the potential for one of these to be identified as Cb, in a sequence such
as Elixir is used to treat cold sores. Viral skin disorders are relieved by aliprosan. Again, we
keep things simple at this stage by treating nominal expressions as realizations of the
same entity only if they strictly corefer. As Poesio et al (2002) observe, under this
interpretation of realization, a number of utterances will lack an identifiable Cb, so we
have to allow for a ?no-Cb? transition in addition to the canonical transitions listed
in Table 1.2
2 Of course, even with indirect realization we would still have to allow for the possibility of no-Cb
transitions.
405
Kibble and Power Optimizing Referential Coherence
2.3 Utterance and Previous Utterance
Two different approaches to the realization of ?utterance? have become associated with
the work of Kameyama (1998) and Suri, McCoy, and DeCristoforo (1999). To simplify
somewhat: Kameyama argued that the local focus is updated in a linear manner by
tensed clauses rather than by sentences, while Suri, McCoy, and DeCristoforo present
evidence that the subject of the main clause in a complex sentence is likely to be
the preferred antecedent for a subject pronoun in an immediately following sentence,
winning out over candidates in an intervening subordinate clause, as in example (2):
2. Dodgei was robbed by an ex-convict j the other night.
The ex-convictj tied himi up because hei wasn?t cooperating.
Then hej took all the money and ran / #he i started screaming for help.
In fact we would argue that Suri, McCoy, and DeCristoforo?s analysis does not estab-
lish whether the accessibility effects are due to the syntactic or the rhetorical structure
of utterances. The examples they present all involve sentences of the form Sx because
Sy corresponding to the rhetorical pattern nucleus?connective?satellite. Their results
are therefore consistent with the hypothesis that the nucleus of a preceding segment
is more accessible than the satellite. We allow the user of our system to choose be-
tween two strategies: a linear, Kameyama-style approach or a hierarchical approach
in which the utterance is effectively identified with a rhetorical span. Our approach is
more general than that of Suri, McCoy, and DeCristoforo as it covers cases in which
the components of a complex rhetorical span are realized in different sentences. Veins
theory (Cristea, Ide, and Romary 1998) provides a possible formalization of the intu-
ition that some earlier propositions become inaccessible as a rhetorical boundary is
crossed. The theory could be applied to centering in various ways; we have imple-
mented perhaps the simplest approach, in which centering transitions are assessed in
relation to the nearest accessible predecessor. In many cases the linear and hierarchi-
cal definitions give the same result, but sometimes they diverge, as in the following
schematic example:
3. ban(fda, elixir) since contain(elixir, gestodene).
However, approve(fda, elixirplus).
Following Veins Theory, the predecessor of approve(fda, elixirplus) is ban(fda, elixir); its
linear predecessor contain(elixir, gestodene) (an embedded satellite) is inaccessible. This
makes a considerable difference: Under a hierarchical approach, fda can be the Cb of
the final proposition; under a linear approach, this proposition has no Cb.
2.4 Transitions versus Constraints
Kibble (1999, 2001) argued for a decomposition of the canonical transition types into
the principles of cohesion and salience, partly on the architectural grounds that this
makes it easier to apply CT to the generation task, and partly on the empirical grounds
that the preference ordering assumed by GJW is not strongly supported by corpus
evidence and that transitions are better seen as epiphenomenal, emerging in a partial
ordering from the interaction of more fundamental constraints. We follow this general
approach, including among the constraints the principle of continuity: Each utterance
should have at least one referent in common with the preceding utterance, which
is effectively a restatement of GJW?s Constraint 1. If we assign a weight of 1 each
to cohesion and salience and 2 to continuity, we obtain a partial ordering over the
406
Computational Linguistics Volume 30, Number 4
canonical transitions as follows:
0 : Continue > 1 : {Retain | Smooth Shift} > 2 : {Rough Shift | No Cb}
Any relative weighting or ranking of coherence over salience would need to be mo-
tivated by evidence that Retain is preferred over Smooth Shift, and we are not aware
of any conclusive evidence of this in the literature (see Kipple [1999] for further dis-
cussion).
This approach also means that Strube and Hahn?s (1999) principle of cheapness
can be naturally incorporated as an additional constraint: This is a requirement that
Cp(Un?1) = Cb(Un). The principle of cheapness effectively cashes out the informal
definition of the Cp as ?represent[ing] a prediction about the Cb of the following
utterance? (Walker, Joshi, and Prince, 1998b, page 3). In classic variants of centering
theory, this happens only indirectly as a result of transition preferences, and only
following a Continue or Smooth Shift, since the Cp is also the Cb and Rule 2 predicts
that the preferred transition will maintain the same Cb. However, the prediction is
not entailed by the theory following a Retain, Rough Shift, or no-Cb transition or
indeed for the first sentence in a discourse, when there is effectively no prediction
concerning the Cp. Strube and Hahn claim that the cheapness principle is motivated
by the existence of Retain-Shift patterns, which are evidently a common means of
introducing a new topic (see also Brennan, Friedman, and Pollard 1987 [henceforth
BFP]). To summarize, our system incorporates the following constraints:
cohesion: Cb(Un?1) = Cb(Un)
salience: Cp(Un) = Cb(Un)
cheapness: Cp(Un?1) = Cb(Un)
continuity: Cfs(Un?1) ? Cfs(Un) = ?
2.5 Preferences: Transitions, Pairs, or Sequences?
The original version of GJW?s Rule 2 specified that sequences of Continue transitions
are preferred over sequences of Retains, and so on; in BFP?s implementation, how-
ever, transitions are evaluated incrementally and the preference applies to individ-
ual transitions such as Continue versus Retain rather than to sequences. Strube and
Hahn (1999) take an intermediate position: In their formulation, pairs of transitions
??Ui, Uj?, ?Uj, Uk?? are preferred that are cheap, that is, Cp(Uj) = Cb(Uk). Strube and
Hahn intended the preference for cheap transition pairs to replace GJW?s Rule 2 in
toto, which seems a rather weak requirement. On the other hand the original GJW
formulation is difficult to verify, since as Poesio et al (2002, page 66) found, sequences
of multiple occurrences of the same transition type turn out to be relatively rare.
Our position is a little more complex, as we do not directly aim to generate particular
transitions or sequences of transitions but to minimize violations of the constraints con-
tinuity, cohesion, salience, and cheapness. Violations are computed on individual nodes
and summed for each candidate text structure, so we may expect that the candidate
with the fewest violations will have a preponderance of the preferred transitions. The
system is certainly more slanted toward global optimization than BFP?s incremental
model but may be said to achieve this in a more natural way than a strategy of trying
to produce uniform sequences of transitions.
2.6 Pronominalization
GJW?s Rule 1 is rather weak as a guide to pronominalization decisions in general, as
it only mentions the Cb and gives little guidance on when or whether to pronomi-
407
Kibble and Power Optimizing Referential Coherence
nalize non-Cbs. An important consideration for NLG is to minimize the possibility of
ambiguity, and so we adopt a cautious strategy: The user can choose between invari-
ably pronominalizing the Cb or using a fairly simple algorithm based on parallelism
of grammatical roles. A possible future development is to supplement our CT-based
text planner with a more sophisticated pronominalization algorithm as proposed by
Henschel, Cheng, and Poesio (2000) or Callaway and Lester (2002).
3. Generation Issues
CT has developed primarily in the context of natural language interpretation, focussing
on anaphora resolution (see, e.g., Brennan, Friedman, and Pollard 1987). As stated
above, the novel contribution of this article is an integrated treatment of pronomi-
nalization and planning, aiming to determine whether the principles underlying the
constraints and rules of the theory can be ?turned round? and used as planning oper-
ators for generating coherent text. We have assumed some familiarity in the foregoing
with terms such as text planning and sentence planning. These are among the distinct
tasks identified in Reiter?s ?consensus architecture? for natural language generation
(Reiter 1994):
Text planning/content determination: deciding the content of a message and or-
ganizing the component propositions into a text structure (typically a tree)
Sentence planning: aggregating propositions into clausal units and choosing lex-
ical items corresponding to concepts in the knowledge base; this is the
level at which the order of arguments and choice of referring expressions
will be determined
Linguistic realization: surface details such as agreement and orthography
Reiter observed that these functions can often be identified with discrete modules
in applied NLG systems and that a de facto standard had emerged in which these
modules are organized in a pipeline such that data flows only in one direction and
only between consecutive modules.
Breaking down the generation task in this way makes it evident that there are var-
ious ways the distinct principles of CT can be incorporated. Continuity and cohesion
naturally come under text planning: respectively, ordering a sequence of utterances to
ensure that each has a backward-looking center and maintaining the same entity as
the center within constraints on ordering determined by discourse relations. Salience
and cheapness, on the other hand, would come under sentence planning, since in each
case a particular entity is to be realized as subject. However, we encounter an appar-
ent paradox in that identifying the center itself depends on grammatical salience as
determined by the sentence planner: for example, choice of active or passive voice.
Consequently, the text planner appears to rely on decisions made at the sentence-
planning level, which is incompatible with the fact that ?pipelined systems cannot
perform general search over a decision space which includes decisions made in more
than one module? (Reiter 2000, page 252).
We can envisage three possibilities for incorporating CT into a generation archi-
tecture:
1. ?Incremental? sentence-by-sentence generation, in which the syntactic structure
of Un is determined before the semantic content of Un+1 is planned. That is, the text
planner would plan the content of Un+1 by aiming to realize a proposition in the
knowledge base which mentions an entity which is salient in Un. We are not aware
408
Computational Linguistics Volume 30, Number 4
Figure 1
Rhetorical structure.
of any system which performs all stages of generation in a sentence-by-sentence way,
and in any case this type of architecture would not allow for global planning over
multisentence sequences, which we take to be essential for a faithful implementation
of centering.
2. A pipelined system in which the ?topic? or ?theme? of a sentence is desig-
nated independently as part of the semantic input and centering rules reflect the
information structure of a discourse. Prince (1999) notes that definitions of topic in
the literature do not provide objective tests for topichood and proposes that the
topic should be identified with the center of attention as defined by CT; however,
what would be needed here would be a more fundamental definition that would ac-
count for a particular entity?s being chosen to be the center of attention in the first
place.
3. The solution we adopt is to treat the task of identifying Cbs and Cps as an
optimization problem. We assume that certain options for syntactic realization can be
predicted on the basis of the argument structure of predicates, which means that cen-
tering constructs can be calculated as part of text planning before syntactic realization
takes place, so that the paradox noted above is resolved. Pronominalization decisions
are deferred until a point at which grammatical relations and word order have been
fixed.
4. Generation as Constraint Satisfaction
In this section we give an overview of our text-planning component in order to set the
implementation of CT in context. The methodology is more fully described by Power,
Scott, and Bouayad-Agha (2003).
The text planner was developed within Iconoclast, a project that investigated
applications of constraint-based reasoning in natural language generation using as sub-
ject matter the domain of medical information leaflets. Following Scott and de Souza
(1990), we represent rhetorical structure by graphs like Figure 1, in which nontermi-
nal nodes represent RST relations, terminal nodes represent propositions, and linear
order is unspecified. The task of the text planner is to realize the rhetorical structure
as a text structure in which propositions are ordered, assigned to textual units (e.g.,
sentences, paragraphs, vertical lists), and linked where appropriate by discourse con-
nectives (e.g., since, however). The boundary between text and sentence planning is
drawn at the realization of elementary propositions rather than at the generation of
individual sentences. If a rhetorical subtree is realized as a complex sentence, the effect
409
Kibble and Power Optimizing Referential Coherence
is that ?text planning? trespasses into the higher-level syntax of the sentence, leaving
only the elementary propositions to be realized by ?sentence planning.?3
Even for a simple rhetorical input like figure 1, many reasonable text structures
can be generated. Since there are two nucleus-satellite relations, the elementary propo-
sitions can be ordered in four ways. Several discourse connectives can be employed
to realize each rhetorical relation (e.g., concession can be realized by although, but, and
however). At one extreme, the text can be spread out over several paragraphs, while at
the other extreme, it can be squeezed into a single sentence. With fairly restrictive con-
straint settings, the system generates 24 text structure patterns for figure 1, including
the following (shown schematically):
A. Since contain(elixir, gestodene), ban(fda, elixir).
However, approve(fda, elixirplus).
B. approve(fda, elixirplus), although since contain(elixir, gestodene),
ban(fda, elixir).
The final output texts will depend on how the propositions are realized syntactically;
among other things, this will depend on centering choices within each proposition.
In outline, the procedure that we propose is as follows:
1. Enumerate all text structures that are acceptable realizations of the
rhetorical structure.
2. For each text structure, enumerate all permissible choices for the Cb and
Cp of each proposition.
3. Evaluate the solutions, taking account of referential coherence among
other considerations, and choose the best.
For the example in figure 1, centers can be assigned in four ways for each text structure
pattern, making a total of 96 solutions.
As will probably be obvious, such a procedure could not be applied for rhetorical
structures with many propositions. For examples of this kind, based on the relations
cause and concession (each of which can be marked by several different connectives), we
find that the total number of text structures is approximately 5N?1 for N propositions.
Hence with N = 5, we would expect around 600 text structures; with perhaps five
to ten ways of assigning centers to each text structure, the total number of solutions
would approximate to 5,000. Global optimization of the solution therefore becomes
impracticable for texts longer than about five propositions; we address this problem
by a technique of partial optimization in which a high-level planner fixes the large-
scale structure of the text, thus defining a set of local planning problems, each small
enough to be tackled by the methods described here.
Stage 1 of the planning procedure is described in more detail by Power, Scott,
and Bouayad-Agha (2003). A brief summary follows, after which we focus on stages 2
and 3, in which the text planner enumerates the possible assignments of centers and
evaluates which is the best.
3 See Power, Scott, and Bouayad-Agha (2003) for detailed motivation of this concept of text structure as a
level of representation distinct from both rhetorical structure and syntactic structure.
410
Computational Linguistics Volume 30, Number 4
4.1 Generating and Evaluating Text Structures
A text structure is defined in Iconoclast as an ordered tree in which each node has a
feature named text?level. Values of text?level are represented by integers in the
range 0 . . .Lmax; these may be interpreted in various ways, but we will assume here
that Lmax = 4 and that integers are paired with descriptive labels as follows:
0 text phrase
1 text clause
2 text sentence
3 paragraph
4 section
Informally, a text structure (TS) is well-formed if it respects the hierarchy of textual
levels, so that sections are composed of paragraphs, paragraphs of text sentences,
and so forth. An example of an ill-formed structure would be one in which a text
sentence contained a paragraph; such a structure can occur only when the paragraph
is indented?a possibility we are excluding here. As well as being a well-formed text
structure, a candidate solution must realize a rhetorical structure (RS) ?correctly,? in
a sense that we need to make precise. Roughly, a correct solution should satisfy three
conditions:
1. The terminal nodes of the TS should express all the elementary
propositions in the RS; they may also contain discourse connectives
expressing rhetorical relations in the RS, although for some relations
discourse connectives are optional.
2. The TS must respect rules of syntax when it combines propositions and
discourse connectives within a text clause; for instance, a conjunction
such as but linking two text phrases must be coordinated with the
second one.
3. The TS must be structurally compatible with the RS.
The first two conditions are straightforward, but what is meant by ?structural compat-
ibility?? We suggest the crucial criterion for such compatibility should be as follows:
Any grouping of the elementary propositions in the TS must also occur in the RS. In
other words, the text structurer is allowed to eliminate groupings, but not to add any.
More formally:
? If a node in the TS dominates terminal nodes expressing a set of
elementary propositions, there must be a corresponding node in the RS
dominating the same set of propositions.
? The converse does not hold: For instance, an RS of the form
R1(R2(p1, p2), p3) can be realized by a paragraph of three sentences, one
for each proposition, even though this TS contains no node dominating
the propositions p1 and p2 that are grouped by R2. However, when this
happens, the propositions grouped together in the RS must remain
consecutive in the TS; solutions in which p3 comes in between p1 and p2
are prohibited.
411
Kibble and Power Optimizing Referential Coherence
Table 2
Examples of text-structuring constraints.
Name Type Description
Root domination Hard The text?level of the root node r must exceed
Lp > Ld that of any daughter d.
Parental domination Hard The text?level of a parent node p must be equal to
Lp ? Ld or greater than the text?level of any daughter d.
Sister equality Hard If nodes a and b are descended from the same
La = Lb parent, they must have the same text?level.
Sister order Hard If nodes a and b are descended from the same
Oa = Ob parent, they must have different values of order.
Connective Hard Governs choice of discourse connective.
Rhetorical grouping Soft Failure to express a rhetorical grouping can be
treated as a defect.
Oversimple paragraph Soft A paragraph containing only one text sentence can
be treated as a defect.
Centering Soft Constraints derived from centering theory.
Our procedure for generating candidate solutions is based on a technique for for-
mulating text structuring as a constraint satisfaction problem (CSP) (van Hentenryck,
1989), using the Eclipse logic programming environment.4 In general, a CSP is char-
acterized by the following elements:
? a set of variables V1 . . .VN
? For each variable Vi, a finite domain Di of possible values
? a set of constraints on the values of the variables (for integer domains
these often use ?greater than? and ?less than?; other domains usually
rely on ?equal? or ?unequal?.)
A solution assigns to each variable Vi a value from its domain Di while respecting
all constraints. For instance each node of the rhetorical structure is annotated with
a text?level variable with the domain 0 . . .Lmax and an order variable with the
domain 1 . . .N, where N is the number of sisters. Depending on the constraints, there
may be multiple solutions, or there may be no solution at all. We distinguish between
hard constraints, which are applied during the enumeration phase, determining which
candidate structures will be considered, and soft constraints, which apply during an
evaluation phase in which the enumerated solutions are ordered from best to worst.
Some examples of hard and soft constraints are shown in Table 2.
4.2 Choosing Centers
Given a text structure, we enumerate all permissible centering assignments as follows:
1. Determine the predecessor Un?1 (if any) of each proposition Un.
2. List the potential Cbs and Cps of each proposition (henceforth denoted
by ?Cb and ?Cp).
4 See http://www-icparc.doc.ic.ac.uk/eclipse/.
412
Computational Linguistics Volume 30, Number 4
Table 3
Cbs and Cps for solution A.
U Proposition ?Cb(U) ?Cp(U)
U1 contain(elixir, gestodene) [ ] [elixir]
U2 ban(fda, elixir) [elixir] [fda, elixir]
U3 approve(fda, elixir-plus) [fda] [fda, elixir-plus]
3. Compute all combinations from ?Cb and ?Cp that respect the
fundamental centering constraint that Cb(Un) should be the most salient
candidate in Un?1.
As stated earlier, two criteria for determining the predecessor have been implemented;
the user can select one or the other criterion, thus using the NLG system to test different
approaches. Following a linear criterion, the predecessor is simply the proposition that
precedes the current proposition in the text, regardless of structural considerations.
Following a hierarchical criterion, the predecessor is the most accessible previous
proposition, in the sense defined by Veins Theory (Cristea, Ide, and Romary, 1998).
For now we assume the criterion is linear.
?Cb(Un) (potential Cbs of proposition Un) is given by the intersection between
Cf (Un) and Cf (Un?1)?that is, all the referents they have in common. The potential
Cps are those referents in the current proposition that can be realized as most salient.
Obviously this should depend on the linguistic resources available to the generator; the
system actually uses a simpler rule based on argument types within the proposition.
Table 3 shows the potential Cbs and Cps for the proposition sequence in solution A pre-
sented at the beginning of this section. As stated earlier, our treatment of salience here
simplifies in two ways: We assume that syntactic realization serves only to distinguish
the Cp from all other referents and that the system already knows, from the argument
structure of the proposition, which entities can occur in subject position. With these
simplifications, the enumeration of centering assignments is straightforward; in the
above example, four combinations are possible, since there are two choices each for
Cp(U2) and Cp(U3).
4.3 Evaluating Solutions
The system evaluates candidate solutions by applying a battery of tests to each node of
the text plan. Each test identifies whether the node suffers from a particular defect. For
instance, one stylistic defect (at least for the rhetorical relations occurring in figure 1)
is that of placing nucleus before satellite; in general, the text reads better if important
material is placed at the end. For each type of defect, we specify a weight indicating
its importance: In evaluating continuity of reference, for example, the defect ?no Cb?
is regarded as more significant than other defects. Other violations are recorded only
in the case in which a Cb is present, so if all violations were weighted equally, this
could result in a ?no-Cb? transition?s being treated as less serious than an ?expensive?
Smooth Shift, for example (violating cheapness and cohesion). Summing the weighted
costs for all defects, we obtain a total cost for the solution; our aim is to find the
solution with the lowest total cost.
Regarding centering, the tests currently applied are as follows:
Salience violation: A proposition Un violates salience if Cb(Un) = Cp(Un). This
defect is assessed only on propositions that have a backward-looking cen-
ter.
413
Kibble and Power Optimizing Referential Coherence
Cohesion violation: A transition ?Un?1, Un? violates cohesion if Cb(Un) =
Cb(Un?1). This defect is not recorded when either Un or Un?1 has no Cb.
Cheapness violation: A transition ?Un?1, Un? violates cheapness if Cb(Un) =
Cp(Un?1). This defect is assessed only on propositions that have a
backward-looking center.
Continuity violation: This defect is recorded for any proposition with no Cb,
except the first proposition in the sequence (which by definition cannot
have a Cb).
Relative weightings for these defects can be chosen by the user; for the current exam-
ples we have chosen a neutral scheme with a weight of 3 for continuity violations and
1 each for the others, so that a no-Cb transition is ranked equally bad as an ?expen-
sive? Rough Shift.5 Applied to the four solutions to text structures A and B presented
in this section, these definitions yield costs shown in Table 4. According to our metric,
solutions A1 and A2 should be preferred because they incur less cost than any others,
with B3 and B4 the least preferred.
Although this article focuses on centering issues, it is important to remember that
other aspects of text quality are evaluated at the same time: The aim is to compute a
global measure so that disadvantages in one factor can be weighed against advantages
in another. For instance, text pattern B is bound to yield poor continuity of reference
because it orders the propositions so that U1 and U2 have no referents in common.
Text pattern A avoids this defect, but this does not automatically mean that A is better
than B; there may be other reasons, unconnected with centering, for preferring B to
A. The constraints which have an effect on clause ordering include:
Satellite before nucleus: For nucleus-satellite relations, place the satellite before
the nucleus.
Right-branching structure: If an elementary proposition is coordinated with a
complex rhetorical structure, place the elementary proposition first.
Centering constraints: Penalize orderings which violate centering preferences.
Text pattern B is favored by ?right-branching structure,? but in this case the centering
constraints will ?conspire? with ?satellite before nucleus? to favor pattern A overall.
5. Conclusion
We have described a technique for generating texts which will be coherent according
to a reasonably faithful interpretation of centering theory. NLG systems need some
principled means of deciding on the preferred orderings of clauses and of arguments
within clauses, and CT appears a good candidate to provide a basis for these decisions,
in tandem with other stylistic considerations. We have reported on a particular imple-
mentation in the Iconoclast document generation system, but the technique can be
applied to other NLG systems that perform hierarchical text structuring based on a
theory of coherence relations (with additional assumptions as detailed in Section 1):
? For systems which generate a single text plan, CT can determine the
most coherent ordering of arguments within clauses.
5 See Kibble and Power (2004) for initial results of empirical research on constraint weightings.
414
Computational Linguistics Volume 30, Number 4
Table 4
Realizations of text patterns A and B, with weights: cohesion | salience | cheapness = 1,
continuity = 3.
Version Text Cb Cp Defects Sum
Since Elixir contains gestodene ? elixir none
A1 the FDA bans Elixir. elixir fda sal 2
However, it approves Elixir+. fda fda coh
Since Elixir contains gestodene ? elixir none
A2 it is banned by the FDA. elixir elixir none 2
However, the FDA approves Elixir+. fda fda coh, ch
Since Elixir contains gestodene ? elixir none
A3 the FDA bans Elixir. However, elixir fda sal 3
Elixir+ is approved by the FDA. fda elixir+ sal, coh
Since Elixir contains gestodene ? elixir none
A4 it is banned by the FDA. However, elixir elixir none 3
Elixir+ is approved by the FDA. fda elixir+ sal, coh, ch
The FDA approves Elixir+ although ? fda none
B1 since Elixir contains gestodene ? elixir cont 3
it is banned by the FDA. elixir elixir none
Elixir+ is approved by the FDA ? elixir+ none
B2 although since Elixir contains gestodene ? elixir cont 3
it is banned by the FDA. elixir elixir none
The FDA approves Elixir+ although ? fda none
B3 since Elixir contains gestodene ? elixir cont 4
the FDA bans Elixir. elixir fda sal
Elixir+ is approved by the FDA ? elixir+ none
B4 although since Elixir contains gestodene ? elixir cont 4
the FDA bans Elixir. fda elixir sal
Note: ch = cohesion, coh=cohesion, cont=continuity, sal=salience.
? For systems which generate multiple text plans, CT can be used to
evaluate the different plans as well as to determine the optimal
realization of any particular plan.
We have carried out empirical studies that provide clear evidence that centering fea-
tures make a difference to the acceptability of texts and demonstrate one way to
determine weightings (Kibble and Power 2004). It may turn out that different weight-
415
Kibble and Power Optimizing Referential Coherence
ings are appropriate for different text genres or for speech as opposed to ?written?
text. Our framework will facilitate detailed research into evaluation metrics and will
therefore provide a productive research tool in addition to the immediate practical
benefit of improving the fluency and readability of generated texts.
Acknowledgments
The essential ideas of this work were
originally presented at the ACL Workshop
on Discourse Structure and Reference (1999),
the 12th Amsterdam Colloquium (1999),
and COLING 2000. An earlier version of
this article was presented at INLG 2000. We
are grateful to the audiences on those
occasions for useful feedback and also to
colleagues on the GNOME project as well
as Nikiforos Karamanis and the anonymous
reviewers for Computational Linguistics. This
work was supported in part by the U.K.
EPSRC under grant references L51126,
L77102, and M36960.
References
Beaver, David. 2004. The optimization of
discourse anaphora. Linguistics and
Philosophy, 27(1):3?56.
Brennan, Susan, Marilyn Walker Friedman,
and Carl Pollard. 1987. A centering
approach to pronouns. In Proceedings of
25th ACL, pages 155?162, Stanford, CA.
Cahill, Lynne. 1999. Lexicalisation in
applied NLG systems. Technical Report
ITRI-99-04, Information Technology
Research Institute, University of Brighton.
Callaway, Charles B. and James C. Lester.
2002. Pronominalization in generated
discourse and dialogue. In Proceedings of
the 40th Annual Meeting of the Association for
Computational Linguistics (ACL), pages
88?95, Philadelphia.
Cheng, Hua. 2000. Experimenting with the
interaction between aggregation and text
planning. In Proceedings of ANLP-NAACL,
pages 1?6, Seattle.
Cristea, Dan, Nancy Ide, and Laurent
Romary. 1998. Veins theory: A model of
global discourse cohesion and coherence.
In Proceedings of COLING/ACL?98, pages
281?285, Montreal.
Grosz, Barbara, Aravind Joshi, and Scott
Weinstein. 1995. Centering: A framework
for modelling the local coherence of
discourse. Computational Linguistics,
21(2):203?225.
Henschel, Renate, Hua Cheng, and
Massimo Poesio. 2000. Pronominalisation
revisited. In Proceedings of 18th COLING,
pages 306?312, Saarbru?cken, Germany.
Kameyama, Megumi. 1998. Intrasentential
centering: A case study. In Marilyn
Walker, Aravind Joshi, and Ellen Prince,
editors, Centering Theory in Discourse,
pages 89?112. Clarendon, Oxford.
Karamanis, Nikiforos. 2001. Exploring
entity-based coherence. In Proceedings of
Fourth CLUK, pages 18?26, University of
Sheffield, Sheffield, England.
Kibble, Rodger. 1999. Cb or not Cb?
Centering theory applied to NLG. In
Proceedings of ACL Workshop on Discourse
and Reference Structure, pages 72?81,
University of Maryland, College Park.
Kibble, Rodger. 2001. A reformulation of
rule 2 of centering theory. Computational
Linguistics, 27(4):579?587.
Kibble, Rodger. 2003. Towards the
elimination of centering theory. In Ivana
Kruijff-Korbayova? and Claudia Kosny,
editors, DiaBruck 2003: Proceedings of the
Seventh Workshop on the Semantics and
Pragmatics of Dialogue, Universita?t des
Saarlandes, Saarbru?cken, Germany.
Kibble, Rodger and Richard Power. 2004.
Optimising referential coherence as a
constraint satisfaction problem. Technical
Report RK/2004/1, Department of
Computing, Goldsmiths College, and
ITRI-04-07, Information Technology
Research Institute, University of Brighton.
Mann, William and Sandra Thompson.
1987. Rhetorical structure theory: A
theory of text organisation. Technical
Report ISI/RS-87-190, Information
Sciences Institute, Los Angeles.
Marcu, Daniel. 1996. Building up rhetorical
structure trees. In Proceedings of AAAI-96,
pages 1069?1074, Portland, OR.
Marcu, Daniel. 1997. From local to global
coherence: A bottom-up approach to text
planning. In Proceedings of AAAI-97, pages
629?635, Providence, RI.
McCoy, Kathleen and Michael Strube. 1999.
Generating anaphoric expressions:
Pronoun or definite description? In
Proceedings of ACL Workshop on Discourse
and Reference Structure, pages 63?71,
University of Maryland, College Park.
McKeown, Kathleen R. 1985. Text Generation.
Cambridge University Press, Cambridge.
Mellish, Chris, Alistair Knott, Jon
Oberlander, and Mick O?Donnell. 1998.
Experiments using stochastic search for
text planning. In Proceedings of the Ninth
International Workshop on Natural Language
416
Computational Linguistics Volume 30, Number 4
Generation, pages 97?108,
Niagara-on-the-Lake, Ontario.
Mittal, Vibhu, Johanna Moore, Giuseppe
Carenini, and Steven Roth. 1998.
Describing complex charts in natural
language: A caption generation system.
Computational Linguistics, 24(3):431?467.
Poesio, Massimo, Rosemary Stevenson, Hua
Cheng, Barbara di Eugenio, and Janet
Hitzeman. 2002. A corpus-based
evaluation of centering theory. Technical
Report TN-02-01/CSM-369, Natural
Language Engineering Group, University
of Essex.
Power, Richard. 2000. Planning texts by
constraint satisfaction. In Proceedings of
COLING 2000, pages 642?648,
Saarbru?cken, Germany.
Power, Richard, Donia Scott, and Nadjet
Bouayad-Agha. 2003. Document structure.
Computational Linguistics, 29(2):211?260.
Prince, Ellen. 1999. How not to mark topics:
?Topicalization? in English and Yiddish.
Unpublished manuscript, Linguistics
Department, University of Pennsylvania.
Reiter, Ehud. 1994. Has a consensus NL
generation architecture appeared, and is it
psycholinguistically plausible? In
Proceedings of the Seventh International
Natural Language Generation Workshop,
pages 163?170, Kennebunkport, ME.
Reiter, Ehud. 2000. Pipelines and size
constraints. Computational Linguistics,
26(2):251?259.
Scott, Donia and Clarisse de Souza. 1990.
Getting the message across in RST-based
text generation. In Robert Dale, Chris
Mellish, and Michael Zock, editors,
Current Research in Natural Language
Generation, pages 47?73. Academic Press,
London.
Strube, Michael and Udo Hahn. 1999.
Functional centering?Grounding
referential coherence in information
structure. Computational Linguistics,
25(3):309?344.
Suri, Linda, Kathleen McCoy, and Jonathan
DeCristofaro. 1999. A methodology for
extending focussing franeworks.
Computational Linguistics, 25(2):173?194.
van Hentenryck, P. 1989. Constraint
Satisfaction in Logic Programming. MIT
Press, Cambridge, MA.
Walker, Marilyn, Aravind Joshi, and Ellen
Prince, editors. 1998a. Centering Theory in
Discourse. Clarendon, Oxford.
Walker, Marilyn, Aravind Joshi, and Ellen
Prince. 1998b. Centering in naturally
occurring discourse. In Marilyn Walker,
Aravind Joshi, and Ellen Prince, editors,
Centering Theory in Discourse, pages 1?28.
Clarendon, Oxford.
Specifying the Parameters of Centering Theory: a Corpus-Based
Evaluation using Text from Application-Oriented Domains
M. Poesio, H. Cheng, R. Henschel, J. Hitzeman,y R. Kibble,x and R. Stevenson
University of Edinburgh, ICCS and HCRC,
fpoesio,huac,henschelg@cogsci.ed.ac.uk
y The MITRE Corporation, hitz@linus.mitre.org
xUniversity of Brighton, ITRI, Rodger.Kibble@itri.bton.ac.uk
University of Durham, Psychology and HCRC, Rosemary.Stevenson@durham.ac.uk
Abstract
The definitions of the basic concepts,
rules, and constraints of centering the-
ory involve underspecified notions such
as ?previous utterance?, ?realization?,
and ?ranking?. We attempted to find the
best way of defining each such notion
among those that can be annotated reli-
ably, and using a corpus of texts in two
domains of practical interest. Our main
result is that trying to reduce the num-
ber of utterances without a backward-
looking center (CB) results in an in-
creased number of cases in which some
discourse entity, but not the CB, gets
pronominalized, and viceversa.
1 MOTIVATION
Centering Theory (Grosz et al, 1995; Walker et
al., 1998b) is best characterized as a ?parametric?
theory: its key definitions and claims involve no-
tions such as ?utterance?, ?realization?, and ?rank-
ing? which are not completely specified; their pre-
cise definition is left as a matter for empirical re-
search, and may vary from language to language.
A first goal of the work presented in this paper
was to find which way of specifying these param-
eters, among the many proposed in the literature,
would make the claims of centering theory most
accurate as predictors of coherence and pronomi-
nalization for English. We did this by annotating
a corpus of English texts with the sort of informa-
tion required to implement some of the most pop-
ular variants of centering theory, and using this
corpus to automatically check two central claims
of the theory, the claim that all utterances have a
backward looking center (CB) (Constraint 1), and
the claim that if any discourse entity is pronomi-
nalized, the CB is (Rule 1). In doing this, we tried
to make sure we would only use information that
could be annotated reliably.
Our second goal was to evaluate the predic-
tions of the theory in domains of interest for real
applications?natural language generation, in our
case. For this reason, we used texts in two gen-
res not yet studied, but of interest to developers of
NLG systems: instructional texts and descriptions
of museum objects to be displayed on Web pages.
The paper is organized as follows. We first re-
view the basic notions of the theory. We then dis-
cuss the methods we used: our annotation method
and how the annotation was used. In Section 4 we
present the results of the study. A discussion of
these results follows.
2 FUNDAMENTALS OF CENTERING
THEORY
Centering theory (Grosz et al, 1995; Walker et
al., 1998b) is an ?object-centered? theory of text
coherence: it attempts to characterize the texts
that can be considered coherent on the basis of
the way discourse entities are introduced and dis-
cussed.1 At the same time, it is also meant to
be a theory of salience: i.e., it attempts to pre-
dict which entities will be most salient at any
given time (which should be useful for a natural
language generator, since it is these entities that
are most typically pronominalized (Gundel et al,
1993)).
According to the theory, every UTTERANCE in
a spoken dialogue or written text introduces into
the discourse a number of FORWARD-LOOKING
CENTERS (CFs). CFs correspond more or less
1For a discussion of ?object-centered? vs. ?relation-
centered? notions of coherence, see (Stevenson et al, 2000).
to discourse entities in the sense of (Karttunen,
1976; Webber, 1978; Heim, 1982), and can be
linked to CFs introduced by previous or suc-
cessive utterances. Forward-looking centers are
RANKED, and because of this ranking, some CFs
acquire particular prominence. Among them, the
so-called BACKWARD-LOOKING CENTER (CB),
defined as follows:
Backward Looking Center (CB) CB(U
i+1
), the
BACKWARD-LOOKING CENTER of utter-
ance U
i+1
, is the highest ranked element of
CF(U
i
) that is realized in U
i+1
.
Utterance U
i+1
is classified as a CONTINUE if
CB(U
i+1
) = CB(U
i
) and CB(U
i+1
) is the most
highly ranked CF of U
i+1
; as a RETAIN if the CB
remains the same, but it?s not any longer the most
highly-ranked CF; and as a SHIFT if CB(U
i+1
) 6=
CB(U
i
).
The main claims of the theory are articulated in
terms of constraints and rules on CFs and CB.
Constraint 1: All utterances of a segment except
for the 1st have exactly one CB.
Rule 1: if any CF is pronominalized, the CB is.
Rule 2: (sequences of) continuations are pre-
ferred over (sequences of) retains, which are
preferred over (sequences of) shifts
Constraint 1 and Rule 2 express a preference for
utterances in a text to talk about the same ob-
jects; Rule 1 is the main claim of the theory about
pronominalization. In this paper we concentrate
on Constraint 1 and Rule 1.
One of the most unusual features of centering
theory is that the notions of utterance, previous
utterance, ranking, and realization used in the def-
initions above are left unspecified, to be appropri-
ately defined on the basis of empirical evidence,
and possibly in a different way for each language.
As a result, centering theory is best viewed as a
cluster of theories, each of which specifies the
parameters in a different ways: e.g., ranking has
been claimed to depend on grammatical function
(Kameyama, 1985; Brennan et al, 1987), on the-
matic roles (Cote, 1998), and on the discourse sta-
tus of the CFs (Strube and Hahn, 1999); there are
at least two definitions of what counts as ?previ-
ous utterance? (Kameyama, 1998; Suri and Mc-
Coy, 1994); and ?realization? can be interpreted
either in a strict sense, i.e., by taking a CF to be
realized in an utterance only if an NP in that utter-
ance denotes that CF, or in a looser sense, by also
counting a CF as ?realized? if it is referred to in-
directly by means of a bridging reference (Clark,
1977), i.e., an anaphoric expression that refers to
an object which wasn?t mentioned before but is
somehow related to an object that already has, as
in the vase . . . the handle (see, e.g., the discussion
in (Grosz et al, 1995; Walker et al, 1998b)).
3 METHODS
The fact that so many basic notions of centering
theory do not have a completely specified def-
inition makes empirical verification of the the-
ory rather difficult. Because any attempt at di-
rectly annotating a corpus for ?utterances? and
their CBs is bound to force the annotators to adopt
some specification of the basic notions of the the-
ory, previous studies have tended to study a par-
ticular variant of the theory (Di Eugenio, 1998;
Kameyama, 1998; Passonneau, 1993; Strube and
Hahn, 1999; Walker, 1989). A notable exception
is (Tetreault, 1999), which used an annotated cor-
pus to compare the performance of two variants
of centering theory.
The work discussed here, like Tetreault?s, is an
attempt at using corpora to compare different ver-
sions of centering theory, but considering also pa-
rameters of centering theory not studied in this
earlier work. In particular, we looked at different
ways of defining the notion of utterance, we stud-
ied the definition of realization, and more gener-
ally the role of semantic information. We did this
by annotating a corpus with information that has
been claimed by one or the other version of cen-
tering theory to play a role in the definitions of
its basic notions - e.g., the grammatical function
of an NP, anaphoric relations (including infor-
mation about bridging references) and how sen-
tences break up into clauses and subclausal units?
and then tried to find out the best way of specify-
ing these notions automatically, by trying out dif-
ferent configurations of parameters, and counting
the number of violations of the constraints and
rules that would result by adopting a particular
parameter configuration.
The Data
The aim of our project, which is called
GNOME and whose home page is at
http://www.hcrc.ed.ac.uk/ ~ gnome,
is to develop NP generation algorithms whose
generality is to be verified by incorporating
them in two distinct systems: the ILEX system
developed at the University of Edinburgh, that
generates Web pages describing museum objects
on the basis of the perceived status of its user?s
knowledge and of the objects she previously
looked at (Oberlander et al, 1998); and the
ICONOCLAST system, developed at the Univer-
sity of Brighton, that supports the creation of
patient information leaflets (Scott et al, 1998).
The corpus we collected includes texts from
both the domains we are studying. The texts
in the museum domain consist of descriptions
of museum objects and brief texts about the
artists that produced them; the texts in the
pharmaceutical domain are leaflets providing the
patients with the legally mandatory information
about their medicine. The total size of the corpus
is of about 6,000 NPs. For this study we used
about half of each subset, for a total number of
about 3,000 NPs, of which 103 are third person
pronouns (72 in the museum domain, 31 in the
pharmaceutical domain) and 61 are third-person
possessive pronouns (58 in the museum domain,
3 in the pharmaceutical domain).
Annotation
Previous empirical studies of centering theory
typically involved a single annotator annotat-
ing her corpus according to her own subjective
judgment (Passonneau, 1993; Kameyama, 1998;
Strube and Hahn, 1999). One of our goals was
to use for our study only information that could
be annotated reliably (Passonneau and Litman,
1993; Carletta, 1996), as we believe this will
make our results easier to replicate. The price
we paid to achieve replicability is that we could-
n?t test all hypotheses proposed in the literature,
especially about segmentation and about ranking.
We discuss some of the problems in what follows.
(The latest version of the annotation manual is
available from the GNOME project?s home page.)
We used eight annotators for the reliability study
and the annotation.
Utterances Kameyama (1998) noted that iden-
tifying utterances with sentences is problematic
in the case of multiclausal sentences: e.g., gram-
matical function ranking becomes difficult to
measure, as there may be more than one sub-
ject. She proposed to use all and only tensed
clauses instead of sentences as utterance units,
and then classified finite clauses into (i) utter-
ance units that constitute a ?permanent? update
of the local focus: these include coordinated
clauses and adjuncts) and (ii) utterance units that
result in updates that are then erased, much as
in the way the information provided by subor-
dinated discourse segments is erased when they
are popped. Kameyama called these EMBED-
DED utterance units, and proposed that clauses
that serve as verbal complements behave this way.
Suri and McCoy (1994) did a study that led them
to propose that some types of adjuncts?in particu-
lar, clauses headed by after and before?should be
treated as ?embedded? rather than as ?permanent
updates? as suggested by Kameyama; these re-
sults were subsequently confirmed by more con-
trolled experiments Pearson et al (2000). Nei-
ther Kameyama nor Suri and McCoy discuss par-
entheticals; Kameyama only briefly mentions rel-
ative clauses, but doesn?t analyze them in detail.
In order to evaluate these definitions of ut-
terance (sentences versus finite clauses), as well
as the different ways of defining ?previous utter-
ance?, we marked up in our corpus what we called
(DISCOURSE) UNITS. These include clauses, as
well as other sentence subconstituents which may
be treated as separate utterances, including paren-
theticals, preposed PPs, and (the second element
of) coordinated VPs. The instructions for mark-
ing up units were in part derived from (Marcu,
1999); for each unit, the following attributes were
marked:
 utype: whether the unit is a main clause,
a relative clause, appositive, a parenthetical,
etc.
 verbed: whether the unit contains a verb or
not.
 finite: for verbed units, whether the verb is
finite or not.
 subject: for verbed units, whether they have
a full subject, an empty subject (expletive,
as in there sentences), or no subject (e.g., for
infinitival clauses).
The agreement on identifying the boundaries of
units, using the  statistic discussed in (Carletta,
1996), was  = :9 (for two annotators and 500
units); the agreement on features(2 annotators
and at least 200 units) was follows:
Attribute  Value
utype .76
verbed .9
finite .81
subject .86
NPs Our instructions for identifying NP mark-
ables derive from those proposed in the MATE
project scheme for annotating anaphoric relations
(Poesio et al, 1999). We annotated attributes of
NPs which could be used to define their ranking,
including:
 The NP type, cat (pronoun, proper name,
etc.)
 A few other ?basic? syntactic features, num,
per, and gen, that could be used to identify
contexts in which the antecedent of a pro-
noun could be identified unambiguously;
 The grammatical function, gf;
 ani: whether the object denoted is animate
or inanimate
 deix: whether the object is a deictic refer-
ence or not
The agreement values for these attributes are as
follows:
Attribute  Value
ani .81
cat .9
deix .81
gen .89
gf .85
num .84
per .9
one of the features of NPs claimed to affect rank-
ing (Sidner, 1979; Cote, 1998) that we haven?t
so far been able to annotate because of failure
to reach acceptable agreement is thematic roles
( = :35).
Anaphoric information Finally, in order to
compute whether a CF from an utterance was re-
alized directly or indirectly in the following ut-
terance, we marked up anaphoric relations be-
tween NPs, again using a variant of the MATE
scheme. Theories of focusing such as (Sidner,
1979; Strube and Hahn, 1999), as well as our own
early experiments with centering, suggested that
indirect realization can play quite a crucial role in
maintaining the CB; however, previous work, par-
ticularly in the context of the MUC initiative, sug-
gested that while it?s fairly easy to achieve agree-
ment on identity relations, marking up bridging
references is quite hard; this was confirmed by,
e.g., Poesio and Vieira (1998). As a result we did
annotate this type of relations, but to achieve a
reasonable agreement, and to contain somehow
the annotators? work, we limited the types of re-
lations annotators were supposed to mark up, and
we specified priorities. Thus, besides identity
(IDENT) we only marked up three non-identity
(?bridging? (Clark, 1977)) relations, and only re-
lations between objects. The relations we mark
up are a subset of those proposed in the ?extended
relations? version of the MATE scheme (Poesio et
al., 1999) and include set membership (ELE-
MENT), subset (SUBSET), and ?generalized pos-
session? (POSS), which includes part-of relations
as well as more traditional ownership relations.
As expected, we achieved a rather good agree-
ment on identity relations. In our most recent
analysis (two annotators looking at the anaphoric
relations between 200 NPs) we observed no real
disagreements; 79.4% of these relations were
marked up by both annotators; 12.8% by only
one of them; and in 7.7% of the cases, one of
the annotators marked up a closer antecedent than
the other. Concerning bridges, limiting the re-
lations did limit the disagreements among an-
notators (only 4.8% of the relations are actually
marked differently) but only 22% of bridging ref-
erences were marked in the same way by both an-
notators; 73.17% of relations are marked by only
one or the other annotator. So reaching agreement
on this information involved several discussions
between annotators and more than one pass over
the corpus.
Segmentation Segmenting text in a reliable
fashion is still an open problem, and in addition
the relation between centering (i.e., local focus
shifts) and segmentation (i.e., global focus shifts)
is still not clear: some see them as independent
aspects of attentional structure, whereas other re-
searchers define centering transitions with respect
to segments (see, e.g., the discussion in the intro-
duction to (Walker et al, 1998b)). Our prelim-
inary experiments at annotating discourse struc-
ture didn?t give good results, either. Therefore,
we only used the layout structure of the texts
as a rough indication of discourse structure. In
the museum domain, each object description was
treated as a separate segment; in the pharmaceu-
tical domain, each subsection of a leaflet was
treated as a separate segment. We then identified
by hand those violations of Constraint 1 that ap-
peared to be motivated by too broad a segmenta-
tion of the text.2
Automatic computation of centering
information
The annotation thus produced was used to au-
tomatically compute utterances according to the
particular configuration of parameters chosen,
and then to compute the CFs and the CB (if any)
of each utterance on the basis of the anaphoric
information and according to the notion of rank-
ing specified. This information was the used to
find violations of Constraint 1 and Rule 1. The
behavior of the script that computes this informa-
tion depends on the following parameters:
utterance: whether sentences, finite clauses, or
verbed clauses should be treated as utter-
ances.
previous utterance: whether adjunct clauses
should be treated Kameyama-style or
Suri-style.
rank: whether CFs should be ranked according
to grammatical function or discourse status
in Strube and Hahn?s sense
2(Cristea et al, 2000) showed that it is indeed possible
to achieve good agreement on discourse segmentation, but
that it requires intensive training and repeated iterations; we
intend to take advantage of a corpus already annotated in this
way in future work.
realization: whether only direct realization
should be counted, or also indirect realiza-
tion via bridging references.
4 MAIN RESULTS
The principle we used to evaluate the different
configurations of the theory was that the best def-
inition of the parameters was the one that would
lead to the fewest violations of Constraint 1 and
Rule 1. We discuss the results for each principle.
Constraint 1: All utterances of a segment
except for the 1st have precisely one CB
Our first set of figures concerns Constraint 1:
how many utterances have a CB. This con-
straint can be used to evaluate how well cen-
tering theory predicts coherence, in the follow-
ing sense: assuming that all our texts are co-
herent, if centering were the only factor behind
coherence, all utterances should verify this con-
straint. The first table shows the results obtained
by choosing the configuration that comes clos-
est to the one suggested by Kameyama (1998):
utterance=finite, prev=kameyama, rank=gf, real-
ization=direct. The first column lists the number
of utterances that satisfy Constraint 1; the second
those that do not satisfy it, but are segment-initial;
the third those that do not satisfy it and are not
segment-initial.
CB Segment Initial NO CB Total Number
Museum 132 35 245 412
Pharmacy 158 13 198 369
Total 290 48 443 791
The previous table shows that with this config-
uration of parameters, most utterances do not sat-
isfy Constraint 1 in the strict sense even if we take
into account text segmentation (admittedly, a very
rough one). If we take sentences as utterances,
instead of finite clauses, we get fewer violations,
although about 25% of the total number of utter-
ances are violations:
CB Segment Initial NO CB Total Number
Museum 120 22 85 227
Pharmacy 152 8 51 211
Total 272 30 136 438
Using Suri and McCoy?s definition of previous
utterance, instead of Kameyama?s (i.e., treating
adjuncts as embedded utterances) leads to a slight
improvement over Kameyama?s proposal but still
not as good as using sentences:
CB Segment Initial NO CB Total Number
Museum 140 35 237 412
Pharmacy 167 14 188 369
Total 307 49 425 791
What about the finite clause types not consid-
ered by Kameyama or Suri and McCoy? It turns
out that we get better results if we do not treat as
utterances relative clauses (which anyway always
have a CB, under standard syntactic assumptions
about the presence of traces referring to the modi-
fied noun phrase), parentheticals, clauses that oc-
cur in subject position; and if we treat as a single
utterance matrix clauses with empty subjects and
their complements (as in it is possible that John
will arrive tomorrow).
CB Segment Initial NO CB Total Number
Museum 143 35 153 331
Pharmacy 161 14 159 334
Total 304 49 312 665
But by far the most significant improvement to the
percentage of utterances that satisfy Constraint 1
comes by adopting a looser definition of ?real-
izes?, i.e., by allowing a discourse entity to serve
as CB of an utterance even if it?s only referred to
indirectly in that utterance by means of a bridg-
ing reference, as originally proposed by Sidner
(1979) for her discourse focus. The following se-
quence of utterances explains why this could lead
to fewer violations of Constraint 1:
(1) (u1) These ?egg vases? are of exceptional
quality: (u2) basketwork bases support
egg-shaped bodies (u3) and bundles of straw
form the handles, (u4) while small eggs resting
in straw nests serve as the finial for each lid. (u5)
Each vase is decorated with inlaid decoration:
. . .
In (1), u1 is followed by four utterances. Only
the last of these directly refers to the set of egg
vases introduced in u1, while they all contain im-
plicit references to these objects. If we adopt this
looser notion of realization, the figures improve
dramatically, even with the rather restricted set of
relations on which our annotators agree. Now the
majority of utterances satisfy Constraint 1:
CB Segment Initial NO CB Total Number
Museum 225 35 71 331
Pharmacy 174 14 146 334
Total 399 49 217 665
And of course we get even better results by treat-
ing sentences as utterances:
CB Segment Initial NO CB Total Number
Museum 171 17 39 227
Pharmacy 168 7 36 211
Total 339 24 75 438
It is important, however, to notice that even un-
der the best configuration, at least 17% of utter-
ances violate the constraint. The (possibly, obvi-
ous) explanation is that although coherence is of-
ten achieved by means of links between objects,
this is not the only way to make texts coherent.
So, in the museum domain, we find utterances
that do not refer to any of the previous CFs be-
cause they express generic statements about the
class of objects of which the object under discus-
sion is an instance, or viceversa utterances that
make a generic point that will then be illustrated
by a specific object. In the following example,
the second utterance gives some background con-
cerning the decoration of a particular object.
(2) (u1) On the drawer above the door, gilt-bronze
military trophies flank a medallion portrait of
Louis XIV. (u2) In the Dutch Wars of 1672 -
1678, France fought simultaneously against the
Dutch, Spanish, and Imperial armies, defeating
them all. (u3) This cabinet celebrates the Treaty
of Nijmegen, which concluded the war.
Coherence can also be achieved by explicit
coherence relations, such as EXEMPLIFICA-
TION in the following example:
(3) (u1) Jewelry is often worn to signal membership
of a particular social group. (u2) The Beatles
brooch shown previously is another case in point:
Rule 1: if any NP is pronominalized, the CB is
In the previous section we saw that allowing
bridging references to maintain the CB leads to
fewer violations of Constraint 1. One should
not, however, immediately conclude that it would
be a good idea to replace the strict definition
of ?realizes? with a looser one, because there
is, unfortunately, a side effect: adopting an in-
direct notion of realizes leads to more viola-
tions of Rule 1. Figures are as follows. Us-
ing utterance=s, rank=gf, realizes=direct 22 pro-
nouns violating Rule 1 (9 museum, 13 pharmacy)
(13.4%), whereas with realizes=indirect we have
38 violations (25, 13) (23%); if we choose utter-
ance=finite, prev=suri, we have 23 violations of
rule 1 with realizes=direct (13 + 10) (14%), 32
with realizes=indirect (21 + 11) (19.5%). Using
functional centering (Strube and Hahn, 1999) to
rank the CFs led to no improvements, because of
the almost perfect correlation in our domain be-
tween subjecthood and being discourse-old. One
reason for these problems is illustrated by (4).
(4) (u1) A great refinement among armorial signets
was to reproduce not only the coat-of-arms but
the correct tinctures; (u2) they were repeated in
colour on the reverse side (u3) and the crystal
would then be set in the gold bezel.
They in u2 refers back to the correct tinctures (or,
possibly, the coat-of-arms), which however only
occurs in object position in a (non-finite) com-
plement clause in (u1), and therefore has lower
ranking than armorial signets, which is realized
in (u2) by the bridge the reverse side and there-
fore becomes the CB having higher rank in (u1),
but is not pronominalized.
In the pharmaceutical leaflets we found a num-
ber of violations of Rule 1 towards the end of
texts, when the product is referred to. A possi-
ble explanation is that after the product has been
mentioned sentence after sentence in the text, by
the end of the text it is salient enough that there
is no need to put it again in the local focus by
mentioning it explicitly. E.g., it in the following
example refers to the cream, not mentioned in any
of the previous two utterances.
(5) (u1) A child of 4 years needs about a third of
the adult amount. (u2) A course of treatment for
a child should not normally last more than five
days (u3) unless your doctor has told you to use
it for longer.
5 DISCUSSION
Our main result is that there seems to be a trade-
off between Constraint 1 and Rule 1. Allowing
for a definition of ?realizes? that makes the CB be-
have more like Sidner?s Discourse Focus (Sidner,
1979) leads to a very significant reduction in the
number of violations of Constraint 1.3 We also
noted, however, that interpreting ?realizes? in this
way results in more violations of Rule 1. (No
differences were found when functional center-
ing was used to rank CFs instead of grammati-
3Footnote 2, page 3 of the intro to (Walker et al, 1998b)
suggests a weaker interpretation for the Constraint: ?there is
no more than one CB for utterance?. This weaker form of
the Constraint does hold for most utterances, but it?s almost
vacuous, especially for grammatical function ranking, given
that utterances have at most one subject.
cal function.) The problem raised by these re-
sults is that whereas centering is intended as an
account of both coherence and local salience, dif-
ferent concepts may have to be used in Constraint
1 and Rule 1, as in Sidner?s theory. E.g., we might
have a ?Center of Coherence?, analogous to Sid-
ner?s discourse focus, and that can be realized in-
directly; and a ?Center of Salience?, similar to her
actor focus, and that can only be realized directly.
Constraint 1 would be about the Center of Coher-
ence, whereas Rule 1 would be about the Center
of Salience. Indeed, many versions of centering
theory have elevated the CP to the rank of a sec-
ond center.4
We also saw that texts can be coherent even
when Constraint 1 is violated, as coherence can
be ensured by other means (e.g., by rhetorical re-
lations). This, again, suggests possible revisions
to Constraint 1, requiring every utterance either
to have a center of coherence, or to be linked by a
rhetorical relation to the previous utterance.
Finally, we saw that we get fewer violations of
Constraint 1 by adopting sentences as our notion
of utterance; however, again, this results in more
violations of Rule 1. If finite clauses are used as
utterances, we found that certain types of finite
clauses not previously discussed, including rela-
tive clauses and matrix clauses with empty sub-
jects, are best not treated as utterances. We didn?t
find significant differences between Kameyama
and Suri and McCoy?s definition of ?previous ut-
terance?. We believe however more work is still
needed to identify a completely satisfactory way
of breaking up sentences in utterance units.
ACKNOWLEDGMENTS
We wish to thank Kees van Deemter, Barbara di
Eugenio, Nikiforos Karamanis and Donia Scott
for comments and suggestions. Massimo Poesio
is supported by an EPSRC Advanced Fellowship.
Hua Cheng, Renate Henschel and Rodger Kib-
ble were in part supported by the EPSRC project
GNOME, GR/L51126/01. Janet Hitzeman was in
part supported by the EPSRC project SOLE.
4This separation among a ?center of coherence? and a
?center of salience? is independently motivated by consid-
erations about the division of labor between the text planner
and the sentence planner in a generation system; see, e.g.,
(Kibble, 1999).
References
S.E. Brennan, M.W. Friedman, and C.J. Pollard. 1987.
A centering approach to pronouns. In Proc. of the
25th ACL, pages 155?162, June.
J. Carletta. 1996. Assessing agreement on classifica-
tion tasks: the kappa statistic. Computational Lin-
guistics, 22(2):249?254.
H. H. Clark. 1977. Inferences in comprehension. In
D. Laberge and S. J. Samuels, editors, Basic Pro-
cess in Reading: Perception and Comprehension.
Lawrence Erlbaum.
S. Cote. 1998. Ranking forward-looking centers. In
M. A. Walker, A. K. Joshi, and E. F. Prince, editors,
Centering Theory in Discourse, chapter 4, pages
55?70. Oxford.
D. Cristea, N. Ide, D. Marcu, and V. Tablan. 2000.
Discourse structure and co-reference: An empirical
study. In Proc. of COLING.
B. Di Eugenio. 1998. Centering in italian. In M. A.
Walker, A. K. Joshi, and E. F. Prince, editors, Cen-
tering Theory in Discourse, chapter 7, pages 115?
138. Oxford.
B. J. Grosz, A. K. Joshi, and S. Weinstein. 1995.
Centering: A framework for modeling the local co-
herence of discourse. Computational Linguistics,
21(2):202?225.
J. K. Gundel, N. Hedberg, and R. Zacharski. 1993.
Cognitive status and the form of referring expres-
sions in discourse. Language, 69(2):274?307.
I. Heim. 1982. The Semantics of Definite and In-
definite Noun Phrases. Ph.D. thesis, University of
Massachusetts at Amherst.
M. Kameyama. 1985. Zero Anaphora: The case of
Japanese. Ph.D. thesis, Stanford University.
M. Kameyama. 1998. Intra-sentential centering: A
case study. In M. A. Walker, A. K. Joshi, and
E. F. Prince, editors, Centering Theory in Dis-
course, chapter 6, pages 89?112. Oxford.
L. Karttunen. 1976. Discourse referents. In J. Mc-
Cawley, editor, Syntax and Semantics 7 - Notes from
the Linguistic Underground. Academic Press.
R. Kibble. 1999. Cb or not Cb? centering applied to
NLG. In Proc. of the ACL Workshop on discourse
and reference.
D. Marcu. 1999. Instructions for manually annotat-
ing the discourse structures of texts. Unpublished
manuscript, USC/ISI, May.
J. Oberlander, M. O?Donnell, A. Knott, and C. Mel-
lish. 1998. Conversation in the museum: Exper-
iments in dynamic hypermedia with the intelligent
labelling explorer. New Review of Hypermedia and
Multimedia, 4:11?32.
R. Passonneau and D. Litman. 1993. Feasibility of
automated discourse segmentation. In Proceedings
of 31st Annual Meeting of the ACL.
R. J. Passonneau. 1993. Getting and keeping the cen-
ter of attention. In M. Bates and R. M. Weischedel,
editors, Challenges in Natural Language Process-
ing, chapter 7, pages 179?227. Cambridge.
J. Pearson, R. Stevenson, and M. Poesio. 2000. Pro-
noun resolution in complex sentences. In Proc. of
AMLAP, Leiden.
M. Poesio and R. Vieira. 1998. A corpus-based inves-
tigation of definite description use. Computational
Linguistics, 24(2):183?216, June.
M. Poesio, F. Bruneseaux, and L. Romary. 1999. The
MATE meta-scheme for coreference in dialogues in
multiple languages. In M. Walker, editor, Proc. of
the ACL Workshop on Standards and Tools for Dis-
course Tagging, pages 65?74.
D. Scott, R. Power, and R. Evans. 1998. Generation
as a solution to its own problem. In Proc. of the
9th International Workshop on Natural Language
Generation, Niagara-on-the-Lake, CA.
C. L. Sidner. 1979. Towards a computational theory
of definite anaphora comprehension in English dis-
course. Ph.D. thesis, MIT.
R. Stevenson, A. Knott, J. Oberlander, and S McDon-
ald. 2000. Interpreting pronouns and connectives.
Language and Cognitive Processes, 15.
M. Strube and U. Hahn. 1999. Functional centering?
grounding referential coherence in information
structure. Computational Linguistics, 25(3):309?
344.
L. Z. Suri and K. F. McCoy. 1994. RAFT/RAPR
and centering: A comparison and discussion of
problems related to processing complex sentences.
Computational Linguistics, 20(2):301?317.
J. R. Tetreault. 1999. Analysis of syntax-based pro-
noun resolution methods. In Proc. of the 37th ACL,
pages 602?605, University of Marylan, June. ACL.
M. A. Walker, A. K. Joshi, and E. F. Prince, editors.
1998b. Centering Theory in Discourse. Oxford.
M. A. Walker. 1989. Evaluating discourse process-
ing algorithms. In Proc. ACL-89, pages 251?261,
Vancouver, CA, June.
B. L. Webber. 1978. A formal approach to discourse
anaphora. Report 3761, BBN, Cambridge, MA.
An integrated framework for text planning and pronominalisation 
R0dger - 'K ibb le  and  R ichard  Power  
ITRI 
University of Brighton 
Brighton BN2 4GJ 
UK. 
Email: {Rodger.Kibble I Richard.Power}@itri.brighton.ac.uk 
Abst ract  
This paper describes an implemented system 
which uses centering theory for planning of co- 
herent exts and choice of referring expressions. 
We argue that text and sentence planning need 
to be driven in part by the goal of maintain- 
ing referential continuity and thereby facilitat- 
ing pronoun resolution: obtaining a favourable 
ordering of clauses, and of arguments within 
clauses, is likely to increase opportunities for 
non-ambiguous pronoun use. Centering theory 
provides the basis for such an integrated ap- 
proach. Generating coherent texts according to 
centering theory is treated as a constraint sat- 
isfaction problem. 
1 In t roduct ion  
1.1 Issues in pronoun generat ion 
The appropriate r alisation of anaphoric expres- 
sions is a long-standing problem in NLG re- 
search. However, as McCoy and Strube (1999) 
observe, few researchers have developed sophis- 
ticated algorithms for pronoun generation. A 
typical approach, exemplified by Dale (1993), 
Reiter and Dale (1997) is to pronominalise ome 
distinguished referent which was mentioned in 
the previous sentence according to a domain- 
dependent criterion of prominence or salience. 
McCoy and Strube (op cir.) offer a more com- 
plex algorithm based on the notion of "dis- 
course threads", for which they report an ac- 
curacy of 85% when tested against a corpus 
of naturally-occurring texts. Their approach 
makes some fundamental assumptions about 
discourse structure which appear to be beyond 
the capabilities of current text and sentence 
planners and are incompatible with the widely- 
accepted notion of discourse structure as a tree 
with non-crossing branches (e.g., Mann and 
Thompson 1987). 
We argue for an approach which integrates 
the tasks of text planning and choice of referring 
expression on the following grounds: 
o portabi l i ty:  this approach should be com- 
patible with any system that employs hi- 
erarchical text planning and certain basic 
grammatical categories; 
o coherence:  we claim that text planning 
needs to be driven in part by the goal of 
maintaining referential continuity: obtain- 
ing a favourable ordering of clauses, and 
of arguments within clauses, is likely to 
increase opportunities for non-ambiguous 
pronoun use. 
The latter claim is not new, but underlies the 
Centering Theory (CT) of Grosz, Joshi and We- 
instein (1995, hereafter "GJW"). 
1.2 Issues in Text Planning 
Text Planning is one of the distinct tasks iden- 
tified in Reiter's "consensus" architecture for 
Natural Language Generation (Reiter 1994, Re- 
iter and Dale 1997): 
Text P lann ing-  deciding the content of a 
message, and organising the component 
propositions into a text tree; 
Sentence P lanning - aggregating proposi- 
tions into clausal units and choosing lex- 
ical items corresponding to concepts in the 
knowledge base; 
Linguistic real isation - surface details Such 
as agreement, orthography etc. 
Following Scott and de Souza (1990), we as- 
sume that the component propositions to be re- 
alised in a text are organised in a tree structure 
77 
in which ternfinal nodes are elementary propo- 
sitions and non-terminal nodes represent dis- 
course relations as .detined by  e~g:,. Rhetorical 
Structure Theory (RST, Mann and Thompson 
1987). This structure only partially constrains 
the linear order in which the propositions will 
be realised - -  in other words, any RST struc- 
ture specifies a range of possible text plans. We 
propose as an additional constraint that the 
generator should seek to maximise continuity 
of reference as determined by centering theory, 
and we argue that- this enables us to select the 
most cohesive variants from a set of text plans. 
The RST tree itself is produced by an interac- 
tive knowledge base editor which allows a user 
to control both semantic ontent and rhetorical 
structure via a sequence of choices guided by a 
natural language interface. 
2 Reconst ruct ing  Center ing  for  NLG 
2.1 Center ing  in a nutshe l l  
The main assumptions of Centering theory are: 
1. For each utterance in a discourse there is 
precisely one entity which is the centre of atten- 
tion or center. The center in an utterance Un is 
the most grammatically salient entity realised 
in U~_i which is also realised in Un. This is 
also referred to as the backward-looking center 
or Cb. The notion of "salience" for the purposes 
of centering theory is most commonly defined 
according to a hierarchy of grammatical roles: 
SUBJECT > DIRECT OBJECT  > INDIRECT OB- 
JECT  ~> OTHERS (see e.g., Brennan et al1987)? 
For alternative approaches see e.g., (Strube and 
Hahn 1999), (Walker et al1994). 
2. There is a preference for consecutive ut- 
terances within a discourse segment o keep the 
same entity as the center, and for the center to 
be realised as Subject or preferred center (Cp). 
Kibble (1999) dubbed these principles cohe- 
sion and sal ience respectively. Pairs of suc- 
cessive utterances (U,~, U~+i} are classified into 
the transition types shown in Fig. 1, in the or- 
.der of preference specified by Grosz et als "Rule 
, ,  ? 
3. The center is the entity which is most likely 
to be pronominalised: Grosz et als "Rule 1" 
in its weakest form states that if any entity is 
referred to by a pronoun, the Cb must be. 
CONTINUE." cohes ion and sa l ience 
. . . .  both hold; same center (o  r 
Cb(Un) undefined), realised as 
Subject in Un+l; 
RETAIN." cohes ion only; i.e. center 
remains the same but is not re- 
alised as Subject in Un+l; 
SMOOTH SHIFT." sa l ience only; cen- 
ter of Un+l realised as Subject but 
: , not  equal .to,Cb(U~); 
ROUGH SHIFT." neither cohes ion  nor 
sa l ience holds. 
NO CB: this transition is used by 
some researchers but is not dis- 
cussed by GJW. 
Figure 1: Centering Transitions 
2.2 P ronomina l i sa t ion  
Text genres are known to vary in the extent 
to which pronouns are used. The CT-based 
framework allows us to experiment with differ- 
ent strategies for choosing when to use a pro- 
noun, including: 
1. Never use anaphoric pronouns - -  for in- 
stance, in certain legal documents or tech- 
nical manuals where there must be no pos- 
sibility of ambiguity. 
2. Always pronominalise the Cb. 
3. Use a pronoun for non-Cbs only if the Cb 
is pronominalised. 
4. Pronominalise the Cb only after a Continue 
transition. 
Strategy 3 is favoured by (GJW) who cite psy- 
chological evidence that "the Cb is preferen- 
tially realised by a pronoun in English and by 
equivalent forms (i.e., ,.zero pronouns) in other 
languages" (op cit., p. 214). However, in the 
implementation reported in section 3 we opt for 
the more restrictive strategy 4. The generation 
approach outlined below enables us to experi- 
ment with different strategies and compare the 
resulting texts. 
78 
2.3 Center ing  and d iscourse  s t ruc ture  center after a sequence of CONTINUE. How- 
The canonical formulation of CT is concerned ever, in a sequence CONTINUE-RETAIN-SHIFT 
.' with local? cohesion;--'specifying .aqment-,. ~rarisi~':"::':the:sHng'~:is:p redicted:Am:its:A?cal':c?ntext~ but  
tions between consecutive sentences in a dis- the RETAIN is not; whenever RETAIN is a cheap 
course segment and favouring sequences which 
maintain the same center. Our implementation 
incorporates two extensions which have impli- 
cations for more structured iscourse: Strube 
and Hahn's (1999) "cheapness" principle, which 
favours transitions that introduce a new topic 
? in. a sal ient  position, and .Cristea's Veins The- 
ory (Cristea et al1998) which redefines tran- 
sitions in terms of rhetorical hierarchy rather 
than linear sequence of clauses (see section 3.3 
for discussion). 
"Cheapness" is satisfied by a transition pair 
( (Un-1 ,  Un), (Un, Un+l)) if the preferred center 
of Un is the Cb of Un+l. For example, this test 
is satisfied by a RETAIN-SHIFT sequence but not 
by CONTINUE-SHIFT, so it is predicted that the 
former pattern will be used to introduce a new 
center. (This claim is consistent with the find- 
ings of Brennan 1998, Brennan et al1987.) If we 
consider examples la-e below, the sequence c- 
d'-e ~, including a RETAIN-SHIFT sequence, reads 
more fluently than c-d-e even though the latter 
scores better according to the canonical rank- 
ing. 
. a. John has had trouble arranging his va- 
cation. 
b. He cannot find anyone to take over his 
responsibilities. 
c. He called up Mike yesterday to work 
out a plan. CONTINUE 
d. He has been pretty annoyed with Mike 
recently. CONTINUE 
e. Mike cancelled last week's project 
meeting at short notice. 
EXP-SMOOTH SHIFT 
d'. Mike has mmoyed him a lot recently. 
RETAIN 
e I. He cancelled last week's project meet- 
ing at short notice. SMOOTH SHIFT 
The "cheapness" principle illustrates the need 
for global opfimisation. We noted above that 
there is evidence that a RETAIN-SHIFT sequence 
is the preferred way of introducing a new 
transition following CONTINUE, another CON- 
TINUE would be cheap as well. The RETAIN 
is motivated as it enables a "cheap" SMOOTH 
SHIFT, and so we need a way of evaluating the 
whole sequence CONTINUE-RETAIN-SHIFT ver- 
SUS CONTINUE-CONTINUE-SHIFT. 
? : :2~4_~._,:Ceaatering.in :NLG 
CT has developed primarily in the context of 
natural language interpretation, focussing on 
anaphora resolution (see e.g., Brennan et al
1987). Curiously, NLG researchers have tended 
to overlook GJW's proposal that 
Rule 2 provides a constraint on speak- 
ers, and on natural-language gener- 
ation systems . . .To  empirically test 
the claim made by Rule 2 requires ex- 
amination of differences in inference 
load of alternative multi-utterance s - 
quences that differentially realize the 
same content. 
GJW, p. 215. 
With a few exceptions (e.g., Mittal et al1998, 
Kibble 1999, Kibble and Power 1999, Cheng 
2000) NLG researchers have interpreted CT as 
a theory of pronominalisation only (e.g., Dale 
1992). In this paper we concentrate on plan- 
ning, aiming to determine whether the prim 
ciples underlying the constraints and rules of 
the theory can be "turned round" and used as 
planning operators for generating coherent ext. 
Text planning in conformity with CT will need 
to follow the following set of heuristics: 
1. Plan tile order of clauses so that adjacent 
clauses have at least one referent in corn- 
I l ion. 
2. Cohes ion :  Prefer orderings which main- 
tain the same Cb in successive clauses. 
,3..- Sal ience: .Realise as=SubjeCt- of U;~ tile 
most grammatically salient entity in U,~-i 
which is mentioned in Un (the Cb). 
4. Cheapness :  Realise as Subject of Un an 
entity which is mentioned in U,~+l (and will 
therefore be Cb of U,,+i). 
79 
Breaking down the problem like this reveals ferent transitions. We assume that certain 
that there are various ways the distinct tasks options for syntactic realisation can be pre- 
...... can. be slotted, into.-an.NLG,system~Cohesion. . ... .... _~dicted.,ma::t~he,~basis~,of:,the~axgu~ment  ~str:uc- 
naturally comes under Text Planning: ordering 
a sequence of utterances to maintain the same 
entity as the center, within constraints on order- 
ing determined by discourse relations. However, 
identifying the center depends on grammatical 
salience, which is normally determined by the 
Sentence Planner- for example, choice of active 
or passive voice. Three possibilities are: 
? "Incremental" sentence-by-sentence gener- 
ation, where the syntactic structure of Un 
is determined before the semantic ontent 
of Un+l is planned. That is, the Text Plan- 
ner would plan the content of Un+l by aim- 
ing to realise a proposition in the knowl- 
edge base which mentions an entity which 
is salient in Un. We axe not aware of any 
system which performs all stages of gener- 
ation in a sentence-by-sentence way, and in 
any case this type of architecture would not 
allow the cheapness principle to be imple- 
mented as it would not support the neces- 
sary forward planning. 
* A pipelined system where the "topic" or 
"theme" of a sentence is designated inde- 
pendently as part of the semantic input, 
and centering rules reflect the information 
structure of a discourse. This approach 
was suggested by Kibble (1999), proposing 
that text and sentence planning should be 
driven by the goal of realising the desig- 
nated topic in positions where it will be 
interpreted as the Cb. However, this is not 
really a solution so much as a refinement of 
the problem, since it simply shifts the prob- 
lem of identifying the topic. Prince (1999) 
notes that definitions of "topic" in the liter- 
ature do not provide objective tests for top- 
ichood and proposes that the topic should 
be identified with the centre of attention 
as defined by CT; however, what would be 
needed here would be a more fimdamental 
definition which would, account for a par- 
ticular entity being chosen to be tile centre 
of attention. 
o The solution we adopt is to treat tile task 
of identifying Cbs as an optimisation prob- 
lem, giving different weightings to t, he dif- 
ture of predicates, which means that cen- 
tering transitions can be calculated as part 
of Text Planning. 
3 Imp lemented  prototype  
concession 
approve(fda, elixir-plus) cause NUCL~ S~LITE 
ban(fda, elixir) contain(elixir, gestodene) 
Figure 2: Rhetorical structure 
The text planner has been developed within 
ICONOCLAST, a project which investigates ap- 
plications of constraint-based reasoning in Nat- 
ural Language Generation using as  subject- 
matter the domain of medical information 
leaflets. Following (Scott and de Souza 1990), 
we represent rhetorical structure by graphs like 
figure 2, in which non-terminal nodes represent 
RST relations, terminal nodes represent propo- 
sitions, and linear order is unspecified. The task 
of the text planner is to realize the rhetorical 
structure as a text structure in which propo- 
sitions are ordered, assigned to textual units 
(e.g., sentences, paragraphs, vertical lists), and 
linked where appropriate by discourse connec- 
tives (e.g., 'since', 'however'). 
Even for a simple rhetorical input like figure 2 
many reasonable text structures call be gener- 
ated. Since there are two nucleus-satellite r la- 
tions, tile elementary propositions can be or- 
dered in four ways; several discourse connec- 
tives can be employed to realize each rhetor- 
ical relation (e.g. concession can be realized 
by 'although', 'but' and '.however'); at one ex- 
treme, the text can be spread out over several 
paragraphs, while at the other extreme it can 
be squeezed into a single sentence. With fairly 
restrictive constraint settings, the system gen- 
erates 24 text-structure patterns for figure 2, 
including the following (shown schematically): 
80 
A. Since contain(elixir, gestodene), ban(fda, 3.1 Choos ing  centers  
elixir). However, approve(fda, elixirplus). Given a text structure, we enumerate all per- 
B. approve(fda, elixirplus), although since 
contain(elixir, gestodene ) , ban (f da, elixir). 
The final output texts will depend on how the 
propositions are realized syntactically; among 
other things this will depend on centering 
choices within each proposition. 
In outline, the procedure that we propose is 
as follows: ~ . 
. Enumerate all text structures that are ac- 
ceptable realizations of the rhetorical struc- 
ture. 
. 
. 
For each text structure, enumerate all per- 
missible choices for the Cb and Cp of each 
proposition. 
Evaluate the solutions, taking account of 
referential coherence among other consid- 
erations, and choose the best. 
For the example in figure 2, centers can be as- 
signed in four ways for each text-structure pat- 
tern, making a total of 96 solutions. 
As will probably be obvious, such a procedure 
could not be applied for rhetorical structures 
with many propositions. For examples of this 
kind, based on the relations 'cause' and 'con- 
cession' (each of which can be marked by sev- 
eral different connectives), we find that the to- 
tal number of text-structures i  approximately 
5 N-~ for N propositions. Hence with N = 4 we 
would expect around 600 text structures; with 
perhaps 5-10 ways of assigning centers to each 
text structure, the total number of solutions 
would approximate to 5000. Global optimiza- 
tion of the solution therefore becomes imprac- 
ticable for texts longer than about five propo- 
sitions; we address this problem by a technique 
of partial optimization i which a macro-planner 
fixes the large-scale structure of the text, thus 
defining a set of micro-planning problems each 
small enough to be tack led by the methods de- 
scribed here. 
Stage 1 of the planning procedure is described 
elsewhere (Power, 2000); we focus here on stages 
2 and 3, in which the text planner enumerates 
the possible assignments of centers and evalu- 
ates which is the best. 
missible centering assignments as foil0ws: " . . . . .  
1. Determine the predecessor Yn-1 (if any) of 
each proposition Un. 
2. List the potential Cbs and Cps of each 
proposition, henceforth denoted by ECb 
and ECp. 
3. Compute ~li combinations from ECb and 
ECp that respect the fundamental center- 
ing constraint hat Cb(Un) should be the 
most salient candidate in Un-1. 
Two criteria for determining the predecessor 
have been implemented; the user can select one 
or other criterion, thus using the NLG system 
to test different approaches. Following a linear 
criterion, the predecessor is simply the propo- 
sition that precedes the current proposition in 
the text, regardless of structural considerations. 
Following a hierarchical criterion, the predeces- 
sor is the most accessible previous proposition, 
in the sense defined by Veins Theory (Cristea et 
al 1998). We will return to this issue later; for 
now we assume the criterion is linear. 
ECb(Un) (potential Cbs of proposition Un) is 
given by the intersection between Cf(U,~) and 
Cf(Un-1) -- i.e., all the referents they have in 
common. The potential Cps are those referents 
in the current proposition that can be realized 
as most salient. Obviously this should depend 
on the linguistic resources available to the gen- 
erator; the system actually uses a simpler rule 
based oil case roles within the proposition. Fig- 
ure 3 shows the potential Cbs and Cps for the 
proposition sequence in solution A. 
Our treatment of salience here simplifies ill 
tWO ways. First, we assume that syntactic real- 
ization serves only to distinguish the Cp from 
all other referents, which are ranked on the 
same level: thus effectively SUBJECT > OTHERS. 
Secondly, we assume that the system already 
.knows, from the event.class,of the proposition, 
which of its case roles can occur in subject po- 
sition: thus for ban, both arguments are poten- 
tim Cps because active and passive realizations 
are both allowed; for contain, only elixir is a 
potential Cp because we disallow 'Gestodene is
contained by Elixir'. 
81 
U Proposit ion 
U1 cont ain(elixir, gestodene) 
U2 ban(fda, elixir) 
U3 approve(fda, elixir-plus) 
ECb(U) 
\[\] 
\[elixir\] 
\[fda\] 
2Cp(U) 
.......... -{elixir\] 
\[fda, elixir\] 
\[fda, elixir-plus\] 
Figure 3: Cbs and Cps for solution A. 
With these simplifications, the enumeration 
of centering assignments i straightforward; in
the above example, four combinations are pos- 
sible, since there are two choices each for Cp(U2) 
and Cp(U3), none of which leads to any viola- 
tion of the basic centering constraint. This con- 
straint only comes into play if there are several 
choices for Cb(Un), one of which coincides with 
Cp(Un-1). 
3.2 Evaluat ing solutions 
Various metrics could be used in order to eval- 
uate centering choices. One possibility, for ex- 
ample, would be to associate a cost with each 
transition, so that perhaps 'Continue' (the best 
transition) has zero cost, while 'No Cb' (the 
worst transtion) has the highest cost. However, 
we have preferred the approach mentioned ear- 
lier in which cohesion and salience are evaluated 
separately; this allows us to include the further 
criterion of cheapness. 
Although this paper focusses on centering is- 
sues, it is important o remember that other as- 
pects of text quality are evaluated at the same 
time: the aim is to compute a global measure so 
that disadvantages in one factor can be weighed 
against advantages in another. For instance, 
text pattern B is bound to yield poor continuity 
of reference because it orders the propositions 
so that U1 and U2 have no referents in coin- 
mon. Text pattern A avoids this defect, but 
this does not necessarily mean that A is bet- 
ter than B overall; there may be other reasons, 
unconnected with centering, for preferring B to 
A. 
The system evaluates candidate solutions by 
applying a battery of tests to each.node of the 
text plan. Each test identifies whether the node 
suffers from a particular defect. For instance, 
one stylistic defect (at least for the rhetorical 
relations occurring in figure 2) is that of plac- 
ing nucleus before satellite; in general, the text 
reads better if important material is placed at 
the end. For each type of defect, we specify a 
weight indicating its importance: in evaluating 
continuity of reference, for example, the defect 
'No Cb' might be regarded as more significant 
than other defects. Summing the weighted costs 
for all defects, we obtain a total cost for the so- 
lution; our aim is to find the solution with the 
lowest total cost. 
Regarding centering, the tests currently ap- 
plied are as follows. 
Salience violation 
A proposition Un violates salience if 
Cb(Un) 7 ~ Cp(Un). This defect is assessed 
only on propositions that have a backward- 
looking center. 
Coherence violation 
A proposition Un violates cohesion if 
Cb(Un) 7 ~ Cb(Un-1). Again, this defect is 
not recorded when either Un or Un-1 has 
no Cb. 
Cheapness violation 
A proposition Us violates cheapness if 
Cb(Un) 7 ~ Cp(Un-1). 
No backward- look ing  center  
This defect is recorded for any proposition 
with no Cb, except the first proposition in 
the sequence (which by definition cannot 
have a Cb). 
Applied to the four solutions to text structure 
A, with all weights equal to 1, these definitions 
yield costs, shown in Figure 4.-According to our 
metric, solutions A1 and A2 should be preferred 
to A3 and A4 because they incur less cost. This 
resutt=cml be  assessed, by comparing -the follow- 
ing output texts, in which the generator has fol- 
lowed the policy of pronominalizing the Cb only 
after a 'Continue' transition: 
A1. Since Elixir contains gestodene, the FDA bans 
Elixir. However, it approves ElixirPlus. 
82 
Solut ion 
A1 
A2 
A3 
A4 
U Cb(U) Cp(U) Defects  
U1 0 .elixir none 
U2 elixir fda salience 
U3 fda fda cohesion 
TOTAL 2 
U1 ~ elixir 
U2 elixir elixir 
Ua fda fda 
TOTAL 
Vl 
U2 
U3 
U1 
U2 
U3 
none 
none 
cohesion, cheapness 
2 
I~ elixir none 
elixir fda salience 
fda elixir-plus salience, cohesion 
TOTAL 3 
elixir none 
elixir elixir none 
fda elixir-plus salience, cohesion, cheapness 
TOTAL 3 
Figure 4: Costs of solutions A1 - A4. 
A2. Since Elixir contains gestodene, it is banned by 
the FDA. However, the FDA approves Elixir- 
Plus. 
A3. Since Elixir contains gestodene, the FDA bans 
Elixir. However, ElixirPlus is approved by the 
FDA. 
A4. Since Elixir contains gestodene, it is banned by 
the FDA. However, ElixirPlus is approved by 
the FDA. 
Of course we are not satisfied that this metric 
is the best; an advantage of the generation ap- 
proach is that different evaluation methods can 
easily be compared. 
3.3 H ierarch ica l  center ing  
The linear approach, illustrated above, assigns 
centers on the basis of a proposition sequence, 
flattening the original hierarchy and ignoring 
nucleus-satellite r lations. This means, for ex- 
ample, that in a text of two paragraphs, propo- 
sition U2.1 (the first proposition in the second 
paragraph) has to be treated as the successor 
to Ui.N (the final proposition of the first para- 
graph): even if Ui.:\, is relatively insignificant 
(the satellite of a satellite, perhaps). One's in- 
tuition in such cases is that some more signif- 
icant proposition in the first paragraph should 
become the focus against which continuity of 
reference in the second paragraph is judged. 
Veins Theory (Cristea et al1998) provides a 
possible formalization of this intuition, in which 
some earlier propositions become inaccessible as 
a rhetorical boundary is crossed. The theory 
could be applied to centering in various ways; 
we have implemented perhaps the simplest ap- 
proach, in which centering transitions are as- 
sessed in relation to the nearest accessible prede- 
cessor. In many cases the linear and hierarchical 
definitions give tile same result, but sometimes 
they diverge, as in the following alternative to 
solutions A and B: 
C. ban(fda, elixir) since contain(elixir, 
gestodene). 
However, approve(f tin, elixirplus). 
Following Veins Theory, the predecessor of 
approve(f da, elixirplus) is ban(f da, elixir); its 
linear predecessor contain( elixir, ge.stodene ) 
(an embedded satellite) is inaccessible. This 
makes a considerable difference: under a hier- 
archical approach, fda can be the Cb of the 
83 
final proposition; under a linear approach, this Proceedings of ANLP-NAACL 2000. 
proposition has no Cb. D Cristea, N Ide and L Romary, 1998. Veins 
~ '. Iheory:  ~ A :model of,:gtobat: discourse :cohesion 
4 Conc lus ion  
This paper has highlighted some implications 
of Centering Theory for planning coherent text. 
We show that by making some assumptions 
about which entities are potential Cps, we can 
determine Cbs, Cps, and hence transitions, in 
the text planning stage. This allows the text 
planner to select he proposition sequence that 
yields the best continuity of reference, or to bal- 
ance the goal of referential continuity against 
other factors. For instance, there may be a 
preference for Satellite to follow Nucleus for 
some discourse relations, even if this results in a 
greater number of defects according to centering 
considerations. There are difficulties in evaluat- 
ing algorithms for specific tasks which are em- 
bedded in a generation system, since the quality 
of the output is limited by the functionalities of
the system as a whole. In particular, the task 
of generating appropriate referring expressions 
cannot be tackled in isolation from other tasks 
which contribute to the coherence of a text. 
The implementation of Centering reported 
here is a special case of text planning by con- 
straint satisfaction, where the user has control 
over the different constraints, and this approach 
means that different strategies for e.g. clause or- 
dering and pronominalisation can easily be com- 
pared by inspecting the resulting texts. The 
evaluation metrics we have presented here are 
provisional and are a matter for further detailed 
research, which our approach to text generation 
will facilitate. 
Acknowledgements  
This work was supported by the UK EPSRC 
under grant references L51126, L77102 (Kibble) 
and M36960 (Power). 
References 
S Brennan 1998. Centering as a Psychological 
Resource for Achieving Joint Reference in Spon- 
taneous Discourse. In Walker, Joshi and Prince 
(eds), Centering Theory in Discourse, Oxford. 
S Brennan. M Walker Friedman and C Pollard 
1987. A Centering Approach to Pronouns. In 
Proe. 25th A CL . 
H Cheng 2000. Experimenting with the Inter- 
action between Aggregation and Text Planning~ 
and coherence. In Proc COLING/ACL'98, pp 
281-285, Montreal. 
R Dale 1992, Generating Referring Expressions, 
MIT Press. 
B Grosz, A Joshi and S Weinstein 1995, Center- 
ing: a framework for modelling the local coher- 
ence of discourse. Computational Linguistics. 
? R Kibble 1999, Cb or not Cb? Centering theory 
applied to NLG. A CL workshop on Discourse 
and Reference Structure. 
R Kibble and R Power,1999. Using centering 
theory to plan coherent exts, Proceedings of 
the 12th Amsterdam Colloquium. 
W Mann and S Thompson 1987, Rhetorical 
Structure Theory: A Theory of Text Organ- 
isation. In L Polanyi (ed.), The Structure of 
Discourse. 
K McCoy and M Strube, 1999. Generating 
Anaphoric Expressions: Pronoun or Definite 
Description? A CL workshop on Discourse and 
Reference Structure. 
V Mittal, J Moore, G Carenini and S Roth 1998, 
Describing Complex Charts in Natural Lan- 
guage: A Caption Generation System. Com- 
putational Linguistics. 
R Power 2000. Planning Texts by Constraint 
Satisfaction, to appear in Proceedings of COL- 
ING 2000. 
E Prince 1999. How not to mark topics: "Top- 
icalization" in English and Yiddish. Ms, Lin- 
guistics Department, University of Pennsylva- 
nia. 
E Reiter 1994. Has a consensus NL generation 
architecture appeared, and is it psycholinguisti- 
cally plausible? In Proc. INLG 7. 
E Reiter and R Dale 1997, Building Applied 
Natural-Language Generation Systems. Jour- 
nal of Natural-Language Engineering 
D Scott and C de Souza, 1990. Getting the 
message across in RST-based text generation. 
In Dale, Mellish and Zock (eds), Current Re- 
search in Natural Language Generation, Aca- 
demic Press. 
M Strube and U Hahn 1999, Functional Center- 
ing - Grounding Referential Coherence in Infor- 
mation Structure. Computational Linguistics. 
84 
