Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 587?591,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Subjectivity and Sentiment Analysis of Modern Standard Arabic
Muhammad Abdul-Mageed
Department of Linguistics &
School of Library & Info. Science,
Indiana University,
Bloomington, USA,
mabdulma@indiana.edu
Mona T. Diab
Center for Computational
Learning Systems,
Columbia University, NYC, USA,
mdiab@ccls.columbia.edu
Mohammed Korayem
School of Informatics
and Computing,
Indiana University,
Bloomington, USA,
mkorayem@indiana.edu
Abstract
Although Subjectivity and Sentiment Analysis
(SSA) has been witnessing a flurry of novel re-
search, there are few attempts to build SSA
systems for Morphologically-Rich Languages
(MRL). In the current study, we report efforts
to partially fill this gap. We present a newly
developed manually annotated corpus of Mod-
ern Standard Arabic (MSA) together with a
new polarity lexicon.The corpus is a collec-
tion of newswire documents annotated on the
sentence level. We also describe an automatic
SSA tagging system that exploits the anno-
tated data. We investigate the impact of differ-
ent levels of preprocessing settings on the SSA
classification task. We show that by explicitly
accounting for the rich morphology the system
is able to achieve significantly higher levels of
performance.
1 Introduction
Subjectivity and Sentiment Analysis (SSA) is an area
that has been witnessing a flurry of novel research.
In natural language, subjectivity refers to expression
of opinions, evaluations, feelings, and speculations
(Banfield, 1982; Wiebe, 1994) and thus incorporates
sentiment. The process of subjectivity classification
refers to the task of classifying texts into either ob-
jective (e.g., Mubarak stepped down) or subjective
(e.g., Mubarak, the hateful dictator, stepped down).
Subjective text is further classified with sentiment or
polarity. For sentiment classification, the task refers
to identifying whether the subjective text is positive
(e.g., What an excellent camera!), negative (e.g., I
hate this camera!), neutral (e.g., I believe there will
be a meeting.), or, sometimes, mixed (e.g., It is good,
but I hate it!) texts.
Most of the SSA literature has focused on En-
glish and other Indio-European languages. Very few
studies have addressed the problem for morphologi-
cally rich languages (MRL) such as Arabic, Hebrew,
Turkish, Czech, etc. (Tsarfaty et al, 2010). MRL
pose significant challenges to NLP systems in gen-
eral, and the SSA task is expected to be no excep-
tion. The problem is even more pronounced in some
MRL due to the lack in annotated resources for SSA
such as labeled corpora, and polarity lexica.
In the current paper, we investigate the task of
sentence-level SSA on Modern Standard Arabic
(MSA) texts from the newswire genre. We run
experiments on three different pre-processing set-
tings based on tokenized text from the Penn Ara-
bic Treebank (PATB) (Maamouri et al, 2004)
and employ both language-independent and Arabic-
specific, morphology-based features. Our work
shows that explicitly using morphology-based fea-
tures in our models improves the system?s perfor-
mance. We also measure the impact of using a wide
coverage polarity lexicon and show that using a tai-
lored resource results in significant improvement in
classification performance.
2 Approach
To our knowledge, no SSA annotated MSA data ex-
ists. Hence we decided to create our own SSA an-
notated data.1
2.1 Data set and Annotation
Corpus: Two college-educated native speakers
of Arabic annotated 2855 sentences from Part
1 V 3.0 of the PATB. The sentences make up
the first 400 documents of that part of PATB
amounting to a total of 54.5% of the PATB
Part 1 data set. For each sentence, the an-
notators assigned one of 4 possible labels: (1)
OBJECTIVE (OBJ), (2) SUBJECTIVE-POSITIVE
(S-POS), (3) SUBJECTIVE-NEGATIVE (S-NEG),
and (4) SUBJECTIVE-NEUTRAL (S-NEUT). Fol-
lowing (Wiebe et al, 1999), if the primary goal
1The data may be obtained by contacting the first author.
587
of a sentence is judged as the objective reporting
of information, it was labeled as OBJ. Otherwise, a
sentence would be a candidate for one of the three
SUBJ classes. Inter-annotator agreement reached
88.06%.2 The distribution of classes in our data set
was as follows: 1281 OBJ, a total of 1574 SUBJ,
where 491 were deemed S-POS, 689 S-NEG, and
394 S-NEUT. Moreover, each of the sentences in our
data set is manually labeled by a domain label. The
domain labels are from the newswire genre and are
adopted from (Abdul-Mageed, 2008).
Polarity Lexicon: We manually created a lexicon
of 3982 adjectives labeled with one of the follow-
ing tags {positive, negative, neutral}. The adjectives
pertain to the newswire domain.
2.2 Automatic Classification
Tokenization scheme and settings: We run experi-
ments on gold-tokenized text from PATB. We adopt
the PATB+Al tokenization scheme, where procli-
tics and enclitics as well as Al are segmented out
from the stem words. We experiment with three dif-
ferent pre-processing lemmatization configurations
that specifically target the stem words: (1) Surface,
where the stem words are left as is with no further
processing of the morpho-tactics that result from the
segmentation of clitics; (2) Lemma, where the stem
words are reduced to their lemma citation forms, for
instance in case of verbs it is the 3rd person mas-
culine singular perfective form; and (3) Stem, which
is the surface form minus inflectional morphemes, it
should be noted that this configuration may result in
non proper Arabic words (a la IR stemming). Ta-
ble 1 illustrates examples of the three configuration
schemes, with each underlined.
Features: The features we employed are of two
main types: Language-independent features and
Morphological features.
Language-Independent Features: This group of
features has been employed in various SSA studies.
Domain: Following (Wilson et al, 2009), we ap-
ply a feature indicating the domain of the document
to which a sentence belongs. As mentioned earlier,
each sentence has a document domain label manu-
ally associated with it.
2A detailed account of issues related to the annotation task
will appear in a separate publication.
UNIQUE: Following Wiebe et al (2004) we ap-
ply a unique feature. Namely words that occur in our
corpus with an absolute frequency < 5, are replaced
with the token ?UNIQUE?.
N-GRAM: We run experiments with N-grams? 4
and all possible combinations of them.
ADJ: For subjectivity classification, we follow
Bruce & Wiebe?s (1999) in adding a binary
has adjective feature indicating whether or not any
of the adjectives in our manually created polarity
lexicon exists in a sentence. For sentiment classi-
fication, we apply two features, has POS adjective
and has NEG adjective, each of these binary fea-
tures indicate whether a POS or NEG adjective oc-
curs in a sentence.
MSA-Morphological Features: MSA exhibits a
very rich morphological system that is templatic,
and agglutinative and it is based on both derivational
and inflectional features. We explicitly model mor-
phological features of person, state, gender, tense,
aspect, and number. We do not use POS informa-
tion. We assume undiacritized text in our models.
2.3 Method: Two-stage Classification Process
In the current study, we adopt a two-stage classifica-
tion approach. In the first stage (i.e., Subjectivity),
we build a binary classifier to sort out OBJ from
SUBJ cases. For the second stage (i.e., Sentiment)
we apply binary classification that distinguishes S-
POS from S-NEG cases. We disregard the neutral
class of S-NEUT for this round of experimentation.
We use an SVM classifier, the SVMlight package
(Joachims, 2008). We experimented with various
kernels and parameter settings and found that linear
kernels yield the best performance. We ran experi-
ments with presence vectors: In each sentence vec-
tor, the value of each dimension is binary either a 1
(regardless of how many times a feature occurs) or
0.
Experimental Conditions: We first run ex-
periments using each of the three lemmatization
settings Surface, Lemma, Stem using various N-
grams and N-gram combinations and then itera-
tively add other features. The morphological fea-
tures (i.e., Morph) are added only to the Stem setting.
Language-independent features (i.e., from the fol-
lowing set {DOMAIN, ADJ, UNIQUE}) are added
to the Lemma and Stem+Morph settings. With all
588
Word POS Surface form Lemma Stem Gloss
AlwlAyAt Noun Al+wlAyAt Al+wlAyp Al+wlAy the states
ltblgh Verb l+tblg+h l+>blg+h l+blg+h to inform him
Table 1: Examples of word lemmatization settings
the three settings, clitics that are split off words are
kept as separate features in the sentence vectors.
3 Results and Evaluation
We divide our data into 80% for 5-fold cross-
validation and 20% for test. For experiments on the
test data, the 80% are used as training data. We have
two settings, a development setting (DEV) and a test
setting (TEST). In the development setting, we run
the typical 5 fold cross validation where we train on
4 folds and test on the 5th and then average the re-
sults. In the test setting, we only ran with the best
configurations yielded from the DEV conditions. In
TEST mode, we still train with 4 folds but we test on
the test data exclusively, averaging across the differ-
ent training rounds.
It is worth noting that the test data is larger than
any given dev data (20% of the overall data set for
test, vs. 16% for any DEV fold). We report results
using F-measure (F). Moreover, for TEST we re-
port only experiments on the Stem+Morph setting
and Stem+Morph+ADJ, Stem+Morph+DOMAIN,
and Stem+Morph+UNIQUE. Below, we only report
the best-performing results across the N-GRAM fea-
tures and their combinations. In each case, our base-
line is the majority class in the training set.
3.1 Subjectivity
Among all the lemmatization settings, the Stem was
found to perform best with 73.17% F (with 1g+2g),
compared to 71.97% F (with 1g+2g+3g) for Sur-
face and 72.74% F (with 1g+2g) for Lemma. In ad-
dition, adding the inflectional morphology features
improves classification (and hence the Stem+Morph
setting, when ran under the same 1g+2g condition
as the Stem, is better by 0.15% F than the Stem
condition alone). As for the language-independent
features, we found that whereas the ADJ feature
does not help neither the Lemma nor Stem+Morph
setting, the DOMAIN feature improves the re-
sults slightly with the two settings. In addition,
the UNIQUE feature helps classification with the
Lemma, but it hurts with the Stem+Morph.
Table 2 shows that although performance on the
test set drops with all settings on Stem+Morph, re-
sults are still at least 10% higher than the bseline.
With the Stem+Morph setting, the best performance
on the TEST set is 71.54% Fand is 16.44% higher
than the baseline.
3.2 Sentiment
Similar to the subjectivity results, the Stem set-
ting performs better than the other two lemmatiza-
tion scheme settings, with 56.87% F compared to
52.53% F for the Surface and 55.01% F for the
Lemma. These best results for the three lemmatiza-
tion schemes are all acquired with 1g. Again, adding
the morphology-based features helps improve the
classification: The Stem+Morph outperforms Stem
by about 1.00% F. We also found that whereas
adding the DOMAIN feature to both the Lemma and
the Stem+Morph settings improves the classification
slightly, the UNIQUE feature only improves classi-
fication with the Stem+Morph.
Adding the ADJ feature improves performance
significantly: An improvement of 20.88% F for the
Lemma setting and 33.09% F for the Stem+Morph
is achieved. As Table 3 shows, performance on test
data drops with applying all features except ADJ, the
latter helping improve performance by 4.60% F. The
best results we thus acquire on the 80% training data
with 5-fold cross validation is 90.93% F with 1g,
and the best performance of the system on the test
data is 95.52% F also with 1g.
4 Related Work
Several sentence- and phrase-level SSA systems
have been built, e.g., (Yi et al 2003; Hu and Liu.,
2004; Kim and Hovy., 2004; Mullen and Collier
2004; Pang and Lee 2004; Wilson et al 2005;
Yu and Hatzivassiloglou, 2003). Yi et al (2003)
present an NLP-based system that detects all ref-
589
Stem+Morph +ADJ +DOMAIN +UNIQUE
DEV 73.32 73.30 73.43 72.92
TEST 65.60 71.54 64.67 65.66
Baseline 55.13 55.13 55.13 55.13
Table 2: Subjectivity results on Stem+Morph+language independent features
Stem+Morph +ADJ +DOMAIN +UNIQUE
DEV 57.84 90.93 58.03 58.22
TEST 52.12 95.52 53.21 51.92
Baseline 58.38 58.38 58.38 58.38
Table 3: Sentiment results on Stem+Morph+language independent features
erences to a given subject, and determines senti-
ment in each of the references. Similar to (2003),
Kim & Hovy (2004) present a sentence-level sys-
tem that, given a topic detects sentiment towards it.
Our approach differs from both (2003) and Kim &
Hovy (2004) in that we do not detect sentiment to-
ward specific topics. Also, we make use of N-gram
features beyond unigrams and employ elaborate N-
gram combinations.
Yu & Hatzivassiloglou (2003) build a document-
and sentence-level subjectivity classification system
using various N-gram-based features and a polarity
lexicon. They report about 97% F-measure on docu-
ments and about 91% F-measure on sentences from
the Wall Street Journal (WSJ) corpus. Some of our
features are similar to those used by Yu & Hatzivas-
siloglou, but we exploit additional features. Wiebe
et al (1999) train a sentence-level probabilistic
classifier on data from the WSJ to identify subjectiv-
ity in these sentences. They use POS features, lex-
ical features, and a paragraph feature and obtain an
average accuracy on subjectivity tagging of 72.17%.
Again, our feature set is richer than Wiebe et al
(1999).
The only work on Arabic SSA we are aware of
is that of Abbasi et al (2008). They use an en-
tropy weighted genetic algorithm for both English
and Arabic Web forums at the document level. They
exploit both syntactic and stylistic features. Abbasi
et al use a root extraction algorithm and do not use
morphological features. They report 93.6% accu-
racy. Their system is not directly comparable to ours
due to the difference in data sets and tagging granu-
larity.
5 Conclusion
In this paper, we build a sentence-level SSA sys-
tem for MSA contrasting language independent only
features vs. combining language independent and
language-specific feature sets, namely morpholog-
ical features specific to Arabic. We also investi-
gate the level of stemming required for the task.
We show that the Stem lemmatization setting outper-
forms both Surface and Lemma settings for the SSA
task. We illustrate empirically that adding language
specific features for MRL yields improved perfor-
mance. Similar to previous studies of SSA for other
languages, we show that exploiting a polarity lexi-
con has the largest impact on performance. Finally,
as part of the contribution of this investigation, we
present a novel MSA data set annotated for SSA lay-
ered on top of the PATB data annotations that will
be made available to the community at large, in ad-
dition to a large scale polarity lexicon.
References
A. Abbasi, H. Chen, and A. Salem. 2008. Sentiment
analysis in multiple languages: Feature selection for
opinion classification in web forums. ACM Trans. Inf.
Syst., 26:1?34.
M. Abdul-Mageed. 2008. Online News Sites and
Journalism 2.0: Reader Comments on Al Jazeera
Arabic. tripleC-Cognition, Communication, Co-
operation, 6(2):59.
A. Banfield. 1982. Unspeakable Sentences: Narration
590
and Representation in the Language of Fiction. Rout-
ledge Kegan Paul, Boston.
R. Bruce and J. Wiebe. 1999. Recognizing subjectivity.
a case study of manual tagging. Natural Language
Engineering, 5(2).
T. Joachims. 2008. Svmlight: Support vector ma-
chine. http://svmlight.joachims.org/, Cornell Univer-
sity, 2008.
S. Kim and E. Hovy. 2004. Determining the senti-
ment of opinions. In Proceedings of the 20th In-
ternational Conference on Computational Linguistics,
pages 1367?1373.
M. Maamouri, A. Bies, T. Buckwalter, and W. Mekki.
2004. The penn arabic treebank: Building a large-
scale annotated arabic corpus. In NEMLAR Confer-
ence on Arabic Language Resources and Tools, pages
102?109.
R. Tsarfaty, D. Seddah, Y. Goldberg, S. Kuebler, Y. Ver-
sley, M. Candito, J. Foster, I. Rehbein, and L. Tounsi.
2010. Statistical parsing of morphologically rich lan-
guages (spmrl) what, how and whither. In Proceedings
of the NAACL HLT 2010 First Workshop on Statistical
Parsing of Morphologically-Rich Languages, Los An-
geles, CA.
J. Wiebe, R. Bruce, and T. O?Hara. 1999. Development
and use of a gold standard data set for subjectivity clas-
sifications. In Proc. 37th Annual Meeting of the Assoc.
for Computational Linguistics (ACL-99), pages 246?
253, University of Maryland: ACL.
J. Wiebe, T. Wilson, R. Bruce, M. Bell, and M. Martin.
2004. Learning subjective language. Computational
linguistics, 30(3):277?308.
J. Wiebe. 1994. Tracking point of view in narrative.
Computional Linguistics, 20(2):233?287.
T. Wilson, J. Wiebe, and P. Hoffmann. 2009. Recogniz-
ing Contextual Polarity: an exploration of features for
phrase-level sentiment analysis. Computational Lin-
guistics, 35(3):399?433.
J. Yi, T. Nasukawa, R. Bunescu, and W. Niblack. 2003.
Sentiment analyzer: Extracting sentiments about a
given topic using natural language processing tech-
niques. In Proceedings of the 3rd IEEE International
Conference on Data Mining, pages 427?434.
H. Yu and V. Hatzivassiloglou. 2003. The penn arabic
treebank: Building a large-scale annotated arabic cor-
pus. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing, pages 129?
136.
591
Proceedings of the Fifth Law Workshop (LAW V), pages 110?118,
Portland, Oregon, 23-24 June 2011. c?2011 Association for Computational Linguistics
Subjectivity and Sentiment Annotation of Modern Standard Arabic
Newswire
Muhammad Abdul-Mageed
Department of Linguistics &
School of Library & Info. Science,
Indiana University,
Bloomington, USA
mabdulma@indiana.edu
Mona T. Diab
Center for Computational Learning Systems,
Columbia University,
NYC, USA
mdiab@ccls.columbia.edu
Abstract
Subjectivity and sentiment analysis (SSA) is
an area that has been witnessing a flurry
of novel research. However, only few at-
tempts have been made to build SSA systems
for morphologically-rich languages (MRL). In
the current study, we report efforts to par-
tially bridge this gap. We present a newly
labeled corpus of Modern Standard Arabic
(MSA) from the news domain manually an-
notated for subjectivity and domain at the sen-
tence level. We summarize our linguistically-
motivated annotation guidelines and provide
examples from our corpus exemplifying the
different phenomena. Throughout the paper,
we discuss expression of subjectivity in nat-
ural language, combining various previously
scattered insights belonging to many branches
of linguistics.
1 Introduction
As the volume of web data continues to phenome-
nally increase, researchers are becoming more inter-
ested in mining that data and making the informa-
tion therein accessible to end-users in various inno-
vative ways. As a result, searches and processing
of data beyond the limiting level of surface words
are becoming increasingly important (Diab et al,
2009). The sentiment expressed in Web data specif-
ically continues to be of high interest and value to
internet users, businesses, and governmental bodies.
Thus, the area of Subjectivity and sentiment analysis
(SSA) has been witnessing a flurry of novel research.
Subjectivity in natural language refers to aspects of
language used to express opinions, feelings, eval-
uations, and speculations (Banfield, 1982; Wiebe,
1994) and it, thus, incorporates sentiment. The pro-
cess of subjectivity classification refers to the task
of classifying texts into either Objective (e.g., More
than 1000 tourists have visited Tahrir Square, in
downtown Cairo, last week.) or Subjective. Sub-
jective text is further classified with sentiment or po-
larity. For sentiment classification, the task refers
to identifying whether a subjective text is positive
(e.g., The Egyptian revolution was really impres-
sive!), negative (e.g., The bloodbaths that took place
in Tripoli were horrifying!), neutral (e.g., The com-
pany may release the software next month.), and,
sometimes, mixed (e.g., I really like this labtop, but
it is prohibitively expensive.). SSA sometimes in-
corporates identifying the holder(s), target(s), and
strength (e.g., low, medium, high) of the expressed
sentiment.
In spite of the great interest in SSA, only few
studies have been conducted on morphologically-
rich languages (MRL) (i.e., languages in which sig-
nificant information concerning syntactic units and
relations are expressed at the word-level (Tsarfaty
et al, 2010)). Arabic, Hebrew, Turkish, Czech, and
Basque are examples of MRLs. SSA work on MRLs
has been hampered by lack of annotated data. In the
current paper we report efforts to manually anno-
tate a corpus of Modern Standard Arabic (MSA), a
morphologically-rich variety of Arabic, e.g., (Diab
et al, 2007; Habash et al, 2009). The corpus is a
collection of documents from the newswire genre
covering several domains such as politics and sports.
We label the data at the sentence level. Our annota-
tion guidelines explicitly incorporate linguistically-
motivated information.
110
The rest of the paper is organized as follows: In
Section 2, we motivate work on the news genre.
In Section 3, we summarize our linguistically-
motivated annotation guidelines. In Section 4, we
introduce the domain annotation task. In Section 5
we provide examples from our dataset. We present
related work in Section 6. We conclude in Section 7.
2 Subjectivity and Sentiment in the News
Most work on SSA has been conducted on data be-
longing to highly subjective, user-generated genres
such as blogs and product or movie reviews where
authors express their opinions quite freely (Balahur
and Steinberger, 2009). In spite of the important
role news play in our lives (e.g., as an influencer
of the social construction of reality (Fowler, 1991),
(Chouliaraki and Fairclough, 1999), (Wodak and
Meyer, 2009)), the news genre has received much
less attention within the SSA community. This role
of news and the connection between news-making
and social contexts and practices motivates the task
of building SSA system. In addition, the many novel
ways online news-making is becoming an interactive
process (Abdul-Mageed, 2008) further motivates
investigating the newswire genre. News-makers re-
produce some of the views of their readers (e.g., by
quoting them) and they devote full stories about the
interactions of web users on social media outlets1.
Although subjectivity in news articles has tradition-
ally tended to be implicit, the fact that news sto-
ries have their own biases (e.g., hiding agents be-
hind negative or positive events via use of passive
voice, variation in lexical choice) has been pointed
out by e.g., (Van Dijk, 1988). The growing trend to
foster interactivity and more heavily report commu-
nication of internet users within the body of news
articles is likely to make expression of subjectivity
in news articles more explicit.
3 Subjectivity and Sentiment Annotation
(SSA)
Two graduate level educated native speakers of Ara-
bic annotated 2855 sentences from Part 1 V 3.0 of
1This trend has increased especially in Arab news organi-
zations like Al-Jazeera and Al-Arabiya with the hightened at-
tention to social media as a result of ongoing revolutions and
protests in the Arab world
OBJ S-POS S-NEG S-NEUT Total
OBJ 1192 21 57 11 1281
S-POS 47 439 2 3 491
S-NEG 69 0 614 6 689
S-NEUT 115 2 9 268 394
Total 1423 462 682 288 2855
Table 1: Agreement for SSA sentences
the Penn Arabic TreeBank (PATB) (Maamouri et al,
2004). The sentences make up the first 400 docu-
ments of that part of PATB amounting to a total of
54.5% of the PATB Part 1 data set. The task was
to annotate MSA news articles at the sentence level.
Each article has been processed such that coders are
provided sentences to label. We prepared annotation
guidelines for this SSA task focusing specifically on
the newswire genre. We summarize the guidelines
next, illustrating related and relevant literature.
3.1 SSA Categories
For each sentence, each annotator assigned one of 4
possible labels: (1) Objective (OBJ), (2) Subjective-
Positive (S-POS), (3) Subjective-Negative (S-NEG),
and (4) Subjective-Neutral (S-NEUT). We followed
(Wiebe et al, 1999) in operationalizing the subjec-
tive vs. the objective categories. In other words,
if the primary goal of a sentence is perceived to be
the objective reporting of information, it was labeled
OBJ. Otherwise, a sentence would be a candidate for
one of the three subjective classes.2 Table 1 shows
the contingency table for the two annotators judg-
ments. Overall agreement is 88.06%, with a Kappa
(k) value of 0.38.
To illustrate, a sentence such as ?The Prime Min-
ister announced that he will visit the city, saying
that he will be glad to see the injured?, has two au-
thors (the story writer and the Prime Minister indi-
rectly quoted). Accordingly to our guidelines, this
sentence should be annotated S-POS tag since the
part related to the person quoted (the Prime Minis-
2It is worth noting that even though some SSA researchers
include subjective mixed categories, we only saw such cate-
gories attested in less than < 0.005% which is expected since
our granularity level is the sentence. If we are to consider larger
units of annotation, we believe mixed categories will become
more frequent. Thus we decided to tag the very few subjective
mixed sentences as S-NEUT.
111
ter) expresses a positive subjective sentiment, ?glad?
which is a private state (i.e., a state that is not sub-
ject to direct verification) (Quirk et al, 1974).
3.2 Good & Bad News
News can be good or bad. For example, whereas
?Five persons were killed in a car accident? is bad
news, ?It is sunny and warm today in Chicago? is
good news. Our coders were instructed not to con-
sider good news positive nor bad news negative if
they think the sentences expressing them are objec-
tively reporting information. Thus, bad news and
good news can be OBJ as is the case in both exam-
ples.
3.3 Perspective
Some sentences are written from a certain perspec-
tive (Lin et al, 2006) or point of view. Consider
the two sentences (1) ?Israeli soldiers, our heroes,
are keen on protecting settlers? and (2) ?Palestinian
freedom fighters are willing to attack these Israeli
targets?. Sentence (1) is written from an Israeli per-
spective, while sentence (2) is written from a Pales-
tinian perspective. The perspective from which a
sentence is written interplays with how sentiment is
assigned. Sentence (1) is considered positive from
an Israeli perspective, yet the act of protecting set-
tlers is considered negative from a Palestinian per-
spective. Similarly, attacking Israeli targets may be
positive from a Palestinian vantage point, but will be
negative from an Israeli perspective. Coders were
instructed to assign a tag based on their understand-
ing of the type of sentiment, if any, the author of a
sentence is trying to communicate. Thus, we have
tagged the sentences from the perspective of their
authors. As it is easy for a human to identify the
perspective of an author (Lin et al, 2006), this mea-
sure facilitated the annotation task. Thus, knowing
that the sentence (1) is written from an Israeli per-
spective the annotator assigns it a S-POS tag.
3.4 Epistemic Modality
Epistemic modality serves to reveal how confident
writers are about the truth of the ideational mate-
rial they convey (Palmer, 1986). Epistemic modal-
ity is classified into hedges and boosters. Hedges
are devices like perhaps and I guess that speakers
employ to reduce the degree of liability or respon-
sibility they might face in expressing the ideational
material. Boosters3 are elements like definitely, I as-
sure that, and of course that writers or speakers use
to emphasize what they really believe. Both hedges
and boosters can (1) turn a given unit of analsysis
from objective into subjective and (2) modify polar-
ity (i.e., either strengthen or weaken it). Consider,
for example, the sentences (1) ?Gaddafi has mur-
dered hundreds of people?, (2) ?Gaddafi may have
murdered hundreds of people?, and (3) ?Unfortu-
nately, Gaddafi has definitely murdered hundreds of
people?. While (1) is OBJ, since it lacks any subjec-
tivity cues), (2) is S-NEUT because the proposition
is not presented as a fact but rather is softened and
hence offered as subject to counter-argument, (3) is
a strong S-NEG (i.e., it is S-NEG as a result of the
use of ?unfortnately?, and strong due to the use of
the booster definitely). Our annotators were explic-
itly alerted to the ways epistemic modality markers
interact with subjectivity.
3.5 Illocutionary Speech Acts
Occurrences of language expressing (e.g. apologies,
congratulations, praise, etc. are referred to as il-
locutionary speech acts (ISA) (Searle, 1975). We
believe that ISAs are relevant to the expression of
sentiment in natural language. For example, the two
categories expressives (e.g., congratulating, thank-
ing, apologizing and commisives (e.g., promising)
of (Searle, 1975)?s taxonomy of ISAs are specially
relevant to SSA. In addition, (Bach and Harnish,
1979) define an ISA as a medium of communicat-
ing attitude and discuss ISAs like banning, bidding,
indicting, penalizing, assessing and convicting. For
example, the sentence ?The army should never do
that again? is a banning act and hence is S-NEG.
Although our coders were not required to assign ISA
tags to the sentences, we have brought the the con-
cept of ISAs to their attention as we believe a good
understanding of the concept facilitates annotating
data for SSA.
3.6 Annotator?s Background Knowledge
The type of sentiment expressed may vary based
on the type of background knowledge of an annota-
3 (Polanyi and Zaenen, 2006) call these intensifiers.
112
Domain # of Cases
Politics 1186
Sports 530
Military & political violence 435
Disaster 228
Economy 208
Culture 78
Light news 72
Crime 62
This day in history 56
Total 2855
Table 2: Domains
tor/reader (Balahur and Steinberger, 2009). For ex-
ample, the sentence ?Secularists will be defeated?,
may be positive to a reader who opposes secularism.
However, if the primary intention of the author is
judged to be communicating negative sentiment, an-
notators are supposed to assign a S-NEG tag. In gen-
eral, annotators have been advised to avoid interpret-
ing the subjectivity of text based on their own eco-
nomic, social, religious, cultural, etc. background
knowledge.
4 Domain Annotation
The same two annotators also manually assigned
each sentence a domain label. The domain labels are
from the news genre and are adopted from (Abdul-
Mageed, 2008). The set of domain labels is as fol-
lows: {Light news, Military and political violence,
Sport, Politics, Crime, Economy, Disaster, Arts and
culture, This day in history}. Table 2 illustrates the
number of sentences deemed for each domain. Do-
main annotation is an easier task than subjectivity
annotation. Inter-annotator agreement for domain
label assignment is at 97%. The two coders dis-
cussed differences and a total agreement was even-
tually reached. Coders disagreed most on cases be-
longing to the Military and political violence and
Politics domains. For example, the following is
a case where the two raters disagreed (and which
was eventutally assigned a Military and political vi-
olence domain):
@PY
	
J?A? ?


k
.
YJ

	
? P 	Qk. ?

	
?

?K. A??@ Z @P
	P??@ ?


KP I. ??

??Qk Q

K@ ?K
A? PAK


@ 19 ?


	
? ?K. iJ
?

@ ?


	
Y?@ ?


PX? ?
.

?????@ ??@ ?

J???k

?XA?AK.
I. ??@ ??J
? @ ,

?J
K. C

?
	
K @
Transliteration: Tlb r}ys AlwzrA? AlsAbq fy jzr
fydjy mAhndrA $wdry Al*y OTyH bh fy 19 OyAr
mAyw Ivr Hrkp AnqlAbyp, Alywm Alsbt bIEAdp
Hkwmth IlY AlslTp.
English: Former Prime Minister of Fiji Mahendra
Chaudhry, who was ousted in May 19 after a
revolutionary movement, asked on Saturday to
return to office.
5 Examples of SSA categories from MSA
news
We illustrate examples of each category in our anno-
tation scheme. We also show and discuss examples
for each category where the annotators differed in
their annotations. Importantly, the two annotators
discussed and adjudicated together the differences.
5.1 Objective Sentences
Sentences where no opinion, sentiment, speculation,
etc. is expressed are tagged as OBJ. Typically such
sentences relay factual information, potentially
expressed by an official source, like examples 1-3
below:
(1)
?mProceedings of the 3rd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis, pages 19?28,
Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational Linguistics
SAMAR: A System for Subjectivity and Sentiment Analysis of Arabic Social
Media
Muhammad Abdul-Mageed, Sandra Ku?bler
Indiana University
Bloomington, IN, USA
{mabdulma,skuebler}@indiana.edu
Mona Diab
Columbia University
New York, NY, USA
mdiab@ccls.columbia.edu
Abstract
In this work, we present SAMAR, a sys-
tem for Subjectivity and Sentiment Analysis
(SSA) for Arabic social media genres. We
investigate: how to best represent lexical in-
formation; whether standard features are use-
ful; how to treat Arabic dialects; and, whether
genre specific features have a measurable im-
pact on performance. Our results suggest that
we need individualized solutions for each do-
main and task, but that lemmatization is a fea-
ture in all the best approaches.
1 Introduction
In natural language, subjectivity refers to aspects of
language used to express opinions, feelings, eval-
uations, and speculations (Banfield, 1982) and, as
such, it incorporates sentiment. The process of sub-
jectivity classification refers to the task of classify-
ing texts as either objective (e.g., The new iPhone
was released.) or subjective. Subjective text can
further be classified with sentiment or polarity. For
sentiment classification, the task consists of iden-
tifying whether a subjective text is positive (e.g.,
The Syrians continue to inspire the world with their
courage!), negative (e.g., The bloodbaths in Syria
are horrifying!), neutral (e.g., Obama may sign the
bill.), or, sometimes, mixed (e.g., The iPad is cool,
but way too expensive).
In this work, we address two main issues in Sub-
jectivity and Sentiment Analysis (SSA): First, SSA
has mainly been conducted on a small number of
genres such as newspaper text, customer reports,
and blogs. This excludes, for example, social me-
dia genres (such as Wikipedia Talk Pages). Second,
despite increased interest in the area of SSA, only
few attempts have been made to build SSA systems
for morphologically-rich languages (Abbasi et al,
2008; Abdul-Mageed et al, 2011b), i.e. languages
in which a significant amount of information con-
cerning syntactic units and relations is expressed at
the word-level, such as Finnish or Arabic. We thus
aim at partially bridging these two gaps in research
by developing an SSA system for Arabic, a mor-
phologically highly complex languages (Diab et al,
2007; Habash et al, 2009). We present SAMAR, a
sentence-level SSA system for Arabic social media
texts. We explore the SSA task on four different gen-
res: chat, Twitter, Web forums, and Wikipedia Talk
Pages. These genres vary considerably in terms of
their functions and the language variety employed.
While the chat genre is overridingly in dialectal Ara-
bic (DA), the other genres are mixed between Mod-
ern Standard Arabic (MSA) and DA in varying de-
grees. In addition to working on multiple genres,
SAMAR handles Arabic that goes beyond MSA.
1.1 Research Questions
In the current work, we focus on investigating four
main research questions:
? RQ1: How can morphological richness be
treated in the context of Arabic SSA?
? RQ2: Can standard features be used for SSA
for social media despite the inherently short
texts typically used in these genres?
? RQ3: How do we treat dialects?
19
? RQ4: Which features specific to social media
can we leverage?
RQ1 is concerned with the fact that SSA has
mainly been conducted for English, which has lit-
tle morphological variation. Since the features used
in machine learning experiments for SSA are highly
lexicalized, a direct application of these methods is
not possible for a language such as Arabic, in which
one lemma can be associated with thousands of sur-
face forms. For this reason, we need to investigate
how to avoid data sparseness resulting from using
lexical features without losing information that is
important for SSA. More specifically, we concen-
trate on two questions: Since we need to reduce
word forms to base forms to combat data sparseness,
is it more useful to use tokenization or lemmatiza-
tion? And given that the part-of-speech (POS) tagset
for Arabic contains a fair amount of morphological
information, how much of this information is useful
for SSA? More specifically, we investigate two dif-
ferent reduced tagsets, the RTS and the ERTS. For
more detailed information see section 4.
RQ2 addresses the impact of using two stan-
dard features, frequently employed in SSA studies
(Wiebe et al, 2004; Turney, 2002), on social media
data, which exhibit DA usage and text length vari-
ations, e.g. in twitter data. First, we investigate the
utility of applying a UNIQUE feature (Wiebe et al,
2004) where low frequency words below a thresh-
old are replaced with the token ?UNIQUE?. Given
that our data includes very short posts (e.g., twitter
data has a limit of only 140 characters per tweet),
it is questionable whether the UNIQUE feature will
be useful or whether it replaces too many content
words. Second, we test whether a polarity lexicon
extracted in a standard domain using Modern Stan-
dard Arabic (MSA) transfers to social media data.
Third, given the inherent lack of a standardized or-
thography for DA, the problem of replacing content
words is expected to be increased since many DA
content words would be spelled in different ways.
RQ3 is concerned with the fact that for Arabic,
there are significant differences between dialects.
However, existing NLP tools such as tokenizers and
POS taggers are exclusively trained on and for MSA.
We thus investigate whether using an explicit feature
that identifies the dialect of the text improves SSA
performance.
RQ4 is concerned with attempting to improve
SSA performance, which suffers from the problems
described above, by leveraging information that is
typical for social media genres, such as author or
gender information.
The rest of the paper is organized as follows: In
Section 2, we review related work. Section 3 de-
scribes the social media corpora and the polarity lex-
icon used in the experiments, Section 4 describes
SAMAR, the SSA system and the features used in
the experiments. Section 5 describes the experi-
ments and discusses the results. In Section 6, we
give an overview of the best settings for the differ-
ent corpora, followed by a conclusion in Section 7.
2 Related Work
The bulk of SSA work has focused on movie and
product reviews (Dave et al, 2003; Hu and Liu,
2004; Turney, 2002). A number of sentence- and
phrase-level classifiers have been built: For exam-
ple, whereas Yi et al (2003) present a system that
detects sentiment toward a given subject, Kim and
Hovy?s (2004) system detects sentiment towards a
specific, predefined topic. Our work is similar to Yu
and Hatzivassiloglou (2003) and Wiebe et al (1999)
in that we use lexical and POS features.
Only few studies have been performed on Arabic.
Abbasi et al (2008) use a genetic algorithm for both
English and Arabic Web forums sentiment detection
on the document level. They exploit both syntactic
and stylistic features, but do not use morphological
features. Their system is not directly comparable to
ours due to the difference in data sets.More related to
our work is our previous effort (2011b) in which we
built an SSA system that exploits newswire data. We
report a slight system improvement using the gold-
labeled morphological features and a significant im-
provement when we use features based on a polarity
lexicon from the news domain. In that work, our
system performs at 71.54% F for subjectivity classi-
fication and 95.52% F for sentiment detection. This
current work is an extension on our previous work
however it differs in that we use automatically pre-
dicted morphological features and work on data be-
longing to more genres and DA varieties, hence ad-
dressing a more challenging task.
20
3 Data Sets and Annotation
To our knowledge, no gold-labeled social media
SSA data exist. Thereby, we create annotated data
comprising a variety of data sets:
DARDASHA (DAR): (Arabic for ?chat?) com-
prises the first 2798 chat turns collected from a ran-
domly selected chat session from ?Egypt?s room? in
Maktoob chat chat.mymaktoob.com. Maktoob
is a popular Arabic portal. DAR is an Egyptian Ara-
bic subset of a larger chat corpus that was harvested
between December 2008 and February 2010.
TAGREED (TGRD): (?tweeting?) is a corpus
of 3015 Arabic tweets collected during May 2010.
TRGD has a mixture of MSA and DA. The MSA
part (TRGD-MSA) has 1466 tweets, and the dialec-
tal part (TRGD-DA) has 1549 tweets.
TAHRIR (THR): (?editing?) is a corpus of 3008
sentences sampled from a larger pool of 30 MSA
Wikipedia Talk Pages that we harvested.
MONTADA (MONT): (?forum?) comprises of
3097 Web forum sentences collected from a larger
pool of threaded conversations pertaining to differ-
ent varieties of Arabic, including both MSA and DA,
from the COLABA data set (Diab et al, 2010). The
discussions covered in the forums pertain to social
issues, religion or politics. The sentences were au-
tomatically filtered to exclude non-MSA threads.
Each of the data sets was labeled at the sentence
level by two college-educated native speakers of
Arabic. For each sentence, the annotators assigned
one of 3 possible labels: (1) objective (OBJ), (2)
subjective-positive (S-POS), (3) subjective-negative
(S-NEG), and (3) subjective-mixed (S-MIXED).
Following (Wiebe et al, 1999), if the primary goal
of a sentence is judged as the objective reporting
of information, it was labeled as OBJ. Otherwise, a
sentence was a candidate for one of the three SUBJ
classes. We also labeled the data with a number of
other metadata1 tags. Metadata labels included the
user gender (GEN), the user identity (UID) (e.g. the
user could be a person or an organization), and the
source document ID (DID). We also mark the lan-
guage variety (LV) (i.e., MSA or DA) used, tagged
at the level of each unit of analysis (i.e., sentence,
tweet, etc.). Annotators were instructed to label a
1We use the term ?metadata? as an approximation, as some
features are more related to social interaction phenomena.
Data set SUBJ GEN LV UID DID
DAR X X
MONT X X X
TRGD X X X X
THR X X
Table 1: Types of annotation labels (features) manually
assigned to the data.
tweet as MSA if it mainly employs MSA words and
adheres syntactically to MSA rules, otherwise it is
treated as dialectal. Table 1 shows the annotations
for each data set. Data statistics, distribution of
classes, and inter-annotator agreement in terms of
Kappa (K) are provided in Table 2.
Polarity Lexicon: We manually created a lexicon
of 3982 adjectives labeled with one of the following
tags {positive, negative, neutral}, as is reported in
our previous work (2011b). We focus on adjectives
since they are primary sentiment bearers. The ad-
jectives pertain to the newswire domain, and were
extracted from the first four parts of the Penn Arabic
Treebank (Maamouri et al, 2004).
4 SAMAR
4.1 Automatic Classification
SAMAR is a machine learning system for Arabic
SSA. For classification, we use SVMlight (Joachims,
2008). In our experiments, we found that linear ker-
nels yield the best performance. We perform all ex-
periments with presence vectors: In each sentence
vector, the value of each dimension is binary, regard-
less of how many times a feature occurs.
In the current study, we adopt a two-stage clas-
sification approach. In the first stage (i.e., Subjec-
tivity), we build a binary classifier to separate objec-
tive from subjective cases. For the second stage (i.e.,
Sentiment) we apply binary classification that distin-
guishes S-POS from S-NEG cases. We disregard the
neutral and mixed classes for this study. SAMAR
uses different feature sets, each of which is designed
to address an individual research question:
4.2 Morphological Features
Word forms: In order to minimize data sparse-
ness as a result of the morphological richness of
Arabic, we tokenize the text automatically. We
use AMIRA (Diab, 2009), a suite for automatic
21
Data set # instances # types # tokens # OBJ # S-POS # S-NEG # S-MIXED Kappa (K)
DAR 2,798 11,810 3,133 328 1647 726 97 0.89
MONT 3,097 82,545 20,003 576 1,101 1,027 393 0.88
TRGD 3,015 63,383 16,894 1,428 483 759 345 0.85
TRGD-MSA 1,466 31,771 9,802 960 226 186 94 0.85
TRGD-DIA 1,549 31,940 10,398 468 257 573 251 0.82
THR 3,008 49,425 10,489 1,206 652 1,014 136 0.85
Table 2: Data and inter-annotator agreement statistics.
processing of MSA, trained on Penn Arabic Tree-
bank (Maamouri et al, 2004) data, which consists
of newswire text. We experiment with two different
configurations to extract base forms of words: (1)
Token (TOK), where the stems are left as is with no
further processing of the morpho-tactics that result
from the segmentation of clitics; (2) Lemma (LEM),
where the words are reduced to their lemma forms,
(citation forms): for verbs, this is the 3rd person
masculine singular perfective form and for nouns,
this corresponds to the singular default form (typi-
cally masculine). For example, the word ??EA 	J?m