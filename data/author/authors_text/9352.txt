Proceedings of HLT/EMNLP 2005 Demonstration Abstracts, pages 4?5,
Vancouver, October 2005.
Classummary:
Introducing Discussion Summarization to Online Classrooms
Liang Zhou, Erin Shaw, Chin-Yew Lin, and Eduard Hovy
University of Southern California
Information Sciences Institute
4676 Admiralty Way
Marina del Rey, CA 90292-6695
{liangz, shaw, hovy}@isi.edu
Abstract
This paper describes a novel summariza-
tion system, Classummary, for interactive
online classroom discussions. This system
is originally designed for Open Source
Software (OSS) development forums.
However, this new application provides
valuable feedback on designing summari-
zation systems and applying them to eve-
ryday use, in addition to the traditional
natural language processing evaluation
methods. In our demonstration at HLT,
new users will be able to direct this sum-
marizer themselves.
1 Introduction
The availability of many chat forums reflects the
formation of globally dispersed virtual communi-
ties, one of which is the very active and growing
movement of Open Source Software (OSS) devel-
opment. Working together in a virtual community
in non-collocated environments, OSS developers
communicate and collaborate using a wide range
of web-based tools including Internet Relay Chat
(IRC), electronic mailing lists, and more.
Another similarly active virtual community is
the distributed education community. Whether
courses are held entirely online or mostly on-
campus, online asynchronous discussion boards
play an increasingly important role, enabling class-
room-like communication and collaboration
amongst students, tutors and instructors. The Uni-
versity of Southern California, like many other
universities, employs a commercial online course
management system (CMS).  In an effort to bridge
research and practice in education, researchers at
ISI replaced the native CMS discussion board with
an open source board that is currently used by se-
lected classes. The board provides a platform for
evaluating new teaching and learning technologies.
Within the discussion board teachers and students
post messages about course-related topics. The
discussions are organized chronologically within
topics and higher-level forums. These ?live? dis-
cussions are now enabling a new opportunity, the
opportunity to apply and evaluate advanced natural
language processing (NLP) technology.
Recently we designed a summarization system
for technical chats and emails on the Linux kernel
(Zhou and Hovy, 2005). It clusters discussions ac-
cording to subtopic structures on the sub-message
level, identifies immediate responding pairs using
machine-learning methods, and generates subtopic-
based mini-summaries for each chat log. Incorpo-
ration of this system into the ISI Discussion Board
framework, called Classummary, benefits both
distance learning and NLP communities. Summa-
ries are created periodically and sent to students
and teachers via their preferred medium (emails,
text messages on mobiles, web, etc). This relieves
users of the burden of reading through a large vol-
ume of messages before participating in a particu-
lar discussion. It also enables users to keep track of
all ongoing discussions without much effort. At the
same time, the discussion summarization system
can be measured beyond the typical NLP evalua-
4
tion methodologies, i.e. measures on content cov-
erage. Teachers and students? willingness and con-
tinuing interest in using the software will be a
concrete acknowledgement and vindication of such
research-based NLP tools. We anticipate a highly
informative survey to be returned by users at the
end of the service.
2  Summarization Framework
In this section, we will give a brief description of
the discussion summarization framework that is
applied to online classroom discussions.
One important component in the original system
(Zhou and Hovy, 2005) is the sub-message clus-
tering. The original chat logs are in-depth technical
discussions that often involve multiple sub-topics,
clustering is used to model this behavior. In Clas-
summary, the discussions are presented in an orga-
nized fashion where users only respond to and
comment on specific topics. Thus, it eliminates the
need for clustering.
 All messages in a discussion are related to the
central topic, but to varying degrees. Some are an-
swers to previously asked questions, some make
suggestions and give advice where they are re-
quested, etc. We can safely assume that for this
type of conversational interactions, the goal of the
participants is to seek help or advice and advance
their current knowledge on various course-related
subjects. This kind of interaction can be modeled
as one problem-initiating message and one or more
corresponding problem-solving messages, formally
defined as Adjacent Pairs (AP). A support vector
machine, pre-trained on lexical and structural fea-
tures for OSS discussions, is used to identify the
most relevant responding messages to the initial
post within a topic.
Having obtained all relevant responses, we
adopt the typical summarization paradigm to ex-
tract informative sentences to produce concise
summaries. This component is modeled after the
BE-based multi-document summarizer (Hovy et
al., 2005). It consists of three steps. First, impor-
tant basic elements (BEs) are identified according
to their likelihood ratio (LR). BEs are automati-
cally created minimal semantic units of the form
head-modifier-relation (for example, ?Libyans |
two | nn?, ?indicted | Libyans | obj?, and ?indicted
| bombing | for?). Next, each sentence is given a
score which is the sum of its BE scores, computed
in the first step, normalized by its length. Lastly,
taking into consideration the interactions among
summary sentences, a MMR (Maximum Marginal
Relevancy) model (Goldstein et al, 1999) is used
to extract sentences from the list of top-ranked
sentences computed from the second step.
3 Accessibility
Classummary is accessible to students and teachers
while classes are in session. At HLT, we will dem-
onstrate an equivalent web-based version. Discus-
sions are displayed on a per-topic basis; and
messages belonging to a specific discussion are
arranged in ascending order according to their
timestamps. While viewing a new message on a
topic, the user can choose to receive a summary of
the discussion so far or an overall summary on the
topic. Upon receiving the summary (for students,
at the end of an academic term), a list of questions
is presented to the user to gather comments on
whether Classummary is useful. We will show the
survey results from the classes (which will have
concluded by then) at the conference.
References
Hovy, E., C.Y. Lin, and L. Zhou. 2005. A BE-based
multi-document summarizer with sentence compres-
sion. To appear in Proceedings of Multilingual Sum-
marization Evaluation (ACL 2005), Ann Arbor, MI.
Goldstein, J., M. Kantrowitz, V. Mittal, and J. Car-
bonell. Summarizing Text Documents: Sentence Se-
lection and Evaluation Metrics. Proceedings of the
22nd International ACM Conference on Research and
Development in Information Retrieval (SIGIR-99),
Berkeley, CA, 121-128.
Zhou, L. and E. Hovy. 2005. Digesting virtual ?geek?
culture: The summarization of technical internet re-
lay chats. To appear in Proceedings of Association of
Computational Linguistics (ACL 2005), Ann Arbor,
MI.
5
A Web-Trained Extraction Summarization System 
 
Liang Zhou and Eduard Hovy 
USC Information Sciences Institute 
4676 Admiralty Way 
Marina del Rey, CA 90292-6695 
{liangz, hovy}@isi.edu 
 
 
 
 
Abstract 
 
A serious bottleneck in the development of 
trainable text summarization systems is the 
shortage of training data. Constructing such 
data is a very tedious task, especially because 
there are in general many different correct 
ways to summarize a text. Fortunately we can 
utilize the Internet as a source of suitable 
training data. In this paper, we present a 
summarization system that uses the web as the 
source of training data. The procedure involves 
structuring the articles downloaded from 
various websites, building adequate corpora of 
(summary, text) and (extract, text) pairs, 
training on positive and negative data, and 
automatically learning to perform the task of 
extraction-based summarization at a level 
comparable to the best DUC systems.  
 
 
1    Introduction 
 
    The task of an extraction-based text summarizer is to 
select from a text the most important sentences that are 
in size a small percentage of the original text yet still as 
informative as the full text (Kupiec et al, 1995). 
Typically, trainable summarization systems characterize 
each sentence according to a set of predefined features 
and then learn from training material which feature 
combinations are indicative of good extract sentences. 
In order to learn the characteristics of indicative 
summarizing sentences, a large enough collection of 
(summary, text) pairs must be provided to the system.  
    Research in automated text summarization is 
constantly troubled by the difficulty of finding or 
constructing large collections of (extract, text) pairs. 
Usually, (abstract, text) pairs are available and can be 
easily obtained (though not in sufficient quantity to 
support fully automated learning for large domains). But 
abstract sentences are not identical to summary 
sentences and hence make direct comparison difficult. 
Therefore, some algorithms have been introduced to 
generate (extract, text) pairs expanded from (abstract, 
text) inputs (Marcu, 1999).  
    The explosion of the World Wide Web has made 
accessible billions of documents and newspaper articles. 
If one could automatically find short forms of longer 
documents, one could build large training sets over 
time. However, one cannot today retrieve short and long 
texts on the same topic directly.  
    News published on the Internet is an exception. 
Although it is not ideally organized, the topic 
orientation and temporal nature of news makes it 
possible to impose an organization and thereby obtain a 
training corpus on the same topic. We hypothesize that 
weekly articles are sophisticated summaries of daily 
ones, and monthly articles are summaries of weekly 
ones, as shown in Figure 1. Under this hypothesis, how 
accurate an extract summarizer can one train? In this 
paper we first describe the corpus reorganization, then 
in Section 3 the training data formulation and the 
system, the system evaluation in Section 4, and finally 
future work in Section 5.  
 
 daily 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
weekly 
monthly 
Figure 1. Corpus structure. 
                                                               Edmonton, May-June 2003
                                                             Main Papers , pp. 205-211
                                                         Proceedings of HLT-NAACL 2003
2    Corpus Construct
    
2.1 Download Initial Collection 
 
    The Yahoo Full Coverage Collection (YFCC) was 
downloaded from http://fullcoverage.yahoo.com during 
December 2001. The full coverage texts were 
downloaded based on a snapshot of the links
in Yahoo Full Coverage at that time. A spid
the top eight categories: U.S., World, Business, 
Technolo tertainment, 
Sports. A y were saved i
index page that contained the headline and its full text 
URL. A page fetcher then nloaded all the pag
listed in the snapshot index
    Under the eight ca
subcategories, 216590 new
 
2.2 Preprocessing 
 
    All the articles in the 
following. Each article is i
with actual contents buried
and markings. Identifying t
process (Finn et al, 2001
main body of the article
templates, and then f
information embedded in th
each opening and closing ta
name indicates the conten
closing tags are images or j
discarded.  
    The clean texts are the
breaker, Lovin?s stemmer, 
converted into standard XM
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
organization 
    The news articles posted under Yahoo Full Coverage 
are from 125 different web publishers. Except for some 
well-known sites, the publishing frequencies for the rest 
of the sites are not known. But Yahoo tends to use those 
publishers over and over again, leaving for each 
r a tra ublishing habit. Our system records 
ishing or each article from each publisher 
chronologically, and then calculates the publishing 
frequency for each publisher. Over all the articles from 
blisher,  comp the minimum 
publishing gap (MPG) between two articles. If the MPG 
is less than 3 days or the MPG is unknown in the case of 
publishers seen  once in the YFCC, then this 
3           6        7  dow
 file.  
tegories, there are 46
s articles.  
YFCC are preprocessed 
n the original raw html for
 in layers of irrelevant ta
he text body is a challengin
). The system identifies t
 using a set of retriev
urther eliminates usele
e main body by considerin
g set. For example, if the t
ts between the opening an
ust meta-info, the contents
n processed by a senten
a part-of-speech tagger, an
L form.  
A
1999 
       8     10 1
1       2 
Figure 2: The hierarchicaes 3 
as 
m 
gs 
g 
he 
al 
ss 
g 
ag 
d 
 is 
ce 
d 
publisher is labe
greater than 3 d
weekly publishe
are labeled as mo
    For each artic
it as a daily, w
domain under e
restructured into
year, weeks of 
week. The vis
hierarchical stru
category World i
 
3 System 
 
    Recognizing 
from the web re
constant shortag
taking a closer e
one notices that f
 
 
 
 
 
 
 
frica 
2000
 
 
 
 
 
 
 
 
 
 
 
 
 
    2    3    ? 1
        3            4 W
Day
Articles 
l structure for domain onlygy, Science, Health, En
ll news links in each categorand 
n an a pu the systemled as a daily pub
ays but less than 
r. Publishers with 
nthly publishers. 
le in the collectio
eekly, or month
ach category in t
 a hierarchy by ye
each month, and 
ualization of a
cture of the d
s shown in Figure
(summary, text) 
pository is the ke
e of summarizatio
xamination of th
or each day, there
2001
    2    3    ? 
eek
Africa.  utes  contained 
er crawled 
publishe
the publil of p
 date fion 2.3 Chronological Re
 lisher. If the MPG is 
15, it is labeled as a 
all other MPG values 
 
n, the system relabels 
ly publication. Each 
he collection is then 
ar, months within the 
finally days of each 
n example of the 
omain Africa under 
 2.  
pairs automatically 
y to overcoming the 
n training data. After 
e reorganized YFCC, 
 are a number of       
Year 
Month 
articles published that update the progress of a particular 
news topic. Daily articles are published by identified 
daily publishers. Then at the end of each week, there are 
several weekly articles published by weekly publishers 
on the same topic. At the end of each month, again there 
are articles on the same topic posted by publishers 
labeled as monthly publishers. There is a common 
thematic connection between the daily articles and the 
weekly articles, and between the weekly articles and the 
monthly articles. The daily articles on a particular event 
are more detailed, and are written step-by-step as it was 
happening. The weekly articles review the daily articles 
and recite important snippets from the daily news. The 
monthly articles are written in a more condensed 
fashion quoting from the weeklies. 
    Instead of asking human judges to identify 
informative sentences in documents, and since 
beautifully written ?summaries? are already available, 
we need to align the sentences from the daily articles 
with each weekly article sentence, and align weekly 
article sentences with each monthly article sentence, in 
order to collect the (summary, text) pairs and eventually 
generate (extract, text) pairs. The pairs are constructed 
at both sentence and document levels. 
 
3.1 Alignment 
 
    In our system, three methods for sentence-level and 
document-level alignment are investigated: 
? extraction-based: Marcu (1999) introduces an 
algorithm that produces corresponding extracts 
given (abstract, text) tuples with maximal 
semantic similarity. We duplicated this algorithm 
but replaced inputs with (summary, text), parents 
and their respective children in the hierarchical 
domain tree. Thus for example, the summary is a 
monthly article when the text is a weekly article or 
a weekly article when the text is a daily one. We 
start with the cosine-similarity metric stated in 
(Marcu 1999) and keep deleting sentences that are 
not related to the summary document until any 
more deletion would result in a drop in similarity 
with the summary. The resulting set of sentences is 
the extract concerning the topic discussed in the 
summary. It forms the pair (extract, text). If there 
is more than one summary for a particular text 
(nonsummary article), the resulting extracts will 
vary if the summary articles are written on the 
same event, but are focused on different 
perspectives. Thus, a summary article may be 
aligned with several extracts and extracts 
generated from a single text may align with many 
summaries. The relationship amongst summaries, 
extracts, and texts forms a network topology.  
    To generate sentence level alignment, we 
replaced the input with (summary sentence, text) 
pairs. Starting  with  a  nonsummary text, 
the sentences that are irrelevant to the summary 
sentence are deleted repeatedly, resulting in the 
preservation of sentences similar in meaning to the 
summary sentence. For each sentence in the 
summary, it is aligned with a number of 
nonsummary sentences to form (summary 
sentence, nonsummary sentences) pairs.  This 
alignment is done for each sentence of the 
summary articles. Finally for each nonsummary 
we group together all the aligned sentences to form 
the pair (extract, text).  
? similarity-based: inspired by sentence alignment 
for multilingual parallel corpora in Machine 
Translation (Church, 1993; Fung and Church, 
1994; Melamed, 1999), we view the alignment 
between sentences from summaries and sentences 
from nonsummaries as the alignment of 
monolingual parallel texts at the sentence level. In 
every domain of the YFCC, each article is 
represented as a vector in a vector space where 
each dimension is a distinct non-stop word 
appearing in this domain. Measuring the cosine-
similarity between two articles, we can decide 
whether they are close semantically. This method 
has been widely used in Information Retrieval 
(Salton, 1975). To extend this idea, we measure 
the cosine-similarity between two sentences, one 
from a summary (weekly or monthly article) and 
the other one from a nonsummary (daily or weekly 
article). If the similarity score between the two 
crosses a predetermined threshold, the two 
sentences are aligned to form the pair (summary 
sentence, text sentence). The relationship between 
sentences is many-to-many. With any particular 
nonsummary article, sentences that are aligned 
with summary sentences form the extract and the 
pair (extract, text).  
? summary-based: concerned with the noise that 
may accompany similarity calculations from 
extraction-based and similarity-based alignments, 
we align an entire summary article with all its 
nonsummary articles published in the same time 
period, as determined from the previously 
described chronological reorganization. The 
alignment results are pairs of the format (summary, 
texts). One summary can only be aligned with a 
certain group of nonsummaries. Each nonsummary 
can be aligned with many summaries. No sentence 
level alignment is done with this method. 
 
 
 
 
3.2 Training Data 
 
    The main goal of a leaning-based extraction 
summarization system is to learn the ability to judge 
whether a particular sentence in a text appear in the 
extract or not. Therefore, two sets of training data are 
needed, one indicative enough for the system to select a 
sentence to be in the extract (labeled as positive data), 
the other indicative enough for the system to keep the 
sentence from being added to the extract (labeled as 
negative data). For each of the alignment methods, we 
produce summary training data and nonsummary 
training data for each domain in the YFCC. 
    From extraction-based and similarity-based alignment 
methods, for each nonsummary article, there are two 
sets of sentences, the set of sentences that compose the 
extract with the respect to some summary article or 
align with summary sentences, and the rest of the 
sentences that are not related to the summary or aligned. 
The two sets of sentences over all articles in the domain 
form the positive and negative training data sets.  
    Using summary-based alignment, all the summary 
articles are in the positive training set, and all the 
nonsummary material is in the negative set. Full texts 
are used.  
 
3.3 Bigram Estimates Extract Desirability 
 
    We treat each domain independently. Using a bigram 
model, we estimate the desirability of a sentence 
appearing in the extract P(S) from the summary training 
data as: 
 
    P(S) = P(w1 | start) P(w2 | w1)?P(wn | wn-1) 
 
  We estimate the desirability of a sentence not 
appearing in the extract P?(S) from the nonsummary 
training data as: 
     
    P?(S) = P?(w1 | start) P?(w2 | w1)?P?(wn | wn-1) 
 
    For each domain in the YFCC, a summary bigram 
table and a nonsummary bigram table are created.  
 
3.4 Extraction Process 
 
    Zajic et al (2002) used a Hidden Markov Model as 
part of their headline generation system. In our system, 
we started with a similar idea of a lattice for summary 
extraction. In Figure 3, E states emit sentences that are 
going to be in the extract, and N states emit all other 
sentences. Given an input sentence, if P(S) is greater 
than P?(S), it means that the sentence has a higher 
desirability of being an extraction sentence; otherwise, 
the sentence will not be included in the resulting extract.  
 
 
 
 
 
 
 
 
 
 
After reading in the last sentence from the input, the 
extract is created by traversing the path from start state 
to end state and only outputting the sentences emitted 
by the E states.  
    The extracts generated are in size shorter than the 
original texts. However, the number of sentences that E 
states emit cannot be predetermined. This results in 
unpredictable extract length. Most frequently, longer 
extracts are produced. The system needs more control 
over how long extracts will be in order for meaningful 
evaluation to be conducted.  
    To follow up on the lattice idea, we used the 
following scoring mechanism: 
 
R = P(S) / P?(S) 
 
R indicates the desirability ratio of the sentence being in 
the extract over it being left out. For each sentence from 
the input, it is assigned an R score. Then all the 
sentences with their R scores are sorted in descending 
order. With respect to the length restriction, we choose 
only the top n R-scored sentences.  
 
3.5 Selecting the Training Domain 
 
    There are 463 domains under the 8 categories of 
YFCC, meaning 463 paired summary-bigram and 
nonsummary-bigram tables. On average for each 
domain, the summary-bigram table contains 20000 
entries; the nonsummary-bigram table contains 173000 
entries. When an unknown text or a set of unknown 
texts come in to be summarized, the system needs to 
select the most appropriate pair of bigram tables to 
create the extract. The most desirable domain for an 
unknown text or texts contains articles focusing on the 
same issues as the unknown ones. Two methods are 
used: 
? topic signature (Lin and Hovy, 2000): a topic 
signature is a family of related terms {topic, 
signature}, where topic is the target concept and 
signature is a vecto  related ms. The topic in 
e formula is assigned with the domain ame. To 
nstruct the set of related words, w consider 
N2 N3 N1
es
E2 E3 E1 
Figure 3. Lattice. th
co
only nou because
major issues discus
those issues evolver of are on
sed in the d
d. Each n terinterested in the ns  we ly 
omain, n
oun in th n
e ot in how 
e domain 
receives a tf.idf score. 30 top-scoring nouns are 
selected to be the signature representing the 
domain. For each test text, its signature is 
computed with the same tf.idf method against each 
domain. The domain that has the highest number 
of overlaps in signature words is selected and its 
bigram tables are used to construct the extract of 
the test text. The following table illustrates. Inputs 
are three sets of 10 documents each from the 
DUC01 training corpus concerning the topics on 
Africa, earthquake, and Iraq, respectively. The 
scores are the total overlaps between a domain and 
each individual test set. The Three sets are all 
correctly classified.  
 
domain/input Africa Earthquake Iraq 
Africa 24 10 9 
Earthquake 7 20 8 
Iraq 48 8 97 
 
? hierarchical signature: each domain is given a 
name when it was downloaded. The name gives a 
description of the domain at the highest level. 
Since the name is the most informative word, if we 
gather the words that most frequently co-occur 
within the sentence(s) that contain the name itself, 
a list of less informative but still important words 
can become part of the domain signature. Using 
this list of words, we find another list of words that 
most frequently co-occur with each of them 
individually. Therefore, a three-layer hierarchical 
domain signature can be created: level one, the 
domain name; level two, 10 words with the highest 
co-occurrences with the domain name; level three, 
10 words that most frequently co-occur with level 
two signatures. Again only nouns are considered. 
For example, for domain on Iraq, the level one 
signature is ?Iraq?; level two signatures are 
?Saddam?, ?sanction?, ?weapon?, ?Baghdad?, and 
etc.; third level signatures are ?Gulf?, ?UN?, 
?Arab?, ?security?, etc. The document signature 
for the test text is computed the same way as in the 
topic signature method. Overlap between the 
domain signature and the document signature is 
computed with a different scoring system, in 
which the weights are chosen by hand. If level one 
is matched, add 10 points; for each match at level 
two, add 2 points; for each match at level three, 
add 1 point. The domain that receives the highest 
points will be selected. A much deeper signature 
hierarchy can be created recursively. Through 
experiment, we see that a three-level signature 
suffices. The following table shows the effects of 
this method: 
 
domain/input Africa Earthquake Iraq 
Africa 86 7 41 
Earthquake 7 74 0 
Iraq 15 26 202 
 
    Since it worked well for our test domains, we 
employed the topic-signature method in selecting 
training domains.  
 
4 Evaluation 
 
4.1 Alignment Choice 
 
    To determine which of the alignment methods of 
Section 3.1 is best, we need true summaries, not 
monthly or weekly articles from the web. We tested the 
equivalencies of the three methods on three sets of 
articles from the DUC01 training corpus, which 
includes human-generated ?gold standard? summaries. 
They are on the topics of Africa, earthquake, and Iraq. 
The following table shows the results of this 
experiment. Each entry demonstrates the cosine 
similarity, using the tf.idf score, of the extracts 
generated by the system using training data created from 
the alignment method in the column, compare to the 
summaries generated by human.  
 
 extraction similarity summary 
Africa 0.273 0.304 0.293 
Earthquake 0.318 0.332 0.342 
Iraq 0.234 0.246 0.247 
 
    We see that all three methods produce roughly equal 
extracts, when compared with the gold standard 
summaries. The summary-based alignment method is 
the least time consuming and the most straightforward 
method to use in practice.  
 
4.2 System Performance 
 
    There are 30 directories in the DUC01 testing corpus. 
All articles in each directory are used to make the 
selection of its corresponding training domain, as 
described in Section 3.5. Even if no domain completely 
covers the event, the best one is selected by the system.  
    To evaluate system performance on summary 
creation, we randomly selected one article from each 
directory from the DUC01 testing corpus, for each 
article, there are three human produced summaries. Our 
system summarizes each article three times with the 
length restriction respectively set to the lengths of the 
three human summaries. We also evaluated the DUC01 
single-document summarization baseline system results 
(first 100 words from each document) to set a lower 
bound. To see the upper bound, each human generated 
summary is judged against the other two human 
summaries on the same article. DUC01 top performer, 
system from SMU, in single-document summarization, 
was also evaluated. In all, 30 * 3! human summary 
judgments, 30 * 3 baseline summary judgments, 30 
SMU system judgments, and 30 * 3 system summary 
judgments are made. The following table is the 
evaluation results using the SEE system version 1.0 (Lin 
2002), with visualization in Figure 4. Summary model 
units are graded as full, partial, or none in completeness 
in coverage with the peer model units. And Figure 5 
shows an example of the comparison between the 
human-created summary and the system-generated 
extract.  
 
 SRECALL SPRECISON LRECALL LPRECISION 
Baseline 0.246 0.306 0.301 0.396 
System 0.452 0.341 0.577 0.509 
SMU 0.499 0.482 0.583 0.672 
Human 0.542 0.500 0.611 0.585 
 
Performance Evaluation
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
SRECALL SPRECI LRECALL LPRECI
baseline
system
SMU
human
 
 
 
 
    The performance is reported on four metrics. Recall 
measures how well a summarizer retains original 
content. Precision measures how well a system 
generates summaries. SRECALL and SPRECISION are 
the strict recall and strict precision that take into 
consideration only units with full completeness in unit 
coverage. LRECALL and LPRECISION are the lenient 
recall and lenient precision that count units with partial 
and full completeness in unit coverage. Extract 
summaries that are produced by our system has 
comparable performance in recall with SMU, meaning 
that the coverage of important information   is   good.   
But    our   system   shows weakness in precision due to 
the fact that each sentence in the system-generated 
extract is not compressed in any way. Each sentence in 
the extract has high coverage over the human summary. 
But sentences that have no value have also been 
included in the result. This causes long extracts on 
average, hence, the low average in precision measure. 
Since our sentence ranking mechanism is based on 
desirability, sentences at the end of the extract are less 
desirable and can be removed. This needs further 
investigation. Clearly there is the need to reduce the size 
of the generated summaries. In order to produce simple 
and concise extracts, sentence compression needs to be 
performed (Knight and Marcu, 2000).  
    Despite the problems, however, our system?s 
performance places it at equal level to the top-scoring 
systems in DUC01. Now that the DUC02 material is 
also available, we will compare our results to their top-
scoring system as well.  
 
4.3 Conclusion 
 
    One important stage in developing a learning-based 
extraction summarization system is to find sufficient 
and relevant collections of (extract, text) pairs. This task 
is also the most difficult one since resources of 
constructing the pairs are scarce. To solve this 
bottleneck, one wonders whether the  web can be seen 
as a vast repository that is waiting to be tailored in order 
to fulfill our quest in finding summarization training 
data. We have discovered a way to find short forms of 
longer documents and have built an extraction-based 
summarizer learning from reorganizing news articles 
from the World Wide Web and performing at a level 
comparable to DUC01 systems. We are excited about 
the power of how reorganization of the web news 
articles has brought us and will explore this idea in other 
tasks of natural language processing.  
 
5 Future Work 
 
Figure 4. System performance.         Multi-document summarization naturally comes 
into picture for future development. Our corpus 
organization itself is in the form of multiple articles 
being summarized into one (monthly or weekly). How 
do we learn and use this structure to summarize a new 
set of articles? 
    Headline generation is another task that we can 
approach equipped with our large restructured web 
corpus.  
    We believe that the answers to these questions are 
embedded in the characteristics of the corpus, namely 
the WWW, and are eager to discover them in the near 
future.  
 
Acknowledgement 
 
    We want to thank Dr. Chin-Yew Lin for making the 
Yahoo Full Coverage Collection download.  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
References 
 
Kenneth W. Church. 1993. Char align: A program for 
aligning parallel texts at the character level. In 
Proceedings of the Workshop on Very Large 
Corpora: Academic and Industrial Perspectives, 
ACL. Association for Computational Linguistics, 
1993. 
 
Aidan Finn, Nicholas Kushmerick, and Barry Smyth. 
2001. Fact or fiction: Content classification for 
digital libraries. In Proceedings of NSF/DELOS 
Workshop on Personalization and Recommender.  
 
Pascale Fung and Kenneth W. Church. 1994. K-vec: A 
new approach for aligning parallel texts. In 
Proceedings from the 15th International Conference 
on Computational Linguistics, Kyoto. 
 
Kevin Knight and Daniel Marcu (2000). Statistics-
Based Summarization--Step One: Sentence 
Compression. The 17th National Conference of the 
American Association for Artificial Intelligence 
AAAI'2000, Outstanding Paper Award, Austin, 
Texas, July 30-August 3, 2000. 
 
Julian Kupiec, Jan Pedersen, and Francine Chen. 1995. 
A trainable document summarizer. In SIGIR ?95, 
Proceedings of the 18th Annual International ACM 
SIGIR Conference on Research and Development in 
Information Retrieval. Seattle, Washington, USA., 
pages 68-73. ACM Press. 
 
 Human-produced summary System-produced summary 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
A major earthquake registering 7.2 on the Richter 
scale shook the Solomon Islands in the South 
Pacific today .  
 
It was the largest earthquake in the Solomons since 
a 7.4 quake on Nov . 5 , 1978 and the strongest in 
the world in the five months .  
 
An 8.3 quake hit the Macquarie Islands south of 
Australia on May 23 .  
 
The preliminary reading of 7.2 is slightly stronger 
than the 7.1 magnitude earthquake that hit the San 
Francisco Bay area Oct . 17 .  
 
Major earthquakes in the Solomons usually don't 
cause much damage or many casualties because 
the area is sparsely populated and not extensive
developed .
ly 
 
A major earthquake registering 7.2 on the Richter scale 
shook the Solomon Islands in the South Pacific today, 
the U.S. Geological Survey says. 
 
The preliminary reading of 7.2 is slightly stronger than 
the 7.1 magnitude earthquake that hit the San Francisco 
Bay area Oct. 17. 
 
It was the largest earthquake in the Solomons since a 7.4 
quake on Nov. 5, 1978. 
 
There were no immediate reports of injury or damage. 
 
An 8.3 quake hit the Macquarie Islands south of 
Australia on May 23. 
 
The Richter scale is a measure of ground motion as 
recorded on seismographs. 
 
Thus a reading of 7.5 reflects an earthquake 10 times 
stronger than one of 6.5. 
 
 Figure 5. Comparison of summaries generated by human and system.  
 
Chin-Yew Lin. 2001. Summary evaluation environment. 
http://www.isi.edu/~cyl/SEE. 
 
Chin-Yew Lin and Eduard Hovy. 2000. The automated 
acquisition of topic signatures for text 
summarization. In Proceedings of the 18th 
International Conference on Computational 
Linguistics (COLING 2000), Saarbr?cken, Germany, 
July 31- August 4, 2000. 
 
   Daniel Marcu. 1999. The automatic construction of 
large-scale corpora for summarization research. The 
22nd International ACM SIGIR Conference on 
Research and Development in Information Retrieval 
(SIGIR'99), pages 137-144, Berkeley, CA, August 
1999.  
 
I. Dan Melamed. 1999. Bitext maps and alignment via 
pattern recognition. Computational Linguistics, 
25(1):107-130. 
 
Gerard Salton. 1975. A vector space model for 
information retrieval. Communications of the ACM, 
18(11):613-620, November 1975.  
 
David Zajic, Bonnie Dorr, and Richard Schwartz. 2002. 
Automatic headline generation for newspaper 
stories. In Proceedings of the ACL-2002 Workshop 
on Text Summarization, Philadelphia, PA, 2002. 
 
 
 
 
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 447?454,
New York, June 2006. c?2006 Association for Computational Linguistics
ParaEval: Using Paraphrases to Evaluate Summaries Automatically
Liang Zhou, Chin-Yew Lin, Dragos Stefan Munteanu, and Eduard Hovy
University of Southern California
Information Sciences Institute
4676 Admiralty Way
Marina del Rey, CA 90292-6695
{liangz, cyl, dragos, hovy} @isi.edu
Abstract
ParaEval is an automated evaluation
method for comparing reference and peer
summaries. It facilitates a tiered-
comparison strategy where recall-oriented
global optimal and local greedy searches
for paraphrase matching are enabled in
the top tiers. We utilize a domain-
independent paraphrase table extracted
from a large bilingual parallel corpus us-
ing methods from Machine Translation
(MT). We show that the quality of ParaE-
val?s evaluations, measured by correlating
with human judgments, closely resembles
that of ROUGE?s.
1 Introduction
Content coverage is commonly measured in sum-
mary comparison to assess how much information
from the reference summary is included in a peer
summary. Both manual and automatic methodolo-
gies have been used. Naturally, there is a great
amount of confidence in manual evaluation since
humans can infer, paraphrase, and use world
knowledge to relate text units with similar mean-
ings, but which are worded differently. Human
efforts are preferred if the evaluation task is easily
conducted and managed, and does not need to be
performed repeatedly. However, when resources
are limited, automated evaluation methods become
more desirable.
For years, the summarization community has
been actively seeking an automatic evaluation
methodology that can be readily applied to various
summarization tasks. ROUGE (Lin and Hovy,
2003) has gained popularity due to its simplicity
and high correlation with human judgments. Even
though validated by high correlations with human
judgments gathered from previous Document Un-
derstanding Conference (DUC) experiments, cur-
rent automatic procedures (Lin and Hovy, 2003;
Hovy et al, 2005) only employ lexical n-gram
matching. The lack of support for word or phrase
matching that stretches beyond strict lexical
matches has limited the expressiveness and utility
of these methods. We need a mechanism that sup-
plements literal matching?i.e. paraphrase and
synonym?and approximates semantic closeness.
In this paper we present ParaEval, an automatic
summarization evaluation method, which facili-
tates paraphrase matching in an overall three-level
comparison strategy. At the top level, favoring
higher coverage in reference, we perform an opti-
mal search via dynamic programming to find
multi-word to multi-word paraphrase matches be-
tween phrases in the reference summary (usually
human-written) and those in the peer summary
(system-generated). The non-matching fragments
from the previous level are then searched by a
greedy algorithm to find single-word para-
phrase/synonym matches. At the third and the low-
est level, we perform literal lexical unigram
matching on the remaining texts. This tiered design
for summary comparison guarantees at least a
ROUGE-1 level of summary content matching if
no paraphrases are found.
The first two levels employ a paraphrase table.
Since manually created multi-word paraphrases-
?phrases determined by humans to be paraphrases
of one another?are not available in sufficient
quantities, we automatically build a paraphrase
447
table using methods from the Machine Translation
(MT) field. The assumption made in creating this
table is that if two English phrases are translated
into the same foreign phrase with high probability
(shown in the alignment results from a statistically
trained alignment algorithm), then the two English
phrases are paraphrases of each other.
This paper is organized in the following way:
Section 2 introduces previous work in summariza-
tion evaluation; Section 3 describes the motivation
behind this work; paraphrase acquisition is dis-
cussed in Section 4; Section 5 explains in detail
our summary comparison mechanism; Section 6
validates ParaEval with human summary judg-
ments; and we conclude and discuss future work in
Section 7.
2 Previous Work
There has been considerable work in both manual
and automatic summarization evaluations. Three
most noticeable efforts in manual evaluation are
SEE (Lin and Hovy, 2001), Factoid (Van Halteren
and Teufel, 2003), and the Pyramid method
(Nenkova and Passonneau, 2004).
SEE provides a user-friendly environment in
which human assessors evaluate the quality of
system-produced peer summary by comparing it to
a reference summary. Summaries are represented
by a list of summary units (sentences, clauses,
etc.). Assessors can assign full or partial content
coverage score to peer summary units in compari-
son to the corresponding reference summary units.
Grammaticality can also be graded unit-wise.
The goal of the Factoid work is to compare the
information content of different summaries of the
same text and determine the minimum number of
summaries, which was shown through experimen-
tation to be 20-30, needed to achieve stable con-
sensus among 50 human-written summaries.
The Pyramid method uses identified consen-
sus?a pyramid of phrases created by annota-
tors?from multiple reference summaries as the
gold-standard reference summary. Summary com-
parisons are performed on Summarization Content
Units (SCUs) that are approximately of clause
length.
To facilitate fast summarization system design-
evaluation cycles, ROUGE was created (Lin and
Hovy, 2003). It is an automatic evaluation package
that measures a number of n-gram co-occurrence
statistics between peer and reference summary
pairs. ROUGE was inspired by BLEU (Papineni et
al., 2001) which was adopted by the machine
translation (MT) community for automatic MT
evaluation. A problem with ROUGE is that the
summary units used in automatic comparison are
of fixed length. A more desirable design is to have
summary units of variable size. This idea was im-
plemented in the Basic Elements (BE) framework
(Hovy et al, 2005) which has not been completed
due to its lack of support for paraphrase matching.
Both ROUGE and BE have been shown to corre-
late well with past DUC human summary judg-
ments, despite incorporating only lexical matching
on summary units (Lin and Hovy, 2003; Hovy et
al., 2005).
3 Motivation
3.1 Paraphrase Matching
An important difference that separates current
manual evaluation methods from their automatic
counterparts is that semantic matching of content
units is performed by human summary assessors.
An essential part of the semantic matching in-
volves paraphrase matching?determining whether
phrases worded differently carry the same semantic
information. This paraphrase matching process is
observed in the Pyramid annotation procedure
shown in (Nenkova and Passonneau, 2004) over
three summary sets (10 summaries each). In the
example shown in Figure 1 (reproduced from
Pyramid results), each of the 10 phrases (numbered
1 to 10) extracted from summary sentences carries
the same semantic content as the overall summary
content unit labeled SCU1 does. Each extracted
phrase is identified as a summary content unit
(SCU). In our work in building an automatic
evaluation procedure that enables paraphrase
SCU1: the crime in question was the Lockerbie {Scotland} bombing
1 [for the Lockerbie bombing]
2 [for blowing up] [over Lockerbie, Scotland]
3 [of bombing] [over Lockerbie, Scotland]
4 [was blown up over Lockerbie, Scotland, ]
5 [the bombing of Pan Am Flight 103]
6 [bombing over Lockerbie, Scotland, ]
7 [for Lockerbie bombing]
8 [bombing of Pan Am flight 103 over Lockerbie. ]
9 [linked to the Lockerbie bombing]
10 [in the Lockerbie bombing case. ]
Figure 1. Paraphrases created by Pyramid annotation.
448
matching, we aim to automatically identify these
10 phrases as paraphrases of one another.
3.2 Synonymy Relations
Synonym matching and paraphrase matching are
often mentioned in the same context in discussions
of extending current automated summarization
evaluation methods to incorporate the matching of
semantic units. While evaluating automatically
extracted paraphrases via WordNet (Miller et al,
1990), Barzilay and McKeown (2001) quantita-
tively validated that synonymy is not the only
source of paraphrasing. We envisage that this
claim is also valid for summary comparisons.
From an in-depth analysis on the manually cre-
ated SCUs of the DUC2003 summary set D30042
(Nenkova and Passonneau, 2004), we find that
54.48% of 1746 cases where a non-stop word from
one SCU did not match with its supposedly hu-
man-aligned pairing SCUs are in need of some
level of paraphrase matching support. For example,
in the first two extracted SCUs (labeled as 1 and 2)
in Figure 1??for the Lockerbie bombing? and ?for
blowing up ? over Lockerbie, Scotland??no
non-stop word other than the word ?Lockerbie?
occurs in both phrases. But these two phrases were
judged to carry the same semantic meaning be-
cause human annotators think the word ?bombing?
and the phrase ?blowing up? refer to the same ac-
tion, namely the one associated with ?explosion.?
However, ?bombing? and ?blowing up? cannot be
matched through synonymy relations by using
WordNet, since one is a noun and the other is a
verb phrase (if tagged within context). Even when
the search is extended to finding synonyms and
hypernyms for their categorical variants and/or
using other parts of speech (verb for ?bombing?
and noun phrase for ?blowing up?), a match still
cannot be found.
To include paraphrase matching in summary
evaluation, a collection of less-strict paraphrases
must be created and a matching strategy needs to
be investigated.
4 Paraphrase Acquisition
Paraphrases are alternative verbalizations for con-
veying the same information and are required by
many Natural Language Processing (NLP) appli-
cations. In particular, summary creation and
evaluation methods need to recognize paraphrases
and their semantic equivalence. Unfortunately, we
have yet to incorporate into the evaluation frame-
work previous findings in paraphrase identification
and extraction (Barzilay and McKeown, 2001;
Pang et al, 2003; Bannard and Callison-Burch,
2005).
4.1 Related Work on Paraphrasing
Three major approaches in paraphrase collection
are manual collection (domain-specific), collection
utilizing existing lexical resources (i.e. WordNet),
and derivation from corpora. Hermjakob et al
(2002) view paraphrase recognition as
reformulation by pattern recognition. Pang et al
(2003) use word lattices as paraphrase representa-
tions from semantically equivalent translations
sets. Using parallel corpora, Barzilay and McKe-
own (2001) identify paraphrases from multiple
translations of classical novels, where as Bannard
and Callison-Burch (2005) develop a probabilistic
representation for paraphrases extracted from large
Machine Translation (MT) data sets.
4.2 Extracting Paraphrases
Our method to automatically construct a large do-
main-independent paraphrase collection is based
on the assumption that two different English
phrases of the same meaning may have the same
translation in a foreign language.
Phrase-based Statistical Machine Translation
(SMT) systems analyze large quantities of bilin-
gual parallel texts in order to learn translational
alignments between pairs of words and phrases in
two languages (Och and Ney, 2004). The sentence-
based translation model makes word/phrase align-
ment decisions probabilistically by computing the
optimal model parameters with application of the
statistical estimation theory. This alignment proc-
ess results in a corpus of word/phrase-aligned par-
allel sentences from which we can extract phrase
pairs that are translations of each other. We ran the
alignment algorithm from (Och and Ney, 2003) on
a Chinese-English parallel corpus of 218 million
English words. Phrase pairs are extracted by fol-
lowing the method described in (Och and Ney,
2004) where all contiguous phrase pairs having
consistent alignments are extraction candidates.
The resulting phrase table is of high quality; both
the alignment models and phrase extraction meth-
449
ods have been shown to produce very good results
for SMT. Using these pairs we build paraphrase
sets by joining together all English phrases with
the same Chinese translation. Figure 2 shows an
example word/phrase alignment for two parallel
sentence pairs from our corpus where the phrases
?blowing up? and ?bombing? have the same Chi-
nese translation. On the right side of the figure we
show the paraphrase set which contains these two
phrases, which is typical in our collection of ex-
tracted paraphrases.
5 Summary Comparison in ParaEval
This section describes the process of comparing a
peer summary against a reference summary and the
summary grading mechanism.
5.1 Description
We adopt a three-tier matching strategy for sum-
mary comparison. The score received by a peer
summary is the ratio of the number of reference
words matched to the total number of words in the
reference summary. The total number of matched
reference words is the sum of matched words in
reference throughout all three tiers. At the top
level, favoring high recall coverage, we perform an
optimal search to find multi-word paraphrase
matches between phrases in the reference summary
and those in the peer. Then a greedy search is per-
formed to find single-word paraphrase/synonym
matches among the remaining text. Operations
conducted in these two top levels are marked as
linked rounded rectangles in Figure 3. At the bot-
tom level, we find lexical identity matches, as
marked in rectangles in the example. If no para-
phrases are found, this last level provides a guar-
antee of lexical comparison that is equivalent to
what other automated systems give. In our system,
the bottom level currently performs unigram
matching. Thus, we are ensured with at least a
ROUGE-1 type of summary comparison. Alterna-
tively, equivalence of other ROUGE configura-
tions can replace the ROUGE-1 implementation.
There is no theoretical reason why the first two
levels should not merge. But due to high computa-
tional cost in modeling an optimal search, the sepa-
ration is needed. We explain this in detail below.
5.2 Multi-Word Paraphrase Matching
In this section we describe the algorithm that per-
forms the multi-word paraphrase matching be-
tween phrases from reference and peer summaries.
Using the example in Figure 3, this algorithm cre-
ates the phrases shown in the rounded rectangles
and establishes the appropriate links indicating
corresponding paraphrase matches.
Problem Description
Measuring content coverage of a peer summary
using a single reference summary requires com-
puting the recall score of how much information
from the reference summary is included in the
peer. A summary unit, either from reference or
peer, cannot be matched for more than once. For
Figure 2. An example of paraphrase extraction.
Figure 3. Comparison of summaries.
450
example, the phrase ?imposed sanctions on Libya?
(r1) in Figure 3?s reference summary was matched
with the peer summary?s ?voted sanctions against
Libya? (p1). If later in the peer summary there is
another phrase p2 that is also a paraphrase of r1, the
match of r1 cannot be counted twice. Conversely,
double counting is not permissible for
phrase/words in the peer summary, either.
We conceptualize the comparison of peer
against reference as a task that is to complete over
several time intervals. If the reference summary
contains n sentences, there will be n time intervals,
where at time ti, phrases from a particular sentence
i of the reference summary are being considered
with all possible phrases from the peer summary
for paraphrase matches. A decision needs to be
made at each time interval:
? Do we employ a local greedy match algo-
rithm that is recall generous (preferring more
matched words from reference) towards only the
reference sentence currently being analyzed,
? Or do we need to explore globally, in-
specting all reference sentences and find the best
overall matching combinations?
Consider the scenario in Figure 4:
1) at t0: L(p1 = r2) > L(p2 = r1) and r2 contains r1.
A local search algorithm leads to match(p1, r2). L() indi-
cates the number of words in reference matched by the
peer phrase through paraphrase matching and match()
indicates a paraphrase match has occurred (more in the
figure).
2) at t1: L(p1 = r3) > L(p1 = r2). A global algo-
rithm reverses the decision match(p1, r2) made at t0 and
concludes match(p1, r3) and match(p2, r1) . A local
search algorithm would have returned no match.
Clearly, the global search algorithm achieves
higher overall recall (in words). The matching of
paraphrases between a reference and its peer be-
comes a global optimization problem, maximizing
the content coverage of the peer compared in refer-
ence.
Solution Model
We use dynamic programming to derive the solu-
tion of finding the best paraphrase-matching com-
binations. The optimization problem is as follows:
Sentences from a reference summary and a peer
summary can be broken into phrases of various
lengths. A paraphrase lookup table is used to find
whether a reference phrase and a peer phrase are
paraphrases of each other. What is the optimal
paraphrase matching combination of phrases from
reference and peer that gives the highest recall
score (in number of matched reference words) for
this given peer? The solution should be recall ori-
ented (favoring a peer phrase that matches more
reference words than those match less).
Following (Trick, 1997), the solution can be
characterized as:
1) This problem can be divided into n stages
corresponding to the n sentences of the reference
summary. At each stage, a decision is required to
determine the best combination of matched para-
phrases between the reference sentence and the
entire peer summary that results in no double
counting of phrases on the peer side. There is no
double counting of reference phrases across stages
since we are processing one reference sentence at a
time and are finding the best paraphrase matches
using the entire peer summary. As long as there is
no double counting in peers, we are guaranteed to
have none in reference, either.
2) At each stage, we define a number of pos-
sible states as follows.  If, out of all possible
phrases of any length extracted from the reference
sentence, m phrases were found to have matching
paraphrases in the peer summary, then a state is
any subset of the m phrases.
3) Since no double counting in matched
phrases/words is allowed in either the reference
summary or the peer summary, the decision of
which phrases (leftover text segments in reference
Pj and ri represent phrases chosen for   paraphrase
matching from peer and reference respectively.
Pj = ri indicates that the phrase Pj from peer is
found to be a paraphrase to the phrase ri from
reference.
L(Pj = ri) indicates the number of words matched
by Pj in ri when they are found to be paraphrases of
each other.
L(Pj = ri) and L(Pj = ri+1) may not be equal if the
number of words in ri, indicated by L(ri), does not
equal to the number of words in ri+1, indicated by
L(ri+1).
Figure 4. Local vs. global paraphrase matching.
451
and in peer) are allowed to match for the next stage
is made in the current stage.
4) Principle of optimality: at a given state, it
is not necessary to know what matches occurred at
previous stages, only on the accumulated recall
score (matched reference words) from previous
stages and what text segments (phrases) in peer
have not been taken/matched in previous stages.
5) There exists a recursive relationship that
identifies the optimal decision for stage s (out of n
total stages), given that stage s+1 has already been
solved.
6) The final stage, n (last sentence in refer-
ence), is solved by choosing the state that has the
highest accumulated recall score and yet resulted
no double counting in any phrase/word in peer the
summary.
Figure 5 demonstrates the optimal solution (12
reference words matched) for the example shown
in Figure 4. We can express the calculations in the
following formulas:
where fy(xb) denotes the optimal recall coverage
(number of words in the reference summary
matched by the phrases from the peer summary) at
state xb in stage y. r(xb) is the recall coverage given
state xb. And c(xb) records the phrases matched in
peer with no double counting, given state xb.
5.3 Synonym Matching
All paraphrases whose pairings do not involve
multi-word to multi-word matching are called
synonyms in our experiment. Since these phrases
have either a n-to-1 or 1-to-n matching ratio (such
as the phrases ?blowing up? and ?bombing?), a
greedy algorithm favoring higher recall coverage
reduces the state creation and stage comparison
costs associated with the optimal procedure
(O(m6): O(m3) for state creation, and for 2 stages at
any time)). The paraphrase table described in Sec-
tion 4 is used.
 Synonym matching is performed only on parts
of the reference and peer summaries that were not
matched from the multi-word paraphrase-matching
phase.
5.4 Lexical Matching
This matching phase performs straightforward
lexical matching, as exemplified by the text frag-
ments marked in rectangles in Figure 3. Unigrams
are used as the units for counting matches in ac-
cordance with the previous two matching phases.
 During all three matching phases, we employed
a ROUGE-1 style of counting. Other alternatives,
such as ROUGE-2, ROUGE-SU4, etc., can easily
be adapted to each phase.
6  Evaluation of ParaEval
To evaluate and validate the effectiveness of an
automatic evaluation metric, it is necessary to
show that automatic evaluations correlate with
human assessments highly, positively, and consis-
tently (Lin and Hovy, 2003). In other words, an
automatic evaluation procedure should be able to
distinguish good and bad summarization systems
by assigning scores with close resemblance to hu-
mans? assessments.
6.1 Document Understanding Conference
The Document Understanding Conference has
provided large-scale evaluations on both human-
created and system-generated summaries annually.
Research teams are invited to participate in solving
summarization problems with their systems. Sys-
tem-generated summaries are then assessed by
humans and/or automatic evaluation procedures.
The collection of human judgments on systems and
their summaries has provided a test-bed for devel-
oping and validating automated summary grading
methods (Lin and Hovy, 2003; Hovy et al, 2005).
The correlations reported by ROUGE and BE
show that the evaluation correlations between these
two systems and DUC human evaluations are
much higher on single-document summarization
tasks. One possible explanation is that when sum-
Figure 5. Solution for the example in Figure 4.
452
marizing from only one source (text), both human-
and system-generated summaries are mostly ex-
tractive. The reason for humans to take phrases (or
maybe even sentences) verbatim is that there is less
motivation to abstract when the input is not highly
redundant, in contrast to input for multi-document
summarization tasks, which we speculate allows
more abstracting. ROUGE and BE both facilitate
lexical n-gram matching, hence, achieving amaz-
ing correlations. Since our baseline matching strat-
egy is lexically based when paraphrase matching is
not activated, validation on single-doc summariza-
tion results is not repeated in our experiment.
6.2 Validation and Discussion
We use summary judgments from DUC2003?s
multi-document summarization (MDS) task to
evaluate ParaEval. During DUC2003, participating
systems created short summaries (~100 words) for
30 document sets. For each set, one assessor-
written summary was used as the reference to
compare peer summaries created by 18 automatic
systems (including baselines) and 3 other human-
written summaries. A system ranking was pro-
duced by taking the averaged performance on all
summaries created by systems. This evaluation
process is replicated in our validation setup for
ParaEval. In all, 630 summary pairs were com-
pared. Pearson?s correlation coefficient is com-
puted for the validation tests, using DUC2003
assessors? results as the gold standard.
Table 1 illustrates the correlation figures from
the DUC2003 test set. ParaEval-para_only shows
the correlation result when using only paraphrase
and synonym matching, without the baseline uni-
gram matching. ParaEval-2 uses multi-word para-
phrase matching and unigram matching, omitting
the greedy synonym-matching phrase. ParaEval-3
incorporates matching at all three granularity lev-
els.
We see that the current implementation of
ParaEval closely resembles the way ROUGE-1
differentiates system-generated summaries. We
believe this is due to the identical calculations of
recall scores. The score that a peer summary re-
ceives from ParaEval depends on the number of
words matched in the reference summary from its
paraphrase, synonym, and unigram matches. The
counting of individual words in reference indicates
a ROUGE-1 design in grading. However, a de-
tailed examination on individual reference-peer
comparisons shows that paraphrase and synonym
comparisons and matches, in addition to lexical n-
gram matching, do measure a higher level of con-
tent coverage. This is demonstrated in Figure 6a
and b. Strict unigram matching reflects the content
retained by a peer summary mostly in the 0.2-0.4
ranges in recall, shown as dark-colored dots in the
graphs. Allowing paraphrase and synonym match-
ing increases the detection of peer coverage to the
range of 0.3-0.5, shown as light-colored dots.
We conducted a manual evaluation to further
examine the paraphrases being matched. Using 10
summaries from the Pyramid data, we asked three
human subjects to judge the validity of 128 (ran-
domly selected) paraphrase pairs extracted and
identified by ParaEval. Each pair of paraphrases
was coupled with its respective sentences as con-
texts. All paraphrases judged were multi-word.
ParaEval received an average precision of 68.0%.
The complete agreement between judges is 0.582
according to the Kappa coefficient (Cohen, 1960).
In Figure 7, we show two examples that the human
judges consider to be good paraphrases produced
and matched by ParaEval. Judges voiced difficul-
DUC-2003 Pearson
ROUGE-1 0.622
ParaEval-para_only 0.41
ParaEval-2 0.651
ParaEval-3 0.657
Table 1. Correlation with DUC 2003 MDS results.
Human Summaries: ParaEval vs. ROUGE-1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
DUC2003 Summary Writers
Re
ca
ll 
(%
 w
or
d 
m
at
ch
)
ROUGE-1 Scores
ParaEval-3 Scores
System Summaries: ParaEval vs. ROUGE-1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
DUC2003 Systems
Re
ca
ll 
(%
 w
or
d 
m
at
ch
)
ROUGE-1 Scores
ParaEval-3 Scores
a).
Human-
written
summaries.
b).
System-
generated
summaries.
Figure 6. A detailed look at the scores assigned by
lexical and paraphrase/synonym comparisons.
453
Figure 7. Paraphrases matched by ParaEval.
ties in determining ?semantic equivalence.? There
were cases where paraphrases would be generally
interchangeable but could not be matched because
of non-semantic equivalence in their contexts. And
there were paraphrases that were determined as
matches, but if taken out of context, would not be
direct replacements of each other. These two situa-
tions are where the judges mostly disagreed.
7 Conclusion and Future Work
In this paper, we have described an automatic
summarization evaluation method, ParaEval, that
facilitates paraphrase matching using a large do-
main-independent paraphrase table extracted from
a bilingual parallel corpus. The three-layer match-
ing strategy guarantees a ROUGE-like baseline
comparison if paraphrase matching fails.
The paraphrase extraction module from the cur-
rent implementation of ParaEval does not dis-
criminate among the phrases that are found to be
paraphrases of one another. We wish to incorporate
the probabilistic paraphrase extraction model from
(Bannard and Callison-Burch, 2005) to better ap-
proximate the relations between paraphrases. This
adaptation will also lead to a stochastic model for
the low-level lexical matching and scoring.
We chose English-Chinese MT parallel data be-
cause they are news-oriented which coincides with
the task genre from DUC. However, it is unknown
how large a parallel corpus is sufficient in provid-
ing a paraphrase collection good enough to help
the evaluation process. The quality of the para-
phrase table is also affected by changes in the do-
main and language pair of the MT parallel data.
We plan to use ParaEval to investigate the impact
of these changes on paraphrase quality under the
assumption that better paraphrase collections lead
to better summary evaluation results.
The immediate impact and continuation of the
described work would be to incorporate paraphrase
matching and extraction into the summary creation
process. And with ParaEval, it is possible for us to
evaluate systems that do incorporate some level of
abstraction, especially paraphrasing.
References
Bannard, C. and C. Callison-Burch. 2005. Paraphrasing with bilingual
parallel corpora. Proceedings of ACL-2005.
Barzilay, R. and K. McKeown. 2001. Extracting paraphrases from a
parallel corpus. Proceedings of ACL/EACL-2001.
Brown, P. F., S. A. Della Pietra, V. J. Della Pietra, R. L. Mercer.
1993. The mathematics of machine translation: Parameter estima-
tion. Computational Linguistics, 19(2): 263?311, 1993.
Cohen, J. 1960. A coefficient of agreement for nominal scales. Edu-
cation and Psychological Measurement, 43(6):37?46.
Diab, M. and P. Resnik. 2002. An unsupervised method for word
sense tagging using parallel corpora. Proceedings of ACL-2002.
DUC. 2001?2005. Document Understanding Conferences.
Hermjakob, U., A. Echihabi, and D. Marcu. 2002. Natural language
based reformulation resource and web exploitation for question
answering. Proceedings of TREC-2002.
Hovy, E, C.Y. Lin, and L. Zhou. 2005. Evaluating DUC 2005 using
basic elements. Proceedings of DUC-2005.
Hovy, E., C.Y. Lin, L. Zhou, and J. Fukumoto. 2005a. Basic Ele-
ments. http://www.isi.edu/~cyl/BE.
Lin, C.Y.  2001. http://www.isi.edu/~cyl/SEE.
Lin, C.Y. and E. Hovy. 2003. Automatic evaluation of summaries
using n-gram co-occurrence statistics. Proceedings of the HLT-
2003.
Miller, G.A., R. Beckwith, C. Fellbaum, D. Gross, and K. J. Miller.
1990. Introduction to WordNet: An on-line lexical database. Inter-
national Journal of Lexicography, 3(4): 235?245.
Nenkova, A. and R. Passonneau. 2004. Evaluating content selection in
summarization: the pyramid method. Proceedings of the HLT-
NAACL 2004.
Och, F. J. and H. Ney. 2003. A systematic comparison of various
statistical alignment models. Computational Linguistics, 29(1): 19-
?51, 2003.
Och, F. J. and H. Ney. 2004. The alignment template approach to
statistical machine translation. Computational Linguistics, 30(4),
2004.
Pang, B. , K. Knight and D. Marcu. 2003. Syntax-based alignment of
multiple translations: extracting paraphrases and generating new
sentences. Proceedings of HLT/NAACL-2003.
Papineni, K., S. Roukos, T. Ward, and W. J. Zhu. IBM research report
Bleu: a method for automatic evaluation of machine translation
IBM Research Division Technical Report, RC22176, 2001.
Trick, M. A. 1997. A tutorial on dynamic program-
ming.http://mat.gsia.cmu.edu/classes/dynamic/dynamic.html.
Van Halteren, H. and S. Teufel. 2003. Examining the consensus be-
tween human summaries: initial experiments with factoid analysis.
Proceedings of HLT-2003.
454
Proceedings of NAACL HLT 2007, Companion Volume, pages 217?220,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
A Semi-Automatic Evaluation Scheme: Automated Nuggetization for 
Manual Annotation 
Liang Zhou, Namhee Kwon, and Eduard Hovy 
Information Sciences Institute 
University of Southern California 
4676 Admiralty Way 
Marina del Rey, CA 90292 
{liangz, nkwon, hovy}@isi.edu 
 
Abstract 
In this paper we describe automatic in-
formation nuggetization and its applica-
tion to text comparison. More 
specifically, we take a close look at how 
machine-generated nuggets can be used to 
create evaluation material. A semi-
automatic annotation scheme is designed 
to produce gold-standard data with excep-
tionally high inter-human agreement.  
1 Introduction 
In many natural language processing (NLP) tasks, 
we are faced with the problem of determining the 
appropriate granularity level for information units. 
Most commonly, we use sentences to model indi-
vidual pieces of information. However, more NLP 
applications require us to define text units smaller 
than sentences, essentially decomposing sentences 
into a collection of phrases. Each phrase carries an 
independent piece of information that can be used 
as a standalone unit. These finer-grained informa-
tion units are usually referred to as nuggets.  
When performing within-sentence comparison 
for redundancy and/or relevancy judgments, with-
out a precise and consistent breakdown of nuggets 
we can only rely on rudimentary n-gram segmenta-
tions of sentences to form nuggets and perform 
subsequent n-gram-wise text comparison. This is 
not satisfactory for a variety of reasons. For exam-
ple, one n-gram window may contain several sepa-
rate pieces of information, while another of the 
same length may not contain even one complete 
piece of information.  
Previous work shows that humans can create 
nuggets in a relatively straightforward fashion. In 
the PYRAMID scheme for manual evaluation of 
summaries (Nenkova and Passonneau, 2004), ma-
chine-generated summaries were compared with 
human-written ones at the nugget level. However, 
automatic creation of the nuggets is not trivial. 
Hamly et al (2005) explore the enumeration and 
combination of all words in a sentence to create the 
set of all possible nuggets. Their automation proc-
ess still requires nuggets to be manually created a 
priori for reference summaries before any sum-
mary comparison takes place. This human in-
volvement allows a much smaller subset of phrase 
segments, resulting from word enumeration, to be 
matched in summary comparisons. Without the 
human-created nuggets, text comparison falls back 
to its dependency on n-grams. Similarly, in ques-
tion-answering (QA) evaluations, gold-standard 
answers use manually created nuggets and com-
pare them against system-produced answers bro-
ken down into n-gram pieces, as shown in 
POURPRE (Lin and Demner-Fushman, 2005) and 
NUGGETEER (Marton and Radul, 2006).  
A serious problem in manual nugget creation is 
the inconsistency in human decisions (Lin and 
Hovy, 2003). The same nugget will not be marked 
consistently with the same words when sentences 
containing multiple instances of it are presented to 
human annotators. And if the annotation is per-
formed over an extended period of time, the con-
sistency is even lower. In recent exercises of the 
PYRAMID evaluation, inconsistent nuggets are 
flagged by a tracking program and returned back to 
the annotators, and resolved manually.  
Given these issues, we address two questions in 
this paper: First, how do we define nuggets so that 
they are consistent in definition? Secondly, how do 
217
we utilize automatically extracted nuggets for vari-
ous evaluation purposes?  
2  Nugget Definition  
 
Based on our manual analysis and computational 
modeling of nuggets, we define them as follows:  
Definition:  
? A nugget is predicated on either an event  or 
an entity .  
? Each nugget consists of two parts: the an-
chor and the content.  
The anchor is either:  
? the head noun of the entity, or 
? the head verb of the event, plus the head 
noun of its associated entity (if more than 
one entity is attached to the verb, then its 
subject).  
The content is a coherent single piece of infor-
mation associated with the anchor. Each anchor 
may have several separate contents.  
When a nugget contains nested sentences, this 
definition is applied recursively. Figure 1 shows an 
example. Anchors are marked with square brack-
ets. If the anchor is a verb, then its entity attach-
ment is marked with curly brackets. If the sentence 
in question is a compound and/or complex sen-
tence, then this definition is applied recursively to 
allow decomposition. For example, in Figure 1, 
without recursive decomposition, only two nuggets 
are formed: 1) ?[girl] working at the bookstore in 
Hollywood?, and 2) ?{girl} [talked] to the diplo-
mat living in Britain?. In this example, recursive 
decomposition produces nuggets with labels 1-a, 1-
b, 2-a, and 2-b.  
2.1  Nugget Extraction 
We use syntactic parse trees produced by the 
Collins parser (Collins, 1999) to obtain the struc-
tural representation of sentences. Nuggets are ex-
tracted by identifying subtrees that are descriptions 
for entities and events. For entity nuggets, we ex-
amine subtrees headed by ?NP?; for event nuggets, 
subtrees headed by ?VP? are examined and their 
corresponding subjects (siblings headed by ?NP?) 
are treated as entity attachments for the verb 
phrases.  
3  Utilizing Nuggets in Evaluations 
In recent QA and summarization evaluation exer-
cises, manually created nuggets play a determinate 
role in judging system qualities. Although the two 
task evaluations are similar, the text comparison 
task in summarization evaluation is more complex 
because systems are required to produce long re-
sponses and thus it is hard to yield high agreement 
if manual annotations are performed. The follow-
ing experiments are conducted in the realm of 
summarization evaluation.  
3.1  Manually Created Nuggets 
During the recent two Document Understanding 
Confereces (DUC-05 and DUC-06) (NIST, 2002?
2007), the PYRAMID framework (Nenkova and 
Passonneau, 2004) was used for manual summary 
evaluations. In this framework, human annotators 
select and highlight portions of reference summa-
ries to form a pyramid of summary content units 
(SCUs) for each docset. A pyramid is constructed 
from SCUs and their corresponding popularity 
scores?the number of reference summaries they 
appeared in individually. SCUs carrying the same 
information do not necessarily have the same sur-
face-level words. Annotators need to make the de-
cisions based on semantic equivalence among 
Figure 1. Nugget definition examples.  
Sentence:  
The girl working at the bookstore in Hollywood 
talked to the diplomat living in Britain.  
 
Nuggets are: 
1)  [girl] working at the bookstore in Holly-
wood 
a. [girl] working at the bookstore  
b. [bookstore] in Hollywood 
2) {girl} [talked] to the diplomat living in 
Britain 
a. {girl} [talked] to the diplomat 
b. [diplomat] living in Britian 
 
Anchors: 
1)  [girl] 
a. [girl] 
b. [bookstore] 
2) {girl} [talked]: talked is the anchor verb 
and girl is its entity attachment.  
a. {girl} [talked] 
b. [diplomat]  
218
various SCUs. To evaluate a peer summary from a 
particular docset, annotators highlight portions of 
text in the peer summary that convey the same in-
formation as those SCUs in previously constructed 
pyramids.  
3. 2  Automatically Created Nuggets 
We envisage the nuggetization process being 
automated and nugget comparison and aggregation 
being performed by humans. It is crucial to involve 
humans in the evaluation process because recog-
nizing semantically equivalent units is not a trivial 
task computationally. In addition, since nuggets are 
system-produced and can be imperfect, annotators 
are allowed to reject and re-create them. We per-
form record-keeping in the background on which 
nugget or nugget groups are edited so that further 
improvements can be made for nuggetization.  
The evaluation scheme is designed as follows: 
 
For reference summaries  (per docset):  
? Nuggets are created for all sentences;  
? Annotators will group equivalent nuggets.  
? Popularity scores are automatically assigned 
to nugget groups.  
For peer summaries :  
? Nuggets are created for all sentences;  
? Annotators will match/align peer?s nuggets 
with reference nugget groups.  
? Recall scores are to be computed.  
3. 3  Consistency in Human Involvement 
The process of creating nuggets has been auto-
mated and we can assume a certain level of consis-
tency based on the usage of the syntactic parser. 
However, a more important issue emerges. When 
given the same set of nuggets, would human anno-
tators agree on nugget group selections and their 
corresponding contributing nuggets? What levels 
of agreement and disagreement should be ex-
pected? Two annotators, one familiar with the no-
tion of nuggetization (C1) and one not (C2), 
participated in the following experiments.  
Figure 2 shows the annotation procedure for 
reference summaries. After two rounds of individ-
ual annotations and consolidations and one final 
round of conflict resolution, a set of gold-standard 
nugget groups is created for each docset and will 
be subsequently used in peer summary annotations. 
The first round of annotation is needed since one 
of the annotators, C2, is not familiar with nuggeti-
zation. After the initial introduction of the task, 
concerns and questions arisen can be addressed. 
Then the annotators proceed to the second round of 
annotation. Naturally, some differences and con-
flicts remain. Annotators must resolve these prob-
lems during the final round of conflict resolution 
and create the agreed-upon gold-standard data.   
Previous manual nugget annotation has used one 
annotator as the primary nugget creator and an-
other annotator as an inspector (Nenkova and Pas-
sonneau, 2004). In our annotation experiment, we 
encourage both annotators to play equally active 
roles. Conflicts between annotators resulting from 
ideology, comprehension, and interpretation differ-
ences helped us to understand that complete 
agreement between annotators is not realistic and 
not achievable, unless one annotator is dominant 
over the other. We should expect a 5-10% annota-
tion variation.  
In Figure 3, we show annotation comparisons 
from first to second round. The x -axis shows the 
nugget groups that C1 and C2 have agreed on. The 
y -axis shows the popularity score a particular nug-
get group received. Selecting from three reference 
summaries, a score of three for a nugget group in-
dicates it was created from nuggets in all three 
 
Figure 2. Reference annotation and gold-standard 
data creation.  
219
summaries. The first round initially appears suc-
cessful because the two annotators had 100% 
agreement on nugget groups and their correspond-
ing scores. However, C2, the novice nuggetizer, 
was much more conservative than C1, because 
only 10 nugget groups were created. The geometric 
mean of agreement on all nugget group assignment 
is merely 0.4786. During the second round, differ-
ences in group-score allocations emerge, 0.9192, 
because C2 is creating more nugget groups. The 
geometric mean of agreement on all nugget group 
assignment has been improved to 0.7465.  
After the final round of conflict resolution, 
gold-standard data was created. Since all conflicts 
must be resolved, annotators have to either con-
vince or be convinced by the other. How much 
change is there between an annotator?s second-
round annotation and the gold-standard? Geomet-
ric mean of agreement on all nugget group assign-
ment for C1 is 0.7543 and for C2 is 0.8099. 
Agreement on nugget group score allocation for 
C1 is 0.9681 and for C2 is 0.9333. From these fig-
ures, we see that while C2 contributed more to the 
gold-standard?s nugget group creations, C1 had 
more accuracy in finding the correct number of 
nugget occurrences in reference summaries. This 
confirms that both annotators played an active role. 
Using the gold-standard nugget groups, the annota-
tors performed 4 peer summary annotations. The 
agreement among peer summary annotations is 
quite high, at approximately 0.95. Among the four, 
annotations on one peer summary from the two 
annotators are completely identical.  
4  Conclusion 
In this paper we have given a concrete definition 
for information nuggets and provided a systematic 
implementation of them. Our main goal is to use 
these machine-generated nuggets in a semi-
automatic evaluation environment for various NLP 
applications. We took a close look at how this can 
be accomplished for summary evaluation, using 
nuggets created from reference summaries to grade 
peer summaries. Inter-annotator agreements are 
measured to insure the quality of the gold-standard 
data created. And the agreements are very high by 
following a meticulous procedure. We are cur-
rently preparing to deploy our design into full-
scale evaluation exercises.  
References 
Collins, M. 1999. Head-driven statistical models for 
natural language processing. Ph D Dissertation , Uni-
versity of Pennsylvania.  
Hamly, A., A. Nenkova, R. Passonneau, and O. Ram-
bow. 2005. Automation of summary evaluation by 
the pyramid method. In Proceedings of RANLP.  
Lin, C.Y. and E. Hovy. 2003. Automatic evaluation of 
summaries using n-gram co-occurrence statistics. In 
Proceedings of NAACL-HLT. 
Lin, J. and D. Demner-Fushman. 2005. Automatically 
evaluating answers to definition questions. In Pro-
ceedings of HLT-EMNLP.  
Marton, G. and A. Radul. 2006. Nuggeteer: automatic 
nugget-based evaluation using description and judg-
ments. In Proceedings NAACL-HLT.  
Nenkova, A. and R. Passonneau. 2004. Evaluating con-
tent selection in summarization: the pyramid method. 
In Proceedings NAACL-HLT.  
NIST. 2001?2007. Document Understanding Confer-
ence. www-nlpir.nist.gov/projects/duc/index.html.  
 
 
Figure 3. Annotation comparisons from 1 st to 
2nd round.  
220
NAACL HLT Demonstration Program, pages 29?30,
Rochester, New York, USA, April 2007. c?2007 Association for Computational Linguistics
Text Comparison Using Machine-Generated Nuggets 
Liang Zhou 
Information Sciences Institute 
University of Southern California 
4676 Admiralty Way 
Marina del Rey, CA 90292 
liangz@isi.edu 
 
 
Abstract 
This paper describes a novel text com-
parison environment that facilities text 
comparison administered through assess-
ing and aggregating information nuggets 
automatically created and extracted from 
the texts in question. Our goal in design-
ing such a tool is to enable and improve 
automatic nugget creation and present its 
application for evaluations of various 
natural language processing tasks. During 
our demonstration at HLT, new users will 
able to experience first hand text analysis 
can be fun, enjoyable, and interesting us-
ing system-created nuggets.  
1 Introduction 
In many natural language processing (NLP) tasks, 
such as question answering (QA), summarization, 
etc., we are faced with the problem of determining 
the appropriate granularity level for information 
units in order to conduct appropriate and effective 
evaluations. Most commonly, we use sentences to 
model individual pieces of information. However, 
more and more NLP applications require us to de-
fine text units smaller than sentences, essentially 
decomposing sentences into a collection of 
phrases. Each phrase carries an independent piece 
of information that can be used as a standalone 
unit. These finer-grained information units are 
usually referred to as nuggets.  
Previous work shows that humans can create 
nuggets in a relatively straightforward fashion. A 
serious problem in manual nugget creation is the 
inconsistency in human decisions (Lin and Hovy, 
2003). The same nugget will not be marked consis-
tently with the same words when sentences con-
taining multiple instances of it are presented to 
human annotators. And if the annotation is per-
formed over an extended period of time, the con-
sistency is even lower. 
Given concerns over these issues, we have set 
out to design an evaluation toolkit to address three 
tasks in particular: 1) provide a consistent defini-
tion of what a nugget is; 2) automate the nugget 
extraction process systematically; and 3) utilize 
automatically extracted nuggets for text compari-
son and aggregation.  
The idea of using semantic equivalent nuggets 
to compare texts is not new. QA and summariza-
tion evaluations (Lin and Demner-Fushman, 2005; 
Nenkova and Passonneau, 2004) have been carried 
out by using a set of manually created nuggets and 
the comparison procedure itself is either automatic 
using n-gram overlap counting or manually per-
formed. We envisage the nuggetization process 
being automated and nugget comparison and ag-
gregation being performed by humans. It?s crucial 
to still involve humans in the process because rec-
ognizing semantic equivalent text units is not a 
trivial task. In addition, since nuggets are system-
produced and can be imperfect, annotators are al-
lowed to reject and re-create them. We provide 
easy-to-use editing functionalities that allow man-
ual overrides. Record keeping on edits over erro-
neous nuggets is conducted in the background so 
that further improvements can be made for nugget 
extraction.  
29
2  Nugget Definition 
Based on our manual analysis and computational 
modeling of nuggets, we define them as follows:  
 
Definition:  
? A nugget is predicated on either an event  or 
an entity .  
? Each nugget consists of two parts: the an-
chor and the content.  
 
The anchor is either:  
? the head noun of the entity, or 
? the head verb of the event, plus the head 
noun of its associated entity (if more than 
one entity is attached to the verb, then its 
subject).  
 
The content is a coherent single piece of infor-
mation associated with the anchor. Each anchor 
may have several separate contents. When the 
nugget contains nested sentences, this definition is 
applied recursively.  
3  Nugget Extraction 
We use syntactic parse trees produced by the 
Collins parser (Collins, 1999) to obtain the struc-
tural representation of sentences. Nuggets are ex-
tracted by identifying subtrees that are descriptions 
for entities and events. For entities, we examine 
subtrees headed by ?NP?; for events, subtrees 
headed by ?VP? are examined and their corre-
sponding subjects (siblings headed by ?NP?) are 
investigated as possible entity attachments for the 
verb phrases. Figure 1 shows an example where 
words in brackets represent corresponding nug-
gets? anchors.  
4  Comparing Texts 
When comparing multiple texts, we present the 
annotator with each text?s sentences along with 
nuggets extracted from individual sentences (see 
Appendix A). Annotators can select multiple nug-
gets from sentences across texts to indicate their 
semantic equivalence. Equivalent nuggets are 
grouped into nugget groups. There is a frequency 
score, the number of texts it appeared in, for each 
nugget group. We allow annotators to modify the 
nugget groups? contents, thus creating a new label 
(or can be viewed as a super-nugget) for each nug-
get group. Record keeping is conducted in the 
background automatically each time a nugget 
group is created. When the annotator changes the 
content of a nugget group, it indicates that either 
the system-extracted nuggets are not perfect or a 
super-nugget is created for the group (see Appen-
dix B and C).  These editing changes are recorded. 
The recorded information affords us the opportu-
nity to improve the nuggetizer and perform subse-
quence study phrase-level paraphrasing, text 
entailment, etc.  
5  Hardware Requirement 
Our toolkit is written in Java and can be run on any 
machine with the latest Java installed.  
References 
Collins, M. 1999. Head-driven statistical models 
for natural language processing. Ph D Disserta-
tion , University of Pennsylvania.  
Lin, C.Y. and E. Hovy. 2003. Automatic evalua-
tion of summaries using n-gram co-occurrence 
statistics. In Proceedings of N A A CL- H LT  2 0 0 3 . 
Lin, J. and D. Demner-Fushman. 2005. Automati-
cally evaluating answers to definition questions. 
In Pr o ceedings of  H LT - E MN L P 2 0 0 5 .  
Nenkova, A. and R. Passonneau. 2004. Evaluating 
content selection in summarization: the pyramid 
method. In Proceedings of NAACL-HLT 2004.  
Sentence:  
The girl working at the bookstore in Hollywood 
talked to the diplomat living in Britain.  
 
Nuggets are: 
[girl] working at the bookstore in Hollywood 
[girl] working at the bookstore  
[bookstore] in Hollywood 
girl [talked] to the diplomat living in Britain 
girl [talked] to the diplomat 
[diplomat] living in Britian 
Figure 1. Nugget example. (words in brackets are 
the anchors).  
30
Proceedings of the 43rd Annual Meeting of the ACL, pages 298?305,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Digesting Virtual ?Geek? Culture:
The Summarization of Technical Internet Relay Chats
Liang Zhou and Eduard Hovy
University of Southern California
Information Sciences Institute
4676 Admiralty Way
Marina del Rey, CA 90292-6695
{liangz, hovy} @isi.edu
Abstract
This paper describes a summarization
system for technical chats and emails on
the Linux kernel. To reflect the complex-
ity and sophistication of the discussions,
they are clustered according to subtopic
structure on the sub-message level, and
immediate responding pairs are identified
through machine learning methods. A re-
sulting summary consists of one or more
mini-summaries, each on a subtopic from
the discussion.
1 Introduction
The availability of many chat forums reflects the
formation of globally dispersed virtual communi-
ties. From them we select the very active and
growing movement of Open Source Software
(OSS) development. Working together in a virtual
community in non-collocated environments, OSS
developers communicate and collaborate using a
wide range of web-based tools including Internet
Relay Chat (IRC), electronic mailing lists, and
more (Elliott and Scacchi, 2004). In contrast to
conventional instant message chats, IRCs convey
engaging and focused discussions on collaborative
software development. Even though all OSS par-
ticipants are technically savvy individually, sum-
maries of IRC content are necessary within a
virtual organization both as a resource and an or-
ganizational memory of activities (Ackerman and
Halverson, 2000). They are regularly produced
manually by volunteers. These summaries can be
used for analyzing the impact of virtual social in-
teractions and virtual organizational culture on
software/product development.
The emergence of email thread discussions and
chat logs as a major information source has
prompted increased interest in thread summariza-
tion within the Natural Language Processing
(NLP) community. One might assume a smooth
transition from text-based summarization to email
and chat-based summarizations. However, chat
falls in the genre of correspondence, which re-
quires dialogue and conversation analysis. This
property makes summarization in this area even
more difficult than traditional summarization. In
particular, topic ?drift? occurs more radically than
in written genres, and interpersonal and pragmatic
content appears more frequently. Questions about
the content and overall organization of the sum-
mary must be addressed in a more thorough way
for chat and other dialogue summarization sys-
tems.
In this paper we present a new system that clus-
ters sub-message segments from correspondences
according to topic, identifies the sub-message
segment containing the leading issue within the
topic, finds immediate responses from other par-
ticipants, and consequently produces a summary
for the entire IRC. Other constructions are possi-
ble. One of the two baseline systems described in
this paper uses the timeline and dialogue structure
to select summary content, and is quite effective.
We use the term chat loosely in this paper. Input
IRCs for our system is a mixture of chats and
298
emails that are indistinguishable in format ob-
served from the downloaded corpus (Section 3).
In the following sections, we summarize previ-
ous work, describe the email/chat data, intra-
message clustering and summary extraction proc-
ess, and discuss the results and future work.
2 Previous and Related Work
There are at least two ways of organizing dialogue
summaries: by dialogue structure and by topic.
Newman and Blitzer (2002) describe methods
for summarizing archived newsgroup conversa-
tions by clustering messages into subtopic groups
and extracting top-ranked sentences per subtopic
group based on the intrinsic scores of position in
the cluster and lexical centrality. Due to the techni-
cal nature of our working corpus, we had to handle
intra-message topic shifts, in which the author of a
message raises or responds to multiple issues in the
same message. This requires that our clustering
component be not message-based but sub-
message-based.
Lam et al (2002) employ an existing summar-
izer for single documents using preprocessed email
messages and context information from previous
emails in the thread.
Rambow et al (2004) show that sentence ex-
traction techniques are applicable to summarizing
email threads, but only with added email-specific
features. Wan and McKeown (2004) introduce a
system that creates overview summaries for ongo-
ing decision-making email exchanges by first de-
tecting the issue being discussed and then
extracting the response to the issue. Both systems
use a corpus that, on average, contains 190 words
and 3.25 messages per thread, much shorter than
the ones in our collection.
Galley et al (2004) describe a system that iden-
tifies agreement and disagreement occurring in
human-to-human multi-party conversations. They
utilize an important concept from conversational
analysis, adjacent pairs (AP), which consists of
initiating and responding utterances from different
speakers. Identifying APs is also required by our
research to find correspondences from different
chat participants.
In automatic summarization of spoken dia-
logues, Zechner (2001) presents an approach to
obtain extractive summaries for multi-party dia-
logues in unrestricted domains by addressing in-
trinsic issues specific to speech transcripts. Auto-
matic question detection is also deemed important
in this work. A decision-tree classifier was trained
on question-triggering words to detect questions
among speech acts (sentences). A search heuristic
procedure then finds the corresponding answers.
Ries (2001) shows how to use keyword repetition,
speaker initiative and speaking style to achieve
topical segmentation of spontaneous dialogues.
3    Technical Internet Relay Chats
GNUe, a meta-project of the GNU project
1
?one of
the most famous free/open source software pro-
jects?is the case study used in (Elliott and Scacchi,
2004) in support of the claim that, even in virtual
organizations, there is still the need for successful
conflict management in order to maintain order
and stability.
The GNUe IRC archive is uniquely suited for
our experimental purpose because each IRC chat
log has a companion summary digest written by
project participants as part of their contribution to
the community. This manual summary constitutes
gold-standard data for evaluation.
3.1 Kernel Traffic
2
Kernel Traffic is a collection of summary digests
of discussions on GNUe development. Each digest
summarizes IRC logs and/or email messages (later
referred to as chat logs) for a period of up to two
weeks. A nice feature is that direct quotes and
hyperlinks are part of the summary. Each digest is
an extractive overview of facts, plus the author?s
dramatic and humorous interpretations.
3.2 Corpus Download
The complete Linux Kernel Archive (LKA) con-
sists of two separate downloads. The Kernel Traf-
fic (summary digests) are in XML format and were
downloaded by crawling the Kernel Traffic site.
The Linux Kernel Archives (individual IRC chat
logs) are downloaded from the archive site. We
matched the summaries with their respective chat
logs based on subject line and publication dates.
3.3 Observation on Chat Logs
                                                           
1
 http://www.gnu.org
2
 http://kt.hoser.ca/kernel-traffic/index.html
299
Upon initial examination of the chat logs, we
found that many conventional assumptions about
chats in general do not apply. For example, in most
instant-message chats, each exchange usually con-
sists of a small number of words in several sen-
tences. Due to the technical nature of GNUe, half
of the chat logs contain in-depth discussions with
lengthy messages. One message might ask and an-
swer several questions, discuss many topics in de-
tail, and make further comments. This property,
which we call subtopic structure, is an important
difference from informal chat/interpersonal banter.
Figure 1 shows the subtopic structure and relation
of the first 4 messages from a chat log, produced
manually. Each message is represented horizon-
tally; the vertical arrows show where participants
responded to each other. Visual inspection reveals
in this example there are three distinctive clusters
(a more complex cluster and two smaller satellite
clusters) of discussions between participants at
sub-message level.
3.4 Observation on Summary Digests
To measure the goodness of system-produced
summaries, gold standards are used as references.
Human-written summaries usually make up the
gold standards. The Kernel Traffic (summary di-
gests) are written by Linux experts who actively
contribute to the production and discussion of the
open source projects. However, participant-
produced digests cannot be used as reference
summaries verbatim. Due to the complex structure
of the dialogue, the summary itself exhibits some
discourse structure, necessitating such reader guid-
ance phrases such as ?for the ? question,? ?on the
? subject,? ?regarding ?,? ?later in the same
thread,? etc., to direct and refocus the reader?s at-
tention. Therefore, further manual editing and par-
titioning is needed to transform a multi-topic digest
into several smaller subtopic-based gold-standard
reference summaries (see Section 6.1 for the trans-
formation).
4 Fine-grained Clustering
To model the subtopic structure of each chat mes-
sage, we apply clustering at the sub-message level.
4.1 Message Segmentation
First, we look at each message and assume that
each participant responds to an ongoing discussion
by stating his/her opinion on several topics or is-
sues that have been discussed in the current chat
log, but not necessarily in the order they were dis-
cussed. Thus, topic shifts can occur sequentially
within a message. Messages are partitioned into
multi-paragraph segments using TextTiling, which
reportedly has an overall precision of 83% and re-
call of 78% (Hearst, 1994).
4.2 Clustering
After distinguishing a set of message segments, we
cluster them. When choosing an appropriate clus-
tering method, because the number of subtopics
under discussion is unknown, we cannot make an
assumption about the total number of resulting
clusters. Thus, nonhierarchical partitioning meth-
ods cannot be used, and we must use a hierarchical
method.  These methods can be either agglomera-
tive, which begin with an unclustered data set and
perform N ? 1 pairwise joins, or divisive, which
add all objects to a single cluster, and then perform
N ? 1 divisions to create a hierarchy of smaller
clusters, where N is the total number of items to be
clustered (Frakes and Baeza-Yates, 1992).
Ward?s Method
Hierarchical agglomerative clustering methods are
commonly used and we employ Ward?s method
(Ward and Hook, 1963), in which the text segment
pair merged at each stage is the one that minimizes
the increase in total within-cluster variance.
Each cluster is represented by an L-dimensional
vector (x
i1
, x
i2
, ?, x
iL
) where each x
ik
 is the word?s
tf ? idf score. If m
i
 is the number of objects in the
cluster, the squared Euclidean distance between
two segments i and j is:
? 
d
ij
2
= (x
ik
K=1
L
?
? x
jk
)
2
Figure 1. An example of chat subtopic structure
and relation between correspondences.
300
When two segments are joined, the increase in
variance I
ij
 is expressed as:
? 
I
ij
=
m
i
m
j
m
i
+ m
j
d
ij
2
Number of Clusters
The process of joining clusters continues until the
combination of any two clusters would destabilize
the entire array of currently existing clusters pro-
duced from previous stages. At each stage, the two
clusters x
ik
 and x
jk
 are chosen whose combination
would cause the minimum increase in variance I
ij
,
expressed as a percentage of the variance change
from the last round. If this percentage reaches a
preset threshold, it means that the nearest two
clusters are much further from each other com-
pared to the previous round; therefore, joining of
the two represents a destabilizing change, and
should not take place.
Sub-message segments from resulting clusters
are arranged according to the sequence the original
messages were posted and the resulting subtopic
structures are similar to the one shown in Figure 1.
5 Summary Extraction
Having obtained clusters of message segments fo-
cused on subtopics, we adopt the typical summari-
zation paradigm to extract informative sentences
and segments from each cluster to produce sub-
topic-based summaries. If a chat log has n clusters,
then the corresponding summary will contain n
mini-summaries.
All message segments in a cluster are related to
the central topic, but to various degrees. Some are
answers to questions asked previously, plus further
elaborative explanations; some make suggestions
and give advice where they are requested, etc.
From careful analysis of the LKA data, we can
safely assume that for this type of conversational
interaction, the goal of the participants is to seek
help or advice and advance their current knowl-
edge on various technical subjects. This kind of
interaction can be modeled as one problem-
initiating segment and one or more corresponding
problem-solving segments. We envisage that iden-
tifying corresponding message segment pairs will
produce adequate summaries. This analysis follows
the structural organization of summaries from Ker-
nel Traffic. Other types of discussions, at least in
part, require different discourse/summary organi-
zation.
These corresponding pairs are formally intro-
duced below, and the methods we experimented
with for identifying them are described.
5.1 Adjacent Response Pairs
An important conversational analysis concept, ad-
jacent pairs (AP), is applied in our system to iden-
tify initiating and responding correspondences
from different participants in one chat log. Adja-
cent pairs are considered fundamental units of
conversational organization (Schegloff and Sacks,
1973). An adjacent pair is said to consist of two
parts that are ordered, adjacent, and produced by
different speakers (Galley et al, 2004). In our
email/chat (LKA) corpus a physically adjacent
message, following the timeline, may not directly
respond to its immediate predecessor. Discussion
participants read the current live thread and decide
what he/she would like to correspond to, not nec-
essarily in a serial fashion. With the added compli-
cation of subtopic structure (see Figure 1) the
definition of adjacency is further violated. Due to
its problematic nature, a relaxation on the adja-
cency requirement is used in extensive research in
conversational analysis (Levinson, 1983). This re-
laxed requirement is adopted in our research.
Information produced by adjacent correspon-
dences can be used to produce the subtopic-based
summary of the chat log.  As described in Section
4, each chat log is partitioned, at sub-message
level, into several subtopic clusters. We take the
message segment that appears first chronologically
in the cluster as the topic-initiating segment in an
adjacent pair. Given the initiating segment, we
need to identify one or more segments from the
same cluster that are the most direct and relevant
responses. This process can be viewed equivalently
as the informative sentence extraction process in
conventional text-based summarization.
5.2 AP Corpus and Baseline
We manually tagged 100 chat logs for adjacent
pairs. There are, on average, 11 messages per chat
log and 3 segments per message (This is consid-
erably larger than threads used in previous re-
search). Each chat log has been clustered into one
or more bags of message segments. The message
segment that appears earliest in time in a cluster
301
was marked as the initiating segment. The annota-
tors were provided with this segment and one other
segment at a time, and were asked to decide
whether the current message segment is a direct
answer to the question asked, the suggestion that
was requested, etc. in the initiating segment. There
are 1521 adjacent response pairs; 1000 were used
for training and 521 for testing.
Our baseline system selects the message seg-
ment (from a different author) immediately fol-
lowing the initiating segment. It is quite effective,
with an accuracy of 64.67%. This is reasonable
because not all adjacent responses are interrupted
by messages responding to different earlier initiat-
ing messages.
In the following sections, we describe two ma-
chine learning methods that were used to identify
the second element in an adjacent response pair
and the features used for training. We view the
problem as a binary classification problem, distin-
guishing less relevant responses from direct re-
sponses. Our approach is to assign a candidate
message segment c an appropriate response class r.
5.3 Features
Structural and durational features have been dem-
onstrated to improve performance significantly in
conversational text analysis tasks. Using them,
Galley et al (2004) report an 8% increase in
speaker identification. Zechner (2001) reports ex-
cellent results (F > .94) for inter-turn sentence
boundary detection when recording the length of
pause between utterances.  In our corpus, dura-
tional information is nonexistent because chats and
emails were mixed and no exact time recordings
beside dates were reported. So we rely solely on
structural and lexical features.
For structural features, we count the number of
messages between the initiating message segment
and the responding message segment. Lexical fea-
tures are listed in Table 1. The tech words are the
words that are uncommon in conventional litera-
ture and unique to Linux discussions.
5.4 Maximum Entropy
Maximum entropy has been proven to be an ef-
fective method in various natural language proc-
essing applications (Berger et al, 1996). For
training and testing, we used YASMET
3
.  To est i-
mate P(r | c) in the exponential form, we have:
? 
P?(r | c) = 1
Z?(c)  exp( ?i,r
i
?
f
i,r
(c,r))
where Z?(c) is a normalizing constant and the fea-
ture function for feature f
i
 and response class r is
defined as:
? 
f
i,r
(c,
? 
r ) =
1, if f
i
> 0 and 
? 
r = r
0, otherwise            
  
? 
? 
? 
.?
i,r
 is the feature-weight parameter for feature f
i 
and
response class r. Then, to determine the best class r
for the candidate message segment c, we have:
? 
r
*
= arg max
r
P(r | c)   .
5.5 Support Vector Machine
Support vector machines (SVMs) have been shown
to outperform other existing methods (na?ve Bayes,
k-NN, and decision trees) in text categorization
(Joachims, 1998). Their advantages are robustness
and the elimination of the need for feature selec-
tion and parameter tuning. SVMs find the hyper-
plane that separates the positive and negative
training examples with maximum margin. Finding
this hyperplane can be translated into an optimiza-
tion problem of finding a set of coefficients ?
i
*
 of
the weight vector 
  
? 
r 
w for document d
i
 of class y
i
 ?
{+1 , ?1}:
  
? 
r 
w = ?
i
*
i
?
y
i
r 
d 
i
,    ?
i
> 0     .
Testing data are classified depending on the side
of the hyperplane they fall on. We used the
LIBSVM
4
 package for training and testing.
                                                           
3
 http://www.fjoch.com/YASMET.html
4
 http://www.csie.ntu.edu.tw/~cjlin/libsvm/
Feature sets baseline MaxEnt SVM
64.67%
Structural 61.22% 71.79%
Lexical 62.24% 72.22%
Structural + Lexical 72.61% 72.79%
? number of overlapping words
? number of overlapping content words
? ratio of overlapping words
? ratio of overlapping content words
? number of overlapping tech words
Table 1. Lexical features.
Table 2. Accuracy on identifying APs.
302
5.6 Results
Entries in Table 2 show the accuracies achieved
using machine learning models and feature sets.
5.7 Summary Generation
After responding message segments are identified,
we couple them with their respective initiating
segment to form a mini-summary based on their
subtopic. Each initializing segment has zero or
more responding segments. We also observed zero
response in human-written summaries where par-
ticipants initiated some question or concern, but
others failed to follow up on the discussion. The
AP process is repeated for each cluster created
previously. One or more subtopic-based mini-
summaries make up one final summary for each
chat log. Figure 2 shows an example. For longer
chat logs, the length of the final summary is arbi-
trarily averaged at 35% of the original.
6 Summary Evaluation
To evaluate the goodness of the system-produced
summaries, a set of reference summaries is used
for comparison. In this section, we describe the
manual procedure used to produce the reference
summaries, and the performances of our system
and two baseline systems.
6.1 Reference Summaries
Kernel Traffic digests are participant-written
summaries of the chat logs. Each digest mixes the
summary writer?s own narrative comments with
direct quotes (citing the authors) from the chat log.
As observed in Section 3.4, subtopics are inter-
mingled in each digest. Authors use key phrases to
link the contents of each subtopic throughout texts.
In Figure 3, we show an example of such a digest.
Discussion participants? names are in italics and
subtopics are in bold. In this example, the conver-
sation was started by Benjamin Reed with two
questions: 1) asking for conventions for writing
/proc drivers, and 2) asking about the status of
sysctl. The summary writer indicated that Linus
Torvalds replied to both questions and used the
phrase ?for the ? question, he added?? to high-
light the answer to the second question. As the di-
Subtopic 1:
Benjamin Reed: I wrote a wireless ethernet driver a
while ago... Are driver writers recommended to use
that over extending /proc or is it deprecated?
Linus Torvalds: Syscyl is deprecated. It?s useful in one
way only ...
Subtopic 2:
Benjamin Reed: I am a bit uncomfortable ... wondering
for a while if there are guidelines on ?
Linus Torvalds: The thing to do is to create ...
Subtopic 3:
Marcin Dalecki: Are you just blind to the never-ending
format/ compatibility/ ? problems the whole idea
behind /proc induces inherently?
Figure 2. A system-produced summary.
Benjamin Reed wrote a wireless Ethernet driver that
used /proc as its interface. But he was a little uncom-
fortable ? asked if there were any conventions he
should follow. He added, ?and finally, what?s up with
sysctl? ??
Linus Torvalds replied with: ?the thing to do is to cre-
ate a ?[program code]. The /proc/drivers/ directory is
already there, so you?d basically do something like ?
[program code].? For the sysctl question, he added
?sysctl is deprecated. ...?
Marcin Dalecki flamed Linus: ?Are you just blind to
the never-ending format/compatibility/? problems the
whole idea behind /proc  induces inherently?
?[example]?
Figure 3. An original Kernel Traffic digest.
Mini 1:
Benjamin Reed wrote a wireless Ethernet driver that
used /proc as its interface. But he was a little uncom-
fortable ? and asked if there were any conventions he
should follow.
Linus Torvalds replied with: the thing to do is to create
a ?[program code]. The /proc/drivers/ directory is
already there, so you?d basically do something like ?
[program code].
Marcin Dalecki flamed Linus: Are you just blind to the
never-ending format/ compatibility/ ? problems the
whole idea behind /proc  induces inherently?
?[example]
Mini 2:
Benjamin Reed: and finally, what?s up with sysctl? ...
Linus Torvalds replied: sysctl is deprecated. ...
Figure 4. A reference summary reproduced
from a summary digest.
303
gest goes on, Marcin Dalecki only responded to the
first question with his excited commentary.
Since our system-produced summaries are sub-
topic-based and partitioned accordingly, if we use
unprocessed Kernel Traffic as references, the com-
parison would be rather complicated and would
increase the level of inconsistency in future as-
sessments.  We manually reorganized each sum-
mary digest into one or more mini-summaries by
subtopic (see Figure 4.) Examples (usually kernel
stats) and programs are reduced to ?[example]?
and ?[program code].? Quotes (originally in sepa-
rate messages but merged by the summary writer)
that contain multiple topics are segmented and the
participant?s name is inserted for each segment.
We follow clues like ?to answer ? question? to
pair up the main topics and their responses.
6.2 Summarization Results
We evaluated 10 chat logs. On average, each con-
tains approximately 50 multi-paragraph tiles (par-
titioned by TextTile) and 5 subtopics (clustered by
the method from Section 4).
A simple baseline system takes the first sentence
from each email in the sequence that they were
posted, based on the assumption that people tend to
put important information in the beginning of texts
(Position Hypothesis).
A second baseline system was built based on
constructing and analyzing the dialogue structure
of each chat log. Participants often quote portions
of previously posted messages in their responses.
These quotes link most of the messages from a
chat log. The message segment that immediately
follows the quote is automatically paired with the
quote itself and added to the summary and sorted
according to the timeline. Segments that are not
quoted in later messages are labeled as less rele-
vant and discarded. A resulting baseline summary
is an inter-connected structure of segments that
quoted and responded to one another. Figure 5 is a
shortened summary produced by this baseline for
the ongoing example.
The summary digests from Kernel Traffic
mostly consist of direct snippets from original
messages, thus making the reference summaries
extractive even after rewriting. This makes it pos-
sible to conduct an automatic evaluation. A com-
puterized procedure calculates the overlap between
reference and system-produced summary units.
Since each system-produced summary is a set of
mini-summaries based on subtopics, we also com-
pared the subtopics against those appearing in ref-
erence summaries (precision = 77.00%, recall =
74.33 %, F = 0.7566).
Recall Precision F-measure
Baseline1
30.79% 16.81% .2175
Baseline2
63.14% 36.54% .4629
Summary
52.57% 52.14% .5235
System
Topic-summ
52.57% 63.66% .5758
Table 3 shows the recall, precision, and F -
measure from the evaluation. From manual analy-
sis on the results, we notice that the original digest
writers often leave large portions of the discussion
out and focus on a few topics. We think this is be-
cause among the participants, some are Linux vet-
erans and others are novice programmers. Digest
writers recognize this difference and reflect it in
their writings, whereas our system does not. The
entry ?Topic-summ? in the table shows system-
produced summaries being compared only against
the topics discussed in the reference summaries.
6.3 Discussion
A recall of 30.79% from the simple baseline reas-
sures us the Position Hypothesis still applies in
conversational discussions. The second baseline
performs extremely well on recall, 63.14%. It
shows that quoted message segments, and thereby
derived dialogue structure, are quite indicative of
where the important information resides. Systems
built on these properties are good summarization
systems and hard-to-beat baselines. The system
described in this paper (Summary) shows an F-
measure of .5235, an improvement from .4629 of
the smart baseline. It gains from a high precision
because less relevant message segments are identi-
fied and excluded from the adjacent response pairs,
[0|0] Benjamin Reed:  ?I wrote an ? driver ? /proc
??
[0|1] Benjamin Reed: ?? /proc/ guideline ??
[0|2] Benjamin Reed: ?? syscyl ??
[1|0] Linus Torvalds responds to [0|0, 0|1, 0|2]: ?the
thing to do is ?? ?sysctl is deprecated ? ?
Figure 5. A short example from Baseline 2.
Table 3. Summary of results.
304
leaving mostly topic-oriented segments in summa-
ries. There is a slight improvement when assessing
against only those subtopics appeared in the refer-
ence summaries (Topic-summ). This shows that we
only identified clusters on their information con-
tent, not on their respective writers? experience and
reliability of knowledge.
In the original summary digests, interactions and
reactions between participants are sometimes de-
scribed. Digest writers insert terms like ?flamed?,
?surprised?, ?felt sorry?, ?excited?, etc. To analyze
social and organizational culture in a virtual envi-
ronment, we need not only information extracts
(implemented so far) but also passages that reveal
the personal aspect of the communications. We
plan to incorporate opinion identification into the
current system in the future.
7 Conclusion and Future Work
In this paper we have described a system that per-
forms intra-message topic-based summarization by
clustering message segments and classifying topic-
initiating and responding pairs. Our approach is an
initial step in developing a framework that can
eventually reflect the human interactions in virtual
environments. In future work, we need to prioritize
information according to the perceived knowl-
edgeability of each participant in the discussion, in
addition to identifying informative content and
recognizing dialogue structure. While the approach
to the detection of initiating-responding pairs is
quite effective, differentiating important and non-
important topic clusters is still unresolved and
must be explored.
References
M. S. Ackerman and C. Halverson. 2000. Reexaming
organizational memory. Communications of the
ACM, 43(1), 59?64.
A. Berger, S. Della Pietra, and V. Della Pietra. 1996. A
maximum entropy approach to natural language
processing. Computational Linguistics, 22(1):39?71.
M. Elliott and W. Scacchi. 2004. Free software devel-
opment: cooperation and conflict in a virtual organi-
zational culture. S. Koch (ed.), Free/Open Source
Software Development, IDEA publishing, 2004.
W. B. Frakes and R. Baeza-Yates. 1992. Information
retrieval: data structures & algorithms. Prentice Hall.
M. Galley, K. McKeown, J. Hirschberg, and E.
Shriberg. 2004. Identifying agreement and disagree-
ment in conversational speech: use of Bayesian net-
works to model pragmatic dependencies. In the
Proceedings of ACL-04.
M. A. Hearst. 1994. Multi-paragraph segmentation of
expository text. In the Proceedings of ACL 1994.
T. Joachims. 1998. Text categorization with support
vector machines: Learning with many relevant fea-
tures. In Proceedings of the ECML, pages 137?142.
D. Lam and S. L. Rohall. 2002. Exploiting e-mail
structure to improve summarization. Technical Paper
at IBM Watson Research Center #20?02.
S. Levinson. 1983. Pragmatics. Cambridge University
Press.
P. Newman and J. Blitzer. 2002. Summarizing archived
discussions: a beginning. In Proceedings of Intelli-
gent User Interfaces.
O. Rambow, L. Shrestha, J. Chen and C. Laurdisen.
2004. Summarizing email threads. In Proceedings of
HLT-NAACL 2004: Short Papers.
K. Ries. 2001. Segmenting conversations by topic, ini-
tiative, and style. In Proceedings of SIGIR Work-
shop: Information Retrieval Techniques for Speech
Applications 2001: 51?66.
E. A. Schegloff and H. Sacks. 1973. Opening up clos-
ings. Semiotica, 7-4:289?327.
S. Wan and K. McKeown. 2004. Generating overview
summaries of ongoing email thread discussions. In
Proceedings of COLING 2004.
J. H. Ward Jr. and M. E. Hook. 1963. Application of an
hierarchical grouping procedure to a problem of
grouping profiles. Educational and Psychological
Measurement, 23, 69?81.
K. Zechner. 2001. Automatic generation of concise
summaries of spoken dialogues in unrestricted do-
mains. In Proceedings of SIGIR 2001.
305
Template-Filtered Headline Summarization 
 
Liang Zhou and Eduard Hovy 
USC Information Sciences Institute 
4676 Admiralty Way 
Marina del Rey, CA 90292-6695 
{liangz, hovy}@isi.edu 
 
 
Abstract 
Headline summarization is a difficult task be-
cause it requires maximizing text content in 
short summary length while maintaining gram-
maticality. This paper describes our first attempt 
toward solving this problem with a system that 
generates key headline clusters and fine-tunes 
them using templates. 
1    Introduction 
Producing headline-length summaries is a chal-
lenging summarization problem. Every word be-
comes important. But the need for 
grammaticality?or at least intelligibility? some-
times requires the inclusion of non-content words. 
Forgoing grammaticality, one might compose a 
?headline? summary by simply listing the most 
important noun phrases one after another. At the 
other extreme, one might pick just one fairly in-
dicative sentence of appropriate length, ignoring 
all other material. Ideally, we want to find a bal-
ance between including raw information and sup-
porting intelligibility.  
We experimented with methods that integrate 
content-based and form-based criteria. The proc-
ess consists two phases. The keyword-clustering 
component finds headline phrases in the begin-
ning of the text using a list of globally selected 
keywords. The template filter then uses a collec-
tion of pre-specified headline templates and sub-
sequently populates them with headline phrases to 
produce the resulting headline.  
In this paper, we describe in Section 2 previous 
work. Section 3 describes a study on the use of 
headline templates. A discussion on the process of 
selecting and expanding key headline phrases is in 
Section 4. And Section 5 goes back to the idea of 
templates but with the help of headline phrases. 
Future work is discussed in Section 6.  
 
2    Related Work 
     
Several previous systems were developed to ad-
dress the need for headline-style summaries.  
A lossy summarizer that ?translates? news sto-
ries into target summaries using the ?IBM-style? 
statistical machine translation (MT) model was 
shown in (Banko, et al, 2000). Conditional prob-
abilities for a limited vocabulary and bigram tran-
sition probabilities as headline syntax 
approximation were incorporated into the transla-
tion model. It was shown to have worked surpris-
ingly well with a stand-alone evaluation of 
quantitative analysis on content coverage. The use 
of a noisy-channel model and a Viterbi search was 
shown in another MT-inspired headline summari-
zation system (Zajic, et al, 2002). The method 
was automatically evaluated by BiLingual Evalua-
tion Understudy (Bleu) (Papineni, et al, 2001) 
and scored 0.1886 with its limited length model.    
A nonstatistical system, coupled with linguisti-
cally motivated heuristics, using a parse-and-trim 
approach based on parse trees was reported in 
(Dorr, et al, 2003). It achieved 0.1341 on Bleu 
with an average of 8.5 words.  
Even though human evaluations were con-
ducted in the past, we still do not have sufficient 
material to perform a comprehensive comparative 
evaluation on a large enough scale to claim that 
one method is superior to others. 
 
3    First Look at the Headline Templates 
 
It is difficult to formulate a rule set that defines 
how headlines are written. However, we may dis-
cover how headlines are related to the templates 
derived from them using a training set of 60933 
(headline, text) pairs.  
 
3.1 Template Creation 
 
We view each headline in our training corpus as a 
potential template. For any new text(s), if we can 
select an appropriate template from the set and fill 
it with content words, then we will have a well-
structured headline. An abstract representation of 
the templates suitable for matching against new 
material is required. In our current work, we build  
templates at the part-of-speech (POS) level.  
 
3.2 Sequential Recognition of Templates 
 
We tested how well headline templates overlap 
with the opening sentences of texts by matching 
POS tags sequentially. The second column of Ta-
ble 1 shows the percentage of files whose POS-
level headline words appeared sequentially within 
the context described in the first column.  
 
Text Size Files from corpus (%) 
First sentence 20.01 
First two sentences 32.41 
First three sentences 41.90 
All sentences 75.55 
Table 1: Study on sequential template matching 
of a headline against its text, on training data  
     
3.3 Filling Templates with Key Words  
 
Filling POS templates sequentially using tagging 
information alone is obviously not the most ap-
propriate way to demonstrate the concept of head-
line summarization using template abstraction, 
since it completely ignores the semantic informa-
tion carried by words themselves.  
Therefore, using the same set of POS headline 
templates, we modified the filling procedure. 
Given a new text, each word (not a stop word) is 
categorized by its POS tag and ranked within each 
POS category according to its tf.idf weight. A 
word with the highest tf.dif weight from that POS 
category is chosen to fill each placeholder in a 
template. If the same tag appears more than once 
in the template, a subsequent placeholder is filled 
with a word whose weight is the next highest from 
the same tag category. The score for each filled 
template is calculated as follows: 
score _ t(i) =
W j
j =1
N
?
| desired _ len - template _ len |+1
 
where score_t(i) denotes the final score assigned 
to template i of up to N placeholders and Wj is the 
tf.idf weight of the word assigned to a placeholder 
in the template. This scoring mechanism prefers 
templates with the most desirable length. The 
highest scoring template-filled headline is chosen 
as the result.  
 
4    Key Phrase Selection 
 
The headlines generated in Section 3 are gram-
matical (by virtue of the templates) and reflect 
some content (by virtue of the tf.idf scores). But 
there is no guarantee of semantic accuracy! This 
led us to the search of key phrases as the candi-
dates for filling headline templates. Headline 
phrases should be expanded from single seed 
words that are important and uniquely reflect the 
contents of the text itself. To select the best seed 
words for key phrase expansion, we studied sev-
eral keyword selection models, described below.  
  
4. 1 Model Selection 
 
Bag-of-Words Models  
 
1) Sentence Position Model: Sentence position 
information has long proven useful in identifying 
topics of texts (Edmundson, 1969). We believe 
this idea also applies to the selection of headline 
words. Given a sentence with its position in text, 
what is the likelihood that it would contain the 
first appearance of a headline word: 
 
Count _ Posi = P(Hk |W j)
j =1
N
?
k=1
M
?  
P(Posi) =
Count _ Posi
Count _ PosQ
i =1
Q
?
 
Over all M texts in the collection and over all 
words from the corresponding M headlines (each 
has up to N words), Count_Pos records the num-
ber of times that sentence position i has the first 
appearance of any headline word Wj. P(Hk | Wj) is 
a binary feature. This is computed for all sentence 
positions from 1 to Q. Resulting P(Posi) is a table 
on the tendency of each sentence position contain-
ing one or more headlines words (without indicat-
ing exact words).  
2) Headline Word Position Model: For each 
headline word Wh , it would most likely first ap-
pear at sentence position Posi: 
 
P(Posi |Wh) =
Count(Posi,Wh )
Count(PosQ,Wh )
i=1
Q
?
 
The difference between models 1 and 2 is that 
for the sentence position model, statistics were 
collected for each sentence position i; for the 
headline word position model, information was 
collected for each headline word Wh.  
3) Text Model: This model captures the correla-
tion between words in text and words in headlines 
(Lin and Hauptmann, 2001):  
 
P(Hw |Tw) =
(doc _ tf (w, j) ? title _ tf (w, j))
j =1
M
?
doc _ tf (w, j)
j=1
M
?
 
doc_tf(w,j) denotes the term frequency of word w 
in the j th document of all M documents in the col-
lection. title_tf(w,j) is the term frequency of word 
w in the j th title. Hw and Tw are words that appear in 
both the headline and the text body. For each in-
stance of Hw and Tw pair, Hw = Tw.  
4) Unigram Headline Model: Unigram probabili-
ties on the headline words from the training set. 
5) Bigram Headline Model: Bigram probabilities 
on the headline words from the training set.  
 
Choice on Model Combinations  
 
Having these five models, we needed to determine 
which model or model combination is best suited 
for headline word selection. The blind data was 
the DUC2001 test set of 108 texts. The reference 
headlines are the original headlines with a total of 
808 words (not including stop words). The evalua-
tion was based on the cumulative unigram overlap 
between the n top-scoring words and the reference 
headlines. The models are numbered as in Section 
4.1. Table 2 shows the effectiveness of each 
model/model combination on the top 10, 20, 30, 
40, and 50 scoring words.  
    Clearly, for all lengths greater than 10,  sen-
tence position (model 1) plays the most important 
role in selecting headline words. Selecting the top 
50 words solely based on position information 
means that sentences in the beginning of a text are 
the most informative. However, when we are wor- 
 
Model(s) 10w 20w 30w 40w 50w 
1 2 3 4 5  79 118 147 189 216 
2 3 4 5  74 110 145 178 206 
1 3 4 5  74 116 146 176 208 
1 2 4 5  63 99 144 176 202 
1 2 3 5  87 122 155 187 223 
1 2 3 4  96 149 187 214 230 
3 4 5  61 103 134 170 199 
2 4 5  54 94 137 168 192 
2 3 5  82 117 148 183 212 
2 3 4  67 119 167 192 217 
1 4 5  55  101 126 149 193 
1 3 5  84 113 144 181 216 
1 3 4  97 144 186 212 234 
1 2 5  70 102 146 179 208 
1 4 5  55 101 126 149 193 
1 2 3  131 181 205 230 250 
4 5  46 84 117 140 182 
3 5 72 107 134 166 204 
3 4 58 103 136 165 196 
2 5 62 96 135 172 204 
2 4 38 80 114 144 179 
2 3  100 150 187 215 235 
1 5 72 98 139 158 203 
1 4 69 111 144 169 193 
1 3 154 204 244 271 292 
1 2 74 138 174 199 232 
5 58 84 114 140 171 
4 35 60 87 111 136 
3 86 137 169 208 227 
2 45 94 135 163 197 
1 113 234 275 298 310 
Table 2: Results on model combinations  
 
king with a more restricted length requirement, 
text model (model 3) adds advantage to the posi-
tion model (highlighted, 7th from the bottom of 
Table 2). As a result, the following combination 
of sentence position and text model was used:  
 
P(H |Wi) = P(H | Posi )? P(Hw i |Twi ) 
4.2    Phrase Candidates to Fill Templates 
Section 4.1 explained how we select headline-
worthy words. We now need to expand them into 
phrases as candidates for filling templates. As il-
lustrated in Table 2 and stated in (Zajic et al, 
2002), headlines from newspaper texts mostly use 
words from the beginning of the text. Therefore, 
we search for n-gram phrases comprising key-
words in the first part of the story. Using the 
model combination selected in Section 4.1, 10 
top-scoring words over the whole story are se-
lected and highlighted in the first 50 words of the 
text. The system should have the ability of pulling 
out the largest window of top-scoring words to 
form the headline. To help achieve grammatical-
ity, we produced bigrams surrounding each head-
line-worthy word (underlined), as shown in Figure 
1. From connecting overlapping bigrams in  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
sequence, one sees interpretable clusters of words 
forming. Multiple headline phrases are considered 
as candidates for template filling. Using a set of 
hand-written rules, dangling words were removed 
from the beginning and end of each headline 
phrase.   
 
 
5   Filling Templates with Phrases 
 
5.1    Method 
 
Key phrase clustering preserves text content, but 
lacks the complete and correct representation for 
structuring phrases. The phrases need to go 
through a grammar filter/reconstruction stage to 
gain grammaticality.  
A set of headline-worthy phrases with their cor-
responding POS tags is presented to the template 
filter. All templates in the collection are matched 
against each candidate headline phrase. Strict tag 
matching produces a small number of matching 
templates. To circumvent this problem, a more 
general tag-matching criterion, where tags belong-
ing to the same part-of-speech category can be 
matched interchangeably, was used.  
Headline phrases tend to be longer than most of 
the templates in the collection. This results in only 
partial matches between the phrases and the tem-
plates. A score of fullness on the phrase-template 
match is computed for each candidate template fti: 
 
fti =
length (t i) + matched _ length(hi )
length(t i) + length(h i)
 
 
ti is a candidate template and hi is a headline 
phrase. The top-scoring template is used to filter 
each headline phrase in composing the final multi-
phrase headline. Table 3 shows a random selec-
tion of the results produced by the system.  
 
Generated Headlines 
First Palestinian airlines flight depart Gaza?s airport  
Jerusalem/ suicide bombers targeted market Friday setting blasts 
U.S. Senate outcome apparently rests small undecided voters.  
Brussels April 30 European parliament approved Thursday join 
currency mechanism 
Hong Kong strong winds Sunday killing 150 / Philippines leav-
ing hundreds thousands homeless 
Chileans wish forget years politics repression 
Table 3: System-generated headlines. A headline 
can be concatenated from several phrases, sepa-
rated by ?/?s  
 
5.2   Evaluation 
 
Ideally, the evaluation should show the system?s 
performance on both content selection and gram-
maticality.  However, it is hard to measure the 
level of grammaticality achieved by a system 
computationally.  Similar to (Banko, et al, 2000), 
we restricted the evaluation to a quantitative 
analysis on content only.  
Our system was evaluated on previously unseen 
DUC2003 test data of 615 files. For each file, 
headlines generated at various lengths were com-
pared against i) the original headline, and ii) head-
lines written by four DUC2003 human assessors. 
The performance metric was to count term over-
laps between the generated headlines and the test 
standards.  
Table 4 shows the human agreement and the 
performance of the system comparing with the 
two test standards. P and R are the precision and 
recall scores. 
 
The system-generated headlines were also evalu-
ated using the automatic summarization evalua-
tion tool ROUGE (Recall-Oriented Understudy 
for Gisting Evaluation) (Lin and Hovy,  
Figure 1: Surrounding bigrams for top-scoring 
words 
Allegations  of police  racism  and   brutality  
 
have   shaken this city that for decades has  
 
prided itself on a progressive attitude toward  
 
civil  rights    and a reputation for racial  
 
harmony.  The death of  two blacks   at   a  
 
drug   raid   that went awry, followed 10 days  
 
later by a scuffle between police and? 
 
 Assessors? Generated 
P R Length  
(words) 
P R 
9 0.1167 0.1566 
12 0.1073 0.2092 
Original 
 
0.3429 
 
0.2336 
13 0.1075 0.2298 
9 0.1482 0.1351 
12 0.1365 0.1811 
Assessors?  
0.2186 
 
0.2186 
13 0.1368 0.1992 
Table 4: Results evaluated using unigram over-
lap  
  
  
 
 
2003). The ROUGE score is a measure of n-gram 
recall between candidate headlines and a set of 
reference headlines. Its simplicity and reliability 
are gaining audience and becoming a standard for 
performing automatic comparative summarization 
evaluation. Table 5 shows the ROUGE perform-
ance results for generated headlines with length 12 
against headlines written by human assessors.  
 
6    Conclusion and Future Work 
 
Generating summaries with headline-length re-
striction is hard because of the difficulty of 
squeezing a full text into a few words in a read-
able fashion. In practice, it often happens in order 
to achieve the optimal informativeness, grammati-
cal structure is overlooked, and vice versa. In this 
paper, we have described a system that was de-
signed to use two methods, individually had ex-
hibited exactly one of the two types of unbalances, 
and integrated them to yield content and gram-
maticality. 
Structural abstraction at the POS level is shown 
to be helpful in our current experiment. However, 
part-of-speech tags do not generalize well and fail 
to model issues like subcategorization and other 
lexical semantic effects. This problem was seen 
from the fact that there are half as many templates 
as the original headlines. A more refined pattern 
language, for example taking into account named 
entity types and verb clusters, will further improve 
performance. We intend to incorporate additional 
natural language processing tools to create a more 
sophisticated and richer hierarchical structure for 
headline summarization. 
References 
Michele Banko, Vibhu Mittal, and Michael Wit-
brock. 2000. Headline generation based on sta-
tistical translation. In ACL-2000, pp. 318-325.  
Bonnie Dorr, David Zajic, and Richard Schwartz. 
2003. Hedge trimmer: a parse-and-trim ap-
proach to headline generation. In Proceedings 
of Workshop on Automatic  Summarization, 
2003.  
H. P. Edmundson. 1969. New methods in auto-
matic extracting. Journal of the ACM , 
16(2):264?285. 
Chin-Yew Lin and Eduard Hovy. 2003. Auto-
matic evaluation of summaries using n-gram 
co-occurrence statistics. In HLT-NAACL 2003, 
pp.150?157.  
Rong Lin and Alexander Hauptmann. 2001. Head-
line generation using a training corpus. In 
CICLING 2000.  
Kishore Papineni, Salim Roukos, Todd Ward, and 
Wei-Jin Zhu. 2001. IBM research report Bleu: a 
method for automatic evaluation of machine 
translation. In IBM Research Division Techni-
cal Report, RC22176 (W0109-22). 
David Zajic, Bonnie Dorr, and Richard Schwartz. 
2002. Automatic headline generation for news-
paper stories. In Proceedings of the ACL-2002 
Workshop on Text Summarization. 
 Human Generated 
Unigrams 0.292 0.169 
Bigrams 0.084 0.042 
Trigrams 0.030 0.010 
4-grams 0.012 0.002 
Table 5: Performance on ROUGE  
Multi-document Biography Summarization
Liang Zhou, Miruna Ticrea, Eduard Hovy
University of Southern California
Information Sciences Institute
4676 Admiralty Way
Marina del Rey, CA 90292-6695
                 {liangz, miruna, hovy} @isi.edu
Abstract
In this paper we describe a biography
summarization system using sentence
classification and ideas from information
retrieval. Although the individual techniques
are not new, assembling and applying them to
generate multi-document biographies is new.
Our system was evaluated in DUC2004. It is
among the top performers in task 5?short
summaries focused by person questions.
1 Introduction
Automatic text summarization is one form of
information management. It is described as
selecting a subset of sentences from a document
that is in size a small percentage of the original and
yet is just as informative. Summaries can serve as
surrogates of the full texts in the context of
Information Retrieval (IR). Summaries are created
from two types of text sources, a single document
or a set of documents. Multi-document
summarization (MDS) is a natural and more
elaborative extension of single-document
summarization, and poses additional difficulties on
algorithm design. Various kinds of summaries fall
into two broad categories: generic summaries are
the direct derivatives of the source texts; special-
interest summaries are generated in response to
queries or topic-oriented questions.
One important application of special-interest
MDS systems is creating biographies to answer
questions like ?who is Kofi Annan??. This task
would be tedious for humans to perform in
situations where the information related to the
person is deeply and sparsely buried in large
quantity of news texts that are not obviously
related. This paper describes a MDS biography
system that responds to the ?who is? questions by
identifying information about the person-in-
question using IR and classification techniques,
and creates multi-document biographical
summaries. The overall system design is shown in
Figure 1.
To determine what and how sentences are
selected and ranked, a simple IR method and
experimental classification methods both
contributed. The set of top-scoring sentences, after
redundancy removal, is the resulting biography.
As yet, the system contains no inter-sentence
?smoothing? stage.
In this paper, work in related areas is discussed
in Section 2; a description of our biography corpus
used for training and testing the classification
component is in Section 3; Section 4 explains the
need and the process of classifying sentences
according to their biographical state; the
application of the classification method in
biography extraction/summarization is described in
Section 5; an accompanying evaluation on the
quality of the biography summaries is shown in
Section 6; and future work is outlined in Section 7.
2 Recent Developments
Two trends have dominated automatic
summarization research (Mani, 2001). One is the
work focusing on generating summaries by
extraction, which is finding a subset of the
document that is indicative of its contents (Kupiec
et al, 1995) using ?shallow? linguistic analysis and
statistics.  The other influence is the exploration of
Figure 1. Overall design of the biography
summarization system.
?deeper? knowledge-based methods for
condensing information. Knight and Marcu (2000)
equate summarization with compression at
sentence level to achieve grammaticality and
information capture, and push a step beyond
sentence extraction. Many systems use machine-
learning methods to learn from readily aligned
corpora of scientific articles and their
corresponding abstracts. Zhou and Hovy (2003)
show a summarization system trained from
automatically obtained text-summary alignments
obeying the chronological occurrences of news
events.
MDS poses more challenges in assessing
similarities and differences among the set of
documents. The simple idea of extract-and-
concatenate does not respond to problems arisen
from coherence and cohesion. Barzilay et al
(1999) introduce a combination of extracted
similar phrases and a reformulation through
sentence generation. Lin and Hovy (2002) apply a
collect ion of  known single-document
summarization techniques, cooperating positional
and topical information, clustering, etc., and extend
them to perform MDS.
While many have suggested that conventional
MDS systems can be applied to biography
generation directly, Mani (2001) illustrates that the
added functionality of biographical MDS comes at
the expense of a substantial increase in system
complexity and is somewhat beyond the
capabilities of present day MDS systems. The
discussion was based in part on the only known
MDS biography system (Schiffman et al, 2001)
that uses corpus statistics along with linguistic
knowledge to select and merge description of
people in news. The focus of this work was on
synthesizing succinct descriptions of people by
merging appositives from semantic processing
using WordNet (Miller, 1995).
3 Corpus Description
In order to extract information that is related to a
person from a large set of news texts written not
exclusively about this person, we need to identify
attributes shared among biographies.
Biographies share certain standard components.
We annotated a corpus of 130 biographies of 12
people (activists, artists, leaders, politicians,
scientists, terrorists, etc.).  We found 9 common
elements: bio  (info on birth and death), f a m e
factor, personality, personal, social, education,
nationality, scandal, and w o r k .  Collected
biographies are appropriately marked at clause-
level with one of the nine tags in XML format, for
example:
Martin Luther King <nationality>
was born in Atlanta, Georgia
</nationality>. ? He <bio>was
assassinated on April 4, 1968
</bio>. ? King <education> entered
the Boston University as a
doctoral student </education>. ?
In all, 3579 biography-related phrases were
identified and recorded for the collection, among
them 321 bio , 423 fame , 114 personality, 465
personal, 293 social, 246 education, 95 nationality,
292 scandal, and 1330 work. We then used 100
biographies for training and 30 for testing the
classification module.
4 Sentence Classification
Relating to human practice on summarizing,
three main points are relevant to aid the automation
process (Sp?rck Jones, 1993).  The first is a strong
emphasis on particular purposes, e.g., abstracting
or extracting articles of particular genres.  The
second is the drafting, writing, and revision cycle
in constructing a summary.  Essentially as a
consequence of these first two points, the
summarizing process can be guided by the use of
checklists.  The idea of a checklist is especially
useful for the purpose of generating biographical
summaries because a complete biography should
contain various aspects of a person?s life.  From a
careful analysis conducted while constructing the
biography corpus, we believe that the checklist is
shared and common among all persons in question,
and consists the 9 biographical elements
introduced in Section 3.
The task of fulfilling the biography checklist
becomes a classification problem. Classification is
defined as a task of classifying examples into one
of a discrete set of possible categories (Mitchell,
1997).  Text categorization techniques have been
used extensively to improve the efficiency on
information retrieval and organization.  Here the
problem is that sentences, from a set of documents,
need to be categorized into different biography-
related classes.
4.1 Task Definitions
We designed two classification tasks:
      1) 10 -Class: Given one or more texts about a
person, the module must categorize each
sentence into one of ten classes.  The classes
are the 9 biographical elements plus a class
called none that collects all sentences without
biographical information. This fine-grained
classification task will be beneficial in
generating comprehensive biographies on
people of interest. The classes are:
bio
fame
personality
social
education
nationality
scandal
personal
work
none
       2) 2-Class: The module must make a binary
decision of whether the sentence should be
included in a biography summary. The classes
are:
bio
none
The label bio appears in both task definitions but
bears different meanings. Under 10-Class, class bio
contains information on a person?s birth or death,
and under 2-Class it sums up all 9 biographical
elements from the 10-Class.
4.2 Machine Learning Methods
We experimented with three machine learning
methods for classifying sentences.
Na?ve Bayes
The Na?ve Bayes classifier is among the most
effective algorithms known for learning to classify
text documents (Mitchell, 1997), calculating
explicit probabilities for hypotheses. Using k
features F
j
: j = 1, ?, k, we assign to a given
sentence S the class C:
? 
C = argmax
C
P(C |F
1
,F
2
,...,F
k
)
It can be expressed using Bayes? rule, as (Kupiec
et al, 1995):
? 
P(S ? C |F
1
,F
2
,...F
k
) =
P(F
1
,F
2
,...F
j
| S ? C)?P(S ? C)
P(F
1
,F
2
,...F
k
)
Assuming statistical independence of the
features:
? 
P(S ? C |F
1
,F
2
,...F
k
) =
P(F
j
| S ? C)?P(S ? C)
j=1
k
?
P(F
j
j=1
k
?
)
Since P(F
j
) has no role in selecting C:
? 
P(S ? C |F
1
,F
2
,...F
k
) = P(F
j
| S ? C)?P(S ? C)
j=1
k
?
We trained on the relative frequency of
P(F
j
|S?C) and P(S?C), with add-one smoothing.
This method was used in classifying both the 10-
Class and the 2-Class tasks.
Support Vector Machine
Support Vector Machines (SVMs) have been
shown to be an effective classifier in text
categorization.  We extend the idea of classifying
documents into predefined categories to classifying
sentences into one of the two biography categories
defined by the 2-Class task.  Sentences are
categorized based on their biographical saliency (a
percentage of clearly identified biography words)
and their non-biographical saliency (a percentage
of clearly identified non-biography words).  We
used LIBSVM (Chang and Lin, 2003) for training
and testing.
Decision Tree (4.5)
In addition to SVM, we also used a decision-tree
algorithm, C4.5 (Quinlan, 1993), with the same
training and testing data as SVM.
4.3 Classification Results
The lower performance bound is set by a
baseline system that randomly assigns a
biographical class given a sentence, for both 10-
Class and 2-Class. 2599 testing sentences are from
30 unseen documents.
10-Class Classification
The Na?ve Bayes classifier was used to perform
the 10-Class task.  Table 1 shows its performance
with various features.
Table 1. Performance of 10-Class sentence
classification, using Na?ve Bayes Classifier.
Part-of-speech (POS) information (Brill, 1995)
and word stems (Lovins, 1968) were used in some
feature sets.
We bootstrapped 10395 more biography-
indicating words by recording the immediate
hypernyms, using WordNet (Fellbaum, 1998), of
the words collected from the controlled biography
corpus described in Section 3. These words are
called Expanded Unigrams and their frequency
scores are reduced to a fraction of the original
word?s frequency score.
Some sentences in the testing set were labeled
with multiple biography classes due to the fact that
the original corpus was annotated at clause level.
Since the classification was done at sentence level,
we relaxed the matching/evaluating program
allowing a hit when any of the several classes was
matched. This is shown in Table 1 as the Relaxed
cases.
A closer look at the instances where the false
negatives occur indicates that the classifier
mislabeled instances of class work as instances of
class none.  To correct this error, we created a list
of 5516 work specific words hoping that this would
set a clearer boundary between the two classes.
However performance did not improve.
2-Class Classification
All three machine learning methods were
evaluated in classifying among 2 classes. The
results are shown in Table 2. The testing data is
slightly skewed with 68% of the sentences being
none.
In addition to using marked biographical phrases
as training data, we also expanded the
marking/tagging perimeter to sentence boundaries.
As shown in the table, this creates noise.
5 Biography Extraction
Biographical sentence classification module is
only one of two components that supply the overall
system with usable biographical contents, and is
followed by other stages of processing (see system
design in Figure 1).  We discuss the other modules
next.
5.1 Name-filter
A filter scans through all documents in the set,
eliminating sentences that are direct quotes,
dialogues, and too short (under 5 words).  Person-
oriented sentences containing any variation (first
name only, last name only, and the full name) of
the person?s name are kept for subsequent steps.
Sentences classified as biography-worthy are
merged with the name-filtered sentences with
duplicates eliminated.
5.2 Sentence Ranking
An essential capability of a multi-document
summarizer is to combine text passages in a useful
manner for the reader (Goldstein et al, 2000).
This includes a sentence ordering parameter (Mani,
2001).  Each of the sentences selected by the
name-filter and the biography classifier is either
related to the person-in-question via some news
event or referred to as part of this person?s
biographical profile, or both.  We need a
mechanism that will select sentences that are of
informative significance within the source
document set.  Using inverse-term-frequency
(ITF), i.e. an estimation of information value,
words with high information value (low ITF) are
distinguished from those with low value (high
ITF).  A sorted list of words along with their ITF
scores from a document set?topic ITFs?displays
the important events, persons, etc., from this
particular set of texts.  This allows us to identify
passages that are unusual with respect to the texts
about the person.
However, we also need to identify passages that
are unusual in general.  We have to quantify how
these important words compare to the rest of the
world.  The world is represented by 413307562
w o r d s  f r o m  T R E C - 9  c o r p u s
(http://trec.nist.gov/data.html), with corresponding
ITFs.
The overall informativeness of each word w is:
? 
C
w
=
d
itf
w
W
itf
w
where d
itf
 is the document set ITF of word w and
W
itf
 is the world ITF of w .  A word that occurs
frequently bears a lower C
w
 score compared to a
rarely used word (bearing high information value)
with a higher C
w
 score.
Top scoring sentences are then extracted
according to:
Table 2. Classification results on 2-Class using
Na?ve Bayes, SVM, and C4.5.
? 
C
s
=
C
w
i
i=1
n
?
len(s)
The following is a set of sentences extracted
according to the method described so far.  The
person-in-question is the famed cyclist Lance
Armstrong.
1. Cycling helped him win his
battle with cancer, and
cancer helped him win the
Tour de France.
2. Armstrong underwent four
rounds of intense
chemotherapy.
3. The surgeries and
chemotherapy eliminated the
cancer, and Armstrong began
his cycling comeback.
4. The foundation supports
cancer patients and survivors
through education, awareness
and research.
5. He underwent months of
chemotherapy.
5.3 Redundancy Elimination
Summaries that emphasize the differences across
documents while synthesizing common
information would be the desirable final results.
Removing similar information is part of all MDS
systems. Redundancy is apparent in the Armstrong
example from Section 5.2.  To eliminate repetition
while retaining interesting singletons, we modified
(Marcu, 1999) so that an extract can be
automatically generated by starting with a full text
and systematically removing a sentence at a time
as long as a stable semantic similarity with the
original text is maintained. The original extraction
algorithm was used to automatically create large
volume of (extract, abstract, text) tuples for
training extraction-based summarization systems
with (abstract, text) input pairs.
Top-scoring sentences selected by the ranking
mechanism described in Section 5.2 were the input
to this component. The removal process was
repeated until the desired summary length was
achieved.
Applying this method to the Armstrong example,
the result leaves only one sentence that contains
the topics ?chemotherapy? and ?cancer?.  It
chooses sentence 3, which is not bad, though
sentence 1 might be preferable.
6 Evaluation
6.1 Overview
Extrinsic and intrinsic evaluations are the two
classes of text summarization evaluation methods
(Sparck Jones and Galliers, 1996).  Measuring
content coverage or summary informativeness is an
approach commonly used for intrinsic evaluation.
It measures how much source content was
preserved in the summary.
A complete evaluation should include
evaluations of the accuracy of components
involved in the summarization process (Schiffman
et al, 2001).  Performance of the sentence
classifier was shown in Section 4.  Here we will
show the performance of the resulting summaries.
6.2 Coverage Evaluation
An intrinsic evaluation of biography summary
was recently conducted under the guidance of
Document Understanding Conference (DUC2004)
using the automatic summarization evaluation tool
ROUGE (Recall-Oriented Understudy for Gisting
Evaluation) by Lin and Hovy (2003).  50 TREC
English document clusters, each containing on
average 10 news articles, were the input to the
system. Summary length was restricted to 665
bytes. Brute force truncation was applied on longer
summaries.
The ROUGE-L metric is based on Longest
Common Subsequence (LCS) overlap (Saggion et
al., 2002).  Figure 2 shows that our system (86)
performs at an equivalent level with the best
systems 9 and 10, that is, they both lie within our
system?s 95% upper confidence interval.  The 2-
class classification module was used in generating
the answers. The figure also shows the
performance data evaluated with lower and higher
confidences set at 95%. The performance data are
from official DUC results.
Figure 3 shows the performance results of our
system 86, using 10-class sentence classification,
comparing to other systems from DUC by
replicating the official evaluating process. Only
system 9 performs slightly better with its score
being higher than our system?s 95% upper
confidence interval.
A baseline system (5) that takes the first 665
bytes of the most recent text from the set as the
resulting biography was also evaluated amongst
the peer systems. Clearly, humans still perform at a
level much superior to any system.
Measuring fluency and coherence is also
important in reflecting the true quality of machine-
generated summaries.  There is no automated tool
for this purpose currently.  We plan to incorporate
one for the future development of this work.
6.3 Discussion
N-gram recall scores are computed by ROUGE,
in addition to ROUGE-L shown here. While cosine
similarity and unigram and bigram overlap
demonstrate a sufficient measure on content
coverage, they are not sensitive on how
information is sequenced in the text (Saggion et al,
2002). In evaluating and analyzing MDS results,
metrics, such as ROUGE-L, that consider linguistic
sequence are essential.
Radev and McKeown (1998) point out when
summarizing interesting news events from multiple
sources, one can expect reports with contradictory
and redundant information. An intelligent
summarizer should attain as much information as
possible, combine it, and present it in the most
concise form to the user. When we look at the
different attributes in a person?s life reported in
news articles, a person is described by the job
positions that he/she has held, by education
institutions that he/she has attended, and etc. Those
data are confirmed biographical information and
do not bear the necessary contradiction associated
with evolving news stories. However, we do feel
the need to address and resolve discrepancies if we
were to create comprehensive and detailed
0.25
0.3
0.35
0.4
0.45
0.5
0.55
B E F H G A D C 9 10 11 12 13 86 15 16 17 18 19 20 5 22 23 24 25 26 27 28 29 30 31
ROUGE-L
95% CI Lower
95% CI Higher
Figure 2. Official ROUGE performance results from DUC2004. Peer systems are labeled with numeric IDs.
Humans are numbered A?H. 86 is our system with 2-class biography classification. Baseline is 5.
0.17
0.22
0.27
0.32
0.37
0.42
0.47
0.52
0.57
B F E G H A D C 9 10 11 12 13 86 15 16 17 18 19 20 21 22 23 24 25 5 27 28 29 30 31
ROUGE-L
95% CL Lower
95% CL Higher
Figure 3. Unofficial ROUGE results. Humans are labeled A?H. Peer systems are labeled with numeric IDs.
86 is our system with 10-class biography classification. Baseline is 5.
biographies on people-in-news since miscellaneous
personal facts are often overlooked and told in
conflicting reports. Misrepresented biographical
information may well be controversies and may
never be clarified. The scandal element from our
corpus study (Section 3) is sufficient to identify
information of the disputed kind.
Extraction-based MDS summarizers, such as this
one, present the inherent problem of lacking the
discourse-level fluency. While sentence ordering
for single document summarization can be
determined from the ordering of sentences in the
input article, sentences extracted by a MDS system
may be from different articles and thus need a
strategy on ordering to produce a fluent surface
summary (Barzilay et al, 2002). Previous
summarization systems have used temporal
sequence as the guideline on ordering. This is
especially true in generating biographies where a
person is represented by a sequence of events that
occurred in his/her life. Barzilay et al also
introduced a combinational method with an
alternative strategy that approximates the
information relatedness across the input texts. We
plan to use a fixed-form structure for the majority
of answer construction, fitted for biographies only.
This will be a top-down ordering strategy, contrary
to the bottom-up algorithm shown by Barzilay et
al.
7 Conclusion and Future Work
In this paper, we described a system that uses IR
and text categorization techniques to provide
summary-length answers to biographical questions.
The core problem lies in extracting biography-
related information from large volumes of news
texts and composing them into fluent, concise,
multi-document summaries.  The summaries
generated by the system address the question about
the person, though not listing the chronological
events occurring in this person?s life due to the
lack of background information in the news
articles themselves.  In order to obtain a ?normal?
biography, one should consult other means of
information repositories.
Question: Who is Sir John Gielgud?
Answer: Sir John Gielgud, one of
the great actors of the English
stage who enthralled audiences for
more than 70 years with his
eloquent voice and consummate
artistry, died Sunday at his home
Gielgud?s last major film role was
as a surreal Prospero in Peter
Greenaway?s controversial
Shakespearean rhapsody.
Above summary does not directly explain who
the person-in-question is, but indirectly does so in
explanatory sentences.  We plan to investigate
combining fixed-form and free-form structures in
answer construction. The summary would include
an introductory sentence of the form ?x is
<type/fame-category> ??, possibly through
querying outside online resources.  A main body
would follow the introduction with an assembly of
checklist items generated from the 10-Class
classifier.  A conclusion would contain open-ended
items of special interest.
Furthermore, we would like to investigate
compression strategies in creating summaries,
specifically for biographies.  Our biography corpus
was tailored for this purpose and will be the
starting point for further investigation.
Acknowledgement
We would like to thank Chin-Yew Lin from ISI
for many insightful discussions on MDS,
biography generation, and ROUGE.
References
Regina Barzilay, Kathleen McKeown, and Michael
Elhadad. 1999. Information fusion in the context
of multi-document summarization. In
Proceedings of the 37
th
 Annual Meeting of the
Association of Computational Linguistics (ACL-
99), University of Maryland, 1999, pp. 550?557.
Regina Barzilay, Noemie Elhadad, Kathleen
McKeown. 2002. Inferring strategies for
sentence ordering in multidocument
summarization. JAIR, 17:35?55, 2002.
Eric Brill. 1995. Transformation-based error-
driven learning and natural language processing:
A case study in part of speech tagging.
Computational Linguistics, December 1995.
Chih-Chung Chang and Chih-Jen Lin. 2003.
LIBSVM?A Library for support vector
machines.
http://www.csie.ntu.edu.tw/~cjlin/libsvm/
Christiane Fellbaum, editor. WordNet: An
electronic lexical database. Cambridge, MA:
MIT Press.
Jade Goldstein, Vibhu Mittal, Jaime Carbonell, and
Mark Kantrowitz. 2000. Multi-document
summarization by sentence extraction. In
Proceedings of the ANLP?2000 Workshop on
Automatic Summarization,  40?48.  New
Brunswick, New Jersey: Association for
Computational Linguistics.
Thorsten Joachims. 1998. Text categorization with
support vector machines: Learning with many
relevant features. In Proceedings of the
European Conference on Machine Learning
(ECML), pages 137?142.
Kevin Knight and Daniel Marcu. 2000. Statistics-
Based summarization step one: sentence
compression. In Proceedings of the 17
th
 National
Conference of the American Association for
Artificial Intelligence (AAAI 2000).
Julian Kupiec, Jan Pedersen, and Francine Chen.
1995. A trainable document summarizer. In
SIGIR?95 , Proceedings of the 18
th
 Annual
International ACM SIGIR Conference on
Research and Development in Information
Retrieval, pp. 68?73.
Chin-Yew Lin and Eduard Hovy. 2002. Automated
multi-document summarization in NeATS. In
Proceedings of the Human Language
Technology Conference (HLT2002), San Diego,
CA, U.S.A., March 23-27, 2002.
Chin-Yew Lin and Eduard Hovy. 2003. Automatic
evaluation of summaries using n-gram co-
occurrence statistics. In HLT-NAACL 2003:
Main Proceedings, pp.150?157.
Julie Beth Lovins. 1968. Development of a
stemming algorithm. Mechanical translation and
computational linguistics, 11:22?31, 1968.
Inderjeet Mani. 2001. Automatic summarization
(natural language processing, 3).
Inderjeet Mani. 2001. Recent developments in text
summarization. In CIKM?2001, Proceedings of
the Tenth International Conference on
Information and Knowledge Management,
November 5-10, 2001, 529?531.
Daniel Marcu. 1999. The automatic construction of
large-scale corpora for summarization research.
The 22
nd
 International ACM SIGIR Conference
on Research and Development in Information
Retrieval (SIGIR?99), pages 137-144, Berkeley,
CA, August 1999.
George Miller. 1995. WordNet: a lexical database
for English. Communications of the ACM, pages
39?41.
Tom Mitchell. 1997. Machine Learning. McGraw
Hill, 1997.
Ross J. Quinlan. 1993. C4.5: Programs for
machine learning. San Mateo, CA: Morgan
Kaufmann.
Dragomir R. Radev, Kathleen McKeown. 1998.
Generating natural language summaries from
multiple on-line sources. Computational
Linguistics 24(3): 469?500 (1998).
Horacio Saggion, Dragomir Radev, Simone Teufel,
and Wai Lam. Meta-evaluation of summaries in
a cross-lingual environment using content-based
metrics. In Proceedings of COLING?2002,
Taipei, Taiwan, August 2002.
Barry Schiffman, Inderjeet Mani, and Kristian
Concepcion. 2001. Producing biographical
summaries: combining linguistic knowledge with
corpus statistics. In Proceedings of the 39
th
Annual Meeting of the Association for
Computational Linguistics (ACL?2001),
450?457. New Brunswick, New Jersey:
Association for Computational Linguistics.
Karen Sp?rck Jones and Julia R. Galliers. 1996.
Evaluating Natural Language Processing
Systems: An Analysis and Review. Lecture Notes
in Artificial Intelligence 1083. Berlin: Springer.
Karen Sp?rck Jones. 1993. What might be in a
summary? Information Retrieval 1993: 9?26.
Liang Zhou and Eduard Hovy. A web-trained
extraction summarization system. In
Proceedings of the Human Language
Technology Conference (HLT-NAACL 2003),
pages 284?290.
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 77?84,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Re-evaluating Machine Translation Results with Paraphrase Support 
Liang Zhou, Chin-Yew Lin, and Eduard Hovy 
University of Southern California 
Information Sciences Institute 
4676 Admiralty Way 
Marina del Rey, CA 90292-6695 
{liangz, cyl, hovy} @isi.edu 
      
 
  
 
Abstract 
In this paper, we present ParaEval, an 
automatic evaluation framework that uses 
paraphrases to improve the quality of 
machine translation evaluations. Previous 
work has focused on fixed n-gram 
evaluation metrics coupled with lexical 
identity matching. ParaEval addresses 
three important issues: support for para-
phrase/synonym matching, recall meas-
urement, and correlation with human 
judgments. We show that ParaEval corre-
lates significantly better than BLEU with 
human assessment in measurements for 
both fluency and adequacy. 
1 Introduction 
The introduction of automated evaluation proce-
dures, such as BLEU (Papineni et al, 2001) for 
machine translation (MT) and ROUGE (Lin and 
Hovy, 2003) for summarization, have prompted 
much progress and development in both of these 
areas of research in Natural Language Processing 
(NLP). Both evaluation tasks employ a compari-
son strategy for comparing textual units from 
machine-generated and gold-standard texts. Ide-
ally, this comparison process would be per-
formed manually, because of humans? abilities to 
infer, paraphrase, and use world knowledge to 
relate differently worded pieces of equivalent 
information. However, manual evaluations are 
time consuming and expensive, thus making 
them a bottleneck in system development cycles.  
BLEU measures how close machine-generated 
translations are to professional human transla-
tions, and ROUGE does the same with respect to 
summaries. Both methods incorporate the com-
parison of a system-produced text to one or more 
corresponding reference texts. The closeness be-
tween texts is measured by the computation of a 
numeric score based on n-gram co-occurrence 
statistics. Although both methods have gained 
mainstream acceptance and have shown good 
correlations with human judgments, their defi-
ciencies have become more evident and serious 
as research in MT and summarization progresses 
(Callison-Burch et al, 2006).   
Text comparisons in MT and summarization 
evaluations are performed at different text granu-
larity levels. Since most of the phrase-based, 
syntax-based, and rule-based MT systems trans-
late one sentence at a time, the text comparison 
in the evaluation process is also performed at the 
single-sentence level. In summarization evalua-
tions, there is no sentence-to-sentence corre-
spondence between summary pairs?essentially 
a multi-sentence-to-multi-sentence comparison, 
making it more difficult and requiring a com-
pletely different implementation for matching 
strategies. In this paper, we focus on the intrica-
cies involved in evaluating MT results and ad-
dress two prominent problems associated with 
the BLEU-esque metrics, namely their lack of 
support for paraphrase matching and the absence 
of recall scoring. Our solution, ParaEval, utilizes 
a large collection of paraphrases acquired 
through an unsupervised process?identifying 
phrase sets that have the same translation in an-
other language?using state-of-the-art statistical 
MT word alignment and phrase extraction meth-
ods. This collection facilitates paraphrase match-
ing, additionally coupled with lexical identity 
matching which is designed for comparing 
text/sentence fragments that are not consumed by 
paraphrase matching. We adopt a unigram count-
ing strategy for contents matched between sen-
tences from peer and reference translations. This 
unweighted scoring scheme, for both precision 
and recall computations, allows us to directly 
examine both the power and limitations of 
77
ParaEval.  We show that ParaEval is a more sta-
ble and reliable comparison mechanism than 
BLEU, in both fluency and adequacy rankings.  
This paper is organized in the following way: 
Section 2 shows an overview on BLEU and lexi-
cal identity n-gram statistics; we describe ParaE-
val?s implementation in detail in Section 3; the 
evaluation of ParaEval is shown in Section 4; 
recall computation is discussed in Section 5; in 
Section 6, we discuss the differences between 
BLEU and ParaEval when the numbers of refer-
ence translations change; and we conclude and 
discuss future work in Section 7.  
2  N-gram Co-occurrence Statistics 
Being an $8 billion industry (Browner, 2006), 
MT calls for rapid development and the ability to 
differentiate good systems from less adequate 
ones. The evaluation process consists of compar-
ing system-generated peer translations  to human 
written reference translations  and assigning a 
numeric score to each system. While human as-
sessments are still the most reliable evaluation 
measurements, it is not practical to solicit manual 
evaluations repeatedly while making incremental 
system design changes that would only result in 
marginal performance gains. To overcome the 
monetary and time constraints associated with 
manual evaluations, automated procedures have 
been successful in delivering benchmarks for 
performance hill-climbing with little or no cost.  
While a variety of automatic evaluation meth-
ods have been introduced, the underlining com-
parison strategy is similar?matching based on 
lexical identity. The most prominent implemen-
tation of this type of matching is demonstrated in 
BLEU (Papineni et al 2002). The remaining part 
of this section is devoted to an overview of 
BLEU, or the BLEU-esque philosophy.  
2 .1  The BL E U-esque Matching Philosophy 
The primary task that a BLEU-esque procedure 
performs is to compare n-grams from the peer 
translation with the n-grams from one or more 
reference translations and count the number of 
matches. The more matches a peer translation 
gets, the better it is.  
BLEU is a precision-based metric, which is 
the ratio of the number of n-grams from the peer 
translation that occurred in reference translations 
to the total number of n-grams in the peer trans-
lation. The notion of Modified n-gram Precisi on  
was introduced to detect and avoid rewarding 
false positives generated by translation systems. 
To gain high precision, systems could potentially 
over-generate ?good? n-grams, which occur mul-
tiple times in multiple references. The solution to 
this problem was to adopt the policy that an n-
gram, from both reference and peer translations, 
is considered exhausted after participating in a 
match. As a result, the maximum number of 
matches an n-gram from a peer translation can 
receive, when comparing to a set of reference 
translations, is the maximum number of times 
this n-gram occurred in any single reference 
translation. Papineni et al (2002) called this cap-
ping technique clipp ing.  Figure 1, taken from the 
original BLEU paper, demonstrates the computa-
tion of the modified unigram precision for a peer 
translation sentence.  
To compute the modified n-gram precision, 
P n, for a whole test set, including all translation 
segments (usually in sentences), the formula is: 
 
2 .2  Lack of Paraphrasing Support 
Humans are very good at finding creative ways 
to convey the same information. There is no one 
definitive reference translation in one language 
for a text written in another. Having acknowl-
edged this phenomenon, however natural it is, 
human evaluations on system-generated transla-
tions are much more preferred and trusted. How-
ever, what humans can do with ease puts ma-
chines at a loss. BLEU-esque procedures recog-
nize equivalence only when two n-grams exhibit 
the same surface-level representations, i.e. the 
same lexical identities. The BLEU implementa-
tion addresses its deficiency in measuring seman-
tic closeness by incorporating the comparison 
with multiple reference translations. The ration-
ale is that multiple references give a higher 
chance that the n-grams, assuming correct trans-
lations, appearing in the peer translation would 
be rewarded by one of the reference?s n-grams. 
The more reference translations used, the better 
Figure 1. Modified n-gram precision from
BLEU.
 
78
the matching and overall evaluation quality. Ide-
ally (and to an extreme), we would need to col-
lect a large set of human-written translations to 
capture all possible combinations of verbalizing 
variations before the translation comparison pro-
cedure reaches its optimal matching ability.  
One can argue that an infinite number of ref-
erences are not needed in practice because any 
matching procedure would stabilize at a certain 
number of references. This is true if precision 
measure is the only metric computed. However, 
using precision scores alone unfairly rewards 
systems that ?under-generate??producing un-
reasonably short translations. Recall measure-
ments would provide more balanced evaluations. 
When using multiple reference translations, if an 
n-gram match is made for the peer, this n-gram 
could appear in any of the references. The com-
putation of recall becomes difficult, if not impos-
sible. This problem can be reversed if there is 
crosschecking for phrases occurring across refer-
ences?paraphrase recognition. BLEU uses the 
calculation of a brevity penalty to compensate 
the lack of recall computation problem. The 
brevity penalty is computed as follows: 
 
Then, the BLEU score for a peer translation is 
computed as: 
 
BLEU?s adoption of the brevity penalty to off-
set the effect of not having a recall computation 
has drawn criticism on its crudeness in measur-
ing translation quality. Callison-Burch et al 
(2006) point out three prominent factors: 
? ``Synonyms and paraphrases are only 
handled if they are in the set of multiple 
reference translations [available].  
? The scores for words are equally 
weighted so missing out on content-
bearing material brings no additional pen-
alty.  
? The brevity penalty is a stop-gap meas-
ure to compensate for the fairly serious 
problem of not being able to calculate re-
call.? 
With the introduction of ParaEval, we will ad-
dress two of these three issues, namely the para-
phrasing problem and providing a recall meas-
ure.  
3  ParaEval for MT Evaluation 
3.1  Overview 
Reference translations are created from the same 
source text (written in the foreign language) to 
the target language. Ideally, they are supposed to 
be semantically equivalent, i.e. overlap com-
pletely. However, as shown in Figure 2, when 
matching based on lexical identity is used (indi-
cated by links), only half (6 from the left and 5 
from the right) of the 12 words from these two 
sentences are matched. Also, ?to? was a mis-
match. In applying paraphrase matching for MT 
evaluation from ParaEval, we aim to match all 
shaded words from both sentences. 
3 .2  Paraphrase Acquisition 
The process of acquiring a large enough collec-
tion of paraphrases is not an easy task. Manual 
corpus analyses produce domain-specific collec-
tions that are used for text generation and are 
application-specific. But operating in multiple 
domains and for multiple tasks translates into 
multiple manual collection efforts, which could 
be very time-consuming and costly. In order to 
facilitate smooth paraphrase utilization across a 
variety of NLP applications, we need an unsu-
pervised paraphrase collection mechanism that 
can be easily conducted, and produces para-
phrases that are of adequate quality and can be 
readily used with minimal amount of adaptation 
effort.  
Our method (Anonymous, 2006), also illus-
trated in (Bannard and Callison-Burch, 2005), to 
automatically construct a large domain-
independent paraphrase collection is based on the 
assumption that two different phrases of the 
same meaning may have the same translation in a 
Figure 2. Two reference translations. Grey
areas are matched by using BLEU.
79
foreign language. Phrase-based Statistical Ma-
chine Translation (SMT) systems analyze large 
quantities of bilingual parallel texts in order to 
learn translational alignments between pairs of 
words and phrases in two languages (Och and 
Ney, 2004). The sentence-based translation 
model makes word/phrase alignment decisions 
probabilistically by computing the optimal model 
parameters with application of the statistical es-
timation theory. This alignment process results in 
a corpus of word/phrase-aligned parallel sen-
tences from which we can extract phrase pairs 
that are translations of each other. We ran the 
alignment algorithm from (Och and Ney, 2003) 
on a Chinese-English parallel corpus of 218 mil-
lion English words, available from the Linguistic 
Data Consortium (LDC). Phrase pairs are ex-
tracted by following the method described in 
(Och and Ney, 2004) where all contiguous 
phrase pairs having consistent alignments are 
extraction candidates. Using these pairs we build 
paraphrase sets by joining together all English 
phrases that have the same Chinese translation. 
Figure 3 shows an example word/phrase align-
ment for two parallel sentence pairs from our 
corpus where the phrases ?blowing up? and 
?bombing? have the same Chinese translation. 
On the right side of the figure we show the para-
phrase set which contains these two phrases, 
which is typical in our collection of extracted 
paraphrases.  
Although our paraphrase extraction method is 
similar to that of (Bannard and Callison-Burch, 
2005), the paraphrases we extracted are for com-
pletely different applications, and have a broader 
definition for what constitutes a paraphrase. In 
(Bannard and Callison-Burch, 2005), a language 
model is used to make sure that the paraphrases 
extracted are direct substitutes, from the same 
syntactic categories, etc. So, using the example 
in Figure 3, the paraphrase table would contain 
only ?bombing? and ?bombing attack?. Para-
phrases that are direct substitutes of one another 
are useful when translating unknown phrases. 
For instance, if a MT system does not have the 
Chinese translation for the word ?bombing?, but 
has seen it in another set of parallel data (not in-
volving Chinese) and has determined it to be a 
direct substitute of the phrase ?bombing attack?, 
then the Chinese translation of ?bombing attack? 
would be used in place of the translation for 
?bombing?. This substitution technique has 
shown some improvement in translation quality 
(Callison-Burch et al, 2006).  
3 .3  The ParaEval Evaluation Procedure 
We adopt a two-tier matching strategy for MT 
evaluation in ParaEval. At the top tier, a para-
phrase match is performed on system-translated 
sentences and corresponding reference sentences. 
Then, unigram matching is performed on the 
words not matched by paraphrases. Precision is 
measured as the ratio of the total number of 
words matched to the total number of words in 
the peer translation.  
Running our system on the example in Figure 
2, the paraphrase-matching phase consumes the 
words marked in grey and aligns ?have been? 
and ?to be?, ?completed? and ?fully?, ?to date? 
and ?up till now?, and ?sequence? and ?se-
quenced?. The subsequent unigram-matching 
aligns words based on lexical identity.  
We maintain the computation of modified uni-
gram precisi on , defined by the BLEU-esque Phi-
losophy, in principle. In addition to clipping in-
dividual candidate words  with their correspond-
ing maximum reference counts (only for words 
not matched by paraphrases), we clip candidate 
paraphrases  by their maximum reference para-
phrase counts. So two completely different 
phrases in a reference sentence can be counted as 
two occurrences of one phrase. For example in 
Figure 4, candidate phrases ?blown up? and 
?bombing? matched with three phrases from the 
references, namely ?bombing? and two instances 
of ?explosion?. Treating these two candidate 
phrases as one (paraphrase match), we can see its 
clip is 2 (from Ref 1, where ?bomb ing? and ?ex-
plosion? are counted as two occurrences of a sin-
gle phrase). The only word that was matched by 
its lexical identity is ?was?. The modified uni-
gram precision calculated by our method is 4/5, 
where as BLEU gives 2/5.  
Figure 3. An example of the paraphrase extraction
process.
80
4  Evaluating ParaEval 
To be effective in MT evaluations, an automated 
procedure should be capable of distinguishing 
good translation systems from bad ones, human 
translations from systems?, and human transla-
tions of differing quality. For a particular evalua-
tion exercise, an evaluation system produces a 
ranking for system and human translations, and 
compares this ranking with one created by hu-
man judges (Turian et al, 2003). The closer a 
system?s ranking is to the human?s, the better the 
evaluation system is. 
4 .1  Validating ParaEval 
To test ParaEval?s ability, NIST 2003 Chinese 
MT evaluation results were used (NIST 2003). 
This collection consists of 100 source documents 
in Chinese, translations from eight individual 
translation systems, reference translations from 
four humans, and human assessments (on flu-
ency and adequacy). The Spearman rank-order 
coefficient is computed as an indicator of how 
close a system ranking is to gold-standard human 
ranking. It should be noted that the 2003 MT 
data is separate from the corpus that we extracted 
paraphrases from.  
For comparison purposes, BLEU 1  was also 
run. Table 1 shows the correlation figures for the 
two automatic systems with the NIST rankings 
on fluency and adequacy. The lower and higher 
95% confidence intervals are labeled as ?L-CI? 
and ?H-CI?. To estimate the significance of the 
rank-order correlation figures, we applied boot-
strap resampling to calculate the confidence in-
tervals.  In each of 1000 runs, systems were 
ranked based on their translations of 100 ran-
domly selected documents.  Each ranking was 
compared with the NIST ranking, producing a 
correlation score for each run. A t-test was then 
                                                
1 Results shown are from BLEU v.11 (NIST).  
performed on the 1000 correlation scores. In both 
fluency and adequacy measurements, ParaEval 
correlates significantly better than BLEU. The 
ParaEval scores used were precision scores. In 
addition to distinguishing the quality of MT sys-
tems, a reliable evaluation procedure must be 
able to distinguish system translations from hu-
mans? (Lin and Och, 2004). Figure 5 shows the 
overall system and human ranking. In the upper 
left corner, human translators are grouped to-
gether, significantly separated from the auto-
matic MT systems clustered into the lower right 
corner.  
4 .2  Implications to Word-alignment 
We experimented with restricting the para-
phrases being matched to various lengths. When 
allowing only paraphrases of three or more 
words to match, the correlation figures become 
stabilized and ParaEval achieves even higher 
correlation with fluency measurement to 0.7619 
on the Spearman ranking coefficient.   
This phenomenon indicates to us that the bi-
gram and unigram paraphrases extracted using 
SMT word-alignment and phrase extraction pro-
grams are not reliable enough to be applied to 
evaluation tasks. We speculate that word pairs 
extracted from (Liang et al, 2006), where a bidi-
rectional discriminative training method was 
used to achieve consensus for word-alignment 
Figure 4. ParaEval?s matching process.
 
BLEU ParaEval
Fluency 0.6978 0.7575
95% L-CI 0.6967 0.7553
95% H-CI 0.6989 0.7596
Adequacy 0.6108 0.6918
95% L-CI 0.6083 0.6895
95% H-CI 0.6133 0.694
Table 1. Ranking correlations with human
assessments.
 
Figure 5. Overall system and human ranking.
 
81
(mostly lower n-grams), would help to elevate 
the level of correlation by ParaEval.  
4 .3  Implications to Evaluating Paraphrase 
Quality 
Utilizing paraphrases in MT evaluations is also a 
realistic way to measure the quality of para-
phrases acquired through unsupervised channels. 
If a comparison strategy, coupled with para-
phrase matching, distinguishes good and bad MT 
and summarization systems in close accordance 
with what human judges do, then this strategy 
and the paraphrases used are of sufficient quality. 
Since our underlining comparison strategy is that 
of BLEU-1 for MT evaluation, and BLEU has 
been proven to be a good metric for their respec-
tive evaluation tasks, the performance of the 
overall comparison is directly and mainly af-
fected by the paraphrase collection.  
5  ParaEval?s Support for Recall Com-
putation 
Due to the use of multiple references and allow-
ing an n-gram from the peer translation to be 
matched with its corresponding n-gram from any 
of the reference translations, BLEU cannot be 
used to compute recall scores, which are conven-
tionally paired with precision to detect length-
related problems from systems under evaluation.  
5 .1  Using Single References for Recall 
The primary goal in using multiple references is 
to overcome the limitation in matching on lexical 
identity. More translation choices give more 
variations in verbalization, which could lead to 
more matches between peer and reference trans-
lations. Since MT results are generated and 
evaluated at a sentence-to-sentence level (or a 
segment level, where each segment may contain 
a small number of sentences) and no text con-
densation is employed, the number of different 
and correct ways to state the same sentence is 
small. This is in comparison to writing generic 
multi-document summaries, each of which con-
tains multiple sentences and requires significant 
amount of ?rewriting?. When using a large col-
lection of paraphrases while evaluating, we are 
provided with the alternative verbalizations 
needed. This property allows us to use single 
references to evaluate MT results and compute 
recall measurements.  
5 .2  Recall and Adequacy Correlations 
When validating the computed recall scores for 
MT systems, we correlate with human assess-
ments on adequacy only. The reason is that ac-
cording to the definition of recall, the content 
coverage in references, and not the fluency re-
flected from the peers, is being measured. Table 
2 shows ParaEval?s recall correlation with NIST 
2003 Chinese MT evaluation results on systems 
ranking. We see that ParaEval?s correlation with 
adequacy has improved significantly when using 
recall scores to rank than using precision scores.  
5 .3  Not All Single References are Created 
Equal 
Human-written translations differ not only in 
word choice, but also in other idiosyncrasies that 
cannot be captured with paraphrase recognition. 
So it would be presumptuous to declare that us-
ing paraphrases from ParaEval is enough to al-
low using just one reference translation to evalu-
ate. Using multiple references allow more para-
phrase sets to be explored in matching.  
In Table 3, we show ParaEval?s correlation 
figures when using single reference translations. 
E01?E04 indicate the sets of human translations 
used correspondingly.  
Notice that the correlation figures vary a great 
deal depending on the set of single references 
used. How do we differentiate human transla-
tions and know which set of references to use? It 
is difficult to quantify the quality that a human 
written translation reflects. We can only define 
?good? human translations as translations that 
are written not very differently from what other 
humans would write, and ?bad? translations as 
the ones that are written in an unconventional 
fashion. Table 4 shows the differences between 
the four sets of reference translations when com-
BLEU ParaEval
Adequacy 0.6108 0.7373
95% L-CI 0.6083 0.7368
95% H-CI 0.6133 0.7377
Table 2. ParaEval?s recall ranking correlation.
 
Table 3. ParaEval?s correlation (precision)
while using only single references.
E01 E02 E03 E04
Fluency 0.683 0.6501 0.7284 0.6192
95% L-CI 0.6795 0.6482 0.7267 0.6172
95% H-CI 0.6864 0.6519 0.73 0.6208
Adequacy 0.6308 0.5741 0.6688 0.5858
95% L-CI 0.6266 0.5705 0.665 0.5821
95% H-CI 0.635 0.5777 0.6727 0.5895
 
82
paring one set of references to the other three. 
The scores here are the raw ParaEval precision 
scores. E01 and E03 are better, which explains 
the higher correlations ParaEval has using these 
two sets of references individually, shown in Ta-
ble 3.  
6  Observation of Change in Number of 
References 
When matching on lexical identity, it is the gen-
eral consensus that using more reference transla-
tions would increase the reliability of the MT 
evaluation (Turian et al, 2003). It is expected 
that we see an improvement in ranking correla-
tions when moving from using one reference 
translation to more. However, when running 
BLEU for the NIST 2003 Chinese MT evalua-
tion, this trend is inverted, and using single refer-
ence translation gave higher correlation than us-
ing all four references, as illustrated in Table 5.  
Turian et al (2003) reports the same peculiar 
behavior from BLEU on Arabic MT evaluations 
in Figure 5b of their paper. When using three 
reference translations, as the number of segments 
(sentences usually) increases, BLEU correlates 
worse than using single references.  
Since the matching and underlining counting 
mechanisms of ParaEval are built upon the 
fundamentals of BLEU, we were keen to find out 
the differences, other than paraphrase matching, 
between the two methods when the number of 
reference translation changes. By following the 
description from the original BLEU paper, three 
incremental steps were set up for duplicating its 
implementation, namely modified unigram preci-
sion (MUP), geometric mean of MUP (GM), and 
multiplying brevity penalty with GM to get the 
final score (BP-BLEU). At each step, correla-
tions were computed for both using single- and 
multi- references, shown in Table 6a, b, and c. 
 Given that many small changes have been 
made to the original BLEU design, our replica-
tion would not produce the same scores from the 
current version of BLEU. Nevertheless, the in-
verted behavior was observed in fluency correla-
tions at the BP-BLEU step, not at MUP and GM. 
This indicates to us that the multiplication of the 
brevity penalty to balance precision scores is 
problematic. According to (Turian et al, 2003), 
correlation scores computed from using fewer 
references are inflated because the comparisons 
exclude the longer n-gram matches that make 
automatic evaluation procedures diverge from 
the human judgments. Using a large collection of 
paraphrases in comparisons allows those longer 
n-gram matches to happen even if single refer-
ences are used. This collection also allows 
ParaEval to directly compute recall scores, 
avoiding an approximation of recall that is 
problematic.  
ParaEval 95% L-CI 95% H- C I
E01 0.8086 0.8 0 .8172
E02 0.7383 0.7268 0.7497
E03 0.7839 0.7754 0.7923
E04 0.7742 0.7617 0.7866
Table 4. Differences among reference
translations (raw ParaEval precision
scores).  
6(a). System-ranking correlation when using modified
unigram precision (MUP) scores.
6(b). System-ranking correlation when using geometric mean
(GM) of MUPs.
6(c). System-ranking correlation when multiplying the
brevity penalty with GM.
Table 6. Incremental implementation of
BLEU and the correlation behavior at the
three steps: MUP, GM, and BP-BLEU.
MUP E01 E02 E03 E04 4 refs
Fluency 0.6597 0.6216 0.6923 0.4912 0.692
95% L-CI 0.6568 0.6189 0.6917 0.4863 0.6915
95% H-CI 0.6626 0.6243 0.6929 0.496 0.6925
Adequacy 0.5818 0.5459 0.6141 0.4602 0.6165
95% L-CI 0.5788 0.5432 0.6132 0.4566 0.6156
95% H-CI 0.5847 0.5486 0.6151 0.4638 0.6174
GM E01 E02 E03 E04 4 refs
Fluency 0.6633 0.6228 0.6925 0.4911 0.6922
95% L-CI 0.6604 0.6201 0.692 0.4862 0.6918
95% H-CI 0.6662 0.6255 0.6931 0.4961 0.6929
Adequacy 0.5817 0.548 0.615 0.4641 0.6159
95% L-CI 0.5813 0.5453 0.614 0.4606 0.615
95% H-CI 0.5871 0.5508 0.616 0.4676 0.6169
BP-BLEU E01 E02 E03 E04 4 refs
Fluency 0.6637 0.6227 0.6921 0.4947 0.5743
95% L-CI 0.6608 0.62 0.6916 0.4899 0.5699
95% H-CI 0.6666 0.6254 0.6927 0.4996 0.5786
Adequacy 0.5812 0.5486 0.5486 0.5486 0.6671
95% L-CI 0.5782 0.5481 0.5458 0.5458 0.6645
95% H-CI 0.5842 0.5514 0.5514 0.5514 0.6697
 
Table 5. BLEU?s correlating behavior with
multi- and single-reference.
BLEU E01 E02 E03 E04 4 refs
Fluency 0.7114 0.701 0.7084 0.7192 0.6978
95% L-CI 0.7099 0.6993 0.7065 0.7177 0.6967
95% H-CI 0.7129 0.7026 0.7102 0.7208 0.6989
Adequacy 0.644 0.6238 0.6535 0.675 0.6108
95% L-CI 0.6404 0.6202 0.6496 0.6714 0.6083
95% H-CI 0.6476 0.6274 0.6574 0.6786 0.6133
83
7  Conclusion and Future Work 
In this paper, we have described ParaEval, an 
automatic evaluation framework for measuring 
machine translation results. A large collection of 
paraphrases, extracted through an unsupervised 
fashion using SMT methods, is used to improve 
the quality of the evaluations. We addressed 
three important issues, the paraphrasing support, 
the computation of recall measurement, and pro-
viding high correlations with human judgments.  
Having seen that using paraphrases helps a 
great deal in evaluation tasks, naturally the next 
task is to explore the possibility in paraphrase 
induction. The question becomes how to use con-
textual information to calculate semantic close-
ness between two phrases. Can we expand the 
identification of paraphrases to longer ones, ide-
ally sentences?  
The problem in which content bearing words 
carry the same weights as the non-content bear-
ing ones is not addressed. From examining the 
paraphrase extraction process, it is unclear how 
to relate translation probabilities and confidences 
with semantic closeness. We plan to explore the 
parallels between the two to enable a weighted 
implementation of ParaEval.  
Reference 
Anonymous. 2006. Complete citation omitted due to 
the blind review process.  
Bannard, C. and C. Callison-Burch. 2005. Paraphras-
ing with bilingual parallel corpora. Proceedings of 
ACL-20 0 5 .  
Browner, J. 2006. The translator?s blues. 
http://www.slate.com/id/2133922/.  
Callison-Burch, P. Koehn, and M. Osborne. 2006. 
Improved statistical machine translation using 
paraphrases. In Proceedings of H L T / NA A CL-20 0 6 .  
Callison-Burch, C., M. Osborne, and P. Koehn. 2006. 
Re-evaluating the role of bleu in machine transla-
tion research. In Proceedings of EA CL-20 0 6 .  
Inkpen, D. Z. and G. Hirst. 2003. Near-synonym 
choice in natural language generation. Proceedings 
of R A NL P-20 0 3 .  
Leusch, G., N. Ueffing, and H. Ney. 2003. A novel 
string-to-string distance measure with applications 
to machine translation evaluation. In Proceedings 
of MT  Summit I X .  
Liang, P., B. Taskar, and D. Klein. Consensus of sim-
ple unsupervised models for word alignment. In 
Proceedings in H LT / N AA CL-2 0 0 6 .  
Lin, C.Y. and E. Hovy. 2003. Automatic evaluation of 
summaries using n-gram co-occurrence statistics. 
Proceedings of the H L T-20 0 3 . 
Lin, C.Y. and F. J. Och. 2004. Automatic evaluation 
of machine translation quality using longest com-
mon subsequence and skip-bigram statistics. Pro-
ceedings of A CL- 20 0 4 .  
Och, F. J. and H. Ney. 2003. A systematic comparison 
of various statistical alignment models. Computa-
tional Linguist ics , 29(1): 19?51, 2003.  
Och, F. J. and H. Ney. 2004. The alignment template 
approach to statistical machine translation. Compu-
tational L inguis tics , 30(4), 2004.  
Papineni, K., S. Roukos, T. Ward, and W. J. Zhu. 
2002. IBM research report Bleu: a method for 
automatic evaluation of machine translation I B M  
Research D iv is ion Technical Report , RC22176, 
2001.  
Turian, J. P., L. Shen, and I. D. Melamed. 2003. 
Evaluation of machine translation and its evalua-
tion. Proceedings of MT  Summit I X .  
 
84
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 434?439,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Improving Twitter Sentiment Analysis with Topic-Based Mixture
Modeling and Semi-Supervised Training
Bing Xiang *
IBM Watson
1101 Kitchawan Rd
Yorktown Heights, NY 10598, USA
bingxia@us.ibm.com
Liang Zhou
Thomson Reuters
3 Times Square
New York, NY 10036, USA
l.zhou@thomsonreuters.com
Abstract
In this paper, we present multiple ap-
proaches to improve sentiment analysis
on Twitter data. We first establish a
state-of-the-art baseline with a rich fea-
ture set. Then we build a topic-based sen-
timent mixture model with topic-specific
data in a semi-supervised training frame-
work. The topic information is generated
through topic modeling based on an ef-
ficient implementation of Latent Dirich-
let Allocation (LDA). The proposed sen-
timent model outperforms the top system
in the task of Sentiment Analysis in Twit-
ter in SemEval-2013 in terms of averaged
F scores.
1 Introduction
Social media, such as Twitter and Facebook, has
attracted significant attention in recent years. The
vast amount of data available online provides a
unique opportunity to the people working on nat-
ural language processing (NLP) and related fields.
Sentiment analysis is one of the areas that has
large potential in real-world applications. For ex-
ample, monitoring the trend of sentiment for a spe-
cific company or product mentioned in social me-
dia can be useful in stock prediction and product
marketing.
In this paper, we focus on sentiment analysis of
Twitter data (tweets). It is one of the challenging
tasks in NLP given the length limit on each tweet
(up to 140 characters) and also the informal con-
versation. Many approaches have been proposed
previously to improve sentiment analysis on Twit-
ter data. For example, Nakov et al (2013) provide
an overview on the systems submitted to one of the
SemEval-2013 tasks, Sentiment Analysis in Twit-
ter. A variety of features have been utilized for
* This work was done when the author was with Thom-
son Reuters.
sentiment classification on tweets. They include
lexical features (e.g. word lexicon), syntactic fea-
tures (e.g. Part-of-Speech), Twitter-specific fea-
tures (e.g. emoticons), etc. However, all of these
features only capture local information in the data
and do not take into account of the global higher-
level information, such as topic information.
Two example tweets are given below, with the
word ?offensive? appearing in both of them.
? Im gonna post something that might be offen-
sive to people in Singapore.
? #FSU offensive coordinator Randy Sanders
coached for Tennessee in 1st #BCS title
game.
Generally ?offensive? is used as a negative word
(as in the first tweet), but it bears no sentiment in
the second tweet when people are talking about a
football game. Even though some local contextual
features could be helpful to distinguish the two
cases above, they still may not be enough to get the
sentiment on the whole message correct. Also, the
local features often suffer from the sparsity prob-
lem. This motivates us to explore topic informa-
tion explicitly in the task of sentiment analysis on
Twitter data.
There exists some work on applying topic in-
formation in sentiment analysis, such as (Mei et
al., 2007), (Branavan et al, 2008), (Jo and Oh,
2011) and (He et al, 2012). All these work are
significantly different from what we propose in
this work. Also they are conducted in a domain
other than Twitter. Most recently, Si et al (2013)
propose a continuous Dirichlet Process Mixture
model for Twitter sentiment, for the purpose of
stock prediction. Unfortunately there is no eval-
uation on the accuracy of sentiment classification
alone in that work. Furthermore, no standard train-
ing or test corpus is used, which makes compari-
son with other approaches difficult.
Our work is organized in the following way:
434
? We first propose a universal sentiment model
that utilizes various features and resources.
The universal model outperforms the top
system submitted to the SemEval-2013 task
(Mohammad et al, 2013), which was trained
and tested on the same data. The universal
model serves as a strong baseline and also
provides an option for smoothing later.
? We introduce a topic-based mixture model
for Twitter sentiment. The model is inte-
grated in the framework of semi-supervised
training that takes advantage of large amount
of un-annotated Twitter data. Such a mixture
model results in further improvement on the
sentiment classification accuracy.
? We propose a smoothing technique through
interpolation between universal model and
topic-based mixture model.
? We also compare different approaches for
topic modeling, such as cross-domain topic
identification by utilizing data from newswire
domain.
2 Universal Sentiment Classifier
In this section we present a universal topic-
independent sentiment classifier to establish a
state-of-the-art baseline. The sentiment labels are
either positive, neutral or negative.
2.1 SVM Classifier
Support Vector Machine (SVM) is an effec-
tive classifier that can achieve good performance
in high-dimensional feature space. An SVM
model represents the examples as points in space,
mapped so that the examples of the different cate-
gories are separated by a clear margin as wide as
possible. In this work an SVM classifier is trained
with LibSVM (Chang and Lin, 2011), a widely
used toolkit. The linear kernel is found to achieve
higher accuracy than other kernels in our initial ex-
periments. The option of probability estimation in
LibSVM is turned on so that it can produce the
probability of sentiment class c given tweet x at
the classification time, i.e. P (c|x).
2.2 Features
The training and testing data are run through
tweet-specific tokenization, similar to that used in
the CMU Twitter NLP tool (Gimpel et al, 2011).
It is shown in Section 5 that such customized tok-
enization is helpful. Here are the features that we
use for classification:
? Word N-grams: if certain N-gram (unigram,
bigram, trigram or 4-gram) appears in the
tweet, the corresponding feature is set to 1,
otherwise 0. These features are collected
from training data, with a count cutoff to
avoid overtraining.
? Manual lexicons: it has been shown in other
work (Nakov et al, 2013) that lexicons with
positive and negative words are important to
sentiment classification. In this work, we
adopt the lexicon from Bing Liu (Hu and
Liu, 2004) which includes about 2000 posi-
tive words and 4700 negative words. We also
experimented with the popular MPQA (Wil-
son et al, 2005) lexicon but found no extra
improvement on accuracies. A short list of
Twitter-specific positive/negative words are
also added to enhance the lexicons. We gen-
erate two features based on the lexicons: total
number of positive words or negative words
found in each tweet.
? Emoticons: it is known that people use emoti-
cons in social media data to express their
emotions. A set of popular emoticons are col-
lected from the Twitter data we have. Two
features are created to represent the presence
or absence of any positive/negative emoti-
cons.
? Last sentiment word: a ?sentiment word? is
any word in the positive/negative lexicons
mentioned above. If the last sentiment word
found in the tweet is positive (or negative),
this feature is set to 1 (or -1). If none of the
words in the tweet is sentiment word, it is set
to 0 by default.
? PMI unigram lexicons: in (Mohammad et
al., 2013) two lexicons were automatically
generated based on pointwise mutual infor-
mation (PMI). One is NRC Hashtag Senti-
ment Lexicon with 54K unigrams, and the
other is Sentiment140 Lexicon with 62K un-
igrams. Each word in the lexicon has an as-
sociated sentiment score. We compute 7 fea-
tures based on each of the two lexicons: (1)
sum of sentiment score; (2) total number of
435
positive words (with score s > 1); (3) to-
tal number of negative words (s < ?1); (4)
maximal positive score; (5) minimal negative
score; (6) score of the last positive words; (7)
score of the last negative words. Note that for
the second and third features, we ignore those
with sentiment scores between -1 and 1, since
we found that inclusion of those weak subjec-
tive words results in unstable performance.
? PMI bigram lexicon: there are also 316K bi-
grams in the NRC Hashtag Sentiment Lexi-
con. For bigrams, we did not find the sen-
timent scores useful. Instead, we only com-
pute two features based on counts only: total
number of positive bigrams; total number of
negative bigrams.
? Punctuations: if there exists exclamation
mark or question mark in the tweet, the fea-
ture is set to 1, otherwise set to 0.
? Hashtag count: the number of hashtags in
each tweet.
? Negation: we collect a list of negation words,
including some informal words frequently
observed in online conversations, such as
?dunno? (?don?t know?), ?nvr? (?never?),
etc. For any sentiment words within a win-
dow following a negation word and not af-
ter punctuations ?.?, ?,?, ?;?, ???, or ?!?, we re-
verse its sentiment from positive to negative,
or vice versa, before computing the lexicon-
based features mentioned earlier. The win-
dow size was set to 4 in this work.
? Elongated words: the number of words in the
tweet that have letters repeated by at least 3
times in a row, e.g. the word ?gooood?.
3 Topic-Based Sentiment Mixture
3.1 Topic Modeling
Latent Dirichlet Allocation (LDA) (Blei et al,
2003) is one of the widely adopted generative
models for topic modeling. The fundamental idea
is that a document is a mixture of topics. For each
document there is a multinomial distribution over
topics, and a Dirichlet prior Dir(?) is introduced
on such distribution. For each topic, there is an-
other multinomial distribution over words. One of
the popular algorithms for LDA model parameter
estimation and inference is Gibbs sampling (Grif-
fiths and Steyvers, 2004), a form of Markov Chain
Monte Carlo. We adopt the efficient implementa-
tion of Gibbs sampling as proposed in (Yao et al,
2009) in this work.
Each tweet is regarded as one document. We
conduct pre-processing by removing stop words
and some of the frequent words found in Twitter
data. Suppose that there are T topics in total in the
training data, i.e. t
1
, t
2
, ..., t
T
. The posterior prob-
ability of each topic given tweet x
i
is computed as
in Eq. 1:
P
t
(t
j
|x
i
) =
C
ij
+ ?
j
?
T
k=1
C
ik
+ T?
j
(1)
where C
ij
is the number of times that topic t
j
is
assigned to some word in tweet x
i
, usually aver-
aged over multiple iterations of Gibbs sampling.
?
j
is the j-th dimension of the hyperparameter of
Dirichlet distribution that can be optimized during
model estimation.
3.2 Sentiment Mixture Model
Once we identify the topics for tweets in the train-
ing data, we can split the data into multiple sub-
sets based on topic distributions. For each subset,
a separate sentiment model can be trained. There
are many ways of splitting the data. For example,
K-means clustering can be conducted based on
the similarity between the topic distribution vec-
tors or their transformed versions. In this work,
we assign tweet x
i
to cluster j if P
t
(t
j
|x
i
) > ?
or P
t
(t
j
|x
i
) = max
k
P
t
(t
k
|x
i
). Note that this is
a soft clustering, with some tweets possibily as-
signed to multiple topic-specific clusters. Similar
to the universal model, we train T topic-specific
sentiment models with LibSVM.
During classification on test tweets, we run
topic inference and sentiment classification with
multiple sentiment models. They jointly deter-
mine the final probability of sentiment class c
given tweet x
i
as the following in a sentiment mix-
ture model:
P (c|x
i
) =
T
?
j=1
P
m
(c|t
j
, x
i
)P
t
(t
j
|x
i
) (2)
where P
m
(c|t
j
, x
i
) is the probability of sentiment
c from topic-specific sentiment model trained on
topic t
j
.
436
3.3 Smoothing
Additionally, we also experiment with a smooth-
ing technique through linear interpolation between
the universal sentiment model and topic-based
sentiment mixture model.
P (c|x
i
) = ? ? P
U
(c|x
i
) + (1? ?)
?
T
?
j=1
P
m
(c|t
j
, x
i
)P
t
(t
j
|x
i
) (3)
where ? is the interpolation parameter and
P
U
(c|x
i
) is the probability of sentiment c given
tweet x
i
from the universal sentiment model.
4 Semi-supervised Training
In this section we propose an integrated frame-
work of semi-supervised training that contains
both topic modeling and sentiment classification.
The idea of semi-supervised training is to take
advantage of large amount low-cost un-annotated
data (tweets in this case) to further improve the ac-
curacy of sentiment classification. The algorithm
is as follows:
1. Set training corpus D for sentiment classifi-
cation to be the annotated training data D
a
;
2. Train a sentiment model with current training
corpus D;
3. Run sentiment classification on the un-
annotated data D
u
with the current sentiment
model and generate probabilities of sentiment
classes for each tweet, P (c|x
i
);
4. Perform data selection. For those tweets with
P (c|x
i
) > p, add them to current training
corpus D. The rest is used to replace the un-
annotated corpus D
u
;
5. Train a topic model on D, and store the topic
inference model and topic distributions of
each tweet;
6. Cluster data in D based on the topic distribu-
tions from Step 5 and train a separate senti-
ment model for each cluster. Replace current
sentiment model with the new sentiment mix-
ture model;
7. Repeat from Step 3 until finishing a pre-
determined number of iterations or no more
data is added to D in Step 4.
5 Experimental Results
5.1 Data and Evaluation
We conduct experiments on the data from the task
B of Sentiment Analysis in Twitter in SemEval-
2013. The distribution of positive, neutral and
negative data is shown in Table 1. The develop-
ment set is used to tune parameters and features.
The test set is for the blind evaluation.
Set Pos Neu Neg Total
Training 3640 4586 1458 9684
Dev 575 739 340 1654
Test 1572 1640 601 3813
Table 1: Data from SemEval-2013. Pos: positive;
Neu: neutral; Neg: negative.
For semi-supervised training experiments, we
explored two sets of additional data. The first
one contains 2M tweets randomly sampled from
the collection in January and February 2014. The
other contains 74K news documents with 50M
words collected during the first half year of 2013
from online newswire.
For evaluation, we use macro averaged F score
as in (Nakov et al, 2013), i.e. average of the F
scores computed on positive and negative classes
only. Note that this does not make the task a binary
classification problem. Any errors related to neu-
tral class (false positives or false negatives) will
negatively impact the F scores.
5.2 Universal Model
In Table 2, we show the incremental improvement
in adding various features described in Section 2,
measured on the test set. In addition to the fea-
tures, we also find SVM weighting on the training
samples is helpful. Due to the skewness in class
distribution in the training set, it is observed dur-
ing error analysis on the development set that sub-
jective (positive/negative) tweets are more likely
to be classified as neutral tweets. The weights for
positive, neutral and negative samples are set to
be (1, 0.4, 1) based on the results on the develop-
ment set. As shown in Table 2, weighting adds a
2% improvement. With all features combined, the
universal sentiment model achieves 69.7 on aver-
age F score. The F score from the best system in
SemEval-2013 (Mohammad et al, 2013) is also
listed in the last row of Table 2 for a comparison.
437
Model Avg. F score
Baseline with word N-grams 55.0
+ tweet tokenization 56.1
+ manual lexicon features 62.4
+ emoticons 62.8
+ last sentiment word 63.7
+ PMI unigram lexicons 64.5
+ hashtag counts 65.0
+ SVM weighting 67.0
+ PMI bigram lexicons 68.2
+ negations 69.0
+ elongated words 69.7
Mohammad et al, 2013 69.0
Table 2: Results on the test set with universal sen-
timent model.
5.3 Topic-Based Mixture Model
For the topic-based mixture model and semi-
supervised training, based on the experiments on
the development set, we set the parameter ? used
in soft clustering to 0.4, the data selection pa-
rameter p to 0.96, and the interpolation parame-
ter for smoothing ? to 0.3. We found no more
noticeable benefits after two iterations of semi-
supervised training. The number of topics is set
to 100.
The results on the test set are shown Table 3,
with the topic information inferred from either
Twitter data (second column) or newswire data
(third column). The first row shows the per-
formance of the universal sentiment model as
a baseline. The second row shows the results
from re-training the universal model by simply
adding tweets selected from two iterations of
semi-supervised training (about 100K). It serves
as another baseline with more training data, for
a fair comparison with the topic-based mixture
modeling that uses the same amount of training
data.
We also conduct an experiment by only consid-
ering the most likely topic for each tweet when
computing the sentiment probabilities. The results
show that the topic-based mixture model outper-
forms both the baseline and the one that considers
the top topics only. Smoothing with the universal
model adds further improvement in addition to the
un-smoothed mixture model. With the topic in-
formation inferred from Twitter data, the F score
is 2 points higher than the baseline without semi-
Model Tweet-topic News-topic
Baseline 69.7 69.7
+ semi-supervised 70.3 70.2
top topic only 70.6 70.4
mixture 71.2 70.8
+ smoothing 71.7 71.1
Table 3: Results of topic-based sentiment mixture
model on SemEval test set.
supervised training and 1.4 higher than the base-
line with semi-supervised data.
As shown in the third column in Table 3, sur-
prisingly, the model with topic information in-
ferred from the newswire data works well on the
Twitter domain. A 1.4 points of improvement can
be obtained compared to the baseline. This pro-
vides an opportunity for cross-domain topic iden-
tification when data from certain domain is more
difficult to obtain than others.
In Table 4, we provide some examples from the
topics identified in tweets as well as the newswire
data. The most frequent words in each topic are
listed in the table. We can clearly see that the top-
ics are about phones, sports, sales and politics, re-
spectively.
Tweet-1 Tweet-2 News-1 News-2
phone game sales party
call great stores government
answer play online election
question team retail minister
service win store political
text tonight retailer prime
texting super business state
Table 4: The most frequent words in example top-
ics from tweets and newswire data.
6 Conclusions
In this paper, we presented multiple approaches
for advanced Twitter sentiment analysis. We es-
tablished a state-of-the-art baseline that utilizes a
variety of features, and built a topic-based sen-
timent mixture model with topic-specific Twitter
data, all integrated in a semi-supervised training
framework. The proposed model outperforms the
top system in SemEval-2013. Further research is
needed to continue to improve the accuracy in this
difficult domain.
438
References
David Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet alocation. In Journal of Ma-
chine Learning Research. 3(2003), 993?1022.
S. R. K. Branavan, Harr Chen, Jacob Eisenstein, and
Regina Barzilay. 2008. Learning document-level
semantic properties from free-text annotations. In
Proceedings of the Annual Meeting of the Associa-
tion for Computational Linguistics (ACL-2008).
Chih-Chung Chang and Chih-Jen Lin. 2011. LIB-
SVM: A library for support vector machines. In
ACM Transactions on Intelligent Systems and Tech-
nology.
Kevin Gimpel, Nathan Schneider, Brendan O?Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein,
Michael Heilman, Dani Yogatama, Jeffrey Flanigan,
and Noah A. Smith. 2011. Part-of-speech tagging
for Twitter: annotation, features, and experiments.
In Proceedings of the Annual Meeting of the Associ-
ation for Computational Linguistics.
Thomas L. Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. In Proceedings of the National
Academy of Science. 101, 5228?5235.
Yulan He, Chenghua Lin, Wei Gao, and Kam-Fai
Wong. 2012. Tracking sentiment and topic dynam-
ics from social media. In Proceedings of the 6th In-
ternational AAAI Conference on Weblogs and Social
Media (ICWSM-2012).
Mingqing Hu and Bing Liu. 2004. Mining and sum-
marizing customer reviews. In Proceedings of the
Tenth ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining.
Yohan Jo and Alice Oh. 2011. Aspect and senti-
ment unification model for online review analysis.
In Proceedings of ACM Conference in Web Search
and Data Mining (WSDM-2011).
Qiaozhu Mei, Xu Ling, Matthew Wondra, Hang Su,
and ChengXiang Zhai. 2007. Topic sentiment mix-
ture: modeling facets and opinions in weblogs. In
Proceedings of International Conference on World
Wide Web (WWW-2007).
Saif M. Mohammad, Svetlana Kiritchenko, and Xiao-
dan Zhu. 2013. NRC-Canada: Building the state-
of-the-art in sentiment analysis of tweets. In Pro-
ceedings of the Seventh International Workshop on
Semantic Evaluation (SemEval 2013). 312-320, At-
lanta, Georgia, June 14-15, 2013.
Preslav Nakov, Zornitsa Kozareva, Alan Ritter, Sara
Rosenthal, Veselin Stoyanov, and Theresa Wilson.
2013. SemEval-2013 Task 2: Sentiment Analysis in
Twitter. In Proceedings of the Seventh International
Workshop on Semantic Evaluation (SemEval 2013).
312-320, Atlanta, Georgia, June 14-15, 2013.
Jianfeng Si, Arjun Mukherjee, Bing Liu, Qing Li,
Huayi Li, and Xiaotie Deng. 2013. Exploiting topic
based Twitter sentiment for stock prediction. In Pro-
ceedings of the 51st Annual Meeting of the Associ-
ation for Computational Linguistics. 24-29, Sofia,
Bulgaria, August 4-9,2013.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of the Con-
ference on Human Language Technology and Em-
pirical Methods in Natural Language Processing,
HLT 05.
Limin Yao, David Mimno, and Andrew McCallum.
2009. Efficient methods for topic model inference
on streaming document collections. KDD?09.
439
