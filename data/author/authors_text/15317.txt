Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 307?311,
Portland, Oregon, June 17-18, 2011. c?2011 Association for Computational Linguistics
Improving Pronominal and Deictic Co-Reference Resolution with
Multi-Modal Features
Lin Chen, Anruo Wang, Barbara Di Eugenio
Department of Computer Science
University of Illinois at Chicago
851 S Morgan ST, Chicago, IL 60607, USA
{lchen43,awang28,bdieugen}@uic.edu
Abstract
Within our ongoing effort to develop a com-
putational model to understand multi-modal
human dialogue in the field of elderly care,
this paper focuses on pronominal and deictic
co-reference resolution. After describing our
data collection effort, we discuss our anno-
tation scheme. We developed a co-reference
model that employs both a simple notion of
markable type, and multiple statistical mod-
els. Our results show that knowing the type
of the markable, and the presence of simulta-
neous pointing gestures improve co-reference
resolution for personal and deictic pronouns.
1 Introduction
Our ongoing research project, called RoboHelper,
focuses on developing an interface for older people
to effectively communicate with a robotic assistant
that can help them perform Activities of Daily Liv-
ing (ADLs) (Krapp, 2002), so that they can safely re-
main living in their home (Di Eugenio et al, 2010).
We are devising a multi-modal interface since peo-
ple communicate with one another using a variety of
verbal and non-verbal signals, including haptics, i.e.,
force exchange (as when one person hands a bowl to
another person, and lets go only when s/he senses
that the other is holding it). We have collected a
mid size multi-modal human-human dialogue cor-
pus, that we are currently processing and analyz-
ing. Meanwhile, we have started developing one
core component of our multi-modal interface, a co-
reference resolution system. In this paper, we will
present the component of the system that resolves
pronouns, both personal (I, you, it, they), and deictic
(this, that, these, those, here, there). Hence, this pa-
per presents our first steps toward a full co-reference
resolution module, and ultimately, the multi-modal
interface.
Co-reference resolution is likely the discourse
and dialogue processing task that has received the
most attention. However, as Eisenstein and Davis
(2006) notes, research on co-reference resolution
has mostly been applied to written text; this task
is more difficult in dialogue. First, utterances may
be informal, ungrammatical or disfluent; second,
people spontaneously use hand gestures, body ges-
tures and gaze. Pointing gestures are the eas-
iest gestures to identify, and vision researchers in
our project are working on recognizing pointing and
other hand gestures (Di Eugenio et al, 2010). In this
paper, we replicate the results from (Eisenstein and
Davis, 2006), that pointing gestures help improve
co-reference, in a very different domain. Other work
has shown that gestures can help detect sentence
boundaries (Chen and Harper, 2010) or user inten-
tions (Qu and Chai, 2008).
The rest of the paper is organized as follows. In
Section 2 we describe the data collection and the on-
going annotation. In Section 3 we discuss our co-
reference resolution system, and we present experi-
ments and results in Section 4.
2 The ELDERLY-AT-HOME corpus
Due to the absence of multi-modal collaborative
human-human dialogue corpora that include haptic
data beyond what can be acquired via point-and-
touch interfaces, and in the population of interest,
307
Figure 1: Experiment Excerpts
we undertook a new data collection effort. Our ex-
periments were conducted in a fully functional stu-
dio apartment at Rush University in Chicago ? Fig-
ure 1 shows two screen-shots from our recorded ex-
periments. We equipped the room with 7 web cam-
eras to ensure multiple points of view. Each of the
two participants in the experiments wears a micro-
phone, and a data glove on their dominant hand to
collect haptics data. The ADLs we focused on in-
clude ambulating, getting up from a bed or a chair,
finding pots, opening cans and containers, putting
pots on a stove, setting the table etc. Two students
in gerontological nursing play the role of the helper
(HEL), both in pilot studies and with real subjects.
In 5 pilot dialogues, two faculty members played the
role of the elderly person (ELD). In the 15 real ex-
periments, ELD resides in an assisted living facil-
ity and was transported to the apartment mentioned
above. All elderly subjects are highly functioning at
a cognitive level and do not have any major physical
impairment.
The size of our collected video data is shown
in Table 1. The number of subjects refers to the
number of different ELD?s and does not include the
helpers; we do include our 5 pilot dialogues though,
since those pilot interactions do not measurably dif-
fer from those with the real subjects. Usually one
experiment lasts about 50? (recording starts after in-
formed consent and after the microphones and data
gloves have been put on). Further, we eliminated
irrelevant content such as interruptions, e.g. by the
person who accompanied the elderly subjects, and
further explanations of the tasks. This resulted in
about 15 minutes of what we call effective data for
each subject; the effective data comprises 4782 turns
(see Table 1).
Subjects Raw(Mins) Effective(Mins) Turns
20 482 301 4782
Table 1: ELDERLY-AT-HOME Corpus Size
The effective portion of the data was transcribed
by the first two authors using the Anvil video anno-
tation tool (Kipp, 2001). A subset of the transcribed
data was annotated for co-reference, yielding 114
sub-dialogues corresponding to the tasks subjects
perform, such as finding bowls, filling a pot with wa-
ter, etc. (see Table 2).
An annotation excerpt is shown in Figure 2.
Markable tokens are classified into PLC(Place),
PERS(Person), OBJ(Object) types, and numbered
by type, e.g., PLC#5. Accordingly, we mark pro-
nouns with types as well, RPLC, RPERS, ROBJ, e.g.
RPLC#5. If a subject produced a pointing gesture,
we generate a markable token to mark what is being
pointed to at the end of the utterance (see Utt. 4 and 5
in Figure 2). Within the same task, if two markables
have the same type and the same markable index,
they are taken to co-refer (hence, longer chains of
reference across tasks are cut into shorter spans).
Haptics annotation is at the beginning. We have
identified grab, hold, give and receive as high-level
haptics phonemes that may be useful from the lan-
guage point of view. We have recently started anno-
tating our corpus with those labels.
Subjects Tasks Utterances Gestures Pronouns
12 114 1920 896 1635
Table 2: Annotated Corpus Size
In order to test the reliability of our annotation,
we double coded about 18% of the data, namely 21
sub-dialogues comprising 213 pronouns, on which
we computed the Kappa coefficient (Carletta, 1996).
Similar to (Rodr?guez et al, 2010), we measured the
reliability of markable annotations, and of link to
the antecedent annotations. As concerns the mark-
able level, we obtained ?=0.945, which is high but
no surprisingly for such a simple task. At the link to
the antecedent level, we compared the links from
pronouns to antecedents in a specified context of 4
utterances, obtaining a reasonable ?=0.723.
308
3: PERS#1(HEL/NNP) : RPERS#1(I/PRP) do/VBP n?t/RB see/VB any/DT OBJ#3(pasta/NN) ./.
4: PERS#2(ELD/NNP) : Try/VB over/IN RPLC#5(there/RB) ./. {PLC#5(cabinet/NN)}
5: PERS#1(HEL/NNP) : This/DT RPLC#5(one/NN) ?/. {PLC#5(cabinet/NN)}
6: PERS#2(ELD/NNP) : Oh/UH ,/, yes/RB ./.
Figure 2: Annotation Excerpt
3 Our approach
Utterances and Gestures
Find Markables Generate Candidates
Coreference Pairs
Preprocessing
Markable Model Coreference Model
Figure 3: Co-reference System Architecture
The architecture of our co-reference resolution
system is shown in Figure 3.
We first pre-process a dialogue by splitting turns
into sentences, tokenizing sentences into tokens,
POS tagging tokens. The Markable model is used
to classify whether a token can be referred to and
what type of markable it is. The Markable model?s
feature set includes the POS tag of the token, the
word, the surrounding tokens? POS tags in a win-
dow size of 3. The model outputs markable classes:
Place/Object/Person, or None, which means the to-
ken is not markable. A pointed-to entity serves as a
markable by default.
To perform resolution, each pronoun to be re-
solved ( I, you, it, they; this, that, these, those, here,
there) is paired with markables in the context of the
previous 2 utterances, the current utterance and the
utterance that follows, by using {pronoun, markable
type} compatibility rules. For example, let?s con-
sider the excerpt in Figure 2. To resolve one in
utterance 5, the system will generate 3 candidate
token pairs: <one(5,2), pasta(3,6)>, <one(5,2),
cabinet(4,-1)>, <one(5,2), cabinet(5,-1)> (includ-
ing the pointed-to markable is a way of roughly ap-
proximating information that will be returned by the
vision component). The elements in those pairs
are tokens with their coordinates in the format (Sen-
tenceIndex, TokenIndex); markables pointed to are
given negative token indices.
The Co-reference model will filter out the pairs
<pronoun, markable> that it judges to be incor-
rect. For the Co-reference model, we adopted a
subset of features which are commonly used in co-
reference resolution in written text. These features
apply to each <pronoun, markable> pair and in-
clude: Lexical features, i.e. words and POS tags for
both anaphora and antecedent; Syntactic features,
i.e. syntactic constraints such as number and per-
son agreement; Distance features, i.e. sentence dis-
tance, token distance and markable distance. Addi-
tionally, the Co-reference model uses pointing ges-
ture information. If the antecedent in the <pronoun,
markable> was pointed to, the pair is tagged as Is-
Pointed. In our data, people often use pronouns
and hand gestures instead of nouns when introduc-
ing new entities. It is not possible to map these
pronouns to a textual antecedent since none exists.
This confirms the findings from (Kehler, 2000): in
a multi-modal corpus, he found that no pronoun is
used without a gesture when it refers to a referent
which is not in focus.
4 Experiments and Discussion
The classification models described above were im-
plemented using the Weka package (Hall et al,
2009). Specifically, for each model, we experi-
mented with J48 (a decision tree implementation)
and LibSVM (a Support Vector Machine implemen-
tation). All the results reported below are calculated
using 10 fold cross-validation.
We evaluated the performances of individual
models separately (Tables 3 and 4), and of the sys-
tem as a whole (Table 5).
Algorithm Precision Recall F-Measure
J48 0.984 0.984 0.984
LibSVM 0.979 0.936 0.954
Baseline 0.971 0.971 0.971
Table 3: Markable Model Performance
The results in Table 3 are not surprising, since de-
tecting the type of markables is a simple task. In-
deed the results of the baseline model are extremely
309
Method J48 LibSVMPrecision Recall F-Measure Precision Recall F-Measure
Text + Gesture 0.700 0.684 0.686 0.672 0.669 0.670
Text Only 0.655 0.656 0.656 0.624 0.624 0.624
Table 4: Co-reference Model Performance
Words Method Features Precision Recall F-Measure
All Pronouns
J48 Text Only 0.544 0.332 0.412Text + Gesture 0.482 0.783 0.596
LibSVM Text Only 0.56 0.27 0.364Text + Gesture 0.522 0.6 0.559
Baseline Text Only 0.367 0.254 0.300Text + Gesture 0.376 0.392 0.384
3rd Person + Deictic
J48 Text Only 0.264 0.028 0.05Text + Gesture 0.438 0.902 0.589
LibSVM Text Only 0.6 0.009 0.017Text + Gesture 0.525 0.695 0.598
Baseline Text Only 0.172 0.114 0.137Text + Gesture 0.301 0.431 0.354
Table 5: Co-reference System Performance (Markable + Co-reference Models)
high as well. We compute the baseline by assigning
to the potential markable (i.e., each word) its most
frequent class in the training set (recall that the four
classes include None as well).
For the Co-reference model, we conducted 2 sets
of experiments to ascertain the effect of including
Gesture in the model. As shown in Table 4, both J48
and LibSVM obtain better results when we include
gestures in the model. ?2 shows that differences in
precision and recall 1 are significant at the p ? 0.01
level, though the absolute improvement is not high.
As concerns the evaluation of the whole system,
we ran a 4-way experiment, where we examine the
performance of the system on all pronouns, and on
those pronouns left after eliminating first and second
person pronouns, without and with Gesture informa-
tion. We also ran two sets of baseline experiments.
In the baseline experiments, we link each pronoun
we want to resolve, to the most recent utterance-
markable token and to a pointed-to markable token
(if applicable). Markables are filtered by the same
compatibility rules mentioned above.
Regarding the metrics we used for evaluation, we
used the same method as Strube and Mu?ller (2003),
which is also similar to MUC standard (Hirschman,
1?2 does not apply to the F-Measure.
1997). As the golden set, we used the human an-
notated links from the pronouns to markables in the
same context of four utterances used by the system.
Then, we compared the co-reference links found by
the system against the golden set, and we finally cal-
culated precision, recall and F-Measure.
Table 5 shows that the F-measure is higher when
including gestures, no matter the type of pronouns.
When we include gestures, there is no difference be-
tween ?All Pronouns? and ?3rd Person + Deictic?.
In the ?3rd Person + Deictic? experiments, we ob-
served huge drops in recall, from 0.902 to 0.028 for
J48, and from 0.695 to 0.009 for LibSVM algorithm.
This confirms the point we made earlier, that 3rd
person pronouns/deictic words (Kehler, 2000) often
do not have textual antecedents, since when accom-
panied by simultaneous pointing they introduce new
entities in a dialogue.
Comparison to previous work is feasible only at a
high level, because of the usage of different corpora
and/or measurement metrics. This said, our model
with gestures outperforms Strube andMu?ller (2003),
who did not use gesture information to resolve pro-
nouns in spoken dialogue. Strube and Mu?ller (2003)
used the 20 Switchboard dialogues as their experi-
ment dataset, and used the MUC metrics. Our re-
310
sults are similar to Eisenstein and Davis (2006), but
there are two main differences. First, the corpus
they used is smaller than what we used in this pa-
per. Their corpus was collected by themselves and
consisted of 16 videos, each video was 2-3 minutes
in length. Second, they used a difference measure-
ment metrics called CEAF (Luo, 2005).
5 Conclusions
In this paper, we presented the new ELDERLY-AT-
HOME multi-modal corpus we collected. A co-
reference resolution system for personal and deic-
tic pronouns has been developed on the basis of the
annotated corpus. Our results confirm that gestures
improve co-reference resolution; a simple notion of
type also helps. The Markable and Co-reference
modules we presented are a first start in developing
a full multi-modal co-reference resolution module.
Apart from completing the annotation of our cor-
pus, we will develop an annotation scheme for hap-
tics, and investigate how haptics information affects
co-reference and other dialogue phenomena. Ulti-
mately, both pointing gestures and haptic informa-
tion will automatically be recognized by the collab-
orators in the project we are members of.
Acknowledgments
This work is supported by award IIS 0905593 from
the National Science Foundation. Thanks to the
other members of the RoboHelper project, for their
many contributions, especially to the data collection
effort.
References
Jean Carletta. 1996. Assessing agreement on classifi-
cation tasks: The kappa statistic. Computational Lin-
guistics, 22:249?254.
Lei Chen and Mary P. Harper. 2010. Utilizing gestures
to improve sentence boundary detection. Multimedia
Tools and Applications, pages 1?33.
Barbara Di Eugenio, Milos? Z?efran, Jezekiel Ben-
Arie, Mark Foreman, Lin Chen, Simone Franzini,
Shankaranand Jagadeesan, Maria Javaid, and Kai
Ma. 2010. Towards Effective Communication with
Robotic Assistants for the Elderly: Integrating Speech,
Vision and Haptics. InDialog with Robots, AAAI 2010
Fall Symposium, Arlington, VA, USA, November.
Jacob Eisenstein and Randall Davis. 2006. Gesture
Improves Coreference Resolution. In Proceedings of
the Human Language Technology Conference of the
NAACL, Companion Volume: Short Papers, pages 37?
40.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA data mining software: An update.
SIGKDD Explorations, 11(1).
Lynette Hirschman. 1997. Muc-7 coreference task defi-
nition.
Andrew Kehler. 2000. Cognitive Status and Form of
Reference in Multimodal Human-Computer Interac-
tion. In AAAI 00, The 15th Annual Conference of the
American Association for Artificial Intelligence, pages
685?689.
Michael Kipp. 2001. Anvil-a generic annotation tool
for multimodal dialogue. In Proceedings of the 7th
European Conference on Speech Communication and
Technology, pages 1367?1370.
Kristine M. Krapp. 2002. The Gale Encyclopedia of
Nursing & Allied Health. Gale Group, Inc. Chapter
Activities of Daily Living Evaluation.
Xiaoqiang Luo. 2005. On coreference resolution perfor-
mance metrics. In Proceedings of the conference on
Human Language Technology and Empirical Methods
in Natural Language Processing, HLT ?05, pages 25?
32, Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Shaolin Qu and Joyce Y. Chai. 2008. Beyond attention:
the role of deictic gesture in intention recognition in
multimodal conversational interfaces. In Proceedings
of the 13th international conference on Intelligent user
interfaces, pages 237?246.
Kepa Joseba Rodr?guez, Francesca Delogu, Yannick Ver-
sley, Egon Stemle, and Massimo Poesio. 2010.
Anaphoric annotation of wikipedia and blogs in the
live memories corpus. In Proceedings of the 7th In-
ternational Conference on Language Ressources and
Evaluation (LREC 2010), pages 157?163.
Michael Strube and Christoph Mu?ller. 2003. A machine
learning approach to pronoun resolution in spoken di-
alogue. In Proceedings of the 41st Annual Meeting on
Association for Computational Linguistics-Volume 1.
311
Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 290?294,
Seoul, South Korea, 5-6 July 2012. c?2012 Association for Computational Linguistics
Improving Sentence Completion in Dialogues with Multi-Modal Features
Anruo Wang, Barbara Di Eugenio, Lin Chen
Department of Computer Science
University of Illinois at Chicago
851 S Morgan ST, Chicago, IL 60607, USA
awang28, bdieugen, lchen43@uic.edu
Abstract
With the aim of investigating how humans un-
derstand each other through language and ges-
tures, this paper focuses on how people un-
derstand incomplete sentences. We trained a
system based on interrupted but resumed sen-
tences, in order to find plausible completions
for incomplete sentences. Our promising re-
sults are based on multi-modal features.
1 Introduction
Our project, called RoboHelper, focuses on devel-
oping an interface for elderly people to effectively
communicate with robotic assistants that can help
them perform Activities of Daily Living (ADLs)
(Krapp, 2002), so that they can safely remain living
in their home (Di Eugenio et al, 2010; Chen et al,
2011). We are developing a multi-modal interface
since people communicate with each other using a
variety of verbal and non-verbal signals, including
haptics, i.e., force exchange (as when one person
hands a bowl to another person, and lets go only
when s/he senses that the other is holding it). We
collected a medium size multi-modal human-human
dialogue corpus, then processed and analyzed it. We
observed that a fair number of sentences are incom-
plete, namely, the speaker does not finish the utter-
ance. Because of that, we developed a core compo-
nent of our multi-modal interface, a sentence com-
pletion system, trained on the set of interrupted but
eventually completed sentences from our corpus. In
this paper, we will present the component of the sys-
tem that predicts reasonable completion structures
for an incomplete sentence.
Sentence completion has been addressed within
information retrieval, to satisfy user?s information
needs (Grabski and Scheffer, 2004). Completing
sentences in human-human dialogue is more diffi-
cult than in written text. First, utterances may be in-
formal, ungrammatical or dis-fluent; second, people
interrupt each other during conversations (DeVault
et al, 2010; Yang et al, 2011). Additionally, the
interaction is complex, as people spontaneously use
hand gestures, body language and gaze besides spo-
ken language. As noticed by (Bolden, 2003), during
face-to-face interaction, the completion problem is
not only an exclusively verbal phenomenon but ?an
action embedded within a complex web of differ-
ent meaning-making fields?. Accordingly, among
our features, we will include pointing gestures, and
haptic-ostensive (H-O) actions, e.g., referring to an
object by manipulating it in the real world (Landra-
gin et al, 2002; Foster et al, 2008).
The paper is organized as follows. In Section 2 we
describe our data collection and multi-modal anno-
tation. In Section 3 we discuss how we generate our
training data, and in Section 4 the model we train
for sentence completion, and the results we obtain.
2 Dataset
In contrast with other sentence completion systems
that focus on text input, the dataset we use in this
paper is a subset of the ELDERLY-AT-HOME cor-
pus, a multi-modal corpus in the domain of elderly
care, which includes collaborative human-human di-
alogues, pointing gestures and haptic-ostensive (H-
O) actions. Our experiments were conducted in
a fully functional apartment and included a helper
290
(HEL) and an elderly person (ELD). HEL helps
ELD to complete several realistic tasks, such as
putting on shoes, finding a pot, cooking pasta and
setting the table for dinner. We used 7 web cameras
to videotape the whole experiment, one microphone
each to record the audio and one data glove each to
collect haptics data. We ran 20 realistic experiments
in total, and then imported the videos and audios (in
avi format), haptics data (in csv format) and tran-
scribed utterances (in xml format) into Anvil (Kipp,
2001) to build the multi-modal corpus.
Among other annotations (for example Dialogue
Acts) we have annotated these dialogues for Point-
ing gestures and H-O actions. Due to the setting
of our experiments, the targets of pointing gestures
and H-O actions are real life objects, thus we de-
signed a reference index system to annotate them.
We give pre-defined indices to targets which can-
not be moved, such as cabinets, draws, and fridge.
We also assign runtime indices to targets which can
be moved, like pots, glasses, and plates. For exam-
ple, ?Glass1? refers to the first glass that appears in
one experiment. In our annotation, a ?Pointing? ges-
ture is defined as a hand gesture without any phys-
ical contact between human and objects. Hand
gestures with physical contact to objects are anno-
tated as H-O actions. H-O actions are further subdi-
vided into 7 subtypes, including ?Holding?, ?Touch-
ing?,?Open? and ?Close?. In order to verify the reli-
ability of our annotations, we double coded 15% of
the pointing gestures and H-O actions. Kappa val-
ues of 0.751 for pointing gestures, and of 0.703 for
H-O actions, are considered acceptable, especially
considering the complexity of these real life tasks
(Chen and Di Eugenio, 2012).
In this paper, we focus on specific sub-dialogues
in the corpus, which we call interruptions. An inter-
ruption can occur at any point in human-human dia-
logues: it happens when presumably the interrupter
(ITR) thinks s/he has already understood what the
speaker (SPK) means before listening to the entire
sentence. By observing the data from our corpus,
we conclude that there are generally three cases of
interruptions. First, the speaker (SPK) stops speak-
ing and does not complete the sentence ? these are
the incomplete sentences whose completion a robot
would need to infer. In the second type of inter-
ruption, after being interrupted SPK continues with
(a) few words, and then stops without finishing the
whole sentence: hence, there is a short time over-
lap between two sentences (7 cases). The third case
occurs when the SPK ignores the ITR and finishes
the entire sentence. In this case, the SPK and the
ITR speak simultaneously (198 cases). The number
of interruptions ranges from 1 to 37 in each experi-
ment. An excerpt from an interruption with a subse-
quent completion (an example of case 3) is shown
below. The interruption occurs at the start of the
overlap between the two speakers, marked by < and
>. This example also includes annotations for point-
ing gestures and for H-O actions.
Elder: I need some glasses from < that cabinet >.
[Point (Elder, Cabinet1)]
Helper: < From this > cabinet?
[Point (Helper, Cabinet2)]
Helper: Is this the glass you < ?re looking for? >
[Touching (Helper, Glass1)]
Elder: < No, that one.>
[Point (Elder, Cabinet1, Glass2)]
As concerns annotation for interruptions, it proceeds
from identifying interrupted sentences to finding
<interrupted sentences, candidate structure> pairs
which will be used for generating grammatical com-
pletion for an incomplete sentence. Each in-
terrupted sentence is marked with two categories:
incomplete form, from the start of the sentence
to where it is interrupted, such as ?I need some
glasses?; complete form, from the start of a sentence
to where the speaker stops, ?I need some glasses
from that cabinet.?
Table 2 shows distribution statistics for our
ELDERLY-AT-HOME corpus. It contains a total of
4839 sentences, which in turn contain 7219 clauses.
320 sentences are incomplete in the sense of case 1
(after interruption SPK never completes his/her sen-
tence); whereas 205 sentences are completed after
interruption (cases 2 and 3).
Sentences 4,839
Clauses 7,219
Pointing Gestures 362
H-O Actions 629
Incomplete sentences 320
Interrupted sentences 205
Table 1: Corpus Distributions
291
3 Candidate Pairs Generation
The question is now, how to generate plausible train-
ing instances to predict completions for incomplete
sentences. We use the 205 sentences that have
been interrupted but for which we have comple-
tions; however, we cannot only use those pairs for
training, since we would run the risk of overfit-
ting, and not being able to infer appropriate com-
pletions for other sentences. To generate addi-
tional<Interrupted sentences, candidate structure>
pairs, we need to match an interrupted sentence IntS
with its potential completions ? basically, to check
whether IntS can match the prefix of other sentences
in the corpus. We do so by comparing the POS se-
quence and parse tree of IntS with the POS sequence
and parse tree of the prefix of another sentence. Both
IntS and other sentences in the corpus are parsed via
the Stanford Parser (Klein and Manning, 2003).
Before discussing the details though, we need
to deal with one potential problem: the POS se-
quence for the incomplete portion of IntS may not
be correctly assigned. For example, when the sen-
tence ?The/DT, top/JJ, cabinet/NN.? is interrupted as
?The/DT, top/NN?, the POS tag of NN is assigned
to ?top?; this is incorrect, and engenders noise for
finding correct completions.
We first pre-process a dialogue by splitting turns
into sentences, tokenizing sentences into tokens, and
POS tagging tokens. Although for the interrupted
sentences, we could obtain a correct POS tag se-
quence by parsing the incomplete and resumed por-
tions together, this would not work for a truly incom-
plete sentence (whose completion is our goal). Thus,
to treat both interrupted sentences and incomplete
sentences in the same way, we train a POS tag Cor-
rection Model to correct fallaciously assigned POS
tags. The POS tag Correction Model?s feature set
includes the POS tag of the token, the word, and the
previous tokens? POS tags in a window size of 3.
The model outputs the corrected POS tags.
The POS tag Correction model described above
was implemented using the Weka package (Hall et
al., 2009). Specifically, we experimented with J48
(a decision tree implementation), Naive Bayes (NB),
and LibSVM (a Support Vector Machine implemen-
tation). All the results reported below are calculated
using 10 fold cross-validation.
J48 NB LibSVM
Accuracy 0.829 0.680 0.532
Table 2: POS tag Correction Model Performance
The results in Table 2 are not surprising, since de-
tecting the POS tag of a known word is a simple
task. Additionally, it is not surprising that J48 is
more accurate than NB, since NB is known to of-
ten behave as a baseline method. What is surprising
though is the poor performance of SVMs, which are
generally among the top performers for a broad va-
riety of tasks. We are investigating why this may be
the case. At any rate, by applying the J48 model, we
obtain more accurate POS tag assignments for inter-
rupted sentences (and in our future application, for
the incomplete sentence we need to complete).
Once we have corrected the POS assignments for
each interrupted sentence IntS, we retrieve poten-
tial grammatical structures for IntS, by comparing
IntS with the prefixes of all complete sentences in
the corpus via POS tags and parse trees. Note that
due to the complexity of building a parse tree cor-
rection model in our corpus, we only build a model
to correct the POS tags, but ignore the possible in-
correct parse trees of the incomplete portion of an
interrupted sentence. The matching starts from the
last word in IntS back to the first word, with weights
assigned to each position in decreasing order. Due to
the size of our corpus, it is not possible to find ex-
actly matched POS tag sequences for every incom-
plete sentence; thus, we also consider the parsed tree
structures and mismatched POS tags between IntS?s
and complete sentences by reducing weights accord-
ing to the size of the matched phrases and distances
of mismatched POS tags. After this, a matching
score is calculated for each incomplete and candi-
date structure pair.
Due to the large number of candidate structures,
only the top 150 candidate structures for each IntS
are selected and manually annotated with three
classifications: ?R?, when the candidate structure
provides a grammatically ?reasonable? structure,
which can be used as a template for completion;
?U?, which means the candidate structure gives
an ?ungrammatical? structure, thus this candidate
structure cannot be used as template for completion;
292
?T?, the candidate structure is exactly the same as
what the speaker was originally saying, as judged
based on the video and audio records. An example
of an incomplete sentence with candidate structures
in each of the three categories is shown below.
It/PRP, feels/VBZ | It/PRP, feels/VBZ, good/JJR
[R] It/PRP, ?s/VBZ, fine/JJ, like/IN, this/DT]
[U] We/PRP, did/VBD, n?t/RB
[T] It/PRP, is/VBZ, better/JJR
10543 interrupted sentences and candidate pairs
are generated. 5268 of those 10543 pairs
(49.97%) were annotated as ?Reasonable?, 4727
pairs (44.85%) were annotated as ?Unreasonable?,
and 545 pairs (5.17%) were annotated as ?Same with
original sentence?.
Incomplete Sentence and Structure pairs 10,543
Reasonable structures (R) 5,268
Unreasonable structures (U) 4,729
Exactly same structures (T) 545
Table 3: Distribution of completion classifications
4 Results and Discussion
On the basis of the annotation, we trained a ?Rea-
sonable Structure Selection (RSS)? model via su-
pervised learning methods. For each pair <IntS,
Candidate>, the feature set includes word and POS
tag of the tokens of IntS and its candidate structure
sentence. Co-occurring pointing gestures and H-O
actions for both IntS and Candidate are also included
in the model. Co-occurrence is defined as tempo-
ral overlap between the gesture (pointing or H-O ac-
tion) and the duration of the utterance. For each
training instance, we include the following features:
IntS: <words, POS tags>, <Pointing (Person / Ob-
ject / Location)>, <H-O action (Person / Object /
Location / Type)>;
Candidate: <words/POS tags)>, <Pointing (Per-
son / Object / Location)>, <H-O action (Person /
Object / Location / Type)>;
<Matching Score>;
<Classification: R, U, or T>.
We trained the RSS model also using the Weka
package. The same methods mentioned earlier
(J48, NB and SVM) are used, with 10-fold cross-
validations. Results are shown in Table 4. We
J48 NB LibSVM
Precision R, U, T 0.822 0.724 0.567
R, U 0.843 0.761 0.600
Recall R, U, T 0.820 0.725 0.512
R, U 0.842 0.762 0.563
F-Measure R, U, T 0.818 0.711 0.390
R, U 0.841 0.761 0.440
Table 4: Reasonable Structure Selection models
ran two different sets of experiments using two ver-
sions of training instances: Classification with three
classes, R, U and T, and classification with two
classes, R and U. When training with only two
classes, the T instances are marked as R. We exper-
imented with collapsing R and T candidates since T
candidates may lead to overfitting, and some R can-
didates might even provide better structures for an
incomplete sentence than what exactly one speaker
had originally said. Not surprisingly, results im-
prove for two-way classification. Based on the J48
model, we observed that the POS tag features play
a significant part in classification, whereas the word
features are redundant. Further, pointing gestures
and H-O actions do appear in some subtrees of the
larger decision tree, but not on every branch. We
speculate that this is due to the fact that pointing ges-
tures or H-O actions do not accompany every utter-
ance.
5 Conclusions and Future Work
In this paper, we introduced our multi-modal sen-
tence completion schema which includes pointing
gestures and H-O actions in the corpus ELDERLY-
AT-HOME. Our data shows that it is possible to pre-
dict what people will say, even if the utterance is
not complete. Our promising results include multi-
modal features, which as we have shown elsewhere
(Chen and Di Eugenio, 2012) improve traditional
co-reference resolution models. In the near future,
we will implement the last module of our sentence
completion system, the one that fills the chosen can-
didate structure with actual words.
293
References
G.B. Bolden. 2003. Multiple modalities in collaborative
turn sequences. Gesture, 3(2):187?212.
L. Chen and B. Di Eugenio. 2012. Co-reference via
pointing and haptics in multi-modal dialogues. In The
2012 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies. Association for Compu-
tational Linguistics. short paper, to appear.
L. Chen, A. Wang, and B. Di Eugenio. 2011. Im-
proving pronominal and deictic co-reference resolu-
tion with multi-modal features. In Proceedings of the
SIGDIAL 2011 Conference, pages 307?311. Associa-
tion for Computational Linguistics.
David DeVault, Kenji Sagae, and David Traum. 2010.
Incremental interpretation and prediction of utterance
meaning for interactive dialogue. Dialogue and Dis-
course, 2(1):143170.
B. Di Eugenio, M. Zefran, J. Ben-Arie, M. Foreman,
L. Chen, S. Franzini, S. Jagadeesan, M. Javaid, and
K. Ma. 2010. Towards effective communication with
robotic assistants for the elderly: Integrating speech,
vision and haptics. In 2010 AAAI Fall Symposium Se-
ries.
M.E. Foster, E.G. Bard, M. Guhe, R.L. Hill, J. Ober-
lander, and A. Knoll. 2008. The roles of haptic-
ostensive referring expressions in cooperative, task-
based human-robot dialogue. In Proceedings of the
3rd ACM/IEEE international conference on Human
robot interaction, pages 295?302. ACM.
K. Grabski and T. Scheffer. 2004. Sentence completion.
In Proceedings of the 27th annual international ACM
SIGIR conference on Research and development in in-
formation retrieval, pages 433?439. ACM.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bern-
hard Pfahringer, Peter Reutemann, and Ian H. Wit-
ten. 2009. The WEKA data mining soft-
ware: An update. SIGKDD Explorations, 11(1).
http://www.cs.waikato.ac.nz/ml/weka/.
M. Kipp. 2001. Anvil-a generic annotation tool for mul-
timodal dialogue. In Seventh European Conference on
Speech Communication and Technology.
D. Klein and C.D. Manning. 2003. Accurate unlexical-
ized parsing. In Proceedings of the 41st Annual Meet-
ing on Association for Computational Linguistics-
Volume 1, pages 423?430. Association for Computa-
tional Linguistics.
K.M. Krapp. 2002. The Gale Encyclopedia of Nursing
& Allied Health: DH, volume 2. Gale Cengage.
F. Landragin, N. Bellalem, L. Romary, et al 2002. Re-
ferring to objects with spoken and haptic modalities.
F. Yang, P.A. Heeman, and A.L. Kun. 2011. An
investigation of interruptions and resumptions in
multi-tasking dialogues. Computational Linguistics,
37(1):75?104.
294
