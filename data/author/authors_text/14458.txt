Squibs and Discussions
Real versus Template-Based Natural Language
Generation: A False Opposition?
Kees van Deemter
University of Aberdeen
Emiel Krahmer.
Tilburg University
Marie?t Theune-
University of Twente
This article challenges the received wisdom that template-based approaches to the generation of
language are necessarily inferior to other approaches as regards their maintainability, linguistic
well-foundedness, and quality of output. Some recent NLG systems that call themselves
??template-based?? will illustrate our claims.
1. Introduction
Natural language generation (NLG) systems are sometimes partitioned into application-
dependent systems which lack a proper theoretical foundation, on the one hand, and
theoretically well-founded systems which embody generic linguistic insights, on the
other. Template-based systems are often regarded as automatically falling into the first
category. We argue against this view. First, we describe the received view of both
template-based and ??standard?? NLG systems (section 2). Then we describe a class of
recent template-based systems (section 3) that will serve as a basis for a comparison
between template-based and other NLG systems with respect to their potential for
performing NLG tasks (section 4). We ask what the real difference between template-
based and other systems is and argue that the distinction between the two is becoming
increasingly blurred (section 5). Finally, we discuss the implications of engineering
shortcuts (Mellish 2000) and corpus-based methods (section 6).
2. Templates versus Real NLG: The Received View
Before we can argue against the distinction between template-based and ??real?? NLG
systems, we should first sketch how these two classes are commonly understood. It is
surprisingly difficult to give a precise characterization of the difference between them
(and we will later argue against the usefulness of such a characterization), but the idea
is the following. Template-based systems are natural-language-generating systems
that map their nonlinguistic input directly (i.e., without intermediate representations)
to the linguistic surface structure (cf. Reiter and Dale 1997, pages 83?84). Crucially, this
linguistic structure may contain gaps; well-formed output results when the gaps are
* 2005 Association for Computational Linguistics
 Computing Science Department, King?s College, University of Aberdeen, United Kingdom.
E-mail: KvDeemter@csd.abdn.ac.uk.
. Communication and Cognition/Computational Linguistics, Faculty of Arts, Tilburg University,
Tilburg, The Netherlands. E-mail: E.J.Krahmer@uvt.nl.
- Human Media Interaction Group, Computer Science, University of Twente, The Netherlands.
E-mail: M.Theune@ewi.utwente.nl.
filled or, more precisely, when all the gaps have been replaced by linguistic structures
that do not contain gaps. (Canned text is the borderline case of a template without
gaps.) Adapting an example from Reiter and Dale (1997), a simple template-based
system might start out from a semantic representation saying that the 306 train leaves
Aberdeen at 10:00 AM:
Departure?train306; locationabdn; time1000?
and associate it directly with a template such as
?train is leaving ?townnow
where the gaps represented by [train] and [town] are filled by looking up the relevant
information in a table. Note that this template will be used only when the time referred
to is close to the intended time of speaking; other templates must be used for
generating departure announcements relating to the past or future. ??Real?? or, as we
shall say, standard NLG systems, by contrast, use a less direct mapping between input
and surface form (Reiter 1995; Reiter and Dale 1997). Such systems could start from
the same input semantic representation, subjecting it to a number of consecutive
transformations until a surface structure results. Various NLG submodules would
operate on it (determining, for instance, that 10:00 AM is essentially the intended time
of speaking), jointly transforming the representation into an intermediate representa-
tion like
Leavepresent ?traindemonstrative; Aberdeen; now?
where lexical items and style of reference have been determined while linguistic
morphology is still absent. This intermediate representation may in turn be transformed
into a proper sentence, for example: This train is leaving Aberdeen now. Details vary; in
particular, many systems will contain more intermediate representations.
Template-based and standard NLG systems are said to be ??Turing equivalent??
(Reiter and Dale 1997); that is, each of them can generate all recursively enumerable
languages. However, template-based systems have been claimed to be inferior with
respect to maintainability, output quality and variation, and well-foundedness. Reiter
and Dale (1997) state that template-based systems are more difficult to maintain and
update (page 61) and that they produce poorer and less varied output (pages 60, 84)
than standard NLG systems. Busemann and Horacek (1998) go even further by
suggesting that template-based systems do not embody generic linguistic insights
(page 238). Consistent with this view, template-based systems are sometimes over-
looked. In fact, the only current textbook on NLG (Reiter and Dale 2000) does not
pay any attention to template-based generation, except for a passing mention of the
ECRAN system (Geldof and van de Velde 1997). Another example is a recent overview
of NLG systems in the RAGS project (Cahill et al 1999). The selection criteria employed
by the authors were that the systems had to be fully implemented, complete (i.e.,
generating text from nontextual input), and accepting non-hand-crafted input; al-
though these criteria appear to favor template based systems, none of the 19 systems
investigated were template-based. In what follows, we claim that the two types of
systems have more in common than is generally thought and that it is counter-
productive to treat them as distant cousins instead of close siblings. In fact, we argue
that there is no crisp distinction between the two.
16
Computational Linguistics Volume 31, Number 1
17
3. Template-Based NLG Systems in Practice
In recent years, a number of new template-based systems have seen the light,
including TG/2 (Busemann and Horacek 1998), D2S (van Deemter and Odijk 1997;
Theune et al 2001), EXEMPLARS (White and Caldwell 1998), YAG (McRoy, Channarukul,
and Ali 2003), and XTRAGEN (Stenzhorn 2002). Each of these systems represents a
substantial research effort, achieving generative capabilities beyond what is usually
expected from template-based systems, yet they call themselves template-based,
and they clearly fall within the characterization of template-based systems offered
above.
In this article we draw on our own experiences with a data-to-speech method
called D2S. D2S has been used as the foundation of a number of language-generating
systems, including GOALGETTER, a system that generates soccer reports in Dutch.1 D2S
consists of two modules: (1) a language generation module (LGM) and (2) a speech
generation module (SGM) which turns the generated text into a speech signal. Here
we focus on the LGM and in particular on its use of syntactically structured templates
to convert a typed data structure into a natural language text (annotated with prosodic
information). Data structures in GOALGETTER are simple representations describing
lists of facts, such as

goal-event
TEAM Ajax
PLAYER Kluivert
MINUTE 38
GOAL-TYPE penalty

Besides goal events, there are several other types of events, such as players receiving
yellow or red cards. Figure 1 shows a simple template, which the LGM might use to
express the above fact as, for instance, Kluivert scored a penalty in the 38th minute.
1 See http://www.cs.utwente.nl/?theune/GG/GG_index.html for some example reports.
Figure 1
Sample syntactic template from the GOALGETTER system.
van Deemter, Krahmer, and Theune Real versus Template-Based NLG
Formally, a syntactic template s = bS, E, C, T?, where S is a syntax tree (typically for
a sentence) with open slots in it, E is a set of links to additional syntactic structures
(typically NPs and PPs) which may be substituted in the gaps of S, C is a condition on
the applicability of s, and T is a set of topics. We discuss the four components of a
template in more detail, starting with the syntax tree, S. All S?s interior nodes are
labeled by nonterminal symbols, while the nodes on the frontier are labeled by
terminal or nonterminal symbols: the nonterminal nodes (??gaps??) are open for
substitution and they are marked by a ,. The second element of a syntactic template is
E: the slot fillers. Each open slot in the tree S is associated with a call of some Express
function (ExpressTime, ExpressObject, etc.), which generates a set of expressions that
can be used to fill the slot. The right-hand side of Figure 2 shows an example Express
function, namely, ExpressObject, which generates a set of NP trees and is used to
generate fillers for the player and goal slots in the template of Figure 1. The first, for
example, leads to the generation of NPs such as Kluivert (proper name), the forward
Kluivert, Ajax player Kluivert, Ajax? Kluivert, the striker, and he, depending on the context
in which the NP is generated.
The left-hand side of Figure 2 shows the function ApplyTemplate, which handles
the choice among all possible combinations of slot fillers. ApplyTemplate first calls
FillSlots to obtain the set of all possible trees (all_trees) that can be generated from the
template, using all possible combinations of slot fillers generated by the Express
functions associated with the slots. For each tree in this set, it is checked (1) whether it
does not violate a version of the Chomskyan binding theory and (2) whether it is
compatible with the context model, which is a record containing all the objects
introduced so far and the anaphoric relations among them. From the resulting set of
allowed_trees, one is selected randomly (using the function PickAny) and returned to
the main generation algorithm. The random-choice option was chosen to maximize the
variety of sentences produced by the system.
The mechanisms described so far take care of sentence planning and language
realization. Text planning is performed by components C and T. C is a Boolean
condition. A template s is applicable only if its associated condition is true. An
example is the condition from Figure 1 saying that the template can be used only if
the result of the current match has been conveyed to the user (i.e., is known) and
the current goal is the first one which has not been conveyed (i.e., is not known). To
cater to aspects of text planning that allow a less knowledge-intensive approach,
GOALGETTER associates every template with a set of topics T, which the LGM algo-
rithm uses to group sentences together into coherent chunks of text. For example, any
18
Figure 2
Functions ApplyTemplate (left) and ExpressObject (right).
Computational Linguistics Volume 31, Number 1
19
template associated with the topic of goal scoring can ??fire?? throughout the opening
paragraph of the report.
4. Template-Based NLG: Deep or Shallow?
How do template-based systems measure up against the criteria mentioned in
section 2? When dealing with this question, we are interested as much in what could
be done in principle as in what has been achieved in practice. After some preliminary
remarks, we focus on the criterion of linguistic well-foundedness.
It is far from obvious that template-based systems should always score low on
maintainability. Several template-based systems such as TG/2, EXEMPLARS, and
XTRAGEN have been reused for generation in different languages or in different
domains (cf. Kittredge et al 1994). In the case of D2S, the basic generation algorithm
and such functions as ApplyTemplate and ExpressObject have been used for different
application domains (music, soccer games, route descriptions, and public transport)
and different languages (English, Dutch, and German); D2S has been used for the
generation of both monologues and dialogue contributions (van Deemter and Odijk
1997; Theune et al 2001). When a template-based system is applied to a new domain or
language, many of the templates will have to be written anew (much as new grammar
fragments need to be developed for standard NLG systems), but the underlying
generation mechanisms generally require little or no modification.
As for the output quality and variability of the output, if template-based systems
have the same generative power as standard NLG systems (Reiter and Dale 1997),
there cannot be a difference between the types of output that they are able to generate
in principle. The fact that templates can be specified by hand gives template-based
systems an advantage in cases in which good linguistic rules are not (yet) available or
for constructions which have unpredictable meanings or highly specific conditions of
use. Some template-based systems have variability as one of their central design
specifications: Current D2S-based systems rely mainly on random choice to achieve
variation, but more context-sensitive variations (e.g., varying the output depending on
user characteristics) can also be achieved through the use of parametrized templates
(XTRAGEN) or template specialization hierarchies (EXEMPLARS).
The most crucial question, in our view, is whether a template-based NLG system
can be linguistically well-founded (or ??deep?? in terms of Busemann and Horacek
[1998]), in the sense that the choices inherent in its mapping from input to output are
based on sound linguistic principles. To judge the well-foundedness of template-based
systems, let us look at the different types of decisions that an NLG system needs to
make, as distinguished by Cahill et al (1999) and Reiter and Dale (2000).
4.1 Content Determination
During content determination, it is decided what information is to be conveyed. Since
content determination precedes language generation proper, it is clear that in principle,
template-based systems can treat it in the exact same ways as standard NLG systems.
In practice, template-based systems tend to take their departure from ??flat data?? (e.g.,
database records), whereas standard systems often use richer input, in which some
decisions concerning the linguistic structure of the output (e.g., decisions about
quantificational or rhetorical structure) have already been made. To the extent that this
is the case, the ??generation gap?? to be bridged by template-based systems is actually
wider than the one to be bridged by standard NLG systems.
van Deemter, Krahmer, and Theune Real versus Template-Based NLG
4.2 Referring Expressions
As for the generation of referring expressions, template-based systems vary widely:
The simplest of them (e.g., MSWord-based systems for mail merge) can fill their gaps
with only a limited number of phrases, but more sophisticated systems (called
??hybrid?? systems in Reiter [1995]) have long existed; these effectively use standard
NLG to fill their gaps. Recent systems have moved further in this direction. D2S, for
example, uses well-established rules for constraining the use of anaphors (see, e.g., the
Chomskyan ViolateBindingTheory and Wellformed in ApplyTemplate) and a new
variant of Dale and Reiter?s (1995) algorithm for the generation of referring expressions
that takes contextual salience into account (MakeReferringExp in ExpressObject)
(Krahmer and Theune 2002). A similar range of approaches can be found among NLG
systems that are not template-based; in fact, several systems from the RAGS inventory
do not really address referring expression generation at all (Cahill et al 1999).
4.3 Aggregation
Aggregation is an NLG task in which differences between the two types of systems
may be expected. After all, every template contains a ??fixed?? part, and surely this part
cannot be recombined with other parts? The reality is slightly more complex. The
GOALGETTER system, for instance, uses the following approach: In order to generate a
subject-aggregated sentence of the form A and B got a red card, a separate template is
called of the form X got a red card [syntactic structure omitted], subject to conditions
requiring that the gap X be filled with an appropriate conjoined noun phrase, referring
to the set {A, B}. Other approaches are possible. For example, the system could first
generate A got a red card and B got a red card, then aggregate these two structures (whose
syntactic and semantic structure is known) into the desired conjunctive structure (van
Deemter and Odijk 1997). Whether a system is able to perform operations of this kind
does not depend on whether the system is template based, but on whether it possesses
the required syntactic and semantic information.
4.4 Lexicalization
The same point is relevant for lexicalization. Let us suppose (perhaps rather charitably;
Cahill et al 1999) that a variety of near-synonymous verbs are present in the lexicon of
the NLG system (e.g., give, offer, donate, entrust, present to). How would a standard NLG
system choose among them? Typically, the system does not have a clue, because our
understanding of the differences among these verbs is too imperfect. (The input to the
system might prejudge such decisions by pairing each of these verbs with different
input relations, but that would be cheating.) As with the previous tasks, it is not clear
that standard NLG systems are in a better position to perform them than template-
based ones: The latter could use templates that vary in the choice of words and
stipulate that they are applicable under slightly different conditions (cf. the use of
specialization hierarchies in EXEMPLARS). The condition C for X kicked the ball in the net,
for example (as opposed to X scored or X nudged the ball in) might require that the ball
did not touch the ground after departing the previous player.
4.5 Linguistic Realization
It is in linguistic realization that the most obvious differences between standard and
template-based approaches appear to exist. Many template-based approaches lack a
general mechanism for gender, number, and person agreement, for example. Systems
in the D2S tradition avoid errors by letting functions like ExpressObject use handmade
rules, but this approach becomes cumbersome when coverage increases; general-
20
Computational Linguistics Volume 31, Number 1
21
izations are likely to be missed and portability is reduced, for example, if different
templates are used for John walks and John and Mary walk. One should not, however, let
one?s judgment depend on accidental properties of one or two systems: Nothing keeps
the designer of a template-based system from adding morphological rules; witness
systems like YAG (McRoy, Channarukul, and Ali 2003) and XTRAGEN (Stenzhorn 2002).
The YAG system, for example, allows the subject and verb of a template to be
underspecified for number and person, while using attribute grammar rules to
complete the specification: Returning to the example above, the number attribute of
John and Mary is inferred to be plural (unlike, e.g., John and I); a subject-verb
agreement rule makes the further inference that the verb must be realized as walk,
rather than walks.
5. Templates: An Updated View
A new generation of systems that call themselves template-based have blurred the line
between template-based and standard NLG. This is not only because some systems
combine standard NLG with templates and canned text (Piwek 2003), but also because
modern template-based systems tend to use syntactically structured templates and
allow the gaps in them to be filled recursively (i.e., by filling a gap, a new gap may
result). Some ??template-based?? systems, finally, use grammars to aid linguistic
realization. These developments call into question the very definition of ??template
based?? (section 2), since the systems that call themselves template-based have come to
express their nonlinguistic input with varying degrees of directness.
??Template-based?? systems vary in terms of linguistic coverage, the amount of
syntactic knowledge used, and the number of steps involved in filling the templates,
among other things. Here, we highlight one particular dimension, namely, the size of
(the fixed part of) the templates. A comparison with tree-adjoining grammar (TAG)?
based-approaches to NLG may be useful (Joshi 1987; see also Becker 2002). Joshi (1987,
page 234) points out that ??The initial . . . trees are not constrained in any other manner
than. . . . The idea, however, is that [they] will be minimal in some sense.?? Minimality is
usually interpreted as saying that a tree should not contain more than the lexical head
plus its arguments. Initial trees may be likened to templates. Nonminimal templates/
elementary trees are essential for the treatment of idioms and special collocations.
Generally speaking, however, the larger the templates/elementary trees, the less sys-
tematic the treatment, the less insight it gives into the compositional structure of lan-
guage, and the larger the number of templates/elementary trees needed. Again, the
history of D2S is instructive: The earliest D2S-based NLG system (DYD; van Deemter and
Odijk 1997) used long templates, but the majority of the templates in GOALGETTER are
minimal in the sense explicated above (Theune et al 2001).
6. Discussion: Shortcuts and Statistics in NLG
Let us compare our views with those of Mellish (2000). Mellish points out that NLG
systems often use shortcuts, whereby one or more modules are trivialized, either by
bypassing them (and the representations that they create) or by letting their operations
be dictated by what the other modules expect (e.g., lexical choice may be trivialized
by using a one-to-one mapping between semantic relations/predicates and lexical
items). Mellish argues that shortcuts have a legitimate role in practical NLG when
linguistic rules are missing, provided the existence of the shortcuts is acknowledged:
Even though they lead to diminished generality and maintainability, the unavailability
van Deemter, Krahmer, and Theune Real versus Template-Based NLG
of ??deep?? rules means that there is no alternative (yet). For instance, there is little
added value in using abstract representations from which either a passive or an active
sentence can be generated if we are unable to state a general rule that governs the
choice, in which case one can be forgiven for explicitly specifying which sentences
should be active and which ones passive, avoiding a pretense of linguistic sophis-
tication. It is shortcuts of this kind that a template-based system is well placed to make,
of course. But crucially, template-based systems do not have to use shortcuts any more
than standard NLG systems: Where linguistic rules are available, both types of sys-
tems can use them, as we have seen.
Another response to the absence of linguistic rules is the use of statistical
information derived from corpora, as is increasingly more common in realization, but
also for instance in aggregation (e.g., Walker, Rambow, and Rogati 2002). The point we
want to make here, however, is that ??template-based?? systems may profit from such
corpus-based approaches just as much as ??standard?? NLG systems. The approach of
Langkilde and Knight (1998), for example, in which corpus-derived n-grams are used
for selecting the best ones from among a set of candidates produced by overgenera-
tion, can also be applied to template-based systems (witness the mixed template/
stochastic system of Galley, Fosler-Lussier, and Potamianos [2001]).
We have argued that systems that call themselves template based can, in principle,
perform all NLG tasks in a linguistically well-founded way and that more and more
actually implemented systems of this kind deviate dramatically from the stereotypical
systems that are often associated with the term template. Conversely, most standard
NLG systems perform many NLG tasks in a less than well-founded fashion (e.g.,
relying heavily on shortcuts, and nontransparent ones at that). We doubt that there is
still any important difference between the two classes of systems, since the variation
within each of them is as great as that between them.
22
Acknowledgments
This is a remote descendant of a paper
presented at the workshop ??May I Speak
Freely??? (Becker and Busemann 1999). We
thank three reviewers for comments.
References
Becker, Tilman. 2002. Practical,
template-based natural language
generation with TAG. In Proceedings
of TAG+6, Venice.
Becker, Tilman and Stephan Busemann,
editors. 1999. ??May I Speak Freely??? Between
Templates and Free Choice in Natural
Language Generation: KI-99 Workshop.
DFKI, Saarbru?cken, Germany.
Busemann, Stephan and Helmut
Horacek. 1998. A flexible shallow
approach to text generation. In Proceedings
of the Ninth International Workshop on
Natural Language Generation,
pages 238?247: Niagara-on-the-Lake,
Ontario, Canada.
Cahill, Lynn, Christy Doran, Roger Evans,
Chris Mellish, Daniel Paiva, Mike Reape,
and Donia Scott. 1999. In search of a
reference architecture for NLG systems.
In Proceedings of the Seventh European
Workshop on Natural Language Generation,
pages 77?85: Toulouse, France.
Dale, Robert and Ehud Reiter. 1995.
Computational interpretations of the
Gricean maxims in the generation of
referring expressions. Cognitive Science,
18:233?263.
Galley, Michel, Eric Fosler-Lussier, and
Alexandros Potamianos. 2001. Hybrid
natural language generation for spoken
dialogue systems. In Proceedings of
the Seventh European Conference on
Speech Communication and Technology.
Aalborg, Denmark.
Geldof, Sabine and Walter van de Velde.
1997. An architecture for template based
(hyper)text generation. In Proceedings of
the Sixth European Workshop on Natural
Language Generation, pages 28?37,
Duisburg, Germany.
Joshi, Aravind. 1987. The relevance of
tree adjoining grammar to generation.
In Gerard Kempen, editor. Natural Language
Computational Linguistics Volume 31, Number 1
23
Generation, Martinus Nijhoff, Leiden,
The Netherlands, pages 233?252.
Kittredge, Richard, Eli Goldberg, Myunghee
Kim, and Alain Polgue`re. 1994.
Sublanguage engineering in the FOG
system. In Fourth Conference on Applied
Natural Language Processing, pages 215?216,
Stuttgart, Germany.
Krahmer, Emiel and Marie?t Theune. 2002.
Efficient context-sensitive generation of
descriptions in context. In Kees van
Deemter and Rodger Kibble, editors,
Information Sharing. CSLI Publications,
Stanford, CA, pages 223?264.
Langkilde, Irene and Kevin Knight. 1998.
Generation that exploits corpus-based
statistical knowledge. In Proceedings of
the ACL, pages 704?710, Montreal,
Quebec, Canada.
McRoy, Susan W., Songsak Channarukul,
and Syed S. Ali. 2003. An augmented
template-based approach to text
realization. Natural Language Engineering,
9(4):381?420.
Mellish, Chris. 2000. Understanding shortcuts
in NLG systems. In Proceedings of Impacts in
Natural Language Generation: NLG between
Technology and Applications, pages 43?50,
Dagstuhl, Germany.
Piwek, Paul. 2003. A flexible
pragmatics-driven language generator for
animated agents. In Proceedings of EACL03
(Research Notes), pages 151?154,
Budapest, Hungary.
Reiter, Ehud. 1995. NLG vs. templates. In
Proceedings of the Fifth European Workshop on
Natural Language Generation, pages 95?105,
Leiden, The Netherlands.
Reiter, Ehud and Robert Dale. 1997. Building
applied natural language generation
systems. Natural Language Engineering,
3(1):57?87.
Reiter, Ehud and Robert Dale. 2000.
Building Natural Language Generation
Systems. Cambridge University
Press, Cambridge.
Stenzhorn, Holger. 2002. A natural language
generation system using XML- and
Java-technologies. In Proceedings of the
Second Workshop on NLP and XML,
Taipei, Taiwan.
Theune, Marie?t, Esther Klabbers, Jan-Roelof
de Pijper, Emiel Krahmer, and Jan Odijk.
2001. From data to speech: A general
approach. Natural Language Engineering,
7(1):47?86.
van Deemter, Kees and Jan Odijk. 1997.
Context modelling and the generation of
spoken discourse. Speech Communication,
21(1/2):101?121.
Walker, Marilyn, Owen Rambow, and
Monica Rogati. 2002. Training a sentence
planner for spoken dialogue using
boosting. Computer Speech and Language,
16:409?433.
White, Michael and Ted Caldwell. 1998.
EXEMPLARS: A practical, extensible
framework for dynamic text generation.
In Proceedings of the Ninth International
Workshop on Natural Language Generation,
pages 266?275, Niagara-on-the-Lake,
Ontario, Canada.
van Deemter, Krahmer, and Theune Real versus Template-Based NLG

Proceedings of the Workshop on Embodied Language Processing, pages 25?32,
Prague, Czech Republic, June 28, 2007. c?2007 Association for Computational Linguistics
Which way to turn? Guide orientation in virtual way finding
Mark Evers
Technical & Professional
Communication
University of Twente
The Netherlands
M.Evers@alumnus.utwente.nl
Marie?t Theune
Human Media Interaction
University of Twente
The Netherlands
M.Theune@utwente.nl
Joyce Karreman
Technical & Professional
Communication
University of Twente
The Netherlands
J.Karreman@utwente.nl
Abstract
In this paper we describe an experiment
aimed at determining the most effective and
natural orientation of a virtual guide that
gives route directions in a 3D virtual envi-
ronment. We hypothesized that, due to the
presence of mirrored gestures, having the
route provider directly face the route seeker
would result in a less effective and less nat-
ural route description than having the route
provider adapt his orientation to that of the
route seeker. To compare the effectiveness
of the different orientations, after having re-
ceived a route description the participants in
our experiment had to ?virtually? traverse the
route using prerecorded route segments. The
results showed no difference in effectiveness
between the two orientations, but suggested
that the orientation where the speaker di-
rectly faces the route seeker is more natural.
1 Introduction
When someone approaches us and asks which way
to go, we naturally turn ? if necessary ? so we face
the direction to take (which makes it also easier for
ourselves to imagine traversing the route). Gener-
ally, the route seeker then also turns to adapt his
or her orientation to match ours, and we end up
sharing the same perspective on the route to take.1
Presumably, this matching of physical orientation is
1This observation is based on personal experience. We also
observed this behaviour in a small corpus of route description
video?s.
meant to reduce the mental effort that is involved
in matching another person?s perspective on a spa-
tial scene for both speaker and hearer (Shelton and
McNamara, 2004). However, someone who faces
an embodied virtual agent presenting a route de-
scription in a virtual environment (projected on a
computer screen) cannot turn to match his or her
perspective with that of the agent, as turning away
from the screen would result in losing sight of both
the agent and the virtual environment. In this sit-
uation, the only way to bring the perspectives of
route provider (agent) and route seeker (user) closer
together is for the agent to adapt its orientation to
match that of the user. In this paper, we describe an
experiment carried out to determine if such a change
in orientation by the route provider helps the route
seeker with virtual way finding. Although the ex-
periment was aimed at determining the most effec-
tive and natural orientation of a Virtual Guide, we
used prerecorded route descriptions presented by a
human route provider. The Virtual Guide that we
have developed (see next section) was still being im-
plemented at the time.
2 The Virtual Guide
We have developed an embodied Virtual Guide2 that
can give route directions in a 3D environment, which
is a virtual reality replica of a public building in our
home town. When navigating through this virtual
environment, shown on the computer screen from a
first person perspective, the user can approach the
Virtual Guide to ask for directions. Currently the
2See http://wwwhome.cs.utwente.nl/?hofs/dialogue for an
online demo.
25
Guide is behind the reception desk (see Figure 1),
but she can be situated anywhere in the building.
The first part of the interaction between the Vir-
tual Guide and the user consists of a natural lan-
guage dialogue in which the Guide tries to find out
the user?s intended destination. This may involve
subdialogues, in which either the Guide or the user
asks the other for clarification, and the resolution of
anaphoric expressions (e.g., How do I get there?).
Input and output modalities include text, speech and
pointing. For an in-depth description of the dialogue
module of the Virtual Guide, see Hofs et al (2003).
When the user?s destination has been established,
the Virtual Guide gives a natural language route de-
scription, in the form of a monologue that cannot
be interrupted. This is somewhat unnatural since
in real direction giving, the route seeker tends to
give feedback and, if necessary, ask for clarification
while the route is being described. However, since
in our system dialogue management and the gener-
ation of route descriptions are handled by separate,
specialised modules this is currently not possible.
The route is presented as a sequence of segments,
which are mostly expressed as ?point+direction?
combinations (Dale et al, 2005). That is, they con-
sist of a turn direction combined with the location
where this turn is to be made, specified in terms
of a landmark. For example, You go left at the in-
formation sign. The route description is generated
as follows. First, the shortest path between starting
point and destination is computed based on prede-
fined paths in the virtual environment. Turn direc-
tions are derived from the relative angles of sub-
sequent path segments, and landmarks are selected
based on their relative salience (e.g., in terms of size
or colour) and proximity to a turning point. The se-
quence of turn directions and associated landmarks
is then given as input to the natural language gen-
eration component, which is based on Exemplars
(White and Caldwell, 1998). After a first version
of the route description has been generated using a
collection of standard sentence structures, this ini-
tial description is revised by randomly aggregating
some sentences and adding cue phrases such as and
then, after that etc. to achieve some variation in the
generated text.
To generate appropriate gestures to accompany
the verbal route description, the generated text is
Figure 1: The Virtual Guide.
extended with tags associating the words in the
route description with different types of gestures.
Currently this is done using a simple keyword ap-
proach. Direction words (left, right) are associated
with pointing gestures in the corresponding direc-
tions, and references to landmarks are associated
with deictic gestures pointing to either the absolute
or the relative location of these objects (see Sec-
tion 3). Some iconic gestures (i.e., gestures that have
a resemblance in shape to what they depict) are also
available, for example a horizontal tube-like gesture
that can be used in references to corridors and tun-
nels. Unlike the pointing gestures, which are gener-
ated ?on the fly?, the iconic gestures of the Virtual
Guide are generated by using canned animations.
For a more sophisticated approach to the generation
of iconic gestures, see the work by Kopp et al (in
press) who describe the dynamic planning of novel
iconic gestures by NUMACK, an embodied conver-
sational agent that functions as a virtual guide for the
Northwestern University campus.
The last stage of the route description process in
our Virtual Guide is to send the marked-up text to
the animation planner, which actually generates the
required animations in synchronization with text-to-
speech output. The animation planner is based on
the work by Welbergen et al (2006).
3 The Guide?s gestures and orientation
During the route description, the Virtual Guide can
make pointing gestures from either an ?objective?
viewpoint, i.e., pointing at the absolute locations of
objects, or from a ?character? viewpoint, i.e., point-
26
ing at locations relative to the position of a person
who is walking the route. An objective viewpoint
makes most sense when pointing at objects that are
(in principle) visible to both the agent and the user,
which is only the case for objects that are located at
the start of the route. So, most of the time the Guide
will be using the character viewpoint, pointing left
and right relative to its own body to indicate land-
marks and directions from the perspective of some-
one who is walking along the route being described.
The typical orientation of information presenting
agents is facing the user. However, it is not a priori
clear that this would be the best option for the Vir-
tual Guide. When facing the user, all pointing ges-
tures made by the guide from a character viewpoint
would mirrored in the eyes of the user, so the latter
would have to perform a mental 180? re-orientation
of the gestures. This would demand extra cognitive
effort on top of processing and storing the verbally
presented route information, and might negatively
influence the user?s ability to reproduce the route di-
rections during actual traversal of the route.
In actual direction giving situations, people of-
ten tend to minimize the difference in orientation
between them. Therefore we wondered if reducing
the difference in orientation between the agent and
the user would help the user to find his way dur-
ing traversal. If the agent would turn to face almost
the same direction as the user, its gestures could be
expressed as close to the route seeker?s perspective
as possible, thus reducing the cognitive load for the
user in processing them. Also, we wondered if this
configuration would yield a more natural effect than
having the agent directly face the user during the
route description. We investigated these questions
in an experiment where participants had to virtu-
ally follow a route, presented to them in one of two
versions that differed in the orientation of the route
provider. Because the Virtual Guide was still be-
ing implemented at the time, we used route descrip-
tions by a human route provider. The experimental
setup and its results are presented below, followed
by some conclusions and future research directions.
4 The orientation experiment
The goal of the experiment was to investigate the ef-
fect of speaker orientation on the effectiveness and
Figure 2: Angle between route provider and route
seeker (camera)
naturalness of a route description. For our exper-
iment, we opted to use prerecorded route descrip-
tions, as this matched the capabilities of our Vir-
tual Guide (which can only present the route as a
monologue with no interaction) and also ensured
an unlimited number of reproductions of constant
quality and content. We recorded two separate
route descriptions that differed in speaker orienta-
tion with respect to the route seeker, but were other-
wise (largely) the same:
180? version The route provider is oriented at a
180? angle with respect to the route seeker, i.e.,
he directly faces the camera lens, creating mir-
rored gestures (his left is seen as right by the
viewer and vice versa). See Figures 2(a) and
3(a).
120? version The route provider is oriented at a
120? angle toward the route seeker, as if to
adapt his orientation to that of the route seeker.
See Figures 2(b) and 3(b).
We chose an orientation of 120? for the route
seeker-oriented version, so as to maintain visibility
of non-verbal signals. If the route provider were to
assume an orientation of 90? or less, as illustrated
in Figure 2(c), not all gestures would be visible and
maintaining eye contact could make his posture un-
natural.
The 120? and the 180? condition only differed
in bodily orientation while eye contact remained
unchanged and facial expressions remained visi-
ble. Also, although wording slightly varied, the
presented information was the same in both condi-
tions. The route descriptions were recorded on lo-
cation in a small town with short streets and plenty
27
a) b)
Figure 3: ?Turn left at the white building? (a: 180?, b: 120?)
of landmarks. The route being described led from
the recording location to the town hotel. The verbal
description was similar in structure to those gener-
ated by the Virtual Guide. It mentioned five decision
points, each connected with one or two characteris-
tic landmarks. For example, At the men?s fashion
shop, you turn right. During the route description,
the route provider made beat gestures and pointing
gestures from a character viewpoint, taking his own
body orientation as a reference for left and right.
Apart from a few slight variations, the gestures used
in both versions of the route description were the
same; see Figure 3. At the start of the route de-
scription, both route provider and route seeker were
exactly (180? version) or almost (120? version) per-
pendicular to the starting direction of the route.
After viewing one of the two versions of the route
description, the participants in the experiment had
to ?virtually traverse? the route (to measure effec-
tiveness of the route description) and were asked
how natural they found the route description. The
most realistic way to measure effectiveness of the
route description would have been to have the partic-
ipants walk the route in reality after having received
the description, as was done by Fujii et al (2000)
and Michon and Denis (2001). However, conduct-
ing such an experiment is a very time consuming
activity. As a more practical alternative we devel-
oped a reconstructive method allowing participants
to traverse the route on the computer, instead of in
a real (live) environment. In this set-up, participants
?traversed? the route by viewing prerecorded route
segments, showing a moving scene from a first per-
son perspective as if they walked through the streets
themselves, accompanied by street sounds. Apart
from practical considerations, an additional advan-
tage of this set-up is that it yields full control with
respect to repeatability and the participation setting
because of its playback nature.
Our hypotheses were as follows:
1. The 120? version is more effective, i.e., yields
a more successful traversal than its 180? coun-
terpart.
2. The 120? version yields a more natural route
description than its 180? counterpart.
4.1 Participants
A total of 49 participants were involved in the ex-
periment, aged 20 to 64 years (with an average of 33
years). Since no participants were younger than 12
or post 70, no specific effect of age on their spatial
skills was expected (Hunt and Waller, 1999). Since
gender is an influential factor in orientation and way
finding (Hunt and Waller, 1999; Lawton, 1994), we
used a 50% male - 50% female test population. The
120? version of the route description was shown to
13 male and 12 female participants; the 180? version
to 11 male and 13 female participants.
4.2 Procedure
The experiment consisted of the following steps.
Introduction - After reading an introductory text
explaining the experiment, the participant filled in a
pre-questionnaire asking for age, gender, and edu-
cational level. We also asked how familiar the par-
ticipant was with the route location, indicated on a
28
5-point scale ranging from not at all familiar (1) to
very familiar (5). If the participant indicated being
moderately or more familiar with the location, his or
her results were discarded. The questionnaire was
followed by an example question to familiarize the
participant with the controls and with the set-up of
the traversal part of the experiment.
Route description - First, the participant was
shown a video impression of the location where he
or she, being lost in an unfamiliar town, supposedly
approached someone to ask the way to the hotel.
Then the participant watched one of the two pre-
recorded route descriptions. To compensate for the
fact that, unlike a real-life situation, there was no
opportunity to verify understanding or ask for clar-
ifications, the participants were allowed to play the
route description video twice.
Traversal - After having received the route de-
scription, the participant had to virtually traverse
the route by watching six prerecorded traversal seg-
ments in succession, appearing in a pop-up window.
The first segment began at the starting point of the
route and ended at the first decision point (intersec-
tion). Each following segment started where the pre-
vious one ended, with the final segment ending at
the destination of the route. At the end of each route
segment, an overview of the next intersection was
provided by moving the camera viewpoint gradu-
ally so the entire intersection was shown. The av-
erage length of each traversal segment was around
1.5 minutes.
After watching each segment, the participant had
to select which direction to take next from a lim-
ited set of options: left, straight ahead or right (if
applicable). Each option was accompanied with a
photo of the corresponding view from the crossing.
After answering the question, the participant was in-
formed which direction was correct. Then the par-
ticipant proceeded with the route traversal from the
correct turn, regardless whether the correct direction
had been chosen or not.3
3This differs from the effectiveness measure of Fujii et al
(2000), who used a movement failure rate defined as Out/N,
with Out being the number of times a participant lost the way
and was unable to return to the route, and N being the number
of trials. We found this method too complicated in design and
too confusing for the participants to be used in this experiment.
In our set-up, the participant was only allowed one trial per de-
cision point and always traveled along the correct route.
120? 180? Total
Male 3.46 (0.88) 3.27 (1.19) 3.38 (1.01)
Female 4.00 (1.04) 3.62 (0.77) 3.80 (0.91)
Total 3.72 (0.98) 3.46 (0.98) 3.59 (0.98)
Table 1: Number of correct decisions as a func-
tion of gender and version (results are presented as
Means with Std. Deviations in brackets).
Post-questionnaire - After route traversal, the
participants answered several questions about the
route description. Here we only focus on one of the
questions, i.e., ?Do you think the route provider de-
scribed the route in a natural way??, to be answered
on a 5-point scale ranging from very natural (1) to
very artificial (5). The participants were also offered
the opportunity to comment on their answer.
5 Results and discussion
Here we present and discuss the main findings from
our experiment.
5.1 Effectiveness of the route description
Hypothesis 1 concerned the influence of speaker ori-
entation on the effectiveness of the route description.
We measured this by counting the number of correct
turns taken by the participants during route traver-
sal. The route contained five decision points (inter-
sections), so participants? scores ranged from 0 to 5
correct turns. Gender has been proved to strongly in-
fluence way finding ability (Hunt and Waller, 1999;
Lawton, 1994), so gender was accounted for as a
fixed factor in our analysis.
The results are summarized in Table 1, which
shows that participants performed slightly better in
the 120? version than in the 180? version, and that
women performed slightly better than men. How-
ever, these differences were not significant; neither
for version nor gender. Thus, our first hypothesis is
not supported.
This lack of effect might be taken as evidence
that gestures hardly play a role in conveying in-
formation, so that a difference in their orientation
would not affect the route seeker?s mental process-
ing of the route description. It has been argued
that the main function of gestures in conversation
is not to transfer information to the interlocutor,
but to facilitate the cognitive process of speaking
29
(Rime? and Schiaratura, 1991; Morsella and Krauss,
2004). Still, though most spontaneous gestures may
not be produced for the interlocutor?s benefit, it has
been shown experimentally that people do make use
of the information conveyed by gestures (Kendon,
1994; Cassell et al, 1999; Kelly et al, 1999). The
communicative power of gestures does seem to de-
pend on the task and the type of gesture, however
(Bangerter and Chevalley, 2007). In fact, in our ex-
periment the gestures were not essential for under-
standing the route description. All pointing gestures
were accompanied by explicit verbal descriptions of
the corresponding landmarks and/or directions; in
other words, the gestures were redundant with re-
spect to speech. So, regarded from a purely informa-
tional point of view, these gestures were superfluous
and the participants may have paid only limited at-
tention to them or even consciously ignored them.
This explanation is supported by the comments of
various participants who said they tried to focus on
the verbal instructions because the description was
extensive and they found the gestures distracting.
We consciously limited the number of decision
points in the experiment to five, well within the 7?2
range of short term memory, but for each decision
point the route provider not only mentioned the di-
rection to take, but also one or two landmarks. Fur-
thermore, he gave some auxiliary hints of what to do
in-between turns (Walk straight ahead until you see
a traffic sign; there you keep walking straight ahead)
and some more details. In their comments, several
participants mentioned being distracted by too much
detail in the description, and said they found the di-
rections hard to remember. As a consequence, some
participants tended to ignore the gestures or look
away from the computer screen altogether. Obvi-
ously, doing so would clearly impair the effect of
speaker orientation to be demonstrated by the exper-
iment. On the other hand, not all participants ig-
nored the gestures (at least not initially) as in the
180? version, some participants declared that they
found the mirrored gestures annoying.
5.2 Naturalness of the route description
In Table 2, test results on the naturalness of the route
description are shown for speaker orientation and
gender. Orientation had an almost-significant effect
on participants? judgement of naturalness (two-way
ANOVA; F(1,45)=3.35, p=0.07 two-tailed).4 The
effect would have been significant if it had been the
other way around. The effect of gender was not sig-
nificant, and neither was the interaction of version
and gender.
Contrary to our hypothesis, the participants
judged the 180? version as being more natural than
the 120? version. This was contrary to what was ex-
pected, because ?in the real world? route providers
and seekers tend to minimize the difference in their
orientation. In fact, as mentioned above, several
participants reported being annoyed by the mirrored
gestures in the 180? version. These contradictory
findings suggest that it was not the route provider?s
gestures or their orientation that were crucial for
the judgement on naturalness, but only whether the
route provider?s body was fully turned toward his au-
dience ? directly addressing them ? or not. This may
be the result of many previous confrontations with
presenters (human or other) displayed on television
or computer screens, explaining things to an audi-
ence. Perhaps the natural tendency to make orienta-
tions as similar as possible when explaining a route
to someone does not transfer to a situation where the
route is presented by somebody on a screen: a form
of presentation in which we expect someone to be
facing us.
Furthermore, the fixed position of the camera dur-
ing the route description may also have interfered
with its naturalness. If the route provider points into
some direction, we tend to turn our heads to that di-
rection, maybe in the assumption he will point at
some landmark that can help us orientate or navi-
gate. The fixed position of the camera, in contrast
with the adaptive orientation of the route provider,
may have yielded an unnatural combination in the
case of the 120? version of the route description.
5.3 Gender effects
For both versions of the route description, women
performed better than men. Although not signifi-
cant, the difference in performance is sufficiently re-
markable to merit some discussion. We believe the
difference may be explained by the fact that women
and men employ different strategies for way find-
4A two-tailed test was performed in spite of our one-sided
hypothesis 2, because the effect was contrary to what was ex-
pected.
30
120? 180? Total
Male 2.62 (1.26) 1.73 (0.91) 2.21 (1.18)
Female 2.75 (1.14) 2.46 (1.13) 2.60 (1.12)
Total 2.68 (1.18) 2.13 (1.08) 2.41 (1.15)
Table 2: Naturalness as a function of gender and ver-
sion (results are presented as Means with Std. Devi-
ations in brackets).
ing (Hunt and Waller, 1999): women?s strategies are
most suited for tracking and piloting, whereas men
use strategies appropriate for navigation. Tracking
is a point-to-point way finding strategy that relies on
information limited to environmental characteristics
along the route. Piloting combines these environ-
mental characteristics with self-centered orientation
and direction (e.g., ?When you?re facing the main
entrance, turn to the right?). Navigation, on the other
hand, uses configurational information: routes are
derived from knowledge of the surroundings of the
destination or its global position. Thus, men tend to
pay attention to bearings while women often rely on
descriptions of control points and cues to the route
such as landmarks (Lawton, 1994).
Looking at the set-up of our experiment, we see
that it seems to favour a strategy of point-to-point
decision making instead of relying on a more gen-
eral and global sense of direction, as in naviga-
tion. First, the route description consisted entirely
of landmarks to identify decision points and turns
to be made when encountering them, fitting a track-
ing and piloting approach to way finding. Second,
both the route description and the traversal segments
were shown on a screen, with a restricted and forced
field of vision. This may have impeded the estima-
tion of global position, direction and distance, i.e.,
the kind of spatial knowledge men rely on for orien-
tation and way finding. So, the way finding strategy
that women already tend to employ in everyday life
may have been most suited to this experiment and
hence their higher score.
6 Conclusions and future work
The goal of this study was to find out which ori-
entation of the Virtual Guide would be most ef-
fective and natural for providing route descriptions
in a virtual environment. To test effectiveness, we
devised a method that allowed participants to ?vir-
tually? traverse a route by watching pre-recorded
route segments and making turn decisions at inter-
sections. We hypothesized that a speaker orientation
of 120? with respect to the route seeker would re-
sult in a more effective and natural route description
than a 180? orientation, because it would take the
route seeker less effort to match the speaker?s ges-
tures with his or her own perspective. However, we
found no effect of speaker orientation on task per-
formance. A possible explanation lies in the com-
plexity of our route description, which caused some
participants to focus only on the verbal part of the
description. Contrary to our expectation, the 180?
orientation was judged to be more natural, in spite
of the fact that some participants found the mirrored
gestures annoying. The reason for this may be that
people expect a speaker to be directly facing them
when presenting information on a screen.
Based on these results, we decided to stick to
the standard 180? orientation for our Virtual Guide.
However, some reservations are in order when ap-
plying the results of our study to the Virtual Guide.
For one thing, the route descriptions used in the ex-
periment were not given by an agent but by a real
human, albeit pre-recorded. This is still far from
the situation in which an embodied agent is com-
municating with a user by means of an interface.
A second difference with the Virtual Guide lies in
the participant?s navigational control. In the con-
text of the Virtual Guide, the user can actively nav-
igate through, and look around in, the environment
to be traversed. In our experiment, the participants?
view was restricted and forced by that of the camera
which severely restricted their possibilities for ori-
entation and navigation.
An obvious line of future research is therefore to
repeat our experiment with the Virtual Guide, and
have participants actually traverse the route by nav-
igating through the 3D virtual environment, with to-
tal freedom of movement. This will make the traver-
sal part more realistic and also more suitable for
male way finding strategies, thus providing a bet-
ter and more neutral measure for the effectiveness of
the route description. In addition, we expect that the
participants will be less inclined to see the guide as
a kind of TV presenter and more as a real presence,
because they will (virtually) share the same 3D en-
vironment with it. This may lead the participants to
31
be less biased toward a 180? orientation of the route
provider. Finally, all information not strictly nec-
essary for way finding will be left out of the route
description. This includes landmarks located along
traversal segments rather than at intersections, and
instructions to go ?straight ahead? (which several
participants found confusing in the current experi-
ment). With a less complex description, participants
may refrain from ignoring the gestures made by the
route provider and thereby be more susceptible to
manipulation of speaker orientation.
Acknowledgements
The authors would like to thank Mark Tempelman
and Job van den Wildenberg for their help with
the experiment. The Virtual Guide was imple-
mented by Dennis Hofs, Rieks op den Akker, Marco
van Kessel, Richard Korthuis and Martin Bouman.
The research reported here was carried out within
the context of the project ANGELICA (A Natural-
language Generator for Embodied, Lifelike Con-
versational Agents) sponsored by the Netherlands
Organisation for Scientific Research, NWO (grant
number 532.001.301).
References
A. Bangerter and E. Chevalley. 2007. Pointing and
describing in referential communication: When are
pointing gestures used to communicate? In Proceed-
ings of the Workshop on Multimodal Output Genera-
tion (MOG 2007), pages 17?28.
J. Cassell, D. McNeill, and K.E. McCullough. 1999.
Speech-gesture mismatches: Evidence for one under-
lying representation of linguistic and non-linguistic in-
formation. Pragmatics and Cognition, 7(1):1?33.
R. Dale, S. Geldof, and J. Prost. 2005. Using natural lan-
guage generation in automatic route description. Jour-
nal of Research and Practice in Information Technol-
ogy, 37(1):89?105.
K. Fujii, S. Nagai, Y. Miyazaki, and K. Sugiyama. 2000.
Navigation support in a real city using city metaphors.
In T. Ishida and K. Isbister, editors, Digital Cities, Lec-
ture Notes in Computer Science 1765, pages 338?349.
Springer-Verlag, Berlin Heidelberg.
D. Hofs, R. op den Akker, and A. Nijholt. 2003. A
generic architecture and dialogue model for multi-
modal interaction. In P. Paggio, K. Jokinen, and
A. Jnsson, editors, Proceedings of the 1st Nordic Sym-
posium on Multimodal Communication, volume 1,
pages 79?91, Copenhagen. CST Publication, Center
for Sprogteknologi.
E. Hunt and D. Waller. 1999. Orientation and wayfind-
ing: A review. ONR technical report N00014-96-
0380, Office of Naval Research, Arlington, VA.
S. D. Kelly, D. Barr, R.B. Church, and K. Lynch. 1999.
Offering a hand to pragmatic understanding: The role
of speech and gesture in comprehension and memory.
Journal of Memory and Language, 40:577?592.
A. Kendon. 1994. Do gestures communicate? a re-
view. Research on Language and Social Interaction,
27(3):175?200.
S. Kopp, P. Tepper, K. Striegnitz, and J. Cassell. in press.
Trading spaces: How humans and humanoids use
speech and gesture to give directions. In T. Nishida,
editor, Engineering Approaches to Conversational In-
formatics. John Wiley and Sons.
C.A. Lawton. 1994. Gender differences in wayfinding
strategies: Relationship to spatial ability and spatial
anxiety. Sex Roles, 30(11-12):765?779.
P. Michon and M. Denis. 2001. When and why are vi-
sual landmarks used in giving directions? In D.R.
Montello, editor, Spatial Information Theory. Foun-
dations of Geographic Information Science: Inter-
national Conference, COSIT 2001, Lecture Notes in
Computer Science 2205, pages 292?305. Springer-
Verlag, Berlin Heidelberg.
E. Morsella and R. Krauss. 2004. The role of gestures in
spatial working memory and speech. American Jour-
nal of Psychology, 117(3):251?270.
B. Rime? and L. Schiaratura. 1991. Gesture and speech.
In R. Feldman and B. Rime?, editors, Fundamentals of
Nonverbal Behavior, pages 239?281. Cambridge Uni-
versity Press, Cambridge.
A.L. Shelton and T.P. McNamara. 2004. Spatial mem-
ory and perspective taking. Memory and Cognition,
32(3):416?426.
H. van Welbergen, A. Nijholt, D. Reidsma, and J. Zwiers.
2006. Presenting in virtual worlds: Towards an archi-
tecture for a 3d presenter explaining 2d-presented in-
formation. IEEE Intelligent Systems, 21(5):47?53.
M. White and T. Caldwell. 1998. EXEMPLARS: A
practical, extensible framework for dynamic text gen-
eration. In Proceedings of the Ninth International
Workshop on Natural Language Generation, pages
266?275.
32
Proceedings of the 12th European Workshop on Natural Language Generation, pages 183?184,
Athens, Greece, 30 ? 31 March 2009. c?2009 Association for Computational Linguistics
Realizing the Costs: Template-Based Surface Realisation in the GRAPH
Approach to Referring Expression Generation
Ivo Brugman
University of Twente
The Netherlands
i.h.g.brugman@student.utwente.nl
Marie?t Theune
University of Twente
The Netherlands
m.theune@utwente.nl
Emiel Krahmer
Tilburg University
The Netherlands
e.j.krahmer@uvt.nl
Jette Viethen
Macquarie University
Australia
jviethen@ics.mq.edu.au
Abstract
We describe a new realiser developed for
the TUNA 2009 Challenge, and present its
evaluation scores on the development set,
showing a clear increase in performance
compared to last year?s simple realiser.
1 Introduction
The TUNA Challenge 2009 is the last in a series
of challenges using the TUNA corpus of refer-
ring expressions (Gatt et al 2007) for compara-
tive evaluation of referring expression generation.
The 2009 Challenge is aimed at end-to-end re-
ferring expression generation, which encompasses
two subtasks: (1) attribute selection, choosing a
number of attributes that uniquely characterize a
target object, distinguishing it from other objects
in a visual scene, and (2) realisation, converting
the selected set of attributes into a word string.
Our contributions to the previous Challenges fo-
cused on subtask (1), but this year we focus on
subtask (2). Below, we briefly sketch how attribute
selection is performed in our system, describe our
newly developed realiser, and present our evalua-
tion results on the TUNA 2009 development set.
2 Attribute selection
We use the Graph-based algorithm of Krahmer
et al (2003) for attribute selection. In this ap-
proach, objects and their attributes are represented
in a graph as nodes and edges respectively, and
attribute selection is seen as a graph search prob-
lem that outputs the cheapest distinguishing graph,
given a particular cost function that assigns costs
to attributes. By assigning zero costs to some at-
tributes, e.g., the type of an object, the human
tendency to mention redundant properties can be
mimicked. For the TUNA Challenge 2009 we
use the same settings as last year (Krahmer et al
2008). The used cost function assigns a zero cost
to attributes that are highly frequent in the TUNA
corpus, while the other attributes have a cost of
either 1 (somewhat infrequent) or 2 (very infre-
quent). The order in which attributes are added
is also controlled: to ensure that the cheapest at-
tributes are added first, they are tried in the order
of their frequency in the TUNA (2008) training
corpus. Using these settings, last year the GRAPH
attribute selection algorithm made the top 3 on all
evaluation measures (Gatt et al 2008, Table 11).
3 Realisation
The main resource for realisation is a set of tem-
plates, derived from the human-produced object
descriptions in the TUNA 2009 training data. To
construct the templates, we first grouped the de-
scriptions by the combination of attributes they
expressed. For instance, in the domain of furni-
ture references, all descriptions expressing the at-
tributes colour, type and orientation were grouped
together. This was done for all combinations of
attributes. Next, for each description, parts of the
word string were related to the attributes in the set.
For instance, for the string ?red couch facing left?,
we linked ?red? to colour, ?couch? to type, and
?facing left? to orientation.1 This provided us with
information on how the attributes were expressed
(e.g., by adjectives or prepositional phrases) and
in which order they appeared in the word string.
For each combination of attributes, the surface or-
der that occurred most frequently was selected as
the basis for a template. If multiple orderings
were equally frequent, we chose the most natural-
seeming one. This resulted in templates such as
?the [colour] [type] facing [orientation]? for the at-
tribute set {type, colour, orientation}.
During realisation, the templates are used as fol-
1This corresponds to the ANNOTATED-WORD-STRING
nodes already present in the TUNA corpus. Unfortunately,
various problems prevented us from automatically deriving
our templates from those existing annotations.
183
lows. When a set of attributes is input to the re-
aliser, it checks if there is a template matching this
particular attribute combination. If so, the tem-
plate is selected, and the gaps in the template are
filled with lexical expressions for the attribute val-
ues. The words used to express the values are
those that occurred most frequently in the train-
ing data for this particular template. If no match-
ing template is found, a description is generated
in a simple rule-based fashion, based on the re-
aliser we used last year, but with improved lexical
choices. For example, the old realiser always used
the word ?person? to express the type attribute in
descriptions of people, whereas in the TUNA cor-
pus ?man? is used most frequently. We changed
the realiser to reflect such human preferences.
Template construction for the furniture domain
was fairly straightforward, resulting in 25 tem-
plates. In practice, only 13 of these are used. Since
the GRAPH attribute selection algorithm adds the
type and colour attributes to a description for free,
these attributes are always selected, making any
templates lacking them irrelevant given the current
settings of the algorithm.
For the more realistic people domain, template
construction was more complicated. For exam-
ple, when the hairColour attribute is mentioned in
human descriptions it can refer either to the hair
on a person?s head (?white-haired?) or his beard
(?with a white beard?). The attribute selection al-
gorithm does not make this distinction, leaving it
unclear which of the two realisations should be
used when hairColour and hasBeard attributes are
both to be included in a description. We solved
this by simply using the expression that occurred
most frequently in the training data for each at-
tribute combination, even allowing hairColour to
be mentioned twice if this happened in most hu-
man descriptions. Another problem is that many
attribute combinations occurred only once in the
training data, leading to a very large number (50+)
of potential templates. We reduced this number in
an ad hoc manner, by ignoring combinations in-
volving attributes (such as hasHair) that are very
unlikely to be selected given the current settings
of the attribute selection algorithm. This approach
left us with 40 templates in the people domain.
4 Evaluation
System performance is measured by comparing
the generated word strings to the human descrip-
MED MNED BLEU 3
Furniture 4.94 (5.48) 0.48 (0.50) 0.27 (0.22)
People 5.15 (7.53) 0.46 (0.67) 0.33 (0.07)
Overall 5.03 (6.42) 0.47 (0.58) 0.30 (0.15)
Table 1: Results on the 2009 development set (be-
tween brackets are those using last year?s realiser).
tions in the TUNA development set, comprising
80 furniture and 68 people descriptions. The eval-
uation measures reported here are mean edit dis-
tance (MED), the mean of the token-based Lev-
enshtein edit distance between the reference word
strings and the system word strings, mean nor-
malised edit distance (MNED), where the edit dis-
tance is normalised by the number of tokens, and
cumulative BLEU 3 score. Table 1 summarizes
our evaluation results. For comparison, we also
provide the results obtained when using last year?s
simple realiser, which we reimplemented in Java.
We see a clear improvement when we compare
the performance of the new and the old realiser, in
particular in the people domain. However, further
evaluation experiments are required to determine
whether the improvements are mostly due to our
use of templates derived from human descriptions,
or to the simple improvements in lexical choice
incorporated in the rules used as fall-back in case
no matching templates are found.
To further improve the realiser, we need to add
templates for all remaining attribute combinations
found in the corpus. This should not be difficult,
as the set-up of the realiser allows easy creation of
templates. It should also be easily portable to other
languages; in fact we intend to explore its use for
the realisation of referring expressions in Dutch.
References
Gatt, A., I. van der Sluis and K. van Deemter 2007.
Evaluating algorithms for the generation of referring
expressions using a balanced corpus. Proceedings of
ENLG 2007 49-56.
Gatt, A., A. Belz and E. Kow 2008. The TUNA chal-
lenge 2008: Overview and evaluation results Pro-
ceedings of INLG 2008 198-206.
Krahmer, E., S. van Erk and A. Verleg 2003. Graph-
based generation of referring expressions. Compu-
tational Linguistics, 29(1), 53-72.
Krahmer, E., M. Theune, J. Viethen, and I. Hendrickx
2008. GRAPH: The costs of redundancy in referring
expressions. Proceedings of INLG 2008 227-229.
184
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 660?664,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Does Size Matter ? How Much Data is Required to Train a REG Algorithm?
Marie?t Theune
University of Twente
P.O. Box 217
7500 AE Enschede
The Netherlands
m.theune@utwente.nl
Ruud Koolen
Tilburg University
P.O. Box 90135
5000 LE Tilburg
The Netherlands
r.m.f.koolen@uvt.nl
Emiel Krahmer
Tilburg University
P.O. Box 90135
5000 LE Tilburg
The Netherlands
e.j.krahmer@uvt.nl
Sander Wubben
Tilburg University
P.O. Box 90135
5000 LE Tilburg
The Netherlands
s.wubben@uvt.nl
Abstract
In this paper we investigate how much data
is required to train an algorithm for attribute
selection, a subtask of Referring Expressions
Generation (REG). To enable comparison be-
tween different-sized training sets, a system-
atic training method was developed. The re-
sults show that depending on the complexity
of the domain, training on 10 to 20 items may
already lead to a good performance.
1 Introduction
There are many ways in which we can refer to ob-
jects and people in the real world. A chair, for ex-
ample, can be referred to as red, large, or seen from
the front, while men may be singled out in terms
of their pogonotrophy (facial hairstyle), clothing and
many other attributes. This poses a problem for al-
gorithms that automatically generate referring ex-
pressions: how to determine which attributes to use?
One solution is to assume that some attributes
are preferred over others, and this is indeed what
many Referring Expressions Generation (REG) al-
gorithms do. A classic example is the Incremental
Algorithm (IA), which postulates the existence of
a complete ranking of relevant attributes (Dale and
Reiter, 1995). The IA essentially iterates through
this list of preferred attributes, selecting an attribute
for inclusion in a referring expression if it helps sin-
gling out the target from the other objects in the
scene (the distractors). Crucially, Dale and Reiter do
not specify how the ranking of attributes should be
determined. They refer to psycholinguistic research
suggesting that, in general, absolute attributes (such
as color) are preferred over relative ones (such as
size), but stress that constructing a preference order
is essentially an empirical question, which will dif-
fer from one domain to another.
Many other REG algorithms similarly rely on
preferences. The graph-based based REG algorithm
(Krahmer et al, 2003), for example, models prefer-
ences in terms of costs, with cheaper properties be-
ing more preferred. Various ways to compute costs
are possible; they can be defined, for instance, in
terms of log probabilities, which makes frequently
encountered properties cheap, and infrequent ones
more expensive. Krahmer et al (2008) argue that
a less fine-grained cost function might generalize
better, and propose to use frequency information
to, somewhat ad hoc, define three costs: 0 (free),
1 (cheap) and 2 (expensive). This approach was
shown to work well: the graph-based algorithm was
the best performing system in the most recent REG
Challenge (Gatt et al, 2009).
Many other attribute selection algorithms also
rely on training data to determine preferences in one
form or another (Fabbrizio et al, 2008; Gerva?s et
al., 2008; Kelleher, 2007; Spanger et al, 2008; Vi-
ethen and Dale, 2010). Unfortunately, suitable data
is hard to come by. It has been argued that determin-
ing which properties to include in a referring expres-
sion requires a ?semantically transparent? corpus
(van Deemter et al, 2006): a corpus that contains
the actual properties of all domain objects as well
as the properties that were selected for inclusion in
a given reference to the target. Obviously, text cor-
pora tend not to meet this requirement, which is why
660
semantically transparent corpora are often collected
using human participants who are asked to produce
referring expressions for targets in controlled visual
scenes for a given domain. Since this is a time con-
suming exercise, it will not be surprising that such
corpora are thin on the ground (and are often only
available for English). An important question there-
fore is how many human-produced references are
needed to achieve a certain level of performance. Do
we really need hundreds of instances, or can we al-
ready make informed decisions about preferences on
a few or even one training instance?
In this paper, we address this question by sys-
tematically training the graph-based REG algorithm
on a number of ?semantically transparent? data sets
of various sizes and evaluating on a held-out test
set. The graph-based algorithm seems a good can-
didate for this exercise, in view of its performance
in the REG challenges. For the sake of compari-
son, we also follow the evaluation methodology of
the REG challenges, training and testing on two do-
mains (a furniture and a people domain), and using
two automatic metrics (Dice and accuracy) to mea-
sure human-likeness. One hurdle needs to be taken
beforehand. Krahmer et al (2008) manually as-
signed one of three costs to properties, loosely based
on corpus frequencies. For our current evaluation
experiments, this would hamper comparison across
data sets, because it is difficult to do it in a manner
that is both consistent and meaningful. Therefore we
first experiment with a more systematic way of as-
signing a limited number of frequency-based costs
to properties using k-means clustering.
2 Experiment I: k-means clustering costs
In this section we describe our experiment with k-
means clustering to derive property costs from En-
glish and Dutch corpus data. For this experiment we
looked at both English and Dutch, to make sure the
chosen method does not only work well for English.
2.1 Materials
Our English training and test data were taken from
the TUNA corpus (Gatt et al, 2007). This semanti-
cally transparent corpus contains referring expres-
sions in two domains (furniture and people), col-
lected in one of two conditions: in the -LOC con-
dition, participants were discouraged from mention-
ing the location of the target in the visual scene,
whereas in the +LOC condition they could mention
any properties they wanted. The TUNA corpus was
used for comparative evaluation in the REG Chal-
lenges (2007-2009). For training in our current ex-
periment, we used the -LOC data from the training
set of the REG Challenge 2009 (Gatt et al, 2009):
165 furniture descriptions and 136 people descrip-
tions. For testing, we used the -LOC data from the
TUNA 2009 development set: 38 furniture descrip-
tions and 38 people descriptions.
Dutch data were taken from the D-TUNA corpus
(Koolen and Krahmer, 2010). This corpus uses the
same visual scenes and annotation scheme as the
TUNA corpus, but with Dutch instead of English
descriptions. D-TUNA does not include locations as
object properties at all, hence our restriction to -LOC
data for English (to make the Dutch and English data
more comparable). As Dutch test data, we used 40
furniture items and 40 people items, randomly se-
lected from the textual descriptions in the D-TUNA
corpus. The remaining furniture and people descrip-
tions (160 items each) were used for training.
2.2 Method
We first determined the frequency with which each
property was mentioned in our training data, relative
to the number of target objects with this property.
Then we created different cost functions (mapping
properties to costs) by means of k-means clustering,
using the Weka toolkit. The k-means clustering al-
gorithm assigns n points in a vector space to k clus-
ters (S1 to Sk) by assigning each point to the clus-
ter with the nearest centroid. The total intra-cluster
variance V is minimized by the function
V =
k
?
i=1
?
xj?Si
(xj ? ?i)2
where ?i is the centroid of all the points xj ? Si.
In our case, the points n are properties, the vector
space is one-dimensional (frequency being the only
dimension) and ?i is the average frequency of the
properties in Si. The cluster-based costs are defined
as follows:
?xj ? Si, cost(xj) = i? 1
661
where S1 is the cluster with the most frequent
properties, S2 is the cluster with the next most fre-
quent properties, and so on. Using this approach,
properties from cluster S1 get cost 0 and thus can be
added ?for free? to a description. Free properties are
always included, provided they help distinguish the
target. This may lead to overspecified descriptions,
mimicking the human tendency to mention redun-
dant properties (Dale and Reiter, 1995).
We ran the clustering algorithm on our English
and Dutch training data for up to six clusters (k = 2
to k = 6). Then we evaluated the performance of
the resulting cost functions on the test data from
the same language, using Dice (overlap between at-
tribute sets) and Accuracy (perfect match between
sets) as evaluation metrics. For comparison, we also
evaluated the best scoring cost functions from Theu-
ne et al (2010) on our test data. These ?Free-Na??ve?
(FN) functions were created using the manual ap-
proach sketched in the introduction.
The order in which the graph-based algorithm
tries to add attributes to a description is explicitly
controlled to ensure that ?free? distinguishing prop-
erties are included (Viethen et al, 2008). In our
tests, we used an order of decreasing frequency; i.e.,
always examining more frequent properties first.1
2.3 Results
For the cluster-based cost functions, the best perfor-
mance was achieved with k = 2, for both domains
and both languages. Interestingly, this is the coarsest
possible k-means function: with only two costs (0
and 1) it is even less fine-grained than the FN func-
tions advocated by Krahmer et al (2008). The re-
sults for the k-means costs with k = 2 and the FN
costs of Theune et al (2010) are shown in Table 1.
No significant differences were found, which sug-
gests that k-means clustering, with k = 2, can be
used as a more systematic alternative for the manual
assignment of frequency-based costs. We therefore
applied this method in the next experiment.
3 Experiment II: varying training set size
To find out how much training data is required
to achieve an acceptable attribute selection perfor-
1We used slightly different property orders than Theune et
al. (2010), leading to minor differences in our FN results.
Furniture People
Language Costs Dice Acc. Dice Acc.
English k-means 0.810 0.50 0.733 0.29
FN 0.829 0.55 0.733 0.29
Dutch k-means 0.929 0.68 0.812 0.33
FN 0.929 0.68 0.812 0.33
Table 1: Results for k-means costs with k = 2 and the
FN costs of Theune et al (2010) on Dutch and English.
mance, in the second experiment we derived cost
functions and property orders from different sized
training sets, and evaluated them on our test data.
For this experiment, we only used English data.
3.1 Materials
As training sets, we used randomly selected subsets
of the full English training set from Experiment I,
with set sizes of 1, 5, 10, 20 and 30 items. Be-
cause the accidental composition of a training set
may strongly influence the results, we created 5 dif-
ferent sets of each size. The training sets were built
up in a cumulative fashion: we started with five sets
of size 1, then added 4 items to each of them to cre-
ate five sets of size 5, etc. This resulted in five series
of increasingly sized training sets. As test data, we
used the same English test set as in Experiment I.
3.2 Method
We derived cost functions (using k-means clustering
with k = 2) and orders from each of the training
sets, following the method described in Section 2.2.
In doing so, we had to deal with missing data: not all
properties were present in all data sets.2 For the cost
functions, we simply assigned the highest cost (1)
to the missing properties. For the order, we listed
properties with the same frequency (0 for missing
properties) in alphabetical order. This was done for
the sake of comparability between training sets.
3.3 Results
To determine significance, we calculated the means
of the scores of the five training sets for each set
size, so that we could compare them with the scores
of the entire set. We applied repeated measures of
2This problem mostly affected the smaller training sets. By
set size 10 only a few properties were missing, while by set size
20, all properties were present in all sets.
662
variance (ANOVA) to the Dice and Accuracy scores,
using set size (1, 5, 10, 20, 30, entire set) as a within
variable. The mean results for each training set size
are shown in Table 2.3 The general pattern is that
the scores increase with the size of the training set,
but the increase gets smaller as the set sizes become
larger.
Furniture People
Set size Dice Acc. Dice Acc.
1 0.693 0.25 0.560 0.13
5 0.756 0.34 0.620 0.15
10 0.777 0.40 0.686 0.20
20 0.788 0.41 0.719 0.25
30 0.782 0.41 0.718 0.27
Entire set 0.810 0.50 0.733 0.29
Table 2: Mean results for the different set sizes.
In the furniture domain, we found a main effect
of set size (Dice: F(5,185) = 7.209, p < .001; Ac-
curacy: F(5,185) = 6.140, p < .001). To see which
set sizes performed significantly different as com-
pared to the entire set, we conducted Tukey?s HSD
post hoc comparisons. For Dice, the scores of set
size 10 (p = .141), set size 20 (p = .353), and set
size 30 (p = .197) did not significantly differ from
the scores of the entire set of 165 items. The Accu-
racy scores in the furniture domain show a slightly
different pattern: the scores of the entire training set
were still significantly higher than those of set size
30 (p < .05). This better performance when trained
on the entire set may be caused by the fact that not
all of the five training sets that were used for set sizes
1, 5, 10, 20 and 30 performed equally well.
In the people domain we also found a main effect
of set size (Dice: F(5,185) = 21.359, p < .001; Accu-
racy: F(5,185) = 8.074, p < .001). Post hoc pairwise
comparisons showed that the scores of set size 20
(Dice: p = .416; Accuracy: p = .146) and set size
30 (Dice: p = .238; Accuracy: p = .324) did not
significantly differ from those of the full set of 136
items.
3For comparison: in the REG Challenge 2008, (which in-
volved a different test set, but the same type of data), the best
systems obtained overall Dice and accuracy scores of around
0.80 and 0.55 respectively (Gatt et al, 2008). These scores may
well represent the performance ceiling for speaker and context
independent algorithms on this task.
4 Discussion
Experiment II has shown that when using small data
sets to train an attribute selection algorithm, results
can be achieved that are not significantly different
from those obtained using a much larger training
set. Domain complexity appears to be a factor in
how much training data is needed: using Dice as an
evaluation metric, training sets of 10 sufficed in the
simple furniture domain, while in the more complex
people domain it took a set size of 20 to achieve re-
sults that do not significantly differ from those ob-
tained using the full training set.
The accidental composition of the training sets
may strongly influence the attribute selection per-
formance. In the furniture domain, we found clear
differences between the results of specific training
sets, with ?bad sets? pulling the overall performance
down. This affected Accuracy but not Dice, possibly
because the latter is a less strict metric.
Whether the encouraging results found for the
graph-based algorithm generalize to other REG ap-
proaches is still an open question. We also need
to investigate how the use of small training sets af-
fects effectiveness and efficiency of target identifica-
tion by human subjects; as shown by Belz and Gatt
(2008), task-performance measures do not necessar-
ily correlate with similarity measures such as Dice.
Finally, it will be interesting to repeat Experiment II
with Dutch data. The D-TUNA data are cleaner than
the TUNA data (Theune et al, 2010), so the risk of
?bad? training data will be smaller, which may lead
to more consistent results across training sets.
5 Conclusion
Our experiment has shown that with 20 or less train-
ing instances, acceptable attribute selection results
can be achieved; that is, results that do not signif-
icantly differ from those obtained using the entire
training set. This is good news, because collecting
such small amounts of training data should not take
too much time and effort, making it relatively easy
to do REG for new domains and languages.
Acknowledgments
Krahmer and Koolen received financial support from
The Netherlands Organization for Scientific Re-
search (Vici grant 27770007).
663
References
Anja Belz and Albert Gatt. 2008. Intrinsic vs. extrinsic
evaluation measures for referring expression genera-
tion. In Proceedings of ACL-08: HLT, Short Papers,
pages 197?200.
Robert Dale and Ehud Reiter. 1995. Computational in-
terpretation of the Gricean maxims in the generation of
referring expressions. Cognitive Science, 19(2):233?
263.
Giuseppe Di Fabbrizio, Amanda Stent, and Srinivas
Bangalore. 2008. Trainable speaker-based refer-
ring expression generation. In Twelfth Conference on
Computational Natural Language Learning (CoNLL-
2008), pages 151?158.
Albert Gatt, Ielka van der Sluis, and Kees van Deemter.
2007. Evaluating algorithms for the generation of re-
ferring expressions using a balanced corpus. In Pro-
ceedings of the 11th European Workshop on Natural
Language Generation (ENLG 2007), pages 49?56.
Albert Gatt, Anja Belz, and Eric Kow. 2008. The
TUNA Challenge 2008: Overview and evaluation re-
sults. In Proceedings of the 5th International Natural
Language Generation Conference (INLG 2008), pages
198?206.
Albert Gatt, Anja Belz, and Eric Kow. 2009. The TUNA-
REG Challenge 2009: Overview and evaluation re-
sults. In Proceedings of the 12th European Workshop
on Natural Language Generation (ENLG 2009), pages
174?182.
Pablo Gerva?s, Raquel Herva?s, and Carlos Le?on. 2008.
NIL-UCM: Most-frequent-value-first attribute selec-
tion and best-scoring-choice realization. In Proceed-
ings of the 5th International Natural Language Gener-
ation Conference (INLG 2008), pages 215?218.
John Kelleher. 2007. DIT - frequency based incremen-
tal attribute selection for GRE. In Proceedings of the
MT Summit XI Workshop Using Corpora for Natural
Language Generation: Language Generation and Ma-
chine Translation (UCNLG+MT), pages 90?92.
Ruud Koolen and Emiel Krahmer. 2010. The D-TUNA
corpus: A Dutch dataset for the evaluation of refer-
ring expression generation algorithms. In Proceedings
of the 7th international conference on Language Re-
sources and Evaluation (LREC 2010).
Emiel Krahmer, Sebastiaan van Erk, and Andre? Verleg.
2003. Graph-based generation of referring expres-
sions. Computational Linguistics, 29(1):53?72.
Emiel Krahmer, Marie?t Theune, Jette Viethen, and Iris
Hendrickx. 2008. GRAPH: The costs of redundancy
in referring expressions. In Proceedings of the 5th In-
ternational Natural Language Generation Conference
(INLG 2008), pages 227?229.
Philipp Spanger, Takehiro Kurosawa, and Takenobu
Tokunaga. 2008. On ?redundancy? in selecting at-
tributes for generating referring expressions. In COL-
ING 2008: Companion volume: Posters, pages 115?
118.
Marie?t Theune, Ruud Koolen, and Emiel Krahmer. 2010.
Cross-linguistic attribute selection for REG: Compar-
ing Dutch and English. In Proceedings of the 6th In-
ternational Natural Language Generation Conference
(INLG 2010), pages 174?182.
Kees van Deemter, Ielka van der Sluis, and Albert Gatt.
2006. Building a semantically transparent corpus for
the generation of referring expressions. In Proceed-
ings of the 4th International Natural Language Gener-
ation Conference (INLG 2006), pages 130?132.
Jette Viethen and Robert Dale. 2010. Speaker-dependent
variation in content selection for referring expression
generation. In Proceedings of the 8th Australasian
Language Technology Workshop, pages 81?89.
Jette Viethen, Robert Dale, Emiel Krahmer, Marie?t Theu-
ne, and Pascal Touset. 2008. Controlling redundancy
in referring expressions. In Proceedings of the Sixth
International Conference on Language Resources and
Evaluation (LREC 2008), pages 239?246.
664
GRAPH: The Costs of Redundancy in Referring Expressions
Emiel Krahmer
Tilburg University
The Netherlands
e.j.krahmer@uvt.nl
Marie?t Theune
University of Twente
The Netherlands
m.theune@utwente.nl
Jette Viethen
Macquarie University
Australia
jviethen@ics.mq.edu.au
Iris Hendrickx
University of Antwerp
Belgium
iris.hendrickx@ua.ac.be
Abstract
We describe a graph-based generation sys-
tem that participated in the TUNA attribute se-
lection and realisation task of the REG 2008
Challenge. Using a stochastic cost function
(with certain properties for free), and trying
attributes from cheapest to more expensive,
the system achieves overall .76 DICE and .54
MASI scores for attribute selection on the de-
velopment set. For realisation, it turns out
that in some cases higher attribute selection
accuracy leads to larger differences between
system-generated and human descriptions.
1 Introduction
Referring Expression Generation (REG) is a key-
task in NLG, and the topic of the REG 2008 Chal-
lenge.1 In this context, referring expressions are
understood as distinguishing descriptions: descrip-
tions that uniquely characterize a target object in a
visual scene (e.g., ?the red sofa?), and do not ap-
ply to any of the other objects in the scene (the dis-
tractors). Generating such descriptions is usually as-
sumed to be a two-step procedure: first, it has to be
decided which attributes of the target suffice to char-
acterize it uniquely, and then the selected set of at-
tributes should be converted into natural language.
For the first step, attribute selection, we use a ver-
sion of the Graph-based REG algorithm of Krahmer
et al (2003). In this approach, a visual scene is rep-
resented as a directed labelled graph, where vertices
represent the objects in the scene and edges their at-
tributes. A key ingredient of the approach is that
1See http://www.itri.brighton.ac.uk/research/reg08/.
costs can be assigned to attributes; the generation
of referring expressions can then be defined as a
graph search problem, which outputs the cheapest
distinguishing graph (if one exists) given a particu-
lar cost function. For the second step, realisation, we
use a simple template-based realiser written by Irene
Langkilde-Geary from Brighton University that was
made available to all REG 2008 participants.
A version of the Graph-based algorithm was sub-
mitted for the ASGRE 2007 Challenge (Theune et
al. 2007). For us, one of the most striking, gen-
eral outcomes was the observed ?trend for the mean
DICE score obtained by a system to decrease as the
proportion of minimal descriptions increases? (Belz
and Gatt 2007).2 Thus, while REG systems have
a tendency to produce minimal descriptions, hu-
man speakers tend to include redundant properties in
their descriptions, which is in line with recent find-
ings in psycholinguistics on the production of refer-
ring expressions (e.g., Engelhardt et al 2006).
In principle, the graph-based approach has the po-
tential to deal with redundancy by allowing some at-
tributes to have zero costs. Viethen et al (2008),
however, show that merely assigning zero costs to
an attribute is not a sufficient condition for inclu-
sion; if the search terminates before the free prop-
erties are tried, they will not be included. In other
words: the order in which attributes are tried should
be explicitly controlled as well. In the experiment
we describe here, we consider both these factors and
their interplay.
2DICE (like MASI) is a measure for similarity between a pre-
dicted attribute set and a (human produced) reference set.
227
2 Method
We experimentally combine four cost functions and
two search orders (Table 1). (1) Simple simply as-
signs each edge a 1-point cost. (2) Stochastic asso-
ciates each edge with a frequency-based cost, based
on both the 2008 training and development sets (as-
suming that a larger data set alows for more ac-
curate frequency estimates). (3) Free-Stochastic is
like the previous cost function, except that highly
frequent attributes are assigned 0 costs. For the Fur-
niture domain, this applies to ?colour?; for People
to ?hasBeard = 1? and ?hasGlasses = 1.? (4) Free-
Naive, finally, reduces the relatively fine-grained
costs of Free-Stochastic to three values (0 = free,
1 = cheap, 2 = expensive). In addition, we com-
pare results for two property orderings: (A) Proper-
ties are tried in a Random order. (B) Cost-based,
where properties are tried (in stochastic order) from
cheapest to most expensive. Finally, since human
speakers nearly always include the ?type? property,
we decided to simply always include it. Tables 2 to
4 summarize the evaluation results for all combina-
tions of cost functions and search orders.
3 Attribute Selection Results
The measures used to evaluate attribute selection are
DICE, MASI, attribute accuracy (A-A, the proportion
of times the generated attribute set was identical to
the reference set), and minimality (MIN).
Notice first that the order in which attributes are
tried in the search process matters; the B-systems
nearly always outperform their A-counterparts. Sec-
ond, assigning varying costs also helps; both 1-
variants (Simple costs) perform worse than the sys-
tems building on Stochastic cost functions (2, 3
and 4). Third, adding free properties is also ben-
eficial; the 3 and 4 variants clearly outperform the
1 and 2 variants. It is interesting to observe that
the Free-naive cost function (4) performs equally
well as the more principled Free-stochastic (3), but
only in combination with the Cost-based order (B).
To the extent that it is possible to compare the re-
sults, the submitted GRAPH 4+B outperforms our
best 2007 variant (GRAPH FP in Table 2). This sug-
gests that the interplay between property ordering
and cost function is a flexible and efficient approach
to attribute selection.
Table 1: Overview of cost functions and search orders.
The GRAPH 4+B settings were submitted to the REG
2008 Challenge.
Costs Orders
1 Simple A Random
2 Stochastic B Cost-based
3 Free-stochastic
4 Free-naive
Table 2: Furniture development set results (80 trials).
GRAPH DICE MASI A-A MIN EDIT S-A
1+A .61 .32 .12 .29 5.90 .04
1+B .61 .31 .12 .29 5.89 .04
2+A .71 .47 .31 .11 5.06 .05
2+B .69 .44 .28 .16 5.19 .05
3+A .80 .58 .45 .00 4.90 .05
3+B .80 .58 .45 .00 4.90 .05
4+A .80 .59 .48 .00 4.61 .05
4+B .80 .59 .48 .00 4.61 .05
FP 2007 .71 ? ? ? ? ?
Table 3: People development set results (68 trials).
GRAPH DICE MASI A-A MIN EDIT S-A
1+A .59 .36 .24 .00 6.54 .00
1+B .66 .42 .24 .00 6.78 .00
2+A .66 .42 .24 .00 6.78 .00
2+B .66 .42 .24 .00 6.78 .00
3+A .68 .41 .19 .00 6.79 .00
3+B .72 .48 .28 .00 6.96 .00
4+A .59 .34 .18 .00 6.56 .00
4+B .72 .48 .28 .00 6.96 .00
FP 2007 .67 ? ? ? ? ?
Table 4: Combined Furniture and People development set
results.
GRAPH DICE MASI A-A MIN EDIT S-A
1+A .60 .34 .18 .16 6.20 .02
1+B .63 .36 .18 .16 6.30 .02
2+A .69 .45 .28 .06 5.85 .03
2+B .68 .43 .26 .09 5.92 .03
3+A .74 .51 .33 .00 5.77 .03
3+B .76 .54 .37 .00 5.84 .03
4+A .70 .48 .34 .00 5.51 .03
4+B .76 .54 .39 .00 5.69 .03
FP 2007 .69 ? ? ? ? ?
228
4 Realization Results
To evaluate realisation, the following two word-
string comparison measures were used: string-edit
distance (EDIT), which is the Levenshtein distance
between generated word string and human reference
output, and string accuracy (S-A), which is the pro-
portion of times the word string was identical to the
reference string.
For all settings of the algorithm, we see that S-A
is much lower than A-A. This is as expected, since
any set of attributes can be expressed in many differ-
ent ways, and the chance that the realizer produces
exactly the same string as the human reference is
quite small. For the furniture domain, we see that
S-A has a fairly constant low score, while EDIT fol-
lows the same pattern as A-A: including redundant
(free) properties leads to better results. For the peo-
ple domain, S-A is always 0, and surprisingly EDIT
gets worse as A-A gets better.
To explain these results, we inspect those descrip-
tions where A-A = 1 but S-A = 0, i.e., the attribute
set is identical to the human reference but the word
string is not. In setting 4+B (submitted to REG 2008)
this is the case for 34 furniture and 19 people de-
scriptions. For furniture, we see that the low S-A
score can be largely explained by the fact that in 23
of the 34 descriptions the human reference either in-
cluded no determiner or an indefinite one, whereas
the system always included a definite determiner.
This also explains why S-A hardly improves with
higher A-A scores, since determiner choice is inde-
pendent from attribute selection.
In the people domain, the zero scores for S-A can
be explained by the fact that the realizer always uses
?person? to express the type attribute, where the hu-
man references have either ?man? or ?guy? (in line
with the human preference for basic level values; cf.
Krahmer et al 2003). We also encounter the de-
terminer problem again, aggravated by the fact that
many person descriptions include embedded noun
phrases (e.g., ?man with beard?).
To find out why EDIT gets worse as A-A increases
for different system settings in the people domain,
we look at the six descriptions that have A-A = 1
for setting 4+B but not for 4+A. It turns out that
five of these descriptions are realized as ?the light-
haired person with a beard?, while the human refer-
ence strings are variations of ?the man with a white
beard?, resulting in a relatively high EDIT value. The
problem here is that the link between beard and hair
colour has been lost in the data annotation process.
In general, we can conclude that simply combin-
ing more or less human-like attribute selection with
an off-the-shelf surface realiser is not sufficient to
produce human-like referring expressions.
Acknowledgements We thank the REG 2008 orga-
nizers for making the realiser available, and Hendri
Hondorp for his help with installing and using it.
References
Belz, A. and A. Gatt 2007. The attribute selection for
GRE challenge: Overview and evaluation results Pro-
ceedings of UCNLG+MT 75-83
Engelhardt, P., K. Bailey and F. Ferreira 2006. Do speak-
ers and listeners observe the Gricean Maxim of Quan-
tity? Journal of Memory and Language, 54, 554-573.
Krahmer, E., S. van Erk and A. Verleg 2003. Graph-
based generation of referring expressions. Computa-
tional Linguistics, 29(1), 5372.
Theune, M., P. Touset, J. Viethen, and E. Krahmer. 2007.
Cost-based attribute selection for generating referring
expressions (GRAPH-FP and GRAPH-SC). Proceedings
of the ASGRE Challenge 2007, Copenhagen, Denmark
Viethen, J., R. Dale, E. Krahmer, M. Theune and P. Tou-
set. 2008. Controlling redundancy in referring expres-
sions. Proceedings LREC 08, Marrakech, Morroco.
229
Cross-Linguistic Attribute Selection for REG:
Comparing Dutch and English
Marie?t Theune
University of Twente
The Netherlands
M.Theune@utwente.nl
Ruud Koolen
Tilburg University
The Netherlands
R.M.F.Koolen@uvt.nl
Emiel Krahmer
Tilburg University
The Netherlands
E.J.Krahmer@uvt.nl
Abstract
In this paper we describe a cross-linguistic
experiment in attribute selection for refer-
ring expression generation. We used a
graph-based attribute selection algorithm
that was trained and cross-evaluated on
English and Dutch data. The results indi-
cate that attribute selection can be done in
a largely language independent way.
1 Introduction
A key task in natural language generation is refer-
ring expression generation (REG). Most work on
REG is aimed at producing distinguishing descrip-
tions: descriptions that uniquely characterize a tar-
get object in a visual scene (e.g., ?the red sofa?),
and do not apply to any of the other objects in the
scene (the distractors). The first step in generating
such descriptions is attribute selection: choosing a
number of attributes that uniquely characterize the
target object. In the next step, realization, the se-
lected attributes are expressed in natural language.
Here we focus on the attribute selection step. We
investigate to which extent attribute selection can
be done in a language independent way; that is,
we aim to find out if attribute selection algorithms
trained on data from one language can be success-
fully applied to another language. The languages
we investigate are English and Dutch.
Many REG algorithms require training data, be-
fore they can successfully be applied to generate
references in a particular domain. The Incremen-
tal Algorithm (Dale and Reiter, 1995), for exam-
ple, assumes that certain attributes are more pre-
ferred than others, and it is assumed that determin-
ing the preference order of attributes is an empir-
ical matter that needs to be settled for each new
domain. The graph-based algorithm (Krahmer et
al., 2003), to give a second example, similarly
assumes that certain attributes are preferred (are
?cheaper?) than others, and that data are required
to compute the attribute-cost functions.
Traditional text corpora have been argued to be
of restricted value for REG, since these typically
are not ?semantically transparent? (van Deemter
et al, 2006). Rather what seems to be needed is
data collected from human participants, who pro-
duce referring expressions for specific targets in
settings where all properties of the target and its
distractors are known. Needless to say, collecting
and annotating such data takes a lot of time and ef-
fort. So what to do if one wants to develop a REG
algorithm for a new language? Would this require
a new data collection, or could existing data col-
lected for a different language be used? Clearly,
linguistic realization is language dependent, but to
what extent is attribute selection language depen-
dent? This is the question addressed in this paper.
Below we describe the English and Dutch cor-
pora used in our experiments (Section 2), the
graph-based algorithm we used for attribute se-
lection (Section 3), and the corpus-based attribute
costs and orders used by the algorithm (Section 4).
We present the results of our cross-linguistic at-
tribute selection experiments (Section 5) and end
with a discussion and conclusions (Section 6).
2 Corpora
2.1 English: the TUNA Corpus
For English data, we used the TUNA corpus of
object descriptions (Gatt et al, 2007). This cor-
pus was created by presenting the participants in
an on-line experiment with a visual scene consist-
ing of seven objects and asking them to describe
one of the objects, the target, in such a way that it
could be uniquely identified. There were two ex-
perimental conditions: in the +LOC condition, the
participants were free to describe the target object
using any of its properties, including its location
on the screen, whereas in the -LOC condition they
were discouraged (but not prevented) from men-
tioning object locations. The resulting object de-
scriptions were annotated using XML and com-
bined with an XML representation of the visual
scene, listing all objects and their properties in
terms of attribute-value pairs. The TUNA corpus
is split into two domains: one with descriptions of
furniture and one with descriptions of people.
The TUNA corpus was used for the comparative
evaluation of REG systems in the TUNA Chal-
lenges (2007-2009). For our current experiments,
we used the TUNA 2008 Challenge training and
development sets (Gatt et al, 2008) to train and
evaluate the graph-based algorithm on.
2.2 Dutch: the D-TUNA Corpus
For Dutch, we used the D(utch)-TUNA corpus of
object descriptions (Koolen and Krahmer, 2010).
The collection of this corpus was inspired by the
TUNA experiment described above, and was done
using the same visual scenes. There were three
conditions: text, speech and face-to-face. The
text condition was a replication (in Dutch) of the
TUNA experiment: participants typed identify-
ing descriptions of target referents, distinguishing
them from distractor objects in the scene. In the
other two conditions participants produced spo-
ken descriptions for an addressee, who was either
visible to the speaker (face-to-face condition) or
not (speech condition). The resulting descriptions
were annotated semantically using the XML anno-
tation scheme of the English TUNA corpus.
The procedure in the D-TUNA experiment dif-
fered from that used in the original TUNA exper-
iment in two ways. First, the D-TUNA experi-
ment used a laboratory-based set-up, whereas the
TUNA study was conducted on-line in a relatively
uncontrolled setting. Second, participants in the
D-TUNA experiment were completely prevented
from mentioning object locations.
3 Graph-Based Attribute Selection
For attribute selection, we use the graph-based al-
gorithm of Krahmer et al (2003), one of the
highest scoring attribute selection methods in the
TUNA 2008 Challenge (Gatt et al (2008), table
11). In this approach, a visual scene with tar-
get and distractor objects is represented as a la-
belled directed graph, in which the objects are
modelled as nodes and their properties as looping
edges on the corresponding nodes. To select the
attributes for a distinguishing description, the al-
gorithm searches for a subgraph of the scene graph
that uniquely refers to the target referent. Starting
from the node representing the target, it performs a
depth-first search over the edges connected to the
subgraph found so far. The algorithm?s output is
the cheapest distinguishing subgraph, given a par-
ticular cost function that assigns costs to attributes.
By assigning zero costs to some attributes, e.g.,
the type of an object, the human tendency to men-
tion redundant attributes can be mimicked. How-
ever, as shown by Viethen et al (2008), merely
assigning zero costs to an attribute is not a suffi-
cient condition for inclusion; if the graph search
terminates before the free attributes are tried, they
will not be included. Therefore, the order in which
attributes are tried must be explicitly controlled.
Thus, when using the graph-based algorithm for
attribute selection, two things must be specified:
(1) the cost function, and (2) the order in which the
attributes should be searched. Both can be based
on corpus data, as described in the next section.
4 Costs and Orders
For our experiments, we used the graph-based at-
tribute selection algorithm with two types of cost
functions: Stochastic costs and Free-Na??ve costs.
Both reflect (to a different extent) the relative at-
tribute frequencies found in a training corpus: the
more frequently an attribute occurs in the training
data, the cheaper it is in the cost functions.
Stochastic costs are directly based on the at-
tribute frequencies in the training corpus. They
are derived by rounding ?log2(P (v)) to the first
decimal and multiplying by 10, where P (v) is the
probability that attribute v occurs in a description,
given that the target object actually has this prop-
erty. The probability P (v) is estimated by deter-
mining the frequency of each attribute in the train-
ing corpus, relative to the number of target ob-
jects that possess this attribute. Free-Na??ve costs
more coarsely reflect the corpus frequencies: very
frequent attributes are ?free? (cost 0), somewhat
frequent attributes have cost 1 and infrequent at-
tributes have cost 2. Both types of cost functions
are used in combination with a stochastic ordering,
where attributes are tried in the order of increasing
stochastic costs.
In total, four cost functions were derived from
the English corpus data and four cost functions de-
rived from the Dutch corpus data. For each lan-
guage, we had two Stochastic cost functions (one
for the furniture domain and one for the people do-
main), and two Free-Na??ve cost functions (idem),
giving eight different cost functions in total. For
each language we determined two attribute orders
to be used with the cost functions: one for the fur-
niture domain and one for the people domain.
4.1 English Costs and Order
For English, we used the Stochastic and Free-
Na??ve cost functions and the stochastic order from
Krahmer et al (2008). The Stochastic costs
and order were derived from the attribute frequen-
cies in the combined training and development
sets of the TUNA 2008 Challenge (Gatt et al,
2008), containing 399 items in the furniture do-
main and 342 items in the people domain. The
Free-Na??ve costs are simplified versions of the
stochastic costs. ?Free? attributes are TYPE in
both domains, COLOUR for the furniture domain
and HASBEARD and HASGLASSES for the people
domain. Expensive attributes (cost 2) are X- and
Y-DIMENSION in the furniture domain and HAS-
SUIT, HASSHIRT and HASTIE in the people do-
main. All other attributes have cost 1.
4.2 Dutch Costs and Order
The Dutch Stochastic costs and order were de-
rived from the attribute frequencies in a set of 160
items (for both furniture and people) randomly se-
lected from the text condition in the D-TUNA cor-
pus. Interestingly, our Stochastic cost computa-
tion method led to an assignment of 0 costs to
the COLOUR attribute in the furniture domain, thus
enabling the Dutch Stochastic cost function to in-
clude colour as a redundant property in the gener-
ated descriptions. In the English stochastic costs,
none of the attributes are free. Another difference
is that in the furniture domain, the Dutch stochas-
tic costs for ORIENTATION attributes are much
lower than the English costs (except with value
FRONT); in the people domain, the same holds for
attributes such as HASSUIT and HASTIE. These
cost differences, which are largely reflected in the
Dutch Free-Na??ve costs, do not seem to be caused
by differences in expressibility, i.e., the ease with
which the attributes can be expressed in the two
languages (Koolen et al, 2010); rather, they may
be due to the fact that the human descriptions in D-
TUNA do not include any DIMENSION attributes.
Language Furniture People
Training Test Dice Acc. Dice Acc.
Dutch Dutch 0.92 0.63 0.78 0.28
English 0.83 0.55 0.73 0.29
English Dutch 0.87 0.58 0.75 0.25
English 0.67 0.29 0.67 0.24
Table 1: Evaluation results for stochastic costs.
Language Furniture People
Training Test Dice Acc. Dice Acc.
Dutch Dutch 0.94 0.70 0.78 0.28
English 0.83 0.55 0.73 0.29
English Dutch 0.94 0.70 0.78 0.28
English 0.83 0.55 0.73 0.29
Table 2: Evaluation results for Free-Na??ve costs.
5 Results
All cost functions were applied to both Dutch and
English test data. As Dutch test data, we used a set
of 40 furniture items and a set of 40 people items,
randomly selected from the text condition in the
D-TUNA corpus. These items had not been used
for training the Dutch cost functions. As English
test data, we used a subset of the TUNA 2008 de-
velopment set (Gatt et al, 2008). To make the En-
glish test data comparable to the Dutch ones, we
only included items from the -LOC condition (see
Section 2.1). This resulted in 38 test items for the
furniture domain, and 38 for the people domain.
Tables 1 and 2 show the results of applying the
Dutch and English cost functions (with Dutch and
English attribute orders respectively) to the Dutch
and English test data. The evaluation metrics used,
Dice and Accuracy (Acc.), both evaluate human-
likeness by comparing the automatically selected
attribute sets to those in the human test data. Dice
is a set-comparison metric ranging between 0 and
1, where 1 indicates a perfect match between sets.
Accuracy is the proportion of system outputs that
exactly match the corresponding human data. The
results were computed using the ?teval? evaluation
tool provided to participants in the TUNA 2008
Challenge (Gatt et al, 2008).
To determine significance, we applied repeated
measures analyses of variance (ANOVA) to the
evaluation results, with three within factors: train-
ing language (Dutch or English), cost function
(Stochastic or Free-Na??ve), and domain (furniture
or people), and one between factor representing
test language (Dutch or English).
An overall effect of cost function shows that the
Free-Na??ve cost functions generally perform better
than the Stochastic cost functions (Dice: F(1,76) =
34.853, p < .001; Accuracy: F(1,76) = 13.052, p =
.001). Therefore, in the remainder of this section
we mainly focus on the results for the Free-Na??ve
cost functions (Table 2).
As can be clearly seen in Table 2, Dutch and
English Free-Na??ve cost functions give almost the
same scores in both the furniture and the people
domain, when applied to the same test language.
The English Free-Na??ve cost function performs
slightly better than the Dutch one on the Dutch
people data, but this difference is not significant.
An overall effect of test language shows that the
cost functions (both Stochastic and Free-Na??ve)
generally give better Dice results on the Dutch
data than for the English data (Dice: F(1,76) =
7.797, p = .007). In line with this, a two-way in-
teraction between test language and training lan-
guage (Dice: F(1,76) = 6.870, p = .011) shows that
both the Dutch and the English cost functions per-
form better on the Dutch data than on the English
data. However, the overall effect of test language
did not reach significance for Accuracy, presum-
ably due to the fact that the Accuracy scores on the
English people data are slightly higher than those
on the Dutch people data.
Finally, the cost functions generally perform
better in the furniture domain than in the people
domain (Dice: F(1,76) = 10.877, p = .001; Accu-
racy: F(1,76) = 16.629, p < .001).
6 Discussion
The results of our cross-linguistic attribute selec-
tion experiments show that Free-Na??ve cost func-
tions, which only roughly reflect the attribute fre-
quencies in the training corpus, have an overall
better performance than Stochastic cost functions,
which are directly based on the attribute frequen-
cies. This holds across the two languages we in-
vestigated, and corresponds with the findings of
Krahmer et al (2008), who compared Stochas-
tic and Free-Na??ve functions that were trained and
evaluated on English data only. The difference in
performance is probably due to the fact that Free-
Na??ve costs are less sensitive to the specifics of
the training data (and are therefore more generally
applicable) and do a better job of mimicking the
human tendency towards redundancy.
Moreover, we found that Free-Na??ve cost func-
tions trained on different languages (English or
Dutch) performed equally well when tested on the
same data (English or Dutch), in both the furniture
and people domain. This suggests that attribute
selection can in fact be done in a language inde-
pendent way, using cost functions that have been
derived from corpus data in one language to per-
form attribute selection for another language.
Our results did show an effect of test language
on performance: both English and Dutch cost
functions performed better when tested on the
Dutch D-TUNA data than on the English TUNA
data. However, this difference does not seem to
be caused by language-specific factors but rather
by the quality of the respective test sets. Although
the English test data were restricted to the -LOC
condition, in which using DIMENSION attributes
was discouraged, still more than 25% of the En-
glish test data (both furniture and people) included
one or more DIMENSION attributes, which were
never selected for inclusion by either the English
or the Dutch Free-Na??ve cost functions. The Dutch
test data, on the other hand, did not include any
DIMENSION attributes. In addition, the English
test data contained more non-unique descriptions
of target objects than the Dutch data, in particu-
lar in the furniture domain. These differences may
be due to the fact that data collection was done
in a more controlled setting for D-TUNA than for
TUNA. In other words, the seeming effect of test
language does not contradict our main conclusion
that attribute selection is largely language inde-
pendent, at least for English and Dutch.
The success of our cross-linguistic experiments
may have to do with the fact that English and
Dutch hardly differ in the expressibility of object
attributes (Koolen et al, 2010). To determine the
full extent to which attribute selection can be done
in a language-dependent way, additional experi-
ments with less similar languages are necessary.
Acknowledgements
We thank the TUNA Challenge organizers for the
English data and the evaluation tool used in our
experiments; Martijn Goudbeek for helping with
the statistical analysis; and Pascal Touset, Ivo
Brugman, Jette Viethen, and Iris Hendrickx for
their contributions to the graph-based algorithm.
This research is part of the VICI project ?Bridg-
ing the gap between psycholinguistics and com-
putational linguistics: the case of referring expres-
sions?, funded by the Netherlands Organization for
Scientific Research (NWO Grant 277-70-007).
References
R. Dale and E. Reiter. 1995. Computational interpre-
tation of the Gricean maxims in the generation of re-
ferring expressions. Cognitive Science, 19(2):233?
263.
A. Gatt, I. van der Sluis, and K. van Deemter. 2007.
Evaluating algorithms for the generation of refer-
ring expressions using a balanced corpus. In Pro-
ceedings of the 11th European Workshop on Natural
Language Generation (ENLG 2007), pages 49?56.
A. Gatt, A. Belz, and E. Kow. 2008. The TUNA Chal-
lenge 2008: Overview and evaluation results. In
Proceedings of the 5th International Natural Lan-
guage Generation Conference (INLG 2008), pages
198?206.
R. Koolen and E. Krahmer. 2010. The D-TUNA cor-
pus: A Dutch dataset for the evaluation of referring
expression generation algorithms. In Proceedings
of the 7th international conference on Language Re-
sources and Evaluation (LREC 2010).
R. Koolen, A. Gatt, M. Goudbeek, and E. Krahmer.
2010. Overspecification in referring expressions:
Causal factors and language differences. Submitted.
E. Krahmer, S. van Erk, and A. Verleg. 2003. Graph-
based generation of referring expressions. Compu-
tational Linguistics, 29(1):53?72.
E. Krahmer, M. Theune, J. Viethen, and I. Hendrickx.
2008. Graph: The costs of redundancy in refer-
ring expressions. In Proceedings of the 5th Inter-
national Natural Language Generation Conference
(INLG 2008), pages 227?229.
K. van Deemter, I. I. van der Sluis, and A. Gatt. 2006.
Building a semantically transparent corpus for the
generation of referring expressions. In Proceedings
of the 4th International Natural Language Genera-
tion Conference (INLG 2006), pages 130?132.
J. Viethen, R. Dale, E. Krahmer, M. Theune, and
P. Touset. 2008. Controlling redundancy in refer-
ring expressions. In Proceedings of the Sixth In-
ternational Conference on Language Resources and
Evaluation (LREC 2008), pages 239?246.
Proceedings of the Sixth Workshop on Innovative Use of NLP for Building Educational Applications, pages 20?29,
Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguistics
Generating Varied Narrative Probability Exercises
Marie?t Theune1 Roan Boer Rookhuiszen1 Rieks op den Akker1 Hanneke Geerlings2
Department of Computer Science1
Department of Research Methodology, Measurement and Data Analysis2
University of Twente
Enschede, The Netherlands
m.theune@utwente.nl, a.r.boerrookhuiszen@alumnus.utwente.nl,
h.j.a.opdenakker@utwente.nl, h.geerlings@gw.utwente.nl
Abstract
This paper presents Genpex, a system for au-
tomatic generation of narrative probability ex-
ercises. Generation of exercises in Genpex is
done in two steps. First, the system creates
a specification of a solvable probability prob-
lem, based on input from the user (a researcher
or test developer) who selects a specific ques-
tion type and a narrative context for the prob-
lem. Then, a text expressing the probability
problem is generated. The user can tune the
generated text by setting the values of some
linguistic variation parameters. By varying
the mathematical content of the exercise, its
narrative context and the linguistic parameter
settings, many different exercises can be pro-
duced. Here we focus on the natural language
generation part of Genpex. After describing
how the system works, we briefly present our
first evaluation results, and discuss some as-
pects requiring further investigation.
1 Introduction
Narrative exercises (also called word problems or
story problems) are mathematical exercises embed-
ded in a story or text. They are commonly used as
test items, to assess or train a student?s understand-
ing of the underlying mathematical concepts. When
solving a narrative exercise, the student is required
to derive the underlying mathematical question from
the story and to calculate the correct answer to this
mathematical problem.
This paper presents Genpex, a system for generat-
ing narrative exercises expressing probability prob-
lems. Genpex was created in the context of an inter-
national project on item generation for testing stu-
dent competencies in solving probability problems.
Automatic item generation is an effective way of
constructing many items with controlled difficulties,
based on a set of predefined task parameters (Enright
et al, 2002; Deane and Sheehan, 2003; Arendasy et
al., 2006; Holling et al, 2009). The goal of our item
generation project is to develop a model to support
optimal problem and test construction. A large col-
lection of narrative exercises is needed to test the de-
veloped models in field trials. All of these narrative
exercises should be different, but the properties that
define the difficulty of the exercise should be known.
Genpex was designed to enable easy creation of new
exercises meeting these requirements.
Figure 1 shows a narrative probability exercise
generated by Genpex. The text of the exercise is in
German, because the target group of our project are
German high school students. The texts produced
by Genpex are based on a set of example narrative
exercises that were created earlier within the project
(Zeuch, In preparation).
A property that sets Genpex apart from other
narrative exercise generation systems is that it was
specifically designed to support variation in the gen-
erated exercises. Unlike other systems, it not only
changes the context of the narrative exercise (e.g.,
instead of bikes, the example exercise could also
have been about hotel rooms with different proper-
ties) but it also varies the way the texts are formu-
lated. Most existing systems for narrative exercise
generation use fixed sentence templates to express
mathematical content, which means that the same
content is always expressed in the same way (Fa-
20
In einer gro?en Halle ist eine Mischung von Fahrra?dern. In a big hall there are a variety of bicycles.
Es gibt insgesamt 100 Fahrra?der. There are 100 bicycles in total.
Es gibt 30 gru?ne Fahrra?der und es gibt 70 wei?e. 40
Fahrra?der sind Mountainbikes, 50 sind Rennra?der und
es gibt 10 Hollandra?der. 70 Fahrra?der sind billiger als
500 Euro und 30 Fahrra?der teurer als 500 Euro. 41
Fahrra?der sind billiger als 500 Euro und sind Rennra?der.
There are 30 green bicycles and there are 70 white ones.
40 bicycles are mountain bikes, 50 are road bikes, and
there are 10 Dutch bikes. 70 bicycles are less expensive
than 500 Euros and 30 bicycles more expensive than 500
Euros. 41 bicycles are less expensive than 500 Euros and
are road bikes.
Fahrradtyp und Preis sind abha?ngig voneinander und
alle anderen Merkmale sind unabha?ngig voneinander.
Bicycle type and price are dependent on each other and
all other properties are independent of each other.
Wie gro? ist die Wahrscheinlichkeit, dass ein Fahrrad
nicht sowohl ein Mountainrad als auch gru?n ist?
What is the probability that a bicycle is not both a moun-
tain bike and green?
Wie gro? ist die Wahrscheinlichkeit, dass ein Fahrrad
entweder billiger als 500 Euro oder ein Rennrad ist?
What is the probability that a bicycle is either cheaper
than 500 Euros or a road bike?
Figure 1: The text of an exercise generated by Genpex. (Left: German original, right: English translation.)
iron and Williamson, 2002; Arendasy et al, 2006;
Holling et al, 2009). A system that uses a linguisti-
cally sophisticated approach, thus in principle allow-
ing for similar text variations as Genpex, is Model-
Creator (Deane and Sheehan, 2003; Higgins et al,
2005). However, this system focuses on semantic
factors influencing the expression of events with dif-
ferent participants (e.g., different types of vehicles)
rather than on generating linguistic variations.
Below, we first describe how a probability prob-
lem is constructed by Genpex, based on input by the
user. Then we explain in some detail how the nat-
ural language generation (NLG) module of Genpex
creates a text expressing the probability problem, fo-
cusing on the creation of variation in the generated
texts. We end with a brief discussion of our first
evaluation results and some pointers to future work.
2 Probability Problems
Figure 2 presents the probability problem underly-
ing the narrative exercise of Figure 1. It specifies the
context, the total number of entities (numEntities),
and the distribution of (combinations of) attribute
values over the entities. Number information may be
suppressed so as not to give the answer away; this is
done by inserting a question mark in the place of the
number (e.g., colour[green] = ?). Explicitly listing
such ?hidden? information in the probability prob-
lem ensures that all possible values of each attribute
are mentioned in the text of the exercise. A basic as-
sumption in creating the probability problems is that
all entities have exactly one value for each attribute.
For example, all bikes must have some colour, and
they cannot have two colours at the same time.
In addition to the number statements, the proba-
bility problem also lists which pairs of attributes are
dependent on each other. In the example, these are
type and price. This means that if we look at the
subset of bikes of a specific type, the probability that
one of these bikes has a certain price is not the same
as when we look at the entire collection of bikes (and
vice versa). If a pair of attributes is not specified as
being dependent, it is independent.
Q delineates the question part of the probability
problem; we refer to the other parts (except Con-
text) as ?statements?. All questions require the cal-
culation of a probability. A question of the form Q:
P(A) asks for the probability of event A, which can
be described as ?Someone randomly draws one en-
tity out of a (sub)set of entities and this entity has
property A?. For example, the question could be to
calculate the probability that a bike is black if we
randomly pick one bike from the set of all bikes. We
equate the probability of event A with the relative
frequency of the set A of objects that satisfy prop-
erty A, computed as |A|/|U |, where U is the set of
all entities (that is, |U | = numEntities). In general,
the set we draw from is the entire set of entities, but
this set can be limited by a conditional statement:
the event A|B can be described as ?Someone ran-
domly draws one entity with property A from a sub-
set of entities that have property B?. In this case, the
21
Context: bikes
numEntities: 100
colour[green] = 30
colour[white] = 70
type[mountainbike] = 40
type[sportsbike] = 50
type[hollandbike] = 10
price[<500] = 70
price[>500] = 30
price[<500] ? type[sportsbike] = 41
dependentAttributes: price & type
Q: P(?(type[mountainbike] ? colour[green]))
Q: P(price[<500] ? type[sportsbike])
Figure 2: The probability problem underlying Figure 1.
probability P (A|B) is computed as |A?B|/|B|. All
events involve a single draw of exactly one entity.
Probability problems such as the one in Figure 2
are automatically created by Genpex; the only thing
the user has to do is to select one or more question
types (defining the difficulty of the exercise) and a
context for the exercise. All available question types
are of the form P(A) or P(A|B), where A (but not B)
can be a complex event, i.e., involving a conjunc-
tion or disjunction of properties. For example, Q:
P(A ? B) asks for the probability that an entity has
both property A and property B. Moreover, parts of
a question can be negated.
Currently, Genpex can handle 25 different ques-
tion types. Some restrictions we put on the avail-
able questions are the following. Each question in-
volves at most two different attributes, to avoid com-
plex dependencies. There are no recursive questions
(e.g., double negations) and no conditional questions
about independent attributes. Finally, we exclude
questions that are likely to result in ambiguous lan-
guage. For example, if we try to express the ques-
tion Q: P(? (colour[white]) ? type[sportsbike]) in
English, it will be something like ?What is the prob-
ability that a bike is not white and a road bike??.
Due to scope ambiguity of the negation, this sen-
tence may be misinterpreted as ?What is the prob-
ability that a bike is not white and also not a road
bike??. The same ambiguity is found in the Ger-
man sentence expressing this question.1 Excluding
1Genpex does include a re-ordered, mathematically equi-
these types of questions does not simplify the task
for Genpex; the excluded questions are not more dif-
ficult to generate than the included ones. The main
reason to exclude certain question types was to avoid
creating exercises that might be unclear to the reader.
In addition to selecting one or more question
types as input for Genpex, the user also selects a
context for the exercise. As a resource, Genpex uses
a repository of context files2 with information con-
cerning the entities that the exercise should be about
(?bikes? in our example) and the properties they may
have. Each attribute in the context file is linked to
a lexical lemma for the word that expresses its rela-
tion to the entity (e.g., bikes are of a certain colour
or type but have a certain price). Similarly, for each
attribute, a list of possible attribute values and the
words expressing them is provided. For example,
the type attribute in the bikes context can have the
values ?mountainbike?, ?sportsbike?, ?hollandbike?
and ?seniorbike?, respectively associated with the
words ?Mountainbike? (mountain bike), ?Rennrad?
(road bike), ?Hollandrad? (Dutch bike) and ?Se-
niorenrad? (senior bike). Other NLG-related infor-
mation in the context files is discussed in Section 3.
The context file also specifies world knowledge such
as the range of numEntities (a context about rooms
in a hotel will involve fewer entities than a context
about books in a bookshop) and possible dependen-
cies between attributes (in the bikes context, price is
more likely to be dependent on type than on colour).
Taking the selected question type(s) and context
as input, Genpex automatically constructs a proba-
bility problem. This involves selecting a number of
attributes and values, depending on the question or
questions that need to be answered, and creating a
correct and complete world: an internal represen-
tation of the situation in which all entities are fully
defined (all their properties are known), and there
are no inconsistencies. A part of this world is re-
flected in the statements of the probability problem.
Currently, all statements provide information that is
valent version of the same question: Q: P(type[sportsbike] ?
? (colour[white])). Because the generated questions follow the
order of the attributes in the question specification, this version
can be expressed without ambiguity as ?What is the probability
that a bike is a road bike and not white??
2In the current Genpex prototype, five different contexts are
available. The system comes with an editor for the creation of
new context files.
22
required to solve the exercise; redundant informa-
tion is not included. If the user manually edits the
generated probability problem, Genpex reconstructs
the world, and tries to solve the exercise using the in-
formation in the edited problem. The user is warned
in case of inconsistencies or missing information. A
warning is also issued if the edited problem contains
properties for which no lexical information is avail-
able. See Boer Rookhuiszen (2011) for more details
on how probability problems are constructed.
3 Language Generation
The NLG process of Genpex has two goals: generat-
ing a correct textual representation of a given prob-
ability problem, and enabling variation, so that mul-
tiple runs will result in different texts. The gener-
ated texts should be in grammatically correct Ger-
man, and they must be unambiguous: the formula-
tion of the text should not leave the reader uncertain
about the underlying mathematical exercise.
An overview of the NLG component of Genpex
is given in Figure 3. Its architecture reflects the lan-
guage generation pipeline of Reiter and Dale (2000),
with three modules: Document Planner, Microplan-
ner and Surface Realizer. Information between the
modules is exchanged in the form of a list of sen-
tence trees, each defining the content and grammat-
ical structure of a sentence. The Document Planner
creates basic sentence trees. These are manipulated
by the Microplanner to create variations. The mi-
croplanning stage can in principle be skipped, but
that will result in very monotonous texts. Finally,
the Surface Realizer applies the correct morphology
to the sentence trees and creates the layout of the
text. Below, we discuss each module in turn.
3.1 Document Planning: Creating Basic
Sentence Structures
The input of the Document Planner is a probability
problem, which defines the content and the structure
of the narrative exercise. The output is a document
plan: a structured list of sentence trees expressing
the statements and questions in the probability prob-
lem. The document plan also includes an introduc-
tion: a simple ?canned? text specified in the context
file. If multiple introduction texts are available, one
is randomly selected.
Figure 3: The NLG module of Genpex.
The sentences included in the document plan are
all very simple, with the same basic structure. Take
for example the statement colour[white] = 70. The
Document Planner first creates a subject NP ex-
pressing the number of entities involved, e.g., ?70
Fahrra?der? (70 bicycles). Then it creates a VP ex-
pressing the relation and the attribute value, e.g.,
?sind wei?? (are white). The relevant words and
their parts of speech are looked up in the context
file. For the example statement, this process results
in the following basic tree, shown in a simplified no-
tation. Note that the words in the tree have not yet
been inflected.
[s]
[np grammaticalRole=su]
[det grammaticalRole=num]70[/det]
[noun grammaticalRole=hd]Fahrrad[/noun]
[/np]
[vp]
[verb grammaticalRole=hd]sind[/verb]
[adj grammaticalRole=predc]weiss[/adj]
[/vp]
[/s]
All sentence trees for questions start with the
phrase ?Wie gro? ist die Wahrscheinlichkeit dass?
(What is the probability that), included as canned
text in a tree node with syntactic category ?clause?.
23
This main clause is followed by an indefinite NP re-
ferring to the type of entities discussed in the exer-
cise, e.g., ?ein Fahrrad? (a bicycle). The structure
of the rest of the sentence tree depends on the ques-
tion type. Sentence tree templates are available for
all possible question types. They can be used recur-
sively: slots in the templates can be filled with an
expression for an attribute value, or with one of the
other templates.
Figure 4 shows the construction of a sentence tree
for a fairly complex question of type P(A ? B |
? C), using multiple question templates. For ques-
tions about conditional probabilities Genpex uses
the slightly formal ?vorausgesetzt? (given that), be-
cause simpler phrasings are likely to be ambiguous.
For example, assume we want to ask the question
Q: P(type[mountainbike] | colour[green]). A sim-
ple way to ask this question would be ?Wie gro?
ist die Wahrscheinlichkeit dass ein gru?nes Fahrrad
ein Mountainbike ist?? (What is the probability that
a green bike is a mountain bike?). However, such
a question could be mistakenly interpreted as ask-
ing for a joint probability: Q: P(colour[green] ?
type[mountainbike]). For this reason, the more com-
plex formulation is preferred.
3.2 Microplanning: Creating Variation
The Microplanner modifies the sentence trees pro-
duced by the Document Planner by applying a num-
ber of variation techniques. These techniques
place specific requirements on the sentences to
which they can be applied, and therefore not every
technique can be applied to all sentence trees.
When introducing variation in the narrative form
of the exercise, it is important that variations of the
same exercise should all have the same meaning
and approximately the same difficulty. According to
Deane and Sheehan (2003), it is possible to change
the wording of a text without changing its difficulty.
Reiter and Dale (2000) state that for example ag-
gregating multiple sentences does not change the in-
formation they express, but improves the readabil-
ity and fluency of the text. This is what we want
to achieve: adding variation to the text without af-
fecting its interpretation. Genpex therefore uses
aggregation as well as a number of text variation
techniques, assuming that they do not influence the
meaning or difficulty of an exercise.
Figure 4: Construction of a question combining multiple
templates. Translation, with brackets marking the tem-
plate boundaries: ?What is the probability that a bicycle
[[is either black or white] given that this bicycle [is not a
mountainbike]]??
Below we discuss the operations applied to basic
sentence trees in the Microplanner. They are only
applied to sentences expressing statements, even
though it would be practically possible to apply
some of the variations to the questions too. Given
that understanding the question is crucial for solv-
ing the exercise, and that varying the way the ques-
tions are asked might cause confusion, we chose to
adhere to a fixed format for the questions, cf. Fairon
and Williams (2002).
Aggregation. As a first step, the Microplanner
applies aggregation: grouping multiple simple sen-
tences and combining them into one complex sen-
tence. This process leaves the original order of the
sentences in the Document Plan intact. Sentences
referring to different attributes are never grouped to-
gether, to avoid possible misinterpretations. For ex-
ample, a complex sentence such as ?70 bicycles are
white and 40 bicycles are mountain bikes? might
suggest that the 40 mountain bikes are different en-
tities than the 70 white bikes, excluding the possibil-
ity of white mountain bikes. Since this is not the in-
tended meaning, we avoid creating this kind of com-
plex sentences. Sentences referring to the same at-
tribute can be grouped together without risk, because
there can never be any overlap between the sets of
entities mentioned in these sentences (an entity can-
not have multiple values for the same attribute).
Aggregation is performed on a maximum of three
sentences to prevent the generation of overly large
conjunctions. Groups of four basic sentences are ag-
24
gregated into two new complex sentences. This way
we avoid creating unbalanced texts like example 1
below, preferring to generate sentences that are sim-
ilar in both length and complexity, as in example 2.
1. 42 Fahrra?der sind Mountainbikes, 168 Fahrra?der
sind Rennra?der und 200 Fahrra?der sind Hol-
landra?der. 10 Fahrra?der sind Seniorenra?der.
(42 bicycles are mountain bikes, 168 bicycles are
road bikes, and 200 bicycles are Dutch bikes. 10
bicycles are senior bikes.)
2. 42 Fahrra?der sind Mountainbikes und 168 Fahrra?der
sind Rennra?der. 200 Fahrra?der sind Hollandra?der
und 10 Fahrra?der sind Seniorenra?der.
(42 bicycles are mountain bikes and 168 bicycles
are road bikes. 200 bicycles are Dutch bikes and 10
bicycles are senior bikes.)
Aggregation in Genpex is not optional; it is al-
ways applied under the assumption that this will
make the generated texts more coherent and pleas-
ant to read. Moreover, it enables variation through
ellipsis, as discussed later in this section. Variations
in aggregation can be achieved by manually reorder-
ing the statements in the probability problem. This
will lead to a different Document Plan and as a con-
sequence, to different aggregations, within the re-
strictions stated above.
Adjectivication. The text variation technique we
call ?adjectivication? changes the position and gram-
matical role of the adjective (if any) expressing the
attribute value in a sentence. In basic sentence trees,
attribute values expressed by adjectives are included
as predicative complements in the VP. If we apply
adjectivication to a sentence, the adjective is instead
added as a modifier to the subject NP, and the orig-
inal verb is removed. To make the sentence tree
complete again, the words ?Es gibt? (There are?)
are added in front. For example, the sentence ?30
Fahrra?der sind gru?n? (30 bicycles are green) will be
changed to ?Es gibt 30 gru?ne Fahrra?der? (There are
30 green bikes). In German, adjectivication may
cause the inflection of the adjective to change, be-
cause it gets a different grammatical role: when used
as a modifier its inflection reflects the gender and
case of the noun it modifies. This is taken care of by
the Surface Realizer.
Entity substitution. In case an attribute value
is expressed as a noun, e.g., ?Rennrad? (road bike)
the text variation technique we call ?entity substitu-
tion? can be applied. It involves replacing the noun
that represents the entity in a basic sentence with the
noun that represents the attribute value. As with ad-
jectivication, the original verb is removed and in-
stead ?Es gibt? (There are) is added to the sentence.
For example, entity substitution changes the basic
sentence ?50 Fahrra?der sind Rennra?der? (50 bicycles
are road bikes) to ?Es gibt 50 Rennra?der? (There are
50 road bikes).
Marked word order. Another source of variation
is topicalizing the phrase expressing the attribute
value by moving it to the front of the sentence.
Applying this variation technique changes the ba-
sic sentence ?30 Fahrra?der sind teurer als 500 Euro?
(30 bicycles are more expensive than 500 Euros) to
?Teurer als 500 Euro sind 30 Fahrra?der? (More ex-
pensive than 500 Euros are 30 bicycles). Since using
such a marked word order may come across as un-
natural in a neutral discourse context, this type of
variation should be applied with caution.
Ellipsis. This is the removal of duplicate words
from sentences, which typically applies to aggre-
gated sentences (Harbusch and Kempen, 2009).
Genpex can apply different types of ellipsis, such as
Gapping and Conjunction Reduction. Gapping is the
removal of all except the first verb in an aggregated
sentence. An example from Figure 1 is the sen-
tence ?70 Fahrra?der sind billiger als 500 Euro und
30 Fahrra?der teurer als 500 Euro? (70 bicycles are
less expensive than 500 Euros and 30 bicycles more
expensive than 500 Euros), where the verb ?sind?
(are) has been deleted from the second clause. (For-
ward) Conjunction Reduction deletes the subject of
subsequent clauses if it is identical to the subject of
the first clause. The following sentence is an exam-
ple: ?40 Fahrra?der sind Mountainbikes und 50 sind
Rennra?der? (40 bicycles are mountain bikes and 50
are road bikes). It is possible to combine Gapping
and Conjunction Reduction, e.g., ?40 Fahrra?der sind
Mountainbikes und 50 Rennra?der? (40 bicycles are
mountain bikes and 50 road bikes).
Ellipsis is also possible in sentences with marked
word order. For example, ?Gru?n sind 30 Fahrra?der
und wei? sind 40 Fahrra?der?(Green are 30 bicycles
and white are 40 bicycles) could be reduced to
?Gru?n sind 30 Fahrra?der und wei? sind 40? (Green
25
are 30 bicycles and white are 40). However, in
this sentence, the verb cannot be removed from the
last clause. Genpex currently allows aggregated
sentences in which some of the clauses have
marked word order. In these cases, ellipsis is
not applied, because it will most likely result in
grammatically incorrect sentences. For example, in
the sentence ?30 Fahrra?der sind gru?n und Wei? sind
40 Fahrra?der? (30 bicycles are green and white are
40 bicycles) the system will not apply ellipsis.
For every sentence in the document plan, the sys-
tem will check which of the variation techniques de-
scribed above can be applied to it, by analyzing the
structure of the sentence tree. If a technique is in
principle applicable, the probability of it being ac-
tually applied depends on information in the context
file, and on parameters set by the user through the
GUI of Genpex. For every attribute in the context
file, the author of the file can prevent Genpex from
applying a specific technique by giving it a probabil-
ity of 0, if it is never suitable in the case of this spe-
cific attribute. For example, applying marked word
order to a sentence expressing the ?type? attribute in
the bikes context would lead to odd sentences such
as ?Mountainbikes sind 40 Fahrra?der? (Mountain-
bikes are 40 bicycles). Though grammatically cor-
rect, such sentences would be hard to interpret and
therefore are best avoided.
During generation, the user can directly influ-
ence the probability that certain variations are ap-
plied through sliders in the GUI; see Figure 5. The
probability holds for every sentence that satisfies the
structural requirements of the variation technique
in question, unless the technique is excluded based
on the information in the context file, as explained
above. After having set the variation probabilities,
the user can click ?Update Text? to see the effect.
The user can also choose to have the text automati-
cally updated every time a slider is moved.
When the user saves a generated exercise, infor-
mation about the variation techniques that have been
applied is logged and saved together with the exer-
cise. If further research shows that a certain varia-
tion technique has an unintended influence on exer-
cise difficulty, it will be easy to exclude this tech-
nique from the creation of new exercises by setting
its probability to 0 in the GUI.
3.3 Surface Realisation: the Final Polish
The main task of the Surface Realizer is to convert
the sentence trees that have been manipulated by the
Microplanner to actual text, applying correct mor-
phology and orthography.
Information about German morphology is re-
trieved from a lexicon listing the possible word
forms of each lemma in the context files. German
has a rich inflectional system compared to English,
with suffixes reflecting the gender, number and case
of determiners, adjectives and nouns. Gender can be
masculine, feminine or neuter, number is singular or
plural, and case is nominative, accusative, dative or
genitive. In the type of exercises currently generated
by Genpex, all words are in nominative case. Num-
ber information for nouns and verbs is given in the
sentence tree, while the inflection of determiners and
adjectives in an NP depends on the properties of the
noun. For the inflection of adjectives, Genpex also
has to consider the determiner that is used before
the adjective. In German, so-called ?strong inflec-
tion? has to be used after a cardinal number, ?weak
inflection? after a definite determiner and ?mixed in-
flection? after an indefinite determiner. We currently
use canoonet3 as the source for German morpholog-
ical information in Genpex.
Orthography is the process that converts the sen-
tence trees to text. This is quite easy, because the
word order is already defined by the tree structure.
All values of the nodes in the tree can be joined
together in a sentence in that order, separated by
white spaces. The clauses in aggregated sentences
are joined by a comma, except for the last conjunc-
tion where the word ?und? is used. A characteristic
of German is that all nouns are capitalized. The Sur-
face Realiser takes care of this, and also of the cap-
italization of the first word in each sentence, punc-
tuation and the placement of paragraph boundaries.
The generated texts are marked up with HTML for
easy display in web browsers.
4 Evaluation
Potential users of Genpex (researchers working on
test design) have been involved at different stages
of development of the system, such as requirements
specification and usability testing. Field trials with
3http://www.canoo.net/
26
Figure 5: Screenshot of the GUI of Genpex, showing a variation of the narrative exercise in Figure 1. The introductory
text was taken from Zeuch (In preparation).
students are future work, but we did carry out some
preliminary, qualitative evaluations with a few na-
tive speakers of German (including one item gen-
eration expert) to test the grammaticality and un-
derstandability of the generated exercises. This re-
vealed some small mistakes that have since been cor-
rected, but also a few bigger problems with some of
the variation techniques and other NLG aspects.
One of the things noted by the native speakers was
that applying ellipsis sometimes leads to slightly un-
natural sentences. The preferred type and degree of
ellipsis is different for each type of sentence, but this
is not taken into account by Genpex. As a conse-
quence, the system frequently applies too much or
too little ellipsis to the generated sentences, with less
than ideal (though not ungrammatical) results. The
existence of such preferred formulations is in line
with the results of Cahill and Forst (2010), who car-
ried out an experiment in which native speakers of
German evaluated a number of alternative realisa-
tions of the same sentence. Their subjects accepted
some variation in word order, but showed a clear
preference for some of the alternatives.
Some of the generated question sentences also
sounded a bit forced to the native speakers. For ex-
ample, the question template for joint probabilities
(A?B) uses the formal phrasing ?sowohl... als auch?
(both ... and), whereas a simple ?und? (and) would
be the more natural choice for most questions. How-
ever, in some question contexts, in particular those
involving negations, using the simpler formulation
might lead to the kind of scope ambiguities men-
tioned in Section 2. Therefore, the choice was made
to use ?sowohl... als auch? in all cases, even in those
where it is not strictly necessary. Similarly, ques-
tions asking for a conditional probability were found
to be somewhat difficult to understand. For these
questions, readability might be improved by using
27
two sentences to express them, along the lines of
?Consider the set of bicycles that are not mountain
bikes. What is the probability that one of those bi-
cycles is either black or white?? as an alternative to
the more complex formulation given in Figure 4.
The comments by the native speakers suggest that
in some cases, Genpex goes too far in its ?one size
fits all? approach, and that we should try to add
more flexibility to the NLG component, allowing it
to make finer distinctions in the application of varia-
tion techniques to specific sentences and of question
templates to specific question types.
5 Discussion
The texts currently being generated by Genpex are
grammatical, but our native speakers reported that
some sentences had to be studied carefully before it
was possible to get the information needed to solve
the problem. No actual misinterpretations occurred,
but the increased reading time (as compared to more
preferred formulations) may still increase the diffi-
culty of the exercise. A thorough investigation into
the effect of textual variations on item difficulty is
therefore necessary. Genpex supports this type of
research by enabling the systematic application of
different variations, while logging all textual oper-
ations that have been applied and saving them to-
gether with the generated text. The underlying prob-
ability problem is saved together with the text as
well, so all factors that certainly or potentially in-
fluence item difficulty are known. This makes it rel-
atively easy to test the influence of those factors on
the difficulty of the exercise, for example by carry-
ing out the kind of statistical and cognitive analysis
advocated by Graf et al (2005).
The effect of the main parameters of the proba-
bility problems in Genpex (i.e., the type of question
being asked) was already statistically analyzed by
Holling et al (2009) and Zeuch (In preparation).
They used automatically generated items similar to
the exercises generated by Genpex, except that their
exercises did not have variations in wording apart
from context-related ones. Also, the exercises used
by Holling et al (2009) mentioned probabilities in-
stead of counts in the statements.
Once we know more about the effects of the tex-
tual variations, Genpex can be of great value to test
developers, given that there exists a great need for
large amounts of learning and assessment materi-
als with a controlled level of difficulty (Enright et
al., 2002; Fairon and Williamson, 2002; Deane and
Sheehan, 2003; Arendasy et al, 2006; Holling et al,
2008; Holling et al, 2009). The initial development
and testing of the system is a one-time investment,
which we expect will pay off afterward when large
amounts of test items can be created with little effort.
In particular, we think Genpex can be very useful
in combination with Computerized Adaptive Test-
ing (CAT). The system could be used for on-the-fly
generation of new items for each individual student,
adapted to that student?s skill level estimated from
his or her previous answers. Because every student
gets custom exercises, the risk of frequently used
items becoming known among students is reduced,
thus increasing test security.
In principle, given that the factors influencing
item difficulty are known, generating difficult items
is not more complicated than generating easy ones.
However, as illustrated in Section 2, combining mul-
tiple difficulty factors such as negation and joint
probability may lead to textual formulations that are
ambiguous or hard to understand, and which ? if not
successfully prevented in advance ? may need to be
filtered out or corrected by hand. For that reason,
Genpex seems most suitable for the generation of
exercises up to a moderate level of complexity. Still,
even if the need for hand-crafting will not be com-
pletely eliminated, reducing it to complex items that
require particularly careful wording already repre-
sents a big gain in efficiency.
Acknowledgements
We thank our anonymous reviewers and everybody
who helped testing Genpex. Special thanks go
to our colleagues from the University of Mu?nster
for providing the reference exercises on which
the Genpex output was modelled. This research
was funded by the Deutsche Forschungsgemein-
schaft (DFG), Schwerpunktprogramm ?Kompetenz-
modelle zur Erfassung individueller Lernergebnisse
und zur Bilanzierung von Bildungsprozessen? (SPP
1293), project ?Rule-based Item Generation of Al-
gebra Word Problems Based upon Linear Logistic
Test Models for Item Cloning and Optimal Design?.
28
References
Martin Arendasy, Markus Sommer, Georg Gittler, and
Andreas Hergovich. 2006. Automatic generation of
quantitative reasoning items: A pilot study. Journal of
Individual Differences, 27(1):2?14.
Roan Boer Rookhuiszen. 2011. Generation of German
narrative probability excercises. Master?s thesis, Uni-
versity of Twente.
Aiofe Cahill and Martin Forst. 2010. Human evaluation
of a German surface realisation ranker. In E. Krahmer
and M. Theune, editors, Empirical Methods in Natural
Language Generation, volume 5790 of Lecture Notes
in Computer Science, pages 201?221. Springer, Berlin
/ Heidelberg.
Paul Deane and Kathleen Sheehan. 2003. Automatic
item generation via frame semantics: Natural language
generation of math word problems. Educational Test-
ing Service, Princeton. Paper presented at the Annual
Meeting of the National Council on Measurement in
Education (Chicago, IL, April 22-24, 2003).
Mary K. Enright, Mary Morley, and Kathleen M. Shee-
han. 2002. Items by design: The impact of systematic
feature variation on item statistical characteristics. Ap-
plied Measurement in Education, 15(1):49?74.
Ce?drick Fairon and David M. Williamson. 2002. Auto-
matic item text generation in educational assessment.
In Proceedings of TALN 2002, pages 395?401.
Edith Aurora Graf, Stephen Peterson, Manfred Steffen,
and Rene? Lawless. 2005. Psychometric and cogni-
tive analysis as a basis for the design and revision of
quantitative item models. Technical Report RR-05-25,
Educational Testing Service, Princeton.
Karin Harbusch and Gerard Kempen. 2009. Generating
clausal coordinate ellipsis multilingually: A uniform
approach based on postediting. In Proceedings of the
12th European Workshop on Natural Language Gen-
eration, pages 138?145.
Derrick Higgins, Yoko Futagi, and Paul Deane. 2005.
Multilingual generalization of the ModelCreator soft-
ware for math item generation. Technical Report RR-
05-02, Educational Testing Service, Princeton.
Heinz Holling, Helen Blank, Karoline Kuchenba?cker,
and Jo?rg-Tobias Kuhn. 2008. Rule-based item design
of statistical word problems: A review and first imple-
mentation. Psychology Science Quarterly, 50(3):363?
378.
Heinz Holling, Jonas P. Bertling, and Nina Zeuch. 2009.
Automatic item generation of probability word prob-
lems. Studies In Educational Evaluation, 35(2-3):71?
76.
Ehud Reiter and Robert Dale. 2000. Building Natural
Language Generation Systems. Cambridge University
Press, Cambridge, UK.
Nina Zeuch. In preparation. Rule-based item construc-
tion: Analysis with and comparison of linear logistic
test models and cognitive diagnostic models with two
item types. Ph.D. thesis, University of Mu?nster.
29
Proceedings of the 7th Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities, pages 65?73,
Sofia, Bulgaria, August 8 2013. c?2013 Association for Computational Linguistics
Learning to Extract Folktale Keywords
Dolf Trieschnigg, Dong Nguyen and Marie?t Theune
University of Twente
Enschede, The Netherlands
{d.trieschnigg,d.nguyen,m.theune}@utwente.nl
Abstract
Manually assigned keywords provide a
valuable means for accessing large docu-
ment collections. They can serve as a shal-
low document summary and enable more
efficient retrieval and aggregation of infor-
mation. In this paper we investigate key-
words in the context of the Dutch Folk-
tale Database, a large collection of stories
including fairy tales, jokes and urban leg-
ends. We carry out a quantitative and qual-
itative analysis of the keywords in the col-
lection. Up to 80% of the assigned key-
words (or a minor variation) appear in the
text itself. Human annotators show moder-
ate to substantial agreement in their judg-
ment of keywords. Finally, we evaluate a
learning to rank approach to extract and
rank keyword candidates. We conclude
that this is a promising approach to auto-
mate this time intensive task.
1 Introduction
Keywords are frequently used as a simple way
to provide descriptive metadata about collections
of documents. A set of keywords can concisely
present the most important aspects of a document
and enable quick summaries of multiple docu-
ments. The word cloud in Figure 1, for instance,
gives a quick impression of the most important
topics in a collection of over 40,000 documents (a
collection of Dutch folktales).
Keyword assignment or generation is the task
of finding the most important, topical keywords or
keyphrases to describe a document (Turney, 2000;
Frank et al, 1999). Based on keywords, small
groups of documents (Hammouda et al, 2005) or
large collections of documents (Park et al, 2002)
can be summarized. Keyword extraction is a re-
stricted case of keyword assignment: the assigned
keywords are a selection of the words or phrases
appearing in the document itself (Turney, 2000;
Frank et al, 1999).
In this paper we look into keyword extraction
in the domain of cultural heritage, in particular
for extracting keywords from folktale narratives
found in the Dutch Folktale Database (more on
this collection in section 3). These narratives
might require a different approach for extraction
than in other domains, such as news stories and
scholarly articles (Jiang et al, 2009). Stories in the
Dutch Folktale Database are annotated with un-
controlled, free-text, keywords. Because suggest-
ing keywords which do not appear in the text is a
considerably harder task to automate and to eval-
uate, we restrict ourselves to keywords extracted
from the text itself.
In the first part of this paper we study the cur-
rent practice of keyword assignment for this col-
lection. We analyze the assigned keywords in the
collection as a whole and present a more fine-
grained analysis of a sample of documents. More-
over, we investigate to what extent human anno-
tators agree on suitable keywords extracted from
the text. Manually assigning keywords is an ex-
pensive and time-consuming process. Automatic
assignment would bring down the cost and time to
archive material. In the second part of this paper
we evaluate a number of automatic keyword ex-
traction methods. We show that a learning to rank
approach gives promising results.
The overview of this paper is as follows. We
first describe related work in automatic keyword
assignment. In section 3 we introduce the Dutch
Folktale Database. In section 4 we present an anal-
ysis of the keywords currently used in the folktale
database. In section 5 we investigate the agree-
ment of human annotators on keyword extraction.
In section 6 we present and evaluate an automatic
method for extracting and ranking keywords. We
end with a discussion and conclusion in section 7.
65
Keyword (translation) Frequency
dood (death) 5,861
man (man) 4,547
vrouw (woman) 4,154
sterven (to die) 3,168
huis (house) 2,894
spokerij (haunting) 2,491
duivel (devil) 2,487
nacht (night) 2,449
voorteken (omen) 2,380
voorloop (forerunnings) 2,372
geld (money) 2,370
toverij (sorcery) 2,322
zien (to see) 2,270
heks (witch) 2,233
boer (farmer) 2,189
water (water) 2,177
angst (fear) 2,091
hekserij (witchcraft) 1,911
kind (child) 1,853
spoken (ghosts) 1,747
spook (ghost) 1,742
seks (sex) 1,659
Figure 1: Frequent keywords in the Dutch Folktale Database
2 Related Work
Because of space limitations, we limit our dis-
cussion of related work to keyword extraction in
the context of free-text indexing. Automated con-
trolled vocabulary indexing is a fundamentally
different task (see for instance Medelyan and Wit-
ten (2006) and Plaunt and Norgard (1998)).
Typically, keyword extraction consists of two
steps. In the first step candidate keywords are de-
termined and features, such as the frequency or
position in the document, are calculated to char-
acterize these keywords. In the second step the
candidates are filtered and ranked based on these
features. Both unsupervised and supervised algo-
rithms have been used to do this.
2.1 Candidate Extraction
Candidate keywords can be extracted in a number
of ways. The simplest approach is to treat each
single word as a candidate keyword, optionally
filtering out stop words or only selecting words
with a particular Part-of-Speech (Liu et al, 2009a;
Jiang et al, 2009). More sophisticated approaches
allow for multi-word keywords, by extracting con-
secutive words from the text, optionally limited to
keywords adhering to specific lexical patterns (Os-
inski and Weiss, 2005; Hulth, 2003; Rose et al,
2010; Frank et al, 1999; Turney, 2000).
2.2 Features to Characterize Keywords
Many features for characterizing candidate key-
words have been investigated previously, with
varying computational complexities and resource
requirements. The simplest features are based
on document and collection statistics, for instance
the frequency of a potential keyword in the doc-
ument and the inverse document frequency in the
collection (Turney, 2000; Hulth, 2003; Frank et
al., 1999). Examples of more complex features
are: features based on characteristics of lexical
chains, requiring a lexical database with word
meanings (Ercan and Cicekli, 2007); features re-
lated to frequencies in external document collec-
tions and query logs (Bendersky and Croft, 2008;
Yih et al, 2006; Liu et al, 2009b; Xu et al, 2010);
and a feature to determine the cohesiveness of re-
trieved documents with that keyword (Bendersky
and Croft, 2008).
2.3 Unsupervised Methods for Keyword
Extraction
Unsupervised methods for keyword extraction
typically rely on heuristics to filter and rank the
keywords in order of importance. For instance,
by ranking the candidates by their importance in
the collection ? estimated by the inverse docu-
ment frequency. Another approach is to apply the
PageRank algorithm to determine the most impor-
tant keywords based on their co-occurrence link-
structure (Mihalcea and Tarau, 2004). Liu et al
(2009b) employed clustering to extract keywords
that cover all important topics from the original
text. From each topic cluster an exemplar is deter-
mined and for each exemplar the best correspond-
ing keyword is determined.
2.4 Supervised Methods for Keyword
Extraction
Early supervised methods used training data to set
the optimal parameters for (unsupervised) systems
66
based on heuristics (Turney, 2000). Other methods
approached keyword extraction as a binary classi-
fication problem: given a candidate keyword it has
to be classified as either a keyword or not. Meth-
ods include decision trees (Bendersky and Croft,
2008), Naive Bayes (Frank et al, 1999) and Sup-
port Vector Machines (Zhang et al, 2006). Zhang
et al (2008) approached keyword extraction as a
labeling problem for which they employed condi-
tional random fields. Recently, keyword extrac-
tion has been cast as a ranking problem and learn-
ing to rank techniques have been applied to solve
it (Jiang et al, 2009). Jiang et al (2009) concluded
that learning to rank approaches performed better
than binary classifiers in the context of extracting
keywords from scholarly texts and websites. Dif-
ferent variations of learning to rank exist, see (Li,
2011) for an overview.
3 The Dutch Folktale Database
The Dutch Folktale Database is a repository of
over 40,000 folktales in Dutch, old Dutch, Frisian
and a large number of Dutch dialects. The mate-
rial has been collected in the 19th, 20th and 21th
centuries, and consists of stories from various pe-
riods, including the Middle Ages and the Renais-
sance. The collection has both an archival and a
research function. It preserves an important part
of the oral cultural heritage of the Netherlands and
can be used for comparative folk narrative studies.
Since 2004 the database is available online1.
The real value of the database does not only lie
the stories themselves, but also in their manually
added set of descriptive metadata fields. These
fields include, for example, a summary in Dutch,
a list of proper names present in the folktales, and
a list of keywords. Adding these metadata is a
time-consuming and demanding task. In fact, the
amount of work involved hampers the growth of
the folktale database. A large backlog of digitized
folktales is awaiting metadata assignment before
they can be archived in the collection. Being able
to automatically assign keywords to these docu-
ments would be a first step to speed up the archiv-
ing process.
4 Analysis of Assigned Keywords
In this section we analyze the keywords that have
been manually assigned to the stories in the Dutch
Folktale Database. First we look at the keywords
1http://www.verhalenbank.nl, in Dutch only
0 10 20 30 40 50Number of assigned keywords0
500
1000
1500
2000
Doc
ume
nt fr
equ
ency
Figure 2: Number of assigned keywords per doc-
ument
assigned to the collection as a whole. After that we
make a more fine-grained analysis of the keywords
assigned to a selection of the documents.
4.1 Quantitative Analysis
We analyzed a snapshot from the Dutch Folktale
Database (from early 2012) that consists of 41,336
folktales. On average, 15 keywords have been as-
signed to each of these documents (see Figure 2).
The median number of assigned keywords is 10,
however. The keywords vocabulary has 43,195
unique keywords, most of which consist of a sin-
gle word (90%). Figure 1 shows a word cloud
of keywords used in the collection; more frequent
keyword types appear larger. On the right, it lists
the most frequent keyword types (and their trans-
lations). The assignment of keywords to docu-
ments has a Zipfian distribution: a few keyword
types are assigned to many documents, whereas
many keyword types are assigned to few docu-
ments.
When we limit our collection to stories in Dutch
(15,147 documents), we can determine how many
of the manually assigned keywords can be found
literally in the story text2. We define the keyword
coverage of a document as the fraction of its as-
signed keywords which is found in the full text
or its summary. The average keyword coverage
of the Dutch stories is 65%. Figure 3 shows a
histogram of the coverage. It shows that most of
the documents have a keyword coverage of 0.5 or
more.
2Stories in other languages or dialects have been assigned
Dutch keywords.
67
0.0 0.2 0.4 0.6 0.8 1.0Keyword coverage by full-text and summary0
500
1000
1500
2000
2500
3000
Doc
ume
nt fr
equ
ency
Figure 3: Keyword coverage of folktales in Dutch
4.2 Qualitative Analysis
The quantitative analysis does not provide insight
into what kind of keywords have been assigned.
Therefore, we analyzed a selection of documents
more thoroughly. For each of the five largest gen-
res in the collection (fairy tale, traditional legend,
joke, urban legend and riddle) we sampled 10 tales
and manually classified the keywords assigned to
these folktales. A total of almost 1000 keywords
was analyzed. Table 1 summarizes the statistics of
this analysis. Almost 80% of the keywords appear
literally or almost literally in the text. The almost
literal appearances include keywords which differ
in quantity (plural versus singular form) and verb
forms. Verb forms vary in tense (present rather
than past tense) and infinitive keywords of sepa-
rable verbs. An example of the latter is the as-
signment of the keyword ?terugkeren?, to return,
where ?keren? (? turn) and ?terug? (? back) are
used in a sentence. Of the analyzed keywords
5% are synonyms of words appearing the text and
2.3% are hypernyms of words appearing the text
(e.g. ?wapen?, weapon, is used as a keyword with
?mes?, knife, mentioned in the text). The remain-
ing 13% of the keywords represent abstract topic,
event and activity descriptions. For example, the
keyword ?wegsturen?, to send away, when one of
the characters explicitly asks someone to leave.
Other examples are the keywords ?baan?, job, and
?arbeid?, labor, when the story is about an unem-
ployed person.
Based on these numbers we can conclude that
based on extraction techniques alone we should
be able to reproduce a large portion of the manual
keyword assignment. When thesauri are employed
to find synonyms and hypernyms, up to 87% of the
manually assigned keywords could be found. A
much harder task is to obtain the remaining 13%
Classification Count Perc.
Literal 669 67.6%
Almost literal 120 12.1%
Synonym 49 5.0%
Hypernym 23 2.3%
Typing error 2 0.2%
Other 126 12.7%
Total 989 100.0%
Table 1: Keyword types in a set of 1000 folktales
of more abstract keywords, which we will study in
future research.
5 Evaluating Agreement in Keyword
Assignment
The previous analyses raise the question whether
the keywords have been consistently assigned: do
annotators choose the same keywords when pre-
sented with the same text? Moreover, knowing
the difficulty of the task for human annotators will
give us an indication of the level of performance
we may expect from automatic keyword assign-
ment. To determine the agreement between an-
notators we asked ten annotators to classify the
vocabulary of five folktales from different genres.
Frog3 (van den Bosch et al, 2007) was used to
extract the vocabulary of lemmas. After carefully
reading a folktale, the annotator classified the al-
phabetically sorted list of lemmas extracted from
the text. Each lemma was classified as either: 1)
not a relevant keyword ? should not be assigned
to this document (non); 2) a relevant keyword ?
should be assigned (rel); 3) a highly relevant key-
word ? should definitely be assigned (hrel). The
three levels of relevance were used to see whether
annotators have a preference for certain keywords.
The pairwise agreement between annotators was
measured using Cohen?s kappa. Each document
was judged twice, totaling a set of 25 documents.
Most of the annotators were familiar with the folk-
tale database and its keywords; two were active
contributors to the database and thus had previous
experience in assigning keywords to folktales.
On average, the annotators judged 79% of the
vocabulary as non-relevant as keywords. 9% and
12% of the vocabulary was judged as relevant and
highly relevant respectively, but there was a large
variation in these percentages: some annotators
assigned more highly relevant keywords, others
assigned more relevant keywords.
3http://ilk.uvt.nl/frog/
68
Cohen?s Kappa
Classes Average ? Min Max
non, rel, hrel 0.48 0.14 0.16 0.77
non, rel + hrel 0.62 0.16 0.25 0.92
non + rel, hrel 0.47 0.20 0.0 0.84
Table 2: Classification agreement between annota-
tors. Non: non-relevant, rel: relevant, hrel: highly
relevant.
The two experienced annotators showed a con-
sistently higher average agreement in comparison
to the other annotators (0.56 and 0.50 for non, rel,
hrel; 0.7 and 0.64 for non, rel + hrel; 0.56 and 0.50
for non + rel, hrel). Moreover, they assigned more
(relevant and highly relevant) keywords to the doc-
uments on average.
Table 2 summarizes the agreement measured
between annotators. The first row indicates the
agreement when considering agreement over all
three classes; the second row indicates the agree-
ment when treating relevant and highly relevant
keywords as the same class; the last row shows
the agreement in indicating the same highly rel-
evant keywords. The numbers indicate moder-
ate agreement between annotators over all three
classes and when considering the choice of highly
relevant keywords. Annotators show substantial
agreement on deciding between non-relevant and
relevant keywords. Table 3 shows the agreement
between annotators on keywords with different
parts of speech (CGN4 tagset). Most disagree-
ments are on nouns, adjectives and verbs. Verbs
and adjectives show few agreements on relevant
and highly relevant keywords. In contrast, on
20% of the nouns annotators agree on their rele-
vance. It appears that the annotators do not agree
whether adjectives and verbs should be used as
keywords at all. We can give three other reasons
why annotators did not agree. First, for longer
stories annotators were presented with long lists
of candidate keywords. Sometimes relevant key-
words might have been simply overlooked. Sec-
ond, it turned out that some annotators selected
some keywords in favor to other keywords (for in-
stance a hyponym rather than a hypernym), where
others simply annotated both as relevant. Third,
the disagreement can be explained by lack of de-
tailed instructions. The annotators were not told
how many (highly) relevant keywords to select or
4Corpus Gesproken Nederlands (Spoken Dutch Corpus),
http://lands.let.kun.nl/cgn/ehome.htm
what criteria should be met by the keywords. Such
instructions are not available to current annotators
of the collection either.
We conclude that annotators typically agree on
the keywords from a text, but have a varying
notion of highly relevant keywords. The aver-
age keywords-based representation strongly con-
denses the documents vocabulary: a document can
be represented by a fifth (21%) of its vocabulary5.
This value can be used as a cut-off point for meth-
ods ranking extracted keywords, discussed here-
after.
6 Automatically Extracting Keywords
In the last part of this paper we look into automati-
cally extracting keywords. We compare a learning
to rank classifier to baselines based on frequency
and reuse in their ability to reproduce keywords
found in manually classified folktales.
In all cases we use the same method for extract-
ing keyword candidates. Since most of the manual
keywords are single words (90% of the used key-
word types in the collection), we simply extract
single words as keyword candidates. We use Frog
for tokenization and part of speech tagging. Stop
words are not removed.
6.1 Baseline Systems
We use a basic unsupervised baseline for keyword
extraction: the words are ranked according to de-
scending TF-IDF. We refer to this system as TF-
IDF. TF, term frequency, and IDF, inverse docu-
ment frequency, are indicators of the term?s local
and global importance and are frequently used in
information retrieval to indicate the relative impor-
tance of a word (Baeza-Yates and Ribeiro-Neto,
2011).
Note that a word appearing once in the collec-
tion has the highest IDF score. This would imply
that the most uncommon words are also the most
important resulting in a bias towards spelling er-
rors, proper names, and other uncommon words.
Hence, our second baseline takes into account
whether a keyword has been used before in a train-
ing set. Again, the candidates are ranked by de-
scending TF-IDF, but now keywords appearing in
the training collection are ranked above the key-
words not appearing in the collection. We refer to
this baseline as TF-IDF-T.
5Based on the figures that on average 9% of the vocabu-
lary is judged as relevant and 12% as highly relevant
69
Part of speech Adjective Adverb Noun Special Numeral Prep. Verb
Number of words 272 257 646 131 53 268 664
Agreement non 70% 96% 40% 95% 81% 99% 73%
rel 4% 0% 6% 0% 0% 0% 3%
hrel 1% 0% 14% 2% 2% 0% 4%
Disagreement non? rel 15% 2% 17% 2% 11% 0% 12%
non? hrel 5% 1% 8% 2% 4% 1% 5%
rel? hrel 5% 0% 15% 0% 2% 0% 4%
Table 3: Agreement and disagreement of annotators on keywords with different parts of speech. Values
are column-wise percentages. Tags with full agreement are not shown.
6.2 Learning to Rank Keywords
Following Jiang et al (2009) we apply a learn-
ing to rank technique to rank the list of extracted
keywords. We train an SVM to classify the rel-
ative ordering of pairs of keywords. Words cor-
responding to manual keywords should be ranked
higher than other words appearing in the docu-
ment. We use SVM-rank to train a linear ranking
SVM (Joachims, 2006). We use the following fea-
tures.
6.2.1 Word Context
We use the following word context features:
starts uppercase: indicates whether the token
starts with an uppercase letter (1) or not (0). Since
proper names are not used as keywords in the folk-
tale database, this feature is expected to be a neg-
ative indicator of a word being a keyword.
contains space: indicates whether the token con-
tains a space (Frog extracts some Dutch multi-
word phrases as a single token). Tokens with
spaces are not very common.
is number: indicates whether the token consists
of only digits. Numbers are expected not to be a
keyword.
contains letters: indicates whether the token con-
tains at least a single letter. Keywords are expected
to contain letters.
all capital letters: indicates whether the token
consists of only capital letters. Words with only
capital letters are not expected to be keywords.
single letter: indicates whether the token consists
of only one letter. One letter keywords are very
uncommon.
contains punctuation: indicates whether the to-
ken contains punctuation such as apostrophes.
Keywords are expected not to contain punctuation.
part of speech: indicates the part of speech of
the token (each tag is a binary feature). Nouns
are expected to be a positive indicator of key-
words (Jiang et al, 2009).
6.2.2 Document Context
We use the following document context features:
tf: the term frequency indicates the number of ap-
pearances of the word divided by the total number
of tokens in the document.
first offset: indicates the offset of the word?s
first appearance in the document, normalized by
the number of tokens in the document (follow-
ing Zhang et al (2008)). Important (key)words are
expected to be mentioned early.
first sentence offset: indicates the offset of the
first sentence in which the token appears, normal-
ized by the number of sentences in the document.
sentence importance: indicates the maxi-
mum importance of a sentence in which the
word appears, as measured by the SumBasic
score (Nenkova and Vanderwende, 2005). Sum-
Basic determines the relative importance of sen-
tences solely on word probability distributions in
the text.
dispersion: indicates the dispersion or scattering
of the word in the document. Words which are
highly dispersed are expected to be more impor-
tant. The DPnorm is used as a dispersion measure,
proposed in Gries (2008).
6.2.3 Collection Context
We use the following features from the collec-
tion/training context:
idf: the inverse document frequency indicates the
collection importance of the word based on fre-
quency: frequent terms in the collection are less
important than rare terms in the collection.
tf.idf: combines the tf and idf features by multi-
plying them. It indicates a trade-off between local
and global word importance.
is training keyword: indicates whether the word
is used in the training collection as a keyword.
assignment ratio: indicates the percentage of
documents in which the term is present in the text
and in which it is also assigned as a keyword.
70
6.3 Evaluation Method
We evaluate the ranking methods on their ability
to reproduce the manual assignment of keywords.
Ideally the ranking methods rank these manual
keywords highest. We measure the effectiveness
of ranking in terms of (mean) average precision
(MAP), precision at rank 5 (P@5) and precision at
rank R (P@R), similar to Jiang et al (2009). Note
that we use all the manually assigned keywords
as a ground truth, including words which do not
occur in the text itself. This lowers the highest
achievable performance, but it will give a better
idea of the performance for the real task.
We perform a 10-fold stratified cross-validation
with a set of 10,900 documents from the Dutch
Folktale Database, all written in modern Dutch.
6.4 Results
Table 4 lists the performance of the three tested
systems. The TF-IDF system performs worst,
and is significantly outperformed by the TF-IDF-
T system, which in turn is significantly outper-
formed by the rank-SVM system. On average,
rank-SVM returns 3 relevant keywords in its top
5. The reported mean average precision values
are affected by manual keywords which are not
present in the text itself. To put these numbers
in perspective: if we would put the manual key-
words which are in the text in an optimal ranking,
i.e. return these keywords first, we would achieve
an upper bound mean average precision of 0.5675.
Taking into account the likelihood that some of
the highly ranked false positives are relevant af-
ter all (the annotator might have missed a relevant
keyword) and considering the difficulty of the task
(given the variation in agreement between manual
annotators), we argue that the rank-SVM performs
quite well.
Jiang et al (2009) reported MAPs of 0.288 and
0.503 on the ranking of extracted keyphrases from
scholarly articles and tags from websites respec-
tively. Based on these numbers, we could argue
that assigning keywords to folktales is harder than
reproducing the tags of websites, and slightly eas-
ier than reproducing keyphrases from scientific ar-
ticles. Because of differences in the experimental
setup (e.g. size of the training set, features and
system used), it is difficult to make strong claims
on the difficulty of the task.
System MAP P@5 P@R
TF-IDF 0.260 0.394 0.317
TF-IDF-T 0.336 0.541 0.384
rank-SVM 0.399 0.631 0.453
Table 4: Keyword extraction effectiveness. The
differences between systems are statistically sig-
nificant (paired t-test, p< 0.001)
Change in
Feature MAP P@5 P@R
assignment ratio -0.036 -0.056 -0.038
is training keyword 0.006 0.002 0.005
tf.idf -0.004 -0.010 -0.002
part of speech -0.003 -0.007 0.000
dispersion -0.001 -0.001 0.000
idf 0.001 0.002 0.000
starts uppercase 0.000 0.000 -0.001
first offset 0.000 0.000 0.000
tf 0.000 0.000 0.000
contains space 0.000 0.000 0.000
is number 0.000 0.000 0.000
all capital letters 0.000 0.000 0.000
contains punctuation 0.000 0.000 0.000
contains letters 0.000 0.000 0.000
sentence importance 0.000 0.000 0.000
first sentence offset 0.000 0.000 0.000
single letter 0.000 0.000 0.000
Table 5: Differences in performance when leaving
out features. The features are ordered by descend-
ing difference in MAP.
6.5 Feature Ablation
To determine the added value of the individual fea-
tures we carried out an ablation study. Table 5
lists the changes in performance when leaving out
a particular feature (or group of features in case
of part of speech). It turns out that many features
can be left out without hurting the performance.
All the features testing simple word characteristics
(such as single letter) do not, or only marginally
influence the results. Also taking into account the
importance of sentences (sentence importance), or
the first appearance of a word (first offset and first
sentence offset) does not contribute to the results.
System MAP P@5 P@R
rank-SVM 0.399 0.631 0.453
minimum set 0.405 0.631 0.459
Table 6: Results using the full set of features and
the minimum set of features (assignment ratio,
tf.idf, part of speech and dispersion). Differences
between systems are statistically significant (t-test,
p < 0.001).
71
Genre (# stories) MAP P@5 P@R
Trad. legend (3783) 0.439 0.662 0.494
Joke (2793) 0.353 0.599 0.405
Urban legend (1729) 0.398 0.653 0.459
Riddle (1067) 0.391 0.573 0.415
Fairy tale (558) 0.404 0.670 0.477
Pers. narrative (514) 0.376 0.593 0.437
Legend (221) 0.409 0.622 0.478
None (122) 0.366 0.602 0.421
Other (113) 0.405 0.648 0.472
All (10900) 0.399 0.631 0.453
Table 7: SVM performance split according to
story genre. Values in bold are significantly dif-
ferent from the results on the other genres (inde-
pendent t-test, p-value < 0.01)
These observations suggest that almost identi-
cal results can be obtained using only the features
assignment ratio, tf.idf, part of speech and disper-
sion. The results reported in Table 6 confirm this
(we do note that these results were obtained by op-
timizing on the test set).
6.6 Performance on Folktale Genres
The folktale database contains stories from differ-
ent folktale genres, varying from legends to fairy
tales and jokes. Table 7 lists the performance mea-
sures per story genre. Values in bold indicate sig-
nificant differences with the stories from the other
genres combined. The performance on traditional
legends turns out to be significantly better than
other genres: this could be explained by the fact
that on average these stories are longer and there-
fore contain more keywords. Similarly, the de-
crease can be explained for jokes, which are much
shorter on average. Another explanation could be
that more abstract keywords are used to indicate
the type of joke. Interestingly, the riddles, which
are even shorter than jokes, do not perform sig-
nificantly worse than the other genres. Personal
narratives also underperformed in comparison to
the other genres. We cannot readily explain this,
but we suspect it may have something to do with
the fact that personal narratives are more varied in
content and contain more proper names.
7 Discussion and Conclusion
In this work we analyzed keywords in the context
of the Dutch Folktale Database. In this database,
on average 15 keywords have been assigned to a
story, many of which are single keywords which
appear literally or almost literally in the text itself.
Keyword annotators show moderate to substantial
agreement in extracting the same keywords for a
story. We showed that a learning to rank method
using features based on assignment ratio, tf.idf,
part of speech and dispersion can be effectively
used to extract and rank keyword candidates. We
believe that this system can be used to suggest
highly relevant keyword candidates to human an-
notators to speed up the archiving process.
In our evaluation we aimed to reproduce the
manual annotations, but it is unclear whether bet-
ter performing systems are actually more helpful
to the user. In an ad hoc retrieval scenario, in
which the user issues a single query and reviews
a list of retrieved documents, extracted keywords
might be used to boost the early precision of the
results. However, a user might not even notice
a difference when a different keyword extraction
system is used. Moreover, the more abstract key-
words which do not appear in the text might be
more important for the user experience. In fu-
ture work we want to get insight in how keywords
contribute to the end user experience. Ideally, the
evaluation should directly measure how useful the
various keywords are for accessing the collection.
In this work we considered only extracting key-
words from the text we want to annotate. Given
the multilingual content of the database this is a
limited approach: if the goal of assigning key-
words is to obtain a normalized representation of
the stories, this approach will require translation
of either the source text (before extraction) or the
extracted keywords. Even in the monolingual sce-
nario, the extraction of keywords is limited in deal-
ing with differences in style and word use. Writers
may use different words or use words in a differ-
ent way; ideally the representation based on key-
words is a normalized representation which closes
this semantic gap. In future work we will look into
annotation with keywords from multi-lingual the-
sauri combined with free-text keywords extracted
from the text itself. Finally, we want to look into
classification of abstract themes and topics.
Acknowledgments
This research was supported by the Folktales as
Classifiable Texts (FACT) project, part of the
CATCH programme funded by the Netherlands
Organisation for Scientific Research (NWO).
72
References
R Baeza-Yates and B. Ribeiro-Neto. 2011. Modern In-
formation Retrieval. The Concepts and Technology
Behind Search. Addison-Wesley.
M. Bendersky and W.B. Croft. 2008. Discovering key
concepts in verbose queries. In Proceedings of SI-
GIR 2008, pages 491?498.
G. Ercan and I. Cicekli. 2007. Using lexical chains
for keyword extraction. Information Processing &
Management, 43(6):1705?1714.
E. Frank, G.W. Paynter, I.H. Witten, C. Gutwin, and
C.G. Nevill-Manning. 1999. Domain-specific
keyphrase extraction. In Proceedings of IJCAI-99,
pages 668?673. Morgan Kaufmann Publishers Inc.,
San Francisco, CA, USA.
Stefan Th. Gries. 2008. Dispersions and adjusted fre-
quencies in corpora. International Journal of Cor-
pus Linguistics, 13(4):403?437.
K. Hammouda, D. Matute, and M. Kamel. 2005.
Corephrase: Keyphrase extraction for document
clustering. Machine Learning and Data Mining in
Pattern Recognition, pages 265?274.
A. Hulth. 2003. Improved automatic keyword extrac-
tion given more linguistic knowledge. In Proceed-
ings of EMNLP, volume 10, pages 216?223, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
X. Jiang, Y. Hu, and H. Li. 2009. A ranking ap-
proach to keyphrase extraction. In Proceedings of
the 32nd international ACM SIGIR conference on
Research and development in information retrieval,
pages 756?757. ACM.
T. Joachims. 2006. Training Linear SVMs in Lin-
ear Time. In the 12th ACM SIGKDD international
conference, pages 217?226, New York, NY, USA.
ACM.
H. Li. 2011. Learning to Rank for Information Re-
trieval and Natural Language Processing. Synthesis
Lectures on Human Language Technology. Morgan
& Claypool Publishers.
F. Liu, D. Pennell, F. Liu, and Y. Liu. 2009a. Unsu-
pervised approaches for automatic keyword extrac-
tion using meeting transcripts. In Proceedings of
NAACL 2009, pages 620?628. Association for Com-
putational Linguistics.
Z. Liu, P. Li, Y. Zheng, and M. Sun. 2009b. Cluster-
ing to find exemplar terms for keyphrase extraction.
In Proceedings of EMNLP, pages 257?266. Associ-
ation for Computational Linguistics.
O Medelyan and Ian H Witten. 2006. Thesaurus based
automatic keyphrase indexing. In JCDL 2006, pages
296?297. ACM.
R. Mihalcea and P. Tarau. 2004. Textrank: Bringing
order into texts. In Proceedings of EMNLP, vol-
ume 4, pages 404?411. Barcelona, Spain.
A. Nenkova and L. Vanderwende. 2005. The impact of
frequency on summarization. Microsoft Research,
Redmond, Washington, Tech. Rep. MSR-TR-2005-
101.
S. Osinski and D. Weiss. 2005. A concept-driven al-
gorithm for clustering search results. Intelligent Sys-
tems, IEEE, 20(3):48?54.
Y. Park, R.J. Byrd, and B.K. Boguraev. 2002. Auto-
matic glossary extraction: beyond terminology iden-
tification. In Proceedings of COLING 2002, pages
1?7. Association for Computational Linguistics.
Christian Plaunt and Barbara A Norgard. 1998. An
Association Based Method for Automatic Indexing
with a Controlled Vocabulary. Journal of the Ameri-
can Society for Information Science and Technology,
49(10):888?902.
S. Rose, D. Engel, N. Cramer, and W. Cowley. 2010.
Automatic keyword extraction from individual doc-
uments. In Michael W. Berry and Jacob Kogan, ed-
itors, Text Mining: Applications and Theory, pages
3?20. John Wiley & Sons.
P.D. Turney. 2000. Learning algorithms for keyphrase
extraction. Information Retrieval, 2(4):303?336.
A. van den Bosch, G.J. Busser, W. Daelemans, and
S Canisius. 2007. An efficient memory-based mor-
phosyntactic tagger and parser for Dutch. In F. van
Eynde, P. Dirix, I. Schuurman, and V. Vandeghinste,
editors, Selected Papers of the 17th Computational
Linguistics in the Netherlands Meeting, pages 99?
114, Leuven, Belgium.
S. Xu, S. Yang, and F.C.M. Lau. 2010. Keyword ex-
traction and headline generation using novel word
features. Proceedings of the Twenty-Fourth AAAI
Conference on Artificial Intelligence (AAAI-10).
W. Yih, J. Goodman, and V.R. Carvalho. 2006. Find-
ing advertising keywords on web pages. In Proceed-
ings of the 15th international conference on World
Wide Web, pages 213?222. ACM.
K. Zhang, H. Xu, J. Tang, and J. Li. 2006. Keyword
extraction using support vector machine. Advances
in Web-Age Information Management, pages 85?96.
C. Zhang, H. Wang, Y. Liu, D. Wu, Y. Liao, and
B. Wang. 2008. Automatic keyword extrac-
tion from documents using conditional random
fields. Journal of Computational Information Sys-
tems, 4(3):1169?1180.
73
