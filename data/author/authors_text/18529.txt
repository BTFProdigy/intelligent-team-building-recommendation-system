Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1124?1135,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Identifying Multiple Userids of the Same Author 
 
 
Tieyun Qian Bing Liu 
State Key Laboratory of Software Eng.  
Wuhan University 
Department of Computer Science 
University of Illinois at Chicago 
16 Luojiashan Road 851 South Morgan St., Chicago 
Wuhan, Hubei 430072, China  IL, USA, 60607 
qty@whu.edu.cn liub@cs.uic.edu 
 
  
 
Abstract 
This paper studies the problem of identifying 
users who use multiple userids to post in so-
cial media. Since multiple userids may belong 
to the same author, it is hard to directly apply 
supervised learning to solve the problem. This 
paper proposes a new method, which still uses 
supervised learning but does not require train-
ing documents from the involved userids. In-
stead, it uses documents from other userids 
for classifier building. The classifier can be 
applied to documents of the involved userids. 
This is possible because we transform the 
document space to a similarity space and 
learning is performed in this new space. Our 
evaluation is done in the online review do-
main. The experimental results using a large 
number of userids and their reviews show that 
the proposed method is highly effective. 
1 Introduction 
It is common knowledge that some users in social 
media register multiple accounts/userids to post 
articles, blogs, reviews, etc. There are many rea-
sons for doing this. For example, due to past post-
ings, a user may become despised by others. 
He/she then registers another userid in order to 
regain his/her status. A user may also use multiple 
userids to instigate controversy or debates to popu-
larize a topic to make it ?hot? or even just to pro-
mote activities at a website. Yet, a user may also 
use multiple userids to post fake or deceptive opin-
ions to promote or demote some products (Liu, 
2012). It is thus important to develop technologies 
to identify such multi-id users. This paper deals 
with this problem based on writing style and other 
linguistic clues.  
Problem definition: Given a set of userids ID = 
{id1, ?, idn} and each idi has a set of documents 
Di, we want to identify userids that belong to the 
same physical author.  
The main related works to ours are in the area of 
authorship attribution (AA), which aims to identify 
authors of documents. AA is often solved using 
supervised learning. Let A = {a1, ?, ak} be a set of 
authors (or classes) and each author ai ? A has a 
set of training documents Di. A classifier is then 
built to decide the author a of each test document 
d, where a ? A. We will discuss this and other re-
lated works in Section 2.  
This supervised AA formulation, however, is 
not suitable for our task because we only have 
userids but not real authors. Since some of the 
userids may belong to the same author, we cannot 
treat each userid as a class because in that case, we 
will be classifying based on userids, which won?t 
help us find authors with multiple userids (see Sec-
tion 7 also).  
This paper proposes a novel algorithm. To sim-
plify the presentation, we assume that at most two 
userids can belong to a single author, but the algo-
rithm can be extended to handle more than two 
userids from the same author. Using this assump-
tion, the algorithm works in two steps:  
1.  Candidate identification: For each userid idi, 
we first find the most likely userid idj (i ? j) that 
may have the same author as idi. We call idj the 
candidate of idi. We also call this function can-
did-iden, i.e., idj = candid-iden(idi). For easy 
presentation, here we only use one argument for 
*  The work was mainly done when the first author was visit-
ing the University of Illinois at Chicago.  
1124
candid-iden. In the computation, it needs more 
arguments (see Section 4).  
2.  Candidate confirmation: In the reverse order, 
we apply the function candid-iden on idj, which 
produces idk, i.e., idk = candid-iden(idj). 
Decision making: If k = i, we conclude that idi 
and idj are from the same author. Otherwise, idi 
and idj are not from the same author.   
The key of the algorithm is candid-iden. An ob-
vious approach for candid-iden is to use an infor-
mation retrieval method. We can first split the 
documents Di of each idi into two subsets, a query 
set Qi and a sample set Si. We then compare each 
query document in Qi with each sample document 
in Sj from other userids idj (? ID ? {idi}). Cosine 
can be used here for similarity comparison. All the 
similarity scores are then aggregated and used to 
rank the userids in ID ? {idi}. The top ranked 
userid is the candidate for idi. Note that partition-
ing the documents of a userid idi into the query set 
Qi and the sample set Si is crucial here. We cannot 
use all documents in Di to compare with all docu-
ments in Dj. If so and we get candid-iden(idi) = idj, 
we will definitely get candid-iden(idj) = idi since 
the similarity function is symmetric.  
This cosine similarity based method, however, 
does not work well (see Section 7). We propose a 
supervised learning method to compute the scores. 
For this, we need to reformulate the problem.  
The idea of this reformulation is to learn in a 
similarity space rather than in the original docu-
ment space as in traditional AA. In the new formu-
lation, each document d is still represented as a 
feature vector, but the vector no longer represents 
the document d itself. Instead, it represents a set of 
similarities between the document d and a query q. 
We call this method learning in the similarity 
space (LSS).  
Specifically, in LSS, each document d is first 
represented with a document space vector (called a 
d-vector) based on the document itself as in the 
traditional classification learning of AA. Each fea-
ture in the d-vector is called a d-feature (docu-
ment-feature). A query document q is represented 
in the same way. We then produce a similarity vec-
tor sv (called s-vector) for d. sv consists of a set of 
similarity values between document d (in a d-
vector) and query q (in a d-vector):  
sv =Sim(d, q), 
where Sim is a similarity function consists of a set 
of similarity measures. Thus, the d-vector for doc-
ument d in the document space is transformed to 
an s-vector sv for d in the similarity space. Each 
feature in sv is called an s-feature. For example, 
we have the following d-vector for query q:  
 q: 1:1 2:1 6:2 
where x:z represents a d-feature x (a word) and its 
frequency z in q. We also have two non-query 
documents, one is d1 which is written by the author 
of query q and the other is d2 which is not written 
by query author q. Their d-vectors are: 
 d1:  1:2 2:1 3:1  d2:  2:2 3:1 5:2   
If we use cosine as the first similarity measure in 
Sim, we can generate an s-feature 1:0.50 for d1 
(cosine(q, d1) = 0.50) and an s-feature 1:0.27 for d2 
(cosine(q, d2) = 0.27). If we have more similarity 
measures more s-features can be produced. The 
resulting two s-vectors for d1 and d2 with their 
class labels, 1 and -1, are as follows:  
 d1: 1 1:0.50 ? d2:  -1  1:0.27 ? 
Class 1 means ?written by author of query q?, also 
called q-positive, and class -1 means ?not written 
by author of query q?, also called q-negative.  
LSS gives us a two-class classification problem. 
In this formulation, a test userid and his/her docu-
ments do not have to be seen in training as long as 
a set of known documents from this userid is 
available. Any supervised learning method can be 
used to build a classifier. We use SVM. The result-
ing classifier is employed to compute a score for 
each review to be used in the two-step algorithm 
above to find the candidate for each userid and 
then the userids with the same authors.  
Due to the use of query documents, the LSS 
formulation has some resemblance to document 
ranking based on learning to rank (Li, 2011; Liu, 
2011). However, LSS is very different because we 
turn the problem into a supervised classification 
problem. The key difference between learning to 
rank and classification is that ranking will always 
put some documents at the top even if the desired 
documents do not exist. However, classification 
will not return any document if the desired docu-
ments do not exist in the test data (unless there are 
classification errors). Our Type II experiments in 
Section 7 were specifically designed for testing 
such non-existence situations. 
1125
Using online review as the application domain, 
we conduct experiments on a large number of re-
views and their author/reviewer userids from Am-
azon.com. The results show that the proposed 
algorithm is highly accurate and outperforms three 
strong baselines markedly. 
2 Related Work 
A similar problem was attempted in (Chen et al, 
2004) in the context of open forums where users 
interact with each other in their discussions. Their 
method is based on post relationships and intervals 
between posts. It does not use any linguistic clues. 
It is thus not applicable to domains like online re-
views. Reviews do not involve user interactions 
since each review is independent of other reviews. 
Novak et al also solved the same problem under 
the name of ?Anti-aliasing? (Novak et al, 2004). 
They used a clustering based method which as-
sumed the number of actual authors is known. This 
is unrealistic in practice as there is no way to know 
which author has and does not have multiple ids. 
Our work is also related to authorship attribu-
tion (AA). However, to our knowledge, our prob-
lem has not been attempted in AA. Existing works 
focused on two main themes: finding good writing 
style features, and developing effective classifica-
tion methods. On finding good features (d-features 
in our case), it was found that the most promising 
features are function words (Mosteller, 1964; Ar-
gamon and Levitan, 2004; Argamon et al, 2007) 
and rewrite rules (Halteren et al, 1996). Length 
(Gamon 2004; Graham et al, 2005), richness (Hal-
teren et al, 1996; Koppel and Schler, 2004), punc-
tuations (Graham et al, 2005), character n-grams 
(Grieve, 2007; Hedegaard and Simonsen, 2011), 
word n-grams (Burrows, 1992; Sanderson and 
Guenter 2006), POS n-grams (Gamon, 2004; Hirst 
and Feiguina, 2007), syntactic category pairs (Na-
rayanan et al, 2012) are also useful.  
On classification, numerous methods have been 
tried, e.g., Bayesian analysis (Mosteller, 1964), 
discriminant analysis (Stamatatos et al, 2000), 
PCA (Hoover, 2001), neural networks (Graham et 
al., 2005; Zheng et al, 2006; Graham et al, 2005), 
clustering (Sanderson and Guenter, 2006), decision 
trees (Uzuner and Katz, 2005; Zhao and Zobel, 
2005), regularized least squares classification 
(Narayanan et al, 2012),   and SVM (Diederich et 
al., 2000; Gamon 2004; Koppel and Schler, 2004; 
Hedegaard and Simonsen, 2011). Among them, 
SVM was found to be most accurate (Li et al, 
2006; Kim et al, 2011). Although we also use 
supervised learning, we do not learn in the original 
document space as these existing methods do. The 
transformation is important because it enables us 
to use documents from other authors in training. 
The traditional supervised learning (TSL) cannot 
do that. In our case, the only documents that TSL 
can use for training are the queries in the testing 
set. However, as we will see in our experiments, 
such a method performs poorly.  
Since we use online reviews as our experiment 
domain, our work is related to fake review detec-
tion (Jindal and Liu, 2008) as imposters can use 
multiple userids to post fake reviews. Existing re-
search has proposed many methods to detect fake 
reviewers (Lim et al, 2010; Wang et al, 2011; 
Mukherjee et al, 2012) and fake reviews (Jindal 
and Liu, 2008; Ott et al, 2011, 2012; Li et al, 
2011; Feng et al, 2012). However, none of them 
identifies userids belonging to the same person. 
3 Learning in the Similarity Space 
We now formulate the proposed supervised 
learning in the similarity space (LSS), which will 
be used in the candid-iden function in our algo-
rithm to be discussed in Section 4.  
The key difference between LSS and the classic 
document space learning is in the document repre-
sentation. Another difference is in the testing 
phase. We discuss testing first.  
Test data: We are given: 
? A query q from query author (userid) aq 
? A set of test documents DT = {dt1, ?, dtm}. 
Goal: classify the test documents into those au-
thored by aq and those not authored by aq.  
We note the following points:  
i)  This is like a retrieval scenario, but we use su-
pervised learning to perform the task.  
ii) Unlike traditional supervised classification, 
here the test query author aq does not have to 
be used in training. But we are given a query 
document q from aq. Clearly, in practice, we 
can have multiple query documents from aq, 
which we will discuss in Section 4.  
Training document representation: As noted 
earlier, each document is represented with a simi-
larity vector (s-vector) computed using a similarity 
1126
function Sim. Sim takes a query document and a 
non-query document and produces a vector of sim-
ilarity values or s-features to represent the non-
query document. We present the detail below:  
Let the set of training authors be AR = {ar1, .., 
arn}. Each author ari has a set of documents DRi. 
Each document in DRi is first represented with a 
document vector (or d-vector). The algorithm for 
producing the training set, called s-training set, is 
given in Figure 1.  
We randomly select a small set of queries Qi 
from documents DRi of each author ari (lines 1, 
and 2). For each query qij ? Qi (line 3), it selects a 
set of documents DRij also from DRi (excluding qij) 
of the same author (line 4) to be the positive doc-
uments for qij, called q-positive and labeled 1. 
Then, for each document drijk in DRij, a q-positive 
s-training example with the label 1 is generated for 
drijk by computing the similarities of qij and drijk 
using the similarity function Sim (lines 5, 6). In 
line 7, it selects a set of documents DRij,rest from 
other authors to be the negative documents for qij, 
called q-negative and labeled -1. For each docu-
ment drijk,rest in DRij,rest (line 8), a q-negative s-
training example with label -1 is generated for drijk 
by computing the similarities of qij and drijk,rest us-
ing Sim (line 9). How to select Qi, DRij and DRij,rest 
(lines 2, 4 and 7) is left open intentionally to give 
flexibility in implementation.  
This formulation gives us a two-class classifica-
tion problem. The classes are 1 (q-positive mean-
ing ?written by author of query qij?) and -1 (q-
negative meaning ?not written by author of query 
qij.?  Figure 2 shows what the s-training data looks 
like. For easy presentation, we assume that there 
are k queries in every Qi, and p documents in every 
DRij and u documents in every DRij,rest. The num-
ber of authors is n. Each author ari generates 
k?(p+u) s-training examples. As we will see in 
Section 7, k can be very small, even 1. 
Complexity: In the worst case, every document 
1. For each document set Di of idi ? ID do 
2.  partition Di into two subsets:    
 (1) query set Qi and (2) sample set Si;   
3. For each document set Di of idi ? ID do 
 // step 1: candidate identification 
4. idj = candid-iden(idi, ID), i < j; 
 // step 2: candidate confirmation  
5. idk = candid-iden(idj, ID), k ? j; 
6. If k = i then idi and idj are from the same author 
8. else  idi and idj are not from the same author  
Figure 3: Identifying userids from the same authors 
Function candidate-iden(idi, ID) 
1. For each sample document set Sj of idj ? ID-{idi} do 
2.  pcount[idj],  psum[idj], psqsum[idj], max[idj] = 0; 
3.  For each query qi ? Qi do 
4. For each sample sjf ? Sj do 
5.  ssjf = <(idi, qi), (Sim(sjf, qi), ?)>;   
6.  Classify ssjf  using the classifier built earlier; 
7. If ssjf is classified positive, i.e., 1 then 
8.  pcount[idj] = pcount[idj] + 1; 
9.  psum[idj] = psum[idj] + ssjf.score 
10 psqsum[idj] = psqsum[idj] + (ssjf.score)2 
11. If ssif.score > max[idj] then 
12. max[idj] = srjf.score 
// Four methods to decide which idj is the candidate for idi 
13. If for all idj ? ID-{idi}, pcount[idi] = 0 then 
14.  
])(max[maxarg }{ jidIDid idcid ij ???
 
15. Else 
)||
][(maxarg
}{ j
j
idIDid S
idpcountcid
ij ??
?
 // 1. Voting 
16. 
)||
][(maxarg
}{ j
j
idIDid S
idpsumcid
ij ??
?
 // 2. ScoreSum 
17. 
)||
])[((maxarg
2
}{ j
j
idIDid S
idpsumcid
ij ??
?
 // 3. ScoreSqSum 
18.         
])(max[maxarg }{ jidIDid idcid ij ???
 // 4. ScoreMax 
19. return cid; 
Figure 4: Identifying the candidate 
1. For each author ari ? AR 
2. select a set of query documents Qi  ? DRi  
3.  For each query qij ? Qi  
 // produce positive s-training examples 
4. select a set of documents from author ari  
 DRij ? DRi ? {qij} 
5. For each document drijk ? DRij  
6. produce an s-training example for drijk,  
 (Sim(drijk, qij), 1) 
 // produce negative s-training examples 
7. select a set of documents from the rest of authors  
 DRij,rest ? (DR1 ? ? ? DRn) ? DRi  
8. For each document drijk,rest ? DRij,rest  
9. produce an s-training example for drijk,rest,  
 (Sim(drijk,rest, qij), -1) 
Figure 1: Generating s-training examples 
//  Author ar1 ?  
// positive (1) s-training examples 
(Sim(dr111, q11), 1),  ?,  (Sim(dr11p, q11), 1)  
? 
(Sim(dr1k1, q1k), 1),   ?,   (Sim(dr1kp, q1k), 1)  
// negative (-1) s-training examples 
(Sim(dr111.rest, q11), -1),  ?,  (Sim(dr11u.rest, q11), -1)  
? 
(Sim(dr1k1.rest, q1k), -1),  ?,   (Sim(dr1ku.rest, q1k), -1)  
? 
Figure 2: s-training examples 
1127
can serve as a query or a non-query document. 
Then we need to compute all pairwise document 
similarities. If the number of training documents is 
m, the complexity is O(m2), which is both space 
and computation expensive. However, in practice, 
we don?t need all pairwise comparisons. Only a 
small subset is sufficient (see Section 7).  
Test document representation: Like training 
documents, test documents are represented as s-
vectors as well in the similarity space.  
Given a query q from author aq and a set of test 
documents DT, each test document dti is converted 
to a s-vector svi = Sim(dti, q). To reflect svi is com-
puted based on query q from author aq, a s-test 
case is thus represented as <(aq, q), (svi, ?)>.  
Training: A binary classifier is learned using the 
s-training data. Each s-training example is repre-
sented with (sv, y), where sv is an s-vector and y 
(? {1, -1}) is its class. Any supervised learning 
algorithm, e.g., SVM, can be applied.  
Testing: The classifier is applied to each s-test 
case <(aq, q), (svi, ?)> (where svi = S(dti, q)) to 
give it a class q-positive or q-negative. Note that 
the classifier is only applied on svi.  
In most cases, classification based on a single que-
ry is inaccurate. Using multiple queries of an au-
thor can classify much more accurately.   
4 Identify Userids of the Same Author 
We now expand the sketch of the two-step algo-
rithm in Section 1 based on the problem statement 
in Section 1. The algorithm is given in Figure 3.   
Lines 1-2 partitions the documents set Di of 
each idi in ID = {id1, id2, ?, idn}, the set of userids 
that we are working on. How to do the partition is 
flexible (see Section 7). Line 4 is the step 1 of 
candidate identification, and line 5 is the step2 of 
candidate confirmation. Lines 6-8 is the decision 
making of step 2 (see Section 1). Line 6 produced 
a classification score using the classifier described 
in Section 3. The key function here is candid-iden. 
Its algorithm is in Figure 4.  
The candid-iden function takes two arguments: 
the query userid idi and the whole set of userids 
ID. It classifies each sample ssjf in sample set Sj of 
idj ? ID-{idi} to positive (qi-positive) or negative 
(qi-negative) (lines 4, 5, 6). We then aggregate the 
classification results to determine which userid is 
likely to have the same author as idi.   
One simple aggregation method is voting. We 
count the total number of positive classifications of 
the sample documents of each userid in ID-{idi}. 
The userid idj with the highest count is the candi-
date cid which may share the same author as query 
idi. cid is returned as the candidate.  
There are also other methods, which can depend 
on what output value the classifier produces. Here 
we propose four methods including the voting 
method above. The other three methods requires 
the classifier to produces a prediction score, which 
reflects the positive and negative certainty. Many 
classification algorithms produce such a score. 
Here we use SVM. For each classification, SVM 
outputs a positive or negative score indicating the 
certainty that the test case is positive or negative.  
To save space, all four alternative methods are 
given in Figure 4. Line 2 initializes some variables 
for recording the aggregated values for the final 
decision making. The four methods are as follows:  
1). Voting: For each sample from userid idj, if it is 
classified as positive, one vote/count is added 
to pcount[idj]. The userid with the highest 
pcount is regarded as the candidate userid, cid 
(line 15). Note that the normalization is ap-
plied because the sizes of the sample sets Sj 
can be different for different userids. Lines 13 
and 14 mean that if all documents of all 
userids are classified as negative (pcount[idj] = 
0, which also implies psum[idj] = psqsum[idj] 
= 0), we use method 4).  
2). ScoreSum: This method works similarly to the 
voting method above except that instead of 
counting positive classifications, this method 
sums up all scores of positive classifications in 
psum[idj] for each userid (line 9). The decision 
is also made similarly (line 16). 
3). ScoreSqSum: This method works similarly to 
ScoreSum above except that it sums up the 
squared scores of positive classifications in 
psqsum[idj] for each userid (line 10). The deci-
sion is also made similarly (line 17). 
4). ScoreMax: This method works similarly to the 
voting method as well except that it finds the 
maximum classification score for the docu-
ments of each userid (lines 11 and 12). The 
decision is made in line 18. 
5 D-features 
We now compute s-features (similarity features) 
1128
for each non-query document based on a query 
document. Since s-features are calculated using d-
features of a non-query document and a query 
document, we thus discuss d-features first, which 
are extracted from each document itself. We em-
ploy 26 d-features in four categories: length d-
features, frequency based d-features, tf.idf based d-
features, and richness d-features. Although many 
features below have been used in various tasks 
before, our key contribution is solving a new prob-
lem based on a new learning formulation (LSS).  
Length d-feature: We derive three length d-
features from each raw document: (1) average 
sentence length (in terms of word count); (2) 
average word length (in terms of character count 
in one word); (3) average document length (in 
terms of word count in one document). 
Frequency based d-features: We extract lexical, 
syntactic, and stylistic tokens from the raw docu-
ments and the parsed syntactic trees to produce the 
following features:   
? Lexical tokens: word unigrams 
? Syntactic tokens: content-independent struc-
tures: POS n-grams (1 ? n ? 3) and rewrite rules 
(Halteren et al, 1996; Hirst and Feiguina, 2007). 
A rewrite rule is a combination of a node and 
its immediate constituents in a syntactic tree. 
For example, the rewrite rule for "the best 
book" is NP->DT+JJS+NN. 
? Common stylistic token: K-length word (1 ? K ? 
15), punctuations, and 157 function words 
(www.flesl.net/Vocabulary/SinglewordLists/fun
ctionwordlist.php). 
? Review specific stylistic tokens: These tokens 
reflect styles of reviews: all cap words, pairs of 
quotation marks, pairs of brackets, exclamatory 
marks, contractions, two or more consecutive 
non-alphanumeric characters, model auxilia-
ries (e.g., should, must), word ?recommend? or 
?recommended?, sentences with the first letter 
capitalized, sentences starting with This is (this 
is) or This was (this was). We then treat these 
tokens as pseudo-words and count their fre-
quency to form frequency d-features.  
TF-IDF based d-feature: For the tokens listed in 
the frequency based features above, we also com-
pute their tf.idf values. We list these two kinds of 
d-features separately because they will be used for 
different s-features later.  
Richness d-features: This is a set of vocabulary 
richness functions used to quantify the diversity of 
vocabulary in text (Holmes and Forsyth, 1995). In 
this paper, we apply them to the counts of word 
unigrams, POS n-grams (1 ? n ? 3), and rewrite 
rules. Here POS n-grams and rewrite rules are 
treated as pseudo-words. Let T be the total number 
of tokens (words or pseudo-words), and V(T) be 
the number of different tokens in a document, v be 
the highest frequency of occurrence of a token, and 
V(m, T) be the number of tokens which occur m 
times in the document. We use the following six 
richness measures (Yule, 1944; Burrows, 1992; 
Halteren et al, 1996) given in Table 1: Yule?s 
characteristic (K), Hapax dislegomena (S), Simp-
son?s index (D), Honor?s measure (R), Brunet?s 
measure (W), and Hapax legomena (H). They give 
us a set of richness d-features about word uni-
grams, POS n-grams, and rewrite rules. 
Table 1. Richness metrics 
2
4 1
2
( * ( , ) )
10 *
v
m
m V m T T
K T
?
?
?
?  (2, )( )V TS V T?
 
1
( *( 1)* ( , ))
*( 1)
v
m
m m V m T
D T T
?
?
? ?
?
 
100*log( )
1 (1, ) / ( )
TR V T V T? ?  
(1, )H V T?  ( ) , 0.17aV TW T a?? ? 
 
6 S-Features  
The extracted d-features are transformed into s-
features, which are a set of similarity functions on 
two documents. We adopt five types of s-features.  
Sim4 Length s-features: This is a set of four simi-
larity functions defined by us. They are used for d-
feature vectors of length. The four formulae are 
given in Table 2, where lwq. (lwd), lsq. (lsd), and  lrq. 
(lrd) denote the average word, sentence, and docu-
ment length respectively, either in query q or non-
query document d. They produce four s-features.  
Table 2. Sim4 for computing length s-features 
1/ (1 log(1 | |))wq wdl l? ? ?
 
1/ (1 log(1 | |))sq sdl l? ? ?
 
1/ (1 log(1 | |))rq rdl l? ? ? 
{ , , } { , , } { , , }
22( * ) / ( ) * ( )
m w s r m w s r m w s r
mq md mq mdl l l l
? ? ?
? ? ?
 
1129
Sim3 Sentence s-features: This is a set of three 
sentence similarity functions (Metzler et al, 2005). 
We apply them (called Sim3) to documents. Sim3 
s-features are used for frequency based d-features. 
The three formulae are given in Table 3, where f(t, 
s) is the frequency count of token t in a document s, 
and lq and ld are the average document length of 
the query and non-query document, respectively. 
Table 3. Sim3 for computing sentence s-features 
( , ) / ( ( , ) ( , ) ( , ))
t q d t q t d t q d
f t d f t q f t d f t d
? ? ? ? ? ?
? ?? ? ? ?
 
( , )
log ( )*( , ) ( , ) ( , ) ( , )
t q d
t q d
t q t d t q d
f t d
N
f t d f t q f t d f t d
? ?
? ?
? ? ? ?
? ?
?
? ? ?
 
1 * ( )*1 log(1 | |) 1 | ( , ) ( , ) |t q dq d
N idf t
l l f t q f t d? ?? ? ? ? ?? 
Sim7 Retrieval s-features: This is a set of seven 
similarity functions (Table 4) applicable to all fre-
quency based d-features. These functions were 
used in information retrieval (Cao et al, 2006).  
Table 4. Sim7 for computing retrieval s-features 
log( ( , ) 1)
t q d
f t d
? ?
??
 
| |log( 1)( , )t q d
D
f t d? ? ??
 
log( ( ))
t q d
idf t
? ??
 
( , )log( 1)| |t q d
f t d
d? ? ??
 
( , )log( * ( ) 1)| |t q d
f t d idf td? ? ??
 
log( 25 )BM score 
( , ) | |log( * 1)| | ( , )t q d
f t d D
d f t d? ? ??
 
 
In Table 4, f(t, d) denotes the frequency count of 
token t in a non-query document d, q denotes the 
query, D is the entire collection, |.| is the size of a 
set, and idf is the inverse document frequency. 
These 7 formulae can produce 7 s-features. 
SimC tf-idf s-feature: This is the cosine similarity 
used for d-vectors represented by the tf.idf based 
d-features. SimC tf-idf produces one s-feature. 
SimC Richness s-feature: This is also cosine sim-
ilarity. However, it is applied to the richness d-
feature vectors, and produces one s-feature. 
7 Experimental Evaluation  
We now evaluate the proposed approach and com-
pare it with baselines. All our experiments use the 
SVMperf classifier (Joachims, 2006). 
7.1  Experiment Setup 
Experiment Data: We use a set of reviews and 
their authors/reviewers from Amazon.com as our 
experiment data. We select the authors who have 
posted more than 30 reviews in the book category. 
After cleaning, we have 831 authors, 731 authors 
for training and 100 authors for testing. The num-
bers of reviews in the training and test author set 
are 59256 and 14308, respectively. We use the 
Stanford parser (Klein and Manning, 2003) to gen-
erate the grammar structure of review sentences 
for extracting syntactic d-features. Note that the 
authors here are in fact userids. However, since 
they are randomly selected from a large number of 
userids, the probability that two sampled userids 
belong to the same person is very small. Thus, it 
should be safe to assume that each userid here rep-
resents a unique author.  
Training data: We randomly choose 1 (one) re-
view for each author as the query and all of his/her 
other reviews as q-positive reviews. The q-
negative reviews consist of reviews randomly se-
lected from the other 730 authors, two reviews per 
author. We also tried to use more queries from 
each author, but they make little difference.  
Test data: The test authors are all unseen, i.e., 
their reviews have not been used in training. We 
prepare the test case for each author as follows.  
We first divide the reviews of each author into 
two equal subsets. The purpose is to simulate the 
situation where there are two userids idia and idib 
from the same author ai. Our objective is that giv-
en one userid idia and its query set, we want to find 
the other userid idib from the same author.   
For the review subset of idia (or idib), we ran-
domly select 9 reviews as the query set and anoth-
er 10 reviews as the sample set for the userid. The 
two sets are disjoint. We don?t use more queries or 
sample reviews from each author since in the re-
view domain most authors do not have many re-
views (Jindal and Liu, 2008). In the experiments, 
we will vary the number of test userids, the num-
ber of queries, and the number of samples. We use 
the following format to describe each test data: 
T<n>_Q<n>S<n>, where T denotes the total num-
ber of test userids, Q the query set and S the sam-
ple set, and <n> a number. For example, 
T50_Q9S10 stands for a test data with 50 userids, 
and for each userid, 9 reviews are selected as que-
ries and 10 reviews are selected as samples. * rep-
1130
resents a wildcard whose value we can vary. 
Note that we use this ?artificial? data rather than 
manually labeled data for our experiments because 
it is very hard to reliably label any gold-standard 
data manually in this case. The problem is similar 
to labeling fake reviews. In the fake review detec-
tion research, researchers have manually label fake 
reviews and reviewers (Yoo and Gretzel 2009; 
Lim et al, 2010; Li et al, 2011; Wang et al, 2011). 
However, based on the actual fake reviews written 
using Amazon Mechanical Turk, Ott et al (2011) 
have showed that the accuracy of human labeling 
of fake reviews is very poor. We also believe that 
our test data is realistic for evaluation as we can 
image that the two sets of reviews are from two 
accounts (userids) of the same author (reviewer).   
Two types of experiments: For each author with 
two userids, we conduct two types of tests.   
? Type I: Identify two userids belong to the same 
author. The experiment runs iteratively to test 
every userid. In each iteration, we plant one 
userid of an author in the test set and use the 
other userid of the same author as the query 
userid. That is, in the ith run, the test data con-
sist of the following two components: 
1.  Query userid idia and its query set Qia 
2. Test userids {id1a, ?, id(i-1)a, idib, ?, idma} 
and their corresponding sample review sets 
{S1a, ?, S(i-1)a, Sib, ?, Sma}.  
Note that the query userid idia and the test 
userid idib are from the same author. Our objec-
tive is to use Qia to find idib through Sib. 
Evaluation measure: We use precision, recall, 
and F1 score to evaluate Type I experiments as 
we want to identify all matching pairs. The er-
rors are ?no pair? and ?wrong pair? found.  
? Type II: Type II experiments test the cases 
when no pair exists. That is, we do not plant 
any matching userid for the query userid. Then, 
the algorithm should not find anything. For the 
ith run, the test data has these components: 
1.  Query userid idia and its query set Qia 
2.Test userids {id1a, ?, id(i-1)a, id(i+1)a, ?, idma} 
and their sample review sets {S1a, ?, S(i-1)a, 
S(i+1)a, ?, Sma}. idib is not planted.  
Evaluation measure: Here we cannot use pre-
cision and recall because we are not trying to 
find any pairs. We thus use accuracy as our 
measure. For each idi, if no pair is found, it is 
correct. If a pair is found, it is wrong.  
Baseline methods:  As mentioned eariler, there 
are  only two works that tried to identify multi-id 
users. The first is that in (Chen et al, 2004). 
However, as we discussed in related work, their 
approach is not applicable to reviews. The other is 
that in (Novak et al, 2004), which used clustering 
but assumed that the number of actual authors (or 
clusters) is known. This is unrealistic in practice. 
Thus we designed three new baselines:  
TSL: This baseline is based on the traditional su-
pervised learning (TSL). We use it to evaluate 
how the traditional approach performs in the 
original feature space. In this case, each docu-
ment in TSL has to be represented as a vector of 
d-features or traditional n-gram features. For 
each test userid id, we build a SVM classifier 
based on the one vs. all strategy. That is, for 
training we use id?s queries in T*_Q*S10 as the 
positive documents, and all queries of the other 
test userids (e.g., 99 userids if the test data has 
100 userids) as the negative documents. Note 
that TSL cannot use the 731 userids for training 
as in LSS because they do not appear in the test 
data. In testing, userid id?s sample (non-query) 
documents in T*_Q*S10 are used as positive 
documents, and the sample documents of all oth-
er test userids are used as negative documents. 
SimUG: It uses the word unigrams to compare the 
cosine similarity of queries and samples. Cosine 
similarity with unigrams is the most widely used 
document similarity measure.  
SimAD: It uses all d-features to compare the cosine 
similarity of queries and samples.  
For both SimUG and SimAD, their cosine simi-
larity values are used in place of SVM scores of 
LSS or TSL. We then apply the same 4 strategies 
to decide the final author attribution except voting 
as cosine similarity cannot classify.  
7.2  Results and analysis 
1) Effects of positive/total ratio in training set: 
Since our data is highly skewed and too many neg-
ative cases may not be good for classification, we 
thus performed this experiment to find a good ratio.  
Table 5 shows the results for Type I experiments. 
From Table 5, we can see that the results are high-
ly accurate. Even for 100 userids, our method can 
correctly identify 85% cases. Here we use the data 
sets T*_Q9S10 and the decision method is 
ScoreSqSum, which produces the best result. The 
1131
results for Type II experiments (Table 6) are also 
accurate. In most cases, the values of accuracy are 
higher than 90%. For all our experiments below, 
we use the model/classifier trained with 0.4 ratio. 
Table 5. Positive(p)/total(t) ratio in training (Type I) 
 
F1 
 
p/t 10 30 50 80 100 
0.3 100.00 84.62 86.36 88.89 83.72 
0.4 100.00 91.91 90.11 88.89 85.71 
0.5 100.00 90.91 91.30 88.89 87.01 
0.6 94.74 82.35 87.64 85.71 86.36 
0.7 94.74 84.62 86.36 86.53 87.64 
Table 6. Positive(p)/total(t) ratio in training (Type II) 
 
Accuracy 
 
p/t 10 30 50 80 100 
0.3 90.00 90.00 92.00 97.50 94.00 
0.4 90.00 90.00 94.00 98.75 95.00 
0.5 80.00 86.67 94.00 97.75 95.00 
0.6 80.00 86.67 90.00 93.75 92.00 
0.7 80.00 86.67 90.00 95.00 92.00 
(2) Effects of different decision methods: We 
show the results of the four proposed decision 
methods: Voting, ScoreSum, ScoreSqSum, and 
ScoreMax, using our basic data of T*_Q9S10 with 
varied number of test userids. Figure 5(a) shows 
that ScoreSqSum is the best for Type I experi-
ments. Figure 5(b) shows ScoreMax is the best for 
Type II, but ScoreSqSum also does very well. Be-
low, ScoreSqSum is used as our default method 
because Type I is more important than Type II.  
 
              (a)  Type I                   (b) Type II 
Figure 5: Effect of different decision methods 
(3) Effects of number of queries per userid: 
Figure 6 shows the results of different numbers of 
queries. We see that more queries give better re-
sults, which is easy to understand because more 
queries give more information. We use 9 queries 
per userid in all other experiments. 
 
 (a)  Type I                   (b) Type II 
Figure 6: Effect of different numbers of queries 
(4) Effects of number of samples per userid: We 
tried 2, 4, 6, 8, 10 samples per userid. Although 
there are some fluctuations for Type II (Fig.7(b)), 
we can see an upward trend for Type I in Fig. 7(a). 
This indicates that more sample documents give 
better results in general. The main reason again is 
that more samples from a userid give more identi-
fying information about the userid. We use 10 test 
documents (samples) per userid in all experiments. 
 
 (a)  Type I                   (b) Type II 
Figure 7: Effect of different number of samples 
(5) Impact of individual s-feature sets: Here we 
show the effectiveness of individual s-feature sets. 
From Table 7, we see that Sim7Retrieval s-
features are extremely important for Type I test. 
Removing Sim7Retrieval causes about 10% to 
20% F1 score drop on different datasets. SimCT-
fidf s-features are also useful. The impacts of other 
s-features are small. The same applies to Type II 
test (Table 8). On average, using all features is the 
best. Hence we use all features in all other experi-
ments above.  
Table 7. Using different s-features (Type I) 
T*_Q9S10 F1 
10  
F1 
30 
F1 
50  
F1 
80 
F1 
100  
All features 100.00 90.91 90.11 88.89 85.71 
No Sim4Len 100.00 88.89 86.36 87.32 85.06 
No SimCRichness 100.00 88.89 91.30 88.89 85.71 
No SimCTfidf 100.00 80.00 86.36 86.53 83.72 
No Sim7Retrieval 82.35 72.34 75.80 78.79 77.30 
No Sim3Sent 94.74 84.62 86.36 88.11 87.64 
Table 8. Using different s-features (Type II) 
T*_Q9S10 Acc. 
10  
Acc 
30 
Acc. 
50  
Acc 
80 
Acc. 
100  
All features 90.00 90.00 94.00 98.75 99.00 
No Sim4Len 90.00 93.33 96.00 96.25 96.00 
No SimCRichness 90.00 90.00 94.00 96.25 96.00 
No SimCTfidf 90.00 86.67 94.00 93.75 97.00 
No Sim7Retrieval 80.00 90.00 94.00 94.00 96.00 
No Sim3Sent 90.00 93.33 92.00 98.75 93.00 
(6) Comparing with the three baselines: Similar 
to our method, the training data for TSL is highly 
skewed as it uses a one-vs.-all strategy. Hence we 
also investigate the effect of p/t ratio in training for 
TSL. Results show that 0.4 ratio is the best setting. 
1132
Thus this setting is adopted for TSL in the follow-
ing experiments. Note that we cannot conduct p/t 
ratio experiments for SimAD and SimUG as they 
are unsupervised methods. We use ScoreMax for 
TSL, ScoreSqSum for SimUG and SimAD, re-
spectively, since they perform the best for their 
corresponding approaches. Tables 9 and 10 show 
the results of our LSS method and the baseline 
methods for Type I and II tests respectively. For 
TSL, we use all d-features. Unigram features gave 
TSL much worse results and are thus not included 
here.  
Table 9: Comparison with baselines (Type I) 
 10 30 50 80 100 
LSS Pre 100.00 100.00 100.00 100.00 98.68 
Rec 100.00 83.33 82.00 80.00 75.76 
F1 100.00 90.91 90.11 88.89 85.71 
TSL Pre 50.00 50.00 33.33 0.00 0.00 
Rec 11.11 3.45 2.08 0.00 0.00 
F1 18.18 6.45 3.92 0.00 0.00 
SimUG Pre 100.00 100.00 100.00 100.00 100.00 
Rec 70.00 46.67 48.00 48.75 43.00 
F1 82.35 63.64 64.86 65.55 60.14 
SimAD Pre 100.00 75.00 100.00 33.33 0.00 
Rec 20.00 10.35 2.00 1.28 0.00 
F1 33.33 18.18 3.92 2.47 0.00 
Table 10: Comparison with baselines (Type II) 
Accuracy 10 30 50 80 100 
LSS 90.00 90.00 94.00 98.75 95.00 
TSL 90.00 96.67 98.00 98.75 99.00 
SimUG 96.00 93.33 96.00 96.25 97.00 
SimAD 90.00 96.67 98.00 98.75 99.00 
From Tables 9 and 10, we can make the follow-
ing observations. 
? For Type I, F1 scores of LSS are markedly bet-
ter than those of the three baselines. The results 
of SimUG also drop more quickly than LSS 
with the increased number of userids. SimAD?s 
results are extremely poor. These show that 
LSS is much more superior to the unsupervised 
methods. TSL performed the worst, indicating 
that traditional supervised learning is inappro-
priate for this task. There are two main reasons: 
First, for one vs. all learning, the negative train-
ing data actually contain positive documents 
which are written by the same author using an-
other userid as the positive data, which confus-
es the classifier. Second, TSL is unable to build 
an accurate classifier using the small number of 
queries (which are training data). In contrast, 
our LSS method can exploit a large number of 
other authors who do not have to appear in test-
ing and thus achieves the huge improvements.  
? For Type II, LSS also performs very well. The 
baselines perform well too and even better, 
which is not surprising because they have diffi-
culty in finding matching pairs for Type I. 
Since Type II datasets have no author with mul-
tiple userids, naturally the baselines will do 
well for Type II. But that is useless because 
when there are authors with multiple usersids 
(Type I), they are unable to find them well.  
In summary, we can conclude that for Type I tests 
(there are authors with multiple userids), LSS is 
dramatically better than all baseline methods. For 
Type II tests (there is no author with multiple 
userids), it also performs very well.   
8 Conclusion  
This paper proposed a novel method to identify 
userids that may be from the same author. The 
core of the method is a supervised learning method 
which learns in a similarity space rather than the 
document space. This learning method is able to 
better determine whether a document may be writ-
ten by a known author, although no document 
from the author has been used in training (as long 
as we have some documents from the author to 
serve as queries). To the best of our knowledge, 
there is no existing method based on linguistic 
analysis for solving the problem. Our experimental 
results based on a large number of reviewers and 
their reviews show that the proposed algorithm is 
highly accurate. It outperforms three baselines 
markedly.  
 
Acknowledgements 
We are grateful to the anonymous reviewers for 
their thoughtful comments. Tieyun Qian was sup-
ported in part by the NSFC Projects (61272275, 
61272110, 61202036), and the 111 Project 
(B07037). Bing Liu was supported in part by a 
grant from National Science Foundation (NSF) 
under no. IIS-1111092. 
 
References  
Shlomo Argamon and Shlomo Levitan. 2004. 
Measuring the usefulness of function words for 
authorship attribution. Literary and Linguistic 
Computing 1-3. 
1133
Shlomo Argamon, Casey Whitelaw, Paul Chase, 
Sobhan Raj Hota, Navendu Garg, and Shlomo 
Levitan. 2007. Stylistic text classification using 
functional lexical features: Research articles. J. 
Am. Soc. Inf. Sci. Technol. 58:802-822.  
John F. Burrows. 1992. Not unless you ask nicely: 
The interpretative nexus between analysis and 
information. Literary and Linguistic Computing 
7:91-109. 
Yunbo Cao, Jun Xu, Tie-Yan Liu, Hang Li, Yalou 
Huang, and Hsiao-Wuen Hon. 2006. Adapting 
ranking svm to document retrieval. Proc. of 
SIGIR, Pages 186-193. 
Hung-Ching Chen, Mark K. Goldberg, Malik 
Magdon-Ismail. 2004. Identifying multi-ID users 
in open forums. Intelligence and Security 
Informatics, Pages 176-186. 
Joachim Diederich, J?rg Kindermann, Edda 
Leopold, and Gerhard Paass, 2000. Authorship 
attribution with support vector machines. 
Applied Intelligence 19:109-123. 
Hugo Jair Escalante, Thamar Solorio, and Manuel 
Montes-y-G?mez. 2011. Local histograms of 
character n-grams for authorship attribution. 
Proc. of ACL-HLT, Volume I: 288-298. 
Song Feng, Longfei Xing, Anupam Gogar, and 
Yejin Choi. 2012. Distributional Footprints of 
Deceptive Product Reviews.  Proc. of ICWSM.  
Michael Gamon. 2004. Linguistic correlates of 
style: authorship classification with deep 
linguistic analysis features. Proc. of Coling. 
Neil Graham, Graeme Hirst, and Bhaskara Marthi. 
2005. Segmenting documents by stylistic 
character. Natural Language Engineering, 
11:397-415.  
Jack Grieve. 2007. Quantitative authorship 
attribution: An evaluation of techniques. Literary 
and Linguistic Computing 22:251-270. 
Hans van Halteren, Fiona Tweedie, and Harald 
Baayen. 1996. Outside the cave of shadows: 
using syntactic annotation to enhance authorship 
attribution. Literary and Linguistic Computing 
11:121-132. 
Steffen Hedegaard and Jakob Grue Simonsen. 
2011. Lost in translation: authorship attribution 
using frame semantics. Proc. of ACL-HLT, short 
papers - Volume 2, 65-70. 
Graeme Hirst and Ol?ga Feiguina. 2007. Bigrams 
of syntactic labels for authorship discrimination 
of short texts. Literary and Linguistic Computing 
22:405-417. 
David I. Holmes and R. S. Forsyth. 1995. The 
Federalist Revisited: New Directions in 
Authorship Attribution, Literary and Linguistic 
Computing, 10(2): 111-127. 
David L. Hoover. 2001. Statistical stylistics and 
authorship attribution: an empirical 
investigation. Literary and Linguistic Computing 
16:421-424. 
Nitin Jindal and Bing Liu. 2008. Opinion Spam 
and Analysis. Proc. of WSDM, California, USA. 
Thorsten Joachims. 2006. Training linear svms in 
linear time. Proc. of KDD. 
Sangkyum Kim, Hyungsul Kim, Tim Weninger, 
Jiawei Han, and Hyun Duk Kim. 2011. 
Authorship classification: a discriminative 
syntactic tree mining approach. Proc. of SIGIR, 
Pages 455-464. 
Dan Klein, and Christopher D. Manning. 2003. 
Accurate unlexicalized parsing. Proc. of ACL, 
423-430.  
Moshe Koppel and Jonathan Schler. 2004. 
Authorship verification as a one-class 
classification problem. Proc. of ICML.  
Moshe Koppel, Jonathan Schler, Shlomo 
Argamon. 2011. Authorship attribution in the 
wild. Lang Resources & Evaluation, 45:83-94 
Fangtao Li, Minlie Huang, Yi Yang and Xiaoyan 
Zhu. 2011. Learning to identify review Spam. 
Proc. of IJCAI. 
Hang Li. 2011. Learning to Rank for Information 
Retrieval and Natural Language Processing. 
Morgan & Claypool publishers. 
Jiexun Li, Rong Zheng, and Hsinchun Chen. 2006. 
From fingerprint to writeprint. Communications 
of the ACM, 49:76-82. 
Ee-Peng Lim, Viet-An Nguyen, Nitin Jindal, Bing 
Liu, Hady W. Lauw. 2010. Detecting product 
review spammers using rating behaviors. Proc. 
of CIKM, 2010. 
Bing Liu. 2012. Sentiment Analysis and Opinion 
Mining, Morgan & Claypool publishers.  
1134
Tieyan Liu. 2011. Learning to Rank for Infor-
mation Retrieval. Springer. 
Kim Luyckx, Walter Daelemans. 2008. Authorship 
Attribution and Verification with Many Authors 
and Limited Data. Proc. of Coling, pages 513-
520. 
David Madigan, Alexander Genkin, David D. 
Lewis, Shlomo Argamon, Dmitriy Fradkin, and 
Li Ye. 2005. Author Identification on the Large 
Scale. Proc. of CSNA.  
Donald Metzler, Yaniv Bernstein, W. Bruce Croft, 
Alistair Moffat, and Justin Zobel. 2005. 
Similarity measures for tracking information 
flow. Proc. of CIKM. Pages 517-524. 
Frederick Mosteller, David Lee Wallace. 1964. 
Inference and disputed authorship: The 
Federalist. Addison-Wesley.  
Arjun Mukherjee, Bing Liu, and Natalie Glance. 
2012. Spotting Fake Reviewer Groups in Con-
sumer Reviews. Proc. of WWW, Pages 191-200. 
Arvind Narayanan, Hristo Paskov, Neil Zhenqiang 
Gong, et al 2012. On the feasibility of internet-
scale author identification. Proceedings of the 
2012 IEEE Symposium on Security and Privacy. 
Pages 300-314 
Jasmine Novak, Prabhakar Raghavan, Andrew 
Tomkins. 2004. Anti-aliasing on the web. Proc. 
of WWW, Pages 30-39 
Myle Ott, Yejin Choi, Claire Cardie, Jeffrey T. 
Hancock. 2011. Finding Deceptive Opinion 
Spam by Any Stretch of the Imagination. Proc. 
of ACL. 
Myle Ott, Claire Cardie, Jeffrey T. Hancock. 2012. 
Estimating the prevalence of deception in online 
review communities. Proc. of WWW. 
Fuchun Peng, Dale Schuurmans, Shaojun Wang, 
and Vlado Keselj. 2003. Language independent 
authorship attribution using character level 
language models. Proc. of EACL, Pages 267-
274. 
Conrad Sanderson and Simon Guenter. 2006. 
Short text authorship attribution via sequence 
kernels, markov chains and author unmasking: 
an investigation. Proc. of EMNLP, Pages 482-
491. 
Yanir Seroussi, Fabian Bohnert, Ingrid Zukerman. 
2012. Authorship Attribution with Author-
aware Topic Models. Proc. of ACL, 2:264-269. 
Thamar Solorio, Sangita Pillay, Sindhu Raghavan, 
Manuel Montes y G?omez. 2011. Modality 
Specific Meta Features for Authorship 
Attribution in Web Forum Posts. Proc. of 
IJCNLP, Pages 156-164.  
Efstathios Stamatatos. 2009. A Survey of Modern 
Authorship Attribution Methods. Journal of the 
American Society for Information Science and 
Technology, 60(3):538-556, Wiley. 
Efstathios Stamatatos, George Kokkinakis, and 
Nikos Fakotakis. 2000. Automatic text 
categorization in terms of genre and author. 
Comput. Linguist. 26:471-495. 
?zlem Uzuner and Boris Katz. 2005. A 
comparative study of language models for book 
and author recognition. Proc. of IJCNLP, Pages 
969-980.  
Vladimir N. Vapnik. 1998. Statistical Learning 
Theory. Wiley-Interscience, NY.  
O. de Vel, A. Anderson, M. Corney and G. 
Mohay. 2001. Mining Email Content for Author 
Identification Forensics. Sigmod Record, 30:55-
64. 
Kyung-Hyan Yoo and Ulrike Gretzel. 2009. 
Comparison of Deceptive and Truthful Travel 
Reviews. Information and Communication 
Technologies in Tourism, Pages 37-47. 
Georgy Udnv Yule. 1944. The statistical study of 
literary vocabulary. Cambridge University 
Press. 
Guan Wang, Sihong Xie, Bing Liu, Philip S. Yu. 
2011. Review Graph based Online Store 
Review Spammer Detection. Proc. of ICDM. 
Ying Zhao and Justin Zobel. 2005. Effective and 
scalable authorship attribution using function 
words. Proceeding of Information Retrival 
Technology, Pages 174-189.  
Rong Zheng, Jiexun Li, Hsinchun Chen, and Zan 
Huang. 2006. A framework for authorship iden-
tification of online messages: Writing style fea-
tures and classification techniques. Journal of 
the American Society of Information Science 
and Technology 57:378-393. 
1135
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 345?351,
Baltimore, Maryland, USA, June 23-25 2014. c?2014 Association for Computational Linguistics
Tri-Training for Authorship Attribution with Limited Training Data 
 
Tieyun Qian 
State Key Laboratory 
of Software Eng., 
Wuhan University 
430072, Hubei, China 
qty@whu.edu.cn 
Bing Liu 
Dept. of Computer Sci-
ence, Univ. of Illinois at 
Chicago 
IL, USA, 60607 
liub@cs.uic.edu 
Li Chen 
State Key Laboratory of 
Software Eng., 
Wuhan University 
430072, Hubei, China 
ccnuchenli@163.com 
Zhiyong Peng 
Computer School, 
Wuhan University 
430072, Hubei, China 
peng@whu.edu.cn 
 
  
 
Abstract 
Authorship attribution (AA) aims to identify 
the authors of a set of documents. Traditional 
studies in this area often assume that there are 
a large set of labeled documents available for 
training. However, in the real life, it is often 
difficult or expensive to collect a large set of 
labeled data. For example, in the online review 
domain, most reviewers (authors) only write a 
few reviews, which are not enough to serve as 
the training data for accurate classification. In 
this paper, we present a novel three-view tri-
training method to iteratively identify authors 
of unlabeled data to augment the training set. 
The key idea is to first represent each docu-
ment in three distinct views, and then perform 
tri-training to exploit the large amount of un-
labeled documents. Starting from 10 training 
documents per author, we systematically eval-
uate the effectiveness of the proposed tri-
training method for AA. Experimental results 
show that the proposed approach outperforms 
the state-of-the-art semi-supervised method 
CNG+SVM and other baselines.  
1 Introduction 
Existing approaches to authorship attribution 
(AA) are mainly based on supervised classifica-
tion (Stamatatos, 2009, Kim et al, 2011, Serous-
si et al, 2012). Although this is an effective ap-
proach, it has a major weakness, i.e., for each 
author a large number of his/her articles are 
needed as the training data. This is possible if the 
author has written a large number of articles, but 
will be difficult if he/she has not. For example, in 
the online review domain, most authors (review-
ers) only write a few reviews (documents). It was 
shown that on average each reviewer only has 
2.72 reviews in amazon.com, and only 8% of the 
reviewers have at least 5 reviews (Jindal and Liu, 
2008). The small number of labeled documents 
makes it extremely challenging for supervised 
learning to train an accurate classifier. 
In this paper, we consider AA with only a few 
labeled examples. By exploiting the redundancy 
in human languages, we tackle the problem using 
a new three-view tri-training algorithm (TTA). 
Specifically, we first represent each document in 
three distinct views, and then tri-train three clas-
sifiers in these views. The predictions of two 
classifiers on unlabeled examples are used to 
augment the training set for the third classifier. 
This process repeats until a termination condition 
is met. The enlarged labeled sets are finally used 
to train classifiers to classify the test data.  
To our knowledge, no existing work has ad-
dressed AA in a tri-training framework. The AA 
problem with limited training data was attempted 
in (Stamatatos, 2007; Luyckx and Daelemans, 
2008). However, neither of them used a semi-
supervised approach to augment the training set 
with additional documents. Kourtis and Stama-
tatos (2011) introduced a variant of the self-
training method in (Nigam and Ghani, 2000). 
Note that the original self-training uses one clas-
sifier on one view. However, the self-training 
method in (Kourtis and Stamatatos, 2011) uses 
two classifiers (CNG and SVM) on one view. 
Both the self-training and tri-training are semi-
supervised learning methods. However, the pro-
posed approach is not a simple extension of the 
self-training method CNG+SVM of (Kourtis and 
Stamatatos, 2011). There are key differences. 
First, in their experimental setting, about 115 
and 129 documents per author on average are 
used for two experimental corpora. This number 
of labeled documents is still very large. We con-
sider a much more realistic problem, where the 
size of the training set is very small. Only 10 
samples per author are used in training.  
Second, CNG+SVM uses two learning methods 
on a single character n-gram view. In contrast, 
besides the character n-gram view, we also make 
use of the lexical and syntactic views. That is, 
345
three distinct views are used for building classi-
fiers. The redundant information in human lan-
guage is combined in the tri-training procedure.  
Third, in each round of self-training in 
CNG+SVM, each classifier is refined by the same 
newly labeled examples. However, in the pro-
posed tri-training method (TTA), the examples 
labeled by the classifiers of every two views are 
added to the third view. By doing so, each classi-
fier can borrow information from the other two 
views. And the predictions made by two classifi-
ers are more reliable than those by one classifier. 
The main contribution of this paper is thus the 
proposed three-view tri-training scheme which 
has a much better generalization ability by ex-
ploiting three different views of the same docu-
ment. Experimental results on the IMDb review 
dataset show that the proposed method dramati-
cally improves the CNG+SVM method. It also 
outperforms the co-training method (Blum and 
Mitchell, 1998) based on our proposed views.  
2 Related Work 
Existing AA methods either focused on finding 
suitable features or on developing effective 
techniques. Example features include function 
words (Argamon et al, 2007), richness features 
(Gamon 2004), punctuation frequencies (Graham 
et al, 2005), character (Grieve, 2007), word 
(Burrows, 1992) and POS n-grams (Gamon, 
2004; Hirst and Feiguina, 2007), rewrite rules 
(Halteren et al, 1996), and similarities (Qian and 
Liu, 2013). On developing effective learning 
techniques, supervised classification has been the 
dominant approach, e.g., neural networks 
(Graham et al, 2005; Zheng et al, 2006), 
decision tree (Uzuner and Katz, 2005; Zhao and 
Zobel, 2005), logistic regression (Madigan et al, 
2005), SVM (Diederich et al, 2000; Gamon 
2004; Li et al, 2006; Kim et al, 2011), etc. 
The main problem in the traditional research is 
the unrealistic size of the training set. A size of 
about 10,000 words per author is regarded as a 
reasonable training set size (Argamon et al, 
2007, Burrows, 2003). When no long documents 
are available, tens or hundreds of short texts are 
used (Halteren, 2007; Hirst and Feiguina, 2007; 
Schwartz et al, 2013).  
Apart from the existing works dealing with 
limited data discussed in the introduction, our 
preliminary study in (Qian et al, 2014) used one 
learning method on two views, but it is inferior 
to the proposed method in this paper.  
 
3 Proposed Tri-Training Algorithm  
3.1 Overall Framework 
We represent each document in three feature 
views: the character view, the lexical view and 
the syntactic view. Each view consists of a set of 
features in the respective type. A classifier can 
be learned from any of these views. We propose 
a three-view training algorithm to deal with the 
problem of limited training data. Logistic 
regression (LR) is used as the learner. The 
overall framework is shown in Figure 1. 
Given the labeled, unlabeled, and test sets L, 
U, and T, step 1 extracts the character, lexical, 
and syntactic views from L, U, and T, 
respectively. Steps 2-13 iteratively tri-train three 
classifiers by adding the data which are assigned 
the same label by two classifiers into the training 
set of the third classifier. The algorithm first 
randomly selects u unlabeled documents from U 
to create a pool U? of examples. Note that we can 
directly select from the large unlabeled set U. 
However, it is shown in (Blum and Mitchell 
2008) that a smaller pool can force the classifiers 
to select instances that are more representative of 
the underlying distribution that generates U. 
Hence we set the parameter u to a size of about 
1% of the whole unlabeled set, which allows us 
to observe the effects of different number of 
iterations. It then iterates over the following 
steps. First, use character, lexical and syntactic 
views on the current labeled set to train three 
classifiers C1, C2, and C3. See Steps 4-9. Second, 
Input: A small set of labeled documents L = {l1,?, lr}, a large 
set of unlabeled documents U = {u1,?, us}, and a set of test 
documents T = {t1,?, tt}, 
Parameters: the number of iterations k, the size of selected un-
labeled documents u 
Output: tk?s class assignment 
1   Extract views Lc, Ll, Ls, Uc, Ul, Us, Tc, Tl, Ts from L, U, T 
2  Loop for k iterations: 
3  Randomly select u unlabeled documents U' from U; 
4       Learn the first view classifier C1 from L1 (L1=Lc, Ll, or Ls); 
5        Use C1 to label docs in U' based on U1(U1=Uc, Ul, or Us) 
6        Learn the second view classifier C2 from L2 (L2?L1) 
7        Use C2 to label documents in U' based on U2 (U2?U1); 
8        Learn the third view classifier C3 from L3 (L2?L1, L2) 
9        Use C3 to label documents in U' based on U3 (U2?U1, U2); 
10      Up1 = {u | u? U', u.label by C2 = u.label by C3}; 
11      Up2 = {u | u? U', u.label by C1 = u.label by C3}; 
12      Up3 = {u | u? U', u.label by C1 = u.label by C2}; 
13      U = U - U', Li = Li ? Upi (i=1..3);             
14 Learn three classifiers C1, C2, C3 from L1, L2, L3; 
15 Use Ci to label tk in Ti (i=1..3); 
16  Aggregate results from three views 
Figure 1: The tri-training algorithm (TTA) 
346
allow two of these three classifiers to classify the 
unlabeled set U? and choose p documents with 
agreed labels. See Steps 10-12. The selected 
documents are then added to the third labeled set 
for the label assigned (a label is an author here), 
and the u documents are removed from the 
unlabeled pool U? (line 13). We call this way of 
augmenting the training sets InterAdding. The 
one used in (Kourtis and Stamatatos, 2011) is 
called SelfAdding as it uses only a single view 
and adds to the same training set. Steps 14-15 
assign the test document to a category (author) 
using the classifier learned from the three views 
in the augmented labeled data, respectively. Step 
16 aggregates the results from three classifiers. 
3.2 Character View 
The features in the character view are the 
character n-grams of a document. Character n-
grams are simple and easily available for any 
natural language. For a fair comparison with the 
previous work in (Kourtis and Stamatatos, 2011), 
we extract frequencies of 3-grams at the 
character-level. The vocabulary size for character 
3-grams in our experiment is 28584.  
3.3 Lexical View 
The lexical view consists of word unigrams of a 
document. We represent each article by a vector 
of word frequencies. The vocabulary size for 
unigrams in our experiment is 195274.  
3.4 Syntactic View 
The syntactic view consists of the syntactic 
features of a document. We use four content-
independent structures including n-grams of POS 
tags (n = 1..3) and rewrite rules (Kim et al, 
2011). The vocabulary sizes for POS 1-grams, 
POS 2-grams, POS 3-grams, and rewrite rules in 
our experiment are 63, 1917, 21950, and 19240, 
respectively. These four types of syntactic 
structures are merged into a single vector. Hence 
the syntactic view of a document is represented 
as a vector of 43140 components. 
3.5 Aggregating Results from Three Views 
In testing, once we obtain the prediction values 
from three classifiers for a test document tk, an 
additional algorithm is used to decide the final 
author attribution. One simple method is voting. 
However, this method is weaker than the three 
methods below. It is also hard to compare with 
the self-training method CNG+SVM in (Kourtis 
and Stamatatos, 2011) as it only has two classifi-
ers. Hence we present three other strategies to 
further aggregate the results from the three 
views. These methods require the classifier to 
produce a numeric score to reflect the positive or 
negative certainty. Many classification algo-
rithms give such scores, e.g., SVM and logistic 
regression. The three methods are as follows:  
1)  ScoreSum: The learned model first classifies 
all test cases in T. Then for each test case tk, 
this method sums up all scores of positive 
classifications from the three views. It then 
assigns tk to the author with the highest score. 
2)  ScoreSqSum: This method works similarly to 
ScoreSum above except that it sums up the 
squared scores of positive classifications. 
3)  ScoreMax: This method works similarly to the 
ScoreSum method as well except that it finds 
the maximum classification score for each test 
document. 
4 Experimental Evaluation  
We now evaluate the proposed method. We use 
logistic regression (LR) with L2 regularization 
(Fan et al, 2008) and the SVMmulticlass (SVM) 
system (Joachims, 2007) with its default settings 
as the classifiers.  
4.1 Experiment Setup 
We conduct experiments on the IMDb dataset 
(Seroussi et al, 2010). This data set contains the 
IMDb reviews in May 2009. It has 62,000 re-
views by 62 users (1,000 reviews per user). For 
each author/reviewer, we further split his/her 
documents into the labeled, unlabeled, and test 
sets. 1% of one author?s documents, i.e., 10 doc-
uments per author, are used as the labeled data 
for training, 79% are used as unlabeled data, and 
the rest 20% are used for testing. We extract and 
compute the character and lexical features direct-
ly from the raw data, and use the Stanford PCFG 
parser (Klein and Manning, 2003) to generate the 
grammar structures of sentences in each review 
for extracting syntactic features. We normalize 
each feature?s value to the [0, 1] interval by di-
viding the maximum value of this feature in the 
training set. We use the micro-averaged classifi-
cation accuracy as the evaluation metric. 
4.2 Baseline methods 
We use six self-training baselines and three co-
training baselines. Self-training in (Kourtis and 
Stamatatos, 2011) uses two different classifiers 
on one view, and co-training uses one classifier 
on two views. All baselines except CNG+SVM 
347
on the character view are our extensions. 
Self-training using CNG+SVM on character, 
lexical and syntactic views respectively: This 
gives three baselines. It self-trains two classifi-
ers from the character 3-gram, lexical, and syn-
tactic views using CNG and SVM classifiers 
(Kourtis and Stamatatos, 2011). CNG is a pro-
file-based method which represents the author 
as the N most frequent character n-grams of all 
his/her training texts. The original method ap-
plied only CNG and SVM on the character n-
gram view. Since our results show that its per-
formance is extremely poor, we are curious 
what the reason is. Can this be due to the clas-
sifier or to the view? In order to differentiate 
the effects of views and classifiers, we present 
two additional types of baselines. The first type 
is to extend CNG+SVM method to lexical and 
syntactic views as well. The second type is to 
extend CNG+SVM method by replacing CNG 
with LR to show a fair comparison with our 
framework.  
Self-training using LR+SVM on character, lexi-
cal, and syntactic views: This is the second 
type extension. It also gives us three baselines. 
It again uses the character, lexical and syntac-
tic view and SVM as one of the two classifiers. 
The other classifier uses LR rather than CNG.  
Co-training using LR on Char+Lex, Char+Syn, 
and Lex+Syn views: This also gives us three 
baselines. Each baseline co-trains two classifi-
ers from every two views of the character 3-
gram, lexical, and syntactic views. 
4.3 Results and analysis 
(1) Effects of learning algorithms  
We first evaluate the effects of learning algo-
rithms on tri-training. We use SVM and LR as 
the learners as they are among the best methods.  
Figure 2. Effects of SVM and LR on tri-training 
The effects of SVM and LR on tri-training are 
shown in Fig. 2. For the aggregation results, we 
draw the curves for ScoreSum. The results for 
other two stratigies are similar. It is clear that LR 
outperforms SVM by a large margin for tri-
training when the number of iterations (k) is 
small. One possible reason is that LR is more 
tolerant to over-fitting caused by the small 
number of training samples. Hence, we use LR 
for tri-training in all experiments. 
(2) Effects of aggregation strategies  
We show the effects of the three proposed 
aggregation strategies. Table 1 indicates that 
ScoreSum (SS) is the best.  
Table 1. Effects of three aggregation strategies: 
ScoreMax(SM), ScoreSum(SS), and ScoreSq-Sum(SQ) 
We also observe that both ScoreSum and 
ScoreSqSum (SQ) perform better than ScoreMax 
(SM) and all single view cases. This suggests 
that the decision made from a number of scores 
is much more reliable than that made from only 
one score. ScoreSum is our default strategy. 
(3) Effects of data augmenting strategies  
We now see the effects of data adding methods 
to augment the labeled set in Fig. 3.  
 
Figure 3. Effects of data augmenting methods on 
tri-training 
We use two strategies. One is our InterAdding 
approach and the other is the SelfAdding 
approach in (Kourtis and Stamatatos, 2011), as 
introduced in Section 3.1. We can see that by 
adding newly classified samples by two 
classifiers to the third view, tri-training gets 
better and better results rapidly. For example, the 
accuracy for k = 10 iterations grows from 61.24 
for SelfAdding to 78.82 for InterAdding, an 
absolute increase of 17.58%. This implies that by 
integrating more information from other views, 
learning can improve greatly.  
(4) Comparison with self-training baselines 
We show the results of CNG+SVM in Table 2. It 
is clear that CNG is almost unable to correctly 
k 
Single  View Results Aggregated Results 
Lex Char Syn SM SS SQ 
0 45.75 32.88 33.96 41.11 46.85 44.61 
10 74.63 66.05 56.99 73.41 78.82 76.41 
20 82.30 74.92 65.05 81.63 86.19 84.05 
30 86.86 79.12 68.85 85.29 89.69 87.74 
40 89.16 81.81 70.85 87.83 91.52 89.99 
50 90.56 83.14 72.06 89.11 92.58 91.17 
60 91.69 84.13 73.23 90.05 93.15 91.82 
348
classify any test case. Its accuracy is only 1.26% 
at the start. This directly leads to the failure of 
the self-training. The reason is that the other 
classifier SVM can augment nearly 0 documents 
from the unlabeled set. We also tuned the param-
eter N for CNG, but it makes little difference. 
k 
Self-Training on Char  Aggregated Results 
CNG SVM SM SS SQ 
0 1.26 33.22 32.35 32.47 27.00 
10 1.26 32.35 32.35 32.47 27.00 
20 1.26 32.35 32.35 32.47 27.00 
30 1.26 32.35 32.35 32.47 27.00 
40 1.26 33.60 33.60 33.69 29.07 
50 1.26 33.60 33.60 33.69 29.07 
60 1.27 33.54 33.60 33.69 29.07 
Table 2. Results for the CNG+SVM baseline 
To distinguish the effects of views from classi-
fiers, we conduct two more types of experiments. 
First, we apply CNG+SVM to the lexical and 
syntactic views. The results are even worse. Its 
accuracy drops to 0.58% and 1.21%, respectively. 
Next, we replace CNG with LR and apply 
LR+SVM to all three views. We only show their 
best results in Table 3, either on a single view or 
aggregation. The details are omitted due to space 
limitations. We can see significant improvements 
over their corresponding results of CNG+SVM. 
This demonstrates that the learning methods are 
critical to self-training as well.  
k Tri 
Train 
SelfTrain:CNG+SVM SelfTrain:LR+SVM 
Char lex Syn Char Lex Syn 
0 46.85 33.22 45.44 34.50 33.22 45.75 34.48 
10 78.82 32.47 45.44 34.50 62.56 73.78 51.94 
20 86.19 32.47 45.44 34.09 71.21 81.44 59.88 
30 89.69 32.47 45.44 34.09 75.21 84.68 63.70 
40 91.52 33.69 45.44 34.09 77.46 88.25 65.74 
50 92.58 33.69 45.44 34.09 78.64 88.25 67.45 
60 93.15 33.69 45.44 34.09 79.54 89.31 68.37 
Table 3. Self-training variations 
From Table 3, we can also see that our tri-
training approach outperforms all self-training 
baselines by a large margin. For example, the 
accuracy for LR+SVM on the lexical view is 
89.31%.Although this is the best for self-training, 
it is worse than 93.15% of tri-training.  
The reason that self-training does not work 
well in general is the following: When the train-
ing set is small, the available data may not reflect 
the true distribution of the whole data. Then clas-
sifiers will be biased and their classifications will 
be biased too. In testing, the biased classifiers 
will not have good accuracy. However, in tri-
training, and co-training, each individual view 
may be biased but the views are independent. 
Then each view is more likely to produce ran-
dom samples for the other views and thus reduce 
the bias of each view as the iterations progress.  
(5) Comparison with co-training baselines 
We now compare tri-training with co-training 
(Blum and Mitchell, 1998) in Table 4. Again, tri-
training beats co-training consistently. The best 
performance of co-training is 92.81% achieved 
on the character and lexical views after 60 itera-
tions. However, the accuracy is worse than that 
of tri-training. The key reason is that tri-training 
considers three views, while co-training uses on-
ly two. Also, the predictions by two classifiers 
are more reliable than those by one classifier. 
k Tri 
Train 
Co-Train 
Char+Lex Char+Syn Lex+Syn 
0 46.85 45.75 42.02 45.75 
10 78.82 78.84 75.89 78.85 
20 86.19 86.02 82.59 85.63 
30 89.69 89.32 85.77 88.98 
40 91.52 91.14 87.52 91.16 
50 92.58 92.19 88.46 92.02 
60 93.15 92.81 89.21 92.50 
Table 4. Co-training vs. tri-training 
In (Qian, et al, 2014), we systematically inves-
tigated the effects of learning methods and views 
using a special co-training approach with two 
views. Learning was applied on two views but 
the data augmentation method was like that in 
self-training. The best result there was 91.23%, 
worse than 92.81% of co-training here in Table 4, 
which is worse than 93.15% of Tri-Training.   
Overall, Tri-training performs the best and co-
training is better than self-training and co-self-
training. This indicates that learning on different 
views can better exploit the redundancy in texts 
to achieve superior classification results. 
5 Conclusion  
In this paper, we investigated the problem of au-
thorship attribution with very few labeled exam-
ples. A novel three-view tri-training method was 
proposed to utilize natural views of human lan-
guages, i.e., the character, lexical and syntactic 
views, for classification. We evaluated the pro-
posed method and compared it with state-of-the-
art baselines. Results showed that the proposed 
method outperformed all baseline methods.  
Our future work will extend the work by in-
cluding more views such as the stylistic and vo-
cabulary richness views. Additional experiments 
will also be conducted to determine the general 
behavior of the tri-training approach. 
Acknowledgements 
This work was supported in part by the NSFC 
projects (61272275, 61232002, 61379044), and 
the 111 project (B07037).  
349
References  
S. Argamon, C. Whitelaw,  P. Chase, S. R. Hota,  N. 
Garg, and S. Levitan. 2007. Stylistic text 
classification using functional lexical features. 
JASIST 58, 802?822 
 
A. Blum and T. Mitchell. 1998. Combining labeled 
and unlabeled data with co-training. In: COLT. pp. 
92?100  
 
J. Burrows. 1992. Not unless you ask nicely: The 
interpretative nexus between analysis and 
information. Literary and Linguistic Computing 
7:91-109. 
 
J. Burrows. 2007. All the way through: Testing for 
authorship in different frequency data. LLC 22, 
27?47 
  
R-E. Fan, K-W. Chang,   C-J. Hsieh,  X-R. Wang, and 
C-J. Lin. 2008. Liblinear: A library for large linear 
classification. JMLR 9, 1871?1874 
 
J. Diederich, J. Kindermann, E. Leopold, G. Paass, G. 
F. Informationstechnik, and D-S. Augustin. 2000. 
Authorship attribution with support vector 
machines. Applied Intelligence 19:109-123. 
 
M. Gamon. 2004. Linguistic correlates of style: 
authorship classification with deep linguistic 
analysis features. In COLING. 
 
N. Graham, G. Hirst, and B. Marthi. 2005. 
Segmenting documents by stylistic character. 
Natural Language Engineering, 11:397-415.  
 
J. Grieve. 2007. Quantitative authorship attribution: 
An evaluation of techniques. LLC 22:251-270. 
 
H. van Halteren, F. Tweedie, and H. Baayen. 1996. 
Outside the cave of shadows: using syntactic 
annotation to enhance authorship attribution.  
Literary and Linguistic Computing 11:121-132. 
 
H. van Halteren. 2007. Author verification by 
linguistic profiling: An exploration of the 
parameter space. TSLP 4, 1?17 
 
G. Hirst, and O. Feiguina. 2007. Bigrams of syntactic 
labels for authorship discrimination of short texts. 
LLC 22, 405?417  
 
N. Jindal and B. Liu. 2008. Opinion spam and 
analysis. In: WSDM. pp. 29?230 
 
T. Joachims. 2007. www.cs.cornell.edu/people 
/tj/svmlight/old/svmmulticlassv2.12.html  
 
S. Kim,  H. Kim,  T. Weninger,  J. Han, and H. D. 
Kim. 2011. Authorship classification: a 
discriminative syntactic tree mining approach. In: 
SIGIR. pp. 455?464 
  
D. Klein and C. D. Manning. 2003 Accurate 
unlexicalized parsing. In: ACL. pp. 423?430  
 
I. Kourtis and E. Stamatatos, 2011. Author 
identification using semi-supervised learning. In: 
Notebook for PAN at CLEF 2011  
 
J. Li, R. Zheng, and H. Chen. 2006. From fingerprint 
to writeprint. Communications of the ACM 49:76-
82. 
 
K. Luyckx and W. Daelemans, 2008. Authorship 
attribution and verification with many authors and 
limited data. In: COLING. pp. 513?520 
  
D. Madigan, A. Genkin, D. Lewis, A. Argamon, D. 
Fradkin, and L. Ye, 2005. Author Identification on 
the Large Scale. In CSNA. 
 
K. Nigam and R. Ghani. 2000. Analyzing the 
effectiveness and applicability  of co-training.  In 
Proc. of CIKM, pp.86?93  
 
T. Qian, B. Liu. 2013 Identifying Multiple Userids of 
the Same Author. EMNLP, pp. 1124-1135 
 
T. Qian, B. Liu, M. Zhong, G. He. 2014. Co-Training 
on Authorship Attribution with Very Few Labeled 
Examples: Methods. vs. Views. In SIGIR, to 
appear. 
 
R. Schwartz, O. Tsur, A. Rappoport, M. Koppel. 2013. 
Authorship Attribution of Micro-Messages. 
EMNLP. pp. 1880-1891 
 
Y. Seroussi, F. Bohnert and Zukerman,.2012. 
Authorship attribution with author-aware topic 
models. In: ACL. pp. 264?269  
 
Y. Seroussi,  I. Zukerman, and F. Bohnert. 2010. 
Collaborative inference of sentiments from texts. 
In: UMAP. pp. 195?206 
 
E. Stamatatos. 2007. Author identification using 
imbalanced and limited training texts. In: TIR. pp. 
237?241 
 
E. Stamatatos. 2009. A survey of modern authorship 
attribution methods. JASIST 60:538?556 
 
?. Uzuner and B. Katz. 2005. A comparative study of 
language models for book and author recognition. 
Proc. of the 2nd IJCNLP, 969-980. 
350
 Y. Zhao and J. Zobel. 2005. Effective and scalable 
authorship attribution using function words. In 
Proc. of Information Retrival Technology, 174-
189.  
 
R. Zheng, J. Li, H. Chen, and Z. Huang. 2006. A 
framework for authorship identification of online 
messages: Writing style features and classification 
techniques. JASIST 57:378-393. 
351
