	

		
	
Robust Knowledge Discovery from Parallel Speech and
Text Sources
F. Jelinek, W. Byrne, S. Khudanpur, B. Hladka?. CLSP, Johns Hopkins University, Baltimore, MD.
H. Ney, F. J. Och. RWTH Aachen University, Aachen, Germany
J. Cur???n. Charles University, Prague, Czech Rep.
J. Psutka. University of West Bohemia, Pilsen, Czech Rep.
1. INTRODUCTION
As a by-product of the recent information explosion, the same
basic facts are often available from multiple sources such as the In-
ternet, television, radio and newspapers. We present here a project
currently in its early stages that aims to take advantage of the re-
dundancies in parallel sources to achieve robustness in automatic
knowledge extraction.
Consider, for instance, the following sampling of actual news
from various sources on a particular day:
CNN: James McDougal, President Bill Clinton?s former business
partner in Arkansas and a cooperating witness in the White-
water investigation, died Sunday while serving a federal prison
term. He was 57.
MSNBC: Fort Worth, Texas, March 8. Whitewater figure James
McDougal died of an apparent heart attack in a private com-
munity hospital in Fort Worth, Texas, Sunday. He was 57.
ABC News: Washington, March 8. James McDougal, a key figure
in Independent Counsel Kenneth Starr?s Whitewater investi-
gation, is dead.
The Detroit News: Fort Worth. James McDougal, a key witness
in Kenneth Starr?s Whitewater investigation of President Clin-
ton and First Lady Hillary Rodham Clinton, died of a heart
attack in a prison hospital Sunday. He was 57.
San Jose Mercury News: James McDougal, the wily Arkansas
banking rogue who drew Bill Clinton and Hillary Rodham
Clinton into real estate deals that have come to haunt them,
died Sunday of cardiac arrest just months before he hoped to
be released from prison. He was 57.
The Miami Herald: Washington. James McDougal, the wily
Arkansas financier and land speculator at the center of the
original Whitewater probe against President Clinton, died
Sunday.
.
Story
Alignment
Speech
RecognitionSpeech Sources
Basic Models:
acoustic
lexical
language
Topic specific
acoustic and language
models
stories
Aligned Sentence
retrieval
Ranked
Answers
Query
Text sources
Figure 1: Information Flow in Alignment and Extraction
We propose to align collections of stories, much like the exam-
ple above, from multiple text and speech sources and then develop
methods that exploit the resulting parallelism both as a tool to im-
prove recognition accuracy and to enable the development of sys-
tems that can reliably extract information from parallel sources.
Our goal is to develop systems that align text sources and rec-
ognize parallel speech streams simultaneously in several languages
by making use of all related text and speech. The initial systems
we intend to develop will process each language independently.
However, our ultimate and most ambitious objective is to align text
sources and recognize speech using a single, integrated multilin-
gual ASR system. Of course, if sufficiently accurate automatic ma-
chine translation (MT) techniques ([1]) were available, we could
address multilingual processing and single language systems in the
same way. However MT techniques are not yet reliable enough
that we expect all words and phrases recognized within languages
to contribute to recognition across languages. We intend to develop
methods that identify the particular words and phrases that both can
be translated reliably and also used to improve story recognition.
As MT technology improves it can be incorporated more exten-
sively within the processing paradigm we propose. We consider
this proposal a framework within which successful MT techniques
can eventually be used for multilingual acoustic processing.
2. PROJECT OBJECTIVES
The first objective is to enhance multi-lingual information sys-
tems by exploiting the processing capabilities for resource-rich lan-
guages to enhance the capabilities for resource-impoverished lan-
guage. The second objective is to advance information retrieval and
knowledge information systems by providing them with consider-
ably improved multi-lingual speech recognition capabilities. Our
research plan proceeds in several steps to (i) collect and (ii) align
multi-lingual parallel speech and text sources, (iii) exploit paral-
lelism for improving ASR within a language, and to (iv) exploit
parallelism for improving ASR across languages. The main infor-
mation flows involved in aligning and exploiting parallel sources
are illustrated in Figure 1. We will initially focus on German, En-
glish and Czech language sources. This section summarizes the
major components of our project.
2.1 Parallel Speech and Text Sources
The monolingual speech and text collections that we will use
to develop techniques to exploit parallelism for improving ASR
within a language are readily available. For instance, the North
American News Text corpus of parallel news streams from 16 US
newspapers and newswire is available from LDC. A 3-year period
yields over 350 million words of multi-source news text.
In addition to data developed within the TIDES and other HLT
programs, we are in the process of identifying and creating our own
multilingual parallel speech and text sources.
FBIS TIDES Multilingual Newstext Collection
For the purposes of developing multilingual alignment techniques,
we intend to use the 240 day, contemporaneous, multilingual news
text collection made available for use to TIDES projects by FBIS.
This corpus contains news in our initial target languages of English,
German, and Czech. The collections are highly parallel, in that
much of the stories are direct translations.
Radio Prague Multilingual Speech and Text Corpus
Speech and news text from Radio Prague was collected under the
direction of J. Psutka with the consent of Radio Prague. The col-
lection contains speech and text in 5 languages: Czech, English,
German, French, and Spanish. The collection began June 1, 2000
and continued for approximately 3 months. The text collection con-
tains the news scripts used for the broadcast; the broadcasts more
or less follow the scripts. The speech is about 3 minutes per day
in each language, which should yield a total of about 5 hours of
speech per language.
Our initial analysis of the Radio Prague corpus suggest that only
approximately 5% of the stories coincide in topic, and that there
is little, if any, direct translation of stories. We anticipate that this
sparseness will make this corpus significantly hard to analyze than
another, highly-parallel corpus. However, we expect this is the
sort of difficulty that will likely be encountered in processing ?real-
world? multilingual news sources.
2.2 Story-level Alignment
Once we have the multiple streams of information we must be
able to align them according to story. A story is the description of
one or more events that happened in a single day and that are re-
ported in a single article by a daily news source the next day. We
expect that we will use the same techniques used in the Topic De-
tection (TDT) field ([5]). Independently of the specific details of
the alignment procedure, there is now substantial evidence that re-
lated stories from parallel streams can be identified using standard
statistical Information Retrieval (IR) techniques.
Sentence Alignment As part of the infrastructure needed to in-
corporate cross-lingual information into language models, we are
employing statistical MT systems to generate English/German and
English/Czech alignments of sentences in the FBIS Newstext Col-
lection. For the English/German sentence and single-word based
alignments, we plan to use statistical models ([4]) [3] which gen-
erate both sentence and word alignments. For English/Czech sen-
tence alignment, we will employ the statistical models trained as
part of the Czech-English MT system developed during the 1999
Johns Hopkins Summer Workshop ([2]).
2.3 Multi-Source Automatic Speech
Recognition
The scenario we propose is extraction of information from paral-
lel text followed by repeated recognition of parallel broadcasts, re-
sulting in a gradual lowering the WER. The first pass is performed
in order to find the likely topics discussed in the story and to iden-
tify the topics relevant to the query. In this process, the acoustic
model will be improved by deriving pronunciation specifications
for out-of-vocabulary words and fixed phrases extracted from the
parallel stories. The language model will be improved by extending
the coverage of the underlying word and phrase vocabulary, and by
specializing the model?s statistics to the narrow topic at hand. As
long as a round of recognition yields new information, the corre-
sponding improvement is incorporated into the recognizer modules
and bootstrapping of the system continues.
Story-specific Language Models from Parallel Speech and Text
Our goal is to create language models combining specific but sparse
statistics, derived from relevant parallel material, with reliable but
unspecific statistics obtainable from large general corpora. We will
create special n-gram language models from the available text, re-
lated or parallel to the spoken stories. We can then interpolate
this special model with a larger pre-existing model, possibly de-
rived from training text associated to the topic of the story. Our
recent STIMULATE work demonstrated success in construction of
topic-specific language models on the basis of hierarchically topic-
organized corpora [8].
Unlike building models from parallel texts, the training of story
specific language models from recognized speech is also affected
by recognition errors in the data which will be used for language
modeling. Confidence measures can be used to estimate the cor-
rectness of individual words or phrases on the recognizer output.
Using this information, n-gram statistics can be extracted from the
recognizer output by selecting those events which are likely to be
correct and which can therefore be used to adjust the original lan-
guage model without introducing new errors to the recognition sys-
tem.
Language Models with Cross-Lingual Lexical Triggers
A trigger language model ([6], [7]) will be constructed for the tar-
get language from the text corpus, where the lexical triggers are not
from the word-history in the target language, but from the aligned
recognized stories in the source language. The trigger informa-
tion becomes most important in those cases in which the baseline
n-gram model in the target language does not supply sufficient in-
formation to predict a word. We expect that content words in the
source language are good predictors for content words in the target
language and that these words are difficult to predict using the tar-
get language alone, and the mutual information techniques used to
identify trigger pairs will be useful here.
Once a spoken source-language story has been recognized, the
words found here there will be used as triggers in the language
model for the recognition of the target-language news broadcasts.
3. SUMMARY
Our goal is to align collections of stories from multiple text and
speech sources in more than one language and then develop meth-
ods that exploit the resulting parallelism both as a tool to improve
recognition accuracy and to enable the development of systems that
can reliably extract information from parallel sources. Much like
a teacher rephrases a concept in a variety of ways to help a class
understand it, the multiple sources, we expect, will increase the po-
tential of success in knowledge extraction. We envision techniques
that will operate repeatedly on multilingual sources by incorporat-
ing newly discovered information in one language into the models
used for all the other languages. Applications of these methods ex-
tend beyond news sources to other multiple-source domains such
as office email and voice-mail, or classroom materials such as lec-
tures, notes and texts.
4. REFERENCES
[1] P. F. Brown, S. A. DellaPietra, V. J. D. Pietra, and R. L.
Mercer. The mathematics of statistical translation.
Computational Linguistics, 19(2), 1993.
[2] K. K. et al Statistical machine translation, WS?99 Final
Report, Johns Hopkins University, 1999.
http://www.clsp.jhu.edu/ws99/projects/mt.
[3] F. J. Och and H. Ney. Improved statistical alignment models.
In ACL?00, pages 440?447, 2000.
[4] F. J. Och, C. Tillmann, and H. Ney. Improved alignment
models for statistical machine translation. In EMNLP/VLC?99,
pages 20?28, 1999.
[5] Proceedings of the Topic Detection and Tracking workshop.
University of Maryland, College Park, MD, October 1997.
[6] C. Tillmann and H. Ney. Selection criteria for word trigger
pairs in language modelling. In ICGI?96, pages 95?106, 1996.
[7] C. Tillmann and H. Ney. Statistical language modeling and
word triggers. In SPECOM?96, pages 22?27, 1996.
[8] D. Yarowsky. Exploiting nonlocal and syntactic word
relationships in language models for conversational speech
recognition, a NSF STIMULATE Project IRI9618874, 1997.
Johns Hopkins University.
 	
ffDesparately Seeking Cebuano
Douglas W. Oard, David Doermann, Bonnie Dorr, Daqing He, Philip Resnik, and Amy Weinberg
UMIACS, University of Maryland, College Park, MD, 20742
(oard,doermann,bonnie,resnik,weinberg)@umiacs.umd.edu
William Byrne, Sanjeev Khudanpur and David Yarowsky
CLSP, Johns Hopkins University, 3400 North Charles Street, Barton Hall, Baltimore, MD 21218
(byrne,khudanpur,yarowsky)@jhu.edu
Anton Leuski, Philipp Koehn and Kevin Knight
USC Information Sciences Institute, 4676 Admiralty Way, Marina Del Rey, CA 90292
(leuski,koehn,knight)@isi.edu
Abstract
This paper describes an effort to rapidly de-
velop language resources and component tech-
nology to support searching Cebuano news sto-
ries using English queries. Results from the
first 60 hours of the exercise are presented.
1 Introduction
The Los Angeles Times reported that at about 5:20 P.M.
on Tuesday March 4, 2003, a bomb concealed in a back-
pack exploded at the airport in Davao City, the second
largest city in the Philippines. At least 23 people were
reported dead, with more than 140 injured, and Pres-
ident Arroyo of the Philippines characterized the blast
as a terrorist act. With the 13 hour time difference, it
was then 4:20 A.M on the same date in Washington, DC.
Twenty-four hours later, at 4:13 A.M. on March 5, partic-
ipants in the Translingual Information Detection, Extrac-
tion and Summarization (TIDES) program were notified
that Cebuano had been chosen as the language of interest
for a ?surprise language? practice exercise that had been
planned quite independently to begin on that date. The
notification observed that Cebuano is spoken by 24% of
the population of the Philippines, and that it is the lingua
franca in the south Philippines, where the event occurred.
One goal of the TIDES program is to develop the abil-
ity to rapidly deploy a broad array of language technolo-
gies for previously unforeseen languages in response to
unexpected events. That capability will be formally ex-
ercised for the first time during June 2003, in a month-
long ?Surprise Language Experiment.? To prepare for
that event, the Linguistic Data Consortium (LDC) orga-
nized a ?dry run? for March 5-14 in order to refine pro-
cedures for rapidly developing language resources of the
type that the TIDES community will need during the July
evaluation.
Development of interactive Cross-Language Informa-
tion Retrieval (CLIR) systems that can be rapidly adapted
to accommodate new languages has been the focus of
extensive collaboration between the University of Mary-
land and The Johns Hopkins University, and more re-
cently with the University of Southern California. The
capability for rapid development of necessary language
resources is an essential part of that process, so we had
been planning to participate in the surprise language dry
run to refine our procedures for sharing those resources
with other members of the TIDES community. Naturally,
we chose CLIR as a driving application to focus our ef-
fort. Our goal, therefore, was to build an interactive sys-
tem that would allow a searcher posing English queries
to find relevant Cebuano news articles from the period
immediately following the bombing.
2 Obtaining Language Resources
Our basic approach to development of an agile system for
interactive CLIR relies on three strategies: (1) create an
infrastructure in advance for English as a query language
that makes only minimal assumptions about the docu-
ment language; (2) leverage the asymmetry inherent in
the problem by assembling strong resources for English
in advance; and (3) develop a robust suite of capabilities
to exploit any language resources that can be found for
the ?surprise language.? We defer the first two topics to
the next section, and focus here on the third. We know of
five possible sources of translation expertise:
People. People who know the language are an excellent
source of insight, and universities are an excellent
place to find such people. We were able to locate
a speaker of Cebuano within 50 feet of one of our
offices, and to schedule an interview with a second
Cebuano speaker within 36 hours of the announce-
ment of the language.
Scholarly literature. Major research universities are
also an excellent place to find written materials de-
scribing a broad array of languages. Within 12 hours
of the announcement, reference librarians at the Uni-
versity of Maryland had identified a textbook on
?Beginning Cebuano,? and we had located a copy
at the University of Southern California. Together
with the excellent electronic resources located by the
LDC, this allowed us to develop a rudimentary stem-
mer within 36 hours.
Translation lexicons. Simple bilingual term lists are
available for many language pairs. Using links pro-
vided by the LDC and our own Web searches, we
were able to construct an English-Cebuano term list
with over 14,000 translation pairs within 12 hours of
the announcement. This largely duplicated a simul-
taneous effort at the LDC, and we later merged our
term list with theirs.
Parallel text. Translation-equivalent documents, when
aligned at the word level, provide an excellent
source of information about not just possible trans-
lations, but their relative predominance. Within 24
hours of the announcement, we had aligned Ce-
buano and English versions of the Holy Bible at
the word level using Giza++. An evaluation by a
native Cebuano speaker of a stratified random sam-
ple of 88 translation pairs showed remarkably high
precision. On a 4-point scale with 1=correct and
4=incorrect the most frequent 100 words averaged
1.3, the next 400 most frequent terms averaged 1.6,
and the 500 next most frequent terms after that aver-
aged 1.7. The Bible?s vocabulary covers only about
half of the words found in typical English news text
(counted by-token), so it is useful to have additional
sources of parallel text. For this reason, we have ex-
tended our previously developed STRAND system
to locate likely translations in the Internet Archive.
Those runs were not yet complete when this paper
was submitted.
Printed Dictionaries. People learning a new language
make extensive use of bilingual dictionaries, so we
have developed a system that mimics that process
to some extent. Within 12 hours of the announce-
ment we had zoned page images from a Cebuano-
English dictionary that was available commercially
in Adobe Page Description Format (PDF) to iden-
tify each dictionary entry, performed optical charac-
ter recognition, and parsed the entries to construct a
bilingual term list. We were aided in this process by
the fact that Cebuano is written in a Roman script.
Again, we achieved good precision, with a sampled
word error rate for OCR of 6.9% and a precision for
a random sample of translation pairs of 87%. Part of
speech tags were also extracted, although they are
not used in our process.
As this description illustrates, these five sources pro-
vide complementary information. Since there is some
uncertainty at the outset about how long it will be before
each delivers useful results, we chose a strategy based
on concurrency, balancing our investment over each the
five sources. This allowed us to use whatever resources
became available first to get an initial system running,
with refinements subsequently being made as additional
resources became available. Because Cebuano and En-
glish are written in the same script, we did not need char-
acter set conversion or phonetic cognate matching in this
case. The CLIR system described in the next section
was therefore constructed using only English resources
that were (or could have been) pre-assembled, plus a
Cebuano-English bilingual term list, a rule-based stem-
mer, and the Cebuano Bible.
3 Building a Cross-Language Retrieval
System
Ideally, we would like to build a system that would find
whatever documents the searcher would wish to read in a
fully automatic mode. In practice, fully automatic search
systems are imperfect even in monolingual applications.
We therefore have developed an interactive approach that
functions something like a typical Web search engine: (1)
the searcher poses their query in English, (2) the sys-
tem ranks the Cebuano documents in decreasing order
of likely relevance to the query, (3) the searcher exam-
ines a list of document titles in something approximat-
ing English, and (4) the searcher may optionally exam-
ine the full text of any document in something approx-
imating English. The intent is to support an iterative
process in which searchers learn to better express their
query through experience. We are only able to provide
very rough translations, so we expect that such a sys-
tem would be used in an environment where searchers
could send documents that appear promising off for pro-
fessional translation when necessary.
At the core of our system is the capability to au-
tomatically rank Cebuano documents based on an En-
glish query. We chose a query translation architecture
using backoff translation and Pirkola?s structured query
method, implemented using Inquery version 3.1p1. The
key idea in backoff translation is to first try to find con-
secutive sequences of query words on the English side
of the bilingual term list, where that fails to try to find
the surface form of each remaining English term, to fall
back to stem matching when necessary, and ultimately to
fall back to retaining the English term unchanged in the
hope that it might be a proper name or some other form
of cognate with Cebuano. Accents are stripped from the
documents and all language resources to facilitate match-
ing at that final step.
Although we have chosen techniques that are relatively
robust and therefore require relatively little domain-
specific tuning, stemmer design is an area of uncertainty
that could adversely affect retrieval effectiveness. We
therefore needed a test collection on which we could try
out variants of the Cebuano stemmer. We built this test
collection using 34,000 Cebuano Bible verses and 50 En-
glish questions that we found on the Web for which ap-
propriate Bible verses were known. Each question was
posed as a query using the batch mode of Inquery, and
the rank of the known relevant verse was taken as a mea-
sure of effectiveness. We took the mean reciprocal rank
(the inverse of the harmonic mean) as a figure of merit
for each configuration, and used a paired two-tailed   -
test (with p  0.05) to assess the statistical significance of
observed differences. Our initial configuration, without
stemming, obtained a mean inverse rank of 0.14, which
is a statistically significant improvement over no transla-
tion at all (mean inverse rank 0.02 from felicitous cognate
and loan word matches). The addition of Cebuano stem-
ming resulted in a reduction in mean inverse rank to 0.09.
Although the reduction is not statistically significant in
that case, the result suggests that our initial stemmer is
not yet useful for information retrieval tasks.
The other key capability that is needed is title and doc-
ument translation. We can accomplish this in one of two
ways. The simplest approach is to reverse the bilingual
term list, and to reverse the role of Cebuano and En-
glish in the process described above for query transla-
tion. Our user interface is capable of displaying multi-
ple translations for a single term (arranged horizontally
for compact depiction or vertically for clearer depiction),
but searchers can choose to display only the single most
likely translation. When reliable translation probability
statistics (from parallel text) are not available, we use the
relative word unigram frequency of each translation of a
Cebuano term in a representative English collection as a
substitute for that probability. A more sophisticated way
is to build a statistical machine translation system using
parallel text. We built our first statistical machine trans-
lation system within 40 hours of the announcement, and
one sentence of the resulting translation using each tech-
nique is shown below:
Cebuano: ?ang rebeldeng milf, kinsa
lakip sa nangamatay, nagdala og
backpack nga dunay explosives nga
niguba sa waiting lounge sa airport,
matod sa mga defense official.?
Term-by-term translation:
?(carelessness, circumference,
conveyence) rebeldeng milf, who lakip
(at in of) nangamatay, nagdala og
backpack nga valid explosives nga
niguba (at, in of) waiting lounge
(at, in, of) airport, matod (at, in,
of) mga defense official?
Statistical translation: ?who was
accused of rank, ehud og niguba
waiting lounge defense of those dumah
milf rebeldeng explosives backpack
airport matod official.?
At this point, term-by-term translation is clearly the bet-
ter choice. But as more parallel text becomes available,
we expect the situation to reverse. The LDC is prepar-
ing a set of human reference translations that will allow
us to detect that changeover point automatically using the
NIST variant of the BLEU measure for machine transla-
tion effectiveness.
4 Conclusion
The results reported in this paper were accomplished by
a team of 20 people with expertise in various facets of te
task that invested about 250 person-hours over two and
a half days. As additional Cebuano-specific evaluation
resources are developed, we expect to gain additional in-
sight into the quality of these early resources. Moreover,
once we see what works best for Cebuano by the end of
the process, we plan to revisit our process design with
an eye towards better optimizing our initial time invest-
ments. We expect to be able to address both of those
points in detail by the time of the conference.
This exercise was originally envisioned as a dry run to
work out the kinks in our process, and indeed we have
already learned a lot on that score. First, we learned that
our basic approach seems sound; we built the key com-
ponents of an interactive CLIR system in about 40 hours,
and by the 60-hour point we had some basis for believing
that each of those components could at least minimally
fulfill their role in a fully integrated system. Some of our
time was, however, spent on things that could have been
done in advance. Perhaps the most important of these
was the development of an information retrieval test col-
lection using the Bible. That job, and numerous smaller
ones, are now done, so we expect that we will be able to
obtain similar results with about half the effort next time
around.
Acknowledgments
Thanks to Clara Cabezas, Tim Hackman, Margie Hi-
nonangan, Burcu Karagol-Ayan, Okan Kolak, Huanfeng
Ma, Grazia Russo-Lassner, Michael Subotin, Jianqiang
Wang and the LDC! This work has been supported in part
by DARPA contract N660010028910.
A Smorgasbord of Features for Statistical Machine Translation
Franz Josef Och
USC/ISI
Daniel Gildea
U. of Rochester
Sanjeev Khudanpur
Johns Hopkins U.
Anoop Sarkar
Simon Fraser U.
Kenji Yamada
Xerox/XRCE
Alex Fraser
USC/ISI
Shankar Kumar
Johns Hopkins U.
Libin Shen
U. of Pennsylvania
David Smith
Johns Hopkins U.
Katherine Eng
Stanford U.
Viren Jain
U. of Pennsylvania
Zhen Jin
Mt. Holyoke
Dragomir Radev
U. of Michigan
Abstract
We describe a methodology for rapid exper-
imentation in statistical machine translation
which we use to add a large number of features
to a baseline system exploiting features from a
wide range of levels of syntactic representation.
Feature values were combined in a log-linear
model to select the highest scoring candidate
translation from an n-best list. Feature weights
were optimized directly against the BLEU eval-
uation metric on held-out data. We present re-
sults for a small selection of features at each
level of syntactic representation.
1 Introduction
Despite the enormous progress in machine translation
(MT) due to the use of statistical techniques in recent
years, state-of-the-art statistical systems often produce
translations with obvious errors. Grammatical errors in-
clude lack of a main verb, wrong word order, and wrong
choice of function words. Frequent problems of a less
grammatical nature include missing content words and
incorrect punctuation.
In this paper, we attempt to address these problems by
exploring a variety of new features for scoring candidate
translations. A high-quality statistical translation system
is our baseline, and we add new features to the exist-
ing set, which are then combined in a log-linear model.
To allow an easy integration of new features, the base-
line system provides an n-best list of candidate transla-
tions which is then reranked using the new features. This
framework allows us to incorporate different types of fea-
tures, including features based on syntactic analyses of
the source and target sentences, which we hope will ad-
dress the grammaticality of the translations, as well as
lower-level features. As we work on n-best lists, we can
easily use global sentence-level features.
We begin by describing our baseline system and the
n-best rescoring framework within which we conducted
our experiments. We then present a selection of new fea-
tures, progressing from word-level features to those based
to part-of-speech tags and syntactic chunks, and then to
features based on Treebank-based syntactic parses of the
source and target sentences.
2 Log-linear Models for Statistical MT
The goal is the translation of a text given in some source
language into a target language. We are given a source
(?Chinese?) sentence f = fJ1 = f1, . . . , fj , . . . , fJ ,
which is to be translated into a target (?English?) sentence
e = eI1 = e1, . . . , ei, . . . , eI Among all possible target
sentences, we will choose the sentence with the highest
probability:
e?I1 = argmax
eI1
{Pr(eI1|f
J
1 )} (1)
As an alternative to the often used source-channel ap-
proach (Brown et al, 1993), we directly model the pos-
terior probability Pr(eI1|fJ1 ) (Och and Ney, 2002) us-
ing a log-linear combination of feature functions. In
this framework, we have a set of M feature functions
hm(eI1, f
J
1 ),m = 1, . . . ,M . For each feature function,
there exists a model parameter ?m,m = 1, . . . ,M . The
direct translation probability is given by:
Pr(eI1|f
J
1 ) =
exp[
?M
m=1 ?mhm(e
I
1, f
J
1 )]
?
e?I1
exp[
?M
m=1 ?mhm(e
?I
1, f
J
1 )]
(2)
We obtain the following decision rule:
e?I1 = argmax
eI1
{ M?
m=1
?mhm(e
I
1, f
J
1 )
}
(3)
The standard criterion for training such a log-linear
model is to maximize the probability of the parallel train-
ing corpus consisting of S sentence pairs {(fs, es) : s =
1, . . . , S}. However, this does not guarantee optimal per-
formance on the metric of translation quality by which
our system will ultimately be evaluated. For this reason,
we optimize the parameters directly against the BLEU
metric on held-out data. This is a more difficult optimiza-
tion problem, as the search space is no longer convex.
Figure 1: Example segmentation of Chinese sentence and
its English translation into alignment templates.
However, certain properties of the BLEU metric can be
exploited to speed up search, as described in detail by
Och (2003). We use this method of optimizing feature
weights throughout this paper.
2.1 Baseline MT System: Alignment Templates
Our baseline MT system is the alignment template system
described in detail by Och, Tillmann, and Ney (1999)
and Och and Ney (2004). In the following, we give a
short description of this baseline model.
The probability model of the alignment template sys-
tem for translating a sentence can be thought of in distinct
stages. First, the source sentence words fJ1 are grouped to
phrases f?K1 . For each phrase f? an alignment template z is
chosen and the sequence of chosen alignment templates
is reordered (according to piK1 ). Then, every phrase f?
produces its translation e? (using the corresponding align-
ment template z). Finally, the sequence of phrases e?K1
constitutes the sequence of words eI1.
Our baseline system incorporated the following feature
functions:
Alignment Template Selection Each alignment
template is chosen with probability p(z|f?), estimated by
relative frequency. The corresponding feature function in
our log-linear model is the log probability of the product
of p(z|f?) for all used alignment templates used.
Word Selection This feature is based on the lexical
translation probabilities p(e|f), estimated using relative
frequencies according to the highest-probability word-
level alignment for each training sentence. A translation
probability conditioned on the source and target position
within the alignment template p(e|f, i, j) is interpolated
with the position-independent probability p(e|f).
Phrase Alignment This feature favors monotonic
alignment at the phrase level. It measures the ?amount
of non-monotonicity? by summing over the distance (in
the source language) of alignment templates which are
consecutive in the target language.
Language Model Features As a language model
feature, we use a standard backing off word-based tri-
gram language model (Ney, Generet, and Wessel, 1995).
The baseline system actually includes four different lan-
guage model features trained on four different corpora:
the news part of the bilingual training data, a large Xin-
hua news corpus, a large AFP news corpus, and a set of
Chinese news texts downloaded from the web.
Word/Phrase Penalty This word penalty feature
counts the length in words of the target sentence. Without
this feature, the sentences produced tend to be too short.
The phrase penalty feature counts the number of phrases
produced, and can allow the model to prefer either short
or long phrases.
Phrases from Conventional Lexicon The baseline
alignment template system makes use of the Chinese-
English lexicon provided by LDC. Each lexicon entry is
a potential phrase translation pair in the alignment tem-
plate system. To score the use of these lexicon entries
(which have no normal translation probability), this fea-
ture function counts the number of times such a lexicon
entry is used.
Additional Features A major advantage of the log-
linear modeling approach is that it is easy to add new
features. In this paper, we explore a variety of features
based on successively deeper syntactic representations of
the source and target sentences, and their alignment. For
each of the new features discussed below, we added the
feature value to the set of baseline features, re-estimated
feature weights on development data, and obtained re-
sults on test data.
3 Experimental Framework
We worked with the Chinese-English data from the recent
evaluations, as both large amounts of sentence-aligned
training corpora and multiple gold standard reference
translations are available. This is a standard data set,
making it possible to compare results with other systems.
In addition, working on Chinese allows us to use the ex-
isting Chinese syntactic treebank and parsers based on it.
For the baseline MT system, we distinguish the fol-
lowing three different sentence- or chunk-aligned parallel
training corpora:
? training corpus (train): This is the basic training
corpus used to train the alignment template transla-
tion model (word lexicon and phrase lexicon). This
corpus consists of about 170M English words. Large
parts of this corpus are aligned on a sub-sentence
level to avoid the existence of very long sentences
which would be filtered out in the training process
to allow a manageable word alignment training.
? development corpus (dev): This is the training cor-
pus used in discriminative training of the model-
parameters of the log-linear translation model. In
most experiments described in this report this cor-
pus consists of 993 sentences (about 25K words) in
both languages.
? test corpus (test): This is the test corpus used to
assess the quality of the newly developed feature
functions. It consists of 878 sentences (about 25K
words).
For development and test data, we have four English (ref-
erence) translations for each Chinese sentence.
3.1 Reranking, n-best lists, and oracles
For each sentence in the development, test, and the blind
test corpus a set of 16,384 different alternative transla-
tions has been produced using the baseline system. For
extracting the n-best candidate translations, an A* search
is used. These n-best candidate translations are the basis
for discriminative training of the model parameters and
for re-ranking.
We used n-best reranking rather than implementing
new search algorithms. The development of efficient
search algorithms for long-range dependencies is very
complicated and a research topic in itself. The rerank-
ing strategy enabled us to quickly try out a lot of new
dependencies, which would not have been be possible if
the search algorithm had to be changed for each new de-
pendency.
On the other hand, the use of n-best list rescoring lim-
its the possibility of improvements to what is available
in the n-best list. Hence, it is important to analyze the
quality of the n-best lists by determining how much of an
improvement would be possible given a perfect reranking
algorithm. We computed the oracle translations, that is,
the set of translations from our n-best list that yields the
best BLEU score.1
We use the following two methods to compute the
BLEU score of an oracle translation:
1. optimal oracle (opt): We select the oracle sentences
which give the highest BLEU score compared to the
set of 4 reference translations. Then, we compute
BLEU score of oracle sentences using the same set
of reference translations.
2. round-robin oracle (rr): We select four differ-
ent sets of oracle sentences which give the highest
BLEU score compared to each of the 4 references
translations. Then, we compute for each set of or-
acle sentences a BLEU score using always those
three references to score that have not been cho-
sen to select the oracle. Then, these 4 3-reference
BLEU scores are averaged.
1Note that due to the corpus-level holistic nature of the
BLEU score it is not trivial to compute the optimal set of oracle
translations. We use a greedy search algorithm for the oracle
translations that might find only a local optimum. Empirically,
we do not observe a dependence on the starting point, hence we
believe that this does not pose a significant problem.
Table 1: Oracle BLEU scores for different sizes of the
n-best list. The avBLEUr3 scores are computed with
respect to three reference translations averaged over the
four different choices of holding out one reference.
avBLEUr3[%] BLEUr4
n rr opt opt
human 35.8 -
1 28.3 28.3 31.6
4 29.1 30.8 34.5
16 29.9 33.2 37.3
64 30.6 35.6 38.7
256 31.3 37.8 42.8
1024 31.7 40.0 45.3
4096 32.0 41.8 47.3
The first method provides the theoretical upper bound of
what BLEU score can be obtained by rescoring a given n-
best list. Using this method with a 1000-best list, we ob-
tain oracle translations that outperform the BLEU score
of the human translations. The oracle translations achieve
113% against the human BLEU score on the test data
(Table 1), while the first best translations obtain 79.2%
against the human BLEU score. The second method uses
a different references for selection and scoring. Here, us-
ing an 1000-best list, we obtain oracle translations with a
relative human BLEU score of 88.5%.
Based on the results of the oracle experiment, and
in order to make rescoring computationally feasible for
features requiring significant computation for each hy-
pothesis, we used the top 1000 translation candidates for
our experiments. The baseline system?s BLEU score is
31.6% on the test set (equivalent to the 1-best oracle in
Table 1). This is the benchmark against which the contri-
butions of the additional features described in the remain-
der of this paper are to be judged.
3.2 Preprocessing
As a precursor to developing the various syntactic fea-
tures described in this report, the syntactic represen-
tations on which they are based needed to be com-
puted. This involved part-of-speech tagging, chunking,
and parsing both the Chinese and English side of our
training, development, and test sets.
Applying the part-of-speech tagger to the often un-
grammatical MT output from our n-best lists sometimes
led to unexpected results. Often the tagger tries to ?fix
up? ungrammatical sentences, for example by looking for
a verb when none is present:
China NNP 14 CD open JJ border NN
cities NNS achievements VBZ remarkable JJ
Here, although achievements has never been seen as a
verb in the tagger?s training data, the prior for a verb
in this position is high enough to cause a present tense
verb tag to be produced. In addition to the inaccura-
cies of the MT system, the difference in genre from the
tagger?s training text can cause problems. For example,
while our MT data include news article headlines with no
verb, headlines are not included in the Wall Street Journal
text on which the tagger is trained. Similarly, the tagger
is trained on full sentences with normalized punctuation,
leading it to expect punctuation at the end of every sen-
tence, and produce a punctuation tag even when the evi-
dence does not support it:
China NNP ?s POS economic JJ
development NN and CC opening VBG
up RP 14 CD border NN cities NNS
remarkable JJ achievements .
The same issues affect the parser. For example the
parser can create verb phrases where none exist, as in the
following example in which the tagger correctly did not
identify a verb in the sentence:
These effects have serious implications for designing
syntactic feature functions. Features such ?is there a verb
phrase? may not do what you expect. One solution would
be features that involve the probability of a parse subtree
or tag sequence, allowing us to ask ?how good a verb
phrase is it?? Another solution is more detailed features
examining more of the structure, such as ?is there a verb
phrase with a verb??
4 Word-Level Feature Functions
These features, directly based on the source and target
strings of words, are intended to address such problems as
translation choice, missing content words, and incorrect
punctuation.
4.1 Model 1 Score
We used IBM Model 1 (Brown et al, 1993) as one of the
feature functions. Since Model 1 is a bag-of-word trans-
lation model and it gives the sum of all possible alignment
probabilities, a lexical co-occurrence effect, or triggering
effect, is expected. This captures a sort of topic or seman-
tic coherence in translations.
As defined by Brown et al (1993), Model 1 gives a
probability of any given translation pair, which is
p(f |e; M1) =

(l + 1)m
m?
j=1
l?
i=0
t(fj |ei).
We used GIZA++ to train the model. The training data is
a subset (30 million words on the English side) of the en-
tire corpus that was used to train the baseline MT system.
For a missing translation word pair or unknown words,
where t(fj |ei) = 0 according to the model, a constant
t(fj |ei) = 10?40 was used as a smoothing value.
The average %BLEU score (average of the best four
among different 20 search initial points) is 32.5. We also
tried p(e|f ; M1) as feature function, but did not obtain
improvements which might be due to an overlap with the
word selection feature in the baseline system.
The Model 1 score is one of the best performing fea-
tures. It seems to ?fix? the tendency of our baseline sys-
tem to delete content words and it improves word selec-
tion coherence by the triggering effect. It is also possible
that the triggering effect might work on selecting a proper
verb-noun combination, or a verb-preposition combina-
tion.
4.2 Lexical Re-ordering of Alignment Templates
As shown in Figure 1 the alignment templates (ATs)
used in the baseline system can appear in various con-
figurations which we will call left/right-monotone and
left/right-continuous. We built 2 out of these 4 models to
distinguish two types of lexicalized re-ordering of these
ATs:
The left-monotone model computes the total proba-
bility of all ATs being left monotone: where the lower
left corner of the AT touches the upper right corner of the
previous AT. Note that the first word in the current AT
may or may not immediately follow the last word in the
previous AT. The total probability is the product over all
alignment templates i, either P (ATi is left-monotone) or
1 ? P (ATi is left-monotone).
The right-continuous model computes the total prob-
ability of all ATs being right continuous: where the
lower left corner of the AT touches the upper right cor-
ner of the previous AT and the first word in the cur-
rent AT immediately follows the last word in the pre-
vious AT. The total probability is the product over all
alignment templates i, either P (ATi is right-continuous)
or 1 ? P (ATi is right-continuous).
In both models, the probabilities P have been esti-
mated from the full training data (train).
5 Shallow Syntactic Feature Functions
By shallow syntax, we mean the output of the part-of-
speech tagger and chunkers. We hope that such features
can combine the strengths of tag- and chunk-based trans-
lation systems (Schafer and Yarowsky, 2003) with our
baseline system.
5.1 Projected POS Language Model
This feature uses Chinese POS tag sequences as surro-
gates for Chinese words to model movement. Chinese
words are too sparse to model movement, but an attempt
to model movement using Chinese POS may be more
successful. We hope that this feature will compensate for
a weak model of word movement in the baseline system.
Chinese POS sequences are projected to English us-
ing the word alignment. Relative positions are indicated
for each Chinese tag. The feature function was also tried
without the relative positions:
CD +0 M +1 NN +3 NN -1 NN +2 NN +3
14 (measure) open border cities
The table shows an example tagging of an English hy-
pothesis showing how it was generated from the Chinese
sentence. The feature function is the log probability out-
put by a trigram language model over this sequence. This
is similar to the HMM Alignment model (Vogel, Ney, and
Tillmann, 1996) but in this case movement is calculated
on the basis of parts of speech.
The Projected POS feature function was one of the
strongest performing shallow syntactic feature functions,
with a %BLEU score of 31.8. This feature function can
be thought of as a trade-off between purely word-based
models, and full generative models based upon shallow
syntax.
6 Tree-Based Feature Functions
Syntax-based MT has shown promise in the
work of, among others, Wu and Wong (1998) and
Alshawi, Bangalore, and Douglas (2000). We hope that
adding features based on Treebank-based syntactic
analyses of the source and target sentences will address
grammatical errors in the output of the baseline system.
6.1 Parse Tree Probability
The most straightforward way to integrate a statistical
parser in the system would be the use of the (log of the)
parser probability as a feature function. Unfortunately,
this feature function did not help to obtain better results
(it actually seems to significantly hurt performance).
To analyze the reason for this, we performed an ex-
periment to test if the used statistical parser assigns a
higher probability to presumably grammatical sentences.
The following table shows the average log probability as-
signed by the Collins parser to the 1-best (produced), or-
acle and the reference translations:
Hypothesis 1-best Oracle Reference
log(parseProb) -147.2 -148.5 -154.9
We observe that the average parser log-probability of
the 1-best translation is higher than the average parse
log probability of the oracle or the reference translations.
Hence, it turns out that the parser is actually assigning
higher probabilities to the ungrammatical MT output than
to the presumably grammatical human translations. One
reason for that is that the MT output uses fewer unseen
words and typically more frequent words which lead to
a higher language model probability. We also performed
experiments to balance this effect by dividing the parser
probability by the word unigram probability and using
this ?normalized parser probability? as a feature function,
but also this did not yield improvements.
6.2 Tree-to-String Alignment
A tree-to-string model is one of several syntax-
based translation models used. The model is a
conditional probability p(f |T (e)). Here, we used
a model defined by Yamada and Knight (2001) and
Yamada and Knight (2002).
Internally, the model performs three types of opera-
tions on each node of a parse tree. First, it reorders the
child nodes, such as changing VP ? VB NP PP into
VP ? NP PP VB. Second, it inserts an optional word at
each node. Third, it translates the leaf English words into
Chinese words. These operations are stochastic and their
probabilities are assumed to depend only on the node, and
are independent of other operations on the node, or other
nodes. The probability of each operation is automatically
obtained by a training algorithm, using about 780,000 En-
glish parse tree-Chinese sentence pairs. The probability
of these operations ?(eki,j) is assumed to depend on the
edge of the tree being modified, eki,j , but independent of
everything else, giving the following equation,
p(f |T (e)) =
?
?
?
?(eki,j)
p(?(eki,j)|e
k
i,j) (4)
where ? varies over the possible alignments between the
f and e and ?(eki,j) is the particular operations (in ?) for
the edge eki,j .
The model is further extended to incorporate phrasal
translations performed at each node of the input parse
tree (Yamada and Knight, 2002). An English phrase cov-
ered by a node can be directly translated into a Chinese
phrase without regular reorderings, insertions, and leaf-
word translations.
The model was trained using about 780,000 English
parse tree-Chinese sentence pairs. There are about 3 mil-
lion words on the English side, and they were parsed by
Collins? parser.
Since the model is computationally expensive, we
added some limitations on the model operations. As the
base MT system does not produce a translation with a
big word jump, we restrict the model not to reorder child
nodes when the node covers more than seven words. For
a node that has more than four children, the reordering
probability is set to be uniform. We also introduced prun-
ing, which discards partial (subtree-substring) alignments
if the probability is lower than a threshold.
The model gives a sum of all possible alignment prob-
abilities for a pair of a Chinese sentence and an English
parse tree. We also calculate the probability of the best
alignment according to the model. Thus, we have the fol-
lowing two feature functions:
hTreeToStringSum(e, f) = log(
?
?
?
?(eki,j)
p(?(eki,j)|e
k
i,j))
hTreeToStringViterbi(e, f) = log(max
?
?
?(eki,j)
p(?(eki,j)|e
k
i,j))
As the model is computationally expensive, we sorted the
n-best list by the sentence length, and processed them
from the shorter ones to the longer ones. We used 10
CPUs for about five days, and 273/997 development sen-
tences and 237/878 test sentences were processed.
The average %BLEU score (average of the best four
among different 20 search initial points) was 31.7 for
both hTreeToStringSum and hTreeToStringViterbi. Among the pro-
cessed development sentences, the model preferred the
oracle sentences over the produced sentence in 61% of
the cases.
The biggest problem of this model is that it is compu-
tationally very expensive. It processed less than 30% of
the n-best lists in long CPU hours. In addition, we pro-
cessed short sentences only. For long sentences, it is not
practical to use this model as it is.
6.3 Tree-to-Tree Alignment
A tree-to-tree translation model makes use of syntac-
tic tree for both the source and target language. As in
the tree-to-string model, a set of operations apply, each
with some probability, to transform one tree into another.
However, when training the model, trees for both the
source and target languages are provided, in our case
from the Chinese and English parsers.
We began with the tree-to-tree alignment model pre-
sented by Gildea (2003). The model was extended to han-
dle dependency trees, and to make use of the word-level
alignments produced by the baseline MT system. The
probability assigned by the tree-to-tree alignment model,
given the word-level alignment with which the candidate
translation was generated, was used as a feature in our
rescoring system.
We trained the parameters of the tree transformation
operations on 42,000 sentence pairs of parallel Chinese-
English data from the Foreign Broadcast Information Ser-
vice (FBIS) corpus. The lexical translation probabili-
ties Pt were trained using IBM Model 1 on the 30 mil-
lion word training corpus. This was done to overcome
the sparseness of the lexical translation probabilities es-
timated while training the tree-to-tree model, which was
not able to make use of as much training data.
As a test of the tree-to-tree model?s discrimination, we
performed an oracle experiment, comparing the model
scores on the first sentence in the n-best list with candi-
date giving highest BLEU score. On the 1000-best list for
the 993-sentence development set, restricting ourselves
to sentences with no more than 60 words and a branching
factor of no more than five in either the Chinese or En-
glish tree, we achieved results for 480, or 48% of the 993
sentences. Of these 480, the model preferred the pro-
duced over the oracle 52% of the time, indicating that
it does not in fact seem likely to significantly improve
BLEU scores when used for reranking. Using the prob-
ability of the source Chinese dependency parse aligning
with the n-best hypothesis dependency parse as a feature
function, making use of the word-level alignments, yields
a 31.6 %BLEU score ? identical to our baseline.
6.4 Markov Assumption for Tree Alignments
The tree-based feature functions described so far have the
following limitations: full parse tree models are expen-
sive to compute for long sentences and for trees with flat
constituents and there is limited reordering observed in
the n-best lists that form the basis of our experiments. In
addition to this, higher levels of parse tree are rarely ob-
served to be reordered between source and target parse
trees.
In this section we attack these problems using a simple
Markov model for tree-based alignments. It guarantees
tractability: compared to a coverage of approximately
30% of the n-best list by the unconstrained tree-based
models, using the Markov model approach provides 98%
coverage of the n-best list. In addition, this approach is
robust to inaccurate parse trees.
The algorithm works as follows: we start with word
alignments and two parameters: n for maximum number
of words in tree fragment and k for maximum height of
tree fragment. We proceed from left to right in the Chi-
nese sentence and incrementally grow a pair of subtrees,
one subtree in Chinese and the other in English, such that
each word in the Chinese subtree is aligned to a word in
the English subtree. We grow this pair of subtrees un-
til we can no longer grow either subtree without violat-
ing the two parameter values n and k. Note that these
aligned subtree pairs have properties similar to alignment
templates. They can rearrange in complex ways between
source and target. Figure 2 shows how subtree-pairs for
parameters n = 3 and k = 3 can be drawn for this
sentence pair. In our experiments, we use substantially
bigger tree fragments with parameters set to n = 8 and
k = 9.
Once these subtree-pairs have been obtained, we can
easily assert a Markov assumption for the tree-to-tree and
tree-to-string translation models that exploits these pair-
ings. Let consider a sentence pair in which we have dis-
covered n subtree-pairs which we can call Frag0, . . .,
Fragn. We can then compute a feature function for the
sentence pair using the tree-to-string translation model as
follows:
hMarkovTreeToString =
logPtree-to-string(Frag0) + . . . + logPtree-to-string(Fragn)
Using this Markov assumption on tree alignments with
Figure 2: Markov assumption on tree alignments.
the Tree to String model described in Section 6.2 we ob-
tain a coverage improvement to 98% coverage from the
original 30%. The accuracy of the tree to string model
also improved with a %BLEU score of 32.0 which is the
best performing single syntactic feature.
6.5 Using TAG elementary trees for scoring word
alignments
In this section, we consider another method for carving
up the full parse tree. However, in this method, instead of
subtree-pairs we consider a decomposition of parse trees
that provides each word with a fragment of the original
parse tree as shown in Figure 3. The formalism of Tree-
Adjoining Grammar (TAG) provides the definition what
each tree fragment should be and in addition how to de-
compose the original parse trees to provide the fragments.
Each fragment is a TAG elementary tree and the compo-
sition of these TAG elementary trees in a TAG deriva-
tion tree provides the decomposition of the parse trees.
The decomposition into TAG elementary trees is done by
augmenting the parse tree for source and target sentence
with head-word and argument (or complement) informa-
tion using heuristics that are common to most contempo-
rary statistical parsers and easily available for both En-
glish and Chinese. Note that we do not use the word
alignment information for the decomposition into TAG
elementary trees.
Once we have a TAG elementary tree per word,
we can create several models that score word align-
ments by exploiting the alignments between TAG ele-
mentary trees between source and target. Let tfi and
tei be the TAG elementary trees associated with the
aligned words fi and ei respectively. We experimented
with two models over alignments: unigram model over
alignments:
?
i P (fi, tfi , ei, tei) and conditional model:?
i P (ei, tei | fi, tfi) ? P (fi+1, tfi+1 | fi, tfi)
We trained both of these models using the SRI Lan-
guage Modeling Toolkit using 60K aligned parse trees.
We extracted 1300 TAG elementary trees each for Chi-
Figure 3: Word alignments with TAG elementary trees.
nese and for English. The unigram model gets a %BLEU
score of 31.7 and the conditional model gets a %BLEU
score of 31.9.
%BLEU
Baseline 31.6
IBM Model 1 p(f |e) 32.5
Tree-to-String Markov fragments 32.0
Right-continuous alignment template 32.0
TAG conditional bigrams 31.9
Left-monotone alignment template 31.9
Projected POS LM 31.8
Tree-to-String 31.7
TAG unigram 31.7
Tree-to-Tree 31.6
combination 32.9
Table 2: Results for the baseline features, each new fea-
ture added to the baseline features on its own, and a com-
bination of new features.
7 Conclusions
The use of discriminative reranking of an n-best list pro-
duced with a state-of-the-art statistical MT system al-
lowed us to rapidly evaluate the benefits of off-the-shelf
parsers, chunkers, and POS taggers for improving syntac-
tic well-formedness of the MT output. Results are sum-
marized in Table 2; the best single new feature improved
the %BLEU score from 31.6 to 32.5. The 95% confi-
dence intervals computed with the bootstrap resampling
method are about 0.8%. In addition to experiments with
single features we also integrated multiple features using
a greedy approach where we integrated at each step the
feature that most improves the BLEU score. This feature
integration produced a statistically significant improve-
ment of absolute 1.3% to 32.9 %BLEU score.
Our single best feature, and in fact the only single fea-
ture to produce a truly significant improvement, was the
IBM Model 1 score. We attribute its success that it ad-
dresses the weakness of the baseline system to omit con-
tent words and that it improves word selection by em-
ploying a triggering effect. We hypothesize that this al-
lows for better use of context in, for example, choosing
among senses of the source language word.
A major goal of this work was to find out if we can ex-
ploit annotated data such as treebanks for Chinese and
English and make use of state-of-the-art deep or shal-
low parsers to improve MT quality. Unfortunately, none
of the implemented syntactic features achieved a statisti-
cally significant improvement in the BLEU score. Poten-
tial reasons for this might be:
? As described in Section 3.2, the use of off-the-shelf
taggers and parsers has various problems due to vari-
ous mismatches between the parser training data and
our application domain. This might explain that the
use of the parser probability as feature function was
not successful. A potential improvement might be to
adapt the parser by retraining it on the full training
data that has been used by the baseline system.
? The use of a 1000-best list limits the potential im-
provements. It is possible that more improvements
could be obtained using a larger n-best list or a word
graph representation of the candidates.
? The BLEU score is possibly not sufficiently sensi-
tive to the grammaticality of MT output. This could
not only make it difficult to see an improvement in
the system?s output, but also potentially mislead the
BLEU-based optimization of the feature weights. A
significantly larger corpus for discriminative train-
ing and for evaluation would yield much smaller
confidence intervals.
? Our discriminative training technique, which di-
rectly optimizes the BLEU score on a development
corpus, seems to have overfitting problems with
large number of features. One could use a larger de-
velopment corpus for discriminative training or in-
vestigate alternative discriminative training criteria.
? The amount of annotated data that has been used
to train the taggers and parsers is two orders of
magnitude smaller than the parallel training data
that has been used to train the baseline system (or
the word-based features). Possibly, a comparable
amount of annotated data (e.g. a treebank with 100
million words) is needed to obtain significant im-
provements.
This is the first large scale integration of syntactic analy-
sis operating on many different levels with a state-of-the-
art phrase-based MT system. The methodology of using
a log-linear feature combination approach, discriminative
reranking of n-best lists computed with a state-of-the-art
baseline system allowed members of a large team to si-
multaneously experiment with hundreds of syntactic fea-
ture functions on a common platform.
Acknowledgments
This material is based upon work supported by the Na-
tional Science Foundation under Grant No. 0121285.
References
Alshawi, Hiyan, Srinivas Bangalore, and Shona Douglas. 2000.
Learning dependency translation models as collections of
finite state head transducers. Computational Linguistics,
26(1):45?60.
Brown, Peter F., Stephen A. Della Pietra, Vincent J. Della
Pietra, and R. L. Mercer. 1993. The mathematics of sta-
tistical machine translation: Parameter estimation. Compu-
tational Linguistics, 19(2):263?311.
Gildea, Daniel. 2003. Loosely tree-based alignment for ma-
chine translation. In Proc. of the 41st Annual Meeting of the
Association for Computational Linguistics (ACL), Sapporo,
Japan.
Ney, Hermann, M. Generet, and Frank Wessel. 1995. Ex-
tensions of absolute discounting for language modeling. In
Proc. of the Fourth European Conf. on Speech Communica-
tion and Technology, Madrid, Spain.
Och, Franz Josef. 2003. Minimum error rate training in statisti-
cal machine translation. In Proc. of the 41st Annual Meeting
of the Association for Computational Linguistics (ACL), Sap-
poro, Japan.
Och, Franz Josef and Hermann Ney. 2002. Discriminative
training and maximum entropy models for statistical ma-
chine translation. In Proc. of the 40th Annual Meeting of the
Association for Computational Linguistics (ACL), Philadel-
phia, PA.
Och, Franz Josef and Hermann Ney. 2004. The alignment tem-
plate approach to statistical machine translation. Computa-
tional Linguistics. Accepted for Publication.
Och, Franz Josef, Christoph Tillmann, and Hermann Ney.
1999. Improved alignment models for statistical machine
translation. In Proc. of the Joint SIGDAT Conf. on Empiri-
cal Methods in Natural Language Processing and Very Large
Corpora, College Park, MD.
Schafer, Charles and David Yarowsky. 2003. Statistical ma-
chine translation using coercive two-level syntactic transduc-
tion. In Proc. of the 2003 Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP), Philadel-
phia, PA.
Vogel, Stephan, Hermann Ney, and Christoph Tillmann. 1996.
HMM-based word alignment in statistical translation. In
COLING ?96: The 16th Int. Conf. on Computational Lin-
guistics, Copenhagen, Denmark.
Wu, Dekai and H. Wong. 1998. Machine translation with a
stochastic grammatical channel. In COLING-ACL ?98: 36th
Annual Meeting of the Association for Computational Lin-
guistics and 17th Int. Conf. on Computational Linguistics,
Montreal, Canada.
Yamada, Kenji and Kevin Knight. 2001. A syntax-based sta-
tistical translation model. In Proc. of the 39th Annual Meet-
ing of the Association for Computational Linguistics (ACL),
Toulouse, France.
Yamada, Kenji and Kevin Knight. 2002. A decoder for syntax-
based MT. In Proc. of the 40th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), Philadelphia,
PA.
Proceedings of NAACL HLT 2007, pages 252?259,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Cross-Instance Tuning of Unsupervised Document Clustering Algorithms?
Damianos Karakos, Jason Eisner
and Sanjeev Khudanpur
Center for Language and Speech Processing
Johns Hopkins University
Baltimore, MD 21218
{damianos,eisner,khudanpur}@jhu.edu
Carey E. Priebe
Dept. of Applied Mathematics
and Statistics
Johns Hopkins University
Baltimore, MD 21218
cep@jhu.edu
Abstract
In unsupervised learning, where no train-
ing takes place, one simply hopes that
the unsupervised learner will work well
on any unlabeled test collection. How-
ever, when the variability in the data is
large, such hope may be unrealistic; a
tuning of the unsupervised algorithm may
then be necessary in order to perform well
on new test collections. In this paper,
we show how to perform such a tuning
in the context of unsupervised document
clustering, by (i) introducing a degree of
freedom, ?, into two leading information-
theoretic clustering algorithms, through
the use of generalized mutual informa-
tion quantities; and (ii) selecting the value
of ? based on clusterings of similar, but
supervised document collections (cross-
instance tuning). One option is to perform
a tuning that directly minimizes the error
on the supervised data sets; another option
is to use ?strapping? (Eisner and Karakos,
2005), which builds a classifier that learns
to distinguish good from bad clusterings,
and then selects the ? with the best pre-
dicted clustering on the test set. Experi-
ments from the ?20 Newsgroups? corpus
show that, although both techniques im-
prove the performance of the baseline al-
gorithms, ?strapping? is clearly a better
choice for cross-instance tuning.
?This work was partially supported by the DARPA GALE
program (Contract No
?
HR0011-06-2-0001) and by the JHU
WSE/APL Partnership Fund.
1 Introduction
The problem of combining labeled and unlabeled
examples in a learning task (semi-supervised learn-
ing) has been studied in the literature under various
guises. A variety of algorithms (e.g., bootstrapping
(Yarowsky, 1995), co-training (Blum and Mitchell,
1998), alternating structure optimization (Ando and
Zhang, 2005), etc.) have been developed in order to
improve the performance of supervised algorithms,
by automatically extracting knowledge from lots of
unlabeled examples. Of special interest is the work
of Ando and Zhang (2005), where the goal is to build
many supervised auxiliary tasks from the unsuper-
vised data, by creating artificial labels; this proce-
dure helps learn a transformation of the input space
that captures the relatedness of the auxiliary prob-
lems to the task at hand. In essence, Ando and Zhang
(2005) transform the semi-supervised learning prob-
lem to a multi-task learning problem; in multi-task
learning, a (usually large) set of supervised tasks is
available for training, and the goal is to build mod-
els which can simultaneously do well on all of them
(Caruana, 1997; Ben-David and Schuller, 2003; Ev-
geniou and Pontil, 2004).
Little work, however, has been devoted to study
the situation where lots of labeled examples, of one
kind, are used to build a model which is tested on
unlabeled data of a ?different? kind. This problem,
which is the topic of this paper, cannot be cast as a
multi-task learning problem (since there are labeled
examples of only one kind), neither can be cast as a
semi-supervised problem (since there are no training
labels for the test task). Note that we are interested
in the case where the hidden test labels may have
no semantic relationship with the training labels; in
252
some cases, there may not even be any informa-
tion about the test labels?what they represent, how
many they are, or at what granularity they describe
the data. This situation can arise in the case of un-
supervised clustering of documents from a large and
diverse corpus: it may not be known in what way the
resulting clusters split the corpus (is it in terms of
topic? genre? style? authorship? a combination of
the above?), unless one inspects each resulting clus-
ter to determine its ?meaning.?
At this point, we would like to differentiate be-
tween two concepts: a target task refers to a class
of problems that have a common, high-level de-
scription (e.g., the text document clustering task, the
speech recognition task, etc.). On the other hand,
a task instance refers to a particular example from
the class. For instance, if the task is ?document
clustering,? a task instance could be ?clustering of
a set of scientific documents into particular fields?;
or, if the task is ?parsing,? a task instance could be
?parsing of English sentences from the Wall Street
Journal corpus?. For the purposes of this paper, we
further assume that there are task instances which
are unrelated, in the sense that that there are no
common labels between them. For example, if the
task is ?clustering from the 20 Newsgroups corpus,?
then ?clustering of the computer-related documents
into PC-related and Mac-related? and ?clustering
of the politics-related documents into Middle-East-
related and non-Middle-East-related? are two dis-
tinct, unrelated instances. In more mathematical
terms, if task instances T1, T2 take sets of observa-
tionsX1,X2 as input, and try to predict labels from
sets S1, S2, respectively, then they are called unre-
lated if X1 ?X2 = ? and S1 ? S2 = ?.
The focus of this paper is to study the problem
of cross-instance tuning of unsupervised algorithms:
how one can tune an algorithm, which is used to
solve a particular task instance, using knowledge
from an unrelated task instance. To the best of our
knowledge, this cross-instance learning problem has
only been tackled in (Eisner and Karakos, 2005),
whose ?strapping? procedure learns a meta-classifier
for distinguishing good from bad clusterings.
In this paper, we introduce a scalar parameter ?
(a new degree of freedom) into two basic unsuper-
vised clustering algorithms. We can tune ? to max-
imize unsupervised clustering performance on dif-
ferent task instances where the correct clustering is
known. The hope is that tuning the parameter learns
something about the task in general, which trans-
fers from the supervised task instances to the un-
supervised one. Alternatively, we can tune a meta-
classifier so as to select good values of ? on the su-
pervised task instances, and then use the same meta-
classifier to select a good (possibly different) value
of ? in the unsupervised case.
The paper is organized as follows: Section 2 gives
a background on text categorization, and briefly de-
scribes the algorithms that we use in our experi-
ments. Section 3 describes our parameterization of
the clustering algorithms using Jensen-Re?nyi diver-
gence and Csisza?r?s mutual information. Experi-
mental results from the ?20 Newsgroups? data set
are shown in Section 4, along with two techniques
for cross-instance learning: (i) ?strapping,? which, at
test time, picks a parameter based on various ?good-
ness? cues that were learned from the labeled data
set, and (ii) learning the parameter from a supervised
data set which is chosen to statistically match the test
set. Finally, concluding remarks appear in Section 5.
2 Document Categorization
Document categorization is the task of deciding
whether a piece of text belongs to any of a set of
prespecified categories. It is a generic text process-
ing task useful in indexing documents for later re-
trieval, as a stage in natural language processing
systems, for content analysis, and in many other
roles (Lewis and Hayes, 1994). Here, we deal
with the unsupervised version of document cate-
gorization, in which we are interested in cluster-
ing together documents which (hopefully) belong to
the same topic, without having any training exam-
ples.1 Supervised information-theoretic clustering
approaches (Torkkola, 2002; Dhillon et al, 2003)
have been shown to be very effective, even with a
small amount of labeled data, while unsupervised
methods (which are of particular interest to us) have
been shown to be competitive, matching the classifi-
cation accuracy of supervised methods.
Our focus in this paper is on document catego-
rization algorithms which use information-theoretic
1By this, we mean that training examples having the same
category labels as the test examples are not available.
253
criteria, since there are natural ways of generalizing
these criteria through the introduction of tunable pa-
rameters. We use two such algorithms in our exper-
iments, the sequential Information Bottleneck (sIB)
and Iterative Denoising Trees (IDTs); details about
these algorithms appear below.
A note on mathematical notation: We assume
that we have a collection A = {X(1), . . . , X(N)}
of N documents. Each document X(i) is essentially
a ?bag of words?, and induces an empirical distri-
bution P?X(i) on the vocabulary X . Given a sub-
set (cluster) C of documents, the conditional dis-
tribution on X , given the cluster, is just the cen-
troid: P?X|C = 1|C|
?
X(i)?C P?X(i). If a subcollec-
tion S ? A of documents is partitioned into clusters
C1, . . . , Cm, and each document X(i) ? S is as-
signed to a cluster CZ(i), where Z(i) ? {1, . . . ,m}
is the cluster index, then the mutual information be-
tween words and corresponding clusters is given by
I(X;Z|S) =
?
z?{1,...,m}
P (z|S)D(P?X|Cz?P?X|S),
where P (z|S) , |Cz|/|S| is the ?prior? distribution
on the clusters and D(???) is the Kullback-Leibler
divergence (Cover and Thomas, 1991).
2.1 The Information Bottleneck Method
The Information Bottleneck (IB) method (Tishby et
al., 1999; Slonim and Tishby, 2000; Slonim et al,
2002) is one popular approach to unsupervised cat-
egorization. The goal of the IB (with ?hard? clus-
tering) is to find clusters such that the mutual in-
formation I(X;Z) between words and clusters is as
large as possible, under a constraint on the number
of clusters. The procedure for finding the maximiz-
ing clustering in (Slonim and Tishby, 2000) is ag-
glomerative clustering, while in (Slonim et al, 2002)
it is based on many random clusterings, combined
with a sequential update algorithm, similar to K-
means. The update algorithm re-assigns each data
point (document) d to its most ?similar? cluster C,
in order to minimize I(X;Z|C ? {d}), i.e.,
?D(P?X|{d}?P?X|{d}?C)+(1??)D(P?X|C?P?X|{d}?C),
where ? = 1|C|+1 . This latter procedure is called
the sequential Information Bottleneck (sIB) method,
and is considered the state-of-the-art in unsuper-
vised document categorization.
2.2 Iterative Denoising Trees
Decision trees are a powerful technique for equiva-
lence classification, accomplished through a recur-
sive successive refinement (Jelinek, 1997). In the
context of unsupervised classification, the goal of
decision trees is to cluster empirical distributions
(bags of words) into a given number of classes, with
each class corresponding to a leaf in the tree. They
are built top-down (as opposed to the bottom-up
construction in IB) using maximization of mutual
information between words and clusters I(X;Z|t)
to drive the splitting of each node t; the hope is that
each leaf will contain data points which belong to
only one latent category.
Iterative Denoising Trees (also called Integrated
Sensing and Processing Decision Trees) were intro-
duced in (Priebe et al, 2004a), as an extension of
regular decision trees. Their main feature is that
they transform the data at each node, before split-
ting, by projecting into a low-dimensional space.
This transformation corresponds to feature extrac-
tion; different features are suppressed (or ampli-
fied) by each transformation, depending on what
data points fall into each node (corpus-dependent-
feature-extraction property (Priebe et al, 2004b)).
Thus, dimensionality reduction and clustering are
chosen so that they jointly optimize the local objec-
tive.
In (Karakos et al, 2005), IDTs were used for an
unsupervised hyperspectral image segmentation ap-
plication. The objective at each node t was to maxi-
mize the mutual information between spectral com-
ponents and clusters given the pixels at node t, com-
puted from the projected empirical distributions. At
each step of the tree-growing procedure, the node
which yielded the highest increase in the average,
per-node mutual information, was selected for split-
ting (until a desired number of leaves was reached).
In (Karakos et al, 2007b), the mutual information
objective was replaced with a parameterized form of
mutual information, namely the Jensen-Re?nyi diver-
gence (Hero et al, 2001; Hamza and Krim, 2003), of
which more details are provided in the next section.
3 Parameterizing Unsupervised Clustering
As mentioned above, the algorithms considered in
this paper (sIB and IDTs) are unsupervised, in the
254
sense that they can be applied to test data with-
out any need for tuning. Our procedure of adapt-
ing them, based on some supervision on a different
task instance, is by introducing a parameter into the
unsupervised algorithm. At least for simple cross-
instance tuning, this parameter represents the infor-
mation which is passed between the supervised and
the unsupervised instances.
The parameterizations that we focused on have
to do with the information-theoretic objectives in
the two unsupervised algorithms. Specifically, fol-
lowing (Karakos et al, 2007b), we replace the mu-
tual information quantities in IDTs as well as sIB
with the parameterized mutual information mea-
sures mentioned above. These two quantities pro-
vide estimates of the dependence between the ran-
dom quantities in their arguments, just as the usual
mutual information does, but also have a scalar pa-
rameter ? ? (0, 1] that controls the sensitivity of the
computed dependence on the details of the joint dis-
tribution of X and Z. As a result, the effect of data
sparseness on estimation of the joint distribution can
be mitigated when computing these measures.
3.1 Jensen-Re?nyi Divergence
The Jensen-Re?nyi divergence was used in (Hero et
al., 2001; Hamza and Krim, 2003) as a measure of
similarity for image classification and retrieval. For
two discrete random variables X,Z with distribu-
tions PX , PZ and conditional PX|Z , it is defined as
I?(X;Z) = H?(PX)?
?
z
PZ(z)H?(PX|Z(?|z)),
(1)
where H?(?) is the Re?nyi entropy, given by
H?(P ) =
1
1? ?
log
(
?
x?X
P (x)?
)
, ? 6= 1. (2)
If ? ? (0, 1), H? is a concave function, and hence
I?(X;Z) is non-negative (and it is equal to zero if
and only if X and Z are independent). In the limit
as ? ? 1, H?(?) approaches the Shannon entropy
(not an obvious fact), so I?(?) reduces to the regular
mutual information. Similarly, we define
I?(X;Z|W ) =
?
w
PW (w)I?(X;Z|W = w),
where I?(X;Z|W = w) is computed via (1) using
the conditional distribution of X and Z given W .
Except in trivial cases, H?(?) is strictly larger
than H(?) when 0 < ? < 1; this means that the ef-
fects of extreme sparsity (few words per document,
or too few occurrences of non-frequent words) on
the estimation of entropy and mutual information
can be dampened with an appropriate choice of ?.
This happens because extreme sparsity in the data
yields empirical distributions which lie at, or close
to, the boundary of the probability simplex. The
entropy of such distributions is usually underesti-
mated, compared to the smooth distributions which
generate the data. Re?nyi?s entropy is larger than
Shannon?s entropy, especially in those regions close
to the boundary, and can thus provide an estimate
which is closer to the true entropy.
3.2 Csisza?r?s Mutual Information
Csisza?r defined the mutual information of order ? as
IC? (X;Z) = min
Q
?
z
PZ(z)D?(PX|Z(?|z)?Q(?)),
(3)
where D?(???) is the Re?nyi divergence (Csisza?r,
1995). It was shown that IC? (X;Z) retains most
of the properties of I(X;Z)?it is a non-negative,
continuous, and concave function of PX , it is con-
vex in PX|Z for ? < 1, and converges to I(X;Z) as
? ? 1.
Notably, IC? (X;Z) ? I(X;Z) for 0 < ? < 1;
this means, as above, that ? regulates the overesti-
mation of mutual information that may result from
data sparseness.
There is no analytic form for the minimizer of the
right-hand-side of (3) (Csisza?r, 1995), but it may be
computed via an alternating minimization algorithm
(Karakos et al, 2007a).
4 Experimental Methods and Results
We demonstrate the feasibility of cross-instance tun-
ing with experiments on unsupervised document cat-
egorization from the 20 Newsgroups corpus (Lang,
1995); this corpus consists of roughly 20,000 news
articles, evenly divided among 20 Usenet groups.
Random samples of 500 articles each were chosen
by (Slonim et al, 2002) to create multiple test col-
lections: 250 each from 2 arbitrarily chosen Usenet
255
groups for the Binary test collection, 100 articles
each from 5 groups for the Multi5 test collection,
and 50 each from 10 groups for the Multi10 test col-
lection. Three independent test collections of each
kind (Binary, Multi5 and Multi10) were created, for
a total of 9 collections. The sIB method was used to
separately cluster each collection, given the correct
number of clusters.
A comparison of sIB and IDTs on the same 9 test
collections was reported in (Karakos et al, 2007b;
Karakos et al, 2007a). Matlab code from (Slonim,
2003) was used for the sIB experiments, while the
parameterized mutual information measures of Sec-
tion 3 were used for the IDTs. A comparison was
also made with the EM-based Gaussian mixtures
clustering tool mclust (Fraley and Raftery, 1999),
and with a simple K-means algorithm. Since the
two latter techniques gave uniformly worse cluster-
ings than those of sIB and IDTs, we omit them from
the following discussion.
To show that our methods work beyond the 9 par-
ticular 500-document collections described above,
in this paper we instead use five different randomly
sampled test collections for each of the Binary,
Multi5 and Multi10 cases, making for a total of 15
new test collections in this paper. For diversity, we
ensure that none of the five test collections (in each
case) contain any documents used in the three col-
lections of (Slonim et al, 2002) (for the same case).
We pre-process the documents of each test col-
lection using the procedure2 mentioned in (Karakos
et al, 2007b). The 15 test collections are then
converted to feature matrices?term-document fre-
quency matrices for sIB, and discounted tf/idf ma-
trices (according to the Okapi formula (Gatford et
al., 1995)) for IDTs?with each row of a matrix rep-
resenting one document in that test collection.
2Excluding the subject line, the header of each abstract is
removed. Stop-words such as a, the, is, etc. are removed, and
stemming is performed (e.g., common suffixes such as -ing, -
er, -ed, etc., are removed). Also, all numbers are collapsed
to one symbol, and non-alphanumeric sequences are converted
to whitespace. Moreover, as suggested in (Yang and Pedersen,
1997) as an effective method for reducing the dimensionality of
the feature space (number of distinct words), all words which
occur fewer than t times in the corpus are removed. For the
sIB experiments, we use t = 2 (as was done in (Slonim et al,
2002)), while for the IDT experiments we use t = 3; these
choices result in the best performance for each method, respec-
tively, on another dataset.
4.1 Selecting ? with ?Strapping?
In order to pick the value of the parameter ? for
each of the sIB and IDT test experiments, we use
?strapping? (Eisner and Karakos, 2005), which, as
we mentioned earlier, is a technique for training a
meta-classifier that chooses among possible cluster-
ings. The training is based on unrelated instances of
the same clustering task. The final choice of cluster-
ing is still unsupervised, since no labels (or ground
truth, in general) for the instance of interest are used.
Here, our collection of possible clusterings for
each test collection is generated by varying the ? pa-
rameter. Strapping does not care, however, how the
collection was generated. (In the original strapping
paper, for example, Eisner and Karakos (2005) gen-
erated their collection by bootstrapping word-sense
classifiers from 200 different seeds.)
Here is how we choose a particular unsupervised
?-clustering to output for a given test collection:
? We cluster the test collection (e.g., the first Multi5
collection) with various values of ?, namely ? =
0.1, 0.2, . . . , 1.0.
? We compute a feature vector from each of the
clusterings. Note that the features are computed
from only the clusterings and the data points,
since no labels are available.
? Based on the feature vectors, we predict the
?goodness? of each clustering, and return the
?best? one.
How do we predict the ?goodness? of a cluster-
ing? By first learning to distinguish good cluster-
ings from bad ones, by using unrelated instances of
the task on which we know the true labels:
? We cluster some unrelated datasets with various
values of ?, just as we will do in the test condi-
tion.
? We evaluate each of the resulting clusterings us-
ing the true labels on its dataset.3
? We train a ?meta-classifier? that predicts the true
rank (or accuracy) of each clustering based on the
feature vector of the clustering.
3To evaluate a clustering, one only really needs the true la-
bels on a sample of the dataset, although in our experiments we
did have true labels on the entire dataset.
256
Specifically, for each task (Binary, Multi5, and
Multi10) and each clustering method (sIB and IDT),
a meta-classifier is learned thus:
? We obtain 10 clusterings (? = 0.1, 0.2, . . . , 1.0)
for each of 5 unrelated task instances (datasets
whose construction is described below).
? For each of these 50 clusterings, we compute the
following 14 features: (i) One minus the aver-
age cosine of the angle (in tf/idf space) between
each example and the centroid of the cluster to
which it belongs. (ii) The average Re?nyi diver-
gence, computed for parameters 1.0, 0.5, 0.1, be-
tween the empirical distribution of each example
and the centroid of the cluster to which it belongs.
(iii) We create 10 more features, one per ?. For
the ? used in this clustering, the feature value is
equal to e?0.1r?, where r? is the average rank of the
clustering (i.e., the average of the 4 ranks result-
ing from sorting all 10 clusterings (per training
example) according to one of the 4 features in (i)
and (ii)). For all other ??s, the feature is set to
zero. Thus, only ??s which yield relatively good
rankings can have non-zero features in the model.
? We normalize each group of 10 feature vectors,
translating and scaling each of the 14 dimensions
to make it range from 0 to 1. (We will do the same
at test time.)
? We train ranking SVMs (Joachims, 2002), with
a Gaussian kernel, to learn how to rank these 50
clusterings given their respective normalized fea-
ture vectors. The values of c, ? (which control
regularization and the Gaussian kernel) were op-
timized through leave-one-out cross validation in
order to maximize the average accuracy of the
top-ranked clustering, over the 5 training sets.
Once a local maximum of the average accuracy
was obtained, further tuning of c, ? to maximize
the Spearman rank correlation between the pre-
dicted and true ranks was performed.
A model trained in this way knows something
about the task, and may work well for many new,
unseen instances of the task. However, we pre-
sume that it will work best on a given test instance
if trained on similar instances. The ideal would be
to match the test collection in every aspect: (i) the
number of training labels should be equal to the
number of desired clusters of the test collection; (ii)
the training clusters should be topically similar to
the desired test clusters.
In our scenario, we enjoy the luxury of plenty
of labeled data that can be used to create similar
instances. Thus, given a test collection A to be
clustered into L clusters, we create similar train-
ing sets by identifying the L training newsgroups
whose centroids in tf/idf space (using the Okapi for-
mula mentioned earlier) have the smallest angle to
the centroid of A.4 (Of course, we exclude news-
groups that appear in A.) We then form a supervised
500-document training set A? by randomly choosing
500/L documents from each of these L newsgroups;
we do this 5 times to obtain 5 supervised training
sets.
Table 1 shows averaged classification errors re-
sulting from strapping (?str? rows) for the Jensen-
Re?nyi divergence and Csisza?r?s mutual information,
used within IDTs and sIB, respectively. (We also
tried the reverse, using Jensen-Re?nyi in sIB and
Csisza?r?s in IDTs, but the results were uniformly
worse in the former case and no better in the latter
case.) The ?MI? rows show the classification errors
of the untuned algorithms (? = 1), which, in almost
all cases, are worse than the tuned ones.
4.2 Tuning ? on Statistically Similar Examples
We now show that strapping outperforms a simpler
and more obvious method for cross-instance tun-
ing. To cluster a test collection A, we could simply
tune the clustering algorithm by choosing the ? that
works best on a related task instance.
We again take care to construct a training instance
A? that is closely related to the test instance A. In
fact, we take even greater care this time. Given A,
4For each of the Binary collections, the closest training
newsgroups in our experiments were talk.politics.guns,
talk.religion.misc; for each of the Multi5 collections
the closest newsgroups were sci.electronics, rec.autos,
sci.med, talk.politics.misc, talk.religion.misc, and for
the Multi10 collections they were talk.politics.misc,
rec.motorcycles, talk.religion.misc, comp.graphics,
comp.sys.ibm.pc.hardware, rec.sport.baseball, comp.os.ms-
windows.misc, comp.windows.x, soc.religion.christian,
talk.politics.mideast. Note that each of the Binary test
collections happens to be closest to the same two training
newsgroups; a similar behavior was observed for the Multi5
and Multi10 newsgroups.
257
PPPPPPPPMethod
Set Binary Multi5 Multi10
ID
Ts
MI 11.3% 9.9% 42.2%
I? (str) 10.4% 9.2% 39.0%
I? (rls) 10.1% 10.4% 42.7%
sI
B
MI 12.0% 6.8% 38.5%
IC? (str) 11.2% 6.9% 35.8%
IC? (rls) 11.1% 7.4% 37.4%
Table 1: Average classification errors for IDTs and
sIB, using strapping (?str? rows) and regularized
least squares (?rls? rows) to pick ? in Jensen-Re?nyi
divergence and Csisza?r?s mutual information. Rows
?MI? show the errors resulting from the untuned al-
gorithms, which use the regular mutual information
objective (? = 1). Results which are better than the
corresponding ?MI? results are shown in bold.
we identify the same set of L closest newsgroups as
described above. This time, however, we carefully
select |A|/L documents from each newsgroup rather
than randomly choosing 500/L of them. Specifi-
cally, for each test example (document) X ? A, we
add a similar training example X ? into A?, chosen as
follows:
We associate each test example X to the most
similar of the L training newsgroups, under a con-
straint that only |A|/L training examples may be as-
sociated to each newsgroup. To do this, we iterate
through all pairs (X,G) where X is a test example
and G is a training newsgroup, in increasing order
by the angle between X and G. If X is not yet asso-
ciated and G is not yet ?full,? then we associate X
with G, and choose X ? to be the document in G with
the smallest angle to X .
We cluster A? 10 times, for ? = 0.1, . . . , 1.0,
and we collect supervised error results E(?), ? ?
{0.1, . . . , 1.0}. Now, instead of using the single best
?? = argmin? E(?) to cluster A (which may re-
sult in overfitting) we use regularized least-squares
(RLS) (Hastie et al, 2001), where we try to approx-
imate the probability that an ? is the best. The esti-
mated probabilities are given by
p? = K(?I+K)?1p,
where I is the unit matrix, p is the training prob-
ability of the best ? (i.e., it is 1 at the position of
?? and zero elsewhere), and K is the kernel matrix,
where K(i, j) = exp(?(E(?i) ? E(?j))2/?2) is
the value of the kernel which expresses the ?sim-
ilarity? between two clusterings of the same train-
ing dataset, in terms of their errors. The parame-
ters ?, ? are set to 0.5, 0.1, respectively, after per-
forming a (local) maximization of the Spearman cor-
relation between training accuracies and predicted
probabilities p?, for all 15 training instances. Af-
ter performing a linear normalization of p? to make
it a probability vector, the average predicted value
of ?, i.e., ?? =
?10
i=1 p?i ?i, (rounded-off to one of
{0.1, . . . , 1.0}) is used to cluster A.
Table 1 shows the average classification error re-
sults using RLS (?rls? rows). We can see that, on
average over the 15 test instances, the error rate of
the tuned IDTs and sIB algorithms is lower than that
of the untuned algorithms, so cross-instance tuning
was effective. On the other hand, the errors are
generally higher than that of the strapping method,
which examines the results of using different ? val-
ues on A.
5 Concluding Remarks
We have considered the problem of cross-instance
tuning of two unsupervised document clustering al-
gorithms, through the introduction of a degree of
freedom into their mutual information objective.
This degree of freedom is tuned using labeled doc-
ument collections (which are unrelated to the test
collections); we explored two approaches for per-
forming the tuning: (i) through a judicious sampling
of training data, to match the marginal statistics of
the test data, and (ii) via ?strapping?, which trains a
meta-classifier to distinguish between good and bad
clusterings. Our unsupervised categorization exper-
iments from the ?20 Newsgroups? corpus indicate
that, although both approaches improve the base-
line algorithms, ?strapping? is clearly a better choice
for knowledge transfer between unrelated task in-
stances.
References
R. K. Ando and T. Zhang. 2005. A framework for
learning predictive structures from multiple tasks
and unlabeled data. Journal of Machine Learning
Research, 6:1817?1853, Nov.
258
S. Ben-David and R. Schuller. 2003. Exploiting task
relatedness for multiple task learning. In Proc. of
the Sixteenth Annual Conference on Learning Theory
(COLT-03).
A. Blum and T. Mitchell. 1998. Combining labeled
and unlabeled data with co-training. In Proceedings
of the Workshop on Computational Learning Theory
(COLT-98), pages 92?100.
R. Caruana. 1997. Multitask learning. Machine Learn-
ing, 28(1):41?75.
T. Cover and J. Thomas. 1991. Elements of Information
Theory. John Wiley and Sons.
I. Csisza?r. 1995. Generalized cutoff rates and Re?nyi?s
information measures. IEEE Trans. on Information
Theory, 41(1):26?34, January.
I. Dhillon, S. Mallela, and R. Kumar. 2003. A divisive
information-theoretic feature clustering algorithm
for text classification. Journal of Machine Learning
Research (JMLR), Special Issue on Variable and
Feature Selection, pages 1265?1287, March.
J. Eisner and D. Karakos. 2005. Bootstrapping without
the boot. In Proc. 2005 Conference on Human
Language Technology / Empirical Methods in Natural
Language Processing (HLT/EMNLP 2005), October.
T. Evgeniou and M. Pontil. 2004. Regularized multi-task
learning. In Proc. Knowledge Discovery and Data
Mining.
C. Fraley and A. E. Raftery. 1999. Mclust: Software for
model-based cluster analysis. Journal on Classifica-
tion, 16:297?306.
M. Gatford, M. M. Hancock-Beaulieu, S. Jones,
S. Walker, and S. E. Robertson. 1995. Okapi at
TREC-3. In The Third Text Retrieval Conference
(TREC-3), pages 109?126.
A. Ben Hamza and H. Krim. 2003. Jensen-Re?nyi
divergence measure: Theoretical and computational
perspectives. In Proc. IEEE Int. Symp. on Information
Theory, Yokohama, Japan, June.
T. Hastie, R. Tibshirani, and J. Friedman. 2001. The
Elements of Statistical Learning. Springer-Verlag.
A. O. Hero, B. Ma, O. Michel, and J. Gorman. 2001.
Alpha-divergence for classification, indexing and
retrieval. Technical Report CSPL-328, University of
Michigan Ann Arbor, Communications and Signal
Processing Laboratory, May.
F. Jelinek. 1997. Statistical Methods for Speech Recog-
nition. MIT Press.
T. Joachims. 2002. Optimizing search engines using
clickthrough data. In ACM Conf. on Knowledge
Discovery and Data Mining (KDD).
D. Karakos, S. Khudanpur, J. Eisner, and C. E. Priebe.
2005. Unsupervised classification via decision trees:
An information-theoretic perspective. In Proc. 2005
International Conference on Acoustics, Speech and
Signal Processing (ICASSP 2005), March.
D. Karakos, S. Khudanpur, J. Eisner, and C. E. Priebe.
2007a. Information-theoretic aspects of iterative
denoising. Submitted to the Journal of Machine
Learning Research, February.
D. Karakos, S. Khudanpur, J. Eisner, and C. E. Priebe.
2007b. Iterative denoising using Jensen-Re?nyi diver-
gences with an application to unsupervised document
categorization. In Proc. 2007 International Confer-
ence on Acoustics, Speech and Signal Processing
(ICASSP 2007), April.
K. Lang. 1995. Learning to filter netnews. In Proc. 13th
Int. Conf. on Machine Learning, pages 331?339.
David D. Lewis and Philip J. Hayes. 1994. Guest
editorial. ACM Transactions on Information Systems,
12(3):231, July.
C. E. Priebe, D. J. Marchette, and D. M. Healy.
2004a. Integrated sensing and processing decision
trees. IEEE Trans. on Pat. Anal. and Mach. Intel.,
26(6):699?708, June.
C. E. Priebe, D. J. Marchette, Y. Park, E. Wegman,
J. Solka, D. Socolinsky, D. Karakos, K. Church,
R. Guglielmi, R. Coifman, D. Lin, D. Healy, M. Ja-
cobs, and A. Tsao. 2004b. Iterative denoising for
cross-corpus discovery. In Proc. 2004 International
Symposium on Computational Statistics (COMPSTAT
2004), August.
N. Slonim and N. Tishby. 2000. Document clustering
using word clusters via the information bottleneck
method. In Research and Development in Information
Retrieval, pages 208?215.
N. Slonim, N. Friedman, and N. Tishby. 2002. Un-
supervised document classification using sequential
information maximization. In Proc. SIGIR?02, 25th
ACM Int. Conf. on Research and Development of
Inform. Retrieval.
N. Slonim. 2003. IBA 1.0: Matlab code for information
bottleneck clustering algorithms. Available from
http://www.princeton.edu/?nslonim/IB Release1.0/
IB Release1 0.tar.
N. Tishby, F. Pereira, and W. Bialek. 1999. The informa-
tion bottleneck method. In 37th Allerton Conference
on Communication and Computation.
K. Torkkola. 2002. On feature extraction by mutual in-
formation maximization. In Proc. IEEE Int. Conf. on
Acoustics, Speech and Signal Proc. (ICASSP-2002),
May.
Y. Yang and J. Pedersen. 1997. A comparative study on
feature selection in text categorization. In Intl. Conf.
on Machine Learning (ICML-97), pages 412?420.
D. Yarowsky. 1995. Unsupervised word sense disam-
biguation rivaling supervised methods. In Proc. 33rd
Annual Meeting of the Association for Computational
Linguistics, pages 189?196, Cambridge, MA.
259
Proceedings of NAACL HLT 2009: Short Papers, pages 9?12,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Efficient Extraction of Oracle-best Translations from Hypergraphs
Zhifei Li and Sanjeev Khudanpur
Center for Language and Speech Processing and Department of Computer Science
The Johns Hopkins University, Baltimore, MD 21218, USA
zhifei.work@gmail.com and khudanpur@jhu.edu
Abstract
Hypergraphs are used in several syntax-
inspired methods of machine translation to
compactly encode exponentially many trans-
lation hypotheses. The hypotheses closest to
given reference translations therefore cannot
be found via brute force, particularly for pop-
ular measures of closeness such as BLEU. We
develop a dynamic program for extracting the
so called oracle-best hypothesis from a hyper-
graph by viewing it as the problem of finding
the most likely hypothesis under an n-gram
language model trained from only the refer-
ence translations. We further identify and re-
move massive redundancies in the dynamic
program state due to the sparsity of n-grams
present in the reference translations, resulting
in a very efficient program. We present run-
time statistics for this program, and demon-
strate successful application of the hypothe-
ses thus found as the targets for discriminative
training of translation system components.
1 Introduction
A hypergraph, as demonstrated by Huang and Chi-
ang (2007), is a compact data-structure that can en-
code an exponential number of hypotheses gener-
ated by a regular phrase-based machine translation
(MT) system (e.g., Koehn et al (2003)) or a syntax-
based MT system (e.g., Chiang (2007)). While the
hypergraph represents a very large set of transla-
tions, it is quite possible that some desired transla-
tions (e.g., the reference translations) are not con-
tained in the hypergraph, due to pruning or inherent
deficiency of the translation model. In this case, one
is often required to find the translation(s) in the hy-
pergraph that are most similar to the desired transla-
tions, with similarity computed via some automatic
metric such as BLEU (Papineni et al, 2002). Such
maximally similar translations will be called oracle-
best translations, and the process of extracting them
oracle extraction. Oracle extraction is a nontrivial
task because computing the similarity of any one
hypothesis requires information scattered over many
items in the hypergraph, and the exponentially large
number of hypotheses makes a brute-force linear
search intractable. Therefore, efficient algorithms
that can exploit the structure of the hypergraph are
required.
We present an efficient oracle extraction algo-
rithm, which involves two key ideas. Firstly, we
view the oracle extraction as a bottom-up model
scoring process on a hypergraph, where the model is
?trained? on the reference translation(s). This is sim-
ilar to the algorithm proposed for a lattice by Dreyer
et al (2007). Their algorithm, however, requires
maintaining a separate dynamic programming state
for each distinguished sequence of ?state? words and
the number of such sequences can be huge, mak-
ing the search very slow. Secondly, therefore, we
present a novel look-ahead technique, called equiv-
alent oracle-state maintenance, to merge multiple
states that are equivalent for similarity computation.
Our experiments show that the equivalent oracle-
state maintenance technique significantly speeds up
(more than 40 times) the oracle extraction.
Efficient oracle extraction has at least three im-
portant applications in machine translation.
Discriminative Training: In discriminative train-
ing, the objective is to tune the model parameters,
e.g. weights of a perceptron model or conditional
random field, such that the reference translations are
preferred over competitors. However, the reference
translations may not be reachable by the translation
system, in which case the oracle-best hypotheses
should be substituted in training.
9
System Combination: In a typical system combi-
nation task, e.g. Rosti et al (2007), each compo-
nent system produces a set of translations, which
are then grafted to form a confusion network. The
confusion network is then rescored, often employ-
ing additional (language) models, to select the fi-
nal translation. When measuring the goodness of a
hypothesis in the confusion network, one requires
its score under each component system. However,
some translations in the confusion network may not
be reachable by some component systems, in which
case a system?s score for the most similar reachable
translation serves as a good approximation.
Multi-source Translation: In a multi-source
translation task (Och and Ney, 2001) the input is
given in multiple source languages. This leads
to a situation analogous to system combination,
except that each component translation system now
corresponds to a specific source language.
2 Oracle Extraction on a Hypergraph
In this section, we present the oracle extraction al-
gorithm: it extracts one or more translations in a hy-
pergraph that have the maximum BLEU score1 with
respect to the corresponding reference translation(s).
The BLEU score of a hypothesis h relative to a
reference r may be expressed in the log domain as,
log BLEU(r, h) = min
[
1? |r||h| , 0
]
+
4?
n=1
1
4 log pn.
The first component is the brevity penalty when
|h|<|r|, while the second component corresponds to
the geometric mean of n-gram precisions pn (with
clipping). While BLEU is normally defined at the
corpus level, we use the sentence-level BLEU for
the purpose of oracle extraction.
Two key ideas for extracting the oracle-best hy-
pothesis from a hypergraph are presented next.
2.1 Oracle Extraction as Model Scoring
Our first key idea is to view the oracle extraction
as a bottom-up model scoring process on the hy-
pergraph. Specifically, we train a 4-gram language
model (LM) on only the reference translation(s),
1We believe our method is general and can be extended to
other metrics capturing only n-gram dependency and other com-
pact data structures, e.g. lattices.
and use this LM as the only model to do a Viterbi
search on the hypergraph to find the hypothesis that
has the maximum (oracle) LM score. Essentially,
the LM is simply a table memorizing the counts of
n-grams found in the reference translation(s), and
the LM score is the log-BLEU value (instead of log-
probability, as in a regular LM). During the search,
the dynamic programming (DP) states maintained
at each item include the left- and right-side LM
context, and the length of the partial translation.
To compute the n-gram precisions pn incrementally
during the search, the algorithm also memorizes at
each item a vector of maximum numbers of n-gram
matches between the partial translation and the ref-
erence(s). Note however that the oracle state of an
item (which decides the uniqueness of an item) de-
pends only on the LM contexts and span lengths, not
on this vector of n-gram match counts.
The computation of BLEU also requires the
brevity penalty, but since there is no explicit align-
ment between the source and the reference(s), we
cannot get the exact reference length |r| at an inter-
mediate item. The exact value of brevity penalty is
thus not computable. We approximate the true refer-
ence length for an item with a product between the
length of the source string spanned by that item and
a ratio (which is between the lengths of the whole
reference and the whole source sentence). Another
approximation is that we do not consider the effect
of clipping, since it is a global feature, making the
strict computation intractable. This does not signifi-
cantly affect the quality of the oracle-best hypothesis
as shown later. Table 1 shows an example how the
BLEU scores are computed in the hypergraph.
The process above may be used either in a first-
stage decoding or a hypergraph-rescoring stage. In
the latter case, if the hypergraph generated by the
first-stage decoding does not have a set of DP states
that is a superset of the DP states required for ora-
cle extraction, we need to split the items of the first-
stage hypergraph and create new items with suffi-
ciently detailed states.
It is worth mentioning that if the hypergraph items
contain the state information necessary for extract-
ing the oracle-best hypothesis, it is straightforward
to further extract the k-best hypotheses in the hyper-
graph (according to BLEU) for any k ? 1 using the
algorithm of Huang and Chiang (2005).
10
Item |h| |r?| matches log BLEU
Item A 5 6.2 (3, 2, 2, 1) -0.82
Item B 10 9.8 (8, 7, 6, 5) -0.27
Item C 17 18.3 (12, 10, 9, 6) -0.62
Table 1: Example computation when items A and B are
combined by a rule to produce item C. |r?| is the approxi-
mated reference length as described in the text.
2.2 Equivalent Oracle State Maintenance
The process above, while able to extract the oracle-
best hypothesis from a hypergraph, is very slow due
to the need to maintain a dedicated item for each or-
acle state (i.e., a combination of left-LM state, right-
LM state, and hypothesis length). This is especially
true if the baseline system uses a LM whose order is
smaller than four, since we need to split the items in
the original hypergraph into many sub-items during
the search. To speed up the extraction, our second
key idea is to maintain an equivalent oracle state.
Roughly speaking, instead of maintaining a dif-
ferent state for different language model words, we
collapse them into a single state whenever it does not
affect BLEU. For example, if we have two left-side
LM states a b c and a b d, and we know that
the reference(s) do not have any n-gram ending with
them, then we can reduce them both to a b and ig-
nore the last word. This is because the combination
of neither left-side LM state (a b c or a b d) can
contribute an n-gram match to the BLEU computa-
tion, regardless of which prefix in the hypergraph
they combine with. Similarly, if we have two right-
side LM states a b c and d b c, and if we know
that the reference(s) do not have any n-gram starting
with either, then we can ignore the first word and re-
duce them both to b c. We can continue this reduc-
tion recursively as shown in Figures 1 and 2, where
IS-A-PREFIX(emi ) (or IS-A-SUFFIX(ei1)) checks if
emi (resp. ei1) is a prefix (suffix) of any n-gram in
the reference translation(s). For BLEU, 1 ? n ? 4.
This equivalent oracle state maintenance tech-
nique, in practice, dramatically reduces the number
of distinct items preserved in the hypergraph for or-
acle extraction. To understand this, observe that if
all hypotheses in the hypergraph together contain m
unique n-grams, for any fixed n, then the total num-
ber of equivalent items takes a multiplicative factor
that is O(m2) due to left- and right-side LM state
EQ-L-STATE (em1 )
1 els? em1
2 for i? m to 1  right to left
3 if IS-A-SUFFIX(ei1)
4 break  stop reducing els
5 else
6 els? ei?11  reduce state
7 return els
Figure 1: Equivalent Left LM State Computation.
EQ-R-STATE (em1 )
1 ers? em1
2 for i? 1 to m  left to right
3 if IS-A-PREFIX (emi )
4 break  stop reducing ers
5 else
6 ers? emi+1  reduce state
7 return ers
Figure 2: Equivalent Right LM State Computation.
maintenance of Section 2.1. This multiplicative fac-
tor under the equivalent state maintenance above is
O(m?2), where m? is the number of unique n-grams
in the reference translations. Clearly, m?  m by
several orders of magnitude, leading to effectively
much fewer items to process in the chart.
One may view this idea of maintaining equivalent
states more generally as an outside look-ahead dur-
ing bottom-up inside parsing. The look-ahead uses
some external information, e.g. IS-A-SUFFIX(?), to
anticipate whether maintaining a detailed state now
will be of consequence later; if not then the in-
side parsing eliminates or collapses the state into
a coarser state. The technique proposed by Li and
Khudanpur (2008a) for decoding with large LMs is
a special case of this general theme.
3 Experimental Results
We report experimental results on a Chinese to En-
glish task, for a system that is trained using a similar
pipeline and data resource as in Chiang (2007).
3.1 Goodness of the Oracle-Best Translations
Table 2 reports the average speed (seconds/sentence)
for oracle extraction. Hypergraphs were generated
with a trigram LM and expanded on the fly for 4-
gram BLEU computation.
11
Basic DP Collapse equiv. states speed-up
25.4 sec/sent 0.6 sec/sent ? 42
Table 2: Speed of oracle extraction from hypergraphs.
The basic dynamic program (Sec. 2.1) improves signifi-
cantly by collapsing equivalent oracle states (Sec. 2.2).
Table 3 reports the goodness of the oracle-best hy-
potheses on three standard data sets. The highest
achievable BLEU score in a hypergraph is clearly
much higher than in the 500-best unique strings.
This shows that a hypergraph provides a much better
basis, e.g., for reranking than an n-best list.
As mentioned in Section 2.1, we use several ap-
proximations in computing BLEU (e.g., no clipping
and approximate reference length). To justify these
approximations, we first extract 500-best unique or-
acles from the hypergraph, and then rerank the ora-
cles based on the true sentence-level BLEU. The last
row of Table 3 reports the reranked one-best oracle
BLEU scores. Clearly, the approximations do not
hurt the oracle BLEU very much.
Hypothesis space MT?04 MT?05 MT?06
1-best (Baseline) 35.7 32.6 28.3
500-unique-best 44.0 41.2 35.1
Hypergraph 52.8 51.8 37.8
500-best oracles 53.2 52.2 38.0
Table 3: Baseline and oracle-best 4-gram BLEU scores
with 4 references for NIST Chinese-English MT datasets.
3.2 Discriminative Hypergraph-Reranking
Oracle extraction is a critical component for
hypergraph-based discriminative reranking, where
millions of model parameters are discriminatively
tuned to prefer the oracle-best hypotheses over oth-
ers. Hypergraph-reranking in MT is similar to the
forest-reranking for monolingual parsing (Huang,
2008). Moreover, once the oracle-best hypothesis
is identified, discriminative models may be trained
on hypergraphs in the same way as on n-best lists
(cf e.g. Li and Khudanpur (2008b)). The results in
Table 4 demonstrate that hypergraph-reranking with
a discriminative LM or TM improves upon the base-
line models on all three test sets. Jointly training
both the LM and TM likely suffers from over-fitting.
Test Set MT?04 MT?05 MT?06
Baseline 35.7 32.6 28.3
Discrim. LM 35.9 33.0 28.2
Discrim. TM 36.1 33.2 28.7
Discrim. TM+LM 36.0 33.1 28.6
Table 4: BLEU scores after discriminative hypergraph-
reranking. Only the language model (LM) or the transla-
tion model (TM) or both (LM+TM) may be discrimina-
tively trained to prefer the oracle-best hypotheses.
4 Conclusions
We have presented an efficient algorithm to extract
the oracle-best translation hypothesis from a hyper-
graph. To this end, we introduced a novel technique
for equivalent oracle state maintenance, which sig-
nificantly speeds up the oracle extraction process.
Our algorithm has clear applications in diverse tasks
such as discriminative training, system combination
and multi-source translation.
References
D. Chiang. 2007. Hierarchical phrase-based translation.
Computational Linguistics, 33(2):201-228.
M. Dreyer, K. Hall, and S. Khudanpur. 2007. Compar-
ing Reordering Constraints for SMT Using Efficient
BLEU Oracle Computation. In Proc. of SSST.
L. Huang. 2008. Forest Reranking: Discriminative Pars-
ing with Non-Local Features. In Proc. of ACL.
L. Huang and D. Chiang. 2005. Better k-best parsing. In
Proc. of IWPT.
L. Huang and D. Chiang. 2007. Forest Rescoring: Faster
Decoding with Integrated Language Models. In Proc.
of ACL.
P. Koehn, F. J. Och, and D. Marcu.2003. Statistical
phrase-based translation. In Proc. of NAACL.
Z. Li and S. Khudanpur. 2008a. A Scalable Decoder for
Parsing-based Machine Translation with Equivalent
Language Model State Maintenance. In Proc. SSST.
Z. Li and S. Khudanpur. 2008b. Large-scale Discrimina-
tive n-gram Language Models for Statistical Machine
Translation. In Proc. of AMTA.
F. Och and H. Ney. 2001. Statistical multisource transla-
tion. In Proc. MT Summit VIII.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
BLEU: a method for automatic evaluation of machine
translation. In Proc. of ACL.
A.I. Rosti, S. Matsoukas, and R. Schwartz. 2007. Im-
proved word-level system combination for machine
translation. In Proc. of ACL.
12
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 81?84,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Machine Translation System Combination
using ITG-based Alignments?
Damianos Karakos, Jason Eisner, Sanjeev Khudanpur, Markus Dreyer
Center for Language and Speech Processing
Johns Hopkins University, Baltimore, MD 21218
{damianos,eisner,khudanpur,dreyer}@jhu.edu
Abstract
Given several systems? automatic translations
of the same sentence, we show how to com-
bine them into a confusion network, whose
various paths represent composite translations
that could be considered in a subsequent
rescoring step. We build our confusion net-
works using the method of Rosti et al (2007),
but, instead of forming alignments using the
tercom script (Snover et al, 2006), we create
alignments that minimize invWER (Leusch
et al, 2003), a form of edit distance that
permits properly nested block movements of
substrings. Oracle experiments with Chinese
newswire and weblog translations show that
our confusion networks contain paths which
are significantly better (in terms of BLEU and
TER) than those in tercom-based confusion
networks.
1 Introduction
Large improvements in machine translation (MT)
may result from combining different approaches
to MT with mutually complementary strengths.
System-level combination of translation outputs is
a promising path towards such improvements. Yet
there are some significant hurdles in this path. One
must somehow align the multiple outputs?to iden-
tify where different hypotheses reinforce each other
and where they offer alternatives. One must then
?This work was partially supported by the DARPA GALE
program (Contract No HR0011-06-2-0001). Also, we would
like to thank the IBM Rosetta team for the availability of several
MT system outputs.
use this alignment to hypothesize a set of new, com-
posite translations, and select the best composite hy-
pothesis from this set. The alignment step is difficult
because different MT approaches usually reorder the
translated words differently. Training the selection
step is difficult because identifying the best hypothe-
sis (relative to a known reference translation) means
scoring all the composite hypotheses, of which there
may be exponentially many.
Most MT combination methods do create an ex-
ponentially large hypothesis set, representing it as a
confusion network of strings in the target language
(e.g., English). (A confusion network is a lattice
where every node is on every path; i.e., each time
step presents an independent choice among several
phrases. Note that our contributions in this paper
could be applied to arbitrary lattice topologies.) For
example, Bangalore et al (2001) show how to build
a confusion network following a multistring align-
ment procedure of several MT outputs. The proce-
dure (used primarily in biology, (Thompson et al,
1994)) yields monotone alignments that minimize
the number of insertions, deletions, and substitu-
tions. Unfortunately, monotone alignments are often
poor, since machine translations (particularly from
different models) can vary significantly in their word
order. Thus, when Matusov et al (2006) use this
procedure, they deterministically reorder each trans-
lation prior to the monotone alignment.
The procedure described by Rosti et al (2007)
has been shown to yield significant improvements in
translation quality, and uses an estimate of Trans-
lation Error Rate (TER) to guide the alignment.
(TER is defined as the minimum number of inser-
81
tions, deletions, substitutions and block shifts be-
tween two strings.) A remarkable feature of that
procedure is that it performs the alignment of the
output translations (i) without any knowledge of the
translation model used to generate the translations,
and (ii) without any knowledge of how the target
words in each translation align back to the source
words. In fact, it only requires a procedure for cre-
ating pairwise alignments of translations that allow
appropriate re-orderings. For this, Rosti et al (2007)
use the tercom script (Snover et al, 2006), which
uses a number of heuristics (as well as dynamic pro-
gramming) for finding a sequence of edits (inser-
tions, deletions, substitutions and block shifts) that
convert an input string to another. In this paper, we
show that one can build better confusion networks
(in terms of the best translation possible from the
confusion network) when the pairwise alignments
are computed not by tercom, which approximately
minimizes TER, but instead by an exact minimiza-
tion of invWER (Leusch et al, 2003), which is a re-
stricted version of TER that permits only properly
nested sets of block shifts, and can be computed in
polynomial time.
The paper is organized as follows: a summary of
TER, tercom, and invWER, is presented in Section
2. The system combination procedure is summa-
rized in Section 3, while experimental (oracle) re-
sults are presented in Section 4. Conclusions are
given in Section 5.
2 Comparing tercom and invWER
The tercom script was created mainly in order to
measure translation quality based on TER. As is
proved by Shapira and Storer (2002), computation
of TER is an NP-complete problem. For this reason,
tercom uses some heuristics in order to compute an
approximation to TER in polynomial time. In the
rest of the paper, we will denote this approximation
as tercomTER, to distinguish it from (the intractable)
TER. The block shifts which are allowed in tercom
have to adhere to the following constraints: (i) A
block that has an exact match cannot be moved, and
(ii) for a block to be moved, it should have an exact
match in its new position. However, this sometimes
leads to counter-intuitive sequences of edits; for in-
stance, for the sentence pair
?thomas jefferson says eat your vegetables?
?eat your cereal thomas edison says?,
tercom finds an edit sequence of cost 5, instead of
the optimum 3. Furthermore, the block selection is
done in a greedy manner, and the final outcome is
dependent on the shift order, even when the above
constraints are imposed.
An alternative to tercom, considered in this pa-
per, is to use the Inversion Transduction Grammar
(ITG) formalism (Wu, 1997) which allows one to
view the problem of alignment as a problem of bilin-
gual parsing. Specifically, ITGs can be used to find
the optimal edit sequence under the restriction that
block moves must be properly nested, like paren-
theses. That is, if an edit sequence swaps adjacent
substrings A and B of the original string, then any
other block move that affects A (or B) must stay
completely within A (or B). An edit sequence with
this restriction corresponds to a synchronous parse
tree under a simple ITG that has one nonterminal
and whose terminal symbols allow insertion, dele-
tion, and substitution.
The minimum-cost ITG tree can be found by dy-
namic programming. This leads to invWER (Leusch
et al, 2003), which is defined as the minimum num-
ber of edits (insertions, deletions, substitutions and
block shifts allowed by the ITG) needed to convert
one string to another. In this paper, the minimum-
invWER alignments are used for generating confu-
sion networks. The alignments are found with a 11-
rule Dyna program (Dyna is an environment that fa-
cilitates the development of dynamic programs?see
(Eisner et al, 2005) for more details). This pro-
gram was further sped up (by about a factor of 2)
with an A? search heuristic computed by additional
code. Specifically, our admissible outside heuris-
tic for aligning two substrings estimated the cost of
aligning the words outside those substrings as if re-
ordering those words were free. This was compli-
cated somewhat by type/token issues and by the fact
that we were aligning (possibly weighted) lattices.
Moreover, the same Dyna program was used for the
computation of the minimum invWER path in these
confusion networks (oracle path), without having to
invoke tercom numerous times to compute the best
sentence in an N -best list.
The two competing alignment procedures were
82
Lang. / Genre tercomTER invWER
Arabic NW 15.1% 14.9%
Arabic WB 26.0% 25.8%
Chinese NW 26.1% 25.6%
Chinese WB 30.9% 30.4%
Table 1: Comparison of average per-document ter-
comTER with invWER on the EVAL07 GALE Newswire
(?NW?) and Weblogs (?WB?) data sets.
used to estimate the TER between machine transla-
tion system outputs and reference translations. Ta-
ble 1 shows the TER estimates using tercom and
invWER. These were computed on the translations
submitted by a system to NIST for the GALE eval-
uation in June 2007. The references used are the
post-edited translations for that system (i.e., these
are ?HTER? approximations). As can be seen from
the table, in all language and genre conditions, in-
vWER gives a better approximation to TER than
tercomTER. In fact, out of the roughly 2000 total
segments in all languages/genres, tercomTER gives
a lower number of edits in only 8 cases! This is a
clear indication that ITGs can explore the space of
string permutations more effectively than tercom.
3 The System Combination Approach
ITG-based alignments and tercom-based alignments
were also compared in oracle experiments involving
confusion networks created through the algorithm of
Rosti et al (2007). The algorithm entails the follow-
ing steps:
? Computation of all pairwise alignments be-
tween system hypotheses (either using ITGs or
tercom); for each pair, one of the hypotheses
plays the role of the ?reference?.
? Selection of a system output as the ?skele-
ton? of the confusion network, whose words
are used as anchors for aligning all other ma-
chine translation outputs together. Each arc has
a translation output word as its label, with the
special token ?NULL? used to denote an inser-
tion/deletion between the skeleton and another
system output.
? Multiple consecutive words which are inserted
relative to the skeleton form a phrase that gets
Genre CNs with tercom CNs with ITG
NW 50.1% (27.7%) 48.8% (28.3%)
WB 51.0% (25.5%) 50.5% (26.0%)
Table 2: TercomTERs of invWER-oracles and (in paren-
theses) oracle BLEU scores of confusion networks gen-
erated with tercom and ITG alignments. The best results
per row are shown in bold.
aligned with an epsilon arc of the confusion
network.
? Setting the weight of each arc equal to the
negative log (posterior) probability of its la-
bel; this probability is proportional to the num-
ber of systems which output the word that gets
aligned in that location. Note that the algo-
rithm of Rosti et al (2007) used N -best lists in
the combination. Instead, we used the single-
best output of each system; this was done be-
cause not all systems were providing N -best
lists, and an unbalanced inclusion would favor
some systems much more than others. Further-
more, for each genre, one of our MT systems
was significantly better than the others in terms
of word order, and it was chosen as the skele-
ton.
4 Experimental Results
Table 2 shows tercomTERs of invWER-oracles (as
computed by the aforementioned Dyna program)
and oracle BLEU scores of the confusion networks.
The confusion networks were generated using 9
MT systems applied to the Chinese GALE 2007
Dev set, which consists of roughly 550 Newswire
segments, and 650 Weblog segments. The confu-
sion networks which were generated with the ITG-
based alignments gave significantly better oracle ter-
comTERs (significance tested with a Fisher sign
test, p ? 0.02) and better oracle BLEU scores.
The BLEU oracle sentences were found using the
dynamic-programming algorithm given in Dreyer et
al. (2007) and measured using Philipp Koehn?s eval-
uation script. On the other hand, a comparison be-
tween the 1-best paths did not reveal significant dif-
ferences that would favor one approach or the other
(either in terms of tercomTER or BLEU).
83
We also tried to understand which alignment
method gives higher probability to paths ?close?
to the corresponding oracle. To do that, we com-
puted the probability that a random path from a
confusion network is within x edits from its ora-
cle. This computation was done efficiently using
finite-state-machine operations, and did not involve
any randomization. Preliminary experiments with
the invWER-oracles show that the probability of all
paths which are within x = 3 edits from the oracle
is roughly the same for ITG-based and tercom-based
confusion networks. We plan to report our findings
for a whole range of x-values in future work. Fi-
nally, a runtime comparison of the two techniques
shows that ITGs are much more computationally
intensive: on average, ITG-based alignments took
1.5 hours/sentence (owing to their O(n6) complex-
ity), while tercom-based alignments only took 0.4
sec/sentence.
5 Concluding Remarks
We compared alignments obtained using the widely
used program tercom with alignments obtained with
ITGs and we established that the ITG alignments are
superior in two ways. Specifically: (a) we showed
that invWER (computed using the ITG alignments)
gives a better approximation to TER between ma-
chine translation outputs and human references than
tercom; and (b) in an oracle system combination ex-
periment, we found that confusion networks gen-
erated with ITG alignments contain better oracles,
both in terms of tercomTER and in terms of BLEU.
Future work will include rescoring results with a
language model, as well as exploration of heuristics
(e.g., allowing only ?short? block moves) that can
reduce the ITG alignment complexity to O(n4).
References
S. Bangalore, G. Bordel, and G. Riccardi. 2001. Com-
puting consensus translation from multiple machine
translation systems. In Proceedings of ASRU, pages
351?354.
M. Dreyer, K. Hall, and S. Khudanpur. 2007. Compar-
ing reordering constraints for smt using efficient bleu
oracle computation. In Proceedings of SSST, NAACL-
HLT 2007 / AMTA Workshop on Syntax and Structure
in Statistical Translation, pages 103?110, Rochester,
New York, April. Association for Computational Lin-
guistics.
Jason Eisner, Eric Goldlust, and Noah A. Smith. 2005.
Compiling comp ling: Weighted dynamic program-
ming and the Dyna language. In Proceedings of HLT-
EMNLP, pages 281?290. Association for Computa-
tional Linguistics, October.
G. Leusch, N. Ueffing, and H. Ney. 2003. A novel
string-to-string distance measure with applications to
machine translation evaluation. In Proceedings of the
Machine Translation Summit 2003, pages 240?247,
September.
E. Matusov, N. Ueffing, and H. Ney. 2006. Computing
consensus translation from multiple machine transla-
tion systems using enhanced hypotheses alignment. In
Proceedings of EACL, pages 33?40.
A.-V.I. Rosti, S. Matsoukas, and R. Schwartz. 2007.
Improved word-level system combination for machine
translation. In Proceedings of the ACL, pages 312?
319, June.
D. Shapira and J. A. Storer. 2002. Edit distance with
move operations. In Proceedings of the 13th Annual
Symposium on Combinatorial Pattern Matching, vol-
ume 2373/2002, pages 85?98, Fukuoka, Japan, July.
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. 2006. A study of translation edit rate with
targeted human annotation. In Proceedings of Associ-
ation for Machine Translation in the Americas, Cam-
bridge, MA, August.
J. D. Thompson, D. G. Higgins, and T. J. Gibson.
1994. Clustalw: Improving the sensitivity of progres-
sive multiple sequence alignment through sequence
weighting, position-specific gap penalties and weight
matrix choice. Nucleic Acids Research, 22(22):4673?
4680.
D. Wu. 1997. Stochastic inversion transduction gram-
mars and bilingual parsing of parallel corpora. Com-
putational Linguistics, 23(3):377?403, September.
84
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 165?168,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Unsupervised Learning of Acoustic Sub-word Units
Balakrishnan Varadarajan? and Sanjeev Khudanpur?
Center for Language and Speech Processing
Johns Hopkins University
Baltimore, MD 21218
{bvarada2,khudanpur}@jhu.edu
Emmanuel Dupoux
Laboratoire de Science Cognitive
et Psycholinguistique
75005, Paris, France
emmanuel.dupoux@gmail.com
Abstract
Accurate unsupervised learning of phonemes
of a language directly from speech is demon-
strated via an algorithm for joint unsupervised
learning of the topology and parameters of
a hidden Markov model (HMM); states and
short state-sequences through this HMM cor-
respond to the learnt sub-word units. The
algorithm, originally proposed for unsuper-
vised learning of allophonic variations within
a given phoneme set, has been adapted to
learn without any knowledge of the phonemes.
An evaluation methodology is also proposed,
whereby the state-sequence that aligns to
a test utterance is transduced in an auto-
matic manner to a phoneme-sequence and
compared to its manual transcription. Over
85% phoneme recognition accuracy is demon-
strated for speaker-dependent learning from
fluent, large-vocabulary speech.
1 Automatic Discovery of Phone(me)s
Statistical models learnt from data are extensively
used in modern automatic speech recognition (ASR)
systems. Transcribed speech is used to estimate con-
ditional models of the acoustics given a phoneme-
sequence. The phonemic pronunciation of words
and the phonemes of the language, however, are
derived almost entirely from linguistic knowledge.
In this paper, we investigate whether the phonemes
may be learnt automatically from the speech signal.
Automatic learning of phoneme-like units has sig-
nificant implications for theories of language ac-
quisition in babies, but our considerations here are
somewhat more technological. We are interested in
developing ASR systems for languages or dialects
? This work was partially supported by National Science
Foundation Grants No
?
IIS-0534359 and OISE-0530118.
for which such linguistic knowledge is scarce or
nonexistent, and in extending ASR techniques to
recognition of signals other than speech, such as ma-
nipulative gestures in endoscopic surgery. Hence an
algorithm for automatically learning an inventory of
intermediate symbolic units?intermediate relative
to the acoustic or kinematic signal on one end and
the word-sequence or surgical act on the other?is
very desirable.
Except for some early work on isolated word/digit
recognition (Paliwal and Kulkarni, 1987; Wilpon
et al, 1987, etc), not much attention has been
paid to automatic derivation of sub-word units from
speech, perhaps because pronunciation lexicons are
now available1 in languages of immediate interest.
What has been investigated is automatically learn-
ing allophonic variations of each phoneme due to
co-articulation or contextual effects (Takami and
Sagayama, 1992; Fukada et al, 1996); the phoneme
inventory is usually assumed to be known.
The general idea in allophone learning is to be-
gin with an inventory of only one allophone per
phoneme, and incrementally refine the inventory to
better fit the speech signal. Typically, each phoneme
is modeled by a separate HMM. In early stages of
refinement, when very few allophones are available,
it is hoped that ?similar? allophones of a phoneme
will be modeled by shared HMM states, and that
subsequent refinement will result in distinct states
for different allophones. The key therefore is to de-
vise a scheme for successive refinement of a model
shared by many allophones. In the HMM setting,
this amounts to simultaneously refining the topol-
ogy and the model parameters. A successive state
splitting (SSS) algorithm to achieve this was pro-
posed by Takami and Sagayama (1992), and en-
1See http://www.ldc.upenn.edu/Catalog/byType.jsp
165
hanced by Singer and Ostendorf (1996). Improve-
ments in phoneme recognition accuracy using these
derived allophonic models over phonemic models
were obtained.
In this paper, we investigate directly learning the
allophone inventory of a language from speech with-
out recourse to its phoneme set. We begin with a
one-state HMM for all speech sounds and modify
the SSS algorithm to successively learn the topol-
ogy and parameters of HMMs with even larger num-
bers of states. States sequences through this HMM
are expected to correspond to allophones. The most
likely state-sequence for a speech segment is inter-
preted as an ?allophonic labeling? of that speech by
the learnt model. Performance is measured by map-
ping the resultant state-sequence to phonemes.
One contribution of this paper is a significant im-
provement in the efficacy of the SSS algorithm as
described in Section 2. It is based on observing
that the improvement in the goodness of fit by up
to two consecutive splits of any of the current HMM
states can be evaluated concurrently and efficiently.
Choosing the best subset of splits from among these
is then cast as a constrained knapsack problem, to
which an efficient solution is devised. Another con-
tribution of this paper is a method to evaluate the
accuracy of the resulting ?allophonic labeling,? as
described in Section 3. It is demonstrated that if
a small amount of phonetically transcribed speech
is used to learn a Markov (bigram) model of state-
sequences that arise from each phone, an evalua-
tion tool results with which we may measure phone
recognition accuracy, even though the HMM labels
the speech signal not with phonemes but merely a
state-sequence. Section 4 presents experimental re-
sults, where the performance accuracies with differ-
ent learning setups are tabulated. We also see how as
little as 5 minutes of speech is adequate for learning
the acoustic units.
2 An Improved and Fast SSS Algorithm
The improvement of the SSS algorithm of Takami
and Sagayama (1992), renamed ML-SSS by Singer
and Ostendorf (1996), proceeds roughly as follows.
1. Model all the speech2 using a 1-state HMM
with a diagonal-covariance Gaussian. (N=1.)
2Note that the original application of SSS was for learning
Figure 1: Modified four-way split of a state s.
2. For each HMM state s, compute the gain in log-
likelihood (LL) of the speech by either a con-
textual or a temporal split of s into two states
s1 and s2. Among the N states, select and and
split the one that yields the most gain in LL.
3. If the gain is above a threshold, retain the split
and set N = N + 1; furthermore, if N is less
than desired, re-estimate all parameters of the
new HMM, and go to Step 2.
Note that the key computational steps are the for-
loop of Step 2 and the re-estimation of Step 3.
Modifications to the ML-SSS Algorithm: We
made the following modifications that are favorable
in terms of greater speed and larger search space,
thereby yielding a gain in likelihood that is poten-
tially greater than the original ML-SSS.
1. Model all the speech using a 1-state HMM with
a full-covariance Gaussian density. Set N = 1.
2. Simultaneously replace each state s of the
HMM with the 4-state topology shown in Fig-
ure 1, yielding a 4N -state HMM. If the state s
had parameters (?s,?s), then means of its 4-
state replacement are ?s1 = ?s? ? = ?s4 and
?s2 = ?s +? = ?s3 , with ? = ?
?v?, where ??
and v? are the principal eigenvalue and eigen-
vector of ?s and 0 <  1 is typically 0.2.
3. Re-estimate all parameters of this (overgrown)
HMM. Gather the Gaussian sufficient statistics
for each of the 4N states from the last pass
of re-estimation: the state occupancy pisi . The
sample mean ?si , and sample covariance ?si .
4. Each quartet of states (see Figure 1) that re-
sulted from the same original state s can be
the allophonic variations of a phoneme; hence the phrase ?all
the speech? meant all the speech corresponding separately to
each phoneme. Here it really means all the speech.
166
merged back in different ways to produce 3, 2
or 1 HMM states. There are 6 ways to end up
with 3 states, and 7 to end up with 2 states. Re-
tain for further consideration the 4 state split of
s, the best merge back to 3 states among the 6
ways, the best merge back to 2 states among the
7 ways, and the merge back to 1 state.
5. Reduce the number of states from 4N toN+?
by optimally3 merging back quartets that cause
the least loss in log-likelihood of the speech.
6. Set N = N + ?. If N is less than the desired
HMM size, retrain the HMM and go to Step 2.
Observe that the 4-state split of Figure 1 permits a
slight look-ahead in our scheme in the sense that the
goodness of a contextual or temporal split of two dif-
ferent states can be compared in the same iteration
with two consecutive splits of a single state. Also,
the split/merge statistics for a state are gathered in
our modified SSS assuming that the other states have
already been split, which facilitates consideration of
concurrent state splitting. If s1, . . . , sm are merged
into s?, the loss of log-likelihood in Step 4 is:
d
2
m?
i=1
pisi log |?s?| ?
d
2
m?
i=1
pisi log |?si | , (1)
where ?s? =
?m
i=1 pisi
(
?si + ?si?
?
si
)
?m
i=1 pisi
? ?s??
?
s?.
Finally, in selecting the best ? states to add to the
HMM, we consider many more ways of splitting the
N original states than SSS does. E.g. going up from
N = 6 toN+? = 9 HMM states could be achieved
by a 4-way split of a single state, a 3-way split of one
state and 2-way of another, or a 2-way split of three
distinct states; all of them are explored in the process
of merging from 4N = 24 down to 9 states. Yet, like
SSS, no original state s is permitted to merge with
another original state s?. This latter restriction leads
to an O(N5) algorithm for finding the best states to
merge down4. Details of the algorithm are ommited
for the sake of brevity.
In summary, our modified ML-SSS algorithm can
leap-frog by ? states at a time, e.g. ? = ?N , com-
pared to the standard algorithm, and it has the benefit
of some lookahead to avoid greediness.
3This entails solving a constrained knapsack problem.
4This is a restricted version of the 0-1 knapsack problem.
3 Evaluating the Goodness of the Labels
The HMM learnt in Section 2 is capable of assign-
ing state-labels to speech via the Viterbi algorithm.
Evaluating whether these labels are linguistically
meaningful requires interpreting the labels in terms
of phonemes. We do so as follows.
Some phonetically transcribed speech is labeled
with the learnt HMM, and the label sequences cor-
responding to each phone segment are extracted.
Since the HMM was learnt from unlabeled speech,
the labels and short label-sequences usually corre-
spond to allophones, not phonemes. Therefore, for
each triphone, i.e. each phone tagged with its left-
and right-phone context, a simple bigram model of
label sequences is estimated. An unweighted ?phone
loop? that accepts all phone sequences is created,
and composed with these bigram models to cre-
ate a label-to-phone transducer capable of mapping
HMM label sequences to phone sequences.
Finally, the test speech (not used for HMM learn-
ing, nor for estimating the bigram model) is treated
as having been ?generated? by a source-channel
model in which the label-to-phone transducer is the
source?generating an HMM state-sequence?and
the Gaussian densities of the learnt HMM states con-
stitute the channel?taking the HMM state-sequence
as the channel input and generating the observed
speech signal as the output. Standard Viterbi decod-
ing determines the most likely phone sequence for
the test speech, and phone accuracy is measured by
comparison with the manual phonetic transcription.
4 Experimental Results
4.1 Impact of the Modified State Splitting
The ML-SSS procedure estimates 2N different
N+1-state HMMs to grow from N to N+1 states.
Our procedure estimates one 4N state HMM to
grow to N+?, making it hugely faster for large N .
Table 1 compares the log-likelihood of the train-
ing speech for ML-SSS and our procedure. The re-
sults validate our modifications, demonstrating that
at least in the regimes feasible for ML-SSS, there is
no loss (in fact a tiny gain) in fitting the speech data,
and a big gain in computational effort5.
5ML-SSS with ?=1 was impractical beyond N=22.
167
# of states SSS (? = 1) ? = 3 ? = N
8 -7.14 -7.13 -7.13
10 -7.08 -7.06 -7.06
22 -6.78 -6.76 N/A
40 N/A -6.23 -6.20
Table 1: Aggressive state splitting does not cause any
degradation in log-likelihood relative to ML-SSS.
4.2 Unsupervised Learning of Sub-word Units
We used about 30 minutes of phonetically tran-
scribed Japanese speech from one speaker6 provided
by Maekawa (2003) for our unsupervised learning
experiments. The speech was segmented via silence
detection into 800 utterances, which were further
partitioned into a 24-minute training set (80%) and
6-minute test set (20%).
Our first experiment was to learn an HMM from
the training speech using our modified ML-SSS pro-
cedure; we tried N = 22, 70 and 376. For each N ,
we then labeled the training speech using the learnt
HMM, used the phonetic transcription of the train-
ing speech to estimate label-bigram models for each
triphone, and built the label-to-phone transducer as
described in Section 3. We also investigated (i) using
only 5 minutes of training speech to learn the HMM,
but still labeling and using all 24 minutes to build
the label-to-phone transducer, and (ii) setting aside
5 minutes of training speech to learn the transducer
and using the rest to learn the HMM. For each learnt
HMM+transducer pair, we phonetically labeled the
test speech.
The results in the first column of Table 2 suggest
that the sub-word units learnt by the HMM are in-
deed interpretable as phones. The second column
suggests that a small amount of speech (5 minutes)
may be adequate to learn these units consistently.
The third column indicates that learning how to map
the learnt (allophonic) units to phones requires rela-
tively more transcribed speech.
4.3 Inspecting the Learnt Sub-word Units
The most frequent 3-, 4- and 5-state sequences in the
automatically labeled speech consistently matched
particular phones in specific articulatory contexts, as
6We heeded advice from the literature indicating that au-
tomatic methods model gross channel- and speaker-differences
before capturing differences between speech sounds.
HMM 24 min 5 min 19 min
label-to-phone 24 min 24 min 5 min
27 states 71.4% 70.9% 60.2%
70 states 84.4% 84.7% 75.8%
376 states 87.2% 86.8% 76.6%
Table 2: Phone recognition accuracy for different HMM
sizes (N), and with different amounts of speech used to
learn the HMM labeler and the label-to-phone transducer.
shown below, i.e. the HMM learns allophones.
HMM labels L-contxt Phone R-contxt
11, 28, 32 vowel t [e|a|o]
15, 17, 2 [g|k] [u|o] [?]
3, 17, 2 [k|t|g|d] a [k|t|g|d]
31, 5, 13, 5 vowel [s|sj|sy] vowel
17, 2, 31, 11 [g|t|k|d] [a|o] [t|k]
3, 30, 22, 34 [?] a silence
6, 24, 8, 15, 22 [?] o silence
4, 3, 17, 2, 21 [k|t] a [k|t]
4, 17, 24, 2, 31 [s|sy|z] o [t|d]
[t|d] o [s|sy|z]
For instance, the label sequence 3, 17, 2, corre-
sponds to an ?a? surrounded by stop consonants
{t, d, k, g}; further restricting the sequence to
4, 3, 17, 2, 21, results in restricting the context to the
unvoiced stops {t, k}. That such clusters are learnt
without knowledge of phones is remarkable.
References
T. Fukada, M. Bacchiani, K. K. Paliwal, and Y. Sagisaka.
1996. Speech recognition based on acoustically de-
rived segment units. In ICSLP, pages 1077?1080.
K. Maekawa. 2003. Corpus of spontaneous japanese:
its design and evaluation. In ISCA/IEEE Workshop on
Spontaneous Speech Processing and Recognition.
K. K. Paliwal and A. M. Kulkarni. 1987. Segmenta-
tion and labeling using vector quantization and its ap-
plication in isolated word recognition. Journal of the
Acoustical Society of India, 15:102?110.
H. Singer and M. Ostendorf. 1996. Maximum likelihood
successive state splitting. In ICASSP, pages 601?604.
J. Takami and S. Sagayama. 1992. A successive state
splitting algorithm for efficient allophone modeling.
In ICASSP, pages 573?576.
J. G. Wilpon, B. H. Juang, and L. R. Rabiner. 1987. An
investigation on the use of acoustic sub-word units for
automatic speech recognition. In ICASSP, pages 821?
824.
168
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 593?601,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Variational Decoding for Statistical Machine Translation
Zhifei Li and Jason Eisner and Sanjeev Khudanpur
Department of Computer Science and Center for Language and Speech Processing
Johns Hopkins University, Baltimore, MD 21218, USA
zhifei.work@gmail.com, jason@cs.jhu.edu, khudanpur@jhu.edu
Abstract
Statistical models in machine translation
exhibit spurious ambiguity. That is, the
probability of an output string is split
among many distinct derivations (e.g.,
trees or segmentations). In principle, the
goodness of a string is measured by the
total probability of its many derivations.
However, finding the best string (e.g., dur-
ing decoding) is then computationally in-
tractable. Therefore, most systems use
a simple Viterbi approximation that mea-
sures the goodness of a string using only
its most probable derivation. Instead,
we develop a variational approximation,
which considers all the derivations but still
allows tractable decoding. Our particular
variational distributions are parameterized
as n-gram models. We also analytically
show that interpolating these n-gram mod-
els for different n is similar to minimum-
risk decoding for BLEU (Tromble et al,
2008). Experiments show that our ap-
proach improves the state of the art.
1 Introduction
Ambiguity is a central issue in natural language
processing. Many systems try to resolve ambigu-
ities in the input, for example by tagging words
with their senses or choosing a particular syntax
tree for a sentence. These systems are designed to
recover the values of interesting latent variables,
such as word senses, syntax trees, or translations,
given the observed input.
However, some systems resolve too many ambi-
guities. They recover additional latent variables?
so-called nuisance variables?that are not of in-
terest to the user.1 For example, though machine
translation (MT) seeks to output a string, typical
MT systems (Koehn et al, 2003; Chiang, 2007)
1These nuisance variables may be annotated in training
data, but it is more common for them to be latent even there,
i.e., there is no supervision as to their ?correct? values.
will also recover a particular derivation of that out-
put string, which specifies a tree or segmentation
and its alignment to the input string. The compet-
ing derivations of a string are interchangeable for
a user who is only interested in the string itself, so
a system that unnecessarily tries to choose among
them is said to be resolving spurious ambiguity.
Of course, the nuisance variables are important
components of the system?s model. For example,
the translation process from one language to an-
other language may follow some hidden tree trans-
formation process, in a recursive fashion. Many
features of the model will crucially make reference
to such hidden structures or alignments.
However, collapsing the resulting spurious
ambiguity?i.e., marginalizing out the nuisance
variables?causes significant computational dif-
ficulties. The goodness of a possible MT out-
put string should be measured by summing up
the probabilities of all its derivations. Unfortu-
nately, finding the best string is then computation-
ally intractable (Sima?an, 1996; Casacuberta and
Higuera, 2000).2 Therefore, most systems merely
identify the single most probable derivation and
report the corresponding string. This corresponds
to a Viterbi approximation that measures the good-
ness of an output string using only its most proba-
ble derivation, ignoring all the others.
In this paper, we propose a variational method
that considers all the derivations but still allows
tractable decoding. Given an input string, the orig-
inal system produces a probability distribution p
over possible output strings and their derivations
(nuisance variables). Our method constructs a sec-
ond distribution q ? Q that approximates p as well
as possible, and then finds the best string accord-
ing to q. The last step is tractable because each
q ? Q is defined (unlike p) without reference to
nuisance variables. Notice that q here does not ap-
proximate the entire translation process, but only
2May and Knight (2006) have successfully used tree-
automaton determinization to exactly marginalize out some
of the nuisance variables, obtaining a distribution over parsed
translations. However, they do not marginalize over these
parse trees to obtain a distribution over translation strings.
593
the distribution over output strings for a particular
input. This is why it can be a fairly good approxi-
mation even without using the nuisance variables.
In practice, we approximate with several dif-
ferent variational families Q, corresponding to n-
gram (Markov) models of different orders. We
geometrically interpolate the resulting approxima-
tions q with one another (and with the original dis-
tribution p), justifying this interpolation as similar
to the minimum-risk decoding for BLEU proposed
by Tromble et al (2008). Experiments show that
our approach improves the state of the art.
The methods presented in this paper should be
applicable to collapsing spurious ambiguity for
other tasks as well. Such tasks include data-
oriented parsing (DOP), applications of Hidden
Markov Models (HMMs) and mixture models, and
other models with latent variables. Indeed, our
methods were inspired by past work on varia-
tional decoding for DOP (Goodman, 1996) and for
latent-variable parsing (Matsuzaki et al, 2005).
2 Background
2.1 Terminology
In MT, spurious ambiguity occurs both in regular
phrase-based systems (e.g., Koehn et al (2003)),
where different segmentations lead to the same
translation string (Figure 1), and in syntax-based
systems (e.g., Chiang (2007)), where different
derivation trees yield the same string (Figure 2).
In the Hiero system (Chiang, 2007) we are us-
ing, each string corresponds to about 115 distinct
derivations on average.
We use x to denote the input string, and D(x) to
consider the set of derivations then considered by
the system. Each derivation d ? D(x) yields some
translation string y = Y(d) in the target language.
We write D(x, y)
def
= {d ? D(x) : Y(d) = y} to
denote the set of all derivations that yield y. Thus,
the set of translations permitted by the model is
T(y)
def
= {y : D(x, y) 6= ?} (or equivalently,
T(y)
def
= {Y(d) : d ? D(x)}). We write y? for
the translation string that is actually output.
2.2 Maximum A Posterior (MAP) Decoding
For a given input sentence x, a decoding method
identifies a particular ?best? output string y?. The
maximum a posteriori (MAP) decision rule is
y? = argmax
y?T(x)
p(y | x) (1)
machine translation software
?  ?  ?  ?  ?  ?
machine translation software
?  ?  ?  ?  ?  ?
Figure 1: Segmentation ambiguity in phrase-based MT: two
different segmentations lead to the same translation string.
S->(? ?, machine) S->( ? , translation) S->( ? , software)
S->(? ?, machine)
?
S->( ? , software)
S->(S0 S1, S0 S1)
S->(S0 S1, S0 S1)
S->(S0 ?  S1, S0 translation S1)
Figure 2: Tree ambiguity in syntax-based MT: two derivation
trees yield the same translation string.
(An alternative decision rule, minimum Bayes
risk (MBR), will be discussed in Section 4.)
To obtain p(y | x) above, we need to marginal-
ize over a nuisance variable, the derivation of y.
Therefore, the MAP decision rule becomes
y? = argmax
y?T(x)
?
d?D(x,y)
p(y, d | x) (2)
where p(y, d | x) is typically derived from a log-
linear model as follows,
p(y, d | x) =
e??s(x,y,d)
Z(x)
=
e??s(x,y,d)
?
y,d e
??s(x,y,d)
(3)
where ? is a scaling factor to adjust the sharp-
ness of the distribution, the score s(x, y, d) is a
learned linear combination of features of the triple
(x, y, d), and Z(x) is a normalization constant.
Note that p(y, d | x) = 0 if y 6= Y(d). Our deriva-
tion set D(x) is encoded in polynomial space, us-
ing a hypergraph or lattice.3 However, both |D(x)|
and |T(x)| may be exponential in |x|. Since the
marginalization needs to be carried out for each
member of T(x), the decoding problem of (2)
turns out to be NP-hard,4 as shown by Sima?an
(1996) for a similar problem.
3A hypergraph is analogous to a parse forest (Huang and
Chiang, 2007). (A finite-state lattice is a special case.) It can
be used to encode exponentially many hypotheses generated
by a phrase-based MT system (e.g., Koehn et al (2003)) or a
syntax-based MT system (e.g., Chiang (2007)).
4Note that the marginalization for a particular y would be
tractable; it is used at training time in certain training objec-
tive functions, e.g., maximizing the conditional likelihood of
a reference translation (Blunsom et al, 2008).
594
2.3 Viterbi Approximation
To approximate the intractable decoding problem
of (2), most MT systems (Koehn et al, 2003; Chi-
ang, 2007) use a simple Viterbi approximation,
y? = argmax
y?T(x)
pViterbi(y | x) (4)
= argmax
y?T(x)
max
d?D(x,y)
p(y, d | x) (5)
= Y
(
argmax
d?D(x)
p(y, d | x)
)
(6)
Clearly, (5) replaces the sum in (2) with a max.
In other words, it approximates the probability of
a translation string by the probability of its most-
probable derivation. (5) is found quickly via (6).
The Viterbi approximation is simple and tractable,
but it ignores most derivations.
2.4 N-best Approximation (or Crunching)
Another popular approximation enumerates the N
best derivations in D(x), a set that we call ND(x).
Modifying (2) to sum over only these derivations
is called crunching by May and Knight (2006):
y? = argmax
y?T(x)
pcrunch(y | x) (7)
= argmax
y?T(x)
?
d?D(x,y)?ND(x)
p(y, d | x)
3 Variational Approximate Decoding
The Viterbi and crunching methods above approx-
imate the intractable decoding of (2) by ignor-
ing most of the derivations. In this section, we
will present a novel variational approximation,
which considers all the derivations but still allows
tractable decoding.
3.1 Approximate Inference
There are several popular approaches to approxi-
mate inference when exact inference is intractable
(Bishop, 2006). Stochastic techniques such as
Markov Chain Monte Carlo are exact in the limit
of infinite runtime, but tend to be too slow for large
problems. By contrast, deterministic variational
methods (Jordan et al, 1999), including message-
passing (Minka, 2005), are inexact but scale up
well. They approximate the original intractable
distribution with one that factorizes better or has
a specific parametric form (e.g., Gaussian).
In our work, we use a fast variational method.
Variational methods generally work as follows.
When exact inference under a complex model p
is intractable, one can approximate the posterior
p(y | x) by a tractable model q(y), where q ? Q is
chosen to minimize some information loss such as
the KL divergence KL(p ? q). The simpler model
q can then act as a surrogate for p during inference.
3.2 Variational Decoding for MT
For each input sentence x, we assume that a base-
line MT system generates a hypergraph HG(x)
that compactly encodes the derivation set D(x)
along with a score for each d ? D(x),5 which we
interpret as p(y, d | x) (or proportional to it). For
any single y ? T(x), it would be tractable using
HG(x) to compute p(y | x) =
?
d p(y, d | x).
However, as mentioned, it is intractable to find
argmaxy p(y | x) as required by the MAP de-
coding (2), so we seek an approximate distribution
q(y) ? p(y | x).6
For a fixed x, we seek a distribution q ? Q that
minimizes the KL divergence from p to q (both
regarded as distributions over y):7
q? = argmin
q?Q
KL(p ? q) (8)
= argmin
q?Q
?
y?T(x)
(p log p? p log q) (9)
= argmax
q?Q
?
y?T(x)
p log q (10)
So far, in order to approximate the intractable
optimization problem (2), we have defined an-
other optimization problem (10). If computing
p(y | x) during decoding is computationally in-
tractable, one might wonder if the optimization
problem (10) is any simpler. We will show this is
the case. The trick is to parameterize q as a fac-
torized distribution such that the estimation of q?
and decoding using q? are both tractable through
efficient dynamic programs. In the next three sub-
sections, we will discuss the parameterization, es-
timation, and decoding, respectively.
3.2.1 Parameterization of q
In (10), Q is a family of distributions. If we se-
lect a large family Q, we can allow more com-
plex distributions, so that q? will better approxi-
mate p. If we select a smaller family Q, we can
5The baseline system may return a pruned hypergraph,
which has the effect of pruning D(x) and T(x) as well.
6Following the convention in describing variational infer-
ence, we write q(y) instead of q(y | x), even though q(y)
always depends on x implicitly.
7To avoid clutter, we denote p(y | x) by p, and q(y) by q.
We drop p log p from (9) because it is constant with respect
to q. We then flip the sign and change argmin to argmax.
595
guarantee that q? will have a simple form with
many conditional independencies, so that q?(y)
and y? = argmaxy q
?(y) are easier to compute.
Since each q(y) is a distribution over output
strings, a natural choice for Q is the family of
n-gram models. To obtain a small KL diver-
gence (8), we should make n as large as possible.
In fact, q? ? p as n ? ?. Of course, this last
point also means that our computation becomes
intractable as n??.8 However, if p(y | x) is de-
fined by a hypergraph HG(x) whose structure ex-
plicitly incorporates an m-gram language model,
both training and decoding will be efficient when
m ? n. We will give algorithms for this case that
are linear in the size of HG(x).9
Formally, each q ? Q takes the form
q(y) =
?
w?W
q(r(w) | h(w))cw(y) (11)
where W is a set of n-gram types. Each w ?W is
an n-gram, which occurs cw(y) times in the string
y, and w may be divided into an (n ? 1)-gram
prefix h(w) (the history) and a 1-gram suffix r(w)
(the rightmost or current word).
8Blunsom et al (2008) effectively do take n = ?, by
maintaining the whole translation string in the dynamic pro-
gramming state. They alleviate the computation cost some-
how by using aggressive beam pruning, which might be sen-
sible for their relatively small task (e.g., input sentences of
< 10 words). But, we are interested in improving the perfor-
mance for a large-scale system, and thus their method is not
a viable solution. Moreover, we observe in our experiments
that using a larger n does not improve much over n = 2.
9A reviewer asks about the interaction with backed-off
language models. The issue is that the most compact finite-
state representations of these (Allauzen et al, 2003), which
exploit backoff structure, are not purely m-gram for any
m. They yield more compact hypergraphs (Li and Khudan-
pur, 2008), but unfortunately those hypergraphs might not be
treatable by Fig. 4?since where they back off to less than an
n-gram, e is not informative enough for line 8 to find w.
We sketch a method that works for any language model
given by a weighted FSA, L. The variational family Q can
be specified by any deterministic weighted FSA, Q, with
weights parameterized by ?. One seeks ? to minimize (8).
Intersect HG(x) with an ?unweighted? version of Q in
which all arcs have weight 1, so that Q does not prefer
any string to another. By lifting weights into an expectation
semiring (Eisner, 2002), it is then possible to obtain expected
transition counts in Q (where the expectation is taken under
p), or other sufficient statistics needed to estimate ?.
This takes only time O(|HG(x)|) when L is a left-to-right
refinement of Q (meaning that any two prefix strings that
reach the same state in L also reach the same state in Q),
for then intersecting L or HG(x) with Q does not split any
states. That is the case when L and Q are respectively pure
m-gram and n-gram models withm ? n, as assumed in (12)
and Figure 4. It is also the case when Q is a pure n-gram
model and L is constructed not to back off beyond n-grams;
or when the variational family Q is defined by deliberately
taking the FSA Q to have the same topology as L.
The parameters that specify a particular q ? Q
are the (normalized) conditional probability distri-
butions q(r(w) | h(w)). We will now see how to
estimate these parameters to approximate p(? | x)
for a given x at test time.
3.2.2 Estimation of q?
Note that the objective function (8)?(10) asks us to
approximate p as closely as possible, without any
further smoothing. (It is assumed that p is already
smoothed appropriately, having been constructed
from channel and language models that were esti-
mated with smoothing from finite training data.)
In fact, if p were the empirical distribution over
strings in a training corpus, then q? of (10) is just
the maximum-likelihood n-gram model?whose
parameters, trivially, are just unsmoothed ratios of
the n-gram and (n?1)-gram counts in the training
corpus. That is, q?(r(w) | h(w)) = c(w)c(h(w)) .
Our actual job is exactly the same, except that p
is specified not by a corpus but by the hypergraph
HG(x). The only change is that the n-gram counts
c?(w) are no longer integers from a corpus, but are
expected counts under p:10
q?(r(w) | h(w)) =
c?(w)
c?(h(w))
= (12)
?
y cw(y)p(y | x)
?
y ch(w)(y)p(y | x)
=
?
y,d cw(y)p(y, d | x)
?
y,d ch(w)(y)p(y, d | x)
Now, the question is how to efficiently compute
(12) from the hypergraph HG(x). To develop the
intuition, we first present a brute-force algorithm
in Figure 3. The algorithm is brute-force since
it first needs to unpack the hypergraph and enu-
merate each possible derivation in the hypergraph
(see line 1), which is computationally intractable.
The algorithm then enumerates each n-gram and
(n ? 1)-gram in y and accumulates its soft count
into the expected count, and finally obtains the pa-
rameters of q? by taking count ratios via (12).
Figure 4 shows an efficient version that exploits
the packed-forest structure of HG(x) in com-
puting the expected counts. Specifically, it first
runs the inside-outside procedure, which annotates
each node (say v) with both an inside weight ?(v)
and an outside weight ?(v). The inside-outside
also finds Z(x), the total weight of all derivations.
With these weights, the algorithm then explores
the hypergraph once more to collect the expected
10One can prove (12) via Lagrange multipliers, with q?(? |
h) constrained to be a normalized distribution for each h.
596
Brute-Force-MLE(HG(x ))
1 for y , d in HG(x)  each derivation
2 forw in y  each n-gram type
3  accumulate soft count
4 c?(w) + = cw(y) ? p(y, d | x)
5 c?(h(w)) + = cw(y) ? p(y, d | x)
6 q? ? MLE using formula (12)
7 return q?
Figure 3: Brute-force estimation of q?.
Dynamic-Programming-MLE(HG(x ))
1 run inside-outside on the hypergraph HG(x)
2 for v in HG(x)  each node
3 for e ? B(v)  each incoming hyperedge
4 ce ? pe ? ?(v)/Z(x)
5 for u ? T (e)  each antecedent node
6 ce ? ce ? ?(u)
7  accumulate soft count
8 forw in e  each n-gram type
9 c?(w) + = cw(e) ? ce
10 c?(h(w)) + = cw(e) ? ce
11 q? ? MLE using formula (12)
12 return q?
Figure 4: Dynamic programming estimation of q?. B(v) rep-
resents the set of incoming hyperedges of node v; pe repre-
sents the weight of the hyperedge e itself; T (e) represents
the set of antecedent nodes of hyperedge e. Please refer to
the text for the meanings of other notations.
counts. For each hyperedge (say e), it first gets the
posterior weight ce (see lines 4-6). Then, for each
n-gram type (say w), it increments the expected
count by cw(e) ? ce, where cw(e) is the number of
copies of n-gram w that are added by hyperedge
e, i.e., that appear in the yield of e but not in the
yields of any of its antecedents u ? T (e).
While there may be exponentially many deriva-
tions, the hypergraph data structure represents
them in polynomial space by allowing multiple
derivations to share subderivations. The algorithm
of Figure 4 may be run over this packed forest
in time O(|HG(x)|) where |HG(x)| is the hyper-
graph?s size (number of hyperedges).
3.2.3 Decoding with q?
When translating x at runtime, the q? constructed
from HG(x) will be used as a surrogate for p dur-
ing decoding. We want its most probable string:
y? = argmax
y
q?(y) (13)
Since q? is an n-gram model, finding y? is equiv-
alent to a shortest-path problem in a certain graph
whose edges correspond to n-grams (weighted
with negative log-probabilities) and whose ver-
tices correspond to (n? 1)-grams.
However, because q? only approximates p, y? of
(13) may be locally appropriate but globally inade-
quate as a translation of x. Observe, e.g., that an n-
gram model q?(y) will tend to favor short strings
y, regardless of the length of x. Suppose x = le
chat chasse la souris (?the cat chases the mouse?)
and q? is a bigram approximation to p(y | x). Pre-
sumably q?(the | START), q?(mouse | the), and
q?(END | mouse) are all large in HG(x). So the
most probable string y? under q? may be simply
?the mouse,? which is short and has a high proba-
bility but fails to cover x.
Therefore, a better way of using q? is to restrict
the search space to the original hypergraph, i.e.:
y? = argmax
y?T(x)
q?(y) (14)
This ensures that y? is a valid string in the origi-
nal hypergraph HG(x), which will tend to rule out
inadequate translations like ?the mouse.?
If our sole objective is to get a good approxi-
mation to p(y | x), we should just use a single
n-gram model q? whose order n is as large as pos-
sible, given computational constraints. This may
be regarded as favoring n-grams that are likely to
appear in the reference translation (because they
are likely in the derivation forest). However, in or-
der to score well on the BLEU metric for MT eval-
uation (Papineni et al, 2001), which gives partial
credit, we would also like to favor lower-order n-
grams that are likely to appear in the reference,
even if this means picking some less-likely high-
order n-grams. For this reason, it is useful to in-
terpolate different orders of variational models,
y? = argmax
y?T(x)
?
n
?n ? log q
?
n(y) (15)
where n may include the value of zero, in which
case log q?0(y)
def
= |y|, corresponding to a conven-
tional word penalty feature. In the geometric inter-
polation above, the weight ?n controls the relative
veto power of the n-gram approximation and can
be tuned using MERT (Och, 2003) or a minimum
risk procedure (Smith and Eisner, 2006).
Lastly, note that Viterbi and variational approx-
imation are different ways to approximate the ex-
act probability p(y | x), and each of them has
pros and cons. Specifically, Viterbi approxima-
tion uses the correct probability of one complete
597
derivation, but ignores most of the derivations in
the hypergraph. In comparison, the variational ap-
proximation considers all the derivations in the hy-
pergraph, but uses only aggregate statistics of frag-
ments of derivations. Therefore, it is desirable to
interpolate further with the Viterbi approximation
when choosing the final translation output:11
y? = argmax
y?T(x)
?
n
?n ? log q
?
n(y)
+ ?v ? log pViterbi(y | x) (16)
where the first term corresponds to the interpolated
variational decoding of (15) and the second term
corresponds to the Viterbi decoding of (4).12 As-
suming ?v > 0, the second term penalizes transla-
tions with no good derivation in the hypergraph.13
For n ? m, any of these decoders (14)?
(16) may be implemented efficiently by using the
n-gram variational approximations q? to rescore
HG(x)?preserving its hypergraph topology, but
modifying the hyperedge weights.14 While the
original weights gave derivation d a score of
log p(d | x), the weights as modified for (16)
will give d a score of
?
n ?n ? log q
?
n(Y(d)) + ?v ?
log p(d | x). We then find the best-scoring deriva-
tion and output its target yield; that is, we find
argmaxy?T(x) via Y(argmaxd?D(x)).
4 Variational vs. Min-Risk Decoding
In place of the MAP decoding, another commonly
used decision rule is minimum Bayes risk (MBR):
y? = argmin
y
R(y) = argmin
y
?
y?
l(y, y?)p(y? | x)
(17)
11It would also be possible to interpolate with the N -best
approximations (see Section 2.4), with some complications.
12Zens and Ney (2006) use a similar decision rule as here
and they also use posterior n-gram probabilities as feature
functions, but their model estimation and decoding are over
an N -best, which is trivial in terms of computation.
13Already at (14), we explicitly ruled out translations y
having no derivation at all in the hypergraph. However,
suppose the hypergraph were very large (thanks to a large
or smoothed translation model and weak pruning). Then
(14)?s heuristic would fail to eliminate bad translations (?the
mouse?), since nearly every string y ? ?? would be derived
as a translation with at least a tiny probability. The ?soft? ver-
sion (16) solves this problem, since unlike the ?hard? (14), it
penalizes translations that appear only weakly in the hyper-
graph. As an extreme case, translations not in the hypergraph
at all are infinitely penalized (log pViterbi(y) = log 0 =
??), making it natural for the decoder not to consider them,
i.e., to do only argmaxy?T(x) rather than argmaxy??? .
14One might also want to use the q?n or smoothed versions
of them to rescore additional hypotheses, e.g., hypotheses
proposed by other systems or by system combination.
where l(y, y?) represents the loss of y if the true
answer is y?, and the risk of y is its expected
loss.15 Statistical decision theory shows MBR is
optimal if p(y? | x) is the true distribution, while
in practice p(y? | x) is given by a model at hand.
We now observe that our variational decoding
resembles the MBR decoding of Tromble et al
(2008). They use the following loss function, of
which a linear approximation to BLEU (Papineni
et al, 2001) is a special case,
l(y, y?) = ?(?0|y|+
?
w?N
?wcw(y)?w(y
?)) (18)
where w is an n-gram type, N is a set of n-gram
types with n ? [1, 4], cw(y) is the number of oc-
currence of the n-gram w in y, and ?w(y?) is an
indicator function to check if y? contains at least
one occurrence of w. With the above loss func-
tion, Tromble et al (2008) derive the MBR rule16
y? = argmax
y
(?0|y|+
?
w?N
?wcw(y)g(w | x))
(19)
where g(w | x) is a specialized ?posterior? proba-
bility of the n-gram w, and is defined as
g(w | x) =
?
y?
?w(y
?)p(y? | x) (20)
Now, let us divide N , which contains n-gram
types of different n, into several subsets Wn, each
of which contains only the n-grams with a given
length n. We can now rewrite (19) as follows,
y? = argmax
y
?
n
?n ? gn(y | x) (21)
by assuming ?w = ?|w| and,
gn(y | x)=
{
|y| if n = 0
?
w?Wn g(w | x)cw(y) if n > 0
(22)
Clearly, their rule (21) has a quite similar form
to our rule (15), and we can relate (20) to (12) and
(22) to (11). This justifies the use of interpolation
in Section 3.2.3. However, there are several im-
portant differences. First, the n-gram ?posterior?
of (20) is very expensive to compute. In fact, it re-
quires an intersection between each n-gram in the
lattice and the lattice itself, as is done by Tromble
15The MBR becomes the MAP decision rule of (1) if a so-
called zero-one loss function is used: l(y, y?) = 0 if y = y?;
otherwise l(y, y?) = 1.
16Note that Tromble et al (2008) only consider MBR for a
lattice without hidden structures, though their method can be
in principle applied in a hypergraph with spurious ambiguity.
598
et al (2008). In comparison, the optimal n-gram
probabilities of (12) can be computed using the
inside-outside algorithm, once and for all. Also,
g(w | x) of (20) is not normalized over the history
of w, while q?(r(w) | h(w)) of (12) is. Lastly, the
definition of the n-gram model is different. While
the model (11) is a proper probabilistic model, the
function of (22) is simply an approximation of the
average n-gram precisions of y.
A connection between variational decoding and
minimum-risk decoding has been noted before
(e.g., Matsuzaki et al (2005)), but the derivation
above makes the connection formal.
DeNero et al (2009) concurrently developed
an alternate to MBR, called consensus decoding,
which is similar to ours in practice although moti-
vated quite differently.
5 Experimental Results
We report results using an open source MT toolkit,
called Joshua (Li et al, 2009), which implements
Hiero (Chiang, 2007).
5.1 Experimental Setup
We work on a Chinese to English translation task.
Our translation model was trained on about 1M
parallel sentence pairs (about 28M words in each
language), which are sub-sampled from corpora
distributed by LDC for the NIST MT evalua-
tion using a sampling method based on the n-
gram matches between training and test sets in
the foreign side. We also used a 5-gram lan-
guage model with modified Kneser-Ney smooth-
ing (Chen and Goodman, 1998), trained on a data
set consisting of a 130M words in English Giga-
word (LDC2007T07) and the English side of the
parallel corpora. We use GIZA++ (Och and Ney,
2000), a suffix-array (Lopez, 2007), SRILM (Stol-
cke, 2002), and risk-based deterministic annealing
(Smith and Eisner, 2006)17 to obtain word align-
ments, translation models, language models, and
the optimal weights for combining these models,
respectively. We use standard beam-pruning and
cube-pruning parameter settings, following Chi-
ang (2007), when generating the hypergraphs.
The NIST MT?03 set is used to tune model
weights (e.g. those of (16)) and the scaling factor
17We have also experimented with MERT (Och, 2003), and
found that the deterministic annealing gave results that were
more consistent across runs and often better.
Decoding scheme MT?04 MT?05
Viterbi 35.4 32.6
MBR (K=1000) 35.8 32.7
Crunching (N=10000) 35.7 32.8
Crunching+MBR (N=10000) 35.8 32.7
Variational (1to4gram+wp+vt) 36.6 33.5
Table 1: BLEU scores for Viterbi, Crunching, MBR, and vari-
ational decoding. All the systems improve significantly over
the Viterbi baseline (paired permutation test, p < 0.05). In
each column, we boldface the best result as well as all results
that are statistically indistinguishable from it. In MBR, K is
the number of unique strings. For Crunching and Crunch-
ing+MBR, N represents the number of derivations. On av-
erage, each string has about 115 distinct derivations. The
variational method ?1to4gram+wp+vt? is our full interpola-
tion (16) of four variational n-gram models (?1to4gram?), the
Viterbi baseline (?vt?), and a word penalty feature (?wp?).
? of (3),18 and MT?04 and MT?05 are blind test-
sets. We will report results for lowercase BLEU-4,
using the shortest reference translation in comput-
ing brevity penalty.
5.2 Main Results
Table 1 presents the BLEU scores under Viterbi,
crunching, MBR, and variational decoding. Both
crunching and MBR show slight significant im-
provements over the Viterbi baseline; variational
decoding gives a substantial improvement.
The difference between MBR and Crunch-
ing+MBR lies in how we approximate the distri-
bution p(y? | x) in (17).19 For MBR, we take
p(y? | x) to be proportional to pViterbi(y? | x) if y?
is among the K best distinct strings on that mea-
sure, and 0 otherwise. For Crunching+MBR, we
take p(y? | x) to be proportional to pcrunch(y? | x),
which is based on the N best derivations.
5.3 Results of Different Variational Decoding
Table 2 presents the BLEU results under different
ways in using the variational models, as discussed
in Section 3.2.3. As shown in Table 2a, decod-
ing with a single variational n-gram model (VM)
as per (14) improves the Viterbi baseline (except
the case with a unigram VM), though often not
statistically significant. Moreover, a bigram (i.e.,
?2gram?) achieves the best BLEU scores among
the four different orders of VMs.
The interpolation between a VM and a word
penalty feature (?wp?) improves over the unigram
18We found the BLEU scores are not very sensitive to ?,
contrasting to the observations by Tromble et al (2008).
19We also restrict T(x) to {y : p(y | x) > 0}, using the
same approximation for p(y | x) as we did for p(y? | x).
599
(a) decoding with a single variational model
Decoding scheme MT?04 MT?05
Viterbi 35.4 32.6
1gram 25.9 24.5
2gram 36.1 33.4
3gram 36.0? 33.1
4gram 35.8? 32.9
(b) interpolation between a single variational
model and a word penalty feature
1gram+wp 29.7 27.7
2gram+wp 35.5 32.6
3gram+wp 36.1? 33.1
4gram+wp 35.7? 32.8?
(c) interpolation of a single variational model, the
Viterbi model, and a word penalty feature
1gram+wp+vt 35.6? 32.8?
2gram+wp+vt 36.5? 33.5?
3gram+wp+vt 35.8? 32.9?
4gram+wp+vt 35.6? 32.8?
(d) interpolation of several n-gram VMs, the
Viterbi model, and a word penalty feature
1to2gram+wp+vt 36.6? 33.6?
1to3gram+wp+vt 36.6? 33.5?
1to4gram+wp+vt 36.6? 33.5?
Table 2: BLEU scores under different variational decoders
discussed in Section 3.2.3. A star ? indicates a result that is
significantly better than Viterbi decoding (paired permutation
test, p < 0.05). We boldface the best system and all systems
that are not significantly worse than it. The brevity penalty
BP in BLEU is always 1, meaning that on average y? is no
shorter than the reference translation, except for the ?1gram?
systems in (a), which suffer from brevity penalties of 0.826
and 0.831.
VM dramatically, but does not improve higher-
order VMs (Table 2b). Adding the Viterbi fea-
ture (?vt?) into the interpolation further improves
the lower-order models (Table 2c), and all the im-
provements over the Viterbi baseline become sta-
tistically significant. At last, interpolation of sev-
eral variational models does not yield much fur-
ther improvement over the best previous model,
but makes the results more stable (Table 2d).
5.4 KL Divergence of Approximate Models
While the BLEU scores reported show the prac-
tical utility of the variational models, it is also
interesting to measure how well each individual
variational model q(y) approximates the distribu-
tion p(y | x). Ideally, the quality of approxima-
tion should be measured by the KL divergence
KL(p ? q)
def
= H(p, q) ? H(p), where the cross-
entropy H(p, q)
def
= ?
?
y p(y | x) log q(y), and
Measure H(p, ?) Hd(p) H(p)
bits/word q?1 q
?
2 q
?
3 q
?
4 ?
MT?04 2.33 1.68 1.57 1.53 1.36 1.03
MT?05 2.31 1.69 1.58 1.54 1.37 1.04
Table 3: Cross-entropies H(p, q) achieved by various ap-
proximations q. The notation H denotes the sum of cross-
entropies of all test sentences, divided by the total number
of test words. A perfect approximation would achieve H(p),
which we estimate using the true Hd(p) and a 10000-best list.
the entropy H(p)
def
= ?
?
y p(y | x) log p(y | x).
Unfortunately H(p) (and hence KL = H(p, q) ?
H(p)) is intractable to compute. But, since H(p)
is the same for all q, we can simply use H(p, q)
to compare different models q. Table 3 reports the
cross-entropies H(p, q) for various models q.
We also report the derivational entropy
Hd(p)
def
= ?
?
d p(d | x) log p(d | x).
20 From this,
we obtain an estimate of H(p) by observing that
the ?gap? Hd(p) ? H(p) equals Ep(y)[H(d | y)],
which we estimate from our 10000-best list.
Table 3 confirms that higher-order variational
models (drawn from a larger family Q) approxi-
mate p better. This is necessarily true, but it is
interesting to see that most of the improvement is
obtained just by moving from a unigram to a bi-
gram model. Indeed, although Table 3 shows that
better approximations can be obtained by using
higher-order models, the best BLEU score in Ta-
bles 2a and 2c was obtained by the bigram model.
After all, p cannot perfectly predict the reference
translation anyway, hence may not be worth ap-
proximating closely; but p may do a good job
of predicting bigrams of the reference translation,
and the BLEU score rewards us for those.
6 Conclusions and Future Work
We have successfully applied the general varia-
tional inference framework to a large-scale MT
task, to approximate the intractable problem of
MAP decoding in the presence of spurious am-
biguity. We also showed that interpolating vari-
ational models with the Viterbi approximation can
compensate for poor approximations, and that in-
terpolating them with one another can reduce the
Bayes risk and improve BLEU. Our empirical re-
sults improve the state of the art.
20Both H(p, q) and Hd(p) involve an expectation over ex-
ponentially many derivations, but they can be computed in
time only linear in the size of HG(x) using an expectation
semiring (Eisner, 2002). In particular, H(p, q) can be found
as ?
?
d?D(x) p(d | x) log q(Y(d)).
600
Many interesting research directions remain
open. To approximate the intractable MAP de-
coding problem of (2), we can use different vari-
ational distributions other than the n-gram model
of (11). Interpolation with other models is also
interesting, e.g., the constituent model in Zhang
and Gildea (2008). We might also attempt to min-
imize KL(q ? p) rather than KL(p ? q), in order
to approximate the mode (which may be prefer-
able since we care most about the 1-best transla-
tion under p) rather than the mean of p (Minka,
2005). One could also augment our n-gram mod-
els with non-local string features (Rosenfeld et al,
2001) provided that the expectations of these fea-
tures could be extracted from the hypergraph.
Variational inference can also be exploited to
solve many other intractable problems in MT (e.g.,
word/phrase alignment and system combination).
Finally, our method can be used for tasks beyond
MT. For example, it can be used to approximate
the intractable MAP decoding inherent in systems
using HMMs (e.g. speech recognition). It can also
be used to approximate a context-free grammar
with a finite state automaton (Nederhof, 2005).
References
Cyril Allauzen, Mehryar Mohri, and Brian Roark.
2003. Generalized algorithms for constructing sta-
tistical language models. In ACL, pages 40?47.
Christopher M. Bishop. 2006. Pattern recognition and
machine learning. Springer.
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008.
A discriminative latent variable model for statistical
machine translation. In ACL, pages 200?208.
Francisco Casacuberta and Colin De La Higuera. 2000.
Computational complexity of problems on proba-
bilistic grammars and transducers. In ICGI, pages
15?24.
Stanley F. Chen and Joshua Goodman. 1998. An em-
pirical study of smoothing techniques for language
modeling. Technical report.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
John DeNero, David Chiang, and Kevin Knight. 2009.
Fast consensus decoding over translation forests. In
ACL-IJCNLP.
Jason Eisner. 2002. Parameter estimation for proba-
bilistic finite-state transducers. In ACL, pages 1?8.
Joshua Goodman. 1996. Efficient algorithms for pars-
ing the DOP model. In EMNLP, pages 143?152.
Liang Huang and David Chiang. 2007. Forest rescor-
ing: Faster decoding with integrated language mod-
els. In ACL, pages 144?151.
M. I. Jordan, Z. Ghahramani, T. S. Jaakkola, and L. K.
Saul. 1999. An introduction to variational meth-
ods for graphical models. In Learning in Graphical
Models. MIT press.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
NAACL, pages 48?54.
Zhifei Li and Sanjeev Khudanpur. 2008. A scalable
decoder for parsing-based machine translation with
equivalent language model state maintenance. In
ACL SSST, pages 10?18.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri
Ganitkevitch, Sanjeev Khudanpur, Lane Schwartz,
Wren Thornton, Jonathan Weese, and Omar. Zaidan.
2009. Joshua: An open source toolkit for parsing-
based machine translation. In WMT09, pages 135?
139.
Adam Lopez. 2007. Hierarchical phrase-based trans-
lation with suffix arrays. In EMNLP-CoNLL, pages
976?985.
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.
2005. Probabilistic CFG with latent annotations. In
ACL, pages 75?82.
Jonathan May and Kevin Knight. 2006. A better n-best
list: practical determinization of weighted finite tree
automata. In NAACL, pages 351?358.
Tom Minka. 2005. Divergence measures and message
passing. In Microsoft Research Technical Report
(MSR-TR-2005-173). Microsoft Research.
Mark-Jan Nederhof. 2005. A general technique to
train language models on language models. Com-
put. Linguist., 31(2):173?186.
Franz Josef Och and Hermann Ney. 2000. Improved
statistical alignment models. In ACL, pages 440?
447.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In ACL, pages 160?
167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. Bleu: a method for automatic eval-
uation of machine translation. In ACL, pages 311?
318.
Roni Rosenfeld, Stanley F. Chen, and Xiaojin Zhu.
2001. Whole-sentence exponential language mod-
els: A vehicle for linguistic-statistical integration.
Computer Speech and Language, 15(1).
Khalil Sima?an. 1996. Computational complexity
of probabilistic disambiguation by means of tree-
grammars. In COLING, pages 1175?1180.
David A. Smith and Jason Eisner. 2006. Minimum risk
annealing for training log-linear models. In ACL,
pages 787?794.
Andreas Stolcke. 2002. Srilm - an extensible language
modeling toolkit. In ICSLP, pages 901?904.
Roy Tromble, Shankar Kumar, Franz Och, and Wolf-
gang Macherey. 2008. Lattice Minimum Bayes-
Risk decoding for statistical machine translation. In
EMNLP, pages 620?629.
Richard Zens and Hermann Ney. 2006. N-gram poste-
rior probabilities for statistical machine translation.
In WMT06, pages 72?77.
Hao Zhang and Daniel Gildea. 2008. Efficient multi-
pass decoding for synchronous context free gram-
mars. In ACL, pages 209?217.
601
Proceedings of the ACL-IJCNLP 2009 Software Demonstrations, pages 25?28,
Suntec, Singapore, 3 August 2009.
c?2009 ACL and AFNLP
Demonstration of Joshua: An Open Source Toolkit
for Parsing-based Machine Translation
?
Zhifei Li, Chris Callison-Burch, Chris Dyer
?
, Juri Ganitkevitch
+
, Sanjeev Khudanpur,
Lane Schwartz
?
, Wren N. G. Thornton, Jonathan Weese, and Omar F. Zaidan
Center for Language and Speech Processing, Johns Hopkins University
? Computational Linguistics and Information Processing Lab, University of Maryland
+ Human Language Technology and Pattern Recognition Group, RWTH Aachen University
? Natural Language Processing Lab, University of Minnesota
Abstract
We describe Joshua (Li et al, 2009a)
1
,
an open source toolkit for statistical ma-
chine translation. Joshua implements all
of the algorithms required for transla-
tion via synchronous context free gram-
mars (SCFGs): chart-parsing, n-gram lan-
guage model integration, beam- and cube-
pruning, and k-best extraction. The toolkit
also implements suffix-array grammar ex-
traction and minimum error rate training.
It uses parallel and distributed computing
techniques for scalability. We also pro-
vide a demonstration outline for illustrat-
ing the toolkit?s features to potential users,
whether they be newcomers to the field
or power users interested in extending the
toolkit.
1 Introduction
Large scale parsing-based statistical machine
translation (e.g., Chiang (2007), Quirk et al
(2005), Galley et al (2006), and Liu et al (2006))
has made remarkable progress in the last few
years. However, most of the systems mentioned
above employ tailor-made, dedicated software that
is not open source. This results in a high barrier
to entry for other researchers, and makes experi-
ments difficult to duplicate and compare. In this
paper, we describe Joshua, a Java-based general-
purpose open source toolkit for parsing-based ma-
chine translation, serving the same role as Moses
(Koehn et al, 2007) does for regular phrase-based
machine translation.
?
This research was supported in part by the Defense Ad-
vanced Research Projects Agency?s GALE program under
Contract No. HR0011-06-2-0001 and the National Science
Foundation under grants No. 0713448 and 0840112. The
views and findings are the authors? alone.
1
Please cite Li et al (2009a) if you use Joshua in your
research, and not this demonstration description paper.
2 Joshua Toolkit
When designing our toolkit, we applied general
principles of software engineering to achieve three
major goals: Extensibility, end-to-end coherence,
and scalability.
Extensibility: Joshua?s codebase consists of
a separate Java package for each major aspect
of functionality. This way, researchers can focus
on a single package of their choosing. Fuur-
thermore, extensible components are defined by
Java interfaces to minimize unintended inter-
actions and unseen dependencies, a common hin-
drance to extensibility in large projects. Where
there is a clear point of departure for research,
a basic implementation of each interface is
provided as an abstract class to minimize
work necessary for extensions.
End-to-end Cohesion: An MT pipeline con-
sists of many diverse components, often designed
by separate groups that have different file formats
and interaction requirements. This leads to a large
number of scripts for format conversion and to
facilitate interaction between the components, re-
sulting in untenable and non-portable projects, and
hindering repeatability of experiments. Joshua, on
the other hand, integrates the critical components
of an MT pipeline seamlessly. Still, each compo-
nent can be used as a stand-alone tool that does not
rely on the rest of the toolkit.
Scalability: Joshua, especially the decoder, is
scalable to large models and data sets. For ex-
ample, the parsing and pruning algorithms are im-
plemented with dynamic programming strategies
and efficient data structures. We also utilize suffix-
array grammar extraction, parallel/distributed de-
coding, and bloom filter language models.
Joshua offers state-of-the-art quality, having
been ranked 4th out of 16 systems in the French-
English task of the 2009 WMT evaluation, both in
automatic (Table 1) and human evaluation.
25
System BLEU-4
google 31.14
lium 26.89
dcu 26.86
joshua 26.52
uka 25.96
limsi 25.51
uedin 25.44
rwth 24.89
cmu-statxfer 23.65
Table 1: BLEU scores for top primary systems on
the WMT-09 French-English Task from Callison-
Burch et al (2009), who also provide human eval-
uation results.
2.1 Joshua Toolkit Features
Here is a short description of Joshua?s main fea-
tures, described in more detail in Li et al (2009a):
? Training Corpus Sub-sampling: We sup-
port inducing a grammar from a subset
of the training data, that consists of sen-
tences needed to translate a particular test
set. To accomplish this, we make use of the
method proposed by Kishore Papineni (per-
sonal communication), outlined in further de-
tail in (Li et al, 2009a). The method achieves
a 90% reduction in training corpus size while
maintaining state-of-the-art performance.
? Suffix-array Grammar Extraction: Gram-
mars extracted from large training corpora
are often far too large to fit into available
memory. Instead, we follow Callison-Burch
et al (2005) and Lopez (2007), and use a
source language suffix array to extract only
rules that will actually be used in translating
a particular test set. Direct access to the suffix
array is incorporated into the decoder, allow-
ing rule extraction to be performed for each
input sentence individually, but it can also be
executed as a standalone pre-processing step.
? Grammar formalism: Our decoder as-
sumes a probabilistic synchronous context-
free grammar (SCFG). It handles SCFGs
of the kind extracted by Hiero (Chiang,
2007), but is easily extensible to more gen-
eral SCFGs (as in Galley et al (2006)) and
closely related formalisms like synchronous
tree substitution grammars (Eisner, 2003).
? Pruning: We incorporate beam- and cube-
pruning (Chiang, 2007) to make decoding
feasible for large SCFGs.
? k-best extraction: Given a source sentence,
the chart-parsing algorithm produces a hy-
pergraph representing an exponential num-
ber of derivation hypotheses. We implement
the extraction algorithm of Huang and Chi-
ang (2005) to extract the k most likely deriva-
tions from the hypergraph.
? Oracle Extraction: Even within the large
set of translations represented by a hyper-
graph, some desired translations (e.g. the ref-
erences) may not be contained due to pruning
or inherent modeling deficiency. We imple-
ment an efficient dynamic programming al-
gorithm (Li and Khudanpur, 2009) for find-
ing the oracle translations, which are most
similar to the desired translations, as mea-
sured by a metric such as BLEU.
? Parallel and distributed decoding: We
support parallel decoding and a distributed
language model that exploit multi-core and
multi-processor architectures and distributed
computing (Li and Khudanpur, 2008).
? Language Models: We implement three lo-
cal n-gram language models: a straightfor-
ward implementation of the n-gram scoring
function in Java, capable of reading stan-
dard ARPA backoff n-gram models; a na-
tive code bridge that allows the decoder to
use the SRILM toolkit to read and score n-
grams
2
; and finally a Bloom Filter implemen-
tation following Talbot and Osborne (2007).
? Minimum Error Rate Training: Joshua?s
MERT module optimizes parameter weights
so as to maximize performance on a develop-
ment set as measured by an automatic evalu-
ation metric, such as BLEU. The optimization
consists of a series of line-optimizations us-
ing the efficient method of Och (2003). More
details on the MERT method and the imple-
mentation can be found in Zaidan (2009).
3
2
The first implementation allows users to easily try the
Joshua toolkit without installing SRILM. However, users
should note that the basic Java LM implementation is not as
scalable as the SRILM native bridge code.
3
The module is also available as a standalone applica-
tion, Z-MERT, that can be used with other MT systems.
26
? Variational Decoding: spurious ambiguity
causes the probability of an output string
among to be split among many derivations.
The goodness of a string is measured by
the total probability of its derivations, which
means that finding the best output string is
computationally intractable. The standard
Viterbi approximation is based on the most
probable derivation, but we also implement
a variational approximation, which considers
all the derivations but still allows tractable
decoding (Li et al, 2009b).
3 Demonstration Outline
The purpose of the demonstration is 4-fold: 1) to
give newcomers to the field of statistical machine
translation an idea of the state-of-the-art; 2) to
show actual, live, end-to-end operation of the sys-
tem, highlighting its main components, targeting
potential users; 3) to illustrate, through visual aids,
the underlying algorithms, for those interested in
the technical details; and 4) to explain how those
components can be extended, for potential power
users who want to be familiar with the code itself.
The first component of the demonstration will
be an interactive user interface, where arbitrary
user input in a source language is entered into a
web form and then translated into a target lan-
guage by the system. This component specifically
targets newcomers to SMT, and demonstrates the
current state of the art in the field. We will have
trained multiple systems (for multiple language
pairs), hosted on a remote server, which will be
queried with the sample source sentences.
Potential users of the system would be inter-
ested in seeing an actual operation of the system,
in a similar fashion to what they would observe
on their own machines when using the toolkit. For
this purpose, we will demonstrate three main mod-
ules of the toolkit: the rule extraction module, the
MERT module, and the decoding module. Each
module will have a separate terminal window ex-
ecuting it, hence demonstrating both the module?s
expected output as well as its speed of operation.
In addition to demonstrating the functionality
of each module, we will also provide accompa-
nying visual aids that illustrate the underlying al-
gorithms and the technical operational details. We
will provide visualization of the search graph and
(Software and documentation at: http://cs.jhu.edu/
?
ozaidan/zmert.)
the 1-best derivation, which would illustrate the
functionality of the decoder, as well as alterna-
tive translations for phrases of the source sentence,
and where they were learned in the parallel cor-
pus, illustrating the functionality of the grammar
rule extraction. For the MERT module, we will
provide figures that illustrate Och?s efficient line
search method.
4 Demonstration Requirements
The different components of the demonstration
will be spread across at most 3 machines (Fig-
ure 1): one for the live ?instant translation? user
interface, one for demonstrating the different com-
ponents of the system and algorithmic visualiza-
tions, and one designated for technical discussion
of the code. We will provide the machines our-
selves and ensure the proper software is installed
and configured. However, we are requesting that
large LCD monitors be made available, if possi-
ble, since that would allow more space to demon-
strate the different components with clarity than
our laptop displays would provide. We will also
require Internet connectivity for the live demon-
stration, in order to gain access to remote servers
where trained models will be hosted.
References
Chris Callison-Burch, Colin Bannard, and Josh
Schroeder. 2005. Scaling phrase-based statisti-
cal machine translation to larger corpora and longer
phrases. In Proceedings of ACL.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the 2009
Workshop on Statistical Machine Translation. In
Proceedings of the Fourth Workshop on Statistical
Machine Translation, pages 1?28, Athens, Greece,
March. Association for Computational Linguistics.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
Jason Eisner. 2003. Learning non-isomorphic tree
mappings for machine translation. In Proceedings
of ACL.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Pro-
ceedings of the ACL/Coling.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proceedings of the International Work-
shop on Parsing Technologies.
27
We will rely on 3 workstations: 
one for the instant translation 
demo, where arbitrary input is 
translated from/to a language pair 
of choice (top); one for runtime 
demonstration of the system, with 
a terminal window for each of the 
three main components of the 
systems, as well as visual aids, 
such as derivation trees (left); and 
one (not shown) designated for 
technical discussion of the code.
Remote server 
hosting trained 
translation models
JHU
Grammar extraction
Decoder
M
E
R
T
Figure 1: Proposed setup of our demonstration. When this paper is viewed as a PDF, the reader may
zoom in further to see more details.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the ACL-2007 Demo and Poster Ses-
sions.
Zhifei Li and Sanjeev Khudanpur. 2008. A scalable
decoder for parsing-based machine translation with
equivalent language model state maintenance. In
Proceedings Workshop on Syntax and Structure in
Statistical Translation.
Zhifei Li and Sanjeev Khudanpur. 2009. Efficient
extraction of oracle-best translations from hyper-
graphs. In Proceedings of NAACL.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri
Ganitkevitch, Sanjeev Khudanpur, Lane Schwartz,
Wren Thornton, Jonathan Weese, and Omar Zaidan.
2009a. Joshua: An open source toolkit for parsing-
based machine translation. In Proceedings of the
Fourth Workshop on Statistical Machine Transla-
tion, pages 135?139, Athens, Greece, March. As-
sociation for Computational Linguistics.
Zhifei Li, Jason Eisner, and Sanjeev Khudanpur.
2009b. Variational decoding for statistical machine
translation. In Proceedings of ACL.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment templates for statistical machine
translation. In Proceedings of the ACL/Coling.
Adam Lopez. 2007. Hierarchical phrase-based trans-
lation with suffix arrays. In Proceedings of EMNLP-
CoLing.
Franz Josef Och. 2003. Minimum error rate training
for statistical machine translation. In Proceedings
of ACL.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005.
Dependency treelet translation: Syntactically in-
formed phrasal smt. In Proceedings of ACL.
David Talbot and Miles Osborne. 2007. Randomised
language modelling for statistical machine transla-
tion. In Proceedings of ACL.
Omar F. Zaidan. 2009. Z-MERT: A fully configurable
open source tool for minimum error rate training of
machine translation systems. The Prague Bulletin of
Mathematical Linguistics, 91:79?88.
28
Mandarin-English Information (MEI): 
Investigating Translingual Speech Retrieval 
Helen Meng, 1 Sanjeev Khudanpur, ~ Gina Levow, 3 Douglas W. Oard, 3 Hsin-Min Wang' 
1The Chinese University of Hong Kong, 2Johns Hopkins University, 
3University of Maryland and 4Academia Sinica (Taiwan) 
{hmmeng@se.cuhk.edu.hk, sanjeev@clsp.jhu.edu, gina@umiacs.umd.edu, 
oard@, glue.umd.edu, whm@ iis.sinica.edu.tw } 
Abstract 
We describe a system which supports 
English text queries searching for 
Mandarin Chinese spoken documents. 
This is one of the first attempts to tightly 
couple speech recognition with machine 
translation technologies for cross-media 
and cross-language retrieval. The 
Mandarin Chinese news audio are indexed 
with word and subword units by speech 
recognition. Translation of these multi- 
scale units can effect cross-language 
information retrieval. The integrated 
technologies will be evaluated based on 
the performance of translingnal speech 
retrieval. 
1. Introduction 
Massive quantities of audio and multimedia 
programs are becoming available. For example, 
in mid-February 2000, www.real.com listed 
1432 radio stations, 381 Internet-only 
broadcasters, and 86 television stations with 
Internet-accessible content, with 529 
broadcasting in languages other than English. 
Monolingual speech retrieval is now practical, as 
evidenced by services such as SpeechBot 
(speechbot.research.compaq.com), and it is clear 
that there is a potential demand for translingual 
speech retrieval if effective techniques can be 
developed. The Mandarin-English Information 
(MEI) project represents one of the first efforts 
in that direction. 
MEI is one of the four projects elected for 
the Johns Hopkins University (JHU) Summer 
Workshop 2000.1 Our research focus is on the 
integration of speech recognition and embedded 
translation technologies in the context of 
translingual speech retrieval. Possible 
applications of this work include audio and 
video browsing, spoken document retrieval, 
automated routing of information, and 
automatically alerting the user when special 
events occur. 
At the time of this writing, most of the MEI 
team members have been identified. This paper 
provides an update beyond our first proposal 
\[Meng et al, 2000\]. We present some ongoing 
work of our current eam members, as well as 
our ideas on an evolving plan for the upcoming 
JHU Summer Workshop 2000. We believe the 
input from the research community will benefit 
us greatly in formulating ourfinal plan. 
2. Background 
2.1 Translingual Information Retrieval 
The earliest work on large-vocabulary cross- 
language information retrieval from free-text 
(i,e., without manual topic indexing) was 
reported in 1990 \[Landauer and Littman, 1990\], 
and the topic has received increasing attention 
over the last five years \[Oard and Diekema, 
1998\]. Work on large-vocabulary retrieval from 
recorded speech is more recent, with some initial 
work reported in 1995 using subword indexing 
\[Wechsler and Schauble, 1995\], followed by the 
first TREC 2 Spoken Document Retrieval (SDR) 
I http://www.clsp,jhu.edu/ws2000/ 
2 Text REtrieval Conference, http://trec.nist.gov 
23 
evaluation \[Garofolo et al, 2000\]. The Topic 
Detection and Tracking (TDT) evaluations, 
which started in 1998, fall within our definition 
of speech retrieval for this purpose, differing 
from other evaluations principally in the nature 
of the criteria that human assessors use when 
assessing the relevance of a news stow to an 
information eed. In TDT, stories are assessed 
for relevance to an event, while in TREC stories 
are assessed for relevance to an explicitly stated 
information eed that is often subject- rather 
than event-oriented. 
The TDT-33 evaluation marked the first 
case of translingual speech retrieval - the task of 
finding information in a collection of recorded 
speech based on evidence of the information 
need that might be expressed (at least partially) 
in a different language. Translingual speech 
retrieval thus merges two lines of research that 
have developed separately until now. In the 
TDT-3 topic tracking evaluation, recognizer 
transcripts which have recognition errors were 
available, and it appears that every team made 
use of them. This provides a valuable point of 
reference for investigation of techniques that 
more tightly couple speech recognition with 
translingual retrieval. We plan to explore one 
way of doing this in the Mandarin-English 
Information (MEI) project. 
2.2 The Chinese Language 
In order to retrieve Mandarin audio documents, 
we should consider a number of linguistic 
characteristics of the Chinese language: 
The Chinese language has many dialects. 
Different dialects are characterized by their 
differences in the phonetics, vocabularies and 
syntax. Mandarin, also known as Putonglma 
("the common language"), is the most widely 
used dialect. Another major dialect is Cantonese, 
predominant in Hong Kong, Macau, South 
China and many overseas Chinese communities. 
Chinese is a syllable-based language, 
where each syllable carries a lexical tone. 
Mandarin has about 400 base syllables and four 
lexical tones, plus a "light" tone for reduced 
syllables. There are about 1,200 distinct, tonal 
syllables for Mandarin. Certain syllable-tone 
3 http://morph.ldc.upenn.edu/Projects/TDT3/ 
combinations are non-existent in the language. 
The acoustic correlates of the lexical tone 
include the syllable's fundamental frequency 
(pitch contour) and duration. However, these 
acoustic features are also highly dependent on 
prosodic variations of spoken utterances. 
The structure of Mandarin (base) syllables 
is (CG)V(X), where (CG) the syllable onset - C 
the initial consonant, G is the optional medial 
glide, V is the nuclear vowel, and X is the coda 
(which may be a glide, alveolar nasal or velar 
nasal). Syllable onsets and codas are optional. 
Generally C is known as the syllable initial, and 
the rest (GVX) syllable final. 4 Mandarin has 
approximately 21 initials and 39 finals. 5 
In its written form, Chinese is a sequence 
of characters. A word may contain one or more 
characters. Each character is pronounced as a 
tonal syllable. The character-syllable mapping is 
degenerate. On one hand, a given character may 
have multiple syllable pronunciations - for 
example, the character/d" may be pronounced as 
/hang2/, 6/hang4/, or/xing2/. On the other hand, 
a given tonal syllable may correspond to 
multiple characters. Consider the two-syllable 
pronunciation/fu4 shu4/, which corresponds toa 
two-character word. Possible homophones 
include ~, ,  (meaning "rich"), ~ ~tR, ("negative 
number"), ~1~1~, ("complex number" or 
"plural"), ~1~ ("repeat"). 7 
Aside from homographs and homophones, 
another source of ambiguity in the Chinese 
language is the definition of a Chinese word. 
The word has no delimiters, and the distinction 
between a word and a phrase is often vague. The 
lexical structure of the Chinese word is very 
different compared to English. Inflectional 
forms are minimal, while morphology and word 
derivations abide by a different set of rules. A 
word may inherit the syntax and semantics of 
(some of) its compositional characters, for 
4 http://m?rph'ldc'upenn'edu/Pr?jects/Chinese/intr?'html 
5 The corresponding linguistic haracteristics of Cantonese 
are very similar. 
6 These are Mandarin pinyin, the number encodes the tone 
of the syllable. 
7 Example drawn from \[Leung, 1999\]. 
24 
example, 8 ~ means red (a noun or an 
adjective), ~., means color (a noun), and ~. ,  
together means "the color red"(a noun) or 
simply "red" (an adjective). Alternatively, a 
word may take on totally different 
characteristics of its own, e.g. ~. means east (a 
noun or an adjective), ~ means west (a noun or 
an adjective), and .~.~ together means thing (a 
noun). Yet another case is where the 
compositional characters of a word do not form 
independent lexical entries in isolation, e.g. D~ 
means fancy (a verb), but its characters do not 
occur individually. Possible ways of deriving 
new words from characters are legion. The 
problem of identifying the words string in a 
character sequence is known as the segmentation 
/ tokenization problem. Consider the syllable 
string: 
/zhe4 yil wan3 hui4 ru2 chang2 ju3 xing2/ 
The corresponding character string has three 
possible segmentations - all are correct, but each 
involves a distinct set of words: 
(Meaning: It will be take place tonight as usual.) 
(Meaning: The evening banquet will take place 
as usual.) 
(Meaning: If this evening banquet akes place 
frequently...) 
The above considerations lead to a number 
of techniques we plan to use for our task. We 
concentrate on three equally critical problems 
related to our theme of translingual speech 
retrieval: (i) indexing Mandarin Chinese audio 
with word and subword units, (ii) translating 
variable-size units for cross-language 
information retrieval, and (iii) devising effective 
retrieval strategies for English text queries and 
Mandarin Chinese news audio. 
3. Multiscale Audio Indexing 
A popular approach to spoken document 
retrieval is to apply Large-Vocabulary 
s Examples drawn from \[Meng and Ip, 1999\]. 
Continuous Speech Recognition (LVCSR) 9 for 
audio indexing, followed by text retrieval 
techniques. Mandarin Chinese presents a 
challenge for word-level indexing by LVCSR, 
because of the ambiguity in tokenizing a 
sentence into words (as mentioned earlier). 
Furthermore, LVCSR with a static vocabulary is
hampered by the out-of-vocabulary (OOV) 
problem, especially when searching sources with 
topical coverage as diverse as that found in 
broadcast news. 
By virtue of the monosyllabic nature of the 
Chinese language and its dialects, the syllable 
inventory can provide a complete phonological 
coverage for spoken documents, and circumvent 
the OOV problem in news audio indexing, 
offering the potential for greater recall in 
subsequent retrieval. The approach thus supports 
searches for previously unknown query terms in 
the indexed audio. 
The pros and cons of subword indexing for 
an English spoken document retrieval task was 
studied in \[Ng, 2000\]. Ng pointed out that the  
exclusion of lexical knowledge when subword 
indexing is performed in isolation may adversely 
impact discrimination power for retrieval, but 
that some of that impact can be mitigated by 
modeling sequential constraints among subword 
units. We plan to investigate the efficacy of 
using both word and subword units for 
Mandarin audio indexing \[Meng et al, 2000\]. 
Although Ng found that such an approach 
produced little gain over words alone for 
English, the structure of Mandarin Chinese may 
produce more useful subword features. 
3.1 Modeling Syllable Sequence Constraints 
We have thus far used overlapping syllable N- 
grams for spoken document retrieval for two 
Chinese dialects - Mandarin and Cantonese. 
Results on a known-item retrieval task with over 
1,800 error-free news transcripts \[Meng et al, 
1999\] indicate that constraints from overlapping 
bigrams can yield significant improvements in 
retrieval performance over syllable unigrams, 
producing retrieval performance competitive 
9 The lexicon size of a typical large-vocabulary 
continuous speech recognizer can range from 10,000 
to 100,000 word forms. 
25 
with that obtained using automatically tokenized 
Chinese words. 
The study in \[Chen, Wang and Lee, 2000\] 
also used syllable pairs with skipped syllables in 
between. This is because many Chinese 
abbreviations are derived from skipping 
characters, e.g. J .~:~.~t:~  ~ National 
Science Council" can be abbreviated as l~r~ 
(including only the first, third and the last 
characters). Moreover, synonyms often differ by 
one or two characters, e.g. both ~ ' /~4~ and 
~.~,,Ag mean "Chinese culture". Inclusion o f  
these "skipped syllable pairs" also contributed to
retrieval performance. 
When modeling sequential syllable 
constraints, lexical constraints on recognized 
words may be helpful. We thus plan to exp\]Iore 
the potential for integrated sequential model\]ling 
of both words and syllables \[Meng et al, 20013\]. 
4. Multiseale Embedded Translation 
Figures 1 and 2 illustrate two translingual 
retrieval strategies. In query translation, English 
text queries are transformed into Mandarin and 
then used to retrieve Mandarin documents. For 
document translation, Mandarin documents are 
translated into English before they are indexed 
and then matched with English queries. 
McCarley has reported improved effectiveness 
from techniques that couple the two techniques 
\[McCarley, 1999\], but time constraints may 
limit us to explonng only the query translation 
strategy dunng the six-week Workshop. 
4,1 Word  Translat ion 
While we make use of sub-word 
transcription tosmooth out-of-vocabulary(OOV) 
problems in speech recognition as described 
above, and to alleviate the OOV problem :for 
translation as we discuss in the next section, 
accurate translation generally relies on the 
additional information available at the word and 
phrase levels. Since the "bag of words" 
information retrieval techniques do not 
incorporate any meaningful degree of language 
understanding to assess similarity between 
queries and documents, a word-for-word (or, 
more generally, term-for-term) embedded 
translation approach can achieve a useful level 
of effectiveness for many translingual retrieval 
applications \[Oard and Diekema, 1998\]. 
We have developed such a technique for the 
TDT-3 topic tracking evaluation \[Levow and 
Oard, 2000\]. For that work we extracted an 
enriched bilingual Mandarin-English term list by 
combining two term lists: (i) A list assembled 
by the Linguistic Data Consortium from freely 
available on-line resources; and (ii) entries from 
the CETA file (sometimes referred to as 
"Optilex"). This is a Chinese to English 
translation resource that was manually compiled 
by a team of linguists from more than 250 text 
sources, including special and general-purpose 
print dictionaries, and other text sources uch as 
newspapers. The CETA file contains over 
250,000 entries, but for our lexical work we 
extracted a subset of those entries drawn from 
contemporary general-purpose sources. We also 
excluded efinitions uch as "particle indicating 
a yes/no question." Our resulting Chinese to 
English merged bilingual term list contains 
translations for almost 200,000 Chinese terms, 
with average of almost two translation 
alternatives per term. We have also used the 
same resources to construct an initial English to 
Chinese bilingual term list that we plan to refine 
before the Workshop. 
Three significant challenges faced by term- 
to-term translation systems are term selection in 
the source language, the source language 
coverage of the bilingual term list, and 
translation selection in the target language when 
more than one alternative translation is known. 
Word segmentation is a natural by-product of 
large vocabulary Mandarin speech recognition, 
and white space provides word boundaries for 
the English queries. We thus plan to choose 
words as our basic term set, perhaps augmenting 
this with the multiword expressions found in the 
bilingual term list. 
Achieving adequate source language 
coverage is challenging in news retrieval 
applications of the type modelled by TDT, 
because proper names and technical terms that 
may not be present in general-purpose lexical 
resources often provide important retrieval cues. 
Parallel (translation equivalent) corpora have 
proven to be a useful source of translation 
26 
equivalent terms, but obtaining appropriate 
domain-specific parallel corpora in electronic 
form may not be practical in some applications. 
We therefore plan to investigate the use of 
comparable corpora to learn translation 
equivalents, based on techniques in \[Fung, 
1998\]. Subword translation, described below, 
provides a complementary way of handling 
terms for which translation equivalents cannot 
be reliably extracted from the available 
comparable corpora. 
One way of dealing with multiple 
translations is to weight the alternative 
translations using either a statistical translation 
model trained on parallel or comparable corpora 
to estimate translation probability conditioned 
on the source language term. When such 
resources are not sufficiently informative, it is 
generally possible to back off to an 
unconditioned preference statistic based on 
usage frequency of each possible translation i  a 
representative monolingual corpus in the target 
language. In retrospective r trieval applications 
the collection being searched can be used for 
this purpose. We have applied simple versions 
of this approach with good results \[Levow and 
Oard, 2000\]. 
We have recently observed that a simpler 
technique introduced by \[Pirkola, 1998\] can 
produce xcellent results. The key idea is to use 
the structure of the lexicon, in which several 
target language terms can represent a single 
source language term, to induce structure in the 
translated query that the retrieval system can 
automatically exploit. In essence, the translated 
query becomes a bag of bags of terms, where 
each smaller bag corresponds to the set of 
possible translations for one source-language 
term. We plan to implement his structured 
query translation approach using the Inquery 
\[Callan, 1992\] "synonym" operator in the same 
manner as \[Pirkola, 1998\], and to the potential to 
extend the technique to accommodate alternative 
recognition hypothesis and subword units as 
well: 
4.2 Subword  Translat ion 
Since Mandarin spoken documents can be 
indexed with both words and subwords, the 
translation (or "phonetic transliteration") of 
subword units is of particular interest. We plan 
to make use of cross-language phonetic 
mappings derived from English and Mandarin 
pronunciation rules for this purpose. This should 
be especially useful for handling named entities 
in the queries, e.g. names of people, places and 
organizations, etc. which are generally important 
for retrieval, but may not be easily translated. 
Chinese translations of English proper nouns 
may involve semantic as well as phonetic 
mappings. For example, "Northern Ireland" is 
translated as :~b~ttlM - -  where the first 
character ~ means 'north', and the remaining 
characters ~tllllll are pronounced as /ai4-er3- 
lan2L Hence the translation is both semantic 
and phonetic. When Chinese translations strive 
to attain phonetic similarity, the mapping may 
be inconsistent. For example, consider the 
translation of "Kosovo" - sampling Chinese 
newspapers in China, Taiwan and Hong Kong 
produces the following translations: 
~-~r~ /kel-suo3-wo4?, ~-~ /kel-suo3-fo2/, 
~'~&/kel-suo3-ful/f l4"~dt/kel-suo3-fu2/, or 
~/ke  1-suo3-fo2/. 
As can be seen, there is no systematic 
mapping to the Chinese character sequences, but 
the translated Chinese pronunciations bear some 
resemblance to the English pronunciation (/k ow 
s ax vow/). In order to support retrieval under 
these circumstances, the approach should 
involve approximate matches between the 
English pronunciation and the Chinese 
pronunciation. The matching algorithm should 
also accommodate phonological variations. 
Pronunciation dictionaries, or pronunciation 
generation tools for both English words and 
Chinese words / characters will be useful for the 
matching algorithm. We can probably leverage 
off of ideas in the development of universal 
speech recognizers \[Cohen et al, 1997\]. 
5. Mulfiscale Retrieval 
5.1 Coupling Words and Subwords 
We intend to use both words and subwords for 
retrieval. Loose coupling would involve separate 
retrieval runs using words and subwords, 
producing two ranked lists, followed by list 
merging using techniques such as those explored 
by \[Voorhees, 1995\]. Tight coupling, by 
27 
contrast, would require creation of a unified 
index containing both word and subword units, 
resulting in a single ranked list. We hope to 
explore both techniques during the Workshop. 
5.2 Imperfect Indexing and Translat ion 
It should be noted that speech recognition 
exacerbates uncertainty when indexing audio, 
and that translation or transliteration exacerbates 
uncertainty when translating queries and/or 
documents. To achieve robustness for retrieval, 
we have tried three techniques that we have 
found useful: (i) Syllable lattices were used in 
\[Wang, 1999\] and \[Chien et al, 2000\] for 
monolingual Chinese retrieval experiments. The 
lattices were pruned to constrain the search 
space, but were able to achieve robust retrieval 
based on imperfect recognized transcripts. (ii) 
Query expansion, in which syllable transcription 
were expanded to include possibly confusable 
syllable sequences based on a syllable confusion 
matrix derived from recognition errors, was used 
in \[Meng et al, 1999\]. (iii) We have expanded 
the document representation using terms 
extracted from similar documents in a 
comparable collection \[Levow and Oard, 2000\], 
and similar techniques are known to work well 
in the case of query translation (Ballesteros and 
Croft, 1997). We hope to add to this set: of 
techniques by exploring the potential for query 
expansion based on cross-language phonetic 
mapping. 
6. Using the TDT-3 Collection 
We plan to use the TDT-2 collection for 
development testing and the TDT-3 collection 
for evaluation. Both collections provide 
documents from two English newswire sources, 
six English broadcast news audio sources, two 
Mandarin Chinese newswire sources, and one 
Mandarin broadcast news source (Voice of 
America). Manually established story 
boundaries are available for all audio 
collections, and we plan to exploit that 
information to simplify our experiment design. 
The TDT-2 collection includes complete 
relevance assessments for 20 topics, and the 
TDT-3 collection provides the same for 60 
additional topics, 56 of which have at least one 
relevant audio story. For each topic, at least four 
English stories and four Chinese stories are 
known. 
We plan to automatically derive text queries 
based on one or more English stories that are 
presented as exemplars, and to use those queries 
to search the Mandarin audio collection. 
Manually constructed queries will provide a 
contrastive condition. Unlike the TDT "topic 
tracking" task in which stories must be declared 
relevant or not relevant in the order of their 
arrival, we plan to perform retrospective 
retrieval experiments in which all documents are 
known when the query is issued. By relaxing 
the temporal ordering of the TDT topic tracking 
task, we can meaningfully search for Mandarin 
Chinese stories that may have arrived before the 
exemplar story or stories. We thus plan to report 
ranked retrieval measures of effectiveness uch 
as average precision in addition to the detection 
statistics (miss and false alarm) typically 
reported in TDT. 
7.  Summary 
This paper presents our current ideas and 
evolving plan for the MEI project, to take place 
at the Johns Hopkins University Summer 
Workshop 2000. Translingual speech retrieval is 
a long-term research direction, and our team 
looks forward to jointly taking an initial step to 
tackle the problem. The authors welcome all 
comments and suggestions, aswe strive to better 
define the problem in preparation for the six- 
week Workshop. 
Acknowledgments 
The authors wish to thank Patrick Schone, Erika 
Grams, Fred Jelinek, Charles Wayne, Kenney 
? Ng, John Garofolo, and the participants in the 
December 1999 WS2000 planning meeting and 
the TDT-3 workshop for their many helpful 
suggestions. The Hopkins Summer Workshop 
series is supported by grants from the National 
Science Foundation. Our results reported in this 
paper eference thesis work in progress of Wai- 
Kit Lo (Ph.D. candidate, The Chinese Unversity 
of Hong Kong) and Berlin Chen (Ph.D. 
candidate, National Taiwan University). 
28 
References 
Ballesteros and W. B. Croft, "Phrasal 
Translation and Query Expansion Techniques 
for Cross-Language Information Retrieval," 
Proceedings ofACM SIGIR, 1997. 
Callan, J. P., W. B. Croft, and S. M. Harding, 
"The INQUERY Retrieval System," 
Proceedings of the 3rd International Conference 
on Database and Expert Systems Applications, 
1992. 
Carbonnell, J., Y. Yang, R. Frederking and R.D. 
Brown, "Translingual Information Retrieval: A 
Comparative Evaluation," Proceedings ofIJCAI, 
1997. 
Chen, B., H.M. Wang, and L.S. Lee, "Retrieval 
of Broadcast News Speech in Mandarin Chinese 
Collected in Taiwan using Syllable-Level 
Statistical Characteristics," Proceedings of 
ICASSP, 2000. 
Chien, L. F., H. M. Wang, B. R. Bai, and S. C. 
Lin, "A Spoken-Access Approach for Chinese 
Text and Speech Information Retrieval," Journal 
of the American Society for Information 
Science, 51 (4), pp. 313-323, 2000. 
Choy, C. Y., "Acoustic Units for Mandarin 
Chinese Speech Recognition," M.Phil. Thesis, 
The Chinese University of Hong Kong, Hong 
Kong SAR, China, 1999. 
Cohen, P., S. Dharanipragada, J. Gros, M. 
Mondowski, C. Neti, S. Roukos and T. Ward, 
"Towards a Universal Speech Recognizer for 
Multiple Languages," Proceedings of ASRU, 
1997. 
Fung, P., "A Statistical View on Bilingual 
Lexicon Extraction: From parallel corpora to 
non-parallel corpora," Proceedings of AMTA, 
1998. 
Garofolo, J.S., Auzanne, G.P., Voorhees, E.M., 
"The TREC Spoken Document Retrieval Track: 
A Success Story," Proceedings of the Recherche 
d'informations A sistre par Ordinateur: Content- 
Based Multimedia Information Access 
Conference, April 12-14, 2000,to be published. 
Knight, K. and J. Graehl, "Machine 
Transliteration," Proceedings ofACL, 1997. 
Landauer, T. K. and M.L. Littman, "Fully 
Automatic Cross-Language Document Retrieval 
Using Latent Semantic Indexing," Proceedings 
of the 6 th Annual Conference of the UW Centre 
for the New Oxford English Dictionary, 1990. 
Leung, R., "Lexical Access for Large 
Vocabulary Chinese Speech Recognition," M. 
Phil. Thesis, The Chinese University of Hong 
Kong, Hong Kong SAR, China 1999. 
Levow, G. and D.W. Oard, "Translingual Topic 
Tracking with PRISE," Working notes of the 
DARPA TDT-3 Workshop, 2000. 
Lin, C. H., L. S. Lee, and P. Y. Ting, "A New 
Framework for Recognition of Mandarin 
Syllables with Tones using Sub-Syllabic Units," 
Proceedings ofICASSP, 1993. 
Liu~ F. H., M. Picheny, P. Srinivasa, M. 
Monkowski and J. Chen, "Speech Recognition 
on Mandarin Call Home: A Large-Vocabulary, 
Conversational, nd Telephone Speech Corpus," 
Proceedings ofICASSP, 1996. 
McCarley, S., "Should we Translate the 
Documents or the Queries in Cross-Language 
Information Retrieval," Proceedings of ACL, 
1999. 
Meng, H. and C. W. Ip, "An Analytical Study o f  
Transformational Tagging of Chinese Text," 
Proceedings of the Research On Computational 
Lingustics (ROCLING) Conference, 1999. 
Meng, H., W. K. Lo, Y. C. Li and P. C. Ching, 
"A Study on the Use of Syllables for Chinese 
Spoken Document Retrieval," Technical Report 
SEEM1999-11, The Chinese University of Hong 
Kong, 1999. 
Meng, H., Khudanpur, S., Oard, D. W. and 
Wang, H. M., "Mandarin-English Information 
(MEI)," Working notes of the DARPA TDT-3 
Workshop, 2000. 
Ng, K., "Subword-based Approaches for Spoken 
Document Retrieval," Ph.D. Thesis, MIT, 
February 2000. 
Oard, D. W. and A.R. Diekema, "Cross- 
Language Information Retrieval," Annual 
Review of Information Science and Technology, 
vol.33, 1998. 
Pirkola, A., "The effects of query structure and 
dictionary setups in dictionary-based cross- 
language information retrieval," Proceedings of 
ACM SIGIR, 1998. 
Sheridan P. and J. P. Ballerini, "Experiments in
Multilingual Information Retrieval using the 
29 
SPIDER System," Proceedings of ACM SIGIR, 
1996. 
Voorhees, E., "Learning Collection Fusion 
Strategies," Proceedings of SIGIR, 1995. 
Wang, H. M., "Retrieval of Mandarin Spoken 
Documents Based on Syllable Lattice 
Matching," Proceedings of the Fourth 
International Workshop on Information 
Retrieval in Asian Languages, 1999. 
Wechsler, M. and P. Schaiible, "Speech 
Retrieval Based on Automatic Indexing," 
Proceedings of MIRO- 1995. 
English Text Queries 
(words) 
Words that are present entities and unknown words 
translation dictionary 
\[ Trans'a~on I I Transliteration 
Mand~Sn Queries (with words and syllables) 
Mandarin Spoken Documents \[ 
(indexed with word and subword units) 
"7 
Information Retrieval 
Engine 
I Evaluate Retrieval 
Performance 
Figure 1. Query translation strategy. 
Mandarin Spoken Documents 
(indexed with word and subword units) 
l 
Translation 
Documents in 
English 
English Text Queries 
(words) 
I Information Retrieval 
Engine 
Evaluate 
Retrieval 
Performance 
Figure 2. Document translation strategy. 
3O 
Cross-Lingual Lexical Triggers in Statistical Language Modeling  
Woosung Kim
The Johns Hopkins University
3400 N. Charles St., Baltimore, MD
woosung@cs.jhu.edu
Sanjeev Khudanpur
The Johns Hopkins University
3400 N. Charles St., Baltimore, MD
khudanpur@jhu.edu
Abstract
We propose new methods to take advan-
tage of text in resource-rich languages
to sharpen statistical language models in
resource-deficient languages. We achieve
this through an extension of the method
of lexical triggers to the cross-language
problem, and by developing a likelihood-
based adaptation scheme for combining
a trigger model with an  -gram model.
We describe the application of such lan-
guage models for automatic speech recog-
nition. By exploiting a side-corpus of con-
temporaneous English news articles for
adapting a static Chinese language model
to transcribe Mandarin news stories, we
demonstrate significant reductions in both
perplexity and recognition errors. We
also compare our cross-lingual adaptation
scheme to monolingual language model
adaptation, and to an alternate method for
exploiting cross-lingual cues, via cross-
lingual information retrieval and machine
translation, proposed elsewhere.
1 Data Sparseness in Language Modeling
Statistical techniques have been remarkably suc-
cessful in automatic speech recognition (ASR) and
natural language processing (NLP) over the last two
decades. This success, however, depends crucially

This research was supported by the National Science Foun-
dation (via Grant No
?
ITR-0225656 and IIS-9982329) and the
Office of Naval Research (via Contract No
?
N00014-01-1-0685).
on the availability of accurate and large amounts
of suitably annotated training data and it is difficult
to build a usable statistical model in their absence.
Most of the success, therefore, has been witnessed
in the so called resource-rich languages. More re-
cently, there has been an increasing interest in lan-
guages such as Mandarin and Arabic for ASR and
NLP, and data resources are being created for them
at considerable cost. The data-resource bottleneck,
however, is likely to remain for a majority of the
world?s languages in the foreseeable future.
Methods have been proposed to bootstrap acous-
tic models for ASR in resource deficient languages
by reusing acoustic models from resource-rich lan-
guages (Schultz and Waibel, 1998; Byrne et al,
2000). Morphological analyzers, noun-phrase chun-
kers, POS taggers, etc., have also been developed
for resource deficient languages by exploiting trans-
lated or parallel text (Yarowsky et al, 2001). Khu-
danpur and Kim (2002) recently proposed using
cross-lingual information retrieval (CLIR) and ma-
chine translation (MT) to improve a statistical lan-
guage model (LM) in a resource-deficient language
by exploiting copious amounts of text available in
resource-rich languages. When transcribing a news
story in a resource-deficient language, their core
idea is to use the first pass output of a rudimentary
ASR system as a query for CLIR, identify a contem-
poraneous English document on that news topic, fol-
lowed by MT to provide a rough translation which,
even if not fluent, is adequate to update estimates of
word frequencies and the LM vocabulary. They re-
port up to a 28% reduction in perplexity on Chinese
text from the Hong Kong News corpus.
In spite of their considerable success, some short-
comings remain in the method used by Khudanpur
and Kim (2002). Specifically, stochastic translation
lexicons estimated using the IBM method (Brown
et al, 1993) from a fairly large sentence-aligned
Chinese-English parallel corpus are used in their ap-
proach ? a considerable demand for a resource-
deficient language. It is suggested that an easier-
to-obtain document-aligned comparable corpus may
suffice, but no results are reported. Furthermore, for
each Mandarin news story, the single best match-
ing English article obtained via CLIR is translated
and used for priming the Chinese LM, no matter
how good the CLIR similarity, nor are other well-
matching English articles considered. This issue
clearly deserves further attention. Finally, ASR re-
sults are not reported in their work, though their pro-
posed solution is clearly motivated by an ASR task.
We address these three issues in this paper.
Section 2 begins, for the sake of completeness,
with a review of the cross-lingual story-specific LM
proposed by Khudanpur and Kim (2002). A notion
of cross-lingual lexical triggers is proposed in Sec-
tion 3, which overcomes the need for a sentence-
aligned parallel corpus for obtaining translation lex-
icons. After a brief detour to describe topic-
dependent LMs in Section 4, a description of the
ASR task is provided in Section 5, and ASR results
on Mandarin Broadcast News are presented in Sec-
tion 6. The issue of how many English articles to
retrieve and translate into Chinese is resolved by a
likelihood-based scheme proposed in Section 6.1.
2 Cross-Lingual Story-Specific LMs
For the sake of illustration, consider the task of
sharpening a Chinese language model for transcrib-
ing Mandarin news stories by using a large corpus
of contemporaneous English newswire text. Man-
darin Chinese is, of course, not resource-deficient
for language modeling ? 100s of millions of words
are available on-line. However, we choose it for our
experiments partly because it is sufficiently different
from English to pose a real challenge, and because
the availability of large text corpora in fact permits
us to simulate controlled resource deficiency.
Let 
	
	
	 denote the text of  test sto-
ries to be transcribed by an ASR system, and let

	
	
	
 denote their corresponding or aligned
English newswire articles. Correspondence here
does not imply that the English document  needs
to be an exact translation of the Mandarin story   .
It is quite adequate, for instance, if the two stories re-
port the same news event. This approach is expected
to be helpful even when the English document is
merely on the same general topic as the Mandarin
story, although the closer the content of a pair of ar-
ticles the better the proposed methods are likely to
work. Assume for the time being that a sufficiently
good Chinese-English story alignment is given.
Assume further that we have at our disposal a
stochastic translation dictionary ? a probabilistic
model of the form Transliteration of Proper Names in Cross-Lingual Information Retrieval
Paola Virga
Johns Hopkins University
3400 North Charles Street
Baltimore, MD 21218, USA
paola@jhu.edu
Sanjeev Khudanpur
Johns Hopkins University
3400 North Charles Street
Baltimore, MD 21218, USA
khudanpur@jhu.edu
Abstract
We address the problem of transliterating
English names using Chinese orthogra-
phy in support of cross-lingual speech and
text processing applications. We demon-
strate the application of statistical ma-
chine translation techniques to ?translate?
the phonemic representation of an En-
glish name, obtained by using an auto-
matic text-to-speech system, to a sequence
of initials and finals, commonly used sub-
word units of pronunciation for Chinese.
We then use another statistical translation
model to map the initial/final sequence
to Chinese characters. We also present
an evaluation of this module in retrieval
of Mandarin spoken documents from the
TDT corpus using English text queries.
1 Introduction
Translation of proper names is generally recognized
as a significant problem in many multi-lingual text
and speech processing applications. Even when
hand-crafted translation lexicons used for machine
translation (MT) and cross-lingual information re-
trieval (CLIR) provide significant coverage of the
words encountered in the text, a significant portion
of the tokens not covered by the lexicon are proper
names and domain-specific terminology (cf., e.g.,
Meng et al(2000)). This lack of translations ad-
versely affects performance. For CLIR applications
in particular, proper names and technical terms are
especially important, as they carry the most distinc-
tive information in a query as corroborated by their
relatively low document frequency. Finally, in in-
teractive IR systems where users provide very short
queries (e.g. 2-5 words), their importance grows
even further.
Unlike specialized terminology, however, proper
names are amenable to a speech-inspired translation
approach. One tries, when writing foreign names in
ones own language, to preserve the way it sounds.
i.e. one uses an orthographic representation which,
when ?read aloud? by a speaker of ones language
sounds as much like it would when spoken by a
speaker of the foreign language ? a process re-
ferred to as transliteration. Therefore, if a mecha-
nism were available to render, say, an English name
in its phonemic form, and another mechanism were
available to convert this phonemic string into the or-
thography of, say, Chinese, then one would have
a mechanism for transliterating English names us-
ing Chinese characters. The first step has been ad-
dressed extensively, for other obvious reasons, in the
automatic speech synthesis literature. This paper de-
scribes a statistical approach for the second step.
Several techniques have been proposed in the
recent past for name transliteration. Rather than
providing a comprehensive survey we highlight a
few representative approaches here. Finite state
transducers that implement transformation rules
for back-transliteration from Japanese to English
have been described by Knight and Graehl (1997),
and extended to Arabic by Glover-Stalls and
Knight (1998). In both cases, the goal is to recog-
nize words in Japanese or Arabic text which hap-
Figure 1: Four steps in English-to-Chinese transliteration of names.
pen to be transliterations of English names. If the
orthography of a language is strongly phonetic, as
is the case for Korean, then one may use relatively
simple hidden Markov models to transform English
pronunciations, as shown by Jung et al(2000). The
work closest to our application scenario, and the one
with which we will be making several direct com-
parisons, is that of Meng et al(2001). In their work,
a set of hand-crafted transformations for locally edit-
ing the phonemic spelling of an English word to con-
form to rules of Mandarin syllabification are used
to seed a transformation-based learning algorithm.
The algorithm examines some data and learns the
proper sequence of application of the transforma-
tions to convert an English phoneme sequence to a
Mandarin syllable sequence. Our paper describes a
data driven counterpart to this technique, in which
a cascade of two source-channel translation models
is used to go from English names to their Chinese
transliteration. Thus even the initial requirement of
creating candidate transformation rules, which may
require knowledge of the phonology of the target
language, is eliminated.
We also investigate incorporation of this translit-
eration system in a cross-lingual spoken document
retrieval application, in which English text queries
are used to index and retrieve Mandarin audio from
the TDT corpus.
2 Translation System Description
We break down the transliteration process into vari-
ous steps as depicted in Figure 1.
1. Conversion of an English name into a phone-
mic representation using the Festival1 speech
synthesis system.
2. Translation of the English phoneme sequence
into a sequence of generalized initials and fi-
nals or GIFs ? commonly used sub-syllabic
units for expressing pronunciations of Chinese
characters.
3. Transformation of the GIF sequence into a se-
quence of pin-yin symbols without tone.
4. Translation of the pin-yin sequence to a charac-
ter sequence.
Steps 1. and 3. are deterministic transformations,
while Steps 2. and 4. are accomplished using statis-
tical means.
The IBM source-channel model for statistical ma-
chine translation (P. Brown et al, 1993) plays a cen-
tral role in our system. We therefore describe it very
briefly here for completeness. In this model, a   -
word foreign language sentence        
is modeled as the output of a ?noisy channel? whose
input is its correct  -word English translation 	 





   


, and having observed the channel out-
put  , one seeks a posteriori the most likely English
sentence

	   
  
	     
  
 	
 
	 
The translation model
 
 	  is estimated from
a paired corpus of foreign-language sentences and
their English translations, and the language model
 
	 is trained from English text. Software tools
1http://www.speech.cs.cmu.edu/festival
Figure 2: Schematic of a English-to-Chinese name transliteration system.
are available both for training models2 as well as for
decoding3 ? the task of determining the most likely
translation

	.
Since we seek Chinese names which are translit-
eration of a given English name, the notion of
words in a sentence in the IBM model above is
replaced with phonemes in a word. The roles of
English and Chinese are also reversed. Therefore,
        represents a sequence of English
phonemes, and 	  
 
    
 , for instance, a se-
quence of GIF symbols in Step 2. described above.
The overall architecture of the proposed translitera-
tion system is illustrated in Figure 2.
2.1 Translation Model Training
We have available from Meng et al(2000) a small
list of about 3875 English names and their Chinese
transliteration. A pin-yin rendering of the Chinese
transliteration is also provided. We use the Festi-
val text-to-speech system to obtain a phonemic pro-
nunciation of each English name. We also replace
all pin-yin symbols by their pronunciations, which
are described using an inventory of generalized ini-
tials and finals. The pronunciation table for this pur-
pose is obtained from an elementary Mandarin text-
book (Practical Chinese Reader, 1981). The net re-
2http://www-i6.informatik.rwth-
aachen.de/ och/software/GIZA++.html.
3http://www.isi.edu/licensed-sw/rewrite-decoder.
sult is a corpus of 3875 pairs of ?sentences? of the
kind depicted in the second and third lines of Figure
1. The vocabulary of the English side of this parallel
corpus is 43 phonemes, and the Chinese side is 58
(21 initials and 37 finals). Note, however, that only
409 of the 21 37 possible initial-final combinations
constitute legal pin-yin symbols.
A second corpus of 3875 ?sentence? pairs is de-
rived corresponding to the fourth and fifth lines of
Figure 1, this time to train a statistical model to
translate pin-yin sequences to Chinese characters.
The vocabulary of the pin-yin side of this corpus
is 282 and that of the character side is about 680.
These, of course, are much smaller than the inven-
tory of Chinese pin-yin- and character-sets. We
note that certain characters are preferentially used
in transliteration over others, and the resulting fre-
quency of character-usage is not the same as unre-
stricted Chinese text. However, there isn?t a distinct
set of characters exclusively for transliteration.
For purposes of comparison with the translitera-
tion accuracy reported by Meng et al(2001), we di-
vide this list into 2233 training name-pairs and 1541
test name-pairs. For subsequent CLIR experiments,
we create a larger training set of 3625 name-pairs,
leaving only 250 names-pairs for intrinsic testing of
transliteration performance. The actual training of
all translation models proceeds according to a stan-
dard recipe recommended in GIZA++, namely 5 it-
erations of Model 1, followed by 5 of Model 2, 10
HMM-iterations and 10 iterations of Model 4.
2.2 Language Model Training
The GIF language model required for translating En-
glish phoneme sequences to GIF sequences is esti-
mated from the training portion of the 3875 Chinese
names. A trigram language model on the GIF vo-
cabulary is estimated with the CMU toolkit, using
Good-Turing smoothing and Katz back-off. Note
that due to the smoothing, this language model does
not necessarily assign zero probability to an ille-
gal GIF sequence, e.g., one containing two consec-
utive initials. This causes the first translation sys-
tem to sometimes, though very rarely, produce GIF
sequences which do not correspond to any pin-yin
sequence. We make an ad hoc correction of such se-
quences when mapping a GIF sequence to pin-yin,
which is otherwise trivial for all legal sequences of
initials and finals. Specifically, a final e or i or a is
tried, in that order, between consecutive initials until
a legitimate sequence of pin-yin symbols obtains.
The language model required for translating pin-
yin sequences to Chinese characters is relatively
straightforward. A character trigram model with
Good-Turing discounting and Katz back-off is es-
timated from the list of transliterated names.
2.3 Decoding Issues
We use the ReWrite decoder provided by ISI, along
with the two translation models and their corre-
sponding language models trained, either on 2233
or 3625 name-pairs, as described above, to perform
transliteration of English names in the respective test
sets with 1541 or 250 name-pairs respectively.
1. An English name is first converted to a
phoneme sequence via Festival.
2. The phoneme sequence is translated into an
GIF sequence using the first translation model
described above.
3. The translation output is corrected if necessary
to create a legitimate pin-yin sequence.
4. The pin-yin sequence is translated into a se-
quence of Chinese characters using a second
translation model, also described above.
A small but important manual setting in the ReWrite
decoder is a list of zero fertility words. In the IBM
model described earlier, these are the words 
  which
may be ?deleted? by the noisy channel when trans-
forming 	 into  . For the decoder, these are there-
fore the words which may be optionally inserted in

	 even when there is no word in  of which they are
considered a direct translation. For the usual case of
Chinese to English translation, these would usually
be articles and other function words which may not
be prevalent in the foreign language but frequent in
English.
For the phoneme-to-GIF translation model, the
?words? which need to be inserted in this manner
are syllabic nuclei! This is because Mandarin does
not permit complex consonant clusters in a way that
is quite prevalent in English. This linguistic knowl-
edge, however, need not be imparted by hand in the
IBM model. One can, indeed, derive such a list from
the trained models by simply reading off the list of
symbols which have zero fertility with high proba-
bility. This list, in our case, is

-i, e, u, o, r, u?,
ou, c, iu, ie.
The second translation system, for converting pin-
yin sequences to character sequences, has a one-to-
one mapping between symbols and therefore has no
words with zero fertility.
2.4 Intrinsic Evaluation of Transliteration
We evaluate the efficacy of our transliteration at two
levels. For comparison with the very comparable
set-up of Meng et al(2001), we measure the accu-
racy of the pin-yin output produced by our system
after Step 3. in Section 2.3. The results are shown in
Table 1, where pin-yin error rate is the edit distance
between the ?correct? pin-yin representation of the
correct transliteration and the pin-yin sequence out-
put by the system.
Translation Training Test Pin-yin Char
System Size Size Errors Errors
Meng et al2233 1541 52.5% N/A
Small MT 2233 1541 50.8% 57.4%
Big MT 3625 250 49.1% 57.4%
Table 1: Pin-yin and character error rates in auto-
matic transliteration.
Note that the pin-yin error performance of our
fully statistical method is quite competitive with pre-
vious results. We further note that increasing the
training data results in further reduction of the syl-
lable error rate. We concede that this performance,
while comparable to other systems, is not satisfac-
tory and merits further investigation.
We also evaluate the efficacy of our second trans-
lation system which maps the pin-yin sequence pro-
duced by the previous stages to a sequence of Chi-
nese characters, and obtain character error rates of
12.6%. Thus every correctly recognized pin-yin
symbol has a chance of being transformed with
some error, resulting in higher character error rate
than the pin-yin error rate. Note that while signifi-
cantly lower error rates have been reported for con-
verting pin-yin to characters in generic Chinese text,
ours is a highly specialized subset of transliterated
foreign names, where the choice between several
characters sharing the same pin-yin symbol is some-
what arbitrary.
3 Spoken Document Retrieval System
Several multi-lingual speech and text applications
require some form of name transliteration, cross-
lingual spoken document retrieval being a proto-
typical example. We build upon the experimen-
tal infrastructure developed at the 2000 Johns Hop-
kins Summer Workshop (Meng et al, 2000) where
considerable work was done towards indexing and
retrieving Mandarin audio to match English text
queries. Specifically, we find that in a large number
of queries used in those experiments, English proper
names are not available in the translation lexicon,
and are subsequently ignored during retrieval. We
use the technique described above to transliterate all
such names into Chinese characters and observe the
effect on retrieval performance.
The TDT-2 corpus, which we use for our experi-
ments, contains 2265 audio clips of Mandarin news
stories, along with several thousand contemporane-
ously published Chinese text articles, and English
text and audio broadcasts. The articles tend to be
several hundred to a few thousand words long, while
the audio clips tend to be two minutes or less on av-
erage. The purpose of the corpus is to facilitate re-
search in topic detection and tracking and exhaustive
relevance judgments are provided for several topics.
i.e. for each of at least 17 topics, every English and
Chinese article and news clip has been examined by
a human assessor and determined to be either on-
or off-topic. We randomly select an English arti-
cle on each of the 17 topics as a query, and wish
to retrieve all the Mandarin audio clips on the same
topic without retrieving any that are off-topic. For
mitigating the variability due to query selection, we
choose up to 12 different English articles for each of
the 17 topics and average retrieval performance over
this selection before reporting any results. We use
the query term-selection and translation technique
described by Meng et al(2000) to convert the En-
glish document to Chinese, the only augmentation
being the transliterated names ? there are roughly
2000 tokens in the queries which are not translat-
able, and almost all of them are proper names. We
report IR performance with and without the name-
transliteration.
We use a different information retrieval system
from the one used in the 2000 Workshop (Meng et
al., 2000) to perform the retrieval task. A brief de-
scription of the system is therefore in order.
3.1 The HAIRCUT System
The Hopkins Automated Information Retriever for
Combing Unstructured Text (HAIRCUT) is a re-
search retrieval system developed at the Johns Hop-
kins University Applied Physics Laboratory. The
system was developed to investigate knowledge-
light methods for linguistic processing in text re-
trieval. HAIRCUT uses a statistical language model
of retrieval such as the one explored by Hiem-
stra (2001). The model ranks documents according
to the probability that the terms in a query are gen-
erated by a document. Various smoothing methods
have been proposed to combine the contributions for
each term based on the document model and also a
generic model of the language. Many have found
that a simple mixture model using document term
frequencies for the former, and occurrence statistics
from a large corpus for the later, works quite well.
McNamee and Mayfield (2001) have shown using
HAIRCUT that overlapping character n-grams are
effective for retrieval in non-Asian languages (e.g.,
using n=6) and that translingual retrieval between
closely related languages is quite feasible even with-
CLIR mean Average Precision
System No NE Transliteration Automatic NE Transliteration LDC NE Look-Up
Meng et al(2001) 0.514 0.522 NA
Haircut 0.501 0.515 0.506
Table 2: Cross-lingual retrieval performance with and without name transliteration
out translation resources of any kind (McNamee and
Mayfield, 2002).
For the task of retrieving Mandarin audio from
Chinese text queries on the TDT-2 task, the system
described by Meng et al(2000) achieved a mean av-
erage precision of 0.733 using character bigrams for
indexing. On identical queries, HAIRCUT achieved
0.762 using character bigrams. This figure forms the
monolingual baseline for our CLIR system.
3.2 Cross-Lingual Retrieval Performance
We first indexed the automatic transcription of the
TDT-2 Mandarin audio collection using character
bigrams, as done by Meng et al(2000). We per-
formed CLIR using the Chinese translations of the
English queries, with and without transliteration of
proper names, and compared the standard 11-step
mean average precision (mAP) on the TDT-2 audio
corpus. Our results and the corresponding results
from Meng et al(2001) are reported in Table 2.
Without name transliteration, the performance of
the two CLIR systems is nearly identical: a paired
t-test shows that the difference in the mAPs of 0.514
and 0.501 is significant only at a   -value of 0.74.
A small improvement in mAP is obtained by the
Haircut system with name transliteration over the
system without name transliteration: the improve-
ment from 0.501 to 0.515 is statistically significant
at a   -value of 0.084. The statistical significance of
the improvement from 0.514 to 0.522 by Meng et
al (2001) is not known to us. In any event, a need
for improvement in transliteration is suggested by
this result.
We recently received a large list of nearly 2M
Chinese-English named-entity pairs from the LDC.
As a pilot experiment, we simply added this list
to the translation lexicon of the CLIR system, i.e.,
we ?translated? those names in our English queries
which happened to be available in this LDC list.
This happens to cover more than 85% of the pre-
viously untranslatable names in our queries. For the
remaining names, we continued to use our automatic
transliterator. To our surprise, the mAP improve-
ment from 0.501 to 0.506 was statistically insignif-
icant (  -value of 0.421) and the reason why the use
of the ostensibly correct transliteration most of the
time still does not result in any significant gain in
CLIR performance continues to elude us.
We conjecture that the fact that the audio has been
processed by an automatic speech recognition sys-
tem, which in all likelihood did not have many of
the proper names in question in its vocabulary, may
be the cause of this dismal performance. It is plausi-
ble, though we cannot find a stronger justification for
it, that by using the 10-best transliterations produced
by our automatic system, we are adding robustness
against ASR errors in the retrieval of proper names.
4 A Large Chinese-English Translation
Table of Named Entities
The LDC Chinese-English named entity list was
compiled from Xinhua News sources, and consists
of nine pairs of lists, one each to cover person-
names, place-names, organizations, etc. While there
are indeed nearly 2 million name-pairs in this list, a
large number of formatting, character encoding and
other errors exist in this beta release, making it dif-
ficult to use the corpus as is in our statistical MT
system. We have tried using from this resource the
two lists corresponding to person-names and place-
names respectively, and have attempted to augment
the training data for our system described previously
in Section 2.1. However, we further screened these
lists as well in order to eliminate possible errors.
4.1 Extracting Named Entity Transliteration
Pairs for Translation Model Training
There are nearly 1 million pairs of person or place-
names in the LDC corpus. In order to obtain a
clean corpus of Named Entity transliterations we
performed the following steps:
1. We coverted all name-pairs into a parallel cor-
pus of English phonemes on one side and Chi-
nese GIFs on the other by the procedure de-
scribed earlier.
2. We trained a statistical MT system for trans-
lating from English phonemes to Chinese GIFs
from this corpus.
3. We then aligned all the (nearly 1M) training
?sentence? pairs with this translation model,
and extracted roughly a third of the sentences
with an alignment score above a certain tunable
threshold ( ). This resulted in the extrac-
tion of 346860 name-pairs.
4. We divided the set into 343738 pairs for train-
ing and 3122 for testing.
5. We estimated a pin-yin language model from
the training portion above.
6. We retrained the statistical MT system on this
presumably ?good? training set and evaluated
the pin-yin error rate of the transliteration.
The result of this evaluation is reported in Table 3
against the line ?Huge MT (Self),? where we also re-
port the transliteration performance of the so-called
Big MT system of Table 1 on this new test set. We
note, again with some dismay, that the additional
training data did not result in a significant improve-
ment in transliteration performance.
MT System Training Test Pin-yin
(Data filtered by) Size Size Errors
Big MT 3625 3122 51.1%
Huge MT (Itself) 343738 3122 51.5%
Huge MT (Big MT) 309019 3122 42.5%
Table 3: Pin-yin error rates for MT systems with
varying amounts of training data and different data
selection procedures.
We continue to believe that careful data-selection
is the key to successful use of this beta-release of the
LDC Named Entity corpus. We therefore went back
to Step 3 of the procedure outlined above, where we
had used alignment scores from an MT system to
select ?good? sentence-pairs from our training data,
and instead of using the MT system trained in Step
2 immediately preceding it, we used the previously
built Big MT system of Section 2.1, which we know
is trained on a small but clean data-set of 3625 name-
pairs. With a similar threshold as above, we again
selected roughly 300K name-pairs, being careful to
leave out any pair which appears in the 3122 pair
test set described above, and reestimated the entire
phoneme-to-GIF translation system on this new cor-
pus. We evaluated this system on the 3122 name-
pair test set for transliteration performance, and the
results are included in Table 3.
Note that significant improvements in translitera-
tion performance result from this alternate method
of data selection.
4.2 Cross-Lingual Retrieval Performance ? II
We reran the CLIR experiments on the TDT-2 cor-
pus using the somewhat improved entity translitera-
tor described above, with the same query and doc-
ument collection specifications as the experiments
reported in Table 2. The results of this second exper-
iment is reported in Table 4, where the performance
of the Big MT transliterator is reproduced for com-
parison.
Transliterator mean Average Precision
(Data filtered by) No NE Automatic NE
Big MT 0.501 0.515
Huge MT (Big MT) ? 0.517
Table 4: Cross-lingual retrieval performance with
and without name transliteration
Note that the gain in CLIR performance is again
only somewhat significant, with the improvement in
mAP from 0.501 to 0.517 being significant only at a
  -value of 0.080.
5 Concluding Remarks
We have presented a name transliteration procedure
based on statistical machine translation techniques
and have investigated its use in a cross lingual spo-
ken document retrieval task. We have found small
gains in the extrinsic evaluation of our procedure:
mAP improvement from 0.501 to 0.517. In a more
intrinsic and direct evaluation, we have found ways
to gainfully filter a large but noisy training corpus
to augment the training data for our models and im-
prove transliteration accuracy considerably beyond
our starting point, e.g., to reduce Pin-yin error rates
from 51.1% to 42.5%. We expect to further refine
the translation models in the future and apply them
in other tasks such as text translation.
References
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, 19(2):263-311.
Sung Young Jung, SungLim Hong, and Eunok Paek.
2000. An English to Korean Transliteration Model of
Extended Markov Window. Proceedings of COLING.
K. Knight and J. Graehl. 1997. Machine Transliteration.
Proceedings of ACL.
Paul McNamee and Jim Mayfield. 2001. JHU/APL Ex-
periments at CLEF-2001: Translation Resources and
Score Normalization. Proceedings of CLEF.
Paul McNamee and Jim Mayfield. 2002. Comparing
Cross-Language Query Expansion Techniques by De-
grading Translation Resources. Proceedings of SIGIR.
Helen M. Meng et al
?
. 2000. Mandarin-English Infor-
mation (MEI): Investigating Translingual Speech Re-
trieval. Technical Report for the Johns Hopkins Univ.
Summer Workshop.
Helen M. Meng, Wai-Kit Lo, Berlin Chen, and Karen
Tang. 2001. Generating Phonetic Cognates to Handle
Named Entities in English-Chinese Cross-Language
Spoken Document Retrieval. Proceedings of ASRU.
Practical Chinese Reader, Book I. The Commercial Press
LTD. 1981.
Bonnie Glover Stalls and Kevin Knight. 1998. Translat-
ing Names and Technical Terms in Arabic Text. Pro-
ceedings of the COLING/ACL Workshop on Computa-
tional Approaches to Semitic Languages.
Djoerd Hiemstra. 2001. Using Language Models
for Information Retrieval. Ph.D. thesis,University of
Twente, Netherlands.
Proceedings of the BioNLP Workshop on Linking Natural Language Processing and Biology at HLT-NAACL 06, pages 65?72,
New York City, June 2006. c?2006 Association for Computational Linguistics
Generative Content Models for Structural Analysis of Medical Abstracts
Jimmy Lin1,2, Damianos Karakos3, Dina Demner-Fushman2, and Sanjeev Khudanpur3
1College of Information Studies 3Center for Language and
2Institute for Advanced Computer Studies Speech Processing
University of Maryland Johns Hopkins University
College Park, MD 20742, USA Baltimore, MD 21218, USA
jimmylin@umd.edu, demner@cs.umd.edu (damianos, khudanpur)@jhu.edu
Abstract
The ability to accurately model the con-
tent structure of text is important for
many natural language processing appli-
cations. This paper describes experi-
ments with generative models for analyz-
ing the discourse structure of medical ab-
stracts, which generally follow the pattern
of ?introduction?, ?methods?, ?results?,
and ?conclusions?. We demonstrate that
Hidden Markov Models are capable of ac-
curately capturing the structure of such
texts, and can achieve classification ac-
curacy comparable to that of discrimina-
tive techniques. In addition, generative
approaches provide advantages that may
make them preferable to discriminative
techniques such as Support Vector Ma-
chines under certain conditions. Our work
makes two contributions: at the applica-
tion level, we report good performance
on an interesting task in an important do-
main; more generally, our results con-
tribute to an ongoing discussion regarding
the tradeoffs between generative and dis-
criminative techniques.
1 Introduction
Certain types of text follow a predictable structure,
the knowledge of which would be useful in many
natural language processing applications. As an
example, scientific abstracts across many different
fields generally follow the pattern of ?introduction?,
?methods?, ?results?, and ?conclusions? (Salanger-
Meyer, 1990; Swales, 1990; Ora?san, 2001). The
ability to explicitly identify these sections in un-
structured text could play an important role in ap-
plications such as document summarization (Teufel
and Moens, 2000), information retrieval (Tbahriti
et al, 2005), information extraction (Mizuta et al,
2005), and question answering. Although there is
a trend towards analysis of full article texts, we
believe that abstracts still provide a tremendous
amount of information, and much value can still be
extracted from them. For example, Gay et al (2005)
experimented with abstracts and full article texts in
the task of automatically generating index term rec-
ommendations and discovered that using full article
texts yields at most a 7.4% improvement in F-score.
Demner-Fushman et al (2005) found a correlation
between the quality and strength of clinical conclu-
sions in the full article texts and abstracts.
This paper presents experiments with generative
content models for analyzing the discourse struc-
ture of medical abstracts, which has been con-
firmed to follow the four-section pattern discussed
above (Salanger-Meyer, 1990). For a variety of rea-
sons, medicine is an interesting domain of research.
The need for information systems to support physi-
cians at the point of care has been well studied (Cov-
ell et al, 1985; Gorman et al, 1994; Ely et al,
2005). Retrieval techniques can have a large im-
pact on how physicians access and leverage clini-
cal evidence. Information that satisfies physicians?
needs can be found in theMEDLINE database main-
tained by the U.S. National Library of Medicine
65
(NLM), which also serves as a readily available
corpus of abstracts for our experiments. Further-
more, the availability of rich ontological resources,
in the form of the Unified Medical Language Sys-
tem (UMLS) (Lindberg et al, 1993), and the avail-
ability of software that leverages this knowledge?
MetaMap (Aronson, 2001) for concept identification
and SemRep (Rindflesch and Fiszman, 2003) for re-
lation extraction?provide a foundation for studying
the role of semantics in various tasks.
McKnight and Srinivasan (2003) have previously
examined the task of categorizing sentences in med-
ical abstracts using supervised discriminative ma-
chine learning techniques. Building on the work of
Ruch et al (2003) in the same domain, we present a
generative approach that attempts to directly model
the discourse structure of MEDLINE abstracts us-
ing Hidden Markov Models (HMMs); cf. (Barzilay
and Lee, 2004). Although our results were not ob-
tained from the same exact collection as those used
by authors of these two previous studies, comparable
experiments suggest that our techniques are compet-
itive in terms of performance, and may offer addi-
tional advantages as well.
Discriminative approaches (especially SVMs)
have been shown to be very effective for many
supervised classification tasks; see, for exam-
ple, (Joachims, 1998; Ng and Jordan, 2001). How-
ever, their high computational complexity (quadratic
in the number of training samples) renders them pro-
hibitive for massive data processing. Under certain
conditions, generative approaches with linear com-
plexity are preferable, even if their performance is
lower than that which can be achieved through dis-
criminative training. Since HMMs are very well-
suited to modeling sequences, our discourse model-
ing task lends itself naturally to this particular gener-
ative approach. In fact, we demonstrate that HMMs
are competitive with SVMs, with the added advan-
tage of lower computational complexity. In addition,
generative models can be directly applied to tackle
certain classes of problems, such as sentence order-
ing, in ways that discriminative approaches cannot
readily. In the context of machine learning, we see
our work as contributing to the ongoing debate be-
tween generative and discriminative approaches?
we provide a case study in an interesting domain that
begins to explore some of these tradeoffs.
2 Methods
2.1 Corpus and Data Preparation
Our experiments involved MEDLINE, the biblio-
graphical database of biomedical articles maintained
by the U.S. National Library of Medicine (NLM).
We used the subset of MEDLINE that was extracted
for the TREC 2004 Genomics Track, consisting of
citations from 1994 to 2003. In total, 4,591,008
records (abstract text and associated metadata) were
extracted using the Date Completed (DCOM) field
for all references in the range of 19940101 to
20031231.
Viewing structural modeling of medical abstracts
as a sentence classification task, we leveraged the
existence of so-called structured abstracts (see Fig-
ure 1 for an example) in order to obtain the appro-
priate section label for each sentence. The use of
section headings is a device recommended by the
Ad Hoc Working Group for Critical Appraisal of the
Medical Literature (1987) to help humans assess the
reliability and content of a publication and to facil-
itate the indexing and retrieval processes. Although
structured abstracts loosely adhere to the introduc-
tion, methods, results, and conclusions format, the
exact choice of section headings varies from ab-
stract to abstract and from journal to journal. In our
test collection, we observed a total of 2688 unique
section headings in structured abstracts?these were
manually mapped to the four broad classes of ?intro-
duction?, ?methods?, ?results?, and ?conclusions?.
All sentences falling under a section heading were
assigned the label of its appropriately-mapped head-
ing (naturally, the actual section headings were re-
moved in our test collection). As a concrete exam-
ple, in the abstract shown in Figure 1, the ?OBJEC-
TIVE? section would be mapped to ?introduction?,
the ?RESEARCH DESIGN AND METHODS? sec-
tion to ?methods?. The ?RESULTS? and ?CON-
CLUSIONS? sections map directly to our own la-
bels. In total, 308,055 structured abstracts were ex-
tracted and prepared in this manner, serving as the
complete dataset. In addition, we created a reduced
collection of 27,075 abstracts consisting of only
Randomized Controlled Trials (RCTs), which rep-
resent definitive sources of evidence highly-valued
in the clinical decision-making process.
Separately, we manually annotated 49 unstruc-
66
Integrating medical management with diabetes self-management training: a randomized control trial of the Diabetes
Outpatient Intensive Treatment program.
OBJECTIVE? This study evaluated the Diabetes Outpatient Intensive Treatment (DOIT) program, a multiday group educa-
tion and skills training experience combined with daily medical management, followed by case management over 6 months.
Using a randomized control design, the study explored how DOIT affected glycemic control and self-care behaviors over a
short term. The impact of two additional factors on clinical outcomes were also examined (frequency of case management
contacts and whether or not insulin was started during the program). RESEARCH DESIGN AND METHODS? Patients
with type 1 and type 2 diabetes in poor glycemic control (A1c ?8.5%) were randomly assigned to DOIT or a second con-
dition, entitled EDUPOST, which was standard diabetes care with the addition of quarterly educational mailings. A total
of 167 patients (78 EDUPOST, 89 DOIT) completed all baseline measures, including A1c and a questionnaire assessing
diabetes-related self-care behaviors. At 6 months, 117 patients (52 EDUPOST, 65 DOIT) returned to complete a follow-up
A1c and the identical self-care questionnaire. RESULTS? At follow-up, DOIT evidenced a significantly greater drop in A1c
than EDUPOST. DOIT patients also reported significantly more frequent blood glucose monitoring and greater attention to
carbohydrate and fat contents (ACFC) of food compared with EDUPOST patients. An increase in ACFC over the 6-month
period was associated with improved glycemic control among DOIT patients. Also, the frequency of nurse case manager
follow-up contacts was positively linked to better A1c outcomes. The addition of insulin did not appear to be a significant
contributor to glycemic change. CONCLUSIONS? DOIT appears to be effective in promoting better diabetes care and posi-
tively influencing glycemia and diabetes-related self-care behaviors. However, it demands significant time, commitment, and
careful coordination with many health care professionals. The role of the nurse case manager in providing ongoing follow-up
contact seems important.
Figure 1: Sample structured abstract from MEDLINE.
tured abstracts of randomized controlled trials re-
trieved to answer a question about the manage-
ment of elevated low-density lipoprotein cholesterol
(LDL-C). We submitted a PubMed query (?elevated
LDL-C?) and restricted results to English abstracts
of RCTs, gathering 49 unstructured abstracts from
26 journals. Each sentence was annotated with its
section label by the third author, who is a medical
doctor?this collection served as our blind held-out
testset. Note that the annotation process preceded
our experiments, which helped to guard against
annotator-introduced bias. Of 49 abstracts, 35 con-
tained all four sections (which we refer to as ?com-
plete?), while 14 abstracts were missing one or more
sections (which we refer to as ?partial?).
Two different types of experiments were con-
ducted: the first consisted of cross-validation on the
structured abstracts; the second consisted of train-
ing on the structured abstracts and testing on the
unstructured abstracts. We hypothesized that struc-
tured and unstructured abstracts share the same un-
derlying discourse patterns, and that content models
trained with one can be applied to the other.
2.2 Generative Models of Content
Following Ruch et al (2003) and Barzilay and
Lee (2004), we employed Hidden Markov Models
to model the discourse structure of MEDLINE ab-
stracts. The four states in our HMMs correspond
to the information that characterizes each section
(?introduction?, ?methods?, ?results?, and ?conclu-
sions?) and state transitions capture the discourse
flow from section to section.
Using the SRI language modeling toolkit, we
first computed bigram language models for each
of the four sections using Kneser-Ney discounting
and Katz backoff. All words in the training set
were downcased, all numbers were converted into
a generic symbol, and all singleton unigrams and bi-
grams were removed. Using these results, each sen-
tence was converted into a four dimensional vector,
where each component represents the log probabil-
ity, divided by the number of words, of the sentence
under each of the four language models.
We then built a four-state Hidden Markov Model
that outputs these four-dimensional vectors. The
transition probability matrix of the HMM was ini-
tialized with uniform probabilities over a fully
connected graph. The output probabilities were
modeled as four-dimensional Gaussians mixtures
with diagonal covariance matrices. Using the sec-
tion labels, the HMM was trained using the HTK
toolkit (Young et al, 2002), which efficiently per-
forms the forward-backward algorithm and Baum-
Welch estimation. For testing, we performed a
Viterbi (maximum likelihood) estimation of the la-
bel of each test sentence/vector (also using the HTK
toolkit).
67
In an attempt to further boost performance, we
employed Linear Discriminant Analysis (LDA) to
find a linear projection of the four-dimensional vec-
tors that maximizes the separation of the Gaussians
(corresponding to the HMM states). Venables and
Ripley (1994) describe an efficient algorithm (of lin-
ear complexity in the number of training sentences)
for computing the LDA transform matrix, which en-
tails computing the within- and between-covariance
matrices of the classes, and using Singular Value De-
composition (SVD) to compute the eigenvectors of
the new space. Each sentence/vector is then mul-
tiplied by this matrix, and new HMM models are
re-computed from the projected data.
An important aspect of our work is modeling con-
tent structure using generative techniques. To as-
sess the impact of taking discourse transitions into
account, we compare our fully trained model to
one that does not take advantage of the Markov
assumption?i.e., it assumes that the labels are in-
dependently and identically distributed.
To facilitate comparison with previous work, we
also experimented with binary classifiers specifi-
cally tuned to each section. This was done by creat-
ing a two-state HMM: one state corresponds to the
label we want to detect, and the other state corre-
sponds to all the other labels. We built four such
classifiers, one for each section, and trained them in
the same manner as above.
3 Results
We report results on three distinct sets of experi-
ments: (1) ten-fold cross-validation (90/10 split) on
all structured abstracts from the TREC 2004 MED-
LINE corpus, (2) ten-fold cross-validation (90/10
split) on the RCT subset of structured abstracts from
the TREC 2004 MEDLINE corpus, (3) training on
the RCT subset of the TREC 2004 MEDLINE cor-
pus and testing on the 49 hand-annotated held-out
testset.
The results of our first set of experiments are
shown in Tables 1(a) and 1(b). Table 1(a) reports
the classification error in assigning a unique label to
every sentence, drawn from the set {?introduction?,
?methods?, ?results?, ?conclusions?}. For this task,
we compare the performance of three separate mod-
els: one that does not make the Markov assumption,
Model Error
non-HMM .220
HMM .148
HMM + LDA .118
(a)
Section Acc Prec Rec F
Introduction .957 .930 .840 .885
Methods .921 .810 .875 .843
Results .921 .898 .898 .898
Conclusions .963 .898 .896 .897
(b)
Table 1: Ten-fold cross-validation results on all
structured abstracts from the TREC 2004 MED-
LINE corpus: multi-way classification on complete
abstract structure (a) and by-section binary classifi-
cation (b).
the basic four-state HMM, and the improved four-
state HMM with LDA. As expected, explicitly mod-
eling the discourse transitions significantly reduces
the error rate. Applying LDA further enhances clas-
sification performance. Table 1(b) reports accuracy,
precision, recall, and F-measure for four separate bi-
nary classifiers specifically trained for each of the
sections (one per row in the table). We only dis-
play results with our best model, namely HMM with
LDA.
The results of our second set of experiments (with
RCTs only) are shown in Tables 2(a) and 2(b).
Table 2(a) reports the multi-way classification er-
ror rate; once again, applying the Markov assump-
tion to model discourse transitions improves perfor-
mance, and using LDA further reduces error rate.
Table 2(b) reports accuracy, precision, recall, and F-
measure for four separate binary classifiers (HMM
with LDA) specifically trained for each of the sec-
tions (one per row in the table). The table also
presents the closest comparable experimental re-
sults reported by McKnight and Srinivasan (2003).1
McKnight and Srinivasan (henceforth, M&S) cre-
ated a test collection consisting of 37,151 RCTs
from approximately 12 million MEDLINE abstracts
dated between 1976 and 2001. This collection has
1After contacting the authors, we were unable to obtain the
same exact dataset that they used for their experiments.
68
Model Error
non-HMM .238
HMM .212
HMM + LDA .209
(a)
Present study McKnight and Srinivasan
Section Acc Prec Rec F Acc Prec Rec F
Introduction .931 .898 .715 .807 .967 .920 .970 .945
Methods .904 .812 .847 .830 .895 .810 .830 .820
Results .902 .902 .831 .867 .860 .810 .830 .820
Conclusions .929 .772 .790 .781 .970 .880 .910 .820
(b)
Table 2: Ten-fold cross-validation results on the structured RCT subset of the TREC 2004 MEDLINE
corpus: multi-way classification (a) and binary classification (b). Table (b) also reproduces the results from
McKnight and Srinivasan (2003) for a comparable task on a different RCT-subset of structured abstracts.
Model Complete Partial
non-HMM .247 .371
HMM .226 .314
HMM + LDA .217 .279
(a)
Complete Partial McKnight and Srinivasan
Section Acc Prec Rec F Acc Prec Rec F Acc Prec Rec F
Introduction .923 .739 .723 .731 .867 .368 .636 .502 .896 .630 .450 .524
Methods .905 .841 .793 .817 .859 .958 .589 .774 .897 .880 .730 .799
Results .899 .913 .857 .885 .892 .942 .830 .886 .872 .840 .880 .861
Conclusions .911 .639 .847 .743 .884 .361 .995 .678 .941 .830 .750 .785
(b)
Table 3: Training on the structured RCT subset of the TREC 2004 MEDLINE corpus, testing on corpus of
hand-annotated abstracts: multi-way classification (a) and binary classification (b). Unstructured abstracts
with all four sections (complete), and with missing sections (partial) are shown. Table (b) again repro-
duces the results from McKnight and Srinivasan (2003) for a comparable task on a different subset of 206
unstructured abstracts.
69
significantly more training examples than our corpus
of 27,075 abstracts, which could be a source of per-
formance differences. Furthermore, details regard-
ing their procedure for mapping structured abstract
headings to one of the four general labels was not
discussed in their paper. Nevertheless, our HMM-
based approach is at least competitive with SVMs,
perhaps better in some cases.
The results of our third set of experiments (train-
ing on RCTs and testing on a held-out testset of
hand-annotated abstracts) is shown in Tables 3(a)
and 3(b). Mirroring the presentation format above,
Table 3(a) shows the classification error for the four-
way label assignment problem. We noticed that
some unstructured abstracts are qualitatively differ-
ent from structured abstracts in that some sections
are missing. For example, some unstructured ab-
stracts lack an introduction, and instead dive straight
into methods; other unstructured abstracts lack a
conclusion. As a result, classification error is higher
in this experiment than in the cross-validation ex-
periments. We report performance figures for 35 ab-
stracts that contained all four sections (?complete?)
and for 14 abstracts that had one or more miss-
ing sections (?partial?). Table 3(b) reports accu-
racy, precision, recall, and F-measure for four sep-
arate binary classifiers (HMM with LDA) specifi-
cally trained for each section (one per row in the
table). The table also presents the closest compa-
rable experimental results reported by M&S?over
206 hand-annotated unstructured abstracts. Interest-
ingly, M&S did not specifically note missing sec-
tions in their testset.
4 Discussion
An interesting aspect of our generative approach
is that we model HMM outputs as Gaussian vec-
tors (log probabilities of observing entire sentences
based on our language models), as opposed to se-
quences of terms, as done in (Barzilay and Lee,
2004). This technique provides two important ad-
vantages. First, Gaussian modeling adds an ex-
tra degree of freedom during training, by capturing
second-order statistics. This is not possible when
modeling word sequences, where only the probabil-
ity of a sentence is actually used in the HMM train-
ing. Second, using continuous distributions allows
us to leverage a variety of tools (e.g., LDA) that have
been shown to be successful in other fields, such as
speech recognition (Evermann et al, 2004).
Table 2(b) represents the closest head-to-head
comparison between our generative approach
(HMM with LDA) and state-of-the-art results
reported by M&S using SVMs. In some ways, the
results reported by M&S have an advantage because
they use significantly more training examples. Yet,
we can see that generative techniques for the model-
ing of content structure are at least competitive?we
even outperform SVMs on detecting ?methods?
and ?results?. Moreover, the fact that the training
and testing of HMMs have linear complexity (as
opposed to the quadratic complexity of SVMs)
makes our approach a very attractive alternative,
given the amount of training data that is available
for such experiments.
Although exploration of the tradeoffs between
generative and discriminative machine learning
techniques is one of the aims of this work, our ul-
timate goal, however, is to build clinical systems
that provide timely access to information essential
to the patient treatment process. In truth, our cross-
validation experiments do not correspond to any
meaningful naturally-occurring task?structured ab-
stracts are, after all, already appropriately labeled.
The true utility of content models is to struc-
ture abstracts that have no structure to begin with.
Thus, our exploratory experiments in applying con-
tent models trained with structured RCTs on un-
structured RCTs is a closer approximation of an
extrinsically-valid measure of performance. Such a
component would serve as the first stage of a clin-
ical question answering system (Demner-Fushman
and Lin, 2005) or summarization system (McKe-
own et al, 2003). We chose to focus on randomized
controlled trials because they represent the standard
benchmark by which all other clinical studies are
measured.
Table 3(b) shows the effectiveness of our trained
content models on abstracts that had no explicit
structure to begin with. We can see that although
classification accuracy is lower than that from our
cross-validation experiments, performance is quite
respectable. Thus, our hypothesis that unstructured
abstracts are not qualitatively different from struc-
tured abstracts appears to be mostly valid.
70
5 Related Work
Although not the first to employ a generative ap-
proach to directly model content, the seminal work
of Barzilay and Lee (2004) is a noteworthy point
of reference and comparison. However, our study
differs in several important respects. Barzilay and
Lee employed an unsupervised approach to building
topic sequence models for the newswire text genre
using clustering techniques. In contrast, because
the discourse structure of medical abstracts is well-
defined and training data is relatively easy to ob-
tain, we were able to apply a supervised approach.
Whereas Barzilay and Lee evaluated their work in
the context of document summarization, the four-
part structure of medical abstracts allows us to con-
duct meaningful intrinsic evaluations and focus on
the sentence classification task. Nevertheless, their
work bolsters our claims regarding the usefulness of
generative models in extrinsic tasks, which we do
not describe here.
Although this study falls under the general topic
of discourse modeling, our work differs from previ-
ous attempts to characterize text in terms of domain-
independent rhetorical elements (McKeown, 1985;
Marcu and Echihabi, 2002). Our task is closer to the
work of Teufel and Moens (2000), who looked at the
problem of intellectual attribution in scientific texts.
6 Conclusion
We believe that there are two contributions as a re-
sult of our work. From the perspective of machine
learning, the assignment of sequentially-occurring
labels represents an underexplored problem with re-
spect to the generative vs. discriminative debate?
previous work has mostly focused on stateless clas-
sification tasks. This paper demonstrates that Hid-
den Markov Models are capable of capturing dis-
course transitions from section to section, and are
at least competitive with Support Vector Machines
from a purely performance point of view.
The other contribution of our work is that it con-
tributes to building advanced clinical information
systems. From an application point of view, the abil-
ity to assign structure to otherwise unstructured text
represents a key capability that may assist in ques-
tion answering, document summarization, and other
natural language processing applications.
Much research in computational linguistics has
focused on corpora comprised of newswire articles.
We would like to point out that clinical texts provide
another attractive genre in which to conduct experi-
ments. Such texts are easy to acquire, and the avail-
ability of domain ontologies provides new opportu-
nities for knowledge-rich approaches to shine. Al-
though we have only experimented with lexical fea-
tures in this study, the door is wide open for follow-
on studies based on semantic features.
7 Acknowledgments
The first author would like to thank Esther and Kiri
for their loving support.
References
Ad Hoc Working Group for Critical Appraisal of the
Medical Literature. 1987. A proposal for more infor-
mative abstracts of clinical articles. Annals of Internal
Medicine, 106:595?604.
Alan R. Aronson. 2001. Effective mapping of biomed-
ical text to the UMLS Metathesaurus: The MetaMap
program. In Proceeding of the 2001 Annual Sympo-
sium of the American Medical Informatics Association
(AMIA 2001), pages 17?21.
Regina Barzilay and Lillian Lee. 2004. Catching the
drift: Probabilistic content models, with applications
to generation and summarization. In Proceedings
of the 2004 Human Language Technology Confer-
ence and the North American Chapter of the Associ-
ation for Computational Linguistics Annual Meeting
(HLT/NAACL 2004).
David G. Covell, Gwen C. Uman, and Phil R. Manning.
1985. Information needs in office practice: Are they
being met? Annals of Internal Medicine, 103(4):596?
599, October.
Dina Demner-Fushman and Jimmy Lin. 2005. Knowl-
edge extraction for clinical question answering: Pre-
liminary results. In Proceedings of the AAAI-05 Work-
shop on Question Answering in Restricted Domains.
Dina Demner-Fushman, Susan E. Hauser, and George R.
Thoma. 2005. The role of title, metadata and ab-
stract in identifying clinically relevant journal arti-
cles. In Proceeding of the 2005 Annual Symposium of
the American Medical Informatics Association (AMIA
2005), pages 191?195.
John W. Ely, Jerome A. Osheroff, M. Lee Chambliss,
Mark H. Ebell, and Marcy E. Rosenbaum. 2005. An-
swering physicians? clinical questions: Obstacles and
71
potential solutions. Journal of the American Medical
Informatics Association, 12(2):217?224, March-April.
Gunnar Evermann, H. Y. Chan, Mark J. F. Gales, Thomas
Hain, Xunying Liu, David Mrva, Lan Wang, and Phil
Woodland. 2004. Development of the 2003 CU-HTK
Conversational Telephone Speech Transcription Sys-
tem. In Proceedings of the 2004 International Con-
ference on Acoustics, Speech and Signal Processing
(ICASSP04).
Clifford W. Gay, Mehmet Kayaalp, and Alan R. Aronson.
2005. Semi-automatic indexing of full text biomedi-
cal articles. In Proceeding of the 2005 Annual Sympo-
sium of the American Medical Informatics Association
(AMIA 2005), pages 271?275.
Paul N. Gorman, Joan S. Ash, and Leslie W. Wykoff.
1994. Can primary care physicians? questions be an-
swered using the medical journal literature? Bulletin
of the Medical Library Association, 82(2):140?146,
April.
Thorsten Joachims. 1998. Text categorization with Sup-
port Vector Machines: Learning with many relevant
features. In Proceedings of the European Conference
on Machine Learning (ECML 1998).
Donald A. Lindberg, Betsy L. Humphreys, and Alexa T.
McCray. 1993. The Unified Medical Language Sys-
tem. Methods of Information in Medicine, 32(4):281?
291, August.
Daniel Marcu and Abdessamad Echihabi. 2002. An
unsupervised approach to recognizing discourse rela-
tions. In Proceedings of the 40th Annual Meeting of
the Association for Computational Linguistics (ACL
2002).
Kathleen McKeown, Noemie Elhadad, and Vasileios
Hatzivassiloglou. 2003. Leveraging a common rep-
resentation for personalized search and summarization
in a medical digital library. In Proceedings of the
3rd ACM/IEEE Joint Conference on Digital Libraries
(JCDL 2003).
Kathleen R. McKeown. 1985. Text Generation: Using
Discourse Strategies and Focus Constraints to Gen-
erate Natural Language Text. Cambridge University
Press, Cambridge, England.
Larry McKnight and Padmini Srinivasan. 2003. Catego-
rization of sentence types in medical abstracts. In Pro-
ceeding of the 2003 Annual Symposium of the Ameri-
can Medical Informatics Association (AMIA 2003).
Yoko Mizuta, Anna Korhonen, Tony Mullen, and Nigel
Collier. 2005. Zone analysis in biology articles as a
basis for information extraction. International Journal
of Medical Informatics, in press.
Andrew Y. Ng and Michael Jordan. 2001. On discrim-
inative vs. generative classifiers: A comparison of lo-
gistic regression and naive Bayes. In Advances in Neu-
ral Information Processing Systems 14.
Constantin Ora?san. 2001. Patterns in scientific abstracts.
In Proceedings of the 2001 Corpus Linguistics Confer-
ence.
Thomas C. Rindflesch and Marcelo Fiszman. 2003. The
interaction of domain knowledge and linguistic struc-
ture in natural language processing: Interpreting hy-
pernymic propositions in biomedical text. Journal of
Biomedical Informatics, 36(6):462?477, December.
Patrick Ruch, Christine Chichester, Gilles Cohen, Gio-
vanni Coray, Fre?de?ric Ehrler, Hatem Ghorbel, Hen-
ning Mu?ller, and Vincenzo Pallotta. 2003. Report
on the TREC 2003 experiment: Genomic track. In
Proceedings of the Twelfth Text REtrieval Conference
(TREC 2003).
Franc?oise Salanger-Meyer. 1990. Discoursal movements
in medical English abstracts and their linguistic expo-
nents: A genre analysis study. INTERFACE: Journal
of Applied Linguistics, 4(2):107?124.
John M. Swales. 1990. Genre Analysis: English in Aca-
demic and Research Settings. Cambridge University
Press, Cambridge, England.
Imad Tbahriti, Christine Chichester, Fre?de?rique Lisacek,
and Patrick Ruch. 2005. Using argumentation to re-
trieve articles with similar citations: An inquiry into
improving related articles search in the MEDLINE
digital library. International Journal of Medical In-
formatics, in press.
Simone Teufel and Marc Moens. 2000. What?s yours
and what?s mine: Determining intellectual attribu-
tion in scientific text. In Proceedings of the Joint
SIGDAT Conference on Empirical Methods in Nat-
ural Language Processing and Very Large Corpora
(EMNLP/VLC-2000).
William N. Venables and Brian D. Ripley. 1994. Modern
Applied Statistics with S-Plus. Springer-Verlag.
Steve Young, Gunnar Evermann, Thomas Hain, Dan Ker-
shaw, Gareth Moore, Julian Odell, Dave Ollason, Dan
Povey, Valtcho Valtchev, and Phil Woodland. 2002.
The HTK Book. Cambridge University Press.
72
Proceedings of SSST, NAACL-HLT 2007 / AMTA Workshop on Syntax and Structure in Statistical Translation, pages 103?110,
Rochester, New York, April 2007. c?2007 Association for Computational Linguistics
Comparing Reordering Constraints for SMT
Using Efficient BLEU Oracle Computation
Markus Dreyer, Keith Hall, and Sanjeev Khudanpur
Center for Language and Speech Processing
Johns Hopkins University
3400 North Charles Street, Baltimore, MD 21218 USA
{dreyer,keith hall,khudanpur}@jhu.edu
Abstract
This paper describes a new method to
compare reordering constraints for Statis-
tical Machine Translation. We investi-
gate the best possible (oracle) BLEU score
achievable under different reordering con-
straints. Using dynamic programming, we
efficiently find a reordering that approxi-
mates the highest attainable BLEU score
given a reference and a set of reordering
constraints. We present an empirical eval-
uation of popular reordering constraints:
local constraints, the IBM constraints,
and the Inversion Transduction Grammar
(ITG) constraints. We present results for a
German-English translation task and show
that reordering under the ITG constraints
can improve over the baseline by more
than 7.5 BLEU points.
1 Introduction
Reordering the words and phrases of a foreign sen-
tence to obtain the target word order is a fundamen-
tal, and potentially the hardest, problem in machine
translation. The search space for all possible per-
mutations of a sentence is factorial in the number
of words/phrases; therefore a variety of models have
been proposed that constrain the set of possible per-
mutations by allowing certain reorderings while dis-
allowing others. Some models (Brown et al (1996),
Kumar and Byrne (2005)) allow words to change
place with their local neighbors, but disallow global
reorderings. Other models (Wu (1997), Xiong et al
(2006)) explicitly allow global reorderings, but do
not allow all possible permutations, including some
local permutations.
We present a novel technique to compare achiev-
able translation accuracies under different reorder-
ing constraints. While earlier work has trained and
tested instantiations of different reordering models
and then compared the translation results (Zens and
Ney, 2003) we provide a more general mechanism
to evaluate the potential efficacy of reordering con-
straints, independent of specific training paradigms.
Our technique attempts to answer the question:
What is the highest BLEU score that a given trans-
lation system could reach when using reordering
constraints X? Using this oracle approach, we ab-
stract away from issues that are not inherent in the
reordering constraints, but may nevertheless influ-
ence the comparison results, such as model and fea-
ture design, feature selection, or parameter estima-
tion. In fact, we compare several sets of reorder-
ing constraints empirically, but do not train them as
models. We merely decode by efficiently search-
ing over possible translations allowed by each model
and choosing the reordering that achieves the high-
est BLEU score.
We start by introducing popular reordering con-
straints (Section 2). Then, we present dynamic-
programming algorithms that find the highest-
scoring permutations of sentences under given re-
ordering constraints (Section 3). We use this tech-
nique to compare several reordering constraints em-
pirically. We combine a basic translation framework
with different reordering constraints (Section 4) and
103
present results on a German-English translation task
(Section 5). Finally, we offer an analysis of the
results and provide a review of related work (Sec-
tions 6?8).
2 Reordering Constraints
Reordering constraints restrict the movement of
words or phrases in order to reach or approximate
the word order of the target language. Some of
the constraints considered in this paper were origi-
nally proposed for reordering words, but we will de-
scribe all constraints in terms of reordering phrases.
Phrases are units of consecutive words read off a
phrase translation table.
2.1 Local Constraints
Local constraints allow phrases to swap with one
another only if they are adjacent or very close to
each other. Kumar and Byrne (2005) define two
local reordering models for their Translation Tem-
plate Model (TTM): In the first one, called MJ-1,
only adjacent phrases are allowed to swap, and the
movement has to be done within a window of 2. A
sequence consisting of three phrases abc can there-
fore become acb or bac, but not cba. One phrase
can jump at most one phrase ahead and cannot take
part in more than one swap. In their second strategy,
called MJ-2, phrases are allowed to swap with their
immediate neighbor or with the phrase next to the
immediate neighbor; the maximum jump length is 2.
This allows for all six possible permutations of abc.
The movement here has to take place within a win-
dow of 3 phrases. Therefore, a four-phrase sequence
abcd cannot be reordered to cadb, for example. MJ-
1 and MJ-2 are shown in Figure 1.
2.2 IBM Constraints
First introduced by Brown et al (1996), the IBM
constraints are among the most well-known and
most widely used reordering paradigms. Transla-
tion is done from the beginning of the sentence to
the end, phrase by phrase; at each point in time, the
constraints allow one of the first k still untranslated
phrases to be selected for translation (see Figure 1d,
for k=2). The IBM constraints are much less restric-
tive than local constraints. The first word of the in-
put, for example, can move all the way to the end,
independent of the value of k. Typically, k is set to
4 (Zens and Ney, 2003). We write IBM with k=4 as
IBM(4). The IBM constraints are supersets of the
local constraints.
0
1
if
2
you
3
to-m
e
4
that
5
expla
in
6
cou
ld
(a) The sentence in foreign word order.
0
3
you
1
if
4
if you
2
to-m
e
5
to-m
e
7
that you
8
that
6
expla
in
to-m
e
9
expla
in
10
cou
ld that
11
cou
ld
expla
in
(b) MJ-1
0
8
you
6
to-m
e
1
if
9
if
60
to-m
e you
7
ifyou
3
that
2
to-m
e
15
to-m
e
12
that
10
expla
in
4
to-m
e
5
you youthat
17
that
19
cou
ld
16
expla
inyouto-me
18
expla
in
21
cou
ld
if you
to-m
e
13
expla
inthat
11
to-m
e
to-m
ethat
22
cou
ld
20
expla
inthatthat could
thatexpla
in
(c) MJ-2
0
6
you
1
if
7
if
11
to-m
e you
2
to-m
e
12
to-m
e
8
that you
3
that
16
that
13
expla
in you
4
expla
in
19
expla
in
17
cou
ld you
5
cou
ld
21
cou
ld you
if
15
that
to-m
e
9
expla
in
to-m
e
10
cou
ld
to-m
e
if
18
expla
in
that
60
cou
ld
that
if
20
cou
ld
expla
inif
(d) IBM(2)
Figure 1: The German word order if you to-me that explain
could (?wenn Sie mir das erkla?ren ko?nnten?) and all possible
reorderings under different constraints, represented as lattices.
None of these lattices contains the correct English order if you
could explain that to-me. See also Table 1.
2.3 ITG Constraints
The Inversion Transduction Grammar (ITG) (Wu,
1997), a derivative of the Syntax Directed Trans-
duction Grammars (Aho and Ullman, 1972), con-
strains the possible permutations of the input string
by defining rewrite rules that indicate permutations
of the string. In particular, the ITG allows all per-
mutations defined by all binary branching struc-
tures where the children of any constituent may be
swapped in order. The ITG constraint is different
from the other reordering constraints presented in
that it is not based on finite-state operations. An
104
Model # perm. ?Best? sentence n-gram precisions BLEU
MJ-1 13 if you that to-me could explain 100.0/66.7/20.0/0.0 0.0
MJ-2 52 to-me if you could explain that 100.0/83.3/60.0/50.0 70.71
IBM(2) 32 if to-me that you could explain 100.0/50.0/20.0/0.0 0.0
IBM(4) 384 if you could explain that to-me 100.0/100.0/100.0/100.0 100.0
IBM(4) (prune) 42 if you could explain that to-me 100.0/100.0/100.0/100.0 100.0
ITG 394 if you could explain that to-me 100.0/100.0/100.0/100.0 100.0
ITG (prune) 78 if you could explain that to-me 100.0/100.0/100.0/100.0 100.0
Table 1: Illustrating example: The number of permutations (# perm.) that different reordering paradigms consider for the input
sequence if you to-me that explain could, and the permutation with highest BLEU score. The sentence length is 7, but there are
only 6! possible permutations, since the phrase to-me counts as one word during reordering. ITG (prune) is the ITG BLEU decoder
with the pruning settings we used in our experiments (beam threshold 10?4). For comparison, IBM(4) (prune) is the lattice
BLEU decoder with the same pruning settings, but we use pruning only for ITG permutations in our experiments.
Figure 2: The example if
you to-me that explain could
and its reordering to if you
could explain that to-me us-
ing an ITG. The alignments
are added below the tree, and
the horizontal bars in the tree
indicate a swap.
ITG decoder runs in polynomial time and allows for
long-distance phrasal reordering. A phrase can, for
example, move from the first position in the input
to the last position in the output and vice versa, by
swapping the topmost node in the constructed bi-
nary tree. However, due to the binary bracketing
constraint, some permutations are not modeled. A
four-phrase sequence abcd cannot be permuted into
cadb or bdac. Therefore, the ITG constraints are not
supersets of the IBM constraints. IBM(4), for exam-
ple, allows abcd to be permuted into cadb and bdac.
3 Factored BLEU Computation
The different reordering strategies described allow
for different permutations and restrict the search
space in different ways. We are concerned with
the maximal achievable accuracy under given con-
straints, independent of feature design or parameter
estimation. This is what we call the oracle accuracy
under the reordering constraints and it is computed
on a dataset with reference translations.
We now describe algorithms that can be used
to find such oracle translations among unreordered
translation candidates. There are two equivalent
strategies: The reordering constraints that are be-
ing tested can be expressed as a special dynamic-
programming decoder which, when applied to an
unreordered hypothesis, searches the space of per-
mutations defined by the reordering constraints and
returns the highest-scoring permutation. We employ
this strategy for the ITG reorderings (Section 3.2).
For the other reordering constraints, we employ a
more generic strategy: Given the set of reorder-
ing constraints, all permutations of an unreordered
translation candidate are precomputed and explicitly
represented as a lattice. This lattice is passed as in-
put to a Dijkstra-style decoder (Section 3.1) which
traverses it and finds the solution that reachest the
highest BLEU score.1
3.1 Dijkstra BLEU Decoder
The Dijkstra-style decoder takes as input a lattice in
which each path represents one possible permutation
of an unreordered hypothesis under a given reorder-
ing paradigm, as in Figure 1. It traverses the lat-
tice and finds the solution that has the highest ap-
proximate BLEU score, given the reference. The
dynamic-programming algorithm divides the prob-
lem into subproblems that are solved independently,
the solutions of which contribute to the solutions
of other subproblems. The general procedure is
sketched in Figure 3: for each subpath of the lat-
tice containing the precomputed permutations, we
store the three most recently attached words (Fig-
1For both strategies, several unreordered translation candi-
dates do not have to be regarded separately, but can be repre-
sented as a weighted lattice and be used as input to the special
dynamic program or to the process that precomputes possible
permutations.
105
?([0, k, len + 1, w2, w3, wnew]) = max
w1
( get bleu ( [0, j, len, w1, w2, w3], [j, k, wnew] ) ) (1)
function get bleu ( [0, j, len, w1, w2, w3], [j, k, wnew] ) :=
update ngrams (0, j, k, len, w1, w2, w3, wnew) ;
return exp
(
1
4
4?
n=1
log
(
ngramsi([0, k, len + 1, w2, w3, wnew])
len ? n + 1
))
;
(2)
Figure 3: Top: The BLEU score is used as inside score for a subpath from 0 to k with the rightmost words w2, w3, wnew in the
Dijkstra decoder. Bottom: Pseudo code for a function get bleu which updates the n-gram matches ngrams1(. . . ), ngrams2(. . . ),
ngrams3(. . . ), ngrams4(. . . ) for the resulting subpath in a hash table [0, k, len + 1, w2, w3, wnew] and returns its approximate
BLEU score.
("",
"","
")
0/0
/0/
0
("",
"to"
,"m
e")
2/1
/0/
0
("to
","m
e","
if")
3/1
/0/
0
("m
e","
if",
"yo
u")
4/2
/0/
0
("if
","y
ou"
,"co
uld
")
5/3
/1/
0
("y
ou"
,"co
uld
","e
xpl
ain
")
6/4
/2/
1
("co
uld
","e
xpl
ain
","t
hat
")
7/5
/3/
2
0
6
to-
me if yo
u
7
if yo
u
15
yo
u
19
co
uld tha
t
ex
pla
in
20
ex
pla
in
tha
t
22
tha
t
Figure 4: Three right-most words and n-gram matches: This shows the best path for the MJ-2 reordering of if you to-me that
explain could, along with the words stored at each state and the progressively updated n-gram matches. The full path to-me if you
could explain that has 7 unigram matches, 5 bigram, 3 trigram, and 2 fourgram matches. See the full MJ-2 lattice in Figure 1c.
ure 4). A context of three words is needed to com-
pute fourgram precisions used in the BLEU score.
Starting from the start state, we recursively extend
a subpath word by word, following the paths in
the lattice. Whenever we extend the path by a
word to the right we incorporate that word and use
update ngrams to update the four n-gram counts
for the subpath. The function update ngrams has
access to the reference string2 and stores the updated
n-gram counts for the resulting path in a hash table.3
The inside score of each subpath is the approximate
BLEU score, calculated as the average of the four
n-gram log precisions. An n-gram precision is al-
ways the number of n-gram matches divided by the
length len of the path minus (n ? 1). A path of
length 4 with 2 bigram matches, for example, has
a bigram precision of 2/3. This method is similar to
Dijkstra?s algorithm (Dijkstra, 1959) composed with
a fourgram finite-state language model, where the
scoring is done using n-gram counts and precision
2Multiple reference strings can be used if available.
3An epsilon value of 1?10 is used for zero precisions.
scores. We call this the Dijkstra BLEU decoder.
3.2 ITG BLEU Decoder
For the ITG reordering constraints, we use a dy-
namic program that computes the permutations im-
plicitly. It takes only the unreordered hypothesis
as input and creates the possible reorderings under
the ITG constraints during decoding, as it creates
a parse chart. The algorithm is similar to a CKY
parsing algorithm in that it proceeds bottom-up and
combines smaller constituents into larger ones re-
cursively. Figure 5 contains details of the algo-
rithm. The ITG BLEU decoder stores the three left-
most and the three rightmost words in each con-
stituent. A constituent from position i to posi-
tion k, with wa, wb, and wc as leftmost words,
and wx, wy, wz as rightmost words is written as
[i, k, (wa, wb, wc), (wx, wy, wz)]. Such a constituent
can be built by straight or inverted rules. Using an
inverted rule means swapping the order of the chil-
dren in the built constituent. The successive bottom-
up combinations of adjacent constituents result in hi-
erarchical binary bracketing with swapped and non-
106
? ([i, k, (wa, wb, wc), (wx, wy, wz)]) = max
(
?() ([i, k, (wa, wb, wc), (wx, wy, wz)]) ,
?<> ([i, k, (wa, wb, wc), (wx, wy, wz)])
)
(3)
?<>([i, k, (wa, wb, wc), (wx, wy, wz)]) =
max
j,wa? ,wb? ,wc? ,wx? ,wy? ,wz?
(
get bleu
( [
j, k, (wa, wb, wc), (wx? , wy? , wz?)
]
,
[i, j, (wa? , wb? , wc?), (wx, wy, wz)]
) ) (4)
Figure 5: Equations for the ITG oracle BLEU decoder. [i, k, (wa, wb, wc), (wx, wy, wz)] is a constituent from i to k with leftmost
words wa,wb,wc and rightmost words wx,wy ,wz . Top: A constituent can be built with a straight or a swapped rule. Bottom: A
swapped rule. The get bleu function can be adapted from Figure 3
swapped constituents. Our ITG BLEU decoder uses
standard beam search pruning. As in Zens and Ney
(2003), phrases are not broken up, but every phrase
is, at the beginning of reordering, stored in the chart
as one lexical token together with the precomputed
n-gram matches and the n-gram precision score.
In addition to standard ITG we run experiments
with a constrained ITG, in which we impose a bound
? on the maximum length of reordered constituents,
measured in phrases. If the combined length of two
constituents exceeds this bound they can only be
combined in the given monotone order. Experiments
with this ITG variant give insight into the effect that
various long-distance reorderings have on the final
BLEU scores (see Table 3). Such bounds are also
effective speedup techniques(Eisner and Tromble,
2006).
3.3 BLEU Approximations
BLEU is defined to use the modified n-gram preci-
sion, which means that a correct n-gram that oc-
curs once in the reference, but several times in the
system translation will be counted only once as
correct. The other occurrences are clipped. We
do not include this global feature since we want
a dynamic-programming solution with polynomial
size and runtime. The decoder processes subprob-
lems independently; words are attached locally and
stored only as boundary words of covered paths/
constituents. Therefore we cannot discount a locally
attached word that has already been attached else-
where to an alternative path/constituent. However,
clipping affects most heavily the unigram scores
which are constant, like the length of the sentence.4
4Since the sentence lengths are constant for all reorderings
of a given sentence we can in our experiments also ignore the
brevity penalty which cancels out. If the input consists of sev-
We also adopt the approximation that treats every
sentence with its reference as a separate corpus (Till-
mann and Zhang, 2006) so that ngram counts are not
accumulated, and parallel processing of sentences
becomes possible. Due to these two approximations,
our method is not guaranteed to find the best reorder-
ing defined by the reordering constraints. However,
we have found on our heldout data that an oracle
that does not accumulate n-gram counts is only min-
imally worse than an oracle that does accumulate
them (up to 0.25 BLEU points).5 If, in addition,
clipping is ignored, the resulting oracle stays virtu-
ally the same, at most 0.02 BLEU points worse than
the oracle found otherwise. All results in this paper
are computed with the original BLEU formula on the
sentences found by the oracle algorithms.
4 Creating a Monotone Translation
Baseline
To compare the reordering constraints under ora-
cle conditions we first obtain unreordered candi-
date translations from a simple baseline translation
model. For each reordering paradigm, we take the
candidate translations, get the best oracle reorder-
ings under the given reordering constraints and pick
the best sentence according to the BLEU score.
The baseline translation system is created using
probabilistic word-to-word and phrase-to-phrase ta-
eral sentences of different lengths (see fn. 1) then the brevity
penalty can be built in by keeping track of length ratios of at-
tached phrases.
5The accumulating oracle algorithm makes a greedy deci-
sion for every sentence given the ngram counts so far accumu-
lated (Zens and Ney, 2005). The result of such a greedy or-
acle method may depend on the order of the input sentences.
We tried 100 shuffles of these and received 100 very simi-
lar results, with a variance of under 0.006 BLEU points. The
non-accumulating oracles use an epsilon value (1?10) for zero
counts.
107
bles. Using the translation probabilities, we create
a lattice that contains word and phrase translations
for every substring of the source sentence. The re-
sulting lattice is made of English words and phrases
of different lengths. Every word or phrase transla-
tion probability p is a mixture of p(f |e) and p(e|f).
We discard short phrase translations exponentially
by a parameter that is trained on heldout data. Inser-
tions and deletions are handled exclusively by the
use of a phrase table: an insertion takes place wher-
ever the English side of a phrase translation is longer
than the foreign side (e.g. English presidential can-
didate for German Pra?sidentschaftskandidat), and
vice versa for deletions (e.g. we discussed for wir
haben diskutiert). Gaps or discontinuous phrases
are not handled. The baseline decoder outputs the
n-best paths through the lattice according to the lat-
tice scores6, marking consecutive phrases so that the
oracle reordering algorithms can recognize them and
keep them together. Note that the baseline system is
trained on real data, while the reordering constraints
that we want to test are not trained.
5 Empirical Comparison of Reordering
Constraints
We use the monotone translation baseline model and
the oracle BLEU computation to evaluate different
popular reordering strategies. We now describe the
experimental settings. The word and phrase transla-
tion probabilities of the baseline model are trained
on the Europarl German-English training set, using
GIZA++ and the Pharaoh phrase extraction algo-
rithm. For testing we use the NAACL 2006 SMT
Shared Task test data. For each sentence of the test
set, a lattice is created in the way described in Sec-
tion 4, with parameters optimized on a small heldout
set.7 For each sentence, the 1000-best candidates ac-
cording to the lattice scores are extracted. We take
the 10-best oracle candidates, according to the ref-
erence, and use a BLEU decoder to create the best
permutation of each of them and pick the best one.
Using this procedure, we make sure that we get the
highest-scoring unreordered candidates and choose
the best one among their oracle reorderings. Table 2
6We use a straightforward adaption of Algorithm 3 in Huang
and Chiang (2005)
7We fill the initial phrase and word lattice with the 20 best
candidates, using phrases of 3 or less words.
and Figure 6 show the resulting BLEU scores for dif-
ferent sentence lengths. Table 3 shows results of the
ITG runs with different length bounds ?. The aver-
age phrase length in the candidate translations of the
test set is 1.42 words.
Oracle decodings under the ITG and under
IBM(4) constraints were up to 1000 times slower
than under the other tested oracle reordering meth-
ods in our implementations. Among the faster meth-
ods, decoding under MJ-2 constraints was up to 40%
faster than under IBM(2) constraints in our imple-
mentation.
 
20
 
25
 
30
 
35
 
40
 
45  5
 
10
 
15
 
20
 
25
 
30
 
35
 
40
BLEU
Senten
ce leng
thI
TG
IBM, 
k=4
IBM, 
k=2 MJ-2 MJ-1 Baseli
ne
Figure 6: Reordering oracle scores for different sentence
lengths. See also Table 2.
6 Discussion
The empirical results show that reordering un-
der sufficiently permissive constraints can improve
a monotone baseline oracle by more than 7.5
BLEU points. This gap between choosing the best
unreordered sentences versus choosing the best op-
timally reordered sentences is small for short sen-
tences and widens dramatically (more than nine
BLEU points) for longer sentences.
The ITG constraints and the IBM(4) constraints
both give very high oracle translation accuracies on
the German-English translation task. Overall, their
BLEU scores are about 2 to more than 4 points bet-
ter than the BLEU scores of the best other meth-
ods. This gap between the two highest-scoring con-
straints and the other methods becomes bigger as
the sentence lengths grow and is greater than 4
108
Sen
tenc
e le
ngth
# of
test
sen
ten
ces
BLEU (NIST) scores
ITG (prune) IBM, k=4 IBM, k=2 MJ-2 MJ-1 No reordering
1?5 61 48.21 (5.35) 48.21 (5.35) 48.21 (5.35) 48.21 (5.35) 48.21 (5.35) 48.17 (5.68)
6?10 230 43.83 (6.75) 43.71 (6.74) 41.94 (6.68) 42.50 (6.71) 40.85 (6.66) 39.21 (6.99)
11?15 440 33.66 (6.71) 33.37 (6.71) 31.23 (6.62) 31.49 (6.64) 29.67 (6.56) 28.21 (6.76)
16?20 447 30.47 (6.66) 29.99 (6.65) 27.00 (6.52) 27.06 (6.50) 25.15 (6.45) 23.34 (6.52)
21?25 454 30.13 (6.80) 29.83 (6.79) 27.21 (6.67) 27.22 (6.65) 25.46 (6.58) 23.32 (6.63)
26?30 399 26.85 (6.42) 26.36 (6.42) 22.79 (6.25) 22.47 (6.22) 20.38 (6.12) 18.31 (6.11)
31?35 298 28.11 (6.45) 27.47 (6.43) 23.79 (6.25) 23.28 (6.21) 21.09 (6.12) 18.94 (6.06)
36?40 242 27.65 (6.37) 26.97 (6.35) 23.31 (6.19) 22.73 (6.16) 20.70 (6.06) 18.22 (5.94)
1?40 2571 29.63 (7.48) 29.17 (7.46) 26.07 (7.24) 25.89 (7.22) 23.95 (7.08) 21.89 (7.07)
Table 2: BLEU and NIST results for different reordering methods on binned sentence lengths. The ITG results are, unlike the
other results, with pruning (beam 10?4). The BLEU results are plotted in Figure 6. All results are computed with the original
BLEU formula on the sentences found by the oracle algorithms.
BLEU scores for sentences longer than 30 sentences.
This advantage in translation accuracy comes with
high computational cost, as mentioned above.
Among the computationally more lightweight re-
ordering methods tested, IBM(2) and MJ-2 are very
close to each other in translation accuracy, with
IBM(2) obtaining slightly better scores on longer
sentences, while MJ-2 is more efficient. MJ-1 is
less successful in reordering, improving the mono-
tone baseline by only about 2.5 BLEU points at best,
but is the best choice if speed is an issue.
As described above, the reorderings defined by
the local constraints MJ-1 and MJ-2 are subsets of
IBM(2) and IBM(3). We did not test IBM(3), but
the values can be interpolated between IBM(2) and
IBM(4). The ITG constraints do not belong in this
family of finite-state contraints; they allow reorder-
ings that none of the other methods allow, and vice
versa. The fact that ITG constraints can reach such
high translation accuracies supports the findings in
Zens et al (2004) and is an empirical validation of
the ITG hypothesis.
The experiments with the constrained ITG show
the effect of reorderings spanning different lengths
(see Table 3). While most reorderings are short-
distance (<5 phrases) a lot of improvements can still
be obtained when ? is increased from length 5 to 10
and even from 10 to 20 phrases.
7 Related Work
There exist related algorithms that search the space
of reorderings and compute BLEU oracle approxi-
Len. ?=0 ?=5 ?=10 ?=20 ?=30 ?=40
26?30 18.31 24.07 26.40 26.79 26.85 26.85
31?35 18.94 25.10 27.21 28.00 28.09 28.11
36?40 18.22 24.46 26.66 27.53 27.64 27.65
26?40 18.49 24.74 26.74 27.41 27.50 27.51
Table 3: BLEU results of ITGs that are constrained to reorder-
ings not exceeding a certain span length ?. Results shown for
different sentence lengths.
mations. Zens and Ney (2005) describe a dynamic-
programming algorithm in which at every state the
number of n-gram matches is stored, along with a
multiset that contains all words from the reference
that have not yet been matched. This makes it pos-
sible to compute the modified ngram precision, but
the search space is exponential. Tillmann and Zhang
(2006) use a BLEU oracle decoder for discrimina-
tive training of a local reordering model. No de-
tails about the algorithm are given. Zens and Ney
(2003) perform a comparison of different reorder-
ing strategies. Their study differs from ours in that
they use reordering models trained on real data and
may therefore be influenced by feature selection,
parameter estimation and other training-specific is-
sues. In our study, only the baseline translation
model is trained on data. Zens et al (2004) con-
duct a study similar to Zens and Ney (2003) and note
that the results for the ITG reordering constraints
were quite dependent on the very simple probability
model used. Our study avoids this issue by using the
109
BLEU oracle approach. In Wellington et al (2006),
hand-aligned data are used to compare the standard
ITG constraints to ITGs that allow gaps.
8 Conclusions
We have presented a training-independent method
to compare different reordering constraints for ma-
chine translation. Given a sentence in foreign word
order, its reference translation(s) and reordering
constraints, our dynamic-programming algorithms
efficiently find the oracle reordering that has the ap-
proximately highest BLEU score. This allows eval-
uating different reordering constraints experimen-
tally, but abstracting away from specific features,
the probability model or training methods of the re-
ordering strategies. The presented method evaluates
the theoretical capabilities of reordering constraints,
as opposed to more arbitrary accuracies of specifi-
cally trained instances of reordering models.
Using our oracle method, we presented an em-
pirical evaluation of different reordering constraints
for a German-English translation task. The results
show that a good reordering of a given monotone
translation can improve the translation quality dra-
matically. Both short- and long-distance reorderings
contribute to the BLEU score improvements, which
are generally greater for longer sentences. Reorder-
ing constraints that allow global reorderings tend
to reach better oracles scores than ones that search
more locally. The ITG constraints and the IBM(4)
constraints both give the highest oracle scores.
The presented BLEU decoder algorithms can be
useful in many ways: They can generally help de-
cide what reordering constraints to choose for a
given translation system. They can be used for
discriminative training of reordering models (Till-
mann and Zhang, 2006). Furthermore, they can help
detecting insufficient parameterization or incapable
training algorithms: If two trained reordering model
instances show similar performances on a given task,
but the oracle scores differ greatly then the training
methods might not be optimal.
Acknowledgments
This work was partially supported by the National
Science Foundation via an ITR grant (No 0121285),
the Defense Advanced Research Projects Agency
via a GALE contract (No HR0011-06-2-0001), and
the Office of Naval Research via a MURI grant (No
N00014-01-1-0685). We thank Jason Eisner, David
Smith, Roy Tromble and the anonymous reviewers
for helpful comments and suggestions.
References
A. V. Aho and J. D. Ullman. 1972. The Theory of Parsing,
Translation, and Compiling. Prentice Hall.
A.L. Berger P. F. Brown, S. A. Della Pietra, V. J. Della Pietra,
J. R. Gillett, J. D. Lafferty, R. L. Mercer, H. Printz, and
L. Ures. 1996. Language translation apparatus and method
using context-based translation models. United States Patent
No. 5,510,981.
E.W. Dijkstra. 1959. A note on two problems in connexion
with graphs. Numerische Mathematik., 1:269?271.
J. Eisner and R. W. Tromble. 2006. Local search with very
large-scale neighborhoods for optimal permutations in Ma-
chine Translation. In Proc. of the Workshop on Computa-
tionally Hard Problems and Joint Inference, New York.
L. Huang and D. Chiang. 2005. Better k-best parsing. In Proc.
of IWPT, Vancouver, B.C., Canada.
S. Kumar and W. Byrne. 2005. Local phrase reordering
models for Statistical Machine Translation. In Proc. of
HLT/EMNLP, pages 161?168, Vancouver, B.C., Canada.
C. Tillmann and T. Zhang. 2006. A discriminative global train-
ing algorithm for Statistical MT. In Proc. of ACL, pages
721?728, Sydney, Australia.
B. Wellington, S. Waxmonsky, and D. Melamed. 2006. Empir-
ical lower bounds on the complexity of translational equiv-
alence. In Proc. of COLING-ACL, pages 977?984, Sydney,
Australia.
D. Wu. 1997. Stochastic inversion transduction grammars and
bilingual parsing of parallel corpora. Computational Lin-
guistics, 23(3):377?404.
D. Xiong, Q. Liu, and S. Lin. 2006. Maximum entropy based
phrase reordering model for Statistical Machine Translation.
In Proc. of COLING-ACL, pages 521?528, Sydney, Aus-
tralia.
R. Zens and H. Ney. 2003. A comparative study on reordering
constraints in Statistical Machine Translation. In Proc. of
ACL, pages 144?151, Sapporo, Japan.
R. Zens and H. Ney. 2005. Word graphs for Statistical Machine
Translation. In Proc. of the ACL Workshop on Building and
Using Parallel Texts, pages 191?198, Ann Arbor, MI.
R. Zens, H. Ney, T. Watanabe, and E. Sumita. 2004. Reorder-
ing constraints for phrase-based Statistical Machine Transla-
tion. In Proc. of CoLing, pages 205?211, Geneva.
110
Proceedings of the Second ACL Workshop on Syntax and Structure in Statistical Translation (SSST-2), pages 10?18,
ACL-08: HLT, Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
A Scalable Decoder for Parsing-based Machine Translation
with Equivalent Language Model State Maintenance
Zhifei Li and Sanjeev Khudanpur
Department of Computer Science and Center for Language and Speech Processing
Johns Hopkins University, Baltimore, MD 21218, USA
zhifei.work@gmail.com and khudanpur@jhu.edu
Abstract
We describe a scalable decoder for parsing-
based machine translation. The decoder is
written in JAVA and implements all the es-
sential algorithms described in Chiang (2007):
chart-parsing, m-gram language model inte-
gration, beam- and cube-pruning, and unique
k-best extraction. Additionally, parallel
and distributed computing techniques are ex-
ploited to make it scalable. We also propose
an algorithm to maintain equivalent language
model states that exploits the back-off prop-
erty of m-gram language models: instead of
maintaining a separate state for each distin-
guished sequence of ?state? words, we merge
multiple states that can be made equivalent for
language model probability calculations due
to back-off. We demonstrate experimentally
that our decoder is more than 30 times faster
than a baseline decoder written in PYTHON.
We propose to release our decoder as an open-
source toolkit.
1 Introduction
Large-scale parsing-based statistical machine trans-
lation (MT) has made remarkable progress in the
last few years. The systems being developed differ
in whether they use source- or target-language syn-
tax. For instance, the hierarchical translation sys-
tem of Chiang (2007) extracts a synchronous gram-
mar from pairs of strings, Quirk et al (2005), Liu et
al. (2006) and Huang et al (2006) perform syntac-
tic analyses in the source-language, and Galley et al
(2006) use target-language syntax.
A critical component in parsing-based MT sys-
tems is the decoder, which is complex to imple-
ment and scale up. Most of the systems described
above employ tailor-made, dedicated decoders that
are not open-source, which results in a high barrier
to entry for other researchers in the field. How-
ever, with the algorithms proposed in (Huang and
Chiang, 2005; Chiang, 2007; Huang and Chiang,
2007), it is possible to develop a general-purpose de-
coder that can be used by all the parsing-based sys-
tems. In this paper, we describe an important first-
step towards an extensible, general-purpose, scal-
able, and open-source parsing-based MT decoder.
Our decoder is written in JAVA and implements all
the essential algorithms described in Chiang (2007):
chart-parsing, m-gram language model integration,
beam- and cube-pruning, and unique k-best extrac-
tion. Additionally, parallel and distributed comput-
ing techniques are exploited to make it scalable.
Straightforward integration of an m-gram lan-
guage model (LM) into a parsing-based decoder
substantially increases its computational complex-
ity. Therefore, it is important to develop efficient
methods for LM integration. We propose an algo-
rithm to maintain equivalent LM states by exploit-
ing the back-off property of m-gram LMs. Specifi-
cally, instead of maintaining a separate state for each
distinguished sequence of ?state? words, we merge
multiple states that can be made equivalent for LM
calculations by anticipating such back-off.
We demonstrate experimentally that our decoder
is 38 times faster than a previous decoder written in
PYTHON. Furthermore, the distributed computing
permits improving translation quality via large-scale
LMs. We have successfully use our decoder to trans-
late about a million sentences in a parallel corpus for
large-scale discriminative training experiments.
10
2 Parsing-based MT Decoder
In this section, we discuss the core algorithms imple-
mented in our decoder. These algorithms have been
discussed by Chiang (2007) in detail, and we reca-
pitulate the essential parts here for completeness.
2.1 Grammar Formalism
Our decoder assumes a probabilistic synchronous
context-free grammar (SCFG). Following the nota-
tion in Venugopal et al (2007), a probabilistic SCFG
comprises a set of source-language terminal sym-
bols TS , a set of target-language terminal symbols
TT , a shared set of nonterminal symbols N , and a
set of rules of the form
X ? ??, ?,?, w? , (1)
where X ? N , ? ? [N?TS ]? is a (mixed) sequence
of nonterminals and source terminals, ? ? [N?TT ]?
is a sequence of nonterminals and target terminals,
? is a one-to-one correspondence or alignment be-
tween the nonterminal elements of ? and ?, and
w ? 0 is a weight assigned to the rule. An illus-
trative rule for Chinese-to-English translation is
NP ? ?NP0{ NP1 , NP1 of NP0 ? ,
where the Chinese word { (pronounced de or di)
means of, and the alignment, encoded via subscripts
on the nonterminals, causes the two noun phrases
around { to be reordered around of in the transla-
tion. The rule weight is omitted in this example.
A bilingual SCFG derivation is analogous to a
monolingual CFG derivation. It begins with a pair
of aligned start symbols. At each step, an aligned
pair of nonterminals is rewritten as the two corre-
sponding components of a single rule. In this sense,
the derivations are generated synchronously.
Our decoder presently handles SCFGs of the kind
extracted by Heiro (Chiang, 2007), but is easily ex-
tensible to more general SCFGs and closely related
formalisms such as synchronous tree substitution
grammars (Eisner, 2003; Chiang, 2006).
2.2 MT Decoding as Chart Parsing
Given a source-language sentence f?, the decoder
must find the target-language yield e(D) of the best
derivation D among all derivations with source-
language yield f(D) = f?, i.e.
e? = e
(
arg max
D : f(D)=f?
w(D)
)
, (2)
where w(D) is the composite weight of D.
The parser may be treated as a deductive proof
system (Shieber et al, 1995). Formally (cf. (Chiang,
2007)), a parser defines a space of weighted items,
with some items designated as axioms and some as
goals, and a set of inference rules of the form
I1 : w1 ? ? ? Ik : wk
I : w ? ,
which states that if all the antecedent items Ii are
provable, respectively with weight wi, then the con-
sequent item I is provable with weight w, provided
the side condition ? holds. For a grammar with a
maximum of two (pairs of) nonterminals per rule1,
Figure 1 illustrates the resulting chart parsing proce-
dure, including the integration of an m-gram LM.
The actual decoding algorithm maintains a chart,
which contains an array of cells. Each cell in turn
maintains a list of proved items. The parsing process
starts with the axioms, and proceeds by applying the
inference rules to prove more and more items until
a goal item is proved. Whenever the parser proves a
new item, it adds the item to the appropriate chart
cell. It also maintains backpointers to antecedent
items, which are used for k-best extraction, as dis-
cussed in Section 2.4 below.
In a SCFG-based decoder, an item is identi-
fied by its source-language span, left-side non-
terminal label, and left- and right-context for the
target-language m-gram LM. Therefore, in a given
cell, the maximum possible number of items is
O(|N ||TT |2(m?1)), and the worst case decoding
complexity is
O
(
|N |K |TT |2K(m?1)n3
)
, (3)
where K is the maximum number of nonterminal
pairs per rule and n is the source-language sentence
length (Venugopal et al, 2007).
1For more general grammars with K ? 2 pairs of non-
terminals per rule, see Venugopal et al (2007).
11
X???,??:w (X ? ??, ?,w?) ? G
X??fji+1, ?? : w
[X, i, j; q(?)] : wp(?)
Z??f i1i+1Xfjj1+1, ?? : w [X,i1,j1;e1] : w1
[Z, i, j; q(?? )] : ww1p(?? ) ?
? = ?[e1/X]
Z??f i1i+1X1f
i2
j1+1Y2f
j
j2+1, ?? : w [X,i1,j1;e1] : w1 [Y,i2,j2;e2] : w2
[Z, i, j; q(?? )] : ww1w2p(?? ) ?
? = ?[e1/X1, e2/Y2]
Goal item: [S, 0, n; ?s?m?1 ? e?/s?]
Figure 1: Inference rules from Chiang (2007) for a parser with an m-gram LM. G denotes the translation grammar.
w[x/X] denotes substitution of the string x for the symbol X in the string w. The function p(?) provides the LM
probability for all complete m-grams in a string, while the function q(?) elides symbols whose m-grams have been
accounted for by p(?). Details about the functions p(?) and q(?) are provided in Section 4.
2.3 Pruning in a Decoder
Severe pruning is needed in order to make the decod-
ing computationally feasible for SCFGs with large
vocabularies TT and detailed nonterminal sets. In
our decoder, we incorporate two pruning techniques
described by (Chiang, 2007; Huang and Chiang,
2007). For beam pruning, in each cell, we discard
all items whose weight is worse, by a relative thresh-
old ?, than the weight of the best item in the same
cell. If too many items pass the threshold, a cell only
retains the top-b items by weight. When combining
smaller items to obtain a larger item by applying an
inference rule, we use cube-pruning to simulate k-
best extraction in each destination cell, and discard
combinations that lead to an item whose weight is
worse than the best item in that cell by a margin of
?.
2.4 k-best Extraction Over Hyper-graphs
For each source-language sentence f?, the output
of the chart-parsing algorithm may be treated as a
hyper-graph representing a set of likely hypotheses
D in (2). Briefly, a hyper-graph is a set of vertices
and hyper-edges, with each hyper-edge connecting
a set of antecedent vertices to a consequent vertex,
and a special vertex designated as the target vertex.
In parsing parlance, a vertex corresponds to an item
in the chart, a hyper-edge corresponds to a SCFG
rule with the nonterminals on the right-side replaced
by back-pointers to antecedent items, and the target
vertex corresponds to the goal item2.
Given a hyper-graph for a source-language sen-
tence f?, we use the k-best extraction algorithm of
Huang and Chiang (2005) to extract its k most likely
translations. Moreover, since many different deriva-
tions D in (2) may lead to the same target-language
yield e(D), we adopt the modification described in
Huang et al (2006) to efficiently generate the unique
k best translations of f?.
3 Parallel and Distributed Computing
Many applications of parsing-based MT entail the
use of SCFGs extracted from millions of bilin-
gual sentence pairs and LMs extracted from bil-
lions of words of target-language text. This requires
the decoder to make use of distributed computing
to spread the memory required to load large-scale
SCFGs and LMs onto multiple processors. Further-
more, techniques such as iterative minimum error-
rate training (Och et al, 2003) as well as web-based
MT services require the decoder to translate a large
number of source-language sentences per unit time.
This requires the decoder to make use of parallel
computing to utilize each individual multi-core pro-
cessor more effectively. We have incorporated two
such performance enhancements in our decoder.
2In a decoder integrating an m-gram LM, there may be mul-
tiple goal items due to different LM contexts. However, one can
image a single goal item identified by the span [0, n] and the
goal nonterminal S, but not by the LM contexts.
12
3.1 Parallel Decoding
We have enhanced our decoder to translate multiple
source-language sentences in parallel by exploiting
the ability of a multi-core processor to concurrently
run several threads that share memory. Specifi-
cally, given one (or more) document(s) containing
multiple source-language sentences, the decoder au-
tomatically splits the set of sentences into several
subsets, and initiates concurrent decoding threads;
once all the threads finish, the main thread merges
back the translations. Since all the threads naturally
share memory, the decoder needs to load the (large)
SCFG and LM into memory only once. This multi-
threading provides a very significant speed-up.
3.2 Distributed Language Models
It is not possible in some cases to load a very large
LM into memory on a single machine, particularly
if the SCFG is also very large. In other cases, load-
ing the LM each time the decoder runs may be too
time-consuming relative to the time required for de-
coding itself, such as in iterative decoding with up-
dated combination weights during minimum error-
rate training. It is therefore desirable to have dedi-
cated servers to load parts of the LM3 ? an idea that
has been exploited by (Zhang et al, 2006; Emami et
al., 2007; Brants et al, 2007).
Our implementation can load a (partitioned) LM
on different servers before initiating decoding. The
decoder remotely calls the servers to obtain individ-
ual LM probabilities, and linearly interpolates them
on the fly using a given set of interpolation weights.
With this architecture, one can deal with a very large
target-language text corpus by splitting it into many
parts and training separate LMs from each. The run-
time interpolation capability may also be used for
LM adaptation, e.g. for building document-specific
language models.
To mitigate potential network communication de-
lays inherent to a distributed LM, we implement a
simple cache mechanism in the decoder. The cache
saves the outcomes of the most recent LM calls,
including interpolated LM probabilities; the cache
is reset whenever its size exceeds a threshold. We
could have maintained a cache at each LM server
as well; however, the resultant saving is not signif-
3Similarly, distributing the SCFG is also possible.
icant because the trie data-structures used to imple-
ment m-gram LMs are quite fast relative to the cache
lookup overhead.
4 Equivalent LM-state Maintenance
It is clear from the complexity (3) of the inference
rules (Figure 1) that a straightforward integration
of an m-gram LM adds a multiplicative factor of
|TT |2K(m?1) to the computational complexity of the
decoder, where TT is the set of target-language ter-
minal symbols. We illustrate in this section how this
potentially very large multiplier can be dramatically
reduced by exploiting the structure of the LM.
4.1 Applying an m-gram LM in the Decoder
Integrating an LM into chart parsing requires two
functions p(?) and q(?) (see Figure 1) that oper-
ate on strings over TT ? {?}, where ? is a special
?placeholder? symbol for an elided part of a target-
language string.
The function p(e) calculates the LM probability
of the complete m-grams in e ? e1 . . . el, i.e.
p(e1 . . . el) =
?
m? i? l & ? 6? eii?(m?1)
PLM(ei |hi) , (4)
where hi = ei?(m?1) . . . ei?1 is the m?1-word
?LM history? of the target-language word ei.
Since the p-probability of e does not include the
LM probability for the partial m-grams (i.e., the first
(m? 1) words) of e, the exact weights of two items
[X, i, j; e] and [X, i, j; e?] in the chart are not avail-
able during the bottom-up pruning of Section 2.3.
Therefore, as an approximation, we also compute
p?(e) =
min{m?1, |e|}?
k=1
PLM(ek | e1 . . . ek?1), (5)
an estimate of the LM probability of the m?1-gram
prefix of e. This estimated probability is taken into
account for pruning purposes (only).
The function q(e1 . . . el) determines the left and
right LM states that must be maintained for future
computation of the exact LM probability, respec-
tively, of e1 . . . em?1 and el+1 . . . el+m?1.
q(e1 . . . el) (6)
=
{
e1 . . . el if l < m? 1,
e1 . . . em?1 ? el?(m?2) . . . el otherwise.
13
4.2 Back-off Parameterization of m-gram LMs
While many different methods are popular for esti-
mating m-gram LMs, most store the estimated LM
parameters in the ARPA back-off file format; using
the notation eji to denote a target-language word se-
quence ei ei+1 . . . ej , the LM probability calcula-
tion is carried out as
PBO(em | em?11 ) (7)
=
{
pi(em1 ) if em1 ? LM
?(em?11 )? PBO(em | em?12 ) otherwise,
where the lower order probability PBO(em | em?12 )
is recursively defined in the same way, and ?(em?11 )
is the back-off weight of the history. The LM file
contains the parameter pi(?) for each listed m-gram,
and the parameters pi(?) and ?(?) for each listed m?-
gram, 1 ? m? < m; for unlisted m?-grams, ?(?) = 1
by definition.
Observe from (7) that if em1 is not listed in the LM,
the back-off weight ?(?) is the same for all words
em, and the backed-off probability PBO(em | ?) is the
same for all words e1. Furthermore, as m grows, the
fraction of possible m-grams actually observed in a
training corpus diminishes rapidly.
4.3 The Equivalent LM State of an Item
The maximum possible number of items in a cell in-
creases exponentially with the LM order m, as dis-
cussed in Section 2.2. With pruning (cf. Section
2.3), we restrict the maximum number of items in
each cell to some threshold b. Intuitively, therefore,
if we increase the LM order m, we should also in-
crease the beam size b to reduce search errors. This
could slow down the decoder significantly.
Recall from the previous subsection, however,
that when m increases, the fraction of m-grams
that will need to back-off also increases. Moreover,
even for modest values of m, the decoder consid-
ers many ?unseen? m-grams (due to reordering and
translation combinations) that do not appear in natu-
ral texts, leading to frequent back-off during the LM
probability calculation (7). In this subsection, we
propose a method to collapse equivalent LM states
so that the decoder effectively considers many more
items in each cell without increasing beam size.
We merge multiple LM states (6) that already
have?or back-off to?the same ?LM history? in
the calculation (7) of LM probabilities, e.g. due
to different unlisted m-grams that back-off to the
same m?1-gram. For simplicity, we only consider
LM state merging by the function q(?) of (6) when
l ? m?1.
Though the equivalent LM state maintenance
technique is discussed here in the context of a
parsing-based MT decoder, it is also applicable to
standard left-to-right phrase-based decoders. In par-
ticular, the right-side equivalent LM state mainte-
nance proposed in Section 4.3.1 may be used.
4.3.1 Obtaining the Equivalent Right LM State
Recall that the right LM state ell?(m?2) of el1
serves as the ?LM history? for calculating the ex-
act LM probabilities of the yet-to-be-determined
word el+1. Recall further the computation (7) of
PBO(el+1 | ell?(m?2)).
? If the m-gram el+1l?(m?2) is not listed in the LM
for any word el+1, then the LM will back-off to
PBO(el+1 | ell?(m?3)), which does not depend
on the word el?(m?2).
? If the m?1-gram ell?(m?2) also is not listed in
the LM, then ?(ell?(m?2)) = 1.
If these two conditions hold true, q(?) may safely
elide the word el?(m?2) in (6) no matter what words
follow el1. The right LM state is thus reduced from
m? 1 words to m? 2 words.
The argument above can be applied recursively
to the resulting right LM state ell?(m?2)+i, where
i ? [0,m ? 2], leading to the equivalent right state
computation procedure of Figure 2. The procedure
IS-A-PREFIX(em?1 ) checks if its argument em?1 is a pre-
fix of any k-gram listed in the LM, k ? [m?,m].
4.3.2 Obtaining the Equivalent Left LM State
Recall that the left LM state em?11 of el1 is
the prefix whose exact LM probability is unknown
during bottom-up parsing, and is replaced by the
estimated probability p?(em?11 ) of (5) for pruning
purposes. Recall further the computation (7) of
PBO(em?1 | em?20 ).
? If the m-gram em?10 is not listed in the LM
for any word e0, then it will back-off to
14
EQ-R-STATE (ell?(m?2))
1 ers ? ell?(m?2)
2 for i ? 0 to m? 2 ? left to right
3 if IS-A-PREFIX (ell?(m?2)+i)
4 break ? stop reducing ers
5 else
6 ers ? ell?(m?2)+i+1 ? reduce state
7 return ers
Figure 2: Equivalent Right LM State Computation.
PBO(em?1 | em?21 ), which can be computed
right away based on em?11 without waiting for
the unknown e0. Moreover, the back-off weight
?(em?20 ) does not depend on the word em?1.
Therefore, q(?) may safely elide the word em?1, and
reduce the left LM state in (6) from em?11 to em?21 .
Also, p(?) should also co-opt PBO(em?1 | em?21 ) into
the complete m-gram probability of (4) and p?(?)
should exclude em?1 in (5).
The argument above can again be applied recur-
sively to the resulting left LM state ei1, i ? [1,m?1],
leading to the equivalent left state procedure of Fig-
ure 3. The procedure IS-A-SUFFIX(em?1 ) checks if
em?1 is a suffix of any listed k-gram in the LM, k ?
[m?,m]. In Figure 3, fin refers to the probability
that can be computed right away based on the state
itself, for co-opting into the complete m-gram prob-
ability of (4) as mentioned above.
4.3.3 Modified Cost Functions for Parsing
When carrying out the reduction of the left and
right LM states to their shortest equivalents, the for-
mula (4) for calculating the probability of the com-
plete m-grams in an item [X, i, j; e], where e = el1,
is modified as
p(el1)
= EQ-L-STATE(em?11 ).fin?
?
m? i? l & ? 6? eii?(m?1)
PLM(ei |hi)
with the further qualification that some care must be
taken later to incorporate the back-off weights of the
?LM histories? of the suffix of em?11 that went miss-
ing due to left LM state reduction.
EQ-L-STATE (em?11 )
1 els ? em?11
2 fin ? 1 ? update to final probability p
3 for i ? m? 1 to 1 ? right to left
4 if IS-A-SUFFIX(ei1)
5 break ? stop reducing els
6 else
7 fin ? PBO(ei | ei?11 )? fin
8 els ? ei?11 ? reduce state
9 return els, fin
Figure 3: Equivalent Left LM State Computation.
The estimated probability of the left LM state is
modified as
p?(e) =
{
p?(e) if |e| < m? 1
p?(EQ-L-STATE(em?11 ).els) otherwise,
with p? as defined in (5).
Finally, the LM state function is
q(el1)
=
?
???
???
e1 . . . el if l < m? 1
EQ-L-STATE(em?11 ).els ?
EQ-R-STATE(ell?(m?2)).ers otherwise.
4.3.4 Suffix and Prefix Look-Up
As done in the SRILM toolkit (Stolcke, 2002), a
back-off m-gram LM is stored using a reverse trie
data structure. We store the suffix and prefix infor-
mation in the same data structure without incurring
much additional memory cost. Specifically, the pre-
fix information is stored at the back-off state, while
the suffix information is stored as one bit alongside
the regular m-gram probability.
5 Experimental Results
In this section, we evaluate the performance of our
decoder on a Chinese to English translation task.
5.1 System Training
We use various parallel text corpora distributed by
the Linguistic Data Consortium (LDC) for the NIST
MT evaluation. The parallel data we select contains
about 570K Chinese-English sentence pairs, adding
15
up to about 19M words on each side. To train the
English language models, we use the English side
of the parallel text and a subset of the English Giga-
word corpus, for a total of about 130M words.
We use the GIZA toolkit (Och and Ney, 2000),
a suffix-array architecture (Lopez, 2007), the
SRILM toolkit (Stolcke, 2002), and minimum er-
ror rate training (Och et al, 2003) to obtain word-
alignments, a translation model, language models,
and the optimal weights for combining these mod-
els, respectively.
5.2 Improvements in Decoding Speed
We use a PYTHON implementation of a state-of-
the-art decoder as our baseline4 for decoder compar-
isons. For a direct comparison, we use exactly the
same models and pruning parameters. The SCFG
contains about 3M rules, the 5-gram LM explicitly
lists about 49M k-grams, k = 1, 2, . . . , 5, and the
pruning uses ? = 10, b = 30 and ? = 0.1.
Decoder Speed BLEU-4(sec/sent) MT ?03 MT ?05
Python 26.5 34.4% 32.7%
Java 1.2 34.5% 32.9%Java (parallel) 0.7
Table 1: Decoder Comparison: Translation speed and
quality on the 2003 and 2005 NIST MT benchmark tests.
As shown in Table 1, the JAVA decoder (without
explicit parallelization) is 22 times faster than the
PYTHON decoder, while achieving slightly better
translation quality as measured by BLEU-4 (Pap-
ineni et al, 2002). The parallelization further speeds
it up by a factor of 1.7, making the parallel JAVA de-
coder is 38 times faster than the PYTHON decoder.
We have used the decoder to successfully decode
about one million sentences for a large-scale dis-
criminative training experiment.
5.3 Impact of a Distributed Language Model
We use the SRILM toolkit to build eight 7-gram lan-
guage models, and load and call the LMs using a
4We are extremely thankful to Philip Resnik at University of
Maryland for allowing us the use of their PYTHON decoder as
the baseline. Thanks also go to David Chiang who originally
implement the decoder.
distributed LM architecture5 as discussed in Section
3.2. As shown in Table 2, the 7-gram distributed lan-
guage model (DLM) significantly improves trans-
lation performance over the 5-gram LM. However,
decoding is significantly slower (12.2 sec/sent when
using the non-parallel decoder) due to the added net-
work communication overhead.
LM type # k-grams MT ?03 MT ?05
5-gram LM 49 M 34.5% 32.9%
7-gram DLM 310 M 35.5% 33.9%
Table 2: Distributed language model: the 7-gram LM
cannot be loaded alongside the SCFG on a single ma-
chine; via distributed computing, it yields significant im-
provement in BLEU-4 over a 5-gram.
5.4 Utility of Equivalent LM States
To reduce the number of search errors, one may ei-
ther increase the beam size, or employ techniques
such as the equivalent LM state maintenance de-
scribed in Section 4. In this subsection, we compare
the tradeoff between the search effort (measured by
decoding time per sentence) and the search qual-
ity (measured by the average model cost of the best
translation found).
Intuitively, collapsing equivalent LM states is use-
ful only when the language model is very sparse, i.e.,
most of the evaluated m-grams will need to back-
off. A sparse LM is obtained in practice by using
a large order m relative to the amount of training
data. To test this intuition, we train a 7-gram LM
using only the English side of the parallel text (?
19M words). Figure 4 compares maintenance of
the full LM state v/s the equivalent LM state. The
beam size b for decoding with equivalent LM states
is fixed at 30; it is increased considerably?30, 50,
70, 90, 120, and 150?with the full LM state in
an effort to reduce search errors. It is clear from
the figure that collapsing items that differ due only
to equivalent LM states improves the search quality
considerably while actually reducing search effort.
This shows the effectiveness of equivalent LM state
maintenance.
5Since our distributed LM architecture dynamically interpo-
lates multiple LM scores, it cannot yet exploit the equivalent
LM state maintenance of Section 4, for different LMs will have
different reduced LM states. We will address this in the future.
16
Search Effort vs Search Quality
19.95
19.97
19.99
20.01
20.03
20.05
20.07
0 2 4 6 8 10
Number of Seconds per Sentence
Av
g M
od
el 
Co
st 
for
 on
e-b
es
ts
Baseline
EquivLM
 
beam size = 30 
Figure 4: Search quality with equivalent 7-gram LM state
maintenance (EquivLM) and without it (Baseline) as a
function of search effort as controlled by the beam size.
We also train a 3-gram LM using an English cor-
pus of about 130M words, and repeat the above ex-
periments. In this case, maintaining equivalent LM
states costs more decoding time than using the full
LM state to achieve the same search quality. This
is due partly to our inefficient implementation of the
prefix- and suffix-lookup required to determine the
equivalent LM state, and partly to the fact that with
130M words, a 3-gram LM backs off less frequently.
6 Conclusions
We have described a scalable decoder for parsing-
based machine translation. It is written in JAVA and
implements all the essential algorithms described
in Chiang (2007): chart-parsing, m-gram language
model integration, beam- and cube-pruning, and
unique k-best extraction. Additionally, parallel and
distributed computing techniques are exploited to
make it scalable. We demonstrate that our decoder
is 38 times faster than a baseline decoder written in
PYTHON, and that the distributed language model
is very useful to improve translation quality in a
large-scale task. We also describe an algorithm that
exploits the back-off property of an m-gram model
to maintain equivalent LM states, and show that bet-
ter search quality is obtained with less search effort
when the search space is organized to exploit this
equivalence. We plan to incorporate some additional
syntax-based components into the decoder and re-
lease it as an open-source toolkit.
Acknowledgments
We thank Philip Resnik, Chris Dyer, Smaranda
Muresan and Adam Lopez for very helpful discus-
sions, and the anonymous reviewers for their con-
structive comments. This research was partially sup-
ported by the Defense Advanced Research Projects
Agency?s GALE program via Contract No? HR0011-
06-2-0001.
References
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och,
and Jeffrey Dean. 2006. Large Language Models in
Machine Translation. In Proceedings of EMNLP 2007.
David Chiang. 2006. An Introduction to
Synchronous Grammars. Available at
http://www.isi.edu/?chiang/papers/synchtut.pdf.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201-228.
Jason Eisner. 2003. Learning non-isomorphic tree map-
pings for machine translation. In Proceedings of ACL
2003.
Ahmad Emami, Kishore Papineni, and Jeffrey Sorensen.
2007. Large-scale distributed language modeling. In
Proceedings of ICASSP 2007.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proceed-
ings of COLING/ACL 2006.
Liang Huang and David Chiang. 2005. Better k-best pars-
ing. In Proceedings of IWPT 2005.
Liang Huang and David Chiang. 2007. Forest Rescoring:
Faster Decoding with Integrated Language Models. In
Proceedings of the ACL 2007.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In Proceedings of AMTA 2006.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-
string alignment template for statistical machine trans-
lation. In Proceedings of COLING-ACL 2006.
Adam Lopez. 2007. Hierarchical Phrase-Based Transla-
tion with Suffix Arrays. In Proceedings of EMNLP
2007.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of ACL
2003.
Franz Josef Och and Hermann Ney. 2000. Improved
statistical alignment models. In Proceedings of ACL
2000.
17
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proceedings of ACL
2002.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. De-
pendency Treelet Translation: Syntactically Informed
Phrasal SMT. In Proceedings of ACL 2005.
Stuart Shieber, Yves Schabes, and Fernando Pereira.
1995. Principles and implementation of deductive
parsing. Journal of Logic Programming, 24:3-15.
Andreas Stolcke. 2002. SRILM - an extensible language
modeling toolkit. In Proceedings of the International
Conference on Spoken Language Processing, volume
2, pages 901-904.
Ashish Venugopal, Andreas Zollmann, Stephan Vo-
gel. 2007. An Efficient Two-Pass Approach to
Synchronous-CFG Driven Statistical MT. In Proceed-
ings of NAACL 2007.
Ying Zhang, Almut Silja Hildebrand, and Stephan Vogel.
2006. Distributed language modeling for n-best list re-
ranking. In Proceedings of EMNLP 2006.
18
Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 135?139,
Athens, Greece, 30 March ? 31 March 2009. c?2009 Association for Computational Linguistics
Joshua: An Open Source Toolkit for Parsing-based Machine Translation
Zhifei Li, Chris Callison-Burch, Chris Dyer,? Juri Ganitkevitch,+ Sanjeev Khudanpur,
Lane Schwartz,? Wren N. G. Thornton, Jonathan Weese and Omar F. Zaidan
Center for Language and Speech Processing, Johns Hopkins University, Baltimore, MD
? Computational Linguistics and Information Processing Lab, University of Maryland, College Park, MD
+ Human Language Technology and Pattern Recognition Group, RWTH Aachen University, Germany
? Natural Language Processing Lab, University of Minnesota, Minneapolis, MN
Abstract
We describe Joshua, an open source
toolkit for statistical machine transla-
tion. Joshua implements all of the algo-
rithms required for synchronous context
free grammars (SCFGs): chart-parsing, n-
gram language model integration, beam-
and cube-pruning, and k-best extraction.
The toolkit also implements suffix-array
grammar extraction and minimum error
rate training. It uses parallel and dis-
tributed computing techniques for scala-
bility. We demonstrate that the toolkit
achieves state of the art translation per-
formance on the WMT09 French-English
translation task.
1 Introduction
Large scale parsing-based statistical machine
translation (e.g., Chiang (2007), Quirk et al
(2005), Galley et al (2006), and Liu et al (2006))
has made remarkable progress in the last few
years. However, most of the systems mentioned
above employ tailor-made, dedicated software that
is not open source. This results in a high bar-
rier to entry for other researchers, and makes ex-
periments difficult to duplicate and compare. In
this paper, we describe Joshua, a general-purpose
open source toolkit for parsing-based machine
translation, serving the same role as Moses (Koehn
et al, 2007) does for regular phrase-based ma-
chine translation.
Our toolkit is written in Java and implements
all the essential algorithms described in Chiang
(2007): chart-parsing, n-gram language model in-
tegration, beam- and cube-pruning, and k-best ex-
traction. The toolkit also implements suffix-array
grammar extraction (Lopez, 2007) and minimum
error rate training (Och, 2003). Additionally, par-
allel and distributed computing techniques are ex-
ploited to make it scalable (Li and Khudanpur,
2008b). We have also made great effort to ensure
that our toolkit is easy to use and to extend.
The toolkit has been used to translate roughly
a million sentences in a parallel corpus for large-
scale discriminative training experiments (Li and
Khudanpur, 2008a). We hope the release of the
toolkit will greatly contribute the progress of the
syntax-based machine translation research.1
2 Joshua Toolkit
When designing our toolkit, we applied general
principles of software engineering to achieve three
major goals: Extensibility, end-to-end coherence,
and scalability.
Extensibility: The Joshua code is organized
into separate packages for each major aspect of
functionality. In this way it is clear which files
contribute to a given functionality and researchers
can focus on a single package without worrying
about the rest of the system. Moreover, to mini-
mize the problems of unintended interactions and
unseen dependencies, which is common hinder-
ance to extensibility in large projects, all exten-
sible components are defined by Java interfaces.
Where there is a clear point of departure for re-
search, a basic implementation of each interface is
provided as an abstract class to minimize the work
necessary for new extensions.
End-to-end Cohesion: There are many compo-
nents to a machine translation pipeline. One of the
great difficulties with current MT pipelines is that
these diverse components are often designed by
separate groups and have different file format and
interaction requirements. This leads to a large in-
vestment in scripts to convert formats and connect
the different components, and often leads to unten-
able and non-portable projects as well as hinder-
1The toolkit can be downloaded at http://www.
sourceforge.net/projects/joshua, and the in-
structions in using the toolkit are at http://cs.jhu.
edu/?ccb/joshua.
135
ing repeatability of experiments. To combat these
issues, the Joshua toolkit integrates most critical
components of the machine translation pipeline.
Moreover, each component can be treated as a
stand-alone tool and does not rely on the rest of
the toolkit we provide.
Scalability: Our third design goal was to en-
sure that the decoder is scalable to large models
and data sets. The parsing and pruning algorithms
are carefully implemented with dynamic program-
ming strategies, and efficient data structures are
used to minimize overhead. Other techniques con-
tributing to scalability includes suffix-array gram-
mar extraction, parallel and distributed decoding,
and bloom filter language models.
Below we give a short description about the
main functions implemented in our Joshua toolkit.
2.1 Training Corpus Sub-sampling
Rather than inducing a grammar from the full par-
allel training data, we made use of a method pro-
posed by Kishore Papineni (personal communica-
tion) to select the subset of the training data con-
sisting of sentences useful for inducing a gram-
mar to translate a particular test set. This method
works as follows: for the development and test
sets that will be translated, every n-gram (up to
length 10) is gathered into a map W and asso-
ciated with an initial count of zero. Proceeding
in order through the training data, for each sen-
tence pair whose source-to-target length ratio is
within one standard deviation of the average, if
any n-gram found in the source sentence is also
found in W with a count of less than k, the sen-
tence is selected. When a sentence is selected, the
count of every n-gram in W that is found in the
source sentence is incremented by the number of
its occurrences in the source sentence. For our
submission, we used k = 20, which resulted in
1.5 million (out of 23 million) sentence pairs be-
ing selected for use as training data. There were
30,037,600 English words and 30,083,927 French
words in the subsampled training corpus.
2.2 Suffix-array Grammar Extraction
Hierarchical phrase-based translation requires a
translation grammar extracted from a parallel cor-
pus, where grammar rules include associated fea-
ture values. In real translation tasks, the grammars
extracted from large training corpora are often far
too large to fit into available memory.
In such tasks, feature calculation is also very ex-
pensive in terms of time required; huge sets of
extracted rules must be sorted in two directions
for relative frequency calculation of such features
as the translation probability p(f |e) and reverse
translation probability p(e|f) (Koehn et al, 2003).
Since the extraction steps must be re-run if any
change is made to the input training data, the time
required can be a major hindrance to researchers,
especially those investigating the effects of tok-
enization or word segmentation.
To alleviate these issues, we extract only a sub-
set of all available rules. Specifically, we follow
Callison-Burch et al (2005; Lopez (2007) and use
a source language suffix array to extract only those
rules which will actually be used in translating a
particular set of test sentences. This results in a
vastly smaller rule set than techniques which ex-
tract all rules from the training set.
The current code requires suffix array rule ex-
traction to be run as a pre-processing step to ex-
tract the rules needed to translate a particular test
set. However, we are currently extending the de-
coder to directly access the suffix array. This will
allow the decoder at runtime to efficiently extract
exactly those rules needed to translate a particu-
lar sentence, without the need for a rule extraction
pre-processing step.
2.3 Decoding Algorithms2
Grammar formalism: Our decoder assumes a
probabilistic synchronous context-free grammar
(SCFG). Currently, it only handles SCFGs of the
kind extracted by Heiro (Chiang, 2007), but is eas-
ily extensible to more general SCFGs (e.g., (Gal-
ley et al, 2006)) and closely related formalisms
like synchronous tree substitution grammars (Eis-
ner, 2003).
Chart parsing: Given a source sentence to de-
code, the decoder generates a one-best or k-best
translations using a CKY algorithm. Specifically,
the decoding algorithm maintains a chart, which
contains an array of cells. Each cell in turn main-
tains a list of proven items. The parsing process
starts with the axioms, and proceeds by applying
the inference rules repeatedly to prove new items
until proving a goal item. Whenever the parser
proves a new item, it adds the item to the appro-
priate chart cell. The item also maintains back-
2More details on the decoding algorithms are provided in
(Li et al, 2009a).
136
pointers to antecedent items, which are used for
k-best extraction.
Pruning: Severe pruning is needed in order to
make the decoding computationally feasible for
SCFGs with large target-language vocabularies.
In our decoder, we incorporate two pruning tech-
niques: beam and cube pruning (Chiang, 2007).
Hypergraphs and k-best extraction: For each
source-language sentence, the chart-parsing algo-
rithm produces a hypergraph, which represents
an exponential set of likely derivation hypotheses.
Using the k-best extraction algorithm (Huang and
Chiang, 2005), we extract the k most likely deriva-
tions from the hypergraph.
Parallel and distributed decoding: We also
implement parallel decoding and a distributed
language model by exploiting multi-core and
multi-processor architectures and distributed com-
puting techniques. More details on these two fea-
tures are provided by Li and Khudanpur (2008b).
2.4 Language Models
In addition to the distributed LM mentioned
above, we implement three local n-gram language
models. Specifically, we first provide a straightfor-
ward implementation of the n-gram scoring func-
tion in Java. This Java implementation is able to
read the standard ARPA backoff n-gram models,
and thus the decoder can be used independently
from the SRILM toolkit.3 We also provide a na-
tive code bridge that allows the decoder to use the
SRILM toolkit to read and score n-grams. This
native implementation is more scalable than the
basic Java LM implementation. We have also im-
plemented a Bloom Filter LM in Joshua, following
Talbot and Osborne (2007).
2.5 Minimum Error Rate Training
Johsua?s MERT module optimizes parameter
weights so as to maximize performance on a de-
velopment set as measuered by an automatic eval-
uation metric, such as Bleu. The optimization
consists of a series of line-optimizations along
the dimensions corresponding to the parameters.
The search across a dimension uses the efficient
method of Och (2003). Each iteration of our
MERT implementation consists of multiple weight
3This feature allows users to easily try the Joshua toolkit
without installing the SRILM toolkit and compiling the native
bridge code. However, users should note that the basic Java
LM implementation is not as scalable as the native bridge
code.
updates, each reflecting a greedy selection of the
dimension giving the most gain. Each iteration
also optimizes several random ?intermediate ini-
tial? points in addition to the one surviving from
the previous iteration, as an approximation to per-
forming multiple random restarts. More details on
the MERT method and the implementation can be
found in Zaidan (2009).4
3 WMT-09 Translation Task Results
3.1 Training and Development Data
We assembled a very large French-English train-
ing corpus (Callison-Burch, 2009) by conducting
a web crawl that targted bilingual web sites from
the Canadian government, the European Union,
and various international organizations like the
Amnesty International and the Olympic Commit-
tee. The crawl gathered approximately 40 million
files, consisting of over 1TB of data. We converted
pdf, doc, html, asp, php, etc. files into text, and
preserved the directory structure of the web crawl.
We wrote set of simple heuristics to transform
French URLs onto English URLs, and considered
matching documents to be translations of each
other. This yielded 2 million French documents
paired with their English equivalents. We split the
sentences and paragraphs in these documents, per-
formed sentence-aligned them using software that
IBM Model 1 probabilities into account (Moore,
2002). We filtered and de-duplcated the result-
ing parallel corpus. After discarding 630 thousand
sentence pairs which had more than 100 words,
our final corpus had 21.9 million sentence pairs
with 587,867,024 English words and 714,137,609
French words.
We distributed the corpus to the other WMT09
participants to use in addition to the Europarl
v4 French-English parallel corpus (Koehn, 2005),
which consists of approximately 1.4 million sen-
tence pairs with 39 million English words and 44
million French words. Our translation model was
trained on these corpora using the subsampling de-
scried in Section 2.1.
For language model training, we used the
monolingual news and blog data that was as-
sembled by the University of Edinburgh and dis-
tributed as part of WMT09. This data consisted
4The module is also available as a standalone applica-
tion, Z-MERT, that can be used with other MT systems.
(Software and documentation at: http://cs.jhu.edu/
?ozaidan/zmert.)
137
of 21.2 million English sentences with half a bil-
lion words. We used SRILM to train a 5-gram
language model using a vocabulary containing the
500,000 most frequent words in this corpus. Note
that we did not use the English side of the parallel
corpus as language model training data.
To tune the system parameters we used News
Test Set from WMT08 (Callison-Burch et al,
2008), which consists of 2,051 sentence pairs
with 43 thousand English words and 46 thou-
sand French words. This is in-domain data that
was gathered from the same news sources as the
WMT09 test set.
3.2 Translation Scores
The translation scores for four different systems
are reported in Table 1.5
Baseline: In this system, we use the GIZA++
toolkit (Och and Ney, 2003), a suffix-array archi-
tecture (Lopez, 2007), the SRILM toolkit (Stol-
cke, 2002), and minimum error rate training (Och,
2003) to obtain word-alignments, a translation
model, language models, and the optimal weights
for combining these models, respectively.
Minimum Bayes Risk Rescoring: In this sys-
tem, we re-ranked the n-best output of our base-
line system using Minimum Bayes Risk (Kumar
and Byrne, 2004). We re-score the top 300 trans-
lations to minimize expected loss under the Bleu
metric.
Deterministic Annealing: In this system, in-
stead of using the regular MERT (Och, 2003)
whose training objective is to minimize the one-
best error, we use the deterministic annealing
training procedure described in Smith and Eisner
(2006), whose objective is to minimize the ex-
pected error (together with the entropy regulariza-
tion technique).
Variational Decoding: Statistical models in
machine translation exhibit spurious ambiguity.
That is, the probability of an output string is split
among many distinct derivations (e.g., trees or
segmentations). In principle, the goodness of a
string is measured by the total probability of its
many derivations. However, finding the best string
(e.g., during decoding) is then computationally in-
tractable. Therefore, most systems use a simple
Viterbi approximation that measures the goodness
5Note that the implementation of the novel techniques
used to produce the non-baseline results is not part of the cur-
rent Joshua release, though we plan to incorporate it in the
next release.
System BLEU-4
Joshua Baseline 25.92
Minimum Bayes Risk Rescoring 26.16
Deterministic Annealing 25.98
Variational Decoding 26.52
Table 1: The uncased BLEU scores on WMT-09
French-English Task. The test set consists of 2525
segments, each with one reference translation.
of a string using only its most probable deriva-
tion. Instead, we develop a variational approxima-
tion, which considers all the derivations but still
allows tractable decoding. More details will be
provided in Li et al (2009b). In this system, we
have used both deterministic annealing (for train-
ing) and variational decoding (for decoding).
4 Conclusions
We have described a scalable toolkit for parsing-
based machine translation. It is written in Java
and implements all the essential algorithms de-
scribed in Chiang (2007) and Li and Khudanpur
(2008b): chart-parsing, n-gram language model
integration, beam- and cube-pruning, and k-best
extraction. The toolkit also implements suffix-
array grammar extraction (Callison-Burch et al,
2005; Lopez, 2007) and minimum error rate train-
ing (Och, 2003). Additionally, parallel and dis-
tributed computing techniques are exploited to
make it scalable. The decoder achieves state of
the art translation performance.
Acknowledgments
This research was supported in part by the Defense
Advanced Research Projects Agency?s GALE pro-
gram under Contract No. HR0011-06-2-0001 and
the National Science Foundation under grants
No. 0713448 and 0840112. The views and find-
ings are the authors? alone.
References
Chris Callison-Burch, Colin Bannard, and Josh
Schroeder. 2005. Scaling phrase-based statisti-
cal machine translation to larger corpora and longer
phrases. In Proceedings of ACL.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder. 2008.
Further meta-evaluation of machine translation. In
Proceedings of the Third Workshop on Statistical
Machine Translation (WMT08).
138
Chris Callison-Burch. 2009. A 109 word parallel cor-
pus. In preparation.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
Jason Eisner. 2003. Learning non-isomorphic tree
mappings for machine translation. In Proceedings
of ACL.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Pro-
ceedings of the ACL/Coling.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proceedings of the International Work-
shop on Parsing Technologies.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of HLT/NAACL.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, , and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the ACL-2007 Demo and Poster Ses-
sions.
Philipp Koehn. 2005. A parallel corpus for statistical
machine translation. In Proceedings of MT-Summit,
Phuket, Thailand.
Shankar Kumar and William Byrne. 2004. Minimum
bayes-risk decoding for statistical machine transla-
tion. In Proceedings of HLT/NAACL.
Zhifei Li and Sanjeev Khudanpur. 2008a. Large-scale
discriminative n-gram language models for statisti-
cal machine translation. In Proceedings of AMTA.
Zhifei Li and Sanjeev Khudanpur. 2008b. A scalable
decoder for parsing-based machine translation with
equivalent language model state maintenance. In In
Proceedings Workshop on Syntax and Structure in
Statistical Translation.
Zhifei Li, Chris Callison-Burch, Sanjeev Khudanpur,
and Wren Thornton. 2009a. Decoding in joshua:
Open source, parsing-based machine translation.
The Prague Bulletin of Mathematical Linguistics,
91:47?56.
Zhifei Li, Jason Eisner, and Sanjeev Khudanpur.
2009b. Variational decoding for statistical machine
translation. In preparation.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment templates for statistical machine
translation. In Proceedings of the ACL/Coling.
Adam Lopez. 2007. Hierarchical phrase-based trans-
lation with suffix arrays. In Proceedings of EMNLP-
CoLing.
Robert C. Moore. 2002. Fast and accurate sentence
alignment of bilingual corpora. In Proceedings of
AMTA.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate training
for statistical machine translation. In Proceedings
of ACL.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005.
Dependency treelet translation: Syntactically in-
formed phrasal smt. In Proceedings of ACL.
David A. Smith and Jason Eisner. 2006. Minimum risk
annealing for training log-linear models. In Pro-
ceedings of the ACL/Coling.
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In Proceedings of the Inter-
national Conference on Spoken Language Process-
ing, Denver, Colorado, September.
David Talbot and Miles Osborne. 2007. Randomised
language modelling for statistical machine transla-
tion. In Proceedings of ACL.
Omar F. Zaidan. 2009. Z-MERT: A fully configurable
open source tool for minimum error rate training of
machine translation systems. The Prague Bulletin of
Mathematical Linguistics, 91:79?88.
139
Coling 2010: Poster Volume, pages 656?664,
Beijing, August 2010
Unsupervised Discriminative Language Model Training
for Machine Translation using Simulated Confusion Sets
Zhifei Li and Ziyuan Wang and Sanjeev Khudanpur and Jason Eisner
Center for Language and Speech Processing
Johns Hopkins University
zhifei.work@gmail.com,{zwang40, khudanpur, eisner}@jhu.edu
Abstract
An unsupervised discriminative training
procedure is proposed for estimating a
language model (LM) for machine trans-
lation (MT). An English-to-English syn-
chronous context-free grammar is derived
from a baseline MT system to capture
translation alternatives: pairs of words,
phrases or other sentence fragments that
potentially compete to be the translation
of the same source-language fragment.
Using this grammar, a set of impostor
sentences is then created for each En-
glish sentence to simulate confusions that
would arise if the system were to process
an (unavailable) input whose correct En-
glish translation is that sentence. An LM
is then trained to discriminate between
the original sentences and the impostors.
The procedure is applied to the IWSLT
Chinese-to-English translation task, and
promising improvements on a state-of-
the-art MT system are demonstrated.
1 Discriminative Language Modeling
A language model (LM) constitutes a crucial com-
ponent in many tasks such as machine translation
(MT), speech recognition, information retrieval,
handwriting recognition, etc. It assigns a pri-
ori probabilities to word sequences. In general,
we expect a low probability for an ungrammat-
ical or implausible word sequence. The domi-
nant LM used in such systems is the so-called
n-gram model, which is typically derived from a
large corpus of target language text via maximum
likelihood estimation, mitigated by some smooth-
ing or regularization. Due to the Markovian as-
sumptions implicit in n-gram models, however,
richer linguistic and semantic dependencies are
not well captured. Rosenfeld (1996) and Khu-
danpur and Wu (2000) address such shortcom-
ing by using maximum entropy models with long-
span features, while still working with a locally
normalized left-to-right LM. The whole-sentence
maximum entropy LM of Rosenfeld et al (2001)
proposes a globally normalized log-linear LM in-
corporating several sentence-wide features.
The n-gram as well as the whole-sentence
model are generative or descriptive models of
text. However, in a task like Chinese-to-English
MT, the de facto role of the LM is to discriminate
among the alternative English translations being
contemplated by the MT system for a particular
Chinese input sentence. We call the set of such
alternative translations a confusion set. Since a
confusion set is typically a minuscule subset of
the set of all possible word sequences, it is ar-
guably better to train the LM parameters so as to
make the best candidate in the confusion set more
likely than its competitors, as done by Roark et al
(2004) for speech recognition and by Li and Khu-
danpur (2008) for MT. Note that identifying the
best candidate requires supervised training data?
bilingual text in case of MT?which is expensive
in many domains (e.g. weblog or newsgroup) and
for most language pairs (e.g. Urdu-English).
We propose a novel discriminative LM in this
paper: a globally normalized log-linear LM that
can be trained in an efficient and unsupervised
manner, using only monolingual (English) text.
The main idea is to exploit (translation) un-
certainties inherent in an MT system to de-
rive an English-to-English confusion grammar
(CG), illustrated in this paper for a Hiero sys-
tem (Chiang, 2007). From the bilingual syn-
chronous context-free grammar (SCFG) used in
Hiero, we extract a monolingual SCFG, with rules
of the kind, X ? ?strong tea, powerful tea? or
656
X ? ?in X1, in the X1?. Thus our CG is also an
SCFG that generates pairs of English sentences
that differ from each other in ways that alterna-
tive English hypothesis considered during transla-
tion would differ from each other. This CG is then
used to ?translate? each sentence in the LM train-
ing corpus into what we call its confusion set ? a
set of other ?sentences? with which that sentence
would likely be confused by the MT system, were
it to be the target translation of a source-language
sentence. Sentences in the training corpus, each
paired with its confusion set, are then used to train
a discriminative LM to prefer the training sen-
tences over the alternatives in their confusion sets.
Since the monolingual CG and the bilingual
Hiero grammar are both SCFGs, the confusion
sets are isomorphic with translation hypergraphs
that are used by supervised discriminative train-
ing. The confusion sets thus simulate the super-
vised case, with a key exception: lack of any
(Chinese) source-language information. There-
fore, only target-side ?language model? probabil-
ities may be estimated from confusion sets.
We carry out this discriminative training proce-
dure, and empirically demonstrate promising im-
provements in translation quality.
2 Discriminative LM Training
2.1 Whole-sentence Maximum Entropy LM
We aim to train a globally normalized log-linear
language model p?(y) of the form
p?(y) = Z?1 ef(y)?? (1)
where y is an English sentence, f(y) is a vector
of arbitrary features of y, ? is the (weight) vec-
tor of model parameters, and Z def= ?y? ef(y?)?? is
a normalization constant. Given a set of English
training sentences {yi}, the parameters ? may be
chosen to maximize likelihood, as
?? = argmax
?
?
i
p?(yi). (2)
This is the so called whole-sentence maximum
entropy (WSME) language model1 proposed by
1Note the contrast with the maximum entropy n-gram
LM (Rosenfeld, 1996; Khudanpur and Wu, 2000), where the
normalization is performed for each n-gram history.
Rosenfeld et al (2001). Training the model of
(2) requires computing Z, a sum over all possible
word sequences y? with any length, which is com-
putationally intractable. Rosenfeld et al (2001)
approximate Z by random sampling.
2.2 Supervised Discriminative LM Training
In addition to the computational disadvantage, (2)
also has a modeling limitation. In particular, in
a task like MT, the primary role of the LM is to
discriminate among alternative translations of a
given source-language sentence. This set of alter-
natives is typically a minuscule subset of all pos-
sible target-language word sequences. Therefore,
a better way to train the global log-linear LM,
given bilingual text {(xi, yi)}, is to generate the
real confusion set N (xi) for each input sentence
xi using a specific MT system, and to adjust ? to
discriminate between the reference translation yi
and y? ? N (xi) (Roark et al, 2004; Li and Khu-
danpur, 2008).
For example, one may maximize the condi-
tional likelihood of the bilingual training data as
?? = argmax
?
?
i
p?(yi |xi) (3)
= argmax
?
?
i
ef(xi,yi)???
y??N (xi) ef(xi,y
?)?? ,
which entails summing over only the candidate
translations y? of the given input xi. Furthermore,
if the features f(xi, y) are depend on only the out-
put y, i.e. on the English-side features of the bilin-
gual text, the resulting discriminative model may
be interpreted as a language model.
Finally, in a Hiero style MT system, if f(xi, y)
depends on the target-side(s) of the bilingual rules
used to construct y from xi, we essentially have a
syntactic LM.
2.3 Unsupervised Discriminative Training
using Simulated Confusion Sets
While the supervised discriminative LM training
has both computational and modeling advantages
over the WSME LM, it relies on bilingual data,
which is expensive to obtain for several domains
and language pairs. For such cases, we propose
a novel discriminative language model, which is
657
still a global log-linear LM with the modeling ad-
vantage and computational efficiency of (3) but re-
quires only monolingual text {yi} for training ?.
Specifically, we propose to modify (3) as
?? = argmax
?
?
i
p?(yi | N (yi)) (4)
= argmax
?
?
i
ef(yi)???
y??N (yi) ef(y
?)?? ,
where N (yi) is a simulated confusion set for yi
obtained by applying a confusion grammar to yi,
as detailed in Section 3. Our hope is that N (yi)
resembles the actual confusion set N (xi) that an
MT system would generate if it were given the in-
put sentence xi.
Like (3), the maximum likelihood training of
(4) does not entail the expensive computation of a
global normalization constant Z, and is therefore
very efficient. Unlike (3) however, where the input
xi for each output yi is needed to create N (xi),
the model of (4) can be trained in an unsupervised
manner with only {yi}.
3 Unsupervised Discriminative Training
of the Language Model for MT
The following is thus the proposed procedure for
unsupervised discriminative training of the LM.
1. Extract a confusion grammar (CG) from the
baseline MT system.
2. ?Translate? each English sentence in the LM
training corpus, using the CG as an English-
to-English translation model, to generate a
simulated confusion set.
3. Train a discriminative language model on the
simulated confusion sets, using the corre-
sponding original English sentences as the
training references.
The trained model may then be used for actual MT
decoding. We next describe each step in detail.
3.1 Extracting a Confusion Grammar
We assume a synchronous context free grammar
(SCFG) formalism for the confusion grammar
(CG). While the SCFG used by the MT system
is bilingual, the CG we extract will be monolin-
gual, with both the source and target sides being
English. Some example CG rules are:
X ? ? strong tea , powerful tea ? ,
X ? ?X0 at beijing , beijing ?s X0 ? ,
X ? ?X0 of X1 , X0 of the X1 ? ,
X ? ?X0 ?s X1 , X1 of X0 ? .
Like a regular SCFG, a CG contains rules with
different ?arities? and reordering of the nontermi-
nals (as shown in the last example) capturing the
confusions that the MT system encounters when
choosing word senses, reordering patterns, etc.
3.1.1 Extracting a Confusion Grammar from
the Bilingual Grammar
The confusion grammar is derived from the MT
system?s bilingual grammar. In Hiero, the bilin-
gual rules are of the form X ? ?c, e?, where
both c and e may contain (a matched number of)
nonterminal symbols. For every c which appears
on the source-side of two different Hiero rules
X ? ?c, e1? and X ? ?c, e2?, we extract two CG
rules, X ? ?e1, e2? and X ? ?e2, e1?, to capture
the confusion the MT system would face were it
to encounter c in its input. For each Hiero rule
X ? ?c, e?, we also extract X ? ?e, e?, the iden-
tity rule. Therefore, if a pattern c appears with |E|
different translation options, we extract |E|2 dif-
ferent CG rules from c. In our current work, the
rules of the CG are unweighted.
3.1.2 Test-set Specific Confusion Grammars
If the bilingual grammar contains all the rules
that are extractable from the bilingual training cor-
pus, the resulting confusion grammar is likely to
be huge. As a way of reducing computation, the
bilingual grammar can be restricted to a specific
test set, and only rules used by the MT system for
translating the test set used for extracting the CG.2
To economize further, one may extract a CG
from the translation hypergraphs that are gener-
ated for the test-set. Recall that a node in a hy-
pergraph corresponds to a specific source (Chi-
nese) span, and the node has many incident hy-
peredges, each associated with a different bilin-
2Test-set specific CGs are of course only practical for off-
line applications.
658
gual rule. Therefore, all the bilingual rules asso-
ciated with the incoming hyperedges of a given
node translate the same Chinese string. At each
hypergraph node, we extract CG rules to represent
the competing English sides as described above.
Note that even though different rules associated
with a node may have different ?arity,? we extract
CG rules only from pairs of bilingual rules that
have the same arity.
A CG extracted from only the bilingual rule
pairs incident on the same node in the test hy-
pergraphs is, of course, much smaller than a CG
extracted from the entire bilingual grammar. It
is also more suitable for our task, since the test
hypergraphs have already benefited from a base-
line n-gram LM and pruning, removing all confu-
sions that are easily resolved (rightly or wrongly)
by other system components.
3.2 Generating Simulated Confusion Sets
For each English sentence y in the training cor-
pus, we use the extracted CG to produce a simu-
lated confusion set N (y). This is done like a reg-
ular MT decoding pass, because we can treat the
CG as a Hiero style ?translation? grammar3 for an
English-to-English translation system.
Since the CG is an SCFG, the confusion set
N (y) generated for a sentence y is a hypergraph,
encoding not only the alternative sentences y? but
also the hierarchical derivation tree for each y?
from y (e.g., which phrase in y has been re-
placed with what in y?). As usual, many differ-
ent derivation trees d may correspond to the same
string/sentence y? due to spurious ambiguity. We
use D(y) to denote the set of derivations d, which
is a hypergraph representation of N (y).
Figure 1 presents an example confusion hy-
pergraph for the English sentence y =?a cat on
the mat,? containing four alternative hypotheses:
3To make sure that we produce at least one derivation tree
for each y, we need to add to the CG the following two glue
rules, as done in Hiero (Chiang, 2007).
S ? ?X0 , X0 ? ,
S ? ?S0 X1 , S0 X1 ? .
We also add an out of vocabulary rule X ? ?word, oov? for
each word in y and set the cost of this rule to a high value so
that the OOV rule will get used only when the CG does not
know how to ?translate? the word.
X ? ? a cat , the cat ?
X ? ? the mat , the mat ?
X ? ?X0 on X1 , X0 X1 ?
X ? ?X0 on X1 , X0 ?s X1 ?
X ? ?X0 on X1 , X1 on X0 ?
X ? ?X0 on X1 , X1 of X0 ?
S ? ?X0 , X0 ?
(a) An example confusion grammar.
a
0
  cat
1             
       on
2
         the
3
 mat
4
S??X0,X0?
X
0,5
X
0,2
X
3,5
X ? ? a cat , the cat ? X ? ? the mat , the mat ?
X ? ?X0 on X1 , X0 X1 ?
X ? ?X0 on X1 , X0 ?s X1 ? X ? ?X0 on X1 , X1 of X0 ?
X ? ?X0 on X1 , X1 on X0 ?
S
0,5
(b) An example hypergraph generated by the confusion
grammar of (a) for the input sentence ?a cat on the mat.?
Figure 1: Example confusion grammar and simulated
confusion hypergraph. Given an input sentence y = ?a cat
on the mat,? the confusion grammar of (a) generates a hyper-
graph D(y) shown in (b), which represents the confusion set
N (y) containing four alternative sentences y?.
N (y) = { ?the cat the mat,? ?the cat ?s the mat,?
?the mat of the cat,? ?the mat on the cat?}.
Notice that each competitor y? ? N (y) can be
regarded as the result of a ?round-trip? translation
y ? x ? y?, in which we reconstruct a possible
Chinese source sentence x that our Hiero bilin-
gual grammar could translate into both y and y?.4
We will train our LM to prefer y, which was ac-
tually observed. Our CG-based round-trip forces
x? y? to use the same hierarchical segmentation
of x as y ? x did. This constraint leads to effi-
cient training but artificially reduces the diversity
4This is because of the way we construct our CG from the
Hiero grammar. However, the identity and glue rules in our
CG allow almost any portion of y to be preserved untrans-
lated through the entire y ? x ? y? process. Much of y
will necessarily be preserved in the situation where the CG is
extracted from a small test set and hence has few non-identity
rules. See (Li, 2010) for further discussion.
659
ofN (y). In other recent work (Li et al, 2010), we
have taken the round-trip view more seriously, by
imputing likely source sentences x and translating
them back to separate, weighted confusion forests
N (y), without any same-segmentation constraint.
3.3 Confusion-based Discriminative Training
With the training sentences yi and their simulated
confusion sets N (yi) ? represented as hyper-
graphs D(yi)) ? we can perform the discrimi-
native training using any of a number of proce-
dures such as MERT (Och, 2003) or MIRA as
used by Chiang et al (2009). In our paper, we
use hypergraph-based minimum risk (Li and Eis-
ner, 2009),
?? = argmin
?
?
i
Risk?(yi) (5)
= argmin
?
?
i
?
d?D(yi)
L(Y(d), yi)p?(d |D(yi)),
where L(y?, yi) is the loss (e.g negated BLEU) in-
curred by producing y? when the true answer is yi,
Y(d) is the English yield of a derivation d, and
p?(d |D(yi)) is defined as,
p?(d |D(yi)) = e
f(d)??
?
d?D(yi) ef(d)??
, (6)
where f(d) is a feature vector over d. We will
specify the features in Section 5, but in general
they should be defined such that the training will
be efficient and the actual MT decoding can use
them conveniently.
The objective of (5) is differentiable and thus
we can optimize ? by a gradient-based method.
The risk and its gradient on a hypergraph can
be computed by using a second-order expectation
semiring (Li and Eisner, 2009).
3.3.1 Iterative Training
In practice, the full confusion set N (y) defined
by a confusion grammar may be too large and we
have to perform pruning when training our model.
But the pruning itself may depend on the model
that we aim to train. How do we solve this circu-
lar dependency problem? We adopt the following
procedure. Given an initial model ?, we generate a
hypergraph (with pruning) for each y, and train an
optimal ?? of (5) on these hypergraphs. Then, we
use the optimal ?? to regenerate a hypergraph for
each y, and do the training again. This iterates un-
til convergence. This procedure is quite similar to
the k-best MERT (Och, 2003) where the training
involves a few iterations, and each iteration uses a
new k-best list generated using the latest model.
3.4 Applying the Discriminative LM
First, we measure the goodness of our language
model in a simulated task. We generate simulated
confusion sets N (y) for some held out English
sentences y, and test how well p?(d |D(y)) can
recover y from N (y). This is merely a proof of
concept, and may be useful in deciding which fea-
tures f(d) to employ for discriminative training.
The intended use of our model is, of course, for
actual MT decoding (e.g., translating Chinese to
English). Specifically, we can add the discrimina-
tive model into an MT pipeline as a feature, and
tune its weight relative to other models in the MT
system, including the baseline n-gram LM.
4 Related and Similar Work
The detailed relation between the proposed pro-
cedure and other language modeling techniques
has been discussed in Sections 1 and 2. Here, we
review two other methods that are related to our
method in a broader context.
4.1 Unsupervised Training of Global
Log-linear Models
Our method is similar to the contrastive estimation
(CE) of Smith and Eisner (2005) and its succes-
sors (Poon et al, 2009). In particular, our confu-
sion grammar is like a neighborhood function in
CE. Also, our goal is to improve both efficiency
and accuracy, just as CE does. However, there
are two important differences. First, the neigh-
borhood function in CE is manually created based
on human insights about the particular task, while
our neighborhood function, generated by the CG,
is automatically learnt (e.g., from the bilingual
grammar) and specific to the MT system being
used. Therefore, our neighborhood function is
more likely to be informative and adaptive to the
task. Secondly, when tuning ?, CE uses the maxi-
mum likelihood training, but we use the minimum
660
risk training of (5). Since our training uses a task-
specific loss function, it is likely to perform better
than maximum likelihood training.
4.2 Paraphrasing Models
Our method is also related to methods for train-
ing paraphrasing models (Quirk et al, 2004; Ban-
nard and Callison-Burch, 2005; Callison-Burch et
al., 2006; Madnani et al, 2007). Specifically, the
form of our confusion grammar is similar to that
of the paraphrase model they use, and the ways
of extracting the grammar/model are also similar
as both employ a second language (e.g., Chinese
in our case) as a pivot. However, while a ?trans-
lation? rule in a paraphrase model is expected to
contain a pair of phrases that are good alterna-
tives for each other, a confusion rule in our CG
is based on an MT system processing unseen test
data and contains pairs of phrases that are typi-
cally bad (and only rarely good) alternatives for
each other.
The motivation and goal are also different. For
example, the goal of Bannard and Callison-Burch
(2005) is to extract paraphrases with the help of
parallel corpora. Callison-Burch et al (2006) aim
to improve MT quality by adding paraphrases in
the translation table, while Madnani et al (2007)
aim to improve the minimum error rate training by
adding the automatically generated paraphrases
into the English reference sets. In contrast, our
motivation is to train a discriminative language
model to improve MT (by using the confusion
grammar to decide what alternatives the model
should learn to discriminate).
5 Experimental Results
We have applied the confusion-based discrimina-
tive language model (CDLM) to the IWSLT 2005
Chinese-to-English text translation task5 (Eck and
Hori, 2005). We see promising improvements
over an n-gram LM for a solid Joshua-based
baseline system (Li et al, 2009).
5.1 Data Partitions for Training & Testing
Four kinds of data are used for CDLM training:
5This is a relatively small task compared to, say, the NIST
MT tasks. We worked on it for a proof-of-concept. Having
been successful, we are now investigating larger MT tasks.
# sentencesData Usage ZH EN
Set1 TM & LM training 40k 40k
Set2 Min-risk training 1006 1006?16
Set3 CDLM training ? 1006?16
Set4 Test 506 506?16
Table 1: Data sets used. Set1 contains translation-equivalent
Chinese-English sentence pairs, while for each Chinese sen-
tence in Set2 and Set4, there are 16 English translations. Set3
happens to be the English side of Set2 due to lack of ad-
ditional in-domain English text, but this is not noteworthy;
Set3 could be any in-domain target-language text corpus.
Set1 a bilingual training set on which 10 individ-
ual MT system components are trained,
Set2 a small bilingual, in-domain set for tuning
relative weights of the system components,
Set3 an in-domain monolingual target-language
corpus for CDLM training, and
Set4 a test set on which improvements in MT per-
formance is measured.
We partition the IWSLT data into four such sub-
sets as listed in Table 1.
5.2 Baseline MT System
Our baseline translation model components are
estimated from 40k pairs of utterances from the
travel domain, called Set1 in Table 1. We use a 5-
gram language model with modified Kneser-Ney
smoothing (Chen and Goodman, 1998), trained on
the English side of Set1, as our baseline LM.
The baseline MT system comprises 10 com-
ponent models (or ?features?) that are standard
in Hiero (Chiang, 2007), namely the baseline
language model (BLM) feature, three baseline
translation model features, one word-insertion
penalty (WP) feature, and five arity features ?
three to count how many rules with an arity of
zero/one/two are used in a derivation, and two
to count how many times the unary and binary
glue rules are used in a derivation. The rela-
tive weights of these 10 features are tuned via
hypergraph-based minimum risk training (Li and
Eisner, 2009) on the bilingual data Set2.
The resulting MT system gives a BLEU score of
48.5% on Set4, which is arguably a solid baseline.
661
5.3 Unsupervised Training of the CDLM
We extract a test-set specific CG from the hyper-
graphs obtained by decoding Set2 and Set4, as de-
scribed in Section 3.1.2. The number of rules in
the bilingual grammar and the CG are about 167k
and 1583k respectively. The CG is used as the
?translation? model to generate confusion hyper-
graphs for sentences in Set3.
Two CDLMs, corresponding to different fea-
ture sets f(d) in equation (6), were trained.
Only n-gram LM Features: We consider a
CDLM with only two features f(d): a base-
line LM feature (BLM) that equals the 5-
gram probability of Y(d) and a word penalty
feature (WP) equal to the length of Y(d).
Target-side Rule Bigram Features6: For each
CG rule used in d, we extract counts of bi-
grams that appear on the target-side of the
CG rule. For example, if the confusion rule
X ? ?X0 of X1 , X0 of the X1 ? is used in
d, the bigram features in f(d) whose counts
are incremented are: ?X of,? ?of the? and
?the X .?7 Note that the indices on the non-
terminals in the rule have been removed. To
avoid very rare features, we only consider
the 250 most freqent terminal symbol (En-
glish words) in the English of Set1 and map
all other terminal symbols into a single class.
Finally, we replace the identities of words
with their dominant POS tags. These restric-
tions result in 525 target-side rule bigram
(TsRB) features f(d) in the model of (6).
For each choice of the feature vector f(d), be it
2- or 527-dimensional, we use the training proce-
dure of Section 3.3.1 to iteratively minimize the
objective of (5) and get the CDLM parameter ??.
Note that each English sentence in Set3 has 15
other paraphrases. We generate a separate confu-
sion hypergraph D(y) for each English sentence
y, but for each such hypergraph we use both y
and its 15 paraphrases as ?reference translations?
when computing the risk L(Y(d), {y}) in (5).8
6Note that these features are novel in MT.
7With these target-side rule-based features, our LM is es-
sentially a syntactic LM, not just an LM on English strings.
8We take unfair advantage of this unusual dataset to com-
5.4 Results on Monolingual Simulation
We first probe how our novel CDLM performs as
a language model itself. One usually uses the per-
plexity of the LM on some unseen text to measure
its goodness. But since we did not optimize the
CDLM for likelihood, we instead examine how
it performs in discriminating between a good En-
glish sentence and sentences with which the MT
system may confuse that sentence. The test is per-
formed as follows. For each test English sentence
y of Set4, the confusion grammar defines a full
confusion set N (y) via a hypergraph D(y). We
use a LM to pick the most likely y? from N (y),
and then compute its BLEU score by using y and
its 15 paraphrase sentences as references. The
higher the BLEU, the better is the LM in picking
out a good translation from N (y).
Table 2 shows the results9 under a regular n-
gram LM and the two CDLMs described in Sec-
tion 5.3.
The baseline LM (BLM) entails no weight op-
timization a la (5) on Set3. The CDLM with the
BLM and word pentaly (WP) features improves
over the baseline LM. Compared to either of them,
the CDLM with the target-side rule bigram fea-
tures (TsRB) performs dramatically better.
5.5 Results on MT Test Data
We now examine how our CDLM performs during
actual MT decoding. To incorporate the CDLM
into MT decoding, we add the log-probability (6)
of a derivation d under the CDLM as an additional
bat an unrelated complication?a seemingly problematic in-
stability in the minimum risk training procedure.
As an illustration of this problem, we note that in super-
vised tuning of the baseline MT system (|f(d)|=10) with
500 sentences from Set2, the BLEU score on Set4 varies from
38.6% to 44.2% to 47.8% if we use 1, 4 and 16 reference
translations during the supervised training respectively. We
choose a system tuned on 16 references on Set2 as our base-
line. In order not to let the unsupervised CDLM training
suffer from this unrelated limitation of the tuning procedure,
we give it too the benefit of being able to compute risk on
Set3 using y plus its 15 paraphrases.
We wish to emphasize that this trait of Set3 having 15
paraphrases for each sentence is otherwise unnecessary, and
does not detract much from the main claim of this paper.
9Note that the scores in Table 2 are very low compared to
scores for actual translation from Chinese shown in Table 3.
This is mainly because in this monolingual simulation, the
LM is the only model used to rank the y? ? N (y). Said dif-
ferently, y? is being chosen in Table 2 entirely for its fluency
with no consideration whatsoever for its adequacy.
662
LM used for Features used BLEU
rescoring BLM WP TsRB on Set4
Baseline LM X 12.8
CDLM X X 14.2
CDLM X X X 25.3
Table 2: BLEU scores in monolingual simulations. Rescor-
ing the confusion sets of English sentences created using the
CG shows that the CDLM with TsRB features recovers hy-
potheses much closer to the sentence that generated the con-
fusion set than does the baseline n-gram LM.
Model used Features used BLEU
for rescoring 10 models TsRB on Set4
Joshua X 48.5
+ CDLM X X 49.5
Table 3: BLEU scores on the test set. The baseline MT sys-
tem has ten models/features, and the proposed system has
one additional model, the CDLM. Note that for the CDLM,
only the TsRB features are used during MT decoding.
feature, on top of the 10 features already present
in baseline MT system (see Section 5.2). We then
(re)tune relative weights for these 11 features on
the bilingual data Set2 of Table 1.
Note that the MT system also uses the BLM and
WP features whose weights are now retuned on
Set2. Therefore, when integrating a CDLM into
MT decoding, it is mathematically equivalent to
use only the TsRB features of the CDLM, with
the corresponding weights as estimated alongside
its ?own? BLM and WP features during unsuper-
vised discriminative training on Set3.
Table 3 reports the results. A BLEU score im-
provement of 1% is seen, reinforcing the claim
that the unsupervised CDLM helps select better
translations from among the system?s alternatives.
5.6 Goodness of Simulated Confusion Sets
The confusion set N (y) generated by applying
the CG to an English sentence y aims to simulate
the real confusion set that would be generated by
the MT system if the system?s input was the Chi-
nese sentence whose English translation is y. We
investigate, in closing, how much the simulated
confusion set resembles to the real one. Since
we know the actual input-output pairs (xi, yi) for
Set4, we generate two confusion sets: the simu-
lated set N (yi) and the real one N (xi).
One way to measure the goodness of N (yi) as
a proxy for N (xi), is to extract the n-gram types
n-gram Precision Recall
unigram 36.5% 48.2%
bigram 10.1% 12.8%
trigram 3.7% 4.6%
4-gram 2.0% 2.4%
Table 4: n-gram precision and recall of simulated con-
fusion sets relative to the true confusions when translating
Chinese sentences. The n-grams are collected from k-best
strings in both cases, with k = 100. The precision and recall
change little when varying k.
witnessed in the two sets, and compute the ratio of
the number of n-grams in the intersection to the
number in their union. Another is to measure the
precision and recall of N (yi) relative to N (xi).
Table 4 presents such precision and recall fig-
ures. For convenience, the n-grams are collected
from the 100-best strings, instead of the hyper-
graph D(yi) and D(xi). Observe that the sim-
ulated confusion set does a reasonably good job
on the real unigram confusions but the simulation
needs improving for higher order n-grams.
6 Conclusions
We proposed a novel procedure to discrimina-
tively train a globally normalized log-linear lan-
guage model for MT, in an efficient and unsu-
pervised manner. Our method relies on the con-
struction of a confusion grammar, an English-to-
English SCFG that captures translation alterna-
tives that an MT system may face when choosing
a translation for a given input. For each English
training sentence, we use this confusion gram-
mar to generate a simulated confusion set, from
which we train a discriminative language model
that will prefer the original English sentence over
sentences in the confusion set. Our experiments
show that the novel CDLM picks better alterna-
tives than a regular n-gram LM from simulated
confusion sets, and improves performance in a
real Chinese-to-English translation task.
7 Acknowledgements
This work was partially supported by the National
Science Foundation via grants No? SGER-0840112
and RI-0963898, and by the DARPA GALE pro-
gram. The authors thank Brian Roark and Dami-
anos Karakos for insightful discussions.
663
References
Bannard, Colin and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In ACL
?05: Proceedings of the 43rd Annual Meeting on
Association for Computational Linguistics, pages
597?604, Morristown, NJ, USA. Association for
Computational Linguistics.
Callison-Burch, Chris, Philipp Koehn, and Miles Os-
borne. 2006. Improved statistical machine transla-
tion using paraphrases. In Proceedings of the main
conference on Human Language Technology Con-
ference of the North American Chapter of the As-
sociation of Computational Linguistics, pages 17?
24, Morristown, NJ, USA. Association for Compu-
tational Linguistics.
Chen, Stanley F. and Joshua Goodman. 1998. An em-
pirical study of smoothing techniques for language
modeling. Technical report.
Chiang, David, Kevin Knight, and Wei Wang. 2009.
11,001 new features for statistical machine transla-
tion. In NAACL, pages 218?226.
Chiang, David. 2007. Hierarchical phrase-based
translation. Computational Linguistics, 33(2):201?
228.
Eck, Matthias and Chiori Hori. 2005. Overview of the
iwslt 2005 evaluation campaign. In In Proc. of the
International Workshop on Spoken Language Trans-
lation.
Khudanpur, Sanjeev and Jun Wu. 2000. Maximum en-
tropy techniques for exploiting syntactic, semantic
and collocational dependencies in language model-
ing. In Computer Speech and Language, number 4,
pages 355?372.
Li, Zhifei and Jason Eisner. 2009. First- and second-
order expectation semirings with applications to
minimum-risk training on translation forests. In
Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing, pages
40?51, Singapore, August. Association for Compu-
tational Linguistics.
Li, Zhifei and Sanjeev Khudanpur. 2008. Large-scale
discriminative n-gram language models for statisti-
cal machine translation. In AMTA, pages 133?142.
Li, Zhifei, Chris Callison-Burch, Chris Dyer, Juri
Ganitkevitch, Sanjeev Khudanpur, Lane Schwartz,
Wren Thornton, Jonathan Weese, and Omar.
Zaidan. 2009. Joshua: An open source toolkit
for parsing-based machine translation. In WMT09,
pages 26?30.
Li, Zhifei, Ziyuan Wang, Jason Eisner, and Sanjeev
Khudanpur. 2010. Minimum imputed risk training
for machine translation. In review.
Li, Zhifei. 2010. Discriminative training and varia-
tional decoding in machine translation via novel al-
gorithms for weighted hypergraphs. PHD Disserta-
tion, Johns Hopkins University.
Madnani, Nitin, Necip Fazil Ayan, Philip Resnik, and
Bonnie J. Dorr. 2007. Using paraphrases for pa-
rameter tuning in statistical machine translation. In
Proceedings of the Workshop on Statistical Machine
Translation, Prague, Czech Republic, June. Associ-
ation for Computational Linguistics.
Och, Franz Josef. 2003. Minimum error rate train-
ing in statistical machine translation. In ACL, pages
160?167.
Poon, Hoifung, Colin Cherry, and Kristina Toutanova.
2009. Unsupervised morphological segmentation
with log-linear models. In NAACL ?09: Proceed-
ings of Human Language Technologies: The 2009
Annual Conference of the North American Chap-
ter of the Association for Computational Linguis-
tics, pages 209?217, Morristown, NJ, USA. Associ-
ation for Computational Linguistics.
Quirk, Chris, Chris Brockett, and William Dolan.
2004. Monolingual machine translation for para-
phrase generation. In In Proceedings of the 2004
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 142?149.
Roark, Brian, Murat Saraclar, Michael Collins, and
Mark Johnson. 2004. Discriminative language
modeling with conditional random fields and the
perceptron algorithm. In Proceedings of the 42nd
Meeting of the Association for Computational Lin-
guistics (ACL?04), Main Volume, pages 47?54,
Barcelona, Spain, July.
Rosenfeld, Roni, Stanley F. Chen, and Xiaojin Zhu.
2001. Whole-sentence exponential language mod-
els: a vehicle for linguistic-statistical integration.
Computers Speech and Language, 15(1).
Rosenfeld, Roni. 1996. A maximum entropy approach
to adaptive statistical language modeling. In Com-
puter Speech and Language, number 3, pages 187?
228.
Smith, Noah A. and Jason Eisner. 2005. Contrastive
estimation: Training log-linear models on unlabeled
data. In Proceedings of the Association for Compu-
tational Linguistics (ACL 2005), Ann Arbor, Michi-
gan.
664
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 920?929,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Minimum Imputed Risk: Unsupervised Discriminative Training for
Machine Translation
Zhifei Li?
Google Research
Mountain View, CA 94043, USA
zhifei.work@gmail.com
Ziyuan Wang, Sanjeev Khudanpur
Johns Hopkins University
Baltimore, MD 21218, USA
zwang40,khudanpur@jhu.edu
Jason Eisner
Johns Hopkins University
Baltimore, MD 21218, USA
eisner@jhu.edu
Brian Roark
Oregon Health & Science University
Beaverton, Oregon 97006, USA
roark@cslu.ogi.edu
Abstract
Discriminative training for machine transla-
tion has been well studied in the recent past.
A limitation of the work to date is that it relies
on the availability of high-quality in-domain
bilingual text for supervised training. We
present an unsupervised discriminative train-
ing framework to incorporate the usually plen-
tiful target-language monolingual data by us-
ing a rough ?reverse? translation system. Intu-
itively, our method strives to ensure that prob-
abilistic ?round-trip? translation from a target-
language sentence to the source-language and
back will have low expected loss. Theoret-
ically, this may be justified as (discrimina-
tively) minimizing an imputed empirical risk.
Empirically, we demonstrate that augment-
ing supervised training with unsupervised data
improves translation performance over the su-
pervised case for both IWSLT and NIST tasks.
1 Introduction
Missing data is a common problem in statistics when
fitting the parameters ? of a model. A common strat-
egy is to attempt to impute, or ?fill in,? the missing
data (Little and Rubin, 1987), as typified by the EM
algorithm. In this paper we develop imputation tech-
niques when ? is to be trained discriminatively.
We focus on machine translation (MT) as our ex-
ample application. A Chinese-to-English machine
translation system is given a Chinese sentence x and
? Zhifei Li is currently working at Google Research, and
this work was done while he was a PHD student at Johns Hop-
kins University.
asked to predict its English translation y. This sys-
tem employs statistical models p?(y | x) whose pa-
rameters ? are discriminatively trained using bilin-
gual sentence pairs (x, y). But bilingual data for
such supervised training may be relatively scarce for
a particular language pair (e.g., Urdu-English), es-
pecially for some topics (e.g., technical manuals) or
genres (e.g., blogs). So systems seek to exploit ad-
ditional monolingual data, i.e., a corpus of English
sentences y with no corresponding source-language
sentences x, to improve estimation of ?. This is our
missing data scenario.1
Discriminative training of the parameters ? of
p?(y | x) using monolingual English data is a cu-
rious idea, since there is no Chinese input x to trans-
late. We propose an unsupervised training approach,
called minimum imputed risk training, which is con-
ceptually straightforward: First guess x (probabilis-
tically) from the observed y using a reverse English-
to-Chinese translation model p?(x | y). Then train
the discriminative Chinese-to-English model p?(y |
x) to do a good job at translating this imputed x
back to y, as measured by a given performance met-
ric. Intuitively, our method strives to ensure that
probabilistic ?round-trip? translation from a target-
language sentence to the source-language and back
again will have low expected loss.
Our approach can be applied in an application
scenario where we have (1) enough out-of-domain
bilingual data to build two baseline translation sys-
tems, with parameters ? for the forward direction,
and ? for the reverse direction; (2) a small amount
1Contrast this with traditional semi-supervised training that
looks to exploit ?unlabeled? inputs x, with missing outputs y.
920
of in-domain bilingual development data to discrim-
inatively tune a small number of parameters in ?;
and (3) a large amount of in-domain English mono-
lingual data.
The novelty here is to exploit (3) to discrimina-
tively tune the parameters ? of all translation model
components,2 p?(y|x) and p?(y), not merely train a
generative language model p?(y), as is the norm.
Following the theoretical development below, the
empirical effectiveness of our approach is demon-
strated by replacing a key supervised discriminative
training step in the development of large MT sys-
tems ? learning the log-linear combination of sev-
eral component model scores (viewed as features) to
optimize a performance metric (e.g. BLEU) on a set
of (x, y) pairs ? with our unsupervised discrimina-
tive training using only y. One may hence contrast
our approach with the traditional supervised meth-
ods applied to the MT task such as minimum error
rate training (Och, 2003; Macherey et al, 2008), the
averaged Perceptron (Liang et al, 2006), maximum
conditional likelihood (Blunsom et al, 2008), min-
imum risk (Smith and Eisner, 2006; Li and Eisner,
2009), and MIRA (Watanabe et al, 2007; Chiang et
al., 2009).
We perform experiments using the open-source
MT toolkit Joshua (Li et al, 2009a), and show that
adding unsupervised data to the traditional super-
vised training setup improves performance.
2 Supervised Discriminative Training via
Minimization of Empirical Risk
Let us first review discriminative training in the su-
pervised setting?as used in MERT (Och, 2003) and
subsequent work.
One wishes to tune the parameters ? of some
complex translation system ??(x). The function ??,
which translates Chinese x to English y = ??(x)
need not be probabilistic. For example, ? may be
the parameters of a scoring function used by ?, along
with pruning and decoding heuristics, for extracting
a high-scoring translation of x.
The goal of discriminative training is to mini-
mize the expected loss of ??(?), under a given task-
specific loss function L(y?, y) that measures how
2Note that the extra monolingual data is used only for tuning
the model weights, but not for inducing new phrases or rules.
bad it would be to output y? when the correct output
is y. For an MT system that is judged by the BLEU
metric (Papineni et al, 2001), for instance, L(y?, y)
may be the negated BLEU score of y? w.r.t. y. To be
precise, the goal3 is to find ? with low Bayes risk,
?? = argmin
?
?
x,y
p(x, y)L(??(x), y) (1)
where p(x, y) is the joint distribution of the input-
output pairs.4
The true p(x, y) is, of course, not known and,
in practice, one typically minimizes empirical risk
by replacing p(x, y) above with the empirical dis-
tribution p?(x, y) given by a supervised training set
{(xi, yi), i = 1, . . . , N}. Therefore,
?? = argmin
?
?
x,y
p?(x, y)L(??(x), y)
= argmin
?
1
N
N?
i=1
L(??(xi), yi). (2)
The search for ?? typically requires the use of nu-
merical methods and some regularization.5
3 Unsupervised Discriminative Training
with Missing Inputs
3.1 Minimization of Imputed Risk
We now turn to the unsupervised case, where we
have training examples {yi} but not their corre-
sponding inputs {xi}. We cannot compute the sum-
mand L(??(xi), yi) for such i in (2), since ??(xi)
requires to know xi. So we propose to replace
3This goal is different from the minimum risk training of
Li and Eisner (2009) in a subtle but important way. In both
cases, ?? minimizes risk or expected loss, but the expectation
is w.r.t. different distributions: the expectation in Li and Eisner
(2009) is under the conditional distribution p(y |x), while the
expectation in (1) is under the joint distribution p(x, y).
4In the terminology of statistical decision theory, p(x, y) is
a distribution over states of nature. We seek a decision rule
??(x) that will incur low expected loss on observations x that
are generated from unseen states of nature.
5To compensate for the shortcut of using the unsmoothed
empirical distribution rather than a posterior estimate of p(x, y)
(Minka, 2000), it is common to add a regularization term ||?||22
in the objective of (2). The regularization term can prevent over-
fitting to a training set that is not large enough to learn all pa-
rameters.
921
L(??(xi), yi) with the expectation
?
x
p?(x | yi)L(??(x), yi), (3)
where p?(? | ?) is a ?reverse prediction model? that
attempts to impute the missing xi data. We call the
resulting variant of (2) the minimization of imputed
empirical risk, and say that
?? = argmin
?
1
N
N?
i=1
?
x
p?(x | yi)L(??(x), yi) (4)
is the estimate with the minimum imputed risk6.
The minimum imputed risk objective of (4) could
be evaluated by brute force as follows.
1. For each unsupervised example yi, use the re-
verse prediction model p?(? | yi) to impute pos-
sible reverse translations Xi = {xi1, xi2, . . .},
and add each (xij , yi) pair, weighted by
p?(xij | yi) ? 1, to an imputed training set .
2. Perform the supervised training of (2) on the
imputed and weighted training data.
The second step means that we must use ?? to
forward-translate each imputed xij , evaluate the loss
of the translations y?ij against the corresponding true
translation yi, and choose the ? that minimizes the
weighted sum of these losses (i.e., the empirical risk
when the empirical distribution p?(x, y) is derived
from the imputed training set). Specific to our MT
task, this tries to ensure that probabilistic ?round-
trip? translation, from the target-language sentence
yi to the source-language and back again, will have
a low expected loss.7
The trouble with this method is that the reverse
model p? generates a weighted lattice or hyper-
graph Xi encoding exponentially many translations
of yi, and it is computationally infeasible to forward-
translate each xij ? Xi. We therefore investigate
several approximations to (4) in Section 3.4.
6One may exploit both supervised data {(xi, yi)} and unsu-
pervised data {yj} to perform semi-supervised training via an
interpolation of (2) and (4). We will do so in our experiments.
7Our approach may be applied to other tasks as well. For
example, in a speech recognition task, ?? is a speech recognizer
that produces text, whereas p? is a speech synthesizer that must
produce a distribution over audio (or at least over acoustic fea-
tures or phone sequences) (Huang et al, 2010).
3.2 The Reverse Prediction Model p?
A crucial ingredient in (4) is the reverse prediction
model p?(?|?) that attempts to impute the missing xi.
We will train this model in advance, doing the best
job we can from available data, including any out-
of-domain bilingual data as well as any in-domain
monolingual data8 x.
In the MT setting, ?? and p? may have similar pa-
rameterization. One translates Chinese to English;
the other translates English to Chinese.
Yet the setup is not quite symmetric. Whereas ??
is a translation system that aims to produce a single,
low-loss translation, the reverse version p? is rather
a probabilistic model. It is supposed to give an accu-
rate probability distribution over possible values xij
of the missing input sentence xi. All of these val-
ues are taken into account in (4), regardless of the
loss that they would incur if they were evaluated for
translation quality relative to the missing xi.
Thus, ? does not need to be trained to minimize
the risk itself (so there is no circularity). Ideally,
it should be trained to match the underlying condi-
tional distribution of x given y, by achieving a low
conditional cross-entropy
H(X |Y ) = ?
?
x,y
p(x, y) log p?(x | y). (5)
In practice, ? is trained by (empirically) minimiz-
ing ? 1M
?N
j=1 log p?(xj | yj) + 12?2 ???22 on some
bilingual data, with the regularization coefficient ?2
tuned on held out data.
It may be tolerable for p? to impute mediocre
translations xij . All that is necessary is that the (for-
ward) translations generated from the imputed xij
?simulate? the competing hypotheses that we would
see when translating the correct Chinese input xi.
3.3 The Forward Translation System ?? and
The Loss Function L(??(xi), yi)
The minimum empirical risk objective of (2) is
quite general and various popular supervised train-
ing methods (Lafferty et al, 2001; Collins, 2002;
Och, 2003; Crammer et al, 2006; Smith and Eisner,
8In a translation task from x to y, one usually does not make
use of in-domain monolingual data x. But we can exploit x to
train a language model p?(x) for the reverse translation system,
which will make the imputed xij look like true Chinese inputs.
922
2006) can be formalized in this framework by choos-
ing different functions for ?? and L(??(xi), yi). The
generality of (2) extends to our minimum imputed
risk objective of (4). Below, we specify the ?? and
L(??(xi), yi) we considered in our investigation.
3.3.1 Deterministic Decoding
A simple translation rule would define
??(x) = argmax
y
p?(y |x) (6)
If this ??(x) is used together with a loss function
L(??(xi), yi) that is the negated BLEU score9, our
minimum imputed risk objective of (4) is equivalent
to MERT (Och, 2003) on the imputed training data.
However, this would not yield a differentiable ob-
jective function. Infinitesimal changes to ? could re-
sult in discrete changes to the winning output string
??(x) in (6), and hence to the loss L(??(x), yi). Och
(2003) developed a specialized line search to per-
form the optimization, which is not scalable when
the number of model parameters ? is large.
3.3.2 Randomized Decoding
Instead of using the argmax of (6), we assume
during training that ??(x) is itself random, i.e. the
MT system randomly outputs a translation y with
probability p?(y |x). As a result, we will modify
our objective function of (4) to take yet another ex-
pectation over the unknown y. Specifically, we will
replace L(??(x), yi) in (4) with
?
y
p?(y |x)L(y, yi). (7)
Now, the minimum imputed empirical risk objective
of (4) becomes
?? = argmin
?
1
N
N?
i=1
?
x,y
p?(x | yi) p?(y |x)L(y, yi)
(8)
If the loss function L(y, yi) is a negated BLEU, this
is equivalent to performing minimum-risk training
described by (Smith and Eisner, 2006; Li and Eisner,
2009) on the imputed data.10
9One can manipulate the loss function to support other
methods that use deterministic decoding, such as Perceptron
(Collins, 2002) and MIRA (Crammer et al, 2006).
10Again, one may manipulate the loss function to support
other probabilistic methods that use randomized decoding, such
as CRFs (Lafferty et al, 2001).
The objective function in (8) is now differentiable,
since each coefficient p?(y |x) is a differentiable
function of ?, and thus amenable to optimization
by gradient-based methods; we use the L-BFGS al-
gorithm (Liu et al, 1989) in our experiments. We
perform experiments with the syntax-based MT sys-
tem Joshua (Li et al, 2009a), which implements
dynamic programming algorithms for second-order
expectation semirings (Li and Eisner, 2009) to effi-
ciently compute the gradients needed for optimizing
(8).
3.4 Approximating p?(x | yi)
As mentioned at the end of Section 3.1, it is com-
putationally infeasible to forward-translate each of
the imputed reverse translations xij . We propose
four approximations that are computationally feasi-
ble. Each may be regarded as a different approxima-
tion of p?(x | yi) in equations (4) or (8).
k-best. For each yi, add to the imputed training set
only the k most probable translations {xi1, . . . xik}
according to p?(x | yi). (These can be extracted
from Xi using standard algorithms (Huang and Chi-
ang, 2005).) Rescale their probabilities to sum to 1.
Sampling. For each yi, add to the training set k in-
dependent samples {xi1, . . . xik} from the distribu-
tion p?(x | yi), each with weight 1/k. (These can be
sampled from Xi using standard algorithms (John-
son et al, 2007).) This method is known in the liter-
ature as multiple imputation (Rubin, 1987).
Lattice. 11 Under certain special cases it is be pos-
sible to compute the expected loss in (3) exactly
via dynamic programming. Although Xi does con-
tain exponentially many translations, it may use a
?packed? representation in which these translations
share structure. This representation may further-
more enable sharing work in forward-translation, so
as to efficiently translate the entire set Xi and ob-
tain a distribution over translations y. Finally, the
expected loss under that distribution, as required by
equation (3), may also be efficiently computable.
All this turns out to be possible if (a) the poste-
rior distribution p?(x | yi) is represented by an un-
11The lattice approximation is presented here as a theoreti-
cal contribution, and we do not empirically evaluate it since its
implementation requires extensive engineering effort that is be-
yond the main scope of this paper.
923
ambiguous weighted finite-state automaton Xi, (b)
the forward translation system ?? is structured in a
certain way as a weighted synchronous context-free
grammar, and (c) the loss function decomposes in a
certain way. We omit the details of the construction
as beyond the scope of this paper.
In our experimental setting described below, (b) is
true (using Joshua), and (c) is true (since we use a
loss function presented by Tromble et al (2008) that
is an approximation to BLEU and is decomposable).
While (a) is not true in our setting because Xi is a
hypergraph (which is ambiguous), Li et al (2009b)
show how to approximate a hypergraph representa-
tion of p?(x | yi) by an unambiguous WFSA. One
could then apply the construction to this WFSA12,
obtaining an approximation to (3).
Rule-level Composition. Intuitively, the reason
why the structure-sharing in the hypergraphXi (gen-
erated by the reverse system) cannot be exploited
during forward translating is that when the forward
Hiero system translates a string xi ? Xi, it must
parse it into recursive phrases.
But the structure-sharing within the hypergraph of
Xi has already parsed xi into recursive phrases, in a
way determined by the reverse Hiero system; each
translation phrase (or rule) corresponding to a hy-
peredge. To exploit structure-sharing, we can use
a forward translation system that decomposes ac-
cording to that existing parse of xi. We can do that
by considering only forward translations that respect
the hypergraph structure of Xi. The simplest way to
do this is to require complete isomorphism of the
SCFG trees used for the reverse and forward trans-
lations. In other words, this does round-trip impu-
tation (i.e., from y to x, and then to y?) at the rule
level. This is essentially the approach taken by Li et
al. (2010).
3.5 The Log-Linear Model p?
We have not yet specified the form of p?. Following
much work in MT, we begin with a linear model
score(x, y) = ? ? f(x, y) =
?
k
?kfk(x, y) (9)
where f(x, y) is a feature vector indexed by k. Our
deterministic test-time translation system ?? simply
12Note that the forward translation of a WFSA is tractable by
using a lattice-based decoder such as that by Dyer et al (2008).
outputs the highest-scoring y for fixed x. At training
time, our randomized decoder (Section 3.3.2) uses
the Boltzmann distribution (here a log-linear model)
p?(y |x) =
e??score(x,y)
Z(x) =
e??score(x,y)?
y? e??score(x,y
?) (10)
The scaling factor ? controls the sharpness of the
training-time distribution, i.e., the degree to which
the randomized decoder favors the highest-scoring
y. For large ?, our training objective approaches
the imputed risk of the deterministic test-time sys-
tem while remaining differentiable.
In a task like MT, in addition to the input x and
output y, we often need to introduce a latent variable
d to represent the hidden derivation that relates x to
y. A derivation d represents a particular phrase seg-
mentation in a phrase-based MT system (Koehn et
al., 2003) and a derivation tree in a typical syntax-
based system (Galley et al, 2006; Chiang, 2007).
We change our model to assign scores not to an
(x, y) pair but to the detailed derivation d; in partic-
ular, now the function f that extracts a feature vector
can look at all of d. We replace y by d in (9)?(10),
and finally define p?(y|x) by marginalizing out d,
p?(y |x) =
?
d?D(x,y)
p?(d |x) (11)
where D(x, y) represents the set of derivations that
yield x and y.
4 Minimum Imputed Risk vs. EM
The notion of imputing missing data is familiar
from other settings (Little and Rubin, 1987), particu-
larly the expectation maximization (EM) algorithm,
a widely used generative approach. So it is instruc-
tive to compare EM with minimum imputed risk.
One can estimate ? by maximizing the log-
likelihood of the data {(xi, yi), i = 1, . . . , N} as
argmax
?
1
N
N?
i=1
log p?(xi, yi). (12)
If the xi?s are missing, EM tries to iteratively maxi-
mize the marginal probability:
argmax
?
1
N
N?
i=1
log
?
x
p?(x, yi). (13)
924
The E-step of each iteration comprises comput-
ing ?x p?t(x | yi) log p?(x, yi), the expected log-
likelihood of the complete data, where p?t(x | yi) is
the conditional part of p?t(x, yi) under the current
iterate ?t, and the M-step comprises maximizing it:
?t+1 = argmax
?
1
N
N?
i=1
?
x
p?t(x | yi) log p?(x, yi).
(14)
Notice that if we replace p?t(x|yi) with p?(x | yi)
in the equation above, and admit negated log-
likelihood as a loss function, then the EM update
(14) becomes identical to (4). In other words, the
minimum imputed risk approach of Section 3.1 dif-
fers from EM in (i) using an externally-provided and
static p?, instead of refining it at each iteration based
on the current p?t , and (ii) using a specific loss func-
tion, namely negated log-likelihood.
So why not simply use the maximum-likelihood
(EM) training procedure for MT? One reason is
that it is not discriminative: the loss function (e.g.
negated BLEU) is ignored during training.
A second reason is that training good joint models
p?(x, y) is computationally expensive. Contempo-
rary MT makes heavy use of log-linear probability
models, which allow the system designer to inject
phrase tables, linguistic intuitions, or prior knowl-
edge through a careful choice of features. Comput-
ing the objective function of (14) in closed form is
difficult if p? is an arbitrary log-linear model, be-
cause the joint probability p?(xi, yi) is then defined
as a ratio whose denominatorZ? involves a sum over
all possible sentence pairs (x, y) of any length.
By contrast, our discriminative framework will
only require us to work with conditional models.
While conditional probabilities such as p?(x | y) and
p?(y |x) are also ratios, computing their denomina-
tors only requires us to sum over a packed forest of
possible translations of a given y or x.13
In summary, EM would impute missing data us-
ing p?(x | y) and predict outputs using p?(y |x),
both being conditional forms of the same joint
model p?(x, y). Our minimum imputed risk train-
ing method is similar, but it instead uses a pair of
13Analogously, discriminative CRFs have become more pop-
ular than generative HMMs because they permit efficient train-
ing even with a wide variety of log-linear features (Lafferty et
al., 2001).
separately parameterized, separately trained mod-
els p?(x | y) and p?(y |x). By sticking to condi-
tional models, we can efficiently use more sophis-
ticated model features, and we can incorporate the
loss function when we train ?, which should improve
both efficiency and accuracy at test time.
5 Experimental Results
We report results on Chinese-to-English translation
tasks using Joshua (Li et al, 2009a), an open-source
implementation of Hiero (Chiang, 2007).
5.1 Baseline Systems
5.1.1 IWSLT Task
We train both reverse and forward baseline sys-
tems. The translation models are built using the cor-
pus for the IWSLT 2005 Chinese to English trans-
lation task (Eck and Hori, 2005), which comprises
40,000 pairs of transcribed utterances in the travel
domain. We use a 5-gram language model with
modified Kneser-Ney smoothing (Chen and Good-
man, 1998), trained on the English (resp. Chi-
nese) side of the bitext. We use a standard train-
ing pipeline and pruning settings recommended by
(Chiang, 2007).
5.1.2 NIST Task
For the NIST task, the TM is trained on about 1M
parallel sentence pairs (about 28M words in each
language), which are sub-sampled from corpora dis-
tributed by LDC for the NIST MT evaluation using a
sampling method implemented in Joshua. We also
used a 5-gram language model, trained on a data set
consisting of a 130M words in English Gigaword
(LDC2007T07) and the bitext?s English side.
5.2 Feature Functions
We use two classes of features fk for discriminative
training of p? as defined in (9).
5.2.1 Regular Hiero Features
We include ten features that are standard in Hi-
ero (Chiang, 2007). In particular, these include
one baseline language model feature, three baseline
translation models, one word penalty feature, three
features to count how many rules with an arity of
925
zero/one/two are used in a derivation, and two fea-
tures to count how many times the unary and binary
glue rules in Hiero are used in a derivation.
5.2.2 Target-rule Bigram Features
In this paper, we do not attempt to discrimina-
tively tune a separate parameter for each bilingual
rule in the Hiero grammar. Instead, we train several
hundred features that generalize across these rules.
For each bilingual rule, we extract bigram fea-
tures over the target-side symbols (including non-
terminals and terminals). For example, if a bilingual
rule?s target-side is ?on the X1 issue of X2? where
X1 and X2 are non-terminals (with a position in-
dex), we extract the bigram features on the, the X ,
X issue, issue of, and of X . (Note that the posi-
tion index of a non-terminal is ignored in the fea-
ture.) Moreover, for the terminal symbols, we will
use their dominant POS tags (instead of the sym-
bol itself). For example, the feature the X becomes
DTX . We use 541 such bigram features for IWSLT
task (and 1023 such features for NIST task) that fire
frequently.
5.3 Data Sets for Discriminative Training
5.3.1 IWSLT Task
In addition to the 40,000 sentence pairs used to
train the baseline generative models (which are used
to compute the features fk), we use three bilingual
data sets listed in Table 1, also from IWSLT, for dis-
criminative training: one to train the reverse model
p? (which uses only the 10 standard Hiero features
as described in Section 5.2.1),14 one to train the for-
ward model ?? (which uses both classes of features
described in Section 5.2, i.e., 551 features in total),
and one for test.
Note that the reverse model ? is always trained us-
ing the supervised data of Dev ?, while the forward
model ? may be trained in a supervised or semi-
supervised manner, as we will show below.
In all three data sets, each Chinese sentence xi
has 16 English reference translations, so each yi is
actually a set of 16 translations. When we impute
data from yi (in the semi-supervised scenario), we
14Ideally, we should train ? to minimize the conditional
cross-entropy (5) as suggested in section 3.2. In the present
results, we trained ? discriminatively to minimize risk, purely
for ease of implementation using well versed steps.
Data set Purpose # of sentencesChinese English
Dev ? training ? 503 503?16
Dev ? training ? 503? 503?16
Eval ? testing 506 506?16
Table 1: IWSLT Data sets used for discriminative
training/test. Dev ? is used for discriminatively training
of the reverse model ?, Dev ? is for the forward model,
and Eval ? is for testing. The star ? for Dev ? empha-
sizes that some of its Chinese side will not be used in the
training (see Table 2 for details).
actually impute 16 different values of xi, by using
p? to separately reverse translate each sentence in
yi. This effectively adds 16 pairs of the form (xi, yi)
to the training set (see section 3.4), where each xi
is a different input sentence (imputed) in each case,
but yi is always the original set of 16 references.
5.3.2 NIST Task
For the NIST task, we use MT03 set (having 919
sentences) to tune the component parameters in both
the forward and reverse baseline systems. Addition-
ally, we use the English side of MT04 (having 1788
sentences) to perform semi-supervised tuning of the
forward model. The test sets are MT05 and MT06
(having 1082 and 1099 sentences, respectively). In
all the data sets, each source sentence has four refer-
ence translations.
5.4 Main Results
We compare two training scenarios: supervised and
semi-supervised. The supervised system (?Sup?)
carries out discriminative training on a bilingual data
set. The semi-supervised system (?+Unsup?) addi-
tionally uses some monolingual English text for dis-
criminative training (where we impute one Chinese
translation per English sentence).
Tables 2 and 3 report the results for the two tasks
under two training scenarios. Clearly, adding unsu-
pervised data improves over the supervised case, by
at least 1.3 BLEU points in IWSLT and 0.5 BLEU in
NIST.
5.5 Results for Analysis Purposes
Below, we will present more results on the IWSLT
data set to help us understand the behavior of the
926
Training scenario Test BLEU
Sup, (200, 200?16) 47.6
+Unsup, 101?16 Eng sentences 49.0
+Unsup, 202?16 Eng sentences 48.9
+Unsup, 303?16 Eng sentences 49.7?
Table 2: BLEU scores for semi-supervised training for
IWSLT task. The supervised system (?Sup?) is trained
on a subset of Dev ? containing 200 Chinese sentences
and 200?16 English translations. ?+Unsup? means that
we include additional (monolingual) English sentences
from Dev ? for semi-supervised training; for each En-
glish sentence, we impute the 1-best Chinese translation.
A star ? indicates a result that is signicantly better than
the ?Sup? baseline (paired permutation test, p < 0.05).
Training scenario Test BLEUMT05 MT06
Sup, (919, 919?4) 32.4 30.6
+Unsup, 1788 Eng sentences 33.0? 31.1?
Table 3: BLEU scores for semi-supervised training for
NIST task. The ?Sup? system is trained on MT03, while
the ?+Unsup? system is trained with additional 1788 En-
glish sentences from MT04. (Note that while MT04 has
1788?4 English sentences as it has four sets of refer-
ences, we only use one such set, for computational ef-
ficiency of discriminative training.) A star ? indicates a
result that is signicantly better than the ?Sup? baseline
(paired permutation test, p < 0.05).
methods proposed in this paper.
5.5.1 Imputation with Different Reverse
Models
A critical component of our unsupervised method
is the reverse translation model p?(x | y). We
wonder how the performance of our unsupervised
method changes when the quality of the reverse sys-
tem varies. To study this question, we used two dif-
ferent reverse translation systems, one with a lan-
guage model trained on the Chinese side of the bi-
text (?WLM?), and the other one without using such
a Chinese LM (?NLM?). Table 4 (in the fully unsu-
pervised case) shows that the imputed Chinese trans-
lations have a far lower BLEU score without the lan-
guage model,15 and that this costs us about 1 English
15The BLEU scores are low even with the language model
because only one Chinese reference is available for scoring.
Data size Imputed-CN BLEU Test-EN BLEUWLM NLM WLM NLM
101 11.8 3.0 48.5 46.7
202 11.7 3.2 48.9 47.6
303 13.4 3.5 48.8 47.9
Table 4: BLEU scores for unsupervised training
with/without using a language model in the reverse
system. A data size of 101 means that we use only
the English sentences from a subset of Dev ? containing
101 Chinese sentences and 101?16 English translations;
for each English sentence we impute the 1-best Chinese
translation. ?WLM? means a Chinese language model
is used in the reverse system, while ?NLM? means no
Chinese language model is used. In addition to reporting
the BLEU score on Eval ?, we also report ?Imputed-CN
BLEU?, the BLEU score of the imputed Chinese sentences
against their corresponding Chinese reference sentences.
BLEU point in the forward translations. Still, even
with the worse imputation (in the case of ?NLM?),
our forward translations improve as we add more
monolingual data.
5.5.2 Imputation with Different k-best Sizes
In all the experiments so far, we used the reverse
translation system to impute only a single Chinese
translation for each English monolingual sentence.
This is the 1-best approximation of section 3.4.
Table 5 shows (in the fully unsupervised case)
that the performance does not change much as k in-
creases.16 This may be because that the 5-best sen-
tences are likely to be quite similar to one another
(May and Knight, 2006). Imputing a longer k-best
list, a sample, or a lattice for xi (see section 3.4)
might achieve more diversity in the training inputs,
which might make the system more robust.
6 Conclusions
In this paper, we present an unsupervised discrimi-
native training method that works with missing in-
puts. The key idea in our method is to use a re-
verse model to impute the missing input from the ob-
served output. The training will then forward trans-
late the imputed input, and choose the parameters of
the forward model such that the imputed risk (i.e.,
16In the present experiments, however, we simply weighted
all k imputed translations equally, rather than in proportion to
their posterior probabilities as suggested in Section 3.4.
927
Training scenario Test BLEU
Unsup, k=1 48.5
Unsup, k=2 48.4
Unsup, k=3 48.9
Unsup, k=4 48.5
Unsup, k=5 48.4
Table 5: BLEU scores for unsupervised training with
different k-best sizes. We use 101?16 monolingual En-
glish sentences, and for each English sentence we impute
the k-best Chinese translations using the reverse system.
the expected loss of the forward translations with
respect to the observed output) is minimized. This
matches the intuition that the probabilistic ?round-
trip? translation from the target-language sentence
to the source-language and back should have low ex-
pected loss.
We applied our method to two Chinese to English
machine translation tasks (i.e. IWSLT and NIST).
We showed that augmenting supervised data with
unsupervised data improved performance over the
supervised case (for both tasks).
Our discriminative model used only a small
amount of training data and relatively few features.
In future work, we plan to test our method in settings
where there are large amounts of monolingual train-
ing data (enabling many discriminative features).
Also, our experiments here were performed on a lan-
guage pair (i.e., Chinese to English) that has quite
rich bilingual resources in the domain of the test
data. In future work, we plan to consider low-
resource test domains and language pairs like Urdu-
English, where bilingual data for novel domains is
sparse.
Acknowledgements
This work was partially supported by NSF Grants
No IIS-0963898 and No IIS-0964102 and the
DARPA GALE Program. The authors thank Markus
Dreyer, Damianos Karakos and Jason Smith for in-
sightful discussions.
References
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008.
A discriminative latent variable model for statistical
machine translation. In ACL, pages 200?208.
Stanley F. Chen and Joshua Goodman. 1998. An empir-
ical study of smoothing techniques for language mod-
eling. Technical report.
David Chiang, Kevin Knight, and Wei Wang. 2009.
11,001 new features for statistical machine translation.
In NAACL, pages 218?226.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
Michael Collins. 2002. Discriminative training methods
for hidden markov models: theory and experiments
with perceptron algorithms. In EMNLP, pages 1?8.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yoram Singer. 2006. Online passive-
aggressive algorithms. J. Mach. Learn. Res., 7:551?
585.
Christopher Dyer, Smaranda Muresan, and Philip Resnik.
2008. Generalizing word lattice translation. In ACL,
pages 1012?1020.
Matthias Eck and Chiori Hori. 2005. Overview of the
iwslt 2005 evaluation campaign. In In IWSLT.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In ACL,
pages 961?968.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In IWPT, pages 53?64.
Jui-Ting Huang, Xiao Li, and Alex Acero. 2010. Dis-
criminative training methods for language models us-
ing conditional entropy criteria. In ICASSP.
Mark Johnson, Thomas Griffiths, and Sharon Goldwa-
ter. 2007. Bayesian inference for PCFGs via Markov
chain Monte Carlo. In NAACL, pages 139?146.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In NAACL,
pages 48?54.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic models
for segmenting and labeling sequence data. In ICML.
Zhifei Li and Jason Eisner. 2009. First- and second-order
expectation semirings with applications to minimum-
risk training on translation forests. In EMNLP, pages
40?51.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-
itkevitch, Sanjeev Khudanpur, Lane Schwartz, Wren
Thornton, Jonathan Weese, and Omar. Zaidan. 2009a.
Joshua: An open source toolkit for parsing-based ma-
chine translation. In WMT09, pages 26?30.
Zhifei Li, Jason Eisner, and Sanjeev Khudanpur. 2009b.
Variational decoding for statistical machine transla-
tion. In ACL, pages 593?601.
Zhifei Li, Ziyuan Wang, Sanjeev Khudanpur, and Jason
Eisner. 2010. Unsupervised discriminative language
928
model training for machine translation using simulated
confusion sets. In COLING, pages 556?664.
Percy Liang, Alexandre Bouchard-Co?te?, Dan Klein, and
Ben Taskar. 2006. An end-to-end discriminative ap-
proach to machine translation. In ACL, pages 761?
768.
R. J. A. Little and D. B. Rubin. 1987. Statistical Analysis
with Missing Data. J. Wiley & Sons, New York.
Dong C. Liu, Jorge Nocedal, and Dong C. 1989. On the
limited memory bfgs method for large scale optimiza-
tion. Mathematical Programming, 45:503?528.
Wolfgang Macherey, Franz Och, Ignacio Thayer, and
Jakob Uszkoreit. 2008. Lattice-based minimum er-
ror rate training for statistical machine translation. In
EMNLP, pages 725?734.
Jonathan May and Kevin Knight. 2006. A better n-best
list: practical determinization of weighted finite tree
automata. In NAACL, pages 351?358.
Thomas Minka. 2000. Empirical risk minimization is
an incomplete inductive principle. In MIT Media Lab
note.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In ACL, pages 160?
167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. BLEU: A method for automatic eval-
uation of machine translation. In ACL, pages 311?318.
D. B. Rubin. 1987. Multiple Imputation for Nonresponse
in Surveys. J. Wiley & Sons, New York.
David A. Smith and Jason Eisner. 2006. Minimum
risk annealing for training log-linear models. In ACL,
pages 787?794.
Roy Tromble, Shankar Kumar, Franz Och, and Wolfgang
Macherey. 2008. Lattice minimum-Bayes-risk de-
coding for statistical machine translation. In EMNLP,
pages 620?629.
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and Hideki
Isozaki. 2007. Online large-margin training for statis-
tical machine translation. In EMNLP-CoNLL, pages
764?773.
929
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1128?1136,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Efficient Subsampling for Training Complex Language Models
Puyang Xu
puyangxu@jhu.edu
Asela Gunawardana#
aselag@microsoft.com
Sanjeev Khudanpur
khudanpur@jhu.edu
Department of Electrical and Computer Engineering
Center for Language and Speech Processing
Johns Hopkins University
Baltimore, MD 21218, USA
#Microsoft Research
Redmond, WA 98052, USA
Abstract
We propose an efficient way to train maximum
entropy language models (MELM) and neural
network language models (NNLM). The ad-
vantage of the proposed method comes from
a more robust and efficient subsampling tech-
nique. The original multi-class language mod-
eling problem is transformed into a set of bi-
nary problems where each binary classifier
predicts whether or not a particular word will
occur. We show that the binarized model is
as powerful as the standard model and allows
us to aggressively subsample negative training
examples without sacrificing predictive per-
formance. Empirical results show that we can
train MELM and NNLM at 1% ? 5% of the
standard complexity with no loss in perfor-
mance.
1 Introduction
Language models (LM) assign probabilities to se-
quences of words. They are widely used in many
natural language processing applications. The prob-
ability of a sequence can be modeled as a product of
local probabilities, as shown in (1), where wi is the
ith word, and hi is the word history preceding wi.
P (w1, w2, ..., wl) =
l?
i=1
P (wi|hi) (1)
Therefore the task of language modeling reduces
to estimating a set of conditional distributions
{P (w|h)}. The n-gram LM is a dominant way to
parametrizeP (w|h), where it is assumed thatw only
depends on the previous n?1 words. More complex
models have also been proposed?MELM (Rosen-
feld, 1996) and NNLM (Bengio et al, 2003) are two
examples.
Modeling P (w|h) can be seen as a multi-class
classification problem. Given the history, we have
to choose a word in the vocabulary, which can eas-
ily be a few hundred thousand words in size. For
complex models such as MELM and NNLM, this
poses a computational challenge for learning, be-
cause the resulting objective functions are expensive
to normalize. In contrast, n-gram LMs do not suf-
fer from this computational challenge. In the web
era, language modelers have access to virtually un-
limited amounts of data, while the computing power
available to process this data is limited. Therefore,
despite the demonstrated effectiveness of complex
LMs, the n-gram is still the predominant approach
for most real world applications.
Subsampling is a simple solution to get around
the constraint of computing resources. For the pur-
pose of language modeling, it amounts to taking
only part of the text corpus to train the LM. For
complex models such as NNLM, it has been shown
that subsampling can speed up training greatly, at
the cost of some degradation in predictive perfor-
mance (Schwenk, 2007), allowing for trade-off be-
tween computational cost and LM quality.
Our contribution is a novel way to train com-
plex LMs such as MELM and NNLM which allows
much more aggressive subsampling without incur-
ring as high a cost in predictive performance. The
key to our approach is reducing the multi-class LM
problem into a set of binary problems. Instead of
training a V -class classifier, where V is the size of
1128
the vocabulary, we train V binary classifiers, each
one of which performs a one-against-all classifica-
tion. The V trained binary probabilities are then re-
normalized to obtain a valid distribution over the V
words. Subsampling here can be done in the nega-
tive examples. Since the majority of training exam-
ples are negative for each of the binary classifiers,
we can achieve substantial computational saving by
only keeping subsets of them. We will show that the
binarized LM is as powerful as its multi-class coun-
terpart, while being able to sustain much more ag-
gressive subsampling. For certain types of LMs such
as MELM, there are more benefits?the binarization
leads to a set of completely independent classifiers
to train, which allows easy parallelization and sig-
nificantly lowers the memory requirement.
Similar one-against-all approaches are often used
in the machine learning community, especially by
SVM (support vector machine) practitioners to solve
multi-class problems (Rifkin and Klautau, 2004;
Allwein et al, 2000). The goal of this paper is to
show that a similar technique can also be used for
language modeling and that it enables us to sub-
sample data much more efficiently. We show that
the proposed approach is useful when the dominant
modeling constraint is computing power as opposed
to training data.
The rest of the paper is organized as follows. In
section 2, we describe our binarization and subsam-
pling techniques for language models with MELM
and NNLM as two specific examples. Experimental
results are presented in Section 3, followed by dis-
cussion in Section 4.
2 Approximating Language Models with
Binary Classifiers
Suppose we have an LM that can be written in the
form
P (w|h) = exp aw(h; ?)?
w? exp aw?(h; ?)
, (2)
where aw(h; ?) is a parametrized history representa-
tion for word w.
Given a training corpus of word history pairs with
empirical distribution P? (h,w), the regularized log
likelihood of the training set can be written as
L =
?
h
P? (h)
?
w
P? (w|h) logP (w|h)? r(?), (3)
where r(?) is the regularizing function over the pa-
rameters.
Assuming that r(?) can be written as a sum over
per-word regularizers, namely r(?) = ?w rw(?),
we can take the gradient of the log likelihood w.r.t ?
to show that the regularized MLE for the LM satis-
fies
?
h
P? (h)
?
w
P (w|h)??aw(h; ?)
=
?
h,w
P? (w, h)??aw(h; ?)?
?
w
??rw(?). (4)
For each word w, we can define a binary classifier
that predicts whether the next word is w by
Pb(w|h) =
exp aw(h; ?)
1 + exp aw(h; ?)
. (5)
The regularized training set log likelihood for all
the binary classifiers is given by
Lb =
?
w
?
h
P? (h)
[
P? (w|h) logPb(w|h)
+P? (w?|h) logPb(w?|h)
]
?
?
w
rw(?), (6)
where Pb(w?|h) = 1? Pb(w|h) is the probability of
w not occurring. Here we assume the same structure
of the regularizer r(?).
The regularized MLE for the binary classifiers
satisfies
?
h
P? (h)
?
w
Pb(w|h)??aw(h; ?)
=
?
h,w
P? (w, h)??aw(h; ?)?
?
w
??rw(?). (7)
Notice the right hand sides of (4) and (7) are the
same. Thus, taking P ?(w|h) = Pb(w|h) from ML
trained binary classifiers gives an LM that meets the
MLE constraints for language models. Therefore,
if ?w Pb(w|h) = 1, ML training for the language
model is equivalent to ML training of the binary
classifiers and using the probabilities given by the
classifiers as our LM probabilities.
Note that in practice, the probabilities given by
the binary classifiers are not guaranteed to sum up
to one. For tasks such as measuring perplexity,
1129
these probabilities have to be normalized explicitly.
Our hope is that for large enough data sets and rich
enough history representation aw(h; ?), we will get?
w Pb(w|h) ? 1 so that renormalizing the classi-
fiers to get
P ?(w|h) = Pb(w|h)?
w??V Pb(w?|h)
(8)
will not change the MLE constraint too much.
2.1 Stratified Sampling
We note that iterative estimation of the LM shown
in (2) in general requires enumerating over the T
training cases in the training set and computing the
denominator of (2) for each case at a cost of O(V ).
Thus, each iteration of training takesO(V T ) in gen-
eral. The complexity of estimating each of the V
binary classifiers is O(T ) per iteration, also giving
O(V T ) per iteration in total.
However, as mentioned earlier, we are able to
maximally subsample negative examples for each
classifier. Thus the classifier for w is trained us-
ing the C(w) positive examples and a proportion
? of the T ? C(w) negative examples. The total
number of training examples for all V classifiers is
then (1 ? ?)T + ?V T . For large V , we choose
? >> 11+V so that this is approximately ?V T .Thus, our complexity for estimating all V classifiers
is O(?V T ).
The resulting training set for each binary classi-
fier is a stratified sample (Neyman, 1934), and our
estimate needs to be calibrated to account for this.
Since the training set subsamples negative examples
by ?, the resulting classifier will have a likelihood
ratio
Pb(w|h)
1? Pb(w|h)
= exp aw(h; ?) (9)
that is overestimated by a factor of 1? . This can becorrected by simply adding log? to the bias (uni-
gram) weight of the classifier.
2.2 Maximum Entropy LM
MELM is an effective alternative to the standard n-
gram LM. It provides a flexible framework to incor-
porate different knowledge sources in the form of
feature constraints. Specifically, MELM takes the
form of (2), for wordw following history h, we have
the following probability definition,
P (w|h) = exp
?
i ?ifi(h,w)?
w??V exp
?
i ?ifi(h,w?)
. (10)
fi is the ith feature function defined over the
word-history pair, ?i is the feature weight associated
with fi. By defining general features, we have a nat-
ural framework to go beyond n-grams and capture
more complex dependencies that exist in language.
Previous research has shown the benefit of including
various kinds of syntactic and semantic information
into the LM (Khudanpur and Wu, 2000). However,
despite providing a promising avenue for language
modeling, MELM are computationally expensive to
estimate. The bottleneck lies in the denominator
of (10).
To estimate ?is, gradient based methods can be
used. The derivative of the likelihood function L
w.r.t ?i has a simple form, namely
?L
??i
=
?
k
fi(wk, hk)?
?
k
?
w??V
P (w?|h)fi(w?, hk),
(11)
where k is the index of word-history pair in the train-
ing corpus. The first term in the derivative is the ob-
served feature count in the training corpus, the sec-
ond term is the expected feature count according to
the model. In order to obtain P (w?|h) in the second
term, we need to compute the normalizer, which in-
volves a very expensive summation over the entire
vocabulary. As described earlier, the complexity for
each iteration of training is at O(V T ), where T is
the size of training corpus.
For feature sets that can be expressed hierarchi-
cally, for example n-gram feature set, where higher
order n-grams imply lower order n-grams, Wu and
Khudanpur (2000) exploit the structure of the nor-
malizer, and precompute components that can be
shared by different histories. For arbitrary feature
sets, however, it may not be possible to establish
the required hierarchical relations and the normal-
izer still needs to be computed explicitly. Good-
man (2001) changes the original LM into a class-
based LM, where each one of the two-step predic-
tions only involves a much smaller summation in the
normalizer. In addition, MELM estimation can be
parallelized, with expected count computation done
1130
separately for different parts of the training data and
merged together at the end of each iteration. For
models with massive parametrizations, this merge
step can be expensive due to communication costs.
Obviously, a different way to expedite MELM
training is to simply train on less data. We propose a
way to do this without incurring a significant loss of
modeling power, by reframing the problem in terms
of binary classification. As mentioned above, we
build V binary classifiers of the form in (5) to model
the distribution over the V words. The binary clas-
sifiers use the same features as the MELM of (10),
and are given by:
Pb(w|h) =
exp
?
i ?ifi(h,w)
1 + exp
?
i ?ifi(h,w)
. (12)
We assume the features are partitioned over the vo-
cabulary, so that each feature fi has an associated
w such that fi(h,w?) = 0 for all w? 6= w. There-
fore, the corresponding ?i affects only the binary
classifier for w. This gives an important advan-
tage in terms of parallelization?we have a set of bi-
nary classifiers with no feature sharing, and can be
trained separately on different machines. The par-
allelized computations are completely independent
and do not require the tedious communication be-
tween machines. Memory-wise, since the compu-
tations are independent, each word trainer only have
to store features that are associated with the word, so
the memory requirement for each individual worker
is significantly reduced.
2.3 Neural Network LM
Neural Network Language Models (NNLM) have
gained a lot of interest since their introduction (Ben-
gio et al, 2003). While in standard language mod-
eling, words are treated as discrete symbols, NNLM
map them into a continuous space and learn their
representations automatically. It is often believed
that NNLM can generalize better to sequences that
are not seen in the training data. However, despite
having been shown to outperform standard n-gram
LM (Schwenk, 2007), NNLM are computationally
expensive to train.
Figure 1 shows the standard feed-forward NNLM
architecture. Starting from the left part of the figure,
each word of the n? 1 words history is mapped to a
Figure 1: Feed-forward NNLM
continuous vector and concatenated. Through a non-
linear hidden layer, the neural network constructs a
multinomial distribution at the output layer. Denot-
ing the concatenated d-dimensional word represen-
tations r, we have the following probability defini-
tion:
P (wi = k|wi?1, ..., wi?n+1) =
eak?
m eam
, (13)
ak = bk +
h?
l=1
Wkl tanh(cl +
(n?1)d?
j=1
Uljrj), (14)
where h denotes the hidden layer size, b and c are
the bias vectors for the output nodes and hidden
nodes respectively. Note that NNLM also has the
form of (2).
Stochastic gradient descent is often used to max-
imize the training data likelihood under such a
model. The gradient can be computed using the
back-propagation method. To analyze the complex-
ity, computing an n-gram conditional probability re-
quires approximately
O((n? 1)dh+ h+ V h+ V ) (15)
operations, where V is the size of the vocabu-
lary. The four terms in the complexity correspond
to computing the hidden layer, applying the nonlin-
earity, computing the output layer and normaliza-
tion, respectively. The error propagation stage can
be analyzed similarly and takes about the same num-
ber of operations. For typical values as used in our
experiments, namely n = 3, d = 50, h = 200,
1131
V = 10000, the majority of the complexity per iter-
ation comes from the term hV . For large scale tasks,
it may be impractical to train an NNLM.
A lot of previous research has focused on
speeding up NNLM training. It usually aims
at removing the computational dependency on V .
Schwenk (2007) used a short list of frequent words
such that a large number of out-of-list words are
taken care of by a back-off LM. To reduce the
gradient computation introduced by the normal-
izer, Bengio and Senecal (2008) proposed a dif-
ferent kind of importance sampling technique. A
recent work (Mikolov et al, 2011) applied Good-
man?s class MELM trick (2001) to NNLM, in or-
der to avoid the gigantic normalization. A similar
technique has been introduced even earlier which
took the idea of factorizing output layer to the ex-
treme (Morin, 2005) by replacing the V -way predic-
tion by a tree-style hierarchical prediction. The au-
thors show a theoretical complexity reduction from
O(V ) to (log V ), but the technique requires a care-
ful clustering which may not be easily attainable in
practice.
Subsampling has also been proposed to acceler-
ate NNLM training (Schwenk, 2007). The idea is to
select random subsets of the training data in each
epoch of stochastic gradient descent. After some
epochs, it is very likely that all of the training exam-
ples have been seen by the model. We will show that
our binary classifier representation leads to a more
robust and promising subsampling strategy.
As with MELM, we notice that the parameters
of (14) can be interpreted as also defining a set of
V per-word binary classifiers
Pb(wi = k|wi?1, ..., wi?n+1) =
eak
1 + eak , (16)
but with a common hidden layer representation. As
in MELM, we will train the classifiers, and renor-
malize them to obtain an NNLM over the V words.
In order to train the classifiers, we need to com-
pute all V output nodes and propagate the errors
back. Since the hidden layer is shared, the classifiers
are not independent, and the computations can not
be easily parallelized to multiple machines. How-
ever, subsampling can be done differently for each
classifier. Each training instance serves as a positive
example for one classifier and as a negative exam-
ple for only a fraction ? of the others. The rest of
the nodes are not computed and do not produce er-
ror signal for the hidden representation. We calibrate
the classifiers after subsampled training as described
above for MELM.
It is straightforward to show that the dominating
term V h in the complexity is reduced to ?V h. We
want to point out that compared with MELM, sub-
sampling the negatives here does not always reduce
the complexity proportionally. In cases where the
vocabulary is very small, as shown in (15), com-
puting the hidden layer can no longer be ignored.
Nonetheless, real world applications such as speech
recognition, usually involves a vocabulary of consid-
erable size, therefore, subsampling in the binary set-
ting can still achieve substantial speedup for NNLM.
3 Experimental Results
3.1 MELM
We evaluate the proposed technique on two datasets
of different sizes. Our first dataset is obtained
from Penn Treebank. Section 00-20 are used for
training(972K tokens), section 21-22 are the val-
idation set(77K), section 23-24(86K) are the test
set. The vocabulary size of the experiment is
10, 000. This is one of the standard setups on which
many researchers have reported perplexity results on
(Mikolov et al, 2011).
The binary MELM is trained using stochastic
gradient descent, no explicit regularization is per-
formed (Zhang, 2004). The learning rate starts at 0.1
and is halved every time the perplexity on the vali-
dation set stops decreasing. It usually takes around
20 iterations before no significant improvement can
be obtained on the validation set. The training stops
at that time.
We compare perplexity with both the standard in-
terpolated Kneser-Ney trigram model and the stan-
dard MELM. The MELM is L2 regularized and es-
timated using a variant of generalized iterative scal-
ing, the regularizer is tuned on the validation data.
To demonstrate the effectiveness of our subsampling
approach, we compare the subsampled versions of
the binary MELM and the standard MELM. In order
to obtain valid perplexities, the binary LMs are first
renormalized explicitly according to equation (8) for
each test history.
1132
Model PPL
KN Trigram 153.0
Standard MELM, Feat-I 154.2
Binary MELM, Feat-I 153.7
Standard MELM, Feat-II 140.2
Binary MELM, Feat-II 141.1
Table 1: Binary MELM vs. Standard MELM
We consider two kinds of feature sets: Feat-I con-
tains only n-gram features, namely unigram, bigram
and trigram features, with no count cutoff, the total
number of features is 0.9M . Feat-II is augmented
with skip-1 bigrams and skip-1 trigrams (Goodman,
2001), as well as word trigger features as described
in (Rosenfeld, 1996). The total number of features
in this set is 1.9M . Note that the speedup trick de-
scribed in (Wu and Khudanpur, 2000) can be used
for feat-I , but not feat-II .
Table 1 shows the perplexity results when no sub-
sampling is performed. With only n-gram features,
the binary MELM is able to match both standard
MELM and the Kneser-Ney model. We can also see
that by adding features that are known to be able to
improve the standard MELM, we can get the same
improvement in the binary setting.
Figure 2 shows the comparisons of the two types
of MELM when the training data are subsampled.
The standard MELM with n-gram features suffers
drastically as we sample more aggressively. In con-
trast, the binary n-gram MELM(Feat-I) does not
appear to be hurt by aggressive subsampling, even
when 99% of the negative examples are discarded.
The robustness also holds for Feat-II where more
complicated features are added into the model. This
suggests a very efficient way of training MELM?
with only 1% of the computational cost, we are able
to train an LM as powerful as the standard MELM.
We further test our approach on a second dataset
which comes from Wall Street Journal corpus. It
contains 26M training tokens and a test set of 22K
tokens. We also have a held-out validation set to
tune parameters. This set of experiments is intended
to demonstrate that the binary subsampling tech-
nique is useful on a large text corpus where training
a standard MELM is not practical, and gives a better
LM than the commonly used Kneser-Ney baseline.
Figure 2: Subsampled Binary MELM vs. Subsampled
Standard MELM
Model PPL
KN Trigram 117.7
Standard MELM, Trigram 116.5
Binary MELM, Feat-III, 10% 110.2
Binary MELM, Feat-III, 5% 110.8
Binary MELM, Feat-III, 2% 112.1
Binary MELM, Feat-III, 1% 112.4
Table 2: Binary Subsampled MELM on WSJ
The binary MELM is trained in the same way as
described in the previous experiment. Besides un-
igram, bigram and trigram features, we also added
skip-1 bigrams and skip-1 trigrams, this gives us
7.5M features in total. We call this set of features
feat-III . We were unable to train a standard MELM
with feat-III or a binary MELM without subsam-
pling because of the computational cost. However,
with our binary subsampling technique, as shown in
Table 2, we are able to benefit from skip n-gram fea-
tures with only 5% of the standard MELM complex-
ity. Also the performance does not degrade much as
we discard more negative examples.
To show that such improvement in perplexity
translates into gains in practical applications, we
conducted a set of speech recognition experiments.
The task is on Wall Street Journal, the LMs are
trained on 37M tokens and are used to rescore the n-
best list generated by the first pass recognizer with
a trigram LM. The details of the experimental setup
can be found in (Xu et al, 2009). Our baseline LM
is an interpolated Kneser-Ney 4-gram model.
Note that the size of the vocabulary for the task
1133
Model Dev WER Eval WER
KN 4-gram 11.8 17.2
Binary MELM, Feat-IV, 5% 11.0 16.7
Binary MELM, Feat-IV, 2% 11.2 16.7
Binary MELM, Feat-IV, 1% 11.2 16.7
Table 3: WSJ WER improvement. Binary MELM are
interpolated with KN 4-gram
is 20K, for the purpose of rescoring, we are only
interested in the words that exist in the n-best list,
therefore, for the binary MELM, we only have to
train about 5300 binary classifiers. For comparison,
the KN 4-gram also uses the same restricted vocabu-
lary. The features for the binary MELM are n-gram
features up to 4-grams plus skip-1 bigrams and skip-
1 trigrams. The total number of features is 10M. We
call this set of features Feat-IV.
Table 3 demonstrates the word error rate(WER)
improvement enabled by our binary subsampling
technique. Note that we can achieve 0.5% abso-
lute WER improvement on the test set at only 1%
of the standard MELM complexity. More specifi-
cally, with only 50 machines, such a reduction in
complexity allows us to train a binary MELM with
skip n-gram features in less than two hours, which is
not possible for the standard MELM on 37M words.
Obviously, with more machines, the estimation
can be even faster, it?s also reasonable to expect that
with more kinds of features, the improvement can
be even larger. We think that the proposed technique
opens the door for the utilization of the modeling
framework provided by MELM at a scale that has
not been possible before.
3.2 NNLM
We evaluate our binary subsampling technique on
the same Penn Treebank corpus as described for the
MELM experiments. Taking random subsets of the
training data with the standard model is our primary
baseline to compare with. The NNLM we train is
a trigram LM with tanh hidden units. The size of
word representation and the size of hidden layer are
tuned minimally on the validation set(Hidden layer
size 200; Representation size 50). We adopt the
same learning rate strategy as for training MELM,
and the validation set is used to track perplexity per-
formance and adjust learning rate correspondingly.
Model PPL100% 20% 10% 5%
Standard NNLM 154.3 239.8 297.0 360.3
Binary NNLM - 152.7 160.0 176.2
KN trigram 153.0 - - -
Table 4: Binary NNLM vs. Standard NNLM. Fixed ran-
dom subset.
Model Interpolated PPL100% 20% 10% 5%
Standard NNLM 132.7 145.6 148.6 150.7
Binary NNLM - 132.1 134.2 138.0
KN trigram 153.0 - - -
Table 5: Binary NNLM vs. Standard NNLM. Fixed ran-
dom subset. Interpolated with KN trigram.
All parameters are initialized randomly with mean 0
and variance 0.01. As with binary MELM, binary
NNLM are explicitly renormalized to obtain valid
perplexities.
In our first experiment, we keep the subsampled
data fixed as we did for MELM. For the standard
NNLM, it means only a subset of the data is seen by
the model and it does not change through epochs;
For binary NNLM, it means the subset of negative
examples for each binary classifier does not change.
Table 4 shows the perplexity results by NNLM itself
and the interpolated results are shown in Table 5.
We can see that both models exhibit a tendency
to deteriorate as we subsample more aggressively.
However, the standard NNLM is clearly impacted
more severely. With binary NNLM, we are able to
retain all the gain after interpolation with only 20%
of the negative examples.
Notice that with a fixed random subset, we are not
replicating the experiments of Schwenk (Schwenk,
2007) exactly, although it is reasonable to expect
both models are able to benefit from seeing different
random subsets of the training data. This is verified
by results in Table 6 and Table 7.
The standard NNLM benefits quite a lot going
from using a fixed random subset to a variable ran-
dom subset, but still demonstrates a clear tendency
to deteriorate as we discard more and more data. On
the constrast, the binary NNLM maintains all the
performance gain with only 5% of the negative ex-
amples and still clearly outperforms its counterpart.
1134
Model PPL100% 20% 10% 5%
Standard NNLM 154.3 157.7 172.2 186.5
Binary NNLM - 151.7 150.1 152.1
Table 6: Binary NNLM vs. Standard NNLM. Variable
random subset.
Model Interpolate PPL100% 20% 10% 5%
Standard NNLM 132.7 133.9 138.1 141.2
Binary NNLM - 132.2 131.7 132.2
Table 7: Binary NNLM vs. Standard NNLM. Variable
random subset. Interpolated with KN trigram.
4 Discussion
For the standard models, the amount of existent pat-
terns fed into training heavily depends on the sub-
sampling rate ?. For a small ?, the models will in-
evitably lose some training patterns given any rea-
sonable number of epochs of training. Taking vari-
able random subsets in each epoch can alleviate this
problem to some extent, but still can not solve the
fundamental problem. In the binary setting, we are
able to do subsampling differently. While the com-
plexity remains the same without subsampling, the
majority of the complexity comes from processing
negatives examples for each binary classifier. There-
fore, we can achieve the same level of speedup as
standard subsampling by only subsampling negative
examples, and most importantly, it allows us to keep
all the existent patterns(positive examples) in the
training data. Of course, negative examples are im-
portant and even in the binary case, we benefit from
including more of them, but since we have so many
of them, they might not be as critical as positive ex-
amples in determining the distribution.
A similar conclusion can be drawn from Google?s
work on large LMs (Brants et al, 2007). Not having
to properly smooth the LM, they are still able to ben-
efit from large volumes of web text as training data.
It is probably more important to have a high n-gram
coverage than having a precise distribution.
The explanation here might lead us to wonder
whether for the multi-class problem, subsampling
the terms in the normalizer would achieve the same
results. More specifically, instead of summing over
all words in the vocabulary, we may choose to only
consider ? of them. In fact, the short-list approach
in (Schwenk, 2007) and the adaptive importance
sampling in (Bengio and Senecal, 2008) have ex-
actly this intuition. However, in the multi-class
setup, subsampling like this has to be very careful.
We have to either have a good estimate of how much
probability mass we?ve thrown away, as in the short-
list approach, or have a good estimate of the entire
normalizer, as in the importance sampling approach.
It is very unlikely that an arbitrary random subsam-
pling will not harm the model. Fortunately, in the bi-
nary case, the effect of random subsampling is much
easier to analyze. We know exactly how much nega-
tive examples we?ve discarded, and they can be com-
pensated easily in the end.
It is worth pointing out that the proposed tech-
nique is not restricted to MELM and NNLM. We
have done experiments to binarize the class trick
sometimes used for language modeling (Goodman,
2001; Mikolov et al, 2011), and it also proves to
be useful. We plan to report these results in the fu-
ture. More generally, for many large-scale multi-
class problems, binarization and subsampling can be
an effective combination to consider.
5 Conclusion
We propose efficient subsampling techniques for
training large multi-class classifiers such as maxi-
mum entropy language models and neural network
language models. The main idea is to replace a
multi-way decision by a set of binary decisions.
Since most of the training instances in the binary
setting are negatives examples, we can achieve sub-
stantial speedup by subsampling only the negatives.
We show by extensive experiments that this is more
robust than subsampling subsets of training data for
the original multi-class classifier. The proposed
method can be very useful for building large lan-
guage models and solving more general multi-class
problems.
Acknowledgments
This work is partially supported by National Science
Foundation Grant No? 0963898, the DARPA GALE
Program and JHU/HLTCOE.
1135
References
Allwein, Erin, Robert Schapire, Yoram Singer and Pack
Kaelbling. 2000. Reducing Multiclass to Binary: A
Unifying Approach for Margin Classifiers. Journal of
Machine Learning Research, 1:113-141.
Bengio, Yoshua, Rejean Ducharme and Pascal Vincent
2003. A neural probabilistic language model Journal
of Machine Learning research, 3:1137?1155.
Bengio, Yoshua and J. Senecal 2008. Adaptive impor-
tance sampling to accelerate training of a neural prob-
abilistic language model IEEE Transaction on Neural
Network, Apr. 2008.
Berger, Adam, Stephen A. Della Pietra and Vicent J.
Della Pietra 1996. A Maximum Entropy approach
to Natural Language Processing. Computational Lin-
guistics, 1996, 22:39-71.
Brants, Thorsten, Ashok C. Popat, Peng Xu, Frank J. Och
and Jeffrey Dean 2007. Large language models in ma-
chine translation. In Proceedings of 2007 Conference
on Empirical Methods in Natural Language Process-
ing, 858?867.
Goodman, Joshua 2001. Classes for Fast Maximum
Entropy Training. Proceedings of 2001 IEEE Inter-
national Conference on Acoustics, Speech and Signal
Processing.
Goodman, Joshua 2001. A bit of Progress in Language
Modeling. Computer Speech and Language, 403-434.
Khudanpur, Sanjeev and Jun Wu 2000. Maximum En-
tropy Techniques for Exploiting Syntactic, Semantic
and Collocational Dependencies in Language Model-
ing. Computer Speech and Language, 14(4):355-372.
Mikolov, Tomas, Stefan Kombrink, Lukas Burget, Jan
?Honza? Cernocky and Sanjeev Khudanpur 2011. Ex-
tensions of recurrent neural network language model.
Proceedings of 2011 IEEE International Conference
on Acoustics, Speech and Signal Processing.
Morin, Frederic 2005. Hierarchical probabilistic neural
network language model. AISTATS?05, pp. 246-252.
Neyman, Jerzy 1934. On the Two Different Aspects
of the Representative Method: The Method of Strati-
fied Sampling and the Method of Purposive Selection.
Journal of the Royal Statistical Society, 97(4):558-
625.
Rifkin, Ryan and Aldebaro Klautau 2004. In Defense of
One-Vs-All Classification. Journal of Machine Learn-
ing Research.
Rosenfeld, Roni. 1996. A maximum entropy approach
to adaptive statistical language modeling. Computer
Speech and Language, 10:187?228.
Schwenk, Holger 2007. Continuous space language
model. Computer Speech and Language, 21(3):492-
518.
Wu, Jun and Sanjeev Khudanpur. 2000. Efficient train-
ing methods for maximum entropy language model-
ing. Proceedings of the 6th International Conference
on Spoken Language Technologies, pp. 114?117.
Xu, Puyang, Damianos Karakos and Sanjeev Khudanpur.
2009. Self-supervised discriminative training of statis-
tical language models. Proceedings of 2009 IEEE Au-
tomatic Speech Recognition and Understanding Work-
shop.
Zhang, Tong 2004. Solving large scale linear prediction
problems using stochastic gradient descent algorithms.
Proceedings of 2004 International Conference on Ma-
chine Learnings.
1136
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 325?328,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
A Comparative Study of Word Co-occurrence for Term Clustering
in Language Model-based Sentence Retrieval
Saeedeh Momtazi
Spoken Language Systems
Saarland University
saeedeh.momtazi
@lsv.uni-saarland.de
Sanjeev Khudanpur
Center for Language
and Speech Processing
Johns Hopkins University
khudanpur@jhu.edu
Dietrich Klakow
Spoken Language Systems
Saarland University
dietrich.klakow
@lsv.uni-saarland.de
Abstract
Sentence retrieval is a very important part of
question answering systems. Term clustering,
in turn, is an effective approach for improving
sentence retrieval performance: the more simi-
lar the terms in each cluster, the better the per-
formance of the retrieval system. A key step in
obtaining appropriate word clusters is accurate
estimation of pairwise word similarities, based
on their tendency to co-occur in similar con-
texts. In this paper, we compare four differ-
ent methods for estimating word co-occurrence
frequencies from two different corpora. The re-
sults show that different, commonly-used con-
texts for defining word co-occurrence differ
significantly in retrieval performance. Using an
appropriate co-occurrence criterion and corpus
is shown to improve the mean average preci-
sion of sentence retrieval form 36.8% to 42.1%.
1 Corpus-Driven Clustering of Terms
Since the search in Question Answering (QA) is con-
ducted over smaller segments of text than in docu-
ment retrieval, the problems of data sparsity and ex-
act matching become more critical. The idea of using
class-based language model by applying term clus-
tering, proposed by Momtazi and Klakow (2009), is
found to be effective in overcoming these problems.
Term clustering has a very long history in natu-
ral language processing. The idea was introduced
by Brown et al (1992) and used in different appli-
cations, including speech recognition, named entity
tagging, machine translation, query expansion, text
categorization, and word sense disambiguation. In
most of the studies in term clustering, one of several
well-know notions of co-occurrence?appearing in
the same document, in the same sentence or follow-
ing the same word?has been used to estimate term
similarity. However, to the best of our knowledge,
none of them explored the relationship between dif-
ferent notions of co-occurrence and the effectiveness
of their resulting clusters in an end task.
In this research, we present a comprehensive study
of how different notions of co-occurrence impact re-
trieval performance. To this end, the Brown algo-
rithm (Brown et al, 1992) is applied to pairwise word
co-occurrence statistics based on different definitions
of word co-occurrence. Then, the word clusters are
used in a class-based language model for sentence
retrieval. Additionally, impact of corpus size and do-
main on co-occurrence estimation is studied.
The paper is organized as follows. In Section 2,
we give a brief description of class-based language
model for sentence retrieval and the Brown word
clustering algorithm. Section 3 presents different
methods for estimating the word co-occurrence. In
Section 4, experimental results are presented. Fi-
nally, Section 5 summarizes the paper.
2 Term Clustering Method and Application
In language model-based sentence retrieval, the prob-
ability P (Q|S) of generating query Q conditioned on
a candidate sentence S is first calculated. Thereafter
sentences in the search collection are ranked in de-
scending order of this probability. For word-based
unigram, P (Q|S) is estimated as
P (Q|S) =
?
i=1...M
P (qi|S), (1)
where M is the number of query terms, qi denotes the
ith query term in Q, and S is the sentence model.
325
For class-based unigrams, P (Q|S) is computed
using only the cluster labels of the query terms as
P (Q|S) =
?
i=1...M
P (qi|Cqi , S)P (Cqi |S), (2)
where Cqi is the cluster that contains qi and
P (qi|Cqi , S) is the emission probability of the
ith query term given its cluster and the sen-
tence. P (Cqi |S) is analogous to the sentence model
P (qi|S) in (1), but is based on clusters instead of
terms. To calculate P (Cqi |S), each cluster is con-
sidered an atomic entity, with Q and S interpreted as
sequences of such entities.
In order to cluster lexical items, we use the al-
gorithm proposed by Brown et al(1992), as imple-
mented in the SRILM toolkit (Stolcke, 2002). The al-
gorithm requires an input corpus statistics in the form
?w,w?, fww??, where fww? is the number of times the
word w? is seen in the context w. Both w and w? are
assumed to come from a common vocabulary. Be-
ginning with each vocabulary item in a separate clus-
ter, a bottom-up approach is used to merge the pair of
clusters that minimizes the loss in Average Mutual In-
formation (AMI) between the word cluster Cw? and
its context cluster Cw. Different words seen in the
same contexts are good candidates for merger, as are
different contexts in which the same words are seen.
While originally proposed with bigram statistics,
the algorithm is agnostic to the definition of co-
occurrence. E.g. if ?w,w?? are verb-object pairs,
the algorithm clusters verbs based on their selectional
preferences, if fww? is the number of times w and w?
appear in the same document, it will produce seman-
tically (or topically) related word-clusters, etc.
Several notions of co-occurrence have been used
in the literature to cluster words, as described next.
3 Notions of Word Co-occurrence
Co-occurrence in a Document
If two content words w and w? are seen in the
same document, they are usually topically related. In
this notion of co-occurrence, how near or far away
from each other they are in the document is irrele-
vant, as is their order of appearance in the document.
Document-wise co-occurrence has been successfully
used in many NLP applications such as automatic
thesaurus generation (Manning et al, 2008)
Statistics of document-wise co-occurrence may be
collected in two different ways. In the first case,
fww? = fw?w is simply the number of documents that
contain both w and w?. This is usually the notion
used in ad hoc retrieval. Alternatively, we may want
to treat each instance of w? in a document that con-
tains an instance of w to be a co-occurrence event.
Therefore if w? appears three times in a document
that contains two instances of w, the former method
counts it as one co-occurrence, while the latter as six
co-occurrences. We use the latter statistic, since we
are concerned with retrieving sentence sized ?docu-
ments,? wherein a repeated word is more significant.
Co-occurrence in a Sentence
Since topic changes sometimes happen within a
single document, and our end task is sentence re-
trieval, we also investigate the notion of word co-
occurrence in a smaller segment of text such as a
sentence. In contrast to the document-wise model,
sentence-wise co-occurrence does not consider whole
documents, and only concerns itself with the number
of times that two words occur in the same sentence.
Co-occurrence in a Window of Text
The window-wise co-occurrence statistic is an even
narrower notion of context, considering only terms in
a window surrounding w?. Specifically, a window of
a fixed size is moved along the text, and fww? is set
as the number of times both w and w? appear in the
window. Since the window size is a free parameter,
different sizes may be applied. In our experiments we
use two window sizes, 2 and 5, that have been studied
in related research (Church and Hanks, 1990).
Co-occurrence in a Syntactic Relationship
Another notion of word similarity derives from
having the same syntactic relationship with the con-
text w. This syntax-wise co-occurrence statistic is
similar to the sentence-wise co-occurrence, in that
co-occurrence is defined at the sentence level. How-
ever, in contrast to the sentence-wise model, w and
w? are said to co-occur only if there is a syntactic re-
lation between them in that sentence. E.g., this type
of co-occurrence can help cluster nouns that are used
as objects of same verb, such as ?tea?, ?water?, and
?cola,? which all are used with the verb ?drink?.
To gather such statistics, all sentences in the corpus
must be syntactically parsed. We found that a depen-
dency parser is an appropriate tool for our goal: it
326
directly captures dependencies between words with-
out the mediation of any virtual (nonterminal) nodes.
Having all sentences in the parsed format, fww? is de-
fined as the number of times that the words w and w?
have a parent-child relationship of any syntactic type
in the dependency parse tree. For our experiments we
use MINIPAR (Lin, 1998) to parse the whole corpus
due to its robustness and speed.
4 Sentence Retrieval Experiments
4.1 Derivatives of the TREC QA Data Sets
The set of questions from the TREC 2006 QA track1
was used as the test data to evaluate our models,
while the TREC 2005 set was used for development.
The TREC 2006 QA task contains 75 question-
series, each on one topic, for a total of 403 factoid
questions which is used as queries for sentence re-
trieval. For sentence-level relevance judgments, the
Question Answer Sentence Pair corpus of Kaisser
and Lowe (2008) was used. All the documents
that contain relevant sentences are from the NIST
AQUAINT1 corpus.
QA systems typically employ sentence retrieval af-
ter initial, high quality document retrieval. To simu-
late this, we created a separate search collection for
each question using all sentences from all documents
relevant to the topic (question-series) from which the
question was derived. On average, there are 17 rel-
evant documents per topic, many not relevant to the
question itself: they may be relevant to another ques-
tion. So the sentence search collection is realistic,
even if somewhat optimistic.
4.2 Corpora for Term Clustering
We investigated two different corpora2, AQUAINT1
and Google n-grams, to obtain word co-occurrence
statistics for term clustering. Based on this we can
also evaluate the impact of corpus size and corpus
domain on the result of term clustering.
AQUAINT1 consists of English newswire text ex-
tracted from the Xinhua, the New York Times and the
Associated Press Worldstream News Services.
The Google n-gram counts were generated from
publicly accessible English web pages. Since there is
1See http://trec.nist.gov.
2See catalog numbers LDC2002T31 and LDC2006T13 re-
spectively at http://www.ldc.upenn.edu/Catalog.
Corpus Co-occurrence # Word Pairs
AQUAINT1 document 368,109,133
AQUAINT1 sentence 104,084,473
AQUAINT1 syntax 12,343,947
AQUAINT1 window-5 46,307,650
AQUAINT1 window-2 14,093,661
Google n-grams window-5 12,005,479
Google n-grams window-2 328,431,792
Table 1: Statistics for different notions of co-occurrence.
no possibility of extracting document-wise, sentence-
wise or syntax-wise co-occurrence statistics from the
Google n-gram corpus, we only collect window-wise
statistics to the extent available in the corpus.
Table 1 shows the number of word pairs extracted
from the two corpora with different definitions of co-
occurrence. The statistics only include word pairs
for which both constituent words are present in the
35,000 word vocabulary of our search collection.
4.3 Sentence Retrieval Results and Discussion
Sentence retrieval performance for term clustering
using different definitions of word co-occurrence is
shown in Figure 1. Since the Brown algorithm re-
quires specifying the number of clusters, tests were
conducted for 50, 100, 200, 500, and 1000 clusters
of the term vocabulary. The baseline system is the
word-based sentence retrieval model of Equation (1).
Figure 1(a) shows the Mean Average Precision
(MAP) for class-based sentence retrieval of Equation
(2) using clusters based on different co-occurrence
statistics from AQUAINT1. Note that
(i) the best result achieved by sentence-wise co-
occurence is better the best result of document-
wise, perhaps due to more local and relevant in-
formation that it captures;
(ii) all the results achieved by syntax-wise co-
occurrence are better than sentence-wise, indi-
cating that merely co-occurring in a sentence
is not very indicative of word similarity, while
relations extracted from syntactic structure im-
prove system performance significantly;
(iii) window-2 significantly outperforms all other
notions of co-occurrence; i.e., the bigram statis-
tics achieve the best clustering results. In com-
parison, window-5 has the worst results, with
performance very close to baseline.
Although window-5 co-occurrence has been reported
327
50 5000.35
0.36
0.37
0.38
0.39
0.40
0.41
0.42
0.43
document sentence window2 window5 syntax base?line
log?of?number?of?clusters
MAP
50 5000.35
0.36
0.37
0.38
0.39
0.40
0.41
0.42
0.43
AQUAINT?window2 Google?window2 Google?window5 base?line
log?of?number?of?clusters
MAP
(b)(a)
Figure 1: MAP of sentence retrieval for different word co-occurrence statistics from AQUAINT1 and Google n-grams.
to be effective in other applications, it is not helpful
in sentence retrieval.
Figure 1(b) shows the MAP for class-based sen-
tence retrieval of Equation (2) when window-wise
co-occurrence statistics from the Google n-grams are
used. For better visualization, we repeated the MAP
results using AQUAINT1 window-2 co-occurrence
statistics from Figure 1(a) in 1(b). Note that
(iv) window-2 co-occurrence statistics significantly
outperform window-5 for the Google n-grams,
consistent with results from AQUAINT1;
(v) Google n-gram window-2 co-occurrence statis-
tics consistently result in better MAP than
AQUAINT window-2.
The last result indicates that even though the Google
n-grams are from a different (and much broader) do-
main than the test data, they significantly improve the
system performance due to sheer size. Finally
(vi) Google n-gram window-2 MAP curve is flatter
than AQUAINT window-2; i.e., performance is
not very sensitive to the number of clusters.
The best overall result is from Google window-2
co-occurrence statistics with 100 clusters, achiev-
ing 42.1% MAP while the best result derived
from AQUAINT1 is 41.7% MAP for window-2 co-
occurrence with 100 clusters, and the MAP of the
word-based model (baseline) is 36.8%.
5 Concluding Remarks
We compared different notions of word co-
occurrence for clustering terms, using document-
wise, sentence-wise, window-wise, and syntax-wise
co-occurrence statistics derived from AQUAINT1.
We found that different notions of co-occurrence sig-
nificantly change the behavior of a sentence retrieval
system, in which window-wise model with size 2
achieves the best result. In addition, Google n-grams
were used for window-wise model to study the im-
pact of corpus size and domain on the clustering re-
sult. The result showed that although the domain of
the Google n-grams is dissimilar to the test set, it
outperforms models derived from AQUAINT1 due to
sheer size.
Acknowledgments
Saeedeh Momtazi is funded by the German research
foundation DFG through the International Research
Training Group (IRTG 715).
References
P.F. Brown, V.J.D. Pietra, P.V. Souza, J.C. Lai, and R.L.
Mercer. 1992. Class-based n-gram models of natural
language. Computational Linguistics, 18(4):467?479.
K.W. Church and P. Hanks. 1990. Word association
norms, mutual information, and lexicography. Com-
putational Linguistics, 16(1):22?29.
M. Kaisser and J.B. Lowe. 2008. Creating a research
collection of question answer sentence pairs with Ama-
zon?s mechanical turk. In Proc. of LREC.
D. Lin. 1998. Dependency-based evaluation of MINI-
PAR. In Proc. of the Evaluation of Parsing Systems
Workshop.
C.D. Manning, P. Raghavan, and H. Schu?tze. 2008. Intro-
duction to Information Retrieval. Cambridge Univer-
sity Press.
S. Momtazi and D. Klakow. 2009. A word clustering
approach for language model-based sentence retrieval
in question answering systems. In Proc. of ACM CIKM.
A. Stolcke. 2002. SRILM - an extensible language mod-
eling toolkit. In Proc. of ICSLP.
328
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 175?183,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Fast Syntactic Analysis for Statistical Language Modeling
via Substructure Sharing and Uptraining
Ariya Rastrow, Mark Dredze, Sanjeev Khudanpur
Human Language Technology Center of Excellence
Center for Language and Speech Processing, Johns Hopkins University
Baltimore, MD USA
{ariya,mdredze,khudanpur}@jhu.edu
Abstract
Long-span features, such as syntax, can im-
prove language models for tasks such as
speech recognition and machine translation.
However, these language models can be dif-
ficult to use in practice because of the time
required to generate features for rescoring a
large hypothesis set. In this work, we pro-
pose substructure sharing, which saves dupli-
cate work in processing hypothesis sets with
redundant hypothesis structures. We apply
substructure sharing to a dependency parser
and part of speech tagger to obtain significant
speedups, and further improve the accuracy
of these tools through up-training. When us-
ing these improved tools in a language model
for speech recognition, we obtain significant
speed improvements with bothN -best and hill
climbing rescoring, and show that up-training
leads to WER reduction.
1 Introduction
Language models (LM) are crucial components in
tasks that require the generation of coherent natu-
ral language text, such as automatic speech recog-
nition (ASR) and machine translation (MT). While
traditional LMs use word n-grams, where the n ? 1
previous words predict the next word, newer mod-
els integrate long-span information in making deci-
sions. For example, incorporating long-distance de-
pendencies and syntactic structure can help the LM
better predict words by complementing the predic-
tive power of n-grams (Chelba and Jelinek, 2000;
Collins et al, 2005; Filimonov and Harper, 2009;
Kuo et al, 2009).
The long-distance dependencies can be modeled
in either a generative or a discriminative framework.
Discriminative models, which directly distinguish
correct from incorrect hypothesis, are particularly
attractive because they allow the inclusion of arbi-
trary features (Kuo et al, 2002; Roark et al, 2007;
Collins et al, 2005); these models with syntactic in-
formation have obtained state of the art results.
However, both generative and discriminative LMs
with long-span dependencies can be slow, for they
often cannot work directly with lattices and require
rescoring large N -best lists (Khudanpur and Wu,
2000; Collins et al, 2005; Kuo et al, 2009). For dis-
criminative models, this limitation applies to train-
ing as well. Moreover, the non-local features used in
rescoring are usually extracted via auxiliary tools ?
which in the case of syntactic features include part of
speech taggers and parsers ? from a set of ASR sys-
tem hypotheses. Separately applying auxiliary tools
to each N -best list hypothesis leads to major ineffi-
ciencies as many hypotheses differ only slightly.
Recent work on hill climbing algorithms for ASR
lattice rescoring iteratively searches for a higher-
scoring hypothesis in a local neighborhood of the
current-best hypothesis, leading to a much more ef-
ficient algorithm in terms of the number, N , of hy-
potheses evaluated (Rastrow et al, 2011b); the idea
also leads to a discriminative hill climbing train-
ing algorithm (Rastrow et al, 2011a). Even so, the
reliance on auxiliary tools slow LM application to
the point of being impractical for real time systems.
While faster auxiliary tools are an option, they are
usually less accurate.
In this paper, we propose a general modifica-
175
tion to the decoders used in auxiliary tools to uti-
lize the commonalities among the set of generated
hypotheses. The key idea is to share substructure
states in transition based structured prediction al-
gorithms, i.e. algorithms where final structures are
composed of a sequence of multiple individual deci-
sions. We demonstrate our approach on a local Per-
ceptron based part of speech tagger (Tsuruoka et al,
2011) and a shift reduce dependency parser (Sagae
and Tsujii, 2007), yielding significantly faster tag-
ging and parsing of ASR hypotheses. While these
simpler structured prediction models are faster, we
compensate for the model?s simplicity through up-
training (Petrov et al, 2010), yielding auxiliary tools
that are both fast and accurate. The result is signif-
icant speed improvements and a reduction in word
error rate (WER) for both N -best list and the al-
ready fast hill climbing rescoring. The net result
is arguably the first syntactic LM fast enough to be
used in a real time ASR system.
2 Syntactic Language Models
There have been several approaches to include syn-
tactic information in both generative and discrimi-
native language models.
For generative LMs, the syntactic information
must be part of the generative process. Structured
language modeling incorporates syntactic parse
trees to identify the head words in a hypothesis for
modeling dependencies beyond n-grams. Chelba
and Jelinek (2000) extract the two previous exposed
head words at each position in a hypothesis, along
with their non-terminal tags, and use them as con-
text for computing the probability of the current po-
sition. Khudanpur and Wu (2000) exploit such syn-
tactic head word dependencies as features in a maxi-
mum entropy framework. Kuo et al (2009) integrate
syntactic features into a neural network LM for Ara-
bic speech recognition.
Discriminative models are more flexible since
they can include arbitrary features, allowing for
a wider range of long-span syntactic dependen-
cies. Additionally, discriminative models are di-
rectly trained to resolve the acoustic confusion in the
decoded hypotheses of an ASR system. This flexi-
bility and training regime translate into better perfor-
mance. Collins et al (2005) uses the Perceptron al-
gorithm to train a global linear discriminative model
which incorporates long-span features, such as head-
to-head dependencies and part of speech tags.
Our Language Model. We work with a discrimi-
native LM with long-span dependencies. We use a
global linear model with Perceptron training. We
rescore the hypotheses (lattices) generated by the
ASR decoder?in a framework most similar to that
of Rastrow et al (2011a).
The LM score S(w,a) for each hypothesis w of
a speech utterance with acoustic sequence a is based
on the baseline ASR system score b(w,a) (initial n-
gram LM score and the acoustic score) and ?0, the
weight assigned to the baseline score.1 The score is
defined as:
S(w,a) = ?0 ? b(w,a) + F (w, s1, . . . , sm)
= ?0 ? b(w,a) +
d?
i=1
?i ? ?i(w, s1, . . . , sm)
where F is the discriminative LM?s score for the
hypothesis w, and s1, . . . , sm are candidate syntac-
tic structures associated with w, as discussed be-
low. Since we use a linear model, the score is a
weighted linear combination of the count of acti-
vated features of the word sequence w and its as-
sociated structures: ?i(w, s1, . . . , sm). Perceptron
training learns the parameters ?. The baseline score
b(w,a) can be a feature, yielding the dot product
notation: S(w,a) = ??,?(a,w, s1, . . . , sm)? Our
LM uses features from the dependency tree and part
of speech (POS) tag sequence. We use the method
described in Kuo et al (2009) to identify the two
previous exposed head words, h?2, h?1, at each po-
sition i in the input hypothesis and include the fol-
lowing syntactic based features into our LM:
1. (h?2.w ? h?1.w ? wi) , (h?1.w ? wi) , (wi)
2. (h?2.t ? h?1.t ? ti) , (h?1.t ? ti) , (ti) , (tiwi)
where h.w and h.t denote the word identity and the
POS tag of the corresponding exposed head word.
2.1 Hill Climbing Rescoring
We adopt the so called hill climbing framework of
Rastrow et al (2011b) to improve both training and
rescoring time as much as possible by reducing the
1We tune ?0 on development data (Collins et al, 2005).
176
number N of explored hypotheses. We summarize
it below for completeness.
Given a speech utterance?s lattice L from a first
pass ASR decoder, the neighborhood N (w, i) of a
hypothesis w = w1w2 . . . wn at position i is de-
fined as the set of all paths in the lattice that may
be obtained by editing wi: deleting it, substituting
it, or inserting a word to its left. In other words,
it is the ?distance-1-at-position i? neighborhood of
w. Given a position i in a word sequence w, all
hypotheses in N (w, i) are rescored using the long-
span model and the hypothesis w??(i) with the high-
est score becomes the new w. The process is re-
peated with a new position ? scanned left to right
? until w = w??(1) = . . . = w??(n), i.e. when w
itself is the highest scoring hypothesis in all its 1-
neighborhoods, and can not be furthered improved
using the model. Incorporating this into training
yields a discriminative hill climbing algorithm (Ras-
trow et al, 2011a).
3 Incorporating Syntactic Structures
Long-span models ? generative or discriminative,
N -best or hill climbing ? rely on auxiliary tools,
such as a POS tagger or a parser, for extracting
features for each hypothesis during rescoring, and
during training for discriminative models. The top-
m candidate structures associated with the ith hy-
pothesis, which we denote as s1i , . . . , smi , are gener-
ated by these tools and used to score the hypothesis:
F (wi, s1i , . . . , smi ). For example, s
j
i can be a part of
speech tag or a syntactic dependency. We formally
define this sequential processing as:
w1
tool(s)????? s11, . . . , sm1
LM??? F (w1, s11, . . . , sm1 )
w2
tool(s)????? s12, . . . , sm2
LM??? F (w2, s12, . . . , sm2 )
...
wk
tool(s)????? s1k, . . . , smk
LM??? F (wk, s1k, . . . , smk )
Here, {w1, . . . ,wk} represents a set of ASR output
hypotheses that need to be rescored. For each hy-
pothesis, we apply an external tool (e.g. parser) to
generate associated structures s1i , . . . , smi (e.g. de-
pendencies.) These are then passed to the language
model along with the word sequence for scoring.
3.1 Substructure Sharing
While long-span LMs have been empirically shown
to improve WER over n-gram LMs, the computa-
tional burden prohibits long-span LMs in practice,
particularly in real-time systems. A major complex-
ity factor is due to processing 100s or 1000s of hy-
potheses for each speech utterance, even during hill
climbing, each of which must be POS tagged and
parsed. However, the candidate hypotheses of an
utterance share equivalent substructures, especially
in hill climbing methods due to the locality present
in the neighborhood generation. Figure 1 demon-
strates such repetition in an N -best list (N=10) and
a hill climbing neighborhood hypothesis set for a
speech utterance from broadcast news. For exam-
ple, the word ?ENDORSE? occurs within the same
local context in all hypotheses and should receive
the same part of speech tag in each case. Processing
each hypothesis separately wastes time.
We propose a general algorithmic approach to re-
duce the complexity of processing a hypothesis set
by sharing common substructures among the hy-
potheses. Critically, unlike many lattice parsing al-
gorithms, our approach is general and produces ex-
act output. We first present our approach and then
demonstrate its generality by applying it to a depen-
dency parser and part of speech tagger.
We work with structured prediction models that
produce output from a series of local decisions: a
transition model. We begin in initial state pi0 and
terminate in a possible final state pif . All states
along the way are chosen from the possible states
?. A transition (or action) ? ? ? advances the
decoder from state to state, where the transition ?i
changes the state from pii to pii+1. The sequence
of states {pi0 . . . pii, pii+1 . . . pif} can be mapped to
an output (the model?s prediction.) The choice of
action ? is given by a learning algorithm, such as
a maximum-entropy classifier, support vector ma-
chine or Perceptron, trained on labeled data. Given
the previous k actions up to pii, the classifier g :
? ? ?k ? R|?| assigns a score to each possi-
ble action, which we can interpret as a probability:
pg(?i|pii, ?i?1?i?2 . . . ?i?k). These actions are ap-
plied to transition to new states pii+1. We note that
state definitions can encode the k previous actions,
which simplifies the probability to pg(?i|pii). The
177
N -best list Hill climbing neighborhood
(1) AL GORE HAS PROMISED THAT HE WOULD ENDORSE A CANDIDATE
(2) TO AL GORE HAS PROMISED THAT HE WOULD ENDORSE A CANDIDATE
(3) AL GORE HAS PROMISE THAT HE WOULD ENDORSE A CANDIDATE
(4) SO AL GORE HAS PROMISED THAT HE WOULD ENDORSE A CANDIDATE (1) YEAH FIFTY CENT GALLON NOMINATION WHICH WAS GREAT
(5) IT?S AL GORE HAS PROMISED THAT HE WOULD ENDORSE A CANDIDATE (2) YEAH FIFTY CENT A GALLON NOMINATION WHICH WAS GREAT
(6) AL GORE HAS PROMISED HE WOULD ENDORSE A CANDIDATE (3) YEAH FIFTY CENT GOT A NOMINATION WHICH WAS GREAT
(7) AL GORE HAS PROMISED THAT HE WOULD ENDORSE THE CANDIDATE
(8) SAID AL GORE HAS PROMISED THAT HE WOULD ENDORSE A CANDIDATE
(9) AL GORE HAS PROMISED THAT HE WOULD ENDORSE A CANDIDATE FOR
(10) AL GORE HIS PROMISE THAT HE WOULD ENDORSE A CANDIDATE
Figure 1: Example of repeated substructures in candidate hypotheses.
score of the new state is then
p(pii+1) = pg(?i|pii) ? p(pii) (1)
Classification decisions require a feature represen-
tation of pii, which is provided by feature functions
f : ?? Y , that map states to features. Features are
conjoined with actions for multi-class classification,
so pg(?i|pii) = pg(f(pi) ? ?i), where ? is a conjunc-
tion operation. In this way, states can be summarized
by features.
Equivalent states are defined as two states pi and
pi? with an identical feature representation:
pi ? pi? iff f(pi) = f(pi?)
If two states are equivalent, then g imposes the same
distribution over actions. We can benefit from this
substructure redundancy, both within and between
hypotheses, by saving these distributions in mem-
ory, sharing a distribution computed just once across
equivalent states. A similar idea of equivalent states
is used by Huang and Sagae (2010), except they use
equivalence to facilitate dynamic programming for
shift-reduce parsing, whereas we generalize it for
improving the processing time of similar hypotheses
in general models. Following Huang and Sagae, we
define kernel features as the smallest set of atomic
features f?(pi) such that,
f?(pi) = f?(pi?) ? pi ? pi?. (2)
Equivalent distributions are stored in a hash table
H : ?? ??R; the hash keys are the states and the
values are distributions2 over actions: {?, pg(?|pi)}.
2For pure greedy search (deterministic search) we need only
retain the best action, since the distribution is only used in prob-
abilistic search, such as beam search or best-first algorithms.
H caches equivalent states in a hypothesis set and re-
sets for each new utterance. For each state, we first
check H for equivalent states before computing the
action distribution; each cache hit reduces decod-
ing time. Distributing hypotheses wi across differ-
ent CPU threads is another way to obtain speedups,
and we can still benefit from substructure sharing by
storing H in shared memory.
We use h(pi) = ?|f?(pi)|i=1 int(f?i(pi)) as the hash
function, where int(f?i(pi)) is an integer mapping of
the ith kernel feature. For integer typed features
the mapping is trivial, for string typed features (e.g.
a POS tag identity) we use a mapping of the cor-
responding vocabulary to integers. We empirically
found that this hash function is very effective and
yielded very few collisions.
To apply substructure sharing to a transition based
model, we need only define the set of states ? (in-
cluding pi0 and pif ), actions ? and kernel feature
functions f? . The resulting speedup depends on the
amount of substructure duplication among the hy-
potheses, which we will show is significant for ASR
lattice rescoring. Note that our algorithm is not an
approximation; we obtain the same output {sji} as
we would without any sharing. We now apply this
algorithm to dependency parsing and POS tagging.
3.2 Dependency Parsing
We use the best-first probabilistic shift-reduce de-
pendency parser of Sagae and Tsujii (2007), a
transition-based parser (Ku?bler et al, 2009) with a
MaxEnt classifier. Dependency trees are built by
processing the words left-to-right and the classifier
assigns a distribution over the actions at each step.
States are defined as pi = {S,Q}: S is a stack of
178
Kernel features f?(pi) for state pi = {S,Q}
S = s0, s1, . . . & Q = q0, q1, . . .
(1) s0.w s0.t s0.r (5) ts0?1
s0.lch.t s0.lch.r ts1+1
s0.rch.t s0.rch.r
(2) s1.w s1.t s1.r (6) dist(s0, s1)
s1.lch.t s1.lch.r dist(q0, s0)
s1.rch.t s1.rch.r
(3) s2.w s2.t s2.r
(4) q0.w q0.t (7) s0.nch
q1.w q1.t s1.nch
q2.w
Table 1: Kernel features for defining parser states. si.w
denotes the head-word in a subtree and t its POS tag.
si.lch and si.rch are the leftmost and rightmost children
of a subtree. si.r is the dependency label that relates a
subtree head-word to its dependent. si.nch is the number
of children of a subtree. qi.w and qi.t are the word and
its POS tag in the queue. dist(s0,s1) is the linear distance
between the head-words of s0 and s1.
subtrees s0, s1, . . . (s0 is the top tree) and Q are
words in the input word sequence. The initial state is
pi0 = {?, {w0, w1, . . .}}, and final states occur when
Q is empty and S contains a single tree (the output).
? is determined by the set of dependency labels
r ? R and one of three transition types:
? Shift: remove the head of Q (wj) and place it on
the top of S as a singleton tree (only wj .)
? Reduce-Leftr: replace the top two trees in S (s0
and s1) with a tree formed by making the root of
s1 a dependent of the root of s0 with label r.
? Reduce-Rightr: same as Reduce-Leftr except re-
verses s0 and s1.
Table 1 shows the kernel features used in our de-
pendency parser. See Sagae and Tsujii (2007) for a
complete list of features.
Goldberg and Elhadad (2010) observed that pars-
ing time is dominated by feature extraction and
score calculation. Substructure sharing reduces
these steps for equivalent states, which are persis-
tent throughout a candidate set. Note that there are
far fewer kernel features than total features, hence
the hash function calculation is very fast.
We summarize substructure sharing for depen-
dency parsing in Algorithm 1. We extend the def-
inition of states to be {S,Q, p} where p denotes the
score of the state: the probability of the action se-
quence that resulted in the current state. Also, fol-
Algorithm 1 Best-first shift-reduce dependency parsing
w ? input hypothesis
S0 = ?, Q0 = w, p0 = 1
pi0 ? {S0, Q0, p0} [initial state]
H ?Hash table (?? ?? R)
Heap? Heap for prioritizing states and performing best-first search
Heap.push(pi0) [initialize the heap]
while Heap 6= ? do
picurrent ?Heap.pop() [the best state so far]
if picurrent = pif [if final state]
return picurrent [terminate if final state]
else ifH.find(picurrent)
ActList? H[picurrent] [retrieve action list from the hash table]
else [need to construct action list]
for all ? ? ? [for all actions]
p? ? pg(?|picurrent) [action score]
ActList.insert({?, p?})
H.insert(picurrent,ActList) [Store the action list into hash table]
end if
for all {?, p?} ? ActList [compute new states]
pinew ? picurrent ? ?
Heap.push(pinew) [push to the heap]
end while
lowing Sagae and Tsujii (2007) a heap is used to
maintain states prioritized by their scores, for apply-
ing the best-first strategy. For each step, a state from
the top of the heap is considered and all actions (and
scores) are either retrieved from H or computed us-
ing g.3 We use pinew ? picurrent ? ? to denote the
operation of extending a state by an action ? ? ?4.
3.3 Part of Speech Tagging
We use the part of speech (POS) tagger of Tsuruoka
et al (2011), a transition based model with a Per-
ceptron and a lookahead heuristic process. The tag-
ger processes w left to right. States are defined as
pii = {ci,w}: a sequence of assigned tags up to wi
(ci = t1t2 . . . ti?1) and the word sequence w. ? is
defined simply as the set of possible POS tags (T )
that can be applied. The final state is reached once
all the positions are tagged. For f we use the features
of Tsuruoka et al (2011). The kernel features are
f?(pii) = {ti?2, ti?1, wi?2, wi?1, wi, wi+1, wi+2}.
While the tagger extracts prefix and suffix features,
it suffices to look at wi for determining state equiv-
alence. The tagger is deterministic (greedy) in that
it only considers the best tag at each step, so we do
not store scores. However, this tagger uses a depth-
3 Sagae and Tsujii (2007) use a beam strategy to increase
speed. Search space pruning is achieved by filtering heap states
for probability greater than 1b the probability of the most likely
state in the heap with the same number of actions. We use b =
100 for our experiments.
4We note that while we have demonstrated substructure
sharing for dependency parsing, the same improvements can
be made to a shift-reduce constituent parser (Sagae and Lavie,
2006).
179
t2t1 ti 2 ti 1
t1i
t2i
t|T |i t|T |i+1
t1i+1
t2i+1
w1 w2 wi 1wi 2 wi wi+1 wi+2 wi+3? ? ?
? ? ?
lookahead search
Figure 2: POS tagger with lookahead search of d=1. At
wi the search considers the current state and next state.
first search lookahead procedure to select the best
action at each step, which considers future decisions
up to depth d5. An example for d = 1 is shown
in Figure 2. Using d = 1 for the lookahead search
strategy, we modify the kernel features since the de-
cision forwi is affected by the state pii+1. The kernel
features in position i should be f?(pii) ? f?(pii+1):
f?(pii) =
{ti?2, ti?1, wi?2, wi?1, wi, wi+1, wi+2, wi+3}
4 Up-Training
While we have fast decoding algorithms for the pars-
ing and tagging, the simpler underlying models can
lead to worse performance. Using more complex
models with higher accuracy is impractical because
they are slow. Instead, we seek to improve the accu-
racy of our fast tools.
To achieve this goal we use up-training, in which
a more complex model is used to improve the accu-
racy of a simpler model. We are given two mod-
els, M1 and M2, as well as a large collection of
unlabeled text. Model M1 is slow but very accu-
rate while M2 is fast but obtains lower accuracy.
Up-training applies M1 to tag the unlabeled data,
which is then used as training data for M2. Like
self-training, a model is retrained on automatic out-
put, but here the output comes form a more accurate
model. Petrov et al (2010) used up-training as a
domain adaptation technique: a constituent parser ?
which is more robust to domain changes ? was used
to label a new domain, and a fast dependency parser
5 Tsuruoka et al (2011) shows that the lookahead search
improves the performance of the local ?history-based? models
for different NLP tasks
was trained on the automatically labeled data. We
use a similar idea where our goal is to recover the
accuracy lost from using simpler models. Note that
while up-training uses two models, it differs from
co-training since we care about improving only one
model (M2). Additionally, the models can vary in
different ways. For example, they could be the same
algorithm with different pruning methods, which
can lead to faster but less accurate models.
We apply up-training to improve the accuracy of
both our fast POS tagger and dependency parser. We
parse a large corpus of text with a very accurate but
very slow constituent parser and use the resulting
data to up-train our tools. We will demonstrate em-
pirically that up-training improves these fast models
to yield better WER results.
5 Related Work
The idea of efficiently processing a hypothesis set is
similar to ?lattice-parsing?, in which a parser con-
sider an entire lattice at once (Hall, 2005; Chep-
palier et al, 1999). These methods typically con-
strain the parsing space using heuristics, which are
often model specific. In other words, they search in
the joint space of word sequences present in the lat-
tice and their syntactic analyses; they are not guaran-
teed to produce a syntactic analysis for all hypothe-
ses. In contrast, substructure sharing is a general
purpose method that we have applied to two differ-
ent algorithms. The output is identical to processing
each hypothesis separately and output is generated
for each hypothesis. Hall (Hall, 2005) uses a lattice
parsing strategy which aims to compute the marginal
probabilities of all word sequences in the lattice by
summing over syntactic analyses of each word se-
quence. The parser sums over multiple parses of a
word sequence implicitly. The lattice parser there-
fore, is itself a language model. In contrast, our
tools are completely separated from the ASR sys-
tem, which allows the system to create whatever fea-
tures are needed. This independence means our tools
are useful for other tasks, such as machine transla-
tion. These differences make substructure sharing a
more attractive option for efficient algorithms.
While Huang and Sagae (2010) use the notion of
?equivalent states?, they do so for dynamic program-
ming in a shift-reduce parser to broaden the search
space. In contrast, we use the idea to identify sub-
180
structures across inputs, where our goal is efficient
parsing in general. Additionally, we extend the defi-
nition of equivalent states to general transition based
structured prediction models, and demonstrate ap-
plications beyond parsing as well as the novel setting
of hypothesis set parsing.
6 Experiments
Our ASR system is based on the 2007 IBM
Speech transcription system for the GALE Distilla-
tion Go/No-go Evaluation (Chen et al, 2006) with
state of the art discriminative acoustic models. See
Table 2 for a data summary. We use a modi-
fied Kneser-Ney (KN) backoff 4-gram baseline LM.
Word-lattices for discriminative training and rescor-
ing come from this baseline ASR system.6 The long-
span discriminative LM?s baseline feature weight
(?0) is tuned on dev data and hill climbing (Rastrow
et al, 2011a) is used for training and rescoring. The
dependency parser and POS tagger are trained on su-
pervised data and up-trained on data labeled by the
CKY-style bottom-up constituent parser of Huang et
al. (2010), a state of the art broadcast news (BN)
parser, with phrase structures converted to labeled
dependencies by the Stanford converter.
While accurate, the parser has a huge grammar
(32GB) from using products of latent variable gram-
mars and requires O(l3) time to parse a sentence of
length l. Therefore, we could not use the constituent
parser for ASR rescoring since utterances can be
very long, although the shorter up-training text data
was not a problem.7 We evaluate both unlabeled
(UAS) and labeled dependency accuracy (LAS).
6.1 Results
Before we demonstrate the speed of our models, we
show that up-training can produce accurate and fast
models. Figure 3 shows improvements to parser ac-
curacy through up-training for different amount of
(randomly selected) data, where the last column in-
dicates constituent parser score (91.4% UAS). We
use the POS tagger to generate tags for depen-
dency training to match the test setting. While
there is a large difference between the constituent
and dependency parser without up-training (91.4%
6For training a 3-gram LM is used to increase confusions.
7Speech utterances are longer as they are not as effectively
sentence segmented as text.
84.0	 ?
85.0	 ?
86.0	 ?
87.0	 ?
88.0	 ?
89.0	 ?
90.0	 ?
91.0	 ?
92.0	 ?
0M	 ? 2.5M	 ? 5M	 ? 10M	 ? 20M	 ? 40M	 ? Cons?tuent	 ?Parser	 ?
Accu
racy
	 ?(%)
	 ?
Amount	 ?of	 ?Added	 ?Uptraining	 ?Data	 ?
Unlabeled	 ?A?achment	 ?Score	 ?
Labeled	 ?A?achment	 ?Score	 ?
Figure 3: Up-training results for dependency parsing for
varying amounts of data (number of words.) The first
column is the dependency parser with supervised training
only and the last column is the constituent parser (after
converting to dependency trees.)
vs. 86.2% UAS), up-training can cut the differ-
ence by 44% to 88.5%, and improvements saturate
around 40m words (about 2m sentences.)8 The de-
pendency parser remains much smaller and faster;
the up-trained dependency model is 700MB with
6m features compared with 32GB for constituency
model. Up-training improves the POS tagger?s accu-
racy from 95.9% to 97%, when trained on the POS
tags produced by the constituent parser, which has a
tagging accuracy of 97.2% on BN.
We train the syntactic discriminative LM, with
head-word and POS tag features, using the faster
parser and tagger and then rescore the ASR hypothe-
ses. Table 3 shows the decoding speedups as well as
the WER reductions compared to the baseline LM.
Note that up-training improvements lead to WER re-
ductions. Detailed speedups on substructure sharing
are shown in Table 4; the POS tagger achieves a 5.3
times speedup, and the parser a 5.7 speedup with-
out changing the output. We also observed speedups
during training (not shown due to space.)
The above results are for the already fast hill
climbing decoding, but substructure sharing can also
be used for N -best list rescoring. Figure 4 (logarith-
mic scale) illustrates the time for the parser and tag-
ger to processN -best lists of varying size, with more
substantial speedups for larger lists. For example,
for N=100 (a typical setting) the parsing time re-
8Better performance is due to the exact CKY-style ? com-
pared with best-first and beam? search and that the constituent
parser uses the product of huge self-trained grammars.
181
Usage Data Size
Acoustic model training Hub4 acoustic train 153k uttr, 400 hrs
Baseline LM training: modified KN 4-gram TDT4 closed captions+EARS BN03 closed caption 193m words
Disc. LM training: long-span w/hill climbing Hub4 (length <50) 115k uttr, 2.6m words
Baseline feature (?0) tuning dev04f BN data 2.5 hrs
Supervised training: dep. parser, POS tagger Ontonotes BN treebank+ WSJ Penn treebank 1.3m words, 59k sent.
Supervised training: constituent parser Ontonotes BN treebank + WSJ Penn treebank 1.3m words, 59k sent.
Up-training: dependency parser, POS tagger TDT4 closed captions+EARS BN03 closed caption 193m words available
Evaluation: up-training BN treebank test (following Huang et al (2010)) 20k words, 1.1k sent.
Evaluation: ASR transcription rt04 BN evaluation 4 hrs, 45k words
Table 2: A summary of the data for training and evaluation. The Ontonotes corpus is from Weischedel et al (2008).
10	 ?
100	 ?
1000	 ?
10000	 ?
100000	 ?
1000000	 ?
1	 ? 10	 ? 100	 ? 1000	 ?
Elap
sed
	 ?Tim
e	 ?(s
ec)	 ?
N-??best	 ?Size	 ?(N)	 ?
No	 ?Sharing	 ?
Substructure	 ?Sharing	 ?
(a)
1	 ?
10	 ?
100	 ?
1000	 ?
10000	 ?
1	 ? 10	 ? 100	 ? 1000	 ?
Elap
sed
	 ?Tim
e	 ?(s
ec)	 ?
N-??best	 ?Size	 ?(N)	 ?
No	 ?Sharing	 ?
Substructure	 ?Sharing	 ?
(b)
Figure 4: Elapsed time for (a) parsing and (b) POS tagging the N -best lists with and without substructure sharing.
Substr. Share (sec)
LM WER No Yes
Baseline 4-gram 15.1 - -
Syntactic LM 14.8
8,658 1,648
+ up-train 14.6
Table 3: Speedups and WER for hill climbing rescor-
ing. Substructure sharing yields a 5.3 times speedup. The
times for with and without up-training are nearly identi-
cal, so we include only one set for clarity. Time spent
is dominated by the parser, so the faster parser accounts
for much of the overall speedup. Timing information in-
cludes neighborhood generation and LM rescoring, so it
is more than the sum of the times in Table 4.
duces from about 20,000 seconds to 2,700 seconds,
about 7.4 times as fast.
7 Conclusion
The computational complexity of accurate syntac-
tic processing can make structured language models
impractical for applications such as ASR that require
scoring hundreds of hypotheses per input. We have
Substr. Share Speedup
No Yes
Parser 8,237.2 1,439.5 5.7
POS tagger 213.3 40.1 5.3
Table 4: Time in seconds for the parser and POS tagger
to process hypotheses during hill climbing rescoring.
presented substructure sharing, a general framework
that greatly improves the speed of syntactic tools
that process candidate hypotheses. Furthermore, we
achieve improved performance through up-training.
The result is a large speedup in rescoring time, even
on top of the already fast hill climbing framework,
and reductions in WER from up-training. Our re-
sults make long-span syntactic LMs practical for
real-time ASR, and can potentially impact machine
translation decoding as well.
Acknowledgments
Thanks to Kenji Sagae for sharing his shift-reduce
dependency parser and the anonymous reviewers for
helpful comments.
182
References
C. Chelba and F. Jelinek. 2000. Structured lan-
guage modeling. Computer Speech and Language,
14(4):283?332.
S. Chen, B. Kingsbury, L. Mangu, D. Povey, G. Saon,
H. Soltau, and G. Zweig. 2006. Advances in speech
transcription at IBM under the DARPA EARS pro-
gram. IEEE Transactions on Audio, Speech and Lan-
guage Processing, pages 1596?1608.
J. Cheppalier, M. Rajman, R. Aragues, and A. Rozen-
knop. 1999. Lattice parsing for speech recognition.
In Sixth Conference sur le Traitement Automatique du
Langage Naturel (TANL?99).
M Collins, B Roark, and M Saraclar. 2005. Discrimina-
tive syntactic language modeling for speech recogni-
tion. In ACL.
Denis Filimonov and Mary Harper. 2009. A joint
language model with fine-grain syntactic tags. In
EMNLP.
Yoav Goldberg and Michael Elhadad. 2010. An Ef-
ficient Algorithm for Easy-First Non-Directional De-
pendency Parsing. In Proc. HLT-NAACL, number
June, pages 742?750.
Keith B Hall. 2005. Best-first word-lattice parsing:
techniques for integrated syntactic language modeling.
Ph.D. thesis, Brown University.
L. Huang and K. Sagae. 2010. Dynamic Programming
for Linear-Time Incremental Parsing. In Proceedings
of ACL.
Zhongqiang Huang, Mary Harper, and Slav Petrov. 2010.
Self-training with Products of Latent Variable Gram-
mars. In Proc. EMNLP, number October, pages 12?
22.
S. Khudanpur and J. Wu. 2000. Maximum entropy tech-
niques for exploiting syntactic, semantic and colloca-
tional dependencies in language modeling. Computer
Speech and Language, pages 355?372.
S. Ku?bler, R. McDonald, and J. Nivre. 2009. Depen-
dency parsing. Synthesis Lectures on Human Lan-
guage Technologies, 2(1):1?127.
Hong-Kwang Jeff Kuo, Eric Fosler-Lussier, Hui Jiang,
and Chin-Hui Lee. 2002. Discriminative training of
language models for speech recognition. In ICASSP.
H. K. J. Kuo, L. Mangu, A. Emami, I. Zitouni, and
L. Young-Suk. 2009. Syntactic features for Arabic
speech recognition. In Proc. ASRU.
Slav Petrov, Pi-Chuan Chang, Michael Ringgaard, and
Hiyan Alshawi. 2010. Uptraining for accurate deter-
ministic question parsing. In Proceedings of the 2010
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 705?713, Cambridge, MA,
October. Association for Computational Linguistics.
Ariya Rastrow, Mark Dredze, and Sanjeev Khudanpur.
2011a. Efficient discrimnative training of long-span
language models. In IEEE Workshop on Automatic
Speech Recognition and Understanding (ASRU).
Ariya Rastrow, Markus Dreyer, Abhinav Sethy, San-
jeev Khudanpur, Bhuvana Ramabhadran, and Mark
Dredze. 2011b. Hill climbing on speech lattices : A
new rescoring framework. In ICASSP.
Brian Roark, Murat Saraclar, and Michael Collins. 2007.
Discriminative n-gram language modeling. Computer
Speech & Language, 21(2).
K. Sagae and A. Lavie. 2006. A best-first probabilis-
tic shift-reduce parser. In Proc. ACL, pages 691?698.
Association for Computational Linguistics.
K. Sagae and J. Tsujii. 2007. Dependency parsing
and domain adaptation with LR models and parser en-
sembles. In Proc. EMNLP-CoNLL, volume 7, pages
1044?1050.
Yoshimasa Tsuruoka, Yusuke Miyao, and Jun?ichi
Kazama. 2011. Learning with Lookahead :
Can History-Based Models Rival Globally Optimized
Models ? In Proc. CoNLL, number June, pages 238?
246.
Ralph Weischedel, Sameer Pradhan, Lance Ramshaw,
Martha Palmer, Nianwen Xue, Mitchell Marcus, Ann
Taylor, Craig Greenberg, Eduard Hovy, Robert Belvin,
and Ann Houston, 2008. OntoNotes Release 2.0. Lin-
guistic Data Consortium, Philadelphia.
183
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 666?675,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Online Learning in Tensor Space
Yuan Cao Sanjeev Khudanpur
Center for Language & Speech Processing and Human Language Technology Center of Excellence
The Johns Hopkins University
Baltimore, MD, USA, 21218
{yuan.cao, khudanpur}@jhu.edu
Abstract
We propose an online learning algorithm
based on tensor-space models. A tensor-
space model represents data in a compact
way, and via rank-1 approximation the
weight tensor can be made highly struc-
tured, resulting in a significantly smaller
number of free parameters to be estimated
than in comparable vector-space models.
This regularizes the model complexity and
makes the tensor model highly effective in
situations where a large feature set is de-
fined but very limited resources are avail-
able for training. We apply with the pro-
posed algorithm to a parsing task, and
show that even with very little training
data the learning algorithm based on a ten-
sor model performs well, and gives signif-
icantly better results than standard learn-
ing algorithms based on traditional vector-
space models.
1 Introduction
Many NLP applications use models that try to in-
corporate a large number of linguistic features so
that as much human knowledge of language can
be brought to bear on the (prediction) task as pos-
sible. This also makes training the model param-
eters a challenging problem, since the amount of
labeled training data is usually small compared to
the size of feature sets: the feature weights cannot
be estimated reliably.
Most traditional models are linear models, in
the sense that both the features of the data and
model parameters are represented as vectors in a
vector space. Many learning algorithms applied
to NLP problems, such as the Perceptron (Collins,
2002), MIRA (Crammer et al, 2006; McDonald
et al, 2005; Chiang et al, 2008), PRO (Hop-
kins and May, 2011), RAMPION (Gimpel and
Smith, 2012) etc., are based on vector-space mod-
els. Such models require learning individual fea-
ture weights directly, so that the number of param-
eters to be estimated is identical to the size of the
feature set. When millions of features are used but
the amount of labeled data is limited, it can be dif-
ficult to precisely estimate each feature weight.
In this paper, we shift the model from vector-
space to tensor-space. Data can be represented
in a compact and structured way using tensors as
containers. Tensor representations have been ap-
plied to computer vision problems (Hazan et al,
2005; Shashua and Hazan, 2005) and information
retrieval (Cai et al, 2006a) a long time ago. More
recently, it has also been applied to parsing (Cohen
and Collins, 2012; Cohen and Satta, 2013) and se-
mantic analysis (Van de Cruys et al, 2013). A
linear tensor model represents both features and
weights in tensor-space, hence the weight tensor
can be factorized and approximated by a linear
sum of rank-1 tensors. This low-rank approxi-
mation imposes structural constraints on the fea-
ture weights and can be regarded as a form of
regularization. With this representation, we no
longer need to estimate individual feature weights
directly but only a small number of ?bases? in-
stead. This property makes the the tensor model
very effective when training a large number of fea-
ture weights in a low-resource environment. On
the other hand, tensor models have many more de-
grees of ?design freedom? than vector space mod-
els. While this makes them very flexible, it also
creates much difficulty in designing an optimal
tensor structure for a given training set.
We give detailed description of the tensor space
666
model in Section 2. Several issues that come
with the tensor model construction are addressed
in Section 3. A tensor weight learning algorithm
is then proposed in 4. Finally we give our exper-
imental results on a parsing task and analysis in
Section 5.
2 Tensor Space Representation
Most of the learning algorithms for NLP problems
are based on vector space models, which represent
data as vectors ? ? R
n
, and try to learn feature
weight vectors w ? R
n
such that a linear model
y = w ? ? is able to discriminate between, say,
good and bad hypotheses. While this is a natural
way of representing data, it is not the only choice.
Below, we reformulate the model from vector to
tensor space.
2.1 Tensor Space Model
A tensor is a multidimensional array, and is a gen-
eralization of commonly used algebraic objects
such as vectors and matrices. Specifically, a vec-
tor is a 1
st
order tensor, a matrix is a 2
nd
order
tensor, and data organized as a rectangular cuboid
is a 3
rd
order tensor etc. In general, a D
th
order
tensor is represented as T ? R
n
1
?n
2
?...n
D
, and an
entry in T is denoted by T
i
1
,i
2
,...,i
D
. Different di-
mensions of a tensor 1, 2, . . . , D are named modes
of the tensor.
Using a D
th
order tensor as container, we can
assign each feature of the task a D-dimensional
index in the tensor and represent the data as ten-
sors. Of course, shifting from a vector to a tensor
representation entails several additional degrees of
freedom, e.g., the order D of the tensor and the
sizes {n
d
}
D
d=1
of the modes, which must be ad-
dressed when selecting a tensor model. This will
be done in Section 3.
2.2 Tensor Decomposition
Just as a matrix can be decomposed as a lin-
ear combination of several rank-1 matrices via
SVD, tensors also admit decompositions
1
into lin-
ear combinations of ?rank-1? tensors. A D
th
or-
der tensor A ? R
n
1
?n
2
?...n
D
is rank-1 if it can be
1
The form of tensor decomposition defined here is named
as CANDECOMP/PARAFAC(CP) decomposition (Kolda
and Bader, 2009). Another popular form of tensor decom-
position is called Tucker decomposition, which decomposes
a tensor into a core tensor multiplied by a matrix along each
mode. We focus only on the CP decomposition in this paper.
written as the outer product of D vectors, i.e.
A = a
1
? a
2
?, . . . ,?a
D
,
where a
i
? R
n
d
, 1 ? d ? D. A D
th
order tensor
T ? R
n
1
?n
2
?...n
D
can be factorized into a sum of
component rank-1 tensors as
T =
R
?
r=1
A
r
=
R
?
r=1
a
1
r
? a
2
r
?, . . . ,?a
D
r
where R, called the rank of the tensor, is the mini-
mum number of rank-1 tensors whose sum equals
T . Via decomposition, one may approximate a
tensor by the sum of H major rank-1 tensors with
H ? R.
2.3 Linear Tensor Model
In tensor space, a linear model may be written (ig-
noring a bias term) as
f(W ) = W ??,
where ? ? R
n
1
?n
2
?...n
D
is the feature tensor, W
is the corresponding weight tensor, and ? denotes
the Hadamard product. If W is further decom-
posed as the sum of H major component rank-1
tensors, i.e. W ?
?
H
h=1
w
1
h
?w
2
h
?, . . . ,?w
D
h
,
then
f(w
1
1
, . . . ,w
D
1
, . . . ,w
1
h
, . . . ,w
D
h
)
=
H
?
h=1
??
1
w
1
h
?
2
w
2
h
. . .?
D
w
D
h
, (1)
where ?
l
is the l-mode product operator between
a D
th
order tensor T and a vector a of dimension
n
d
, yielding a (D ? 1)
th
order tensor such that
(T ?
l
a)
i
1
,...,i
l?1
,i
l+1
,...,i
D
=
n
d
?
i
l
=1
T
i
1
,...,i
l?1
,i
l
,i
l+1
,...,i
D
? a
i
l
.
The linear tensor model is illustrated in Figure 1.
2.4 Why Learning in Tensor Space?
So what is the advantage of learning with a ten-
sor model instead of a vector model? Consider the
case where we have defined 1,000,000 features for
our task. A vector space linear model requires es-
timating 1,000,000 free parameters. However if
we use a 2
nd
order tensor model, organize the fea-
tures into a 1000 ? 1000 matrix ?, and use just
667
Figure 1: A 3
rd
order linear tensor model. The
feature weight tensor W can be decomposed as
the sum of a sequence of rank-1 component ten-
sors.
one rank-1 matrix to approximate the weight ten-
sor, then the linear model becomes
f(w
1
,w
2
) = w
T
1
?w
2
,
where w
1
,w
2
? R
1000
. That is to say, now we
only need to estimate 2000 parameters!
In general, if V features are defined for a learn-
ing problem, and we (i) organize the feature set
as a tensor ? ? R
n
1
?n
2
?...n
D
and (ii) use H
component rank-1 tensors to approximate the cor-
responding target weight tensor. Then the total
number of parameters to be learned for this ten-
sor model is H
?
D
d=1
n
d
, which is usually much
smaller than V =
?
D
d=1
n
d
for a traditional vec-
tor space model. Therefore we expect the tensor
model to be more effective in a low-resource train-
ing environment.
Specifically, a vector space model assumes each
feature weight to be a ?free? parameter, and es-
timating them reliably could therefore be hard
when training data are not sufficient or the fea-
ture set is huge. By contrast, a linear tensor model
only needs to learn H
?
D
d=1
n
d
?bases? of the m
feature weights instead of individual weights di-
rectly. The weight corresponding to the feature
?
i
1
,i
2
,...,i
D
in the tensor model is expressed as
w
i
1
,i
2
,...,i
D
=
H
?
h=1
w
1
h,i
1
w
2
h,i
2
. . . w
D
h,i
D
, (2)
where w
j
h,i
j
is the i
th
j
element in the vector w
j
h
.
In other words, a true feature weight is now ap-
proximated by a set of bases. This reminds us
of the well-known low-rank matrix approximation
of images via SVD, and we are applying similar
techniques to approximate target feature weights,
which is made possible only after we shift from
vector to tensor space models.
This approximation can be treated as a form of
model regularization, since the weight tensor is
represented in a constrained form and made highly
structured via the rank-1 tensor approximation. Of
course, as we reduce the model complexity, e.g. by
choosing a smaller and smaller H , the model?s ex-
pressive ability is weakened at the same time. We
will elaborate on this point in Section 3.1.
3 Tensor Model Construction
To apply a tensor model, we first need to con-
vert the feature vector into a tensor ?. Once the
structure of ? is determined, the structure of W
is fixed as well. As mentioned in Section 2.1, a
tensor model has many more degrees of ?design
freedom? than a vector model, which makes the
problem of finding a good tensor structure a non-
trivial one.
3.1 Tensor Order
The order of a tensor affects the model in two
ways: the expressiveness of the model and the
number of parameters to be estimated. We assume
H = 1 in the analysis below, noting that one can
always add as many rank-1 component tensors as
needed to approximate a tensor with arbitrary pre-
cision.
Obviously, the 1
st
order tensor (vector) model
is the most expressive, since it is structureless and
any arbitrary set of numbers can always be repre-
sented exactly as a vector. The 2
nd
order rank-1
tensor (rank-1 matrix) is less expressive because
not every set of numbers can be organized into
a rank-1 matrix. In general, a D
th
order rank-1
tensor is more expressive than a (D + 1)
th
order
rank-1 tensor, as a lower-order tensor imposes less
structural constraints on the set of numbers it can
express. We formally state this fact as follows:
Theorem 1. A set of real numbers that can be rep-
resented by a (D + 1)
th
order tensor Q can also
be represented by a D
th
order tensor P , provided
P andQ have the same volume. But the reverse is
not true.
Proof. See appendix.
On the other hand, tensor order also affects the
number of parameters to be trained. Assuming
that a D
th
order has equal size on each mode (we
will elaborate on this point in Section 3.2) and
the volume (number of entries) of the tensor is
fixed as V , then the total number of parameters
668
of the model is DV
1
D
. This is a convex func-
tion of D, and the minimum
2
is reached at either
D
?
= blnV c or D
?
= dlnV e.
Therefore, as D increases from 1 to D
?
, we
lose more and more of the expressive power of the
model but reduce the number of parameters to be
trained. However it would be a bad idea to choose
aD beyondD
?
. The optimal tensor order depends
on the nature of the actual problem, and we tune
this hyper-parameter on a held-out set.
3.2 Mode Size
The size n
d
of each tensor mode, d = 1, . . . , D,
determines the structure of feature weights a ten-
sor model can precisely represent, as well as the
number of parameters to estimate (we also as-
sume H = 1 in the analysis below). For exam-
ple, if the tensor order is 2 and the volume V is
12, then we can either choose n
1
= 3, n
2
= 4
or n
1
= 2, n
2
= 6. For n
1
= 3, n
2
= 4, the
numbers that can be precisely represented are di-
vided into 3 groups, each having 4 numbers, that
are scaled versions of one another. Similarly for
n
1
= 2, n
2
= 6, the numbers can be divided into
2 groups with different scales. Obviously, the two
possible choices of (n
1
, n
2
) also lead to different
numbers of free parameters (7 vs. 8).
GivenD and V , there are many possible combi-
nations of n
d
, d = 1, . . . , D, and the optimal com-
bination should indeed be determined by the struc-
ture of target features weights. However it is hard
to know the structure of target feature weights be-
fore learning, and it would be impractical to try ev-
ery possible combination of mode sizes, therefore
we choose the criterion of determining the mode
sizes as minimization of the total number of pa-
rameters, namely we solve the problem:
min
n
1
,...,n
D
D
?
d=1
n
d
s.t
D
?
d=1
n
d
= V
The optimal solution is reached when n
1
= n
2
=
. . . = n
D
= V
1
D
. Of course it is not guaran-
teed that V
1
D
is an integer, therefore we choose
n
d
= bV
1
D
c or dV
1
D
e, d = 1, . . . , D such that
?
D
d=1
n
d
? V and
[
?
D
d=1
n
d
]
? V is minimized.
The
[
?
D
d=1
n
d
]
? V extra entries of the tensor
correspond to no features and are used just for
2
The optimal integer solution can be determined simply
by comparing the two function values.
padding. Since for each n
d
there are only two
possible values to choose, we can simply enumer-
ate all the possible 2
D
(which is usually a small
number) combinations of values and pick the one
that matches the conditions given above. This way
n
1
, . . . , n
D
are fully determined.
Here we are only following the principle of min-
imizing the parameter number. While this strat-
egy might work well with small amount of train-
ing data, it is not guaranteed to be the best strategy
in all cases, especially when more data is avail-
able we might want to increase the number of pa-
rameters, making the model more complex so that
the data can be more precisely modeled. Ideally
the mode size needs to be adaptive to the amount
of training data as well as the property of target
weights. A theoretically guaranteed optimal ap-
proach to determining the mode sizes remains an
open problem, and will be explored in our future
work.
3.3 Number of Rank-1 Tensors
The impact of using H > 1 rank-1 tensors is ob-
vious: a larger H increases the model complexity
and makes the model more expressive, since we
are able to approximate target weight tensor with
smaller error. As a trade-off, the number of param-
eters and training complexity will be increased. To
find out the optimal value of H for a given prob-
lem, we tune this hyper-parameter too on a held-
out set.
3.4 Vector to Tensor Mapping
Finally, we need to find a way to map the orig-
inal feature vector to a tensor, i.e. to associate
each feature with an index in the tensor. Assum-
ing the tensor volume V is the same as the number
of features, then there are in all V ! ways of map-
ping, which is an intractable number of possibili-
ties even for modest sized feature sets, making it
impractical to carry out a brute force search. How-
ever while we are doing the mapping, we hope to
arrange the features in a way such that the corre-
sponding target weight tensor has approximately a
low-rank structure, this way it can be well approx-
imated by very few component rank-1 tensors.
Unfortunately we have no knowledge about the
target weights in advance, since that is what we
need to learn after all. As a way out, we first run
a simple vector-model based learning algorithm
(say the Perceptron) on the training data and es-
timate a weight vector, which serves as a ?surro-
669
gate? weight vector. We then use this surrogate
vector to guide the design of the mapping. Ide-
ally we hope to find a permutation of the surro-
gate weights to map to a tensor in such a way that
the tensor has a rank as low as possible. How-
ever matrix rank minimization is in general a hard
problem (Fazel, 2002). Therefore, we follow an
approximate algorithm given in Figure 2a, whose
main idea is illustrated via an example in Figure
2b.
Basically, what the algorithm does is to di-
vide the surrogate weights into hierarchical groups
such that groups on the same level are approx-
imately proportional to each other. Using these
groups as units we are able to ?fill? the tensor in a
hierarchical way. The resulting tensor will have an
approximate low-rank structure, provided that the
sorted feature weights have roughly group-wise
proportional relations.
For comparison, we also experimented a trivial
solution which maps each entry of the feature ten-
sor to the tensor just in sequential order, namely
?
0
is mapped to ?
0,0,...,0
, ?
1
is mapped to ?
0,0,...,1
etc. This of course ignores correlation between
features since the original feature order in the vec-
tor could be totally meaningless, and this strategy
is not expected to be a good solution for vector to
tensor mapping.
4 Online Learning Algorithm
We now turn to the problem of learning the feature
weight tensor. Here we propose an online learning
algorithm similar to MIRA but modified to accom-
modate tensor models.
Let the model be f(T ) = T ? ?(x, y), where
T =
?
H
h=1
w
1
h
? w
2
h
?, . . . ,?w
D
h
is the weight
tensor, ?(x, y) is the feature tensor for an input-
output pair (x, y). Training samples (x
i
, y
i
), i =
1, . . . ,m, where x
i
is the input and y
i
is the ref-
erence or oracle hypothesis, are fed to the weight
learning algorithm in sequential order. A predic-
tion z
t
is made by the model T
t
at time t from a
set of candidatesZ(x
t
), and the model updates the
weight tensor by solving the following problem:
min
T?Rn1?n2?...nD
1
2
?T ? T
t
?
2
+ C? (3)
s.t.
L
t
? ?, ? ? 0
where T is a decomposed weight tensor and
L
t
= T ??(x
t
, z
t
)? T ??(x
t
, y
t
) + ?(y
t
, z
t
)
Input:
Tensor order D, tensor volume V , mode size
n
d
, d = 1, . . . , D, surrogate weight vector v
Let
v
+
= [v
+
1
, . . . , v
+
p
] be the non-negative part of
v
v
?
= [v
?
1
, . . . , v
?
q
] be the negative part of v
Algorithm:
?
v
+
= sort(v
+
) in descending order
?
v
?
= sort(v
?
) in ascending order
u = V/n
D
e = p?mod(p, u), f = q ?mod(q, u)
Construct vector
X = [v?
+
1
, . . . , v?
+
e
, v?
?
1
, . . . , v?
?
f
,
v?
+
e+1
, . . . , v?
+
p
, v?
?
f+1
, . . . , v?
?
q
]
Map X
a
, a = 1, . . . , p + q to the tensor entry
T
i
1
,...,i
D
, such that
a =
D
?
d=1
(i
d
? 1)l
d?1
+ 1
where l
d
= l
d?1
n
d
, and l
0
= 1
(a) Mapping a surrogate weight vector to a tensor
(b) Illustration of the algorithm
Figure 2: Algorithm for mapping a surrogate
weight vector X to a tensor. (2a) provides the al-
gorithm; (2b) illustrates it by mapping a vector of
length V = 12 to a (n
1
, n
2
, n
3
) = (2, 2, 3) ten-
sor. The bars X
i
represent the surrogate weights
? after separately sorting the positive and nega-
tive parts ? and the labels along a path of the tree
correspond to the tensor-index of the weight rep-
resented by the leaf resulting from the mapping.
670
is the structured hinge loss.
This problem setting follows the same ?passive-
aggressive? strategy as in the original MIRA. To
optimize the vectors w
d
h
, h = 1, . . . ,H, d =
1, . . . , D, we use a similar iterative strategy as pro-
posed in (Cai et al, 2006b). Basically, the idea is
that instead of optimizing w
d
h
all together, we op-
timize w
1
1
,w
2
1
, . . . ,w
D
H
in turn. While we are up-
dating one vector, the rest are fixed. For the prob-
lem setting given above, each of the sub-problems
that need to be solved is convex, and according
to (Cai et al, 2006b) the objective function value
will decrease after each individual weight update
and eventually this procedure will converge.
We now give this procedure in more detail.
Denote the weight vector of the d
th
mode of
the h
th
tensor at time t as w
d
h,t
. We will up-
date the vectors in turn in the following order:
w
1
1,t
, . . . ,w
D
1,t
,w
1
2,t
, . . . ,w
D
2,t
, . . . ,w
1
H,t
, . . . ,w
D
H,t
.
Once a vector has been updated, it is fixed for
future updates.
By way of notation, define
W
d
h,t
= w
1
h,t+1
?, . . . ,?w
d?1
h,t+1
?w
d
h,t
?, . . . ,?w
D
h,t
(and letW
D+1
h,t
, w
1
h,t+1
?, . . . ,?w
D
h,t+1
),
?
W
d
h,t
= w
1
h,t+1
?, . . . ,?w
d?1
h,t+1
?w
d
?, . . . ,?w
D
h,t
(where w
d
? R
n
d
),
T
d
h,t
=
h?1
?
h
?
=1
W
D+1
h
?
,t
+W
d
h,t
+
H
?
h
?
=h+1
W
1
h
?
,t
(4)
?
T
d
h,t
=
h?1
?
h
?
=1
W
D+1
h
?
,t
+
?
W
d
h,t
+
H
?
h
?
=h+1
W
1
h
?
,t
?
d
h,t
(x, y)
= ?(x, y)?
2
w
2
h,t+1
. . .?
d?1
w
d?1
h,t+1
?
d+1
w
d+1
h,t
. . .?
D
w
D
h,t
(5)
In order to update from w
d
h,t
to get w
d
h,t+1
, the
sub-problem to solve is:
min
wd?Rnd
1
2
?
?
T
d
h,t
? T
d
h,t
?
2
+ C?
= min
wd?Rnd
1
2
?
?
W
d
h,t
?W
d
h,t
?
2
+ C?
= min
wd?Rnd
1
2
?
1
h,t+1
. . . ?
d?1
h,t+1
?
d+1
h,t
. . . ?
D
h,t
?w
d
?w
d
h,t
?
2
+ C?
s.t. L
d
h,t
? ?, ? ? 0.
where
?
d
h,t
= ?w
d
h,t
?
2
L
d
h,t
=
?
T
d
h,t
??(x
t
, z
t
)?
?
T
d
h,t
??(x
t
, y
t
)
+?(y
t
, z
t
)
= w
d
?
(
?
d
h,t
(x
t
, z
t
)? ?
d
h,t
(x
t
, y
t
)
)
?
(
h?1
?
h
?
=1
W
D+1
h
?
,t
+
H
?
h
?
=h+1
W
1
h
?
,t
)
?
(?(x
t
, y
t
)??(x
t
, z
t
))
+?(y
t
, z
t
)
Letting
??
d
h,t
, ?
d
h,t
(x
t
, y
t
)? ?
d
h,t
(x
t
, z
t
)
and
s
d
h,t
,
(
h?1
?
h
?
=1
W
D+1
h
?
,t
+
H
?
h
?
=h+1
W
1
h
?
,t
)
?
(?(x
t
, y
t
)??(x
t
, z
t
))
we may compactly write
L
d
h,t
= ?(y
t
, z
t
)? s
d
h,t
?w
d
???
d
h,t
.
This convex optimization problem is just like the
original MIRA and may be solved in a similar way.
The updating strategy for w
d
h,t
is derived as
w
d
h,t+1
= w
d
h,t
+ ???
d
h,t
? = (6)
min
{
C,
?(y
t
, z
t
)? T
d
h,t
? (?(x
t
, y
t
)??(x
t
, z
t
))
???
d
h,t
?
2
}
The initial vectors w
i
h,1
cannot be made all zero,
since otherwise the l-mode product in Equation
(5) would yield all zero ?
d
h,t
(x, y) and the model
would never get a chance to be updated. There-
fore, we initialize the entries of w
i
h,1
uniformly
such that the Frobenius-norm of the weight tensor
W is unity.
We call the algorithm above ?Tensor-MIRA?
and abbreviate it as T-MIRA.
671
5 Experiments
In this section we shows empirical results of the
training algorithm on a parsing task. We used the
Charniak parser (Charniak et al, 2005) for our ex-
periment, and we used the proposed algorithm to
train the reranking feature weights. For compari-
son, we also investigated training the reranker with
Perceptron and MIRA.
5.1 Experimental Settings
To simulate a low-resource training environment,
our training sets were selected from sections 2-9
of the Penn WSJ treebank, section 24 was used as
the held-out set and section 23 as the evaluation
set. We applied the default settings of the parser.
There are around V = 1.33 million features in
all defined for reranking, and the n-best size for
reranking is set to 50. We selected the parse with
the highest f -score from the 50-best list as the or-
acle.
We would like to observe from the experiments
how the amount of training data as well as dif-
ferent settings of the tensor degrees of freedom
affects the algorithm performance. Therefore we
tried all combinations of the following experimen-
tal parameters:
Parameters Settings
Training data (m) Sec. 2, 2-3, 2-5, 2-9
Tensor order (D) 2, 3, 4
# rank-1 tensors (H) 1, 2, 3
Vec. to tensor mapping approximate, sequential
Here ?approximate? and ?sequential? means us-
ing, respectively, the algorithm given in Figure 2
and the sequential mapping mentioned in Section
3.4. According to the strategy given in 3.2, once
the tensor order and number of features are fixed,
the sizes of modes and total number of parameters
to estimate are fixed as well, as shown in the tables
below:
D Size of modes Number of parameters
2 1155? 1155 2310
3 110? 110? 111 331
4 34? 34? 34? 34 136
5.2 Results and Analysis
The f -scores of the held-out and evaluation set
given by T-MIRA as well as the Perceptron and
MIRA baseline are given in Table 1. From the re-
sults, we have the following observations:
1. When very few labeled data are available for
training (compared with the number of fea-
tures), T-MIRA performs much better than
the vector-based models MIRA and Percep-
tron. However as the amount of training data
increases, the advantage of T-MIRA fades
away, and vector-based models catch up.
This is because the weight tensors learned
by T-MIRA are highly structured, which sig-
nificantly reduces model/training complex-
ity and makes the learning process very ef-
fective in a low-resource environment, but
as the amount of data increases, the more
complex and expressive vector-based models
adapt to the data better, whereas further im-
provements from the tensor model is impeded
by its structural constraints, making it insen-
sitive to the increase of training data.
2. To further contrast the behavior of T-MIRA,
MIRA and Perceptron, we plot the f -scores
on both the training and held-out sets given
by these algorithms after each training epoch
in Figure 3. The plots are for the exper-
imental setting with mapping=surrogate, #
rank-1 tensors=2, tensor order=2, training
data=sections 2-3. It is clearly seen that both
MIRA and Perceptron do much better than T-
MIRA on the training set. Nevertheless, with
a huge number of parameters to fit a limited
amount of data, they tend to over-fit and give
much worse results on the held-out set than
T-MIRA does.
As an aside, observe that MIRA consistently
outperformed Perceptron, as expected.
3. Properties of linear tensor model: The heuris-
tic vector-to-tensor mapping strategy given
by Figure 2 gives consistently better results
than the sequential mapping strategy, as ex-
pected.
To make further comparison of the two strate-
gies, in Figure 4 we plot the 20 largest sin-
gular values of the matrices which the surro-
gate weights (given by the Perceptron after
running for 1 epoch) are mapped to by both
strategies (from the experiment with training
data sections 2-5). From the contrast between
the largest and the 2
nd
-largest singular val-
ues, it can be seen that the matrix generated
672
by the first strategy approximates a low-rank
structure much better than the second strat-
egy. Therefore, the performance of T-MIRA
is influenced significantly by the way features
are mapped to the tensor. If the correspond-
ing target weight tensor has internal struc-
ture that makes it approximately low-rank,
the learning procedure becomes more effec-
tive.
The best results are consistently given by 2
nd
order tensor models, and the differences be-
tween the 3
rd
and 4
th
order tensors are not
significant. As discussed in Section 3.1, al-
though 3
rd
and 4
th
order tensors have less pa-
rameters, the benefit of reduced training com-
plexity does not compensate for the loss of
expressiveness. A 2
nd
order tensor has al-
ready reduced the number of parameters from
the original 1.33 million to only 2310, and it
does not help to further reduce the number of
parameters using higher order tensors.
4. As the amount of training data increases,
there is a trend that the best results come from
models with more rank-1 component tensors.
Adding more rank-1 tensors increases the
model?s complexity and ability of expression,
making the model more adaptive to larger
data sets.
6 Conclusion and Future Work
In this paper, we reformulated the traditional lin-
ear vector-space models as tensor-space models,
and proposed an online learning algorithm named
Tensor-MIRA. A tensor-space model is a com-
pact representation of data, and via rank-1 ten-
sor approximation, the weight tensor can be made
highly structured hence the number of parame-
ters to be trained is significantly reduced. This
can be regarded as a form of model regular-
ization.Therefore, compared with the traditional
vector-space models, learning in the tensor space
is very effective when a large feature set is defined,
but only small amount of training data is available.
Our experimental results corroborated this argu-
ment.
As mentioned in Section 3.2, one interesting
problem that merits further investigation is how
to determine optimal mode sizes. The challenge
of applying a tensor model comes from finding a
proper tensor structure for a given problem, and
 
95.5 96
 
96.5 97
 
97.5 98
 
98.5 99  1
 
2 3
 
4 5
 
6 7
 
8 9
 
10
f-score
Iteratio
ns
Trainin
g set f-
score T-MI
RA MIRA Percep
tron
(a) Training set
 
87
 
87.5 88
 
88.5 89
 
89.5 90  1
 
2 3
 
4 5
 
6 7
 
8 9
 
10
f-score
Iteratio
ns
Trainin
g set f-
score T-MI
RA MIRA Percep
tron
(b) Held-out set
Figure 3: f -scores given by three algorithms on
training and held-out set (see text for the setting).
the key to solving this problem is to find a bal-
ance between the model complexity (indicated by
the order and sizes of modes) and the number of
parameters. Developing a theoretically guaran-
teed approach of finding the optimal structure for
a given task will make the tensor model not only
perform well in low-resource environments, but
adaptive to larger data sets.
7 Acknowledgements
This work was partially supported by IBM via
DARPA/BOLT contract number HR0011-12-C-
0015 and by the National Science Foundation via
award number IIS-0963898.
References
Deng Cai , Xiaofei He , and Jiawei Han. 2006. Tensor
Space Model for Document Analysis Proceedings
of the 29th Annual International ACM SIGIR Con-
ference on Research and Development in Informa-
tion Retrieval(SIGIR), 625?626.
Deng Cai, Xiaofei He, and Jiawei Han. 2006. Learn-
ing with Tensor Representation Technical Report,
Department of Computer Science, University of Illi-
nois at Urbana-Champaign.
673
Mapping Approximate Sequential
Rank-1 tensors 1 2 3 1 2 3
Tensor order 2 3 4 2 3 4 2 3 4 2 3 4 2 3 4 2 3 4
Held-out score 89.43 89.16 89.22 89.16 89.21 89.24 89.27 89.14 89.24 89.21 88.90 88.89 89.13 88.88 88.88 89.15 88.87 88.99
Evaluation score 89.83 89.69
MIRA 88.57
Percep 88.23
(a) Training data: Section 2 only
Mapping Approximate Sequential
Rank-1 tensors 1 2 3 1 2 3
Tensor order 2 3 4 2 3 4 2 3 4 2 3 4 2 3 4 2 3 4
Held-out score 89.26 89.06 89.12 89.33 89.11 89.19 89.18 89.14 89.15 89.2 89.01 88.82 89.24 88.94 88.95 89.19 88.91 88.98
Evaluation score 90.02 89.82
MIRA 89.00
Percep 88.59
(b) Training data: Section 2-3
Mapping Approximate Sequential
Rank-1 tensors 1 2 3 1 2 3
Tensor order 2 3 4 2 3 4 2 3 4 2 3 4 2 3 4 2 3 4
Held-out score 89.40 89.44 89.17 89.5 89.37 89.18 89.47 89.32 89.18 89.23 89.03 88.93 89.24 88.98 88.94 89.16 89.01 88.85
Evaluation score 89.96 89.78
MIRA 89.49
Percep 89.10
(c) Training data: Section 2-5
Mapping Approximate Sequential
Rank-1 tensors 2 3 4 2 3 4
Tensor order 2 3 4 2 3 4 2 3 4 2 3 4 2 3 4 2 3 4
Held-out score 89.43 89.23 89.06 89.37 89.23 89.1 89.44 89.22 89.06 89.21 88.92 88.94 89.23 88.94 88.93 89.23 88.95 88.93
Evaluation score 89.95 89.84
MIRA 89.95
Percep 89.77
(d) Training data: Section 2-9
Table 1: Parsing f -scores. Tables (a) to (d) correspond to training data with increasing size. The upper-part of
each table shows the T-MIRA results with different settings, the lower-part shows the MIRA and Perceptron
baselines. The evaluation scores come from the settings indicated by the best held-out scores. The best results
on the held-out and evaluation data are marked in bold.
 
0
 
100
 
200
 
300
 
400
 
500
 
2 4
 
6 8
 
10 1
2 14
 
16 1
8 20
Singular value
Approx
imate Seque
ntial
Figure 4: The top 20 singular values of the surro-
gate weight matrices given by two mapping algo-
rithms.
Eugene Charniak, and Mark Johnson 2005. Coarse-
to-fine n-Best Parsing and MaxEnt Discriminative
Reranking Proceedings of the 43th Annual Meeting
on Association for Computational Linguistics(ACL)
173?180.
David Chiang, Yuval Marton, and Philip Resnik.
2008. Online Large-Margin Training of Syntactic
and Structural Translation Features Proceedings of
Empirical Methods in Natural Language Process-
ing(EMNLP), 224?233.
Shay Cohen and Michael Collins. 2012. Tensor De-
composition for Fast Parsing with Latent-Variable
PCFGs Proceedings of Advances in Neural Infor-
mation Processing Systems(NIPS).
Shay Cohen and Giorgio Satta. 2013. Approximate
PCFG Parsing Using Tensor Decomposition Pro-
ceedings of NAACL-HLT, 487?496.
Michael Collins. 2002. Discriminative training meth-
ods for hidden Markov Models: Theory and Exper-
iments with Perceptron. Algorithms Proceedings of
Empirical Methods in Natural Language Process-
ing(EMNLP), 10:1?8.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai
Shalev-Schwartz, and Yoram Singer. 2006. Online
Passive-Aggressive Algorithms Journal of Machine
Learning Research(JMLR), 7:551?585.
Maryam Fazel. 2002. Matrix Rank Minimization with
Applications PhD thesis, Stanford University.
Kevin Gimpel, and Noah A. Smith 2012. Structured
Ramp Loss Minimization for Machine Translation
Proceedings of North American Chapter of the As-
sociation for Computational Linguistics(NAACL),
221-231.
674
Tamir Hazan, Simon Polak, and Amnon Shashua 2005.
Sparse Image Coding using a 3D Non-negative Ten-
sor Factorization Proceedings of the International
Conference on Computer Vision (ICCV).
Mark Hopkins and Jonathan May. 2011. Tuning
as Reranking Proceedings of Empirical Methods
in Natural Language Processing(EMNLP), 1352-
1362.
Tamara Kolda and Brett Bader. 2009. Tensor Decom-
positions and Applications SIAM Review, 51:455-
550.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online Large-Margin Training of
Dependency Parsers Proceedings of the 43rd An-
nual Meeting of the ACL, 91?98.
Amnon Shashua, and Tamir Hazan. 2005. Non-
Negative Tensor Factorization with Applications to
Statistics and Computer Vision Proceedings of
the International Conference on Machine Learning
(ICML).
Tim Van de Cruys, Thierry Poibeau, and Anna Korho-
nen. 2013. A Tensor-based Factorization Model of
Semantic Compositionality Proceedings of NAACL-
HLT, 1142?1151.
A Proof of Theorem 1
Proof. For D = 1, it is obvious that if a set of
real numbers {x
1
, . . . , x
n
} can be represented by
a rank-1 matrix, it can always be represented by a
vector, but the reverse is not true.
For D > 1, if {x
1
, . . . , x
n
} can be repre-
sented by P = p
1
? p
2
? . . . ? p
D
, namely
x
i
= P
i
1
,...,i
D
=
?
D
d=1
p
d
i
d
, then for any compo-
nent vector in mode d,
[p
d
1
, p
d
2
, . . . , p
d
n
d
] = [s
d
1
p
d
1
, s
d
2
p
d
1
, . . . , s
d
n
p
d
p
d
1
]
where n
p
d
is the size of mode d of P , s
d
j
is a con-
stant and s
d
j
=
p
i1,...,i
d?1
,j,i
d+1
,...,i
D
p
i1,...,i
d?1
,1,i
d+1
,...,i
D
Therefore
x
i
= P
i
1
,...,i
D
= x
1,...,1
D
?
d=1
s
d
i
d
(7)
and this representation is unique for a given D(up
to the ordering of p
j
and s
d
j
in p
j
, which simply
assigns {x
1
, . . . , x
n
} with different indices in the
tensor), due to the pairwise proportional constraint
imposed by x
i
/x
j
, i, j = 1, . . . , n.
If x
i
can also be represented by Q, then x
i
=
Q
i
1
,...,i
D+1
= x
1,...,1
?
D+1
d=1
t
d
i
d
, where t
d
j
has a
similar definition as s
d
j
. Then it must be the case
that
?d
1
, d
2
? {1, . . . , D + 1}, d ? {1, . . . , D}, d
1
6= d
2
s.t.
t
d
1
i
d
1
t
d
2
i
d
2
= s
d
i
d
, (8)
t
d
a
i
d
a
= s
d
b
i
d
b
, d
a
6= d
1
, d
2
, d
b
6= d
since otherwise {x
1
, . . . , x
n
} would be repre-
sented by a different set of factors than those given
in Equation (7).
Therefore, in order for tensor Q to represent
the same set of real numbers that P represents,
there needs to exist a vector [s
d
1
, . . . , s
d
n
d
] that can
be represented by a rank-1 matrix as indicated by
Equation (8), which is in general not guaranteed.
On the other hand, if {x
1
, . . . , x
n
} can be rep-
resented by Q, namely
x
i
= Q
i
1
,...,i
D+1
=
D+1
?
d=1
q
d
i
d
then we can just pick d
1
? {1, . . . , D}, d
2
= d
1
+
1 and let
q
?
= [q
d
1
1
q
d
2
1
, q
d
1
1
q
d
2
2
, . . . , q
d
1
n
q
d
2
q
d
2
n
q
d
1
]
and
Q
?
= q
1
? . . .?q
d
1
?1
?q
?
?q
d
2
+1
? . . .?q
D+1
Hence {x
1
, . . . , x
n
} can also be represented by a
D
th
order tensor Q
?
.
675
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1316?1325,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Can You Repeat That?
Using Word Repetition to Improve Spoken Term Detection
Jonathan Wintrode and Sanjeev Khudanpur
Center for Language and Speech Processing
Johns Hopkins University
jcwintr@cs.jhu.edu , khudanpur@jhu.edu
Abstract
We aim to improve spoken term detec-
tion performance by incorporating con-
textual information beyond traditional N-
gram language models. Instead of taking a
broad view of topic context in spoken doc-
uments, variability of word co-occurrence
statistics across corpora leads us to fo-
cus instead the on phenomenon of word
repetition within single documents. We
show that given the detection of one in-
stance of a term we are more likely to
find additional instances of that term in the
same document. We leverage this bursti-
ness of keywords by taking the most con-
fident keyword hypothesis in each docu-
ment and interpolating with lower scor-
ing hits. We then develop a principled
approach to select interpolation weights
using only the ASR training data. Us-
ing this re-weighting approach we demon-
strate consistent improvement in the term
detection performance across all five lan-
guages in the BABEL program.
1 Introduction
The spoken term detection task arises as a key sub-
task in applying NLP applications to spoken con-
tent. Tasks like topic identification and named-
entity detection require transforming a continu-
ous acoustic signal into a stream of discrete to-
kens which can then be handled by NLP and other
statistical machine learning techniques. Given a
small vocabulary of interest (1000-2000 words or
multi-word terms) the aim of the term detection
task is to enumerate occurrences of the keywords
within a target corpus. Spoken term detection con-
verts the raw acoustics into time-marked keyword
occurrences, which may subsequently be fed (e.g.
as a bag-of-terms) to standard NLP algorithms.
Although spoken term detection does not re-
quire the use of word-based automatic speech
recognition (ASR), it is closely related. If we
had perfectly accurate ASR in the language of
the corpus, term detection is reduced to an exact
string matching task. The word error rate (WER)
and term detection performance are clearly corre-
lated. Given resource constraints, domain, chan-
nel, and vocabulary limitations, particularly for
languages other than English, the errorful token
stream makes term detection a non-trivial task.
In order to improve detection performance, and
restricting ourselves to an existing ASR system
or systems at our disposal, we focus on leverag-
ing broad document context around detection hy-
potheses. ASR systems traditionally use N-gram
language models to incorporate prior knowledge
of word occurrence patterns into prediction of the
next word in the token stream. N-gram mod-
els cannot, however, capture complex linguistic or
topical phenomena that occur outside the typical
3-5 word scope of the model. Yet, though many
language models more sophisticated than N-grams
have been proposed, N-grams are empirically hard
to beat in terms of WER.
We consider term detection rather than the tran-
scription task in considering how to exploit topic
context, because in evaluating the retrieval of cer-
tain key terms we need not focus on improving
the entire word sequence. Confidence scores from
an ASR system (which incorporate N-gram prob-
abilities) are optimized in order to produce the
most likely sequence of words rather than the ac-
curacy of individual word detections. Looking at
broader document context within a more limited
task might allow us to escape the limits of N-gram
performance. We will show that by focusing on
contextual information in the form of word repe-
tition within documents, we obtain consistent im-
provement across five languages in the so called
Base Phase of the IARPA BABEL program.
1316
1.1 Task Overview
We evaluate term detection and word repetition-
based re-scoring on the IARPA BABEL training
and development corpora
1
for five languages Can-
tonese, Pashto, Turkish, Tagalog and Vietnamese
(Harper, 2011). The BABEL task is modeled on
the 2006 NIST Spoken Term Detection evaluation
(NIST, 2006) but focuses on limited resource con-
ditions. We focus specifically on the so called no
target audio reuse (NTAR) condition to make our
method broadly applicable.
In order to arrive at our eventual solution, we
take the BABEL Tagalog corpus and analyze word
co-occurrence and repetition statistics in detail.
Our observation of the variability in co-occurrence
statistics between Tagalog training and develop-
ment partitions leads us to narrow the scope of
document context to same word co-occurrences,
i.e. word repetitions.
We then analyze the tendency towards within-
document repetition. The strength of this phe-
nomenon suggests it may be more viable for im-
proving term-detection than, say, topic-sensitive
language models. We validate this by develop-
ing an interpolation formula to boost putative word
repetitions in the search results, and then inves-
tigate a method for setting interpolation weights
without manually tuning on a development set.
We then demonstrate that the method general-
izes well, by applying it to the 2006 English data
and the remaining four 2013 BABEL languages.
We demonstrate consistent improvements in all
languages in both the Full LP (80 hours of ASR
training data) and Limited LP (10 hours) settings.
2 Motivation
We seek a workable definition of broad docu-
ment context beyond N-gram models that will im-
prove term detection performance on an arbitrary
set of queries. Given the rise of unsupervised la-
tent topic modeling with Latent Dirchlet Alloca-
tion (Blei et al, 2003) and similar latent variable
approaches for discovering meaningful word co-
occurrence patterns in large text corpora, we ought
to be able to leverage these topic contexts instead
of merely N-grams. Indeed there is work in the
literature that shows that various topic models, la-
tent or otherwise, can be useful for improving lan-
1
Language collection releases IARPA-babel101-v0.4c,
IARPA-babel104b-v0.4bY, IARPA-babel105b-v0.4, IARPA-
babel106-v0.2g and IARPA-babel107b-v0.7 respectively.
guage model perplexity and word error rate (Khu-
danpur and Wu, 1999; Chen, 2009; Naptali et
al., 2012). However, given the preponderance of
highly frequent non-content words in the compu-
tation of a corpus? WER, it?s not clear that a 1-2%
improvement in WER would translate into an im-
provement in term detection.
Still, intuition suggests that knowing the topic
context of a detected word ought to be useful
in predicting whether or not a term does belong
in that context. For example, if we determine
the context of the detection hypothesis is about
computers, containing words like ?monitor,? ?in-
ternet? and ?mouse,? then we would be more con-
fident of a term such as ?keyboard? and less con-
fident of a term such as ?cheese board?. The dif-
ficulty in this approach arises from the variabil-
ity in word co-occurrence statistics. Using topic
information will be helpful if ?monitor,? ?key-
board? and ?mouse? consistently predict that ?key-
board? is present. Unfortunately, estimates of co-
occurrence from small corpora are not very consis-
tent, and often over- or underestimate concurrence
probabilities needed for term detection.
We illustrate this variability by looking at how
consistent word co-occurrences are between two
separate corpora in the same language: i.e., if we
observe words that frequently co-occur with a key-
word in the training corpus, do they also co-occur
with the keywords in a second held-out corpus?
Figure 1, based on the BABEL Tagalog corpus, sug-
gests this is true only for high frequency keywords.
Figure 1: Correlation between the co-occurrence
counts in the training and held-out sets for a fixed
keyword (term) and all its ?context? words.
Each point in Figure 1 represents one of 355
1317
(a) High frequency keyword ?bukas? (b) Low frequency keyword ?Davao?
Figure 2: The number of times a fixed keyword k co-occurs with a vocabulary word w in the training
speech collection ? T (k,w) ? versus the search collection ? D(k,w).
Tagalog keywords used for system development
by all BABEL participants. For each keyword k,
we count how often it co-occurs in the same con-
versation as a vocabulary word w in the ASR
training data and the development data, and des-
ignate the counts T (k,w) and D(k,w) respec-
tively. The x-coordinate of each point in Figure 1
is the frequency of k in the training data, and the
y-coordinate is the correlation coefficient ?
k
be-
tween T (k,w) and D(k,w). A high ?
k
implies
that wordsw that co-occur frequently with k in the
training data also do so in the search collection.
To further illustrate how Figure 1 was obtained,
consider the high-frequency keyword bukas (count
= 879) and the low-frequency keyword Davao
(count = 11), and plot T (k, ?) versus D(k, ?),
as done in Figure 2. The correlation coefficients
?
bukas
and ?
Davao
from the two plots end up as two
points in Figure 1.
Figure 1 suggests that (k,w) co-occurrences are
consistent between the two corpora (?
k
> 0.8) for
keywords occurring 100 or more times. However,
if the goal is to help a speech retrieval system de-
tect content-rich (and presumably infrequent) key-
words, then using word co-occurrence informa-
tion (i.e. topic context) does not appear to be
too promising, even though intuition suggests that
such information ought to be helpful.
In light of this finding, we will restrict the type
of context we use for term detection to the co-
occurrence of the term itself elsewhere within the
document. As it turns out this ?burstiness? of
words within documents, as the term is defined by
Church and Gale in their work on Poisson mix-
tures (1995), provides a more reliable framework
for successfully exploiting document context.
2.1 Related Work
A number of efforts have been made to augment
traditional N-gram models with latent topic infor-
mation (Khudanpur and Wu, 1999; Florian and
Yarowsky, 1999; Liu and Liu, 2008; Hsu and
Glass, 2006; Naptali et al, 2012) including some
of the early work on Probabilistic Latent Semantic
Analysis by Hofmann (2001). In all of these cases
WER gains in the 1-2% range were observed by
interpolating latent topic information with N-gram
models.
The re-scoring approach we present is closely
related to adaptive or cache language models (Je-
linek, 1997; Kuhn and De Mori, 1990; Kneser and
Steinbiss, 1993). The primary difference between
this and previous work on similar language mod-
els is the narrower focus here on the term detec-
tion task, in which we consider each search term in
isolation, rather than all words in the vocabulary.
Most recently, Chiu and Rudnicky (2013) looked
at word bursts in the IARPA BABEL conversational
corpora, and were also able to successfully im-
prove performance by leveraging the burstiness of
language. One advantage of the approach pro-
posed here, relative to their approach, is its sim-
plicity and its not requiring an additional tuning
set to estimate parameters.
In the information retrieval community, cluster-
ing and latent topic models have yielded improve-
ments over traditional vector space models. We
will discuss in detail in the following section re-
lated works by Church and Gale (1995, 1999, and
2000). Work by Wei and Croft (2006) and Chen
(2009) take a language model-based approach to
1318
(a) f
w
versus IDF
w
?
(b) Obsered versus predicted IDF
w
Figure 3: Tagalog corpus frequency statistics, unigrams
information retrieval, and again, interpolate latent
topic models with N-grams to improve retrieval
performance. However, in many text retrieval
tasks, queries are often tens or hundreds of words
in length rather than short spoken phrases. In these
efforts, the topic model information was helpful in
boosting retrieval performance above the baseline
vector space or N-gram models.
Clearly topic or context information is relevant
to a retrieval type task, but we need a stable, con-
sistent framework in which to apply it.
3 Term and Document Frequency
Statistics
To this point we have assumed an implicit property
of low-frequency words which Church and Gale
state concisely in their 1999 study of inverse doc-
ument frequency:
Low frequency words tend to be rich
in content, and vice versa. But not
all equally frequent words are equally
meaningful. Church and Gale (1999).
The typical use of Document Frequency (DF) in
information retrieval or text categorization is to
emphasize words that occur in only a few docu-
ments and are thus more ?rich in content?. Close
examination of DF statistics by Church and Gale
in their work on Poisson Mixtures (1995) resulted
in an analysis of the burstiness of content words.
In this section we look at DF and burstiness
statistics applying some of the analyses of Church
and Gale (1999) to the BABEL Tagalog corpus.
We observe, in 648 Tagalog conversations, simi-
lar phenomena as observed by Church and Gale on
89,000 AP English newswire articles. We proceed
in this fashion to make a case for why burstiness
ought to help in the term detection task.
For the Tagalog conversations, as with En-
glish newswire, we observe that the document fre-
quency, DF
w
, of a word w is not a linear function
of word frequency f
w
in the log domain, as would
be expected under a naive Poisson generative as-
sumption. The implication of deviations from a
Poisson model is that words tend to be concen-
trated in a small number of documents rather than
occurring uniformly across the corpus. This is the
burstiness we leverage to improve term detection.
The first illustration of word burstiness can be
seen by plotting observed inverse document fre-
quency, IDF
w
, versus f
w
in the log domain (Fig-
ure 3a). We use the same definition of IDF
w
as
Church and Gale (1999):
IDF
w
= ? log
2
DF
w
N
, (1)
where N is the number of documents (i.e. conver-
sations) in the corpus.
There is good linear correlation (? = 0.73) be-
tween log f
w
and IDF
w
. Yet, visually, the rela-
tionship in Figure 3a is clearly not linear. In con-
trast, the AP English data exhibits a correlation of
? = 0.93 (Church and Gale, 1999). Thus the devi-
ation in the Tagalog corpus is more pronounced,
i.e. words are less uniformly distributed across
documents.
A second perspective on word burstiness that
follows from Church and Gale (1999) is that a
Poisson assumption should lead us to predict:
?
IDF
w
= ? log
2
(
1? e
?
f
w
N
)
. (2)
1319
Figure 4: Difference between observed and pre-
dicted IDF
w
for Tagalog unigrams.
For the AP newswire, Church and Gale found the
largest deviation between the predicted
?
IDF
w
and
observed IDF
w
to occur in the middle of the fre-
quency range. We see a somewhat different pic-
ture for Tagalog speech in Figure 3b. Observed
IDF
w
values again deviate significantly from their
predictions (2), but all along the frequency range.
There is a noticeable quantization effect occur-
ring in the high IDF range, given that our N is at
least a factor of 100 smaller than the number of
AP articles they studied: 648 vs. 89,000. Figure 4
also shows the difference between and observed
IDF
w
and Poisson estimate
?
IDF
w
and further il-
lustrates the high variance in IDF
w
for low fre-
quency words.
Two questions arise: what is happening with in-
frequent words, and why does this matter for term
detection? To look at the data from a different
perspective, we consider the random variable k,
which is the number of times a word occurs in a
particular document. In Figure 5 we plot the fol-
lowing ratio, which Church and Gale (1995) define
as burstiness :
E
w
[k|k > 0] =
f
w
DF
w
(3)
as a function of f
w
. We denote this as E[k] and
can interpret burstiness as the expected word count
given we see w at least once.
In Figure 5 we see two classes of words emerge.
A similar phenomenon is observed concerning
adaptive language models (Church, 2000). In
general, we can think of using word repetitions
to re-score term detection as applying a limited
form of adaptive or cache language model (Je-
linek, 1997). Likewise, Katz attempts to capture
Figure 5: Tagalog burstiness.
these two classes in his G model of word frequen-
cies (1996).
For the first class, burstiness increases slowly
but steadily as w occurs more frequently. Let us
label these Class A words. Since our corpus size
is fixed, we might expect this to occur, as more
word occurrences must be pigeon-holed into the
same number of documents
Looking close to the y-axis in Figure 5, we ob-
serve a second class of exclusively low frequency
words whose burstiness ranges from highly con-
centrated to singletons. We will refer to these as
Class B words. If we take the Class A concentra-
tion trend as typical, we can argue that most Class
B words exhibit a larger than average concentra-
tion. In either case we see evidence that both high
and low frequency words tend towards repeating
within a document.
3.1 Unigram Probabilities
In applying the burstiness quantity to term detec-
tion, we recall that the task requires us to locate a
particular instance of a term, not estimate a count,
hence the utility of N-gram language models pre-
dicting words in sequence.
We encounter the burstiness property of words
again by looking at unigram occurrence probabili-
ties. We compare the unconditional unigram prob-
ability (the probability that a given word token is
w) with the conditional unigram probability, given
the term has occurred once in the document. We
compute the conditional probability for w using
frequency information.
1320
Figure 6: Difference between conditional and un-
conditional unigram probabilities for Tagalog
P (w|k > 0) =
f
w
?DF
w
?
D:w?D
|D|
(4)
Figure 6 shows the difference between con-
ditional and unconditional unigram probabilities.
Without any other information, Zipf?s law sug-
gests that most word types do not occur in a partic-
ular document. However, conditioning on one oc-
currence, most word types are more likely to occur
again, due to their burstiness.
Finally we measure the adaptation of a word,
which is defined by Church and Gale (1995) as:
P
adapt
(w) = P
w
(k > 1|k > 0) (5)
When we plot adaptation versus f
w
(Figure 7)
we see that all high-frequency and a significant
number of low-frequency terms have adaptation
greater that 50%. To be precise, 26% of all to-
kens and 25% of low-frequency (f
w
< 100) have
at least 50% adaptation. Given that adaptation val-
ues are roughly an order of magnitude higher than
the conditional unigram probabilities, in the next
two sections we describe how we use adaptation
to boost term detection scores.
4 Term Detection Re-scoring
We summarize our re-scoring of repeated words
with the observation: given a correct detection,
the likelihood of additional terms in the same doc-
uments should increase. When we observe a term
detection score with high confidence, we boost the
other lower-scoring terms in the same document to
reflect this increased likelihood of repeated terms.
Figure 7: Tagalog word adaptation probability
For each term t and document d we propose in-
terpolating the ASR confidence score for a partic-
ular detection t
d
with the top scoring hit in dwhich
we?ll call
?
t
d
.
S(t
d
) = (1? ?)P
asr
(t
d
|O) + ?P
asr
(
?
t
d
|O) (6)
We will we develop a principled approach to se-
lecting ? using the adaptation property of the cor-
pus. However to verify that this approach is worth
pursuing, we sweep a range of small ? values, on
the assumption that we still do want to mostly rely
on the ASR confidence score for term detection.
For the Tagalog data, we let ? range from 0 (the
baseline) to 0.4 and re-score each term detection
score according to (6). Table 1 shows the results
of this parameter sweep and yields us 1 to 2% ab-
solute performance gains in a number of term de-
tection metrics.
? ATWV P (Miss)
0.00 0.470 0.430
0.05 0.481 0.422
0.10 0.483 0.420
0.15 0.484 0.418
0.20 0.483 0.416
0.25 0.480 0.417
0.30 0.477 0.417
0.35 0.475 0.415
0.40 0.471 0.413
0.45 0.465 0.413
0.50 0.462 0.410
Table 1: Term detection scores for swept ? values
on Tagalog development data
1321
The primary metric for the BABEL program, Ac-
tual Term Weighted Value (ATWV) is defined by
NIST using a cost function of the false alarm prob-
ability P (FA) and P (Miss), averaged over a set
of queries (NIST, 2006). The manner in which the
components of ATWV are defined:
P (Miss) = 1?N
true
(term)/f
term
(7)
P (FA) = N
false
/Duration
corpus
(8)
implies that cost of a miss is inversely proportional
to the frequency of the term in the corpus, but the
cost of a false alarm is fixed. For this reason, we
report both ATWV and the P (Miss) component.
A decrease in P (Miss) reflects the fact that we
are able to boost correct detections of the repeated
terms.
4.1 Interpolation Weights
We would prefer to use prior knowledge rather
than naive tuning to select an interpolation weight
?. Our analysis of word burstiness suggests that
adaptation, is a reasonable candidate. Adaptation
also has the desirable property that we can esti-
mate it for each word in the training vocabulary
directly from training data and not post-hoc on a
per-query basis. We consider several different es-
timates and we can show that the favorable result
extends across languages.
Intuition suggests that we prefer per-term in-
terpolation weights related to the term?s adapta-
tion. But despite the strong evidence of the adapta-
tion phenomenon in both high and low-frequency
words (Figure 7), we have less confidence in the
adaptation strength of any particular word.
As with word co-occurrence, we consider if es-
timates of P
adapt
(w) from training data are con-
sistent when estimated on development data. Fig-
ure 8 shows the difference between P
adapt
(w)
measured on the two corpora (for words occurring
in both).
We see that the adaptation estimates are only
consistent between corpora for high-frequency
words. Using this P
adapt
(w) estimate directly ac-
tually hurts ATWV performance by 4.7% absolute
on the 355 term development query set (Table 2).
Given the variability in estimating P
adapt
(w),
an alternative approach would be take
?
P
w
as an
upper bound on ?, reached as the DF
w
increases
(cf. Equation 9). We would discount the adapta-
tion factor when DF
w
is low and we are unsure of
Figure 8: Difference in adaptation estimates be-
tween Tagalog training and development corpora
Interpolation Weight ATWV P (Miss)
None 0.470 0.430
P
adapt
(w) 0.423 0.474
(1? e
?DF
w
)P
adapt
(w) 0.477 0.415
?? = 0.20 0.483 0.416
Table 2: Term detection performance using vari-
ous interpolation weight strategies on Tagalog dev
data
the effect.
?
w
= (1? e
?DF
w
) ?
?
P
adapt
(w) (9)
This approach shows a significant improvement
(0.7% absolute) over the baseline. However, con-
sidering this estimate in light of the two classes of
words in Figure 5, there are clearly words in Class
B with high burstiness that will be ignored by try-
ing to compensate for the high adaptation variabil-
ity in the low-frequency range.
Alternatively, we take a weighted average of
?
w
?s estimated on training transcripts to obtain a
single ?? per language (cf. Equation 10).
?? = Avg
w
[
(
1? e
?DF
w
)
?
?
P
adapt
(w)
]
(10)
Using this average as a single interpolation weight
for all terms gives near the best performance as
we observed in our parameter sweep. Table 2
contrasts the results for using the three different
interpolation heuristics on the Tagalog develop-
ment queries. Using the mean ?? instead of indi-
vidual ?
w
?s provides an additional 0.5% absolute
1322
Language ?? ATWV (%?) P (Miss) (%?)
Full LP setting
Tagalog 0.20 0.523 (+1.1) 0.396 (-1.9)
Cantonese 0.23 0.418 (+1.3) 0.458 (-1.9)
Pashto 0.19 0.419 (+1.1) 0.453 (-1.6)
Turkish 0.14 0.466 (+0.8) 0.430 (-1.3)
Vietnamese 0.30 0.420 (+0.7) 0.445 (-1.0)
English (Dev06) 0.20 0.670 (+0.3) 0.240 (-0.4)
Limited LP setting
Tagalog 0.22 0.228 (+0.9) 0.692 (-1.7)
Cantonese 0.26 0.205 (+1.0) 0.684 (-1.3)
Pashto 0.21 0.206 (+0.9) 0.682 (-0.9)
Turkish 0.16 0.202 (+1.1) 0.700 (-0.8)
Vietnamese 0.34 0.227 (+1.0) 0.646 (+0.4)
Table 3: Word-repetition re-scored results for available CTS term detection corpora
improvement, suggesting that we find additional
gains boosting low-frequency words.
5 Results
Now that we have tested word repetition-based
re-scoring on a small Tagalog development set
we want to know if our approach, and particu-
larly our ?? estimate is sufficiently robust to apply
broadly. At our disposal, we have the five BABEL
languages ? Tagalog, Cantonese, Pashto, Turk-
ish and Vietnamese ? as well as the development
data from the NIST 2006 English evaluation. The
BABEL evaluation query sets contain roughly 2000
terms each and the 2006 English query set con-
tains roughly 1000 terms.
The procedure we follow for each language
condition is as follows. We first estimate adap-
tation probabilities from the ASR training tran-
scripts. From these we take the weighted aver-
age as described previously to obtain a single in-
terpolation weight ?? for each training condition.
We train ASR acoustic and language models from
the training corpus using the Kaldi speech recog-
nition toolkit (Povey et al, 2011) following the
default BABEL training and search recipe which is
described in detail by Chen et al (2013). Lastly,
we re-score the search output by interpolating the
top term detection score for a document with sub-
sequent hits according to Equation 6 using the ??
estimated for this training condition.
For each of the BABEL languages we consider
both the FullLP (80 hours) and LimitedLP (10
hours) training conditions. For the English sys-
tem, we also train a Kaldi system on the 240 hours
of the Switchboard conversational English cor-
pus. Although Kaldi can produce multiple types
of acoustic models, for simplicity we report results
using discriminatively trained Subspace Gaussian
Mixture Model (SGMM) acoustic output densi-
ties, but we do find that similar results can be ob-
tained with other acoustic model configurations.
Using our final algorithm, we are able to boost
repeated term detections and improve results in all
languages and training conditions. Table 3 lists
complete results and the associated estimates for
??. For the BABEL languages, we observe improve-
ments in ATWV from 0.7% to 1.3% absolute and
reductions in the miss rate of 0.8% to 1.9%. The
only test for which P (Miss) did not improve was
the Vietnamese Limited LP setting, although over-
all ATWV did improve, reflecting a lower P (FA).
In all conditions we also obtain ? estimates
which correspond to our expectations for partic-
ular languages. For example, adaptation is low-
est for the agglutinative Turkish language where
longer word tokens should be less likely to re-
peat. For Vietnamese, with shorter, syllable length
word tokens, we observe the lowest adaptation es-
timates.
Lastly, the reductions in P (Miss) suggests that
we are improving the term detection metric, which
is sensitive to threshold changes, by doing what
we set out to do, which is to boost lower confi-
dence repeated words and correctly asserting them
1323
as true hits. Moreover, we are able to accomplish
this in a wide variety of languages.
6 Conclusions
Leveraging the burstiness of content words, we
have developed a simple technique to consis-
tently boost term detection performance across
languages. Using word repetitions, we effectively
use a broad document context outside of the typi-
cal 2-5 N-gram window. Furthermore, we see im-
provements across a broad spectrum of languages:
languages with syllable-based word tokens (Viet-
namese, Cantonese), complex morphology (Turk-
ish), and dialect variability (Pashto).
Secondly, our results are not only effective but
also intuitive, given that the interpolation weight
parameter matches our expectations for the bursti-
ness of the word tokens in the language on which
it is estimated.
We have focused primarily on re-scoring results
for the term detection task. Given the effective-
ness of the technique across multiple languages,
we hope to extend our effort to exploit our hu-
man tendency towards redundancy to decoding or
other aspects of the spoken document processing
pipeline.
Acknowledgements
This work was partially supported by the In-
telligence Advanced Research Projects Activity
(IARPA) via Department of Defense U.S. Army
Research Laboratory (DoD / ARL) contract num-
ber W911NF-12-C-0015. The U.S. Government
is authorized to reproduce and distribute reprints
for Governmental purposes notwithstanding any
copyright annotation thereon. Disclaimer: The
views and conclusions contained herein are those
of the authors and should not be interpreted as
necessarily representing the official policies or
endorsements, either expressed or implied, of
IARPA, DoD/ARL, or the U.S. Government.
Insightful discussions with Chiu and Rudnicky
(2013) are also gratefully acknowledged.
References
David Blei, Andrew Ng, and Michael Jordan. 2003.
Latent Dirichlet Allocation. Journal of Machine
Learning Research, 3:993?1022.
Guoguo Chen, Sanjeev Khudanpur, Daniel Povey, Jan
Trmal, David Yarowsky, and Oguz Yilmaz. 2013.
Quantifying the value of pronunciation lexicons for
keyword search in low resource languages. In Inter-
national Conference on Acoustics, Speech and Sig-
nal Processing (ICASSP). IEEE.
Berlin Chen. 2009. Latent topic modelling of word
co-occurence information for spoken document re-
trieval. In Proceedings of the International Con-
ference on Acoustics, Speech and Signal Processing
(ICASSP), pages 3961?3964. IEEE.
Justin Chiu and Alexander Rudnicky. 2013. Using
conversational word bursts in spoken term detection.
In Proceedings of the 14th Annual Conference of
the International Speech Communication Associa-
tion, pages 2247?2251. ISCA.
Kenneth Church and William Gale. 1995. Pois-
son Mixtures. Natural Language Engineering,
1(2):163?190.
Kenneth Church and William Gale. 1999. Inverse Foc-
ument Frequency (IDF): A measure of deviations
from Poisson. In Natural Language Processing Us-
ing Very Large Corpora, pages 283?295. Springer.
Kenneth Church. 2000. Empirical estimates of adap-
tation: the chance of two Noriegas is closer to p/2
than p 2. In Proceedings of the 18th Conference
on Computational Linguistics, volume 1, pages 180?
186. ACL.
Radu Florian and David Yarowsky. 1999. Dynamic
nonlocal language modeling via hierarchical topic-
based adaptation. In Proceedings of the 37th annual
meeting of the Association for Computational Lin-
guistics, pages 167?174. ACL.
Mary Harper. 2011. IARPA Solicitation IARPA-
BAA-11-02. http://www.iarpa.gov/
solicitations_babel.html.
Thomas Hofmann. 2001. Unsupervised learning
by probabilistic latent semantic analysis. Machine
Learning, 42(1):177?196.
Bo-June Paul Hsu and James Glass. 2006. Style &
topic language model adaptation using HMM-LDA.
In Proceedings of the 2006 Conference on Empirical
Methods in Natural Language Processing. ACL.
Fred Jelinek. 1997. Statistical Methods for Speech
Recognition. MIT Press.
Slava Katz. 1996. Distribution of content words and
phrases in text and language modelling. Natural
Language Engineering, 2(1):15?59.
Sanjeev Khudanpur and Jun Wu. 1999. A maxi-
mum entropy language model integrating n-grams
and topic dependencies for conversational speech
recognition. In Proceedings of the International
Conference on Acoustics, Speech, and Signal Pro-
cessing (ICASSP), volume 1, pages 553?556. IEEE.
1324
Reinhard Kneser and Volker Steinbiss. 1993. On the
dynamic adaptation of stochastic language models.
In Proceedings of the International Conference on
Acoustics, Speech, and Signal Processing (ICASSP),
volume 2, pages 586?589. IEEE.
Roland Kuhn and Renato De Mori. 1990. A cache-
based natural language model for speech recogni-
tion. Transactions on Pattern Analysis and Machine
Intelligence, 12(6):570?583.
Yang Liu and Feifan Liu. 2008. Unsupervised lan-
guage model adaptation via topic modeling based
on named entity hypotheses. In Proceedings of the
International Conference on Acoustics, Speech and
Signal Processing, (ICASSP), pages 4921?4924.
IEEE.
Welly Naptali, Masatoshi Tsuchiya, and Seiichi Naka-
gawa. 2012. Topic-dependent-class-based n-gram
language model. Transactions on Audio, Speech,
and Language Processing, 20(5):1513?1525.
NIST. 2006. The Spoken Term Detection (STD)
2006 Evaluation Plan. http://www.itl.
nist.gov/iad/mig/tests/std/2006/
docs/std06-evalplan-v10.pdf. [Online;
accessed 28-Feb-2013].
Daniel Povey, Arnab Ghoshal, Gilles Boulianne, Lukas
Burget, Ondrej Glembek, Nagendra Goel, Mirko
Hannemann, Petr Motlicek, Yanmin Qian, Petr
Schwarz, et al 2011. The Kaldi speech recogni-
tion toolkit. In Proceedings of the Automatic Speech
Recognition and Understanding Workshop (ASRU).
Xing Wei and W Bruce Croft. 2006. LDA-based doc-
ument models for ad-hoc retrieval. In Proceedings
of the ACM SIGIR Conference on Research and De-
velopment in Information Retrieval, pages 178?185.
ACM.
1325
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 133?137,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Joshua 2.0: A Toolkit for Parsing-Based Machine Translation
with Syntax, Semirings, Discriminative Training and Other Goodies
Zhifei Li, Chris Callison-Burch, Chris Dyer,? Juri Ganitkevitch,
Ann Irvine, Sanjeev Khudanpur, Lane Schwartz,? Wren N.G. Thornton,
Ziyuan Wang, Jonathan Weese and Omar F. Zaidan
Center for Language and Speech Processing, Johns Hopkins University, Baltimore, MD
? Computational Linguistics and Information Processing Lab, University of Maryland, College Park, MD
? Natural Language Processing Lab, University of Minnesota, Minneapolis, MN
Abstract
We describe the progress we have made in
the past year on Joshua (Li et al, 2009a),
an open source toolkit for parsing based
machine translation. The new functional-
ity includes: support for translation gram-
mars with a rich set of syntactic nonter-
minals, the ability for external modules to
posit constraints on how spans in the in-
put sentence should be translated, lattice
parsing for dealing with input uncertainty,
a semiring framework that provides a uni-
fied way of doing various dynamic pro-
gramming calculations, variational decod-
ing for approximating the intractable MAP
decoding, hypergraph-based discrimina-
tive training for better feature engineering,
a parallelized MERT module, document-
level and tail-based MERT, visualization
of the derivation trees, and a cleaner
pipeline for MT experiments.
1 Introduction
Joshua is an open-source toolkit for parsing-based
machine translation that is written in Java. The
initial release of Joshua (Li et al, 2009a) was a
re-implementation of the Hiero system (Chiang,
2007) and all its associated algorithms, includ-
ing: chart parsing, n-gram language model inte-
gration, beam and cube pruning, and k-best ex-
traction. The Joshua 1.0 release also included
re-implementations of suffix array grammar ex-
traction (Lopez, 2007; Schwartz and Callison-
Burch, 2010) and minimum error rate training
(Och, 2003; Zaidan, 2009). Additionally, it in-
cluded parallel and distributed computing tech-
niques for scalability (Li and Khudanpur, 2008).
This paper describes the additions to the toolkit
over the past year, which together form the 2.0 re-
lease. The software has been heavily used by the
authors and several other groups in their daily re-
search, and has been substantially refined since the
first release. The most important new functions in
the toolkit are:
? Support for any style of synchronous context
free grammar (SCFG) including syntax aug-
ment machine translation (SAMT) grammars
(Zollmann and Venugopal, 2006)
? Support for external modules to posit transla-
tions for spans in the input sentence that con-
strain decoding (Irvine et al, 2010)
? Lattice parsing for dealing with input un-
certainty, including ambiguous output from
speech recognizers or Chinese word seg-
menters (Dyer et al, 2008)
? A semiring architecture over hypergraphs
that allows many inference operations to be
implemented easily and elegantly (Li and
Eisner, 2009)
? Improvements to decoding through varia-
tional decoding and other approximate meth-
ods that overcome intractable MAP decoding
(Li et al, 2009b)
? Hypergraph-based discriminative training for
better feature engineering (Li and Khudan-
pur, 2009b)
? A parallelization of MERT?s computations,
and supporting document-level and tail-based
optimization (Zaidan, 2010)
? Visualization of the derivation trees and hy-
pergraphs (Weese and Callison-Burch, 2010)
? A convenient framework for designing and
running reproducible machine translation ex-
periments (Schwartz, under review)
The sections below give short descriptions for
each of these new functions.
133
2 Support for Syntax-based Translation
The initial release of Joshua supported only
Hiero-style SCFGs, which use a single nontermi-
nal symbol X. This release includes support for ar-
bitrary SCFGs, including ones that use a rich set
of linguistic nonterminal symbols. In particular
we have added support for Zollmann and Venu-
gopal (2006)?s syntax-augmented machine trans-
lation. SAMT grammar extraction is identical to
Hiero grammar extraction, except that one side of
the parallel corpus is parsed, and syntactic labels
replace the X nonterminals in Hiero-style rules.
Instead of extracting this Hiero rule from the bi-
text
[X]? [X,1] sans [X,2] | [X,1] without [X,2]
the nonterminals can be labeled according to
which constituents cover the nonterminal span on
the parsed side of the bitext. This constrains what
types of phrases the decoder can use when produc-
ing a translation.
[VP]? [VBN] sans [NP] | [VBN] without [NP]
[NP]? [NP] sans [NP] | [NP] without [NP]
Unlike GHKM (Galley et al, 2004), SAMT has
the same coverage as Hiero, because it allows
non-constituent phrases to get syntactic labels us-
ing CCG-style slash notation. Experimentally, we
have found that the derivations created using syn-
tactically motivated grammars exhibit more coher-
ent syntactic structure than Hiero and typically re-
sult in better reordering, especially for languages
with word orders that diverge from English, like
Urdu (Baker et al, 2009).
3 Specifying Constraints on Translation
Integrating output from specialized modules
(like transliterators, morphological analyzers, and
modality translators) into the MT pipeline can
improve translation performance, particularly for
low-resource languages. We have implemented
an XML interface that allows external modules
to propose alternate translation rules (constraints)
for a particular word span to the decoder (Irvine
et al, 2010). Processing that is separate from
the MT engine can suggest translations for some
set of source side words and phrases. The XML
format allows for both hard constraints, which
must be used, and soft constraints, which compete
with standard extracted translation rules, as well
as specifying associated feature weights. In ad-
dition to specifying translations, the XML format
allows constraints on the lefthand side of SCFG
rules, which allows constraints like forcing a par-
ticular span to be translated as an NP. We modi-
fied Joshua?s chart-based decoder to support these
constraints.
4 Semiring Parsing
In Joshua, we use a hypergraph (or packed forest)
to compactly represent the exponentially many
derivation trees generated by the decoder for an
input sentence. Given a hypergraph, we may per-
form many atomic inference operations, such as
finding one-best or k-best translations, or com-
puting expectations over the hypergraph. For
each such operation, we could implement a ded-
icated dynamic programming algorithm. How-
ever, a more general framework to specify these
algorithms is semiring-weighted parsing (Good-
man, 1999). We have implemented the in-
side algorithm, the outside algorithm, and the
inside-outside speedup described by Li and Eis-
ner (2009), plut the first-order expectation semir-
ing (Eisner, 2002) and its second-order version (Li
and Eisner, 2009). All of these use our newly im-
plemented semiring framework.
The first- and second-order expectation semi-
rings can also be used to compute many interesting
quantities over hypergraphs. These quantities in-
clude expected translation length, feature expec-
tation, entropy, cross-entropy, Kullback-Leibler
divergence, Bayes risk, variance of hypothesis
length, gradient of entropy and Bayes risk, covari-
ance and Hessian matrix, and so on.
5 Word Lattice Input
We generalized the bottom-up parsing algorithm
that generates the translation hypergraph so that
it supports translation of word lattices instead of
just sentences. Our implementation?s runtime and
memory overhead is proportional to the size of the
lattice, rather than the number of paths in the lat-
tice (Dyer et al, 2008). Accepting lattice-based
input allows the decoder to explore a distribution
over input sentences, allowing it to select the best
translation from among all of them. This is es-
pecially useful when Joshua is used to translate
the output of statistical preprocessing components,
such as speech recognizers or Chinese word seg-
menters, which can encode their alternative analy-
ses as confusion networks or lattices.
134
6 Variational Decoding
Statistical models in machine translation exhibit
spurious ambiguity. That is, the probability of an
output string is split among many distinct deriva-
tions (e.g., trees or segmentations) that have the
same yield. In principle, the goodness of a string
is measured by the total probability of its many
derivations. However, finding the best string dur-
ing decoding is then NP-hard. The first version of
Joshua implemented the Viterbi approximation,
which measures the goodness of a translation us-
ing only its most probable derivation.
The Viterbi approximation is efficient, but it ig-
nores most of the derivations in the hypergraph.
We implemented variational decoding (Li et al,
2009b), which works as follows. First, given a for-
eign string (or lattice), the MT system produces a
hypergraph, which encodes a probability distribu-
tion p over possible output strings and their deriva-
tions. Second, a distribution q is selected that ap-
proximates p as well as possible but comes from
a family of distributions Q in which inference is
tractable. Third, the best string according to q
(instead of p) is found. In our implementation,
the q distribution is parameterized by an n-gram
model, under which the second and third steps can
be performed efficiently and exactly via dynamic
programming. In this way, variational decoding
considers all derivations in the hypergraph but still
allows tractable decoding.
7 Hypergraph-based Discriminative
Training
Discriminative training with a large number of
features has potential to improve the MT perfor-
mance. We have implemented the hypergraph-
based minimum risk training (Li and Eisner,
2009), which minimizes the expected loss of the
reference translations. The minimum-risk objec-
tive can be optimized by a gradient-based method,
where the risk and its gradient can be computed
using a second-order expectation semiring. For
optimization, we use both L-BFGS (Liu et al,
1989) and Rprop (Riedmiller and Braun, 1993).
We have also implemented the average Percep-
tron algorithm and forest-reranking (Li and Khu-
danpur, 2009b). Since the reference translation
may not be in the hypergraph due to pruning or in-
herent defficiency of the translation grammar, we
need to use an oracle translation (i.e., the transla-
tion in the hypergraph that is most simmilar to the
reference translation) as a surrogate for training.
We implemented the oracle extraction algorithm
described by Li and Khudanpur (2009a) for this
purpose.
Given the current infrastructure, other training
methods (e.g., maximum conditional likelihood or
MIRA as used by Chiang et al (2009)) can also be
easily supported with minimum coding. We plan
to implement a large number of feature functions
in Joshua so that exhaustive feature engineering is
possible for MT.
8 Minimum Error Rate Training
Joshua?s MERT module optimizes parameter
weights so as to maximize performance on a de-
velopment set as measuered by an automatic eval-
uation metric, such as Bleu (Och, 2003).
We have parallelized our MERT module in
two ways: parallelizing the computation of met-
ric scores, and parallelizing the search over pa-
rameters. The computation of metric scores is
a computational concern when tuning to a met-
ric that is slow to compute, such as translation
edit rate (Snover et al, 2006). Since scoring a
candidate is independent from scoring any other
candidate, we parallelize this computation using a
multi-threaded solution1. Similarly, we parallelize
the optimization of the intermediate initial weight
vectors, also using a multi-threaded solution.
Another feature is the module?s awareness of
document information, and the capability to per-
form optimization of document-based variants of
the automatic metric (Zaidan, 2010). For example,
in document-based Bleu, a Bleu score is calculated
for each document, and the tuned score is the aver-
age of those document scores. The MERT module
can furthermore be instructed to target a specific
subset of those documents, namely the tail subset,
where only the subset of documents with the low-
est document Bleu scores are considered.2
More details on the MERT method and the im-
plementation can be found in Zaidan (2009).3
1Based on sample code by Kenneth Heafield.
2This feature is of interest to GALE teams, for instance,
since GALE?s evaluation criteria place a lot of focus on trans-
lation quality of tail documents.
3The module is also available as a standalone applica-
tion, Z-MERT, that can be used with other MT systems.
(Software and documentation at: http://cs.jhu.edu/
?ozaidan/zmert.)
135
9 Visualization
We created tools for visualizing two of the
main data structures used in Joshua (Weese and
Callison-Burch, 2010). The first visualizer dis-
plays hypergraphs. The user can choose from a
set of input sentences, then call the decoder to
build the hypergraph. The second visualizer dis-
plays derivation trees. Setting a flag in the con-
figuration file causes the decoder to output parse
trees instead of strings, where each nonterminal is
annotated with its source-side span. The visual-
izer can read in multiple n-best lists in this format,
then display the resulting derivation trees side-by-
side. We have found that visually inspecting these
derivation trees is useful for debugging grammars.
We would like to add visualization tools for
more parts of the pipeline. For example, a chart
visualizer would make it easier for researchers to
tell where search errors were happening during
decoding, and why. An alignment visualizer for
aligned parallel corpora might help to determine
how grammar extraction could be improved.
10 Pipeline for Running MT
Experiments
Reproducing other researchers? machine transla-
tion experiments is difficult because the pipeline is
too complex to fully detail in short conference pa-
pers. We have put together a workflow framework
for designing and running reproducible machine
translation experiments using Joshua (Schwartz,
under review). Each step in the machine transla-
tion workflow (data preprocessing, grammar train-
ing, MERT, decoding, etc) is modeled by a Make
script that defines how to run the tools used in that
step, and an auxiliary configuration file that de-
fines the exact parameters to be used in that step
for a particular experimental setup. Workflows
configured using this framework allow a complete
experiment to be run ? from downloading data and
software through scoring the final translated re-
sults ? by executing a single Makefile.
This framework encourages researchers to sup-
plement research publications with links to the
complete set of scripts and configurations that
were actually used to run the experiment. The
Johns Hopkins University submission for the
WMT10 shared translation task was implemented
in this framework, so it can be easily and exactly
reproduced.
Acknowledgements
Research funding was provided by the NSF un-
der grant IIS-0713448, by the European Commis-
sion through the EuroMatrixPlus project, and by
the DARPA GALE program under Contract No.
HR0011-06-2-0001. The views and findings are
the authors? alone.
References
Kathy Baker, Steven Bethard, Michael Bloodgood,
Ralf Brown, Chris Callison-Burch, Glen Copper-
smith, Bonnie Dorr, Wes Filardo, Kendall Giles,
Anni Irvine, Mike Kayser, Lori Levin, Justin Mar-
tineau, Jim Mayfield, Scott Miller, Aaron Phillips,
Andrew Philpot, Christine Piatko, Lane Schwartz,
and David Zajic. 2009. Semantically informed ma-
chine translation (SIMT). SCALE summer work-
shop final report, Human Language Technology
Center Of Excellence.
David Chiang, Kevin Knight, and Wei Wang. 2009.
11,001 new features for statistical machine transla-
tion. In NAACL, pages 218?226.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
Christopher Dyer, Smaranda Muresan, and Philip
Resnik. 2008. Generalizing word lattice transla-
tion. In Proceedings of ACL-08: HLT, pages 1012?
1020, Columbus, Ohio, June. Association for Com-
putational Linguistics.
Jason Eisner. 2002. Parameter estimation for proba-
bilistic finite-state transducers. In ACL.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a translation rule?
In HLT-NAACL.
Joshua Goodman. 1999. Semiring parsing. Computa-
tional Linguistics, 25(4):573?605.
Ann Irvine, Mike Kayser, Zhifei Li, Wren Thornton,
and Chris Callison-Burch. 2010. Integrating out-
put from specialized modules in machine transla-
tion: Transliteration in joshua. The Prague Bulletin
of Mathematical Linguistics, 93:107?116.
Zhifei Li and Jason Eisner. 2009. First- and second-
order expectation semirings with applications to
minimum-risk training on translation forests. In
EMNLP, Singapore.
Zhifei Li and Sanjeev Khudanpur. 2008. A scalable
decoder for parsing-based machine translation with
equivalent language model state maintenance. In
ACL SSST, pages 10?18.
Zhifei Li and Sanjeev Khudanpur. 2009a. Efficient
extraction of oracle-best translations from hyper-
graphs. In Proceedings of NAACL.
136
Zhifei Li and Sanjeev Khudanpur. 2009b. Forest
reranking for machine translation with the percep-
tron algorithm. In GALE book chapter on ?MT
From Text?.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri
Ganitkevitch, Sanjeev Khudanpur, Lane Schwartz,
Wren Thornton, Jonathan Weese, and Omar. Zaidan.
2009a. Joshua: An open source toolkit for parsing-
based machine translation. In WMT09.
Zhifei Li, Jason Eisner, and Sanjeev Khudanpur.
2009b. Variational decoding for statistical machine
translation. In ACL.
Dong C. Liu, Jorge Nocedal, Dong C. Liu, and Jorge
Nocedal. 1989. On the limited memory bfgs
method for large scale optimization. Mathematical
Programming, 45:503?528.
Adam Lopez. 2007. Hierarchical phrase-based trans-
lation with suffix arrays. In EMNLP-CoNLL.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In ACL.
Martin Riedmiller and Heinrich Braun. 1993. A
direct adaptive method for faster backpropagation
learning: The rprop algorithm. In IEEE INTER-
NATIONAL CONFERENCE ON NEURAL NET-
WORKS, pages 586?591.
Lane Schwartz and Chris Callison-Burch. 2010. Hier-
archical phrase-based grammar extraction in joshua.
The Prague Bulletin of Mathematical Linguistics,
93:157?166.
Lane Schwartz. under review. Reproducible results in
parsing-based machine translation: The JHU shared
task submission. In WMT10.
Matthew Snover, Bonnie J. Dorr, and Richard
Schwartz. 2006. A study of translation edit rate
with targeted human annotation. In AMTA.
Jonathan Weese and Chris Callison-Burch. 2010. Vi-
sualizing data structures in parsing-based machine
translation. The Prague Bulletin of Mathematical
Linguistics, 93:127?136.
Omar F. Zaidan. 2009. Z-MERT: A fully configurable
open source tool for minimum error rate training of
machine translation systems. The Prague Bulletin of
Mathematical Linguistics, 91:79?88.
Omar F. Zaidan. 2010. Document- and tail-based min-
imum error rate training of machine translation sys-
tems. In preparation.
Andreas Zollmann and Ashish Venugopal. 2006. Syn-
tax augmented machine translation via chart pars-
ing. In Proceedings of the NAACL-2006 Workshop
on Statistical Machine Translation (WMT-06), New
York, New York.
137
NAACL-HLT 2012 Workshop: Will We Ever Really Replace the N-gram Model? On the Future of Language Modeling for HLT, pages 50?58,
Montre?al, Canada, June 8, 2012. c?2012 Association for Computational Linguistics
Revisiting the Case for Explicit Syntactic Information in Language Models
Ariya Rastrow, Sanjeev Khudanpur, Mark Dredze
Human Language Technology Center of Excellence,
Center for Language and Speech Processing, Johns Hopkins University
Baltimore, MD USA
{ariya,khudanpur,mdredze}@jhu.edu
Abstract
Statistical language models used in deployed
systems for speech recognition, machine
translation and other human language tech-
nologies are almost exclusively n-gram mod-
els. They are regarded as linguistically na??ve,
but estimating them from any amount of text,
large or small, is straightforward. Further-
more, they have doggedly matched or out-
performed numerous competing proposals for
syntactically well-motivated models. This un-
usual resilience of n-grams, as well as their
weaknesses, are examined here. It is demon-
strated that n-grams are good word-predictors,
even linguistically speaking, in a large major-
ity of word-positions, and it is suggested that
to improve over n-grams, one must explore
syntax-aware (or other) language models that
focus on positions where n-grams are weak.
1 Introduction
Language models (LM) are crucial components in
tasks that require the generation of coherent natu-
ral language text, such as automatic speech recog-
nition (ASR) and machine translation (MT). Most
language models rely on simple n-gram statistics
and a wide range of smoothing and backoff tech-
niques (Chen and Goodman, 1998). State-of-the-art
ASR systems use (n ? 1)-gram equivalence classi-
fication for the language model (which result in an
n-gram language model).
While simple and efficient, it is widely believed
that limiting the context to only the (n ? 1) most
recent words ignores the structure of language, and
several statistical frameworks have been proposed
to incorporate the ?syntactic structure of language
back into language modeling.? Yet despite consider-
able effort on including longer-dependency features,
such as syntax (Chelba and Jelinek, 2000; Khudan-
pur and Wu, 2000; Collins et al, 2005; Emami
and Jelinek, 2005; Kuo et al, 2009; Filimonov and
Harper, 2009), n-gram language models remain the
dominant technique in automatic speech recognition
and machine translation (MT) systems.
While intuition suggests syntax is important, the
continued dominance of n-gram models could in-
dicate otherwise. While no one would dispute that
syntax informs word choice, perhaps sufficient in-
formation aggregated across a large corpus is avail-
able in the local context for n-gram models to per-
form well even without syntax. To clearly demon-
strate the utility of syntactic information and the de-
ficiency of n-gram models, we empirically show that
n-gram LMs lose significant predictive power in po-
sitions where the syntactic relation spans beyond the
n-gram context. This clearly shows a performance
gap in n-gram LMs that could be bridged by syntax.
As a candidate syntactic LM we consider the
Structured Language Model (SLM) (Chelba and Je-
linek, 2000), one of the first successful attempts to
build a statistical language model based on syntac-
tic information. The SLM assigns a joint probabil-
ity P (W,T ) to every word sequence W and every
possible binary parse tree T , where T ?s terminals
are words W with part-of-speech (POS) tags, and
its internal nodes comprise non-terminal labels and
lexical ?heads? of phrases. Other approaches in-
clude using the exposed headwords in a maximum-
entropy based LM (Khudanpur and Wu, 2000), us-
50
ing exposed headwords from full-sentence parse tree
in a neural network based LM (Kuo et al, 2009),
and the use of syntactic features in discriminative
training (Rastrow et al, 2011). We show that the
long-dependencies modeled by SLM, significantly
improves the predictive power of the LM, specially
in positions where the syntactic relation is beyond
the reach of regular n-gram models.
2 Weaknesses of n-gram LMs
Consider the following sentence, which demon-
strates why the (n? 1)-gram equivalence classifica-
tion of history in n-gram language models may be
insufficient:
<s> i asked the vice president for
his endorsement </s>
In an n-gram LM, the word for would be modeled
based on a 3-gram or 4-gram history, such as <vice
president> or <the vice president>.
Given the syntactic relation between the preposition
for and the verb asked (which together make a
compound verb), the strongest evidence in the his-
tory (and hence the best classification of the history)
for word for should be <asked president>,
which is beyond the 4-gram LM. Clearly, the
syntactic relation between a word position and the
corresponding words in the history spans beyond
the limited (n ? 1)-gram equivalence classification
of the history.
This is but one of many examples used for moti-
vating syntactic features (Chelba and Jelinek, 2000;
Kuo et al, 2009) in language modeling. How-
ever, it is legitimate to ask if this deficiency could
be overcome through sufficient data, that is, accu-
rate statistics could somehow be gathered for the n-
grams even without including syntactic information.
We empirically show that (n? 1)-gram equivalence
classification of history is not adequate to predict
these cases. Specifically, n-gram LMs lose predic-
tive power in the positions where the headword rela-
tion, exposed by the syntactic structure, goes beyond
(n? 1) previous words (in the history.)
We postulate the following three hypotheses:
Hypothesis 1 There is a substantial difference in
the predictive power of n-gram LMs at positions
within a sentence where syntactic dependencies
reach further back than the n-gram context versus
positions where syntactic dependencies are local.
Hypothesis 2 This difference does not diminish by
increasing training data by an order of magnitude.
Hypothesis 3 LMs that specifically target positions
with syntactically distant dependencies will comple-
ment or improve over n-gram LMs for these posi-
tions.
In the following section (Section 3), we present a set
of experiments to support the hypotheses 1 and 2.
Section 4 introduces a SLM which uses dependency
structures followed by experiments in Section 5.
3 Experimental Evidence
In this section, we explain our experimental evi-
dence for supporting the hypotheses stated above.
First, Section 3.1 presents our experimental design
where we use a statistical constituent parser to iden-
tify two types of word positions in a test data,
namely positions where the headword syntactic re-
lation spans beyond recent words in the history and
positions where the headword syntactic relation is
within the n-gram window. The performance of
an n-gram LM is measured on both types of posi-
tions to show substantial difference in the predictive
power of the LM in those positions. Section 3.3 de-
scribes the results and analysis of our experiments
which supports our hypotheses.
Throughout the rest of the paper, we refer to
a position where the headword syntactic relation
reaches further back than the n-gram context as a
syntactically-distant position and other type of posi-
tions is referred to as a syntactically-local position.
3.1 Design
Our experimental design is based on the idea of
comparing the performance of n-gram LMs for
syntactically-distant vs. syntactically-local . To this
end, we first parse each sentence in the test set us-
ing a constituent parser, as illustrated by the exam-
ple in Figure 1. For each word wi in each sentence,
we then check if the ?syntactic heads? of the preced-
ing constituents in the parse ofw1, w2, ? ? ? , wi?1 are
within an (n? 1) window of wi. In this manner, we
split the test data into two disjoint sets, M and N ,
51
!"!#$%&'!!!!!()&!*"+&!,-&$"'&.(!!!!!!/0-!!!!!!!)"$!&.'0-$&1&.(!
232!
42!
5!
67!
62!
89! 442! 442!
42!
:4! 232;! 44!
442!
22!
#$%&'!
#$%&'!
/0-!
&.'0-$&1&.(!,-&$"'&.(!"!
Figure 1: Example of a syntactically distant position in
a sentence: the exposed headwords preceding for are
h.w?2 =asked and h.w?1 = president, while the
two preceding words are wi?2 = vice and wi?1 =
president.
as follows,
M = {j|positions s.t h.w?1, h.w?2 = wj?1, wj?2}
N = {j|positions s.t h.w?1, h.w?2 6= wj?1, wj?2}
Here, h?1 and h?2 correspond, respectively, to the
two previous exposed headwords at position i, based
on the syntactic structure. Therefore, M corre-
sponds to the positions in the test data for which two
previous exposed heads match exactly the two previ-
ous words. Whereas, N corresponds to the position
where at least on of the exposed heads is further back
in the history than the two previous words, possibly
both.
To extract the exposed headwords at each posi-
tion, we use a constituent parser to obtain the syn-
tactic structure of a sentence followed by headword
percolation procedure to get the headwords of cor-
responding syntactic phrases in the parse tree. The
following method, described in (Kuo et al, 2009),
is then used to extract exposed headwords from the
history of position i from the full-sentence parse
trees:
1. Start at the leaf corresponding to the word posi-
tion (wi) and the leaf corresponding to the pre-
vious context word (wi?1).
2. From each leaf, go up the tree until the two
paths meet at the lowest common ancestor
(LCA).
3. Cut the link between the LCA and the child that
is along the path from the context word wi?1.
The head word of the the LCA child, the one
that is cut, is chosen as previous exposed head-
word h.w?1.
These steps may be illustrated using the parse tree
shown in Figure 1. Let us show the procedure for
our example from Section 2. Figure 1 shows the cor-
responding parse tree of our example. Considering
word position wi=for and wi?1=president and
applying the above procedure, the LCA is the node
VPasked. Now, by cutting the link from VPasked to
NPpresident the word president is obtained as
the first exposed headword (h.w?1).
After the first previous exposed headword has
been extracted, the second exposed headword also
can be obtained using the same procedure, with
the constraint that the node corresponding the sec-
ond headword is different from the first (Kuo et al,
2009). More precisely,
1. set k = 2
2. Apply the above headword extraction method
between wi and wi?k.
3. if the extracted headword has previously been
chosen, set k = k + 1 and go to step (2).
4. Otherwise, return the headword as h.w?2.
Continuing with the example of Figure 1, after
president is chosen as h.w?1, asked is cho-
sen as h.w?2 of position for by applying the pro-
cedure above. Therefore, in this example the po-
sition corresponding to word for belongs to the
set N as the two extracted exposed headwords
(asked,president) are different from the two
previous context words (vice,president).
After identifying sets N andM in our test data,
we measure perplexity of n-gram LMs on N , M
and N ?M separately. That is,
PPLN?M = exp
[
?
?
i?N?M log p(wi|W i?1i?n+1)
|N ?M|
]
PPLN = exp
[
?
?
i?N
log p(wi|W i?1i?n+1)
|N |
]
PPLM = exp
[
?
?
i?M
log p(wi|W i?1i?n+1)
|M|
]
,
52
where p(wi|wi?1wi?2 ? ? ?wi?n+1) is the condi-
tional probability calculated by an n-gram LM at
position i and |.| is the size (in number of words)
of the corresponding portion of the test.
In addition, to show the performance of n-gram
LMs as a function of training data size, we train
different n-gram LMs on 10%,20%,? ? ? ,100% of a
large corpus of text and report the PPL numbers us-
ing each trained LM with different training data size.
For all sizes less than 100%, we select 10 random
subset of the training corpus of the required size, and
report the average perplexity of 10 n-gram models.
This will enable us to observe the improvement of
the n-gram LMs on as we increase the training data
size. The idea is to test the hypothesis that not only
is there significant gap between predictive power of
the n-gram LMs on setsN andM, but also that this
difference does not diminish by adding more train-
ing data. In other words, we want to show that the
problem is not due to lack of robust estimation of
the model parameters but due to the fact that the in-
cluded features in the model (n-grams) are not in-
formative enough for the positions N .
3.2 Setup
The n-gram LMs are built on 400M words from
various Broadcast News (BN) data sources includ-
ing (Chen et al, 2006): 1996 CSR Hub4 Language
Model data, EARS BN03 closed captions, GALE
Phase 2 Distillation GNG Evaluation Supplemen-
tal Multilingual data, Hub4 acoustic model training
scripts (corresponding to the 300 Hrs), TDT4 closed
captions, TDT4 newswire, GALE Broadcast Con-
versations, and GALE Broadcast News. All the LMs
are trained using modified Kneser-Ney smoothing.
To build the LMs, we sample from each source and
build a source specific LM on the sampled data. The
final LMs are then built by interpolating those LMs.
Also, we do not apply any pruning to the trained
LMs, a step that is often necessary for speech recog-
nition but not so for perplexity measurement. The
test set consists of the NIST rt04 evaluation data set,
dev04f evaluation set, and rt03 evaluation set. The
test data includes about 70K words.
We use the parser of (Huang and Harper, 2009),
which achieves state-of-the-art performance on
broadcast news data, to identify the word poisons
that belong to N and M, as was described in Sec-
tion 3.1. The parser is trained on the Broadcast News
treebank from Ontonotes (Weischedel et al, 2008)
and the WSJ Penn Treebank (Marcus et al, 1993)
along with self-training on 1996 Hub4 CSR (Garo-
folo et al, 1996) utterances.
3.3 Analysis
We found that |N ||N?M| ? 0.25 in our test data. In
other words, two previous exposed headwords go
beyond 2-gram history for about 25% of the test
data.
!"#
$%#
$"#
&%#
&"#
'%%#
'%# (%# )%# *%# "%# +%# !%# $%# &%# '%%#
,-./
01-#
223#
456#
78/9:9:;#</=/#>9?-#456#
@AB# @# B#
(a)
!"#
$%#
$"#
&%#
&"#
'%#
'"#
(%%#
(%# )%# *%# +%# "%# !%# $%# &%# '%# (%%#
,-./
01-#
223#4
56#
78/9:9:;#</=/#>9?-#456#
@AB# @# B#
(b)
Figure 2: Reduction in perplexity with increasing training
data size on the entire test setN +M, on its syntactically
local subset M, and the syntactically distant subset N .
The figure shows relative perplexity instead of absolute
perplexity ? 100% being the perplexity for the smallest
training set size ? so that (a) 3-gram and (b) 4-gram LMs
may be directly compared.
We train 3-gram and 4-gram LMs on
10%,20%,? ? ? ,100% of the BN training data,
where each 10% increase corresponds to about
40M words of training text data. Figure 2 shows
reduction in perplexity with increasing training data
size on the entire test setN+M, on its syntactically
local subsetM, and the syntactically distant subset
N . The figure basically shows relative perplexity
instead of absolute perplexity ? 100% being the
53
Position Training Data Size
in 40M words 400M words
Test Set 3-gram 4-gram 3-gram 4-gram
M 166 153 126 107
N 228 217 191 171
N +M 183 170 143 123
PPLN
PPLM
138% 142% 151% 161%
Table 1: Perplexity of 3-gram and 4-gram LMs on syntac-
tically local (M) and syntactically distant (N ) positions
in the test set for different training data sizes, showing the
sustained higher perplexity in distant v/s local positions.
perplexity for the smallest training set size ? so the
rate of improvement for 3-grams and 4-gram LMs
can be compared. As can be seen from Figure 2,
there is a substantial gap between the improvement
rate of perplexity in syntactically distant positions
compared to that in syntactically local positions
(with 400M woods of training data, this gap is about
10% for both 3-gram and 4-gram LMs). In other
words, increasing the training data size has much
more effect on improving the predictive power of
the model for the positions included inM. Also, by
comparing Figure 2(a) to 2(b) one can observe that
the gap is not overcome by increasing the context
length (using 4-gram features).
Also, to better illustrate the performance of the n-
gram LMs for different portions of our test data, we
report the absolute values of PPL results in Table 1.
It can be seen that there exits a significant difference
between perplexity of sets N and M and that the
difference gets larger as we increase the training data
size.
4 Dependency Language Models
To overcome the lack of predictive power of n-gram
LMs in syntactically-distant positions, we use the
SLM framework to build a long-span LM. Our hope
is to show not only that long range syntactic depen-
dencies improve over n-gram features, but also that
the improvement is largely due to better predictive
power in the syntactically distant positions N .
Syntactic information may be encoded in terms
of headwords and headtags of phrases, which may
be extracted from a syntactic analysis of a sen-
tence (Chelba and Jelinek, 2000; Kuo et al, 2009),
such as a dependency structure. A dependency in
a sentence holds between a dependent (or modifier)
word and a head (or governor) word: the dependent
depends on the head. These relations are encoded in
a dependency tree (Figure 3), a directed graph where
each edge (arc) encodes a head-dependent relation.
The specific parser used to obtain the syntactic
structure is not important to our investigation. What
is crucial, however, is that the parser proceeds left-
to-right, and only hypothesized structures based on
w1, . . . , wi?1 are used by the SLM to predict wi.
Similarly, the specific features used by the parser
are also not important: more noteworthy is that the
SLM uses (h.w?3, h.w?2, h.w?1) and their POS
tags to predict wi. The question is whether this
yields lower perplexity than predicting wi from
(wi?3, wi?2, wi?1).
For the sake of completeness, we next describe
the parser and SLM in some detail, but either may
be skipped without loss of continuity.
The Parser: We use the shift-reduce incremen-
tal dependency parser of (Sagae and Tsujii, 2007),
which constructs a tree from a transition sequence
governed by a maximum-entropy classifier. Shift-
reduce parsing places input words into a queue Q
and partially built structures are organized by a stack
S. Shift and reduce actions consume the queue and
build the output parse on the stack. The classi-
fier g assigns probabilities to each action, and the
probability of a state pg(pi) can be computed as the
product of the probabilities of a sequence of ac-
tions that resulted in the state. The parser therefore
provides (multiple) syntactic analyses of the history
w1, . . . , wi?1 at each word position wi.
The Dependency Language Model: Parser states
at position wi, called history-states, are denoted
??i = {pi0?i, pi1?i ? ? ? , piKi?i }, where Ki is the total
number of such states. Given ??i, the probability
assignment for wi is given by
p(wi|W?i) =
|??i|?
j=1
p
(
wi|f(pij?i)
)
pg(pij?i|W?i) (1)
where, W?i is the word history w1, . . . , wi?1 for
wi, pij?i is the jth history-state of position i,
pg(pij?i|W?i) is the probability assigned to pi
j
?i by
54
step
action stack queue
i asked the vice president ...-0
asked the vice president ...shift1 i
the vice president for ...shift2 i asked
the vice president for ...left-reduce3 asked
i
for his endorsement ...shift6 asked the vice president
i
for his endorsement ...left-reduce7 asked the president
i vice
<s>   i   asked   the vice president   for    his  endorsement
Thursday, March 29, 12
for his endorse ent ...left-reduce8 asked president
i
vicethe
for his endorsement ...right-reduce9 asked
i
vicethe
president
Thursday, March 29, 12
step
action stack queue
i asked the vice president ...-0
asked the vice president ...shift1 i
the vice president for ...shift2 i asked
the vice president for ...left-reduce3 asked
i
for his endorsement ...shift6 asked the vice president
i
for his endorsement ...left-reduce7 asked the president
i vice
<s>   i   asked   the vice president   for    his  endorsement
Thursday, March 29, 12
Tuesday, April 3, 12
Figure 3: Actions of a shift-reduce parser to produce
the dependency structure (up to the word president)
shown above.
the parser, and f(pij?i) denotes an equivalence clas-
sification of the parser history-state, capturing fea-
tures from pij?i that are useful for predicting wi.
We restrict f(pi) to be based on only the heads of
the partial trees {s0 s1 ? ? ? } in the stack. For exam-
ple, in Figure 3, one possible parser state for pre-
dicting the word for is the entire stack shown after
step 8, but we restrict f(?) to depend only on the
headwords asked/VB and president/NNP.
Given a choice of f(?), the parameters of the
model p(wi|f(pij?i)) are estimated to maximize the
log-likelihood of the training data T using the
Baum-Welch algorithm (Baum, 1972), and the re-
sulting estimate is denoted pML(wi|f(pij?i)).
The estimate pML(w|f(?)) must be smoothed to
handle unseen events, which we do using the method
of Jelinek and Mercer (1980). We use a fine-to-
coarse hierarchy of features of the history-state as
illustrated in Figure 4. With
fM (pi?i) ? fM?1(pi?i) ? . . . ? f1(pi?i)
denoting the set of M increasingly coarser equiv-
alence classifications of the history-state pi?i,
we linearly interpolate the higher order esti-
mates pML
(
w|fm(pi?i)
)
with lower order estimates
pML
(
w|fm?1(pi?i)
)
as
pJM(wi|fm(pi?i))
= ?fmpML(wi|fm(pi?i))
+(1? ?fm)pJM(wi|fm?1(pi?i)),
for 1 ? m ? M , where the 0-th order model
pJM(wi|f0(pi?i)) is a uniform distribution.
HW+HT :
(h.w0h.t0, h.w 1h.t 1, h.w 2h.t 2)
(h.w0h.t0)
()
(h.w0, h.t0, h.w 1, h.t 1, h.t 2)
(h.w0, h.t0, h.t 1)
(h.t0)
Saturday, April 14, 12
Figure 4: The hierarchal scheme of fine-to-coarse con-
texts used for Jelinek-Mercer smoothing in the SLM.
The coefficients ?fm(pi?i) are estimated on a held-
out set using the bucketing algorithm suggested by
Bahl (1983), which ties ?fm(pi?i)?s based on the
count of fm(pi?i)?s in the training data. We use the
expected count of the features from the last iteration
of EM training, since the pi?i are latent states.
We perform the bucketing algorithm for each level
f1, f2, ? ? ? , fM of equivalence classification sepa-
rately, and estimate the bucketed ?c(fm) using the
Baum-Welch algorithm (Baum, 1972) to maximize
the likelihood of held out data, where the word prob-
ability assignment in Eq. 1 is replaced with:
p(wi|W?i) =
|?i|?
j=1
pJM
(
wi|fM (pij?i)
)
pg(pij?i|W?i).
The hierarchy shown in Figure 4 is used1 for obtain-
ing a smooth estimate pJM(?|?) at each level.
5 SLM Experiments
We train a dependency SLM for two different tasks,
namely Broadcast News (BN) and Wall Street Jour-
nal (WSJ). Unlike Section 3.2, where we swept
through multiple training sets of multiple sizes,
1The original SLM hierarchical interpolation scheme is ag-
gressive in that it drops both the tag and headword from the
history. However, in many cases the headword?s tag alone is
sufficient, suggesting a more gradual interpolation. Keeping the
headtag adds more specific information and at the same time
is less sparse. A similar idea is found, e.g., in the back-off hi-
erarchical class n-gram language model (Zitouni, 2007) where
instead of backing off from the n-gram right to the (n ? 1)-
gram a more gradual backoff ? by considering a hierarchy of
fine-to-coarse classes for the last word in the history? is used.
55
training the SLM is computationally intensive. Yet,
useful insights may be gained from the 40M word
case. So we choose the source of text most suitable
for each task, and proceed as follows.
5.1 Setup
The following summarizes the setup for each
task:
? BN setup : EARS BN03 corpus, which has
about 42M words serves as our training text.
We also use rt04 (45K words) as our evaluation
data. Finally, to interpolate our structured lan-
guage models with the baseline 4-gram model,
we use rt03+dev04f (about 40K words) data sets
to serve as our development set. The vocabulary
we use in BN experiments has about 84K words.
? WSJ setup : The training text consists of about
37M words. We use eval92+eval93 (10K
words) as our evaluation set and dev93 (9K
words) serves as our development set for inter-
polating SLMs with the baseline 4-gram model.
In both cases, we sample about 20K sentences from
the training text (we exclude them from training
data) to serve as our heldout data for applying the
bucketing algorithm and estimating ??s. To apply
the dependency parser, all the data sets are first
converted to Treebank-style tokenization and POS-
tagged using the tagger of (Tsuruoka et al, 2011)2.
Both the POS-tagger and the shift-reduce depen-
dency parser are trained on the Broadcast News tree-
bank from Ontonotes (Weischedel et al, 2008) and
the WSJ Penn Treebank (after converting them to
dependency trees) which consists of about 1.2M to-
kens. Finally, we train a modified kneser-ney 4-gram
LM on the tokenized training text to serve as our
baseline LM, for both experiments.
5.2 Results and Analysis
Table 2 shows the perplexity results for BN and WSJ
experiments, respectively. It is evident that the 4-
gram baseline for BN is stronger than the 40M case
of Table 1. Yet, the interpolated SLM significantly
improves over the 4-gram LM, as it does for WSJ.
2To make sure we have a proper LM, the POS-tagger and
dependency parser only use features from history to tag a word
position and produce the dependency structure. All lookahead
features used in (Tsuruoka et al, 2011) and (Sagae and Tsujii,
Language Model Dev Eval
BN
Kneser-Ney 4-gram 165 158
SLM 168 159
KN+SLM Interpolation 147 142
WSJ
Kneser-Ney 4-gram 144 121
SLM 149 125
KN+SLM Interpolation 132 110
Table 2: Test set perplexities for different LMs on the BN
and WSJ tasks.
Also, to show that, in fact, the syntactic depen-
dencies modeled through the SLM parameterization
is enhancing predictive power of the LM in the prob-
lematic regions, i.e. syntactically-distant positions,
we calculate the following (log) probability ratio for
each position in the test data,
log pKN+SLM(wi|W?i)pKN(wi|W?i)
, (2)
where pKN+SLM is the word probability assign-
ment of the interpolated SLM at each position, and
pKN(wi) is the probability assigned by the baseline
4-gram model. The quantity above measures the im-
provement (or degradation) gained as a result of us-
ing the SLM parameterization3.
Figures 5(a) and 5(b) illustrate the histogram of
the above probability ratio for all the word positions
in evaluation data of BN and WSJ tasks, respectively.
In these figures the histograms for syntactically-
distant and syntactically-local are shown separately
to measure the effect of the SLM for either of the
position types. It can be observed in the figures
that for both tasks the percentage of positions with
log pKN+SLM(wi|W?i)pKN(wi|W?i) around zero is much higher for
syntactically-local (blue bars) than the syntactically-
distant (red bars). To confirm this, we calculate
the average log pKN+SLM(wi|W?i)pKN(wi|W?i) ?this is the aver-
age log-likelihood improvement, which is directly
2007) are excluded.
3If log pKN+SLM(wi|W?i)pKN(wi|W?i) is greater than zero, then the SLM
has a better predictive power for word position wi. This is a
meaningful comparison due to the fact that the probability as-
signment using both SLM and n-gram is a proper probability
(which sums to one over all words at each position).
56
?1 ?0.5 0 0.5 1 1.5 2 2.5 3 3.5 40
2
4
6
8
10
12
14
16
Probability Ratio (Log)
Percent
age Pos
itions (%)
 
 Syntactically?local positions    (mean=0.1372)Syntactically?distant postions  (mean=0.2351)
(a) BN
?1 ?0.5 0 0.5 1 1.5 2 2.5 3 3.5 402
46
810
1214
1618
2022
Probability Ratio (Log)
Percent
age Pos
itions (%)
 
 Syntactically?local positions    (mean=0.0984)Syntactically?distant postions  (mean=0.2124)
(b) WSJ
Figure 5: Probability ratio histogram of SLM to 4-gram
model for (a) BN task (b) WSJ task.
related to perplexity improvement? for each posi-
tion type in the figures.
Table 3, reports the perplexity performance of
each LM (baseline 4-gram, SLM and interpolated
SLM) on different positions of the evaluation data
for BN and WSJ tasks. As it can be observed from
this table, the use of long-span dependencies in the
SLM partially fills the gap between the performance
of the baseline 4-gram LM on syntactically-distant
positionsN versus syntactically-local positionsM.
In addition, it can be seen that the SLM by itself
fills the gap substantially, however, due to its under-
lying parameterization which is based on Jelinek-
Mercer smoothing it has a worse performance on
regular syntactically-local positions (which account
for the majority of the positions) compared to the
Kneser-Ney smoothed LM4. Therefore, to improve
the overall performance, the interpolated SLM takes
advantage of both the better modeling performance
of Kneser-Ney for syntactically-local positions and
4This is merely due to the superior modeling power and
better smoothing of the Kneser-Ney LM (Chen and Goodman,
1998).
Test Set 4-gram SLM 4-gram + SLM
Position BN
M 146 152 132
N 201 182 171
N +M 158 159 142
PPLN
PPLM
138% 120% 129%
WSJ
M 114 120 105
N 152 141 131
N +M 121 125 110
PPLN
PPLM
133% 117% 125%
Table 3: Perplexity on the BN and WSJ evaluation sets for
the 4-gram LM, SLM and their interpolation. The SLM
has lower perplexity than the 4-gram in syntactically dis-
tant positions N , and has a smaller discrepancy PPLNPPLM
between preplexity on the distant and local predictions,
complementing the 4-gram model.
the better features included in the SLM for improv-
ing predictive power on syntactically-distant posi-
tions.
6 Conclusion
The results of Table 1 and Figure 2 suggest that
predicting the next word is about 50% more diffi-
cult when its syntactic dependence on the history
reaches beyond n-gram range. They also suggest
that this difficulty does not diminish with increas-
ing training data size. If anything, the relative diffi-
culty of word positions with nonlocal dependencies
relative to those with local dependencies appears to
increase with increasing training data and n-gram
order. Finally, it appears that language models that
exploit long-distance syntactic dependencies explic-
itly at positions where the n-gram is least effective
are beneficial as complementary models.
Tables 2 and 3 demonstrates that a particular,
recently-proposed SLM with such properties im-
proves a 4-gram LM trained on a large corpus.
Acknowledgments
Thanks to Kenji Sagae for sharing his shift-reduce
dependency parser and the anonymous reviewers for
helpful comments.
57
References
LR Bahl. 1983. A maximum likelihood approach to
continuous speech recognition. IEEE Transactions
on Pattern Analysis and Machine Inteligence (PAMI),
5(2):179?190.
L. E. Baum. 1972. An equality and associated maxi-
mization technique in statistical estimation for proba-
bilistic functions of Markov processes. Inequalities,
3:1?8.
C. Chelba and F. Jelinek. 2000. Structured lan-
guage modeling. Computer Speech and Language,
14(4):283?332.
SF Chen and J Goodman. 1998. An empirical study of
smoothing techniques for language modeling. Techni-
cal report, Computer Science Group, Harvard Univer-
sity.
S. Chen, B. Kingsbury, L. Mangu, D. Povey, G. Saon,
H. Soltau, and G. Zweig. 2006. Advances in speech
transcription at IBM under the DARPA EARS pro-
gram. IEEE Transactions on Audio, Speech and Lan-
guage Processing, pages 1596?1608.
M Collins, B Roark, and M Saraclar. 2005. Discrimina-
tive syntactic language modeling for speech recogni-
tion. In ACL.
Ahmad Emami and Frederick Jelinek. 2005. A Neu-
ral Syntactic Language Model. Machine learning,
60:195?227.
Denis Filimonov and Mary Harper. 2009. A joint
language model with fine-grain syntactic tags. In
EMNLP.
John Garofolo, Jonathan Fiscus, William Fisher, and
David Pallett, 1996. CSR-IV HUB4. Linguistic Data
Consortium, Philadelphia.
Zhongqiang Huang and Mary Harper. 2009. Self-
Training PCFG grammars with latent annotations
across languages. In EMNLP.
Frederick Jelinek and Robert L. Mercer. 1980. Inter-
polated estimation of Markov source parameters from
sparse data. In Proceedings of the Workshop on Pat-
tern Recognition in Practice, pages 381?397.
S. Khudanpur and J. Wu. 2000. Maximum entropy tech-
niques for exploiting syntactic, semantic and colloca-
tional dependencies in language modeling. Computer
Speech and Language, pages 355?372.
H. K. J. Kuo, L. Mangu, A. Emami, I. Zitouni, and
L. Young-Suk. 2009. Syntactic features for Arabic
speech recognition. In Proc. ASRU.
M.P. Marcus, M.A. Marcinkiewicz, and B. Santorini.
1993. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational Linguistics,
19(2):330.
Ariya Rastrow, Mark Dredze, and Sanjeev Khudanpur.
2011. Efficient discrimnative training of long-span
language models. In IEEE Workshop on Automatic
Speech Recognition and Understanding (ASRU).
K. Sagae and J. Tsujii. 2007. Dependency parsing
and domain adaptation with LR models and parser en-
sembles. In Proc. EMNLP-CoNLL, volume 7, pages
1044?1050.
Yoshimasa Tsuruoka, Yusuke Miyao, and Jun?ichi
Kazama. 2011. Learning with Lookahead :
Can History-Based Models Rival Globally Optimized
Models ? In Proc. CoNLL, number June, pages 238?
246.
Ralph Weischedel, Sameer Pradhan, Lance Ramshaw,
Martha Palmer, Nianwen Xue, Mitchell Marcus, Ann
Taylor, Craig Greenberg, Eduard Hovy, Robert Belvin,
and Ann Houston, 2008. OntoNotes Release 2.0. Lin-
guistic Data Consortium, Philadelphia.
Imed Zitouni. 2007. Backoff hierarchical class n-
gram language models: effectiveness to model unseen
events in speech recognition. Computer Speech &
Language, 21(1):88?104.
58
