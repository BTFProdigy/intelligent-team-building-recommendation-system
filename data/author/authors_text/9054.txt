Using Suffix Arrays to Compute Term 
Frequency and Document Frequency for 
All Substrings in a Corpus 
Mikio Yamamoto* 
University of Tsukuba 
Kenneth W. Churcht 
AT&T Labs--Research 
Bigrams and trigrams are commonly used in statistical natural anguage processing; this paper 
will describe techniques for working with much longer n-grams. Suffix arrays (Manber and My- 
ers 1990) were /irst introduced to compute the frequency and location of a substring (n-gram) 
in a sequence (corpus) of length N. To compute frequencies over all N(N + 1)/2 substrings in a 
corpus, the substrings are grouped into a manageable number of equivalence lasses. In this way, 
a prohibitive computation over substrings is reduced to a manageable computation over classes. 
This paper presents both the algorithms and the code that were used to compute term frequency 
(tf) and document frequency (dr)for all n-grams in two large corpora, an English corpus of 
50 million words of Wall Street Journal and a Japanese corpus of 216 million characters of 
Mainichi Shimbun. 
The second half of the paper uses these frequencies to find "interesting" substrings. Lexi- 
cographers have been interested in n-grams with high mutual information (MI) where the joint 
term frequency ishigher than what would be expected by chance, assuming that the parts of the 
n-gram combine independently. Residual inverse document frequency (RIDF) compares docu- 
ment frequency to another model of chance where terms with a particular term frequency are 
distributed randomly throughout the collection. MI tends to pick out phrases with noncompo- 
sitional semantics (which often violate the independence assumption) whereas RIDF tends to 
highlight echnical terminology, names, and good keywords for information retrieval (which tend 
to exhibit nonrandom distributions over documents). The combination of both MI and RIDF is 
better than either by itself in a Japanese word extraction task. 
1. Introduction 
We will use suffix arrays (Manber and Myers 1990) to compute a number of type/token 
statistics of interest, including term frequency and document frequency, for all n-grams 
in large corpora. Type/token statistics model the corpus as a sequence of N tokens 
(characters, words, terms, n-grams, etc.) drawn from a vocabulary of V types. Different 
tokenizing rules will be used for different corpora and for different applications. In 
this work, the English text is tokenized into a sequence of English words delimited by 
white space and the Japanese text is tokenized into a sequence of Japanese characters 
(typically one or two bytes each). 
Term frequency (tf) is the standard notion of frequency in corpus-based natural 
language processing (NLP); it counts the number of times that a type (term/word/n- 
gram) appears in a corpus. Document frequency (df) is borrowed for the information 
* Institute of Information Sciences and Electronics, 1-1-1 Tennodai, Tsukuba 305-8573, Japan 
t 180 Park Avenue, Florham Park, NJ 07932 
Computational Linguistics Volume 27, Number 1 
retrieval iterature (Sparck Jones 1972); it counts the number of documents that con- 
tain a type at least once. Term frequency is an integer between 0 and N; document 
frequency is an integer between 0 and D, the number of documents in the corpus. 
The statistics, tf and df, and functions of these statistics uch as mutual information 
(MI) and inverse document frequency (IDF), are usually computed over short n-grams 
such as unigrams, bigrams, and trigrams (substrings of 1-3 tokens) (Charniak 1993; 
Jelinek 1997). This paper will show how to work with much longer n-grams, including 
million-grams and even billion-grams. 
In corpus-based NLP, term frequencies are often converted into probabilities, using 
the maximum likelihood estimator (MLE), the Good-Turing method (Katz 1987), or 
Deleted Interpolation (Jelinek 1997, Chapter 15). These probabilities are used in noisy 
channel applications uch as speech recognition to distinguish more likely sequences 
from less likely sequences, reducing the search space (perplexity) for the acoustic 
recognizer. In information retrieval, document frequencies are converted into inverse 
document frequency (IDF), which plays an important role in term weighting (Sparck 
Jones 1972). 
dr(t) 
IDF(t)  = - log  2 D 
IDF(t)  can be interpreted as the number of bits of information the system is given if it 
is told that the document in question contains the term t. Rare terms contribute more 
bits than common terms. 
Mutual information (MI) and residual IDF (RIDF) (Church and Gale 1995) both 
compare tf and df to what would be expected by chance, using two different notions of 
chance. MI compares the term frequency of an n-gram to what would be expected if the 
parts combined independently, whereas RIDF combines the document frequency of a 
term to what would be expected if a term with a given term frequency were randomly 
distributed throughout the collection. MI tends to pick out phrases with noncompo- 
sitional semantics (which often violate the independence assumption) whereas RIDF 
tends to highlight technical terminology, names, and good keywords for information 
retrieval (which tend to exhibit nonrandom distributions over documents). 
Assuming a random distribution of a term (Poisson model), the probability p~(k) 
that a document will have exactly k instances of the term is: 
e-O Ok 
pa(k) = rc(O,k) - k! ' 
where 0 = np, n is the average length of a document, and p is the occurrence probability 
of the term. That is, 
Nt f  tf 8--  
DN D" 
Residual IDF is defined as the following formula. 
Residual IDF = observed IDF - predicted IDF 
df log(1 = - log  + - 
df ---- - l og~ +log(1 - e-~) 
The rest of the paper is divided into two sections. Section 2 describes the algorithms 
and the code that were used to compute term frequencies and document frequencies 
Yamamoto and Church Term Frequency and Document Frequency for All Substrings 
for all substrings in two large corpora, an English corpus of 50 million words of the 
Wall Street Journal, and a Japanese corpus of 216 million characters of the Mainichi 
Shimbun. 
Section 3 uses these frequencies tofind "interesting" substrings, where what counts 
as "interesting" depends on the application. MI finds phrases of interest o lexicog- 
raphy, general vocabulary whose distribution is far from random combination of the 
parts, whereas RIDF picks out technical terminology, names, and keywords that are 
useful for information retrieval, whose distribution over documents i far from uni- 
form or Poisson. These observations may be particularly useful for a Japanese word 
extraction task. Sequences of characters that are high in both MI and RIDF are more 
likely to be words than sequences that are high in just one, which are more likely than 
sequences that are high in neither. 
2. Computing tf and df for All Substrings 
2.1 Suffix Arrays 
This section will introduce an algorithm based on suffix arrays for computing tf and 
df and many functions of these quantities for all substrings in a corpus in O(NlogN) 
time, even though there are N(N + 1)/2 such substrings in a corpus of size N. The 
algorithm groups the N(N + 1)/2 substrings into at most 2N - 1 equivalence classes. 
By grouping substrings in this way, many of the statistics of interest can be computed 
over the relatively small number of classes, which is manageable, rather than over the 
quadratic number of substrings, which would be prohibitive. 
The suffix array data structure (Manber and Myers 1990) was introduced as a 
database indexing technique. Suffix arrays can be viewed as a compact representa- 
tion of suffix trees (McCreight 1976; Ukkonen 1995), a data structure that has been 
extensively studied over the last thirty years. See Gusfield (1997) for a comprehensive 
introduction to suffix trees. Hui (1992) shows how to compute df for all substrings 
using generalized suffix trees. The major advantage of suffix arrays over suffix trees 
is space. The space requirements for suffix trees (but not for suffix arrays) grow with 
alphabet size: O(N\]~\]) space, where \]~\] is the alphabet size. The dependency on al- 
phabet size is a serious issue for Japanese. Manber and Myers (1990) reported that 
suffix arrays are an order of magnitude more efficient in space than suffix trees, even 
in the case of relatively small alphabet size (IGI = 96). The advantages of suffix arrays 
over suffix trees become much more significant for larger alphabets such as Japanese 
characters (and English words). 
The suffix array data structure makes it convenient to compute the frequency 
and location of a substring (n-gram) in a long sequence (corpus). The early work was 
motivated by biological applications such as matching of DNA sequences. Suffix arrays 
are closely related to PAT arrays (Gonnet, Baeza-Yates, and Snider 1992), which were 
motivated in part by a project at the University of Waterloo to distribute the Oxford 
English Dictionary with indexes on CD-ROM. PAT arrays have also been motivated 
by applications in information retrieval. A similar data structure to suffix arrays was 
proposed by Nagao and Mori (1994) for processing Japanese text. 
The alphabet sizes vary considerably in each of these cases. DNA has a relatively 
small alphabet of just 4 characters, whereas Japanese has a relatively large alphabet of 
more than 5,000 characters. The methods uch as suffix arrays and PAT arrays scale 
naturally over alphabet size. In the experimental section (Section 3) using the Wall Street 
Journal corpus, the suffix array is applied to a large corpus of English text, where the al- 
phabet is assumed to be the set of all English words, an unbounded set. It is sometimes 
assumed that larger alphabets are more challenging than smaller ones, but ironically, 
Computational Linguistics Volume 27, Number 1 
it can be just the reverse because there is often an inverse relationship between the size 
of the alphabet and the length of meaningful or interesting substrings. For expository 
convenience, this section will use the letters of the alphabet, a-z, to denote tokens. 
This section starts by reviewing the construction of suffix arrays and how they 
have been used to compute the frequency and locations of a substring in a sequence. 
We will then show how these methods can be applied to find not only the frequency 
of a particular substring but also the frequency of all substrings. Finally, the methods 
are generalized to compute document frequencies as well as term frequencies. 
A suffix array, s, is an array of all N suffixes, sorted alphabetically. A suffix, s\[i\], 
also known as a semi-infinite string, is a string that starts at position i in the corpus 
and continues to the end of the corpus. In practical implementations, it is typically 
denoted by a four-byte integer, i. In this way, a small (constant) amount of space is 
used to represent a very long substring, which one might have thought would require 
N space. 
A substring, sub(i,j), is a prefix of a suffix. That is, sub(i,j), is the first j characters 
of the suffix s\[i\]. The corpus contains N(N + 1)/2 substrings. 
The algorithm, su f f ix_ar ray ,  presented below takes a corpus and its length N as 
input, and outputs the suffix array, s. 
su f f i x_ar ray  *-- function(corpus, N){ 
Initialize s to be a vector of integers from 0 to N - 1. 
Let each integer denote a suffix starting at s\[i\] in the corpus. 
Sort s so that the suffixes are in alphabetical order. 
Return s. } 
The C program below implements this algorithm. 
char *corpus ; 
int suffix_compare(int *a, int *b) 
{ return strcmp(corpus+*a, corpus+*b);} 
int *suffix_array(int n){ 
int i, *s = (int *)malloc(n*sizeof(int)); 
for(i=O; i < n; i++) s\[i\] = i; /* initialize */ 
qsort(s, n, sizeof(int), suffix_compare); /* sort */ 
return s;} 
Figures i and 2 illustrate a simple example where the corpus ("to_be_ormot_to_be") 
consists of N = 18 (19 bytes): 13 alphabetic haracters plus 5 spaces (and 1 null 
termination). The C program (above) starts by allocating memory for the suffix array 
(18 integers of 4 bytes each). The suffix array is initialized to the integers from 0 
to 17. Finally, the suffix array is sorted by alphabetical order. The suffix array after 
initialization is shown in Figure 1. The suffix array after sorting is shown in Figure 2. 
As mentioned above, suffix arrays were designed to make it easy to compute the 
frequency (tf) and locations of a substring (n-gram or term) in a sequence (corpus). 
Given a substring or term, t, a binary search is used to find the first and last suffix 
that start with t. Let s\[i\] be the first such suffix and s~\] be the last such suffix. Then 
tf(t) = j - i + 1 and the term is located at positions: {s\[i\], s\[i + 1\] . . . .  ,s~\]}, and only 
these positions. 
Figure 2 shows how this procedure can be used to compute the frequency and 
locations of the term "to_be" in the corpus "to_be_or_not_to_be". As illustrated in the 
figure, s\[i = 16\] is the first suffix to start with the term "to_be" and s~ = 17\] is the last 
Yamamoto and Church Term Frequency and Document Frequency for All Substrings 
Input  corpus :  " to_be_or_not_ to_be"  
Position: 
Characters: 
Initialized 
Suffix Array 
s\[O\] 0 
s\[l\] 1 
s \ [2\ ]  2 
s \ [3\ ]  3 
s\ [13\]  13 
s\[18\]  14 
s \[15\] 15 
s \[16\] 15 
s\ [17\]  17 
0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 I t lo l_ lb le l_ lo l r l_ ln lo l t l_ l t lo l_ lb le l  .~" 
: : : : ""{ i i i ! 
' ' ' ! Suffixes denoted by s \[ i \ ]  i i i i i 
-" / ." / to  b~ o~ not  to  b~ I /  / i i i 
.... . j...: o or to j :" ' i i ' '  I ........ " ..,'" I be  or  not  to  be I ." / / / / I " ~  be orNot_ to_be  ...... ,'"'" ...'/" .y ./" / 
Figure 1 
Illustration of a suffix array, s, that has just been initialized and not yet sorted. Each element in 
the suffix array, s\[i\], is an integer denoting a suffix or a semi-infinite string, starting at 
position i in the corpus and extending to the end of the corpus. 
Suf f ix  Ar ray  
s\[0\] 1151 
s \[1\] 2., 
s\[2\] 8 
s\[3\] 5 
s \[4\] 12 
s \[5\] 16 
s \ [6 \ ]  3 
s \[7\] 17 
s \[8\] 4 
s \ [9 \ ]  9 
s \ [10\]  14 
s i l l \ ]  1 
s \ [12 \ ]  6 
s\[13\] 10 
s \[14\]  7 
s \ [15\]  11 
s \[16\] 13 
s\[17\] 0 
Figure 2 
Suf f ixes  denoted  by  s \[ i \] 
_De  
be_or_not_ to_be  
_not to be 
_or_not_ to_be  
_to_be 
be 
be_or_no t_t o_be 
e 
e_or_no t_t o_be 
not_ to_be 
obe  
o be or_not_ to_be 
or_not to be 
ot_to_be 
r_not_ to_be 
t to be 
to_be 
to be or_not_ to_be 
Illustration of the suffix array in Figure 1 after sorting. The integers in s are sorted so that the 
semi-infinite strings are now in alphabetical order. 
Computational Linguistics Volume 27, Number 1 
suffix to start with this term. Consequently, tf("to_be") = 17-16+ 1 = 2. Moreover, the 
term appears at positions("to_be") = {s\[16\], s\[17\] } = {13, 0}, and only these positions. 
Similarly, the substring "to" has the same tf and positions, as do the substrings, "to_" 
and "to_b". Although there may be N(N + 1)/2 ways to pick i and j, it will turn out 
that we need only consider 2N - 1 of them when computing tf for all substrings. 
Nagao and Mori (1994) ran this procedure quite successfully on a large corpus 
of Japanese text. They report that it takes O(NlogN) time, assuming that the sort 
step performs O(N log N) comparisons, and that each comparison takes constant time. 
While these are often reasonable assumptions, we have found that if the corpus con- 
tains long repeated substrings (e.g., duplicated articles), as our English corpus does 
(Paul and Baker 1992), then the sort can consume quadratic time, since each compar- 
ison can take order N time. Like Nagao and Mori (1994), we were also able to apply 
this procedure quite successfully to our Japanese corpus, but for the English corpus, 
after 50 hours of CPU time, we gave up and turned to Manber and Myers's (1990) al- 
gorithm, which took only two hours. 1 Manber and Myers' algorithm uses some clever, 
but difficult to describe, techniques to achieve O(N log N) time, even for a corpus with 
long repeated substrings. For a corpus that would otherwise consume quadratic time, 
the Manber and Myers algorithm is well worth the effort, but otherwise, the procedure 
described above is simpler, and can even be a bit faster. 
The "to_be_or_not_to_be" example used the standard English alphabet (one byte per 
character). As mentioned above, suffix arrays can be generalized in a straightforward 
way to work with larger alphabets such as Japanese (typically two bytes per character). 
In the experimental section (Section 3), we use an open-ended set of English words as 
the alphabet. Each token (English word) is represented as a four-byte pointer into a 
symbol table (dictionary). The corpus "to_be_or_not_to_be", for example, is tokenized 
into six tokens: "to", "be", "or", "not", "to", and "be", where each token is represented 
as a four-byte pointer into a dictionary. 
2.2 Longest Common Prefixes (LCPs) 
Algorithms from the suffix array literature make use of an auxiliary array for storing 
LCPs (longest common prefixes). The lcp array contains N + 1 integers. Each element, 
lcp\[i\], indicates the length of the common prefix between s\[i - 1\] and s\[i\]. We pad the 
lcp vector with zeros (lcp\[O\] = lcp\[N\] = 0) to simplify the discussion. The padding 
avoids the need to test for certain end conditions. 
Figure 3 shows the lcp vector for the suffix array of "to_be_or_not_to_be". For 
example, since s\[10\] and s\[11\] both start with the substring "o_be", lcp\[11\] is set to 4, the 
length of the longest common prefix. Manber and Myers (1990) use the Icp vector in 
their O(P+log N) algorithm for computing the frequency and location of a substring of 
length P in a sequence of length N. They showed that the Icp vector can be computed 
in O(N log N) time. These algorithms are much faster than the obvious traightforward 
implementation when the corpus contains long repeated substrings, though for many 
corpora, the complications required to avoid quadratic behavior are unnecessary. 
2.3 Classes of Substrings 
Thus far we have seen how to compute tf for a single n-gram, but how do we compute 
tf and df for all n-grams? As mentioned above, the N(N + 1)/2 substrings will be 
clustered into a relatively small number of classes, and then the statistics will be 
1 We used Doug McIlroy's implementation, available on the Web at: http://cm.belMabs.com/cm/cs/ 
who/doug/ssort.c. 
Yamamoto and Church Term Frequency and Document Frequency for All Substrings 
computed over the classes rather than over the substrings, which would be prohibitive. 
The reduction of the computat ion over substrings to a computat ion over classes is 
made possible by four properties. 
Properties 1-2: all substrings in a class have the same statistics (at least 
for the statistics of interest, namely tf and dJ), 
Property 3: the set of all substrings is partit ioned by the classes, and 
Property 4: there are many fewer classes (order N) than substrings 
(order N2). 
Classes are defined in terms of intervals. Let (i,j) be an interval on the suffix array, 
s\[i\], s\[i + 1\] . . . . .  s~\]. Class((i,j)) is the set of substrings that start every suffix within the 
interval and no suffix outside the interval. It follows from this construction that all 
substrings in a class have tf = j - i + 1. 
The set of substrings in a class can be constructed from the lcp vector: 
class((i,j)) = {s\[i\]ml ax(Icp\[i\], lcp~ + 1\]) < m ~ min(lcp\[i + 1\], lcp\[i + 2\] . . . . .  lcp~'\])}, 
where s\[i\]m denotes the first m characters of the suffix s\[i\]. We will refer to lcp\[i\] and 
Icp~+l\] as bound ing  lcps and Icp\[i+l\], lcp\[i+2\] . . . . .  lcp~\] as interior lcps. The equation 
Suffix Array i Suffix denoted by s \[ i \] 
s\[O\] 15 _be; 
s\[l\] 2 _b~_or_not  to be 
s\[2\] 8 _}not to be 
s\[3\] 5 br not to be --\] - -  
s\[4\] 12 :--to_be 
s\[5\] IB ibei 
s\[6\] 3 :he'or_not to be 
s\[7\] 
s \[8\] i e~_or_not to be 
s \[ 9 \] not_ to_be 
s \[I0\] o_b4 l en~=4 
sill\] ~ or not to be 
s\[12\] qr_not to be 
s\[13\] q't to be 
s\[14\] r_not to be 
s\[15\] tJ to be 
s \[ 16 \] rio_be, 
s \[ 17 \] to_be\[~or_not to be 
The doted lines denote lcp's. 
Figure 3 
Lcp vector 
icp\[O O" 
icp \[i 3 
icp \[2 I 
icp \[3 I 
Icp \[4 I 
i cp \ [5  0 
icp\[6\] 2 
icp\[7\] 0 
icp\[8\] I 
Icp\[9\] 0 
icp\[lO\] 0 
l cp  \ [12 \ ]  1 
Icp\[13\] 1 
icp \[14\] 0 
icp\[15\] 0 
icp\[16\] I 
icp\[17\] 5 
icp\[18\] O- 
-- always 0 
always 0 
The longest common prefix is a vector of N + 1 integers, lcp\[i\] denotes the length of the 
common prefix between the suffix s\[i - 1\] and the suffix s\[i\]. Thus, for example, s\[10\] and s\[11\] 
share a common prefix of four characters, and therefore lcp\[11\] = 4. The common prefix is 
highlighted by a dotted line in the suffix array. The suffix array is the same as in the previous 
figure. 
7 
Computational Linguistics Volume 27, Number 1 
Vertical lines denote lcps. Gray area denotes Boundaries 
endpoints of substrings in class(<10,11>). 
S \[9\]lli' n o t to be ~'~f7 ? f<10 ' l l>  
I,Ill 0 __ b e - ~ X<";flO,11> . . . . . . . . . .  / s \[ 10 \] 
s\[ll\]\]~o\]_ b el_or_not_to_be ~1 4_ I 
s\[12\]I lo:;Ir n~t  to be ........ <10,13> 
s\[14\]s\[13\]\]i!I~to ~-t~_be e ............ ~ 
Bounding lcps, LBL, SIL, Interior lcp of <10,11> 
LCP-del imited Class 
interval 
<10,11> 
<10,13> 
<9,9> 
<10,10> 
<11,11> 
<12,12> 
<13,13> 
<14,14> 
LBL SIL 
Figure 4 
{"O_", "o_b", "o_be"} 1 4 
{"o"} 0 1 
{"n", "no", "not", ...} 0 infinity 
{} 4 infinity 
{"o_be_","o_beo",. . .} 4 infinity 
{"or", "or_", "or_n", ...} 1 infinity 
{"ot", "ot_", "oCt", ...} 1 infinity 
"r", "r " " " _ , r_n,. . .} 0 infinity 
tf 
Six suffixes are copied from Figure 3, s\[9\]-s\[14\], along with eight of their lcp-delimited 
intervals. Two of the lcp-delimited intervals are nontrivial (tf > 1), and six are trivial (tf = 1). 
Intervals are associated with classes, sets of substrings. These substrings start every suffix 
within the interval and no suffix outside the interval. All of the substrings within a class have 
the same term frequency (and document frequency). 
above can be rewritten as 
class((i, jl ) = {s\[i\]mlLBL((i, j l) < m <_ SIL((i,  j l )},  
where LBL (longest bounding lcp) is 
LBL(( i , j ) )  = max(lcp\[i\], lcp~ + 1\]), 
and SIL (shortest interior lcp) is 
sIn(( i , j ) )  = min(lcp\[ i  + 1\], Icp\[i + 2\] . . . . .  lcp~\]). 
By construction, the class will be empty unless there is some room for m between 
the LBL and SIL. We say that an interval is lcp-delimited when this room exists 
(that is, LBL  < SIL). Except for trivial intervals where tf = 1 (see below), classes are 
nonempty iff the interval is lcp-delimited. Moreover, the number of substrings in a 
nontrivial class depends on the amount of room between the LBL and the SIL. That 
is, \[class((i,j))l = SIL(( i , j ))  - LBL(( i , j ) ) .  
Figure 4 shows eight examples of lcp-delirnited intervals. The top part of the figure 
highlights the interval (10,11) with dashed horizontal lines. Solid vertical ines denote 
Yamamoto and Church Term Frequency and Document Frequency for All Substrings 
bounding lcps, and thin vertical ines denote interior lcps (there is only one interior lcp 
in this case). The interval {10, 11 / is lcp-delimited because the bounding lcps, Icp\[lO\] --= 0 
and lcp\[12\] = 1, are smaller than the interior lcp, lcp\[11\] = 4. That is, the LBL (= 1) is 
less than the SIL (= 4). Thus there is room for m between the LBL of s\[10\]m and the SIL 
of s\[10\]m. The endpoints m between LBL and SIL are highlighted in gray. The class is 
nonempty. Its size depends on the width of the gray area: class((lO, 11)) = {s\[10\]mll < 
m _< 4} = {"o_", "o_b", "o_be"}. These substrings have the same tf: tf = j - i + 1 = 
11 - 10 + 1 = 2. Each of these substrings occurs exactly twice in the corpus. 
Every substring in the class starts every suffix in the interval (10,11), and no suffix 
outside (10,11). In particular, the substring "o" is excluded from the class, because it 
is shared by suffixes outside the interval, namely s\[12\] and s\[13\]. The longer substring, 
"o_be_", is excluded from the class because it is not shared by s\[10\], a suffix within 
the interval. 
We call an interval trivial if the interval starts and ends at the same place: (i, i). 
The remaining six intervals mentioned in Figure 4 are trivial intervals. We call the 
class of a trivial interval a trivial class. As in the nontrivial case, the class contains 
all (and only) the substrings that start every suffix within the interval and no suf- 
fix outside the interval. We can express the class of a trivial interval, class((i, i)), as 
{s\[i\]m\]LBL < m <_ SIL}. The trivial case is the same as the nontrivial case, except hat 
the SIL of a trivial interval is defined to be infinite. As a result, trivial classes are 
usually quite large, because they contain all prefixes of s\[i\] that are longer than the 
LBL. They cover all (and only) the substrings with tf = 1, typically the bulk of the 
N(N+ 1)/2 substrings in a corpus. The trivial class of the interval (11,11), for example, 
contains 13 substrings: "o_be_", "o_be_o", "o_be_or", and so on. Of course, there are 
some exceptions: the trivial class, class((lO, 10)), in Figure 4, for example, is very small 
(= empty set). 
Not every interval is lcp-delimited. The interval, I l l ,  12), for example, is not lcp- 
delimited because there is no room for m of s\[ll\]m between the LBL (= 4) and the 
SIL (= 1). When the interval is not lcp-delimited, the class is empty. There are no 
substrings tarting all the suffixes within the interval (11,12), and not starting any 
suffix outside the interval. 
It is possible for lcp-delimited intervals to be nested, as in the case of (10,11) and 
(10, 13). We say that one interval (i,j) is nested within another (u,v) if i < u < v < j 
(and (i,j) ~ (u, v)). Nested intervals have distinct SILs and disjoint classes. (Two classes 
are disjoint if the corresponding sets of substrings are disjoint.) 2The substrings in the 
class of the nested interval, (u, v), are longer than the substrings in the class of the 
outer interval, (i,j). 
Although it is possible for lcp-delimited intervals to be nested, it is not possible for 
lcp-delimited intervals to overlap. We say that one nontrivial interval Ca, b) overlaps 
another nontrivial interval (c, d) if a < c < b < d. If two intervals overlap, then at 
least one of the intervals is not lcp-delimited and has an empty class. If an interval 
(a, b) is lcp-delimited, an overlapped interval (c, d) is not lcp-delimited. Because a 
bounding lcp of /a, b) must be within (c, d) and an interior lcp of (a, b) must be a 
bounding lcp of (c,d), SIL((c,d)) <_ LBL((a,b)) <_ SIL((a,b)) <_ LBL((c,d)). That is, the 
overlapped interval (c, d) is not lcp-delimited. The fact that lcp-delimited intervals are 
nested and do not overlap will turn out to be convenient for enumerating lcp-delimited 
intervals. 
2 Because (u, v I is lcp-delimited, there must be a bounding lcp of lu, v I that is smaller than any lcp 
within (u, v). This bounding lcp must be within (i,j), and as a result, class((i,j)) and class((u, v)) 
must be disjoint. 
Computational Linguistics Volume 27, Number 1 
2.4 Four Properties 
As mentioned above, classes are constructed so that it is practical to reduce the com- 
putation of various statistics over substrings to a computation over classes. This sub- 
section will discuss four properties of classes that help make this reduction feasible. 
The first two properties are convenient because they allow us to associate tf and 
df with classes rather than with substrings. The substrings in a class all have the same 
tf value (property 1) and the same df value (property 2). That is, if Sl and s2 are two 
substrings in class((i,j)) then 
Property 1: tf(sl) = t f ( s2)  = j - i + 1 
Property 2: dr(s1) = rid(s2 ). 
Both of these properties follow straightforwardly from the construction of intervals. 
The value of tf is a simple function of the endpoints; the calculation of df is more 
complicated and will be discussed in Section 2.6. While tf and df treat each member 
of a class as equivalent, not all statistics do. Mutual information (MI) is an important 
counter example; in most cases, MI(sl) ~ MI(s2). 
The third property is convenient because it allows us to iterate over classes rather 
than substrings, without worrying about missing any of the substrings. 
Property 3: The classes partition the set of all substrings. 
There are two parts to this argument: every substring belongs to at most one class 
(property 3a), and every substring belongs to at least one class (property 3b). 
Demonstration of property 3a (proof by contradiction): Suppose there is a sub- 
string, s, that is a member of two distinct classes: class((i,j}) and class((u,v)). There 
are three possibilities: one interval precedes the other, they are properly nested, or 
they overlap. In all three cases, s cannot be a member of both classes. If one interval 
precedes the other, then there must be a bounding lcp between the two intervals which 
is shorter than s. And therefore, s cannot be in both classes. The nesting case was men- 
tioned previously where it was noted that nested intervals have disjoint classes. The 
overlapping case was also discussed previously where it was noted that two overlap- 
ping intervals cannot both be lcp-delimited, and therefore at least one of the classes 
would have to be empty. 
Demonstration of property 3b (constructive argument): Let s be an arbitrary sub- 
string in the corpus. There will be at least one suffix in the suffix array that starts with 
s. Let i be the first such suffix and let j be the last such suffix. By construction, the 
interval (i,j} is lcp-delimited (LBL((i,j}) < Is I and SIL((i,j}) >__ Is\[), and therefore, s is 
an element of class((i,j}). 
Finally, as mentioned above, computing over classes is much more efficient han 
computing over the substrings themselves because there are many fewer classes (at 
most 2N - 1) than substrings (N(N + 1)/2). 
Property 4: There are at most N nonempty classes with tf = 1 and at 
most N - 1 nonempty classes with tf > 1. 
The first clause is relatively straightforward. There are N trivial intervals (i, i/. 
These are all and only the intervals with tf = 1. By construction, these intervals are 
lcp-delimited, though it is possible that a few of the classes could be empty. 
To argue the second clause, we make use of a uniqueness property: an lcp- 
delimited interval (i, jl can be uniquely determined by an SIL and a representative ele- 
10 
Yamamoto and Church Term Frequency and Document Frequency for All Substrings 
ment k, where i < k < j. For convenience, we will choose k such that SIL(<i,j>) = Icp\[k\], 
but we could have uniquely determined the lcp-delimited interval by choosing any k 
such that i < k G j. 
The uniqueness property can be demonstrated using a proof by contradiction. 
Suppose there were two distinct lcp-delimited intervals, <i,j> and <u, v>, with the same 
representative k, where i < k < j and u < k G v. Since they share a common repre- 
sentative, k, one interval must be nested inside the other. But nested intervals have 
disjoint classes and different SILs. 
Given this uniqueness property, we can determine the N-  1 upper bound on 
the number of lcp-delimited intervals by considering the N - 1 elements in the Icp 
vector. Each of these elements, lcp\[k\], has the opportunity to become the SIL of an 
lcp-delimited interval <i,j> with a representative k. Thus there could be as many as 
N-  1 lcp-delimited intervals (though there could be fewer if some of the opportunities 
don't work out). Moreover, there cannot be any more intervals with tf > 1, because if 
there were one, its SIL should have been in the Icp vector. (Note that this lcp counting 
argument does not count trivial intervals because their S1Ls \[= infinity\] are not in the 
lcp vector; the Icp vector contains integers less than N.) 
From property 4, it follows that there are at most N distinct values of RIDE The 
N trivial intervals <i, i> have just one RIDF value since tf = df = 1 for these intervals. 
The other N - 1 intervals could have as many as another N - 1 RIDF values. Similar 
arguments hold for many other statistics that make use of tf and dr, and treat all 
members of a class as equivalent. 
In summary, the four properties taken collectively make it practical to compute 
tf, dr, and RIDF over a relatively small number of classes; it would be prohibitively 
expensive to compute these quantities directly over the N(N + 1)/2 substrings. 
2.5 Computing All Classes Using Suffix Arrays 
This subsection describes a single-pass procedure, print_LDIs, for computing t / fo r  
all LDIs (lcp-delimited intervals). Since lcp-delimited intervals are properly nested, the 
procedure is based on a push-down stack. The procedure outputs four quantities for 
each lcp-delimited interval, <i,j>. The four quantities are the two endpoints (i and j), the 
term frequency (tf) and a representative (k), such that i < k _< j and lcp\[k\] : SIL(<i,j>). 
This procedure will be described twice. The first implementation is expressed in a 
recursive form; the second implementation avoids recursion by implementing the stack 
explicitly. 
The recursive implementation is presented first, because it is simpler. The function 
print_LDIs is initially called with pr in t  LDIs (0,0), which will cause the function to 
be called once for each value of k between 0 and N - 1. k is a representative in the 
range: i < k < j, where i and j are the endpoints of an interval. For each of the 
N values of k, a trivial LDI is reported at <k, k>. In addition, there could be up to 
N-  1 nontrivial intervals, where k is the representative and lcp\[k\] is the SIL. Recall 
that lcp-delimited intervals are uniquely determined by a representative k such that 
i < k < j where SIL(<i,j)) = Icp\[k\]. Not all of these candidates will produce LDIs. 
The recursion searches for j's such that LBL((i,j>) < SIL(<i,j>), but reports intervals at 
(i,j> only when the inequality is a strict inequality, that is, LBL(<i,j>) < SIL(<i,j>). The 
program stack keeps track of the left and right edges of these intervals. While Icp\[k\] 
is monotonically increasing, the left edge is remembered on the stack, as pr?nt LDIs 
is called recursively. The recursion unwinds as lcp\[j\] < lcp\[k\]. Figure 5 illustrates the 
function calls for computing the nontrivial lcp-delimited intervals in Figure 4. C code 
is provided in Appendix A. 
11 
Computational Linguistics Volume 27, Number 1 
s \ [9  
s\[lO 
s \ [ l l  
s \ [12  
s \ [13 \ ]  o" t  ..... t . . .  
s \ [14 \ ]  r _ n o . . . 
Vert ical  l ines denote  lcps. 
~.print LDIs( i=9,k=10) 
.. . . . . . . .  ~ \[print CDIs( i=10,k=l l )  
\] n o . -~-" j _  . . . .  ,-'" | pr int  <10,11>, rep=11, tf=2" 
\] "5~"" b e . , ' " ' "  \[return(j=11) 
.... , "print_LDIs( i=10,k=12) 
\] o _ b e. . . . . . . . .  -, , \ [pr int_LDIs ( i=12,k=13)  
\] o "?'"'---_.n ... . . .  -:- | pr int  nothing, 
| (<12,13> isn t lcp-de l imi ted)  
Lreturn(j=13) 
pr int  "<10,13>, rep=13, tf=4" 
return( j -13)  
Figure 5 
Trace of function calls for computing the nontrivial cp-delimited intervals in Figure 4. In this 
trace, trivial intervals are omitted. Print_LDIs(i = x,k = y) represents a function call with 
arguments, i and k. Indentation represents he nest of recursive calls. Print_LDIs(i,k) searches 
the right edge, j, of the non-trivial cp-delimited interval, <i,j>, whose SIL is lcp\[k\]. Each 
representative, k, value is given to the function print_LDIs just once (dotted arcs). 
pr int_LDIs +- function(i, k) { 
j~k .  
Output a trivial lcp-delimited interval <k, k> with tf  = 1. 
While lcp\[k\] <_ lcp~ + 1\] and j + 1 < N, do j *-- print_LDIs(k, j + 1). 
Output an interval <i,j> with tf  = j - i + 1 and rep = k, if it is lcp-delimited. 
Return j. } 
The second implementation (below) introduces its own explicit stack, a complica- 
tion that turns out to be important in practice, especially for large corpora. C code is 
provided in Appendix B. 
pr int_LDIs_stack *-- function(N){ 
stack_i ~-- an integer array for the stack of the left edges, i. 
stack_k ~ an integer array for the stack of the representatives, k. 
stack_i\[O\] *-- O. 
stack_k\[O\] *-- O. 
sp *--- 1 (a stack pointer). 
For j  *-- 0,1,2 . . . . .  N -  1 do 
Output an lcp-delimited interval (j,j> with tf  = 1. 
While Icp~ + 1\] < lcp\[stackd~\[sp - 1\]\] do 
Output an interval <i,j> with tf  = j - i + 1, if it is lcp-delimited. 
sp* - - - sp -1 .  
stack_i\[sp\] ~-- stack_k\[sp - 1\]. 
stack_k \[sp\] *--- j + 1. 
sp* - - - sp+l .}  
12 
Yamamoto and Church Term Frequency and Document Frequency for All Substrings 
Suffix Array 
s\[O\] 2 I 
s\[l\] 15l 
s\[2\] !2_\] 
s\[3\] b I 
s \[4\] 8 1 
s \ [5 \ ]  181 
- - - - - -4  
s \ [6 \ ]  3 I 
s \ [7 \ ]  161 
s \ [8 \ ]  4 i 
s \ [9 \ ]  17\[ 
s \[i0\] 9 I 
s\[ll\] 
s\[12\] 141 
s\[13\] 6 1 
s\[14\] 1--51 
s\[15\] 
s\[16\] 111 
s\[iv\] o--I 
s\[18\] 1~1 
Suffixes denoted 
by s \[i\] 
_be$ 
_be$ 
_to_be$ 
$ 
$ 
$ 
be$ 
be$ 
e$ 
e$ 
not_to_be$ 
o_be$ 
o_be$ 
orS 
ot to be$ 
r$ 
t_to_be$ 
to_be$ 
to_be$ 
Document id's 
icp\[i\] ofs\[i\] 
0 0 
4 2 
I 2 
0 0 
I I 
I 2 
0 0 
3 2 
0 0 
2 2 
0 2 
0 0 
5 2 
1 I 
I 2 
0 I 
0 2 
I 0 
6 2 
0 
I nput  documents :  dO = "to_be$" - -  
d l  = "orS" 
d2  = "not_to_be$"  
Corpus  = dO + d l  + d2  = " to_be$or$not_ to_be$"  
Resulting non-trivial lcp-delimited intervals: 
('rep' means a representative, k.) 
(0,1>, rep= 1, tf=2, df=2 
(0, 2>, rep= 2, tf=3, df=2 
(3, 5), rep= 4, tf=3, df=3 
(6, 7>, rep= 7, tf=2, df=2 
<8, 9/, rep= 9, tf=2, df=2 
(11,12>, rep=12, tf=2, df=2 
<11,14}, rep=13, tf=4, df=3 
(17,18>, rep=18, tf=2, df=2 
<16,18), rep=17, tf=3, dr=2 
Figure 6 
A suffix array for a corpus consisting of three documents. The special character $ denotes the 
end of a document. The procedure outputs a sequence of intervals with their term frequencies 
and document frequencies. These results are also presented for the nontrivial intervals. 
2.6 Computing df for All Classes 
Thus far we have seen how to compute term frequency, tf, for all substrings (n-grams) 
in a sequence (corpus). This section will extend the solution to compute document 
frequency, dr, as well as term frequency. The solution runs in O(NlogN) time and 
O(N) space. C code is provided in Appendix C. 
This section will use the running example shown in Figure 6, where the corpus is: 
"to_be$or$not_to_be$'. The corpus consists of three documents, "to_be$', "orS", and 
"not_to_be$'. The special character $ is used to denote the end of a document. The 
procedure outputs a sequence of intervals with their term frequencies and document 
frequencies. These results are also presented for the nontrivial intervals. 
The suffix array is computed using the same procedures discussed above. In ad- 
dition to the suffix array and the lcp vector, Figure 6 introduces a new third table 
that is used to map from suffixes to document ids. This table of document ids will be 
13 
Computational Linguistics Volume 27, Number 1 
used by the function get_docnum to map from suffixes to document ids. Suffixes are 
terminated in Figure 6 after the first end of document symbol, unlike before, where 
suffixes were terminated with the end of corpus symbol. 
A straightforward method for computing df for an interval is to enumerate the 
suffixes within the interval and then compute their document ids, remove duplicates, 
and return the number of distinct documents. Thus, for example, df("o') in Figure 6, 
can be computed by finding corresponding interval, (11,14}, where every suffix within 
the interval starts with "o" and no suffix outside the interval starts with "o'. Then 
we enumerate the suffixes within the interval {s\[11\],s\[12\],s\[13\],s\[14\]}, compute their 
document ids, {0, 2,1, 2}, and remove duplicates. In the end we discover that df("o") -- 
3. That is, "o" appears in all three documents. 
Unfortunately, this straightforward approach is almost certainly too slow. Some 
document ids will be computed multiple times, especially when suffixes appear in 
nested intervals. We take advantage of the nesting property of lcp-delimited intervals 
to compute all df's efficiently. The df of an lcp-delimited interval can be computed 
recursively in terms of its constituents (nested subintervals), thus avoiding unnecessary 
recomputation. 
The procedure print_LDIs_w?th_df presented below is similar to print_LDIs_- 
stack but modified to compute df  as well as tf. The stack keeps track of i and k, as 
before, but now the stack also keeps track of df. 
i, the left edge of an interval, 
k, the representative (SIL = lcp\[k\]), 
df, partial results for dr, counting documents seen thus far, minus duplicates. 
print_LDIs with_dr +--- function(N){ 
stack_i ~- an integer array for the stack of the left edges, i. 
stack_k ~ an integer array for the stack of the representatives, k. 
s tackdf  ~-- an integer array for the stack of the df counter. 
doclink\[O..D\] : an integer array for the document link initialized with -1. 
D = the number of documents. 
stack_i\[O\] ~ O. 
stack_k\[O\] *--- O. 
stack_dr\[O\] *--- 1. 
sp ~ 1 (a stack pointer). 
(1) For j  ~ 0 ,1 ,2 , . . . ,N -  1 do 
(2) (Output a trivial lcp-delimited interval q,j> with tf = 1 and d/= 1.) 
(3) doc *-- get_docnum(s~\]) 
(4) if doclink\[doc\] ~ -1, do 
(5) let x be the largest x such that doclink\[doc\] >_ stack_i N .  
(6) stack_dr\[x\] *--- stack_df\[x\] - 1. 
(7) doclink\[ doc\] ~-- j. 
(8) df ,-- 1. 
(9) While lcp~" + 1\] < lcp\[stack_k\[sp - 1\]\] do 
(10) df *--- stack~tf\[sp - 1\] + df . 
(11) Output a nontrivial interval (i,j) with tf = j - i + 1 and dr, 
if it is lcp-delimited. 
(12) sp ~-- sp - 1. 
(13) stack_i\[sp\] ~-- stack_k\[sp - 1\]. 
(14) stack_k \[sp\] ~-- j + 1. 
14 
Yamamoto and Church Term Frequency and Document Frequency for All Substrings 
(15) staek_e/\[sp\] ~ ad. 
(16) sp *-- sp + 1. } 
Lines 5 and 6 take care of duplicate documents. The duplication processing makes 
use of doclink (an array of length D, the number of documents in the collection), which 
keeps track of which suffixes have been seen in which document, doclink is initialized 
with -1 indicating that no suffixes have been seen yet. As suffixes are processed, oclink 
is updated (on line 7) so that doclink \[d\] contains the most recently processed suffix in 
document d. As illustrated in Figure 7, when j = 16 (snapshot A), the most recently 
processed suffix in document 0 is s\[11\] ("o_be$"), the most recently processed suffix 
in document 1 is s\[15\] ("r$"), and the most recently processed suffix in document 2 
is s\[16\] ("t_to_be$"). Thus, doclink\[0\]= 11,doclink\[1\]= 15, and doclink\[2\]= 16. After 
processing s\[17\] ("to_be$"), which is in document 0, doclink\[0\] is updated from 11 to 
17, as shown in snapshot B of Figure 7. 
Stackdf keeps track of document frequencies as suffixes are processed. The invari- 
ant is: stack_dr\[x\] contains the document frequency for suffixes een thus far starting 
at i = stack_i\[x\]. (x is a stack offset.) When a new suffix is processed, line 5 checks for 
double counting by searching for intervals on the stack (still being processed) that have 
suffixes in the same document as the current suffix. If there is any double counting, 
stackdf is decremented appropriately on line 6. 
There is an example of this decrementing in snapshot C of Figure 7, highlighted 
by the circle around the binding of df to 0 on the stack element: \[i = 0, k = 17, 
df = 0\]. Note that df was previously bound to 1 in snapshot B. The binding of df was 
decremented when processing s\[18\] because s\[18\] is in the same document as s\[16\]. 
This duplication was identified by line 5. The decrementing was performed by line 6. 
Intervals are processed in depth-first order, so that more deeply nested intervals 
are processed before less deeply nested intervals. In this way, double counting is only 
an issue for intervals higher on the stack. The most deeply nested intervals are trivial 
intervals. They are processed first. They have a df of 1 (line 8). For the remaining 
nontrivial intervals, staekdf contains the partial results for intervals in process. As the 
stack is popped, the df values are aggregated up to compute the df value for the outer 
intervals. The aggregation occurs on line 10 and the popping of the stack occurs on line 
12. The aggregation step is illustrated in snapshots C and D of Figure 7 by the two ar- 
rows with the "+" combination symbol pointing at a value of df in an output statement. 
2.7 Class Arrays 
The classes identified by the previous calculation are stored in a data structure we call 
a class array, to make it relatively easy to look up the term frequency and document 
frequency for an arbitrary substring. The class array is a stored list of five-tuples: (SIL, 
LBL, tf, df, longest suffix I. The fifth element of the five-tuple is a canonical member of 
the class (the longest suffix). The five-tuples are sorted by the alphabetical order of 
the canonical members. In our C code implementation, classes are represented by five 
integers, one for each element in the five-tuple. Since there are N trivial classes and 
at most N - 1 nontrivial classes, the class array will require at most 10N - 5 integers. 
However, for many practical applications, the trivial classes can be omitted. 
Figure 8 shows an example of the nontrivial class array for the corpus: "to_be$or$ 
not_to_be$". The class array makes it relatively easy to determine that the substring 
"o" appears in all three documents. That is, df("o") = 3. We use a binary search to 
find that tuple c\[5\] is the relevant five-tuple for "o". Having found the relevant tuple, 
it requires a simple record access to return the document frequency field. 
15 
Computational Linguistics Volume 27, Number 1 
Doclink 
doclink\[0\]=11 
doclink\[1\]=15 
doclink\[2\]=16 
Local variables Stack 
I j=16 df:Ll'  I I 
lines 13-16: push(i=16, k=l 7, df=l ) 
line l : j~ j+ l  
doclink\[0\]=114 J =17 df=l 
doclink\[1\]=15 I~ i=16, k=17, df=l 
doclink\[_2\]--16 I ~ i=15, k=16, df=l 
\ 
lop\[k=17\] ~. Icp\[j+1=18\] lines 3-6: x 
1 6 ~ l ine  7: doclink\[doc(j=17)=0\] .~ j=17~ 
~l ines  13-16:push(i=17, k=l 8, df=l ) 
J:'7 df:\Li=17, k:18, df=1, l )  
doclink\[1\]=15/ "'--.~=16, k=17, df=l~ / 
doclink\[2\]=16.J.............._~ I df=l l Y 
linesl-6:x__l ~ I J \  + - 
I cp \ [k=17\ ]  > Icp\[j+1=18\] _/line 7: ~ " ' " ~  
5 0 ~ lines 8-11: output LDI--<17,18~=2~=2 
/ "  line12:pap0 ~ ~ ~ ~  
~7- - -~/ / \ ]v j :  18 ~~- -  l .... : ........ ------ ) 
doclink\[1\]= 15 dt / -'----~-.=16, k= 17,(.df=O.), " 
~doclink\[2\]=18)l l i - -~ ,  df=l \ 
Icp\[k=16\] > Icp\[j+1=18\] ~ lines 9-11" output LD1=<16,18>, tf=3, df=2 - 
1 0 ' line 12: pop() 
I j=18 df=2 
. . . . . . . . . . . . . . . . . . . . .  
doclink\[0\]=17 
doclink\[1 \]=15 
doclink\[2\]=l 8 i=15, k=16, df=l 
(A) 
(B) 
(c) 
(D) 
Figure 7 
Snapshots of the doclink array and the stack during the processing of print_LDIs_with_df on 
the corpus: "to_be$or$not_to_be$'. The four snapshots A-D illustrate the state as j progresses 
from 16 to 18. Two nontrivial intervals are emitted while j is in this range: (17,18) and (16,18). 
The more deeply nested interval is emitted before the less deeply nested interval. 
16 
Yamamoto and Church Term Frequency and Document Frequency for All Substrings 
Class array The longest suffix 
(Pointer to cor ~us) denoted by c\[i\] SIL 
c \[0\] 0 _ 1 
c \[i\] 0 _be$ 4 
c \ [2 \ ]  5 $ 1 
c \[3\] 3 be$ 3 
c \[4\] 4 e$ 2 
c \ [5 \ ]  1 o 1 
c \[6\] 1 o_be$ 5 
c\[7\] 11 t 1 
c \ [8 \ ]  0 to_be$ 6 
Figure 8 
An example of the class array for the cor 
LBL tf df 
0 3 2 
1 2 2 
0 3 3 
0 2 2 
0 2 2 
0 4 3 
1 2 2 
0 3 2 
1 2 2 
pus: "to _be$or$not_to_be$". 
3. Experimental Results 
3.1 RIDF and MI for English and Japanese 
We used the methods described above to compute dr, tf, and RIDF for all substrings in 
two corpora of newspapers summarized in Table 1. MI was computed for the longest 
substring in each class. The entire computation took a few hours on a MIPS10000 with 
16 Gbytes of main memory. The processing time was dominated by the calculation of 
the suffix array. 
The English collection consists of 50 million words (113 thousand articles) of the 
Wall Street Journal (distributed by the ACL/DCI) and the Japanese collection consists of 
216 million characters (436 thousand articles) of the CD-Mainichi Shimbun from 1991- 
1995 (which are distributed in CD-ROM format). The English corpus was tokenized 
into words delimited by white space, whereas the Japanese corpus was tokenized into 
characters (typically two bytes each). 
Table I indicates that there are a large number of nontrivial classes in both corpora. 
The English corpus has more substrings per nontrivial class than the Japanese corpus. 
It has been noted elsewhere that the English corpus contains quite a few duplicated 
articles (Paul and Baker 1992). The duplicated articles could explain why there are so 
many substrings per nontrivial class in the English corpus when compared with the 
Japanese corpus. 
Table 1 
Statistics of the English and Japanese corpora. 
Statistic Wall Street Journal Mainichi Shimbun 
N (corpus ize in tokens) 
V (vocabulary in types) 
# articles 
# nontrivial classes 
# substrings in nontrivial classes 
substrings per class (in nontrivial 
classes) 
49,810,697 words 215,789,699 characters 
410,957 5,509 
112,915 435,859 
16,519,064 82,442,441 
2,548,140,677 1,388,049,631 
154.3 16.8 
17 
Computational Linguistics Volume 27, Number 1 
25 
20 
15 
\[~10 
5 
0 
-5 
-10 
5f, I
ll.. . . . . . . . . . . . .  __, , . - .  , _ -  . . . . . . . .  i1:  . . . .  ,~- -15  --20 25 3'0 35 40 f - - -5  . . . .  ,(f 15 20 25 3() 35 40 
? . Length  -, Length 
Figure 9 
The left panel plots MI as a function of the length of the n-gram; the right panel plots RIDF as 
a function of the length of the n-gram. Both panels were computed from the Japanese corpus. 
Note that while there is more dynamic range for shorter n-grams than for longer n-grams, 
there is plenty of dynamic range for n-grams well beyond bigrams and trigrams. 
For subsequent processing, we excluded substrings with tf < 10 to avoid noise, 
resulting in about 1.4 million classes (1.6 million substrings) for English and 10 million 
classes (15 million substrings) for Japanese. We computed RIDF and MI values for the 
longest substring in each of these 1.4 million English classes and 10 million Japanese 
classes. These values can be applied to the other substrings in these classes for RIDF, 
but not for MI. (As mentioned above, two substrings in the same class need not have 
the same MI value.) 
MI of each longest substring, t is computed by the following formula. 
MI(t  = xYz) 
p(xvz)  
= log p(zy)p(z ly  ) 
q(~Yz) 
N 
l?r:, q(xY) q(Yz) 
N q(Y) 
ff(~g~)ff(Y) 
= log t f (xY) t f (Yz ) "  
where x and z are tokens, and Y and zYz are n-grams (sequences of tokens). When Y 
is the empty string, tf(Y) -- N. 
Figure 9 plots RIDF and MI values of 5,000 substrings randomly selected as a func- 
tion of string length. In both cases, shorter substrings have more dynamic range. That 
is, RIDF and MI vary more for bigrams than million-grams. But there is considerable 
dynamic range for n-grams well beyond bigrams and trigrams. 
3.2 Little Correlation between RIDF and MI 
We are interested in comparing and contrasting RIDF and MI. Figure 10 shows that 
RIDF and MI are largely independent. There is little if any correlation between the 
RIDF of a string and the MI of the same string. Panel (a) compares RIDF and MI 
for a sample of English word sequences from the WSJ corpus (excluding unigrams); 
panel (b) makes the same comparison but for Japanese phrases identified as keywords 
on the CD-ROM. In both cases, there are many substrings with a large RIDF value 
and a small MI, and vice versa. 
18 
Yamamoto and Church Term Frequency and Document Frequency for All Substrings 
2 
1 
2 'i? ~ 
: , ? ~ .~ : 
a ..," 
~, ? ~ ?" * . 
i ? ? 
' ?  # ? ?  ? ? 1 ? ? ? ? ? ? 
; ' . ' .  - ? ? l L - -M.  ? 
l ,%,  ; ? 
. . . .  ' : "  . . . . .  0 -~:: 
i i i 
0 MI fo 2o -10 0 MI 10 20 
(a) English strings 
(word sequences) 
(b) Japanese strings 
(character sequences) 
F igure  10 
Both panels plot RIDF versus MI. Panel (a) plots RIDF and MI for a sample of English 
n-grams; panel (b) plots RIDF and MI for Japanese phrases identified as keywords on the 
CD-ROM. The right panel highlights the 10% highest RIDF and 10% lowest MI with a box, as 
well as the 10% lowest RIDF and the 10% highest MI. Arrows point to the boxes for clarity. 
We believe the two statistics are both useful but in different ways. Both pick out 
interesting n-grams, but n-grams with large MI are interesting in different ways from 
n-grams with large RIDF. Consider the English word sequences in Table 2, which all 
contain the word having. These sequences have large MI values and small RIDF values. 
In our collaboration with lexicographers, especially those working on dictionaries for 
learners, we have found considerable interest in statistics such as MI that pick out 
these kinds of phrases. Collocations can be quite challenging for nonnative speakers 
of the language. On the other hand, these kinds of phrases are not very good keywords 
for information retrieval. 
Table  2 
English word sequences containing the word having. Note that these phrases have large MI 
and low RIDF. They tend to be more interesting for lexicography than information retrieval. 
The table is sorted by MI. 
tf df RIDF MI Phrase 
18 18 -0.0 10.5 admits to having 
14 14 -0.0 9.7 admit to having 
25 23 0.1 8.9 diagnosed as having 
20 20 -0.0 7.4 suspected of having 
301 293 0.0 7.3 without having 
15 13 0.2 7.0 denies having 
59 59 -0.0 6.8 avoid having 
18 18 -0.0 6.0 without ever having 
12 12 -0.0 5.9 Besides having 
26 26 -0.0 5.8 denied having 
19 
Computational Linguistics Volume 27, Number 1 
Table 3 
English word sequences containing the word Mr. (sorted by RIDF). The word sequences near 
the top of the list are better keywords than the sequences near the bottom of the list. None of 
them are of much interest o lexicography. 
tf df RIDF MI Phrase 
11 3 1.9 0.6 . Mr. Hinz 
18 5 1.8 6.6 Mr. Bradbury 
51 16 1.7 6.7 Mr. Roemer 
67 25 1.4 6.8 Mr. Melamed 
54 27 1.0 5.8 Mr. Burnett 
11 8 0.5 1.1 Mr. Eiszner said 
53 40 0.4 0.3 Mr. Johnson. 
21 16 0.4 0.2 Mr. Nichols said. 
13 10 0.4 0.4 . Mr. Shulman 
176 138 0.3 0.5 Mr. Bush has 
13 11 0.2 1.5 to Mr. Trump's 
13 11 0.2 -0.9 Mr. Bowman, 
35 32 0.1 1.2 wrote Mr. 
12 11 0.1 1.7 Mr. Lee to 
22 21 0.1 1.4 facing Mr. 
11 11 -0.0 0.7 Mr. Poehl also 
13 13 -0.0 1.4 inadequate. " Mr. 
16 16 -0.0 1.6 The 41-year-old Mr. 
19 19 -0.0 0.5 14. Mr. 
26 26 -0.0 0.0 in November. Mr. 
27 27 -0.0 -0.0 " For his part,  Mr. 
38 38 -0.0 1.4 . AMR, 
39 39 -0.0 -0.3 for instance, Mr. 
Table 3 shows MI and RIDF values for a sample of word sequences containing 
the word Mr. The table is sorted by RIDE The sequences near the top of the list are 
better keywords than the sequences further down. None of these sequences would be 
of much interest o a lexicographer (unless he or she were studying names). Many of 
the sequences have rather small MI values. 
Table 4 shows a few word sequences tarting with the word the with large MI 
values. All of these sequences have high MI (by construction), but some are high 
in RIDF as well (labeled B), and some are not (labeled A). Most of the sequences 
are interesting in one way or another, but the A sequences are different from the 
B sequences. The A sequences would be of more interest to someone studying the 
grammar  in the WSJ subdomain,  whereas the B sequences would be of more interest 
to someone studying the terminology in this subdomain. The B sequences in Table 4 
tend to pick out specific events in the news, if not specific stories. The phrase, the Basic 
Law, for example, picks out a pair of stories that discuss the event of the handover  of 
Hong Kong to China, as il lustrated in the concordance shown in Table 5. 
Table 6 shows a number  of word sequences with high MI containing common 
prepositions. The high MI indicates an interesting association, but again most have 
low RIDF and are not particularly good keywords,  though there are a few exceptions 
(Just for Men, a wel l -known brand name, has a high RIDF and is a good keyword). 
The Japanese substrings are similar to the English substrings. Substrings with high 
RIDF pick out specific documents (and/or  events) and therefore tend to be relatively 
good keywords. Substrings with high MI have nonindependent distributions (if not 
noncomposit ional semantics), and are therefore likely to be interesting to a lexicogra- 
pher or linguist. Substrings that are high in both are more likely to be meaningful  units 
20 
Yamamoto and Church Term Frequency and Document Frequency for All Substrings 
Table 4 
English word sequence containing the. All of these phrases have high MI. Some have high 
RIDF, and some do not. 
(A): Low RIDF (poor keywords) 
tf df RIDF MI Phase 
11 11 -0.0 11.1 the up side 
73 66 0.1 9.3 the will of 
16 16 -0.0 8.6 the sell side 
17 16 0.1 8.5 the Stock Exchange of 
16 15 0.1 8.5 the buy side 
20 20 -0.0 8.4 the down side 
55 54 0.0 8.3 the will to 
14 14 -0.0 8.1 the saying goes 
15 15 -0.0 7.6 the going gets 
(B): High RIDF (better keywords) 
tf df RIDF MI Phase 
37 3 3.6 2.3 the joint commission 
66 8 3.0 3.6 the SSC 
55 7 3.0 2.0 The Delaware & 
37 5 2.9 3.6 the NHS 
22 3 2.9 3.4 the kibbutz 
22 3 2.9 4.1 the NSA's 
29 4 2.9 4.2 the DeBartolos 
36 5 2.8 2.3 the Basic Law 
21 3 2.8 2.3 the national output 
(words or phrases) than substrings that are high in just one or the other. Meaningless 
fragments tend to be low in both MI and RIDE 
We grouped the Japanese classes into nine cells depending on whether the RIDF 
was in the top 10%, the bottom 10%, or in between, and whether the MI was in the 
top 10%, the bottom 10%, or in between. Substrings in the top 10% in both RIDF and 
MI tend to be meaningful words such as (in English translation) merger, stock certi~'cate, 
dictionary, wireless, and so on. Substrings in the bottom 10% in both RIDF and MI tend 
to be meaningless fragments, or straightforward compositional combinations of words 
such as current regular-season game. Table 7 shows examples where MI and RIDF point 
in opposite directions (see highlighted rectangles in panel (b) of Figure 10). 
We have observed previously that MI is high for general vocabulary (words found 
in dictionary) and RIDF is high for names, technical terminology, and good keywords 
for information retrieval. Table 7 suggests an intriguing pattern. Japanese uses different 
character sets for general vocabulary and loan words. Words that are high in MI tend 
to use the general vocabulary character sets (hiragana nd kanji) whereas words that 
are high in RIDF tend to use the loan word character sets (katakana nd English). 
(There is an important exception, though, for names, which will be discussed in the 
next subsection.) 
The character sets largely reflect the history of the language. Japanese uses four 
character sets (Shibatani 1990). Typically, functional words of Japanese origin are writ- 
ten in hiragana. Words that were borrowed from Chinese many hundreds of years ago 
are written in kanji. Loan words borrowed more recently from Western languages are 
written in katakana. Truly foreign words are written in the English character set (also 
known as romaji). We were pleasantly surprised to discover that MI and RIDF were 
distinguishing substrings on the basis of these character set distinctions. 
21 
Computational Linguistics Volume 27, Number 1 
Table 5 
Concordance of the phrase the Basic Law. Note that most of the instances of the Basic Law appear 
in just two stories, as indicated by the doc-id (the token-id of the first word in the document). 
token-id left context right context doc-id 
2229521: line in the drafting of the Basic Law that will determine how Hon 2228648 
2229902: s policy as expressed in the Basic Law - as Gov. Wilson's debut s 2228648 
9746758: he U.S. Constitution and the Basic Law of the Federal Republic of 9746014 
11824764: any changes must follow the Basic Law, Hong Kong's miniconstitut 11824269 
33007637: sts a tentative draft of the Basic Law, and although this may be 33007425 
33007720: the relationship between the Basic Law and the Chinese Constitutio 33007425 
33007729: onstitution. Originally the Basic Law was to deal with this topic 33007425 
33007945: wer of interpretation f the Basic Law shall be vested in the NPC 33007425 
33007975: tation of a provision of the Basic Law, the courts of the HKSAR { 33007425 
33008031: interpret provisions of the Basic Law. If a case involves the in 33007425 
33008045: tation of a provision of the Basic Law concerning defense, foreig 33007425 
33008115: etation of an article of the Basic Law regarding " defense, forei 33007425 
33008205: nland representatives of the Basic Law Drafting Committee fear tha 33007425 
33008398: e : Mainland drafters of the Basic Law simply do not appreciate th 33007425 
33008488: pret al the articles of the Basic Law. While recognizing that th 33007425 
33008506: y and power to interpret the Basic Law, it should irrevocably del 33007425 
33008521: pret those provisions of the Basic Law within the scope of Hong Ko 33007425 
33008545: r the tentative draft of the Basic Law, I cannot help but conclud 33007425 
33008690: d of being guaranteed by the Basic Law, are being redefined out o 33007425 
33008712: uncilor, is a member of the Basic Law Drafting Committee. 33007425 
39020313: sts a tentative draft of the Basic Law, and although this may be 39020101 
39020396: the relationship between the Basic Law and the Chinese Constitutio 39020101 
39020405: onstitution. Originally the Basic Law was to deal with this topic 39020101 
39020621: wer of interpretation f the Basic Law shall be vested in the NPC 39020101 
39020651: tation of a provision of the Basic Law, the courts of the HKSAR { 39020101 
39020707: interpret provisions of the Basic Law . If a case involves the in 39020101 
39020721: tation of a provision of the Basic Law concerning defense, foreig 39020101 
39020791: etation of an article of the Basic Law regarding " defense, forei 39020101 
39020881: nland representatives of the Basic Law Drafting Committee fear tha 39020101 
39021074: e : Mainland drafters of the Basic Law simply do not appreciate th 39020101 
39021164: pret al the articles of the Basic Law. While recognizing that th 39020101 
39021182: y and power to interpret the Basic Law, it should irrevocably del 39020101 
39021197: pret those provisions of the Basic Law within the scope of Hong Ko 39020101 
39021221: r the tentative draft of the Basic Law, I cannot help but conclud 39020101 
39021366: d of being guaranteed by the Basic Law, are being redefined out o 39020101 
39021388: uncilor, is a member of the Basic Law Drafting Committee. 39020101 
3.3 Names  
As mentioned above, names are an important exception to the rule that kanji (Chinese 
characters) are used for general vocabulary (words found in the dictionary) that were 
borrowed hundreds of years ago and katakana characters are used for more recent 
loan words (such as technical terminology). As illustrated in Table 7, kanji are also 
used for the names of Japanese people and katakana re used for the names of people 
from other countries. 
Names are quite different in English and Japanese. Figure 11 shows a striking con- 
trast in the distributions of MI and RIDF values. MI has a more compact distribution 
in English than Japanese. Japanese names cluster into two groups, but English names 
do not. 
The names shown in Figure 11 were collected using a simple set of heuristics. For 
English, we selected substrings tarting with the titles Mr., Ms., or Dr. For Japanese, we 
selected keywords (as identified by the CD-ROM) ending with the special character 
22 
Yamamoto and Church Term Frequency and Document Frequency for All Substrings 
Table 6 
English word sequences containing common prepositions. All have high MI (by construction); 
most do not have high RIDF (though there are a few exceptions such as Just for Men, a 
well-known brand name). 
tf df RIDF MI Preposition = "for" 
14 14 -0.0 14.6 feedlots for fattening 
15 15 -0.0 14.4 error for subgroups 
12 12 -0.0 14.1 Voice for Food 
10 5 1.0 13.8 Quest for Value 
12 4 1.6 13.8 Friends for Education 
13 13 -0.0 13.7 Commissioner for Refugees 
23 21 0.1 13.7 meteorologist for Weather 
10 2 2.3 13.4 Just for Men 
10 9 0.2 13.4 Witness for Peace 
19 16 0.2 12.9 priced for reoffering 
tf df RIDF MI Preposition = "on" 
1l 5 1.1 14.3 Terrorist on Trial 
11 10 0.1 13.1 War on Poverty 
13 12 0.1 12.7 Institute on Drug 
16 16 -0.0 12.6 dead on arrival 
12 12 -0.0 11.6 from on high 
12 12 -0.0 11.6 knocking on doors 
22 18 0.3 11.3 warnings on cigarette 
11 11 -0.0 11.2 Subcommittee on Oversight 
17 12 0.5 11.2 Group on Health 
22 20 0.1 11.1 free on bail 
tf df RIDF MI Preposition = "by" 
11 11 -0.0 12.9 piece by piece 
13 13 -0.0 12.6 guilt by association 
13 13 -0.0 12.5 step by step 
15 15 -0.0 12.4 bit by bit 
16 16 -0.0 l l .8 engineer by training 
61 59 0.0 11.5 side by side 
17 17 -0.0 11.5 each by Korea's 
12 12 -0.0 11.3 hemmed in by 
11 11 -0.0 10.8 dictated by formula 
20 20 -0.0 10.7 70%-owned by Exxon 
tf df RIDF MI Preposition = "of" 
11 10 0.1 16.8 Joan of Arc 
12 5 1.3 16.2 Ports of Call 
16 16 -0.0 16.1 Articles of Confederation 
14 13 0.1 16.1 writ of mandamus 
10 9 0.2 15.9 Oil of Olay 
11 11 -0.0 15.8 shortness of breath 
10 9 0.2 15.6 Archbishop of Canterbury 
10 8 0.3 15.3 Secret of My 
12 12 -0.0 15.2 Lukman of Nigeria 
16 4 2.0 15.2 Days of Rage 
(-shi), which  is rough ly  the equ iva lent  of the Engl ish titles Mr. and Ms. In both  cases, 
phrases  were  requ i red  to have  tf > 10. 3 
3 This procedure produced the interesting substring, Mr. From, where both words would normally 
appear on a stop list. This name has a large RIDF. (The MI, though, is small because the parts are so 
high in frequency.) 
23 
Computational Linguistics Volume 27, Number 1 
Tab le  7 
Examples of keywords with extreme values of RIDF and MI that point in opposite directions. 
The top half (high RIDF and low MI) tends to have more loan words, largely written in 
katakana nd English. The bottom half (low RIDF and high MI) tends to have more general 
vocabulary, largely written in Chinese kanji. 
RIDF  MI  Subst r ings  Features 
High Low :~k~E(native lastname) 
10% 10% SUN (company ame) 
U - -  if(foreign name) 
~'c~ U(brush) 
',1 7 7' -- (sofa) 
Low High ~;~< f'~.~,~ (huge) 
10% 10% ~ ~J 1~ (passive) 
~t,~ O (determination) 
J~j(native full name) 
~j~(native full name) 
kanji character 
English character 
katakana character 
hiragana character 
loan word, katakana 
general vocabulary 
general vocabulary, kanji 
general vocabulary 
kanji character 
kanji character 
5 
4 
1 
qZ 
0 
? 
+-+ 
.~ ? 
" ++I +~C ? 
+.+ . . 
'o * 0++ , <~ 
'."? ,+ . , ,<~%,~ " 
: .+ ' ;+  . 
i i 
0 4 MI 8 12 
(a) Eng l i sh  names  
F igure  11 
MI and RIDF of people's names. 
2 
I 
+ o + ??++ ??  
o+~.~.o~,...~$?,,,'~ ? o? ,.. +,#.,.~_.,,.~ le++,++.+.?~ ?? 
? - ' . I L ' ,+ I~+, , .~ , .~  + , I ~ < , Q "  ++CP++ .,, 
? ? <+~lh; , I l lml l I l " \ ] IR~.& ?*<,?  ? ,,~'<,Ir...,~b-~. <, o,. o 
.+ ~+~mlL',ll,,l-l-l-l-l~ "; l ~+? ? ,,? t 
+ <, <+ 
+ ? . 
+ "~."r'I,l~lll3P~+V ." 
. ? . ? ? . ? . i 
-8 -4 0 MI 4 8 12 
(b) Japanese  names  
The Engl ish names have a sharp cutoff a round MI = 7 due in large part  to 
the title Mr. MI( 'Mr. ' ,  x) = log 2 ~ N  - -  l?g2 tf('Mr.',x)tf(x) ---- 7.4 - log 2 ~'t f (x) '  Since 
log 2 ~t f (x )  is a small posit ive number,  typical ly 0-3, MI('Mr',x) < 7.4. 
Names  general ly have RIDF values ranging f rom practical ly noth ing (for com- 
mon names like Jones) to extremely large values for excellent keywords.  The Japanese 
names,  however,  cluster into two groups,  those with RIDF above 0.5, and those with 
RIDF below 0.5. The separat ion above and be low RIDF = 0.5, we believe, is a reflec- 
24 
Yamamoto and Church Term Frequency and Document Frequency for All Substrings 
tion of the well-known distinction between ew information and given information i  
discourse structure. It is common in both English and Japanese, for the first mention 
of a name in a news article to describe the name in more detail than subsequent uses. 
In English, for example, terms like spokesman or spokeswoman and appositives are quite 
common for the first use of a name, and less so, for subsequent uses. In Japanese, the 
pattern appears to be even more rigid than in English. The first use will very often 
list the full name (first name plus last name), unlike subsequent uses, which almost 
always omit the first name. As a consequence, the last name exhibits a large range 
of RIDF values, as in English, but the full name will usually (90%) fall below the 
RIDF = 0.5 threshold. The MI values have a broader ange as well, depending on the 
compositionality of the name. 
To summarize, RIDF and MI can be used to identify a number of interesting 
similarities and differences in the use of names. Names are interestingly different 
from general vocabulary. Many names are very good keywords and have large RIDE 
General vocabulary tends to have large MI. Although we observe this basic pattern 
over both English and Japanese, names bring up some interesting differences between 
the two languages such as the tendency for Japanese names to fall into two groups 
separated by the RIDF = 0.5 threshold. 
3.4 Word Extraction 
RIDF and MI may be useful for word extraction. In many languages such as Chinese, 
Japanese, and Thai, word extraction isnot an easy task, because, unlike English, many 
of these languages do not use delimiters between words. Automatic word extraction 
can be applied to the task of dictionary maintenance. Since most NLP applications 
(including word segmentation for these languages) are dictionary based, word extrac- 
tion is very important for these languages. Nagao and Mori (1994) and Nagata (1996) 
proposed n-gram methods for Japanese. Sproat and Shih (1990) found MI to be useful 
for word extraction i  Chinese. 
We performed the following simple experiment to see if both MI and RIDF could 
be useful for word extraction in Japanese. We extracted four random samples of 100 
substrings each. The four samples cover all four combinations of high and low RIDF 
and high and low MI, where high is defined to be in the top 10% and low is defined 
to be in the bottom 10%. Then we manually scored each sample substring using our 
own subjective judgment. Substrings were labeled "good" (the substring is a word), 
"bad" (the substring is not a word), or "gray" (the judge is not sure). The results are 
presented in Table 8, which shows that substrings with high scores in both dimensions 
are more likely to be words than substrings that score high in just one dimension. 
Substrings with low scores in both dimensions are very unlikely to be words. These 
results demonstrate plausibility for the use of multiple statistics. The approach could 
be combined with other methods in the literature such as Kita et al (1994) to produce 
a more practical system. In any case, automatic word extraction is not an easy task for 
Japanese (Nagata 1996). 
4. Conc lus ion 
Bigrams and trigrams are commonly used in statistical natural anguage processing; 
this paper described techniques for working with much longer n-grams, including 
million-grams and even billion-grams. We presented algorithms (and C code) for com- 
puting term frequency (tf) and document frequency (dr) for all n-grams (substrings) 
in a corpus (sequence). The method took only a few hours to compute tf and df for 
all the n-grams in two large corpora, an English corpus of 50 million words of Wall 
25 
Computational Linguistics Volume 27, Number 1 
Table 8 
The combination of RIDF and MI is better in a word extraction task than either by itself, 
which is better than neither. Each cell reports performance over a sample of 100 substrings. 
Substrings were subjectively judged to be "good" (the substring is a word), "bad" (the 
substring is not a word), or "gray" (the judge is not sure). Two performance values are 
reported, indicating what percentage of the 100 substrings are words. The larger performance 
values count the "gray" substrings as words; the smaller performance values count the "gray" 
substrings as nonwords. 
All MI MI(high 10%) MI(low 10%) 
All RIDF - -  20-44% 2-11% 
RIDF(high 10%) 29-51% 38-55% 11-35% 
RIDF(low 10%) 3-18% 4-13% 0-8% 
Street Journal news articles and a Japanese corpus of 216 million characters of Mainichi 
Shimbun ews articles. 
The method works by grouping substrings into classes so that the computation 
of tf and df over order N 2 substrings can be reduced to a computation over order N 
classes. The reduction makes use of four properties: 
Properties 1-2: all substrings in a class have the same statistics (at least 
for the statistics of interest, namely tf and dJ), 
Property 3: the set of all substrings are partit ioned by the classes, and 
Property 4: there are many fewer classes (order N) than substrings 
(order N2). 
The second half of the paper used the results of computing tf and df for all n-grams 
in the two large corpora mentioned above. We compared and contrasted RIDF and 
MI, statistics that are motivated by work in lexicography and information retrieval. 
Both statistics compare the frequency of an n-gram to chance, but they use different 
notions of chance. RIDF looks for n-grams whose distributions pick out a relatively 
small number documents, unlike a random (Poisson) distribution. These n-grams tend 
to be good keywords for information retrieval, such as technical terms and names. MI 
looks for n-grams whose internal structure cannot be attributed to compositionality. MI
tends to pick out general vocabulary--words and phrases that appear in dictionaries. 
We believe that both statistics are useful, but in different and complementary ways. 
In a Japanese word extraction task, the combination of MI and RIDF performed better 
than either by itself. 
Appendix: C Code 
The following code is also available at http: / /www.mi lab. is . tsukuba.ac. jp /~myama/ 
tfdf. 
A: C Code to Print All LCP-Delimited Intervals using C Language's Stack 
The function output  (below) is called 2N - 1 times. It will output an interval if the 
interval is lcp-delimited (LBL < SIL). Trivial intervals are always lcp-delimited. Non- 
trivial intervals are lcp-delimited if the bounding lcps are smaller than the SIL=lcp\[k\], 
26 
Yamamoto and Church Term Frequency and Document Frequency for All Substrings 
where k is the representative. 
void output(int i, int j, int k){ 
int LBL = (Icp\[i\] > icp\[j+l\]) ? icp\[i\] : icp\[j+l\]; 
int SIL = icp\[k\]; 
if(i==j) printf("trivial <~d,~d>, tf=l\n ", i, j); 
else if(LBL < SIL) printf("non-trivial <~d, ~d>, rep=~d, tf=~d\n", 
i, j, k, j-i+i); 
int print_LDIs(int i, int k){ 
int j = k; 
output(k,k,O); /* trivial intervals */ 
while(icp\[k\] <= icp\[j+l\] && j+l < N) j = print_LDIs(k, j+l); 
output(i,j,k); /~ non-trivial intervals */ 
return j;} 
B: C Code to Print Al l  LCP-Del imited Intervals Us ing an Own Stack 
print_LDIs_stack is similar to print_LDIs, but uses its own stack. It takes the corpus 
size, N, as an argument. 
#define STACK_SIZE i00000 
#define Top_i (stack\[sp-l\].i) 
#define Top_k (stack\[sp-l\].k) 
struct STACK {int i; int k;} stack\[STACK_SIZE\]; 
int sp = O; /~ stack pointer ~/ 
void push(int i, int k) { 
if(sp >= STACK_SIZE) { 
fprintf(stderr, "stack overflow\n"); 
exit(2);} 
stack\[sp\].i = i; 
stack\[sp++\].k = k;} 
void pop() {sp--;} 
void print_LDIs_stack(int N) { 
int j; 
push(O,O); 
for(j = O; j < N; j++) { 
output(j, j, 0); 
while(icp\[j+l\] < Icp\[Top_k\]) { 
output(Top_i, j, Top_k); 
pop();} 
push(Top_k, j+l);}} 
27 
Computational Linguistics Volume 27, Number 1 
C: C Code to Print All LCP-Delimited Intervals with tf and df 
The steps 5 and 6 of the algorithm in Section 2.6 are implemented as the function 
dec df using the binary search. 
#define STACK_SIZE I00000 
#define Top_i (stack\[sp-l\] .i) 
#define Top_k (stack\[sp-l\] .k) 
#define Top_dr (stack\[sp-l\] .dr) 
struct STACK {int i; int k; int dr;} stack\[STACK_SIZE\]; 
int sp = O; /* stack pointer */ 
void push(int i, int k, int dr) { 
if(sp >= STACK_SIZE){ 
fprintf(stderr, "stack overflowkn"); 
exit (2) ;} 
stack\[sp\] .i = i; 
stack\[sp\] .k = k; 
stack \[sp++\] . df = df ; } 
void pop() {sp--;} 
void output(int i, int j, int k, int dr) { 
int LBL; 
if(icp\[i\] > icp\[j+l\]) LBL = icp\[i\]; 
else LBL = icp\[3+l\]; 
if(i==j) printf("trivial <~d,~d>, tf=l\n '', i, j); 
else if(LBL < Icp\[k\]) 
printf("non-trivial <~d, ~d>, rep=~d, tf=~d, df=~d\n", 
i, j, k, j-i+l, df); 
* Print_LDIs with_dr does not only print tf, but also df. 
* It takes the corpus size, N, and the number of documents, D. 
*doc( )  returns the document number of the suffix array's index. 
* dec_dr() decrease a dr-counter in the stack when duplicate 
* counting occurs. 
*/ 
void dec_df(int docid) { 
int beg=0, end=sp, mid=sp/2; 
while(beg != mid) { 
if(doclink\[docid\] >= stack\[mid\].i) beg = mid; 
else end = mid; 
mid = (beg + end) / 2; 
} 
stack\[mid\].df--; 
28 
Yamamoto and Church Term Frequency and Document Frequency for All Substrings 
print_LDIs_with_df( int  N, int D) { 
int i, j, dr; 
docl ink = (int ~)malloc(sizeof( int)  ? D); 
for(i = O; i < D; i++) doclink\[i\] = -I; 
push(O,O, I) ; 
for(j = O; j < N; j++) { 
output (j ,j ,0, I) ; 
i f(docl ink\[doc(j)\]  != -I) dec_df(doc(j));  
doclink\[doc(j)\] = j ; 
df = i; 
while (icp\[j+l\] < icp\[Top_k\]) { 
df = Top_dr + dr; 
output (Top_i, j, Top_k, dr) ; 
pop() ; 
} 
push(Top_k, j+l, dr); 
} 
Acknowledgments 
We would like to thank the anonymous 
reviewers for Computational Linguistics who 
made insightful comments on an earlier 
draft. 
References 
Charniak, Eugene. 1993. Statistical Language 
Learning. MIT Press. 
Church, Kenneth W. and William A. Gale. 
1995. Poisson mixtures. Natural Language 
Engineering, 1(2):163-190. 
Gonnet, Gaston H., Ricardo A. Baeza-Yates, 
and Tim Snider. 1992. New indices for 
text: PAT trees and PAT arrays. In 
Information Retrieval: Data Structure & 
Algorithms, pages 66-82. Prentice Hall 
PTR. 
Gusfield, Dan. 1997. Algorithms on Strings, 
Trees, and Sequences: Computer Science and 
Computational Biology. Cambridge 
University Press. 
Hui, Lucas Chi Kwong. 1992. Color set size 
problem with applications to string 
matching. In Lecture Notes in Computer 
Science, Volume 644. Springer (CPM92: 
Combinatorial Pattern Matching, 3rd 
Annual Symposium), pages 230-243. 
Jelinek, Frederick. 1997. Statistical Methods 
/or Speech Recognition. MIT Press. 
Katz, Slava M. 1987. Estimation of 
probabilities from sparse data for the 
language model component of a speech 
recognizer. IEEE Transactions on Acoustics, 
Speech, and Signal Processing, 
ASSP-35(3):400-401. 
Kita, Kenji, Yasuhiko Kato, Takashi Omoto, 
and Yoneo Yano. 1994. A comparative 
study of automatic extraction of 
collocations from corpora: Mutual 
information vs. cost criteria. Journal of 
Natural Language Processing, 1(1):21-33. 
Manber, Udi and Gene Myers. 1990. Suffix 
arrays: A new method for on-line string 
searches. In Proceedings ofThe First Annual 
ACM-SIAM Symposium on Discrete 
Algorithms, pages 319-327. URL=http:// 
glimpse.cs.arizona.edu / udi.html. 
McCreight, Edward M. 1976. A 
space-economical suffix tree construction 
algorithm. Journal of the ACM, 23:262- 
272. 
Nagao, Makoto and Shinsuke Mori. 1994. A 
new method of n-gram statistics for large 
number of n and automatic extraction of 
words and phrases from large text data of 
Japanese. In Proceedings ofthe 15th 
International Conference on Computational 
Linguistics, pages 611-615. 
Nagata, Masaaki. 1996. Automatic 
extraction of new words from Japanese 
texts using generalized forward-backward 
search. In Proceedings ofthe Conference on 
Empirical Methods in Natural Language 
Processing, pages 48-59. 
Paul, Douglas B. and Janet M. Baker. 1992. 
The design for the Wall Street 
Journal-based CSR corpus. In Proceedings 
of DARPA Speech and Natural Language 
Workshop, pages 357-361. 
Shibatani, Masayoshi. 1990. The Languages of 
Japan. Cambridge Language Surveys. 
Cambridge University Press. 
29 
Computational Linguistics Volume 27, Number 1 
Sparck Jones, Karen. 1972. A statistical 
interpretation of term specificity and its 
application in retrieval. Journal of 
Documentation, 28(1):11-21. 
Sproat, Richard and Chilin Shih. 1990. A 
statistical method for finding word 
boundaries in Chinese text. Computer 
Processing of Chinese and Oriental Languages, 
4(4):336-351. 
Ukkonen, Esko. 1995. On-line construction 
of suffix trees. Algorithmica, 14(3):249-260. 
30 
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 449?456,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Relevance Feedback Models for Recommendation
Masao Utiyama
National Institute of Information and Communications Technology
3-5 Hikari-dai, Soraku-gun, Kyoto 619-0289 Japan
mutiyama@nict.go.jp
Mikio Yamamoto
University of Tsukuba, 1-1-1 Tennodai, Tsukuba, 305-8573 Japan
myama@cs.tsukuba.ac.jp
Abstract
We extended language modeling ap-
proaches in information retrieval (IR) to
combine collaborative filtering (CF) and
content-based filtering (CBF). Our ap-
proach is based on the analogy between
IR and CF, especially between CF and rel-
evance feedback (RF). Both CF and RF
exploit users? preference/relevance judg-
ments to recommend items. We first in-
troduce a multinomial model that com-
bines CF and CBF in a language modeling
framework. We then generalize the model
to another multinomial model that approx-
imates the Polya distribution. This gener-
alized model outperforms the multinomial
model by 3.4% for CBF and 17.4% for
CF in recommending English Wikipedia
articles. The performance of the gener-
alized model for three different datasets
was comparable to that of a state-of-the-
art item-based CF method.
1 Introduction
Recommender systems (Resnick and Varian,
1997) help users select particular items (e.g,
movies, books, music, and TV programs) that
match their taste from a large number of choices
by providing recommendations. The systems ei-
ther recommend a set of N items that will be of
interest to users (top-N recommendation problem)
or predict the degree of users? preference for items
(prediction problem).
For those systems to work, they first have to
aggregate users? evaluations of items explicitly or
implicitly. Users may explicitly evaluate certain
movies as rating five stars to express their prefer-
ence. These evaluations are used by the systems
as explicit ratings (votes) of items or the systems
infer the evaluations of items from the behavior of
users and use these inferred evaluations as implicit
ratings. For example, systems can infer that users
may like certain items if the systems learn which
books they buy, which articles they read, or which
TV programs they watch.
Collaborative filtering (CF) (Resnick et al,
1994; Breese et al, 1998) and content-based (or
adaptive) filtering (CBF) (Allan, 1996; Schapire
et al, 1998) are two of the most popular types
of algorithms used in recommender systems. A
CF system makes recommendations to current
(active) users by exploiting their ratings in the
database. User-based CF (Resnick et al, 1994;
Herlocker et al, 1999) and item-based CF (Sarwar
et al, 2001; Karypis, 2001), among other CF algo-
rithms, have been studied extensively. User-based
CF first identifies a set of users (neighbors) that
are similar to the active user in terms of their rat-
ing patterns in the database. It then uses the neigh-
bors? rating patterns to produce recommendations
for the active user. On the other hand, item-based
CF calculates the similarity between items before-
hand and then recommends items that are similar
to those preferred by the active user. The perfor-
mance of item-based CF has been shown to be
comparable to or better than that of user-based CF
(Sarwar et al, 2001; Karypis, 2001). In contrast
to CF, CBF uses the contents (e.g., texts, genres,
authors, images, and audio) of items to make rec-
ommendations for the active user. Because CF
and CBF are complementary, much work has been
done to combine them (Basu et al, 1998; Yu et
al., 2003; Si and Jin, 2004; Basilico and Hofmann,
2004).
The approach we took in this study is designed
to solve top-N recommendation problems with im-
449
plicit ratings by using an item-based combination
of CF and CBF. The methods described in this
paper will be applied to recommending English
Wikipedia1 articles based on those articles edited
by active users. (This is discussed in Section 3.)
We use their editing histories and the contents of
their articles to make top-N recommendations. We
regard users? editing histories as implicit ratings.
That is, if users have edited articles, we consider
that they have positive attitudes toward the arti-
cles. Those implicit ratings are regarded as pos-
itive examples. We do not have negative examples
for learning their negative attitudes toward arti-
cles. Consequently, handling our application with
standard machine learning algorithms that require
both positive and negative examples for classifica-
tion (e.g., support vector machines) is awkward.
Our approach is based on the advancement in
language modeling approaches to information re-
trieval (IR) (Croft and Lafferty, 2003) and extends
these to incorporate CF. The motivation behind our
approach is the analogy between CF and IR, espe-
cially between CF and relevance feedback (RF).
Both CF and RF recommend items based on user
preference/relevance judgments. Indeed, RF tech-
niques have been applied to CBF, or adaptive fil-
tering, successfully (Allan, 1996; Schapire et al,
1998). Thus, it is likely that RF can also be applied
to CF.
To apply RF, we first extend the representation
of items to combine CF and CBF under the models
developed in Section 2. In Section 3, we report
our experiments with the models. Future work and
conclusion are in Sections 4 and 5.
2 Relevance feedback models
The analogy between IR and CF that will be ex-
ploited in this paper is as follows.2 First, a docu-
ment in IR corresponds to an item in CF. Both are
represented as vectors. A document is represented
as a vector of words (bag-of-words) and an item
is represented as a vector of user ratings (bag-of-
user ratings). In RF, a user specifies documents
that are relevant to his information need. These
documents are used by the system to retrieve new
1http://en.wikipedia.org/wiki/Main Page
2The analogy between IR and CF has been recognized.
For example, Breese et al (1998) used the vector space
model to measure the similarity between users in a user-based
CF framework. Wang et al (2005) used a language modeling
approach different from ours. These works, however, treated
only CF. In contrast with these, our model extends language
modeling approaches to incorporate both CF and CBF.
relevant documents. In CF, an active user (implic-
itly) specifies items that he likes. These items are
used to search new items that will be preferred by
the active user.
We use relevance models (Lavrenko and Croft,
2001; Lavrenko, 2004) as the basic framework
of our relevance feedback models because (1)
they perform relevance feedback well (Lavrenko,
2004) and (2) they can simultaneously handle dif-
ferent kinds of features (e.g., different language
texts (Lavrenko et al, 2002), such as texts and im-
ages (Leon et al, 2003). These two points are es-
sential in our application.
We first introduce a multinomial model follow-
ing the work of Lavrenko (2004). This model is
a novel one that extends relevance feedback ap-
proaches to incorporate CF. It is like a combina-
tion of relevance feedback (Lavrenko, 2004) and
cross-language information retrieval (Lavrenko et
al., 2002). We then generalize that model to an ap-
proximated Polya distribution model that is better
suited to CF and CBF. This generalized model is
the main technical contribution of this work.
2.1 Preparation
Lavrenko (2004) adopts the method of kernels to
estimate probabilities: Let d be an item in the
database or training data, the probability of item x
is estimated as p(x) = 1M
?
d p(x|?d), where M
is the number of items in the training data, ?d is the
parameter vector estimated from d, and p(x|?d) is
the conditional probability of x given ?d.3 This
means that once we have defined a probability dis-
tribution p(x|?) and the method of estimating ?d
from d, then we can assign probability p(x) to x
and apply language modeling approaches to CF
and CBF.
To begin with, we define the representation
of item x as the concatenation of two vectors
{wx,ux}, where wx = wx1wx2 . . . is the se-
quence of words (contents) contained in x and
ux = ux1ux2 . . . is the sequence of users who
have rated x implicitly. We use Vw and Vu to de-
note the set of words and users in the database.
The parameter vector ? is also the concatenation
of two vectors {?, ?}, where ? and ? are the pa-
rameter vectors for Vw and Vu, respectively. The
probability of x given ? is defined as p(x|?) =
p?(wx|?)p?(ux|?).
3Item d in summation ?d and word w in
?
w and
?
wgo over every distinct item d and word w in the training data,
unless otherwise stated.
450
2.2 Multinomial model
Our first model regards that both p? and p? follow
multinomial distributions. In this case, ?(w) and
?(u) are the probabilities of word w and user u.
Then, p?(wx|?) is defined as
p?(wx|?) =
|wx|?
i=1
?(wxi) =
?
w?Vw
?(w)n(w,wx)
(1)
where n(w,wx) is the number of occurrences of w
in wx. In this model, we use a linear interpolation
method to estimate probability ?d(w).
?d(w) = ??Pl(w|wd) + (1? ??)Pg(w) (2)
where Pl(w|wd) = n(w,wd)?
w? n(w?,wd)
, Pg(w) =?
d n(w,wd)?
d
?
w? n(w?,wd)
and ?? (0 ? ?? ? 1) is
a smoothing parameter. The estimation of user
probabilities goes similarly: Let n(u,ux) be the
number of times user u implicitly rated item x,
we define or estimate p?, ?? and ?d in the same
way. In summary, we have defined a probability
distribution p(x|?) and the method of estimating
?d = {?d, ?d} from d.
To recommend top-N items, we have to rank
items in the database in response to the implicit
ratings of active users. We call those implicit rat-
ings query q. It is a set of items and is represented
as q = {q1 . . .qk}, where qi is an item implic-
itly rated by an active user and k is the size of q.
We next estimate ?q = {?q, ?q}. Then, we com-
pare ?q and ?d to rank items by using Kullback-
Leibler divergence D(?q||?d) (Lafferty and Zhai,
2001; Lavrenko, 2004).
?q(w) can be approximated as
?q(w) = 1k
k?
i=1
?qi(w) (3)
where ?qi(w) is obtained by Eq. 2 (Lavrenko,
2004). However, we found in preliminary experi-
ments that smoothing query probabilities hurt per-
formance in our application. Thus, we use
?qi(w) = Pl(w|wqi) =
n(w,wqi)?
w? n(w?,wqi)
(4)
instead of Eq. 2 when qi is in a query.
Because KL-divergence is a distance measure,
we use a score function derived from ?D(?q||?d)
to rank items. We use Sq(d) to denote the score
of d given q. Sq(d) is derived as follows. (We
ignore terms that are irrelevant to ranking items.)
?D(?q||?d) = ?D(?q||?d)?D(?q||?d)
?D(?q||?d) rank= 1k
k?
i=1
S(?qi ||?d) (5)
where
S(?qi ||?d) =
?
w
Pl(w|wqi)?log
(
??Pl(w|wd)
(1? ??)Pg(w) + 1
)
.
(6)
The summation goes over every word w that
is shared by both wqi and wd. We define
S(?qi ||?d) similarly.4 Then, the score of d given
qi, Sqi(d) is defined as
Sqi(d) = ?sS(?qi ||?d) + (1? ?s)S(?qi ||?d)
(7)
where ?s (0 ? ?s ? 1) is a free parameter. Fi-
nally, the score of d given q is
Sq(d) = 1k
k?
i=1
Sqi(d). (8)
The calculation of Sq(d) can be very efficient
because once we cache Sqi(d) for each item pair
of qi and d in the database, we can reuse it to cal-
culate Sq(d) for any query q. We further optimize
the calculation of top-N recommendations by stor-
ing only the top 100 items (neighbors) in decreas-
ing order of Sqi(?) for each item qi and setting
the scores of lower ranked items as 0. (Note that
Sqi(d) >= 0 holds.) Consequently, we only have
to search small part of the search space without
affecting the performance very much. These two
types of optimization are common in item-based
CF (Sarwar et al, 2001; Karypis, 2001).
2.3 Polya model
Our second model is based on the Polya distribu-
tion. We first introduce (hyper) parameter ? =
{??, ??} and denote the probability of x given
? as p(x|?) = p?(wx|??)p?(ux|??). ?? and
?? are the parameter vectors for words and users.
p?(wx|??) is defined as follows.
p?(wx|??) = ?(
?
w ??w)
?(?w nxw + ??w)
?
w
?(nxw + ??w)
?(??w)
(9)
4S(?qi ||?d) =
?
u Pl(u|uqi) ?
log
(
??Pl(u|ud)
(1???)Pg(u) + 1
)
, where Pl(u|uqi) =
n(u,?qi )?
u? n(u
?,?qi )
, Pl(u|ud) = n(u,ud)?
u? n(u
?,ud)
, and
Pg(u) =
?
d n(u,ud)?
d
?
u? n(u
?,ud)
.
451
0
1
2
3
4
5
6
7
8
9
10
0 2 4 6 8 10
0
1
2
3
4
5
6
7
8
9
10
nu
(n,a
lph
a)
n
alpha=1e+5
alpha=38.8
alpha=16.4
alpha=9.0
alpha=5.4
alpha=3.3
alpha=2.0
alpha=1.1
alpha=0.4
alpha=1e-5
Figure 1: Relationship between original count n
and dumped count ?(n, ?)
where ? is known as the gamma function, ??w is a
parameter for word w and nxw = n(w,wx). This
can be approximated as follows (Minka, 2003).
p?(wx|??) ?
?
w
?(w)n?(w,wx) (10)
where
n?(w,wx) = ??w(?(nxw + ??w)??(??w))
? ?(nxw, ??w) (11)
? is known as the digamma function and is sim-
ilar to the natural logarithm. We call Eq. 10 the
approximated Polya model or simply the Polya
model in this paper.
Eq. 10 indicates that the Polya distribution
can be interpreted as a multinomial distribution
over a modified set of counts n?(?) (Minka, 2003).
These modified counts are dumped as shown in
Fig. 1. When ??w ? ?, ?(nxw, ??w) approaches
nxw. When ??w ? 0, ?(nxw, ??w) = 0 if nxw = 0
otherwise it is 1. For intermediate values of ??w,
the mapping ? dumps the original counts.
Under the approximation of Eq. 10, the es-
timation of parameters can be understood as the
maximum-likelihood estimate of a multinomial
distribution from dumped counts n?(?) (Minka,
2003). Indeed, all we have to do to estimate the
parameters for ranking items is replace Pl and Pg
from Section 2.2 with Pl(w|wd) = n?(w,wd)?
w? n?(w?,wd)
,
Pg(w) =
?
d n?(w,wd)?
d
?
w? n?(w?,wd)
, and Pl(w|wqi) =
n?(w,wqi )?
w? n?(w?,wqi )
. Then, as in the multinomial model,
we can define S(?qi ||?d) with these probabilities.
This argument also applies to S(?qi ||?d).
The approximated Polya model is a generaliza-
tion of the multinomial model described in Sec-
tion 2.2. If we set ??w and ??u very large then the
Polya model is identical to the multinomial model.
By comparing Eqs. 1 and 10, we can see why the
Polya model is superior to the multinomial model
for modeling the occurrences of words (and users).
In the multinomial model, if a word with probabil-
ity p occurs twice, its probability becomes p2. In
the Polya model, the word?s probability becomes
p1.5, for example, if we set ??w = 1. Clearly,
p2 < p1.5; therefore, the Polya model assigns
higher probability. In this example, the Polya
model assigns probability p to the first occurrence
and p0.5(> p) to the second. Since words that oc-
cur once are likely to occur again (Church, 2000),
the Polya model is better suited to model the oc-
currences of words and users. See Yamamoto and
Sadamitsu (2005) for further discussion on apply-
ing the Polya distribution to text modeling.
Zaragoza et al(2003) applied the Polya distri-
bution to ad hoc IR. They introduced the exact
Polya distribution (see Eq. 9) as an extension
to the Dirichlet prior method (Zhai and Lafferty,
2001). However, we have introduced a multino-
mial approximation of the Polya distribution. This
approximation allows us to use the linear interpo-
lation method to mix the approximated Polya dis-
tributions. Thus, our model is similar to two-stage
language models (Zhai and Lafferty, 2002) that
combine the Dirichlet prior method and the lin-
ear interpolation method. In contrast to our model,
Zaragoza et al(2003) had difficulty in mixing the
Polya distributions and did not treat that in their
paper.
3 Experiments
We first examined the behavior of the Polya model
by varying the parameters. We tied ??w for every
w and ??u for every u; for any w and u, ??w = ??
and ??u = ??. We then compared the Polya model
to an item-based CF method.
3.1 Behavior of Polya model
3.1.1 Dataset
We made a dataset of articles from English
Wikipedia5 to evaluate the Polya model. English
Wikipedia is an online encyclopedia that anyone
5We downloaded 20050713 pages full.xml.gz
and 20050713 pages current.xml.gz from
http://download.wikimedia.org/wikipedia/en/.
452
can edit, and it has many registered users. Our
aim is to recommend a set of articles to each user
that is likely to be of interest to that user. If we
can successfully recommend interesting articles,
this could be very useful to a wide audience be-
cause Wikipedia is very popular. In addition, be-
cause wikis are popular media for sharing knowl-
edge, developing effective recommender systems
for wikis is important.
In our Wikipedia dataset, each item (article) x
consisted of wx and ux. ux was the sequence of
users who had edited x. If users had edited x mul-
tiple times, then those users occurred in ux multi-
ple times. wx was the sequence of words that were
typical in x. To make wx, we removed stop words
and stemmed the remaining words with a Porter
stemmer. Next, we identified 100 typical words
in each article and extracted only those words
(|wx| ? 100 because some of them occurred
multiple times). Typicality was measured using
the log-likelihood ratio test (Dunning, 1993). We
needed to reduce the number of words to speed up
our recommender system.
To make our dataset, we first extracted 302,606
articles, which had more than 100 tokens after the
stop words were removed. We then selected typi-
cal words in each article. The implicit rating data
were obtained from the histories of users editing
these articles. Each rating consisted of {user, ar-
ticle, number of edits}. The size of this original
rating data was 3,325,746. From this data, we ex-
tracted a dense subset that consisted of users and
articles included in at least 25 units of the original
data. We discarded the users who had edited more
than 999 articles because they were often software
robots or system operators, not casual users. The
resulting 430,096 ratings consisted of 4,193 users
and 9,726 articles. Each user rated (edited) 103
articles on average (the median was 57). The av-
erage number of ratings per item was 44 and the
median was 36.
3.1.2 Evaluation of Polya model
We conducted a four-fold cross validation of
this rating dataset to evaluate the Polya model. We
used three-fourth of the dataset to train the model
and one-fourth to test it.6 All users who existed in
6We needed to estimate probabilities of users and words.
We used only training data to estimate the probabilities of
users. However, we used all 9,726 articles to estimate the
probabilities of words because the articles are usually avail-
able even when editing histories of users are not.
both training and test data were used for evalua-
tion. For each user, we regarded the articles in the
training data that had been edited by the user as a
query and ranked articles in response to it. These
ranked top-N articles were then compared to the
articles in the test data that were edited by the
same user to measure the precisions for the user.
We used P@N (precision at rank N = the ratio of
the articles edited by the user in the top-N articles),
S@N (success at rank N = 1 if some top-N articles
were edited by the user, else 0), and R-precision (=
P@N, where N is the number of articles edited by
the user in the test data). These measures for each
user were averaged over all users to get the mean
precision of each measure. Then, these mean pre-
cisions were averaged over the cross validation re-
peats.
Here, we report the averaged mean pre-
cisions with standard deviations. We first
report how R-precision varied depend-
ing on ? (?? or ??). ? was varied over
10?5, 0.4, 1.1, 2, 3.3, 5.4, 9, 16.4, 38.8, and 105.
The values of ?(10, ?) were approximately 1, 2,
3, 4, 5, 6, 7, 8, 9, and 10, respectively, as shown
in Fig. 1. When ? = 105, the Polya model
represents the multinomial model as discussed in
Section 2.3. For each value of ?, we varied ? (??
or ??) over 0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6,
0.7, 0.8, 0.9, 0.95, and 0.99 to obtain the optimum
R-precision. These optimum R-precisions are
shown in Fig. 2. In this figure, CBF and CF
represent the R-precisions for the content-based
and collaborative filtering part of the Polya model.
The values of CBF and CF were obtained by
setting ?s = 0 and ?s = 1 in Eq. 7 (which
is applied to the Polya model instead of the
multinomial model), respectively. The error bars
represent standard deviations.
At once, we noticed that CBF outperformed
CF. This is reasonable because the contents of
Wikipedia articles should strongly reflect the users
(authors) interest. In addition, each article had
about 100 typical words, and this was richer than
the average number of users per article (44). This
observation contrasts with other work where CBF
performed poorly compared with CF, e.g., (Ali and
van Stam, 2004).
Another important observation is that both
curves in Fig. 2 are concave. The best R-
precisions were obtained at intermediate values of
? for both CF and CBF as shown in Table 1.
453
0.065
0.07
0.075
0.08
0.085
0.09
0.095
1 2 3 4 5 6 7 8 9 10
R-
pre
cis
ion
nu(10,alpha)
CBF
CF
Figure 2: R-precision for Polya model
Table 1: Improvement in R-precision (RP)
best RP (?(?)/?) RP (?(?)/?) %change
CBF 0.091 (7/9.0) 0.088 (10/105) +3.4%
CF 0.081 (2/0.4) 0.069 (10/105) +17.4%
When ? = 105 or ?(10, ?) ? 10, the Polya
model represents the multinomial model as dis-
cussed in Section 2.3. Thus, Fig. 2 and Table 1
show that the best R-precisions achieved by the
Polya model were better than those obtained by
the multinomial model. The improvement was
3.4% for CBF and 17.4% for CF as shown in Ta-
ble 1. The improvement of CF was larger than
that of CBF. This implies that the occurrences of
users are more clustered than those of words. In
other words, the degree of repetition in the editing
histories of users is greater than that in word se-
quences. A user who edits an article are likely to
edit the article again.
From Fig. 2 and Table 1, we concluded that the
generalization of a multinomial model achieved by
the Polya model is effective in improving recom-
mendation performance.
3.1.3 Combination of CBF and CF
Next, we show how the combination of CBF
and CF improves recommendation performance.
We set ? (?? and ??) to the optimum values in
Table 1 and varied ? (?s, ?? and ??) to obtain the
R-precisions for CBF+CF, CBF and CF in Fig. 3.
The values of CBF were obtained as follows. We
first set ?s = 0 in Eq. 7 to use only CBF scores
and then varied ??, which is the smoothing pa-
rameter for word probabilities, in Eq. 2. To get
the values of CF, we set ?s = 1 in Eq. 7 and then
varied ??, which is the smoothing parameter for
user probabilities. The values of CBF+CF were
obtained by varying ?s in Eq. 7 while setting ??
and ?? to the optimum values obtained from CBF
0.055
0.06
0.065
0.07
0.075
0.08
0.085
0.09
0.095
0.1
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
R-
pre
cis
ion
lambda
CBF+CF
CBF
CF
Figure 3: Combination of CBF and CF.
Table 2: Precision and Success at top-N
CBF+CF CBF CF
N P@N S@N P@N S@N P@N S@N
5 0.166 0.470 0.149 0.444 0.137 0.408
10 0.135 0.585 0.123 0.562 0.112 0.516
15 0.117 0.650 0.107 0.628 0.098 0.582
20 0.105 0.694 0.096 0.671 0.089 0.627
R-precision 0.099 0.091 0.081
optimum ? ?s = 0.2 ?? = 0.01 ?? = 0.2
and CF (see Table 2). These parameters (?s, ??
and ??) were defined in the context of the multi-
nomial model in Section 2.2 and used similarly in
the Polya model in this experiment.
We can see that the combination was quite ef-
fective as CBF+CF outperformed both CBF and
CF. Table 2 shows R-precision, P@N and S@N
for N = 5, 10, 15, 20. These values were obtained
by using the optimum values of ? in Fig. 3.
Table 2 shows the same tendency as Fig. 3. For
all values of N , CBF+CF outperformed both CBF
and CF. We attribute this effectiveness of the com-
bination to the feature independence of CBF and
CF. CBF used words as features and CF used user
ratings as features. They are very different kinds
of features and thus can provide complementary
information. Consequently, CBF+CF can exploit
the benefits of both methods. We need to do fur-
ther work to confirm this conjecture.
3.2 Comparison with a baseline method
We compared the Polya model to an implementa-
tion of a state-of-the-art item-based CF method,
CProb (Karypis, 2001). CProb has been tested
with various datasets and found to be effective in
top-N recommendation problems. CProb has also
been used in recent work as a baseline method
(Ziegler et al, 2005; Wang et al, 2005).
In addition to the Wikipedia dataset, we used
two other datasets for comparison. The first was
454
R-precision P@10
WP ML BX WP ML BX
Polya-CF 0.081 0.272 0.066 0.112 0.384 0.054
CProb 0.082 0.258 0.071 0.113 0.373 0.057
%change -1.2% +5.4% -7.0% -0.9% +2.9% -5.3%
Table 3: Comparison of Polya-CF and CProb
the 1 million MovieLens dataset.7 This data con-
sists of 1,000,209 ratings of 3,706 movies by 6,040
users. Each user rated an average of 166 movies
(the median was 96). The average number of rat-
ings per movie was 270 and the median was 124.
The second was the BookCrossing dataset (Ziegler
et al, 2005). This data consists of 1,149,780 rat-
ings of 340,532 books by 105,283 users. From
this data, we removed books rated by less than 20
users. We also removed users who rated less than
5 books. The resulting 296,471 ratings consisted
of 10,345 users and 5,943 books. Each user rated
29 books on average (the median was 10). The av-
erage number of ratings per book was 50 and the
median was 33. Note that in our experiments, we
regarded the ratings of these two datasets as im-
plicit ratings. We regarded the number of occur-
rence of each rating as one.
We conducted a four-fold cross validation for
each dataset to compare CProb and Polya-CF,
which is the collaborative filtering part of the
Polya model as described in the previous section.
For each cross validation repeat, we tuned the pa-
rameters of CProb and Polya-CF on the test data to
get the optimum R-precisions, in order to compare
best results for these models.8 P@N and S@N
were calculated with the same parameters. These
measures were averaged as described above. R-
precision and P@10 are in Table 3. The max-
imum standard deviation of these measures was
0.001. We omitted reporting other measures be-
cause they had similar tendencies. In Table 3, WP,
ML and BX represent the Wikipedia, MovieLens,
and BookCrossing datasets.
In Table 3, we can see that the variation of per-
formance among datasets was greater than that be-
tween Polya-CF and CProb. Both methods per-
7http://www.grouplens.org/
8CProb has two free parameters. Polya-CF also has two
free parameters (?? and ??). However, for MovieLens and
BookCrossing datasets, Polya-CF has only one free parame-
ter ??, because we regarded the number of occurrence of each
rating as one, which means ?(1, ??) = 1 for all ?? > 0 (See
Fig. 1). Consequently, we don?t have to tune ??. Since the
number of free parameters is small, the comparison of perfor-
mance shown in Table 3 is likely to be reproduced when we
tune the parameters on separate development data instead of
test data.
formed best against ML. We think that this is be-
cause ML had the densest ratings. The average
number of ratings per item was 270 for ML while
that for WP was 44 and that for BX was 50.
Table 3 also shows that Polya-CF outperformed
CProb when the dataset was ML and CProb was
better than Polya-CF in the other cases. However,
the differences in precision were small. Overall,
we can say that the performance of Polya-CF is
comparable to that of CProb.
An important advantage of the Polya model
over CProb is that the Polya model can unify CBF
and CF in a single language modeling framework
while CProb handles only CF. Another advantage
of the Polya model is that we can expect to im-
prove its performance by incorporating techniques
developed in IR because the Polya model is based
on language modeling approaches in IR.
4 Future work
We want to investigate two areas in our future
work. One is the parameter estimation and the
other is the refinement of the query model.
We tuned the parameters of the Polya model by
exhaustively searching the parameter space guided
by R-precision. We actually tried to learn ??
and ?? from the training data by using an EM
method (Minka, 2003; Yamamoto and Sadamitsu,
2005). However, the estimated parameters were
about 0.05, too small for better recommendations.
We need further study to understand the relation
between the probabilistic quality (perplexity) of
the Polya model and its recommendation quality.
We approximate the query model as Eq. 3. This
allows us to optimize score calculation consider-
ably. However, this does not consider the interac-
tion among items, which may deteriorate the qual-
ity of probability estimation. We want to inves-
tigate more efficient query models in our future
work.
5 Conclusion
Recommender systems help users select particular
items from a large number of choices by provid-
ing recommendations. Much work has been done
to combine content-based filtering (CBF) and col-
laborative filtering (CF) to provide better recom-
mendations. The contributions reported in this pa-
per are twofold: (1) we extended relevance feed-
back approaches to incorporate CF and (2) we in-
troduced the approximated Polya model as a gen-
455
eralization of the multinomial model and showed
that it is better suited to CF and CBF. The perfor-
mance of the Polya model is comparable to that of
a state-of-the-art item-based CF method.
Our work shows that language modeling ap-
proaches in information retrieval can be extended
to CF. This implies that a large amount of work
in the field of IR could be imported into CF. This
would be interesting to investigate in future work.
References
Kamal Ali and Wijnand van Stam. 2004. TiVo: Mak-
ing show recommendations using a distributed col-
laborative filtering architecture. In KDD?04.
James Allan. 1996. Incremental relevance feedback
for information filtering. In SIGIR?96.
Justin Basilico and Thomas Hofmann. 2004. Uni-
fying collaborative and content-based filtering. In
ICML?04.
Chumki Basu, Haym Hirsh, and William Cohen. 1998.
Recommendation as classification: Using social and
content-based information in recommendation. In
AAAI-98.
John S. Breese, David Heckerman, and Carl Kadie.
1998. Empirical analysis of predictive algorithms
for collaborative filtering. Technical report, MSR-
TR-98-12.
Kenneth W. Church. 2000. Empirical estimates of
adaptation: The chance of two Noriegas is closer to
p/2 than p2. In COLING-2000, pages 180?186.
W. Bruce Croft and John Lafferty, editors. 2003. Lan-
guage Modeling for Information Retrieval. Kluwer
Academic Publishers.
Ted Dunning. 1993. Accurate methods for the statis-
tics of surprise and coincidence. Computational
Linguistics, 19(1):61?74.
Jonathan L. Herlocker, Joseph A. Konstan,
Al Borchers, and John Riedl. 1999. An algo-
rithmic framework for performing collaborative
filtering. In SIGIR?99, pages 230?237.
George Karypis. 2001. Evaluation of item-based top-
N recommendation algorithms. In CIKM?01.
John Lafferty and ChengXiang Zhai. 2001. Document
language models, query models and risk minimiza-
tion for information retrieval. In SIGIR?01.
Victor Lavrenko and W. Bruce Croft. 2001.
Relevance-based language models. In SIGIR?01.
Victor Lavrenko, Martin Choquette, and W. Bruce
Croft. 2002. Cross-lingual relevance models. In
SIGIR?02, pages 175?182.
Victor Lavrenko. 2004. A Generative Theory of Rele-
vance. Ph.D. thesis, University of Massachusetts.
J. Leon, V. Lavrenko, and R. Manmatha. 2003. Au-
tomatic image annotation and retrieval using cross-
media relevance models. In SIGIR?03.
Thomas P. Minka. 2003. Es-
timating a Dirichlet distribution.
http://research.microsoft.com/?minka/papers/dirichlet/.
Paul Resnick and Hal R. Varian. 1997. Recommender
systems. Communications of the ACM, 40(3):56?
58.
Paul Resnick, Neophytos Iacovou, Mitesh Suchak, Pe-
ter Bergstrom, and John Riedl. 1994. GroupLens:
An open architecture for collaborative filtering of
netnews. In CSCW?94, pages 175?186.
Badrul Sarwar, George Karypis, Joseph Konstan, and
John Riedl. 2001. Item-based collaborative filtering
recommendation algorithms. In WWW10.
Robert E. Schapire, Yoram Singer, and Amit Singhal.
1998. Boosting and Rocchio applied to text filtering.
In SIGIR?98, pages 215?223.
Luo Si and Rong Jin. 2004. Unified filtering by com-
bining collaborative filtering and content-based fil-
tering via mixture model and exponential model. In
CIKM-04, pages 156?157.
Jun Wang, Marcel J.T. Reinders, Reginald L. La-
gendijk, and Johan Pouwelse. 2005. Self-
organizing distributed collaborative filtering. In SI-
GIR?05, pages 659?660.
Mikio Yamamoto and Kugatsu Sadamitsu. 2005.
Dirichlet mixtures in text modeling. Technical re-
port, University of Tsukuba, CS-TR-05-1.
Kai Yu, Anton Schwaighofer, Volker Tresp, Wei-Ying
Ma, and HongJiang Zhang. 2003. Collaborative
ensemble learning: Combining collaborative and
content-based information filtering via hierarchical
Bayes. In UAI-2003.
Hugo Zaragoza, Djoerd Hiemstra, and Michael Tip-
ping. 2003. Bayesian extension to the language
model for ad hoc information retrieval. In SIGIR?03.
ChengXiang Zhai and John Lafferty. 2001. A study of
smoothing methods for language models applied to
ad hoc information retrieval. In SIGIR?01.
ChengXiang Zhai and John Lafferty. 2002. Two-stage
language models for information retrieval. In SI-
GIR?02, pages 49?56.
Cai-Nicolas Ziegler, Sean M. McNee, Joseph A. Kon-
stan, and Georg Lausen. 2005. Improving rec-
ommendation lists through topic diversification. In
WWW?05, pages 22?32.
456
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 222?232,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
An Efficient Language Model Using Double-Array Structures
Makoto Yasuhara Toru Tanaka ?Jun-ya Norimatsu Mikio Yamamoto
Department of Computer Science
University of Tsukuba, Japan
?norimatsu@mibel.cs.tsukuba.ac.jp
Abstract
Ngram language models tend to increase in
size with inflating the corpus size, and con-
sume considerable resources. In this pa-
per, we propose an efficient method for im-
plementing ngram models based on double-
array structures. First, we propose a method
for representing backwards suffix trees using
double-array structures and demonstrate its ef-
ficiency. Next, we propose two optimization
methods for improving the efficiency of data
representation in the double-array structures.
Embedding probabilities into unused spaces
in double-array structures reduces the model
size. Moreover, tuning the word IDs in the
language model makes the model smaller and
faster. We also show that our method can be
used for building large language models using
the division method. Lastly, we show that our
method outperforms methods based on recent
related works from the viewpoints of model
size and query speed when both optimization
methods are used.
1 Introduction
Ngram language models (F. Jelinek, 1990) are
widely used as probabilistic models of sentence in
natural language processing. The wide use of the
Internet has entailed a dramatic increase in size of
the available corpora, which can be harnessed to ob-
tain a significant improvement in model quality. In
particular, Brants et al (2007) have shown that the
performance of statistical machine translation sys-
tems is monotonically improved with the increas-
ing size of training corpora for the language model.
However, models using larger corpora also consume
more resources. In recent years, many methods for
improving the efficiency of language models have
been proposed to tackle this problem (Pauls and
Klein, 2011; Kenneth Heafield, 2011). Such meth-
ods not only reduce the required memory size but
also raise query speed.
In this paper, we propose the double-array lan-
guage model (DALM) which uses double-array
structures (Aoe, 1989). Double-array structures
are widely used in text processing, especially for
Japanese. They are known to provide a compact
representation of tries (Fredkin, 1960) and fast tran-
sitions between trie nodes. The ability to store and
manipulate tries efficiently is expected to increase
the performance of language models (i.e., improving
query speed and reducing the model size in terms of
memory) because tries are one of the most common
representations of data structures in language mod-
els. We use double-array structures to implement
a language model since we can utilize their speed
and compactness when querying the model about an
ngram.
In order to utilize of double-array structures as
language models, we modify them to be able to
store probabilities and backoff weights. We also
propose two optimization methods: embedding
and ordering. These methods reduce model size
and increase query speed. Embedding is an ef-
ficient method for storing ngram probabilities and
backoff weights, whereby we find vacant spaces in
the double-array language model structure and pop-
ulate them with language model information, such
as probabilities and backoff weights. Ordering is
222
a method for compacting the double-array structure.
DALM uses word IDs for all words of the ngram,
and ordering assigns a word ID to each word
to reduce the model size. These two optimization
methods can be used simultaneously and are also ex-
pected to work well.
In our experiments, we use a language model
based on corpora of the NTCIR patent retrieval
task (Atsushi Fujii et al, 2007; Atsushi Fujii et al,
2005; Atsushi Fujii et al, 2004; Makoto Iwayama et
al., 2003). The model size is 31 GB in the ARPA
file format. We conducted experiments focusing on
query speed and model size. The results indicate
that when the abovementioned optimization meth-
ods are used together, DALM outperforms state-of-
the-art methods on those points.
2 Related Work
2.1 Tries and Backwards Suffix Trees
Tries (Fredkin, 1960) are one of the most widely
used tree structures in ngram language models since
they can reduce memory requirements by sharing
common prefix. Moreover, since the query speed
for tries depends only on the number of input words,
the query speed remains constant even if the ngram
model increases in size.
Backwards suffix trees (Bell et al, 1990; Stolcke,
2002; Germann et al, 2009) are among the most
efficient representations of tries for language mod-
els. They contain ngrams in reverse order of history
words.
Figure 1 shows an example of a backwards suf-
fix tree representation. In this paper, we denote an
ngram: by the form w1, w2, ? ? ? , wn as wn1 . In this
example, word lists (represented as rectangular ta-
bles) contain target words (here, wn) of ngrams, and
circled words in the tree denote history words (here,
wn?11 ) associated with target words. The history
words ?I eat,? ?you eat?, and ?do you eat? are stored
in reverse order. Querying this trie about an ngram is
simple: just trace history words in reverse and then
find the target word in a list. For example, consider
querying about the trigram ?I eat fish?. First, simply
trace the history in the trie in reverse order (?eat??
?I?); then, find ?fish? in list <1>. Similarly, query-
ing a backwards suffix tree about unknown ngrams
is also efficient, because the backwards suffix tree
Figure 1: Example of a backwards suffix tree. There
are two branch types in a backwards suffix tree: history
words and target words. History words are shown in cir-
cles and target words are stored in word lists.
representation is highly suitable for the backoff cal-
culation. For example, in querying about the 4gram
?do you eat soup?, we first trace ?eat?? ?you??
?do? in a manner similar to above. However, search-
ing for the word ?soup? in list <3> fails because
list <3> does not contain the word ?soup?. In this
case, we return to the node ?you? to search the list
<2>, where we find ?soup?. This means that the tri-
gram ?you eat soup? is contained in the tree while
the 4gram ?do you eat soup? is not. This behavior
can be efficiently used for backoff calculation.
SRILM (Stolcke, 2002) is a widely used language
model toolkit. It utilizes backwards suffix trees for
its data structures. In SRILM, tries are implemented
as 64-bit pointer links, which wastes a lot of mem-
ory. On the other hand, the access speed for ngram
probabilities is relatively high.
2.2 Efficient Language Models
In recent years, several methods have been proposed
for storing language models efficiently in memory.
Talbot and Osborne (2007) have proposed an effi-
cient method based on bloom filters. This method
modifies bloom filters to store count information
about training sets. In prior work, bloom filters
have been used for checking whether certain data
are contained in a set. To store the count informa-
tion, pairs from <ngram,1> to <ngram,count> are
all added to the set for each ngram. To query this
language model about the probability of an ngram,
probabilities are calculated during querying by us-
ing these counts. Talbot and Brants (2008) have pro-
posed a method based on perfect hash functions and
bloomier filters. This method uses perfect hash func-
tions to store ngrams and encode values (for exam-
223
ple, probabilities or counts of ngrams in the training
corpus) to a large array. Guthrie and Hepple (2010)
have proposed a language model called ShefLM that
uses minimal perfect hash functions (Belazzougui et
al., 2009), which can store ngrams without vacant
spaces. Furthermore, values are compressed by sim-
ple dense coding (Fredriksson and Nikitin, 2007).
ShefLM achieves a high compression ratio when
it stores counts of ngrams in the training corpus.
However, when this method stores probabilities of
ngrams, the advantage of using compression is lim-
ited because floating-point numbers are difficult to
compress. Generally, compression is performed by
combining the same values but, two floating-point
numbers are rarely the same, especially in the case
of probability values1. These methods implement
lossy language models, meaning that, we can re-
duce the model size at the expense of model qual-
ity. These methods also reduce the model perfor-
mance (perplexity).
Pauls and Klein (2011) have proposed Berke-
leyLM which is based on an implicit encoding struc-
ture, where ngrams are separated according to their
order, and are sorted by word ID. The sorted ngrams
are linked to each other like a trie structure. Berke-
leyLM provides rather efficient methods. Variable-
length coding and block compression are used if
small model size is more important than query
speed. In addition, Heafield (2011) has proposed
an efficient language model toolkit called KenLM
that has been recently used in machine translation
systems, for which large language models are of-
ten needed. KenLM has two different main structure
types: trie and probing. The trie structure is
compact but relatively slower to query, whereas the
probing structure is relatively larger but faster to
query.
In this paper, we propose a language model struc-
ture based on double-array structures. As we de-
scribe in Section 3, double-array structures can be
used as fast and compact representations of tries.
We propose several techniques for maximizing the
performance of double-array structures from the per-
spective of query speed and model size.
1In our experience, it is considerably easier to compress
backoff weights than to compress probabilities, although both
are represented with floating-point numbers. We use this knowl-
edge in our methods.
3 Double-Array
3.1 Double-Array Structure
In DALM, we use a double-array structure (Aoe,
1989) to represent the trie of a language model.
Double-array structures are trie representations con-
sisting of two parallel arrays: BASE and CHECK .
They are not only fast to query, but also provide a
compact way to store tries. In the structure, nodes in
the trie are represented by slots with the same index
in both arrays. Before proposing several efficient
language model representation techniques in Section
4, we introduce double-array themselves. In addi-
tion, the construction algorithms for double-arrays
are described in Section 3.2 and Section 3.3.
The most naive implementation of a trie will have
a two-dimensional array NEXT . Let WORDID (w)
be a function that returns a word ID as a
number corresponding to its argument word w;
then NEXT [n][WORDID (w)] (that presents the
WORDID(w)-th slot of the nth row in the NEXT
array) stores the node number which can be transit
from the node number n by the word w, and we can
traverse the trie efficiently and easily to serialize the
array in memory. This idea is simple but wastes the
most of the used memory because almost all of the
slots are unused and this results in occupying mem-
ory space. The double-array structures solve this
problem by taking advantage of the sparseness of the
NEXT array. The two-dimensional array NEXT
is merged into a one-dimensional array BASE by
shifting the entries of each row of the NEXT array
and combining the set of resulting arrays. We can
store this result in much less memory than the se-
rialization of the naive implementation above. Ad-
ditionally, a CHECK array is introduced to check
whether the transition is valid or not because we can-
not distinguish which node the information in a par-
ticular slot comes from. Using a CHECK array, we
can avoid transition errors and move safely to the
child node of any chosen node.
As a definition, a node link from a node ns with
a word w to the next node nnext in the trie is defined
as follows:
next ? BASE [s] + WORDID (w)
if CHECK [next ] == s
where s denotes the index of the slot in the double-
224
Figure 2: A trie and a corresponding double-array struc-
ture. Node ns is represented by the slots BASE [s] and
CHECK [s]. A link from a node ns with a word w is
indicated by CHECK [next] == s.
array structure which represents ns. The trie tran-
sition from a node ns with a word w is applied ac-
cording to the following steps:
Step 1 Calculating the ?next? destination and
Step 2 Checking whether the transition is correct.
Step 2 specifically means the following:
1. If CHECK [next ] == s, then we can ?move?
to the node nnext ;
2. otherwise, we can detect that the transition
from the node ns with the word w is not con-
tained in the trie.
Figure 2 shows an example of a transition from a
parent node ns with a word w.
Next, we describe how the existence of an ngram
history can be determined (Aoe, 1989). We can it-
erate over the nodes by the transitions shown above
and may find the node representing an ngram his-
tory. But we should check that it is valid because
nodes except for leaf nodes possiblly represent a
fragment of some total ngram history. We can use
endmarker symbols to determine whether an ngram
history is in the trie. We add nodes meaning the end-
marker symbol after the last node of each ngram his-
tory. When querying aboutwn?11 , we transit repeat-
edly; in other words, we set s = 0 and start by ap-
plying Step 1 and 2 repeatedly for each word. When
we reach the node wn?1, we continue searching for
an endmarker symbol. If the symbol is found, we
know that the ngram history wn?11 is in the trie.
The double-array structure consumes 8 bytes per
node because the BASE and CHECK arrays are
4 byte array variables. Therefore, the structure can
Figure 3: Greedy insertion of trie elements. The children
of a node are collectively inserted into the double-array
when the BASE value of the node is fixed.
store nodes compactly in case of a high filling rate.
Moreover, node transitions are very fast because
they require only one addition and one comparison
per transition. We use a double-array structure in
DALM, which can maximize its potential.
3.2 Greedy Construction
Greedy algorithms are widely used for construct-
ing static double-array structures2. The construction
steps are as follows:
1. Define the root node of a trie to correspond to
index 0 of the double-array structure and
2. Find the BASE value greedily (i.e., in order
1, 2, 3, ? ? ?) for all nodes which have fixed their
indices in the double-array structure.
In practice, once the BASE value of a node is fixed,
the positions of its children are fixed at the same
time, and we can find the BASE values for each
child recursively.
Figure 3 shows an example of such construc-
tion. In this example, three nodes (?I?, ?you? and
?they?) are inserted at the same time. This is be-
cause the above three node positions are fixed by
the BASE value of the node ?eat?. To insert nodes
2We were unable to find an original source for this tech-
nique. However, this method is commonly used in double-array
implementations.
225
?I?, ?you? and ?they?, the following three slots must
be empty (i.e., the slots must not be used by other
nodes.):
? BASE [s] + WORDID(?I?)
? BASE [s] + WORDID(?you?)
? BASE [s] + WORDID(?they?)
where s is the index of the node ?eat?. At the con-
struction step, we need to find BASE [s] which sat-
isfies the above conditions.
3.3 Efficient Construction Algorithm
The construction time for a double-array structure
poses the greatest challenge. We use a more effi-
cient method (Nakamura and Mochizuki, 2006) in-
stead of the naive method for constructing a double-
array structure because the naive method requires a
long time. We call the method ?empty doubly-linked
list?. This algorithm is one of the most efficient con-
struction methods devised to date. Figure 4 shows
an example of an empty doubly-linked list. We can
efficiently define the BASE value of each node by
using theCHECK array to store the next empty slot.
In this example, in searching the BASE value of a
node, the first child node can be set to position 1,
and if that fails, we can successively try positions
3, 4, 6, 8, ? ? ? by tracing the list instead of searching
all BASE values 0, 1, 2, 3, 4, 5, ? ? ?.
As analyzed by Nakamura and Mochizuki(2006),
the computational cost of a node insertion is less
than in the naive method. The original naive method
requires O(NM) time for a node insertion, where
M is a number of unique word types and N is a
number of nodes of the trie; the algorithm using an
empty double-linked list requires O(UM), where U
is the number of unused slots.
As described in Section 5, we divide the trie into
several smaller tries and apply the efficient method
for constructing our largest models. This is because
it is not feasible to wait several weeks for the large
language model structure to be built. The dividing
method is currently the only method allowing us to
build them.
Figure 4: Empty doubly-linked list. Unused CHECK
slots are used to indicate the next unused slots, and un-
used BASE slots are used to indicate previous unused
slots. Thus, the BASE and CHECK arrays are used as a
doubly-linked list which can reduce the number of inef-
fective trials.
4 Proposed Methods
4.1 DALM
In this section, we present the application of the
double-array structure to backwards suffix trees. As
this is the most basic structure based on double-array
structures, we refer to it as the simple structure
and improve its performance as described in the fol-
lowing sections.
To represent a backwards suffix tree as a double-
array structure, we should modify the tree because it
has two types of branches (target words and history
nodes), which must be distinguished in the double-
array structure. Instead, we should distinguish the
branch type which indicates whether the node is a
target word or a history word. We use the endmarker
symbol (<#>) for branch discrimination. In prior
work, the endmarker symbol has been used to indi-
cate whether an ngram is in the trie. However, there
is no need to distinguish whether the node of the tree
is included in the language model because all nodes
of a backwards suffix tree which represents ngrams
surely exist in the model. We use the endmarker
symbol to indicate nodes which are end-of-history
words. Therefore, target words of ngrams are chil-
dren of the endmarker symbols that they follow.
By using the endmarker symbol, target words can
be treated the same as ordinary nodes because all tar-
get words are positioned after <#>. Figure 5 shows
an example of such construction. We can clearly dis-
tinguish target words and history words in the back-
wards suffix tree.
Querying in the tree is rather simple. For exam-
ple, consider the case of a query trigram ?I eat fish?
in the trie of Figure 5. We can trace this trigram in
226
Figure 5: An example of converting a backwards suffix
tree. We introduce endmarker symbols to distinguish the
two branch types. We can treat the tree as an ordinary trie
that can be represented by a double-array structure while
retaining the advantages of the tree structure.
the same way as the original backwards suffix tree.
First, we trace ?eat?? ?I?, then trace that to the end-
marker symbol <#> and finally find the word ?fish?.
Next, we describe the procedure for storing prob-
abilities and backoff weights. We prepare aVALUE
array to store the probabilities and backoff weights
of ngrams. Figure 6 shows the simple DALM
structure. The backwards suffix tree stores a back-
off weight for each node and a probability for each
target word. In simple DALM, each value is
stored for the respective position of the correspond-
ing node.
4.2 Embedding
Embedding is a method for reducing model size.
In the simple DALM structure, there are many va-
cant spaces in the BASE and CHECK arrays. We
use these vacant spaces to store backoff weights and
probabilities. Figure 7 shows vacant spaces in the
simple DALM structure.
First, the BASE array slots of target word nodes
are unused because target words are always in leaf
positions in the backwards suffix tree and do not
have any children nodes. In the example of Figure 7,
BASE [9] is not used, and therefore can be used for
storing a probability value. This method can reduce
the model size because all probabilities are stored
into the BASE array. As a result, the VALUE array
Figure 6: The simple DALM data structure. The
BASE and CHECK arrays are used in the same way
as in a double-array structure. To return probabilities and
backoff weights, a VALUE array is introduced.
Figure 7: Unused slots in the simple DALM structure
used for other types of information, such as probabilities.
contains only backoff weights.
Next, the CHECK array slots of endmarker sym-
bols are also vacant. We do not need to check for
endmarker symbol transition because the endmarker
symbol <#> is seen for all nodes except target word
nodes. This means that all endmarker symbol tran-
sitions are ensured to be correct and the CHECK
array slots of endmarker symbols do not need to be
used. We use this space to store backoff weights.
In order to avoid false positives, we cannot store
backoff weights directly. Instead, we store the po-
sitions of the backoff weights in the VALUE array
as negative numbers. When a query for an unknown
ngram encounters an endmarker symbol node, the
value of the CHECK array is never matched be-
cause the corresponding value stored there is neg-
ative. The same values in the VALUE array can be
unified to reduce the memory requirements. Figure
8 illustrates an example of the embeddingmethod.
227
Figure 8: Implementation of the embedding method.
We use vacant spaces in the VALUE array to store the
probabilities and indices of backoff weights. The in-
dices of backoff weights are taken with a negative sign
to avoid false positives. Backoff weights are stored in the
VALUE array, and the same values in the VALUE array
can be unified.
4.3 Ordering
Ordering is a method for shortening the double-
array structure and increasing the query speed. In
ordering, word IDs are assigned in order of un-
igram probability. This is done at a preprocessing
stage, before the DALM is built.
Before explaining the reasons why this method
is effective, we present an interpretation of double-
array construction in Figure 9 which corresponds to
the case presented in Figure 3. In the previous
section, we pointed out that the insertion problem
is equivalent to the problem of finding the BASE
value of the parent node. Here, we expand this fur-
ther into the idea that finding the BASE value is
equivalent to the problem of finding the shift length
of an insertion array. We can create an insertion
array which is an array of flag bits set to 1 at the po-
sitions of word IDs of children nodes? words. More-
over, we prepare a used array which is also a flag
bit array denoting whether the original slots in the
double-array structure are occupied. In this situ-
ation, finding the shift length is equivalent to the
problem of finding the BASE value of the slot for
the node ?eat?, and the combined used array denotes
the size of the double-array structure after insertion.
Figure 10 shows an intuitive example illustrating
the efficiency of the ordering method. When
word IDs are assigned in order of unigram proba-
bility, 1s in the insertion array are gathered toward
Figure 9: Interpretation of a double-array construction.
The insertion problem for the double-array structure is
interpreted as a finding problem of a shift length of the
insertion array. We can measure the size of the double-
array structure in the used array.
the beginning of the array. This means that 1s in
the insertion array form clusters, which makes in-
sertion easier than for unordered insertion arrays.
This shortens the shift lengths for each insertion ar-
ray: a shorter double-array structure results.
5 Experiment
5.1 Experimental Setup
To compare the performance of DALM with other
methods, we conduct experiments on two ngram
models built from small and large training corpora.
Table 1 shows the specifications of the model.
Training data are extracted from the Publication
of unexamined Japanese patent applications, which
is distributed with the NTCIR 3,4,5,6 patent retrieval
task (Atsushi Fujii et al, 2007; Atsushi Fujii et al,
2005; Atsushi Fujii et al, 2004; Makoto Iwayama
et al, 2003). We used data for the period from
228
Figure 10: An example of word ID ordering effi-
ciency. Word IDs in the insertion array are packed to
the front in advance. Therefore, shift lengths for ordered
arrays are often shorter than unordered ones. The result-
ing size of the double-array structure is expected to be
smaller than that of an unordered array.
Table 1: Corpus and model specifications.
Model
Corpus Unique Ngram
Size Type Type
(words) (words) (1-5gram)
100 Mwords 100 M 195 K 31 M
5 Gwords 5 G 2,140 K 936 M
Test set 100 M 198 K -
1,993 to 2,002 and extracted paragraphs containing
?background? and ?example?. This method is simi-
lar to the NTCIR 7 Patent Translation Task(Fujii et
al., 2008). The small and large training data sets
contained 100 Mwords and 5 Gwords, respectively.
Furthermore, we sampled another 100 Mwords as
a test set to measure the access speed for extract-
ing ngram probabilities. We used an Intel R? Xeon R?
X5675 (3.07 GHz) 24-core server with 142 GB of
RAM.
Our experiments were performed from the view-
points of speed and model size. We executed each
program twice, and the results of the second run
were taken as the final performance.
Figure 11: Comparison between tuned and non-tuned
double-array structures.
Table 2: Comparison between tuned and non-tuned
double-array structures.
Method Size Speed(MB) (queries/s)
Simple 1,152 1,065,536
Embedding 782 1,004,555
Ordering 726 1,083,703
Both 498 1,057,607
5.2 Optimization Methods
We compared the performance of the DALMs
proposed here, namely simple, embedding,
ordering and both, where both indicates that
the language model uses both embedding and
ordering. We conducted experiments examin-
ing how these methods affect the size of the double-
array structures and the query speeds. We used the
100 Mwords model in the comparison because it
was difficult to build a DALM using the 5 Gwords
model.
The results are shown in Figure 11 and Table 2.
While both ordering and embedding decreased
the model size, the query speed was increased by the
former and decreased by the latter. Both was the
smallest and most balanced method.
5.3 Divided Double-Array Structure
Building a double-array structure requires a long
time, which can sometimes be impractical. In fact,
as mentioned above, waiting on construction of the
double-array structure of the 5 Gwords model is in-
feasible.
229
Figure 12: Comparison between divided and original
double-array structures.
As described in Section 3.3, the efficient algo-
rithm requires O(UM) time to insert one node and
the insertion is iterated N (the total number of inser-
tions) times. If we assume that the number of unused
slots at the ith insertion, Ui, is proportional to i, or
that Ui = c ? i where c is a proportionality con-
stant, we can calculate the building time as follows:
?N
i=1 UiM = O
(
MN2
)
.
To shorten the build time, we divided the original
trie into several parts. Building parts of the origi-
nal trie is possible because N is reduced. Moreover,
these double-array structures can be built in parallel.
Note that query results for both original and divided
tries are completely equivalent because divided tries
hold all the ngram statistics of the original trie. This
method is similar to that used in randomized lan-
guage models (Talbot and Brants, 2008).
We compared the differences between the meth-
ods using the original and divided double-array
structures. In the comparison, we also used the 100
Mwords model with the both optimization method
described in the previous section (Figure 12 and Ta-
ble 3).
Although dividing the trie increased the size of
the DALM slightly, the model size was still smaller
than that without optimization. Query speed in-
creased as the number of parts was increased. We
attributed this to the divided DALM consisting of
several double-array structures, each smaller than
the undivided structure which results in an increase.
Figure 12 shows that there is a trade-off relation be-
tween model size and query speed.
Below, we use the 5 Gwords model in our exper-
iments. In our environment, building a 5 Gwords
Table 3: Comparison between divided and original
double-array structures.
Number of parts Size Speed(MB) (queries/s)
1 498 1,057,607
2 502 1,105,358
4 510 1,087,619
8 540 1,098,594
double-array structure required about 4 days when
the double-array structures were divided into 8 parts,
even though we used the more efficient algorithm
described in Section 3.3. The time required for
building the model when the original structure was
divided into less than 8 parts was too long. Thus,
a more efficient building algorithm is essential for
advancing this research further.
5.4 Comparison with Other Methods
Using the 100 Mwords and 5 Gwords mod-
els, we compared DALM with other meth-
ods (KenLM (Kenneth Heafield, 2011) and
SRILM (Stolcke, 2002)). In this experiment, we
used the both method (which is mentioned above)
for DALM and divided the original trie into 8 parts
and built double-array structures.
The results are shown in Figure 13 and Table 4;
the group on the left shows the results for the 100
Mwords model and the group on the right shows the
results for the 5 Gwords model.
The experimental results clearly indicate that
DALM is the fastest of all the compared methods
and that the model size is nearly the same or slightly
smaller than that of KenLM (Probing). Whereas
KenLM (Trie) is the smallest model, it is slower
than DALM.
The differences between the 5 Gwords versions
of DALM and KenLM (Probing) are smaller in
comparison with the 100 Mwords models. This is
because hash-based language models have an ad-
vantage when storing higher-order ngrams. Large
language models have more 5grams, which leads to
shorter backoff times. On the other hand, trie-based
language models have to trace higher-order ngrams
for every query, which requires more time.
Finally, we discuss practical situations. We con-
230
Table 4: Comparison between DALM and other methods.
100 Mwords Model 5 Gwords Model
LM Size Speed Size Speed(MB) (queries/s) (MB) (queries/s)
SRILM 1,194 894,138 31,747 729,447
KenLM (Probing) 665 1,002,489 18,685 913,208
KenLM (Trie) 340 804,513 9,606 635,300
DALM (8 parts) 540 1,098,594 15,345 953,186
Figure 13: Comparison between DALM and other lan-
guage model systems.
ducted this study?s experiments using test set text
written by humans. In some applications such as
statistical machine translations, language model sys-
tems should compute probabilities of many unnatu-
ral ngrams which will be unknown. This may affect
query speed because querying unknown and unnat-
ural ngrams generate many backoffs. They may re-
sults in trie-based LM being slightly faster, because
traversing the trie can stop immediately when it de-
tects that a queried ngram history is not contained in
the trie. On the other hand, hash-based LM such as
KenLM probing would repeat queries until finding
truncated ngram histories in the trie.
6 Conclusion
We proposed a method for implementing language
models based on double-array structures. We call
this method DALM. Moreover, we proposed two
methods for optimizing DALM: embedding and
ordering. Embedding is a method whereby
empty spaces in arrays are used to store ngram prob-
abilities and backoff weights, and ordering is a
method for numbering word IDs; these methods re-
duce model size and increase query speed. These
two optimization methods work well independently,
but even better performance can be achieved if they
are combined.
We also used a division method to build the model
structure in several parts in order to speed up the
construction of double-array structures. Although
this procedure results in a slight increase in model
size, the divided double-array structures mostly re-
tained the compactness and speed of the original
structure. The time required for building double-
array structures is the bottleneck of DALM as it is
sometimes too long to be practical, even though the
model structure itself achieves high performance. In
future work, we will develop a faster algorithm for
building double-array structures.
While DALM has outperformed state-of-the-art
language model implementations methods in our ex-
periments, we should continue to consider ways to
optimize the method for higher-order ngrams.
Acknowledgments
We thank the anonymous reviewers for many valu-
able comments. This work is supported by JSPS
KAKENHI Grant Number 24650063.
References
J.-I. Aoe. 1989. An Efficient Digital Search Algorithm
by Using a Double-Array Structure. IEEE Transac-
tions on Software Engineering, 15(9):1066?1077.
Atsushi Fujii, Makoto Iwayama, and Noriko Kando.
2004. Overview of the Patent Retrieval Task at
NTCIR-4.
Atsushi Fujii, Makoto Iwayama, and Noriko Kando.
2005. Overview of Patent Retrieval Task at NTCIR-
5.
231
Atsushi Fujii, Makoto Iwayama, and Noriko Kando.
2007. Overview of the Patent Retrieval Task at the
NTCIR-6 Workshop. pages 359?365.
Djamal Belazzougui, Fabiano C. Botelho, and Martin Di-
etzfelbinger. 2009. Hash, displace, and compress. In
ESA, pages 682?693.
Timothy C. Bell, John G. Cleary, and Ian H. Witten.
1990. Text compression.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och,
and Jeffrey Dean. 2007. Large Language Models in
Machine Translation. In Proceedings of the 2007 Joint
Conference on EMNLP-CoNLL, pages 858?867. ACL.
B. Merialdo F. Jelinek. 1990. Self-organized language
modeling for speech recognition.
Edward Fredkin. 1960. Trie memory. Communications
of the ACM, 3(9):490?499.
Kimmo Fredriksson and Fedor Nikitin. 2007. Simple
Compression Code Supporting Random Access and
Fast String Matching. In Proceedings of the 6th in-
ternational conference on Experimental algorithms,
WEA?07, pages 203?216. Springer-Verlag.
Atsushi Fujii, Masao Utiyama, Mikio Yamamoto, and
Takehito Utsuro. 2008. Overview of the Patent Trans-
lation Task at the NTCIR-7 Workshop.
Ulrich Germann, Eric Joanis, and Samuel Larkin. 2009.
Tightly Packed Tries: How to Fit Large Models into
Memory, and Make them Load Fast, Too. In Proceed-
ings of the Workshop on SETQA-NLP, pages 31?39.
ACL.
David Guthrie and Mark Hepple. 2010. Storing the Web
in Memory: Space Efficient Language Models with
Constant Time Retrieval. In Proceedings of the 2010
Conference on EMNLP, pages 262?272. ACL.
Kenneth Heafield. 2011. KenLM: Faster and Smaller
Language Model Queries. In Proceedings of the Sixth
Workshop on Statistical Machine Translation. ACL.
Makoto Iwayama, Atsushi Fujii, Noriko Kando, and Aki-
hiko Takano. 2003. Overview of Patent Retrieval Task
at NTCIR-3.
Yasumasa Nakamura and Hisatoshi Mochizuki. 2006.
Fast Computation of Updating Method of a Dictio-
nary for Compression Digital Search Tree. Trans-
actions of Information Processing Society of Japan.
Data, 47(13):16?27.
Adam Pauls and Dan Klein. 2011. Faster and Smaller
N-Gram Language Models. In Proceedings of the
49th Annual Meeting of the ACL-HLT, pages 258?267.
ACL.
A. Stolcke. 2002. SRILM-an Extensible Language Mod-
eling Toolkit. Seventh International Conference on
Spoken Language Processing.
David Talbot and Thorsten Brants. 2008. Randomized
Language Models via Perfect Hash Functions. In Pro-
ceedings of ACL-08: HLT.
David Talbot and Miles Osborne. 2007. Smoothed
Bloom Filter Language Models: Tera-Scale LMs on
the Cheap. In Proceedings of the 2007 Joint Confer-
ence on EMNLP-CoNLL, pages 468?476.
232
