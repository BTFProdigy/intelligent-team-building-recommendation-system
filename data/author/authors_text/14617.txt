Proceedings of the ACL 2010 Student Research Workshop, pages 49?54,
Uppsala, Sweden, 13 July 2010. c?2010 Association for Computational Linguistics
Growing Related Words from Seed via User Behaviors: A Re-ranking 
Based Approach 
Yabin Zheng Zhiyuan Liu Lixing Xie 
State Key Laboratory on Intelligent Technology and Systems 
Tsinghua National Laboratory for Information Science and Technology 
Department of Computer Science and Technology, Tsinghua University, Beijing 100084,China 
{yabin.zheng, lzy.thu, lavender087}@gmail.com 
 
Abstract 
Motivated by Google Sets, we study the prob-
lem of growing related words from a single 
seed word by leveraging user behaviors hiding 
in user records of Chinese input method. Our 
proposed method is motivated by the observa-
tion that the more frequently two words co-
occur in user records, the more related they are. 
First, we utilize user behaviors to generate 
candidate words. Then, we utilize search en-
gine to enrich candidate words with adequate 
semantic features. Finally, we reorder candi-
date words according to their semantic rela-
tedness to the seed word. Experimental results 
on a Chinese input method dataset show that 
our method gains better performance. 
1 Introduction 
What is the relationship between ??????
?? (Natural Language Processing) and ????
? ? (Artificial Intelligence)? We may regard 
NLP as a research branch of AI. Problems arise 
when we want to find more words related to the 
input query/seed word. For example, if seed 
word ??????? ? (Natural Language 
Processing) is entered into Google Sets (Google, 
2010), Google Sets returns an ordered list of re-
lated words such as  ?????? (Artificial In-
telligence) and ????? (Computer). Generally 
speaking, it performs a large-scale clustering al-
gorithm that can gather related words. 
In this paper, we want to investigate the ad-
vantage of user behaviors and re-ranking frame-
work in related words retrieval task using Chi-
nese input method user records. We construct a 
User-Word bipartite graph to represent the in-
formation hiding in user records. The bipartite 
graph keeps users on one side and words on the 
other side. The underlying idea is that the more 
frequently two words co-occur in user records, 
the more related they are. For example, ????
?? (Machine Translation) is quite related to ??
???? (Chinese Word Segmentation) because 
the two words are usually used together by re-
searchers in natural language processing com-
munity. As a result, user behaviors offer a new 
perspective for measuring relatedness between 
words. On the other hand, we can also recom-
mend related words to users in order to enhance 
user experiences. Researchers are always willing 
to accept related terminologies in their research 
fields. 
However, the method is purely statistics based 
if we only consider co-occurrence aspect. We 
want to add semantic features. Sahami and Hel-
man (2006) utilize search engine to supply web 
queries with more semantic context and gains 
better results for query suggestion task. We bor-
row their idea in this paper. User behaviors pro-
vide statistic information to generate candidate 
words. Then, we can enrich candidate words 
with additional semantic features using search 
engine to retrieve more relevant candidates earli-
er. Statistical and semantic features can comple-
ment each other. Therefore, we can gain better 
performance if we consider them together. 
The contributions of this paper are threefold. 
First, we introduce user behaviors in related 
word retrieval task and construct a User-Word 
bipartite graph from user behaviors. Words are 
used by users, and it is reasonable to measure 
relatedness between words by analyzing user 
behaviors. Second, we take the advantage of se-
mantic features using search engine to reorder 
candidate words. We aim to return more relevant 
candidates earlier. Finally, our method is unsu-
pervised and language independent, which means 
that we do not require any training set or manual 
labeling efforts. 
The rest of the paper is organized as follows. 
Some related works are discussed in Section 2. 
Then we introduce our method for related words 
retrieval in Section 3. Experiment results and 
discussions are showed in Section 4. Finally, 
Section 5 concludes the whole paper and gives 
some future works. 
49
2 Related Work 
For related words retrieval task, Google Sets 
(Google, 2010) provides a remarkably interesting 
tool for finding words related to an input word. 
As stated in (Zheng et al, 2009), Google Sets 
performs poor results for input words in Chinese 
language. Bayesian Sets (Ghahramani and Heller, 
2006) offers an alternative method for related 
words retrieval under the framework of Bayesian 
inference. It computes a score for each candidate 
word by comparing the posterior probability of 
that word given the input, to the prior probability 
of that candidate word. Then, it returns a ranked 
list of candidate words according to their com-
puted scores. 
Recently, Zheng et al (2009) introduce user 
behaviors in new word detection task via a colla-
borative filtering manner. They extend their me-
thod to related word retrieval task. Moreover, 
they prove that user behaviors provide a new 
point for new word detection and related word 
retrieval tasks. However, their method is purely 
statistical method without considering semantic 
features. 
We can regard related word retrieval task as 
problem of measuring the semantic relatedness 
between pairs of very short texts. Sahami and 
Helman (2006) introduce a web kernel function 
for measuring semantic similarities using snip-
pets of search results. This work is followed by 
Metzler et al, (2007), Yih and Meek, (2007). 
They combine the web kernel with other metrics 
of similarity between word vectors, such as Jac-
card Coefficient and KL Divergence to enhance 
the result. 
In this paper, we follow the similar idea of us-
ing search engine to enrich semantic features of a 
query word. We regard the returned snippets as 
the context of a query word. And then we reorder 
candidate words and expect more relevant candi-
date words can be retrieved earlier. More details 
are given in Section 3. 
3 Related Words Retrieval 
In this section, we will introduce how to find 
related words from a single seed word via user 
behaviors and re-ranking framework. 
First, we introduce the dataset utilized in this 
paper. All the resource used in this paper comes 
from Sogou Chinese pinyin input method (Sogou, 
2006). We use Sogou for abbreviation hereafter. 
Users can install Sogou on their computers and 
the word lists they have used are kept in their 
user records. Volunteers are encouraged to upl-
oad their anonymous user records to the server 
side. In order to preserve user privacy, user-
names are hidden using MD5 hash algorithm. 
Then we demonstrate how to build a User-
Word bipartite graph based on the dataset. The 
construction can be accomplished while travers-
ing the dataset with linear time cost. We will 
give more details in Section 3.1. 
Second, we adopt conditional probability 
(Deshpande and Karypis, 2004) to measure the 
relatedness of two words. Intuitively, two words 
are supposed to be related if there are a lot of 
users who have used both of them. In other 
words, the two words always co-occur in user 
records. Starting from a single seed word, we can 
generate a set of candidate words. This is the 
candidate generation step. 
Third, in order to take the advantage of seman-
tic features, we carry out feature extraction tech-
niques to represent generated candidate words 
with enriched semantic context. In this paper, we 
generally make use of search engine to conduct 
the feature extraction step. After this step, input 
seed word and candidate words are represented 
as feature vectors in the vector space. 
Finally, we can reorder generated candidate 
words according to their semantic relatedness of 
the input seed word. We expect to retrieve more 
relevant candidate words earlier. We will make 
further explanations about the mentioned steps in 
the next subsections. 
3.1 Bipartite Graph Construction 
As stated before, we first construct a User-Word 
bipartite graph from the dataset. The bipartite 
graph has two layers, with users on one side and 
the words on the other side. We traverse the user 
records, and add a link between user u and word 
w if w appears in the user record of u. Thus this 
procedure can be accomplished in linear time. 
In order to give better explanations of bipartite 
graph construction step, we show some user 
records in Figure 1 and the corresponding bipar-
tite graph in Figure 2. 
 
 
Fig. 1. User Records Sample 
User1 Word1 ????(Natural Language) 
Word2????(Artificial Intelligence) 
Word3 ????(Machine Translation) 
Word2????(Artificial Intelligence) 
Word4 ????(Information Retrieval) 
Word3 ????(Machine Translation) 
Word1 ????(Natural Language) 
 
User2 
User3 
50
 
Fig. 2. Corresponding Bipartite Graph 
 
From Figure 1, we can see that Word1 and 
Word2 appear in User1?s record, which indicates 
that User1 has used Word1 and Word2. As a result, 
in Figure 2, node User1 is linked with node 
Word1 and Word2. The rest can be done in the 
same manner. 
3.2 Candidates Generation 
After the construction of bipartite graph, we can 
measure the relatedness of words from the bipar-
tite graph. Intuitively, if two words always co-
occur in user records, they are related to each 
other. Inspired by (Deshpande and Karypis, 
2004), we adopt conditional probability to meas-
ure the relatedness of two words. 
In particular, the conditional probability of 
word j occurs given that word i has already ap-
peared is the number of users that used both 
word i and word j divided by the total number of 
users that used word i. 
 
( )( | )         (1)( )
Freq ijP j i Freq i?
 
 
In formula 1, Freq(X) is the number of users 
that have used words in the set X. We can clearly 
see that P(j|i) ? P(i|j), which means that condi-
tional probability leads to asymmetric relations. 
The disadvantage is that each word i tends to 
have a close relationship with stop words that are 
used quite frequently in user records, such as 
??? (of) and ???? (a). 
In order to alleviate this problem, we consider 
the conditional probabilities P(j|i) and P(i|j) to-
gether. Word i and word j is said to be quite re-
lated if conditional probabilities P(j|i) and P(i|j) 
are both relatively high. We borrow the idea pro-
posed in (Li and Sun, 2007). In their paper, a 
weighted harmonic averaging is used to define 
the relatedness score between word i and word j 
because either P(j|i) or P(i|j) being too small is a 
severe detriment. 
 
11( , )    (2)( | ) ( | )Score i j P i j P j i
? ? ?? ??? ?? ?? ?
 
In formula 2, parameter [0,1]? ?  is the weight 
for P(i|j), which denotes how much P(i|j) should 
be emphasized. We carry out some comparative 
experiments when parameter ? varies from 0 to 1 
stepped by 0.1. We also tried other co-
occurrence based measures like mutual informa-
tion, Euclidean and Jaccard distance, and found 
that weight harmonic averaging gives relatively 
better results. Due to space limitation, we are not 
able to report detailed results. 
So far, we have introduced how to calculate 
the relatedness Score(i, j) between word i and 
word j. When a user enters an input seed word w, 
we can compute Score(w,c) between seed word 
w and each candidate word c, and then sort can-
didate words in a descending order. Top N can-
didate words are kept for re-ranking, we aim to 
reorder top N candidate words and return the 
more related candidate words earlier. Alterna-
tively, we can also set a threshold for Score(w,c), 
which keeps the candidate word c with Score(w,c) 
larger than the threshold. We argue that this thre-
shold is difficult to set because different seed 
words have different score thresholds. 
Note that this candidate generation step is 
completely statistical method as we only consid-
er the co-occurrence of words. We argue that 
semantic features can be a complement of statis-
tical method. 
3.3 Semantic Feature Representation and 
Re-ranking 
As stated before, we utilize search engine to 
enrich semantic features of the input seed word 
and top N candidate words. To be more specific, 
we issue a word to a search engine (Sogou, 2004) 
and get top 20 returned snippets. We regard 
snippets as the context and the semantic repre-
sentation of this word. 
For an input seed word w, we can generate top 
N candidate words using formula (2). We issue 
each word to search engine and get returned 
snippets. Then, each word is represented as a 
feature vector using bag-of-words model. Fol-
lowing the conventional approach, we calculate 
the relatedness between the input seed word w 
and a candidate word c as the cosine similarity 
between their feature vectors. Intuitively, if we 
introduce more candidate words, we are more 
likely to find related words in the candidate sets. 
However, noisy words are inevitably included. 
We will show how to tune parameter N in the 
experiment part. 
W1 
U1 
U2 
U3 
W2 
W3 
W4 
51
As a result, candidate words with higher se-
mantic similarities can be returned earlier with 
enriched semantic features. Re-ranking can be 
regarded as a complementary step after candidate 
generation. We can improve the performance of 
related word retrieval task if we consider user 
behaviors and re-ranking together. 
4 Experiment 
In this section, we demonstrate our experiment 
results. First, we introduce the dataset used in 
this paper and some statistics of the dataset. Then, 
we build our ground truth for related word re-
trieval task using Baidu encyclopedia. Third, we 
give some example of related word retrieval task. 
We show that more related words can be re-
turned earlier if we consider semantic features. 
Finally, we make further analysis of the parame-
ter tuning mentioned before. 
4.1 Experiment Settings 
We carry out our experiment on Sogou Chinese 
input method dataset. The dataset contains 
10,000 users and 183,870 words, and the number 
of edges in the constructed bipartite graph is 
42,250,718. As we can see, the dataset is quite 
sparse, because most of the users tend to use only 
a small number of words. 
For related word retrieval task, we need to 
judge whether a candidate word is related to the 
input seed word. We can ask domain experts to 
answer this question. However, it needs a lot of 
manual efforts. To alleviate this problem, we 
adopt Baidu encyclopedia (Baidu, 2006) as our 
ground truth. In Baidu encyclopedia, volunteers 
give a set of words that are related to the particu-
lar seed word. As related words are provided by 
human, we are confident enough to use them as 
our ground truth. 
We randomly select 2,000 seed words as our 
validation set. However, whether two words are 
related is quite subjective. In this paper, Baidu 
encyclopedia is only used as a relatively accurate 
standard for evaluation. We just want to investi-
gate whether user behaviors and re-ranking 
framework is helpful in the related word retrieval 
task under various evaluation metrics. 
We give a simple example of our method in 
Table 1. The input seed word is ?????? 
(Machine Learning). Generally speaking, all 
these returned candidate words are relevant to 
the seed word to certain degree, which indicates 
the effectiveness of our method. 
 
????(feature vector) ???(kernel function) 
???(training set) ???(decision tree) 
???(classifier) ???(test set) 
??(dimension reduc-
tion) 
????(feature ex-
traction) 
Table 1. Words Related to ?Machine Learning? 
4.2 Evaluation Metrics 
In this paper, we use three evaluation metrics to 
validate the performance of our method: 
1. Precision@N (P@N). P@N measures how 
much percent of the topmost results returned 
are correct. We consider P@5 and P@10. 
2. Binary preference measure (Bpref) (Buck-
ley and Voorhees, 2004). As we cannot list 
all the related words of an input seed word, 
we use Bpref to evaluate our method. For an 
input seed word with R judged candidate 
words where r is a related word and n is a 
nonrelated word. Bpref is defined as follow: 
 
1 |     |1     (3)
r
n ranked higher than rBpref R R? ??
 
3. Mean reciprocal rank of the first retrieved 
result (MRR). For a sample of input seed 
words W, ranki is the rank of the first related 
candidate word for the input seed word wi, 
MRR is the average of the reciprocal ranks 
of results, which is defined as follow: 
 
1 1      (4) i iMRR W rank? ?
 
4.3 Candidate Re-ranking 
In order to show the effectiveness of semantic 
features and re-ranking framework, we give an 
example in Table 2. The input seed word is ??
??? (Ericsson), and if we only take user beha-
viors into consideration, top 5 words returned are 
shown on the left side. After using search engine 
and semantic representation, we reorder the can-
didate words as shown on the right side. 
 
Input Seed Word: ??? (Ericsson) 
Top 5 Candidates After Re-ranking 
?? (Nortel) ????? (Sony 
Ericsson) 
?? (ZTE Corporation) ?? (Sony Ericsson) 
?? (Base Station) ???? (Alcatel) 
???? (Alcatel) ?? (Sony) 
??? (Core Network) ?? (Huawei) 
Table 2. Candidate Re-ranking 
52
As shown in Table 2, we can clearly see that 
we return the most related candidate words such 
as ??????? (Sony Ericsson) and ???? 
(the abbreviation of Sony Ericsson in Chinese) in 
the first two places. Moreover, after re-ranking, 
top candidate words are some famous brands that 
are quite related to query word ????? (Erics-
son). Some words like ????? (Core Network) 
that are not quite related to the query word are 
removed from the top list. From this observation, 
we can see that semantic features and re-ranking 
framework can improve the performance. 
4.4 Parameter Tuning 
As discussed in Section 3, we have introduced 
two parameters in this paper. The first is the pa-
rameter ? in the candidate generation step, and 
the other is the parameter N in the re-ranking 
step. We show how these two parameters affect 
the performance. In addition, we should emphas-
ize that the ground truth is not a complete answer, 
so all the results are only useful for comparisons. 
The absolute value is not very meaningful. 
As we have shown in Section 3.2, parameter ? 
adjusts the weight of conditional probability be-
tween two word i, j. The parameter ? is varied 
from 0 to 1 stepped by 0.1. We record the cor-
responding values of P@5, P@10, Bpref and 
MRR. The results are shown in Figure 3. 
We can clearly see that all the values increase 
when ? increases first. And then all the values 
decrease dramatically when ? is close to 1. This 
indicates that either P(j|i) or P(i|j) being too 
small is a severe detriment. The result reaches 
peak value when ?=0.5, i.e. we should treat P(j|i) 
and P(i|j)equally to get the best result. Therefore, 
we use ?=0.5 to generate candidate words, those 
candidates are used for re-ranking. 
 
 
Fig. 3. Parameter ? for Candidate Generation 
 
We also carry out the comparisons with Baye-
sian Sets, which is shown in Table 3. It is clear 
that our method gains better results than Baye-
sian Sets with different values of parameter ?. 
Results of Google Sets are omitted here because 
Zheng et al (2009) have already showed that 
Google Sets performs worse than Bayesian Sets 
with query words in Chinese. 
 
 Bpref MRR P@5 P@10 
? = 0.4 0.2057 0.4267 0.2352 0.195 
? = 0.5 0.2035 0.4322 0.2414 0.2019 
? = 0.6 0.2038 0.4292 0.2408 0.2009 
Bayesian Sets 0.2033 0.3291 0.1842 0.1512 
Table 3. Comparisons with Bayesian Sets 
 
To investigate the effectiveness of re-ranking 
framework, we also conduct experiments on the 
parameter N that is used for re-ranking. The ex-
perimental results are shown in Figure 4. 
 
 
Fig. 4. Top N Candidates for Re-ranking 
 
We can observe that more candidates tend to 
harm the performance as noisy words are intro-
duced inevitably. For example, Bpref drops to 
less than 0.25 when N = 100. More comparative 
results are shown in Table 4. We can see that N = 
20 gives relatively best results, which indicates 
that we should select Top 20 candidate words for 
re-ranking. 
 
 Bpref MRR P@5 P@10 
Non Re-ranking 0.2035 0.4322 0.2414 0.2019 
N = 10 0.3208 0.456 0.2752 0.2019 
N = 20 0.3047 0.4511 0.2769 0.2301 
N = 30 0.2899 0.4444 0.272 0.2305 
Table 4. Comparisons with Re-ranking Method 
5 Conclusions and Future Work 
In this paper, we have proposed a novel method 
for related word retrieval task. Different from 
other method, we consider user behaviors, se-
mantic features and re-ranking framework to-
gether. We make a reasonable assumption that if 
two words always co-occur in user records, then 
53
they tend to have a close relationship with each 
other. Based on this assumption, we first gener-
ate a set of candidate words that are related to an 
input seed word via user behaviors. Second, we 
utilize search engine to enrich candidates with 
semantic features. Finally, we can reorder the 
candidate words to return more related candi-
dates earlier. Experiment results show that our 
method is effective and gains better results. 
However, we also observed some noisy words 
in the returned results. As our dataset is generat-
ed from Chinese input method, users can type 
whatever they want, which will bring some noise 
in the dataset. We plan to remove noisy words in 
the future. Furthermore, we want to take the ad-
vantage of learning to rank literature (Liu, 2009) 
to further improve the performance of related 
word retrieval task. We may need to extract more 
features to represent the word pairs and build a 
labeled training set. Then various machine learn-
ing techniques can be used in this task. 
Another important issue is how to build a 
complete and accurate ground truth for related 
word retrieval task. People may have different 
opinions about whether two words are related or 
not, which makes this problem complicate. 
Thirdly, our method can only process a single 
seed word, so we aim to extend our method to 
process multiple seed words. In addition, we 
want to build a network of Chinese word associa-
tion. We can discover how words are organized 
and connected within this network. And this 
word association network will be quite useful for 
foreigners to learn Chinese. 
Fourthly, how to deal with ambiguous query 
word is also left as our future work. For example, 
query word ?apple? can refer to a kind of fruit or 
an IT company. As a result, we are expected to 
return two groups of related words instead of 
mixing them together. 
Finally, our dataset provides a new perspective 
for many interesting research tasks like new 
word detection, social network analysis, user be-
havior analysis, and so on. We are trying to re-
lease our dataset for research use in the future. 
Acknowledgement 
We thank Xiance Si and Wufeng Ke for provid-
ing the Baidu encyclopedia corpus for evaluation. 
We also thank the anonymous reviewers for their 
helpful comments and suggestions. This work is 
supported by a Tsinghua-Sogou joint research 
project. 
References 
Baidu. 2006. Baidu Encyclopedia. Available at 
http://baike.baidu.com 
Chris Buckley and Ellen M. Voorhees. 2004. Retriev-
al Evaluation with Incomplete Information. In Pro-
ceedings of the 27th annual international ACM 
SIGIR conference on Research and development in 
information retrieval, pp 25-32  
Mukund Deshpande and George Karypis. 2004. Item-
Based Top-N Recommendation Algorithms, ACM 
Trans. Information Systems, 22(1): 143-177 
Zoubin Ghahramani and Katherine A. Heller. 2005. 
Bayesian Sets. In Advances in Neural Information 
Processing Systems 
Google. Google Sets. Accessed on Feb. 9th, 2010, 
available at: http://labs.google.com/sets 
Jingyang Li and Maosong Sun. 2007. Scalable term 
selection for text categorization, In Proceedings of 
the 2007 Joint Conference on Empirical Methods 
in Natural Language Processing and Computa-
tional Natural Language Learning, pp. 774-782 
Tie-Yan Liu. 2009. Learning to Rank for Information 
Retrieval, Foundation and Trends on Information 
Retrieval, Now Publishers 
Donald Metzler, Susan T. Dumais, and Christopher 
Meek. 2007. Similarity measures for short seg-
ments of text. In Proceeding of the 29th European 
Conference on Information Retrieval, pp 16-27  
Mehran Sahami and Timothy D. Heilman. 2006. A 
web-based kernel function for measuring the simi-
larity of short text snippets. In Proceedings of the 
15th International Conference on World Wide Web, 
pp 377-386 
Sogou. 2006. Sogou Chinese Pinyin Input Method. 
Available at http://pinyin.sogou.com/ 
Sogou. 2004. Sogou Search Engine. Available at 
http://www.sogou.com  
Wen-Tau Yih and Christopher Meek. 2007. Improv-
ing similarity measures for short segments of text. 
In Proceedings of AAAI 2007, pp 1489-1494 
Yabin Zheng, Zhiyuan Liu, Maosong Sun, Liyun Ru, 
and Yang Zhang. 2009. Incorporating User Beha-
viors in New Word Detection. In Proceedings of 
the Twenty-First International Joint Conference on 
Artificial Intelligence, pp 2101-2106 
54
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 485?490,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Why Press Backspace? Understanding User Input Behaviors in Chinese
Pinyin Input Method
Yabin Zheng1, Lixing Xie1, Zhiyuan Liu1, Maosong Sun1, Yang Zhang2, Liyun Ru1,2
1State Key Laboratory of Intelligent Technology and Systems
Tsinghua National Laboratory for Information Science and Technology
Department of Computer Science and Technology
Tsinghua University, Beijing 100084, China
2Sogou Inc., Beijing 100084, China
{yabin.zheng,lavender087,lzy.thu,sunmaosong}@gmail.com
{zhangyang,ruliyun}@sogou-inc.com
Abstract
Chinese Pinyin input method is very impor-
tant for Chinese language information pro-
cessing. Users may make errors when they
are typing in Chinese words. In this paper, we
are concerned with the reasons that cause the
errors. Inspired by the observation that press-
ing backspace is one of the most common us-
er behaviors to modify the errors, we collect
54, 309, 334 error-correction pairs from a real-
world data set that contains 2, 277, 786 user-
s via backspace operations. In addition, we
present a comparative analysis of the data to
achieve a better understanding of users? input
behaviors. Comparisons with English typos
suggest that some language-specific properties
result in a part of Chinese input errors.
1 Introduction
Unlike western languages, Chinese is unique due
to its logographic writing system. Chinese users
cannot directly type in Chinese words using a QW-
ERTY keyboard. Pinyin is the official system to
transcribe Chinese characters into the Latin alpha-
bet. Based on this transcription system, Pinyin input
methods have been proposed to assist users to type
in Chinese words (Chen, 1997).
The typical way to type in Chinese words is
in a sequential manner (Wang et al, 2001). As-
sume users want to type in the Chinese word ??
?(what)?. First, they mentally generate and type
in corresponding Pinyin ?shenme?. Then, a Chinese
Pinyin input method displays a list of Chinese words
which share that Pinyin, as shown in Fig. 1. Users
Figure 1: Typical Chinese Pinyin input method for a
correct Pinyin (Sogou-Pinyin).
Figure 2: Typical Chinese Pinyin input method for a
mistyped Pinyin (Sogou-Pinyin).
visually search the target word from candidates and
select numeric key ?1? to get the result. The last t-
wo steps do not exist in typing process of English
words, which indicates that it is more complicated
for Chinese users to type in Chinese words.
Chinese users may make errors when they are typ-
ing in Chinese words. As shown in Fig. 2, a user
may mistype ?shenme? as ?shenem?. Typical Chi-
nese Pinyin input method can not return the right
word. Users may not realize that an error occurs and
select the first candidate word ????? (a mean-
ingless word) as the result. This greatly limits us-
er experience since users have to identify errors and
modify them, or cannot get the right word.
In this paper, we analyze the reasons that cause
errors in Chinese Pinyin input method. This analy-
sis is helpful in enhancing the user experience and
the performance of Chinese Pinyin input method. In
practice, users press backspace on the keyboard to
modify the errors, they delete the mistyped word and
re-type in the correct word. Motivated by this ob-
485
servation, we can extract error-correction pairs from
backspace operations. These error-correction pairs
are of great importance in Chinese spelling correc-
tion task which generally relies on sets of confusing
words.
We extract 54, 309, 334 error-correction pairs
from user input behaviors and further study them.
Our comparative analysis of Chinese and English ty-
pos suggests that some language-specific properties
of Chinese lead to a part of input errors. To the best
of our knowledge, this paper is the first one which
analyzes user input behaviors in Chinese Pinyin in-
put method.
The rest of this paper is organized as follows.
Section 2 discusses related works. Section 3 intro-
duces how we collect errors in Chinese Pinyin input
method. In Section 4, we investigate the reasons that
result in these errors. Section 5 concludes the whole
paper and discusses future work.
2 Previous Work
For English spelling correction (Kukich, 1992;
Ahmad and Kondrak, 2005; Chen et al, 2007;
Whitelaw et al, 2009; Gao et al, 2010), most ap-
proaches make use of a lexicon which contains a list
of well-spelled words (Hirst and Budanitsky, 2005;
Islam and Inkpen, 2009). Context features (Ro-
zovskaya and Roth, 2010) of words provide useful
evidences for spelling correction. These features
are usually represented by an n-gram language mod-
el (Cucerzan and Brill, 2004; Wilcox-O?Hearn et
al., 2010). Phonetic features (Toutanova and Moore,
2002; Atkinson, 2008) are proved to be useful in En-
glish spelling correction. A spelling correction sys-
tem is trained using these features by a noisy channel
model (Kernighan et al, 1990; Ristad et al, 1998;
Brill and Moore, 2000).
Chang (1994) first proposes a representative ap-
proach for Chinese spelling correction, which re-
lies on sets of confusing characters. Zhang et al
(2000) propose an approximate word-matching al-
gorithm for Chinese to solve Chinese spell detec-
tion and correction task. Zhang et al (1999) present
a winnow-based approach for Chinese spelling cor-
rection which takes both local language features and
wide-scope semantic features into account. Lin and
Yu (2004) use Chinese frequent strings and report
an accuracy of 87.32%. Liu et al (2009) show that
about 80% of the errors are related to pronunciation-
s. Visual and phonological features are used in Chi-
nese spelling correction (Liu et al, 2010).
Instead of proposing a method for spelling cor-
rection, we mainly investigate the reasons that cause
typing errors in both English and Chinese. Some
errors are caused by specific properties in Chinese
such as the phonetic difference between Mandarin
and dialects spoken in southern China. Meanwhile,
confusion sets of Chinese words play an importan-
t role in Chinese spelling correction. We extract a
large scale of error-correction pairs from real user
input behaviors. These pairs contain important ev-
idence about confusing Pinyins and Chinese words
which are helpful in Chinese spelling correction.
3 User Input Behaviors Analysis
We analyze user input behaviors from anonymous
user typing records in a Chinese input method. Data
set used in this paper is extracted from Sogou Chi-
nese Pinyin input method1. It contains 2, 277, 786
users? typing records in 15 days. The numbers of
Chinese words and characters are 3, 042, 637, 537
and 5, 083, 231, 392, respectively. We show some
user typing records in Fig. 3.
[20100718 11:10:38.790ms] select:2 zhe? WINWORD.exe 
[20100718 11:10:39.770ms] select:1 shi? WINWORD.exe 
[20100718 11:10:40.950ms] select:1 shenem??? WINWORD.exe 
[20100718 11:10:42.300ms] Backspace WINWORD.exe 
[20100718 11:10:42.520ms] Backspace WINWORD.exe 
[20100718 11:10:42.800ms] Backspace WINWORD.exe 
[20100718 11:10:45.090ms] select:1 shenme ?? WINWORD.exe 
 
Figure 3: Backspace in user typing records.
From Fig. 3, we can see the typing process of a
Chinese sentence ?????? (What is this). Each
line represents an input segment or a backspace op-
eration. For example, word ???? (what) is type-
d in using Pinyin ?shenme? with numeric selection
?1? at 11:10am in Microsoft Word application.
The user made a mistake to type in the third
Pinyin (?shenme? is mistyped as ?shenem?). Then,
he/she pressed the backspace to modify the errors
he has made. the word ????? is deleted and re-
placed with the correct word ???? using Pinyin
1Sogou Chinese Pinyin input method, can be accessed from
http://pinyin.sogou.com/
486
?shenme?. As a result, we compare the typed-
in Pinyins before and after backspace operations.
We can find the Pinyin-correction pairs ?shenem-
shenme?, since their edit distance is less than a
threshold. Threshold is set to 2 in this paper, as
Damerau (1964) shows that about 80% of typos are
caused by a single edit operation. Therefore, using a
threshold of 2, we should be able to find most of the
typos. Furthermore, we can extract corresponding
Chinese word-correction pairs ????-??? from
this typing record.
Using heuristic rules discussed above, we extrac-
t 54, 309, 334 Pinyin-correction and Chinese word-
correction pairs. We list some examples of extracted
Pinyin-correction and Chinese word-correction pairs
in Table 1. Most of the mistyped Chinese words are
meaningless.
Pinyin-correction Chinese word-correction
shenem-shenme ???-??(what)
dianao-diannao ??-??(computer)
xieixe-xiexie ????-??(thanks)
laing-liang ???-?(two)
ganam-ganma ???-??(what?s up)
zhdiao-zhidao ??-??(know)
lainxi-lianxi ???-??(contact)
zneme-zenme ???-??(how)
dainhua-dianhua ???-??(phone)
huiali-huilai ???-??(return)
Table 1: Typical Pinyin-correction and Chinese
word-correction pairs.
We want to evaluate the precision and recall of
our extraction method. For precision aspect, we ran-
domly select 1, 000 pairs and ask five native speak-
ers to annotate them as correct or wrong. Annota-
tion results show that the precision of our method is
about 75.8%. Some correct Pinyins are labeled as
errors because we only take edit distance into con-
sideration. We should consider context features as
well, which will be left as our future work.
We choose 15 typical mistyped Pinyins to evalu-
ate the recall of our method. The total occurrences
of these mistyped Pinyins are 259, 051. We success-
fully retrieve 144, 020 of them, which indicates the
recall of our method is about 55.6%. Some errors
are not found because sometimes users do not modi-
fy the errors, especially when they are using Chinese
input method under instant messenger softwares.
4 Comparisons of Pinyin typos and
English Typos
In this section, we compare the Pinyin typos and En-
glish typos. As shown in (Cooper, 1983), typing er-
rors can be classified into four categories: deletions,
insertions, substitutions, and transpositions. We aim
at studying the reasons that result in these four kinds
of typing errors in Chinese Pinyin and English, re-
spectively.
For English typos, we generate mistyped word-
correction pairs from Wikipedia2 and SpellGood.3,
which contain 4, 206 and 10, 084 common mis-
spellings in English, respectively. As shown in Ta-
ble 2, we reach the first conclusion: about half
of the typing errors in Pinyin and English are
caused by deletions, which indicates that users are
more possible to omit some letters than other three
edit operations.
Deletions Insertions Substitutions Transpositions
Pinyin 47.06% 28.17% 19.04% 7.46%
English 43.38% 18.89% 17.32% 18.70%
Table 2: Different errors in Pinyin and English.
Table 3 and Table 4 list Top 5 letters that produce
deletion errors (users forget to type in some letters)
and insertion errors (users type in extra letters) in
Pinyin and English.
Pinyin Examples English Examples
i xianza-xianzai e achive-achieve
g yingai-yinggai i abilties-abilities
e shenm-shenme c acomplish-accomplish
u pengyo-pengyou a agin-again
h senme-shenme t admited-admitted
Table 3: Deletion errors in Pinyin and English.
Pinyin Examples English Examples
g yingwei-yinwei e analogeous-analogous
i tiebie-tebie r arround-around
a xiahuan-xihuan s asside-aside
o huijiao-huijia i aisian-asian
h shuibian-suibian n abandonned-abandoned
Table 4: Insertion errors in Pinyin and English.
2http://en.wikipedia.org/wiki/Wikipedia:
Lists_of_common_misspellings/For_machines
3http://www.spellgood.net/
487
We can see from Table 3 and Table 4 that: (1)
vowels (a, o, e, i, u) are deleted or inserted more fre-
quently than consonants in Pinyin. (2) some specific
properties in Chinese lead to insertion and deletion
errors. Many users in southern China cannot dis-
tinguish the front and the back nasal sound (?ang? -
?an?, ?ing? - ?in?, ?eng? - ?en?) as well as the retroflex
and the blade-alveolar (?zh? - ?z?, ?sh? - ?s?, ?ch? -
?c?). They are confused about whether they should
add letter ?g? or ?h? under these situations. (3) the
same letters can occur continuously in English, such
as ?acomplish-accomplish? and ?admited-admitted?
in our examples. English users sometimes make in-
sertion or deletion errors in these cases. We also
observe this kind of errors in Chinese Pinyin, such
as ?yingai-yinggai?, ?liange-liangge? and ?dianao-
diannao?.
For transposition errors, Table 5 lists Top 10 pat-
terns that produce transposition errors in Pinyin and
English. Our running example ?shenem-shenme?
belongs to this kind of errors. We classify the let-
ters of the keyboard into two categories, i.e. ?left?
and ?right?, according to their positions on the key-
board. Letter ?e? is controlled by left hand while ?m?
is controlled by right hand. Users mistype ?shenme?
as ?shenem? because they mistake the typing order
of ?m? and ?e?.
Fig. 4 is a graphic representation, in which we add
a link between ?m? and ?e?. The rest patterns in Ta-
ble 5 can be done in the same manner. Interestingly,
from Fig. 4, we reach the second conclusion: most
of the transposition errors are caused by mistak-
ing the typing orders across left and right hands.
For instance, users intend to type in a letter (?m?)
controlled by right hand. But they type in a letter
(?e?) controlled by left hand instead.
Pinyin Examples English Examples
ai xaing-xiang ei acheive-achieve
na xinag-xiang ra clera-clear
em shenem-shenme re vrey-very
ia xianzia-xianzai na wnat-want
ne zneme-zenme ie hieght-height
oa zhidoa-zhidao er befoer-before
ei jiejei-jiejie it esitmated-estimated
hs haihsi-haishi ne scinece-science
ah sahng-shang el littel-little
ou rugou-ruguo si epsiode-episode
Table 5: Transpositions errors in Pinyin and English.
Letters Controlled 
by Left Hand
Letters Controlled 
by Right Hand
r a
e
s
t
i
n
m
o
h
l
u
Figure 4: Transpositions errors on the keyboard.
For substitution errors, we study the reason why
users mistype one letter for another. In the Pinyin-
correction pairs, users always mistype ?a? as ?e? and
vice versa. The reason is that they have similar pro-
nunciations in Chinese. As a result, we add two di-
rected edges ?a? and ?e? in Fig. 5. Some letters are
mistyped for each other because they are adjacent
on the keyboard although they do not share similar
pronunciations, such as ?g? and ?f?.
We summarize the substitution errors in English
in Fig. 6. Letters ?q?, ?k? and ?c? are often mixed up
with each other because they sound alike in English
although they are apart on the keyboard. However,
the three letters are not connected in Fig. 5, which
indicates that users can easily distinguish them in
Pinyin.
Figure 5: Substitutions errors in Pinyin.
488
Figure 6: Substitutions errors in English.
Mistyped
letter
pairs
Similar
pronunciations
in Chinese
Similar
pronunciations
in English
Adjacent
on
keyboard
(m,n) X X X
(b,p);(d,t) X X ?
(z,c,s);(g,k,h) X ? X
(j,q,x);(u,v) X ? ?
(i,y) ? X X
(q,k,c) ? X ?
(j,h);(z,x) ? ? X
Table 6: Pronunciation properties and keyboard dis-
tance in Chinese Pinyin and English
We list some examples in Table 6. For example,
letters ?m? and ?n? have similar pronunciations in
both Chinese and English. Moreover, they are adja-
cent on the keyboard, which leads to interferences or
confusion in both Chinese and English. Letters ?j?,
?q? and ?x? are far from each other on the keyboard.
But they sound alike in Chinese, which makes them
connected in Fig. 5. In Fig. 6, letters ?b? and ?p?
are connected to each other because they have simi-
lar pronunciations in English, although they are not
adjacent on the keyboard.
Finally, we summarize the third conclusion: sub-
stitution errors are caused by language specific
similarities (similar pronunciations) or keyboard
neighborhood (adjacent on the keyboard).
All in all, we generally classify typing errors in
English and Chinese into four categories and investi-
gate the reasons that result in these errors respective-
ly. Some language specific properties, such as pro-
nunciations in English and Chinese, lead to substitu-
tion, insertion and deletion errors. Keyboard layouts
play an important role in transposition errors, which
are language-independent.
5 Conclusions and Future Works
In this paper, we study user input behaviors in Chi-
nese Pinyin input method from backspace opera-
tions. We aim at analyzing the reasons that cause
these errors. Users signal that they are very likely
to make errors if they press backspace on the key-
board. Then they modify the errors and type in the
correct words they want. Different from the previous
research, we extract abundant Pinyin-correction and
Chinese word-correction pairs from backspace op-
erations. Compared with English typos, we observe
some language-specific properties in Chinese have
impact on errors. All in all, user behaviors (Zheng
et al, 2009; Zheng et al, 2010; Zheng et al, 2011b)
in Chinese Pinyin input method provide novel per-
spectives for natural language processing tasks.
Below we sketch three possible directions for the
future work: (1) we should consider position fea-
tures in analyzing Pinyin errors. For example, it is
less likely that users make errors in the first letter
of an input Pinyin. (2) we aim at designing a self-
adaptive input method that provide error-tolerant
features (Chen and Lee, 2000; Zheng et al, 2011a).
(3) we want to build a Chinese spelling correction
system based on extracted error-correction pairs.
Acknowledgments
This work is supported by a Tsinghua-Sogou join-
t research project and the National Natural Science
Foundation of China under Grant No. 60873174.
References
F. Ahmad and G. Kondrak. 2005. Learning a spelling
error model from search query logs. In Proceedings of
the conference on Human Language Technology and
Empirical Methods in Natural Language Processing,
pages 955?962.
K. Atkinson. 2008. Gnu aspell 0.60.6.
http://aspell.sourceforge.net.
E. Brill and R.C. Moore. 2000. An improved error model
for noisy channel spelling correction. In Proceedings
of the 38th Annual Meeting on Association for Com-
putational Linguistics, pages 286?293.
C.H. Chang. 1994. A pilot study on automatic Chinese
spelling error correction. Communication of COLIPS,
4(2):143?149.
Z. Chen and K.F. Lee. 2000. A new statistical ap-
proach to Chinese Pinyin input. In Proceedings of the
489
38th Annual Meeting on Association for Computation-
al Linguistics, pages 241?247.
Q. Chen, M. Li, and M. Zhou. 2007. Improving query
spelling correction using web search results. In Pro-
ceedings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Compu-
tational Natural Language Learning, pages 181?189.
Y. Chen. 1997. Chinese Language Processing. Shanghai
Education publishing company.
W.E. Cooper. 1983. Cognitive aspects of skilled type-
writing. Springer-Verlag.
S. Cucerzan and E. Brill. 2004. Spelling correction as an
iterative process that exploits the collective knowledge
of web users. In Proceedings of the 2004 Conference
on Empirical Methods in Natural Language Process-
ing, pages 293?300.
F.J. Damerau. 1964. A technique for computer detection
and correction of spelling errors. Communications of
the ACM, 7(3):171?176.
J. Gao, X. Li, D. Micol, C. Quirk, and X. Sun. 2010.
A large scale ranker-based system for search query
spelling correction. In Proceedings of the 23rd In-
ternational Conference on Computational Linguistics,
pages 358?366.
G. Hirst and A. Budanitsky. 2005. Correcting real-word
spelling errors by restoring lexical cohesion. Natural
Language Engineering, 11(01):87?111.
A. Islam and D. Inkpen. 2009. Real-word spelling cor-
rection using GoogleWeb 1T 3-grams. In Proceedings
of the 2009 Conference on Empirical Methods in Nat-
ural Language Processing, pages 1241?1249.
M.D. Kernighan, K.W. Church, and W.A. Gale. 1990.
A spelling correction program based on a noisy chan-
nel model. In Proceedings of the 13th conference on
Computational linguistics, pages 205?210.
K. Kukich. 1992. Techniques for automatically cor-
recting words in text. ACM Computing Surveys,
24(4):377?439.
Y.J. Lin and M.S. Yu. 2004. The properties and further
applications of Chinese frequent strings. Computa-
tional Linguistics and Chinese Language Processing,
9(1):113?128.
C.L. Liu, K.W. Tien, M.H. Lai, Y.H. Chuang, and S.H.
Wu. 2009. Capturing errors in written Chinese word-
s. In Proceedings of the Joint Conference of the 47th
Annual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing of
the AFNLP, pages 25?28.
C.L. Liu, M.H. Lai, Y.H. Chuang, and C.Y. Lee. 2010.
Visually and phonologically similar characters in in-
correct simplified chinese words. In Proceedings of
the 23rd International Conference on Computational
Linguistics, pages 739?747.
E.S. Ristad, P.N. Yianilos, M.T. Inc, and NJ Princeton.
1998. Learning string-edit distance. IEEE Transac-
tions on Pattern Analysis and Machine Intelligence,
20(5):522?532.
A. Rozovskaya and D. Roth. 2010. Generating confu-
sion sets for context-sensitive error correction. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing, pages 961?970.
K. Toutanova and R.C. Moore. 2002. Pronunciation
modeling for improved spelling correction. In Pro-
ceedings of the 40th Annual Meeting on Association
for Computational Linguistics, pages 144?151.
J. Wang, S. Zhai, and H. Su. 2001. Chinese input with
keyboard and eye-tracking: an anatomical study. In
Proceedings of the SIGCHI conference on Human fac-
tors in computing systems, pages 349?356.
C. Whitelaw, B. Hutchinson, G.Y. Chung, and G. El-
lis. 2009. Using the web for language independent
spellchecking and autocorrection. In Proceedings of
the 2009 Conference on Empirical Methods in Natural
Language Processing, pages 890?899.
A. Wilcox-O?Hearn, G. Hirst, and A. Budanitsky. 2010.
Real-word spelling correction with trigrams: A recon-
sideration of the Mays, Damerau, and Mercer model.
Computational Linguistics and Intelligent Text Pro-
cessing, pages 605?616.
L. Zhang, M. Zhou, C. Huang, and HH Pan. 1999.
Multifeature-based approach to automatic error detec-
tion and correction of Chinese text. In Proceedings of
the First Workshop on Natural Language Processing
and Neural Networks.
L. Zhang, C. Huang, M. Zhou, and H. Pan. 2000. Auto-
matic detecting/correcting errors in Chinese text by an
approximate word-matching algorithm. In Proceed-
ings of the 38th Annual Meeting on Association for
Computational Linguistics, pages 248?254.
Y. Zheng, Z. Liu, M. Sun, L. Ru, and Y. Zhang. 2009. In-
corporating user behaviors in new word detection. In
Proceedings of the 21st International Joint Conference
on Artificial Intelligence, pages 2101?2106.
Y. Zheng, Z. Liu, and L. Xie. 2010. Growing relat-
ed words from seed via user behaviors: a re-ranking
based approach. In Proceedings of the ACL 2010 Stu-
dent Research Workshop, pages 49?54.
Y. Zheng, C. Li, and M. Sun. 2011a. CHIME: An ef-
ficient error-tolerant chinese pinyin input method. In
Proceedings of the 22nd International Joint Confer-
ence on Artificial Intelligence (accepted).
Y. Zheng, Z. Liu, L. Xie, M. Sun, L. Ru, and Y. Zhang.
2011b. User Behaviors in Related Word Retrieval
and New Word Detection: A Collaborative Perspec-
tive. ACM Transactions on Asian Language Informa-
tion Processing, Special Issue on Chinese Language
Processing (accepted).
490
