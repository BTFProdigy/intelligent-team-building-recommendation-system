Proceedings of NAACL HLT 2009: Short Papers, pages 233?236,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Estimating and Exploiting the Entropy of Sense Distributions
Peng Jin
Institute of Computational Linguistics
Peking University
Beijing China
jandp@pku.edu.cn
Diana McCarthy, Rob Koeling and John Carroll
University of Sussex
Falmer, East Sussex
BN1 9QJ, UK
{dianam,robk,johnca}@sussex.ac.uk
Abstract
Word sense distributions are usually skewed.
Predicting the extent of the skew can help a
word sense disambiguation (WSD) system de-
termine whether to consider evidence from the
local context or apply the simple yet effec-
tive heuristic of using the first (most frequent)
sense. In this paper, we propose a method to
estimate the entropy of a sense distribution to
boost the precision of a first sense heuristic by
restricting its application to words with lower
entropy. We show on two standard datasets
that automatic prediction of entropy can in-
crease the performance of an automatic first
sense heuristic.
1 Introduction
Word sense distributions are typically skewed and
WSD systems do best when they exploit this ten-
dency. This is usually done by estimating the most
frequent sense (MFS) for each word from a training
corpus and using that sense as a back-off strategy for
a word when there is no convincing evidence from
the context. This is known as the MFS heuristic 1
and is very powerful since sense distributions are
usually skewed. The heuristic becomes particularly
hard to beat for words with highly skewed sense dis-
tributions (Yarowsky and Florian, 2002). Although
the MFS can be estimated from tagged corpora, there
are always cases where there is insufficient data, or
where the data is inappropriate, for example because
1It is also referred to as the first sense heuristic in the WSD
literature and in this paper.
it comes from a very different domain. This has mo-
tivated some recent work attempting to estimate the
distributions automatically (McCarthy et al, 2004;
Lapata and Keller, 2007). This paper examines the
case for determining the skew of a word sense distri-
bution by estimating entropy and then using this to
increase the precision of an unsupervised first sense
heuristic by restricting application to those words
where the system can automatically detect that it has
the most chance. We use a method based on that
proposed by McCarthy et al (2004) as this approach
does not require hand-labelled corpora. The method
could easily be adapted to other methods for predic-
ing predominant sense.
2 Method
Given a listing of senses from an inventory, the
method proposed by McCarthy et al (2004) pro-
vides a prevalence ranking score to produce a MFS
heuristic. We make a slight modification to Mc-
Carthy et al?s prevalence score and use it to es-
timate the probability distribution over the senses
of a word. We use the same resources as Mc-
Carthy et al (2004): a distributional similarity the-
saurus and a WordNet semantic similarity measure.
The thesaurus was produced using the metric de-
scribed by Lin (1998) with input from the gram-
matical relation data extracted using the 90 mil-
lion words of written English from the British Na-
tional Corpus (BNC) (Leech, 1992) using the RASP
parser (Briscoe and Carroll, 2002). The thesaurus
consists of entries for each word (w) with the top
50 ?nearest neighbours? to w, where the neighbours
are words ranked by the distributional similarity that
233
they share with w. The WordNet similarity score
is obtained with the jcn measure (Jiang and Con-
rath, 1997) using the WordNet Similarity Package
0.05 (Patwardhan and Pedersen, 2003) and WordNet
version 1.6. The jcn measure needs word frequency
information, which we obtained from the BNC.
2.1 Estimates of Predominance, Probability
and Entropy
Following McCarthy et al (2004), we calculate
prevalence of each sense of the word (w) using a
weighted sum of the distributional similarity scores
of the top 50 neighbours of w. The sense of w that
has the highest value is the automatically detected
MFS (predominant sense). The weights are deter-
mined by the WordNet similarity between the sense
in question and the neighbour. We make a modi-
fication to the original method by multiplying the
weight by the inverse rank of the neighbour from
the list of 50 neighbours. This modification magni-
fies the contribution to each sense depending on the
rank of the neighbour while still allowing a neigh-
bour to contribute to all senses that it relates too.
We verified the effect of this change compared to the
original ranking score by measuring cross-entropy. 2
Let Nw = n1,n2 . . .nk denote the ordered set of the
top k = 50 neighbours of w according to the distri-
butional similarity thesaurus, senses(w) is the set of
senses of w and dss(w,n j) is the distributional sim-
ilarity score of a word w and its jth neighbour. Let
wsi be a sense of w then wnss(wsi,n j) is the maxi-
mum WordNet similarity score between wsi and the
WordNet sense of the neighbour (n j) that maximises
this score. The prevalence score is calculated as fol-
lows with 1rankn j being our modification to McCarthyet al
Prevalence Score(wsi) = ?n j?Nw dss(w,n j)?
wnss(wsi,n j)
?wsi??senses(w)wnss(wsi? ,n j)
? 1rankn j
(1)
To turn this score into a probability estimate we sum
the scores over all senses of a word and the proba-
bility for a sense is the original score divided by this
sum:
2Our modified version of the score gave a lower cross-
entropy with SemCor compared to that in McCarthy et al The
result was highly significant with p < 0.01 on the t-test.
p?(wsi) = prevalence score(wsi)?ws j?w prevalence score(ws j)
(2)
To smooth the data, we evenly distribute 1/10 of the
smallest prevalence score to all senses with a unde-
fined prevalence score values. Entropy is measured
as:
H(senses(w)) =? ?
wsi?senses(w)
p(wsi)log(p(wsi))
using our estimate (p?) for the probability distribu-
tion p over the senses of w.
3 Experiments
We conducted two experiments to evaluate the ben-
efit of using our estimate of entropy to restrict appli-
cation of the MFS heuristic. The two experiments
are conducted on the polysemous nouns in SemCor
and the nouns in the SENSEVAL-2 English all words
task (we will refer to this as SE2-EAW).
3.1 SemCor
For this experiment we used all the polysemous
nouns in Semcor 1.6 (excluding multiwords and
proper nouns). We depart slightly from (McCarthy
et al, 2004) in including all polysemous nouns
whereas they limited the experiment to those with
a frequency in SemCor of 3 or more and where there
is one sense with a higher frequency than the others.
Table 1 shows the precision of finding the predomi-
nant sense using equation 1 with respect to different
entropy thresholds. At each threshold, the MFS in
Semcor provides the upper-bound (UB). The random
baseline (RBL) is computed by selecting one of the
senses of the target word randomly as the predomi-
nant sense. As we hypothesized, precision is higher
when the entropy of the sense distribution is lower,
which is an encouraging result given that the entropy
is automatically estimated. The performance of the
random baseline is higher at lower entropy which
shows that the task is easier and involves a lower de-
gree of polysemy of the target words. However, the
gains over the random baseline are greater at lower
entropy levels indicating that the merits of detect-
ing the skew of the distribution cannot all be due to
lower polysemy levels.
234
H precision #
(?) eq 1 RBL UB tokens
0.5 - - - 0
0.9 80.3 50.0 84.8 466
0.95 85.1 50.0 90.9 1360
1 68.5 50.0 87.4 9874
1.5 67.6 42.6 86.9 11287
2 58.0 36.7 79.5 25997
2.5 55.7 34.4 77.6 31599
3.0 50.2 30.6 73.4 41401
4.0 47.6 28.5 70.8 46987
5.0 (all) 47.3 27.3 70.5 47539
Table 1: First sense heuristic on SemCor
Freq ? P #tokens
1 45.9 1132
5 50.1 5765
10 50.7 10736
100 49.4 39543
1000(all) 47.3 47539
#senses ? P #tokens
2 67.2 10736
5 55.4 31181
8 50.1 41393
12 47.8 46041
30(all) 47.3 47539
Table 2: Precision (P) of equation 1 on SemCor with re-
spect to frequency and polysemy
We also conducted a frequency and polysemy
analysis shown in Table 2 to demonstrate that the
increase in precision is not all due to frequency or
polysemy. This is important, since both frequency
and polysemy level (assuming a predefined sense in-
ventory) could be obtained without the need for au-
tomatic estimation. As we can see, while precision
is higher for lower polysemy, the automatic estimate
of entropy can provide a greater increase in preci-
sion than polysemy, and frequency does not seem to
be strongly correlated with precision.
3.2 SENSEVAL-2 English All Words Dataset
The SE2-EAW task provides a hand-tagged test suite
of 5,000 words of running text from three articles
from the Penn Treebank II (Palmer et al, 2001).
Again, we examine whether precision of the MFS
H precision #
(?) eq 1 RBL SC UB tokens
0.5 - - - - 0
0.9 1 50.0 1 1 7
0.95 94.7 50.0 94.7 1 19
1 69.6 50.0 81.3 94.6 112
1.5 68.0 49.0 81.3 93.8 128
2 69.6 34.7 68.2 87.7 421
2.5 65.0 33.0 65.0 86.5 488
3.0 56.6 27.5 60.8 80.1 687
4.0 52.6 25.6 58.8 79.2 766
5.0 (all) 51.5 25.6 58.5 79.3 769
Table 3: First sense heuristic on SE2-EAW
heuristic can be increased by restricting application
depending on entropy. We use the same resources as
for the SemCor experiment. 3 Table 3 gives the re-
sults. The most frequent sense (MFS) from SE2-EAW
itself provides the upper-bound (UB). We also com-
pare performance with the Semcor MFS (SC). Per-
formance is close to the Semcor MFS while not re-
lying on any manual tagging. As before, precision
increases significantly for words with low estimated
entropy, and the gains over the random baseline are
higher compared to the gains including all words.
4 Related Work
There is promising related work on determining the
predominant sense for a MFS heuristic (Lapata and
Keller, 2007; Mohammad and Hirst, 2006) but our
work is the first to use the ranking score to estimate
entropy and apply it to determine the confidence in
the MFS heuristic. It is likely that these methods
would also have increased precision if the ranking
scores were used to estimate entropy. We leave such
investigations for further work.
Chan and Ng (2005) estimate word sense distri-
butions and demonstrate that sense distribution esti-
mation improves a supervised WSD classifier. They
use three sense distribution methods, including that
of McCarthy et al (2004). While the other two
methods outperform the McCarthy et al method,
3We also used a tool for mapping from WordNet 1.7 to
WordNet 1.6 (Daude? et al, 2000) to map the SE2-EAW noun
data (originally distributed with 1.7 sense numbers) to 1.6 sense
numbers.
235
they rely on parallel training data and are not appli-
cable on 9.6% of the test data for which there are
no training examples. Our method does not require
parallel training data.
Agirre and Mart??nez (2004) show that sense dis-
tribution estimation is very important for both super-
vised and unsupervised WSD. They acquire tagged
examples on a large scale by querying Google with
monosemous synonyms of the word senses in ques-
tion. They show that the method of McCarthy et
al. (2004) can be used to produce a better sampling
technique than relying on the bias from web data
or randomly selecting the same number of exam-
ples for each sense. Our work similarly shows that
the automatic MFS is an unsupervised alternative to
SemCor but our work does not focus on sampling
but on an estimation of confidence in an automatic
MFS heuristic.
5 Conclusions
We demonstrate that our variation of the McCarthy
et al (2004) method for finding a MFS heuristic can
be used for estimating the entropy of a sense dis-
tribution which can be exploited to boost precision.
Words which are estimated as having lower entropy
in general get higher precision. This suggests that
automatic estimation of entropy is a good criterion
for getting higher precision. This is in agreement
with Kilgarriff and Rosenzweig (2000) who demon-
strate that entropy is a good measure of the difficulty
of WSD tasks, though their measure of entropy was
taken from the gold-standard distribution itself.
As future work, we want to compare this approach
of estimating entropy with other methods for es-
timating sense distributions which do not require
hand-labelled data or parallel texts. Currently, we
disregard local context. We wish to couple the con-
fidence in the MFS with contextual evidence and in-
vestigate application on coarse-grained datasets.
Acknowledgements
This work was funded by the China Scholarship Council,
the National Grant Fundamental Research 973 Program
of China: Grant No. 2004CB318102, the UK EPSRC
project EP/C537262 ?Ranking Word Senses for Disam-
biguation?, and a UK Royal Society Dorothy Hodgkin
Fellowship to the second author.
References
E. Agirre and D. Mart??nez. 2004. Unsupervised wsd
based on automatically retrieved examples: The im-
portance of bias. In Proceedings of EMNLP-2004,
pages 25?32, Barcelona, Spain.
E. Briscoe and J. Carroll. 2002. Robust accurate sta-
tistical annotation of general text. In Proceedings of
LREC-2002, pages 1499?1504, Las Palmas, Canary
Islands, Spain.
Y.S. Chan and H.T. Ng. 2005. Word sense disambigua-
tion with distribution estimation. In Proceedings of
IJCAI 2005, pages 1010?1015, Edinburgh, Scotland.
J. Daude?, L. Padro?, and G. Rigau. 2000. Mapping word-
nets using structural information. In Proceedings of
the 38th Annual Meeting of the Association for Com-
putational Linguistics, Hong Kong.
J. Jiang and D. Conrath. 1997. Semantic similarity based
on corpus statistics and lexical taxonomy. In Interna-
tional Conference on Research in Computational Lin-
guistics, Taiwan.
A. Kilgarriff and J. Rosenzweig. 2000. Framework and
results for english SENSEVAL. Computers and the
Humanities. Senseval Special Issue, 34(1?2):15?48.
M. Lapata and F. Keller. 2007. An information retrieval
approach to sense ranking. In Proceedings of NAACL-
2007, pages 348?355, Rochester.
G. Leech. 1992. 100 million words of English:
the British National Corpus. Language Research,
28(1):1?13.
D. Lin. 1998. Automatic retrieval and clustering of sim-
ilar words. In Proceedings of COLING-ACL 98, Mon-
treal, Canada.
D. McCarthy, R. Koeling, J. Weeds, and J. Carroll. 2004.
Finding predominant senses in untagged text. In Pro-
ceedings of ACL-2004, pages 280?287, Barcelona,
Spain.
S. Mohammad and G. Hirst. 2006. Determining word
sense dominance using a thesauru s. In Proceedings of
EACL-2006, pages 121?128, Trento, Italy.
M. Palmer, C. Fellbaum, S. Cotton, L. Delfs, and
H. Trang Dang. 2001. English tasks: All-words and
verb lexical sample. In Proceedings of the SENSEVAL-
2 workshop, pages 21?24.
S. Patwardhan and T. Pedersen. 2003. The
wordnet::similarity package. http://wn-
similarity.sourceforge.net/.
D. Yarowsky and R. Florian. 2002. Evaluating sense
disambiguation performance across diverse parame-
ter spaces. Natural Language Engineering, 8(4):293?
310.
236
Proceedings of the Linguistic Annotation Workshop, pages 125?131,
Prague, June 2007. c?2007 Association for Computational Linguistics
Building Chinese Sense Annotated Corpus  
with the Help of Software Tools 
Yunfang Wu 
School of Electronic Engineering and 
Computer Science, Peking University, 
Beijing 100871 
wuyf@pku.edu.cn 
Peng Jin 
School of Electronic Engineering and 
Computer Science, Peking University, 
Beijing 100871 
jandp@pku.edu.cn 
Tao Guo 
School of Electronic Engineering and 
Computer Science, Peking University, 
Beijing 100871 
gtwcq@pku.edu.cn 
Shiwen Yu 
School of Electronic Engineering and 
Computer Science, Peking University, 
Beijing 100871 
yusw@pku.edu.cn 
 
 
Abstract 
This paper presents the building procedure 
of a Chinese sense annotated corpus. A set 
of software tools is designed to help hu-
man annotator to accelerate the annotation 
speed and keep the consistency. The soft-
ware tools include 1) a tagger for word 
segmentation and POS tagging, 2) an an-
notating interface responsible for the sense 
describing in the lexicon and sense anno-
tating in the corpus, 3) a checker for con-
sistency keeping, 4) a transformer respon-
sible for the transforming from text file to 
XML format, and 5) a counter for sense 
frequency distribution calculating. 
1 Introduction 
There is a strong need for a large-scale Chinese 
corpus annotated with word senses both for word 
sense disambiguation (WSD) and linguistic re-
search. Although much research has been carried 
out, there is still a long way to go for WSD tech-
niques to meet the requirements of practical NLP 
programs such as machine translation and infor-
mation retrieval. It was argued that no fundamen-
tal progress in WSD could be made until large-
scale lexical resources were built (Veronis, 2003). 
In English a word sense annotated corpus SEM-
COR (Semantic Concordances) (Landes et al, 
1999) has been built, which was later trained and 
tested by many WSD systems and stimulated large 
amounts of WSD work. In Japanese the Hinoki 
Sensebank is constructed (Tanaka et al, 2006). In 
the field of Chinese corpus construction, plenty of 
attention has been paid to POS tagging and syn-
tactic structures bracketing, for instance the Penn 
Chinese Treebank (Xue et al, 2002) and Sinica 
Corpus (Huang et al, 1992), but very limited 
work has been done with semantic knowledge 
annotation. Huang et al (2004) introduced the 
Sinica sense-based lexical knowledge base, but as 
is well known, Chinese pervasive in Taiwan is not 
the same as mandarin Chinese. SENSEVAL-3 
provides a Chinese word sense annotated corpus, 
which contains 20 words and 15 sentences per 
meaning for most words, but obviously the data is 
too limited to achieve wide coverage, high accu-
racy WSD systems. 
This paper is devoted to building a large-scale 
Chinese corpus annotated with word senses. A 
small part of the Chinese sense annotated corpus 
has been adopted as one of the SemEval-2007 
tasks namely ?Multilingual Chinese-English Lexi-
cal Sample Task? This paper concentrates on the 
description of the manually annotating schemes 
125
with the help of software tools. The software tools 
will help human annotators mainly in the two as-
pects: 1) Reduce the labor time and accelerate the 
speed; 2) Keep the inter-annotator agreement. The 
overall procedure along with the software tools is 
illustrated in figure 1.
.
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
            Preprocessing                                                                                 
Tagger: word segmentation and POS tagging
 
 
Annotating interface: word sense annotating
 
 
 
 
Checker: consistency checking 
 
 
 
 
Word sense annotated corpus
 
 
 
 
 
                                                                                                                                                      Postprocessing
Transformer: XML format transforming Counter: sense frequency distribution calculating
 
This paper is so organized as follows. In section 
2 the preprocessing stage (word segmentation and 
POS tagging) is discussed. Then in section 3 the 
annotating scheme and the annotating interface 
are demonstrated in detail. The strategy to keep 
consistency is addressed in section 4. And then in 
section 5 and 6 the two postprocessing stages are 
respectively presented. Finally in section 7 con-
clusions are drawn and future works are presented. 
2 Word segmentation and POS tagging 
The input data for word sense annotating is firstly 
word segmented and POS tagged using Peking 
University?s POS tagger (Yu et al, 2003). The 
POS tagging precision is up to 97.5%, which lays 
a sound foundation for researches on sense anno-
tating. This is actually to make use of the full-
fledged syntactic processing techniques to deal 
with the semantic annotation problems. Different 
senses of one ambiguous word sometimes behave 
so differently that they bear different POS tags. 
Take ???/hold? in sentence (1) as an example. 
The noun of ???/hold? means ?confidence?, but 
the verb means ?grasp?.  
(1) a ?(have)  ??/n(confidence)  
b ??/v(grasp)  ?(ZHU)  ??(chance) 
Due to the unique characteristic of Chinese lan-
guage that lacks word inflection, the ambiguous 
words with different POSs are very common. Ac-
cording to the research of Li (1999), after POS 
tagging the ratio of ambiguous word occurrences 
in the text of People?s Daily is reduced from 42% 
to 26%. Therefore the emphasis of manually sense 
annotating in this paper falls on the ambiguous 
words with the same part of speech. This will in 
turn save 16% of the annotation effort compared 
with the sense annotating before the preprocessing 
of POS tagging. 
Fig.1.The overall procedure along with the software tools 
3 Word sense annotating 
The resulting lexical knowledge base in this pro-
ject will contain three major components: 1) a 
corpus annotated with Chinese word senses 
namely Chinese Senses Pool (CSP); 2) a lexicon 
containing sense distinction and description 
namely Chinese Semantic Dictionary (CSD); 3) 
the linking between the CSD and the Chinese 
Concept Dictionary (CCD) (Liu et al, 2002). The 
corpus CSP, the lexicon CSD and CCD constitute 
a highly relational and tightly integrated system: 1) 
In CSD the sense distinctions are described rely-
ing on the corpus; 2) In CSP the word occurrences 
are assigned sense tags according to the sense en-
126
try specified in CSD; 3) The linking between the 
sense entry in CSD and CCD synsets are estab-
lished. The dynamic model is shown in figure 2. A 
software tool is developed in Java to be used as 
the word sense annotating interface (figure 3), 
which embodies the spirit of the dynamic model 
properly.
  
. 
 
 
 
 
 
 
 
 
             
 
 
 
 
 
 
 
      
 
 
 
 
 
 
 
3.1 Sense describing in the lexicon and sense 
annotating in the corpus 
In this project the lexicon CSD containing sense 
descriptions and the corpus CSP annotated with 
senses are built interactively, simultaneously and 
dynamically. On one hand, the sense distinctions in 
the lexicon are made relying heavily on the corpus 
usage. On the other hand, using the sense informa-
tion specified in the lexicon the human annotators 
assign semantic tags to all the instances of the 
word in a corpus.  
In the word sense annotating interface, the sen-
tences from CSP containing the target ambiguous 
words are displayed in the upper section, and the 
word senses with feature-based description from 
CSD are displayed in the bottom section. 
Through reading the context in the corpus, the 
human annotator decides to add or delete or edit a 
sense entry in the lexicon. The default value of the 
range of the context is within a sentence, and the 
surrounding characters in the left and right of the 
target word can be specified by the annotator. An-
notators can do four kinds of operations in CSD: 1) 
Add a sense entry and then fill in all the features; 2) 
Delete a sense entry along with all its feature de-
scription; 3) Edit a sense entry and change any of 
the features; 4) Select a sample sentence form the 
CSP and add it to the lexicon in the corresponding 
sense entry. 
        
 
interactive construction 
 
linking 
 
indirect relation  
Corpus 
CSP 
CCD Lexicon 
CSD 
Fig 2. The dynamic model between the CSP, CSD and CCD 
Fig3. The word sense annotating interface 
127
According to the sense specification in CSD the 
human annotator assigns semantic tags to the word 
occurrences in CSP. The operation is quite easy. 
When the annotator double clicks the appropriate 
sense entry in CSD the sense tag is automatically 
added to the target word.  
The notable feature in this word sense annotat-
ing interface is that it provides flexible searching 
schemes. 1) Search sequentially (forward or back-
ward) all the instances of an ambiguous words re-
gardless of the annotating state; 2) Search sequen-
tially (forward or backward) the already annotated 
instances; 3) Search sequentially (forward or back-
ward) the yet un-annotated instances and 4) Search 
the instances of a specific ambiguous word (the 
window named Find/Replace in figure3, and again 
is shown in figure 4 for clearness). 
The tool of Find/Replace is widely used in this 
project and has proven to be effective in annotating 
word senses. It allows the annotator to search for a 
specific word to finish tagging all its occurrences 
in the same period of time rather than move se-
quentially through the text. The consistency is 
more easily kept when the annotator manages 
many different instances of the same word than 
handle a few occurrences of many different words 
in a specific time frame, because the former 
method enables the annotator to establish an inte-
grative knowledge system about a specific word 
and its sense distinction. Also the tool of 
Find/Replace provides flexible searching schemes 
for a specific ambiguous word. For instance, 
search in the corpus with different directions (for-
ward/backward) and search with different annotat-
ing states (annotated/un-annotated/both). Using the 
tool the annotator can also replace some specific 
word occurrences in the corpus (often with special 
POS tags) with a sense tag, thus can finish annotat-
ing the corpus quickly and with a batch method. 
For instance the POS tag of ?vq? (means verb 
complement) often uniquely corresponds to a spe-
cific verb sense such as ??/vq??/vq!8?. 
There is the status bar in the bottom line of the 
word sense annotating interface, and there clearly 
show the annotating status: the total word occur-
rences, the serial number of the current processing 
instance and the number of the already annotated 
instances.  
 
 
Fig.4  The tool of Find/Replace 
 
3.2 Linking between CSD and CCD 
The feature-based description of word meanings in 
CSD describes mainly the syntagmatic information, 
such as the subcategory frames of verbs, the se-
mantic categories of the head noun of adjectives, 
but cannot include the paradigmatic relations. 
WordNet is a popular open resource and has been 
widely experimented in WSD researches. Chinese 
Concept Dictionary (CCD) is a WordNet-like Chi-
nese lexicon (Liu et al, 2002), which carries the 
main relations defined in WordNet and can be seen 
as a bilingual concept lexicon with the parallel 
Chinese-English concepts to be simultaneously 
included. So the linking between the sense entries 
in CSD and the synsets in CCD is tried to establish 
in this project. After the linking has been estab-
lished, the paradigmatic relations (such as hy-
pernym / hyponym, meronym / holonym) ex-
pressed in CCD can map automatically to the sense 
entry in CSD. What?s more, the many existing 
WSD approaches based on WordNet can be trained 
and tested on the Chinese sense tagged corpus. 
In the right section of the word sense annotating 
interface there displays the synset information 
from CCD. When coping with a specific ambigu-
ous word (such as ??/open?) in CSD, the linking 
between CSD and CCD is automatically estab-
lished with the word itself (??/open?) as the pri-
mary key. And then all the synsets of the word 
(??/open?) in CCD, along with the hypernyms of 
each sense (expressed by the first word in a synset), 
are displayed in the right section. A synset selec-
tion window (namely Set synsets) containing the 
offset numbers of the synsets then appears in the 
right section. The annotator clicks on the appropri-
ate box(es) before the corresponding offset number 
and then the offset number is automatically added 
128
to the feature ?CCD? in the currently selected 
sense entry in CSD. 
The linking is now done manually. Unfortu-
nately some of the ambiguous words existing in 
CSD are not included in CCD. This also provides a 
good way to improve the coverage and quality of 
CCD.  
4 Consistency Checking 
Consistency is always an important concern for 
hand-annotated corpus, and is even critical for the 
sense tagged corpus due to the subtle meanings to 
handle. A software tool namely Sense Consistency 
Checker is developed in the checking procedure. 
The checker extracts all the instances of a specific 
ambiguous word into a checking file with the for-
mat of the sense concordances (as shown in figure 
5 ). The checking file enables the checker to have a 
closer examination of how the senses are used and 
distributed, and to form a general view of how the 
sense distinctions are made. The inter-annotator in-
agreement thus can be reached quickly and cor-
rectly. As illustrated in figure 5, it is obviously an 
error to assign the same semantic tag to ??/drive 
??/car? and ???/meeting ?/held?. Simply as 
it is the checker greatly accelerates the checking 
speed and improve the consistency. 
 
 
Fig. 5. Some example sentences in the checking file of ??/open? 
 
Together five researchers took part in the anno-
tation, of which three are majored in linguistics 
and two are majored in computational linguistics. 
In this project the annotators are also checkers, 
who check other annotators? work. A text gener-
ally is first tagged by one annotator and then veri-
fied by two checkers. 
After the preprocessing of word segmentation 
and Pos tagging, the word sense annotating and 
the consistency checking, the Chinese word sense 
annotated corpus is constructed. And then other 
software tools are needed to do further processing 
in the sense annotated corpus. 
5 XML format transforming 
The original format of the Chinese sense anno-
tated corpus is in text file as shown in figure 6. In 
the text file the sign following ?/? denotes the 
POS tag, and the number following ?!? indicates 
the sense ID. The text file complies with the other 
language resources at the Institute of Computa-
tional Linguistics, Peking University, which pro-
vides a quite easy way to make full use of the ex-
isting resources and techniques at ICL/PKU when 
constructing the sense annotated corpus.  
At the same time in order to exchange and 
share information easily with other language re-
sources in the world, a software tool namely Text-
to-XML Transformer is developed to change the 
text to XML format (as shown in figure 7). In the 
XML file, the item ?pos? denotes the POS tag of 
the word, and the item ?senseid? denotes sense ID 
of the ambiguous word. 
Thus there are two kinds of format for the Chi-
nese sense annotated corpus, each of which has its 
advantages and can be adopted to meet different 
requirements in different situations. 
 
129
 
 
 
 
 
Fig. 6. The sense annotated corpus in text file 
??/a  ?/u  ??/vn  ?/vt!2  ??/b  ??/n  ?/p  ?/m  ?/q!1  ?/r2  ??/n  ??/n  ??/vi  ?/u  ??/d  ??/a  
?/u  ??/n  ?/w  ??/vn  ??/n  ??/d  ??/vt  ?/w  ??/t  ???/n  ?/r  ?/q  ??/vn  ??/n  ?/d  ?/vt!3  ?
/v  9000/m  ?/m  ?/q ?/w 
 
<head date="20000201" page="01" articleno="003" passageno="019"> 
<passage> 
????????????????????????????????????????????????? 
??? 9000?? 
</passage> 
<postagging> 
<word id="0" pos="a" senseid=""> 
<token>??</token> 
</word> 
<word id="1" pos="u" senseid=""> 
<token>?</token> 
</word> 
<word id="2" pos="vn" senseid=""> 
<token>??</token> 
</word> 
<word id="3" pos="vt" senseid="2"> 
<token>?</token> 
</word> 
??   ?? 
 
Fig. 7. The sense annotated corpus in XML format 
 
6 Sense frequency calculating 
Word sense frequency distribution in the real texts 
is a vital kind of information both for the algo-
rithms of word sense disambiguation and for the 
research on lexical semantics. In the postprocess-
ing stage a software tool namely Sense Frequency 
Counter is developed to make statistics on the 
sense frequency distribution. Quite valuable in-
formation can be acquired through the counter 
based on the sense annotated corpus: 1) The 
amount of all the instances of an ambiguous word; 
2) The number of the already annotated instances; 
3) The occurrence of each sense of an ambiguous 
word and 4) The sense frequency. Table 1 illus-
trates the sense frequency distribution of ambigu-
ous verb ??/open? in 10 day?s People?s Daily. 
7 Conclusions 
This paper describes the overall building proce-
dure of a Chinese sense annotated corpus. The 
corpus is firstly word segmented and POS tagging 
using Peking University?s tagger in the preproc-
essing stage. Then the lexicon Chinese Semantic 
Dictionary (CSD) containing sense descriptions 
and the corpus Chinese Senses Pool (CSP) anno-
tated with senses are built interactively, simulta-
neously and dynamically using the word sense 
annotating interface. At the same time the linking 
between the sense entries in CSD and the synsets 
in Chinese Concept Dictionary (CCD) are manu-
ally established. And then the Sense Consistency 
Checker is used to keep the inter-annotator 
agreement. Finally two software tools are devel-
oped to do further processing based on the sense 
annotated corpus. A software tool namely Text-to-
XML Transformer is developed to change the text 
to XML format, and the Sense Frequency Counter 
is developed to make statistics on the sense fre-
quency distribution. The annotation schemes and 
all the software tools have been experimented in 
building the SemEval-2007 task 5 ?Multilingual 
Chinese-English Lexical Sample Task?, and have 
proven to be effective. 
 
 
130
Table 1 the sense frequency distribution of ambiguous verb ??/open? 
Ambiguous verbs Sense ID Occurrences Frequency(%) 
? 8 30 32.26
? 4 13 13.98
? 6 12 12.90
? 7 8 8.60
? 0 6 6.45
? 1 6 6.45
? 9 4 4.30
? 12 4 4.30
? 11 3 3.23
? 2 3 3.23
? 10 3 3.23
? 14 1 1.08
? 15 0 0.00
? 3 0 0.00
? 5 0 0.00
? 13 0 0.00
Acknowledgments. This research is supported by 
Humanity and Social Science Research Project of 
China State Education Ministry (No. 06JC740001) 
and National Basic Research Program of China 
(No. 2004CB318102). 
References 
Huang, Ch. R and Chen, K. J. 1992. A Chinese Corpus 
for Linguistics Research. In Proceedings of COL-
ING-1992. 
Huang, Ch. R., Chen, Ch. L., Weng C. X. and Chen. K. 
J. 2004. The Sinica Sense Management System: De-
sign and Implementation. In Recent advancement in 
Chinese lexical semantics. 
Landes, S., Leacock, C. and Tengi, R. 1999. Building 
Semantic Concordances. In Christiane Fellbaum 
(Ed.) WordNet: an Electronic Lexical Database. 
MIT Press, Cambridge. 
Li, J. 1999. The research on Chinese word sense dis-
ambiguation. Doctoral dissertation in computer sci-
ence department of Tsinghua University. 
Liu, Y., Yu, S. W. and Yu, J.S. 2002. Building a Bilin-
gual WordNet-like Lexicon: the New Approach and 
Algorithms. In Proceedings of COLING 2002.  
Tanaka, T., Bond F. and Fujita, S. 2006. The Hinoki 
Sensebank----A large-scale word sense tagged cor-
pus of Japanese. In Proceedings of the Workshop on 
Frontiers in Linguistically Annotated Corpora 2006. 
Veronis, J. 2003. Sense Tagging: Does It Make Sense? 
In Wilson et al (Eds). Corpus Linguistics by the 
Rule: a Festschrift for Geoffrey Leech.  
Xue, N., Chiou, F. D. and Palmer, M. 2002. Building a 
Large-Scale Annotated Chinese Corpus. In Proceed-
ings of COLING 2002. 
Yu, S. W., Duan, H. M., Zhu, X. F., Swen, B. and 
Chang, B. B. 2003. Specification for Corpus Proc-
essing at Peking University: Word Segmentation, 
POS tagging and Phonetic Notation. Journal of Chi-
nese Language and Computing. 
 
131
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 19?23,
Prague, June 2007. c?2007 Association for Computational Linguistics
SemEval-2007 Task 5: Multilingual Chinese-English Lexical Sample 
Peng Jin, Yunfang Wu and Shiwen Yu 
Institute of Computational Linguistics  
 Peking University, Beijing China 
{jandp, wuyf, yusw}@pku.edu.cn 
 
 
Abstract 
The Multilingual Chinese-English lexical 
sample task at SemEval-2007 provides a 
framework to evaluate Chinese word sense 
disambiguation and to promote research. 
This paper reports on the task preparation 
and the results of six participants. 
1 Introduction 
The Multilingual Chinese-English lexical sample 
task is designed following the leading ideas of the 
Senseval-3 Multilingual English-Hindi lexical 
sample task (Chklovski et al, 2004). The ?sense 
tags? for the ambiguous Chinese target words are 
given in the form of their English translations. 
The data preparation is introduced in the second 
section. And then the participating systems are 
briefly described and their scores are listed.  
In the conclusions we bring forward some sug-
gestion for the next campaign. 
2 Chinese Word Sense Annotated Corpus 
All the training and test data come from the 
People?s Daily in January, February and March of 
2000. The People?s Daily is the most popular 
newspaper in China and is open domain. Before 
manually sense annotating, the texts have been 
word-segmented and part of speech (PoS) tagged 
according to the PoS tagging scheme of Institute of 
Computational Linguistics in Peking University 
(ICL/PKU). The corpus had been used as one of 
the gold-standard data set for the second 
international Chinese word segmentation bakeoff 
in 2005.1
2.1 Manual Annotation 
The sense annotated corpus is manually con-
structed with the help of a word sense annotating 
interface developed in Java. Three native annota-
tors, two major in Chinese linguistics and one ma-
jor in computer science took part in the construc-
tion of the sense-annotated corpus. A text generally 
is first annotated by one annotator and then veri-
fied by two checkers. Checking is of course a nec-
essary procedure to keep the consistency. Inspired 
by the observation that checking all the instances 
of a word in a specific time frame will greatly im-
prove the precision and accelerate the speed, a 
software tool is designed in Java to gather all the 
occurrences of a word in the corpus into a check-
ing file with the sense KWIC (Key Word in Con-
text) format in sense tags order. The inter-
annotator agreement gets to 84.8% according to 
Wu. et al (2006). 
The sense entries are specified in the Chinese 
Semantic Dictionary (CSD) developed by 
ICL/PKU. The sense distinctions are made mainly 
according to the Contemporary Chinese Dictionary, 
the most widely used dictionary in mandarin Chi-
nese, with necessary adjustment and improvement 
is implemented according to words usage in real 
texts. Word senses are described using the feature-
based formalism.  The features, which appear in 
the form ?Attribute =Value?, can incorporate ex-
tensive distributional information about a word 
sense. The feature set constitutes the representation 
of a sense, while the verbal definitions of meaning 
                                                 
1 http://sighan.cs.uchicago.edu/bakeoff2005/ 
19
serve only as references for human use. The Eng-
lish translation is assigned to each sense in the at-
tribute ?English translation? in CSD. 
Based on the sense-annotated corpus, a sense is 
replaced by its English translation, which might 
group different senses together under the same 
English word. 
2.2 Instances selection 
In this task together 40 Chinese ambiguous words: 
19 nouns and 21 verbs are selected for the evalua-
tion. Each sense of one word is provided at least 15 
instances and at most 40 instances, in which 
around 2/3 is used as the training data and 1/3 as 
the test data. Table 1 presents the number of words 
under each part of speech, the average number of 
senses for each PoS and the number of instances 
respectively in the training and test set. 
 
 # Average 
senses 
# training 
instances 
# test 
instances
19 
nouns 
2.58 1019 364 
21 
verbs 
3.57 1667 571 
 
Table 1: Summary of the sense inventory and 
number of training data and test set 
 
In order to escape from the sense-skewed distri-
bution that really exists in the corpus of People?s 
Daily, many instances of some senses have been 
removed from the sense annotated corpus. So the 
sense distribution of the ambiguous words in this 
task does not reflect the usages in real texts. 
3 Participating Systems 
In order to facilitate participators to select the fea-
tures, we gave a specification for the PoS-tag set. 
Both word-segmented and un-segmented context 
are provided. 
Two kinds of precisions are evaluated. One is 
micro-average: 
 
??
==
=
N
i
i
N
i
imir nmP
11
/  
 
N  is the number of all target word-types. is 
the number of labeled correctly to one specific tar-
get word-type and  is the number of all test in-
stances for this word-type. 
im
in
The other is macro-average: 
 
?
=
=
N
i
imar NpP
1
/ ,  iii nmp /=
 
All teams attempted all test instances. So the re-
call is the same with the precision. The precision 
baseline is obtained by the most frequent sense. 
Because the corpus is not reflected the real usage, 
the precision is very low. 
Six teams participated in this word sense disam-
biguation task. Four of them used supervised learn-
ing algorithms and two used un-supervised method. 
For each team two kinds of precision are given as 
in table 2.  
 
Team Micro-average Macro-average
SRCB-WSD 0.716578 0.749236 
I2R 0.712299 0.746824 
CITYU-HIF 0.710160 0.748761 
SWAT 0.657754 0.692487 
TorMd 0.375401 0.431243 
HIT 0.336898 0.395993 
baseline 0.4053 0.4618 
 
Table 2: The scores of all participating systems 
 
As follow the participating systems are briefly 
introduced. 
SRCB-WSD system exploited maximum entropy 
model as the classifier from OpenNLP2 The fol-
lowing features are used in this WSD system: 
 
? All the verbs and nouns in the context, that is, 
the words with tags ?n, nr, ns, nt, nz, v, vd, vn?  
? PoS of the left word and the right word 
?noun phrase, verb phrase, adjective phrase, 
time phrase, place phrase and quantity phrase. 
These phrases are considered as constituents of 
context, as well as words and punctuations which 
do not belong to any phrase.  
?the type of these phrases which are around the 
target phrases   
                                                 
2 http:// maxent.sourceforge.net/ 
20
? word category information comes from Chi-
nese thesaurus 
 
I2R system used a semi-supervised classification 
algorithm (label propagation algorithm) (Niu, et al, 
2005). They used three types of features: PoS of 
neighboring words with position information, un-
ordered single words in topical context, and local 
collocations.  
In the label propagation algorithm (LP) (Zhu 
and Ghahramani, 2002), label information of any 
vertex in a graph is propagated to nearby vertices 
through weighted edges until a global stable stage 
is achieved. Larger edge weights allow labels to 
travel through easier. Thus the closer the examples, 
the more likely they have similar labels (the global 
consistency assumption). In label propagation 
process, the soft label of each initial labeled exam-
ple is clamped in each iteration to replenish label 
sources from these labeled data. Thus the labeled 
data act like sources to push out labels through 
unlabeled data. With this push from labeled exam-
ples, the class boundaries will be pushed through 
edges with large weights and settle in gaps along 
edges with small weights. If the data structure fits 
the classification goal, then LP algorithm can use 
these unlabeled data to help learning classification 
plane. 
CITYU-HIF system was a fully supervised one 
based on a Na?ve Bayes classifier with simple fea-
ture selection for each target word.  The features 
used are as follows: 
 
? Local features at specified positions: 
PoS of word at w-2, w-1, w1, w2
Word at w-2, w-1, w1, w2
? Topical features within a given window: 
Content words appearing within w-10 to w10
? Syntactic features: 
PoS bi-gram at w-2w0 , w-1w0 , w0w1 , w0w2
PoS tri-gram at w-2 w-1w0 and w0w1w2
 
One characteristic of this system is the incorpo-
ration of the intrinsic nature of each target word in 
disambiguation. It is assumed that WSD is highly 
lexically sensitive and each word is best character-
ized by different lexical information. Human 
judged to consider for each target word the type of 
disambiguation information if they found useful.  
During disambiguation, they run two Na?ve Bayes 
classifiers, one on all features above, and the other 
only on the type of information deemed useful by 
the human judges. When the probability of the best 
guess from the former is under a certain threshold, 
the best guess from the latter was used instead.  
SWAT system uses a weighted vote from three 
different classifiers to make the prediction. The 
three systems are: a Na?ve Bayes classifier that 
compares similarities based on Bayes' Rule, a clas-
sifier that creates a decision list of context features, 
and a classifier that compares the angles between 
vectors of the features found most commonly with 
each sense. The features include bigrams, and tri-
grams, and unigrams are weighted by distance 
from the ambiguous word. 
TorMd used an unsupervised naive Bayes classi-
fier. They combine Chinese text and an English 
thesaurus to create a `Chinese word'--`English 
category' co-occurrence matrix. This system gener-
ated the prior-probabilities and likelihoods of a 
Na?ve Bayes word sense classifier not from sense-
annotated (in this case English translation anno-
tated) data, but from this word--category co-
occurrence matrix. They used the Macquarie The-
saurus as very coarse sense inventory. 
They asked a native speaker of Chinese to map 
the English translations of the target words to ap-
propriate thesaurus categories. Once the Na?ve 
Bayes classifier identifies a particular category as 
the intended sense, the mapping file is used to label 
the target word with the corresponding English 
translation. They rely simply on the bag of words 
that co-occur with the target word (window size of 
5 words on either side). 
HIT is a fully unsupervised WSD system, which 
puts bag of words of Chinese sentences and the 
English translations of target ambiguous word to 
search engine (Google and Baidu). Then they 
could get al kinds of statistic data. The correct 
translation was found through comparing their 
cross entropy. 
4 Conclusion 
The goal of this task is to create a framework to 
evaluate Chinese word sense disambiguation and 
to promote research. 
21
 
Scores Target 
Word 
Sen
se # 
Train
ing # 
Test 
# 
Base-
line SRCB
-WSD
I2R CITY
U-HIF
SWA
T-MP
TOR
MD 
HIT 
? 3 63 20 .50 .70 .80 .75 .75 .55 .55 
?? 3 73 27 .370 .778 .815 .741 .778 .481 .407 
? 4 69 23 .435 .696 .609 .696 .696 .174 .174 
? 9 222 77 .130 .506 .506 .481 .532 .169 .091 
? 8 197 67 .150 .567 .552 .537 .433 .119 .104 
? 4 58 20 .50 .60 .50 .55 .60 .30 .30 
?? 2 47 16 .625 .875 .875 .875 .563 .50 .438 
? 5 105 36 .278 .694 .667 .611 .889 .25 .139 
? 3 56 18 .50 .667 .722 .667 .667 .389 .333 
? 4 106 39 .256 .718 .615 .641 .538 .256 .256 
? 5 132 44 .227 .659 .75 .727 .568 .25 .114 
?? 2 56 20 .50 .90 .95 .95 .60 .50 .50 
? 4 103 34 .294 .765 .706 .765 .559 .294 .294 
?? 2 20 8 .50 .75 .75 .75 .625 .375 .50 
? 2 46 16 .625 .938 .813 .813 .875 .563 .438 
?? 2 60 18 .556 .667 .722 .778 .722 .444 .556 
? 2 40 14 .429 .571 .643 .571 .571 .143 .286 
?? 2 29 10 .60 .80 .70 .90 .80 .30 .30 
? 2 37 13 .769 .769 .769 .769 .769 .462 .462 
? 4 110 37 .270 .730 .676 .676 .541 .216 .216 
?? 2 38 14 .714 .930 1.0 .929 .786 .714 .571 
Ave. 3.5
7 
1667 571 .342/ 
.44 
.685/   
.728 
.676/   
.721 
.671/   
.723 
.618/   
.66 
.30/     
.355 
.263/   
.335 
 
 Table 3: Performance on verbs. Micro / macro average precisions are spitted by ?/? at the last row. 
 
Together six teams participate in this WSD task, 
four of them adopt supervised learning methods 
and two of them used unsupervised algorithms. All 
of the four supervised learning systems exceed ob-
viously the baseline obtained by the most frequent 
sense. It is noted that the performances of the first 
three systems are very close. Two unsupervised 
methods? scores are below the baseline. More 
unlabeled data maybe improve their performance.  
Although the SRCB-WSD system got the high-
est scores among the six participants, it does not 
perform always better than other system from table 
2 and table 3. But to each word, the four super-
vised systems always predict correctly more in-
stances than the two un-supervised systems.  
Besides the corpus, we provide a specification of 
the PoS tag set. Only SRCB-WSD system utilized 
this knowledge in feature selection. We will pro-
vide more instances in the next campaign. 
22
Scores Target 
Word 
Sen
se # 
Train
ing # 
Test 
# 
Base-
line SRCB
-WSD
I2R CITY
U-HIF
SWA
T-MP
TOR
MD 
HIT 
? 3 68 25 .40 .88 .84 .88 .76 .72 .32 
?? 2 53 18 .611 .611 .722 .722 .833 .556 .333 
? 2 56 19 .526 .842 .842 .684 .789 .474 .632 
?? 3 48 21 .476 .571 .591 .619 .619 .429 .619 
?? 2 50 17 .588 .824 .824 .824 .647 .706 .529 
? 3 53 18 .50 .778 .722 .778 .611 .50 .222 
?? 3 64 22 .455 .591 .591 .636 .545 .318 .364 
?? 2 60 20 .50 1.0 .95 1.0 1.0 .50 .50 
?? 2 38 14 .714 1.0 1.0 1.0 1.0 .643 .571 
?? 2 45 15 .533 .733 .733 .60 .467 .467 .467 
? 3 67 23 .435 .783 .783 .739 .696 .348 .696 
?? 2 44 17 .353 .529 .589 .588 .588 .353 .529 
?? 3 50 18 .556 .611 .611 .722 .722 .50 .111 
?? 2 39 14 .714 .929 .786 .714 .786 .857 .571 
?? 2 47 16 .625 .813 .813 .938 1.0 .438 .563 
?? 3 88 32 .313 .656 .563 .625 .656 .281 .344 
?? 3 65 25 .40 .88 1.0 .92 .60 .56 .44 
?? 2 41 14 .714 .786 .714 .786 .643 .714 .50 
?? 2 43 16 .625 .875 .938 1.0 .875 .438 .50 
Ave. 2.4
5 
1019 364 .506/ 
.528
.766/   
.773 
.761/ 
.769
.772/  
.778 
.72/     
.728 
.50/     
.516 
.456/   
.464 
  
Table 4: Performance on nouns. Micro / macro average precisions are spitted by ?/? at the last row. 
 
5 Acknowledgements 
This research is supported by Humanity and Social 
Science Research Project of China State Education 
Ministry (No. 06JC740001) and National Basic 
Research Program of China (No. 2004CB318102). 
We would like to thank Tao Guo and Yulai Pei 
for their hard work to guarantee the quality of the 
corpus. Huiming Duan provides us the corpus 
which has been word-segmented and PoS-tagged 
and gives some suggestions during the manual an-
notation. 
References 
Rada Mihalcea, Timothy Chklovski and Adam Kilgar-
riff. 2004. The Senseval-3 English lexical sample 
task. Proceedings of SENSEVAL-3. 25-28. 
Timothy Chklovski, Rada Mihalcea, Ted Pedersen and 
Amruta Purandare. 2004. The Senseval-3 Multilin- 
 
 
gual English-Hindi lexical sample task. Proceedings of 
SENSEVAL-3. 5-8. 
Xiaojin Zhu, Zoubin Ghahramani. 2002. Learning from 
Labeled and Unlabeled Data with Label Propagation. 
CMU CALD tech report CMU-CALD-02-107. 
Yunfang Wu, Peng Jin, Yangsen Zhang, and Shiwen Yu. 
2006. A Chinese Corpus with Word Sense Annota-
tion. Proceedings of ICCPOL, Singapore, 414-421. 
Zhen-Yu Niu, Dong-Hong Ji and Chew-Lim Tan. 2005. 
Word Sense Disambiguation Using Label Propaga-
tion Based Semi Supervised Learning. Proceedings 
of the 43rd Annual Meeting of the Association for 
Computational Linguistics.395-402 
23
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 261?263,
Prague, June 2007. c?2007 Association for Computational Linguistics
PKU: Combining Supervised Classifiers with Features Selection 
Peng Jin, Danqing Zhu, *Fuxin Li and Yunfang Wu 
Institute of Computational Linguistics   
Peking University, Beijing, China 
*Institute of Automation Chinese Academy of Sciences  
Beijing, China 
{jandp,zhudanqing,wuyf}@pku.edu.cn *Fuxin.li@ia.ac.cn 
 
 
Abstract 
This paper presents the word sense disam-
biguation system of Peking University 
which was designed for the SemEval-2007 
competition. The system participated in the 
Web track of task 11 ?English Lexical 
Sample Task via English-Chinese Parallel 
Text?. The system is a hybrid model by 
combining two supervised learning algo-
rithms SVM and ME. And the method of 
entropy-based feature chosen was experi-
mented. We obtained precision (and recall) 
of 81.5%. 
1 Introduction 
The PKU system participated in the web track of 
task 11. In this task, the organizers propose an 
English lexical sample task for word sense disam-
biguation (WSD), where the sense-annotated ex-
amples are (semi)-automatically gathered from 
word-aligned English-Chinese parallel texts. After 
assigning appropriate Chinese translations to each 
sense of an English word, the English side of the 
parallel texts can then serve as the training data, as 
they are considered to have been disambiguated 
and "sense-annotated" by the appropriate Chinese 
translations. This proposed task is thus similar to 
the multilingual lexical sample task in Senseval3, 
except that the training and test examples are col-
lected without manually annotating each individual 
ambiguous word occurrence. 
The system consists of two supervised learning 
classifiers, support vector machines (SVM) and 
maximum entropy (ME). A method of entropy-
based feature chosen was experimented to reduce 
the feature dimensions. The training data was lim-
ited to the labeled data provided by the task, and a 
PoS-tagger (tree-tagger) was used to get more fea-
tures. 
2 Features Selection 
We used tree-tagger to PoS-tag the texts before the 
feature extractor. No other resource is used in the 
system. The window size of the context is set to 5 
around the ambiguous word. Only the following 
features are used in the system:      
 
      Local words  
Local PoSs 
      Bag-of-words 
Local collocations 
 
Here local collocation means any two words 
which fall into the context window to form collo-
cation pair.  
Two methods are used to reduce the dimensions 
of feature space. One comes from the linguistic 
knowledge, some words whose PoSs are IN, DT, 
SYM, POS, CC or ?``? are not included as the fea-
tures. 
The second method is based on entropy. To each 
word, the training data was split to two parts for 
parameter estimation. One (usually consist of 30 ? 
50 instances) as the simultaneous test and the rest 
instances form the other part. 
First the entropy of each feature was calculated. 
For example, the target word ?work?, it has two 
senses and the dimensions of its feature space is N. 
For feature , if it appears in m instances belong-
ing to sense A and n instances in sense B. So the 
if
261
probability distributions are:
nm
mp +=1   and 
nm
np +=2 . The entropy of  is: if
 
?
=
=
2
1
1log)(
j j
ji p
pfH   
 
We rank all the features according to their en-
tropy from small to big. And then first percent 
lambda features are chosen as the final feature set. 
Using this smaller feature set, we use the classifier 
to make a new prediction. 
The parameter ? is estimated by comparing the 
system performance on the simultaneous test. In 
our system, .68 is chosen. It means that 68% origi-
nal features used to form the new feature space. 
The same classifier was tried on different feature 
sets to get different outputs and then were com-
bined. 
3 Classifiers 
The Support Vector Machines (SVM) are a group 
of supervised learning methods that can be applied 
to classification or regression. It is developed by 
Vapnik and has been applied into WSD (Lee et al, 
2004). Since most of the target words have more 
than two senses, we used the implementation of 
SVM that includes lib-svm (Chang and Lin, 2001) 
and svm-multiclass (Joachims, 2004). To lib-svm, 
the parameter of ?b? which is used to obtain prob-
ability information after training is set 0 or 1 indi-
vidually to form different classifiers. The default 
linear kernel is used.  
Each vector dimension represents a feature. The 
numerical value of a vector entry is the numerical 
value of the corresponding feature. In our system, 
we use binary features. If the context of an instance 
has a particular feature, then the feature value is set 
to 1, otherwise the value is set to 0. 
ME modeling provides a framework for inte-
grating information for classification from many 
heterogeneous information sources. The intuition 
behind the maximum entropy principle is: given a 
set of training data, model what is known and as-
sume no further knowledge about the unknown by 
assigning them equal probability (entropy is 
maximum). There are also some researchers using 
ME to WSD (Chao and Dyer, 2002). Dekang Lin?s 
implementation of ME was used. He used General-
ized Iterative Scaling (GIS) algorithm. 
4 Development 
Because of time constraints, we could not experi-
ment all the training data by cross-validation. To 
each target word, we extract first 50 training in-
stances as the test.  
 
Lib-svm 
Prob. Output
Target 
Word 
Svm- 
Multi-
class 
ME 
Orig. 
F.S. 
Red.
FS 
Non- 
prob. 
Output
Age .68 .70 .70 .70 .66 
Area .80 .70 .80 .74 .82 
Body .84 .84 .90 .92 .16 
Change .48 .42 .66 .42 .58 
Director .96 .94 .96 .96 .96 
Experience .90 .88 .88 .90 .88 
Future .94 .94 .94 .98 .94 
interest .84 .82 .82 .88 .84 
issue .88 .88 .84 .90 .88 
Life .92 .94 .98 1.0 .94 
Material .88 .92 .94 .94 .88 
Need .86 .86 .86 .86 .86 
performance .78 .82 .80 .82 .80 
Program .70 .74 .72 .72 .72 
Report .94 .94 .94 .94 .94 
System .76 .70 .76 .76 .70 
Time .70 .64 .68 .60 .76 
today .72 .70 .74 .68 .76 
Water .90 .92 .88 .82 .90 
Work .90 .86 .90 .92 .90 
 
Table 1: The Performance on Nouns 
 
For some adjectives, we just extract first 30 be-
cause the training data is small. For ten of adjec-
tives, the training data is too small, we directly use 
the lib-svm (with probability output) as the final 
classifier.  
Both SVM and ME could output the probability 
for each instance to each class. So we try to com-
bine them to improve the performance. Several 
methods of combining classifiers have been inves-
tigated (Radu et al, 2002). The enhanced Counted-
based Voting (CBV) and Rank-Based Voting, 
Probability Mixture Model, and best single Classi-
fier are experimented in the training data. Table 1 
and Table 2 indicate the results of nouns and adjec-
tives individually, which were achieved with each 
of the different methods. In these tables, "Orig 
F.S." and "Red. F.S." mean original feature set and 
reduced feature set. "Prob. output" and "Non Prob. 
262
output" are two implementation of lib-svm. The 
former output the probability of each instance be-
longing to each class, otherwise the latter not. Dif-
ferent from the results of Radu, choosing the best 
single classifier get the better performance than 
any kinds of combination. In this paper, we did not 
list the performances of combining.  
According to Table 1 and Table 2, the particular 
classifier chosen for that word was the one with the 
highest score in the training data. 
 
Lib-svm 
Prob. Output 
Target 
Word 
Svm- 
Multi- 
class 
ME 
Orig. 
F.S. 
Red.
F.S. 
Non- 
prob. 
output
Early .77 .80 .77 .80 .77 
Educational .87 .87 .87 .83 .87 
Free .74 .80 .84 .90 82 
Human .96 .92 .96 .90 .96 
Long .70 .70 .73 .87 .70 
Major .78 .78 .78 .80 .78 
Medical .76 .86 .78 .84 .78 
New .73 .77 .63 .43 .63 
Simple .73 .77 .77 .77 .80 
Third .98 .94 .98 1.0 .96 
 
Table 2:  The performance on Adjectives 
 
Two parameters are different from these two 
SVMs. One is the ?-c?, which is the tradeoff be-
tween training error and margin. In lib-svm the 
value of ?-c? is set 1; but in svm-multiclass is 0.01. 
The other is the strategy of how to utility binary-
classification to resolve multi-class. In svm-
multiclass, no strategy is needed since the algo-
rithm in (Crammer and Singer, 2001) solves the 
multi-class problem directly. In lib-svm, we use 
the one-against-all approach which is the default in 
lib-svm. Down-sampling is used if some result is 
trivial classification. The reason is that the unbal-
anced distribution of training data. We compared 
selecting support vectors and down-sampling. The 
latter is better. 
5 Results 
We participated in the subtask of SemEval-2007 
English lexical sample task via English-Chinese 
parallel text. The organizers make use of English-
Chinese documents gathered from the URL pairs 
given by the STRAND Bilingual Databases. They 
used this corpus for the evaluation of 40 English 
words (20 nouns and 20 adjectives). 
Our system gives exactly one sense for each test 
example. So the recall is always the same as preci-
sion. Micro-average precision is 81.5%. According 
to the task organizers, the recall of the best partici-
pating in this subtask is 81.9%. So the performance 
of our system compares favorably with the best 
participating system. 
6 Acknowledgements 
This research is supported by Humanity and Social 
Science Research Project of China State Education 
Ministry (No. 06JC740001) and National Basic 
Research Program of China (No. 2004CB318102). 
We are indebted to Helmut Schmid, IMS, Uni-
versity of Stuttgart, for making Tree-Tagger avail-
able free of charge. 
Finally, the authors thank the organizers Hwee 
Tou Ng and Yee Seng Chan, for their hard work to 
collect the training and test data. 
References 
Chih-Chung Chang and Chih-Jen Lin. 2001. LIBSVM : 
a library for support vector machines. 
www.csie.ntu.edu.tw/~cjlin/libsvm
Gerald Chao and Michael G. Dyer. 2002. Maximum 
entropy models for word sense disambiguation. Pro-
ceedings of the 19th international conference on 
Computational linguistics.Vol (1):1-7 
Koby Crammer and Yoram Singer. 2001. On the Algo-
rithmic Implementation of Multiclass Kernel-based 
Vector Machines. Journal of Machine Learning Re-
search, 2, 265-292 
Radu Florian, Silviu Cucerzan, Charles Schafer and 
David Yarowsky. 2002. Combining Classifiers for 
Word Sense Disambiguation. Natural Language En-
gineering, 8(4): 327 ? 341. 
Thorsten Joachims. SVM-Multiclass. 
http://svmlight.joachims.org/svm-
multiclass.html,2004. 
Yoong Keok Lee, Hwee Tou Ng and Tee Kiah Chia, 
Supervised Word Sense Disambiguation with Sup-
port Vector Machines and Multiple Knowledge 
Sources. Proceedings of SENSEVAL-3. 137 - 140 
263
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 257?268, Dublin, Ireland, August 23-29 2014.
Multi-view Chinese Treebanking
Likun Qiu
1,2,3
, Yue Zhang
1
, Peng Jin
4
and Houfeng Wang
2
1
Singapore University of Technology and Design, Singapore
2
Institute of Computational Linguistics, Peking University, China
3
School of Chinese Language and Literature, Ludong University, China
4
Lab of Intelligent Information Processing and Application, Leshan Normal University, China
{qiulikun,jandp,wanghf}@pku.edu.cn, yue zhang@sutd.edu.sg
Abstract
We present a multi-view annotation framework for Chinese treebanking, which uses dependen-
cy structures as the base view and supports conversion into phrase structures with minimal loss
of information. A multi-view Chinese treebank was built under the proposed framework, and
the first release (PMT 1.0) containing 14,463 sentences is be made freely available. To verify
the effectiveness of the multi-view framework, we implemented an arc-standard transition-based
dependency parser and added phrase structure features produced by the phrase structure view.
Experimental results show the effectiveness of additional features for dependency parsing. Fur-
ther, experiments on dependency-to-string machine translation show that our treebank and parser
could achieve similar results compared to the Stanford Parser trained on CTB 7.0.
1 Introduction
Phrase structures (PS) and dependency structures (DS) are two of the most popular grammar formalisms
for statistical parsing (Collins, 2003; Charniak, 2000; McDonald et al., 2005; Nivre, 2006; Petrov and
Klein, 2007; Zhang and Clark, 2008). While DS trees emphasize the grammatical relation between heads
and dependents, PS trees stress the hierarchical constituent structures of sentences. Several researchers
have explored DS and PS simultaneously to enhance the quality of syntactic parsing (Wang and Zong,
2010; Farkas and Bohnet, 2012; Sun and Wan, 2013) and tree-to-string machine translation (Meng et al.,
2013), showing that the two types of information complement each other for NLP tasks.
Most existing Chinese and English treebanks fall into the phrase structure category, and much work
has been done to convert PS into DS (Magerman, 1994; Collins et al., 1999; Collins, 2003; Sun and
Jurafsky, 2004; Johansson and Nugues, 2007; Duan et al., 2007; Zhang and Clark, 2008). Research on
statistical dependency parsing has frequently used dependency treebanks converted from phrase structure
treebanks, such as the Penn Treebank (PTB) (Marcus et al., 1993) and Penn Chinese Treebank (CTB)
(Xue et al., 2000). However, previous research shows that dependency categories in converted tree-
banks are simplified (Johansson and Nugues, 2007), and the widely used head-table PS to DS conversion
approach encounters ambiguities and uncertainty, especially for complex coordination structures (Xue,
2007). The main reason is that the PS treebanks were designed without consideration of DS conversion,
leading to inherent ambiguities in the mapping, and loss of information in the resulting DS treebanks. To
minimize information loss during treebank conversions, a treebank could be designed by considering PS
and DS information simultaneously; such treebanks have been proposed as multi-view treebanks (Xia et
al., 2009). We develop a multi-view treebank for Chinese, which treats PS and DS as different views of
the same internal structures of a sentence.
We choose the DS view as the base view, from which PS would be derived. Our choice is based on the
effectiveness of information transfer rather than convenience of annotation (Rambow, 2010; Bhatt and
Xia, 2012). Research on Chinese syntax (Zhu, 1982; Chen, 1999; Chen, 2009) shows that the phrasal
category of a constituent can be derived from the phrasal categories of its immediate subconstituents and
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
257
PKU POS Our POS
Ag, a, ad, ia, ja, la a (adjective)
Bg,b, ib, jb, jm, lb b (distinguishing words)
Dg, d, dc, df, id, jd, ld d (adverb)
m, mq m(number)
n, an, in, jn, ln, Ng, vn, nr, kn n (noun)
Qg,q, qb, qc, qd, qe, qj, ql, qr, qt, qv, qz q (measure word)
Rg,r, rr, ry, ryw, rz, rzw r (pronoun)
Tg, t, tt t (temporal noun)
u, ud, ue, ui, ul, uo, us, uz, Ug u (auxiliary word)
v, iv, im, jv, lv, Vg, vd, vi, vl, vq,vu, vx, vt,kv v (verb)
w, wd, wf, wj, wk, wky, wkz, wm,wp, ws, wt, wu, ww, wy, wyy, wyz w (punctuation)
Table 1: Mapping from PKU POS to our POS.
the dependency categories between them (for terminal words, parts-of-speech can be used as phrasal cat-
egories). Consequently, in Chinese, the canonical PS, containing information of constituent hierarchies
and phrasal categories, can be derived naturally from the canonical DS. As Xia et al. (2009) stated, a rich
set of dependency categories should be designed to ensure lossless conversion from DS to PS. When the
information of PS has been represented in DS explicitly or implicitly, we can convert DS to PS without
ambiguity (Rambow et al., 2002).
Given our framework, a multi-view Chinese treebank, containing 14,463 sentences and 336K words,
is constructed. This main corpus is based on the Peking University People?s Daily Corpus. We name our
treebank the Peking University Multi-view Chinese Treebank (PMT) release 1.0. To verify the useful-
ness of the treebank for statistical NLP, a transition-based dependency parser is implemented to include
PS features produced in the derivation process of phrasal categories. We perform a set of empirical
evaluations, with experimental results on both dependency parsing and dependency-to-string machine
translation showing the effectiveness of the proposed annotation framework and treebank. We make the
treebank, the DS to PS conversion script and the parser freely available.
2 Annotation Framework
2.1 Part-of-speech Tagset
Our part-of-speech (POS) tagset is based on the Peking University (PKU) People?s Daily corpus, which
consists of over 100 tags (Yu et al., 2003). We simplify the PKU tagset by syntactic distribution. The
simplified tagset contains 33 POS tags. The mapping from the original PKU POS to our simplified POS is
shown in Table 1. For instance, Ag (adjective morpheme), ad (adjective acting as an adverb), ia (adjective
idioms), ja (adjective abbreviation) and la (temporary phrase acting as an adjective) are all mapped to one
tag a (adjective). A set of basic PKU POS tags, including c (conjunction), e (interjection), f (localizer),
g (morpheme), h (prefix), i (idiom), j (abbreviation), k (suffix), l (temporary phrase), nr (personal name),
nrf (family name), nrg (surname), ns (toponym), nt (organization name), nx (non-Chinese noun), nz
(other proper noun), o (onomonopeia), p (preposition), q (measure word), r (pronoun), s (locative), x
(other non-Chinese word), y (sentence final particle), z (state adjective), are left unchanged.
2.2 Dependency Category Tagset
In a DS, the modifier is tagged with a dependency category, which denotes the role the modifier plays
with regard to its head. The root word of a sentence is dependent on a virtual root node R and tagged
with the dependency category HED. Table 2 lists the 32 dependency categories used in our annotation
guideline. These categories are designed in consideration of PS conversion with minimal ambiguities,
and can be classified according to the following criteria:
(1) whether the head dominates a compound clause (i.e. has an IC modifier) in the PS view. Accord-
ing to this, dependency categories can be cross-clause or in-clause. For instance, in Figure 1, the last
punctuation (") is labeled with the cross-clause tag PUS, and its head dominates an IC modifier. (2) the
relative position of the modifier to the head. According to this, dependency categories can be left, right
or free. For instance, the LAD, SBV, ADV, COS, DE and ATT labels in Figure 1 are all left. The VOB label
258
Tag Description Tag Description
ACT action object LAD left additive
ADV adverbial MT modality and time
APP appositive element NUM number
ATT attribute POB propositional object
CMP complement PUN punctuation
COO other coordination element PUS cross-clause punctuation
COS share-right-child coordination element QUC post-positional quantity
DE de (modifier of(special function word)) QUCC non-shared post-positional quantity
DEI dei (modifier ofProceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 81?85,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
SemEval-2010 Task 18:
Disambiguating Sentiment Ambiguous Adjectives
Yunfang Wu
Key Laboratory of Computational
Linguistics (Peking University)?
Ministry of Education, China
wuyf@pku.edu.cn
Peng Jin
Laboratory of Intelligent Information
Processing and Application, Leshan
Normal University, China
jinp@lstc.edu.cn
Abstract
Sentiment ambiguous adjectives cause
major difficulties for existing algorithms
of sentiment analysis. We present an
evaluation task designed to provide a
framework for comparing different
approaches in this problem. We define the
task, describe the data creation, list the
participating systems and discuss their
results. There are 8 teams and 16 systems.
1 Introduction
In recent years, sentiment analysis has attracted
considerable attention (Pang and Lee, 2008). It is
the task of mining positive and negative opinions
from natural language, which can be applied to
many natural language processing tasks, such as
document summarization and question answering.
Previous work on this problem falls into three
groups: opinion mining of documents, sentiment
classification of sentences and polarity prediction
of words. Sentiment analysis both at document
and sentence level rely heavily on word level.
The most frequently explored task at word
level is to determine the semantic orientation
(SO) of words, in which most work centers on
assigning a prior polarity to words or word
senses in the lexicon out of context. However,
for some words, the polarity varies strongly with
context, making it hard to attach each to a
specific sentiment category in the lexicon. For
example, consider ?low cost? versus ?low
salary?. The word ?low? has a positive
orientation in the first case but a negative
orientation in the second case.
Turney and Littman (2003) claimed that
sentiment ambiguous words could not be avoided
easily in a real-world application in the future
research. But unfortunately, sentiment
ambiguous words are discarded by most research
concerning sentiment analysis (Hatzivassiloglou
and McKeown, 1997; Turney and Littman, 2003;
Kim and Hovy, 2004). The exception work is
Ding et al (2008). They call these words as
context dependant opinions and propose a
holistic lexicon-based approach to solve this
problem. The language they deal with is English.
The disambiguation of sentiment ambiguous
words can also be considered as a problem of
phrase-level sentiment analysis. Wilson et al
(2005) present a two-step process to recognize
contextual polarity that employs machine
learning and a variety of features. Takamura et al
(2006, 2007) propose latent variable model and
lexical network to determine SO of phrases,
focusing on ?noun+adjective? pairs. Their
experimental results suggest that the
classification of pairs containing ambiguous
adjectives is much harder than those with
unambiguous adjectives.
The task 18 at SemEval 2010 provides a
benchmark data set to encourage studies on this
problem. This paper is organized as follows.
Section 2 defines the task. Section 3 describes
the data annotation. Section 4 gives a brief
summary of 16 participating systems. Finally
Section 5 draws conclusions.
2 Task Set up
2.1 Task description
In this task, we focus on 14 frequently used
sentiment ambiguous adjectives in Chinese,
which all have the meaning of measurement, as
shown below.
81
(1) Sentiment ambiguous adjectives(SAAs)
={? da ?large?, ? duo ?many?, ? gao
?high?, ? hou ?thick?,? shen ?deep?, ?
zhong ?heavy?, ?? ju-da ?huge?, ??
zhong-da ?great?, ? xiao ?small?, ? shao
?few?, ? di ?low?, ? bao ?thin?, ? qian
?shallow?,? qing ?light?}
These adjectives are neutral out of context, but
when they co-occur with some target nouns,
positive or negative emotion will be evoked.
Although the number of such ambiguous
adjectives is not large, they are frequently used
in real text, especially in the texts expressing
opinions and emotions.
The task is designed to automatically
determine the SO of these sentiment ambiguous
adjectives within context: positive or negative.
For example, ? gao ?high?should be assigned
as positive in ??? gong-zi-gao ?salary is
high?but negative in ??? jia-ge-gao ?price is
high?.
This task was carried out in an unsupervised
setting. No training data was provided, but
external resources are encouraged to use.
2.2 Data Creation
We collected data from two sources. The main
part was extracted from Xinhua News Agency of
Chinese Gigaword (Second Edition) released by
LDC. The texts were automatically word-
segmented and POS-tagged using the open
software ICTCLAS1. In order to concentrate on
the disambiguation of sentiment ambiguous
adjectives, and reduce the noise introduced by
the parser, we extracted sentences containing
strings in pattern of (2), where the target nouns
are modified by the adjectives in most cases.
(2) noun+adverb+adjective (adjective?SAAs)
e.g.??/n ?/d ?/a cheng-ben-jiao-di
?the cost is low.?
Another small part of data was extracted from
the Web. Using the search engine Google2, we
searched the queries as in (3):
(3) ? hen ?very?+ adjective (adjective?SAAs )
From the returned snippets, we manually picked
out some sentences that contain the strings of (2).
Also, the sentences were automatically
segmented and POS-tagged using ICTCLAS.
Sentiment ambiguous adjectives in the data
were assigned as positive, negative or neutral,
1 http://www.ictclas.org/.
2 http://www.google.com/.
independently by two annotators. Since we focus
on the distinction between positive and negative
categories, the neutral instances were removed.
The inter-annotator agreement is in a high level
with a kappa of 0.91. After cases with
disagreement were negotiated between the two
annotators, a gold standard annotation was
agreed upon. In total 2917 instances were
provided as the test data in the task, and the
number of sentences of per target adjective is
listed in Table 2.
Evaluation was performed in micro accuracy
and macro accuracy:
1 1
/
N N
mir i i
i i
P m n
? ?
?? ? (1)
1
/
N
mar i
i
P P N
?
?? /i i iP m n? (2)
where N is the number of all target words, in is
the number of all test instances for a specific
word, and im is the number of correctly labeled
instances.
2.3 Baseline
We group 14 sentiment ambiguous adjectives
into two categories: positive-like adjectives and
negative-like adjectives. The former has the
connotation towards large measurement, whereas
the latter towards small measurement.
(4) Positive-like adjectives (Pa) ={? da ?large?,
? duo ?many?, ? gao ?high?, ? hou
?thick?, ? shen ?deep?, ? zhong ?heavy?,
?? ju-da ?huge?,?? zhong-da ?great?}
(5) Negative-like adjectives (Na) ={? xiao
?small?, ? shao ?few?, ? di ?low?,? bao
?thin?, ? qian ?shallow?,? qing ?light?}
We conduct a baseline in the dataset. Not
considering the context, assign all positive-like
adjectives as positive and all negative-like
adjectives as negative. The micro accuracy of the
baseline is 61.20%.
The inter-annotator agreement of 0.91 can be
considered as the upper bound of the dataset.
3 Systems and Results
We published firstly trial data and then test data.
In total 11 different teams downloaded both the
trial and test data. Finally 8 teams submitted their
experimental results, including 16 systems.
82
3.1 Results
Table 1 lists all systems?scores, ranked from
best to worst performance measured by micro
accuracy. To our surprise, the performance of
different systems differs greatly. The micro
accuracy of the best system is 94.20% that is
43.12% higher than the worst system. The
accuracy of the best three systems is even higher
than inter-annotator agreement. The performance
of the worst system is only a little higher than
random baseline, which is 50% when we
randomly assign the SO of sentiment ambiguous
adjectives.
Table 1: The scores of 16 systems
Table 2 shows that the performance of
different systems differs greatly on each of 14
target adjectives. For example, the accuracy of
? da ?large?is 95.53% by one system but only
46.51% by another system.
Table 2: The scores of 14 ambiguous adjectives
3.2 Systems
In this section, we give a brief description of the
systems.
YSC-DSAA This system creates a new word
library named SAAOL (SAA-Oriented Library),
which is built manually with the help of software.
SAAOL consists of positive words, negative
words, NSSA, PSSA, and inverse words. The
system divides the sentences into clauses using
heuristic rules, and disambiguates SAA by
analyzing the relationship between SAA and the
keywords.
HITSZ_CITYU This group submitted three
systems, including one baseline system and two
improved systems.
HITSZ_CITYU_3: The baseline system is
based on collocation of opinion words and their
targets. For the given adjectives, their
collocations are extracted from People?s Daily
Corpus. With human annotation, the system
obtained 412 positive and 191 negative
collocations, which are regarded as seed
collocations. Using the context words of seed
collocations as features, the system trains a one-
class SVM classifier.
HITSZ_CITYU_2 and HITSZ_CITYU_1:
Using HowNet-based word similarity as clue, the
authors expand the seed collocations on both
ambiguous adjectives side and collocated targets
side. The authors then exploit sentence-level
opinion analysis to further improve performance.
The strategy is that if the neighboring sentences
on both sides have the same polarity, the
ambiguous adjective is assigned as the same
polarity; if the neighboring sentences have
conflicted polarity, the SO of ambiguous
adjective is determined by its context words and
the transitive probability of sentence polarity.
The two systems use different parameters and
combination strategy.
OpAL This system combines supervised
methods with unsupervised ones. The authors
employ Google translator to translate the task
dataset from Chinese to English, since their
system is working in English. The system
explores three types of judgments. The first one
trains a SVM classifier based on NTCIR data and
EmotiBlog annotations. The second one uses
search engine, issuing queries of ?noun + SAA +
AND + non-ambiguous adjective?. The non-
ambiguous adjectives include positive set
(?positive, beautiful, good?) and negative set
(?negative, ugly, bad?). An example is ?price
high and good?. The third one uses ?too, very-
System Micro
Acc.(%)
Macro
Acc.(%)
YSC-DSAA 94.20 92.93
HITSZ_CITYU_1 93.62 95.32
HITSZ_CITYU_2 93.32 95.79
Dsaa 88.07 86.20
OpAL 76.04 70.38
CityUHK4 72.47 69.80
CityUHK3 71.55 75.54
HITSZ_CITYU_3 66.58 62.94
QLK_DSAA_R 64.18 69.54
CityUHK2 62.63 60.85
CityUHK1 61.98 67.89
QLK_DSAA_NR 59.72 65.68
Twitter Sentiment 59.00 62.27
Twitter Sentiment_ext 56.77 61.09
Twitter Sentiment_zh 56.46 59.63
Biparty 51.08 51.26
Words Ins# Max% Min% Stdev
? |large 559 95.53 46.51 0.155
? |many 222 95.50 49.10 0.152
? ||high 546 95.60 54.95 0.139
? |thick 20 95.00 35.00 0.160
? |deep 45 100.00 51.11 0.176
? |heavy 259 96.91 34.75 0.184
?? |huge 49 100.00 10.20 0.273
?? |great 28 100.00 7.14 0.243
? |small 290 93.10 49.66 0.167
? few 310 95.81 41.29 0.184
? |low 521 93.67 48.37 0.147
? |thin 33 100.00 18.18 0.248
? |shallow 8 100.00 37.50 0.155
? |light 26 100.00 34.62 0.197
83
rules?. The final result is determined by the
majority vote of the three components.
CityUHK This group submitted four systems.
Both machine learning method and lexicon-
based method are employed in their systems. In
the machine learning method, maximum entropy
model is used to train a classifier based on the
Chinese data from NTCIR opinion task. Clause-
level and sentence-level classifiers are compared.
In the lexicon-based method, the authors classify
SAAs into two clusters: intensifiers (our
positive-like adjectives in (4)) and suppressors
(our negative-like adjectives in (5)), and then use
the polarity of context to determine the SO of
SAAs.
CityUHK4: clause-level machine learning +
lexicon.
CityUHK3: sentence-level machine learning +
lexicon.
CityUHK2: clause-level machine learning.
CityUHK2: sentence-level machine learning.
QLK_DSAA This group submitted two
systems. The authors adopt their SELC model
(Qiu, et al, 2009), which is proposed to exploit
the complementarities between lexicon-based
and corpus-based methods to improve the whole
performance. They determine the sentence
polarity by SELC model, and simply regard the
sentence polarity as the polarity of SAA in the
sentence.
QLK_DSAA_NR: Based on the result of
SELC model, they inverse the SO of SAA when
it is modified by negative terms. Our task
includes only positive and negative categories, so
they replace the neutral value obtained by SELC
model by the predominant polarity of the
adjective.
QLK_DSAA_R: Based on the result of
QLK_DSAA_NR, they add a rule to cope with
two modifiers ? pian ?specially? and ? tai
?too?, which always have the negative meaning.
Twitter sentiment This group submitted three
systems. The authors use a training data collected
from microblogging platform. By exploiting
Twitter, they collected automatically a dataset
consisting of negative and positive expressions.
The sentiment classifier is trained using Naive
Bayes with n-grams of words as features.
Twitter Sentiment: Translating the task dataset
from Chinese to English using Google translator,
and then based on training data in English texts
from Twitter.
Twitter Sentiment_ext: With Twitter
Sentiment as basis, using extended data.
Twitter Sentiment_zh: Based on training data
in Chinese texts from Twitter.
Biparty This system transforms the problem
of disambiguating SAAs to predict the polarity
of target nouns. The system presents a
bootstrapping method to automatically build the
sentiment lexicon, by building a nouns-verbs
biparty graph from a large corpus. Firstly they
select a few nouns as seed words, and then they
use a cross inducing method to expand more
nouns and verbs into the lexicon. The strategy is
based on a random walk model.
4 Discussion
The experimental results of some systems are
promising. The micro accuracy of the best three
systems is over 93%. Therefore, the inter-
annotator agreement (91%) is not an upper
bound on the accuracy that can be achieved. On
the contrary, the experimental results of some
systems are disappointing, which are below our
predefined simple baseline (61.20%), and are
only a little higher than random baseline (50%).
The accuracy variance of different systems
makes this task more interesting.
The participating 8 teams exploit totally
different methods.
Human annotation. In YSC-DSAA system,
the word library of SAAOL is verified by human.
In HITSZ_CITYU systems, the seed collocations
are annotated by human. The three systems rank
top 3. Undoubtedly, human labor can help
improve the performance in this task.
Training data. The OpAL system employs
SVM machine learning based on NTCIR data
and EmotiBlog annotations. The CityUHK
systems trains a maximum entropy classifier
based on the annotated Chinese data from
NTCIR. The Twitter Sentiment systems use a
training data automatically collected from
Twitter. The results show that some of these
supervised methods based on training data
cannot rival unsupervised ones, partly due to the
poor quality of the training data.
English resources. Our task is in Chinese.
Some systems use English resources by
translating Chinese into English, as OpAL and
Twitter Sentiment. The OpAL system achieves a
quite good result, making this method a
promising direction. This also shows that
disambiguating SAAs is a common problem in
natural language.
84
5 Conclusion
This paper describes task 18 at SemEval-2010,
disambiguating sentiment ambiguous adjectives.
The experimental results of the 16 participating
systems are promising, and the used approaches
are quite novel.
We encourage further research into this issue,
and integration of the disambiguation of
sentiment ambiguous adjectives into applications
of sentiment analysis.
Acknowledgments
This work was supported by National Natural
Science Foundation of China (No. 60703063),
National Social Science Foundation of China
(No. 08CYY016), and the Open Projects
Program of Key Laboratory of Computational
Linguistics(Peking University) ? Ministry of
Education. We thank Miaomiao Wen and Tao
Guo for careful annotation of the data.
References
Ding X., Liu B. and Yu, P. 2008. A holistic lexicon-
based approach to opinion mining. Proceedings of
WSDM?08.
Hatzivassiloglou, V. and McKeown, K. 1997
Predicting the semantic orientation of adjectives.
Proceedings of ACL?97.
Kim, S and Hovy, E. 2004. Determining the sentiment
of opinions. Proceedings of COLING?04.
Pang, B. and Lee, L. 2008. Opinion mining and
sentiment analysis. Foundations and Trends in
Information Retrieval.
Qiu L., Zhang W., Hu, C. and Zhao, K. 2009. SELC:
A self-supervised model for sentiment analysis. In
Proceedings of CIKM?09.
Takamura, H., Inui,T. and Okumura, M. 2006. Latent
Variable Models for Semantic Orientations of
phrases. Proceedings of EACL?06.
Takamura, H., Inui,T. and Okumura, M. 2007.
Extracting Semantic Orientations of Phrases from
Dictionary. Proceedings of NAACL HLT ?07.
Turney, P. and Littman, M. 2003. Measuring praise
and criticism: inference of semantic orientation
from association. ACM transaction on information
systems.
Wilson, T., Wiebe, J. and Hoffmann, P. 2005.
Recognizing contextual polarity in phrase-level
sentiment analysis. Proceedings of
HLT/EMNLP?05.
85
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, page 87,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
SemEval-2 Task 15: Infrequent Sense Identification for Mandarin 
Text to Speech Systems   
Peng Jin1 and Yunfang Wu2
1Laboratory of Intelligent Information Processing and Application, Leshan Normal 
University, Leshan China 
2Institute of Computational Linguistics  Peking University, Beijing China 
{jandp, wuyf}@pku.edu.cn 
 
1 Introduction 
There are seven cases of grapheme to phoneme in 
a text to speech  system (Yarowsky, 1997). Among 
them, the most difficult task is disambiguating the 
homograph word, which has the same POS but 
different pronunciation. In this case, different pro-
nunciations of the same word always correspond to 
different word senses. Once the word senses are 
disambiguated, the problem of GTP is resolved. 
There is a little different from traditional WSD, 
in this task two or more senses may correspond to 
one pronunciation. That is, the sense granularity is 
coarser than WSD. For example, the preposition 
???  has three senses: sense1 and sense2 have the 
same pronunciation {wei 4}, while sense3 corre-
sponds to {wei 2}. In this task, to the target word, 
not only the pronunciations but also the sense la-
bels are provided for training; but for test, only the 
pronunciations are evaluated. The challenge of this 
task is the much skewed distribution in real text: 
the most frequent pronunciation occupies usually 
over 80%. 
In this task, we will provide a large volume of 
training data (each homograph word has at least 
300 instances) accordance with the truly distribu-
tion in real text. In the test data, we will provide at 
least 100 instances for each target word. The 
senses distribution in test data is the same as in 
training data.All instances come from People Daily 
newspaper (the most popular newspaper in Manda-
rin). Double blind annotations are executed manu-
ally, and a third annotator checks the annotation. 
2 Participating Systems 
Two kinds of precisions are evaluated. One is 
micro-average: 
??
==
=
N
i
i
N
i
imir nmP
11
/  
N is the number of all target word-types. mi is 
the number of labeled correctly to one specific tar-
get word-type and ni is the number of all test in-
stances for this word-type. The other is macro-
average: 
?
=
=
N
i
imar NpP
1
/ ,  iii nmp /=
 
There are two teams participated in and submit-
ted nine systems. Table 1 shows the results, all sys-
tems are better than baseline (Baseline is using the 
most frequent sense to tag all the tokens). 
 
System Micro-average Macro-average
156-419 0.974432 0.951696 
205-332 0.97028 0.938844 
205-417 0.97028 0.938844 
205-423 0.97028 0.938844 
205-425 0.97028 0.938844 
205-424 0.968531 0.938871 
156-420 0.965472 0.942086 
156-421 0.965472 0.94146 
156-422 0.965472 0.942086 
baseline 0.923514 0.895368 
Table 1: The scores of all participating systems 
References 
Yarowsky, David. 1997. ?Homograph disambiguation 
in text-to-speech synthesis.? In van Santen, Jan T. H.; 
Sproat, Richard; Olive, Joseph P.; and Hirschberg, 
Julia. Progress in Speech Synthesis. Springer-Verlag, 
New York, 157-172. 
87
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 374?377,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
SemEval-2012 Task 4: Evaluating Chinese Word Similarity 
 
 
Peng Jin Yunfang Wu 
School of Computer Science Institute of Computational Linguistics 
Leshan Normal University Peking University 
Leshan, 614000, China Beijing, 100871, China 
jandp@pku.edu.cn wuyf@pku.edu.cn 
 
 
 
 
 
 
Abstract 
This task focuses on evaluating word similari-
ty computation in Chinese. We follow the way 
of Finkelstein et al (2002) to select word 
pairs. Then we organize twenty under-
graduates who are major in Chinese linguis-
tics to annotate the data. Each pair is assigned 
a similarity score by each annotator. We rank 
the word pairs by the average value of similar 
scores among the twenty annotators. This data 
is used as gold standard. Four systems partici-
pating in this task return their results. We 
evaluate their results on gold standard data in 
term of  Kendall's tau value, and the results 
show three of them have a positive correlation 
with the rank manually created while the taus' 
value is very small. 
1 Introduction 
The goal of word similarity is to compute the simi-
larity degree between words. It is widely used in 
natural language processing to alleviate data 
sparseness which is an open problem in this field. 
Many research have focus on English language 
(Lin, 1998; Curran and Moens, 2003; Dinu and 
Lapata, 2010), some of which rely on the manual 
created thesaurus such as WordNet (Budanitsky 
and Hirst, 2006), some of which obtain the similar-
ity of the words via large scale corpus (Lee, 1999), 
and some research integrate both thesaurus and 
corpus (Fujii et al, 1997). This task tries to evalu-
ate the approach on word similarity for Chinese 
language. To the best of our knowledge, this is first 
release of  benchmark data for this study. 
In English language, there are two data sets: Ru-
benstein and Goodenough (1965) and Finkelstein 
et al (2002) created a ranking of word pairs as the 
benchmark data. Both of them are manually anno-
tated. In this task, we follow the way to create the 
data and annotate the similarity score between 
word pairs by twenty Chinese native speakers. 
Finkelstein et al (2002) carried out a psycholin-
guistic experiment: they selected out 353 word 
pairs, then ask the annotators assign a numerical 
similarity score between 0 and 10 (0 denotes that 
words are totally unrelated, 10 denotes that words 
are VERY closely related) to each pair. By defini-
tion, the similarity of the word to itself should be 
10. A fractional score is allowed.  
It should be noted that besides the rank of word 
pairs, the thesaurus such as Roget's thesaurus are 
often used for word similarity study (Gorman and 
Curran, 2006).  
The paper is organized as follows. In section 2 
we describe in detail the process of the data prepa-
ration. Section 3 introduces the four participating 
systems. Section 4 reports their results and gives a  
brief discussion.. And finally in section 5 we bring 
forward some suggestions for the next campaign 
and conclude the paper. 
374
2 Data Preparation 
2.1 Data Set 
We use wordsim 353 (Finkelstein et al, 2002) as 
the original data set. First, each word pair is trans-
lated into Chinese by two undergraduates who are 
fluent in English. 169 word pairs are the same in 
their translation results. To the rest 184 word pairs, 
the third undergraduate student check them   fol-
lowing the rules: 
(i) Single character vs. two characters. If one 
translator translate one English word into the Chi-
nese word which consists only one Chinese charac-
ter and the other use two characters to convey the 
translation, we will prefer to the later provided that 
these two translations are semantically same. For 
example, "tiger" is translated into "?" and "??", 
we will treat them as same and use "??" as the 
final translation. This was the same case in "drug" 
("?" and "??" are same translations). 
(ii) Alias. The typical instance is "potato", both "
??" and "???" are the correct translations. So 
we will treat them as same and prefer "??" as the 
final translation because it is more general used 
than the latter one.  
(iii) There are five distinct word pairs  in the 
translations and are removed.    
At last, 348 word pairs are used in this task. 
Among these 348 word pairs, 50 ones are used as 
the trial data and the rest ones are used as the test 
data1. 
2.2 Manual Annotation 
Each word pair is assigned the similarity score by 
twenty Chinese native speakers. The score ranges 
from 0 to 5 and 0 means two words have nothing 
to do with each other and 5 means they are identi-
cally in semantic meaning. The higher score means 
the more similar between two words. Not only in-
teger but also real is acceptable as the annotated 
score. We get the average of all the scores given by 
the annotators for each word pair and then sort 
them according to the similarity scores. The distri-
bution of word pairs on the similar score is illus-
trated as table 1.   
                                                          
1 In fact there are 297 word pairs are evaluated because one 
pair is missed during the annotation.  
Score 0.0-1.0 1.0-2.0 2.0-3.0 3.0-4.0 4.0-5.0 
# Word pairs 39 90 132 72 13 
Table1: The distribution of similarity score 
 
Ra-
nk 
Word in Chi-
nese/English 
Word 2 in 
Chinese/ Eng-
lish 
Simi-
larity 
score 
Std. 
dev 
RSD 
(%) 
1 ??/football ??/soccer 4.98 0.1 2.0 
2 ??/tiger ??/tiger 4.89 0.320 6.55 
3 ??/planet ??/star 4.72 0.984 20.8 
4 ???
/admission 
??/ticket 4.60 0.516 11.2 
5 ?/money ??/cash 4.58 0.584 12.7 
6 ??/bank ?/cash 4.29 0.708 16.5 
7 ??/cell ??/phone 4.28 0.751 17.5 
8 ??/gem ??/jewel 4.24 0.767 18.1 
9 ??/type ??/kind 4.24 1.000 23.6 
10 ?? / calcu-
lation 
?? / compu-
tation 
4.14 0.780 19.0 
Avg - - 4.496 0.651 14.80 
Table 2: Top ten similar word pairs 
 
Table 2 and table 3 list top ten similar word 
pairs and top ten un-similar word pairs individual-
ly. Standard deviation (Std. dev) and relative standard 
deviation (RSD) are also computed. Obviously, the rela-
tive standard deviation of top ten similar word pairs is 
far less than the un-similar pairs. 
 
2.3 Annotation Analysis 
Figure 1 illustrates the relationship between the 
similarity score and relative standard deviation. 
The digits in "x" axes are the average similarity 
score of every integer interval, for an instance, 
1.506 is the average of all word pairs' similarity 
score between 1.0 and 2.0. 
3 Participating Systems  
Four systems coming from two teams participated 
in this task. 
 
375
 
Figure 1. The relationship between RSD and simi-
lar score 
 
Ra-
nk 
Word1 in Chi-
nese/in English 
Word2 in Chi-
nese/in English 
Simi-
larity 
score 
Std. 
dev 
RSD(
%) 
1 ??/noon ??/string 0.06 .213 338.7 
2 ??/king ???
/cabbage 
0.16 .382 245.3 
3 ??
/production 
??/hike 0.17 .432 247.5 
4 ??/delay ????
/racism 
0.26 .502 191.1 
5 ??/professor ??/cucumber 0.30 .62 211.1 
6 ??/stock ???/jaguar 0.30 .815 268.2 
7 ??/sign ??/recess 0.30 .655 215.4 
8 ??/stock CD/CD 0.31 .540 173.6 
9 ?/drink ??/ear 0.31 .833 264.8 
10 ??/rooster ??/voyage 0.33 .771 236.7 
Avg - - 0.25 .576 239.2 
Table 3: Top ten un-similar word pairs 
 
 
MIXCC: This system used two machine reada-
ble dictionary (MRD), HIT IR-Lab Tongyici Cilin 
(Extended) (Cilin) and the other is Chinese Con-
cept Dictionary (CCD). The extended CiLin con-
sists of 12 large classes, 97 medium classes, 1,400 
small classes (topics), and 17,817 small synonym 
sets which cover 77,343 head terms. All the items 
are constructed as a tree with five levels. With the 
increasing of levels, word senses are more fine-
grained. The Chinese Concept Dictionary is a Chi-
nese WordNet produced by Peking University. 
Word concepts  are presented as synsets   corre-
sponding to WordNet 1.6. Besides synonym, anto-
nym, hypernym/hyponym, holonym/meronym, 
there is another semantic relation type named as 
attribute which happens between two words with 
different part-of-speeches.  
They first divide all word pairs into five parts 
and rank them according to their levels in Cilin in 
descending order. For each part, they computed 
word similarity by Jiang and Conrath (1997) meth-
od2. 
 
MIXCD: Different form MIXCC, this system 
used the trial data to learn a multiple linear regres-
sion functions. The CCD was considered as a di-
rected graph. The nodes were synsets and edges 
were the semantic relations between two synsets. 
The features for this system were derived from  
CCD and a corpus and listed as follows: 
 
? the shortest path between two synsets 
which contain the words 
? the rates of 5 semantic relation types  
? mutual information of a word pair in the 
corpus 
 
They used the result of multiple linear regres-
sions to forecast the similarity of other word pairs 
and get the rank. 
 
GUO-ngram: This system used the method 
proposed by (Gabrilovich and Markovitch, 2007). 
They downloaded the Wikipedia on 25th Novem-
ber, 2011 as the knowledge source. In order to by-
pass the Chinese segmentation, they extract one 
character (uni-gram) and two sequential characters 
(bi-gram) as the features. 
 
GUO-words: This system is very similar to 
GUO-ngram except that the features consist of 
words rather than n-grams. They implemented a 
simple index method which searches all continuous 
character strings appearing in a dictionary. For ex-
ample, given a text string ABCDEFG in which 
ABC, BC, and EF appear in the dictionary. The 
output of the tokenization algorithm is the three 
words ABC, BC, EF and the two characters E and 
G. 
                                                          
2 Because there is no sense-tagged corpus for CCD, the fre-
quency of each concept was set to 1 in this system. 
376
4 Results  
Each system is required to rank these 500 word 
pairs according to their similarity scores. Table 4 
gives the overall results obtained by each of the 
systems. 
 
Rank Team ID System ID Tau's 
value 
1 
lib 
MIXCC 0.050 
2 MIXCD 0.040 
3 
Gfp1987 
Guo-ngram 0.007 
4 Guo-words -0.011 
Table 4: The results of four systmes 
 
The ranks returned by these four systems will be 
compared with the rank from human annotation by 
the Kendall Rank Correlation Coefficient: 
 
? ?
? ?
2 ,1 1 / 2
S
N N
? ?? ? ? ?
 
Where N  is the number of objects. ? and ? are 
two distinct orderings of a object in two ranks. 
( , )S ? ? is the minimum number of adjacent 
transpositions needing to bring ? and ?  (Lapata, 
2006). In this metric, tau's value ranges from -1 to 
+1 and -1 means that the two ranks are inverse to 
each other and +1 means the identical rank.  
From table 4, we can see that except the final 
system, three of them got the positive tau's value. It 
is regret that the tau's is very small even if the 
MIXCC system  is the best one.   
5 Conclusion  
We organize an evaluation task focuses on word 
similarity in Chinese language. Totally 347 word 
pairs are annotated similarity scores by twenty na-
tive speakers. These word pairs are ordered by the 
similarity scores and this rank is used as bench-
mark data for evaluation.  
Four systems participated  in this task.  Except 
the system MIXCD, three ones got their own rank 
only via the corpus. Kendall's tau is used as the 
evaluation metric. Three of them got the positive 
correlation rank compared with the gold standard 
data 
Generally the tau's value is very small, it indi-
cates that obtaining a good rank is still difficult. 
We will provide more word pairs and distinct them 
relatedness from similar, and attract more teams to 
participate in the interesting task. 
 
Acknowledgments 
This research is supported by National Natural 
Science Foundation of China (NSFC) under Grant 
No. 61003206, 60703063. 
References  
A. Budanitsky and G. Hirst. Evaluating WordNet-based 
Measures of Lexical Semantic Relatedness. Compu-
tational Linguistics, 2006, 32(1):13-47. 
J. Curran and M. Moens. Scaling Context Space. Pro-
ceedings of ACL, 2002, pp. 231-238. 
G. Dinu and M. Lapata. Measuring Distributional Simi-
larity in Context. Proceedings of EMNLP, 2010, pp. 
1162-1172. 
L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin, Z. 
Solan, G. Wolfman, and E. Ruppin. 2002. Placing 
Search in Context: The Concept Revisited. ACM 
Transactions on Information Systems, 20(1):116-131. 
A. Fujii, T. Hasegawa, T. Tokunaga and H. Tanaka. 
Integration of Hand-Crafted and Statistical Resources 
in Measuring Word Similarity. 1997. Proceedings of 
Workshop of Automatic Information Extraction and 
Building of Lexical Semantic Resources for NLP Ap-
plications. pp. 45-51. 
E. Gabrilovich and S. Markovitch, Computing Semantic 
Relatedness using Wikipedia-based Explicit Seman-
tic Analysis, Proceedings of IJCAI, Hyderabad, 2007, 
pp. 1606?1611. 
J. Gorman and J. Curran. Scaling Distributional Similar-
ity to Large Corpora. Proceedings of ACL, 2006, pp. 
361-368. 
J. Jiang and D. Conrath. 1997. Semantic similarity 
based on corpus statistics and lexical taxonomy. Pro-
ceedings of International Conference on Research in 
Computational Linguistics, Taiwan. 
M. Lapata. Automatic Evaluation of Information Order-
ing: Kendall's Tau. Computational Linguistics, 2006, 
32(4):471-484. 
D. Lin. Automatic Retrieval and Clustering of Similar 
Words. Proceedings of ACL / COLING, 1998, pp. 
768-774. 
L. Lee. Measures of Distributional Similarity. Proceed-
ings of ACL, 1999, pp. 25-32. 
H. Rubenstein and J.B. Goodenough. 1965. Contextual 
correlates of synonymy. Communications of the ACM, 
8(10):627-633. 
377
 The Chinese Persons Name Disambiguation Evaluation: Exploration of 
Personal Name Disambiguation in Chinese News 
 
 
Ying Chen*, Peng Jin?, Wenjie Li?,Chu-Ren Huang? 
* China Agricultural University ?Leshan Teachers? College ?The Hong Kong Polytechnic University 
chenying3176@gmail.com jandp@pku.edu.cn cswjli@comp.polyu.edu.hk 
  churenhuang@gmail.com 
   
 
 
 
 
 
 
Abstract 
Personal name disambiguation becomes hot as it 
provides a way to incorporate semantic under-
standing into information retrieval. In this cam-
paign, we explore Chinese personal name 
disambiguation in news. In order to examine how 
well disambiguation technologies work, we con-
centrate on news articles, which is well-formatted 
and whose genre is well-studied. We then design a 
diagnosis test to explore the impact of Chinese 
word segmentation to personal name disambigua-
tion. 
1 Introduction 
Incorporating semantic understanding technolo-
gies from the field of NLP becomes one of further 
directions for information retrieval. Among them, 
named entity disambiguation, which intends to 
use state-of-the-art named entity processing to 
enhance a search engine, is a hot research issue. 
Because of the popularity of personal names in 
queries, more efforts are put on personal name 
disambiguation. The personal name disambigua-
tion used both in Web Personal Search (WePS1) 
and our campaign is defined as follow. Given 
documents containing a personal name in interest, 
the task is to cluster them according to which en-
tity the name in a document refers to.  
WePS, which explores English personal name 
disambiguation, has been held twice (Artiles et al, 
                                                           
1
 http://nlp.uned.es/weps/ 
2007, 2009). Compared to the one in English, per-
sonal name disambiguation in Chinese has special 
issues, such as Chinese text processing and Chi-
nese personal naming system. Therefore, we hold 
Chinese personal name disambiguation (CPND) to 
explore those problems. In this campaign, we 
mainly examine the relationships between Chinese 
word segmentation and Chinese personal name 
disambiguation.  
Moreover, from our experiences in WePS 
(Chen et al, 2007, 2009), we notice that webpages 
are so noisy that text pre-processing that extracts 
useful text for disambiguation needs much effort. 
In fact, text pre-processing for webpages is rather 
complicated, such as deleting of HTML tags, the 
detection of JavaScript codes and so on. Therefore, 
the final system performance in the WePS cam-
paign sometimes does not reflect the disambigua-
tion power of the system, and instead it shows the 
comprehensive result of text pre-processing as 
well as disambiguation. In order to focus on per-
sonal name disambiguation, we choose news 
documents in CPND. 
The paper is organized as follows. Section 2 de-
scribes our formal test including datasets and 
evaluation. Section 3 introduces the diagnosis test, 
which explores the impact of Chinese word seg-
mentation to personal name disambiguation. Sec-
tion 4 describes our campaign, and Section 5 
presents the results of the participating systems. 
Finally, Section 6 concludes our main findings in 
this campaign. 
 
2 The Formal Test 
2.1 Datasets 
To avoid the difficulty to clean a webpage, we 
choose news articles in this campaign. Given a 
full name in Chinese, we search the character-
based personal name string in all documents of 
Chinese Gigaword Corpus, a large Chinese news 
collection. If a document contains the name, it is 
belonged to the dataset of this name. To ensure 
the popularity of a personal name, we keep only a 
personal name whose corresponding dataset com-
prises more than 100 documents. In addition, if 
there are more than 300 documents in that dataset, 
we randomly select 300 articles to annotate. Fi-
nally, there are totally 58 personal names and 
12,534 news articles used in our data, where 32 
names are in the development data and 26 names 
are in the test data, as shown Appendix Table 4 
and 5 separately.   
From Table 4 and 5, we can find that the ambi-
guity (the document number per cluster) distribu-
tion is much different between the development 
data and the test data.  In fact, the ambiguity var-
ies with a personal name in interest, such as the 
popularity of the name in the given corpus, the 
celebrity degree of the name, and so on.    
2.2 Evaluation  
In WePS, Artiles et al (2009) made an intensive 
study of clustering evaluation metrics, and found 
that B-Cubed metric is an appropriate evaluation 
approach. Moreover, in order to handle overlap-
ping clusters (i.e. a personal name in a document 
refers to more than one person entity in reality), 
we extend B-Cubed metric as Table 1, where S = 
{S1, S2, ?} is a system clustering  and R = {R1, 
R2, ?} is a gold-standard clustering. The final 
performance of a system clustering for a personal 
name is the F score (?= 0.5), and the final per-
formance of a system is the Mac F score, the aver-
age of the F scores of all personal names. 
Moreover, Artiles et al (2009) also discuss 
three cheat systems: one-in-one, all-in-one, and 
the hybrid cheat system. One-in-one assigns each 
document into a cluster, and in contrast, all-in-one 
put all documents into one cluster. The hybrid 
cheat system just incorporates all clusters both in 
one-in-one and all-in-one clustering. Although the 
hybrid cheat system can achieve fairly good per-
formance, it is not useful for real applications. In 
the formal test, these three systems serve as the 
baseline.  
 
 Formula 
 
Precision 
?
? ?
?
? ? ??
?
S
S d dR
 i
 i i j j
S
i
S S i
ji
R;R
|S|
|S|
|RS|
 
max
 
 
Recall 
?
? ?
?
? ? ??
?
R
R d dS
 i
 i i j j
R
i
R i
ji
R S;S
|R|
|R|
|SR|
 
max
 
Table 1: the formula of the modified B-cubed 
metrics 
3 The Diagnosis Test 
Because of no word delimiter, Chinese text proc-
essing often needs to do Chinese word segmenta-
tion first. In order to explore the relationship 
between personal name disambiguation and word 
segmentation, we provide a diagnosis data which 
attempts to examine the impact of word segmenta-
tion to disambiguation.  
Firstly, for each personal name, its correspond-
ing dataset will be manually divided into three 
groups as follows. The disambiguation system 
then runs for each group of documents. The three 
clustering outputs are merged into the final clus-
tering for that personal name.  
(1) Exactly matching: news articles contain-
ing personal names that exactly match 
the query personal name. 
(2) Partially matching: news articles contain-
ing personal names that are super-strings 
of the query personal name. For instance, 
an article that has a person named with 
????? (Gao Jun Tian)  is retrieved 
for the query personal name ???? (Gao 
Jun).  
(3) Discarded: news articles containing 
character sequences that match the query 
personal name string and however in fact 
are not a personal name. For instance, an 
article that has the string ??????
?? (Zui Gao Jun Shi Fa Yuan: supreme 
military court) is also retrieved for the  
personal name ???? (Gao Jun).  
 
This diagnosis test is designed to simulate the 
realistic scenario where Chinese word segmenta-
tion works before personal name disambiguation. 
If a Chinese word segmenter works perfectly, a 
word-based matching can be used to retrieve the 
documents containing a personal name, and arti-
cles in Groups (2) and (3) should not be returned. 
The personal name disambiguation task that is 
limited to the documents in Group (1) should be 
simpler. 
Moreover, in this diagnosis test, we propose a 
baseline based on the gold-standard word segmen-
tation as follows, namely the word-segment sys-
tem.  
1) All articles in the ?exactly matching? 
group are merged into a cluster, and all 
articles in the ?discarded? group are 
merged into a cluster. 
2) In the ?partially matching? group, enti-
ties exactly sharing the same personal 
name are merged into a cluster.  For ex-
ample, all articles containing ????? 
(Gao Jun Tian) are merged into a cluster, 
and all articles containing???? (Gao 
Jun Hua) are merged into another cluster. 
4 Campaign Design  
4.1 The Participants 
The task of Chinese personal name disambigua-
tion in news has attracted the participation of 10 
teams. As a team can submit at most 2 results, 
there are 17 submissions from the 10 teams in the 
formal test, and there are 11 submissions from 7 
teams in the diagnosis.  
4.2 System descriptions 
Regarding system architecture, all systems are 
based on clustering, and most of them comprise 
two components: feature extraction and clustering. 
However, NEU-1 and HITSZ_CITYU develop a 
different clustering, which in fact is a cascaded 
clustering. Taking the advantage of the properties 
of a news article, both systems first divide the 
dataset for a personal name into two groups ac-
cording to whether the person in question is a re-
porter of the news. They then choose a different 
strategy to make further clustering for each group.    
In terms of feature extraction, we find that all 
systems except SoochowHY use word segmenta-
tion as pre-processing. Moreover, most systems 
choose named entity detection to enhance their 
feature extraction. In addition, character-based 
bigrams are also used in some systems.  In Ap-
pendix Table 6, we give the summary of word 
segmentation and named entity detection used in 
the participating systems. 
Regarding clustering algorithms, agglomerative 
hierarchical clustering is popular in the submis-
sions. Moreover, we find that weight learning is 
very crucial for similarity matrix, which has a big 
impact to the final clustering performance. Be-
sides the popular Boolean and TFIDF weighting 
schemes, SoochowHY and NEU-2 use different 
weighting learning. NEU-2 manually assigns 
weights to different kinds of features. So-
ochowHY develops an algorithm that iteratively 
learns a weight for a character-based n-gram.  
5 Results  
We first provide the performances of the formal 
test, and make some analysis. We then present and 
discuss the performances of the diagnosis test.  
5.1 Results of the Formal test 
For the formal test, we show the performances of 
11 submissions from 10 teams in Table 2. For 
each team, we keep only the better result except 
the NEU team because they use different tech-
nologies in their two submissions (NEU_1 and 
NEU_2).  
From Table 2, we first observe that 7 submis-
sions perform better than the hybrid cheat system. 
In contrast, in Artiles et al (2009), only 3 teams 
can beat the hybrid system. From our analysis, 
this may attribute to the following facts.  
1) Personal name disambiguation on Chinese 
may be easier than the one on English. For 
example, one of key issues in personal name 
disambiguation is to capture the occurrences 
of a query name in text. However, various 
personal name expressions, such as the use of 
 Precision Recall Macro F  
NEU_1 95.76 88.37 91.47 
NEU_2 95.08 88.62 91.15 
HITSZ_CITYU 83.99 93.29 87.42 
ICL_1 83.68 92.23 86.94 
DLUT_1 82.69 91.33 86.36 
BUPT_1 80.33 94.52 85.79 
XMU 90.55 84.88 85.72 
Hybrid cheat system 73.48 100 82.37 
HIT_ITNLP_2 91.08 62.75 71.03 
BIT 80.2 68.75 68.4 
ALL_IN_ONE 52.54 100 61.74 
BUPT_pris02 72.39 58.35 57.68 
SoochowHY_2 84.51 44.17 51.42 
ONE_IN_ONE 94.42 14.41 21.07 
Table 2: The B-Cubed performances of the formal test  
  
 Precision Recall Macro F  
NEU_1 95.6 89.74 92.14 
NEU_2 94.53 89.99 91.66 
XMU 89.84 89.84 89.08 
ICL_1 84.53 93.42 87.96 
BUPT_1 80.43 95.41 86.18 
Word_segment system 71.11 100 80.92 
BUPT_pris01 77.91 75.09 74.25 
BIT 94.62 63.32 72.48 
SoochowHY 87.22 58.52 61.85 
Table 3: The B-Cubed performances of the diagnosis test   
 
middle names in English, cause many prob-
lems during recognizing of the occurrences 
of a personal name in interest. 
2) We works on news articles, which have less 
noisy information compared to webpages 
used in Artiles et al (2009). More efforts are 
put on the exploration directly on disam-
biguation, not on text pre-processing. Fur-
thermore, most of systems extract features 
based on some popular NLP techniques, such 
as Chinese word segmentation, named entity 
recognition and POS tagger. As those tools 
usually are developed based on news corpora, 
they should extract high-quality features for 
disambiguation in our task.  
 
We then notice that the NEU team achieves the 
best performance. From their system description, 
we find that they make some special processing 
just for this task. For example, they develop a per-
sonal name recognition system to detect the occur-
rences of a query name in a news article, and a 
cascaded clustering for different kinds of persons. 
5.2 Results of the Diagnosis test 
We present the performances of 8 submissions for 
the diagnosis test from 7 teams in Table 3 as the 
format of Table 2. Meanwhile, we use the word-
segment system as the baseline.  
  Comparing Table 2 and 3, we first find that the 
word-segment system has a lower performance 
than the hybrid cheat system although the word-
segment system is more useful for real applica-
tions. This implies the importance to develop an 
appropriate evaluation method for clustering. 
From Table 3, five submissions achieve better 
performances than the word-segment system.  
Given the gold-standard word segmentation on 
personal names in the diagnosis test, from Table 3, 
our total impression is that the top systems take 
less advantages, and the bottom systems take 
more. This indicates that bottom systems suffer 
from their low-quality word segmentation and 
named entity detection. For example, 
BUPT_pris01 increases ~22% F score (from 
52.81% to 74.25%). 
6 Conclusions 
This campaign follows the work of WePS, and 
explores Chinese personal name disambiguation 
on news. We examine two issues: one is for Chi-
nese word segmentation, and the other is noisy 
information. As Chinese word segmentation usu-
ally is a pre-processing for most NLP processing, 
we investigate the impact of word segmentation to 
disambiguation. To avoid noisy information for 
disambiguation, such as HTML tags in webpage 
used in WePS, we choose news article to work on 
so that we can capture how good the state-of-the-
art disambiguation technique is. 
References  
 Artiles, Javier, Julio Gonzalo and Satoshi Sekine.2007. 
The SemEval-2007 WePS Evaluation: Establishing 
a benchmark for the Web People Search Task. In 
Proceedings of Semeval 2007, Association for Com-
putational Linguistics. 
Artiles, Javier, Julio Gonzalo and Satoshi Sekine. 2009. 
WePS 2 Evaluation Campaign: overview of the Web 
People Search Clustering Task. In 2nd Web People 
Search Evaluation Workshop (WePS 2009), 18th 
WWW Conference. 
Bagga, Amit and Breck Baldwin.1998. Entity-based 
Cross-document Co-referencing Using the Vector 
Space Model. In Proceedings of the 17th Interna-
tional Conference on Computational Linguistics.  
Chen, Ying and James H. Martin. 2007. CU-COMSEM: 
Exploring Rich Features for Unsupervised Web Per-
sonal Name Disambiguation. In Proceedings of Se-
meval 2007, Association for Computational 
Linguistics.  
Chen, Ying, Sophia Yat Mei Lee and Chu-Ren Huang. 
2009. PolyUHK: A Robust Information Extraction 
System for Web Personal Names. In 2nd Web Peo-
ple Search Evaluation Workshop (WePS 2009), 18th 
WWW Conference.  
 
 
Appendix 
 
name document #  cluster # document #  per cluster 
?? 
155 37 4.19 
?? 
301 42 7.17 
?? 
300 5 60 
?? 
105 30 3.5 
?? 
156 42 3.71 
??? 
350 15 23.33 
?? 
269 70 3.84 
?? 
257 8 32.13 
?? 
211 109 1.94 
?? 
177 36 4.92 
?? 
358 165 2.17 
?? 
300 20 15 
?? 
140 57 2.46 
?? 
300 27 11.11 
?? 
296 73 4.05 
?? 
135 75 1.8 
?? 
297 14 21.21 
?? 
110 24 4.58 
?? 
207 68 3.04 
?? 
131 26 5.04 
?? 
145 22 6.59 
?? 
164 15 10.93 
??? 
247 20 12.35 
?? 
173 34 5.09 
??? 
171 21 8.14 
?? 
170 34 5 
?? 
195 32 6.09 
?? 
301 22 13.68 
?? 
318 76 4.18 
?? 
234 117 2 
?? 
134 9 14.89 
?? 
123 7 17.57 
 
6930 1352 5.13 
Table 4: The training data distribution 
 
 
name document #  cluster # document #  per cluster 
?? 
190 96 1.99 
?? 
191 5 38.2 
?? 
258 16 16.13 
??? 
224 32 7 
??? 
118 29 4.07 
?? 
239 21 11.38 
?? 
208 43 4.84 
??? 
201 17 11.82 
??? 
317 3 105.67 
??? 
151 6 25.17 
?? 
188 61 3.08 
??? 
200 2 100 
?? 
213 69 3.09 
??? 
182 5 36.4 
?? 
278 11 25.27 
?? 
180 4 45 
??? 
286 1 286 
?? 
206 38 5.42 
?? 
193 16 12.06 
??? 
172 9 19.11 
?? 
174 5 34.8 
?? 
299 39 7.67 
?? 
233 90 2.59 
?? 
300 13 23.08 
?? 
141 25 5.64 
??? 
262 13 20.15 
 
5604 669 8.38 
Table5: The test data distribution 
 
 
 Word segmentation Named Entity 
NEU Name: Neucsp 
Source: 1998 People's Daily   
Name: in-house 
HITSZ_CITYU   
ICL Name: LTP 
F score: 96.5% 
Source:  2nd SIGHAN 
Name: LTP   
 
DLUT   
BUPT Name: in-house 
F score: 96.5% 
Source: SIGHAN 2010 
 
XMU Name: in-house 
Source: 1998 People's Daily 
F score: 97.8% 
 
HIT_ITNLP Name: IRLAS 
Source: 1998 People's Daily 
F score: 97.4% 
Name: IRLAS 
 
BIT Name: ICTCLAS2010   
Precision: ~97% 
Source: 1998 People's Daily   
Name: ICTCLAS2010   
 
BUPT_pris Name: LTP 
 
Name: LTP 
SoochowHY None None 
Table 6: The summary of word segmentation and named entity detection used in the participants 
 
* LTP(Language Technology Platform) 
 
LSTC System for Chinese Word Sense Induction 
Peng Jin, Yihao Zhang, Rui Sun 
Laboratory of Intelligent Information Processing and Application  
Leshan Teachers? College 
jandp@pku.edu.cn,yhaozhang@163.com,dram_218@163.com 
 
Abstract 
This paper presents the Chinese word 
sense Induction system of Leshan 
Teachers? College. The system 
participates in the Chinese word sense 
Induction of task 4 in Back offs 
organized by the Chinese Information 
Processing Society of China (CIPS) and 
SIGHAN. The system extracts neighbor 
words and their POSs centered in the 
target words and selected the best one of 
four cluster algorithms: Simple KMeans, 
EM, Farthest First and Hierarchical 
Cluster based on training data. We 
obtained the F-Score of 60.5% on the 
training data otherwise the F-Score is 
57.89% on the test data provided by 
organizers. 
1. Introduction 
Automatically obtain the intended sense of 
polysemous words according to its context has 
been shown to improve performance in 
information retrieval? information extraction 
and machine translation. There are two ways to 
resolve this problem in view of machine 
learning, one is supervised classification and 
the other is unsupervised classification i.e. 
clustering. The former is word sense 
disambiguation (WSD) which relies on large 
scale, high quality manually annotated sense 
corpus, but building a sense-annotated corpus 
is a time-consuming and expensive project. 
Even the corpus were constructed, the system 
trained from this corpus show the low 
performance on different domain test corpus. 
The later is word sense induction (WSI) which 
needs not any training data, and it has become 
one of the most important topics in current 
computational linguistics. 
Chinese Information Processing Society of 
China (CIPS) and SIGHAN organized a task is 
intended to promote the research on Chinese 
WSI. We built a WSI system named 
LSTC-WSI system for this task. This system 
tried four cluster algorithms, i.e.  Simple 
KMeans?EM?Farthest First and Hierarchical 
Cluster implemented by weak 3.7.1 [6], and 
found Simple KMeans compete the other three 
ones according to their performances on 
training data. Finally, the results returned by 
Simple KMeans were submitted.  
2. Features Selection 
Following the feature selection in word sense 
disambiguation, we extract neighbor words and 
their POSs centered in the target words. Word 
segmented and POS-tag tool adapted Chinese 
Lexical Analysis System developed by Institute 
of Computing Technology. No other resource is 
used in the system. The window size of the 
context is set to 5 around the ambiguous word. 
The neighbor words which occur only once 
were removed. Each sample is represented as a 
vector, and feature form is binary: if it occurs 
in is 1 otherwise is 0. 
3. Clusters Algorithms 
Four cluster algorithms were tried in our 
system. I will introduce them simply in the 
next respectively. 
K-means clustering [1] is one of the simplest 
unsupervised learning algorithms that solve the 
well known clustering problem. The main idea 
is to define k centroids, one for each cluster. 
These centroids should be placed in a cunning 
way because of different location causes 
different result. So, the better choice is to place 
them as much as possible far away from each 
other. 
EM algorithm[2] is a method for finding 
maximum likelihood estimates of parameters in 
statistical models, where the model depends on 
unobserved latent variables. EM is an iterative 
method which alternates between performing 
an expectation (E) step, which computes the 
expectation of the log-likelihood evaluated 
using the current estimate for the latent 
variables, and maximization (M) step, which 
computes parameters maximizing the expected 
log-likelihood found on the E step. 
The Farthest First algorithm [3] is an 
implementation of the ?Farthest First Traversal 
Algorithm? by Hochbaum and Shmoys (1985). 
It finds fast, approximate clusters and may be 
useful as an initialiser for k-means.  
A hierarchical clustering [4] is the guarantee 
that for every k, the induced k clustering has 
cost at most eight times that of the optimal 
k-clustering. A hierarchical clustering of n data 
points is a recursive partitioning of the data 
into 2, 3, 4, . . . and finally n, clusters. Each 
intermediate clustering is made more 
fine-grained by dividing one of its clusters. 
4. Development 
4.1 Evaluation method 
We consider the gold standard as a solution to 
the clustering problem. All examples tagged 
with a given sense in the gold standard form a 
class. For the system output, the clusters are 
formed by instances assigned to the same sense 
tag. We will compare clusters output by the 
system with the classes in the gold standard 
and compute F-score as usual [5]. F-score is 
computed with the formula below.  
Suppose  is a class of the gold standard, 
and 
Cr
Si is a cluster of the system generated, then 
)/(**2),( RPRPScoreF SC ir +=?  (1) 
sizecluster total
cluster afor  examples labeledcorrectly  ofnumber  the=p
sizecluster  total
cluster afor  examples labeledcorrectly  ofnumber  the
 R =
 
Then for a given class Cr,  
)),(()( max SCC irr scoreF
S
scoreF
i
?=?
)(
1
Cr
c
r
FScore
n
nr
ScoreF ?
=
=?
  (2) 
where c is total number of classes, is the size 
of class 
nr
Cr , and is the total size. Participants 
will be required to induce the senses of the 
target word using only the dataset provided by 
the organizers.  
n
4.2 Data Set 
The organizers provide 50 Chinese training 
data of SIGHAN2010-WSI-SampleData. The 
training data contain 50 Chinese words; each 
word has 50 example sentences, and gives each 
word the total number of sense. The total 
number of sense is ranging from 2 to 21, but 
more cases are 2. In order to facilitate the team 
participating in the contest to do experiment, 
the organizers also provide answer to each 
word. 
In order to evaluating the system?s 
performance of all participating team, the 
organizers provide 100 test word and each 
word have 50 example sentences, the system of 
each participating team need to run out the 
results which the organizers need. 
4.3  System Setup 
We developed the LSTC-WSI system based on 
Weka. Firstly, we implemented the evaluation 
algorithm described in section 4.1. Then, the 
instances were represented as vectors according 
to the feature selection. Thirdly, four cluster 
algorithms from Weka were tried and set 
different thresholds for feature frequency. 
Because of paper length constraints, we could 
not list all the experience data we get. Table 1 
listed system performance when frequency 
threshold set two and without POS 
information. 
Table 1: The Performance on test data 
Target 
word 
Simple 
Kmeans 
EM 
Farthest 
First 
Hierar 
chical 
?? 0.618 0.680 0.538 0.649 
?? 0.404 0.365 0.400 0.327 
?? 0.711 0.557 0.672 0.636 
?? 0.626 0.700 0.536 0.570 
?? 0.571 0.555 0.572 0.573 
?? 0.789 0.596 0.680 0.548 
?? 0.704 0.617 0.704 0.682 
?? 0.568 0.495 0.461 0.583 
?? 0.5679 0.679 0.625 0.688 
?? 0.601 0.590 0.648 0.603 
?? 0.578 0.554 0.662 0.616 
?? 0.621 0.537 0.615 0.627 
?? 0.560 0.429 0.466 0.527 
?? 0.627 0.537 0.643 0.603 
?? 0.610 0.538 0.643 0.638 
?? 0.643 0.607 0.648 0.632 
?? 0.615 0.545 0.662 0.603 
?? 0.621 0.616 0.615 0.658 
?? 0.538 0.583 0.569 0.609 
?? 0.603 0.540 0.632 0.569 
?? 0.653 0.557 0.657 0.603 
?? 0.627 0.622 0.652 0.690 
?? 0.421 0.438 0.454 0.453 
?? 0.609 0.528 0.583 0.627 
?? 0.634 0.667 0.486 0.652 
?? 0.574 0.546 0.577 0.584 
? 0.462 0.429 0.518 0.501 
?? 0.661 0.584 0.584 0.602 
?? 0.430 0.501 0.549 0.418 
?? 0.596 0.644 0.647 0.654 
?? 0.614 0.580 0.672 0.708 
?? 0.666 0.600 0.615 0.595 
?? 0.638 0.590 0.540 0.678 
?? 0.841 0.734 0.662 0.618 
?? 0.613 0.562 0.670 0.568 
?? 0.635 0.617 0.646 0.649 
?? 0.603 0.594 0.615 0.577 
?? 0.644 0.635 0.661 0.560 
?? 0.599 0.595 0.624 0.638 
?? 0.588 0.575 0.587 0.508 
?? 0.699 0.723 0.673 0.643 
?? 0.585 0.596 0.666 0.603 
?? 0.643 0.639 0.666 0.656 
?? 0.624 0.537 0.663 0.608 
?? 0.632 0.525 0.629 0.617 
?? 0.451 0.472 0.490 0.477 
?? 0.613 0.625 0.6723 0.625 
?? 0.601 0.640 0.646 0.661 
?? 0.591 0.585 0.663 0.639 
?? 0.536 0.505 0.477 0.532 
We tried two ways for feature selection: the 
frequency of features and neighbor words? POS 
were taken into account or not. Table 2 shows 
the average performance on the test data via 
varying the parameter setting. Observing the 
results returned by Hierarchical cluster is very 
imbalance, we set the options ?-L WARD? in 
order to balance the number. 
Table 2: The Average Performance of 50 Training Data 
Features 
Simple 
Kmeans 
EM 
Farthest
First 
Hierar
chical
Word, 
Windows 5 
0.555 0.566 0.607 0.558
Word, 
Windows 5, 
Frequency 1
0.583 0.567 0.599 0.582
Word, 
Windows 5, 
Frequency 2
0.605 0.575 0.605 0.598
Word, 
Windows 5, 
Frequency 3
0.598 0.590 0.600 0.599
Word+POSs, 
Windows 5 
0.562 0.582 0.618 0.569
Word+POSs, 
Windows 5, 
Frequency 1
0.589 0.580 0.610 0.594
Word+POSs, 
Windows 5, 
Frequency 2
0.589 0.580 0.610 0.594
Compared with the average performance of the 
50 test data, we find the performance is best1 
when considering word only and setting the 
frequency is two at the same time simple 
KMeans was adapted. So, we use the same 
parameters setting and clustered the test data 
by simple KMeans. As table 2 shows, the 
F-Score is 60.5% on training data. But on test 
data, our system?s F-Score is 57.89% officially 
evaluated by task organizers. 
5. Conclusion and Future Works 
Four cluster algorithms are tried for Chinese 
word sense induction: Simple KMeans, EM, 
                                                        
1 Although ?Farthest First? got the highest score, the 
results of ?Farthest First? are too imbalance.  
Farthest First and Hierarchical Cluster. We 
construct different feature spaces and select out 
the best combination of cluster and feature 
space. Finally, we apply the best system to the 
test data. 
In the future, we will look for better cluster 
algorithms for word sense induction. 
Furthermore, we observe that it is different 
from word sense disambiguation, different part 
of speech will cause the polysemy. We will 
make use of this character to improve our 
system. 
Acknowledgements 
This work is supported by the Open 
Projects Program of Key Laboratory of 
Computational Linguistics(Peking 
University)?Ministry of Education, Grant 
No. KLCL-1002. This work is also 
supported by Leshan Teachers? College, 
Grant No. Z1046.  
References 
[1] Dekang Lin, Xiaoyun Wu. Phrase Clustering for 
Discriminative Learning. Proceedings of 
ACL ,2009. 
[2]Neal R, & Hinton G. A view of the EM algorithm 
that justifies incremental, sparse, and other 
variants. Learning in Graphical Models, 89, 
355?368. 
[3] Jon Gibson, Firat Tekiner, Peter Halfpenny. 
NCeSS Project: Data Mining for Social Scientists. 
Research Computing Services, University of 
Manchester, U.K.  
[4] Sanjoy Dasgupta, Philip M. Long. Performance 
guarantees for hierarchical clustering. Journal of 
Computer and System Sciences, 555?569, 2005. 
[5] Eneko Agirre, Aitor Soroa. Semeval-2007 Task 
02:Evaluating Word Sense Induction and 
Discrimination Systems. Proceedings of 
SemEval-2007, pages 7?12, 2007. 
[6] http://www.cs.waikato.ac.nz/ml/weka/
