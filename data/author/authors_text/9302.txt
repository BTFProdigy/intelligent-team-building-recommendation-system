Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 585?592,
Sydney, July 2006. c?2006 Association for Computational Linguistics
An Iterative Implicit Feedback Approach to Personalized Search 
 
Yuanhua Lv 1, Le Sun 2, Junlin Zhang 2, Jian-Yun Nie 3, Wan Chen 4, and Wei Zhang 2 
1, 2 Institute of Software, Chinese Academy of Sciences, Beijing, 100080, China 
3 University of Montreal, Canada 
1 lvyuanhua@gmail.com 
2 {sunle, junlin01, zhangwei04}@iscas.cn 
3 nie@iro.umontreal.ca   4 chenwan@nus.edu.sg 
 
Abstract 
General information retrieval systems are 
designed to serve all users without con-
sidering individual needs. In this paper, 
we propose a novel approach to person-
alized search. It can, in a unified way, 
exploit and utilize implicit feedback in-
formation, such as query logs and imme-
diately viewed documents. Moreover, our 
approach can implement result re-ranking 
and query expansion simultaneously and 
collaboratively. Based on this approach, 
we develop a client-side personalized web 
search agent PAIR (Personalized Assis-
tant for Information Retrieval), which 
supports both English and Chinese. Our 
experiments on TREC and HTRDP col-
lections clearly show that the new ap-
proach is both effective and efficient. 
1 Introduction 
Analysis suggests that, while current information 
retrieval systems, e.g., web search engines, do a 
good job of retrieving results to satisfy the range 
of intents people have, they are not so well in 
discerning individuals? search goals (J. Teevan et 
al., 2005). Search engines encounter problems 
such as query ambiguity and results ordered by 
popularity rather than relevance to the user?s in-
dividual needs. 
To overcome the above problems, there have 
been many attempts to improve retrieval accuracy 
based on personalized information. Relevance 
Feedback (G. Salton and C. Buckley, 1990) is the 
main post-query method for automatically im-
proving a system?s accuracy of a user?s individual 
need. The technique relies on explicit relevance 
assessments (i.e. indications of which documents 
contain relevant information). Relevance feed-
back has been proved to be quite effective for 
improving retrieval accuracy (G. Salton and C. 
Buckley, 1990; J. J. Rocchio, 1971). However, 
searchers may be unwilling to provide relevance 
information through explicitly marking relevant 
documents (M. Beaulieu and S. Jones, 1998). 
Implicit Feedback, in which an IR system un-
obtrusively monitors search behavior, removes 
the need for the searcher to explicitly indicate 
which documents are relevant (M. Morita and Y. 
Shinoda, 1994). The technique uses implicit 
relevance indications, although not being as ac-
curate as explicit feedback, is proved can be an 
effective substitute for explicit feedback in in-
teractive information seeking environments (R. 
White et al, 2002). In this paper, we utilize the 
immediately viewed documents, which are the 
clicked results in the same query, as one type of 
implicit feedback information. Research shows 
that relative preferences derived from immedi-
ately viewed documents are reasonably accurate 
on average (T. Joachims et al, 2005). 
Another type of implicit feedback information 
that we exploit is users? query logs. Anyone who 
uses search engines has accumulated lots of click 
through data, from which we can know what 
queries were, when queries occurred, and which 
search results were selected to view. These query 
logs provide valuable information to capture us-
ers? interests and preferences. 
Both types of implicit feedback information 
above can be utilized to do result re-ranking and 
query expansion, (J. Teevan et al, 2005; Xuehua 
Shen. et al, 2005) which are the two general ap-
proaches to personalized search. (J. Pitkow et al, 
2002) However, to the best of our knowledge, 
how to exploit these two types of implicit feed-
back in a unified way, which not only brings col-
laboration between query expansion and result 
re-ranking but also makes the whole system more 
concise, has so far not been well studied in the 
previous work. In this paper, we adopt HITS al-
gorithm (J. Kleinberg, 1998), and propose a 
585
HITS-like iterative approach addressing such a 
problem. 
Our work differs from existing work in several 
aspects: (1) We propose a HITS-like iterative 
approach to personalized search, based on which, 
implicit feedback information, including imme-
diately viewed documents and query logs, can be 
utilized in a unified way. (2) We implement re-
sult re-ranking and query expansion simultane-
ously and collaboratively triggered by every 
click. (3) We develop and evaluate a client-side 
personalized web search agent PAIR, which 
supports both English and Chinese. 
The remaining of this paper is organized as 
follows. Section 2 describes our novel approach 
for personalized search. Section 3 provides the 
architecture of PAIR system and some specific 
techniques. Section 4 presents the details of the 
experiment. Section 5 discusses the previous 
work related to our approach. Section 6 draws 
some conclusions of our work. 
2 Iterative Implicit Feedback Approach 
We propose a HITS-like iterative approach for 
personalized search. HITS (Hyperlink-Induced 
Topic Search) algorithm, first described by (J. 
Kleinberg, 1998), was originally used for the 
detection of high-score hub and authority web 
pages. The Authority pages are the central web 
pages in the context of particular query topics. 
The strongest authority pages consciously do not 
link one another1 ? they can only be linked by 
some relatively anonymous hub pages. The mu-
tual reinforcement principle of HITS states that a 
web page is a good authority page if it is linked by 
many good hub pages, and that a web page is a 
good hub page if it links many good authority 
pages. A directed graph is constructed, of which 
the nodes represent web pages and the directed 
edges represent hyperlinks. After iteratively 
computing based on the reinforcement principle, 
each node gets an authority score and a hub score. 
In our approach, we exploit the relationships 
between documents and terms in a similar way to 
HITS. Unseen search results, those results which 
are retrieved from search engine yet not been 
presented to the user, are considered as ?authority 
pages?. Representative terms are considered as 
?hub pages?. Here the representative terms are the 
terms extracted from and best representing the 
implicit feedback information. Representative 
terms confer a relevance score to the unseen 
                                                          
1 For instance, There is hardly any other company?s Web 
page linked from ?http://www.microsoft.com/? 
search results ? specifically, the unseen search 
results, which contain more good representative 
terms, have a higher possibility of being relevant; 
the representative terms should be more repre-
sentative, if they occur in the unseen search re-
sults that are more likely to be relevant. Thus, 
also there is mutual reinforcement principle ex-
isting between representative terms and unseen 
search results. By the same token, we constructed 
a directed graph, of which the nodes indicate un-
seen search results and representative terms, and 
the directed edges represent the occurrence of the 
representative terms in the unseen search results. 
The following Table 1 shows how our approach 
corresponds to HITS algorithm. 
 
The Directed Graph 
Approaches
Nodes Edges 
HITS Authority Pages Hub Pages Hyperlinks
Our  
Approach
Unseen Search 
Results 
Representative 
Terms Occurrence
2
Table 1. Our approach versus HITS. 
 
Because we have already known that the rep-
resentative terms are ?hub pages?, and that the 
unseen search results are ?authority pages?, with 
respect to the former, only hub scores need to be 
computed; with respect to the latter, only author-
ity scores need to be computed. 
Finally, after iteratively computing based on 
the mutual reinforcement principle we can 
re-rank the unseen search results according to 
their authority scores, as well as select the repre-
sentative terms with highest hub scores to ex-
pand the query. Below we present how to con-
struct a directed graph to begin with. 
2.1 Constructing a Directed Graph 
We can view the unseen search results and the 
representative terms as a directed graph G = (V, E). 
A sample directed graph is shown in Figure 1: 
 
 
Figure 1. A sample directed graph. 
 
The nodes V correspond to the unseen search 
results (the rectangles in Figure 1) and the repre-
                                                          
2 The occurrence of the representative terms in the unseen 
search results. 
586
sentative terms (the circles in Figure 1); a di-
rected edge ?p?q?E? is weighed by the fre-
quency of the occurrence of a representative term 
p in an unseen search result q (e.g., the number 
put on the edge ?t1?r2? indicates that t1 occurs 
twice in r2). We say that each representative term 
only has an out-degree which is the number of the 
unseen search results it occurs in, as well as that 
each unseen search result only has an in-degree 
which is the count of the representative terms it 
contains. Based on this, we assume that the un-
seen search results and the representative terms 
respectively correspond to the authority pages 
and the hub pages ? this assumption is used 
throughout the proposed algorithm. 
2.2 A HITS-like Iterative Algorithm 
In this section, we present how to initialize the 
directed graph and how to iteratively compute the 
authority scores and the hub scores. And then 
according to these scores, we show how to re-rank 
the unseen search results and expand the initial 
query. 
Initially, each unseen search result of the query 
are considered equally authoritative, that is, 
0 0 0
1 2 | |
1 | |
Y
Yy y y= ?= =                  (1) 
Where vector Y indicates authority scores of the 
overall unseen search results, and |Y| is the size of 
such a vector. Meanwhile, each representative 
term, with the term frequency tfj in the history 
query logs that have been judged related to the 
current query, obtains its hub score according to 
the follow formulation: 
0
| |
1
X
j i
j itf tfx == ?                       (2) 
Where vector X indicates hub scores of the overall 
representative terms, and |X| is the size of the 
vector X. The nodes of the directed graph are 
initialized in this way. Next, we associate each 
edge with a weight: 
,( )ji i jw tft r? =                     (3) 
Where tfi,j indicates the term frequency of the 
representative term ti occurring in the unseen 
search result rj; ?w(ti? rj)? is the weight of edge 
that link from ti to rj. For instance, in Figure 1, 
w(t1? r2) = 2. 
After initialization, the iteratively computing of 
hub scores and authority scores starts. 
The hub score of each representative term is 
re-computed based on three factors: the authority 
scores of each unseen search result where this 
term occurs; the occurring frequency of this term 
in each unseen search result; the total occurrence 
of every representative term in each unseen search 
result. The formulation for re-computing hub 
scores is as follows: 
( 1)
:
:
( )
( )'
k ji
i
jnji
jn
k
jj
n
w
wt r
t r
t ryx t r
+
?  ?
?  ?
?= ?? ?
    (4) 
Where x`i(k+1) is the hub score of a representative 
term ti after (k+1)th iteration; yjk is the authority 
score of an unseen search result rj after kth itera-
tion; ??j: ti?rj? indicates the set of all unseen 
search results those ti occurs in; ??n: tn?rj? in-
dicates the set of all representative terms those rj 
contains. 
The authority score of each unseen search re-
sult is also re-computed relying on three factors: 
the hub scores of each representative term that 
this search result contains; the occurring fre-
quency of each representative term in this search 
result; the total occurrence of each representative 
term in every unseen search results. The formu-
lation for re-computing authority scores is as 
follows: 
( 1)
:
:
( )
( )'
k k
ij
miji
mi
ji
i
m
w
wt r
t r
t ry x t r
+
?  ?
?  ?
?= ?? ?
    (5) 
Where y`j(k+1) is the authority score of an unseen 
search result rj after (k+1)th iteration; xik  is the 
hub score of a representative term ti after kth it-
eration; ??i: ti?rj? indicates the set of all repre-
sentative terms those rj contains; ??m: ti?rm? 
indicates the set of all unseen search results those 
ti occurs in. 
After re-computation, the hub scores and the 
authority scores are normalized to 1. The formu-
lation for normalization is as follows: 
| | | |
1 1
and
' '
' '
j i
iY Xj
kkk k
y xy x
y x= =
=   =
? ?
              (6) 
The iteration, including re-computation and 
normalization, is repeated until the changes of the 
hub scores and the authority scores are smaller 
than some predefined threshold ? (e.g. 10-6). 
Specifically, after each repetition, the changes in 
authority scores and hub scores are computed 
using the following formulation: 
2 2( 1) ( 1)
| | | |
1 1
( ) ( )
k k k k
i ij j
Y x
j i
c y y x x
+ +
= =
= ? + ?? ?        (7) 
The iteration stops if c<?. Moreover, the itera-
tion will also stop if repetition has reached a 
587
predefined times k (e.g. 30). The procedure of the 
iteration is shown in Figure 2. 
As soon as the iteration stops, the top n unseen 
search results with highest authority scores are 
selected and recommended to the user; the top m 
representative terms with highest hub scores are 
selected to expand the original query. Here n is a 
predefined number (in PAIR system we set n=3, 
n is given a small number because using implicit 
feedback information is sometimes risky.) m is 
determined according to the position of the big-
gest gap, that is, if ti ? ti+1 is bigger than the gap 
of any other two neighboring ones of the top half 
representative terms, then m is given a value i. 
Furthermore, some of these representative terms 
(e.g. top 50% high score terms) will be again used 
in the next time of implementing the iterative 
algorithm together with some newly incoming 
terms extracted from the just now click. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 2. The HITS-like iterative algorithm. 
 
3 Implementation 
3.1 System Design 
In this section, we present our experimental sys-
tem PAIR, which is an IE Browser Helper Object 
(BHO) based on the popular Web search engine 
Google. PAIR has three main modules: Result 
Retrieval module, User Interactions module, and 
Iterative Algorithm module. The architecture is 
shown in Figure 3. 
The Result Retrieval module runs in back-
grounds and retrieves results from search engine. 
When the query has been expanded, this module 
will use the new keywords to continue retrieving. 
The User Interactions module can handle three 
types of basic user actions: (1) submitting a query; 
(2) clicking to view a search result; (3) clicking 
the ?Next Page? link. For each of these actions, 
the system responds with: (a) exploiting and ex-
tracting representative terms from implicit feed-
back information; (b) fetching the unseen search 
results via Results Retrieval module; (c) sending 
the representative terms and the unseen search 
results to Iterative Algorithm module. 
 
 
Figure 3. The architecture of PAIR. 
 
The Iterative Algorithm module implements 
the HITS-like algorithm described in section 2. 
When this module receives data from User In-
teractions module, it responds with: (a) iteratively 
computing the hub scores and authority scores; (b) 
re-ranking the unseen search results and expand-
ing the original query. 
Some specific techniques for capturing and 
exploiting implicit feedback information are de-
scribed in the following sections. 
3.2 Extract Representative Terms from 
Query Logs 
We judge whether a query log is related to the 
current query based on the similarity between the 
query log and the current query text. Here the 
query log is associated with all documents that 
the user has selected to view. The form of each 
query log is as follows 
<query text><query time> [clicked documents]* 
The ?clicked documents? consist of URL, title 
and snippet of every clicked document. The rea-
son why we utilize the query text of the current 
query but not the search results (including title, 
snippet, etc.) to compute the similarity, is out of 
consideration for efficiency. If we had used the 
search results to determine the similarity, the 
computation could only start once the search en-
gine has returned the search results. In our method, 
instead, we can exploit query logs while search 
engine is doing retrieving. Notice that although 
our system only utilizes the query logs in the last 
24 hours; in practice, we can exploit much more 
because of its low computation cost with respect 
to the retrieval process performed in parallel.
Iterate (T, R, k, ?) 
T: a collection of m terms 
R: a collection of n search results 
k: a natural number 
?: a predefined threshold 
Apply (1) to initialize Y. 
Apply (2) to initialize X. 
Apply (3) to initialize W. 
For i = 1, 2?, k 
Apply (4) to (Xi-1, Yi-1) and obtain X`i. 
Apply (5) to (Xi-1, Yi-1) and obtain Y`i. 
Apply (6) to Normalize X`i and Y`i, and respectively 
obtain Xi and Yi. 
Apply (7) and obtain c. 
If c<?, then break. 
End 
Return (X, Y). 
588
Table 2. Sample results of re-ranking. The search results in boldface are the ones that our system rec-
ommends to the user. ?-3? and ?-2? in the right side of some results indicate the how their ranks descend. 
 
We use the standard vector space retrieval 
model (G. Salton and M. J. McGill, 1983) to 
compute the similarity. If the similarity between 
any query log and the current query exceeds a 
predefined threshold, the query log will be con-
sidered to be related to current query. Our system 
will attempt to extract some (e.g. 30%) represen-
tative terms from such related query logs ac-
cording to the weights computed by applying the 
following formulation: 
( )i i iw f idftt =                      (8) 
Where tfi and idfi respectively are the term fre-
quency and inverse document frequency of ti in 
the clicked documents of a related query log. 
This formulation means that a term is more rep-
resentative if it has a higher frequency as well as 
a broader distribution in the related query log. 
3.3 Extract Representative Terms from 
Immediately Viewed Documents 
The representative terms extracted from immedi-
ately viewed documents are determined based on 
three factors: term frequency in the immediately 
viewed document, inverse document frequency in 
the entire seen search results, and a discriminant 
value. The formulation is as follows:  
( ) ( )Ni ii i
r ddw d
x xtf idfx x= ? ?             (9) 
Where tfxidr is the term frequency of term xi in the 
viewed results set dr; tfxidr is the inverse document 
frequency of xi in the entire seen results set dN. 
And the discriminant value d(xi) of xi is computed 
using the weighting schemes F2 (S. E. Robertson 
and K. Sparck Jones, 1976) as follows: 
( ) ln
( ) ( )i
r Rd
n r N Rx = ? ?
                 (10) 
Where r is the number of the immediately viewed 
documents containing term xi; n is the number of 
the seen results containing term xi; R is the num-
ber of the immediately viewed documents in the 
query; N is the number of the entire seen results.  
3.4 Sample Results 
Unlike other systems which do result re-ranking 
and query expansion respectively in different 
ways, our system implements these two functions 
simultaneously and collaboratively ?  Query 
expansion provides diversified search results 
which must rely on the use of re-ranking to be 
moved forward and recommended to the user. 
 
 
Figure 4. A screen shot for query expansion. 
 
After iteratively computing using our approach, 
the system selects some search results with top 
highest authority scores and recommends them to 
the user. In Table 2, we show that PAIR suc-
cessfully re-ranks the unseen search results of 
?jaguar? respectively using the immediately 
Google result PAIR result  
query = ?jaguar? query = ?jaguar? After the 4th result being clicked 
query = ?jaguar? 
?car? ? query logs 
1 Jaguar www.jaguar.com/ 
Jaguar 
www.jaguar.com/ 
Jaguar UK - Jaguar Cars 
www.jaguar.co.uk/ 
2 Jaguar CA - Jaguar Cars www.jaguar.com/ca/en/ 
Jaguar CA - Jaguar Cars 
www.jaguar.com/ca/en/ 
Jaguar UK - R is for? 
www.jaguar-racing.com/ 
3 Jaguar Cars www.jaguarcars.com/ 
Jaguar Cars 
www.jaguarcars.com/ 
Jaguar 
www.jaguar.com/ 
4 Apple - Mac OS X www.apple.com/macosx/ 
Apple - Mac OS X 
www.apple.com/macosx/ 
Jaguar CA - Jaguar Cars 
www.jaguar.com/ca/en/                      -2 
5 Apple - Support ? www.apple.com/support/... 
Amazon.com: Mac OS X 10.2? 
www.amazon.com/exec/obidos/... 
Jaguar Cars 
www.jaguarcars.com/                        -2 
6 Jaguar UK - Jaguar Cars www.jaguar.co.uk/ 
Mac OS X 10.2 Jaguar? 
arstechnica.com/reviews/os? 
Apple - Mac OS X 
www.apple.com/macosx/                     -2 
7 Jaguar UK - R is for? www.jaguar-racing.com/ 
Macworld: News: Macworld? 
maccentral.macworld.com/news/? 
Apple - Support ? 
www.apple.com/support/...                    -2 
8 Jaguar dspace.dial.pipex.com/? 
Apple - Support? 
www.apple.com/support/...                -3 
Jaguar 
dspace.dial.pipex.com/? 
9 Schr?dinger -> Home www.schrodinger.com/ 
Jaguar UK - Jaguar Cars 
www.jaguar.co.uk/                       -3 
Schr?dinger -> Home 
www.schrodinger.com/ 
10 Schr?dinger -> Site Map www.schrodinger.com/... 
Jaguar UK - R is for? 
www.jaguar-racing.com/                  -3 
Schr?dinger -> Site Map 
www.schrodinger.com/... 
589
viewed documents and the query logs. Simulta-
neously, some representative terms are selected 
to expand the original query. In the query of 
?jaguar? (without query logs), we click some 
results about ?Mac OS?, and then we see that a 
term ?Mac? has been selected to expand the 
original query, and some results of the new query 
?jaguar Mac? are recommended to the user under 
the help of re-ranking, as shown in Figure 4. 
4 Experiment 
4.1 Experimental Methodology 
It is a challenge to quantitatively evaluate the 
potential performance improvement of the pro-
posed approach over Google in an unbiased way 
(D. Hawking et al, 1999; Xuehua Shen et al, 
2005). Here, we adopt a similar quantitative 
evaluation as what Xuehua Shen et al (2005) do 
to evaluate our system PAIR and recruit 9 stu-
dents who have different backgrounds to partici-
pate in our experiment. We use query topics from 
TREC 2005 and 2004 Hard Track, TREC 2004 
Terabyte track for English information retrieval,3 
and use query topics from HTRDP 2005 Evalua-
tion for Chinese information retrieval.4 The rea-
son why we utilize multiple TREC tasks rather 
than using a single one is that more queries are 
more likely to cover the most interesting topics 
for each participant. 
Initially, each participant would freely choose 
some topics (typically 5 TREC topics and 5 
HTRDP topics). Each query of TREC topics will 
be submitted to three systems: UCAIR 5 (Xue-
hua Shen et al, 2005), ?PAIR No QE? (PAIR 
system of which the query expansion function is 
blocked) and PAIR. Each query of HTRDP topics 
needs only to be submitted to ?PAIR No QE? and 
PAIR. We do not evaluate UCAIR using HTRDP 
topics, since it does not support Chinese. For each 
query topic, the participants use the title of the 
topic as the initial keyword to begin with. Also 
they can form some other keywords by them-
selves if the title alone fails to describe some de-
tails of the topic. There is no limit on how many 
queries they must submit. During each query 
process, the participant may click to view some 
results, just as in normal web search. 
Then, at the end of each query, search results 
from these different systems are randomly and 
anonymously mixed together so that every par-
                                                          
3 Text REtrieval Conference. http://trec.nist.gov/ 
4 2005 HTRDP Evaluation. http://www.863data.org.cn/ 
5 The latest version released on November 11, 2005. 
http://sifaka.cs.uiuc.edu/ir/ucair/ 
ticipant would not know where a result comes 
from. The participants would judge which of 
these results are relevant. 
At last, we respectively measure precision at 
top 5, top 10, top 20 and top 30 documents of 
these system. 
4.2 Results and Analysis 
Altogether, 45 TREC topics (62 queries in all) are 
chosen for English information retrieval. 712 
documents are judged as relevant from Google 
search results. The corresponding number of 
relevant documents from UCAIR, ?PAIR No QE? 
and PAIR respectively is: 921, 891 and 1040. 
Figure 5 shows the average precision of these four 
systems at top n documents among such 45 TREC 
topics. 
 
 
Figure 5. Average precision for TREC topics. 
 
45 HTRDP topics (66 queries in all) are chosen 
for Chinese information retrieval. 809 documents 
are judged as relevant from Google search results. 
The corresponding number of relevant documents 
from ?PAIR No QE? and PAIR respectively is: 
1198 and 1416. Figure 6 shows the average pre-
cision of these three systems at top n documents 
among such 45 HTRDP topics. 
 
 
Figure 6. Average precision for HTRDP topics. 
 
PAIR and ?PAIR No QE? versus Google 
We can see clearly from Figure 5 and Figure 6 
that the precision of PAIR is improved a lot 
comparing with that of Google in all measure-
590
ments. Moreover, the improvement scale in-
creases from precision at top 10 to that of top 30. 
One explanation for this is that the more implicit 
feedback information generated, the more repre-
sentative terms can be obtained, and thus, the 
iterative algorithm can perform better, leading to 
more precise search results. ?PAIR No QE? also 
significantly outperforms Google in these meas-
urements, however, with query expansion, PAIR 
can perform even better. Thus, we say that result 
re-ranking and query expansion both play an 
important role in PAIR. 
Comparing Figure 5 with Figure 6, one can see 
that the improvement of PAIR versus Google in 
Chinese IR is even larger than that of English IR. 
One explanation for this is that: before imple-
menting the iterative algorithm, each Chinese 
search result, including title and snippet, is seg-
mented into words (or phrases). And only the 
noun, verb and adjective of these words (or 
phrases) are used in next stages, whereas, we only 
remove the stop words for English search result. 
Another explanation is that there are some Chi-
nese web pages with the same content. If one of 
such pages is clicked, then, occasionally some 
repetition pages are recommended to the user. 
However, since PAIR is based on the search re-
sults of Google and the information concerning 
the result pages that PAIR can obtained is limited, 
which leads to it difficult to avoid the replica-
tions. 
PAIR and ?PAIR No QE? versus UCAIR 
In Figure 5, we can see that the precision of 
?PAIR No QE? is better than that of UCAIR 
among top 5 and top 10 documents, and is almost 
the same as that of UCAIR among top 20 and top 
30 documents. However, PAIR is much better 
than UCAIR in all measurements. This indicates 
that result re-ranking fails to do its best without 
query expansion, since the relevant documents in 
original query are limited, and only the re-ranking 
method alone cannot solve the ?relevant docu-
ments sparseness? problem. Thus, the query ex-
pansion method, which can provide fresh and 
relevant documents, can help the re-ranking 
method to reach an even better performance. 
Efficiency of PAIR 
The iteration statistic in evaluation indicates that 
the average iteration times of our approach is 22 
before convergence on condition that we set the 
threshold ? = 10-6. The experiment shows that the 
computation time of the proposed approach is 
imperceptible for users (less than 1ms.) 
5 Related Work 
There have been many prior attempts to person-
alized search. In this paper, we focus on the re-
lated work doing personalized search based on 
implicit feedback information. 
Some of the existing studies capture users? in-
formation need by exploiting query logs. For 
example, M. Speretta and S. Gauch (2005) build 
user profiles based on activity at the search site 
and study the use of these profiles to provide 
personalized search results. F. Liu et al (2002) 
learn user's favorite categories from his query 
history. Their system maps the input query to a set 
of interesting categories based on the user profile 
and confines the search domain to these catego-
ries. Some studies improve retrieval performance 
by exploiting users? browsing history (F. Tanud-
jaja and L. Mu, 2002; M. Morita and Y. Shinoda, 
1994) or Web communities (A. Kritikopoulos 
and M. Sideri, 2003; K. Sugiyama et al, 2004) 
Some studies utilize client side interactions, for 
example, K. Bharat (2000) automatically discov-
ers related material on behalf of the user by 
serving as an intermediary between the user and 
information retrieval systems. His system ob-
serves users interacting with everyday applica-
tions and then anticipates their information needs 
using a model of the task at hand. Some latest 
studies combine several types of implicit feed-
back information. J. Teevan et al (2005) explore 
rich models of user interests, which are built 
from both search-related information, such as 
previously issued queries and previously visited 
Web pages, and other information about the user 
such as documents and email the user has read 
and created. This information is used to re-rank 
Web search results within a relevance feedback 
framework. 
Our work is partly inspired by the study of 
Xuehua Shen et al (2005), which is closely re-
lated to ours in that they also exploit immediately 
viewed documents and short-term history queries, 
implement query expansion and re-ranking, and 
develop a client-side web search agents that per-
form eager implicit feedback. However, their 
work differs from ours in three ways: First, they 
use the cosine similarity to implement query ex-
pansion, and use Rocchio formulation (J. J. 
Rocchio, 1971) to re-rank the search results. 
Thus, their query expansion and re-ranking are 
computed separately and are not so concise and 
collaborative. Secondly, their query expansion is 
based only on the past queries and is imple-
mented before the query, which leads to that 
591
their query expansion does not benefit from 
user?s click through data. Thirdly, they do not 
compute the relevance of search results and the 
relativity of expanded terms in an iterative fash-
ion. Thus, their approach does not utilize the re-
lation among search results, among expanded 
terms, and between search results and expanded 
terms. 
6 Conclusions 
In this paper, we studied how to exploit implicit 
feedback information to improve retrieval accu-
racy. Unlike most previous work, we propose a 
novel HITS-like iterative algorithm that can 
make use of query logs and immediately viewed 
documents in a unified way, which not only 
brings collaboration between query expansion 
and result re-ranking but also makes the whole 
system more concise. We further propose some 
specific techniques to capture and exploit these 
two types of implicit feedback information. Us-
ing these techniques, we develop a client-side 
web search agent PAIR. Experiments in English 
and Chinese collections show that our approach 
is both effective and efficient. 
However, there is still room to improve the 
performance of the proposed approach, such as 
exploiting other types of personalized informa-
tion, choosing some more effective strategies to 
extract representative terms, studying the effects 
of the parameters used in the approach, etc. 
Acknowledgement 
We would like to thank the anonymous review-
ers for their helpful feedback and corrections, 
and to the nine participants of our evaluation ex-
periments. Additionally, this work is supported 
by the National Science Fund of China under 
contact 60203007. 
References 
A. Kritikopoulos and M. Sideri, 2003. The Compass 
Filter: Search engine result personalization using 
Web communities. In Proceedings of ITWP, pages 
229-240. 
D. Hawking, N. Craswell, P.B. Thistlewaite, and D. 
Harman, 1999. Results and challenges in web 
search evaluation. Computer Networks, 
31(11-16):1321?1330. 
F. Liu, C. Yu, and W. Meng, 2002. Personalized web 
search by mapping user queries to categories. In 
Proceedings of CIKM, pages 558-565. 
F. Tanudjaja and L. Mu, 2002. Persona: a contextual-
ized and personalized web search. HICSS. 
G. Salton and M. J. McGill, 1983. Introduction to 
Modern Information Retrieval. McGraw-Hill. 
G. Salton and C. Buckley, 1990. Improving retrieval 
performance by relevance feedback. Journal of the 
American Society for Information Science, 
41(4):288-297. 
J. J. Rocchio, 1971. Relevance feedback in informa-
tion retrieval. In The SMART Retrieval System :  
Experiments in Automatic Document Processing, 
pages 313?323. Prentice-Hall Inc. 
J. Kleinberg, 1998. Authoritative sources in a hyper-
linked environment. ACM, 46(5):604?632. 
J. Pitkow, H. Schutze, T. Cass, R. Cooley, D. 
Turnbull, A. Edmonds, E. Adar, and T. Breuel, 
2002. Personalized search. Communications of the 
ACM, 45(9):50-55. 
J. Teevan, S. T. Dumais, and E. Horvitz, 2005. Per-
sonalizing search via automated analysis of interests 
and activities. In Proceedings of SIGIR, pages 
449-456. 
K. Bharat, 2000. SearchPad: Explicit capture of 
search context to support Web search. Computer 
Networks, 33(1-6): 493-501. 
K. Sugiyama, K. Hatano, and M. Yoshikawa, 2004. 
Adaptive Web search based on user profile con-
structed without any effort from user. In Proceed-
ings of WWW, pages 675-684. 
M. Beaulieu and S. Jones, 1998. Interactive searching 
and interface issues in the okapi best match retrieval 
system. Interacting with Computers, 10(3):237-248. 
M. Morita and Y. Shinoda, 1994. Information filtering 
based on user behavior analysis and best match text 
retrieval. In Proceedings of SIGIR, pages 272?281. 
M. Speretta and S. Gauch, 2005. Personalizing search 
based on user search history. Web Intelligence, 
pages 622-628. 
R. White, I. Ruthven, and J. M. Jose, 2002. The use of 
implicit evidence for relevance feedback in web 
retrieval. In Proceedings of ECIR, pages 93?109. 
S. E. Robertson and K. Sparck Jones, 1976. Relevance 
weighting of search terms. Journal of the 
American Society for Information Science, 
27(3):129-146. 
T. Joachims, L. Granka, B. Pang, H. Hembrooke, and 
G. Gay, 2005. Accurately Interpreting Clickthrough 
Data as Implicit Feedback, In Proceedings of 
SIGIR, pages 154-161. 
Xuehua Shen, Bin Tan, and Chengxiang Zhai, 2005. 
Implicit User Modeling for Personalized Search. In 
Proceedings of CIKM, pages 824-831. 
592
IBM MASTOR SYSTEM: Multilingual Automatic Speech-to-speech Translator * 
 
Yuqing Gao, Liang Gu, Bowen Zhou, Ruhi Sarikaya, Mohamed Afify, Hong-Kwang Kuo, 
Wei-zhong Zhu, Yonggang Deng, Charles Prosser, Wei Zhang and Laurent Besacier 
IBM T. J. Watson Research Center, Yorktown Heights, NY 10598 
 
ABSTRACT 
In this paper, we describe the IBM MASTOR, a speech-to-speech 
translation system that can translate spontaneous free-form 
speech in real-time on both laptop and hand-held PDAs. Chal-
lenges include speech recognition and machine translation in 
adverse environments, lack of training data and linguistic re-
sources for under-studied languages, and the need to rapidly de-
velop capabilities for new languages. Another challenge is de-
signing algorithms and building models in a scalable manner to 
perform well even on memory and CPU deficient hand-held com-
puters. We describe our approaches, experience, and success in 
building working free-form S2S systems that can handle two 
language pairs (including a low-resource language). 
 
1. INTRODUCTION 
Automatic speech-to-speech (S2S) translation breaks down com-
munication barriers between people who do not share a common 
language and hence enable instant oral cross-lingual communica-
tion for many critical applications such as emergency medical 
care. The development of an accurate, efficient and robust S2S 
translation system poses a lot of challenges. This is especially 
true for colloquial speech and resource deficient languages. 
The IBM MASTOR speech-to-speech translation system has been 
developed for the DARPA CAST and Transtac programs whose 
mission is to develop technologies that enable rapid deployment 
of real-time S2S translation of low-resource languages on port-
able devices. It originated from the IBM MARS S2S system 
handling the air travel reservation domain described in [1], which 
was later significantly improved in all components, including 
ASR, MT and TTS, and later evolved into the MASTOR multi-
lingual S2S system that covers much broader domains such as 
medical treatment and force protection [2,3]. More recently, we 
have further broadened our experience and efforts to very rapidly 
develop systems for under-studied languages, such as regional 
dialects of Arabic. The intent of this program is to provide lan-
guage support to military, medical and humanitarian personnel 
during operations in foreign territories, by deciphering possibly 
critical language communications with a two-way real-time 
speech-to-speech translation system designed for specific tasks 
such as medical triage and force protection.  
The initial data collection effort for the project has shown that the 
domain of force protection and medical triage is, though limited, 
rather broad. In fact, the definition of domain coverage is tough 
when the speech from responding foreign language speakers are 
concerned, as their responses are less constrained and may in-
clude out-of-domain words and concepts. Moreover, flexible 
casual or colloquial speaking style inevitably appears in the hu-
man-to-human conversational communications. Therefore, the 
project is a great challenge that calls for major research efforts. 
Among all the challenges for speech recognition and translation 
for under-studied languages, there are two main issues: 1) Lack of 
appropriate amount of speech data that represent the domain of 
interest and the oral language spoken by the target speakers, re-
sulting in difficulties in accurate estimation of statistical models 
for speech recognition and translation. 2) Lack of linguistic 
knowledge realization in spelling standards, transcriptions, lexi-
cons and dictionaries, or annotated corpora. Therefore, various 
different approaches have to be explored.  
Another critical challenge is to embed complicated algorithms 
and programs into small devices for mobile users. A hand-held 
computing device may have a CPU of 256MHz and 64MB mem-
ory; to fit the programs, as well as the models and data files into 
this memory and operate the system in real-time are tremendous 
challenges [4]. 
In this paper, we will describe the overall framework of the 
MASTOR system and our approaches for each major component, 
i.e., speech recognition and translation. Various statistical ap-
proaches [5,6,7,8] are explored and used to solve different techni-
cal challenges. We will show how we addressed the challenges 
that arise when building automatic speech recognition (ASR) and 
machine translation (MT) for colloquial Arabic on both the laptop 
and handheld PDA platforms. 
 
2. SYSTEM OVERVIEW 
The general framework of our speech translation system is illus-
trated in Figure 1. The general framework of our MASTOR sys-
tem has components of ASR, MT and TTS. The cascaded ap-
proach allows us to deploy the power of the existing advanced 
speech and language processing techniques, while concentrating 
on the unique problems in speech-to-speech translation. Figure 2 
illustrates the MASTOR GUI (Graphic User Interface) on laptop 
and PDA, respectively. 
Acoustic models for English and Mandarin baseline are devel-
oped for large-vocabulary continuous speech and trained on over 
200 hours of speech collected from about 2000 speakers for each 
language. However, the Arabic dialect speech recognizer was 
only trained using about 50 hours of dialectal speech.  The train-
ing data for Arabic consists of about 200K short utterances. Large 
efforts were invested in initial cleaning and normalization of the 
training data because of large number of irregular dialectal words 
and variations in spellings. We experimented with three ap-
proaches for pronunciation and acoustic modeling: i.e. grapheme, 
phonetic, and context-sensitive grapheme as will be described in 
ASR TTS 
Statistical NLU/NLG 
based MT 
Figure 1 IBM MASTOR Speech-to-Speech Translation System 
Statistical MT using 
WFST/SIPL  
* Thanks to DARPA for funding 
section 3.A. We found that using context-sensitive pronunciation 
rules reduces the WER of the grapheme based acoustic model by 
about 3% (from 36.7% to 35.8%). Based on these results, we 
decided to use context-sensitive grapheme models in our system.  
The Arabic language model (LM) is an interpolated model con-
sisting of a trigram LM, a class-based LM and a morphologically 
processed LM, all trained from a corpus of a few hundred thou-
sand words. We also built a compact language model for the 
hand-held system, where singletons are eliminated and bigram 
and trigram counts are pruned with increased thresholds. The LM 
footprint size is 10MB. 
There are two approaches for translation. The concept based ap-
proach uses natural language understanding (NLU) and natural 
language generation models trained from an annotated corpus. 
Another approach is the phrase-based finite state transducer 
which is trained using an un-annotated parallel corpus. 
A trainable, phrase-splicing and variable substitution TTS system 
is adopted to synthesize speech from translated sentences, which 
has a special ability to generate speech of mixed languages seam-
lessly [9]. In addition, a small footprint TTS is developed for the 
handheld devices using embedded concatenative TTS technolo-
gies.[10] 
Next, we will describe our approaches in automatic speech recog-
nition and machine translation in greater detail. 
 
3. AUTOMATIC SPEECH RECOGNITION 
A. Acoustic Models 
Acoustic models and the pronunciation dictionary greatly influ-
ence the ASR performance. In particular, creating an accurate 
pronunciation dictionary poses a major challenge when changing 
the language. Deriving pronunciations for resource rich languages 
like English or Mandarin is relatively straight forward using ex-
isting dictionaries or letter to sound models. In certain languages 
such as Arabic and Hebrew, the written form does not typically 
contain short vowels which a native speaker can infer from con-
text. Deriving automatic phonetic transcription for speech corpora 
is thus difficult. This problem is even more apparent when con-
sidering colloquial Arabic, mainly due to the large number of 
irregular dialectal words. 
One approach to overcome the absence of short vowels is to use 
grapheme based acoustic models. This leads to straightforward 
construction of pronunciation lexicons and hence facilitates 
model training and decoding. However, the same grapheme may 
lead to different phonetic sounds depending on its context. This 
results in less accurate acoustic models. For this reason we ex-
perimented with two other different approaches. The first is a full 
phonetic approach which uses short vowels, and the second uses 
context-sensitive graphemes for the letter "A" (Alif) where two 
different phonemes are used for "A" depending on its position in 
the word. 
Using phoneme based pronunciations would require vowelization 
of every word. To perform vowelization, we used a mix of dic-
tionary search and a statistical approach. The word is first 
searched in an existing vowelized dictionary, and if not found it is 
passed to the statistical vowelizer [11].  Due to the difficulties in 
accurately vowelizing dialectal words, our experiments have not 
shown any improvements using phoneme based ASR compared 
to grapheme based.  
Speech recognition for both the laptop and hand-held systems is 
based on the IBM ViaVoice engine. This highly robust and effi-
cient framework uses rank based acoustic scores [12] which are 
derived from tree-clustered context dependent Gaussian models. 
These acoustic scores together with n-gram LM probabilities are 
incorporated into a stack based search algorithm to yield the most 
probable word sequence given the input speech. 
The English acoustic models use an alphabet of 52 phones. Each 
phone is modeled with a 3-state left-to-right hidden Markov 
model (HMM). The system has approximately 3,500 context-
dependent states modeled using 42K Gaussian distributions and 
trained using 40 dimensional features. The context-dependent 
states are generated using a decision-tree classifier. The collo-
quial Arabic acoustic models use about 30 phones that essentially 
correspond to graphemes in the Arabic alphabet. The colloquial 
Arabic HMM structure is the same as that of the English model. 
The Arabic acoustic models are also built using 40 dimensional 
features. The compact model for the PDA has about 2K leaves 
and 28K Gaussian distributions.  The laptop version has over 3K 
leaves and 60K Gaussians. All acoustic models are trained using 
discriminative training [13]. 
B. Language Modeling   
Language modeling (LM) of the probability of various word se-
quences is crucial for high-performance ASR of free-style open-
 
   
 
 
Figure 2  IBM MASTOR system in Windows XP and Win-
dows CE 
ended coversational systems. Our approaches to build statistical 
tri-gram LMs fall into three categories: 1) obtaining additional 
training material automatically; 2) interpolating domain-specific 
LMs with other LMs; 3) improving distribution estimation ro-
bustness and accuracy with limited in-domain resources. Auto-
matic data collection and expansion is the most straight-forward 
way to achieve efficient LM, especially when little in-domain 
data is available. For resource-rich languages such as English and 
Chinese, we retrieve additional data from the World Wide Web 
(WWW) to enhance our limited domain specific data, which 
shows significant improvement [6]. 
In Arabic, words can take prefixes and suffixes to generate new 
words which are semantically related to the root form of the word 
(stem). As a result, the vocabulary size in Arabic can become 
very large even for specific domains. To alleviate this problem, 
we built a language model on morphologically tokenized data by 
applying morphological analysis and hence splitting some of the 
words into prefix+stem+suffix, prefix+stem or stem+suffix forms. 
We refer the reader to [14] to learn more about the morphological 
tokenization algorithm. Morphological analysis reduced the vo-
cabulary size by about 30% without sacrificing the coverage. 
More specifically, in our MASTOR system, the English language 
model has two components that are linearly interpolated. The first 
one is built using in-domain data. The second component acts as a 
background model and is built using a very large generic text 
inventory that is domain independent. The language model counts 
are also pruned to control the size of this background model. The 
colloquial Arabic language model for our laptop system is com-
posed of three components that are linearly interpolated. The first 
one is the basic word tri-gram model. The second one is a class 
based language model with 13 classes that covers names for Eng-
lish and Arabic, numbers, months, days, etc. The third one is the 
morphological language model described above. 
4. SPEECH TRANSLATION 
A. NLU/NLG-based Speech Translation 
One of the translation algorithms we proposed and applied in 
MASTOR is the statistical translation method based on natural 
language understanding (NLU) and natural language generation 
(NLG). Statistical machine translation methods translate a sen-
tence W in the source language into a sentence A in the target 
language by using a statistical model that estimates the probabil-
ity of A given W, i.e. ( )WAp . Conventionally, ( )WAp  is opti-
mized on a set of pairs of sentences that are translations of one 
another. To alleviate this data sparseness problem and, hence, 
enhance both the accuracy and robustness of estimating ( )WAp , 
we proposed a statistical concept-based machine translation para-
digm that predicts A with not only W but also the underlying con-
cepts embedded in W and/or A. As a result, the optimal sentence 
A is picked by first understanding the meaning of the source sen-
tence W.  
Let C denote the concepts in the source language and S denote the 
concepts in the target language, our proposed statistical concept-
based algorithm should select a word sequence A? as 
( ) ( ) ( ) ( )
??
?
??
?
== ? WCpWCSpWCSApWApA
CSAA
,,,maxargmaxarg?
,
 , 
where the conditional probabilities ( )WCp , ( )WCSp ,  and 
( )WCSAp ,,  are estimated by the Natural Language Understand-
ing (NLU), Natural Concept Generation (NCG) and Natural 
Word Generation (NWG) procedures, respectively. The probabil-
ity distributions are estimated and optimized upon a pre-annotated 
bilingual corpus. In our MASTOR system, ( )WCp  is estimated 
by a decision-tree based statistical semantic parser, and 
( )WCSp ,  and ( )WCSAp ,,  are estimated by maximizing the 
conditional entropy as depicted in [2] and [7], respectively. 
We are currently developing a new translation method that unifies 
statistical phrase-based translation models and the above 
NLU/NLG based approach. We will discuss this work in future 
publications. 
 
B. Fast and Memory Efficient Machine Translation Using SIPL 
Another translation method we proposed in MASTOR is based on 
the Weighted Finite-State Transducer (WFST). In particular, we 
developed a novel phrase-based translation framework using 
WFSTs that achieves both memory efficiency and fast speed, 
which is suitable for real time speech-to-speech translation on 
scalable computational platforms. In the proposed framework [15] 
which we refer to as Statistical Integrated Phrase Lattices (SIPLs), 
we statically construct a single optimized WFST encoding the 
entire translation model. In addition, we introduce a Viterbi de-
coder that can combine the translation model and language model 
FSTs with the input lattice efficiently, resulting in translation 
speeds of up to thousands of words per second on a PC and hun-
dred words per second on a PDA device. This WFST-based ap-
proach is well-suited to devices with limited computation and 
memory. We achieve this efficiency by using methods that allow 
us to perform more composition and graph optimization offline 
(such as, the determinization of the phrase segmentation trans-
ducer P) than in previous work, and by utilizing a specialized 
decoder involving multilayer search.  
During the offline training, we separate the entire translation lat-
tice H into two pieces: the language model L and the translation 
model M: 
( )( )( )WTPDetMinMinM =  
where   is the composition operator, Min  denotes the 
minimization operation, and Det  denotes the determinization 
operation; T is the phrase translation transducer, and W is the 
phrase-to-word transducer. Due to the determinizability of P, M 
can be computed offline using a moderate amount of memory. 
The translation problem can be framed as finding the best path in 
the full search lattice given an input sentence/automaton I. To 
address the problem of efficiently computing LMI  , we have 
developed a multilayer search algorithm. 
Specifically, we have one layer for each of the input FSM's: I, L, 
and M. At each layer, the search process is performed via a state 
traversal procedure starting from the start state 0s

, and consum-
ing an input word in each step in a left-to-right manner.  
We represent each state s in the search space using the following 
7-tuple: Is , Ms , Ls , Mc , Lc , h
 
, prevs , where Is , Ms , and 
Ls record the current state in each input FSM; Mc and Lc  record 
the accumulated cost in L and M in the best path up to this point; 
h
 
 records the target word sequence labeling the best path up to 
this point; and prevs  records the best previous state. 
To reduce the search space, two active search states are merged 
whenever they have identical Is , Ms , and Ls values; the re-
maining state components are inherited from the state with lower 
cost.  In addition, two pruning methods, histogram pruning and 
threshold or beam pruning, are used to achieve the desired bal-
ance between translation accuracy and speed. 
To provide the decoder for the PDA devices as well that lacks a 
floating-point processor, the search algorithm is implemented 
using fixed-point arithmetic. 
 
 
5. CONCLUSION 
We described the framework of the IBM MASTOR system, the 
various technologies used in building major components for lan-
guages with different levels of data resources. The technologies 
have shown successes in building real-time S2S systems on both 
laptop and small computation resource platforms for two lan-
guage pairs, English-Mandarin Chinese, and English-Arabic dia-
lect. In the latter case, we also developed approaches which lead 
to very rapid (in the matter of 3-4 months) development of sys-
tems using very limited language and domain resources. We are 
working on improving spontaneous speech recognition accuracy 
and more naturally integrating two translation approaches.  
 
6. ACKNOWLEDGEMENT 
The authors sincerely thank Drs. Yoshinori Tahara, Fu-hua Liu, 
Yongxing Li, Etienne Marcheret, Raimo Bakis, Ellen Eide, Burn 
Lewis, Tony Lee, Ossama Emam, and Lubos Ures for their help 
and contributions to the MASTOR S2S system. 
 
7. REFERENCES 
[1] Y. Gao et al ?MARS: A Statistical Semantic Parsing and Generation 
Based Multilingual Automatic tRanslation System,? Machine Trans-
lation, vol. 17, pp.185-212, 2004. 
[2] L. Gu et al ?Improving Statistical Natural Concept Generation in 
Interlingua-based Speech-to-Speech Translation,? in Proc. Eu-
rospeech?2003, pp.2769-2772. 
[3] F.-H. Liu, ?Robustness in Speech-to-Speech Translation,? in Proc. 
Eurospeech?2003, pp.2797-2800. 
[4] B. Zhou et al ?Two-way speech-to-speech translation on handheld 
devices,? in Proc. ICSLP'04, South Korea, Oct, 2004. 
[5] H. Erdogan et al ?Using Semantic Analysis to Improve Speech 
Recognition Performance,? Computer Speech and Language, vol.19, 
pp.321-343, 2005. 
[6] R. Sarikaya, et al  ?Rapid Language Model Development Using 
External Resources for New Spoken Dialog Domains,? in Proc. 
ICASSP'05, Philadelphia, PA, Mar, 2005. 
[7] L. Gu et al ?Concept-based Speech-to-Speech Translation using 
Maximum Entropy Models for Statistical Natural Concept Genera-
tion,? IEEE Trans. Speech and Audio Processing, vol.14, no.2, 
pp.377-392, March, 2006. 
[8] B. Zhou et al ?Constrained phrase-based translation using weighted 
finite-state transducers,? in Proc. ICASSP'05, Philadelphia, Mar, 
2005. 
[9] E. Eide et al ?Recent Improvements to the IBM Trainable Speech 
Synthesis System,? in Proc.  ICASSP, Hong Kong, China, 2003. 
[10] Dan Chazan et al ?Reducing the Footprint of the IBM Trainable 
Speech Synthesis System,? in ICSLP-2002, pp.2381-2384  
[11] R. Sarikaya et al ?Maximum Entropy Based Vowelization of Ara-
bic,? Interspeech2006 (submitted for publication). 
[12] L.R. Bahl, et al ?Robust methods for using context-dependent fea-
tures and models in a continuous speech recognizer,? in Proc. 
ICASSP, 1994 
[13] D. Povey & P.C. Woodland, ?Minimum Phone Error and I-
Smoothing for Improved Discriminative Training,? In Proc. ICASSP, 
Orlando, 2002. 
[14] M. Afify et.al, ?On the Use of Morphological Analysis for Dialectal 
Arabic Speech Recognition,? Interspeech 2006 (submitted for publi-
cation). 
[15] B. Zhou, S. Chen, and Y. Gao, ?Fast Machine Translation Using 
Statistical Integrated Phrase Lattices,? submitted to COL-
ING/ACL'2006. 
 
 
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 165?168,
Prague, June 2007. c?2007 Association for Computational Linguistics
HIT-IR-WSD: A WSD System for English Lexical Sample Task 
Yuhang Guo, Wanxiang Che, Yuxuan Hu, Wei Zhang and Ting Liu 
Information Retrieval Lab 
Harbin Institute of technology 
Harbin, China, 150001 
{yhguo,wxche}@ir.hit.edu.cn 
 
 
Abstract 
HIT-IR-WSD is a word sense disambigua-
tion (WSD) system developed for English 
lexical sample task (Task 11) of Semeval 
2007 by Information Retrieval Lab, Harbin 
Institute of Technology. The system is 
based on a supervised method using an 
SVM classifier. Multi-resources including 
words in the surrounding context, the part-
of-speech of neighboring words, colloca-
tions and syntactic relations are used. The 
final micro-avg raw score achieves 81.9% 
on the test set, the best one among partici-
pating runs. 
1 Introduction 
Lexical sample task is a kind of WSD evaluation 
task providing training and test data in which a 
small pre-selected set of target words is chosen and 
the target words are marked up. In the training data 
the target words? senses are given, but in the test 
data are not and need to be predicted by task par-
ticipants. 
HIT-IR-WSD regards the lexical sample task 
as a classification problem, and devotes to extract 
effective features from the instances. We didn?t use 
any additional training data besides the official 
ones the task organizers provided. Section 2 gives 
the architecture of this system. As the task pro-
vides correct word sense for each instance, a su-
pervised learning approach is used. In this system, 
we choose Support Vector Machine (SVM) as 
classifier. SVM is introduced in section 3. Know-
ledge sources are presented in section 4. The last 
section discusses the experimental results and 
present the main conclusion of the work performed. 
2 The Architecture of the System 
HIT-IR-WSD system consists of 2 parts: feature 
extraction and classification. Figure 1 portrays the 
architecture of the system. 
 
Figure?1:?The?architecture?of?HIT?IR?WSD?
165
Features are extracted from original instances 
and are made into digitized features to feed the 
SVM classifier. The classifier gets the features of 
training data to make a model of the target word. 
Then it uses the model to predict the sense of target 
word in the test data. 
3 Learning Algorithm 
SVM is an effective learning algorithm to WSD 
(Lee and Ng, 2002). The SVM tries to find a 
hyperplane with the largest margin separating the 
training samples into two classes. The instances in 
the same side of the hyperplane have the same 
class label. A test instance?s feature decides the 
position where the sample is in the feature space 
and which side of the hyperplane it is. In this way, 
it leads to get a prediction. SVM could be extended 
to tackle multi-classes problems by using one-
against-one or one-against-rest strategy. 
In the WSD problem, input of SVM is the fea-
ture vector of the instance. Features that appear in 
all the training samples are arranged as a vector 
space. Every instance is mapped to a feature vector. 
If the feature of a certain dimension exists in a 
sample, assign this dimension 1 to this sample, else 
assign it 0. For example, assume the feature vector 
space is <x1, x2, x3, x4, x5, x6, x7>; the instance is 
?x2 x6 x5 x7?. The feature vector of this sample 
should be <0, 1, 0, 0, 1, 1, 1>.  
The implementation of SVM here is libsvm 1 
(Chang and Lin, 2001) for multi-classes. 
4 Knowledge Sources 
We used 4 kinds of features of the target word and 
its context as shown in Table 1. 
Part of the original text of an example is ?? 
This is the <head>age</head> of new media , the 
era of ??. 
Name Extraction Tools Example 
Surrounding 
words 
WordNet 
(morph)2 
?, this, be, age, new, 
medium, ,, era, ? 
Part-of-
speech SVMTool
3 
DT_0, VBZ_0, DT_0, 
NN_t, IN_1, JJ_1, 
NNS_1 
                                                 
1?http://www.csie.ntu.edu.tw/~cjlin/libsvm/?
2?http://wordnet.princeton.edu/man/morph.3WN.html?
3?http://www.lsi.upc.es/~nlp/SVMTool/?
Collocation  
this_0, be_0, the_0, 
age_t, of_1, new_1, 
medium_1, ,_1, the_1 
Syntactic 
relation MaltParser
4 
SYN_HEAD_is 
SYN_HEADPOS_VBZ 
SYN_RELATION_PRD 
SYN_HEADRIGHT 
Table?1:?Features?the?system?extracted?
The next 4 subsections elaborate these features. 
4.1 Words in the Surrounding Context 
We take the neighboring words in the context of 
the target word as a kind of features ignoring their 
exact position information, which is called bag-of-
words approach. 
Mostly, a certain sense of a word is tend to ap-
pear in a certain kind of context, so the context 
words could contain some helpful information to 
disambiguate the sense of the target word. 
Because there would be too many context words 
to be added into the feature vector space, data 
sparseness problem is inevitable. We need to re-
duce the sparseness as possible as we can. A sim-
ple way is to use the words? morphological root 
forms. In addition, we filter the tokens which con-
tain no alphabet character (including punctuation 
symbols) and stop words. The stop words are 
tested separately, and only the effective ones 
would be added into the stop words list. All re-
maining words in the instance are gathered, con-
verted to lower case and replaced by their morpho-
logical root forms. The implementation for getting 
the morphological root forms is WordNet (morph). 
4.2 Part-of-Speechs of Neighboring Words 
As mentioned above, the data sparseness is a se-
rious problem in WSD. Besides changing tokens to 
their morphological root forms, part-of-speech is a 
good choice too. The size of POS tag set is much 
smaller than the size of surrounding words set. 
And the neighboring words? part-of-speeches also 
contain useful information for WSD. In this part, 
we use a POS tagger (Gim?nez and M?rquez, 2004) 
to assign POS tags to those tokens.  
We get the left and right 3 words? POS tags to-
gether with their position information in the target 
words? sentence.  
For example, the word age is to be disambi-
guated in the sentence of ?? This is the 
                                                 
4?http://w3.msi.vxu.se/~nivre/research/MaltParser.html?
166
<head>age</head> of new media , the era of ??. 
The features then will be added to the feature vec-
tor are ?DT_0, VBZ_0, DT_0, NN_t, IN_1, JJ_1, 
NNS_1?, in which _0/_1 stands for the word with 
current POS tag is in the left/right side of the target 
word. The POS tag set in use here is Penn Tree-
bank Tagset5. 
4.3 Collocations 
Different from bag-of-words, collocation feature 
contains the position information of the target 
words? neighboring words. To make this feature in 
the same form with the bag-of-words, we appended 
a symbol to each of the neighboring words? mor-
phological root forms to mark whether this word is 
in the left or in the right of the target word. Like 
POS feature, collocation was extracted in the sen-
tence where the target word belongs to. The win-
dow size of this feature is 5 to the left and 5 to the 
right of the target word, which is attained by em-
pirical value. In this part, punctuation symbol and 
stop words are not removed. 
Take the same instance last subsection has men-
tioned as example. The features we extracted are 
?this_0, be_0, the_0, age_t, of_1, new_1, me-
dium_1?. Like POS, _0/_1 stands for the word is 
in the left/right side of the target word. Then the 
features were added to the feature vector space. 
4.4 Syntactic Relations 
Many effective context words are not in a short 
distance to the target word, but we shouldn?t en-
large the window size too much in case of includ-
ing too many noises. A solution to this problem is 
to use the syntactic relations of the target word and 
its parent head word. 
We use Nivre et al, (2006)?s dependency parser. 
In this part, we get 4 features from every instance: 
head word of the target word, the head word?s POS, 
the head word?s dependency relation with the tar-
get word and the relative position of the head word 
to the target word. 
Still take the same instance which has been 
mentioned in the las subsection as example. The 
features we extracted are ?SYN_HEAD_is, 
SYN_HEADPOS_VBZ, SYN_RELATION_PRD, 
SYN_HEADRIGHT?, in which SYN_HEAD_is 
stands for is is the head word of age; 
SYN_HEADPOS_VBZ stands for the POS of the 
                                                 
5?http://www.lsi.upc.es/~nlp/SVMTool/PennTreebank.html?
head word is is VBZ; SYN_RELATION_PRD 
stands for the relationship between the head word 
is and target word age is PRD; and 
SYN_HEADRIGHT stands for the target word age 
is in the right side of the head word is. 
5 Data Set and Results 
This English lexical sample task: Semeval 2007 
task 116 provides two tracks of the data set for par-
ticipants. The first one is from LDC and the second 
from web. 
We took part in this evaluation in the second 
track. The corpus is from web. In this track the task 
organizers provide a training data and test data set 
for 20 nouns and 20 adjectives. 
In order to develop our system, we divided the 
training data into 2 parts: training and development 
sets. The size of the training set is about 2 times of 
the development set. The development set contains 
1,781 instances. 
4 kinds of features were merged into 15 combi-
nations. Here we use a vector (V) to express which 
features are used. The four dimensions stand for 
syntactic relations, POS, surrounding words and 
collocations, respectively. For example, 1010 
means that the syntactic relations feature and the 
surrounding words feature are used. 
V Precision V Precision
0001 78.6% 1001 78.2% 
0010 80.3% 1010 81.9% 
0011 82.0% 1011 82.8% 
0100 70.4% 1100 73.3% 
0101 79.0% 1101 79.1% 
0110 82.1% 1110 82.5% 
0111 82.9% 1111 82.9% 
1000 72.6%   
Table?2:?Results?of?Combinations?of?Features?
From Table 2, we can conclude that the sur-
rounding words feature is the most useful kind of 
features. It obtains much better performance than 
other kinds of features individually. In other words, 
without it, the performance drops a lot. Among 
these features, syntactic relations feature is the 
most unstable one (the improvement with it is un-
stable), partly because the performance of the de-
pendency parser is not good enough. As the ones 
with the vector 0111 and 1111 get the best perfor-
                                                 
6http://nlp.cs.swarthmore.edu/semeval/tasks/task11/descript
ion.shtml?
167
mance, we chose all of these kinds of features for 
our final system. 
A trade-off parameter C in SVM is tuned, and 
the result is shown in Figure 2. We have also tried 
4 types of kernels of the SVM classifier (parame-
ters are set by default). The experimental results 
show that the linear kernel is the most effective as 
Table 3 shows. 
 
Figure?2:?Accuracy?with?different?C?parameters?
Kernel 
Function 
Type 
Linear Poly-nomial RBF
Sig-
moid
Accuracy 82.9% 68.3% 68.3% 68.3%
Table?3:?Accuracy?with?different?kernel?function?
types?
Another experiment (as shown in Figure 3) also 
validate that the linear kernel is the most suitable 
one. We tried using polynomial function. Unlike 
the parameters set by default above (g=1/k, d=3), 
here we set its Gama parameter as 1 (g=1) but oth-
er parameters excepting degree parameter are still 
set by default. The performance gets better when 
the degree parameter is tuned towards 1. That 
means the closer the kernel function to linear func-
tion the better the system performs. 
 
Figure?3:?Accuracy?with?different?degree? in?po?
lynomial?function?
In order to get the relation between the system 
performance and the size of training data, we made 
several groups of training-test data set from the 
training data the organizers provided. Each of them 
has the same test data but different size of training 
data which are 2, 3, 4 and 5 times of the test data 
respectively. Figure 4 shows the performance 
curve with the training data size. Indicated in Fig-
ure 4, the accuracy increases as the size of training 
data enlarge, from which we can infer that we 
could raise the performance by using more training 
data potentially. 
 
Figure?4:?Accuracy?s?trend?with?the?training?da?
ta?size?
Feature extraction is the most time-consuming 
part of the system, especially POS tagging and 
parsing which take 2 hours approximately on the 
training and test data. The classification part (using 
libsvm) takes no more than 5 minutes on the train-
ing and test data. We did our experiment on a PC 
with 2.0GHz CPU and 960 MB system memory. 
Our official result of HIT-IR-WSD is: micro-
avg raw score 81.9% on the test set, the top one 
among the participating runs. 
Acknowledgement 
We gratefully acknowledge the support for this 
study provided by the National Natural Science 
Foundation of China (NSFC) via grant 60435020, 
60575042, 60575042 and 60675034. 
References 
Lee, Y. K., and Ng, H. T. 2002. An empirical evaluation 
of knowledge sources and learning algorithms for 
word sense disambiguation. In Proceedings of 
EMNLP02, 41?48. 
Chih-Chung Chang and Chih-Jen Lin, 2001. LIBSVM: a 
library for support vector machines. 
Jes?s Gim?nez and Llu?s M?rquez. 2004. SVMTool: A 
general POS tagger generator based on Support Vec-
tor Machines. Proceedings of the 4th International 
Conference on Language Resources and Evaluation 
(LREC'04). Lisbon, Portugal. 
Nivre, J., Hall, J., Nilsson, J., Eryigit, G. and Marinov, S. 
2006. Labeled Pseudo-Projective Dependency Pars-
ing with Support Vector Machines. In Proceedings of 
the Tenth Conference on Computational Natural 
Language Learning (CoNLL). 
168
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1290?1298,
Beijing, August 2010
Entity Linking Leveraging
Automatically Generated Annotation 
Wei Zhang?    Jian Su? Chew Lim Tan?   Wen Ting Wang?
?School of Computing 
National University of Singapore 
{z-wei, tancl} 
@comp.nus.edu.sg
? Institute for Infocomm Research 
{sujian, wwang}
@i2r.a-star.edu.sg
Abstract
Entity linking refers entity mentions in a 
document to their representations in a 
knowledge base (KB). In this paper, we 
propose to use additional information 
sources from Wikipedia to find more 
name variations for entity linking task. In 
addition, as manually creating a training 
corpus for entity linking is labor-
intensive and costly, we present a novel 
method to automatically generate a large 
scale corpus annotation for ambiguous 
mentions leveraging on their unambi-
guous synonyms in the document collec-
tion. Then, a binary classifier is trained 
to filter out KB entities that are not simi-
lar to current mentions. This classifier 
not only can effectively reduce the am-
biguities to the existing entities in KB, 
but also be very useful to highlight the 
new entities to KB for the further popu-
lation. Furthermore, we also leverage on 
the Wikipedia documents to provide ad-
ditional information which is not availa-
ble in our generated corpus through a 
domain adaption approach which pro-
vides further performance improve-
ments.  The experiment results show that 
our proposed method outperforms the 
state-of-the-art approaches. 
1 Introduction 
The named entity (NE) ambiguation has raised 
serious problems in many areas, including web 
people search, knowledge base population 
(KBP), and information extraction, because an 
entity (such as Abbott Laboratories, a diversified 
pharmaceuticals health care company) can be 
referred to by multiple mentions (e.g. ?ABT? and 
?Abbott?), and a mention (e.g. ?Abbott?) can be 
shared by different entities (e.g. Abbott Texas: a 
city in United States; Bud Abbott, an American 
actor; and Abbott Laboratories, a diversified 
pharmaceutical health care company).  
Both Web People Search (WePS) task (Artiles 
et al 2007) and Global Entity Detection & Rec-
ognition task (GEDR) in Automatic Content Ex-
traction 2008 (ACE08) disambiguate entity men-
tions by clustering documents with these men-
tions. Each cluster then represents a unique enti-
ty. Recently entity linking has been proposed in 
this field. However, it is quite different from the 
previous tasks.
Given a knowledge base, a document collec-
tion, entity linking task as defined by KBP-091
(McNamee and Dang, 2009) is to determine for 
each name string and the document it appears, 
which knowledge base entity is being referred to, 
or if the entity is a new entity which is not 
present in the reference KB.  
Compared with GEDR and WePS, entity link-
ing has a given entity list (i.e. the reference KB) 
to which we disambiguate the entity mentions. 
Moreover, in document collection, there are new 
entities which are not present in KB and can be 
used for further population. In fact, new entities 
with or without the names in KB cover more 
than half of testing instances. 
1 http://apl.jhu.edu/~paulmac/kbp.html 
1290
Entity linking has been explored by several re-
searchers. Without any training data available, 
most of the previous work ranks the similarity 
between ambiguous mention and candidate enti-
ties through Vector Space Model (VSM). Since 
they always choose the entity with the highest 
rank as the answer, the ranking approaches hard-
ly detect a situation where there may be a new 
entity that is not present in KB. It is also difficult 
to combine bag of words (BOW) with other fea-
tures. For example, to capture the ?category? 
information, the method of Cucerzan (2007) in-
volves a complicated optimization issue and the 
approach has to be simplified for feasible com-
putation, which compromises the accuracy.  Be-
sides unsupervised methods, some supervised 
approaches (Agirre et al 2009, Li et al 2009 and 
McNamee et al 2009) also have been proposed 
recently for entity linking. However, the super-
vised approaches for this problem require large 
amount of training instances. But manually 
creating a corpus is labor-intensive and costly.  
In this paper, we explore how to solve the enti-
ty linking problem. We present a novel method 
that can automatically generate a large scale 
corpus for ambiguous mentions leveraging on 
their unambiguous synonyms in the document 
collection.  A binary classifier based on Support 
Vector Machine (SVM) is trained to filter out 
some candidate entities that are not similar to 
ambiguous mentions. This classifier can effec-
tively reduce the ambiguities to the existing enti-
ties in KB, and it is very useful to highlight the 
new entities to KB for the further population. 
We also leverage on the Wikipedia documents to 
provide additional information which is not 
available in our generated corpus through a do-
main adaption approach which provides further 
performance improvements. Besides, more in-
formation sources for finding more variations 
also contribute to the overall 22.9% accuracy 
improvements on KBP-09 test data over baseline. 
The remainder of the paper is organized as fol-
lows. Section 2 reviews related work for entity 
linking. In Section 3 we detail our algorithm in-
cluding name variation and entity disambigua-
tion. Section 4 describes the experimental setup 
and results. Finally, Section 5 concludes the pa-
per.
2 Related Work 
The crucial component of entity linking is the 
disambiguation process. Raphael et al (2007) 
report a disambiguation algorithm for geography. 
The algorithm ranks the candidates based on the 
manually assigned popularity scores in KB. The 
class with higher popularity will be assigned 
higher score. It causes that the rank of entities 
would never change, such as Lancaster (Califor-
nia) would always have a higher rank than Lan-
caster (UK) for any mentions. However, as the 
popularity scores for the classes change over 
time, it is difficult to accurately assign dynamic 
popularity scores. Cucerzan (2007) proposes a 
disambiguation approach based on vector space 
model for linking ambiguous mention in a doc-
ument with one entity in Wikipedia. The ap-
proach ranks the candidates and chooses the ent-
ity with maximum agreement between the con-
textual information extracted from Wikipedia 
and the context of a document, as well as the 
agreement among the category tags associated 
with the candidate entities. Nguyen and Cao 
(2008) refer the mentions in a document to KIM 
(Popov et al 2004) KB. KIM KB is populated 
with over 40,000 named entities. They represent 
a mention and candidates as vectors of their con-
textual noun phrase and co-occurring NEs, and 
then the similarity is determined by the common 
terms of the vectors and their associated weights. 
For linking mentions in news articles with a Wi-
kipedia-derived KB (KBP-09 data set), Varma et 
al. (2009) rank the entity candidates using a 
search engine. Han and Zhao (2009) rank the 
candidates based on BOW and Wikipedia se-
mantic knowledge similarity. 
All the related work above rank the candidates 
based on the similarity between ambiguous men-
tion and candidate entities. However, the ranking 
approach hardly detects the new entity which is 
not present in KB. 
Some supervised approaches also have been 
proposed. Li et al (2009) and McNamee et al 
(2009) train their models on a small manually 
created data set containing only 1,615 examples. 
But entity linking requires large training data. 
Agirre et al (2009) use Wikipedia to construct 
their training data by utilizing Inter-Wikipedia 
links and the surrounding snippets of text. How-
ever, their training data is created from a         
1291
different domain which does not work well in 
the targeted news article domain.  
3 Approach
In this section we describe our two-stage ap-
proach for entity linking: name variation and 
entity disambiguation. The first stage finds vari-
ations for every entity in the KB and generates 
an entity candidate set for a given query. The 
second stage is entity disambiguation, which 
links an entity mention with the real world entity 
it refers to. 
3.1 Name Variation 
The aim for Name Variation is to build a 
Knowledge Repository of entities that contains 
vast amount of world knowledge of entities like 
name variations, acronyms, confusable names, 
spelling variations, nick names etc. We use 
Wikipedia to build our knowledge repository 
since Wikipedia is the largest encyclopedia in 
the world and surpasses other knowledge bases 
in its coverage of concepts and up-to-date 
content. We obtain useful information from 
Wikipedia by the tool named Java Wikipedia 
Library 2  (Zesch et al 2008), which allows to 
access all information contained in Wikipedia. 
Cucerzan (2007) extracts the name variations 
of an entity by leveraging four knowledge 
sources in Wikipedia: ?entity pages?, ?disam-
biguation pages?  ?redirect pages? and ?anchor 
text?.
Entity page in Wikipedia is uniquely identified 
by its title ? a sequence of words, with the first 
word always capitalized. The title of Entity Page 
represents an unambiguous name variation for 
the entity. A redirect page in Wikipedia is an aid 
to navigation. When a page in Wikipedia is redi-
rected, it means that those set of pages are refer-
ring to the same entity. They often indicate syn-
onym terms, but also can be abbreviations, more 
scientific or more common terms, frequent 
misspellings or alternative spellings etc. Disam-
biguation pages are created only for ambiguous 
mentions which denote two or more entities in 
Wikipedia, typically followed by the word ?dis-
ambiguation? and containing a list of references 
to pages for entities that share the same name. 
This is more useful in extracting the abbrevia-
2 http://www.ukp.tu-darmstadt.de/software/JWPL 
tions of entities, other possible names for an ent-
ity etc. Besides, both outlinks and inlinks in Wi-
kipedia are associated with anchor texts that 
represent name variations for the entities.
Using these four sources above, we extracted 
name variations for every entity in KB to form 
the Knowledge Repository as Cucerzan?s (2007) 
method. For example, the variation set for entity 
E0272065 in KB is {Abbott Laboratories, Ab-
bott Nutrition, Abbott ?}. Finally, we can gen-
erate the entity candidate set for a given query 
using the Knowledge Repository. For example, 
for the query containing ?Abbott?, the entity 
candidate set retrieved is {E0272065, E0064214 
?}.
From our observation, for some queries the re-
trieved candidate set is empty. If the entity for 
the query is a new entity, not present in KB, 
empty candidate set is correct. Otherwise, we 
fail to identify the mention in the query as a var-
iation, commonly because the mention is a miss-
pelling or infrequently used name. So we pro-
pose to use two more sources ?Did You Mean? 
and ?Wikipedia Search Engine? when Cucerzan 
(2007) algorithm returns empty candidate set. 
Our experiment results show that both proposed 
knowledge sources are effective for entity link-
ing. This contributes to a performance improve-
ment on the final entity linking accuracy. 
Did You Mean: The ?did you mean? feature 
of Wikipedia can provide one suggestion for 
misspellings of entities. This feature can help to 
correct the misspellings. For example, ?Abbot 
Nutrition? can be corrected to ?Abbott Nutri-
tion?.
Wikipedia Search Engine: This key word 
based search engine can return a list of relevant 
entity pages of Wikipedia. This feature is more 
useful in extracting infrequently used name. 
Algorithm 1 below presents the approach to 
generate the entity candidate set over the created 
Knowledge Repository. Ref
E
(s) is the entity set 
indexed by mention s retrieved from Knowledge 
Repository.  In Step 8, we use the longest com-
mon subsequence algorithm to measure the simi-
larity between strings s and the title of the entity 
page with highest rank. More details about long-
est common subsequence algorithm can be 
found in Cormen et al (2001). 
1292
Algorithm 1 Candidate Set Generation 
Input: mention s;       
1: if RefE(s) is empty
2:        s??Wikipedia?did you 
           mean?Suggestion 
3:        If s? is not NULL  
4:             s ? s?
5:        else
6:            EntityPageList ? WikipediaSear
               chEngine(s) 
7:            EntityPage?FirstPage of EntityPageL 
               ist 
8:            Sim=Similarity(s,EntityPage.title)
9:            if Sim > Threshold 
10:   s? EntityPage.title
11:          end if 
12: end if 
13: end if 
Output: RefE(s);
3.2 Entity Disambiguation 
The disambiguation component is to link the 
mention in query with the entity it refers to in 
candidate set. If the entity to which the mention 
refers is a new entity which is not present in KB, 
nil will be returned. In this Section, we will de-
scribe the method for automatic data creation, 
domain adaptation from Wikipedia data, and our 
supervised learning approach as well. 
3.2.1 Automatic Data Creation  
The basic idea is to take a document with an un-
ambiguous reference to an entity E1 and replac-
ing it with a phrase which may refer to E1, E2 or 
others.
Observation: Some full names for the entities 
in the world are unambiguous. This phenomenon 
also appears in the given document collection of 
entity linking. The mention ?Abbott Laborato-
ries? appearing at multiple locations in the doc-
ument collection refers to the same entity ?a
pharmaceuticals health care company? in KB.
From this observation, our method takes into 
account the mentions in the Knowledge Reposi-
tory associated with only one entity and we treat 
these mentions as unambiguous name. Let us 
take Abbott Laboratories-{E0272065} in the 
Knowledge Repository as an example. We first 
use an index and search tool to find the docu-
ments with unambiguous mentions. Such as, the 
mention ?Abbott Laboratories? occurs in docu-
ment LDC2009T13 and LDC2007T07 in the 
document collection. The chosen text indexing 
and searching tool is the well-known Apache 
Lucene information retrieval open-source li-
brary3.
Next, to validate the consistency of NE type 
between entities in KB and in document,   we 
run the retrieved documents through a Named 
Entity Recognizer, to tag the named entities in 
the documents. Then we link the document to 
the entity in KB if the document contains a 
named entity whose name exactly matches with 
the unambiguous mention and type (i.e. Person, 
Organization and Geo-Political Entity) exactly 
matches with the type of entity in KB. In this 
example, after Named Entity Recognition, ?Ab-
bott Laboratories? in document LDC2009T13 is
tagged as an Organization which is consistent 
with the entity type of E0272065 in KB. We link 
the ?Abbott Laboratories? occurring in 
LDC2009T13 with entity E0272065.  
Finally, we replace the mention in the selected 
documents with the ambiguous synonyms. For 
example, we replace the mention ?Abbott La-
boratories? in document LDC2009T13 with
?Abbott? where Abbott-{E0064214, 
E0272065?} is an entry in Knowledge Reposi-
tory. ?Abbott? is ambiguous, because it is refer-
ring not only to E0272065, but also to E0064214 
in Knowledge Repository. Then, we can get two 
instances for the created data set as Figure 1, 
where one is positive and the other is negative.  
Figure 1: An instance of the data set 
However, from our studies, we realize some 
limitations on our training data. For example, as 
shown in Figure 1, the negative instance for 
E0272065 and the positive instance for 
3 http://lucene.apache.org 
(Abbott, LDC2009T13)  E0272065    +
(Abbott, LDC2009T13)  E0064214    -
          ? 
                         +   refer to  -  not refer to
1293
E0064214 are not in our created data set. 
However, those instances exist in the current 
document collection. We do not retrieve them 
since there is no unambiguous mention for 
E0064214 in the document collection.   
To reduce the effect of this problem, we pro-
pose to use the Wikipedia data as well, since 
Wikipedia data has training examples for all the 
entities in KB. Articles in Wikipedia often con-
tain mentions of entities that already have a cor-
responding article, and at least the first occur-
rence of the mentions of an entity in a Wikipedia 
article must be linked to its corresponding Wiki-
pedia article, if such an article exists. Therefore, 
if the mention is ambiguous, the hyperlink is 
disambiguating it. Next, we will describe how to 
incorporate Wikipedia data. 
Incorporating Wikipedia Data. The docu-
ment collection for entity linking is commonly 
from other domains, but not Wikipedia. To ben-
efit from Wikipedia data, we introduce a domain 
adaption approach (Daum? III, 2007) which is 
suitable for this work since we have enough 
?target? domain data. The approach is to aug-
ment the feature vectors of the instances. Denote 
by X the input space, and by Y the output space, 
in this case, X is the space of the real vectors 
???? for the instances in data set and Y= {+1,-1} 
is the label. Ds is the Wikipedia domain dataset 
and Dt is our automatically created data set. 
Suppose for simplicity that X=RF for some F > 0 
(RF is the space of F-dimensions). The aug-
mented input space will be defined by ??  =R3F.
Then, define mappings ?s, ?t : X ? ?? for map-
ping the Wikipedia and our created data set re-
spectively.  These are defined as follows:
????? ?? ????? ????? ? ?
????? ?? ??????? ???? ?
Where 0=<0,0,?,0> ?RF is the zero vector. We 
use the simple linear kernel in our experiments. 
However, the following kernelized version can 
help us to gain some insight into the method. K
denotes the dot product of two vectors. 
K(x,x?)=< ?  (x), ?  (x?)>. When the domain is 
the same, we get: ????? ??? ?? ????? ????? ?
?? ????? ????? ? ????? ??? . When they are 
from different domains, we get: ????? ??? ??
????? ????? ?? ???? ???. Putting this togeth-
er, we have: 
?? ? ???
??? ?????????????
???? ???????? ??????
This is an intuitively pleasing result. Loosely 
speaking, this means that data points from our 
created data set have twice as much influence as 
Wikipedia points when making predictions 
about test data from document collection. 
3.2.2 The Disambiguation Framework 
To disambiguate a mention in document collec-
tion, the ranking method is to rank the entities in 
candidate set based on the similarity score. In 
our work, we transform the ranking problem into 
a classification problem: deciding whether a 
mention refers to an entity on an SVM classifier.
If there are 2 or more than 2 candidate entities 
that are assigned positive label by the binary 
classifier, we will use the baseline system (ex-
plained in Section 4.2) to rank the candidates 
and the entity with the highest rank will be cho-
sen.
In the learning framework, the training or test-
ing instance is formed by (query, entity) pair.
For Wikipedia data, (query, entity) is positive if 
there is a hyperlink from the article containing 
the mention in query to the entity, otherwise 
(query, entity) is negative. Our automatically 
created data has been assigned labels in Section 
3.2.1.  Based on the training instances, a binary 
classifier is generated by using particular learn-
ing algorithm.  During disambiguation, (query,
entity) is presented to the classifier which then 
returns a class label.  
Each (query, entity) pair is represented by the 
feature vector using different features and simi-
larity metrics. We chose the following three 
classes of features as they represent a wide range 
of information - lexical features, word-category 
pair, NE type - that have been proved to be ef-
fective in previous works and tasks. We now 
discuss the three categories of features used in 
our framework in details. 
Lexical features. For Bag of Words feature in 
Web People Search, Artiles et al (2009) illu-
strated that noun phrase and n-grams longer than 
2 were not effective in comparison with token-
based features and using bi-grams gives the best 
1294
results only reaching recall 0.7. Thus, we use 
token-based features. The similarity metric we 
choose is cosine (using standard tf.idf weight-
ing). Furthermore, we also take into account the 
co-occurring NEs and represent it in the form of 
token-based features. Then, the single cosine 
similarity feature is based on Co-occurring NEs 
and Bag of Words. 
Word Category Pair. Bunescu (2007) dem-
onstrated that word-category pairs extracted 
from the document and Wikipedia article are a 
good signal for disambiguation. Thus we also 
consider word-category pairs as a feature class, 
i.e., all (w,c) where w is a word from Bag of 
Words of document and c is a category to which 
candidate entity belongs.  
NE Type. This feature is a single binary fea-
ture to guarantee that the type of entity in docu-
ment (i.e. Person, Geo-Political Entity and Or-
ganization) is consistent with the type of entity 
in KB. 
4 Experiments and Discussions 
4.1 Experimental Setup 
    In our study, we use KBP-09 knowledge base 
and document collection for entity linking. In the 
current setting of KBP-09 Data, the KB has been 
generated automatically from Wikipedia. The 
KB contains 818,741 different entities. The doc-
ument collection is mainly composed of news-
wire text from different press agencies. The col-
lection contains 1.3 million documents that span 
from 1994 to the end of 2008. The test data has 
3904 queries across three named entity types: 
Person, Geo-Political Entity and Organization. 
Each query contains a document with an ambi-
guous mention.    
Wikipedia data can be obtained easily from 
the website4 for free research use. It is available 
in the form of database dumps that are released 
periodically. In order to leverage various infor-
mation mentioned in Section 3.1 to derive name 
variations, make use of the links in Wikipedia to 
generate our training corpus and get word cate-
gory information for the disambiguation, we fur-
ther get Wikipedia data directly from the website. 
The version we used in our experiments was re-
leased on Sep. 02, 2009. The automatically 
4 http://download.wikipedia.org   
created corpus (around 10K) was used as the 
training data, and 30K training instances asso-
ciated with the entities in our corpus was derived 
from Wikipedia. 
For pre-processing, we perform sentence 
boundary detection and Chunking derived from 
Stanford parser (Klein and Manning, 2003), 
Named Entity Recognition using a SVM based 
system trained and tested on ACE 2005 with 
92.5(P) 84.3(R) 88.2(F), and coreference resolu-
tion using a SVM based coreference resolver 
trained and tested on ACE 2005 with 79.5%(P), 
66.7%(R) and 72.5%(F).  
We select SVM as the classifier used in this 
paper since SVM can represent the stat-of-the-
art machine learning algorithm. In our imple-
mentation, we use the binary SVMLight devel-
oped by Joachims (1999). The classifier is 
trained with default learning parameters. 
We adopt the measure used in KBP-09 to eva-
luate the performance of entity linking. This 
measure is micro-averaged accuracy: the number 
of correct link divided by the total number of 
queries.
4.2 Baseline Systems 
We build the baseline using the ranking ap-
proach which ranks the candidates based on si-
milarity between mention and candidate entities. 
The entity with the highest rank is chosen. Bag 
of words and co-occurring NEs are represented 
in the form of token-based feature vectors. Then 
tf.idf is employed to calculate similarity between 
feature vectors.  
To make the baseline system with token-
based features state-of-the-art, we conduct a se-
ries of experiments.  Table 1 lists the perfor-
mances of our token-based ranking systems. In 
our experiment, local tokens are text segments 
generated by a text window centered on the 
mention. We set the window size to 55, which is 
the value that was observed to give optimum 
performance for the disambiguation problem 
(Gooi and Allan, 2004). Full tokens and NE are 
all the tokens and named entities co-occurring in 
the text respectively. We notice that tokens of 
the full text as well as the co-occurring named 
entity produce the best baseline performance, 
which we use for the further experiment. 
1295
 Micro-averaged 
Accuracy 
local tokens 60.0 
local tokens + NE 60.6 
full tokens + NE 61.9 
Table 1: Results of the ranking methods 
4.3 Experiment and Result 
As discussed in Section 3.1, we exploit two 
more knowledge sources in Wikipedia: ?did you 
mean? (DYM) and ?Wikipedia search engine? 
(SE) for name variation step. We conduct some 
experiments to compare our name variation me-
thod using Algorithm 1 in Section 3.1 with the 
name variation method of Cucerzan (2007). Ta-
ble 2 shows the comparison results of different 
name variation methods for entity linking. The 
experiments results show that, in entity linking 
task, our name variation method outperforms the 
method of Cucerzan (2007) for both entity dis-
ambiguation methods. 
Name Variation 
Approaches 
Ranking
Method 
Our Disambig-
uation Method 
Cucerzan
(2007) 
60.9 82.2 
+DYM+SE 61.9 83.8 
Table 2: Entity Linking Result for two name 
variation approaches. Column 1 used the base-
line method for entity disambiguation step. Col-
umn 2 used our proposed entity disambiguation 
method.
Table 3 compares the performance of different 
methods for entity linking on the KBP-09 test 
data. Row 1 is the result for baseline system. 
Row 2 and Row 3 show the results training on 
Wikipedia data and our automatically data re-
spectively. Row 4 is the result training on both 
Wikipedia and our created data using the domain 
adaptation method mentioned in Section 3.2.1. It 
shows that our method trained on the automati-
cally generated data alone significantly outper-
forms baseline. Compared Row 3 with Row 2, 
our created data set serves better at training the 
classifier than Wikipedia data. This is due to the 
reason that Wikipedia is a different domain from 
newswire domain. By comparing Row 4 with 
Row 3, we find that by using the domain adapta-
tion method in Section 3.2.1, our method for 
entity linking can be further improved by 1.5%. 
Likely, this is because of the limitation of the 
auto-generated corpus as discussed in Section 
3.2.1. In another hand, Wikipedia can comple-
ment the missing information with the auto-
generated corpus. So combining Wikipedia data 
with our generated data can achieve better result. 
Compared with baseline system using Cucerzan 
(2007) name variation method in Table 2, in to-
tal our proposed method achieves a significant 
22.9% improvement.  
 Micro-averaged Accu-
racy
Baseline 61.9 
Wiki 79.9 
Created Data 82.3 
Wiki? Created Data 83.8 
Table 3: Micro-averaged Accuracy for Entity 
Linking   
     To test the effectiveness of our method to 
deal with new entities not present in KB and ex-
isting entities in KB respectively, we conduct 
some experiments to compare with Baseline.  
Table 4 shows the performances of entity linking 
systems for existing entities (non-NIL) in KB 
and new entity (NIL) which is not present in KB. 
We can see that the binary classifier not only 
effectively reduces the ambiguities to the exist-
ing entities in KB, but also is very useful to 
highlight the new entities to KB for the further 
population. Note that, in baseline system, all the 
new entities are found by the empty candidate 
set of name variation process, while the disam-
biguation component has no contribution.  How-
ever, our approach finds the new entities not on-
ly by the empty candidate set, but also leverag-
ing on disambiguation component which also 
contributes to the performance improvement.  
 non-NIL NIL 
Baseline 72.6  52.4  
Wiki? Created 
Data 
79.2 87.8  
Table 4: Entity Linking on Existing and New 
Entities
1296
Finally, we also compare our method with the 
top 5 systems in KBP-09. Among them, 
Siel_093 (Varma et al 2009) and NLPR_KBP1
(Han and Zhao 2009) use similarity ranking ap-
proach; Stanford_UBC2 (Agirre et al 2009),
QUANTA1 (Li et al 2009) and hltcoe1 (McNa-
mee et al 2009) use supervised approach. From 
the results shown in Figure 2, we observe that 
our method outperforms all the top 5 systems 
and the baseline system of KBP-09. Specifically, 
our method achieves better result than both simi-
larity ranking approaches. This is due to the li-
mitations of the ranking approach which have 
been discussed in Section 2. We also observe 
that our method gets a 5% improvement over 
Stanford_UBC2. This is because they collect 
their training data from Wikipedia which is a 
different domain from document collection of 
entity linking, news articles in this case; while 
our automatic data generation method can create 
a data set from the same domain as the docu-
ment collection. Our system also outperforms 
QUANTA1 and hltcoe1 because they train their 
model on a small manually created data set 
(1,615 examples), while our method can auto-
matically generate a much larger data set. 
Figure 2: A comparison with KBP09 systems 
5 Conclusion
 The purpose of this paper is to explore how 
to leverage the automatically generated large 
scale annotation for entity linking. Traditionally, 
without any training data available, the solution 
is to rank the candidates based on similarity. 
However, it is difficult for the ranking approach 
to detect a new entity that is not present in KB, 
and it is also difficult to combine different fea-
tures. In this paper, we create a large corpus for 
entity linking by an automatic method. A binary 
classifier is then trained to filter out KB entities 
that are not similar to current mentions. We fur-
ther leverage on the Wikipedia documents to 
provide other information which is not available 
in our generated corpus through a domain adap-
tion approach. Furthermore, new information 
sources for finding more variations also contri-
bute to the overall 22.9% accuracy improve-
ments on KBP-09 test data over baseline. 
References  
E. Agirre et al Stanford-UBC at TAC-KBP. In Pro-
ceedings of Test Analysis Conference 2009 (TAC 
09).
J. Artiles, J. Gonzalo, and S. Sekine. 2007. The se-
meval-2007 web evaluation: Establishing a 
benchmark for the web people search task. In Pro-
ceeding of the Fourth International Work-shop on 
Semantic Evaluations (SemEval-2007).
J. Artiles, E. Amigo and J. Gonzalo. 2009. The role 
of named entities in Web People Search. In pro-
ceeding of the 47th Annual Meeting of the Associa-
tion for Computational Linguistics. 
R. Bunescu. 2007. Learning for information extrac-
tion from named entity recognition and disambig-
uation to relation extraction. Ph.D thesis, Universi-
ty of Texas at Austin, 2007. 
T. H. Cormen, et al 2001. Introduction To Algo-
rithms (Second Edition). The MIT Press, Page 
350-355. 
S. Cucerzan. 2007. Large-Scale Named Entity Dis-
ambiguation Based on Wikipedia Data. Empirical 
Methods in Natural Language Processing, June 
28-30, 2007. 
H. Daum? III. 2007. Frustratingly easy domain adap-
tation. In Proceedings of the 45th Annual Meeting 
of the Association of Computational Linguistics . 
C. H. Gooi and J. Allan. 2004. Cross-document core-
ference on a large scale corpus. In proceedings of 
Human Language Technology Conference North 
American Association for Computational Linguis-
tics Annual Meeting, Boston, MA. 
X. Han and J. Zhao. NLPR_KBP in TAC 2009 KBP 
Track: A Two-Stage Method to Entity Linking. In 
Proceedings of Test Analysis Conference 2009 
(TAC 09).
0.838
0.8217
0.8033
0.7984
0.7884
0.7672
0.571
0.5
0.55
0.6
0.65
0.7
0.75
0.8
0.85
0.9
1297
T. Joachims. 1999. Making large-scale SVM learning 
practical. In Advances in Kernel Methods - Sup-
port Vector Learning. MIT Press. 
D. Klein and C. D. Manning. 2003. Fast Exact Infe-
rence with a Factored Model for Natural Language 
Parsing. In Advances in Neural Information 
Processing Systems 15 (NIPS 2002), Cambridge, 
MA: MIT Press, pp. 3-10. 
F. LI et al THU QUANTA at TAC 2009 KBP and 
RTE Track. In Proceedings of Test Analysis Con-
ference 2009 (TAC 09).  
P. McNamee and H. T. Dang. 2009. Overview of the 
TAC 2009 Knowledge Base Population Track. In 
Proceedings of Test Analysis Conference 2009 
(TAC 09).  
P. McNamee et al HLTCOE Approaches to Know-
ledge Base Population at TAC 2009.  In Proceed-
ings of Test Analysis Conference 2009 (TAC 09).  
H. T. Nguyen and T. H. Cao. 2008. Named Entity 
Disambiguation on an Ontology Enriched by Wi-
kipedia. 2008 IEEE International Conference on 
Research, Innovation and Vision for the Future in 
Computing & Communication Technologies. 
B. Popov et al 2004. KIM - a Semantic Platform for 
Information Extraction and Retrieval. In Journal 
of Natural Language Engineering, Vol. 10, Issue 
3-4, Sep 2004, pp. 375-392, Cambridge University 
Press.
V. Raphael, K. Joachim and M. Wolfgang, 2007. 
Towards ontology-based disambiguation of geo-
graphical identifiers. In Proceeding of the 16th
WWW workshop on I3: Identity, Identifiers, Identi-
fications, 2007.  
V. Varma et al 2009. IIIT Hyderabad at TAC 2009. 
In Proceedings of Test Analysis Conference 2009 
(TAC 09).  
T. Zesch, C. Muller and I. Gurevych. 2008. Extrac-
tiong Lexical Semantic Knowledge from Wikipe-
dia and Wiktionary. In Proceedings of the Confe-
rence on Language Resources and Evaluation 
(LREC), 2008.  
1298
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 572?581,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Omni-word Feature and Soft Constraint
for Chinese Relation Extraction
Yanping Chen
?
Qinghua Zheng
?
?
MOEKLINNS Lab, Department of Computer Science and Technology
Xi?an Jiaotong University, China
ypench@gmail.com, qhzheng@mail.xjtu.edu.cn
?
Amazon.com, Inc.
wzhan@amazon.com
Wei Zhang
?
Abstract
Chinese is an ancient hieroglyphic. It is inat-
tentive to structure. Therefore, segmenting
and parsing Chinese are more difficult and less
accurate. In this paper, we propose an Omni-
word feature and a soft constraint method for
Chinese relation extraction. The Omni-word
feature uses every potential word in a sentence
as lexicon feature, reducing errors caused by
word segmentation. In order to utilize the
structure information of a relation instance, we
discuss how soft constraint can be used to cap-
ture the local dependency. Both Omni-word
feature and soft constraint make a better use
of sentence information and minimize the in-
fluences caused by Chinese word segmenta-
tion and parsing. We test these methods on
the ACE 2005 RDC Chinese corpus. The re-
sults show a significant improvement in Chi-
nese relation extraction, outperforming other
methods in F-score by 10% in 6 relation types
and 15% in 18 relation subtypes.
1 Introduction
Information Extraction (IE) aims at extracting
syntactic or semantic units with concrete concepts
or linguistic functions (Grishman, 2012; McCal-
lum, 2005). Instead of dealing with the whole doc-
uments, focusing on designated information, most
of the IE systems extract named entities, relations,
quantifiers or events from sentences.
The relation recognition task is to find the rela-
tionships between two entities. Successful recog-
nition of relation implies correctly detecting both
the relation arguments and relation type. Although
this task has received extensive research. The per-
formance of relation extraction is still unsatisfac-
tory with a F-score of 67.5% for English (23 sub-
types) (Zhou et al, 2010). Chinese relation extrac-
tion also faces a weak performance having F-score
about 66.6% in 18 subtypes (Dandan et al, 2012).
The difficulty of Chinese IE is that Chinese
words are written next to each other without de-
limiter in between. Lacking of orthographic word
makes Chinese word segmentation difficult. In
Chinese, a single sentence often has several seg-
mentation paths leading to the segmentation ambi-
guity problem (Liang, 1984). The lack of delimiter
also causes the Out-of-Vocabulary problem (OOV,
also known as new word detection) (Huang and
Zhao, 2007). These problems are worsened by the
fact that Chinese has a large number of characters
and words. Currently, the state-of-the-art Chinese
OOV recognition system has performance about
75% in recall (Zhong et al, 2012). The errors
caused by segmentation and OOV will accumulate
and propagate to subsequent processing (e.g. part-
of-speech (POS) tagging or parsing).
Therefore, the Chinese relation extraction is
more difficult. According to our survey, com-
pared to the same work in English, the Chinese re-
lation extraction researches make less significant
progress.
Based on the characteristics of Chinese, in this
paper, an Omni-word feature and a soft constraint
method are proposed for Chinese relation extrac-
tion. We apply these approaches in a maximum
entropy based system to extract relations from the
ACE 2005 corpus. Experimental results show that
our method has made a significant improvement.
The contributions of this paper include
1. Propose a novel Omni-word feature for Chi-
nese relation extraction. Unlike the tradi-
tional segmentation based method, which is a
partition of the sentence, the Omni-word fea-
ture uses every potential word in a sentence
as lexicon feature.
2. Aiming at the Chinese inattentive structure,
we utilize the soft constraint to capture the
local dependency in a relation instance. Four
constraint conditions are proposed to gener-
572
ate combined features to capture the local de-
pendency and maximize the classification de-
termination.
The rest of this paper is organized as follows.
Section 2 introduces the related work. The Omni-
word feature and soft constrain are proposed in
Section 3. We give the experimental results in Sec-
tion 3.2 and analyze the performance in Section 4.
Conclusions are given in Section 5.
2 Related Work
There are two paradigms extracting the relation-
ship between two entities: the Open Relation Ex-
traction (ORE) and the Traditional Relation Ex-
traction (TRE) (Banko et al, 2008).
Based on massive and heterogeneous corpora,
the ORE systems deal with millions or billions
of documents. Even strict filtrations or constrains
are employed to filter the redundancy information,
they often generate tens of thousands of relations
dynamically (Hoffmann et al, 2010). The practi-
cability of ORE systems depends on the adequate-
ness of information in a big corpus (Brin, 1999).
Most of the ORE systems utilize weak supervi-
sion knowledge to guide the extracting process,
such as: Databases (Craven and Kumlien, 1999),
Wikipedia (Wu and Weld, 2007; Hoffmann et al,
2010), Regular expression (Brin, 1999; Agichtein
and Gravano, 2000), Ontology (Carlson et al,
2010; Mohamed et al, 2011) or Knowledge Base
extracted automatically from Internet (Mintz et al,
2009; Takamatsu et al, 2012). However, when
iteratively coping with large heterogeneous data,
the ORE systems suffer from the ?semantic drift?
problem, caused by error accumulation (Curran
et al, 2007). Agichtein, Carlson and Fader et
al. (2010; 2011; 2000) propose syntactic and se-
mantic constraints to prevent this deficiency. The
soft constraints, proposed in this paper, are com-
bined features like these syntactic or semantic con-
straints, which will be discussed in Section 3.2.
The TRE paradigm takes hand-tagged ex-
amples as input, extracting predefined relation
types (Banko et al, 2008). The TRE systems
use techniques such as: Rules (Regulars, Pat-
terns and Propositions) (Miller et al, 1998), Ker-
nel method (Zhang et al, 2006b; Zelenko et al,
2003), Belief network (Roth and Yih, 2002), Lin-
ear programming (Roth and Yih, 2007), Maximum
entropy (Kambhatla, 2004) or SVM (GuoDong et
al., 2005). Compared to the ORE systems, the
TRE systems have a robust performance. Disad-
vantages of the TRE systems are that the manu-
ally annotated corpus is required, which is time-
consuming and costly in human labor. And mi-
grating between different applications is difficult.
However, the TRE systems are evaluable and com-
parable. Different systems running on the same
corpus can be evaluated appropriately.
In the field of Chinese relation extraction, Liu
et al (2012) proposed a convolution tree ker-
nel. Combining with external semantic resources,
a better performance was achieved. Che et
al. (2005) introduced a feature based method,
which utilized lexicon information around entities
and was evaluated on Winnow and SVM classi-
fiers. Li and Zhang et al (2008; 2008) explored
the position feature between two entities. For each
type of these relations, a SVM was trained and
tested independently. Based on Deep Belief Net-
work, Chen et al (2010) proposed a model han-
dling the high dimensional feature space. In addi-
tion, there are mixed models. For example, Lin et
al. (2010) employed a model, combining both the
feature based and the tree kernel based methods.
Despite the popularity of kernel based method,
Huang et al (2008) experimented with different
kernel methods and inferred that simply migrating
from English kernel methods can result in a bad
performance in Chinese relation extraction. Chen
and Li et al (2008; 2010) also pointed out that,
due to the inaccuracy of Chinese word segmenta-
tion and parsing, the tree kernel based approach
is inappropriate for Chinese relation extraction.
The reason of the tree kernel based approach not
achieve the same level of accuracy as that from En-
glish may be that segmenting and parsing Chinese
are more difficult and less accurate than process-
ing English.
In our research, we proposed an Omni-word
feature and a soft constraint method. Both ap-
proaches are based on the Chinese characteristics.
Therefore, better performance is expected. In the
following, we introduce the feature construction,
which discusses the proposed two approaches.
3 Feature Construction
In this section, the employed candidate features
are discussed. And four constraint conditions
are proposed to transform the candidate features
into combined features. The soft constraint is the
573
method to generate the combine features
1
.
3.1 Candidate Feature Set
In the ACE corpus, an entity is an object or set of
objects in the world. An entity mention is a ref-
erence to an entity. The entity mention is anno-
tated with its full extent and its head, referred to as
the extend mention and the head mention respec-
tively. The extent mention includes both the head
and its modifiers. Each relation has two entities as
arguments: Arg-1 and Arg-2, referred to as E1 and
E2. A relation mention (or instance) is the embod-
iment of a relation. It is referred by the sentence
(or clause) in which the relation is located in. In
our work, we focus on the detection and recogni-
tion of relation mention.
Relation identification is handled as a classifi-
cation problem. Entity-related information (e.g.
head noun, entity type, subtype, CLASS, LDC-
TYPE, etc.) are supposed to be known and pro-
vided by the corpus. In our experiment, the entity
type, subtype and the head noun are used.
All the employed features are simply classi-
fied into five categories: Entity Type and Subtype,
Head Noun, Position Feature, POS Tag and Omni-
word Feature. The first four are widely used. The
last one is proposed in this paper and is discussed
in detail.
Entity Type and Subtype: In ACE 2005 RDC
Chinese corpus, there are 7 entity types (Person,
Organization, GPE, Location, Facility, Weapon
and Vehicle) and 44 subtypes (e.g. Group, Gov-
ernment, Continent, etc.).
Head Noun: The head noun (or head mention)
of entity mention is manually annotated. This fea-
ture is useful and widely used.
Position Feature: The position structure be-
tween two entity mentions (extend mentions). Be-
cause the entity mentions can be nested, two en-
tity mentions may have four coarse structures: ?E1
is before E2?, ?E1 is after E2?, ?E1 nests in E2?
and ?E2 nests in E1?, encoded as: ?E1_B_E2?,
?E1_A_E2?, ?E1_N_E2? and ?E2_N_E1?.
POS Tag: In our model, we use only the ad-
jacent entity POS tags, which lie in two sides of
the entity mention. These POS tags are labelled
by the ICTCLAS package
2
. The POS tags are not
used independently. It is encoded by combining
1
If without ambiguity, we also use the terminology of
?soft constraint? denoting features generated by the em-
ployed constraint conditions.
2
http://ictclas.org/
the POS tag with the adjacent entity mention in-
formation. For example ?E1_Right_n? means
that the right side of the first entity is a noun (?n?).
Omni-word Feature: The notion of ?word?
in Chinese is vague and has never played a role
in the Chinese philological tradition (Sproat et
al., 1996). Some Chinese segmentation perfor-
mance has been reported precision scores above
95% (Peng et al, 2004; Xue, 2003; Zhang et
al., 2003). However, for the same sentence, even
native peoples in China often disagree on word
boundaries (Hoosain, 1992; Yan et al, 2010).
Sproat et al (1996) has showed that there is a con-
sistence of 75% on the segmentation among differ-
ent native Chinese speakers. The word-formation
of Chinese also implies that the meanings of a
compound word are made up, usually, by the
meanings of words that contained in it (Hu and
Du, 2012). So, fragments of phrase are also infor-
mative.
Because high precision can be received by using
simple lexical features (Kambhatla, 2004; Li et al,
2008). Making better use of such information is
beneficial. In consideration of the Chinese char-
acteristics, we use every potential word in a rela-
tion mention as the lexical features. For example,
relation mention ?????????? (Taipei
Daan Forest Park) has a ?PART-WHOLE? relation
type. The traditional segmentation method may
generate four lexical features {????, ????, ??
??, ????}, which is a partition of the relation
mention. On the other hand, the Omni-word fea-
ture denoting all the possible words in the relation
mention may generate features as:
{???, ???, ???, ???, ???, ???, ???, ???,
????, ????, ????, ????, ??????,
????????}3
Most of these features are nested or overlapped
mutually. So, the traditional character-based or
word-based feature is only a subset of the Omni-
word feature. To extract the Omni-word feature,
only a lexicon is required, then scan the sentence
to collect every word.
Because the number of lexicon entry determines
the dimension of the feature space, performance
of Omni-word feature is influenced by the lexicon
being employed. In this paper, we generate the
lexicon by merging two lexicons. The first lexicon
3
The generated Omni-word features dependent on the em-
ployed lexicon.
574
is obtained by segmenting every relation instance
using the ICTCLAS package, collecting very word
produced by ICTCLAS. Because the ICTCLAS
package was trained on annotated corpus contain-
ing many meaningful lexicon entries. We expect
this lexicon to improve the performance. The sec-
ond lexicon is the Lexicon Common Words in Con-
temporary Chinese
4
.
Despite the Omni-word can be seen as a sub-
set of n-Gram feature. It is not the same as the
n-Gram feature. N-Gram features are more frag-
mented. In most of the instances, the n-Gram fea-
tures have no semantic meanings attached to them,
thus have varied distributions. Furthermore, for
a single Chinese word, occurrences of 4 charac-
ters are frequent. Even 7 or more characters are
not rare. Because Chinese has plenty of char-
acters
5
, when the corpus becoming larger, the n-
Gram (n?4) method is difficult to be adopted. On
the other hand, the Omni-word can avoid these
problems and take advantages of Chinese charac-
teristics (the word-formation and the ambiguity of
word segmentation).
3.2 Soft Constraint
The structure information (or dependent informa-
tion) of relation instance is critical for recognition.
However, even in English, ?deeper? analysis (e.g.
logical syntactic relations or predicate-argument
structure) may suffer from a worse performance
caused by inaccurate chunking or parsing. Hence,
the local dependency contexts around the rela-
tion arguments are more helpful (Zhao and Gr-
ishman, 2005). Zhang et al (2006a) also showed
that Path-enclosed Tree (PT) achieves the best per-
formance in the kernel based relation extraction.
In this field, the tree kernel based method com-
monly uses the parse tree to capture the struc-
ture information (Zelenko et al, 2003; Culotta and
Sorensen, 2004). On the other hand, the feature
based method usually uses the combined feature
to capture such structure information (GuoDong
et al, 2005; Kambhatla, 2004).
In the open relation extraction domain, syntac-
tic and semantic constraints are widely employed
to prevent the ?semantic drift? problem. Such con-
straints can also be seen as structural constraint.
4
Published by Ministry of Education of the People?s Re-
public of China in 2008, containing 56,008 entries.
5
Currently, at least 13000 characters are used by na-
tive Chinese people. Modern Chinese Dictionary: http:
//www.cp.com.cn/
Most of these constraints are hard constraints. Any
relation instance violating these constraints (or be-
low a predefined threshold) will be abandoned.
For example, Agichtein and Gravano (2000) gen-
erates patterns according to a confidence threshold
(?
t
). Fader et al (2011) utilizes a confidence func-
tion. And Carlson et al (2010) filters candidate
instances and patterns using the number of times
they co-occurs.
Deleting of relation instances is acceptable for
open relation extraction because it always deals
with a big data set. But it?s not suitable for tra-
ditional relation extraction, and will result in a
low recall. Utilizing the notion of combined fea-
ture (GuoDong et al, 2005; Kambhatla, 2004), we
replace the hard constraint by the soft constraint.
Each soft constraint (combined feature) has a pa-
rameter trained by the classifier indicating the dis-
crimination ability it has. No subjective or priori
judgement is adopted to delete any potential de-
terminative constraint (except for the reason of di-
mensionality reduction).
Most of the researches make use of the com-
bined feature, but rarely analyze the influence of
the approaches we combine them. In this paper,
we use the soft constraint to model the local de-
pendency. It is a subset of the combined feature,
generated by four constraint conditions: singleton,
position sensitive, bin sensitive and semantic pair
. For every employed candidate feature, an appro-
priate constraint condition is selected to combine
them with additional information to maximize the
classification determination.
Singleton: A feature is employed as a single-
ton feature when it is used without combining with
any information. In our experiments, only the po-
sition feature is used as singleton feature.
Position Sensitive: A position sensitive feature
has a label indicating which entity mention it de-
pends on. In our experiment, the Head noun and
POS Tag are utilized as position sensitive features,
which has been introduced in Section 3.1. For ex-
ample, ???_E1? means that the head noun ??
?? depend on the first entity mention.
Semantic Pair: Semantic pair is generated by
combining two semantic units. Two kinds of
semantic pair are employed. Those are gener-
ated by combining two entity types or two en-
tity subtypes into a semantic pair. For example,
?Person_Location? denotes that the type of
the first relation argument is a ?Person? (entity
575
type) and the second is a ?Location? (entity type).
Semantic pair can capture both the semantic and
structure information in a relation mention.
Bin Sensitive: In our study, Omni-word feature
is not added as ?bag of words?. To use the Omni-
word feature, we segment each relation mention
by two entity mentions. Together with the two en-
tity mentions, we get five parts: ?FIRST?, ?MID-
DLE?, ?END?, ?E1? and ?E2? (or less, if the two
entity mentions are nested). Each part is taken
as an independent bin. A flag is used to distin-
guish them. For example, ???_Bin_F?, ??
?_Bin_E1? and ???_Bin_E? mean that the
lexicon entry ???? appears in three bins: the
FIRST bin, the first entity mention (E1) bin and
the END bin. They will be used as three indepen-
dent features.
To sum up, among the five candidate feature
sets, the position feature is used as a singleton fea-
ture. Both head noun and POS tag are position
sensitive. Entity types and subtypes are employed
as semantic pair. Only Omni-word feature is bin
sensitive. In the following experiments, focusing
on Chinese relation extraction, we will analyze the
performance of candidate feature sets and study
the influence of the constraint conditions.
sectionExperiments
In this section, methodologies of the Omni-
word feature and the soft constraint are tested.
Then they are compared with the state-of-the-art
methods.
3.3 Settings and Results
We use the ACE 2005 RDC Chinese corpus, which
was collected from newswires, broadcasts and we-
blogs, containing 633 documents with 6 major re-
lation types and 18 subtypes. There are 8,023 rela-
tions and 9,317 relation mentions. After deleting
5 documents containing wrong annotations
6
, we
keep 9,244 relation mentions as positive instances.
To get the negative instances, each document is
segmented into sentences
7
. Those sentences that
do not contain any entity mention pair are deleted.
For each of the remained sentences, we iteratively
extract every entity mention pair as the arguments
of relation instances for predicting. For example,
suppose a sentence has three entity mentions: A,B
6
DAVYZW {20041230.1024, 20050110.1403,
20050111.1514, 20050127.1720, 20050201.1538}.
7
The five punctuations are used as sentence boundaries:
Period (?), Question mark (?), Exclamatory mark (?),
Semicolon (?) and Comma (?).
and C. Because the relation arguments are order
sensitive, six entity mention pairs can be gener-
ated: [A,B], [A,C], [B,C], [B,A], [C,A] and [C,B].
After discarding the entity mention pairs that were
used as positive instances, we generated 93,283
negative relation instances labelled as ?OTHER?.
Then, we have 7 relation types and 19 subtypes.
A maximum entropy multi-class classifier is
trained and tested on the generated relation in-
stances. We adopt the five-fold cross validation
for training and testing. Because we are interested
in the 6 annotated major relation types and the 18
subtypes, we average the results of five runs on the
6 positive relation types (and 18 subtypes) as the
final performance. F-score is computed by
2? (Precision?Recall)
Precision+Recall
To implement the maximum entropy model, the
toolkit provided by Le (2004) is employed. The
iteration is set to 30.
Five candidate feature sets are employed to gen-
erate the combined features. The entity type and
subtype, head noun, position feature are referred
to as F
thp
8
. The POS tags are referred to as F
pos
.
The Omni-word feature set is denoted by F
ow
.
Table 1 gives the performance of our system on
the 6 types and 18 subtypes. Note that, in this pa-
per, bare numbers and numbers in the parentheses
represent the results of the 6 types and the 18 sub-
types respectively.
Table 1: Performance on Type (Subtype)
Features P R F
F
thp
61.51 48.85 54.46
(52.92) (36.92) (43.49)
F
ow
80.16 75.45 77.74
(66.98) (54.85) (60.31)
F
thp
? F
pos
83.93 77.81 80.76
(69.83) (61.63) (65.47)
F
thp
? F
ow
92.40 88.37 90.34
(81.94) (70.69) (75.90)
F
thp
? F
pos
? F
ow
92.26 88.51 90.35
(80.52) (70.96) (75.44)
In Row 1, because F
thp
are features directly ob-
tained from annotated corpus, we take this per-
8
?thp? is an acronym of ?type, head, position?. Features
in F
thp
are the candidate features combined with the corre-
sponding constraint conditions. The following F
pos
and F
ow
are the same.
576
formance as our referential performance. In Row
2, with only the F
ow
feature, the F-score already
reaches 77.74% in 6 types and 60.31% in 18 sub-
types. The last row shows that adding the F
pos
al-
most has no effect on the performance when both
the F
thp
and F
ow
are in use. The results show that
F
ow
is effective for Chinese relation extraction.
The superiorities of Owni-word feature depend
on three reasons. First, the specificity of Chi-
nese word-formation indicates that the subphrases
of Chinese word (or phrase) are also informative.
Second, most of relation instances have limited
context. The Owni-word feature, utilizing every
possible word in them, is a better way to capture
more information. Third, the entity mentions are
manually annotated. They can precisely segment
the relation instance into corresponding bins. Seg-
mentation of bins bears the sentence structure in-
formation. Therefore, the Owni-word feature with
bin information can make a better use of both the
syntactic information and the local dependency.
3.4 Comparison
Various systems were proposed for Chinese re-
lation extraction. We mainly focus on systems
trained and tested on the ACE corpus. Table 2 lists
three systems.
Table 2: Survey of Other Systems
System P R F
Che et al (2005) 76.13 70.18 73.27
Zhang et al (2011)
80.71 62.48 70.43
(77.75) (60.20) (67.86)
Liu et al (2012)
81.1 61.0 69.0
(79.1) (57.5) (66.6)
Che et al (2005) was implemented on the ACE
2004 corpus, with 2/3 data for training and 1/3 for
testing. The performance was reported on 7 re-
lation types: 6 major relation types and the none
relation (or negative instance). Zhang et al (2011)
was based on the ACE 2005 corpus with 75% data
for training and 25% for testing. Performances
about the 7 types and 19 subtypes were given.
Both of them are feature based methods. Liu et
al. (2012) is a kernel based method evaluated on
the ACE 2005 corpus. The five-fold cross valida-
tion was used and declared the performances on 6
relation types and 18 subtypes.
The data preprocessing makes differences from
our experiments to others. In order to give a bet-
ter comparison with the state-of-the-art methods,
based on our experiment settings and data, we im-
plement the two feature based methods proposed
by Che et al (2005) and Zhang et al (2011) in Ta-
ble 2. The results are shown in Table 3.
In Table 3, Ei (i ? 1, 2) represents entity men-
tion. ?Order? in Che et al (2005) denotes the posi-
tion structure of entity mention pair. Four types of
order are employed (the same as ours). Word
Ei
+
?
k
and POS
Ei
+
?
k
are the words and POS of Ei, ?
+
?
k?
means that it is the kth word (of POS) after (+)
or before (-) the corresponding entity mention. In
this paper, k = 1 and k = 2 were set.
In Row 2, the ?Uni-Gram? represents the Uni-
gram features of internal and external character
sequences. Internal character sequences are the
four entity extend and head mentions. Five kinds
of external character sequences are used: one In-
Between character sequence between E1 and E2
and four character sequences around E1 and E2 in
a given window size w s. The w s is set to 4. The
?Bi-Gram? is the 2-gram feature of internal and
external character sequences. Instead of the 4 po-
sition structures, the 9 position structures are used.
Please refer to Zhang et al (2011) for the details
of these 9 position structures.
In Table 3, it is shown that our system outper-
forms other systems, in F-score, by 10% on 6 re-
lation types and by 15% on 18 subtypes.
For researchers who are interested in our work,
the source code of our system and our imple-
mentations of Che et al (2005) and Zhang et
al. (2011) are available at https://github.
com/YPench/CRDC.
4 Discussion
In this section, we analyze the influences of em-
ployed feature sets and constraint conditions on
the performances.
Most papers in relation extraction try to aug-
ment the number of employed features. In our ex-
periment, we found that this does not always guar-
antee the best performance, despite the classifier
being adopted is claimed to control these features
independently. Because features may interact mu-
tually in an indirect way, even with the same fea-
ture set, different constraint conditions can have
significant influences on the final performance.
In Section 3, we introduced five candidate fea-
ture sets. Instead of using them as independent
features, we combined them with additional in-
577
Table 3: Comparing With the State-of-the-Art Methods
System Feature Set P R F
(Che et al, 2005)
Ei.Type, Ei.Subtype, Order, Word
Ei
+
?
1
,
Word
Ei
+
?
2
, POS
Ei
+
?
1
, POS
Ei
+
?
2
84.81 75.69 79.99
(64.89) (52.99) (58.34)
(Zhang et al, 2011)
Ei.Type, Ei.Subtype, 9 Position Feature,
Uni-Gram, Bi-Gram
79.56 72.99 76.13
(66.78) (54.56) (60.06)
Ours F
thp
? F
pos
? F
ow
92.26 88.51 90.35
(80.52) (70.96) (75.44)
formation. We proposed four constraint condi-
tions to generate the soft constraint features. In
Table 4, the performances of candidate features
are compared when different constraint conditions
was employed.
In Column 3 of Table 4 (Constraint Condi-
tion), (1), (2), (3), (4) and (5) stand for the referen-
tial feature sets
9
in Table 1. Symbol ?/? means that
the corresponding candidate features in the refer-
ential feature set are substituted by the new con-
straint condition. Par in Column 4 is the num-
ber of parameters in the trained maximum entropy
model, which indicate the model complexity. I in
Column 5 is the influence on performance. ?-? and
?+? mean that the performance is decreased or in-
creased.
The first observation is that the combined fea-
tures are more powerful than used as singletons.
Model parameters are increased by the combined
features. Increasing of parameters projects the
relation extraction problem into a higher dimen-
sional space, making the decision boundaries be-
come more flexible.
The named entities in the ACE corpus are also
annotated with the CLASS and LDCTYPE labels.
Zhou et al (2010) has shown that these labels can
result in a weaker performance. Row 1, 2 and 3
show that, no matter how they are used, the perfor-
mances decrease obviously. The reason of the per-
formance degradation may be caused by the prob-
lem of over-fitting or data sparseness.
At most of the time, increase of model param-
eters can result in a better performance. Except
in Row 8 and Row 11, when two head nouns
of entity pair were combined as semantic pair
and when POS tag were combined with the en-
tity type, the performances are decreased. There
are 7356 head nouns in the training set. Combin-
ing two head nouns may increase the feature space
9
(1), (2), (3), (4) and (5) denote F
thp
, F
ow
, F
thp
?F
pos
,
F
thp
? F
ow
and F
thp
? F
pos
? F
ow
respectively.
by 7356? (7356? 1). Such a large feature space
makes the occurrence of features close to a random
distribution, leading to a worse data sparseness.
In Row 4, 10 and 13, these features are used as
singleton, the performance degrades considerably.
This means that, the missing of sentence structure
information on the employed features can lead to
a bad performance.
Row 9 and 12 show an interesting result. Com-
paring the reference set (5) with the reference set
(3), the Head noun and adjacent entity POS tag
get a better performance when used as singletons.
These results reflect the interactions between dif-
ferent features. Discussion of this issue is be-
yond this paper?s scope. In this paper, for a better
demonstration of the constraint condition, we still
use the Position Sensitive as the default setting to
use the Head noun and the adjacent entity POS
tag.
Row 13 and 14 compare the Omni-word fea-
ture (By-Omni-word) with the traditional seg-
mentation based feature (By-Segmentation). By-
Segmentation denotes the traditional segmentation
based feature set generated by a segmentation tool,
collecting every output of relation mention. In this
place, the ICTCLAS package is adopted too.
Conventionally, if a sentence is perfectly seg-
mented, By-Segmentation is straightforward and
effective. But, our experiment shows different ob-
servations. Row 13 and 14 show that the Omni-
word method outperforms the traditional method.
Especially, when the bin information is used (Row
15), the performance of Omni-word feature in-
creases considerably.
Row 14 shows that, compared with the tradi-
tional method, the Omni-word feature improves
the performance by about 8.79% in 6 relation
types and 11.83% in 18 subtypes in F-core. Such
improvement may reside in the three reasons dis-
cussed in Section 3.3.
In short, from Table 4 we have seen that the en-
578
Table 4: Influence of Feature Set
No. Feature Constraint Condition Par P R F I
1
entity
CLASS and
LDCTYPE
(1)/as singleton
21,112 60.29 42.82 50.07 -4.39
21,910 (41.70) (25.18) (31.40) -12.09
2
(1)/combined with
positional Info
21,159 63.02 44.47 52.15 -2.31
22,013 (41.61) (26.31) (32.24) -11.25
3 (1)/as semantic pair
21,207 63.35 47.67 54.40 -0.06
22,068 (42.98) (31.34) (36.25) -7.24
4
Type,
Subtype
semantic
pair
(1)/as singleton
19,390 51.37 29.16 37.20 -17.26
147,435 (32.8) (18.97) (24.06) -19.43
5
(1)/combined with
positional info
19,524 61.77 43.67 51.17 -3.29
20,297 (41.13) (26.83) (32.47) -11.02
6 (5)/as singleton
105,865 91.39 87.92 89.62 -0.73
121,218 (79.32) (68.73) (73.65) -1.79
7
head noun
(3)/as singleton
21,450 85.66 75.74 80.40 -0.36
22,409 (64.38) (57.14) (60.55) -0.34
8 (3)/as semantic pair
77,333 83.05 73.14 77.78 -2.54
77,947 (59.70) (51.70) (55.41) -5.48
9 (5)/as singleton
100,963 92.50 88.90 90.66 +0.31
115,499 (82.63) (71.67) (76.76) +1.32
10
adjacent
entity POS
tag
(3)/as singleton
21,450 72.66 61.16 66.41 -13.91
22,409 (62.42) (45.69) (52.76) -8.13
11
(3)/combined with
entity type
22,151 80.66 71.67 75.90 -4.42
23,357 (63.41) (53.16) (57.83) -3.06
12 (5)/as singleton
106,931 92.50 88.66 90.54 +0.19
121,194 (82.04) (71.36) (76.33) +0.89
13
Omni-word
feature
(2)/By-Segmentation as
singleton
36,916 67.19 60.12 63.46 -14.28
41,652 (55.85) (44.50) (49.54) -10.77
14
(2)/By-Segmentation
with bins
79,430 71.12 66.90 68.95 -8.79
84,715 (54.76) (43.50) (48.48) -11.83
15
(2)/By-Omni-word as
singleton
47,428 69.67 63.77 66.59 -11.15
57,702 (54.85) (48.84) (51.67) -8.64
16 (5)/as singleton
57,321 91.43 86.37 88.83 -1.52
67,722 (76.43) (69.57) (72.84) -2.60
tity type and subtype maximize the performance
when used as semantic pair. Head noun and
adjacent entity POS tag are employed to com-
bine with positional information. Omni-word fea-
ture with bins information can increase the perfor-
mance considerably. Our model (in Section 3.3)
uses these settings. This insures that the perfor-
mances of the candidate features are optimized.
5 Conclusion
In this paper, We proposed a novel Omni-word
feature taking advantages of Chinese sub-phrases.
We also introduced the soft constraint method for
Chinese relation recognition. The soft constraint
utilizes four constraint conditions to catch the
structure information in a relation instance. Both
the Omni-word feature and soft constrain make
better use of information a sentence has, and min-
imize the deficiency caused by Chinese segmenta-
tion and parsing.
The size of the employed lexicon determines the
dimension of the feature space. The first impres-
sion is that more lexicon entries result in more
power. However, more lexicon entries also in-
crease the computational complexity and bring in
noises. In our future work, we will study this issue.
The notion of soft constraints can also be extended
to include more patterns, rules, regexes or syntac-
579
tic constraints that have been used for information
extraction. The usability of these strategies is also
left for future work.
Acknowledgments
The research was supported in part by NSF of
China (91118005, 91218301, 61221063); 863
Program of China (2012AA011003); Cheung
Kong Scholar?s Program; Pillar Program of
NST (2012BAH16F02); Ministry of Education
of China Humanities and Social Sciences Project
(12YJC880117); The Ministry of Education Inno-
vation Research Team (IRT13035).
References
Eugene Agichtein and Luis Gravano. 2000. Snow-
ball: Extracting relations from large plain-text col-
lections. In Proceedings of DL ?00, pages 85?94.
ACM.
Michele Banko, Oren Etzioni, and Turing Center.
2008. The tradeoffs between open and traditional
relation extraction. Proceedings of ACL-HLT ?08,
pages 28?36.
Sergey Brin. 1999. Extracting patterns and relations
from the world wide web. The World Wide Web and
Databases, pages 172?183.
Andrew Carlson, Justin Betteridge, Richard C. Wang,
Estevam R. Hruschka Jr, and Tom M. Mitchell.
2010. Coupled semi-supervised learning for infor-
mation extraction. In Proceedings of WSDM ?10,
pages 101?110.
Wanxiang Che, Ting Liu, and Sheng Li. 2005. Auto-
matic entity relation extraction. Journal of Chinese
Information Processing, 19(2):1?6.
Yu Chen, Wenjie Li, Yan Liu, Dequan Zheng, and
Tiejun Zhao. 2010. Exploring deep belief network
for chinese relation extraction. In Proceedings of
CLP ?10, pages 28?29.
Mark Craven and Johan Kumlien. 1999. Constructing
biological knowledge bases by extracting informa-
tion from text sources. In Proceedings of ISMB ?99,
pages 77?86.
Aron Culotta and Jeffrey Sorensen. 2004. Dependency
tree kernels for relation extraction. In Proceedings
of ACL ?04, page 423.
James R. Curran, Tara Murphy, and Bernhard Scholz.
2007. Minimising semantic drift with mutual ex-
clusion bootstrapping. In Proceedings of PACLING
?07, pages 172?180.
Liu Dandan, Hu Yanan, and Qian Longhua. 2012.
Exploiting lexical semantic resource for tree kernel-
based chinese relation extraction. Natural Language
Processing and Chinese Computing, pages 213?224.
Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying relations for open information
extraction. In Proceedings of EMNLP ?11, pages
1535?1545.
Ralph Grishman. 2012. Information extraction: Capa-
bilities and challenges. Notes prepared for the.
Zhou GuoDong, Su Jian, Zhang Jie, and Zhang Min.
2005. Exploring various knowledge in relation ex-
traction. In Proceedings of ACL ?05, pages 427?434.
Raphael Hoffmann, Congle Zhang, and Daniel S.
Weld. 2010. Learning 5000 relational extractors.
In Proceedings of ACL ?10, volume 10, pages 286?
295.
Rumjahn Hoosain. 1992. Psychological reality of the
word in chinese. Advances in psychology, 90:111?
130.
He Hu and Xiaoyong Du. 2012. Radical features for
chinese text classification. In Proceedings of FSKD
?12, pages 720?724.
Changning Huang and Hai Zhao. 2007. Chinese word
segmentation : A decade review. Journal of Chinese
Information Processing, 21(3):8?19.
Ruihong Huang, Le Sun, and Yuanyong Feng. 2008.
Study of kernel-based methods for chinese relation
extraction. Information Retrieval Technology, pages
598?604.
Nanda Kambhatla. 2004. Combining lexical, syntac-
tic, and semantic features with maximum entropy
models for extracting relations. In Proceedings of
ACL-demo ?04, page 22.
Zhang Le. 2004. Maximum entropy modeling toolkit
for python and c++. Natural Language Processing
Lab, Northeastern University, China.
Wenjie Li, Peng Zhang, Furu Wei, Yuexian Hou, and
Qin Lu. 2008. A novel feature-based approach to
chinese entity relation extraction. In Proceedings of
HLT-Short ?08, pages 89?92.
Nanyuan Liang. 1984. Written chinese word segmen-
tation system-cdws. Journal of Beijing Institute of
Aeronautics and Astronautics, 4.
Ruqi Lin, Jinxiu Chen, Xiaofang Yang, and Honglei
Xu. 2010. Research on mixed model-based chinese
relation extraction. In Proceedings of ICCSIT ?10,
volume 1, pages 687?691.
Andrew McCallum. 2005. Information extraction:
Distilling structured data from unstructured text.
Queue, 3(9):48?57.
580
Scott Miller, Michael Crystal, Heidi Fox, Lance
Ramshaw, Richard Schwartz, Rebecca Stone, and
Ralph Weischedel. 1998. Algorithms that learn to
extract information: Bbn: Tipster phase iii. In Pro-
ceedings of TIPSTER ?98, pages 75?89.
Mike Mintz, Steven Bills, Rion Snow, and Dan Juraf-
sky. 2009. Distant supervision for relation extrac-
tion without labeled data. In Proceedings of ACL
?09, pages 1003?1011.
Thahir P Mohamed, Estevam R Hruschka Jr., and
Tom M Mitchell. 2011. Discovering relations be-
tween noun categories. In Proceedings of EMNLP
?11, pages 1447?1455.
Fuchun Peng, Fangfang Feng, and Andrew McCallum.
2004. Chinese segmentation and new word detec-
tion using conditional random fields. In Proceedings
of COLING ?04.
Dan Roth and Wen-tau Yih. 2002. Probabilistic rea-
soning for entity & relation recognition. In Proceed-
ings of COLING ?02, pages 1?7.
Dan Roth and Wen-tau Yih. 2007. Global inference
for entity and relation identification via a linear pro-
gramming formulation. Introduction to Statistical
Relational Learning, pages 553?580.
Richard Sproat, William Gale, Chilin Shih, and Nancy
Chang. 1996. A stochastic finite-state word-
segmentation algorithm for chinese. Computational
linguistics, 22(3):377?404.
Shingo Takamatsu, Issei Sato, and Hiroshi Nakagawa.
2012. Reducing wrong labels in distant supervision
for relation extraction. In Proceedings of ACL ?12,
pages 721?729.
Fei Wu and Daniel S. Weld. 2007. Autonomously se-
mantifying wikipedia. In Proceedings of CIKM ?07,
pages 41?50.
Nianwen Xue. 2003. Chinese word segmentation as
character tagging. Computational Linguistics and
Chinese Language Processing, 8(1):29?48.
Ming Yan, Reinhold Kliegl, Eike Richter, Antje Nuth-
mann, and Hua Shu. 2010. Flexible saccade-target
selection in chinese reading. The Quarterly Journal
of Experimental Psychology, 63(4):705?725.
Dmitry Zelenko, Chinatsu Aone, and Anthony
Richardella. 2003. Kernel methods for relation
extraction. The Journal of Machine Learning Re-
search, 3:1083?1106.
Hua-Ping Zhang, Hong-Kui Yu, De-Yi Xiong, and Qun
Liu. 2003. Hhmm-based chinese lexical analyzer
ictclas. In Proceedings of SIGHAN ?03, pages 184?
187.
Min Zhang, Jie Zhang, and Jian Su. 2006a. Exploring
syntactic features for relation extraction using a con-
volution tree kernel. In Proceedings of HLT-NAACL
?06, pages 288?295.
Min Zhang, Jie Zhang, Jian Su, and Guodong Zhou.
2006b. A composite kernel to extract relations be-
tween entities with both flat and structured features.
In Proceedings of ACL ?06, pages 825?832.
Peng Zhang, Wenjie Li, Furu Wei, Qin Lu, and Yuexian
Hou. 2008. Exploiting the role of position feature in
chinese relation extraction. In Proceedings of LREC
?08.
Peng Zhang, Wenjie Li, Yuexian Hou, and Dawei Song.
2011. Developing position structure-based frame-
work for chinese entity relation extraction. ACM
Transactions on Asian Language Information Pro-
cessing (TALIP), 10(3):14.
Shubin Zhao and Ralph Grishman. 2005. Extracting
relations with integrated information using kernel
methods. In Proceedings of ACL ?05, pages 419?
426.
Ming Zhong, Sheng Wang, and Ming Wu. 2012. Re-
vising word lattice using support vector machine for
chinese word segmentation. In Proceedings of II-
WAS ?12, pages 352?355.
Guodong Zhou, Longhua Qian, and Jianxi Fan. 2010.
Tree kernel-based semantic relation extraction with
rich syntactic and semantic information. Informa-
tion Sciences, 180(8):1313?1325.
581
