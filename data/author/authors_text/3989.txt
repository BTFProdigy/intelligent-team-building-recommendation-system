Flexible Mixed-Initiative Dialogue Management using 
Concept-Level Confidence Measures of Speech Recognizer Output 
Kazunor i  Korea |an |  and Tatsuya  Kawahara  
Graduat(~ School of lnt'ormati(:s, I{yoto University 
Kyoto 606-8501, JaI)an 
{kolnatani, kawahara} (@kuis.k.yoto-u. ac.j i) 
Abst rac t  
We i)rcsent a method to r(:aliz(: th:xil)le mix(;(l- 
initiative dialogue, in which the syst(:m can 
mak(, etti:ctive COlflirmation mad guidmn(:(: us- 
ing (-oncel)t-leve,1 confidcn('e mcmsur(,s (CMs) 
derived from st)eech recognizer output in ord(:r 
to handl(: sl)eech recognition errors. W(: d(:tine 
two con('et)t-level CMs, which are oil COllt(~,Ilt - 
words and on semantic-attrilmtes, u ing 10-best 
outtmts of the Sl)e(:ch r(:cognizt:r and l)arsing 
with t)hrmse-level grammars. Content-word CM 
is useflll for s(:lecting 1)\]ausible int(:rl)retati(ms. 
Less contid(:nt illt(:rl)r(:tmtions arc given to con- 
firmation 1)roc(:ss. The strat(:gy iml)roved the 
interpr(:tmtion accuracy l)y 11.5(/0. Moreover, 
th(: semanti(:-mttrilmt(: CM ix us(:d to (:stimmtc 
user's intention and generates syst(mi-initiative 
guidances (:v(,,n wh(:n suc(-(:sstSfl int(:rl)r(:tmtiol~ is 
not o|)tain(:(1. 
1 I n t roduct ion  
In a st)oken dialogu(: system, it fr(:(tuently o(:- 
cm:s that the system incorrectly rccogniz(:s user 
utterances and the user makes exl)ressions the 
system has not (~xt)ccted. These prot)lcms arc 
essentially incvital)le in handling the natural 
language 1)y comlmters , even if vocal)ulary and 
grammar of the system are |~lmed. This lack of 
robustness i one of the reason why spoken dia- 
logue systems have not been widely deployed. 
In order to realize a rol)ust st)oken dialogue 
system, it is inevital)le to handle speech recog- 
nition errors. To sut)t)ress recognition errors, 
system-initiative dialogue is eitbctive. But it 
ca.n 1)e adopted only in a simi)le task. For in- 
stance, the form-tilling task can 1)e realized 1)y a 
simi)le strategy where the system asks a user the 
slut wdues in a fixed order. In such a systeln- 
initiated intera('tion, the recognizer easily nar- 
rows down the vocabulary of the next user's ut- 
tcrance, thus the recognition gets easier. 
()n the other hand, in more eoniplicat('A task 
such ms inforination rctriewd, the vocml)ulmry of 
the llCXI; lltt(2rauco callllot 1)e limited on all oc- 
casions, because the user should be abh~ to in- 
put the values in various orders based on his 
i)rel'erence. Therefore, without imposing a rigid 
teml)late ut)on the user, the system must behav(~ 
at)t)rol)riately even when sl)ecch recognizer out- 
1)ut contains ome errors. 
Obviously, making confirmal;ion is efl'cctive 
to mvoid misun(lerstandings caused by slme(:h 
recognition errors, ttowcver, when contirmm- 
tions are made \]'or every utterance, |;lie di- 
~dogue will l)ccome too redundant mad con- 
sequcntly |;rout)lcsomc, for users. Previous 
works have, shown that confirmation strategy 
shouM 1)c decided according to the frequency of 
stretch recognition errors, using mathematicml 
formula (Niimi and Kolmymshi, 1.996) and using 
comt)uter-to-comlml;er silnulation (W~tanabe et 
al., 1!)98). These works assume tixe(t l)erfof 
mance (averaged speech recognition accuracy) 
in whole (lialogue with any speakers. For flex- 
ible dialogue management, howeve, r the confir- 
mation strategy luust 1)e dynamically changc, d 
bmsed on the individual utterances. For in- 
stmncc, we human make contirmation only when 
we arc not coat|dent. Similarly, confidence, inca- 
sures (CMs) of every speech recognition output 
should be modeled as a criterion to control dia- 
logue management. 
CMs have been calculated in previous works 
using transcripts and various knowledge sources 
(Litman et al, 1999) (Pao et, al., 1998). For 
more tlexible interaction, it, ix desirable that 
CMs are detined on each word rather than whole 
sentence, because the systeln can handle only 
unreliable portions of an utterance instead of 
accepting/rejecting whole sentence. 
467 
In this paper, we propose two concept-level 
CMs that are on content-word level and on 
semantic-attribute level for every content word. 
Because the CMs are defined using only speech 
recognizer output, they can be computed in real 
time. The system can make efficient confir- 
mation and effective guidance according to the 
CMs. Even when successful interpretation is 
not obtained o51 content-word level, the system 
generates ystem-initiative guidances based on 
the semantic-attribute level, which lead the next 
user's utterance to successful interpretation. 
2 Def in i t ion  o f  Conf idence  Measures  
(CMs) 
Confidence Measures (CMs) have been studied 
for utterance verification that verifies speech 
recognition result as a post-processing (Kawa- 
hara et al, 1998). Since an automatic speech 
recognition is a process finding a sentence hy- 
pothesis with the maximum likelihood for an 
input speech, some measures are needed in or- 
der to distinguish a correct recognition result 
from incorrect one. In this section, we de- 
scribe definition of two level CMs which are on 
content-words and on semantic-attritmtes, us- 
ing 10-best output of the speech recognizer and 
parsing with phrase-level grammars. 
2.1 Def init ion of  CM for Content Word 
In the speech recognition process, both acoustic 
probability and linguistic t)robability of words 
are multiplied (summed up in log-scale) over 
a sentence, and the sequence having maximum 
likelihood is obtained by a search algorithm. A 
score of sentence derived from the speech rec- 
ognizer is log-scaled likelihood of a hypothesis 
sequence. We use a grammar-based speech rec- 
ognizer Julian (Lee et al, 1999), which was de- 
veloped in our laboratory. It correctly obtains 
the N-best candidates and their scores by using 
A* search algorithm. 
Using the scores of these N-best candidates, 
we calculate content-word CMs as below. The 
content words are extracted by parsing with 
phrase-level grammars that are used in speech 
recognition process. In this paper, we set N = 
10 after we examined various values of N as the 
nmnber of computed candidates J 
1Even if we set N larger tt,an 10, the scores of i-th 
hypotheses (i > 10) are too small to affect resulting CMs. 
First, each i-th score is multiplied by a factor 
a (a  < 1). This factor smoothes tile difference 
of N-best scores to get adequately distributed 
CMs. Because the distribution of the abso- 
lute values is different among kinds of statisti- 
cal acoustic model (monophone, triphone, and 
so oi1), different values must be used. The value 
of c~ is examined in the preliminary experiment. 
In this paper, we set c~ = 0.05 when using tri- 
phone model as acoustic model. Next, they are 
transtbrnmd from log-scaled value (<t. scaledi) 
to probability dimension by taking its exponen- 
tial, and calculate a posteriori probability tbr 
each i-th candidate (Bouwman et al, 1999). 
e~.scaledi 
Pi = ~n Co~.scaledj j=l 
This Pi represents a posteriori probability of the 
i-th sentence hypothesis. 
Then, we compute a posteriori probability tbr 
a word. If the i-th sentence contains a word w, 
let 5w,i = 1, and 0 otherwise. A posteriori prob- 
ability that a word w is contained (Pw) is de- 
rived as summation of a posteriori prob~bilities 
of sentences that contain the word. 
/L 
Pw = ~ Pi " 5w,i 
i=1  
We define this Pw as the content-word CM 
(CM,,). This CM.,, is calculated tbr every con- 
tent word. Intuitively, words that appear many 
times in N-best hypotheses get high CMs, and 
frequently substituted ones in N-best hypothe- 
ses are judged as mn'eliable. 
In Figure 1, we show an example in CMw 
calculation with recognizer outputs (i-th recog- 
nized candidates and their a posteriori proba- 
bilities) tbr an utterance "Futaishisctsu ni rcsu- 
toran no aru yado (Tell me hotels with restau- 
rant facility.)". It can be observed that a correct 
content word 'restaurant as facility' gets a high 
CM value (CMw = 1). The others, which are 
incorrectly recognized, get low CMs, and shall 
be rejected. 
2.2 CM for Semant ic  At t r ibute  
A concept category is semantic attribute as- 
signed to content words, and it is identified 
by parsing with phrase-level gramnmrs that are 
used in speech recognition process and repre- 
sented with Finite State Automata (FSA). Since 
468 
1 
2 
3 
4 
5 
6 
7 
8 
9 
10 
Recognition candidates 
aa  sh isetsu  ni  resutmnu, no kayacho 
with restaurant facility / Kayacho(location) 
aa  sh isetsu  ni rcsuto7nn no katsurn no 
with restaurant fimility / Katsura(location) 
aa  sh isctsu  ni  resutoran no kamigamo 
with restaurant facility / Kmnigamo(location) 
<g> sh isc tsu  ni  
with restaurant 
<g> sh isetsu  ni  
with restaurant 
<g> sh isetsu  ni 
resutoran no kayacho 
facility / Kayacho(location) 
rcsutor'a~t 7to kat.~'~tra 
facility / Katsura(location) 
7"cs?ttoritTt 7~,o kamigamo 
with restaurant facility / I(amigamo(location) 
aa  sh, i setsu  ni  resutoran no kafc 
with restaurant fimility / care(facility) 
<g> sh isetsu  ni resutoran no kafe 
with restaurant facility / cafc(facility) 
<g> setsub i  wo rcsutoran no kayacho 
with restaurant facility / I(ayacho(locatlon) 
<g> sctsub i  wo resutoran no katsura no 
with restaurant facility / Katsura(location) 
.24 
.24 
.20 
.08 
.08 
.06 
.05 
.02 
.01 
.01 
<g>: tiller model 
CM,,, 
\]. 
0.33 
0.33 
0.25 
0,07 
(content word) ~ (semantic attribute) 
restaurant @ fimility 
Kayacho @ location 
Katsura 0 location 
Kmnigmno ~ location 
care ~ facility 
Figure. 1: Example of content-word CM (CM,,,) 
these FSAs are, classified into (:on(:cl)t categories 
lmforehand, we can auton|atically derive the 
concept categories of words by parsing with 
these grammars. In our hotel query task, there 
are sevelt concept categories uch as qocation', 
'fi, cility' and so on. 
For this concept (:ategory, we also de- 
fine semantic-attritmtc CMs (CM~:) as tbllows. 
First, we (-ah:ulnte a t)osteriori probabilities of 
N-best sentences in the same. way of comtmt- 
ing content-word CM. If a concel)t c~tegory c is 
contained in the i-th sentence, let 5,,,i = 1, and 0 
otherwise. The t)robability that a concept cat- 
egory c is correct (Pc) is derived as below. 
Pc = E pi ' sc,i 
i=1 
We define this Pc as semantic-attribute CM 
(CM~). This CMc estimates which category the 
user refers to and is used to generate tt'ective 
guidances. 
HSel'~ S ut\[el'ancc ) 
v 
~__  speech recognizer 
(___ each content word ) g'N-be~t, candidatcs-~' j. 
/ cont{3n~wo|'d / ', 
CM 
r 
acccpt~ 
i 
semantic atl|ibutc / 
CM S 
fill \] 
semantic slots \[ guidance \] prompt o rcpht'asc 
Figure 2: Overview of OlU" strategy 
3 Mixed-initiative Dialogue Strategy 
using CMs 
There m:e a lot of systems that hawe a(lopted 
a mixed-initiative strategy (Sturm et al, 
1999)(Goddeau et a.l., 1996)(Bennacef e.t al., 
1996). It has several adwmtages. As the. sys- 
tems do not impose rigid system-initiated tem- 
plates, the user can input values he has in 
mind directly, thus the dialogue l)ecomes more 
natural. In conventional systems, the system- 
initiated utterances are considered only when 
semantic mnbiguity occurs. But in order to re- 
alize robust interaction, the system should make 
confirmations to remove recognition errors and 
generate guidances to lead next user's utterance 
to succcssflll interpretation. In this section, we 
describe how to generate the system-initiated 
utterances to deal with recognition errors. An 
overview of our strategy is shown in Figure 2. 
3.1 Making Ef fect ive Conf i rmat ions  
Confidence Measure (CM) is useflll in selecting 
reliable camlidates and controlling coniirnlation 
strategy. By setting two thresholds 01,02(01 > 
0~) on content-word CM (CM.,), we provide the 
confirmation strategy as tbllows. 
469 
? C-Mw > 0~ 
accept the hypothesis 
? Oj >_CM~>02 
-~ make confirmation to the user 
"Did you say ...?" 
? 02 >_ CM~,, 
--* reject the hypothesis 
The. threshold 01 is used to judge whether the 
hypothesis is accepted or should be confirmed, 
and tile threshold 02 is used to judge whether it 
is reiected. 
Because UMw is defined for every content 
word, judgment among acceptance, confirma- 
tion, or rejection is made for every content 
word when one utterance contains several con- 
tent words. Suppose in a single utterance, one 
word has CM,,,, between 0~ and 0~ and tile other 
has t)elow 02, the tbrlner is given to confirma- 
tion process, and tile latter is rejected. Only if 
all content words are rejected, the system will 
prompt the user to utter again. By accepting 
confident words and rejecting mlreliable candi- 
dates, this strategy avoids redundant confirma- 
tions and tbcuses on necessary confirmation. 
We optinfize these thresholds 0t, 02 consider- 
tug the false, acceptance (FA) and the false re- 
jection (FR) using real data. 
Moreover, the system should confirm using 
task-level knowledge. It is not usual that users 
change the already specified slot; values. Thus, 
recognition results that overwrite filled slots are 
likely to be errors, even though its CM~, is high. 
By making confirmations ill such a situation, it 
is expected that false acceptance (FA) is sup- 
pressed. 
3.2  Generat ing  System- In l t ia ted  
Gu idanees  
It is necessary to guide tile users to recover ti'om 
recognition errors. Especially for novice users, 
it is often eflbctive to instruct acceptable slots 
of the system. It will be helpful that tile system 
generates a guidance about the acceptable slots 
when the user is silent without carrying out tile 
dialogue. 
The system-initiated guidances are also effec- 
tive when recognition does not go well. Even 
when any successflfl output of content words is 
not obtained, the system cast generate ffective 
guidances based on the semantic attribute with 
f 
utterance: 
correct: 
"shozai  ga oosakaflt no yado" 
(hotels located in Osaka pref.) 
Osaka-pref?location 
i 
1 
2 
3 
4 
5 
6 
7 
8 
9 
10 
recognition candi(tat, es(<g>: filler model) 
dtozai ga potoairando no <g> 
located in Port-island 
shozai ga potoairando no <g> 
located in Port-island 
shozai ga oosakafu no <g> 
located in Osaka-pref. 
shozai ga oosakafu no <g> 
located in Osaka-pref. 
shozai ga oosa\]cashi no <g> 
located in Osaka-city 
shozai ga oosakashi no <g> 
located in Osaka-city 
shozai ga ohazaki no <g> 
located in Okazaki 
shozai ga otcazaki no <g> 
located in Okazaki 
shozai ga oohara no <g> 
located in Ohara 
shozai ga oohara no <g> 
located in Ohara 
C2~tc semantic attributes 
1 location 
CMw 
0.38 
0.30 
0.13 
0.11 
0.08 
content words 
Port-islandelocation 
Osaka-pref.~location 
Osaka-city,location 
Okazakielocation 
Ohara01ocation 
Figure 3: Example of high semantic attribute 
confidence in spite of low word confidence 
high confidence. An example is shown in Fig- 
ure 3. In this example, all the 10-best candi- 
dates are concerning a name of place but their 
CMw values are lower than the threshold (02). 
As a result, any word will be neither accepted 
nor confirmed. In this case, rather than re- 
jecting the whole sentence and telling the user 
"Please say again", it; is better to guide the user 
based oll the attr ibute having high CM,. ,  such 
as "Which city is your destination?". This guid- 
ance enables tile system to narrow down the 
vocabulary of the next user's utterance and to 
reduce the recognition difficulty. It will conse- 
quently lead next user's utterance to successful 
interpretation. 
When recognition on a content word does not 
470 
go well repeatedly in spite of high semanti(:- 
attr ibute CM, it is reasoned that the content 
word may be out-ofvocalmlary, in such a case, 
the systmn shouht change the que.stion. For 
example, if an uttermme coal;alas all out-of  
vocat)ulary word and its semantic-attribute is 
inibrred as "location", the system can make 
guidance, "Please st)eci(y with the name of t)re- 
fecture", which will lead the next user's utter- 
ance into the system's vocabulary. 
4 Exper imenta l  Eva luat ion  
4.1 Task and Data  
We evaluate our nmthod on the hotel query 
task. We colh;cted 120 mimll;es speech data by 
24 novice users l)y using the 1)rototylm system 
with GUI (Figure 4) (Kawahara et al, 1999). 
The users were given siml)le instruction before- 
hand oll the system's task, retriewfi)le il;(nns, 
how to cancel intmt values, and so o11. The data 
is segmented into 705 utterances, with a t)ause 
of 1.25 seconds. The voeal)ulary of I;he system 
contains 982 words, and the aural)or of database 
records is 2040. 
()tit of 705 lltterailces, \]24 llttelTallces (1.7.6%) 
are beyond the system's eal)al)ility , namely they 
are out-ofvocalmlary, ou|;-ofgrmnmar~ out-of  
task, or fragment of llttel'allC(L \]i1 tbllowing ex- 
1)erim(mt;s, we cvahmte th(', sys|;t',ln \])erl))rm~nce 
using all (lath including these mm,c(:el)tnt)le ut- 
terances in order to evahlalt;e how the system 
can reject unexl)ected utterances at)t)rot)riately 
as well as recognize hernial utterances correctly. 
4.2 Thresho lds  to Make Conf i rmat ions 
In section 3.1, we t)resented confirmation strat- 
egy 1)y setting two thresholds 01,02 (01 > 02) for 
eolfl, enl;-word CM (CMw). We optinlize these 
threshoht wflues using t;11(; collected data. \?e 
count ca:ors 11ol; by the utterance lint by the 
content-word (slot). The number of slots is 804. 
The threshold 01 decides t)etween accel)tanee 
and confirmation. The wdue of 0\] shouhl be 
determined considering both the ratio of ineof  
rectly accepting recognition errors (False At-- 
ceptance; FA) and the ratio of slots that are 
not filh;d with correct wfiues (Slot; Error; SErr). 
Namely, FA and SErr are defined an the (:(mq)le- 
meats of t)recision and recall rate of the outl)ltt , 
respectively. 
FA = ~ el' incorrectly accepted words 
of accepted words 
fl~ of correet;ty aecel)ted words 
SE'rr = I - 
of all correct words 
After experimental optimization to minimize 
FA+SErr, we derive a wflue of 0i as 0.9. 
Similarly, the threshold 02 decides contirlna- 
tion and rejection. The value of 02 should be 
decided considering both the ratio of incorrectly 
rqjeeting content words (False Rejection; FR) 
and the ratio of aceel)ting recognition errors into 
the eonfirlnation 1)recess (conditional False At:- 
eel)tahoe; cFA). 
fl: of incorrectly re.jetted words 
~- of all rejected words 
If we set the threshohl 02 lower, FR de- 
creases and correspondingly cFA increases, 
which means that more candidates are ol)tained 
but more eontirmations are needed. By mini- 
m izing \]q/.+cFA, we deriw; a value of 02 as 0.6. 
4.3 Compar i son  w i th  Convent iona l  
Methods  
Ill many conventional st)oken di~dogue syst;ems, 
only 1-best candidate of a speech recognizer 
outt)ut is used in the subsequent processing. 
\?e (:Oral)are ore' method with a conventional 
method that uses only 1-best ean(lidate in in- 
terpretation ae(:uraey. 'l.'he result is shown in 
%rifle 1. 
1111 the qlo eonfirnlation' strategy, the hy- 
pothes(,s are classified by a single threshohl (0) 
into either the accepted or the rejected. Namely, 
(:ontent words having CM,,, over threshohl 0 are 
aecet)ted, mM otherwise siml)ly r(\[iected. In this 
case, a 1;hreshold wflue of 0 is set to 0.9 that 
gives miniature FA-FSErr. 111 the 'with con- 
firmation' strategy~ the proposed (:oniirmation 
strategy is adol)ted using ()1 and 02. We set 
01 = 0.9 and 02 = 0.6. The qTA+SErr' in Ta- 
ble 1 means FA(0~)+SErr(02), on the assump- 
tion that the contirnmd l)hrases are correctly ei- 
ther accel)ted or rejected. -We regard this as- 
smnt)tion as at)l)rol)riate, because users tend to 
answer ~ye, s' simply to express their affirmation 
(Hockey et al, 1997), so the sys|;em can dis- 
tinguish affirmative answer and negative olle by 
grasping simple 'yes' utterances correctly. 
471 
i~ ........................ % ............. II III I ~t 
(a) A real system in Japanese 
Hote l  Accommodat ion  Search  
hotel type is I Japanese-style I 
location is I downtown Kyoto \] 
room rate is less than I 10,000 I yen 
These are query results ? 
(b) Upper I)ortion translated in English 
Figure 4: An outlook of GUI (Graphical User Interface.) 
Table 1: Comparison of methods 
FA+SErr FA SErf 
only 1st candidate 51.5 27.6 23.9 
no confirmation 46.1 14.8 31.3 
with confirmation 40.0 14.8 25.2 
FA: ratio of incorrectly accepting recognition errors 
SErr: ratio of slots that are not filled with correct values 
As shown in Table 1, interpretation ~,c('u- 
racy is improved by 5.4% in the 'no confirma- 
tion' strategy compared with the conwmtional 
method. And 'with confirmation' strategy, we 
achieve 11.5% improvement in total. This result 
proves that our method successflflly eliminates 
recognition errors. 
By making confirmation, the interaction be- 
comes robust, but accordingly the number of 
whole utterances increases. If all candidates 
having CM, o under 01 are given to confirma- 
tion process without setting 0u, 332 wdn con- 
firmation for incorrect contents are generated 
out of 400 candidates. By setting 02,102 candi- 
dates having CMw between 01 and 02 are con- 
firmed, and the number of incorrect confirma- 
tions is suppressed to 53. Namely, the ratio 
of correct hypotheses and incorrect ones being 
confirmed are ahnost equM. This result shows 
indistinct candidates are given to confirmation 
process whereas scarcely confident candidates 
are rejected. 
content-word CM and semantic-attribute CM 100 
FA+SErr(content word) 
FA+SErr(semantic attribute) - - - - -  
8O 
6O 
+ 
m< 40 
"N ,_~.~.~.~ . . . . . . . . . . . . . . . . . . . . .  ~ ' / /1  
21 , _\] 
0 0.2 0.4 0.6 0.8 1 
threshold 
Figure 5: Pertbrm~mce of the two CMs 
4.4 Effect iveness of Semant ic -At t r ibute  
CM 
In Figure 5, the relationship between content- 
word CM and semantic-attribute CM is shown. 
It is observed that semantic-attribute CMs are 
estimated more correctly than content-word 
CMs. Therefore, even when successful interpre- 
tation is not obtained fl'om content-word CMs, 
semantic-attribute can be estimated correctly. 
In experimental data, there are 148 slots 2 
that are rejected by content-word CMs. It is 
also observed that 52% of semantic-attributes 
2Out-of-vocabulary and out-of-grammar utterances 
are included in their phrases. 
472 
with CA4c over 0.9 is correct. Such slots amomit 
to 34. Namely, our system can generate tt.'cc- 
rive guidances against 23% (34/148) of utter- 
antes that had been only rejected in conven- 
tional methods. 
5 Conc lus ion  
We present dialogue mallagement using two 
concel)t-level CMs in order to realize rolmst ill- 
teractioll. The content-word CM provides a 
criterion to decide whether an interpretation 
should be accel)ted, confirmed, or rejected. This 
strategy is realized by setting two thresholds 
that are optimized balancing false acceptance 
and false rejection. The interpretation error 
(FA+SErr) is reduced by 5.4% with no confir- 
mation and by \] 1.5% with confirmations. More- 
over, we &',line CM on semantic attribut(~s, and 
propose a new met;hod to generate ilbx:tive 
guidances. The concept-t)ased (:onfidence mea- 
sure realizes tlexible mixed-initiative dialogue in 
which the system can make effective contirma- 
tion and guidance by estimating user's inten- 
tion. 
Re ferences  
S. \]~(mnacef, L. Devillers: S. Rosset, and 
L. Lamel. 1996. Dialog in the I/.AIIfl?EL 
telet)hone-1)as(~(t system. In Pwc. \]nt'l Con.fi 
on ,5'pokc'n, Language l)Tvcc.ssi'n.g. 
G. Bouwman, ,1. Sturm, and L. Boves. 1999. In- 
cort)orating contidcnce measures in the. Dutch 
train timetable information system developed 
in the ARISE t)roject. In P'lvc. ICASSP. 
D. God(lean, H. Meng, J. Polifroni, S. Seneff, 
and S. Busayapongchai. 1996. A form-based 
diah)gue manager for spoken language al)pli- 
cations. In P~vc. lnt'l Co7@ on Spoken Lan- 
guage \])rocessing. 
B. A. Hockey, D. l:l,ossen-Knill, B. Spejew- 
ski, M. Stone, and S. Isard. 1997. Can 
you predict resl)onses to yes/no questions? 
yes,no,and stuff. In Proc. EUIl, OSPEECI\]'97. 
T. Kawahara, C.-H. Lee, ~md B.-H. Juang. 
1998. Flexible speech understanding based 
on confl)ined key-t)hrase detection and veri- 
fication. IEEE TTnns. on Speech and Audio 
Processing, 6 (6):558-568. 
T. Kawahara, K. q_?maka, nd S. Doshita. 1999. 
Domain-independent t)latform of spoken di- 
alogue interfaces for information query, in 
Proc. ESCA workshop on Interactive Dia- 
loguc in Multi-Modal Systems, pages 69 72. 
A. Lee, T. Kawahara, and S. Doshita. 1999. 
Large. vocabulary continuous speech recogni- 
tion 1)arser based on A* search using gram- 
mar category category-pair constraint (in 
,lapancsc). Trans. h~:formation Processing 
Society of ,lapau,, 40(4):1374-1382. 
D. J. Litman, M. A. Walker, and M. S. Kearns. 
1999. Automatic detection of 1)oor speech 
recognition at the dialogue level. In Pwc. of 
37th Annual Meeting o.f the A CL. 
Y. Niimi and Y. Kobayashi. 1996. A dialog con- 
trol strategy based on the reliability of st)eech 
recognition. In Proc. Int'l Cml:\[. on ,5'pokcn 
Language Processing. 
C. Pao, P. S('hmid, and J. Glass. 1998. Con- 
tidence scoring tbr st)eech mlderstnnding sys- 
tems. In P~v(:. \]'nt'l Conf. on S"poken Lan- 
guage P~vcessing. 
.}. Sturm, E. Os, and L. Boves. 1999. Issues in 
spoken dialogue systems: Experiences with 
the Dutch ARISE system. In Pwc. of ESCA 
IDS'99 Workshop. 
T. Watanal)e, M. Ar~ki, and S. Doshitm 1998. 
Evaluating diah)gue strategies un(lex colnnm- 
nication errors using COlntmter-to-comlmter 
simulation. 7}'an.s. of IEICE, he:fo g Syst., 
E81-D(9):I025 1033. 
473 
Ecient Dialogue Strategy to Find Users Intended Items
from Information Query Results
Kazunori Komatani Tatsuya Kawahara Ryosuke Ito Hiroshi G Okuno
Graduate School of Informatics Kyoto University
Kyoto  Japan
fkomatani kawahara rito okunogkuis	kyotou	ac	jp
Abstract
We address a dialogue framework that narrows
down the users query results obtained by an in
formation retrieval system The followup dia
logue to constrain query results is signicant es
pecially with the speech interfaces such as tele
phones because a lot of query results cannot be
presented to the user The proposed dialogue
framework generates guiding questions based on
an information theoretic criterion to eliminate
retrieved candidates by a spontaneous query
without assuming a semantic slot structure We
rst describe its concept on general information
query tasks and then deal with a query task
on the appliance manual where structured task
knowledge is available A hierarchical conrma
tion strategy is proposed by making use of a tree
structure of the manual and then three cost
functions for selecting optimal question nodes
are compared Experimental evaluation demon
strates that the proposed system helps users
nd their intended items more eciently
 Introduction
In the past years a great number of spoken
dialogue systems have been developed Their
typical task domains include airline information
Levin et al 			
 Potamianos et al 			

SanSegundo et al 			 and train information
Allen et al 
 Bennacef et al 
 Sturm
et al 
 Lamel et al  Most of them
model speech understanding process as convert
ing recognition results into semantic representa
tions equivalent to database query SQL com
mands and dialogue process as disambiguating
their unxed slots Usually the semantic slots
are dened a priori and manually The approach
is workable only when data structure of the ap
plication is wellorganized typically as a rela
tional database RDB
Dierent and more exible approach is
needed for spoken dialogue interfaces to ac
cess information described in less rigid format
in particular normal text database For the
purpose information retrieval IR technique is
useful to nd a list of matching documents from
the input query Typically keywords are ex
tracted from the query and statistical matching
is performed Call routing task ChuCarroll
and Carpenter  can be regarded as the
special case
In IR systems many candidates are usually
obtained as a query result thus there is a sig
nicant problem of how to nd the users in
tended item among them Especially either on
the telephone or electrical appliances there is
not a large screen displaying the candidates and
all the query results cannot be presented to a
user So it is desirable for the system to narrow
down the query results interactively Moreover
interactive query is more friendly to novice users
rather than requiring them to input a detailed
query from the beginning
In this paper we address a dialogue strat
egy to nd the users intended item from the
retrieved result which is initiated by a spon
taneous query utterance In section  we de
scribe a method to generate a guiding question
that narrows down the query results eciently
using an example of a restaurant query task
The question is selected based on an informa
tion theoretic criterion In section  we present
a dialogue management method for a query task
on the appliance manual where structured task
knowledge is available We propose a conr
mation strategy by making use of a tree struc
ture of the manual and dene three cost func
tions for selecting question nodes The method
is evaluated by the number of average dialogue
turns
Although there are previous studies on
optimizing dialogue strategies Niimi and
Kobayashi 
 Levin et al 
 Litman
et al 			 most of them assume the tasks
of lling semantic slots that are denitely and
manually dened and few focus on followup
dialogue of information retrieval For example
Denecke  proposed a method to generate
guiding questions by making use of a tree struc
ture constructed by unifying retrieved items
based on semantic slots In this paper we do
not assume any structure of semantic slots In
stead we make use of distribution of document
statistics or a structure of task knowledge We
also investigate cost functions for optimal dia
logue control by taking into account of speech
recognition errors
 Dialogue Strategy in General
Information Query Task
Interaction in an information query task can be
regarded as a process seeking a common part
between the users request and system knowl
edge In order to help users to nd their in
tended items from the system knowledge the
system has to carry out not only interpreting
what users say but also showing the relevant
portion of the system knowledge to them
We assume that users freely set and retract
query keys based on their preference for infor
mation query systems If many candidates still
remain even after specifying all possible hisher
preference to the system users may have di
culty in narrowing down further the query re
sult Thus the system should generate ecient
guiding questions to help users nd their in
tended items
In this section we presume the system knowl
edge as a pair of an item and a set of keywords
Figure  We dene keywords as a set of
words representing contents of the items and
their categories such as place food and so on
are given This is similar to indexing words in
a conventional information retrieval task Note
that it is not needed that the system knowledge
is structured like an RDB
Keywords are extracted from a users utter
ance and are matched with the system knowl
edge Here we adopt the following matching
 
Restaurant A
Chinese noodles meat dumpling
Shinjuku Kabukicho Ekoda
Restaurant B
Chinese noodles meat dumpling
Shinjuku Kabukicho
Restaurant C
Chinese noodles meat dumpling
noodles with boiledporkribs
Takadanobaba
Restaurant D
Chinese noodles fried garlic Yebisu

 
Figure  An example of system knowledge
function for each item j
L
j

X
iK
j

CM
i
 log
N
df
i

Here K
j
is a set of keywords for item j CM
i
is
a condence measure of speech recognition for
keyword i Komatani and Kawahara 			 N
is the total number of items and df
i
is the num
ber of items including keyword i Intuitively
keyword that is recognized with high condence
and does not appear in many items gets higher
likelihood L
j
by CM
i
and df
i
 respectively
Then we dene amount of information that
is obtained when the system generates yesno
question and the user answers it Here C is a
current query condition A is a condition that
is added by the systems question and countx
is the number of items that satisfy the condi
tion x The condition consists of the conjunc
tion of the keywords the user specied Suppose
each item occurs by equal likelihood the fol
lowing equation denotes the likelihood p

A
yes

that the yesno question corresponding to the
adding condition A will be answered as yes
p

A
yes
 
countC A
countC
We weight on each item j with the likelihood
L
j

pA
yes
 
P
jfCAg
L
j
P
jfCg
L
j
The amount of information that is obtained
when the users answer is yes is represented
as follows
IA
yes
  log


pA
yes

The following equation gives HA the ex
pected value of amount of information that is
obtained by generating a question about con
dition A and getting users answer yes or
no
HA 
X
xfyesnog
pA
x
 log


pA
x

By calculating HA for all conditions A that
can be added to the current query condition
the system generates the question that has the
maximum value of HA The question is gen
erated using the category information of each
keyword
Because the obtained condition A is selected
by a viewpoint of narrowing down the current
set of items eciently the selected condition
may be unimportant for the user In such a case
it is not cooperative to force the user an ar
mative or negative reply Our system does not
force the reluctant decision by allowing the user
to say It does not matter anyhow Instead
the system presents the second best proposal
We explain the method with the following ex
ample in our restaurant query system in the
Tokyo area When a user says Please tell me a
restaurant where I can eat Chinese noodle and
meat dumpling in Shinjuku area three key
words are extracted Shinjuku Chinese noo
dle and meat dumpling As a result of the
matching using these three keywords  query
results are obtained It is not cooperative to
read out all of the  query results with a TTS
texttospeech system Here the expected val
ues of amount of information HA are calcu
lated for each condition that corresponds to key
words included in the matched items except for
the three keywords Shinjuku Chinese noo
dle and meat dumpling Then we select the
keyword noodles with boiledporkribs that
has the maximum value HA By generating a
question like Would you like one which serves
noodles with boiledporkribs and obtaining
a reply from the user the system adds the new
condition and narrows down the candidates ef
ciently If the user thinks that the condition
noodles with boiledporkribs is not impor
tant and tells the system so for example Ei
ther will do the system can show the second
best proposal Would you like one located in
Kabukicho area Thus the query result can
be narrowed down without forcing the user un
natural yesno answers
 Dialogue Strategy for Query on
Appliance Manuals
In this section we present another ecient solu
tion in the case that the structure or hierarchy
of task knowledge is available The task here
is to nd the appropriate item in the manual
of electric appliances with a spoken dialogue in
terface Such an interface will be useful as the
recent appliances become complex with many
features and so are their manuals In the ap
pliances such as VTR Video Tape Recorder
and FAX machines there is not a large screen
to display the list of matched candidates to be
selected by the user Therefore we address a
spoken dialogue strategy to determine the most
appropriate one from the list of candidates
An alternative system design is the use of di
rectory search as adopted in voice portal sys
tems where the documents are hierarchically
structured and the system prompts users to se
lect one of the menu from the top to the leaf
The method is rigid and not userfriendly since
users often have trouble in selection and want
to specify by their own expression The pro
posed system allows users to make queries spon
taneously and makes use of the directory struc
ture in the followup dialogue to determine the
most appropriate one
 System Overview
An overview of the system is illustrated in Fig
ure  It consists of following processes
 Keyword spotting from user utterances us
ing an ASR automatic speech recognition
system Kawahara et al 
A natural spoken language query is ac
cepted and keywords are extracted A con
dence measure CM
i
is assigned to each
keyword i based on the Nbest recognition
result Komatani and Kawahara 			
yes/no
system user
manual
tree
structure
entries
keyword spotting
matching
follow-up
dialogue
keywords with
confidence
entries with
likelihood
result
spoken query
Figure  System overview
 Matching with manual items documents
The extracted keywords are matched with
a set of manual items The matching is
performed on the initial portion index and
rst summary paragraph of each manual
section We adopt the following matching
score function for an item j K
j
is a set of
keywords for item j
L
j


n
j
X
iK
j
CM
i
 log
N
df
i

Here df
i
is the number of items that con
tain keyword i referred as a document fre
quency and N is the total number of items
The inverse document frequency idf is
weighted with a condence measure CM
i
and summed over keywords then normal
ized by n
j
 the number of keywords in the
item j
 Generating dialogue to determine the most
appropriate one from the list of candidates
As a result of the matching many candi
dates are usually found They may include
irrelevant ones because of speech recogni
tion errors But it is not practical to read
out all of them in order with a TTS text
tospeech system Therefore dialogue is
invoked to narrow down to the intended
one This dialogue is restricted to system
initiated yesno questions in order to
play record search setting
normal
play
slow
play
.............
Figure  Example of tree structure of manual
avoid further recognition errors and back
up dialogue The dialogue strategy is ex
plained in the next subsection
 Dialogue Strategy using Structure
of Manual
If one of the candidates is more plausible than
others with a signicant margin we should
make conrmation on it When there are many
candidates with similar condence and they
can be hierarchically grouped into several cate
gories we had better rst identify which cate
gory the intended one belongs to In this work
we make use of the section structure of the man
ual ie section is the rst layer subsection is
the secondlayer and so on The tree structure
is automatically derived from its table of con
tents An example for VTR manual is shown in
Figure 
For each node of the tree likelihood L

j
is
assigned as follows
 For a leaf node the matching score L
j
is
assigned after normalizing so that the sum
over all leaves manual items is 	
 For a nonleaf node the sum of the likeli
hood of its children nodes is assigned
Then a dialogue is generated as follows
 Among ancestor nodes of the leaf of the
largest likelihood L

j
 pick up the one whose
heuristic cost function described below is
smallest
 Make a yesno question on the node for
example Do you want to know about 
The content of the question is associated
with the section title
 If the users answer is yes eliminate the
nodes other than descendants of the con
0.4 0.1 0 0.1 0.2 0.2 0 0
0.5 0.1 0.4 0
0.6 0.4
0.4 0.1 0 0 0 0 0 0
0.5 0 0 0
0.5 0
0 0 0 0.1 0.2 0.2 0 0
0 0.1 0.4 0
0.1 0.4
"Yes" "No"
leaf with
best score
selected by
cost function
generate a "yes-no" question
on selected node
Figure Efficient Confirmation Strategy for Large-scale Text Retrieval
Systems with Spoken Dialogue Interface
Kazunori Komatani Teruhisa Misu Tatsuya Kawahara Hiroshi G. Okuno
Graduate School of Informatics
Kyoto University
Yoshida-Hommachi, Sakyo, Kyoto 606-8501, Japan
{komatani,kawahara,okuno}@i.kyoto-u.ac.jp
Abstract
Adequate confirmation for keywords is in-
dispensable in spoken dialogue systems
to eliminate misunderstandings caused by
speech recognition errors. Spoken lan-
guage also inherently includes out-of-
domain phrases and redundant expressions
such as disfluency, which do not contribute
to task achievement. It is necessary to
appropriately make confirmation for im-
portant portions. However, a set of key-
words necessary to achieve the tasks can-
not be predefined in retrieval for a large-
scale knowledge base unlike conventional
database query tasks. In this paper, we
describe two statistical measures for iden-
tifying portions to be confirmed. A rele-
vance score represents the matching degree
with the target knowledge base. A sig-
nificance score detects portions that conse-
quently affect the retrieval results. These
measures are defined based on information
that is automatically derived from the tar-
get knowledge base. An experimental eval-
uation shows that our method improved the
success rate of retrieval by generating con-
firmation more efficiently than using a con-
ventional confidence measure.
1 Introduction
Information retrieval systems with spoken lan-
guage have been studied (Harabagiu et al, 2002;
Hori et al, 2003). They require both automatic
speech recognition (ASR) and information re-
trieval (IR) technologies. As a straight mani-
festation to create these systems, we can think
of using ASR results as an input for IR systems
that retrieve a text knowledge base (KB). How-
ever, two problems occur in the characteristics
of speech.
1. Speech recognition errors
2. Redundancy included in spoken language
expressions
One is an ASR error, which is basically in-
evitable in speech communications. Therefore,
an adequate confirmation is indispensable in
spoken dialogue systems to eliminate the mis-
understandings caused by ASR errors.
If keywords to be confirmed are defined, the
system can confirm them using confidence mea-
sures (Komatani and Kawahara, 2000; Hazen
et al, 2000) to manage the errors. In con-
ventional tasks for spoken dialogue systems in
which their target of retrieval was well-defined,
such as the relational database, keywords that
are important to achieve the tasks correspond
to items in the relational database. Most spo-
ken dialogue systems that have been developed,
such as airline information systems (Levin et al,
2000; Potamianos et al, 2000; San-Segundo et
al., 2000) and train information systems (Allen
et al, 1996; Sturm et al, 1999; Lamel et al,
1999), are categorized here. However, it is not
feasible to define such keywords in retrieval for
operation manuals (Komatani et al, 2002) or
WWW pages, where the target of retrieval is
not organized and is written as natural language
text.
Another problem is that a user?s utterances
may include redundant expressions or out-of-
domain phrases. A speech interface has been
said to have the advantage of ease of input. This
means that redundant expressions, such as dis-
fluency and irrelevant phrases, are easily input.
These do not directly contribute to task achieve-
ment and might even be harmful. ASR results
that may include such redundant portions are
not adequate for an input of IR systems.
A novel method is described in this paper
that automatically detects necessary portions
for task achievement from the ASR results of
a user?s utterances; that is, the system deter-
mines if each part of the ASR results is neces-
sary for the retrieval. We introduce two mea-
sures for each portion of the results. One is a
relevance score (RS) with the target document
 
[HOWTO] Use Speech Recognition in
Windows XP
The information in this article applies to:
? Microsoft Windows XP Professional
? Microsoft Windows XP Home Edition
Summary: This article describes how to use
speech recognition in Windows XP. If you
installed speech recognition with Microsoft
Office XP, or if you purchased a new com-
puter that has Office XP installed, you can
use speech recognition in all Office pro-
grams as well as other programs for which
it is enabled.
Detail information: Speech recognition en-
ables the operating system to convert spo-
ken words to written text. An internal
driver, called a speech recognition engine,
recognizes words and converts them to
text. The speech recognition engine ...
 
Figure 1: Example of one article in database
set. The score is computed with a document
language model and is used for making confir-
mation prior to the retrieval. The other is a sig-
nificance score (SS) in the document matching.
It is computed after the retrieval using N-best
results and is used for prompting the user for
post-selection if necessary. Information neces-
sary to define these two measures, such as a doc-
ument language model and retrieval results for
N-best candidates of the ASR, can be automat-
ically derived from the target knowledge base.
Therefore, the system can detect the portions
necessary for the retrieval and make the confir-
mation efficiently without defining the keywords
manually.
2 Text Retrieval System for
Large-scale Knowledge Base
Our task involves text retrieval for a large-
scale knowledge base. As the target domain,
we adopted a software support knowledge base
provided by the Microsoft Corporation. The
knowledge base consists of the following three
components: glossary, frequently asked ques-
tions (FAQ), and a database of support articles.
Figure 1 is an example of the database. The
knowledge base is very large-scale, as shown in
Table 1.
The Dialog Navigator (Kiyota et al, 2002)
was developed in the University of Tokyo as a
Table 1: Document set (Knowledge base)
Text collection # of texts # of characters
Glossary 4,707 700,000
FAQ 11,306 6,000,000
Support articles 23,323 22,000,000
text retrieval system for this knowledge base.
The system accepts a typed-text input as ques-
tions from users and outputs a result of the re-
trieval. The system interprets input sentences
taking a syntactic dependency and synonymous
expression into consideration for matching it
with the knowledge base. The system can also
navigate for the user when he/she makes vague
questions based on scenarios (dialog card) that
were described manually beforehand. Hundreds
of the dialog cards have been prepared to ask
questions back to the users. If a user question
matches its input part, the system generates a
question based on its description.
We adopted the Dialog Navigator as a back-
end system and constructed a text retrieval sys-
tem with a spoken dialogue interface. We then
investigated a confirmation strategy to interpret
the user?s utterances robustly by taking into ac-
count the problems that are characteristic of
spoken language, as previously described.
3 Confirmation Strategy using
Relevance Score and Significance
Score
Making confirmations for every portion that has
the possibility to be an ASR error is tedious.
This is because every erroneous portion does
not necessarily affect the retrieval results. We
therefore take the influence of recognition er-
rors for retrieval into consideration, and control
generation of confirmation.
We make use of N-best results of the ASR
for the query and test if a significant difference
is caused among N-best sets of retrieved can-
didates. If there actually is, we then make a
confirmation on the portion that makes the dif-
ference. This is regarded as a posterior confir-
mation. On the other hand, if a critical error
occurs in the ASR result, such as those in the
product name in software support, the follow-
ing retrieval would make no sense. Therefore,
we also introduce a confirmation prior to the
retrieval for critical words.
The system flow including the confirmation is
summarized below.
1. Recognize a user?s utterance.
Speech input
System User
ASR
(N-best candidates)
Calculation of
relevance score
Language model
for ASR
Language model
trained with KB
Confirmation using
relevance score
Critical
words
Confirmation for
influential words
Reply or rephrase
Matching with KB with
weighting by relevance score
retrieval
result
retrieval
result
retrieval
result
Confirmation using significance score
Final result
Display the result
Confirmation for difference
between candidates
Reply or rephrase
Dialog
Navigator
(text retrieval)
Target text
KB TFIDF
Figure 2: System flow
2. Calculate a relevance score for each phrase
of ASR results.
3. Make a confirmation for critical words with
a low relevance score.
4. Retrieve the knowledge base using the Dia-
log Navigator for N-best candidates of the
ASR.
5. Calculate significance scores and generate
a confirmation based on them.
6. Show the retrieval results to the user.
This flow is also shown in Figure 2 and ex-
plained in the following subsections in detail.
3.1 Definition of Relevance Score
We use test-set perplexity for each portion of
the ASR results as one of the criteria in deter-
mining whether the portion is influential or not
for the retrieval. The language model to cal-
culate the perplexity was trained only with the
target knowledge base. It is different from that
used in the ASR.
The perplexity is defined as an exponential of
entropy per word, and it represents the average
number of the next words when we observe a
word sequence. The perplexity can be denoted
as the following equation because we assume an
ergodicity on language and use a trigram as a
language model.
log PP = ? 1
n
?
k
log P (wk|wk?1, wk?2)
This perplexity PP represents the degree that
a given word sequence, w1w2 ? ? ?wn, matches
the knowledge base with which the language
model P (wn|wn?1, wn?2) was trained. If the
perplexity is small, it indicates the sequence ap-
pears frequently in the knowledge base. On the
contrary, the perplexity for a portion including
the ASR errors increases because it is contex-
tually less frequent. The perplexity for out-of-
domain phrases similarly increases because they
scarcely appear in the knowledge base. It en-
ables us to detect a portion that is not influen-
tial for retrieval or those portions that include
ASR errors. Here, a phrase, called bunsetsu1
in Japanese, is adopted as a portion for which
the perplexity is calculated. We use a syntac-
tic parser KNP (Kurohashi and Nagao, 1994)
to divide the ASR results into the phrases2.
1Bunsetsu is a commonly used linguistic unit in
Japanese, which consists of one or more content words
and zero or more functional words that follow.
2As the parser was designed for written language, the
division often fails for portions including ASR errors.
The division error, however, does not affect the whole
system?s performance because the perplexity for the er-
roneous portions increases, indicating they are irrelevant.
 
User utterance:
?Atarashiku katta XP no pasokon de fax kinou
wo tsukau niha doushitara iidesu ka??
(Please tell me how to use the facsimile func-
tion in the personal computer with Windows
XP that I recently bought.)
Speech recognition result:
?Atarashiku katta XP no pasokon de fax kinou
wo tsukau ni sono e ikou??
[The underlined part was incorrectly recognized
here.]
Division into phrases (bunsetsu):
?Atarashiku / katta / XP no / pasokon de / fax
kinou wo / tsukau ni / sono / e / ikou??
Calculation of perplexity:
phrases (their context) PP RS
(<S>) Atarashiku (katta) 499.57 0.86
(atarashiku) katta (XP) 2079.83 0.47
(katta) XP no (pasokon) 105.64 0.99
(no) pasokon de (FAX) 185.92 0.95
(de) FAX kinou wo (tsukau) 236.23 0.89
(wo) tsukau ni (sono) 98.40 0.99
(ni) sono (e) 1378.72 0.62
(sono) e (ikou) 144.58 0.96
(e) ikou (</S>) 27150.00 0.00
<S>, </S> denote the beginning and end of a sen-
tence.
 
Figure 3: Example of calculating perplexity
(PP ) and relevance score (RS)
We then calculate the perplexity for the
phrases (bunsetsu) to which the preceding and
following words are attached. We then define
the relevance score by applying a sigmoid-like
transform to the perplexity using the following
equation. Thus, the score ranges between 0 and
1 by the transform and can be used as a weight
for each bunsetsu.
RS =
1
1 + exp(? ? (log PP ? ?))
Here, ? and ? are constants and are empiri-
cally set to 2.0 and 11.0. An example of calcu-
lating the relevance score is shown in Figure 3.
In this sample, a portion, ?Atarashiku katta (=
that I bought recently)?, that appeared in the
beginning of the utterance does not contribute
to any retrieval. A portion at the end of the sen-
tence was incorrectly recognized because it may
have been pronounced weakly. The perplexity
for these portions increases as a result, and the
relevance score correspondingly decreases.
3.2 Confirmation for Critical Words
using Relevance Score
Critical words should be confirmed before the
retrieval. This is because a retrieval result will
be severely damaged if the portions are not cor-
rectly recognized. We define a set of words that
are potentially critical using tf?idf values, which
are often used in information retrieval. They
can be derived from the target knowledge base
automatically. We regard a word with the max-
imum tf?idf values in each document as being
its representative, and the words that are rep-
resentative in more documents are regarded as
being more important. When the amount of
documents represented by the more important
words exceeds 10% out of the whole number of
documents, we define a set of the words as being
critical. As a result, 35 words were selected as
potentially critical ones in the knowledge base,
such as ?set up?, ?printer?, and ?(Microsoft) Of-
fice?.
We use the relevance score to determine
whether we should make a confirmation for the
critical words. If a critical word is contained
in a phrase whose relevance score is lower than
threshold ?, the system makes a confirmation.
We set threshold ? through the preliminary ex-
periment. The confirmation is done by present-
ing the recognition results to the user. Users can
either confirm or discard or correct the phrase
before passing it to the following matching mod-
ule.
3.3 Weighted Matching using
Relevance Score
A phrase that has a low relevance score is likely
to be an ASR error or a portion that does not
contribute to retrieval. We therefore use the rel-
evance score RS as a weight for phrases during
the matching with the knowledge base. This re-
lieves damage to the retrieval by ASR errors or
redundant expressions.
3.4 Significance Score using Retrieval
Results
The significance score is defined by using plural
retrieval results corresponding to N-best candi-
dates of the ASR. Ambiguous portions during
the ASR appear as the differences between the
N-best candidates. The score represents the de-
gree to which the portions are influential.
The significance score is calculated for por-
tions that are different among N-best candi-
dates. We define the significance score SS(n,m)
as the difference between the retrieval results of
n-th and m-th candidates. The value is defined
by the equation,
SS(n,m) = 1 ? |res(n) ? res(m)|
2
|res(n)||res(m)| .
Here, res(n) denotes a set of retrieval results
for the n-th candidate, and |res(n)| denotes the
number of elements in the set. That is, the sig-
nificance score decreases if the retrieval results
have a large common part.
Figure 4 has an example of calculating the
significance score. In this sample, the portions
of ?suuzi (numerals)? and ?suushiki (numeral
expressions)? differ between the first and sec-
ond candidates of the ASR. As the retrieval re-
sults for each candidate, 14 and 15 items are
obtained, respectively. The number of com-
mon items between the two retrieval results is
8. Then, the significance score for the portion
is 0.70 by the above equation.
3.5 Confirmation using Significance
Score
The confirmation is also made for the portions
detected by the significance score. If the score
is higher than a threshold, the system makes
the confirmation by presenting the difference to
users3. Here, we set the number of N-best can-
didates of the ASR to 3, and the threshold for
the score is set to 0.5.
In the confirmation phrase, if a user selects
from the list, the system displays the corre-
sponding retrieval results. If the score is lower
than the threshold, the system does not make
the confirmation and presents retrieval results
of the first candidate of the ASR. If a user
judges all candidates as inappropriate, the sys-
tem rejects the current candidates and prompts
him/her to utter the query again.
4 Experimental Evaluation
We implemented and evaluated our method as
a front-end of Dialog Navigator. The front-end
works on a Web browser, Internet Explorer 6.0.
Julius (Lee et al, 2001) for SAPI4 was used as a
speech recognizer on PCs. The system presents
a confirmation to users on the display. He or she
replies to the confirmation by selecting choices
with the mouse.
3Confirmation will be generated practically if one of
the significance scores between the first candidate and
others exceeds the threshold.
4http://julius.sourceforge.jp/sapi/
 
[#1 candidate of ASR]
?WORD2002 de suuzi wo nyuryoku suru
houhou wo oshiete kudasai.? (Please tell me
the way to input numerals in WORD 2002.)
Retrieval results (# of the results was 14.)
1. Input the present date and time in Word
2. WORD: Add a space between Japanese and
alphanumeric characters
3. WORD: Check the form of inputted char-
acters
4. WORD: Input a handwritten signature
5. WORD: Put watermark characters into the
background of a character
6. ...
[#2 candidate of ASR]
?WORD2002 de suushiki wo nyuryoku suru
houhou wo oshiete kudasai.? (Please tell
me the way to input numerical expressions in
WORD 2002.)
Retrieval results (# of the results was 15.)
1. Insert numerical expressions in Word
2. Input the present date and time in Word
3. Input numerical expressions in Spreadsheet
4. Input numerical expressions in PowerPoint
5. Input numerical expressions in Excel
6. ...
Significance score
SS(1, 2) = 1 ? 8214?15 = 0.70
(# of common items in the retrieval results
was 8.)
 
Figure 4: Example of calculating significance
score
We collected the test data by 30 subjects who
had not used our system. Each subject was re-
quested to retrieve support information for 14
tasks, which consisted of 11 prepared scenarios
(query sentences are not given) and 3 sponta-
neous queries. Subjects were allowed to utter
the sentence again up to 3 times per task if a rel-
evant retrieval result was not obtained. We ob-
tained 651 utterances for 420 tasks in total. The
average word accuracy of the ASR was 76.8%.
4.1 Evaluation of Success Rate of
Retrieval
We calculated the success rates of retrieval for
the collected speech data. We regarded the re-
trieval as having succeeded when the retrieval
results contained an answer for the user?s initial
question. We set three experimental conditions:
Table 2: Comparison of success rate of retrieval with method using only ASR results
# utterances Transcription ASR results Our method
651 520 421 457
(79.9%) (64.7%) (70.2%)
1. Transcription: A correct transcription of
user utterances, which was made manually,
was used as an input to the Dialog Naviga-
tor. This condition corresponds to a case of
100% ASR accuracy, indicating an utmost
performance obtained by improvements in
the ASR and our dialogue strategy.
2. ASR results: The first candidate of the
ASR was used as an input (baseline).
3. Our method: The N-best candidates of the
ASR were used as an input, and confirma-
tion was generated based on our method
using both the relevance and significance
scores. It was assumed that the users
responded appropriately to the generated
confirmation.
Table 2 lists the success rate. The rate when
the transcription was used as the input was
79.9%. The remaining errors included those
caused by irrelevant user utterances and those
in the text retrieval system. Our method at-
tained a better success rate than the condition
where the first candidate of the ASR was used.
Improvement of 36 cases (5.5%) was obtained by
our method, including 30 by the confirmations
and 14 by weighting during the matching using
the relevance score, though the retrieval failed
eight times as side effects of the weighting.
We further investigated the results shown in
Table 2. Table 3 lists the relations between the
success rate of the retrieval and the accuracy
of the ASR per utterance. The improvement
rate out of the number of utterances was rather
high between 40% and 60%. This means that
our method was effective not only for utterances
with high ASR accuracy but also for those with
around 50% accuracy. That is, an appropriate
confirmation was generated even for utterances
whose ASR accuracy was not very high.
4.2 Evaluation of Confirmation
Efficiency
We also evaluated our method from the number
of generated confirmations. Our method gener-
ated 221 confirmations. This means that con-
firmations were generated once every three ut-
terances on the average. The 221 confirmations
consisted of 66 prior to the retrieval using the
relevance score and 155 posterior to the retrieval
using the significance score.
We compared our method with a conventional
one, which used a confidence measure (CM)
based on N-best candidates of the ASR (Ko-
matani and Kawahara, 2000)5. In this method,
the system generated confirmation only for con-
tent words with a confidence measure lower
than ?1. The thresholds to generate confirma-
tion (?1) were set to 0.4, 0.6, and 0.8. If a con-
tent word that was confirmed was rejected by
the user, the retrieval was executed after remov-
ing a phrase that included it.
The number of confirmations and retrieval
successes are shown in Table 4. Our method
achieved a higher success rate with a less num-
ber of confirmations (less than half) compared
with the case of ?1 = 0.8 in the conventional
method. Thus, the generated confirmations
based on the two scores were more efficient.
The confidence measure used in the conven-
tional method only reflects the acoustic and
linguistic likelihood of the ASR results. Our
method, however, reflects the domain knowl-
edge because the two scores are derived by ei-
ther a language model trained with the target
knowledge base or by retrieval results for the
N-best candidates. The domain knowledge can
be introduced without any manual deliberation.
The experimental results show that the scores
are appropriate to determine whether a confir-
mation should be generated or not.
5 Conclusion
We described an appropriate confirmation strat-
egy for large-scale text retrieval systems with a
spoken dialogue interface. We introduced two
measures, relevance score and significance score,
for ASR results. The measures are useful to con-
trol confirmation efficiently for portions includ-
ing either ASR errors or redundant expressions.
The portions to be confirmed are determined
5We used a word-level CM only because defining se-
mantic categories for content words is required to cal-
culate the concept-level CM. Because the semantic cate-
gory corresponded to items in a relational database, we
cannot use the concept-level CM in this task.
Table 3: Success rate of retrieval per ASR accuracy
ASR accuracy (%) # utterances ASR results Our method # improvement
?40 37 9 11 2 ( 5.4%)
?60 73 33 42 9 (12.3%)
?80 194 116 129 13 ( 6.7%)
?100 347 263 275 12 ( 3.5%)
Total 651 421 457 36 ( 5.5%)
Table 4: Comparison with method using confidence measure (CM)
Our method CM (?1 = 0.4) CM (?1 = 0.6) CM (?1 = 0.8)
# confirmation 221 77 254 484
# success (success rate) 457 (70.2%) 427 (65.6%) 435 (66.8%) 445 (68.4%)
using information that is automatically derived
from the target knowledge base, such as a statis-
tical language model, tf?idf values, and retrieval
results. An experimental evaluation shows that
our method can efficiently generate confirma-
tions for better task achievement compared with
that using a conventional confidence measure of
the ASR. Our method is not dependent on the
software support task, and expected to be ap-
plicable to general text retrieval tasks.
6 Acknowledgments
The authors are grateful to Prof. Kurohashi and
Mr. Kiyota at the University of Tokyo and Ms.
Kido at Microsoft Corporation for their helpful
advice.
References
J. F. Allen, B. W. Miller, E. K. Ringger, and
T. Sikorski. 1996. A robust system for natu-
ral spoken dialogue. In Proc. of the 34th An-
nual Meeting of the ACL, pages 62?70.
S. Harabagiu, D. Moldovan, and J. Picone.
2002. Open-domain voice-activated question
answering. In Proc. COLING, pages 502?508.
T. J. Hazen, T. Burianek, J. Polifroni, and
S. Seneff. 2000. Integrating recognition con-
fidence scoring with language understanding
and dialogue modeling. In Proc. ICSLP.
C. Hori, T. Hori, H. Isozaki, E. Maeda, S. Kata-
giri, and S. Furui. 2003. Deriving disambigu-
ous queries in a spoken interactive ODQA
system. In Proc. IEEE-ICASSP.
Y. Kiyota, S. Kurohashi, and F. Kido. 2002.
?Dialog Navigator?: A question answering
system based on large text knowledge base.
In Proc. COLING, pages 460?466.
K. Komatani and T. Kawahara. 2000. Flexible
mixed-initiative dialogue management using
concept-level confidence measures of speech
recognizer output. In Proc. COLING, pages
467?473.
K. Komatani, T. Kawahara, R. Ito, and H. G.
Okuno. 2002. Efficient dialogue strategy to
find users? intended items from information
query results. In Proc. COLING, pages 481?
487.
S. Kurohashi and M. Nagao. 1994. A syntactic
analysis method of long Japanese sentences
based on the detection of conjunctive struc-
tures. Computational Linguistics, 20(4):507?
534.
L. F. Lamel, S. Rosset, J-L. S. Gauvain, and
S. K. Bennacef. 1999. The LIMSI ARISE
system for train travel information. In Proc.
IEEE-ICASSP.
A. Lee, T. Kawahara, and K. Shikano. 2001.
Julius ? an open source real-time large vo-
cabulary recognition engine. In Proc. EU-
ROSPEECH, pages 1691?1694.
E. Levin, S. Narayanan, R. Pieraccini, K. Bia-
tov, E. Bocchieri, G. Di Fabbrizio, W. Eck-
ert, S. Lee, A. Pokrovsky, M. Rahim, P. Rus-
citti, and M. Walker. 2000. The AT&T-
DARPA communicator mixed-initiative spo-
ken dialogue system. In Proc. ICSLP.
A. Potamianos, E. Ammicht, and H.-K. J. Kuo.
2000. Dialogue management in the Bell labs
communicator system. In Proc. ICSLP.
R. San-Segundo, B. Pellom, W. Ward, and
J. Pardo. 2000. Confidence measures for dia-
logue management in the CU communicator
system. In Proc. IEEE-ICASSP.
J. Sturm, E. Os, and L. Boves. 1999. Issues in
spoken dialogue systems: Experiences with
the Dutch ARISE system. In Proc. ESCA
workshop on Interactive Dialogue in Multi-
Modal Systems.
Dependency Structure Analysis and Sentence Boundary
Detection in Spontaneous Japanese
Kazuya Shitaoka? Kiyotaka Uchimoto? Tatsuya Kawahara? Hitoshi Isahara?
?School of Informatics,
Kyoto University
Yoshida-honmachi, Sakyo-ku,
Kyoto 606-8501, Japan,
{shitaoka,kawahara}@ar.media.kyoto-u.ac.jp
?National Institute of Information
and Communications Technology
3-5 Hikari-dai, Seika-cho, Soraku-gun,
Kyoto 619-0289, Japan,
{uchimoto,isahara}@nict.go.jp
Abstract
This paper describes a project to detect dependen-
cies between Japanese phrasal units called bunsetsus,
and sentence boundaries in a spontaneous speech
corpus. In monologues, the biggest problem with de-
pendency structure analysis is that sentence bound-
aries are ambiguous. In this paper, we propose
two methods for improving the accuracy of sentence
boundary detection in spontaneous Japanese speech:
One is based on statistical machine translation us-
ing dependency information and the other is based
on text chunking using SVM. An F-measure of 84.9
was achieved for the accuracy of sentence bound-
ary detection by using the proposed methods. The
accuracy of dependency structure analysis was also
improved from 75.2% to 77.2% by using automat-
ically detected sentence boundaries. The accuracy
of dependency structure analysis and that of sen-
tence boundary detection were also improved by in-
teractively using both automatically detected depen-
dency structures and sentence boundaries.
1 Introduction
The ?Spontaneous Speech: Corpus and Pro-
cessing Technology? project has been sponsor-
ing the construction of a large spontaneous
Japanese speech corpus, Corpus of Spontaneous
Japanese (CSJ) (Maekawa et al, 2000). The
CSJ is the biggest spontaneous speech corpus in
the world, and it is a collection of monologues
and dialogues, the majority being monologues
such as academic presentations. The CSJ in-
cludes transcriptions of speeches as well as audio
recordings. Approximately one tenth of the CSJ
has been manually annotated with information
about morphemes, sentence boundaries, depen-
dency structures, discourse structures, and so
on. The remaining nine tenths of the CSJ
have been annotated semi-automatically. A fu-
ture goal of the project is to extract sentence
boundaries, dependency structures, and dis-
course structures from the remaining transcrip-
tions. This paper focuses on methods for au-
tomatically detecting sentence boundaries and
dependency structures in Japanese spoken text.
In many cases, Japanese dependency struc-
tures are defined in terms of the dependency
relationships between Japanese phrasal units
called bunsetsus. To define dependency rela-
tionships between all bunsetsus in spontaneous
speech, we need to define not only the depen-
dency structures in all sentences but also the
inter-sentential relationships, or, discourse re-
lationships, between the sentences, as depen-
dency relationships between bunsetsus. How-
ever, it is difficult to define and detect discourse
relationships between sentences because of sig-
nificant inconsistencies in human annotations
of discourse structures, especially with regard
to spontaneous speech. We also need to know
intra-sentential dependency structures in order
to use the results of dependency structure anal-
ysis for sentence compaction in automatic text
summarization or case frame acquisition. Be-
cause it is difficult to define discourse relation-
ships between sentences, depending on the ac-
tual application, it is usually enough to define
and detect the dependency structure of each
sentence. Therefore, the CSJ was annotated
with intra-sentential dependency structures for
sentences in the same way this is usually done
for a written text corpus. However, there is
a big difference between a written text corpus
and a spontaneous speech corpus: In sponta-
neous speech, especially when it is long, sen-
tence boundaries are often ambiguous. In the
CSJ, therefore, sentence boundaries were de-
fined based on clauses whose boundaries were
automatically detected by using surface infor-
mation (Maruyama et al, 2003), and they were
detected manually (Takanashi et al, 2003). Our
definition of sentence boundaries follows the
definition used in the CSJ.
Almost all previous research on Japanese de-
pendency structure analysis dealt with depen-
dency structures in written text (Fujio and Mat-
sumoto, 1998; Haruno et al, 1998; Uchimoto et
al., 1999; Uchimoto et al, 2000; Kudo and Mat-
sumoto, 2000). Although Matsubara and col-
leagues did investigate dependency structures
in spontaneous speech (Matsubara et al, 2002),
the target speech was dialogues where the ut-
terances were short and sentence boundaries
could be easily defined based on turn-taking
data. In contrast, we investigated dependency
structures in spontaneous and long speeches in
the CSJ. The biggest problem in dependency
structure analysis with spontaneous and long
speeches is that sentence boundaries are am-
biguous. Therefore, sentence boundaries should
be detected before or during dependency struc-
ture analysis in order to obtain the dependency
structure of each sentence.
In this paper, we first describe the problems
with dependency structure analysis of sponta-
neous speech. Because the biggest problem is
ambiguous sentence boundaries, we focus on
sentence boundary detection and propose two
methods for improving the accuracy of detec-
tion.
2 Dependency Structure Analysis
and Sentence Boundary Detection
in Spontaneous Japanese
First, let us briefly describe how dependency
structures can be represented in a Japanese sen-
tence. In Japanese sentences, word order is
rather free, and subjects and objects are often
omitted. In languages having such characteris-
tics, the syntactic structure of a sentence is gen-
erally represented by the relationship between
phrasal units, or bunsetsus, based on a depen-
dency grammar. Phrasal units, or bunsetsus,
are minimal linguistic units obtained by seg-
menting a sentence naturally in terms of seman-
tics and phonetics. Each bunsetsu consists of
one or more morphemes. For example, the sen-
tence ???????????? (kare-wa yukkuri
aruite-iru, He is walking slowly)? can be divided
into three bunsetsus, ??? (kare-wa, he)?, ???
?? (yukkuri, slowly)? and ?????? (aruite-
iru, is walking)?. In this sentence, the first and
second bunsetsus depend on the third one.
There are many differences between writ-
ten text and spontaneous speech, and there
are problems peculiar to spontaneous speech
in dependency structure analysis and sentence
boundary detection. The following sections de-
scribe some typical problems and our solutions.
2.1 Problems with Dependency
Structure Analysis
Ambiguous sentence boundaries
As described in Section 1, in this study, we
assumed that ambiguous sentence bound-
aries is the biggest problem in dependency
structure analysis of spontaneous speech.
So in this paper, we mainly focus on this
problem and describe our solution to it.
Independent bunsetsus
In spontaneous speech, we sometimes find
that modifiees are missing because utter-
ance planning changes in the middle of the
speech. Also, we sometimes find bunsetsus
whose dependency relationships are useless
for understanding the utterance. These in-
clude fillers such as ???? (anoh, well)?
and ???? (sonoh, well)?, adverbs that
behave like fillers such as ??? (mou)?,
responses such as ??? (hai, yes)? and ?
?? (un, yes)?, conjunctions such as ??
(de, and)?, and disfluencies. In these cases,
bunsetsus are assumed to be independent,
and as a result, they have no modifiees in
the CSJ. For example, 14,988 bunsetsus in
188 talks in the CSJ are independent.
We cannot ignore fillers, responses, and
disfluencies because they frequently ap-
pear in spontaneous speech. However,
we can easily detect them by using the
method proposed by Asahara and Mat-
sumoto (Asahara and Matsumoto, 2003).
In this paper, fillers, responses, and disflu-
encies were eliminated before dependency
structure analysis and sentence boundary
detection by using morphological informa-
tion and labels. In the CSJ, fillers and re-
sponses are interjections, and almost all of
them are marked with label (F). Disfluen-
cies are marked with label (D).
In this paper, every independent bunsetsu
was assumed to depend on the next one.
However, practically speaking, indepen-
dent bunsetsus should be correctly detected
as ?independent?. This detection is one of
our future goals.
Crossed dependency
In general, dependencies in Japanese writ-
ten text do not cross. In contrast, de-
pendencies in spontaneous speech some-
times do. For example, ???? (kore-ga,
this)? depends on ????? (tadashii-to, is
right)? and ??? (watashi-wa, I)? depends
on ??? (omou, think)? in the sentence ??
??????????????, where ???
denotes a bunsetsu boundary. Therefore,
the two dependencies cross.
However, there are few number of crossed
dependencies in the CSJ: In 188 talks, we
found 689 such dependencies for total of
170,760 bunsetsus. In our experiments,
therefore, we assumed that dependencies
did not cross. Correctly detecting crossed
dependencies is one of our future goals.
Self-correction
We often find self-corrections in sponta-
neous speech. For example, in the 188 talks
in the CSJ there were 2,544 self-corrections.
In the CSJ, self-corrections are represented
as dependency relationships between bun-
setsus, and label D is assigned to them.
Coordination and appositives are also rep-
resented as dependency relationships be-
tween bunsetsus, and labels P and A are
assigned to them, respectively. The defi-
nitions of coordination and appositives fol-
low those of the Kyoto University text cor-
pus (Kurohashi and Nagao, 1997). Both
the labels and the dependencies should
be detected for applications such as au-
tomatic text summarization. However, in
this study, we detected only the dependen-
cies between bunsetsus, and we did it in the
same manner as in previous studies using
written text.
Inversion
Inversion occurs more frequently in spon-
taneous speech than in written text. For
example, in the 188 talks in the CSJ there
were 172 inversions. In the CSJ, inver-
sions are represented as dependency rela-
tionships going in the direction from right
to left. In this study, we thought it impor-
tant to detect dependencies, and we man-
ually changed their direction to that from
left to right. The direction of dependency
has been changed to that from left to right.
2.2 Problems with Sentence Boundary
Detection
In spontaneous Japanese speech, sentence
boundaries are ambiguous. In the CSJ, there-
fore, sentence boundaries were defined based
on clauses whose boundaries were automatically
detected using surface information (Maruyama
et al, 2003), and they were detected manually
(Takanashi et al, 2003). Clause boundaries can
be classified into the following three groups.
Absolute boundaries , or sentence bound-
aries in their usual meaning. Such bound-
aries are often indicated by verbs in their
basic form.
Strong boundaries , or points that can be re-
garded as major breaks in utterances and
that can be used for segmentation. Such
boundaries are often indicated by clauses
whose rightmost words are ?? (ga, but)?,
or ?? (shi, and)?.
Weak boundaries , or points that can
be used for segmentation because they
strongly depend on other clauses. Such
boundaries are often indicated by clauses
whose rightmost words are ??? (node, be-
cause)?, or ??? (tara, if)?.
These three types of boundary differ in the
degree of their syntactic and semantic com-
pleteness and the dependence of their sub-
sequent clauses. Absolute boundaries and
strong boundaries are usually defined as sen-
tence boundaries. However, sentence bound-
aries in the CSJ are different from these two
types of clause boundaries, and the accuracy
of rule-based automatic sentence boundary de-
tection in the 188 talks in the CSJ has an F-
measure of approximately 81, which is the ac-
curacy for a closed test. Therefore, we need a
more accurate sentence boundary detection sys-
tem.
Shitaoka et al (Shitaoka et al, 2002) pro-
posed a method for detecting sentence bound-
aries in spontaneous Japanese speech. Their
definition of sentence boundaries is approxi-
mately the same as that of absolute bound-
aries described above. In this method, sen-
tence boundary candidates are extracted by
character-based pattern matching using pause
duration. However, it is difficult to extract
appropriate candidates by this method be-
cause there is a low correlation between pauses
and the strong and weak boundaries described
above. It is also hard to detect noun-final
clauses by character-based pattern matching.
One method based on machine learning, a
method based on maximum entropy models,
has been proposed by Reynar and Ratnaparkhi
(Reynar and Ratnaparkhi, 2000). However, the
target in their study was written text. This
method cannot readily used for spontaneous
speech because in speech, there are no punc-
tuation marks such as periods. Other features
of utterances should be used to detect sentence
boundaries in spontaneous speech.
3 Approach of Dependency
Structure Analysis and Sentence
Boundary Detection
The outline of the processes is shown in Fig-
ure 1.
0: Morphological
Analysis
1: Sentence Boundary
Detection (Baseline)
3: Dependency Structure
Analysis (Baseline)
2: Sentence Boundary
Detection (SVM)
5: Sentence Boundary
Detection (Language model)
6: Sentence Boundary
Detection (SVM)
7: Dependency Structure
Analysis (Again)
clause
expression
pause
duration
word 3-gram model
pause
duration
clause
expression
word
information
(A)
(B)
word
Information
distance
between 
bunsetsus
(C)
(A) + information of 
dependencies
(B) + information of
dependencies
4: Dependency 
Structure Analysis
Figure 1: Outline of dependency structure anal-
ysis and sentence boundary detection.
3.1 Dependency Structure Analysis
In statistical dependency structure analysis of
Japanese speech, the likelihood of dependency
is represented by a probability estimated by a
dependency probability model.
Given sentence S, let us assume that it is
uniquely divided into n bunsetsus, b1, . . . , bn,
and that it is represented as an ordered set of
bunsetsus, B = {b1, . . . , bn}. Let D be an or-
dered set of dependencies in the sentence and let
D
i
be a dependency whose modifier is bunsetsu
b
i
(i = 1, . . . , n ? 1). Let us also assume that
D = {D1, . . . ,Dn?1}. Statistical dependency
structure analysis finds dependencies that max-
imize probability P (D|S) given sentence S.
The conventional statistical model (Collins,
1996; Fujio and Matsumoto, 1998; Haruno et
al., 1998; Uchimoto et al, 1999) uses only
the relationship between two bunsetsus to es-
timate the probability of dependency, whereas
the model in this study (Uchimoto et al, 2000)
takes into account not only the relationship be-
tween two bunsetsus but also the relationship
between the left bunsetsu and all the bunsetsus
to its right. This model uses more information
than the conventional model.
We implemented this model within a max-
imum entropy modeling framework. The fea-
tures used in the model were basically attributes
of bunsetsus, such as character strings, parts
of speech, and types of inflections, as well as
those that describe the relationships between
bunsetsus, such as the distance between bun-
setsus. Combinations of these features were also
used. To find D
best
, we analyzed the sentences
backwards (from right to left). In the backward
analysis, we can limit the search space effec-
tively by using a beam search. Sentences can
also be analyzed deterministically without great
loss of accuracy (Uchimoto et al, 1999). So we
analyzed a sentence backwards and determinis-
tically.
3.2 Sentence Boundary Detection
Based on Statistical Machine
Translation (Conventional method
(Shitaoka et al, 2002))
The framework for statistical machine trans-
lation is formulated as follows. Given in-
put sequence X, the goal of statistical ma-
chine translation is to find the best output se-
quence, Y , that maximizes conditional proba-
bility P (Y |X):
max
Y
P (Y |X) = max
Y
P (Y )P (X |Y ) (1)
The problem of sentence boundary detection
can be reduced to the problem of translat-
ing a sequence of words, X, that does not in-
clude periods but instead includes pauses into
a sequence of words, Y , that includes peri-
ods. Specifically, in places where a pause
might be converted into a period, which means
P (X|Y ) = 1, the decision whether a period
should be inserted or not is made by comparing
language model scores P (Y ?) and P (Y ??). Here,
the difference between Y ? and Y ?? is in that one
includes a period in a particular place and the
other one does not.
We used a model that uses pause duration
and surface expressions around pauses as trans-
lation model P (X|Y ). We used expressions
around absolute and strong boundaries as de-
scribed in Section 2.2 as surface expressions
around pauses. A pause preceding or follow-
ing surface expressions can be converted into
a period. Specifically, pauses following expres-
sions ?? (to)?, ??? (nai)?, and ?? (ta)?, and
pauses preceding expression ?? (de)?, can be
converted into a period when these pauses are
longer than average. A pause preceding or fol-
lowing other surface expressions can be con-
verted into a period even if its duration is short.
To calculate P (Y ), we used a word 3-gram
model trained with transcriptions in the CSJ.
3.3 Sentence Boundary Detection
Using Dependency Information
(Method 1)
There are three assumptions that should be sat-
isfied by the rightmost bunsetsu in every sen-
tence. In the following, this bunsetsu is referred
to as the target bunsetsu.
(1) One or more bunsetsus depend on the
target bunsetsu. (Figure 2)
Since every bunsetsu depends on another bun-
setsu in the same sentence, the second rightmost
bunsetsu always depends on the rightmost bun-
setsu in any sentence, except in inverted sen-
tences. In inverted sentences in this study, we
changed the direction of all dependencies to that
from left to right.
One or more  
Bunsetsus depend   
Figure 2: One or more bunsetsus depend on
the target bunsetsu. (?|? represents a sentence
boundary.)
(2) There is no bunsetsu that depends
on a bunsetsu beyond the target bunsetsu.
(Figure 3)
Each bunsetsu in a sentence depends on a bun-
setsu in the same sentence.
(3) The probability of the target bun-
setsu is low. (Figure 4)
The target bunsetsu does not depend on any
bunsetsu.
No bunsetsu depend in this way
Figure 3: There is no bunsetsu that depends on
a bunsetsu beyond the target bunsetsu.
This probability should be low
Figure 4: Probability of the target bunsetsu is
low.
Bunsetsus that satisfy assumptions (1)-(3)
are extracted as rightmost bunsetsu candidates
in a sentence. Then, for every point follow-
ing the extracted bunsetsus and for every pause
preceding or following the expressions described
in Section 3.2, a decision is made regarding
whether a period should be inserted or not.
In assumption (2), bunsetsus that depend on a
bunsetsu beyond 50 bunsetsus are ignored be-
cause no such long-distance dependencies were
found in the 188 talks in the CSJ used in our ex-
periments. Bunsetsus whose dependency prob-
ability is very low are also ignored because there
is a high possibility that these bunsetsus? depen-
dencies are incorrect. Let this threshold proba-
bility be p, and let the threshold probability in
assumption (3) be q. The optimal parameters p
and q are determined by using held-out data.
In this approach, about one third of all
bunsetsu boundaries are extracted as sentence
boundary candidates. So, an output sequence
is selected from all possible conversion patterns
generated using two words to the left and two
words to the right of each sentence boundary
candidate. To perform this operation, we used
a beam search with a width of 10 because a
number of conversion patterns can be generated
with such a search.
3.4 Sentence Boundary Detection
Based on Machine Learning
(Method 2)
We use Support Vector Machine (SVM) as a
machine learning model and we approached the
problem of sentence boundary detection as a
text chunking task. We used YamCha (Kudo
and Matsumoto, 2001) as a text chunker, which
is based on SVM and uses polynomial kernel
functions. To determine the appropriate chunk
label for a target word, YamCha uses two words
to the right and two words to the left of the
target word as statistical features, and it uses
chunk labels that are dynamically assigned to
the two preceding or the two following words
as dynamic features, depending on the analysis
direction. To solve the multi-class problem, we
used pairwise classification. This method gen-
erates N ? (N ? 1)/2 classifiers for all pairs of
classes, N , and makes a final decision by their
weighted voting.
The features used in our experiments are the
following:
1. Morphological information of the three words
to the right and three words to the left of the
target word, such as character strings, pronun-
ciation, part of speech, type of inflection, and
inflection form
2. Pause duration normalized in terms of Maha-
lanobis distance
3. Clause boundaries
4. Dependency probability of the target bunsetsu
5. The number of bunsetsus that depend on the
target bunsetsu and their dependency proba-
bilities
We used the IOE labeling scheme for proper
chunking, and the following parameters for
YamCha.
? Degree of polynomial kernel: 3rd
? Analysis direction: Left to right
? Multi-class method: Pairwise
4 Experiments and Discussion
In our experiments, we used the transcriptions
of 188 talks in the CSJ. We used 10 talks for
testing. Dependency structure analysis results
were evaluated for closed- and open-test data in
terms of accuracy, which was defined as the per-
centage of correct dependencies out of all depen-
dencies. In Tables 1 to 3, we use words ?closed?
and ?open? to describe the results obtained for
closed- and open-test data, respectively. Sen-
tence boundary detection results were evaluated
in terms of F-measure.
First, we show the baseline accuracy of depen-
dency structure analysis and sentence boundary
detection. The method described in Section 3.2
was used as a baseline method for sentence
boundary detection (Process 1 in Figure 1). To
train the language model represented by P (Y ),
we used the transcriptions of 178 talks exclud-
ing the test data. The method described in Sec-
tion 3.1 was used as a baseline method for de-
pendency structure analysis. (Process 3 in Fig-
ure 1) As sentence boundaries, we used the re-
sults of the baseline method for sentence bound-
ary detection. We obtained an F-measure of
75.6, a recall of 64.5%, and a precision of 94.2%
for the sentence boundary detection in our ex-
periments. The dependency structure analysis
accuracy was 75.2% for the open data and 80.7%
for the closed data.
The dependency probability of the rightmost
bunsetsus in a given sentence was not calculated
in our model. So, we assumed that the right-
most bunsetsus depended on the next bunsetsu
and that the dependency probability was 0.5
when we used dependency information in the
experiments described in the following sections.
4.1 Sentence Boundary Detection
Results Obtained by Method 1
We evaluated the results obtained by the
method described in Section 3.3. The results
of baseline dependency structure analysis were
used as dependency information (Process 5 in
Figure 1).
First, we investigated the optimal values of
parameters p and q described in Section 3.3 by
using held-out data, which differed from the test
data and consisted of 15 talks. The optimal val-
ues of p and q were, respectively, 0 and 0.9 for
the open-test data, and 0 and 0.8 for the closed-
test data. These values were used in the follow-
ing experiments. The value of p was 0, and these
results show that bunsetsus that depended on a
bunsetsu beyond 50 bunsetsus were ignored as
described in assumption (2) in Section 3.3.
The obtained results are shown in Table 1.
When dependency information was used, the F-
measure increased by approximately 1.4 for the
open-test data and by 2.0 for the closed test
data, respectively. Although the accuracy of de-
pendency structure analysis for closed test data
was about 5.5% higher than that for the open-
test data, the difference between the accuracies
of sentence boundary detection for the closed-
and open-test data was only about 0.6%. These
results indicate that equivalent accuracies can
be obtained for both open- and closed-test data
in detecting dependencies related to sentence
boundaries.
When all the extracted candidates were con-
sidered as sentence boundaries without us-
ing language models, the accuracy of sentence
boundary detection obtained by using the base-
line method was 68.2%(769/1,127) in recall and
81.5%(769/943) in precision, and that obtained
by using Method 1 was 87.2%(983/1,127) in re-
call and 27.7%(983/3,544) in precision. The re-
sults show that additional 214 sentence bound-
ary candidates were correctly extracted by us-
ing dependency information. However, only
108 sentence boundaries were chosen out of
the 214 candidates when language models were
used. We investigated in detail the points
that were not chosen and found errors in noun-
final clauses, clauses where the rightmost con-
stituents were adjectives or verbs such as ??
?? (it to-omou, think)? or ????? (it wa-
muzukashii, difficult)?, and clauses where the
rightmost constituents were ?????? (it to-
Table 1: Sentence boundary detection results
obtained by using dependency information.
recall precision F
With dependency 74.1% 82.5% 78.0
information (open) (835/1,127) (835/1,012)
With dependency 74.2% 83.5% 78.6
information (closed) (836/1,127) (836/1,001)
baseline 64.5% 94.2% 76.6
(727/1,127) (727/772)
iu-no-wa, because)? and ????? (it to-si-te-
wa, as)?, and so on. Some errors, except for
those in noun-final clauses, could have been cor-
rectly detected if we had had more training
data.
We also found that periods were sometimes
erroneously inserted when preceding expres-
sions were ?? (ga, but)?, ???? (mashite,
and)?, and ????? (keredomo, but)?, which
are typically the rightmost constituents of a sen-
tence, as weel as ?? (te, and)?, which is not,
typically, the rightmost constituent of a sen-
tence. The language models were not good at
discriminating between subtle differences.
4.2 Sentence Boundary Detection
Results Obtained by Method 2
We evaluated the results obtained by the
method described in Section 3.4 (Process 6 in
Figure 1). For training, we used 178 talks ex-
cluding test data.
The results are shown in Table 2. The F-
measure was about 6.9 points higher than that
described in Section 4.1. The results show
that the approach based on machine learning
is more effective than that based on statisti-
cal machine translation. The results also show
that the accuracy of sentence boundary detec-
tion can be increased by using dependency in-
formation in Method 2. However, we found that
the amount of accuracy improvement achieved
by using dependency information depended on
the method used. This may be because other
features used in SVM may provide information
similar to dependency information. For exam-
ple, Feature 1 described in Section 3.4 might
provide information similar to that in Features
4 and 5. Although in our experiments we used
only three words to the right and three words
to the left of the target word, the degradation
in accuracy without dependency information
was slight. This may be because long-distance
dependencies may not be related to sentence
boundaries, or because Feature 5 does not con-
tribute to increasing the accuracy because the
accuracy of dependency structure analysis in de-
tecting long-distance dependencies is not high.
Table 2: Sentence boundary detection results
obtained by using SVM.
recall precision F
With dependency 80.0% 90.3% 84.9
information (open) (902/1,127) (902/999)
With dependency 79.7% 90.5% 84.9
information (closed) (900/1,127) (900/994)
Without 79.3% 90.1% 84.4
dependency information (894/1,127) (894/992)
Table 3: Dependency structure analysis results
obtained with automatically detected sentence
boundaries.
open closed
With results in Section 4.1 75.8% 81.2%
With results in Section 4.2 77.2% 82.5%
Baseline 75.2% 80.7%
4.3 Dependency Structure Analysis
Results
We evaluated the results of dependency struc-
ture analysis obtained when sentence bound-
aries detected automatically by the two meth-
ods described above were used as inputs (Pro-
cess 7 in Figure 1). The results are shown in
Table 3. The accuracy of dependency structure
analysis improved by about 2% when the most
accurate and automatically detected sentence
boundaries were used as inputs. This is be-
cause more sentence boundaries were detected
correctly, and the number of bunsetsus that de-
pended on those in other sentences decreased.
We investigated the accuracy of dependency
structure analysis when 100% accurate sentence
boundaries were used as inputs. The accuracy
was 80.1% for the open-test data, and 86.1%
for the closed-test data. Even when the sen-
tence boundary detection was perfect, the er-
ror rate was approximately 14% even for the
closed-test data. The accuracy of dependency
structure analysis for spoken text was about 8%
lower than that for written text (newspapers).
We speculate that this is because spoken text
has no punctuation marks and many bunsetsus
depend on others far from them because of in-
sertion structures. These problems need to be
addressed in future studies.
5 Conclusion
This paper described a project to detect depen-
dencies between bunsetsus and sentence bound-
aries in a spontaneous speech corpus. It is
more difficult to detect dependency structures
in spontaneous spoken speech than in written
text. The biggest problem is that sentence
boundaries are ambiguous. We proposed two
methods for improving the accuracy of sentence
boundary detection in spontaneous Japanese
speech. Using these methods, we obtained an
F-measure of 84.9 for the accuracy of sentence
boundary detection. The accuracy of depen-
dency structure analysis was also improved from
75.2% to 77.2% by using automatically detected
sentence boundaries. The accuracy of depen-
dency structure analysis and that of sentence
boundary detection were improved by interac-
tively using automatically detected dependency
information and sentence boundaries.
There are several future directions. In the fu-
ture, we would like to solve the problems that
we found in our experiments. In particular, we
want to reduce the number of errors due to in-
serted structures and solve other problems de-
scribed in Section 2.1.
References
Masayuki Asahara and Yuji Matsumoto. 2003. Filler and
Disfluency Identification Based on Morphological Analysis
and Chunking. In Proceedings of the ISCA & IEEE Work-
shop on Spontaneous Speech Processing and Recognition,
pages 163?166.
Michael Collins. 1996. A New Statistical Parser Based on
Bigram Lexical Dependencies. In Proceedings of the ACL,
pages 184?191.
Masakazu Fujio and Yuji Matsumoto. 1998. Japanese Depen-
dency Structure Analysis based on Lexicalized Statistics.
In Proceedings of the EMNLP, pages 87?96.
Masahiko Haruno, Satoshi Shirai, and Yoshifumi Ooyama.
1998. Using Decision Trees to Construct a Practical
Parser. In Proceedings of the COLING-ACL, pages 505?
511.
Taku Kudo and Yuji Matsumoto. 2000. Japanese Depen-
dency Structure Analysis Based on Support Vector Ma-
chines. In Proceedings of the EMLNP, pages 18?25.
Taku Kudo and Yuji Matsumoto. 2001. Chunking with sup-
port vector machines. In Proceedings of the NAACL.
Sadao Kurohashi and Makoto Nagao. 1997. Building a
Japanese Parsed Corpus while Improving the Parsing Sys-
tem. In Proceedings of the NLPRS, pages 451?456.
Kikuo Maekawa, Hanae Koiso, Sadaoki Furui, and Hitoshi
Isahara. 2000. Spontaneous Speech Corpus of Japanese.
In Proceedings of the LREC2000, pages 947?952.
Takehiko Maruyama, Hideki Kashioka, Tadashi Kumano, and
Hideki tanaka. 2003. Rules for Automatic Clause Bound-
ary Detection and Their Evaluation. In Proceedings of
the Nineth Annual Meeting of the Association for Natural
Language proceeding, pages 517?520. (in Japanese).
Shigeki Matsubara, Takahisa Murase, Nobuo Kawaguchi, and
Yasuyoshi Inagaki. 2002. Stochastic Dependency Parsing
of Spontaneous Japanese Spoken Language. In Proceedings
of the COLING2002, pages 640?645.
Jeffrey C. Reynar and Adwait Ratnaparkhi. 2000. A Max-
imum Entropy Approach to Identifying Sentence Bound-
aries. In Proceedings of the ANLP, pages 16?19.
Kazuya Shitaoka, Tatsuya Kawahara, and Hiroshi G. Okuno.
2002. Automatic Transformation of Lecture Transcrip-
tion into Document Style using Statistical Framework. In
IPSJ?WGSLP SLP-41-3, pages 17?24. (in Japanese).
Katsuya Takanashi, Takehiko Maruyama, Kiyotaka Uchi-
moto, and Hitoshi Isahara. 2003. Identification of ?Sen-
tences? in Spontaneous Japanese ? Detection and Mod-
ification of Clause Boundaries ?. In Proceedings of the
ISCA & IEEE Workshop on Spontaneous Speech Process-
ing and Recognition, pages 183?186.
Kiyotaka Uchimoto, Satoshi Sekine, and Hitoshi Isahara.
1999. Japanese Dependency Structure Analysis Based on
Maximum Entropy Models. In Proceedings of the EACL,
pages 196?203.
Kiyotaka Uchimoto, Masaki Murata, Satoshi Sekine, and Hi-
toshi Isahara. 2000. Dependency Model Using Posterior
Context. In Proceedings of the IWPT, pages 321?322.
Coling 2008: Companion volume ? Posters and Demonstrations, pages 59?62
Manchester, August 2008
Bayes Risk-based Dialogue Management
for Document Retrieval System with Speech Interface
Teruhisa Misu ? ?
?School of Informatics,
Kyoto University
Sakyo-ku, Kyoto, Japan
Tatsuya Kawahara?
?National Institute of Information
and Communications Technology
Hikari-dai, Seika-cho, Soraku-gun,
Kyoto, Japan
Abstract
We propose an efficient dialogue manage-
ment for an information navigation sys-
tem based on a document knowledge base
with a spoken dialogue interface. In order
to perform robustly for fragmental speech
input and erroneous output of an auto-
matic speech recognition (ASR), the sys-
tem should selectively use N-best hypothe-
ses of ASR and contextual information.
The system also has several choices in gen-
erating responses or confirmations. In this
work, we formulate the optimization of
the choices based on a unified criterion:
Bayes risk, which is defined based on re-
ward for correct information presentation
and penalty for redundant turns. We have
evaluated this strategy with a spoken di-
alogue system which also has question-
answering capability. Effectiveness of the
proposed framework was confirmed in the
success rate of retrieval and the average
number of turns.
1 Introduction
In the past years, a great number of spoken dia-
logue systems have been developed. Their typi-
cal task domains include airline information (ATIS
& DARPA Communicator) and bus location tasks.
Although the above systems can handle simple
database retrieval or transactions with constrained
dialogue flows, they are expected to handle more
c
? 2008. Teruhisa Misu and Tatsuya Kawahara,
Licensed under the Creative Commons Attribution-
Noncommercial-Share Alike 3.0 Unported license
(http://creativecommons.org/licenses/by-nc-sa/3.0/). Some
rights reserved.
complex tasks. Meanwhile, more and more elec-
tronic text resources are recently being accumu-
lated. Since most documents are indexed (e.g., via
Web search engines), we are potentially capable of
accessing these documents. Reflecting such a situ-
ation, in recent years, the target of spoken dialogue
systems has been extended to retrieval of general
documents (Chang et al, 2002).
There are quite a few choices for handling user
utterances and generating responses in the spo-
ken dialogue systems that require parameter tun-
ing. Since a subtle change in these choices may
affect the behavior the entire system, they are usu-
ally tuned by hand by an expert. It is also the
case in speech-baed document retrieval systems.
We can make use of N-best hypotheses to realize
robust retrieval against errors in automatic speech
recognition (ASR). Input queries are often vague
or fragmented in speech interfaces, thus concate-
nation of contextual information is important to
make meaningful retrieval. Such decisions tend to
be optimized module by module, but they should
be done in an integrated way. For example, we
could make more appropriate retrieval by rescoring
the N-best ASR hypotheses by the information re-
trieval scores. Even if the target document is iden-
tified, the system has several choices for generat-
ing responses. Confirmation is needed to elimi-
nate any misunderstandings caused by ASR errors,
but users easily become irritated with so many re-
dundant confirmations. Although there are several
works dealing with dialogue management in call
routing systems (Levin and Pieraccini, 2006), they
cannot handle the complex decision making pro-
cesses in information guidance tasks.
Therefore, we address the extension of conven-
tional optimization methods of dialogue manage-
ment to be applicable to general document retrieval
59
tasks. In particular, we propose a dialogue man-
agement that optimizes the choices in response
generation by minimizing Bayes risk. The Bayes
risk is defined based on reward for correct informa-
tion presentation and penalty for redundant turns
as well as the score of document retrieval and an-
swer extraction.
2 Task and Knowledge Base (KB)
As the target domain, we adopt a sightseeing
guide for Kyoto city. The KBs of this system are
Wikipedia documents concerning Kyoto and the
official tourist information of Kyoto city (810 doc-
uments, 220K words in total).
?Dialogue Navigator for Kyoto City? is a doc-
ument retrieval system with a spoken dialogue in-
terface. The system can retrieve information from
the above-mentioned document set. This system is
also capable of handling user?s specific question,
such as ?Who built this shrine?? using the QA
technique.
3 Dialogue Management and Response
Generation in Document Retrieval
System
3.1 Choices in Generating Responses
We analyzed the dialogue sessions collected in the
field trial of the ?Dialogue Navigator for Kyoto
City?, and found that we could achieve a higher
success rate by dealing with following issues.
1. Use of N-best hypotheses of ASR
There have been many studies that have used
the N-best hypotheses (or word graph) of ASR
for making robust interpretations of user utter-
ances in relational database query tasks (Ray-
mond et al, 2003). We also improved retrieval
by using all of the nouns in the 3-best hypothe-
ses(Misu and Kawahara, 2007). However, the
analysis also showed that some retrieval fail-
ures were caused by some extraneous nouns in-
cluded in erroneous hypotheses, and a higher
success rate could be achieved by selecting an
optimal hypothesis.
2. Incorporation of contextual information
In interactive query systems, users tend to make
queries that include anaphoric expressions. In
these cases, it is impossible to extract the cor-
rect answer using only the current query. For
example, ?When was it built?? makes no sense
when used by itself. We deal with this problem
by concatenating the contextual information or
keywords from the user?s previous utterances to
generate a query. However, this may include in-
appropriate context when the user changes the
topic.
3. Choices in generating responses or confirma-
tions
An indispensable part of the process to avoid
presenting inappropriate documents is confir-
mation, especially when the score of retrieval
is low. This decision is also affected by points 1
and 2 mentioned above. The presentation of the
entire document may also be ?safer? than pre-
senting the specific answer to the user?s ques-
tion, when the score of answer extraction is low.
3.2 Generation of Response Candidates
The manners of response for a document d con-
sist of the following three actions. One is the pre-
sentation (Pres(d)) of the document d, which is
made by summarizing it. Second is making a con-
firmation (Conf(d)) for presenting the document
d. The last is answering (Ans(d)) the user?s spe-
cific question, which is generated by extracting one
specific sentence from the document d.
For these response candidates, we define the
Bayes risk based on the reward for success, the
penalty for a failure, and the probability of suc-
cess. Then, we select the candidate with the mini-
mal Bayes risk. The system flow of these processes
is summarized below.
1. Make search queries W
i
(i = 1, . . . , 8) using the
1st, 2nd, and 3rd hypothesis of ASR, and all of
them, with/without contextual information.
2. For each query W
i
, retrieve from the KB and
obtain a candidate document d
i
and its likeli-
hood p(d
i
).
3. For each document d
i
, generate presentation
Pres(d
i
), confirmation Conf(d
i
), and answer-
ing Ans(d
i
) response candidates.
4. Calculate the Bayes risk for 25 response can-
didates, which are the combination of 4 (N-best
hypotheses)? 2 (use of contextual information)
? 3 (choice in response generation) + 1 (rejec-
tion).
5. Select the optimal response candidate that has
the minimal Bayes risk.
60
3.3 Definition of Bayes Risk for Candidate
Response
For these response candidates, we define the Bayes
risk based on the reward for success, the penalty
for a failure, and the probability of success (ap-
proximated by the confidence measure). That
is, a reward is given according to the manner
of response (Rwd
Ret
or Rwd
QA
) when the sys-
tem presents an appropriate response. On the
other hand, a penalty is given based on extrane-
ous time, which is approximated by the number
of sentences before obtaining the appropriate in-
formation when the system presents an incorrect
response. For example, the penalty for a confir-
mation is 2 {system?s confirmation + user?s ap-
proval}, and that of a rejection is 1 {system?s re-
jection}. When the system presents incorrect in-
formation, the penalty for a failure FailureRisk
(FR) is calculated, which consists of the improper
presentation, the user?s correction, and the sys-
tem?s request for a rephrasal. Additional sentences
for the completion of a task (AddSent) are also
given as extraneous time before accessing the ap-
propriate document when the user rephrases the
query/question. The value of AddSent is calcu-
lated as an expected number of risks assuming the
probability of success by rephrasal was p1.
The Bayes risk for the response candidates
is formulated as follows using the likelihood
of retrieval p(d), likelihood of answer extrac-
tion p
QA
(d), and the reward pair (Rwd
Ret
and
Rwd
QA
; Rwd
Ret
< Rwd
QA
) for successful pre-
sentations as well as the FR for inappropriate pre-
sentations.
? Presentation of document d (without confir-
mation)
Risk(Pres(d)) = ?Rwd
Ret
? p(d)
+(FR + AddSent) ? (1 ? p(d))
? Confirmation for presenting document d
Risk(Conf(d)) = (?Rwd
Ret
+ 2) ? p(d)
+(2 + AddSent) ? (1 ? p(d))
? Answering user?s question using document d
Risk(Ans(d)) = ?Rwd
QA
? p
QA
(d) ? p(d)
+(FR + AddSent) ? (1 ? p
QA
(d) ? p(d))
? Rejection
Risk(Rej) = 1 + AddSent
1In the experiment, we use the success rate of the field trial
presented in (Misu and Kawahara, 2007).
? ?
User utterance: When did the shogun order to
build the temple?
(Previous query:) Tell me about the Silver
Pavilion.
Response candidates:
* With context:
? p(Silver Pavilion history) = 0.4
? p
QA
(Silver Pavilion history) = 0.2 : In 1485
- Risk(Pres(Silver Pavilion history)) = 6.4
- Risk(Conf(Silver Pavilion history))= 4.8
-Risk(Ans(Silver Pavilion history; In1485)) = 9.7
. . .
* Rejection
- Risk(Rej) = 9.0
?
Response: Conf (Silver Pavilion history)
?Do you want to know the history of the Silver
Pavilion??
? ?
Figure 1: Example of calculating Bayes risk
Figure 1 shows an example of calculating a
Bayes risk (where FR = 6, Rwd
Ret
= 5,
Rwd
QA
= 40). In this example, an appropriate
document is retrieved by incorporating the previ-
ous user query. However, since the answer to the
user?s question does not exist in the knowledge
base, the score of answer extraction is low. There-
fore, the system chooses a confirmation before pre-
senting the entire document.
4 Experimental Evaluation by Cross
Validation
We have evaluated the proposed method using the
user utterances collected in the ?Dialogue Navi-
gator for Kyoto City? field trial. We transcribed
in-domain 1,416 utterances (1,084 queries and
332 questions) and labeled their correct docu-
ments/NEs by hand.
The evaluation measures we used were the suc-
cess rate and the average number of sentences for
information access. We regard a retrieval as suc-
cessful if the system presents (or confirms) the ap-
propriate document/NE for the query. The num-
ber of sentences for information access is used as
an approximation of extraneous time before ac-
cessing the document/NE. That is, it is 1 {user
utterance} if the system presents the requested
document without a confirmation. If the system
makes a confirmation before presentation, it is 3
61
{user utterance + system?s confirmation + user?s
approval}, and that for presenting an incorrect doc-
ument is 15 {user utterance + improper presenta-
tion (3 = # presented sentences) + user?s correction
+ system?s apology + request for rephrasing + ad-
ditional sentences for task completion} (FR = 6 &
AddSent = 8), which are determined based on the
typical recovery pattern observed in the field trial.
We determined the value of the parameters by a
2-fold cross validation by splitting the test set into
two (set-1 & set-2), that is, set-1 was used as a de-
velopment set to estimate FR and Rwd for eval-
uating set-2, and vice versa. The parameters were
tuned to minimize the total number of sentences
for information access in the development set. We
compared the proposed method with the following
conventional methods. Note that method 1 is the
baseline method and method 2 was adopted in the
original ?Dialogue Navigator for Kyoto City? and
used in the field trial.
Method 1 (baseline)
? Make a search query using the 1st hypothesis
of ASR.
? Incorporate the contextual information related
to the current topic.
? Make a confirmation when the ASR confi-
dence of the pre-defined topic word is low.
? Answer the question when the user query is
judged a question.
Method 2 (original system)
? Make a search query using all nouns in the 1st-
3rd hypotheses of ASR.
? The other conditions are the same as in method
1.
The comparisons to these conventional methods
are shown in Table 1. The improvement compared
with that in baseline method 1 is 6.4% in the re-
sponse success rate and 0.78 of a sentence in the
number of sentences for information access.
A breakdown of the selected response candi-
dates by the proposed method is shown in Table
2. Many of the responses were generated using a
single hypothesis from the N-best list of ASR. The
result confirms that the correct hypothesis may not
be the first one, and the proposed method selects
the appropriate one by considering the likelihood
of retrieval. Most of the confirmations were gen-
erated using the 1st hypothesis of ASR. The An-
swers to questions were often generated from the
search queries with contextual information. This
Table 1: Comparison with conventional methods
Success rate # sentences
for presentation
Method 1 (baseline) 59.2% 5.49
Method 2 63.4% 4.98
Proposed method 65.6% 4.71
Table 2: Breakdown of selected candidates
w/o context with context
Pres Conf Ans Pres Conf Ans
1st hyp. 233 134 65 2 151 2
2nd hyp. 140 43 28 2 2 6
3rd hyp. 209 50 46 1 6 5
merge all 75 11 3 18 0 91
rejection 111
result suggests that when users used anaphoric ex-
pressions, the appropriate contextual information
was incorporated into the question.
5 Conclusion
We have proposed a dialogue framework to gener-
ate an optimal response. Specifically, the choices
in response generation are optimized as a mini-
mization of the Bayes risk based on the reward for
a correct information presentation and a penalty
for redundant time. Experimental evaluations us-
ing real user utterances were used to demonstrate
that the proposed method achieved a higher suc-
cess rate for information access with a reduced
number of sentences. Although we implemented
only a simple confirmation using the likelihood of
retrieval, the proposed method is expected to han-
dle more complex dialogue management such as
the confirmation considering the impact for the re-
trieval (Misu and Kawahara, 2006).
References
Chang, E., F. Seide, H. Meng, Z. Chen, Y. Shi, and Y. Li.
2002. A System for Spoken Query Information Retrieval
on Mobile Devices. IEEE Trans. on Speech and Audio
Processing, 10(8):531?541.
Levin, E. and R. Pieraccini. 2006. Value-based Optimal De-
cision for Dialog Systems. In Proc. Spoken Laguage Tech-
nology Workshop (SLT), pages 198?201.
Misu, T. and T. Kawahara. 2006. Dialogue strategy to clarify
user?s queries for document retrieval system with speech
interface. Speech Communication, 48(9):1137?1150.
Misu, T. and T. Kawahara. 2007. Speech-based interactive
information guidance system using question-answering
technique. In Proc. ICASSP.
Raymond, C., Y. Esteve, F. Bechet, R. De Mori, and
G. Damnati. 2003. Belief Confirmation in Spoken Di-
alog Systems using Confidence Measures. In Proc. Au-
tomatic Speech Recognition and Understanding Workshop
(ASRU).
62
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 1003?1010, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Speech-based Information Retrieval System
with Clarification Dialogue Strategy
Teruhisa Misu Tatsuya Kawahara
School of informatics
Kyoto University
Sakyo-ku, Kyoto, Japan
misu@ar.media.kyoto-u.ac.jp
Abstract
This paper addresses a dialogue strategy
to clarify and constrain the queries for
speech-driven document retrieval systems.
In spoken dialogue interfaces, users often
make utterances before the query is com-
pletely generated in their mind; thus input
queries are often vague or fragmental. As
a result, usually many items are matched.
We propose an efficient dialogue frame-
work, where the system dynamically se-
lects an optimal question based on infor-
mation gain (IG), which represents reduc-
tion of matched items. A set of possible
questions is prepared using various knowl-
edge sources. As a bottom-up knowl-
edge source, we extract a list of words
that can take a number of objects and po-
tentially causes ambiguity, using a depen-
dency structure analysis of the document
texts. This is complemented by top-down
knowledge sources of metadata and hand-
crafted questions. An experimental evalu-
ation showed that the method significantly
improved the success rate of retrieval, and
all categories of the prepared questions
contributed to the improvement.
1 Introduction
The target of spoken dialogue systems is being ex-
tended from simple databases such as flight informa-
tion (Levin et al, 2000; Potamianos et al, 2000) to
general documents (Fujii and Itou, 2003) including
newspaper articles (Chang et al, 2002; Hori et al,
2003). In such systems, the automatic speech recog-
nition (ASR) result of the user utterance is matched
against a set of target documents using the vector
space model, and documents with high matching
scores are presented to the user.
In this kind of document retrieval systems, user
queries must include sufficient information to iden-
tify the desired documents. In conventional doc-
ument query tasks with typed-text input, such as
TREC QA Track (NIST and DARPA, 2003), queries
are (supposed to be) definite and specific. However,
this is not the case when speech input is adopted.
The speech interface makes input easier. However,
this also means that users can start utterances before
queries are thoroughly formed in their mind. There-
fore, input queries are often vague or fragmental,
and sentences may be ill-formed or ungrammatical.
Moreover, important information may be lost due to
ASR errors. In such cases, an enormous list of possi-
ble relevant documents is usually obtained because
there is very limited information that can be used
as clues for retrieval. Therefore, it is necessary to
narrow down the documents by clarifying the user?s
intention through a dialogue.
There have been several studies on the follow-up
dialogue, and most of these studies assume that the
target knowledge base has a well-defined structure.
For example, Denecke (Denecke and Waibel, 1997)
addressed a method to generate guiding questions
based on a tree structure constructed by unifying
pre-defined keywords and semantic slots. However,
these approaches are not applicable to general docu-
1003
Query utterance
System User
Automatic speech
recognition (ASR)
Confirmation
for robust
retrieval 
Confirmation
Reply
Matching
with
Knowledge
base (KB)
Knowledge
base
(KB)
Dialogue to 
narrow down
retrieved
documents
Question
Reply
Final resultpresented in this paper
Figure 1: System overview
ment sets without such structures.
In this paper, we propose a dialogue strategy to
clarify the user?s query and constrain the retrieval
for a large-scale text knowledge base, which does
not have a structure nor any semantic slots. In the
proposed scheme, the system dynamically selects an
optimal question, which can reduce the number of
matched items most efficiently. As a criterion of
efficiency of the questions, information gain (IG)
is defined. A set of possible questions is prepared
using bottom-up and top-down knowledge sources.
As a bottom-up knowledge source, we conduct de-
pendency structure analysis of the document texts,
and extract a list of words that can take a number
of objects, thus potentially causing ambiguity. This
is combined with top-down knowledge sources of
metadata and hand-crafted questions. The system
then updates the query sentence using the user?s re-
ply to the question, so as to generate a confirmation
to the user.
2 Document retrieval system for
large-scale knowledge base
2.1 System overview
We have studied a dialogue framework to overcome
the problems in speech-based document retrieval
systems. In the framework, the system can han-
dle three types of problems caused by speech input:
ASR errors, redundancy in spoken language expres-
sion, and vagueness of queries. First, the system re-
alizes robust retrieval against ASR errors and redun-
Table 1: Document set (Knowledge Base: KB)
Text collection # documents text size(byte)
glossary 4,707 1.4M
FAQ 11,306 12M
DB of support articles 23,323 44M
dancies by detecting and confirming them. Then, the
system makes questions to clarify the user?s query
and narrow down the retrieved documents.
The system flow of these processes is summarized
below and also shown in Figure 1.
1. Recognize the user?s query utterance.
2. Make confirmation for phrases which may in-
clude critical ASR errors.
3. Retrieve from knowledge base (KB).
4. Ask possible questions to the user and narrow
down the matched documents.
5. Output the retrieval results.
In this paper, we focus on the latter stage of the
proposed framework, and present a clarification dia-
logue strategy to narrow down documents.
2.2 Task and back-end retrieval system
Our task involves text retrieval from a large-scale
knowledge base. For the target domain, we adopt a
software support knowledge base (KB) provided by
Microsoft Corporation. The knowledge base con-
sists of the following three kinds: glossary, fre-
quently asked questions (FAQ), and support articles.
The specification is listed in Table 1, and there are
about 40K documents in total. An example of sup-
port article is shown in Figure 2.
Dialog Navigator (Kiyota et al, 2002) has been
developed at University of Tokyo as a retrieval sys-
tem for this KB. The system accepts a typed-text in-
put from users and outputs a result of the retrieval.
The system interprets an input sentence by taking
syntactic dependency and synonymous expression
into consideration for matching it with the KB. The
target of the matching is the summaries and detail
information in the support articles, and the titles of
the Glossary and FAQ. The retrieved result is dis-
played to the user as the list of documents like Web
1004
? ?
HOWTO:
Use Speech Recognition in Windows XP
The information in this article applies to:
? Microsoft Windows XP Professional
? Microsoft Windows XP Home Edition
Summary: This article describes how to use speech
recognition in Windows XP. If you installed speech
recognition with Microsoft Office XP, or if you pur-
chased a new computer that has Office XP installed,
you can use speech recognition in all Office pro-
grams as well as other programs for which it is en-
abled.
Detail information: Speech recognition enables the op-
erating system to convert spoken words to written
text. An internal driver, called a speech recognition
engine, recognizes words and converts them to text.
The speech recognition engine ...
? ?
Figure 2: Example of software support article
search engines. Since the user has to read detail
information of the retrieved documents by clicking
their icons one by one, the number of items in the
final result is restricted to about 15.
In this work, we adopt Dialog Navigator as a
back-end system and construct a spoken dialogue in-
terface.
3 Dialogue strategy to clarify user?s vague
queries
3.1 Dialogue strategy based on information
gain (IG)
In the proposed clarification dialogue strategy, the
system asks optimal questions to constrain the given
retrieval results and help users find the intended
ones. Questions are dynamically generated by se-
lecting from a pool of possible candidates that sat-
isfy the precondition. The information gain (IG)
is defined as a criterion for the selection. The IG
represents a reduction of entropy, or how many re-
trieved documents can be eliminated by incorpo-
rating additional information (a reply to a question
in this case). Its computation is straightforward if
the question classifies the document set in a com-
pletely disjointed manner. However, the retrieved
documents may belong to two or more categories for
some questions, or may not belong to any category.
For example, some documents in our KB are related
with multiple versions of MS-Office, but others may
be irrelevant to any of them. Moreover, the match-
ing score of the retrieved documents should be taken
into account in this computation. Therefore, we de-
fine IG H(S) for a candidate question S by the fol-
lowing equations.
H(S) = ?
n
?
i=0
P (i) ? log P (i)
P (i) =
|C
i
|
?
n
i=0
|C
i
|
|C
i
| =
?
D
k
?i
CM(D
k
)
Here, D
k
denotes the k-th retrieved document by
matching the query to the KB, and CM(D) denotes
the matching score of document D. Thus, C
i
rep-
resents the number of documents classified into cat-
egory i by candidate question S, which is weighted
with the matching score. The documents that are not
related to any category are classified as category 0.
The system flow incorporating this strategy is
summarized below and also shown in Figure 3.
1. For a query sentence, retrieve from KB.
2. Calculate IG for all possible candidate ques-
tions which satisfy precondition.
3. Select the question with the largest IG (larger
than a threshold), and ask the question to the
user. Otherwise, output the current retrieval re-
sult.
4. Update the query sentence using the user?s re-
ply to the question.
5. Return to 1.
This procedure is explained in detail in the fol-
lowing sections.
3.2 Question generation based on bottom-up
and top-down knowledge sources
We prepare a pool of questions using three methods
based on bottom-up knowledge together with top-
down knowledge of KB. For a bottom-up knowledge
1005
System User
Matching with 
knowledge base (KB)
Knowledge base
(KB)
Any question 
with large IG? NO
Retrieval result
YES
Select question with 
largest IG for clarification
Question
Update 
query sentence Reply
Question pool
Figure 3: Overview of query clarification
Table 2: Examples of candidate questions (Dependency structure analysis: method 1)
Question Precondition Ratio of IG
applicable doc.
What did you delete? Query sentence includes ?delete? 2.15 (%) 7.44
What did you install? Query sentence includes ?install? 3.17 (%) 6.00
What did you insert? Query sentence includes ?insert? 1.12 (%) 7.12
What did you save? Query sentence includes ?save? 1.81 (%) 6.89
What is the file type? Query sentence includes ?file? 0.94 (%) 6.00
What did you setup? Query sentence includes ?setup? 0.69 (%) 6.45
source, we conducted a dependency structure anal-
ysis on KB. As for top-down knowledge, we make
use of metadata included in KB and human knowl-
edge.
3.2.1 Questions based on dependency structure
analysis (method 1)
This type of question is intended to clarify the
modifier or object of some words, based on de-
pendency structure analysis, when they are uncer-
tain. For instance, the verb ?delete? can have var-
ious objects such as ?application program? or ?ad-
dress book?. Therefore, the query can be clarified by
identifying such objects if they are missing. How-
ever, not all words need to be confirmed because the
modifier or object can be identified almost uniquely
for some words. For instance, the object of the
word ?shutdown? is ?computer? in most cases in this
task domain. It is tedious to identify the object of
such words. We therefore determine the words to be
confirmed by calculating entropy for modifier-head
pairs from the text corpus. The procedure is as fol-
lows.
1. Extract all modifier-head pairs from the text of
KB and query sentences (typed input) to an-
other retrieval system1 provided by Microsoft
Japan.
2. Calculate entropy H(m) for every word based
on probability P (i). This P (i) is calculated
with the occurrence count N(m) of word m
that appears in the text corpus and the count
N(i, m) of word m whose modifier is i.
H(m) = ?
?
i
P (i) ? log P (i)
P (i) =
N(i, m)
N(m)
1http://www.microsoft.com/japan/enable/nlsearch/
1006
Table 3: Examples of candidate questions (Metadata: method 2)
Question Precondition Ratio of IG
applicable doc.
What is the version None 30.03 (%) 2.63
of your Windows?
What is your application? None 30.28 (%) 2.31
What is the version Query sentence includes ?Word? 3.76 (%) 2.71
of your Word?
What is the version Query sentence includes ?Excel? 4.13 (%) 2.44
of your Excel?
Table 4: List of candidate questions (Human knowledge: method 3)
Question Precondition Ratio of IG
applicable doc.
When did the symptom occur? None 15.40 (%) 8.08
Tell me the error message. Query sentence includes ?error? 2.63 (%) 8.61
What do you concretely None 6.98 (%) 8.04
want to do?
As a result, we selected 40 words that have a large
value of entropy. Question sentences for these words
were generated with a template of ?What did you
...?? and unnatural ones were corrected manually.
Categories for IG calculation are defined by objects
of these words included in matched documents. The
system can make question using this method when
these words are included in the user?s query. Ta-
ble 2 lists examples of candidate questions using this
method. In this table, ratio of applicable document
corresponds to the ratio of documents that include
the words selected above, and IG is calculated using
applicable documents.
3.2.2 Questions based on metadata included in
KB (method 2)
We also prepare candidate questions using the
metadata attached to the KB. In general large-scale
KBs, metadata is usually attached to manage them
efficiently. For example, category information is at-
tached to newspaper articles and books in libraries.
In our target KB, a number of documents include
metadata of product names to which the document
applies. The system can generate question to which
the user?s query corresponds using this metadata.
However, some documents are related with multiple
versions, or may not belong to any category. There-
fore, the performance of these questions greatly de-
pends on the characteristics of the metadata.
Fourteen candidate questions are prepared using
this method. Example of candidate questions are
listed in Table 3. Ratio of applicable document cor-
responds to the ratio of documents that have meta-
data of target products.
3.2.3 Questions based on human knowledge
(method 3)
Software support is conventionally provided by
operators at call centers. We therefore prepare can-
didate questions based on the human knowledge that
has been accumulated there. This time, three kinds
of questions are hand-crafted. For instance, the
question ?When did the symptom occur?? tries to
capture key information to identify relevant docu-
ments. The categories for IG caluclation are defined
using hand-crafted rules by focusing on key-phrases
such as ?after ...? or ?during ...?. Candidate ques-
tions are listed in Table 4.
An example dialogue where the system asks ques-
tions based on IG is in Figure 4.
3.3 Update of retrieval query sentence
Through the dialogue to clarify the user?s query,
the system updates the query sentence using the
user?s reply to the question. Our backend informa-
tion retrieval system does not adopt simple ?bag-
1007
S1: What is your problem?
U1: Too garbled to read.
(Retrieval results):
? ?
1. Close button and maximize button are garbled.
2. Characters are garbled in Outlook Today.
3. Characters are garbled while inserting Japanese
text.
4. VB application is garbled to read.
? ? ?
? ?
(Calculate IG)
? Candidate question 1:
What is garbled to read? ? IG 5.27
? Candidate question 2:
What is the version of your Windows? ? IG 1.43
? Candidate question 3:
When did the symptom occur? ? IG 2.47
? ? ?
S2: (Select question with largest IG)
What is garbled to read?
U2: Characters on window button.
S3: (Update query sentence)
Retrieving with ?Characters on window button are too garbled
to read?.
Figure 4: Example dialogue
of-words? model, but conducts a more precise de-
pendency structure analysis for matching; therefore
forming an appropriate query sentence is desirable
rather than simply adding keywords. Moreover, it is
more comprehensible to the user to present the up-
dated query sentence than to show the sequence of
ASR results. Here, the update rules of the query sen-
tence are prepared as follows.
1. Questions based on dependency structure anal-
ysis
The user?s reply is added immediately before
of after the word that is the reply?s modifying
head. For instance, the reply to the question
?What did you delete?? is inserted right after
the word ?delete? in the query sentence.
2. Questions based on metadata of KB
Phrases ?In {Product name} {version name}?
are added to the query sentence.
3. Questions based on human knowledge
The position where the user?s reply is inserted
is specified beforehand for each question can-
didate. For instance, the reply to the question
?Tell me the error message.? is inserted right
after the word ?error? in the query sentence.
A dialogue example where the system updates the
user?s query is shown in Figure 5. In the exam-
ple, the systemmakes confirmation ?Retrieving with
?When I try to open it in explorer, I cannot open Ex-
cel 2002 file? ? at the end of the dialogue before pre-
senting the actual retrieval result.
3.4 Experimental evaluation
We implemented and evaluated the proposed
method. We collected a test data by 14 subjects who
had not used our system Each subject was requested
to retrieve support articles for 14 tasks, which con-
sisted of prepared scenarios (query sentences were
not given). The subjects were allowed to utter a
query again up to twice per task if they thought an
adequate retrieval result was not obtained. As a re-
sult, we collected 238 utterances for 196 (=14 ? 14)
tasks in total. An example of scenario and user ut-
terances are shown in Figure 6. The average word
accuracy of ASR was 82.9%. The threshold value in
IG that the system makes a question is set to 1.0 ini-
1008
S1: What is your problem?
U1: I cannot open the file.
S2: What is the file type? (method 1)
U2: Excel file.
(Update query sentence): ?I cannot open Excel file.?
S3: What is the version of your Excel? (method 2)
U3: My Excel is version 2002.
(Update query sentence): ?I cannot open Excel 2002 file.?
S4: When did the symptom occur? (method 3)
U4: Tried to open it in explorer.
S5: Retrieving with ?When I try to open it in explorer, I cannot
open Excel 2002 file?.
Figure 5: Query sentence update using user?s reply
? ?
? An example of scenario
You are looking for restaurant in Kyoto using
WWW. You have found a nice restaurant and tried
to print out an image of the map showing the restau-
rant. However, it is not printed out. (Your browser
is IE 6.0)
? Examples of users? utterance
? I want to print an image of map.
? I can?t print out.
? I failed to print a picture in homepage using
IE.
? Please tell me how to print out an image.
? ?
Figure 6: Example of scenario and user utterances
tially, and incremented by 0.3 every time the system
generates a question through a dialogue session.
First, we evaluated the success rate of retrieval.
We regarded a retrieval as successful when the re-
trieval result contained a correct document entry for
the scenario. We compared the following cases.
1. Transcript: A correct transcript of the user ut-
terance, prepared manually, was used as an in-
put.
2. ASR result (baseline): The ASR result was
used as an input.
3. Proposed method (log data): The system gener-
ated questions based on the proposed method,
and the user replied to them as he/she thought
appropriate.
We also evaluated the proposed method by simu-
lation in order to confirm its theoretical effect. Var-
ious factors of the entire system might influence the
performance in real dialogue which is evaluated by
the log data. Specifically, the users might not have
answered the questions appropriately, or the replies
might not have been correctly recognized. There-
fore, we also evaluated with the following condition.
4. Proposed method (simulation): The system
generated questions based on the proposed
method, and appropriate answers were given
manually.
Table 5 lists the retrieval success rate and the rank of
the correct document in the retrieval result, by these
cases. The proposed method achieved a better suc-
cess rate than when the ASR result was used. An
improvement of 12.6% was achieved in the simula-
tion case, and 7.7% by the log data. These figures
demonstrate the effectiveness of the proposed ap-
proach. The success rate of the retrieval was about
5% higher in the simulation case than the log data.
This difference is considered to be caused by follow-
ing factors.
1. ASR errors in user?s uttered replies
In the proposed strategy, the retrieval sentence
is updated using the user?s reply to the question
regardless of ASR errors. Even when the user
notices the ASR errors, he/she cannot correct
them. Although it is possible to confirm them
using ASR confidence measures, it makes di-
alogue more complicated. Hence, it was not
implemented this time.
2. User?s misunderstanding of the system?s ques-
tions
Users sometimes misunderstood the system?s
questions. For instance, to the system question
?When did the symptom occur??, some user
1009
Table 5: Success rate and average rank of correct
document in retrieval
Success Rank of
rate correct doc.
Transcript 76.1% 7.20
ASR result (baseline) 70.7% 7.45
Proposed method 78.4% 4.40(log data)
Proposed method 83.3% 3.85(simulation)
Table 6: Comparison of question methods
Success # generated
rate questions(per dialogue)
ASR result (baseline) 70.7% ?
Dependency structure 74.5% 0.38analysis (method 1)
Metadata (method 2) 75.7% 0.89
Human knowledge 74.5% 0.97(method 3)
All methods 83.3% 2.24(method 1-3)
replied simply ?just now? instead of key infor-
mation for the retrieval. To this problem, it may
be necessary to make more specific questions
or to display reply examples.
We also evaluated the efficiency of the individual
methods. In this experiment, each of the three meth-
ods was used to generate questions. The results are
in Table 6. The improvement rate by the three meth-
ods did not differ very much, and most significant
improvement was obtained by using the three meth-
ods together. While the questions based on human
knowledge are rather general and were used more
often, the questions based on the dependency struc-
ture analysis are specific, and thus more effective
when applicable. Hence, the questions based on the
dependency structure analysis (method 1) obtained
a relatively high improvement rate per question.
4 Conclusion
We proposed a dialogue strategy to clarify user?
queries for document retrieval tasks. Candidate
questions are prepared based on the dependency
structure analysis of the KB together with KB meta-
data and human knowledge. The system selects an
optimal question based on information gain (IG).
Then, the query sentence is updated using the user?s
reply. An experimental evaluation showed that the
proposed method significantly improved the success
rate of retrieval, and all categories of the prepared
questions contributed to the improvement.
The proposed approach is intended for restricted
domains, where all KB documents and several
knowledge sources are available, and it is not ap-
plicable to open-domain information retrieval such
as Web search. We believe, however, that there are
many targets of information retrieval in restricted
domains, for example, manuals of electric appli-
ances and medical documents for expert systems.
The methodology proposed here is not so dependent
on the domains, thus applicable to many other tasks
of this category.
5 Acknowledgements
The authors are grateful to Prof. Kurohashi and Dr.
Kiyota at University of Tokyo and Dr. Komatani at
Kyoto University for their helpful advice.
References
E. Chang, F. Seide, H. M. Meng, Z. Chen, Y. Shi, and Y. C. Li.
2002. A system for spoken query information retrieval on
mobile devices. IEEE Trans. on Speech and Audio Process-
ing, 10(8):531?541.
M. Denecke and A. Waibel. 1997. Dialogue strategies guid-
ing users to their communicative goals. In Proc. EU-
ROSPEECH.
A. Fujii and K. Itou. 2003. Building a test collection for
speech-driven Web retrieval. In Proc. EUROSPEECH.
C. Hori, T. Hori, H. Isozaki, E. Maeda, S. Katagiri, and S. Furui.
2003. Deriving disambiguous queries in a spoken interactive
ODQA system. In Proc. IEEE-ICASSP.
Y. Kiyota, S. Kurohashi, and F. Kido. 2002. ?Dialog Nav-
igator?: A question answering system based on large text
knowledge base. In Proc. COLING, pages 460?466.
E. Levin, S. Narayanan, R. Pieraccini, K. Biatov, E. Bocchieri,
G. Di Fabbrizio, W. Eckert, S. Lee, A. Pokrovsky, M. Rahim,
P. Ruscitti, and M. Walker. 2000. The AT&T-DARPA Com-
municator mixed-initiative spoken dialogue system. In Proc.
ICSLP.
NIST and DARPA. 2003. The twelfth Text REtrieval Confer-
ence (TREC 2003). In NIST Special Publication SP 500?
255.
A. Potamianos, E. Ammicht, and H.-K. J. Kuo. 2000. Dia-
logue management in the Bell labs Communicator system.
In Proc. ICSLP.
1010
Flexible Guidance Generation using
User Model in Spoken Dialogue Systems
Kazunori Komatani Shinichi Ueno Tatsuya Kawahara Hiroshi G. Okuno
Graduate School of Informatics
Kyoto University
Yoshida-Hommachi, Sakyo, Kyoto 606-8501, Japan
fkomatani,ueno,kawahara,okunog@kuis.kyoto-u.ac.jp
Abstract
We address appropriate user modeling in
order to generate cooperative responses to
each user in spoken dialogue systems. Un-
like previous studies that focus on user?s
knowledge or typical kinds of users, the
user model we propose is more compre-
hensive. Specifically, we set up three di-
mensions of user models: skill level to
the system, knowledge level on the tar-
get domain and the degree of hastiness.
Moreover, the models are automatically
derived by decision tree learning using
real dialogue data collected by the sys-
tem. We obtained reasonable classifica-
tion accuracy for all dimensions. Dia-
logue strategies based on the user model-
ing are implemented in Kyoto city bus in-
formation system that has been developed
at our laboratory. Experimental evalua-
tion shows that the cooperative responses
adaptive to individual users serve as good
guidance for novice users without increas-
ing the dialogue duration for skilled users.
1 Introduction
A spoken dialogue system is one of the promising
applications of the speech recognition and natural
language understanding technologies. A typical task
of spoken dialogue systems is database retrieval.
Some IVR (interactive voice response) systems us-
ing the speech recognition technology are being put
into practical use as its simplest form. According to
the spread of cellular phones, spoken dialogue sys-
tems via telephone enable us to obtain information
from various places without any other special appa-
ratuses.
However, the speech interface involves two in-
evitable problems: one is speech recognition er-
rors, and the other is that much information can-
not be conveyed at once in speech communications.
Therefore, the dialogue strategies, which determine
when to make guidance and what the system should
tell to the user, are the essential factors. To cope
with speech recognition errors, several confirma-
tion strategies have been proposed: confirmation
management methods based on confidence measures
of speech recognition results (Komatani and Kawa-
hara, 2000; Hazen et al, 2000) and implicit con-
firmation that includes previous recognition results
into system?s prompts (Sturm et al, 1999). In terms
of determining what to say to the user, several stud-
ies have been done not only to output answers cor-
responding to user?s questions but also to generate
cooperative responses (Sadek, 1999). Furthermore,
methods have also been proposed to change the di-
alogue initiative based on various cues (Litman and
Pan, 2000; Chu-Carroll, 2000; Lamel et al, 1999).
Nevertheless, whether a particular response is co-
operative or not depends on individual user?s char-
acteristics. For example, when a user says nothing,
the appropriate response should be different whether
he/she is not accustomed to using the spoken dia-
logue systems or he/she does not know much about
the target domain. Unless we detect the cause of the
silence, the system may fall into the same situation
repeatedly.
In order to adapt the system?s behavior to individ-
ual users, it is necessary to model the user?s patterns
(Kass and Finin, 1988). Most of conventional stud-
ies on user models have focused on the knowledge
of users. Others tried to infer and utilize user?s goals
to generate responses adapted to the user (van Beek,
1987; Paris, 1988). Elzer et al (2000) proposed a
method to generate adaptive suggestions according
to users? preferences.
However, these studies depend on knowledge of
the target domain greatly, and therefore the user
models need to be deliberated manually to be ap-
plied to new domains. Moreover, they assumed that
the input is text only, which does not contain errors.
On the other hand, spoken utterances include various
information such as the interval between utterances,
the presence of barge-in and so on, which can be
utilized to judge the user?s character. These features
also possess generality in spoken dialogue systems
because they are not dependent on domain-specific
knowledge.
We propose more comprehensive user models to
generate user-adapted responses in spoken dialogue
systems taking account of all available information
specific to spoken dialogue. The models change
both the dialogue initiative and the generated re-
sponse. In (Eckert et al, 1997), typical users? be-
haviors are defined to evaluate spoken dialogue sys-
tems by simulation, and stereotypes of users are as-
sumed such as patient, submissive and experienced.
We introduce user models not for defining users? be-
haviors beforehand, but for detecting users? patterns
in real-time interaction.
We define three dimensions in the user models:
?skill level to the system?, ?knowledge level on the
target domain? and ?degree of hastiness?. The for-
mer two are related to the strategies in manage-
ment of the initiative and the response generation.
These two enable the system to adaptively gener-
ate dialogue management information and domain-
specific information, respectively. The last one is
used to manage the situation when users are in hurry.
Namely, it controls generation of the additive con-
tents based on the former two user models. Handling
such a situation becomes more crucial in speech
communications using cellular phones.
The user models are trained by decision tree
Sys: Please tell me your current bus stop, your destination
or the specific bus route.
User: Shijo-Kawaramachi.
Sys: Do you take a bus from Shijo-Kawaramachi?
User: Yes.
Sys: Where will you get off the bus?
User: Arashiyama.
Sys: Do you go from Shijo-Kawaramachi to Arashiyama?
User: Yes.
Sys: Bus number 11 bound for Arashiyama has departed
Sanjo-Keihanmae, two bus stops away.
Figure 1: Example dialogue of the bus system
learning algorithm using real data collected from the
Kyoto city bus information system. Then, we imple-
ment the user models and adaptive dialogue strate-
gies on the system and evaluate them using data col-
lected with 20 novice users.
2 Kyoto City Bus Information System
We have developed the Kyoto City Bus Information
System, which locates the bus a user wants to take,
and tells him/her how long it will take before its
arrival. The system can be accessed via telephone
including cellular phones1. From any places, users
can easily get the bus information that changes ev-
ery minute. Users are requested to input the bus stop
to get on, the destination, or the bus route number
by speech, and get the corresponding bus informa-
tion. The bus stops can be specified by the name of
famous places or public facilities nearby. Figure 1
shows a simple example of the dialogue.
Figure 2 shows an overview of the system.
The system operates by generating VoiceXML
scripts dynamically. The real-time bus information
database is provided on the Web, and can be ac-
cessed via Internet. Then, we explain the modules
in the following.
VWS (Voice Web Server)
The Voice Web Server drives the speech recog-
nition engine and the TTS (Text-To-Speech)
module according to the specifications by the
generated VoiceXML.
Speech Recognizer
The speech recognizer decodes user utterances
1+81-75-326-3116
           VWS
(Voice Web Server)
response
sentences
recognition results
(only language info.)
recognition results
(including features other
 than language info.) Voice
   XML
user
TTS speech
recognizer
VoiceXML
generator
dialogue
manageruser
profiles
real bus
information
user model
identifier
CGI
the system except for
proposed user models
Figure 2: Overview of the bus system with user
models
based on specified grammar rules and vocabu-
lary, which are defined by VoiceXML at each
dialogue state.
Dialogue Manager
The dialogue manager generates response sen-
tences based on speech recognition results (bus
stop names or a route number) received from
the VWS. If sufficient information to locate a
bus is obtained, it retrieves the corresponding
information from the real-time bus information
database.
VoiceXML Generator
This module dynamically generates VoiceXML
files that contain response sentences and spec-
ifications of speech recognition grammars,
which are given by the dialogue manager.
User Model Identifier
This module classifies user?s characters based
on the user models using features specific to
spoken dialogue as well as semantic attributes.
The obtained user profiles are sent to the dia-
logue manager, and are utilized in the dialogue
management and response generation.
3 Response Generation using User Models
3.1 Classification of User Models
We define three dimensions as user models listed be-
low.
 Skill level to the system
 Knowledge level on the target domain
 Degree of hastiness
Skill Level to the System
Since spoken dialogue systems are not
widespread yet, there arises a difference in the
skill level of users in operating the systems. It
is desirable that the system changes its behavior
including response generation and initiative man-
agement in accordance with the skill level of the
user. In conventional systems, a system-initiated
guidance has been invoked on the spur of the
moment either when the user says nothing or
when speech recognition is not successful. In our
framework, by modeling the skill level as the user?s
property, we address a radical solution for the
unskilled users.
Knowledge Level on the Target Domain
There also exists a difference in the knowledge
level on the target domain among users. Thus, it is
necessary for the system to change information to
present to users. For example, it is not cooperative
to tell too detailed information to strangers. On the
other hand, for inhabitants, it is useful to omit too
obvious information and to output additive informa-
tion. Therefore, we introduce a dimension that rep-
resents the knowledge level on the target domain.
Degree of Hastiness
In speech communications, it is more important
to present information promptly and concisely com-
pared with the other communication modes such as
browsing. Especially in the bus system, the concise-
ness is preferred because the bus information is ur-
gent to most users. Therefore, we also take account
of degree of hastiness of the user, and accordingly
change the system?s responses.
3.2 Response Generation Strategy using User
Models
Next, we describe the response generation strategies
adapted to individual users based on the proposed
user models: skill level, knowledge level and hasti-
ness. Basic design of dialogue management is based
on mixed-initiative dialogue, in which the system
makes follow-up questions and guidance if neces-
sary while allowing a user to utter freely. It is in-
vestigated to add various contents to the system re-
sponses as cooperative responses in (Sadek, 1999).
Such additive information is usually cooperative, but
some people may feel such a response redundant.
Thus, we introduce the user models and control
the generation of additive information. By introduc-
ing the proposed user models, the system changes
generated responses by the following two aspects:
dialogue procedure and contents of responses.
Dialogue Procedure
The dialogue procedure is changed based on the
skill level and the hastiness. If a user is identified as
having the high skill level, the dialogue management
is carried out in a user-initiated manner; namely, the
system generates only open-ended prompts. On the
other hand, when user?s skill level is detected as low,
the system takes an initiative and prompts necessary
items in order.
When the degree of hastiness is low, the system
makes confirmation on the input contents. Con-
versely, when the hastiness is detected as high, such
a confirmation procedure is omitted.
Contents of Responses
Information that should be included in the sys-
tem response can be classified into the following two
items.
1. Dialogue management information
2. Domain-specific information
The dialogue management information specifies
how to carry out the dialogue including the instruc-
tion on user?s expression like ?Please reply with ei-
ther yes or no.? and the explanation about the fol-
lowing dialogue procedure like ?Now I will ask in
order.? This dialogue management information is
determined by the user?s skill level to the system,
 58.8>=
the maximum number of filled slots
dialogue state
initial state otherwise
presense of barge-in
rate of no input
0.07>
30 1 2
average of
recognition score
58.8<
skill level
high
skill level
high
skill level
low
skill level
low
Figure 3: Decision tree for the skill level
and is added to system responses when the skill level
is considered as low.
The domain-specific information is generated ac-
cording to the user?s knowledge level on the target
domain. Namely, for users unacquainted with the
local information, the system adds the explanation
about the nearest bus stop, and omits complicated
contents such as a proposal of another route.
The contents described above are also controlled
by the hastiness. For users who are not in hurry, the
system generates the additional contents as cooper-
ative responses. On the other hand, for hasty users,
the contents are omitted in order to prevent the dia-
logue from being redundant.
3.3 Classification of User based on Decision
Tree
In order to implement the proposed user models as a
classifier, we adopt a decision tree. It is constructed
by decision tree learning algorithm C5.0 (Quinlan,
1993) with data collected by our dialogue system.
Figure 3 shows the derived decision tree for the skill
level.
We use the features listed in Figure 4. They in-
clude not only semantic information contained in the
utterances but also information specific to spoken
dialogue systems such as the silence duration prior
to the utterance and the presence of barge-in. Ex-
cept for the last category of Figure 4 including ?at-
tribute of specified bus stops?, most of the features
are domain-independent.
The classification of each dimension is done for
every user utterance except for knowledge level. The
model of a user can change during a dialogue. Fea-
tures extracted from utterances are accumulated as
history information during the session.
Figure 5 shows an example of the system behav-
 features obtained from a single utterance
? dialogue state (defined by already filled slots)
? presence of barge-in
? lapsed time of the current utterance
? recognition result (something recognized / un-
certain / no input)
? score of speech recognizer
? the number of filled slots by the current utter-
ance
 features obtained from the session
? the number of utterances
? dialogue state of the previous utterance
? lapsed time from the beginning of the session
? the number of repetitions of a same question
? the average number of repetitions of a same
question
? ratio of the total time of user utterances in
whole elapsed time
? ratio of the occurrence of barge-in out of the
whole number of utterances
? recognition result of the previous utterance
? ratio of something recognized
? ratio of getting uncertain results
? ratio of no input
? the number of barge-in
? the number of something recognized
? the number of getting uncertain results
? the number of no input
? average of recognition scores
? the maximum number of slots filled by a single
utterance
 features obtained from the session (used only in clas-
sification of knowledge level)
? attribute of specified bus stops
? the way to specify bus stops (whether a bus stop
is specified by its correct name or not)
? ratio of each attribute of specified bus stops
? ratio of each way of specifying bus stops
Figure 4: Features to classify the user models
ior with the proposed user models. The skill level is
classified as being low by the decision tree, because
the first user?s utterance includes only one content
word. Then, dialogue procedure is changed to the
system-initiated one. Similarly, the hastiness is clas-
sified as being low by the decision tree, and the sys-
tem includes the explanation on the dialogue pro-
cedure and instruction on the expression in the re-
sponses. They are omitted if the hastiness is identi-
fied as high.
3.4 Decision Tree Learning for User Models
We train and evaluate the decision tree for the user
models using dialogue data collected by our system.
The data was collected from December 10th 2001 to
case: [skill level: low][knowledge level: low]
[hastiness: low]
Sys: Please tell me your current bus stop, your destination
or the specific bus route number.
User: Maruyama Park.
Sys: Let me confirm one by one.
( The explanation is added by [skill level: low].)
Will you get on at Maruyama Park?
( The content is confirmed by [hastiness: low].)
Please reply with yes or no.
( The instruction is added by [skill level: low].)
User: Yes.
Sys: Where will you get off the bus? Please specify by a
bus stop name or a well-known landmark.
( The explanation is added by [skill level: low].)
User: Tofukuji temple.
Sys: Will you get off the bus at Tofukuji temple?
Please reply with yes or no.
User: Yes.
Sys: The following buses are located.
( The explanation is added by [skill level: low].)
The nearest bus stop to Maruyama Park is Gion.
( The domain information is added by [knowl-
edge level: low].)
Bus number 202 bound for Nishioji-Kujo has de-
parted Higashiyama-Sanjo, which is two stops away.
...
Figure 5: An example dialogue with the proposed
user models
low indeterminable high total
skill level 743 253 496 1492
knowledge level 275 808 409 1492
hastiness 421 932 139 1492
Table 1: Number of manually labeled items for de-
cision tree learning
May 10th 2002. The number of the sessions (tele-
phone calls) is 215, and the total number of utter-
ances included in the sessions is 1492. We anno-
tated the subjective labels by hand. The annotator
judges the user models for every utterances based
on recorded speech data and logs. The labels were
given to the three dimensions described in section
3.3 among ?high?, ?indeterminable? or ?low?. It is
possible that annotated models of a user change dur-
ing a dialogue, especially from ?indeterminable? to
?low? or ?high?. The number of labeled utterances is
shown in Table 1.
Using the labeled data, we evaluated the classi-
fication accuracy of the proposed user models. All
the experiments were carried out by the method of
10-fold cross validation. The process, in which one
tenth of all data is used as the test data and the re-
mainder is used as the training data, is repeated ten
times, and the average of the accuracy is computed.
The result is shown in Table 2. The conditions #1,
#2 and #3 in Table 2 are described as follows.
#1: The 10-fold cross validation is carried out per
utterance.
#2: The 10-fold cross validation is carried out per
session (call).
#3: We calculate the accuracy under more realis-
tic condition. The accuracy is calculated not
in three classes (high / indeterminable / low)
but in two classes that actually affect the dia-
logue strategies. For example, the accuracy for
the skill level is calculated for the two classes:
low and the others. As to the classification of
knowledge level, the accuracy is calculated for
dialogue sessions because the features such as
the attribute of a specified bus stop are not ob-
tained in every utterance. Moreover, in order
to smooth unbalanced distribution of the train-
ing data, a cost corresponding to the reciprocal
ratio of the number of samples in each class is
introduced. By the cost, the chance rate of two
classes becomes 50%.
The difference between condition #1 and #2 is that
the training was carried out in a speaker-closed or
speaker-open manner. The former shows better per-
formance.
The result in condition #3 shows useful accuracy
in the skill level. The following features play im-
portant part in the decision tree for the skill level:
the number of filled slots by the current utterance,
presence of barge-in and ratio of no input. For the
knowledge level, recognition result (something rec-
ognized / uncertain / no input), ratio of no input and
the way to specify bus stops (whether a bus stop is
specified by its exact name or not) are effective. The
hastiness is classified mainly by the three features:
presence of barge-in, ratio of no input and lapsed
time of the current utterance.
condition #1 #2 #3
skill level 80.8% 75.3% 85.6%
knowledge level 73.9% 63.7% 78.2%
hastiness 74.9% 73.7% 78.6%
Table 2: Classification accuracy of the proposed user
models
4 Experimental Evaluation of the System
with User Models
We evaluated the system with the proposed user
models using 20 novice subjects who had not used
the system. The experiment was performed in the
laboratory under adequate control. For the speech
input, the headset microphone was used.
4.1 Experiment Procedure
First, we explained the outline of the system to sub-
jects and gave the document in which experiment
conditions and the scenarios were described. We
prepared two sets of eight scenarios. Subjects were
requested to acquire the bus information using the
system with/without the user models. In the sce-
narios, neither the concrete names of bus stops nor
the bus number were given. For example, one of
the scenarios was as follows: ?You are in Kyoto
for sightseeing. After visiting the Ginkakuji temple,
you go to Maruyama Park. Supposing such a situa-
tion, please get information on the bus.? We also set
the constraint in order to vary the subjects? hastiness
such as ?Please hurry as much as possible in order
to save the charge of your cellular phone.?
The subjects were also told to look over question-
naire items before the experiment, and filled in them
after using each system. This aims to reduce the sub-
ject?s cognitive load and possible confusion due to
switching the systems (Over, 1999). The question-
naire consisted of eight items, for example, ?When
the dialogue did not go well, did the system guide in-
telligibly?? We set seven steps for evaluation about
each item, and the subject selected one of them.
Furthermore, subjects were asked to write down
the obtained information: the name of the bus stop
to get on, the bus number and how much time it
takes before the bus arrives. With this procedure,
we planned to make the experiment condition close
to the realistic one.
duration (sec.) # turn
group 1 with UM 51.9 4.03
(with UM w/o UM) w/o UM 47.1 4.18
group 2 w/o UM 85.4 8.23
(w/o UM with UM) with UM 46.7 4.08
UM: User Model
Table 3: Duration and the number of turns in dia-
logue
The subjects were divided into two groups; a half
(group 1) used the system in the order of ?with
user models  without user models?, the other half
(group 2) used in the reverse order.
The dialogue management in the system without
user models is also based on the mixed-initiative di-
alogue. The system generates follow-up questions
and guidance if necessary, but behaves in a fixed
manner. Namely, additive cooperative contents cor-
responding to skill level described in section 3.2 are
not generated and the dialogue procedure is changed
only after recognition errors occur. The system with-
out user models behaves equivalently to the initial
state of the user models: the hastiness is low, the
knowledge level is low and the skill level is high.
4.2 Results
All of the subjects successfully completed the given
task, although they had been allowed to give up if the
system did not work well. Namely, the task success
rate is 100%.
Average dialogue duration and the number of
turns in respective cases are shown in Table 3.
Though the users had not experienced the system at
all, they got accustomed to the system very rapidly.
Therefore, as shown in Table 3, both the duration
and the number of turns were decreased obviously
in the latter half of the experiment in either group.
However, in the initial half of the experiment, the
group 1 completed with significantly shorter dia-
logue than group 2. This means that the incorpora-
tion of the user models is effective for novice users.
Table 4 shows a ratio of utterances for which the
skill level was identified as high. The ratio is calcu-
lated by dividing the number of utterances that were
judged as high skill level by the number of all utter-
ances in the eight sessions. The ratio is much larger
for group 1 who initially used the system with user
group 1 with UM 0.72
(with UM  w/o UM) w/o UM 0.70
group 2 w/o UM 0.41
(w/o UM  with UM) with UM 0.63
Table 4: Ratio of utterances for which the skill level
was judged as high
models. This fact means that novice users got ac-
customed to the system more rapidly with the user
models, because they were instructed on the usage
by cooperative responses generated when the skill
level is low. The results demonstrate that coopera-
tive responses generated according to the proposed
user models can serve as good guidance for novice
users.
In the latter half of the experiment, the dialogue
duration and the number of turns were almost same
between the two groups. This result shows that the
proposed models prevent the dialogue from becom-
ing redundant for skilled users, although generating
cooperative responses for all users made the dia-
logue verbose in general. It suggests that the pro-
posed user models appropriately control the genera-
tion of cooperative responses by detecting characters
of individual users.
5 Conclusions
We have proposed and evaluated user models for
generating cooperative responses adaptively to in-
dividual users. The proposed user models consist
of the three dimensions: skill level to the system,
knowledge level on the target domain and the de-
gree of hastiness. The user models are identified us-
ing features specific to spoken dialogue systems as
well as semantic attributes. They are automatically
derived by decision tree learning, and all features
used for skill level and hastiness are independent of
domain-specific knowledge. So, it is expected that
the derived user models can be used in other do-
mains generally.
The experimental evaluation with 20 novice users
shows that the skill level of novice users was im-
proved more rapidly by incorporating the user mod-
els, and accordingly the dialogue duration becomes
shorter more immediately. The result is achieved
by the generated cooperative responses based on the
proposed user models. The proposed user models
also suppress the redundancy by changing the dia-
logue procedure and selecting contents of responses.
Thus, they realize user-adaptive dialogue strategies,
in which the generated cooperative responses serve
as good guidance for novice users without increas-
ing the dialogue duration for skilled users.
References
Jennifer Chu-Carroll. 2000. MIMIC: An adaptive
mixed initiative spoken dialogue system for informa-
tion queries. In Proc. of the 6th Conf. on applied Nat-
ural Language Processing, pages 97?104.
Wieland Eckert, Esther Levin, and Roberto Pieraccini.
1997. User modeling for spoken dialogue system eval-
uation. In Proc. IEEE Workshop on Automatic Speech
Recognition and Understanding, pages 80?87.
Stephanie Elzer, Jennifer Chu-Carroll, and Sandra Car-
berry. 2000. Recognizing and utilizing user prefer-
ences in collaborative consultation dialogues. In Proc.
of the 4th Int?l Conf. on User Modeling, pages 19?24.
Timothy J. Hazen, Theresa Burianek, Joseph Polifroni,
and Stephanie Seneff. 2000. Integrating recognition
confidence scoring with language understanding and
dialogue modeling. In Proc. ICSLP.
Robert Kass and Tim Finin. 1988. Modeling the user in
natural language systems. Computational Linguistics,
14(3):5?22.
Kazunori Komatani and Tatsuya Kawahara. 2000.
Flexible mixed-initiative dialogue management using
concept-level confidence measures of speech recog-
nizer output. In Proc. Int?l Conf. Computational Lin-
guistics (COLING), pages 467?473.
Lori Lamel, Sophie Rosset, Jean-Luc Gauvain, and Samir
Bennacef. 1999. The LIMSI ARISE system for
train travel information. In IEEE Int?l Conf. Acoust.,
Speech & Signal Process.
Diane J. Litman and Shimei Pan. 2000. Predicting and
adapting to poor speech recognition in a spoken dia-
logue system. In Proc. of the 17th National Confer-
ence on Artificial Intelligence (AAAI2000).
Paul Over. 1999. Trec-7 interactive track report. In Proc.
of the 7th Text REtrieval Conference (TREC7).
Cecile L. Paris. 1988. Tailoring object descriptions to
a user?s level of expertise. Computational Linguistics,
14(3):64?78.
J. Ross Quinlan. 1993. C4.5: Programs for Ma-
chine Learning. Morgan Kaufmann, San Mateo, CA.
http://www.rulequest.com/see5-info.html.
David Sadek. 1999. Design considerations on dia-
logue systems: From theory to technology -the case
of artimis-. In Proc. ESCA workshop on Interactive
Dialogue in Multi-Modal Systems.
Janienke Sturm, Els den Os, and Lou Boves. 1999. Is-
sues in spoken dialogue systems: Experiences with the
Dutch ARISE system. In Proc. ESCA workshop on In-
teractive Dialogue in Multi-Modal Systems.
Peter van Beek. 1987. A model for generating better
explanations. In Proc. of the 25th Annual Meeting of
the Association for Computational Linguistics (ACL-
87), pages 215?220.
Dialog Navigator : A Spoken Dialog Q-A System
based on Large Text Knowledge Base
Yoji Kiyota, Sadao Kurohashi (The University of Tokyo)
kiyota,kuro@kc.t.u-tokyo.ac.jp
Teruhisa Misu, Kazunori Komatani, Tatsuya Kawahara (Kyoto University)
misu,komatani,kawahara@kuis.kyoto-u.ac.jp
Fuyuko Kido (Microsoft Co., Ltd.)
fkido@microsoft.com
Abstract
This paper describes a spoken dialog Q-
A system as a substitution for call centers.
The system is capable of making dialogs
for both fixing speech recognition errors
and for clarifying vague questions, based
on only large text knowledge base. We in-
troduce two measures to make dialogs for
fixing recognition errors. An experimental
evaluation shows the advantages of these
measures.
1 Introduction
When we use personal computers, we often en-
counter troubles. We usually consult large manu-
als, experts, or call centers to solve such troubles.
However, these solutions have problems: it is diffi-
cult for beginners to retrieve a proper item in large
manuals; experts are not always near us; and call
centers are not always available. Furthermore, op-
eration cost of call centers is a big problem for en-
terprises. Therefore, we proposed a spoken dialog
Q-A system which substitute for call centers, based
on only large text knowledge base.
If we consult a call center, an operator will help
us through a dialog. The substitutable system also
needs to make a dialog. First, asking backs for fixing
speech recognition errors are needed. Note that too
many asking backs make the dialog inefficient. Sec-
ondly, asking backs for clarifying users? problems
are also needed, because they often do not know
their own problems so clearly.
To realize such asking backs, we developed a sys-
tem as shown in Figure 1. The features of our system
are as follows:
 Precise text retrieval.
The system precisely retrieves texts from large
confirmation using
confidence in recognition
confirmation using
significance for retrieval
automatic speech recognizer
(Julius)
speech input
confirmation for
significant parts
user?s selection
N-best candidates
(or reject all)
user?s selection
N-best candidates of speech recognition
asking back(s)
with dialog cards
description extraction
choices in
dialog cards
user?s selection
final result
retrieval result text
retrieval
systemuser
text
knowledge
base
dialog for clarifying
vague questions
dialog
cards
dialog for fixing
speech recognition
errors
Figure 1: Architecture.
text knowledge base provided by Microsoft
Corporation (Table 1), using question types,
products, synonymous expressions, and syntac-
tic information. Dialog cards which can cope
with very vague questions are also retrieved.
 Dialog for fixing speech recognition errors.
When accepting speech input, recognition er-
rors are inevitable. However, it is not obvi-
ous which portions of the utterance the sys-
tem should confirm by asking back to the user.
A great number of spoken dialog systems for
particular task domains, such as (Levin et al,
2000), solved this problem by defining slots,
but it is not applicable to large text knowledge
base. Therefore, we introduce two measures
of confidence in recognition and significance
for retrieval to make dialogs for fixing speech
recognition errors.
 Dialog for clarifying vague questions.
When a user asks a vague question such as
?An error has occurred?, the system navigates
him/her to the desired answer, asking him/her
back using both dialog cards and extraction of
Table 1: Text collections.
# of # of matching
text collection texts characters target
Glossary 4,707 700,000 entries
Help texts 11,306 6,000,000 titles
Support KB 23,323 22,000,000 entire texts
summaries that makes differences between re-
trieved texts more clear.
Our system makes asking backs by showing them
on a display, and users respond them by selecting
the displayed buttons by mouses.
Initially, we developed the system as a keyboard
based Q-A system, and started its service in April
2002 at the web site of Microsoft Corporation. The
extension for speech input was done based on the
one-year operation. Our system uses Julius (Lee et
al., 2001) as a Japanese speech recognizer, and it
uses language model acquired from the text knowl-
edge base of Microsoft Corporation.
In this paper, we describe the above three features
in Section 2, 3, and 4. After that, we show experi-
mental evaluation, and then conclude this paper.
2 Precise Text Retrieval
It is critical for a Q-A system to retrieve relevant
texts for a question precisely. In this section, we
describe the score calculation method, giving large
points to modifier-head relations between bunsetsu1
based on the parse results of KNP (Kurohashi and
Nagao, 1994), to improve precision of text retrieval.
Our system also uses question types, product names,
and synonymous expression dictionary as described
in (Kiyota et al, 2002).
First, scores of all sentences in each text are calcu-
lated as shown in Figure 2. Sentence score is the to-
tal points of matching keywords and modifier-head
relations. We give 1 point to a matching of a key-
word, and 2 points to a matching of a modifier-head
relation (these parameters were set experimentally).
Then sentence score is normalized by the maximum
matching score (MMS) of both sentences as follows
(the MMS is the sentence score with itself):
sentence score

the MMS of a
user question



the MMS of a
text sentence

1Bunsetsu is a commonly used linguistic unit in Japanese,
consisting of one or more adjoining content words and zero or
more following functional words.
Outlook
?Outlook?
tsukau
?use?
meru
?mail?
jushin
?receive?
Outlook wo tsukatte
meru wo jushin dekinai.
?I cannot receive mails using Outlook.?
Outlook
?Outlook?
?mail?
jushin
?receive?
error
?error?
Outlook de meru wo jushin
suru sai no error.
?An error while receiving mails
 using Outlook.?
+1
+1
+1
MMS8 10
user question text sentence
meru
+2
sentence score
= 5
	
  



 
Figure 2: Score calculation.
vague
concrete
Error ga hassei shita.
?An error has occurred.?
Komatte imasu.
?I have a problem.?
Windows 98 de kidouji ni
error ga hassei shita.
?An error has occurred
while booting Windows 98.?
text knowledge base
clarifying questions
using dialog cards
text retrieval &
description extraction
user
questions
Figure 3: User navigation.
Finally, the sentence that has the largest score in
each text is selected as the representative sentence of
the text. Then, the score of the sentence is regarded
as the score of the text.
3 Dialog Strategy for Clarifying Questions
In most cases, users? questions are vague. To cope
with such vagueness, our system uses the following
two methods: asking backs using dialog cards and
extraction of summaries that makes difference be-
tween retrieved texts more clear (Figure 3).
3.1 Dialog cards
If a question is very vague, it matches many texts,
so users have to pay their labor on finding a rele-
vant one. Our system navigates users to the desired
answer using dialog cards as shown in Figure 3.
We made about three hundred of dialog cards
to throw questions back to users. Figure 4 shows
two dialog cards. UQ (User Question) is fol-
lowed by a typical vague user question. If a user
question matches it, the dialog manager asks the
back question after SYS, showing choices be-
[Error]
UQ Error ga hassei suru
?An error occurs?
SYSError wa itsu hassei shimasuka?
?When does the error occurs??
SELECT
Windows kidou ji goto [Error/Booting Windows]
?while booting Windows?
in?satsu ji goto [Error/Printing Out]
?while printing out?
application kidou ji goto [Error/Launching Applications]
?while launching applications?
/SELECT
[Error/Booting Windows]
UQ Windows wo kidou ji ni error ga hassei suru
?An error occurs while booting Windows?
SYSAnata ga otsukai no Windows wo erande kudasai.
?Choose your Windows.?
SELECT
Windows 95 retrieve Windows 95 wo kidou ji ni error ga hassei suru
?An error occurs while booting Windows 95?
Windows 98 retrieve Windows 98 wo kidou ji ni error ga hassei suru
?An error occurs while booting Windows 98?
Windows ME retrieve Windows ME wo kidou ji ni error ga hassei suru
?An error occurs while booting Windows ME?
/SELECT
Figure 4: Dialog cards.
tween SELECT and /SELECT. Every choice is
followed by goto or retrieve. goto means that the
system follow the another dialog cards if this choice
is selected. retrieve means that the system retrieve
texts using the query specified there.
3.2 Description extraction from retrieved texts
In most cases, the neighborhood of the part that
matches the user question describes specific symp-
toms and conditions of the problem users encounter.
Our system extracts such descriptions from the re-
trieved texts as the summaries of them. The algo-
rithm is described in (Kiyota et al, 2002).
4 Dialog Strategy for Speech Input
It is necessary for a spoken dialog system to deter-
mine which portions of the speech input should be
confirmed. Moreover, criteria for judging whether
it should make confirmation or not are needed, be-
cause too many confirmations make the dialog inef-
ficient. Therefore, we introduce two criteria of con-
fidence in recognition and significance for retrieval.
Our system makes two types of asking backs for
fixing recognition errors (Figure 1). First, Julius out-
puts  -best candidates of speech recognition. Then,
the system makes confirmation for significant parts
based on confidence in recognition. After that, the
system retrieves relevant texts in the text knowledge
base using each candidate, and makes confirmation
based on significance for retrieval.
4.1 Confidence in recognition
We define the confidence in recognition for each
phrase in order to reject partial recognition errors. It
is calculated based on word perplexity, which is of-
ten used in order to evaluate suitability of language
models for test-set sentences. We adopt word per-
plexity because of the following reasons: incorrectly
recognized parts are often unnatural in context, and
words that are unnatural in context have high per-
plexity values.
As Julius uses trigram as its language model, the
word perplexity  is calculated as follows:
  




 



 


 s are summed up in each bunsetsu (phrases).
As a result, the system assigned the sum of  s
to each bunsetsu as the criterion for confidence in
recognition.
We preliminarily defined the set of product names
as significant phrases2. If the sums of  s for any
significant phrases are beyond the threshold (now,
we set it 50), the system makes confirmation for
these phrases.
4.2 Significance for retrieval
The system calculates significance for retrieval us-
ing  -best candidates of speech recognition. Be-
cause slight speech recognition errors are not harm-
ful for retrieval results, we regard a difference that
affects its retrieval result as significant. Namely,
when the difference between retrieval results for
each recognition candidate is large, we regard that
the difference is significant.
Significance for retrieval is defined as a rate
of disagreement of five high-scored retrieved texts
among  recognition candidates. For example, if
there is a substituted part in two recognition candi-
dates, and only one text is commonly retrieved out
of five high-scored texts by both candidates, the sig-
nificance for retrieval for the substituted part is 0.8
(   ).
The system makes confirmation which candidate
should be used, if significance for retrieval is beyond
the threshold (now, we set it 0.5).
2We are now developing a method to define the set of sig-
nificant phrases semi-automatically.
Table 2: Number of successful retrieval for each speaker.
speaker # of ASR transcription speech recognition with confidence with significance with both
ID utterances corr. (1) results (2) in recognition (3) for retrieval (4) measures (5)
A 13 87.8% 10/13 8/13 8/13 10/13 10/13
B 14 55.0% 6/14 3/14 5/14 3/14 5/14
C 13 61.7% 5/13 5/13 5/13 5/13 5/13
D 13 83.1% 11/13 9/13 9/13 9/13 9/13
total 53 68.5% 32/53 25/53 27/53 27/53 29/53
U: OS ? ME ???????????????????????????
????????????????????????????????
?????????????
?I am using (Windows) ME as OS(operating system), then ...?
ASR: OS ? IME ???????????????????????????
???????????????????????????
(?ME? was incorrectly recognized as ?IME?.)
S: The underlined product name seems to be uncorrectly recognized. May I
remove it, or leave it?
U: (selected ?remove?)
S: The following texts are retrieved.
1. [AC97]????????????? [???] ??????????
2. [??????]? [???]??????????
3. [WinMe] TrueType??????????????? (a correct answer)
U: Word 2002 ? ?? ???????????????
?Please tell me the way to input formulas in Word 2002.?
ASR: 1. Word 2002 ? ?? (numbers) ???????????????
2. Word 2002 ? ?? (formulas) ???????????????
3. Word 2002 ? ?? (values) ???????????????
S: Please select the most correct recognition result from the above candidates.
U: (selected No. 2)
S: The following texts are retrieved.
1. Word ???????? (a correct answer)
2. Word ??????????????
3. ????????????????
Figure 5: Dialogs for fixing speech recognition er-
rors.
(U: user, S: system, ASR: automatic speech recognition)
5 Experimental Evaluation
We evaluated the system performance experimen-
tally. For the experiments, we had 4 subjects, who
were accustomed to using computers. They made
utterances by following given 10 scenarios and also
made several utterances freely. In total, 53 utter-
ances were recorded. Figure 5 shows two successful
dialogs by confirmation using confidence in recog-
nition and by that using significance for retrieval.
We experimented on the system using the 53
recorded utterances by the following methods:
(1) Using correct transcription of recorded utter-
ance, including fillers.
(2) Using speech recognition results from which
only fillers were removed.
(3) Using speech recognition results and making
confirmation by confidence in recognition.
(4) Using  -best candidates of speech recognition
and making confirmation by significance for re-
trieval. Here,   .
(5) Using  -best candidates of speech recognition
and both measures in (3) and (4).
In these experiments, we assumed that users al-
ways correctly answer system?s asking backs. We
regarded a retrieval as a successful one if a relevant
text was contained in ten high-scored retrieval texts.
Table 2 shows the result. It indicates that our
confirmation methods for fixing speech recognition
errors improve the success rate. Furthermore, the
success rate with both measures gets close to that
with the transcriptions. Considering that the speech
recognition correctness is about 70%, the proposed
dialog strategy is effective.
6 Conclusion
We proposed a spoken dialog Q-A system in which
asking backs for fixing speech recognition errors and
those for clarifying vague questions are integrated.
To realize dialog for fixing recognition errors based
on large text knowledge base, we introduced two
measures of confidence in recognition and signif-
icance for retrieval. The experimental evaluation
shows the advantages of these measures.
References
Yoji Kiyota, Sadao Kurohashi, and Fuyuko Kido. 2002.
?Dialog Navigator? : A Question Answering System
based on Large Text Knowledge Base. In Proceedings
of COLING 2002, pages 460?466.
Sadao Kurohashi and Makoto Nagao. 1994. A syntactic
analysis method of long Japanese sentences based on
the detection of conjunctive structures. Computational
Linguistics, 20(4).
A. Lee, T. Kawahara, and K. Shikano. 2001. Julius ? an
open source real-time large vocabulary recognition en-
gine. In Proceedings of European Conf. Speech Com-
mun. & Tech. (EUROSPEECH), pages 1691?1694.
E. Levin, S. Narayanan, R. Pieraccini, K. Biatov,
E. Bocchieri, G. Di Fabbrizio, W. Eckert, S. Lee,
A. Pokrovsky, M. Rahim, P. Ruscitti, and M. Walker.
2000. The AT&T-DARPA communicator mixed-
initiative spoken dialogue system. In Proceedings of
Int?l Conf. Spoken Language Processing (ICSLP).
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 324?330,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Detection of Quotations and Inserted Clauses and its Application
to Dependency Structure Analysis in Spontaneous Japanese
Ryoji Hamabe  Kiyotaka Uchimoto
 School of Informatics,
Kyoto University
Yoshida-honmachi, Sakyo-ku,
Kyoto 606-8501, Japan
Tatsuya Kawahara  Hitoshi Isahara
National Institute of Information
and Communications Technology
3-5 Hikari-dai, Seika-cho, Soraku-gun,
Kyoto 619-0289, Japan
Abstract
Japanese dependency structure is usu-
ally represented by relationships between
phrasal units called bunsetsus. One of the
biggest problems with dependency struc-
ture analysis in spontaneous speech is that
clause boundaries are ambiguous. This
paper describes a method for detecting
the boundaries of quotations and inserted
clauses and that for improving the depen-
dency accuracy by applying the detected
boundaries to dependency structure anal-
ysis. The quotations and inserted clauses
are determined by using an SVM-based
text chunking method that considers in-
formation on morphemes, pauses, fillers,
etc. The information on automatically an-
alyzed dependency structure is also used
to detect the beginning of the clauses.
Our evaluation experiment using Corpus
of Spontaneous Japanese (CSJ) showed
that the automatically estimated bound-
aries of quotations and inserted clauses
helped to improve the accuracy of depen-
dency structure analysis.
1 Introduction
The ?Spontaneous Speech: Corpus and Pro-
cessing Technology? project sponsored the con-
struction of the Corpus of Spontaneous Japanese
(CSJ) (Maekawa et al, 2000). The CSJ is the
biggest spontaneous speech corpus in the world,
consisting of roughly 7M words with the total
speech length of 700 hours, and is a collection of
monologues such as academic presentations and
simulated public speeches. The CSJ includes tran-
scriptions of the speeches as well as audio record-
ings of them. Approximately one tenth of the
speeches in the CSJ were manually annotated with
various kinds of information such as morphemes,
sentence boundaries, dependency structures, and
discourse structures.
In Japanese sentences, word order is rather
free, and subjects or objects are often omitted.
In Japanese, therefore, the syntactic structure of
a sentence is generally represented by the re-
lationships between phrasal units, or bunsetsus
in Japanese, based on a dependency grammar,
as represented in the Kyoto University text cor-
pus (Kurohashi and Nagao, 1997). In the same
way, the syntactic structure of a sentence is repre-
sented by dependency relationships between bun-
setsus in the CSJ. For example, the sentence ???
?????????? (He is walking slowly) can
be divided into three bunsetsus, ???, kare-wa?
(he), ?????, yukkuri? (slowly), and ????
??, arui-te-iru? (is walking). In this sentence,
the first and second bunsetsus depend on the third
one. The dependency structure is described as fol-
lows.
??????????? (he)
???????????????????? (slowly)
??????????? (is walking)
In this paper, we first describe the problems
with dependency structure analysis of spontaneous
speech. We focus on ambiguous clause boundaries
as the biggest problem and present a solution.
2 Problems with Dependency Structure
Analysis in Spontaneous Japanese
There are many differences between written text
and spontaneous speech, and consequently, prob-
lems peculiar to spontaneous speech arise in de-
324
pendency structure analysis, such as ambiguous
clause boundaries, independent bunsetsus, crossed
dependencies, self-corrections, and inversions. In
this study, we address the problem of ambiguous
clause boundaries in dependency structure analy-
sis in spontaneous speech. We treated the other
problems in the same way as Shitaoka et al (Shi-
taoka et al, 2004). For example, inversions are
represented as dependency relationships going in
the direction from right to left in the CSJ, and their
direction was changed to that from left to right in
our experiments. In this paper, therefore, all the
dependency relationships were assumed to go in
the direction from left to right (Uchimoto et al,
2006).
There are several types of clause boundaries
such as sentence boundaries, boundaries of quo-
tations and inserted clauses. In the CSJ, clause
boundaries were automatically detected by using
surface information (Maruyama et al, 2003), and
sentence boundaries were manually selected from
them (Takanashi et al, 2003). Boundaries of
quotations and inserted clauses were also defined
and detected manually. Dependency relationships
between bunsetsus were annotated within sen-
tences (Uchimoto et al, 2006). Our definition of
clause boundaries follows the definition used in
the CSJ.
Shitaoka et al worked on automatic sen-
tence boundary detection by using SVM-based
text chunking. However, quotations and inserted
clauses were not considered. In this paper, we fo-
cus on these problems in a context of ambiguous
clause boundaries.
Quotations
In written text, quotations are often bracketed by
??(angle brackets), but no brackets are inserted in
spontaneous speech.
ex) ???????????????? (I want to
go there at any rate) is a quotation. In the CSJ,
quotations were manually annotated as follows.
???????????????? (here)
???????????????? (since early times)
???????????????? (once)
???????????????? (at any rate)
???????????????? (want to go)
???????????????? (is the place I think)
Inserted Clauses
In spontaneous speech, speakers insert clauses in
the middle of other clauses. This occurs when
speakers change their speech plans while produc-
Detection of
Sentence Boundary
Dependency Structure
Analysis (Baseline)
Detection of Quotations
and Inserted Clauses
Dependency Structure
Analysis (Enhanced)
word information
filler existence
pause duration
speaking rate
information of dependencies
word information
distance between bunsetsus
(A) + boundaries of quotations
and inserted clauses
...(A)
Figure 1: Outline of proposed processes
ing utterances, which results in supplements, an-
notations, or paraphrases of main clauses.
ex) ???????????? (where I arrived at
night) is an inserted clause.
????????????????? (hotel)
????????????????? (room)
????????????????? (inside)
????????????????? (without delay)
????????????????? (at night)
????????????????? (arrived)
????????????????? (I checked)
Dependency relationships are closed within a
quotation or an inserted clause. Therefore, de-
pendencies except the rightmost bunsetsu in each
clause do not cross boundaries of the same clause,
meaning no dependency exists between the bun-
setsu inside a clause and that outside the clause.
However, automatically detected dependencies of-
ten cross clause boundaries erroneously because
sentences including quotations or inserted clauses
can have complicated clause structures. This is
one of the reasons dependency structure analysis
of spontaneous speech has more errors than that of
written texts. We propose a method for improving
dependency structure analysis based on automatic
detection of quotations and inserted clauses.
3 Dependency Structure Analysis and
Detection of Quotations and Inserted
Clauses
The outline of the proposed processes is shown in
Figure 1. Here, we use ?clause? to describe a quo-
tation and an inserted clause.
3.1 Dependency Structure Analysis
In this research, we use the method proposed by
Uchimoto et al (Uchimoto et al, 2000) to ana-
325
lyze dependency structures. This method is a two-
step procedure, and the first step is preparation of
a dependency matrix in which each element repre-
sents the likelihood that one bunsetsu depends on
another. The second step of the analysis is find-
ing an optimal set of dependencies for the entire
sentence. The likelihood of dependency is repre-
sented by a probability, using a dependency proba-
bility model. The model in this study (Uchimoto et
al., 2000) takes into account not only the relation-
ship between two bunsetsus but also the relation-
ship between the left bunsetsu and all the bunsetsu
to its right.
We implemented this model within a maximum
entropy modeling framework. The features used
in the model were basically attributes related to
the target two bunsetsus: attributes of a bunsetsu
itself, such as character strings, parts of speech,
and inflection types of a bunsetsu together with at-
tributes between bunsetsus, such as the distance
between bunsetsus, etc. Combinations of these
features were also used. In this work, we added
to the features whether there is a boundary of quo-
tations or inserted clauses between the target bun-
setsus. If there is, the probability that the left bun-
setsu depends on the right bunsetsu is estimated to
be low.
In the CSJ, some bunsetsus are defined to have
no modifiee. In our experiments, we defined their
dependencies as follows.
  The rightmost bunsetsu in a quotation or an
inserted clause depends on the rightmost one
in the sentence.
  If a sentence boundary is included in a quo-
tation or an inserted clause, the bunsetsu to
the immediate left of the boundary depends
on the rightmost bunsetsu in the quotation or
the inserted clause.
  Other bunsetsus that have no modifiee de-
pend on the next one.
3.2 Detection of Quotations and Inserted
Clauses
We regard the problem of clause boundary de-
tection as a text chunking task. We used Yam-
Cha (Kudo and Matsumoto, 2001) as a text chun-
ker, which is based on Support Vector Machine
(SVM). We used the chunk labels consisting of
three tags which correspond to sentence bound-
aries, boundaries of quotations, and boundaries of
inserted clauses, respectively. The tag for sentence
Table 1: Tag categories used for chunking
Tag Explanation of tag
B Beginning of a clause
E End of a clause
I Interior of a clause (except B and E)
O Exterior of a clause
S Clause consisting of one bunsetsu
boundaries can be either E (the rightmost bunsetsu
in a sentence) or I (the others). The tags for the
boundaries of quotations and inserted clauses are
shown in Table 1. An example of chunk labels as-
signed to each bunsetsu in a sentence is as follows.
ex) ???????? (It is because of the budget)
is a quotation, and ??????????????
(which I think is because of the budget) is an in-
serted clause. For a chunk label, for example, the
bunsetsu that the chunk label (I, B, B) is assigned
to means that it is not related to a sentence bound-
ary but is related to the beginning of a quotation
and an inserted clause.
(I,O,O)??????????????? (now)
(I,B,B)?????? ? (budget)
(I,E,I)??????????????? (because of)
(I,O,E)??????????????? (I think)
(I,O,O)??????????????? (in summer)
(I,O,O)??????????????? (three times)
(E,O,O)??????????????? (they do it)
The three tags of each chunk label are simulta-
neously estimated. Therefore, the relationships
between sentence boundaries, quotations, and in-
serted clauses are considered in this model. For in-
stance, quotations and inserted clauses should not
cross the sentence boundaries, and the chunk label
such as (E,I,O) is never estimated because this la-
bel means that a sentence boundary exists within a
quotation.
We used the following parameters for YamCha.
  Degree of polynomial kernel: 3rd
  Analysis direction: Right to left
  Dynamic features: Following three chunk la-
bels
  Multi-class method: Pairwise
The chunk label is estimated for each bunsetsu,
The features used to estimate the chunk labels are
as follows.
(1) word information We used word information
such as character strings, pronunciation, part
of speech, inflection type, and inflection
form. Specific expressions are often used at
the ends of quotations and inserted clauses.
326
B?
E
(1) No bunsetsu to left of B
depends on bunsetsu between B and E
?
B
?
E
(2) Bunsetsu to immediate left of B
depends on bunsetsu to right of E
?
?
Figure 2: Dependency structures of bunsetsus to
left of beginning of quotations or inserted clauses
For instance, ????, to-omou? (think) and
?????, tte-iu? (say) are used at the ends
of quotations. Expressions such as ???
?, desu-ga? and ?????, keredo-mo? are
used at the ends of inserted clauses.
(2) fillers and pauses Fillers and pauses are often
inserted just before or after quotations and in-
serted clauses. Pause duration is normalized
in a talk with its mean and variance.
(3) speaking rate Inside inserted clauses, speak-
ers tend to speak fast. The speaking rate is
also normalized in a talk.
Detecting the ends of clauses appears easy be-
cause specific expressions are frequently used at
the ends of clauses as previously mentioned. How-
ever, determining the beginnings of clauses is dif-
ficult in a single process because all features men-
tioned above are local information. Therefore, the
global information is also used to detect the begin-
ning of the clauses. If the end of a clause is given,
the bunsetsus to the left of the clause should sat-
isfy the two conditions described in Figure 2. Our
method uses the constraint as global information.
They are considered as additional features based
on dependency probabilities estimated for the bun-
setsus to the left of the clause. Thus, our chunk-
ing method has two steps. First, clause boundaries
are detected based on the three types of features
itemized above. Second, the beginnings of clauses
are determined after adding to the features the fol-
lowing probabilities obtained by automatic depen-
dency structure analysis.
(4) probability that bunsetsu to left of target de-
pends on bunsetsu inside clause
(5) probability that bunsetsu to immediate left
of target depends on bunsetsu to right of clause
Figure 2 shows that the target bunsetsu is likely
to be the beginning of the clause if probability (4)
is low and probability (5) is high. For instance,
the following example sentence has an inserted
clause. In the first chunking step, the bunsetsu
????????? (which is a story) is found to
be the end of the inserted clause.
ex) ??????????????? (which is a
story that I heard from my father) is an inserted
clause.
????????????????? (this)
????????????????? (area)
????????????????? (from my father)
????????????????? (heard)
????????????????? (story)
????????????????? (in the old days)
????????????????? (was a rice field)
The three bunsetsus ????, atari-wa?, ???
?, kii-ta?, and ????????, hanashi-na-
ndesu-kedo? are less likely to be the beginning
of the inserted clause because in the three cases
the bunsetsu to the immediate left depends on the
target bunsetsu. On the other hand, the bunsetsu
????, chichi-kara? is the most likely to be the
beginning since the bunsetsu to its immediate left
????, atari-wa? depends on the bunsetsu to the
right of the inserted clause ??????????,
tanbo-datta-ndesu?.
4 Experiments and Discussion
For experimental evaluation, we used the tran-
scriptions of 188 talks in the CSJ, which contain
6,255 quotations and 818 inserted clauses. We
used 20 talks for testing. The test data included
643 quotations and 76 inserted clauses. For train-
ing, we used 168 talks excluding the test data to
conduct the open test and all the 188 talks to con-
duct the closed test.
First, we detected sentence boundaries by using
the method (Shitaoka et al, 2004) and analyzed
the dependency structure of each sentence by the
method described in Section 3.1 without using in-
formation on quotations and inserted clauses. We
obtained an F-measure of 85.9 for the sentence
boundary detection, and the baseline accuracy of
the dependency structure analysis was 77.7% for
the open test and 86.5% for the closed test.
327
(a) Results of clause boundary detection
The results obtained by the method described in
Section 3.2 are shown in Table 2. The table shows
five kinds of results:
  results obtained without dependency struc-
ture (in the first chunking step)
  results obtained with dependency structure
analyzed for the open test (in the second
chunking step)
  results obtained with dependency structure
analyzed for the closed test (in the second
chunking step)
  results obtained with manually annotated de-
pendency structure (in the second chunking
step)
  the rate that the ends of clauses are detected
correctly
These results indicate that around 90% of quo-
tations were detected correctly, and the boundary
detection accuracy of quotations was improved by
using automatically analyzed dependency struc-
ture. We found that features (4) and (5) in Section
3.2 obtained from automatically analyzed depen-
dency structure contributed to the improvement.
In the following example, a part of the quotation
?????????????? (my good virtue)
was erroneously detected as a quotation in the first
chunking step. But, in the second chunking step,
automatically analyzed dependency structure con-
tributed to detection of the correct part ?????
???????????? (this is my good virtue)
as a quotation.
????????????????? (this)
????????????????? (my)
????????????????? (good)
????????????????? (virtue)
????????????????? (I)
????????????????? (think)
We also found that the boundary detection accu-
racy of quotations was significantly improved by
using manually annotated dependency structure.
This indicates that the boundary detection accu-
racy of quotations improves as the accuracy of de-
pendency structure analysis improves.
By contrast, only a few inserted clauses were
detected even if dependency structures were used.
Most of the ends of the inserted clauses were de-
tected incorrectly as sentence boundaries. The
main reason for this is our method could not distin-
guish between the ends of the inserted clauses and
those of the sentences, since the same words often
appeared at the ends of both, and it was difficult
Table 2: Clause boundary detection results (sen-
tence boundaries automatically detected)
Quotations Inserted clauses
recall precision F recall precision F
Without dependency information
41.1% 44.3% 42.6 1.3% 20.0% 2.5
(264/643) (264/596) (1/76) (1/5)
With dependency information (open)
42.1% 45.5% 43.7 2.6% 40.0% 4.9
(271/643) (271/596) (2/76) (2/5)
With dependency information (closed)
50.9% 54.9% 52.8 2.6% 40.0% 4.9
(327/643) (327/596) (2/76) (2/5)
With dependency information (correct)
74.2% 80.0% 77.0 2.6% 33.3% 4.9
(477/643) (477/596) (2/76) (2/6)
Correct end of clauses
89.1% 96.1% 92.5 2.6% 40.0% 4.9
(573/643) (573/596) (2/76) (2/5)
Table 3: Dependency structure analysis results ob-
tained with clause boundaries (sentence bound-
aries automatically detected)
Without boundaries of quotations open 77.7%
and inserted clauses closed 86.5%
With boundaries of quotations and open 78.5%
inserted clauses (automatically detected) closed 86.6%
With boundaries of quotations and open 79.4%
inserted clauses (correct) closed 87.4%
to learn the difference between them even though
our method used features based on acoustic infor-
mation.
(b) Dependency structure analysis results
We investigated the accuracies of dependency
structure analysis obtained when the automatically
or manually detected boundaries of quotations and
inserted clauses were used. The results are shown
in Table 3. Although the accuracy of detecting the
boundaries of quotations and inserted clauses us-
ing automatically analyzed dependency structure
was not high, the accuracy of dependency struc-
ture analysis was improved by 0.7% absolute for
the open test. This shows that the model for depen-
dency structure analysis could robustly learn use-
ful information on clause boundaries even if errors
were included in the results of clause boundary de-
tection. In the following example, for instance,
????????????? (to go out with its
face stuck) was correctly detected as a quotation
in the first chunking step. Then, the initial in-
appropriate modifiee ??????, oboe-te-ki-te?
(learn) of the bunsetsu inside the quotation ???
?, hasan-de? (stick) was correctly modified to the
bunsetsu inside the quotation ?????????,
de-te-shimau-to-iu? (to go) by using the automati-
cally detected boundary of the quotation.
328
??????????????????? (face)
??????????????????? (stick)
??????????????????? (out)
??????????????????? (to go)
??????????????????? (stunt)
??????????????????? (somewhere)
??????????????????? (learn)
(c) Results obtained when correct sentence
boundaries are given
We investigated the clause boundary detection
accuracy of quotations and inserted clauses and
the dependency accuracy when correct sentence
boundaries were given manually. The results are
shown in Tables 4 and 5, respectively.
When correct sentence boundaries were given,
the accuracy of clause detection and dependency
structure analysis was improved significantly. Ta-
ble 4 shows that the boundary detection accuracy
of inserted clauses as well as that of quotations
was significantly improved by using information
of dependencies. Table 5 indicates that when us-
ing automatically detected clause boundaries, the
accuracy of dependency structure analysis was im-
proved by 0.7% for the open test, and it was further
improved by using correct clause boundaries.
These experimental results show that detecting
the boundaries of quotations and inserted clauses
as well as sentence boundaries is sensitive to the
accuracy of dependency structure analysis and the
improvements of the boundary detection of quo-
tations and inserted clauses contribute to improve-
ment of dependency structure analysis. Especially,
the difference between Table3 and 5 shows that
the sentence boundary detection accuracy is more
sensitive to the accuracy of dependency structure
analysis than the boundary detection accuracy of
quotations and inserted clauses. This indicates that
sentence boundaries rather than quotations and in-
serted clauses should be manually examined first
to effectively improve the accuracy of dependency
structure analysis in a semi-automatic way.
5 Conclusion
This paper described the method for detecting the
boundaries of quotations and inserted clauses and
that for applying it to dependency structure analy-
sis. The experiment results showed that the auto-
matically estimated boundaries of quotations and
inserted clauses contributed to improvement of de-
pendency structure analysis. In the future, we plan
to solve the problems found in the experiments and
investigate the robustness of our method when the
Table 4: Clause boundary detection results (sen-
tence boundaries given)
Quotations Inserted clauses
recall precision F recall precision F
Without dependency information
46.0% 50.8% 48.3 22.4% 23.6% 23.0
(296/643) (296/583) (17/76) (17/72)
With dependency information (open)
46.7% 53.3% 49.8 30.3% 38.3% 33.8
(300/643) (300/563) (23/76) (23/60)
With dependency information (closed)
55.1% 62.9% 58.7 30.3% 39.0% 34.1
(354/643) (354/563) (23/76) (23/59)
With dependency information (correct)
75.3% 86.0% 80.3 46.1% 60.3% 52.2
(484/643) (484/563) (35/76) (35/58)
Correct end of clauses
86.5% 95.4% 90.7 64.5% 68.1% 66.2
(556/643) (556/583) (49/76) (49/72)
Table 5: Dependency structure analysis results ob-
tained with clause boundaries (sentence bound-
aries given)
Without boundaries of quotations open 81.0%
and inserted clauses closed 90.3%
With boundaries of quotations and open 81.7%
inserted clauses (automatically detected) closed 90.3%
With boundaries of quotations open 82.8%
and inserted clauses (correct) closed 91.3%
results of automatic speech recognition are given
as the inputs. We will also study use of informa-
tion on quotations and inserted clauses to text for-
matting, such as text summarization.
References
Taku Kudo and Yuji Matsumoto. 2001. Chunking
with support vector machines. In Proceedings of the
NAACL.
Sadao Kurohashi and Makoto Nagao. 1997. Building
a Japanese Parsed Corpus while Improving the Pars-
ing System. In Proceedings of the NLPRS, pages
451?456.
Kikuo Maekawa, Hanae Koiso, Sadaoki Furui, and Hi-
toshi Isahara. 2000. Spontaneous Speech Corpus of
Japanese. In Proceedings of the LREC2000, pages
947?952.
Takehiko Maruyama, Hideki Kashioka, Tadashi Ku-
mano, and Hideki Tanaka. 2003. Rules for Auto-
matic Clause Boundary Detection and Their Evalu-
ation. In Proceedings of the Nineth Annual Meeting
of the Association for Natural Language proceeding,
pages 517?520. (in Japanese).
Katsuya Takanashi, Takehiko Maruyama, Kiy-
otaka Uchimoto, and Hitoshi Isahara. 2003.
Identification of ?Sentences? in Spontaneous
329
Japanese ? Detection and Modification of Clause
Boundaries ?. In Proceedings of the ISCA & IEEE
Workshop on Spontaneous Speech Processing and
Recognition, pages 183?186.
Kiyotaka Uchimoto, Masaki Murata, Satoshi Sekine,
and Hitoshi Isahara. 2000. Dependency Model Us-
ing Posterior Context. In Proceedings of the IWPT,
pages 321?322.
Kiyotaka Uchimoto, Ryoji Hamabe, Take-
hiko Maruyama, Katsuya Takanashi, Tatsuya Kawa-
hara, and Hitoshi Isahara. 2006. Dependency-
structure Annotation to Corpus of Spontaneous
Japanese. In Proceedings of the LREC2006, pages
635-638.
Kazuya Shitaoka, Kiyotaka Uchimoto, Tatsuya Kawa-
hara, and Hitoshi Isahara. 2004. Dependency Struc-
ture Analysis and Sentence Boundary Detection in
Spontaneous Japanese. In Proceedings of the COL-
ING2004, pages 1107?1113.
330
Flexible Spoken Dialogue System based on User Models
and Dynamic Generation of VoiceXML Scripts
Kazunori Komatani Fumihiro Adachi Shinichi Ueno
Tatsuya Kawahara Hiroshi G. Okuno
Graduate School of Informatics
Kyoto University
Yoshida-Hommachi, Sakyo, Kyoto 606-8501, Japan
{komatani,adachi,ueno,kawahara,okuno}@kuis.kyoto-u.ac.jp
Abstract
We realize a telephone-based collab-
orative natural language dialogue sys-
tem. Since natural language involves
very various expressions, a large num-
ber of VoiceXML scripts need to be pre-
pared to handle all possible input patterns.
We realize flexible dialogue management
for various user utterances by generating
VoiceXML scripts dynamically. More-
over, we address appropriate user mod-
eling in order to generate cooperative re-
sponses to each user. Specifically, we set
up three dimensions of user models: skill
level to the system, knowledge level on
the target domain and the degree of hasti-
ness. The models are automatically de-
rived by decision tree learning using real
dialogue data collected by the system. Ex-
perimental evaluation shows that the co-
operative responses adapted to individual
users serve as good guidance for novice
users without increasing the dialogue du-
ration for skilled users.
Keywords: spoken dialogue system, user model,
VoiceXML, cooperative responses, dialogue
strategy
1 Introduction
A Spoken dialogue system is one of the promising
applications of the speech recognition and natural
language understanding technologies. A typical task
of spoken dialogue systems is database retrieval.
Some IVR (interactive voice response) systems us-
ing the speech recognition technology are being put
into practical use as its simplest form. According to
the spread of cellular phones, spoken dialogue sys-
tems via telephone enable us to obtain information
from various places without any other special appa-
ratuses.
In order to realize user-friendly interaction, spo-
ken dialogue systems should be able to (1) accept
various user utterances to enable mixed-initiative di-
alogue and (2) generate cooperative responses. Cur-
rently, a lot of IVR systems via telephone operate
by using VoiceXML, which is a script language to
prescribe procedures of spoken dialogues. How-
ever, only the next behaviors corresponding to ev-
ery input are prescribed in the VoiceXML scripts,
so the dialogue procedure is basically designed as
system-initiated one, in which the system asked re-
quired items one by one. In order to realize mixed-
initiative dialogue, the system should be able to ac-
cept various user-initiated utterances. By allowing
to accept various user utterances, the combination
of words included in the utterances accordingly gets
enormous, and then it is practically impossible to
prepare VoiceXML scripts that correspond to the
enormous combinations of input words in advance.
It is also difficult to generate cooperative responses
adaptively in the above framework.
We propose a framework to generate VoiceXML
scripts dynamically in order to realize the mixed-
initiative dialogue, in which the system is needed to
accept various user utterances. This framework real-
izes flexible dialogue management without requiring
preparation for a large number of VoiceXML scripts
in advance. Furthermore, it enables various behav-
iors adaptive to the dialogue situations such as ob-
tained query results.
Another problem to realize user-friendly interac-
tion is how to generate cooperative responses. When
we consider the responses generated from the sys-
tem side, the dialogue strategies, which determine
when to make guidance and what the system should
tell to the user, are the essential factors in spoken di-
alogue systems. There are many studies in respect of
the dialogue strategy such as confirmation manage-
ment using confidence measures of speech recogni-
tion results (Komatani and Kawahara, 2000; Hazen
et al, 2000), dynamic change of dialogue initiative
(Litman and Pan, 2000; Chu-Carroll, 2000; Lamel
et al, 1999), and addition of cooperative contents
to system responses (Sadek, 1999). Nevertheless,
whether a particular response is cooperative or not
depends on individual user?s characteristic.
In order to adapt the system?s behavior to individ-
ual users, it is necessary to model the user?s patterns
(Kass and Finin, 1988). Most of conventional stud-
ies on user models have focused on the knowledge
of users. Others tried to infer and utilize user?s goals
to generate responses adapted to the user (van Beek,
1987; Paris, 1988). Elzer et al (2000) proposed
a method to generate adaptive suggestions accord-
ing to users? preferences. However, these studies
depend on knowledge of the target domain greatly,
and therefore the user models need to be deliberated
manually to be applied to new domains. Moreover,
they assumed that the input is text only, which does
not contain errors.
We propose more comprehensive user models to
generate user-adapted responses in spoken dialogue
systems taking account of information specific to
spoken dialogue. Spoken utterances include vari-
ous information such as the interval between the ut-
terances, the presence of barge-in and so on, which
can be utilized to judge the user?s character. These
features also possess generality in spoken dialogue
systems because they are not dependent on domain-
specific knowledge. As user models in spoken di-
alogue systems, Eckert et al (1997) defined stereo-
types of users such as patient, submissive and ex-
perienced, in order to evaluate spoken dialogue sys-
tems by simulation. We introduce user models not
for defining users? behaviors beforehand, but for de-
tecting users? patterns in real-time interaction.
We define three dimensions in the user models:
?skill level to the system?, ?knowledge level on the
target domain? and ?degree of hastiness?. The user
models are trained by decision tree learning algo-
rithm, using real data collected from the Kyoto city
bus information system. Then, we implement the
user models on the system and evaluate them using
data collected with 20 novice users.
2 Flexible Spoken Dialogue System based
on Dynamic Generation of VoiceXML
Scripts
VoiceXML1 is a script language to prescribe pro-
cedures in spoken dialogues mainly on telephone,
and is becoming to a standard language of IVR sys-
tems. The VoiceXML scripts consist of three parts:
(1) specifications of system?s prompts, (2) specifica-
tions of grammars to accept a user?s utterance, and
(3) description of the next behaviors.
However, most of existing services using the
VoiceXML imposes rigid interaction, in which user
utterances are restricted by system-initiated prompts
and a user is accordingly allowed to specify only re-
quested items one by one. It is more user-friendly
that users can freely convey their requests by natural
language expressions.
We present a framework to realize flexible inter-
action by generating VoiceXML scripts dynamically
(Pargellis et al, 1999; Nyberg et al, 2002). The
framework enables users to express their requests
by natural language even in VoiceXML-based sys-
tems. Furthermore, cooperative responses in the Ky-
oto city bus information system that has been devel-
oped in our laboratory are also presented in this sec-
tion.
2.1 Dynamic Generation of VoiceXML Scripts
In VoiceXML scripts, acceptable keywords and cor-
responding next states must be explicitly specified.
However, since there exists enormous combinations
of keywords in natural language expressions, it is
practically impossible to describe all VoiceXML
scripts that correspond to the combinations. Then,
we introduce the framework in which VoiceXML
1VoiceXML Forum. http://www.voicexml.org/
speech synthesizedspeech
VoiceXML
dialogue
manager
TTS enginespeech
recognizer
grammar
ruleskeywords
response
sentences
Front end
CGI script
user
VWS (Voice Web Server)
VoiceXML
generator
database
Figure 1: Overview of spoken dialogue system
based on dynamic generation of VoiceXML scripts
scripts are generated dynamically to enable the sys-
tem to accept natural language expressions.
Figure 1 shows the overview of the framework.
The front end that operates based on VoiceXML
scripts is separated from the dialogue management
portion, which accepts speech recognition results
and generates corresponding VoiceXML scripts.
The user utterance is recognized based on a gram-
mar rule specified in VoiceXML scripts, and key-
words extracted from a speech recognition result are
passed to a CGI script. The CGI script retrieves cor-
responding information from the database on Web,
and generates a VoiceXML script for the succeed-
ing interaction. If sufficient information is not ob-
tained from a user utterance, a script that prompts to
fill remaining contents is generated, and if the user
utterance contains ambiguity, a script that makes a
disambiguating question is generated.
Consequently, the generation of VoiceXML
scripts enables to accept natural language expres-
sions without preparing a large number of the scripts
corresponding to various inputs beforehand. The
framework also enables to generate cooperative re-
sponses adapted to the situation such as retrieval re-
sults without spoiling portability.
Sys: Please tell me your current bus stop, your destination
or the specific bus route.
User: Shijo-Kawaramachi.
Sys: Do you take a bus from Shijo-Kawaramachi?
User: Yes.
Sys: Where will you get off the bus?
User: Arashiyama.
Sys: Do you go from Shijo-Kawaramachi to Arashiyama?
User: Yes.
Sys: Bus number 11 bound for Arashiyama has departed
Sanjo-Keihanmae, two bus stops away.
Figure 2: Example dialogue of the bus system
2.2 Kyoto City Bus Information System
We have developed the Kyoto city bus information
system, which locates the bus a user wants to take,
and tell him/her how long it will take before arriving.
The system can be accessed via telephone including
cellular phones2. From any places, users can easily
get the information on the bus that changes every
minute. Users are requested to input the bus stop to
get on, the destination, or the bus route number by
speech, and consequently get the bus information.
There are 652 bus stops and 85 bus routes in Kyoto
city. The bus stops can also be specified by the name
of famous places or public facilities nearby. Figure
2 shows a simple example of the dialogue.
2.3 Generation of Cooperative Responses in
Kyoto City Bus Information System
In spoken dialogue systems, it is important to gener-
ate cooperative responses by guessing the user?s in-
tention. The cooperative responses consist of adding
information more than the user has requested, telling
the reason when there is no query result, present-
ing the alternatives, and so on (Sadek, 1999). In
order to generate cooperative responses, it is neces-
sary to change responses according to the situations.
Since we separate language-understanding modules
from individual VoiceXML scripts that are gener-
ated dynamically, our system can generate appropri-
ate responses cooperatively according to query re-
sults. The examples of the cooperative responses
implemented in our Kyoto city bus information sys-
tem are listed below.
2+81-75-326-3116
(1) Suggestive answers
The information related to a query result is offered
when the query result is negative.
(Example)
User: From Hyakumanben to Kyoto Station by bus
number 17.
Sys: Bus number 17 does not come for a while, but
bus number 206 has departed Takano, four bus
stops away.
(2) Corrective answers
The reason is presented when there is no query result
or when a user utterance contains an error.
(Example)
User: From Kyoto Station to Arashiyama.
Sys: There is no bus that directly goes from Kyoto
station to Arashiyama.
(3) Intensional answers
If several query results are obtained, generating in-
tensional answer is more cooperative than listing all
the results.
(Example)
User: From Shijo-Kawaramachi to Shijo-Karasuma.
Sys: All buses to Shijo-Karasuma do not come for a
while.
(4) Completion answers
It is cooperative to generate responses including in-
formation more than the user?s request. However,
whether the information should be presented or not
differs in individual users.
(Example)
User: From Kyoto station by bus number 5.
Sys: Bus number 5 will depart at 17:34 from plat-
form ?A1?.
(There are a lot of platforms in Kyoto station.)
3 Response Generation Using User Models
We propose comprehensive user models to generate
user-adaptive responses taking account of informa-
tion specific to spoken dialogue. Whether a partic-
ular response is regarded as cooperative depends on
individual user?s characteristics. So, we address ap-
propriate user modeling in order to generate cooper-
ative responses to the users.
3.1 Classification of User Models
We define three dimensions as user models listed be-
low.
? Skill level to the system
? Knowledge level on the target domain
? Degree of hastiness
Skill Level to the System
Since spoken dialogue systems are not
widespread yet, there arises a difference in the
skill level of users in operating the systems. It
is desirable that the system changes its behavior
including response generation and initiative man-
agement in accordance with the skill level of the
user. In conventional systems, a system-initiated
guidance has been invoked on the spur of the
moment either when the user says nothing or
when speech recognition is not successful. In our
framework, we address a radical solution for the
unskilled users by modeling the skill level as the
user?s property before such a problem arises.
Knowledge Level on the Target Domain
There also exists a difference in the knowledge
level on the target domain among users. Thus, it is
necessary for the system to change information to
present to users. For example, it is not cooperative
to tell too detailed information to strangers. On the
other hand, for inhabitants, it is useful to omit too
obvious information and to output more detailed in-
formation. Therefore, we introduce a dimension that
represents the knowledge level on the target domain.
Degree of Hastiness
In speech communications, it is more important
to present information promptly and concisely com-
pared with the other communication modes such as
browsing. Especially in the bus system, the concise-
ness is preferred because the bus information is ur-
gent to most users. Therefore, we also take account
of degree of hastiness of the user, and accordingly
change the system?s responses.
3.2 Response Generation Strategy Using User
Models
Next, we describe the response generation strategies
adapted to individual users based on the proposed
user models: skill level, knowledge level and hasti-
ness. Basic design of dialogue management is based
on mixed-initiative dialogue, in which the system
makes follow-up questions and guidance if neces-
sary while allowing a user to utter freely. It is in-
vestigated to add various contents to the system re-
sponses as cooperative responses in (Sadek, 1999).
Such additional information is usually cooperative,
but some people may feel such a response redun-
dant.
Thus, we introduce the user models and con-
trol the generation of additional information. By
introducing the proposed user models, the system
changes generated responses by the following two
aspects: dialogue procedure and contents of re-
sponses.
Dialogue Procedure
The dialogue procedure is changed based on the
skill level and the hastiness. If a user is identified as
having the high skill level, the dialogue management
is carried out in a user-initiated manner; namely, the
system generates only open-ended prompts. On the
other hand, when user?s skill level is detected as low,
the system takes an initiative and prompts necessary
items in order.
When the degree of hastiness is low, the system
makes confirmation on the input contents. Con-
versely, when the hastiness is detected as high, such
a confirmation procedure is omitted; namely, the
system immediately makes a query and outputs the
result without making such a confirmation.
Contents of Responses
Information that should be included in the sys-
tem response can be classified into the following two
items.
1. Dialogue management information
2. Domain-specific information
The dialogue management information specifies
how to carry out the dialogue including the instruc-
tion on user?s expression for yes/no questions like
?Please reply with either yes or no.? and the expla-
nation about the following dialogue procedure like
?Now I will ask in order.? This dialogue manage-
ment information is determined by the user?s skill
 58.8>=
the maximum number of filled slots
dialogue state
initial state otherwise
presense of barge-in
rate of no input
0.07>
30 1 2
average of
recognition score
58.8<
skill level
high
skill level
high
skill level
low
skill level
low
Figure 3: Decision tree for the skill level
level to the system, and is added to system responses
when the skill level is considered as low.
The domain-specific information is generated ac-
cording to the user?s knowledge level on the target
domain. Namely, for users unacquainted with the
local information, the system adds the explanation
about the nearest bus stop, and omits complicated
contents such as a proposal of another route.
The contents described above are also controlled
by the hastiness. For users who are not in hurry, the
system generates the additional contents that corre-
spond to their skill level and knowledge level as co-
operative responses. On the other hand, for hasty
users, the contents are omitted to prevent the dia-
logue from being redundant.
3.3 Classification of User based on Decision
Tree
In order to implement the proposed user models as
classifiers, we adopt a decision tree. It is constructed
by decision tree learning algorithm C5.0 (Quinlan,
1993) with data collected by our dialogue system.
Figure 3 shows an example of the derived decision
tree for the skill level.
We use the features listed in Figure 4. They in-
clude not only semantic information contained in the
utterances but also information specific to spoken di-
alogue systems such as the silence duration prior to
the utterance, the presence of barge-in and so on.
Except for the last category of Figure 4 including
?attribute of specified bus stops?, most of the fea-
tures are domain-independent.
The classification of each dimension is done for
every user utterance except for knowledge level. The
model of a user can change during a dialogue. Fea-
tures extracted from utterances are accumulated as
history information during the session.
? features obtained from a single utterance
? dialogue state (defined by already filled slots)
? presence of barge-in
? lapsed time of the current utterance
? recognition result (something recognized / un-
certain / no input)
? score of speech recognizer
? the number of filled slots by the current utter-
ance
? features obtained from the session
? the number of utterances
? dialogue state of the previous utterance
? lapsed time from the beginning of the session
? the number of the repetition of a same question
? the average number of the repetition of a same
question
? ratio of the time of user utterances in whole
elapsed time
? ratio of the occurrence of barge-in out of the
whole number of utterances
? recognition result of the previous utterance
? ratio of something recognized
? ratio of getting uncertain results
? ratio of no input
? the number of barge-in
? the number of something recognized
? the number of getting uncertain results
? the number of no input
? average of recognition scores
? the maximum number of slots filled by a single
utterance
? features obtained from the session (used only in clas-
sification of knowledge level)
? attribute of specified bus stops
? the way of specifying bus stops (whether a bus
stop is specified by its correct name or not)
? ratio of each attribute of specified bus stops
? ratio of each way of specifying bus stops
Figure 4: Features to classify the user models
Figure 5 shows an example of the system behav-
ior with the proposed user models. The skill level is
classified as being low by the decision tree, because
the first user?s utterance includes only one content
word. Then, dialogue procedure is changed to the
system-initiated one. Similarly, the hastiness is clas-
sified as being low by the decision tree, and the sys-
tem includes the explanation on the dialogue pro-
cedure and the instruction on the expression in the
responses. They are omitted if the hastiness is iden-
tified as high.
3.4 Decision Tree Learning for User Models
We train and evaluate the decision tree for the user
models using dialogue data collected by our sys-
case: [skill level: low][knowledge level: low]
[hastiness: low]
Sys: Please tell me your current bus stop, your destination
or the specific bus route number.
User: Maruyama Park.
Sys: Let me confirm one by one.
(? The explanation is added by [skill level: low].)
Will you get on at Maruyama Park?
(? The content is confirmed by [hastiness: low].)
Please reply with yes or no.
(? The instruction is added by [skill level: low].)
User: Yes.
Sys: Where will you get off the bus? Please specify by a
bus stop name or a well-known landmark.
(? The explanation is added by [skill level: low].)
User: Tofukuji temple.
Sys: Will you get off the bus at Tofukuji temple?
Please reply with yes or no.
User: Yes.
Sys: The following buses are located.
(? The explanation is added by [skill level: low].)
The nearest bus stop to Maruyama Park is Gion.
(? The domain information is added by [knowl-
edge level: low].)
Bus number 202 bound for Nishioji-Kujo has de-
parted Higashiyama-Sanjo, which is two stops away.
...
Figure 5: An example dialogue with the proposed
user models
low indeterminable high total
skill level 743 253 496 1492
knowledge level 275 808 409 1492
hastiness 421 932 139 1492
Table 1: Number of manually annotated labels for
decision tree learning
tem. The data was collected from December 10th
2001 to May 10th 2002. The number of the ses-
sions (telephone calls) is 215, and the total number
of utterances included in the sessions is 1492. We
annotated the subjective labels of the user models
by hand. The annotator judges the user models for
every utterance based on the recorded speech data
and logs. The labels were given to the three dimen-
sions described in section 3.1 among ?high?, ?inde-
terminable? or ?low?. It is possible that the annotated
model of a user changes during a dialogue, espe-
cially from ?indeterminable? to ?low? or ?high?. The
number of the labeled utterances is shown in Table
1.
condition #1 #2 #3
skill level 80.8% 75.3% 85.6%
knowledge level 73.9% 63.7% 78.2%
hastiness 74.9% 73.7% 78.6%
Table 2: Classification accuracy of the proposed user
models
Using the labeled data, we trained the decision
tree and evaluated the classification accuracy of the
proposed user models. All the experiments were car-
ried out by the method of 10-fold cross validation.
The process, in which one tenth of all data is used as
the test data, and the remainder is used as the train-
ing data, is repeated ten times, and the average of
the classification accuracy is computed. The result
is shown in Table 2. The conditions #1, #2 and #3 in
Table 2 are described as follows.
#1: The 10-fold cross validation is carried out per
utterance.
#2: The 10-fold cross validation is carried out per
session (call).
#3: We calculate the accuracy under more realis-
tic condition. The accuracy is calculated not
in three classes (high / indeterminable / low)
but in two classes that actually affect the dia-
logue strategies. For example, the accuracy for
the skill level is calculated for the two classes:
low and the others. As to the classification of
knowledge level, the accuracy is calculated for
dialogue sessions, because the features such as
the attribute of a specified bus stop are not ob-
tained in every utterance. Moreover, in order
to smooth unbalanced distribution of the train-
ing data, a cost corresponding to the reciprocal
ratio of the number of samples in each class is
introduced. By the cost, the chance rate of two
classes becomes 50%.
The difference between condition #1 and #2 is
that the training was carried out in a speaker-closed
or speaker-open manner. The former shows better
performance.
The result in condition #3 shows useful accuracy
in the skill level. The following features play im-
portant part in the decision tree for the skill level:
user
profiles
database
on Web
CGI
the system except for
proposed user models
user
VWS
(Voice Web Server)
VoiceXML
generator
dialogue
manager
user model
identifier
VoiceXMLrecognition results(keywords)
recognition results
(including features other
than language info.)
Figure 6: Overview of the Kyoto city bus informa-
tion system with user models
the number of filled slots by the current utterance,
presence of barge-in and ratio of no input. For the
knowledge level, recognition result (something rec-
ognized / uncertain / no input), ratio of no input and
the way to specify bus stops (whether a bus stop is
specified by its exact name or not) are effective. The
hastiness is classified mainly by the three features:
presence of barge-in, ratio of no input and lapsed
time of the current utterance.
3.5 System Overview
Figure 6 shows an overview of the Kyoto city bus in-
formation system with the user models. The system
operates by generating VoiceXML scripts dynami-
cally as described in section 2.1. The real-time bus
information database is provided on the Web, which
can be accessed via Internet. Then, we explain the
modules in the following.
VWS (Voice Web Server)
The Voice Web Server drives the speech recog-
nition engine and the TTS (Text-To-Speech)
module accordingly to the specifications by the
generated VoiceXML script.
Speech Recognizer
The speech recognizer decodes user utterances
based on specified grammar rules and vocabu-
lary, which are defined by VoiceXML at each
dialogue state.
Dialogue Manager
The dialogue manager generates response sen-
tences based on recognition results (bus stop
names or a route number) received from the
VWS. If sufficient information to locate a bus
is obtained, it retrieves the corresponding bus
information on the Web.
VoiceXML Generator
This module dynamically generates VoiceXML
scripts that contain response sentences and
specifications of speech recognition grammars,
which are given by the dialogue manager.
User model identifier
This module classifies user?s characters based
on the user models using features specific to
spoken dialogue as well as semantic attributes.
The obtained user profiles are sent to the dia-
logue manager, and are utilized in the dialogue
management and response generation.
4 Experimental Evaluation of the System
with User Models
We evaluated the system with the proposed user
models using 20 novice subjects who had not used
the system. The experiment was performed in the
laboratory under adequate control. For the speech
input, the headset microphone was used.
4.1 Experiment Procedure
First, we explained the outline of the system to sub-
jects and gave a document in which experiment con-
ditions and scenarios were described. We prepared
two sets of eight scenarios. Subjects were requested
to acquire the bus information according to the sce-
narios using the system with/without the user mod-
els. In the scenarios, neither the concrete names of
bus stops nor the bus number were given. For exam-
ple, one of the scenarios was as follows: ?You are in
Kyoto for sightseeing. After visiting the Ginkakuji
temple, you go to Maruyama Park. Supposing such
a situation, please get information on the bus.? We
also set the constraint in order to vary the subjects?
hastiness such as ?Please hurry as much as possible
in order to save the charge of your cellular phone.?
The subjects were also told to look over question-
naire items before the experiment, and filled in them
duration (sec.) # turn
group 1 with UM 51.9 4.03
(with UM? w/o UM) w/o UM 47.1 4.18
group 2 w/o UM 85.4 8.23
(w/o UM? with UM) with UM 46.7 4.08
UM: User Model
Table 3: Duration and the number of turns in dia-
logue
after using each system. This aims to reduce the sub-
ject?s cognitive load and possible confusion due to
switching the systems (Over, 1999). The question-
naire consisted of eight items, for example, ?When
the dialogue did not go well, did the system guide in-
telligibly?? We set seven steps for evaluation about
each item, and the subject selected one of them.
Furthermore, subjects were asked to write down
the obtained information: the name of the bus stop
to get on, the bus number and how much time it
takes before the bus arrives. With this procedure,
we planned to make the experiment condition close
to the realistic one.
The subjects were divided into two groups; a half
(group 1) used the system in the order of ?with
user models ? without user models?, the other half
(group 2) used in the reverse order.
The dialogue management in the system without
user models is also based on the mixed-initiative
dialogue. The system generates follow-up ques-
tions and guidance if necessary, but behaves in a
fixed manner. Namely, additional cooperative con-
tents corresponding to skill level described in section
3.2 are not generated and the dialogue procedure is
changed only after recognition errors occur. The
system without user models behaves equivalently to
the initial state of the user models: the hastiness is
low, the knowledge level is low and the skill level is
high.
4.2 Results
All of the subjects successfully completed the given
task, although they had been allowed to give up if the
system did not work well. Namely, the task success
rate is 100%.
Average dialogue duration and the number of
turns in respective cases are shown in Table 3.
Though the users had not experienced the system at
group 1 with UM 0.72
(with UM ? w/o UM) w/o UM 0.70
group 2 w/o UM 0.41
(w/o UM ? with UM) with UM 0.63
Table 4: Ratio of utterances for which the skill level
was judged as high
all, they got accustomed to the system very rapidly.
Therefore, as shown in Table 3, the duration and
the number of turns were decreased obviously in the
latter half of the experiment in both groups. How-
ever, in the initial half of the experiment, the group
1 completed with significantly shorter dialogue than
group 2. This means that the incorporation of the
user models is effective for novice users. Table 4
shows a ratio of utterances for which the skill level
was identified as high. The ratio is calculated by di-
viding the number of utterances that were judged as
high skill level by the number of all utterances. The
ratio is much larger for group 1 who initially used
the system with user models. This fact means that
the novice users got accustomed to the system more
rapidly with the user models, because they were in-
structed on the usage by cooperative responses gen-
erated when the skill level is low. The results demon-
strate that cooperative responses generated accord-
ing to the proposed user models can serve as good
guidance for novice users.
In the latter half of the experiment, the dialogue
duration and the number of turns were almost same
between the two groups. This result shows that the
proposed models prevent the dialogue from becom-
ing redundant for skilled users, although generating
cooperative responses for all users made the dia-
logue verbose in general. It suggests that the pro-
posed user models appropriately control the genera-
tion of cooperative responses by detecting characters
of individual users.
5 Conclusions
We have presented a framework to realize flexible
interaction by dynamically generating VoiceXML
scripts. This framework realizes mixed-initiative di-
alogues and the generation of cooperative responses
in VoiceXML-based systems.
We have also proposed and evaluated user mod-
els for generating cooperative responses adaptively
to individual users. The proposed user models con-
sist of the three dimensions: skill level to the sys-
tem, knowledge level on the target domain and the
degree of hastiness. The user models are identified
by decision tree using features specific to spoken di-
alogue systems as well as semantic attributes. They
are automatically derived by decision tree learning,
and all features used for skill level and hastiness are
independent of domain-specific knowledge. So, it is
expected that the derived user models can be gener-
ally used in other domains.
The experimental evaluation with 20 novice users
shows that the skill level of novice users was im-
proved more rapidly by incorporating user mod-
els, and accordingly the dialogue duration becomes
shorter more immediately. The result is achieved
by the generated cooperative responses based on the
proposed user models. The proposed user models
also suppress the redundancy by changing the dia-
logue procedure and selecting contents of responses.
Thus, the framework generating VoiceXML
scripts dynamically and the proposed user models
realize a user-adaptive dialogue strategies, in which
the generated cooperative responses serve as good
guidance for novice users without increasing the di-
alogue duration for skilled users.
References
Jennifer Chu-Carroll. 2000. MIMIC: An adaptive
mixed initiative spoken dialogue system for informa-
tion queries. In Proc. of the 6th Conf. on applied Nat-
ural Language Processing, pages 97?104.
Wieland Eckert, Esther Levin, and Roberto Pieraccini.
1997. User modeling for spoken dialogue system eval-
uation. In Proc. IEEE Workshop on Automatic Speech
Recognition and Understanding, pages 80?87.
Stephanie Elzer, Jennifer Chu-Carroll, and Sandra Car-
berry. 2000. Recognizing and utilizing user prefer-
ences in collaborative consultation dialogues. In Proc.
of the 4th Int?l Conf. on User Modeling, pages 19?24.
Timothy J. Hazen, Theresa Burianek, Joseph Polifroni,
and Stephanie Seneff. 2000. Integrating recognition
confidence scoring with language understanding and
dialogue modeling. In Proc. Int?l Conf. Spoken Lan-
guage Processing (ICSLP).
Robert Kass and Tim Finin. 1988. Modeling the user in
natural language systems. Computational Linguistics,
14(3):5?22.
Kazunori Komatani and Tatsuya Kawahara. 2000.
Flexible mixed-initiative dialogue management using
concept-level confidence measures of speech recog-
nizer output. In Proc. Int?l Conf. Computational Lin-
guistics (COLING), pages 467?473.
Lori Lamel, Sophie Rosset, Jean-Luc Gauvain, and Samir
Bennacef. 1999. The LIMSI ARISE system for
train travel information. In IEEE Int?l Conf. Acoust.,
Speech & Signal Processing (ICASSP).
Diane J. Litman and Shimei Pan. 2000. Predicting and
adapting to poor speech recognition in a spoken dia-
logue system. In Proc. of the 17th National Confer-
ence on Artificial Intelligence (AAAI2000).
Eric Nyberg, Teruko Mitamura, Paul Placeway, Michael
Duggan, and Nobuo Hataoka. 2002. Dialogxml:
Extending voicexml for dynamic dialog manage-
ment. In Proc. of Human Language Technology 2002
(HLT2002), pages 286?291.
Paul Over. 1999. Trec-7 interactive track report. In Proc.
of the 7th Text REtrieval Conference (TREC7).
Andrew Pargellis, Jeff Kuo, and Chin-Hui Lee. 1999.
Automatic dialogue generator creates user defined ap-
plications. In Proc. European Conf. Speech Commun.
& Tech. (EUROSPEECH).
Cecile L. Paris. 1988. Tailoring object descriptions to
a user?s level of expertise. Computational Linguistics,
14(3):64?78.
J. Ross Quinlan. 1993. C4.5: Programs for Ma-
chine Learning. Morgan Kaufmann, San Mateo, CA.
http://www.rulequest.com/see5-info.html.
David Sadek. 1999. Design considerations on dia-
logue systems: From theory to technology -the case
of artimis-. In Proc. ESCA workshop on Interactive
Dialogue in Multi-Modal Systems.
Peter van Beek. 1987. A model for generating better
explanations. In Proc. of the 25th Annual Meeting of
the Association for Computational Linguistics (ACL-
87), pages 215?220.
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 632?641,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
An Unsupervised Model for Joint Phrase Alignment and Extraction
Graham Neubig1,2 Taro Watanabe2, Eiichiro Sumita2, Shinsuke Mori1, Tatsuya Kawahara1
1Graduate School of Informatics, Kyoto University
Yoshida Honmachi, Sakyo-ku, Kyoto, Japan
2National Institute of Information and Communication Technology
3-5 Hikari-dai, Seika-cho, Soraku-gun, Kyoto, Japan
Abstract
We present an unsupervised model for joint
phrase alignment and extraction using non-
parametric Bayesian methods and inversion
transduction grammars (ITGs). The key con-
tribution is that phrases of many granulari-
ties are included directly in the model through
the use of a novel formulation that memorizes
phrases generated not only by terminal, but
also non-terminal symbols. This allows for
a completely probabilistic model that is able
to create a phrase table that achieves com-
petitive accuracy on phrase-based machine
translation tasks directly from unaligned sen-
tence pairs. Experiments on several language
pairs demonstrate that the proposed model
matches the accuracy of traditional two-step
word alignment/phrase extraction approach
while reducing the phrase table to a fraction
of the original size.
1 Introduction
The training of translation models for phrase-
based statistical machine translation (SMT) systems
(Koehn et al, 2003) takes unaligned bilingual train-
ing data as input, and outputs a scored table of
phrase pairs. This phrase table is traditionally gen-
erated by going through a pipeline of two steps, first
generating word (or minimal phrase) alignments,
then extracting a phrase table that is consistent with
these alignments.
However, as DeNero and Klein (2010) note, this
two step approach results in word alignments that
are not optimal for the final task of generating
phrase tables that are used in translation. As a so-
lution to this, they proposed a supervised discrimi-
native model that performs joint word alignment and
phrase extraction, and found that joint estimation of
word alignments and extraction sets improves both
word alignment accuracy and translation results.
In this paper, we propose the first unsuper-
vised approach to joint alignment and extraction of
phrases at multiple granularities. This is achieved
by constructing a generative model that includes
phrases at many levels of granularity, from minimal
phrases all the way up to full sentences. The model
is similar to previously proposed phrase alignment
models based on inversion transduction grammars
(ITGs) (Cherry and Lin, 2007; Zhang et al, 2008;
Blunsom et al, 2009), with one important change:
ITG symbols and phrase pairs are generated in
the opposite order. In traditional ITG models, the
branches of a biparse tree are generated from a non-
terminal distribution, and each leaf is generated by
a word or phrase pair distribution. As a result, only
minimal phrases are directly included in the model,
while larger phrases must be generated by heuris-
tic extraction methods. In the proposed model, at
each branch in the tree, we first attempt to gener-
ate a phrase pair from the phrase pair distribution,
falling back to ITG-based divide and conquer strat-
egy to generate phrase pairs that do not exist (or are
given low probability) in the phrase distribution.
We combine this model with the Bayesian non-
parametric Pitman-Yor process (Pitman and Yor,
1997; Teh, 2006), realizing ITG-based divide and
conquer through a novel formulation where the
Pitman-Yor process uses two copies of itself as a
632
base measure. As a result of this modeling strategy,
phrases of multiple granularities are generated, and
thus memorized, by the Pitman-Yor process. This
makes it possible to directly use probabilities of the
phrase model as a replacement for the phrase table
generated by heuristic extraction techniques.
Using this model, we perform machine transla-
tion experiments over four language pairs. We ob-
serve that the proposed joint phrase alignment and
extraction approach is able to meet or exceed results
attained by a combination of GIZA++ and heuristic
phrase extraction with significantly smaller phrase
table size. We also find that it achieves superior
BLEU scores over previously proposed ITG-based
phrase alignment approaches.
2 A Probabilistic Model for Phrase Table
Extraction
The problem of SMT can be defined as finding the
most probable target sentence e for the source sen-
tence f given a parallel training corpus ?E ,F?
e? = argmax
e
P (e|f , ?E ,F?).
We assume that there is a hidden set of parameters
? learned from the training data, and that e is condi-
tionally independent from the training corpus given
?. We take a Bayesian approach, integrating over all
possible values of the hidden parameters:
P (e|f , ?E ,F?) =
?
?
P (e|f , ?)P (?|?E ,F?). (1)
If ? takes the form of a scored phrase table, we
can use traditional methods for phrase-based SMT to
find P (e|f , ?) and concentrate on creating a model
for P (?|?E ,F?). We decompose this posterior prob-
ability using Bayes law into the corpus likelihood
and parameter prior probabilities
P (?|?E ,F?) ? P (?E ,F?|?)P (?).
In Section 3 we describe an existing method, and
in Section 4 we describe our proposed method for
modeling these two probabilities.
3 Flat ITG Model
There has been a significant amount of work in
many-to-many alignment techniques (Marcu and
Wong (2002), DeNero et al (2008), inter alia), and
in particular a number of recent works (Cherry and
Lin, 2007; Zhang et al, 2008; Blunsom et al, 2009)
have used the formalism of inversion transduction
grammars (ITGs) (Wu, 1997) to learn phrase align-
ments. By slightly limit reordering of words, ITGs
make it possible to exactly calculate probabilities
of phrasal alignments in polynomial time, which is
a computationally hard problem when arbitrary re-
ordering is allowed (DeNero and Klein, 2008).
The traditional flat ITG generative probabil-
ity for a particular phrase (or sentence) pair
Pflat(?e, f?; ?x, ?t) is parameterized by a phrase ta-
ble ?t and a symbol distribution ?x. We use the fol-
lowing generative story as a representative of the flat
ITG model.
1. Generate symbol x from the multinomial distri-
bution Px(x; ?x). x can take the values TERM,
REG, or INV.
2. According to the x take the following actions.
(a) If x = TERM, generate a phrase pair from
the phrase table Pt(?e, f?; ?t).
(b) If x = REG, a regular ITG rule, gener-
ate phrase pairs ?e1, f1? and ?e2, f2? from
Pflat, and concatenate them into a single
phrase pair ?e1e2, f1f2?.
(c) If x = INV, an inverted ITG rule, follows
the same process as (b), but concatenate
f1 and f2 in reverse order ?e1e2, f2f1?.
By taking the product of Pflat over every sentence
in the corpus, we are able to calculate the likelihood
P (?E ,F?|?) =
?
?e,f???E,F?
Pflat(?e, f?; ?).
We will refer to this model as FLAT.
3.1 Bayesian Modeling
While the previous formulation can be used as-is in
maximum likelihood training, this leads to a degen-
erate solution where every sentence is memorized as
a single phrase pair. Zhang et al (2008) and others
propose dealing with this problem by putting a prior
probability P (?x, ?t) on the parameters.
633
We assign ?x a Dirichlet prior1, and assign the
phrase table parameters ?t a prior using the Pitman-
Yor process (Pitman and Yor, 1997; Teh, 2006),
which is a generalization of the Dirichlet process
prior used in previous research. It is expressed as
?t ?PY (d, s, Pbase) (2)
where d is the discount parameter, s is the strength
parameter, and Pbase is the base measure. The dis-
count d is subtracted from observed counts, and
when it is given a large value (close to one), less
frequent phrase pairs will be given lower relative
probability than more common phrase pairs. The
strength s controls the overall sparseness of the dis-
tribution, and when it is given a small value the dis-
tribution will be sparse. Pbase is the prior probability
of generating a particular phrase pair, which we de-
scribe in more detail in the following section.
Non-parametric priors are well suited for mod-
eling the phrase distribution because every time a
phrase is generated by the model, it is ?memorized?
and given higher probability. Because of this, com-
mon phrase pairs are more likely to be re-used (the
rich-get-richer effect), which results in the induc-
tion of phrase tables with fewer, but more helpful
phrases. It is important to note that only phrases
generated by Pt are actually memorized and given
higher probability by the model. In FLAT, only min-
imal phrases generated after Px outputs the terminal
symbol TERM are generated from Pt, and thus only
minimal phrases are memorized by the model.
While the Dirichlet process is simply the Pitman-
Yor process with d = 0, it has been shown that the
discount parameter allows for more effective mod-
eling of the long-tailed distributions that are often
found in natural language (Teh, 2006). We con-
firmed in preliminary experiments (using the data
described in Section 7) that the Pitman-Yor process
with automatically adjusted parameters results in su-
perior alignment results, outperforming the sparse
Dirichlet process priors used in previous research2.
The average gain across all data sets was approxi-
mately 0.8 BLEU points.
1The value of ? had little effect on the results, so we arbi-
trarily set ? = 1.
2We put weak priors on s (Gamma(? = 2, ? = 1)) and
d (Beta(? = 2, ? = 2)) for the Pitman-Yor process, and set
? = 1?10 for the Dirichlet process.
3.2 Base Measure
Pbase in Equation (2) indicates the prior probability
of phrase pairs according to the model. By choosing
this probability appropriately, we can incorporate
prior knowledge of what phrases tend to be aligned
to each other. We calculate Pbase by first choosing
whether to generate an unaligned phrase pair (where
|e| = 0 or |f | = 0) according to a fixed probabil-
ity pu3, then generating from Pba for aligned phrase
pairs, or Pbu for unaligned phrase pairs.
For Pba, we adopt a base measure similar to that
used by DeNero et al (2008):
Pba(?e, f?) =M0(?e, f?)Ppois(|e|;?)Ppois(|f |;?)
M0(?e, f?) =(Pm1(f |e)Puni(e)Pm1(e|f)Puni(f))
1
2 .
Ppois is the Poisson distribution with the average
length parameter ?. As long phrases lead to spar-
sity, we set ? to a relatively small value to allow
us to bias against overly long phrases4. Pm1 is the
word-based Model 1 (Brown et al, 1993) probabil-
ity of one phrase given the other, which incorporates
word-based alignment information as prior knowl-
edge in the phrase translation probability. We take
the geometric mean5of the Model 1 probabilities in
both directions to encourage alignments that are sup-
ported by both models (Liang et al, 2006). It should
be noted that while Model 1 probabilities are used,
they are only soft constraints, compared with the
hard constraint of choosing a single word alignment
used in most previous phrase extraction approaches.
For Pbu, if g is the non-null phrase in e and f , we
calculate the probability as follows:
Pbu(?e, f?) = Puni(g)Ppois(|g|;?)/2.
Note that Pbu is divided by 2 as the probability is
considering null alignments in both directions.
4 Hierarchical ITG Model
While in FLAT only minimal phrases were memo-
rized by the model, as DeNero et al (2008) note
3We choose 10?2, 10?3, or 10?10 based on which value
gave the best accuracy on the development set.
4We tune ? to 1, 0.1, or 0.01 based on which value gives the
best performance on the development set.
5The probabilities of the geometric mean do not add to one,
but we found empirically that even when left unnormalized, this
provided much better results than the using the arithmetic mean,
which is more theoretically correct.
634
and we confirm in the experiments in Section 7, us-
ing only minimal phrases leads to inferior transla-
tion results for phrase-based SMT. Because of this,
previous research has combined FLAT with heuris-
tic phrase extraction, which exhaustively combines
all adjacent phrases permitted by the word align-
ments (Och et al, 1999). We propose an alterna-
tive, fully statistical approach that directly models
phrases at multiple granularities, which we will refer
to as HIER. By doing so, we are able to do away with
heuristic phrase extraction, creating a fully proba-
bilistic model for phrase probabilities that still yields
competitive results.
Similarly to FLAT, HIER assigns a probability
Phier(?e, f?; ?x, ?t) to phrase pairs, and is parame-
terized by a phrase table ?t and a symbol distribu-
tion ?x. The main difference from the generative
story of the traditional ITG model is that symbols
and phrase pairs are generated in the opposite order.
While FLAT first generates branches of the derivation
tree using Px, then generates leaves using the phrase
distribution Pt, HIER first attempts to generate the
full sentence as a single phrase from Pt, then falls
back to ITG-style derivations to cope with sparsity.
We allow for this within the Bayesian ITG context
by defining a new base measure Pdac (?divide-and-
conquer?) to replace Pbase in Equation (2), resulting
in the following distribution for ?t.
?t ? PY (d, s, Pdac) (3)
Pdac essentially breaks the generation of a sin-
gle longer phrase into two generations of shorter
phrases, allowing even phrase pairs for which
c(?e, f?) = 0 to be given some probability. The
generative process of Pdac, similar to that of Pflat
from the previous section, is as follows:
1. Generate symbol x from Px(x; ?x). x can take
the values BASE, REG, or INV.
2. According to x take the following actions.
(a) If x = BASE, generate a new phrase pair
directly from Pbase of Section 3.2.
(b) If x = REG, generate ?e1, f1? and ?e2, f2?
from Phier, and concatenate them into a
single phrase pair ?e1e2, f1f2?.
Figure 1: A word alignment (a), and its derivations ac-
cording to FLAT (b), and HIER (c). Solid and dotted lines
indicate minimal and non-minimal pairs respectively, and
phrases are written under their corresponding instance of
Pt. The pair hate/cou?te is generated from Pbase.
(c) If x = INV, follow the same process as
(b), but concatenate f1 and f2 in reverse
order ?e1e2, f2f1?.
A comparison of derivation trees for FLAT and
HIER is shown in Figure 1. As previously de-
scribed, FLAT first generates from the symbol dis-
tribution Px, then from the phrase distribution Pt,
while HIER generates directly from Pt, which falls
back to divide-and-conquer based on Px when nec-
essary. It can be seen that while Pt in FLAT only gen-
erates minimal phrases, Pt in HIER generates (and
thus memorizes) phrases at all levels of granularity.
4.1 Length-based Parameter Tuning
There are still two problems with HIER, one theo-
retical, and one practical. Theoretically, HIER con-
tains itself as its base measure, and stochastic pro-
cess models that include themselves as base mea-
sures are deficient, as noted in Cohen et al (2010).
Practically, while the Pitman-Yor process in HIER
shares the parameters s and d over all phrase pairs in
the model, long phrase pairs are much more sparse
635
Figure 2: Learned discount values by phrase pair length.
than short phrase pairs, and thus it is desirable to
appropriately adjust the parameters of Equation (2)
according to phrase pair length.
In order to solve these problems, we reformulate
the model so that each phrase length l = |f |+|e| has
its own phrase parameters ?t,l and symbol parame-
ters ?x,l, which are given separate priors:
?t,l ? PY (s, d, Pdac,l)
?x,l ? Dirichlet(?)
We will call this model HLEN.
The generative story is largely similar to HIER
with a few minor changes. When we generate a sen-
tence, we first choose its length l according to a uni-
form distribution over all possible sentence lengths
l ? Uniform(1, L),
where L is the size |e| + |f | of the longest sentence
in the corpus. We then generate a phrase pair from
the probability Pt,l(?e, f?) for length l. The base
measure for HLEN is identical to that of HIER, with
one minor change: when we fall back to two shorter
phrases, we choose the length of the left phrase from
ll ? Uniform(1, l ? 1), set the length of the right
phrase to lr = l?ll, and generate the smaller phrases
from Pt,ll and Pt,lr respectively.
It can be seen that phrases at each length are gen-
erated from different distributions, and thus the pa-
rameters for the Pitman-Yor process will be differ-
ent for each distribution. Further, as ll and lr must
be smaller than l, Pt,l no longer contains itself as a
base measure, and is thus not deficient.
An example of the actual discount values learned
in one of the experiments described in Section 7
is shown in Figure 2. It can be seen that, as ex-
pected, the discounts for short phrases are lower than
those of long phrases. In particular, phrase pairs of
length up to six (for example, |e| = 3, |f | = 3) are
given discounts of nearly zero while larger phrases
are more heavily discounted. We conjecture that this
is related to the observation by Koehn et al (2003)
that using phrases where max(|e|, |f |) ? 3 cause
significant improvements in BLEU score, while us-
ing larger phrases results in diminishing returns.
4.2 Implementation
Previous research has used a variety of sampling
methods to learn Bayesian phrase based alignment
models (DeNero et al, 2008; Blunsom et al, 2009;
Blunsom and Cohn, 2010). All of these techniques
are applicable to the proposed model, but we choose
to apply the sentence-based blocked sampling of
Blunsom and Cohn (2010), which has desirable con-
vergence properties compared to sampling single
alignments. As exhaustive sampling is too slow for
practical purpose, we adopt the beam search algo-
rithm of Saers et al (2009), and use a probability
beam, trimming spans where the probability is at
least 1010 times smaller than that of the best hypoth-
esis in the bucket.
One important implementation detail that is dif-
ferent from previous models is the management of
phrase counts. As a phrase pair ta may have been
generated from two smaller component phrases tb
and tc, when a sample containing ta is removed from
the distribution, it may also be necessary to decre-
ment the counts of tb and tc as well. The Chinese
Restaurant Process representation of Pt (Teh, 2006)
lends itself to a natural and easily implementable so-
lution to this problem. For each table representing a
phrase pair ta, we maintain not only the number of
customers sitting at the table, but also the identities
of phrases tb and tc that were originally used when
generating the table. When the count of the table
ta is reduced to zero and the table is removed, the
counts of tb and tc are also decremented.
5 Phrase Extraction
In this section, we describe both traditional heuris-
tic phrase extraction, and the proposed model-based
extraction method.
636
Figure 3: The phrase, block, and word alignments used
in heuristic phrase extraction.
5.1 Heuristic Phrase Extraction
The traditional method for heuristic phrase extrac-
tion from word alignments exhaustively enumerates
all phrases up to a certain length consistent with the
alignment (Och et al, 1999). Five features are used
in the phrase table: the conditional phrase proba-
bilities in both directions estimated using maximum
likelihood Pml(f |e) and Pml(e|f), lexical weight-
ing probabilities (Koehn et al, 2003), and a fixed
penalty for each phrase. We will call this heuristic
extraction from word alignments HEUR-W. These
word alignments can be acquired through the stan-
dard GIZA++ training regimen.
We use the combination of our ITG-based align-
ment with traditional heuristic phrase extraction as
a second baseline. An example of these alignments
is shown in Figure 3. In model HEUR-P, minimal
phrases generated from Pt are treated as aligned, and
we perform phrase extraction on these alignments.
However, as the proposed models tend to align rel-
atively large phrases, we also use two other tech-
niques to create smaller alignment chunks that pre-
vent sparsity. We perform regular sampling of the
trees, but if we reach a minimal phrase generated
from Pt, we continue traveling down the tree un-
til we reach either a one-to-many alignment, which
we will call HEUR-B as it creates alignments simi-
lar to the block ITG, or an at-most-one alignment,
which we will call HEUR-W as it generates word
alignments. It should be noted that forcing align-
ments smaller than the model suggests is only used
for generating alignments for use in heuristic extrac-
tion, and does not affect the training process.
5.2 Model-Based Phrase Extraction
We also propose a method for phrase table ex-
traction that directly utilizes the phrase probabil-
ities Pt(?e, f?). Similarly to the heuristic phrase
tables, we use conditional probabilities Pt(f |e)
and Pt(e|f), lexical weighting probabilities, and a
phrase penalty. Here, instead of using maximum
likelihood, we calculate conditional probabilities di-
rectly from Pt probabilities:
Pt(f |e) = Pt(?e, f?)/
?
{f? :c(?e,f??)?1}
Pt(?e, f??)
Pt(e|f) = Pt(?e, f?)/
?
{e?:c(?e?,f?)?1}
Pt(?e?, f?).
To limit phrase table size, we include only phrase
pairs that are aligned at least once in the sample.
We also include two more features: the phrase
pair joint probability Pt(?e, f?), and the average
posterior probability of each span that generated
?e, f? as computed by the inside-outside algorithm
during training. We use the span probability as it
gives a hint about the reliability of the phrase pair. It
will be high for common phrase pairs that are gen-
erated directly from the model, and also for phrases
that, while not directly included in the model, are
composed of two high probability child phrases.
It should be noted that while for FLAT and HIER Pt
can be used directly, as HLEN learns separate models
for each length, we must combine these probabilities
into a single value. We do this by setting
Pt(?e, f?) = Pt,l(?e, f?)c(l)/
L
?
l?=1
c(l?)
for every phrase pair, where l = |e|+ |f | and c(l) is
the number of phrases of length l in the sample.
We call this model-based extraction method MOD.
5.3 Sample Combination
As has been noted in previous works, (Koehn et al,
2003; DeNero et al, 2006) exhaustive phrase extrac-
tion tends to out-perform approaches that use syn-
tax or generative models to limit phrase boundaries.
DeNero et al (2006) state that this is because gen-
erative models choose only a single phrase segmen-
tation, and thus throw away many good phrase pairs
that are in conflict with this segmentation.
Luckily, in the Bayesian framework it is simple to
overcome this problem by combining phrase tables
637
from multiple samples. This is equivalent to approx-
imating the integral over various parameter configu-
rations in Equation (1). In MOD, we do this by taking
the average of the joint probability and span prob-
ability features, and re-calculating the conditional
probabilities from the averaged joint probabilities.
6 Related Work
In addition to the previously mentioned phrase
alignment techniques, there has also been a signif-
icant body of work on phrase extraction (Moore and
Quirk (2007), Johnson et al (2007a), inter alia).
DeNero and Klein (2010) presented the first work
on joint phrase alignment and extraction at multiple
levels. While they take a supervised approach based
on discriminative methods, we present a fully unsu-
pervised generative model.
A generative probabilistic model where longer
units are built through the binary combination of
shorter units was proposed by deMarcken (1996) for
monolingual word segmentation using the minimum
description length (MDL) framework. Our work dif-
fers in that it uses Bayesian techniques instead of
MDL, and works on two languages, not one.
Adaptor grammars, models in which non-
terminals memorize subtrees that lie below them,
have been used for word segmentation or other
monolingual tasks (Johnson et al, 2007b). The pro-
posed method could be thought of as synchronous
adaptor grammars over two languages. However,
adaptor grammars have generally been used to spec-
ify only two or a few levels as in the FLAT model in
this paper, as opposed to recursive models such as
HIER or many-leveled models such as HLEN. One
exception is the variational inference method for
adaptor grammars presented by Cohen et al (2010)
that is applicable to recursive grammars such as
HIER. We plan to examine variational inference for
the proposed models in future work.
7 Experimental Evaluation
We evaluate the proposed method on translation
tasks from four languages, French, German, Span-
ish, and Japanese, into English.
de-en es-en fr-en ja-en
TM (en) 1.80M 1.62M 1.35M 2.38M
TM (other) 1.85M 1.82M 1.56M 2.78M
LM (en) 52.7M 52.7M 52.7M 44.7M
Tune (en ) 49.8k 49.8k 49.8k 68.9k
Tune (other) 47.2k 52.6k 55.4k 80.4k
Test (en) 65.6k 65.6k 65.6k 40.4k
Test (other) 62.7k 68.1k 72.6k 48.7k
Table 1: The number of words in each corpus for TM and
LM training, tuning, and testing.
7.1 Experimental Setup
The data for French, German, and Spanish are from
the 2010 Workshop on Statistical Machine Transla-
tion (Callison-Burch et al, 2010). We use the news
commentary corpus for training the TM, and the
news commentary and Europarl corpora for training
the LM. For Japanese, we use data from the NTCIR
patent translation task (Fujii et al, 2008). We use
the first 100k sentences of the parallel corpus for the
TM, and the whole parallel corpus for the LM. De-
tails of both corpora can be found in Table 1. Cor-
pora are tokenized, lower-cased, and sentences of
over 40 words on either side are removed for TM
training. For both tasks, we perform weight tuning
and testing on specified development and test sets.
We compare the accuracy of our proposed method
of joint phrase alignment and extraction using the
FLAT, HIER and HLEN models, with a baseline of
using word alignments from GIZA++ and heuris-
tic phrase extraction. Decoding is performed using
Moses (Koehn and others, 2007) using the phrase
tables learned by each method under consideration,
as well as standard bidirectional lexical reordering
probabilities (Koehn et al, 2005). Maximum phrase
length is limited to 7 in all models, and for the LM
we use an interpolated Kneser-Ney 5-gram model.
For GIZA++, we use the standard training reg-
imen up to Model 4, and combine alignments
with grow-diag-final-and. For the proposed
models, we train for 100 iterations, and use the final
sample acquired at the end of the training process for
our experiments using a single sample6. In addition,
6For most models, while likelihood continued to increase
gradually for all 100 iterations, BLEU score gains plateaued af-
ter 5-10 iterations, likely due to the strong prior information
638
de-en es-en fr-en ja-en
Align Extract # Samp. BLEU Size BLEU Size BLEU Size BLEU Size
GIZA++ HEUR-W 1 16.62 4.91M 22.00 4.30M 21.35 4.01M 23.20 4.22M
FLAT MOD 1 13.48 136k 19.15 125k 17.97 117k 16.10 89.7k
HIER MOD 1 16.58 1.02M 21.79 859k 21.50 751k 23.23 723k
HLEN MOD 1 16.49 1.17M 21.57 930k 21.31 860k 23.19 820k
HIER MOD 10 16.53 3.44M 21.84 2.56M 21.57 2.63M 23.12 2.21M
HLEN MOD 10 16.51 3.74M 21.69 3.00M 21.53 3.09M 23.20 2.70M
Table 2: BLEU score and phrase table size by alignment method, extraction method, and samples combined. Bold
numbers are not significantly different from the best result according to the sign test (p < 0.05) (Collins et al, 2005).
we also try averaging the phrase tables from the last
ten samples as described in Section 5.3.
7.2 Experimental Results
The results for these experiments can be found in Ta-
ble 2. From these results we can see that when using
a single sample, the combination of using HIER and
model probabilities achieves results approximately
equal to GIZA++ and heuristic phrase extraction.
This is the first reported result in which an unsu-
pervised phrase alignment model has built a phrase
table directly from model probabilities and achieved
results that compare to heuristic phrase extraction. It
can also be seen that the phrase table created by the
proposed method is approximately 5 times smaller
than that obtained by the traditional pipeline.
In addition, HIER significantly outperforms FLAT
when using the model probabilities. This confirms
that phrase tables containing only minimal phrases
are not able to achieve results that compete with
phrase tables that use multiple granularities.
Somewhat surprisingly, HLEN consistently
slightly underperforms HIER. This indicates
potential gains to be provided by length-based
parameter tuning were outweighed by losses due
to the increased complexity of the model. In
particular, we believe the necessity to combine
probabilities from multiple Pt,l models into a single
phrase table may have resulted in a distortion of the
phrase probabilities. In addition, the assumption
that phrase lengths are generated from a uniform
distribution is likely too strong, and further gains
provided by Pbase. As iterations took 1.3 hours on a single
processor, good translation results can be achieved in approxi-
mately 13 hours, which could further reduced using distributed
sampling (Newman et al, 2009; Blunsom et al, 2009).
FLAT HIER
MOD 17.97 117k 21.50 751k
HEUR-W 21.52 5.65M 21.68 5.39M
HEUR-B 21.45 4.93M 21.41 2.61M
HEUR-P 21.56 4.88M 21.47 1.62M
Table 3: Translation results and phrase table size for var-
ious phrase extraction techniques (French-English).
could likely be achieved by more accurate modeling
of phrase lengths. We leave further adjustments to
the HLEN model to future work.
It can also be seen that combining phrase tables
from multiple samples improved the BLEU score
for HLEN, but not for HIER. This suggests that for
HIER, most of the useful phrase pairs discovered by
the model are included in every iteration, and the in-
creased recall obtained by combining multiple sam-
ples does not consistently outweigh the increased
confusion caused by the larger phrase table.
We also evaluated the effectiveness of model-
based phrase extraction compared to heuristic phrase
extraction. Using the alignments from HIER, we cre-
ated phrase tables using model probabilities (MOD),
and heuristic extraction on words (HEUR-W), blocks
(HEUR-B), and minimal phrases (HEUR-P) as de-
scribed in Section 5. The results of these ex-
periments are shown in Table 3. It can be seen
that model-based phrase extraction using HIER out-
performs or insignificantly underperforms heuris-
tic phrase extraction over all experimental settings,
while keeping the phrase table to a fraction of the
size of most heuristic extraction methods.
Finally, we varied the size of the parallel corpus
for the Japanese-English task from 50k to 400k sen-
639
Figure 4: The effect of corpus size on the accuracy (a) and
phrase table size (b) for each method (Japanese-English).
tences and measured the effect of corpus size on
translation accuracy. From the results in Figure 4
(a), it can be seen that at all corpus sizes, the re-
sults from all three methods are comparable, with
insignificant differences between GIZA++ and HIER
at all levels, and HLEN lagging slightly behind HIER.
Figure 4 (b) shows the size of the phrase table in-
duced by each method over the various corpus sizes.
It can be seen that the tables created by GIZA++ are
significantly larger at all corpus sizes, with the dif-
ference being particularly pronounced at larger cor-
pus sizes.
8 Conclusion
In this paper, we presented a novel approach to joint
phrase alignment and extraction through a hierar-
chical model using non-parametric Bayesian meth-
ods and inversion transduction grammars. Machine
translation systems using phrase tables learned di-
rectly by the proposed model were able to achieve
accuracy competitive with the traditional pipeline of
word alignment and heuristic phrase extraction, the
first such result for an unsupervised model.
For future work, we plan to refine HLEN to use
a more appropriate model of phrase length than
the uniform distribution, particularly by attempting
to bias against phrase pairs where one of the two
phrases is much longer than the other. In addition,
we will test probabilities learned using the proposed
model with an ITG-based decoder. We will also ex-
amine the applicability of the proposed model in the
context of hierarchical phrases (Chiang, 2007), or
in alignment using syntactic structure (Galley et al,
2006). It is also worth examining the plausibility
of variational inference as proposed by Cohen et al
(2010) in the alignment context.
Acknowledgments
This work was performed while the first author
was supported by the JSPS Research Fellowship for
Young Scientists.
References
Phil Blunsom and Trevor Cohn. 2010. Inducing syn-
chronous grammars with slice sampling. In Proceed-
ings of the Human Language Technology: The 11th
Annual Conference of the North American Chapter of
the Association for Computational Linguistics.
Phil Blunsom, Trevor Cohn, Chris Dyer, and Miles Os-
borne. 2009. A Gibbs sampler for phrasal syn-
chronous grammar induction. In Proceedings of the
47th Annual Meeting of the Association for Computa-
tional Linguistics, pages 782?790.
Peter F. Brown, Vincent J.Della Pietra, Stephen A. Della
Pietra, and Robert. L. Mercer. 1993. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, 19:263?311.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Kay Peterson, Mark Przybocki, and Omar F. Zaidan.
2010. Findings of the 2010 joint workshop on sta-
tistical machine translation and metrics for machine
translation. In Proceedings of the Joint 5th Workshop
on Statistical Machine Translation and MetricsMATR,
pages 17?53.
Colin Cherry and Dekang Lin. 2007. Inversion transduc-
tion grammar for joint phrasal translation modeling.
In Proceedings of the NAACL Workshop on Syntax and
Structure in Machine Translation.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
Shay B. Cohen, David M. Blei, and Noah A. Smith.
2010. Variational inference for adaptor grammars. In
Proceedings of the Human Language Technology: The
640
11th Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 564?572.
Michael Collins, Philipp Koehn, and Ivona Kuc?erova?.
2005. Clause restructuring for statistical machine
translation. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguistics,
pages 531?540.
Carl de Marcken. 1996. Unsupervised Language Acqui-
sition. Ph.D. thesis, Massachusetts Institute of Tech-
nology.
John DeNero and Dan Klein. 2008. The complexity of
phrase alignment problems. In Proceedings of the 46th
Annual Meeting of the Association for Computational
Linguistics, pages 25?28.
John DeNero and Dan Klein. 2010. Discriminative mod-
eling of extraction sets for machine translation. In Pro-
ceedings of the 48th Annual Meeting of the Association
for Computational Linguistics, pages 1453?1463.
John DeNero, Dan Gillick, James Zhang, and Dan Klein.
2006. Why generative phrase models underperform
surface heuristics. In Proceedings of the 1st Workshop
on Statistical Machine Translation, pages 31?38.
John DeNero, Alex Bouchard-Co?te?, and Dan Klein.
2008. Sampling alignment structure under a Bayesian
translation model. In Proceedings of the Conference
on Empirical Methods in Natural Language Process-
ing, pages 314?323.
Atsushi Fujii, Masao Utiyama, Mikio Yamamoto, and
Takehito Utsuro. 2008. Overview of the patent trans-
lation task at the NTCIR-7 workshop. In Proceedings
of the 7th NTCIR Workshop Meeting on Evaluation of
Information Access Technologies, pages 389?400.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proceed-
ings of the 44th Annual Meeting of the Association for
Computational Linguistics, pages 961?968.
J. Howard Johnson, Joel Martin, George Foster, and
Roland Kuhn. 2007a. Improving translation quality
by discarding most of the phrasetable. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing.
Mark Johnson, Thomas L. Griffiths, and Sharon Goldwa-
ter. 2007b. Adaptor grammars: A framework for spec-
ifying compositional nonparametric Bayesian models.
Advances in Neural Information Processing Systems,
19:641.
Philipp Koehn et al 2007. Moses: Open source toolkit
for statistical machine translation. In Proceedings of
the 45th Annual Meeting of the Association for Com-
putational Linguistics.
Phillip Koehn, Franz Josef Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceedings of
the Human Language Technology Conference (HLT-
NAACL), pages 48?54.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne, and
David Talbot. 2005. Edinburgh system description
for the 2005 IWSLT speech translation evaluation. In
Proceedings of the International Workshop on Spoken
Language Translation.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of the Human
Language Technology Conference - North American
Chapter of the Association for Computational Linguis-
tics Annual Meeting (HLT-NAACL), pages 104?111.
Daniel Marcu andWilliamWong. 2002. A phrase-based,
joint probability model for statistical machine transla-
tion. pages 133?139.
Robert C. Moore and Chris Quirk. 2007. An iteratively-
trained segmentation-free phrase translation model for
statistical machine translation. In Proceedings of
the 2nd Workshop on Statistical Machine Translation,
pages 112?119.
David Newman, Arthur Asuncion, Padhraic Smyth, and
Max Welling. 2009. Distributed algorithms for
topic models. Journal of Machine Learning Research,
10:1801?1828.
Franz Josef Och, Christoph Tillmann, and Hermann Ney.
1999. Improved alignment models for statistical ma-
chine translation. In Proceedings of the 4th Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 20?28.
Jim Pitman and Marc Yor. 1997. The two-parameter
Poisson-Dirichlet distribution derived from a stable
subordinator. The Annals of Probability, 25(2):855?
900.
Markus Saers, Joakim Nivre, and Dekai Wu. 2009.
Learning stochastic bracketing inversion transduction
grammars with a cubic time biparsing algorithm. In
Proceedings of the The 11th International Workshop
on Parsing Technologies.
Yee Whye Teh. 2006. A hierarchical Bayesian language
model based on Pitman-Yor processes. In Proceed-
ings of the 44th Annual Meeting of the Association for
Computational Linguistics.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?403.
Hao Zhang, Chris Quirk, Robert C. Moore, and
Daniel Gildea. 2008. Bayesian learning of non-
compositional phrases with synchronous parsing. Pro-
ceedings of the 46th Annual Meeting of the Association
for Computational Linguistics, pages 97?105.
641
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 165?174,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Machine Translation without Words through Substring Alignment
Graham Neubig1,2, Taro Watanabe2, Shinsuke Mori1, Tatsuya Kawahara1
1Graduate School of Informatics, Kyoto University
Yoshida Honmachi, Sakyo-ku, Kyoto, Japan
2National Institute of Information and Communication Technology
3-5 Hikari-dai, Seika-cho, Soraku-gun, Kyoto, Japan
Abstract
In this paper, we demonstrate that accu-
rate machine translation is possible without
the concept of ?words,? treating MT as a
problem of transformation between character
strings. We achieve this result by applying
phrasal inversion transduction grammar align-
ment techniques to character strings to train
a character-based translation model, and us-
ing this in the phrase-based MT framework.
We also propose a look-ahead parsing algo-
rithm and substring-informed prior probabil-
ities to achieve more effective and efficient
alignment. In an evaluation, we demonstrate
that character-based translation can achieve
results that compare to word-based systems
while effectively translating unknown and un-
common words over several language pairs.
1 Introduction
Traditionally, the task of statistical machine trans-
lation (SMT) is defined as translating a source sen-
tence fJ1 = {f1, . . . , fJ} to a target sentence eI1 =
{e1, . . ., eI}, where each element of fJ1 and eI1 is
assumed to be a word in the source and target lan-
guages. However, the definition of a ?word? is of-
ten problematic. The most obvious example of this
lies in languages that do not separate words with
white space such as Chinese, Japanese, or Thai, in
which the choice of a segmentation standard has
a large effect on translation accuracy (Chang et
al., 2008). Even for languages with explicit word
The first author is now affiliated with the Nara Institute of Sci-
ence and Technology.
boundaries, all machine translation systems perform
at least some precursory form of tokenization, split-
ting punctuation and words to prevent the sparsity
that would occur if punctuated and non-punctuated
words were treated as different entities. Sparsity
also manifests itself in other forms, including the
large vocabularies produced by morphological pro-
ductivity, word compounding, numbers, and proper
names. A myriad of methods have been proposed
to handle each of these phenomena individually,
including morphological analysis, stemming, com-
pound breaking, number regularization, optimizing
word segmentation, and transliteration, which we
outline in more detail in Section 2.
These difficulties occur because we are translat-
ing sequences of words as our basic unit. On the
other hand, Vilar et al (2007) examine the possibil-
ity of instead treating each sentence as sequences of
characters to be translated. This method is attrac-
tive, as it is theoretically able to handle all sparsity
phenomena in a single unified framework, but has
only been shown feasible between similar language
pairs such as Spanish-Catalan (Vilar et al, 2007),
Swedish-Norwegian (Tiedemann, 2009), and Thai-
Lao (Sornlertlamvanich et al, 2008), which have
a strong co-occurrence between single characters.
As Vilar et al (2007) state and we confirm, accu-
rate translations cannot be achieved when applying
traditional translation techniques to character-based
translation for less similar language pairs.
In this paper, we propose improvements to the
alignment process tailored to character-based ma-
chine translation, and demonstrate that it is, in fact,
possible to achieve translation accuracies that ap-
165
proach those of traditional word-based systems us-
ing only character strings. We draw upon recent
advances in many-to-many alignment, which allows
for the automatic choice of the length of units to
be aligned. As these units may be at the charac-
ter, subword, word, or multi-word phrase level, we
conjecture that this will allow for better character
alignments than one-to-many alignment techniques,
and will allow for better translation of uncommon
words than traditional word-based models by break-
ing down words into their component parts.
We also propose two improvements to the many-
to-many alignment method of Neubig et al (2011).
One barrier to applying many-to-many alignment
models to character strings is training cost. In the
inversion transduction grammar (ITG) framework
(Wu, 1997), which is widely used in many-to-many
alignment, search is cumbersome for longer sen-
tences, a problem that is further exacerbated when
using characters instead of words as the basic unit.
As a step towards overcoming this difficulty, we in-
crease the efficiency of the beam-search technique of
Saers et al (2009) by augmenting it with look-ahead
probabilities in the spirit of A* search. Secondly,
we describe a method to seed the search process us-
ing counts of all substring pairs in the corpus to bias
the phrase alignment model. We do this by defining
prior probabilities based on these substring counts
within the Bayesian phrasal ITG framework.
An evaluation on four language pairs with differ-
ing morphological properties shows that for distant
language pairs, character-based SMT can achieve
translation accuracy comparable to word-based sys-
tems. In addition, we perform ablation studies,
showing that these results were not possible with-
out the proposed enhancements to the model. Fi-
nally, we perform a qualitative analysis, which finds
that character-based translation can handle unseg-
mented text, conjugation, and proper names in a uni-
fied framework with no additional processing.
2 Related Work on Data Sparsity in SMT
As traditional SMT systems treat all words as single
tokens without considering their internal structure,
major problems of data sparsity occur for less fre-
quent tokens. In fact, it has been shown that there
is a direct negative correlation between vocabulary
size (and thus sparsity) of a language and transla-
tion accuracy (Koehn, 2005). Sparsity causes trou-
ble for alignment models, both in the form of incor-
rectly aligned uncommon words, and in the form of
garbage collection, where uncommon words in one
language are incorrectly aligned to large segments
of the sentence in the other language (Och and Ney,
2003). Unknown words are also a problem during
the translation process, and the default approach is
to map them as-is into the target sentence.
This is a major problem in agglutinative lan-
guages such as Finnish or compounding languages
such as German. Previous works have attempted to
handle morphology, decompounding and regulariza-
tion through lemmatization, morphological analysis,
or unsupervised techniques (Nie?en and Ney, 2000;
Brown, 2002; Lee, 2004; Goldwater and McClosky,
2005; Talbot and Osborne, 2006; Mermer and Ak?n,
2010; Macherey et al, 2011). It has also been noted
that it is more difficult to translate into morpho-
logically rich languages, and methods for modeling
target-side morphology have attracted interest in re-
cent years (Bojar, 2007; Subotin, 2011).
Another source of data sparsity that occurs in all
languages is proper names, which have been handled
by using cognates or transliteration to improve trans-
lation (Knight and Graehl, 1998; Kondrak et al,
2003; Finch and Sumita, 2007), and more sophisti-
cated methods for named entity translation that com-
bine translation and transliteration have also been
proposed (Al-Onaizan and Knight, 2002).
Choosing word units is also essential for creat-
ing good translation results for languages that do
not explicitly mark word boundaries, such as Chi-
nese, Japanese, and Thai. A number of works have
dealt with this word segmentation problem in trans-
lation, mainly focusing on Chinese-to-English trans-
lation (Bai et al, 2008; Chang et al, 2008; Zhang et
al., 2008b; Chung and Gildea, 2009; Nguyen et al,
2010), although these works generally assume that a
word segmentation exists in one language (English)
and attempt to optimize the word segmentation in
the other language (Chinese).
We have enumerated these related works to
demonstrate the myriad of data sparsity problems
and proposed solutions. Character-based transla-
tion has the potential to handle all of the phenom-
ena in the previously mentioned research in a single
166
unified framework, requiring no language specific
tools such as morphological analyzers or word seg-
menters. However, while the approach is attractive
conceptually, previous research has only been shown
effective for closely related language pairs (Vilar et
al., 2007; Tiedemann, 2009; Sornlertlamvanich et
al., 2008). In this work, we propose effective align-
ment techniques that allow character-based transla-
tion to achieve accurate translation results for both
close and distant language pairs.
3 Alignment Methods
SMT systems are generally constructed from a par-
allel corpus consisting of target language sentences
E and source language sentences F . The first step
of training is to find alignments A for the words in
each sentence pair.
We represent our target and source sentences as
eI1 and fJ1 . ei and fj represent single elements of
the target and source sentences respectively. These
may be words in word-based alignment models or
single characters in character-based alignment mod-
els.1 We define our alignment as aK1 , where each
element is a span ak = ?s, t, u, v? indicating that the
target string es, . . . , et and source string fu, . . . , fv
are aligned to each-other.
3.1 One-to-Many Alignment
The most well-known and widely-used models for
bitext alignment are for one-to-many alignment, in-
cluding the IBM models (Brown et al, 1993) and
HMM alignment model (Vogel et al, 1996). These
models are by nature directional, attempting to find
the alignments that maximize the conditional prob-
ability of the target sentence P (eI1|fJ1 ,aK1 ). For
computational reasons, the IBM models are re-
stricted to aligning each word on the target side to
a single word on the source side. In the formal-
ism presented above, this means that each ei must
be included in at most one span, and for each span
u = v. Traditionally, these models are run in both
directions and combined using heuristics to create
many-to-many alignments (Koehn et al, 2003).
However, in order for one-to-many alignment
methods to be effective, each fj must contain
1Some previous work has also performed alignment using
morphological analyzers to normalize or split the sentence into
morpheme streams (Corston-Oliver and Gamon, 2004).
enough information to allow for effective alignment
with its corresponding elements in eI1. While this is
often the case in word-based models, for character-
based models this assumption breaks down, as there
is often no clear correspondence between characters.
3.2 Many-to-Many Alignment
On the other hand, in recent years, there have been
advances in many-to-many alignment techniques
that are able to align multi-element chunks on both
sides of the translation (Marcu and Wong, 2002;
DeNero et al, 2008; Blunsom et al, 2009; Neu-
big et al, 2011). Many-to-many methods can be ex-
pected to achieve superior results on character-based
alignment, as the aligner can use information about
substrings, which may correspond to letters, mor-
phemes, words, or short phrases.
Here, we focus on the model presented by Neu-
big et al (2011), which uses Bayesian inference in
the phrasal inversion transduction grammar (ITG,
Wu (1997)) framework. ITGs are a variety of syn-
chronous context free grammar (SCFG) that allows
for many-to-many alignment to be achieved in poly-
nomial time through the process of biparsing, which
we explain more in the following section. Phrasal
ITGs are ITGs that allow for non-terminals that can
emit phrase pairs with multiple elements on both
the source and target sides. It should be noted
that there are other many-to-many alignment meth-
ods that have been used for simultaneously discov-
ering morphological boundaries over multiple lan-
guages (Snyder and Barzilay, 2008; Naradowsky
and Toutanova, 2011), but these have generally been
applied to single words or short phrases, and it is not
immediately clear that they will scale to aligning full
sentences.
4 Look-Ahead Biparsing
In this work, we experiment with the alignment
method of Neubig et al (2011), which can achieve
competitive accuracy with a much smaller phrase ta-
ble than traditional methods. This is important in
the character-based translation context, as we would
like to use phrases that contain large numbers of
characters without creating a phrase table so large
that it cannot be used in actual decoding. In this
framework, training is performed using sentence-
167
Figure 1: (a) A chart with inside probabilities in boxes
and forward/backward probabilities marking the sur-
rounding arrows. (b) Spans with corresponding look-
aheads added, and the minimum probability underlined.
Lightly and darkly shaded spans will be trimmed when
the beam is log(P ) ? ?3 and log(P ) ? ?6 respectively.
wise block sampling, acquiring a sample for each
sentence by first performing bottom-up biparsing to
create a chart of probabilities, then performing top-
down sampling of a new tree based on the probabil-
ities in this chart.
An example of a chart used in this parsing can
be found in Figure 1 (a). Within each cell of the
chart spanning ets and fvu is an ?inside? probabil-
ity I(as,t,u,v). This probability is the combination
of the generative probability of each phrase pair
Pt(ets,fvu) as well as the sum the probabilities over
all shorter spans in straight and inverted order2
I(as,t,u,v) = Pt(ets, fvu)
+
?
s?S?t
?
u?U?v
Px(str)I(as,S,u,U )I(aS,t,U,v)
+
?
s?S?t
?
u?U?v
Px(inv)I(as,S,U,v)I(aS,t,u,U )
where Px(str) and Px(inv) are the probability of
straight and inverted ITG productions.
While the exact calculation of these probabilities
can be performed in O(n6) time, where n is the
2Pt can be specified according to Bayesian statistics as de-
scribed by Neubig et al (2011).
length of the sentence, this is impractical for all but
the shortest sentences. Thus it is necessary to use
methods to reduce the search space such as beam-
search based chart parsing (Saers et al, 2009) or
slice sampling (Blunsom and Cohn, 2010).3
In this section we propose the use of a look-ahead
probability to increase the efficiency of this chart
parsing. Taking the example of Saers et al (2009),
spans are pushed onto a different queue based on
their size, and queues are processed in ascending or-
der of size. Agendas can further be trimmed based
on a histogram beam (Saers et al, 2009) or probabil-
ity beam (Neubig et al, 2011) compared to the best
hypothesis a?. In other words, we have a queue dis-
cipline based on the inside probability, and all spans
ak where I(ak) < cI(a?) are pruned. c is a constant
describing the width of the beam, and a smaller con-
stant probability will indicate a wider beam.
This method is insensitive to the existence of
competing hypotheses when performing pruning.
Figure 1 (a) provides an example of why it is unwise
to ignore competing hypotheses during beam prun-
ing. Particularly, the alignment ?les/1960s? com-
petes with the high-probability alignment ?les/the,?
so intuitively should be a good candidate for prun-
ing. However its probability is only slightly higher
than ?anne?es/1960s,? which has no competing hy-
potheses and thus should not be trimmed.
In order to take into account competing hypothe-
ses, we can use for our queue discipline not only the
inside probability I(ak), but also the outside proba-
bility O(ak), the probability of generating all spans
other than ak, as in A* search for CFGs (Klein and
Manning, 2003), and tic-tac-toe pruning for word-
based ITGs (Zhang and Gildea, 2005). As the cal-
culation of the actual outside probability O(ak) is
just as expensive as parsing itself, it is necessary to
approximate this with heuristic function O? that can
be calculated efficiently.
Here we propose a heuristic function that is de-
signed specifically for phrasal ITGs and is com-
putable with worst-case complexity of n2, compared
with the n3 amortized time of the tic-tac-toe pruning
3Applying beam-search before sampling will sample from
an improper distribution, although Metropolis-in-Gibbs sam-
pling (Johnson et al, 2007) can be used to compensate. How-
ever, we found that this had no significant effect on results, so
we omit the Metropolis-in-Gibbs step for experiments.
168
algorithm described by (Zhang et al, 2008a). Dur-
ing the calculation of the phrase generation proba-
bilities Pt, we save the best inside probability I? for
each monolingual span.
I?e (s, t) = max
{a?=?s?,t?,u?,v??;s?=s,t?=t}
Pt(a?)
I?f (u, v) = max
{a?=?s?,t?,u?,v??;u?=u,v?=v}
Pt(a?)
For each language independently, we calculate for-
ward probabilities ? and backward probabilities ?.
For example, ?e(s) is the maximum probability of
the span (0, s) of e that can be created by concate-
nating together consecutive values of I?e :
?e(s) = max
{S1,...,Sx}
I?e (0, S1)I?e (S1, S2) . . . I?e (Sx, s).
Backwards probabilities and probabilities over f can
be defined similarly. These probabilities are calcu-
lated for e and f independently, and can be calcu-
lated in n2 time by processing each ? in ascending
order, and each ? in descending order in a fashion
similar to that of the forward-backward algorithm.
Finally, for any span, we define the outside heuristic
as the minimum of the two independent look-ahead
probabilities over each language
O?(as,t,u,v) = min(?e(s) ? ?e(t), ?f (u) ? ?f (v)).
Looking again at Figure 1 (b), it can be seen
that the relative probability difference between the
highest probability span ?les/the? and the spans
?anne?es/1960s? and ?60/1960s? decreases, allowing
for tighter beam pruning without losing these good
hypotheses. In contrast, the relative probability of
?les/1960s? remains low as it is in conflict with a
high-probability alignment, allowing it to be dis-
carded.
5 Substring Prior Probabilities
While the Bayesian phrasal ITG framework uses
the previously mentioned phrase distribution Pt dur-
ing search, it also allows for definition of a phrase
pair prior probability Pprior(ets,fvu), which can ef-
ficiently seed the search process with a bias towards
phrase pairs that satisfy certain properties. In this
section, we overview an existing method used to cal-
culate these prior probabilities, and also propose a
new way to calculate priors based on substring co-
occurrence statistics.
5.1 Word-based Priors
Previous research on many-to-many translation has
used IBM model 1 probabilities to bias phrasal
alignments so that phrases whose member words are
good translations are also aligned. As a representa-
tive of this existing method, we adopt a base mea-
sure similar to that used by DeNero et al (2008):
Pm1(e,f) =M0(e,f)Ppois(|e|;?)Ppois(|f |;?)
M0(e,f) =(Pm1(f |e)Puni(e)Pm1(e|f)Puni(f))
1
2 .
Ppois is the Poisson distribution with the average
length parameter ?, which we set to 0.01. Pm1 is the
word-based (or character-based) Model 1 probabil-
ity, which can be efficiently calculated using the dy-
namic programming algorithm described by Brown
et al (1993). However, for reasons previously stated
in Section 3, these methods are less satisfactory
when performing character-based alignment, as the
amount of information contained in a character does
not allow for proper alignment.
5.2 Substring Co-occurrence Priors
Instead, we propose a method for using raw sub-
string co-occurrence statistics to bias alignments to-
wards substrings that often co-occur in the entire
training corpus. This is similar to the method of
Cromieres (2006), but instead of using these co-
occurrence statistics as a heuristic alignment crite-
rion, we incorporate them as a prior probability in
a statistical model that can take into account mutual
exclusivity of overlapping substrings in a sentence.
We define this prior probability using three counts
over substrings c(e), c(f), and c(e,f). c(e) and
c(f) count the total number of sentences in which
the substrings e and f occur respectively. c(e,f) is
a count of the total number of sentences in which the
substring e occurs on the target side, and f occurs
on the source side. We perform the calculation of
these statistics using enhanced suffix arrays, a data
structure that can efficiently calculate all substrings
in a corpus (Abouelhoda et al, 2004).4
While suffix arrays allow for efficient calculation
of these statistics, storing all co-occurrence counts
c(e,f) is an unrealistic memory burden for larger
4Using the open-source implementation esaxx http://
code.google.com/p/esaxx/
169
corpora. In order to reduce the amount of mem-
ory used, we discount every count by a constant d,
which we set to 5. This has a dual effect of reducing
the amount of memory needed to hold co-occurrence
counts by removing values for which c(e,f) < d, as
well as preventing over-fitting of the training data. In
addition, we heuristically prune values for which the
conditional probabilities P (e|f) or P (f |e) are less
than some fixed value, which we set to 0.1 for the
reported experiments.
To determine how to combine c(e), c(f), and
c(e,f) into prior probabilities, we performed pre-
liminary experiments testing methods proposed by
previous research including plain co-occurrence
counts, the Dice coefficient, and ?-squared statistics
(Cromieres, 2006), as well as a newmethod of defin-
ing substring pair probabilities to be proportional to
bidirectional conditional probabilities
Pcooc(e,f) = Pcooc(e|f)Pcooc(f |e)/Z
=
(
c(e,f) ? d
c(f) ? d
)(
c(e,f) ? d
c(e) ? d
)
/Z
for all substring pairs where c(e,f) > d and where
Z is a normalization term equal to
Z =
?
{e,f ;c(e,f)>d}
Pcooc(e|f)Pcooc(f |e).
The experiments showed that the bidirectional con-
ditional probability method gave significantly better
results than all other methods, so we adopt this for
the remainder of our experiments.
It should be noted that as we are using discount-
ing, many substring pairs will be given zero proba-
bility according to Pcooc. As the prior is only sup-
posed to bias the model towards good solutions and
not explicitly rule out any possibilities, we linearly
interpolate the co-occurrence probability with the
one-to-many Model 1 probability, which will give
at least some probability mass to all substring pairs
Pprior(e,f) = ?Pcooc(e,f) + (1 ? ?)Pm1(e,f).
We put a Dirichlet prior (? = 1) on the interpolation
coefficient ? and learn it during training.
6 Experiments
In order to test the effectiveness of character-based
translation, we performed experiments over a variety
of language pairs and experimental settings.
de-en fi-en fr-en ja-en
TM (en) 2.80M 3.10M 2.77M 2.13M
TM (other) 2.56M 2.23M 3.05M 2.34M
LM (en) 16.0M 15.5M 13.8M 11.5M
LM (other) 15.3M 11.3M 15.6M 11.9M
Tune (en) 58.7k 58.7k 58.7k 30.8k
Tune (other) 55.1k 42.0k 67.3k 34.4k
Test (en) 58.0k 58.0k 58.0k 26.6k
Test (other) 54.3k 41.4k 66.2k 28.5k
Table 1: The number of words in each corpus for TM and
LM training, tuning, and testing.
6.1 Experimental Setup
We use a combination of four languages with En-
glish, using freely available data. We selected
French-English, German-English, Finnish-English
data from EuroParl (Koehn, 2005), with develop-
ment and test sets designated for the 2005 ACL
shared task on machine translation.5 We also did
experiments with Japanese-English Wikipedia arti-
cles from the Kyoto Free Translation Task (Neu-
big, 2011) using the designated training and tuning
sets, and reporting results on the test set. These lan-
guages were chosen as they have a variety of inter-
esting characteristics. French has some inflection,
but among the test languages has the strongest one-
to-one correspondence with English, and is gener-
ally considered easy to translate. German has many
compound words, which must be broken apart to
translate properly into English. Finnish is an ag-
glutinative language with extremely rich morphol-
ogy, resulting in long words and the largest vocab-
ulary of the languages in EuroParl. Japanese does
not have any clear word boundaries, and uses logo-
graphic characters, which contain more information
than phonetic characters.
With regards to data preparation, the EuroParl
data was pre-tokenized, so we simply used the to-
kenized data as-is for the training and evaluation of
all models. For word-based translation in the Kyoto
task, training was performed using the provided tok-
enization scripts. For character-based translation, no
tokenization was performed, using the original text
for both training and decoding. For both tasks, we
selected as training data all sentences for which both
5http://statmt.org/wpt05/mt-shared-task
170
de-en fi-en fr-en ja-en
GIZA-word 24.58 / 64.28 / 30.43 20.41 / 60.01 / 27.89 30.23 / 68.79 / 34.20 17.95 / 56.47 / 24.70
ITG-word 23.87 / 64.89 / 30.71 20.83 / 61.04 / 28.46 29.92 / 68.64 / 34.29 17.14 / 56.60 / 24.89
GIZA-char 08.05 / 45.01 / 15.35 06.91 / 41.62 / 14.39 11.05 / 48.23 / 17.80 09.46 / 49.02 / 18.34
ITG-char 21.79 / 64.47 / 30.12 18.38 / 62.44 / 28.94 26.70 / 66.76 / 32.47 15.84 / 58.41 / 24.58
en-de en-fi en-fr en-ja
GIZA-word 17.94 / 62.71 / 37.88 13.22 / 58.50 / 27.03 32.19 / 69.20 / 52.39 20.79 / 27.01 / 38.41
ITG-word 17.47 / 63.18 / 37.79 13.12 / 59.27 / 27.09 31.66 / 69.61 / 51.98 20.26 / 28.34 / 38.34
GIZA-char 06.17 / 41.04 / 19.90 04.58 / 35.09 / 11.76 10.31 / 42.84 / 25.06 01.48 / 00.72 / 06.67
ITG-char 15.35 / 61.95 / 35.45 12.14 / 59.02 / 25.31 27.74 / 67.44 / 48.56 17.90 / 28.46 / 35.71
Table 2: Translation results in word-based BLEU, character-based BLEU, and METEOR for the GIZA++ and phrasal
ITG models for word and character-based translation, with bold numbers indicating a statistically insignificant differ-
ence from the best system according to the bootstrap resampling method at p = 0.05 (Koehn, 2004).
source and target were 100 characters or less,6 the
total size of which is shown in Table 1. In character-
based translation, white spaces between words were
treated as any other character and not given any spe-
cial treatment. Evaluation was performed on tok-
enized and lower-cased data.
For alignment, we use the GIZA++ implementa-
tion of one-to-many alignment7 and the pialign im-
plementation of the phrasal ITG models8 modified
with the proposed improvements. For GIZA++, we
used the default settings for word-based alignment,
but used the HMM model for character-based align-
ment to allow for alignment of longer sentences.
For pialign, default settings were used except for
character-based ITG alignment, which used a prob-
ability beam of 10?4 instead 10?10.9 For decoding,
we use the Moses decoder,10 using the default set-
tings except for the stack size, which we set to 1000
instead of 200. Minimum error rate training was per-
formed to maximize word-based BLEU score for all
systems.11 For language models, word-based trans-
lation uses a word 5-gram model, and character-
based translation uses a character 12-gram model,
both smoothed using interpolated Kneser-Ney.
6100 characters is an average of 18.8 English words
7http://code.google.com/p/giza-pp/
8http://phontron.com/pialign/
9Improvement by using a beam larger than 10?4 was
marginal, especially with co-occurrence prior probabilities.
10http://statmt.org/moses/
11We chose this set-up to minimize the effect of tuning crite-
rion on our experiments, although it does indicate that we must
have access to tokenized data for the development set.
6.2 Quantitative Evaluation
Table 2 presents a quantitative analysis of the trans-
lation results for each of the proposed methods. As
previous research has shown that it is more diffi-
cult to translate into morphologically rich languages
than into English (Koehn, 2005), we perform exper-
iments translating in both directions for all language
pairs. We evaluate translation quality using BLEU
score (Papineni et al, 2002), both on the word and
character level (with n = 4), as well as METEOR
(Denkowski and Lavie, 2011) on the word level.
It can be seen that character-based translation
with all of the proposed alignment improvements
greatly exceeds character-based translation using
one-to-many alignment, confirming that substring-
based information is necessary for accurate align-
ments. When compared with word-based trans-
lation, character-based translation achieves better,
comparable, or inferior results on character-based
BLEU, comparable or inferior results on METEOR,
and inferior results on word-based BLEU. The dif-
ferences between the evaluation metrics are due to
the fact that character-based translation often gets
words mostly correct other than one or two letters.
These are given partial credit by character-based
BLEU (and to a lesser extent METEOR), but marked
entirely wrong by word-based BLEU.
Interestingly, for translation into English,
character-based translation achieves higher ac-
curacy compared to word-based translation on
Japanese and Finnish input, followed by German,
171
fi-en ja-en
ITG-word 2.851 2.085
ITG-char 2.826 2.154
Table 3: Human evaluation scores (0-5 scale).
Ref: directive on equality
Source Unk. Word: tasa-arvodirektiivi
(13/26) Char: equality directive
Ref: yoshiwara-juku station
Target Unk. Word: yoshiwara no eki
(5/26) Char: yoshiwara-juku station
Ref: world health organisation
Uncommon Word: world health
(5/26) Char: world health organisation
Table 4: The major gains of character-based translation,
unknown, hyphenated, and uncommon words.
and finally French. This confirms that character-
based translation is performing well on languages
that have long words or ambiguous boundaries, and
less well on language pairs with relatively strong
one-to-one correspondence between words.
6.3 Qualitative Evaluation
In addition, we performed a subjective evaluation of
Japanese-English and Finnish-English translations.
Two raters evaluated 100 sentences each, assigning
a score of 0-5 based on how well the translation con-
veys the information contained in the reference. We
focus on shorter sentences of 8-16 English words to
ease rating and interpretation. Table 3 shows that
the results are comparable, with no significant dif-
ference in average scores for either language pair.
Table 4 shows a breakdown of the sentences for
which character-based translation received a score
of at 2+ points more than word-based. It can be seen
that character-based translation is properly handling
sparsity phenomena. On the other hand, word-based
translation was generally stronger with reordering
and lexical choice of more common words.
6.4 Effect of Alignment Method
In this section, we compare the translation accura-
cies for character-based translation using the phrasal
ITG model with and without the proposed improve-
ments of substring co-occurrence priors and look-
ahead parsing as described in Sections 4 and 5.2.
fi-en en-fi ja-en en-ja
ITG +cooc +look 28.94 25.31 24.58 35.71
ITG +cooc -look 28.51 24.24 24.32 35.74
ITG -cooc +look 28.65 24.49 24.36 35.05
ITG -cooc -look 27.45 23.30 23.57 34.50
Table 5: METEOR scores for alignment with and without
look-ahead and co-occurrence priors.
Figure 5 shows METEOR scores12 for experi-
ments translating Japanese and Finnish. It can be
seen that the co-occurrence prior gives gains in all
cases, indicating that substring statistics are effec-
tively seeding the ITG aligner. The introduced look-
ahead probabilities improve accuracy significantly
when substring co-occurrence counts are not used,
and slightly when co-occurrence counts are used.
More importantly, they allow for more aggressive
beam pruning, increasing sampling speed from 1.3
sent/s to 2.5 sent/s for Finnish, and 6.8 sent/s to 11.6
sent/s for Japanese.
7 Conclusion and Future Directions
This paper demonstrated that character-based trans-
lation can act as a unified framework for handling
difficult problems in translation: morphology, com-
pound words, transliteration, and segmentation.
One future challenge includes scaling training up
to longer sentences, which can likely be achieved
through methods such as the heuristic span prun-
ing of Haghighi et al (2009) or sentence splitting
of Vilar et al (2007). Monolingual data could also
be used to improve estimates of our substring-based
prior. In addition, error analysis showed that word-
based translation performed better than character-
based translation on reordering and lexical choice,
indicating that improved decoding (or pre-ordering)
and language modeling tailored to character-based
translation will likely greatly improve accuracy. Fi-
nally, we plan to explore the middle ground between
word-based and character based translation, allow-
ing for the flexibility of character-based translation,
while using word boundary information to increase
efficiency and accuracy.
12Similar results were found for character and word-based
BLEU, but are omitted for lack of space.
172
References
Mohamed I. Abouelhoda, Stefan Kurtz, and Enno Ohle-
busch. 2004. Replacing suffix trees with enhanced
suffix arrays. Journal of Discrete Algorithms, 2(1).
Yaser Al-Onaizan and Kevin Knight. 2002. Translat-
ing named entities using monolingual and bilingual re-
sources. In Proc. ACL.
Ming-Hong Bai, Keh-Jiann Chen, and Jason S. Chang.
2008. Improving word alignment by adjusting Chi-
nese word segmentation. In Proc. IJCNLP.
Phil Blunsom and Trevor Cohn. 2010. Inducing syn-
chronous grammars with slice sampling. In Proc.
HLT-NAACL, pages 238?241.
Phil Blunsom, Trevor Cohn, Chris Dyer, and Miles Os-
borne. 2009. A Gibbs sampler for phrasal syn-
chronous grammar induction. In Proc. ACL.
Ondr?ej Bojar. 2007. English-to-Czech factored machine
translation. In Proc. WMT.
Peter F. Brown, Vincent J.Della Pietra, Stephen A. Della
Pietra, and Robert L. Mercer. 1993. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, 19.
Ralf D. Brown. 2002. Corpus-driven splitting of com-
pound words. In Proc. TMI.
Pi-Chuan Chang, Michel Galley, and Christopher D.
Manning. 2008. Optimizing Chinese word segmen-
tation for machine translation performance. In Proc.
WMT.
Tagyoung Chung and Daniel Gildea. 2009. Unsuper-
vised tokenization for machine translation. In Proc.
EMNLP.
Simon Corston-Oliver and Michael Gamon. 2004. Nor-
malizing German and English inflectional morphology
to improve statistical word alignment. Machine Trans-
lation: From Real Users to Research.
Fabien Cromieres. 2006. Sub-sentential alignment us-
ing substring co-occurrence counts. In Proc. COL-
ING/ACL 2006 Student Research Workshop.
John DeNero, Alex Bouchard-Co?te?, and Dan Klein.
2008. Sampling alignment structure under a Bayesian
translation model. In Proc. EMNLP.
Michael Denkowski and Alon Lavie. 2011. Meteor
1.3: Automatic Metric for Reliable Optimization and
Evaluation of Machine Translation Systems. In Proc.
WMT.
Andrew Finch and Eiichiro Sumita. 2007. Phrase-based
machine transliteration. In Proc. TCAST.
Sharon Goldwater and David McClosky. 2005. Improv-
ing statistical MT through morphological analysis. In
Proc. EMNLP.
Aria Haghighi, John Blitzer, John DeNero, and Dan
Klein. 2009. Better word alignments with supervised
ITG models. In Proc. ACL.
Mark Johnson, Thomas Griffiths, and Sharon Goldwa-
ter. 2007. Bayesian inference for PCFGs via Markov
chain Monte Carlo. In Proc. NAACL.
Dan Klein and Christopher D. Manning. 2003. A* pars-
ing: fast exact Viterbi parse selection. In Proc. HLT.
Kevin Knight and Jonathan Graehl. 1998. Machine
transliteration. Computational Linguistics, 24(4).
Phillip Koehn, Franz Josef Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proc. HLT,
pages 48?54.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proc. EMNLP.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In MT Summit.
Grzegorz Kondrak, Daniel Marcu, and Kevin Knight.
2003. Cognates can improve statistical translation
models. In Proc. HLT.
Young-Suk Lee. 2004. Morphological analysis for sta-
tistical machine translation. In Proc. HLT.
Klaus Macherey, Andrew Dai, David Talbot, Ashok
Popat, and Franz Och. 2011. Language-independent
compound splitting with morphological operations. In
Proc. ACL.
Daniel Marcu and William Wong. 2002. A phrase-based,
joint probability model for statistical machine transla-
tion. In Proc. EMNLP.
Cos?kun Mermer and Ahmet Afs??n Ak?n. 2010. Unsu-
pervised search for the optimal segmentation for sta-
tistical machine translation. In Proc. ACL Student Re-
search Workshop.
Jason Naradowsky and Kristina Toutanova. 2011. Unsu-
pervised bilingual morpheme segmentation and align-
ment with context-rich hidden semi-Markov models.
In Proc. ACL.
Graham Neubig, Taro Watanabe, Eiichiro Sumita, Shin-
suke Mori, and Tatsuya Kawahara. 2011. An unsuper-
vised model for joint phrase alignment and extraction.
In Proc. ACL, pages 632?641, Portland, USA, June.
Graham Neubig. 2011. The Kyoto free translation task.
http://www.phontron.com/kftt.
ThuyLinh Nguyen, Stephan Vogel, and Noah A. Smith.
2010. Nonparametric word segmentation for machine
translation. In Proc. COLING.
Sonja Nie?en and Hermann Ney. 2000. Improving SMT
quality with morpho-syntactic analysis. In Proc. COL-
ING.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proc. COLING.
173
Markus Saers, Joakim Nivre, and Dekai Wu. 2009.
Learning stochastic bracketing inversion transduction
grammars with a cubic time biparsing algorithm. In
Proc. IWPT, pages 29?32.
Benjamin Snyder and Regina Barzilay. 2008. Unsuper-
vised multilingual learning for morphological segmen-
tation. Proc. ACL.
Virach Sornlertlamvanich, Chumpol Mokarat, and Hi-
toshi Isahara. 2008. Thai-lao machine translation
based on phoneme transfer. In Proc. 14th Annual
Meeting of the Association for Natural Language Pro-
cessing.
Michael Subotin. 2011. An exponential translation
model for target language morphology. In Proc. ACL.
David Talbot and Miles Osborne. 2006. Modelling lexi-
cal redundancy for machine translation. In Proc. ACL.
Jo?rg Tiedemann. 2009. Character-based PSMT for
closely related languages. In Proc. 13th Annual
Conference of the European Association for Machine
Translation.
David Vilar, Jan-T. Peter, and Hermann Ney. 2007. Can
we translate letters. In Proc. WMT.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical trans-
lation. In Proc. COLING.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3).
Hao Zhang and Daniel Gildea. 2005. Stochastic lexical-
ized inversion transduction grammar for alignment. In
Proc. ACL.
Hao Zhang, Chris Quirk, Robert C. Moore, and
Daniel Gildea. 2008a. Bayesian learning of
non-compositional phrases with synchronous parsing.
Proc. ACL.
Ruiqiang Zhang, Keiji Yasuda, and Eiichiro Sumita.
2008b. Improved statistical machine translation by
multiple Chinese word segmentation. In Proc. WMT.
174
Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 59?66,
Portland, Oregon, June 17-18, 2011. c?2011 Association for Computational Linguistics
Spoken Dialogue System based on Information Extraction
using Similarity of Predicate Argument Structures
Koichiro Yoshino, Shinsuke Mori and Tatsuya Kawahara
School of Informatics, Kyoto University
Sakyo-ku, Kyoto, 606-8501, Japan
Abstract
We present a novel scheme of spoken dialogue
systems which uses the up-to-date informa-
tion on the web. The scheme is based on in-
formation extraction which is defined by the
predicate-argument (P-A) structure and real-
ized by semantic parsing. Based on the in-
formation structure, the dialogue system can
perform question answering and also proac-
tive information presentation. Feasibility of
this scheme is demonstrated with experiments
using a domain of baseball news. In order to
automatically select useful domain-dependent
P-A templates, statistical measures are intro-
duced, resulting to a completely unsupervised
learning of the information structure given a
corpus. Similarity measures of P-A structures
are also introduced to select relevant infor-
mation. An experimental evaluation shows
that the proposed system can make more rel-
evant responses compared with the conven-
tional ?bag-of-words? scheme.
1 Introduction
Recently, a huge amount of information is accumu-
lated and distributed on the web day by day. As a
result, many people get information via web rather
than the conventional mass media. On the other
hand, the amount of information on the web is so
huge that we often encounter the difficulty in finding
information we want. Keyword search is the most
widely-used means for the web information access.
However, this style is not necessarily the best for
information demands of all users who do not have
definite goals or just want to know what would be
interesting. To cope with user?s vague information
demands is an important mission for interactive spo-
ken dialogue systems. Moreover, supporting user?s
information collection in a small-talk style is one of
the new directions of spoken dialogue systems.
Existing spoken dialogue systems can be clas-
sified into two types (T.Kawahara, 2009): those
using relational databases (RDB) such as the Air-
line Travel Information System (ATIS) (D.A.Dahl,
1994), and those using information retrieval tech-
niques based on statistical document matching
(T.Misu and T.Kawahara, 2010). The first scheme
can achieve a well-defined task by using a struc-
tural database, but this scheme cannot be applied to
the web information in which the structure and task
are not well defined. The second scheme has been
studied to handle large-scale texts such as web, but
most of the conventional systems adopt a ?bag-of-
words? model, and naive statistical matching often
generates irrelevant responses which have nothing
to do with the user?s requests. Our proposed scheme
solves this problem by using information extraction
based on semantic parsing from web texts, with-
out constructing an RDB. We adopt the predicate-
argument (P-A) structure generated by a parser as
a baseline, but every P-A structure is not useful for
information extraction and retrieval(Y.Kiyota et al,
2002; M.O.Dzikovska et al, 2003; S.Harabagiu et
al., 2005). In fact, the useful information structure
is dependent on domains. Conventionally, the tem-
plates for information extraction were hand-crafted
(R.Grishman, 2003), but this heuristic process is so
costly that it cannot be applied to a variety of do-
mains on the web. In this paper, therefore, we pro-
59
	

 	 
Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 1?9,
Seoul, South Korea, 5-6 July 2012. c?2012 Association for Computational Linguistics
Multi-modal Sensing and Analysis of Poster Conversations
toward Smart Posterboard
Tatsuya Kawahara
Kyoto University, Academic Center for Computing and Media Studies
Sakyo-ku, Kyoto 606-8501, Japan
http://www.ar.media.kyoto-u.ac.jp/crest/
Abstract
Conversations in poster sessions in academic
events, referred to as poster conversations,
pose interesting and challenging topics on
multi-modal analysis of multi-party dialogue.
This article gives an overview of our project
on multi-modal sensing, analysis and ?under-
standing? of poster conversations. We fo-
cus on the audience?s feedback behaviors such
as non-lexical backchannels (reactive tokens)
and noddings as well as joint eye-gaze events
by the presenter and the audience. We inves-
tigate whether we can predict when and who
will ask what kind of questions, and also inter-
est level of the audience. Based on these anal-
yses, we design a smart posterboard which can
sense human behaviors and annotate interac-
tions and interest level during poster sessions.
1 Introduction
As a variety of spoken dialogue systems have been
developed and deployed in the real world, the fron-
tier of spoken dialogue research, with engineering
applications in scope, has been extended from the
conventional human-machine speech interface. One
direction is a multi-modal interface, which includes
not only graphics but also humanoid robots. An-
other new direction is a multi-party dialogue sys-
tem that can talk with multiple persons as an as-
sistant agent (D.Bohus and E.Horvitz, 2009) or a
companion robot (S.Fujie et al, 2009). While these
are extensions of the human-machine speech in-
terface, several projects have focused on human-
human interactions such as meetings (S.Renals et
al., 2007) and free conversations (K.Otsuka et al,
2008; C.Oertel et al, 2011), toward ambient systems
supervising the human communications.
We have been conducting a project which focuses
on conversations in poster sessions, hereafter re-
ferred to as poster conversations. Poster sessions
have become a norm in many academic conventions
and open laboratories because of the flexible and in-
teractive characteristics. Poster conversations have
a mixture characteristics of lectures and meetings;
typically a presenter explains his/her work to a small
audience using a poster, and the audience gives feed-
back in real time by nodding and verbal backchan-
nels, and occasionally makes questions and com-
ments. Conversations are interactive and also multi-
modal because people are standing and moving un-
like in meetings. Another good point of poster con-
versations is that we can easily make a setting for
data collection, which is controlled in terms of fa-
miliarity with topics or other participants and yet is
?natural and real?.
The goal of the project is signal-level sensing
and high-level ?understanding? of human interac-
tions, including speaker diarization and annotation
of comprehension and interest level of the audience.
These will realize a new indexing scheme of speech
archives. For example, after a long session of poster
presentation, we often want to get a short review of
the question-answers and what looked difficult for
audience to follow. The research will also provide
a model of intelligent conversational agents that can
make autonomous presentation.
As opposed to the conventional content-based in-
dexing approach which focuses on the presenter?s
1
Figure 1: Overview of multi-modal interaction analysis
speech by conducting speech recognition and nat-
ural language analysis, we adopt an interaction-
oriented approach which looks into the audience?s
reaction. Specifically we focus on non-linguistic in-
formation such as backchannel, nodding and eye-
gaze information, because we assume the audience
better understands the key points of the presentation
than the current machines. An overview of the pro-
posed scheme is depicted in Figure 1.
Therefore, we set up an infrastructure for multi-
modal sensing and analysis of multi-party interac-
tions. Its process overview is shown in Figure 2.
From the audio channel, we detect utterances as
well as laughters and backchannels. We also de-
tect eye-gaze, nodding, and pointing information.
Special devices such as a motion-capturing system
and eye-tracking recorders are used to make a ?gold-
standard? corpus, but only video cameras and distant
microphones will be used in the practical system.
Our goal is then annotation of comprehension and
interest level of the audience by combining these in-
formation sources. This annotation will be useful
in speech archives because people would be inter-
ested in listening to the points other people were
interested in. Since this is apparently difficult to
be well-defined, however, we set up several mile-
stones that can be formulated in objective manners
and presumably related with the above-mentioned
goal. They are introduced in this article after de-
scription of the sensing environment and the col-
lected corpus in Section 2. In Section 3, annota-
tion of interest level is addressed through detection
of laughters and non-lexical kinds of backchannels,
referred to as reactive tokens. In Section 4 and 5,
eye-gaze and nodding information is incorporated
to predict when and who in the audience will ask
questions, and also what kind of questions. With
Figure 2: Flow of multi-modal sensing and analysis
these analyses, we expect that we can get clues to
high-level ?understanding? of the conversations, for
example, whether the presentation is understood or
liked by the audience.
2 Multi-modal Corpus of Poster
Conversations
2.1 Recording Environment
We have designed a special environment (?IMADE
Room?) to record audio, video, human mo-
tion, and eye-gaze information in poster conversa-
tions (T.Kawahara et al, 2008). An array of micro-
phones (8 to 19) has been designed to be mounted on
top of the posterboard, while each participant used
a wireless head-set microphone for recording voice
for the ?gold-standard? corpus annotation. A set of
cameras (6 or 8) has also been designed to cover all
participants and the poster, while a motion captur-
ing system was used for the ?gold-standard? annota-
tion. Each participant was equipped with a dozen of
motion-capturing markers as well as an eye-tracking
recorder and an accelerometer, but all devices are
attached with a cap or stored in a compact belt bag,
so they can be naturally engaged in the conversation.
An outlook of session recording is given in Figure 3.
2.2 Corpus Collection and Annotation
We have recorded a number of poster conversations
(31 in total) using this environment, but for some of
them, failed to collect all sensor data accurately. In
the analyses of the following sections, we use four
poster sessions, in which the presenters and audi-
ences are different from each other. They are all
in Japanese, although we recently recorded sessions
in English as well. In each session, one presenter
(labeled as ?A?) prepared a poster on his/her own
2
Figure 3: Outlook of poster session recording
academic research, and there was an audience of
two persons (labeled as ?B? and ?C?), standing in
front of the poster and listening to the presentation.
They were not familiar with the presenter and had
not heard the presentation before. The duration of
each session was 20-30 minutes.
All speech data, collected via the head-set mi-
crophones, were segmented into IPUs (Inter-Pausal
Unit) with time and speaker labels, and transcribed
according to the guideline of the Corpus of Sponta-
neous Japanese (CSJ) (K.Maekawa, 2003). We also
manually annotated fillers, verbal backchannels and
laughters.
Eye-gaze information is derived from the eye-
tracking recorder and the motion capturing system
by matching the gaze vector against the position of
the other participants and the poster. Noddings are
automatically detected with the accelerometer at-
tached with the cap.
3 Detection of Interesting Level with
Reactive Tokens of Audience
We hypothesize that the audience signals their in-
terest level with their feedback behaviors. Specif-
ically, we focus on the audience?s reactive tokens
and laughters. By reactive tokens (Aizuchi in
Japanese), we mean the listener?s verbal short re-
sponse, which expresses his/her state of the mind
during the conversation. The prototypical lexical en-
tries of backchannels include ?hai? in Japanese and
?yeah? or ?okay? in English, but many of them are
non-lexical and used only for reactive tokens, such
as ?hu:n?, ?he:? in Japanese and ?wow?, ?uh-huh?
in English. We focus on the latter kind of reactive
tokens, which are not used for simple acknowledg-
ment.
We also investigate detection of laughters and its
relationship with interesting level. The detection
method and performance were reported in (K.Sumi
et al, 2009).
3.1 Relationship between Prosodic Patterns of
Reactive Tokens and Interest Level
In this subsection, we hypothesize that the audience
expresses their interest with specific syllabic and
prosodic patterns. Generally, prosodic features play
an important role in conveying para-linguistic and
non-verbal information. In previous works (F.Yang
et al, 2008; A.Gravano et al, 2007), it was re-
ported that prosodic features are useful in identi-
fying backchannels. Ward (N.Ward, 2004) made
an analysis of pragmatic functions conveyed by the
prosodic features in English non-lexical tokens.
In this study, we designed an experiment to iden-
tify the syllabic and prosodic patterns closely related
with interest level. For this investigation, we select
three syllabic patterns of ?hu:N?, ?he:? and ?a:?,
which are presumably related with interest level and
also most frequently observed in the corpus, except
lexical tokens.
We computed following prosodic features for
each reactive token: duration, F0 (maximum and
range) and power (maximum). The prosodic fea-
tures are normalized for every person; for each fea-
ture, we compute the mean, and this mean is sub-
tracted from the feature values.
For each syllabic kind of reactive token and for
each prosodic feature, we picked up top-ten and
bottom-ten samples, i.e. samples that have the
largest/smallest values of the prosodic feature. For
each of them, an audio segment was extracted to
cover the reactive token and its preceding utterances.
Then, we had five subjects to listen to the audio seg-
ments and evaluate the audience?s state of the mind.
We prepared twelve items to be evaluated in a scale
of four (?strongly feel? to ?do not feel?), among
which two items are related to interest level and
3
Table 1: Significant combinations of syllabic and
prosodic patterns of reactive tokens
interest surprise
hu:N duration * *
F0 max
F0 range
power
he: duration * *
F0 max * *
F0 range *
power * *
a: duration
F0 max *
F0 range
power *
other two items are related to surprise level 1. Ta-
ble 1 lists the results (marked by ?*?) that have a sta-
tistically significant (p < 0.05) difference between
top-ten and bottom-ten samples. It is observed that
prolonged ?hu:N? means interest and surprise while
?a:? with higher pitch or larger power means inter-
est. On the other hand, ?he:? can be emphasized in
all prosodic features to express interest and surprise.
The tokens with larger power and/or a longer du-
ration is apparently easier to detect than indistinct
tokens, and they are more related with interest level.
It is expected that this rather simple prosodic infor-
mation is useful for indexing poster conversations.
3.2 Third-party Evaluation of Hot Spots
In this subsection, we define those segments which
induced (or elicited) laughters or non-lexical reac-
tive tokens as hot spots, 2 and investigate whether
these hot spots are really funny or interesting to the
third-party viewers of the poster session.
We had four subjects, who had not attended the
presentation nor listened the recorded audio content.
They were asked to listen to each of the segmented
hot spots in the original time sequence, and to make
evaluations on the questionnaire, as below.
1We used different Japanese wording for interest and for sur-
prise to enhance the reliability of the evaluation; we adopt the
result if the two matches.
2Wrede et al(B.Wrede and E.Shriberg, 2003; D.Gatica-
Perez et al, 2005) defined ?hot spots? as the regions where two
or more participants are highly involved in a meeting. Our def-
inition is different from it.
Q1: Do you understand the reason why the reactive
token/laughter occurred?
Q2: Do you find this segment interesting/funny?
Q3: Do you think this segment is necessary or use-
ful for listening to the content?
The percentage of ?yes? on Question 1 was 89%
for laughters and 95% for reactive tokens, confirm-
ing that a large majority of the hot spots are appro-
priate.
The answers to Questions 2 and 3 are more sub-
jective, but suggest the usefulness of the hot spots.
It turned out that only a half of the spots associated
with laughters are funny for the subjects (Q2), and
they found 35% of the spots not funny. The result
suggests that feeling funny largely depends on the
person. And we should note that there are not many
funny parts in poster sessions by nature.
On the other hand, more than 90% of the spots
associated with reactive tokens are interesting (Q2),
and useful or necessary (Q3) for the subjects. The
result supports the effectiveness of the hot spots ex-
tracted based on the reaction of the audience.
4 Prediction of Turn-taking with Eye-gaze
and Backchannel Information
Turn-taking is an elaborate process especially in
multi-party conversations. Predicting whom the turn
is yielded to or who will take the turn is significant
for an intelligent conversational agent handling mul-
tiple partners (D.Bohus and E.Horvitz, 2009; S.Fujie
et al, 2009) as well as an automated system to beam-
form microphones or zoom in cameras on the speak-
ers. There are a number of previous studies on turn-
taking behaviors in dialogue, but studies on com-
putational modeling to predict turn-taking in multi-
party interactions are very limited (K.Laskowski et
al., 2011; K.Jokinen et al, 2011). Conversations
in poster sessions are different from those in meet-
ings and free conversations addressed in the previ-
ous works, in that presenters hold most of turns and
thus the amount of utterances is very unbalanced.
However, the segments of audiences? questions and
comments are more informative and should not be
missed. Therefore, we focus on prediction of turn-
taking by the audience in poster conversations, and,
if that happens, which person in the audience will
take the turn to speak.
4
Table 2: Duration (sec.) of eye-gaze and its relationship
with turn-taking
turn held by turn taken by
presenter A B C
A gazed at B 0.220 0.589 0.299
A gazed at C 0.387 0.391 0.791
B gazed at A 0.161 0.205 0.078
C gazed at A 0.308 0.215 0.355
We also presume that turn-taking by the audience
is related with their interest level because they want
to know more and better when they are more at-
tracted to the presentation.
It is widely-known that eye-gaze information
plays a significant role in turn-taking (A.Kendon,
1967; B.Xiao et al, 2011; K.Jokinen et al, 2011;
D.Bohus and E.Horvitz, 2009). The existence of
posters, however, requires different modeling in
poster conversations as the eye-gaze of the partici-
pants are focused on the posters in most of the time.
This is true to other kinds of interactions using some
materials such as maps and computers. Moreover,
we investigate the use of backchannel information
by the audience during the presenter?s utterances.
4.1 Relationship between Eye-gaze and
Turn-taking
We identify the object of the eye-gaze of all partic-
ipants at the end of the presenter?s utterances. The
target object can be either the poster or other partic-
ipants. Then, we measure the duration of the eye-
gaze within the segment of 2.5 seconds before the
end of the presenter?s utterances because the major-
ity of the IPUs are less than 2.5 seconds. It is listed
in Table 2 in relation with the turn-taking events. We
can see the presenter gazed at the person right before
yielding the turn to him/her significantly longer than
other cases. However, there is no significant differ-
ence in the duration of the eye-gaze by the audience
according to the turn-taking events.
4.2 Relationship between Joint Eye-gaze
Events and Turn-taking
Next, we define joint eye-gaze events by the presen-
ter and the audience as shown in Table 3. In this
table, we use notation of ?audience?, but actually
these events are defined for each person in the audi-
Table 3: Definition of joint eye-gaze events by presenter
and audience
who presenter
gazes at audience poster
(I) (P)
audience presenter (i) Ii Pi
poster (p) Ip Pp
Table 4: Statistics of joint eye-gaze events by presenter
and audience in relation with turn-taking
#turn held #turn taken total
by presenter by audience
(self) (other)
Ii 125 17 3 145
Ip 320 71 26 417
Pi 190 11 9 210
Pp 2974 147 145 3266
ence. Thus, ?Ii? means the mutual gaze by the pre-
senter and a particular person in the audience, and
?Pp? means the joint attention to the poster object.
Statistics of these events at the end of the presen-
ter?s utterances are summarized in Table 4. Here,
the counts of the events are summed over the two
persons in the audience. They are classified accord-
ing to the turn-taking events, and turn-taking by the
audience is classified into two cases: the person in-
volved in the eye-gaze event actually took the turn
(self), and the other person took the turn (other).
The mutual gaze (?Ii?) is expected to be related with
turn-taking, but its frequency is not so high. The
frequency of ?Pi? is not high, either. The most po-
tentially useful event is ?Ip?, in which the presenter
gazes at the person in the audience before giving the
turn. This is consistent with the observation in the
previous subsection.
4.3 Relationship between Backchannels and
Turn-taking
As shown in Section 3, verbal backchannels suggest
the listener?s interest level. Nodding is regarded as
a non-verbal backchannel, and it is more frequently
observed in poster conversations than in simple spo-
ken dialogues.
The occurrence frequencies of these events are
counted within the segment of 2.5 seconds before
the end of the presenter?s utterances. They are
shown in Figure 4 according to the joint eye-gaze
5
Figure 4: Statistics of backchannels and their relationship
with turn-taking
events. It is observed that the person in the audi-
ence who takes the turn (=turn-taker) made more
backchannels both in verbal and non-verbal man-
ners, and the tendency is more apparent in the par-
ticular eye-gaze events of ?Ii? and ?Ip? which are
closely related with the turn-taking events.
4.4 Prediction of Turn-taking by Audience
Based on the analyses in the previous subsections,
we conduct an experiment to predict turn-taking by
the audience. The prediction task is divided into two
sub-tasks: detection of speaker change and identifi-
cation of the next speaker. In the first sub-task, we
predict whether the turn is given from the presen-
ter to someone in the audience, and if that happens,
then we predict who in the audience takes the turn
in the second sub-task. Note that these predictions
are done at every end-point of the presenter?s utter-
ance (IPU) using the information prior to the speaker
change or the utterance by the new speaker.
For the first sub-task of speaker change predic-
tion, prosodic features are adopted as a baseline.
Specifically, we compute F0 (mean, max, min, and
range) and power (mean and max) of the presenter?s
utterance prior to the prediction point. Backchan-
nel features are defined by taking occurrence counts
prior to the prediction point for each type (verbal
backchannel and non-verbal nodding). Eye-gaze
features are defined in terms of eye-gaze objects
and joint eye-gaze events, as described in previous
subsections, and are parameterized with occurrence
counts and duration. These parameterizations, how-
ever, show no significant difference nor synergetic
Table 5: Prediction result of speaker change
feature recall precision F-measure
prosody 0.667 0.178 0.280
backchannel (BC) 0.459 0.113 0.179
eye-gaze (gaze) 0.461 0.216 0.290
prosody+BC 0.668 0.165 0.263
prosody+gaze 0.706 0.209 0.319
prosody+BC+gaze 0.678 0.189 0.294
effect in terms of prediction performance.
SVM is adopted to predict whether speaker
change happens or not by using these features. The
result is summarized in Table 5. Here, we compute
recall, precision and F-measure for speaker change,
or turn-taking by the audience. This case accounts
for only 11.9% and its prediction is very challeng-
ing, while we can easily get an accuracy of over 90%
for prediction of turn-holding by the presenter. We
are particularly concerned on the recall of speaker
change, considering the nature of the task and appli-
cation scenarios.
Among the individual features, the prosodic fea-
tures obtain the best recall while the eye-gaze fea-
tures achieve the best precision and F-measure.
Combination of these two is effective in improving
both recall and precision. On the other hand, the
backchannel features get the lowest performance,
and its combination with the other features is not ef-
fective, resulting in degradation of the performance.
Next, we conduct the second sub-task of speaker
prediction. Predicting the next speaker in a multi-
party conversation (before he/she actually speaks) is
also challenging, and has not been addressed in the
previous work (K.Jokinen et al, 2011). For this sub-
task, the prosodic features of the current speaker are
not usable because it does not have information sug-
gesting who the turn will be yielded to. Therefore,
we adopt the backchannel features and eye-gaze fea-
tures. Note that these features are computed for in-
dividual persons in the audience, instead of taking
the maximum or selecting among them.
The result is summarized in Table 6. In this exper-
iment, the backchannel features have some effect,
and by combining them with the eye-gaze features,
the accuracy reaches almost 70%.
6
Table 6: Prediction result of the next speaker
feature accuracy
eye-gaze (gaze) 66.4%
backchannel (BC) 52.6%
gaze+BC 69.7%
5 Relationship between Feedback
Behaviors and Question Type
Next, we investigate the relationship between feed-
back behaviors of the audience and the kind of ques-
tions they ask after they take a turn. In this work,
questions are classified into confirming questions
and substantive questions. The confirming questions
are asked to make sure of the understanding of the
current explanation, thus they can be answered sim-
ply by ?Yes? or ?No?.3 The substantive questions,
on the other hand, are asking about what was not
explained by the presenter, thus they cannot be an-
swered by ?Yes? or ?No? only; an additional expla-
nation is needed.
This annotation together with the preceding ex-
planation segment is not so straightforward when the
conversation got into the QA phase after the presen-
ter went through an entire poster presentation. Thus,
we exclude the QA phase and focus on the questions
asked during the explanation phase. In this section,
we analyze the behaviors during the explanation seg-
ment that precedes the question by merging all con-
secutive IPUs of the presenter. This is a reasonable
assumption once turn-taking is predicted in the pre-
vious section. These are major differences from the
analysis of the previous section.
5.1 Relationship between Backchannels and
Question Type
The occurrence frequencies of verbal backchannels
and non-verbal noddings, normalized by the dura-
tion of the explanation segment (seconds), are listed
according to the question type in Tables 7 and 8.
In these tables, statistics of the person who actu-
ally asked questions are compared with those of the
person who did not. We can observe the turn-taker
made significantly more verbal backchannels when
asking substantive questions. On the other hand,
3This does not mean the presenter actually answered simply
by ?Yes? or ?No?.
Table 7: Frequencies (per sec.) of verbal backchannels
and their relationship with question type
confirming substantive
turn-taker 0.034 0.063
non-turn-taker 0.041 0.038
Table 8: Frequencies (per sec.) of non-verbal noddings
and their relationship with question type
confirming substantive
turn-taker 0.111 0.127
non-turn-taker 0.109 0.132
Table 9: Duration (ratio) of joint eye-gaze events and
their relationship with question type
confirming substantive
Ii 0.053 0.015
Ip 0.116 0.081
Pi 0.060 0.035
Pp 0.657 0.818
there is no significant difference in the frequency of
non-verbal noddings among the audience and among
the question types.
5.2 Relationship between Eye-gaze Events and
Question Type
We also investigate the relationship between eye-
gaze events and the question type. Among several
parameterizations introduced in the previous sec-
tion, we observe a significant tendency in the du-
ration of the joint eye-gaze events, which is normal-
ized by the duration of the presenter?s explanation
segment. It is summarized in Table 9. We can see
the increase of ?Ip? (and decrease of ?Pp? accord-
ingly) in confirming questions. By combining with
the analysis in the previous section, we can reason
the majority of turn-taking signaled by the presen-
ter?s gazing is attributed to confirmation.
6 Smart Posterboard
We have designed and implemented a smart poster-
board, which can record a poster session, sense hu-
man behaviors and annotate interactions. Since it
is not practical to ask every participant to wear spe-
cial devices such as a head-set microphone and an
eye-tracking recorder and also to set up any devices
attached to a room, all sensing devices are attached
7
Figure 5: Outlook of smart posterboard
to the posterboard, which is actually a 65-inch LCD
display. An outlook of the posterboard is given in
Figure 5.
It is equipped with a 19-channel microphone array
on the top, and attached with six cameras and two
Kinect sensors. Speech separation and enhancement
has been realized with Blind Spatial Subtraction Ar-
ray (BSSA), which consists of the delay-and-sum
(DS) beamformer and a noise estimator based on in-
dependent component analysis (ICA) (Y.Takahashi
et al, 2009). In this step, the audio input is separated
to the presenter and the audience, but discrimination
among the audience is not done. Visual information
should be combined to annotate persons in the au-
dience. Voice activity detection (VAD) is conducted
on each of the two channels to make speaker diariza-
tion. Localization of the persons in the audience and
estimation of their head direction, which approxi-
mates their eye-gaze, are conducted using the video
information captured by the six cameras.
Although high-level annotations addressed in the
previous sections have not been yet implemented in
the current system, the above-mentioned processing
realizes a browser of poster sessions which visual-
izes the interaction.
The Kinect sensors are used for a portable and on-
line version, in which speech enhancement, speaker
localization and head direction estimation are per-
formed in real time.
We made a demonstration of the system in IEEE-
ICASSP 2012 as shown in Figure 5, and plan further
improvements and trials in the future.
7 Conclusions
This article has given an overview of our multi-
modal data collection and analysis of poster conver-
sations. Poster conversations provide us with a num-
ber of interesting topics in spoken dialogue research
as they are essentially multi-modal and multi-party.
By focusing on the audience?s feedback behaviors
and joint eye-gaze events, it is suggested that we can
annotate interest level of the audience and hot spots
in the session.
Nowadays, presentation using a poster is one of
the common and important activities in academic
and business communities. As large LCD displays
become ubiquitous, its style will be more interac-
tive. Accordingly, sensing and archiving functions
introduced in the smart posterboard will be useful.
Acknowledgments
The work presented in this article was conducted
jointly with Hisao Setoguchi, Zhi-Qiang Chang,
Takanori Tsuchiya, Takuma Iwatate, and Katsuya
Takanashi. The smart posterboard system has been
developed by a number of researchers in Kyoto Uni-
versity and Nara Institute of Science and Technol-
ogy (NAIST).
This work was supported by JST CREST and
JSPS Grant-in-Aid for Scientific Research.
References
A.Gravano, S.Benus, J.Hirschberg, S.Mitchell, and
I.Vovsha. 2007. Classification of discourse functions
of affirmative words in spoken dialogue. In Proc. IN-
TERSPEECH, pages 1613?1616.
A.Kendon. 1967. Some functions of gaze direction in
social interaction. Acta Psychologica, 26:22?63.
B.Wrede and E.Shriberg. 2003. Spotting ?hot spots? in
meetings: Human judgments and prosodic cues. In
Proc. EUROSPEECH, pages 2805?2808.
B.Xiao, V.Rozgic, A.Katsamanis, B.R.Baucom,
P.G.Georgiou, and S.Narayanan. 2011. Acous-
tic and visual cues of turn-taking dynamics in
dyadic interactions. In Proc. INTERSPEECH, pages
2441?2444.
C.Oertel, S.Scherer, and N.Campbell. 2011. On the use
of multimodal cues for the prediction of degrees of in-
volvement in spontaneous conversation. In Proc. IN-
TERSPEECH, pages 1541?1545.
8
D.Bohus and E.Horvitz. 2009. Models for multiparty
engagement in open-world dialog. In Proc. SIGdial.
D.Gatica-Perez, I.McCowan, D.Zhang, and S.Bengio.
2005. Detecting group interest-level in meetings. In
Proc. IEEE-ICASSP, volume 1, pages 489?492.
F.Yang, G.Tur, and E.Shriberg. 2008. Exploiting dialog
act tagging and prosodic information for action item
identification. In Proc. IEEE-ICASSP, pages 4941?
4944.
K.Jokinen, K.Harada, M.Nishida, and S.Yamamoto.
2011. Turn-alignment using eye-gaze and speech in
conversational interaction. In Proc. INTERSPEECH,
pages 2018?2021.
K.Laskowski, J.Edlund, and M.Heldner. 2011. A single-
port non-parametric model of turn-taking in multi-
party conversation. In Proc. IEEE-ICASSP, pages
5600?5603.
K.Maekawa. 2003. Corpus of Spontaneous Japanese: Its
design and evaluation. In Proc. ISCA & IEEE Work-
shop on Spontaneous Speech Processing and Recogni-
tion, pages 7?12.
K.Otsuka, S.Araki, K.Ishizuka, M.Fujimoto, M.Heinrich,
and J.Yamato. 2008. A realtime multimodal system
for analyzing group meetings by combining face pose
tracking and speaker diarization. In Proc. ICMI, pages
257?262.
K.Sumi, T.Kawahara, J.Ogata, and M.Goto. 2009.
Acoustic event detection for spotting hot spots in pod-
casts. In Proc. INTERSPEECH, pages 1143?1146.
N.Ward. 2004. Pragmatic functions of prosodic features
in non-lexical utterances. In Speech Prosody, pages
325?328.
S.Fujie, Y.Matsuyama, H.Taniyama, and T.Kobayashi.
2009. Conversation robot participating in and activat-
ing a group communication. In Proc. INTERSPEECH,
pages 264?267.
S.Renals, T.Hain, and H.Bourlard. 2007. Recognition
and understanding of meetings: The AMI and AMIDA
projects. In Proc. IEEE Workshop Automatic Speech
Recognition & Understanding.
T.Kawahara, H.Setoguchi, K.Takanashi, K.Ishizuka, and
S.Araki. 2008. Multi-modal recording, analysis and
indexing of poster sessions. In Proc. INTERSPEECH,
pages 1622?1625.
Y.Takahashi, T.Takatani, K.Osako, H.Saruwatari, and
K.Shikano. 2009. Blind spatial subtraction ar-
ray for speech enhancement in noisy environment.
IEEE Trans. Audio, Speech & Language Process.,
17(4):650?664.
9
Proceedings of the SIGDIAL 2014 Conference, pages 32?40,
Philadelphia, U.S.A., 18-20 June 2014.
c?2014 Association for Computational Linguistics
Information Navigation System Based on POMDP that Tracks User Focus
Koichiro Yoshino Tatsuya Kawahara
School of Informatics, Kyoto University
Sakyo-ku, Kyoto, 606-8501, Japan
yoshino@ar.media.kyoto-u.ac.jp
Abstract
We present a spoken dialogue system for
navigating information (such as news ar-
ticles), and which can engage in small
talk. At the core is a partially observ-
able Markov decision process (POMDP),
which tracks user?s state and focus of at-
tention. The input to the POMDP is pro-
vided by a spoken language understanding
(SLU) component implemented with lo-
gistic regression (LR) and conditional ran-
dom fields (CRFs). The POMDP selects
one of six action classes; each action class
is implemented with its own module.
1 Introduction
A large number of spoken dialogue systems have
been investigated and many systems are deployed
in the real world. Spoken dialogue applications
that interact with a diversity of users are avail-
able on smart-phones. However, current appli-
cations are based on simple question answering
and the system requires a clear query or a def-
inite task goal. Therefore, next-generation dia-
logue systems should engage in casual interactions
with users who do not have a clear intention or a
task goal. Such systems include a sightseeing nav-
igation system that uses tour guide books or doc-
uments in Wikipedia (Misu and Kawahara, 2010),
and a news navigation system that introduces news
articles updated day-by-day (Yoshino et al., 2011;
Pan et al., 2012). In this paper, we develop an in-
formation navigation system that provides infor-
mation even if the user request is not necessarily
clear and there is not a matching document in the
knowledge base. The user and the system converse
on the current topic and the system provides po-
tentially useful information for the user.
Dialogue management of this kind of systems
was usually made in a heuristic manner and based
on simple rules (Dahl et al., 1994; Bohus and Rud-
nicky, 2003). There is not a clear principle nor
established methodology to design and implement
casual conversation systems. In the past years, ma-
chine learning, particularly reinforcement learn-
ing, have been investigated for dialogue manage-
ment. MDPs and POMDPs are now widely used
to model and train dialogue managers (Levin et
al., 2000; Williams and Young, 2007; Young et
al., 2010; Yoshino et al., 2013b). However, the
conventional scheme assumes that the task and di-
alogue goal can be clearly stated and readily en-
coded in the RL reward function. This is not true
in casual conversation or when browsing informa-
tion.
Some previous work has tackled with this prob-
lem. In a conversational chatting system (Shibata
et al., 2014), users were asked to make evalua-
tion at the end of each dialogue session, to define
rewards for reinforcement learning. In a listen-
ing dialogue system (Meguro et al., 2010), levels
of satisfaction were annotated in logs of dialogue
sessions to train a discriminative model. These
approaches require costly input from users or de-
velopers, who provide labels and evaluative judg-
ments.
In this work, we present a framework in which
reward is defined for the quality of system actions
and also for encouraging long interactions, in con-
trast to the conventional framework. Moreover,
user focus is tracked to make appropriate actions,
which are more rewarded.
2 Conversational Information
Navigation System
In natural human-human conversation, partici-
pants have topics they plan to talk about, and they
progress through the dialogue in accordance with
the topics (Schegloff and Sacks, 1973). We call
this dialogue style ?information navigation.? An
example is shown in Figure 1. First, the speaker
32
Dialogue states
Speaker (system) Listener (user)
Offer a topic Be interested in the topicPresent the detail Make a questionAnswer the question Be silentOffer a new topic (topic 2) Not be interested in
Offer a new topic (topic 3)
???
Make a questionTopic 3
Topic 2
???
Topic 1
Figure 1: An example of information navigation.
Story Telling(ST)System-initiative
Modules of related topics
Question Answering(QA)User-initiative
Proactive 
initiative
Presentation(PP)System-
Draw new topic
Related topics
Topic
Topic
Topic
Topic
Topic
Topic Topic
???
???
???
Selected topic
Modules of current topic
Topic Presentation (TP)
Topic N
Topic 3
Topic 2
???
Topic 1
Other modules
Greeting(GR) Keep silence(KS)
Figure 2: Overview of the information navigation
system.
offers a new topic and probes the interest of the
listener. If the listener shows interest, the speaker
describes details of the topic. If the listener asks
a specific question, the speaker answers the ques-
tion. On the other hand, if the listener is not inter-
ested in the topic, the speaker avoids the details of
that topic, and changes the topic. Topics are often
taken from current news.
In our past work, we have developed a news
navigation system (Yoshino et al., 2011) based on
this dialogue structure. The system provides top-
ics collected from Web news texts, and the user
gets information according to his interests and
queries.
2.1 System overview
An overview of the proposed system is depicted
in Figure 2. The system has six modules, each of
which implements a class of actions. Each module
takes as input a recognized user utterance, an an-
alyzed predicate-argument (P-A) structure and the
detected user focus.
The system begins dialogues by selecting the
?topic presentation (TP)? module, which presents
a new topic selected from a news article. The sys-
tem chooses the next module based on the user?s
response. In our task, the system assumes that
each news article corresponds to a single topic,
and the system presents a headline of news in the
TP module. If the user shows interest (positive
response) in the topic without any specific ques-
tions, the system selects the ?story telling (ST)?
module to give details of the news. In the STmod-
ule, the system provides a summary of the news
article by using lead sentences. The system can
also provide related topics with the ?proactive pre-
sentation (PP)? module. This module is invoked
by system initiative; this module is not invoked by
any user request. If the user makes a specific ques-
tion regarding the topic, the system switches to the
?question answering (QA)? module to answer the
question. This module answers questions on the
presented topic and related topics.
The modules of PP and QA are based on a di-
alogue framework which uses the similarity of P-
A structures (Yoshino et al., 2011). This frame-
work defines the similarity of P-A structures be-
tween user queries and news articles, and retrieves
or recommends the appropriate sentence from the
news articles. This method searches for appropri-
ate information from automatically parsed docu-
ments by referring to domain knowledge that is
automatically extracted from domain corpus.
Transitions between the modules are allowed as
shown in Figure 2. The modules ?greeting (GR)?
and ?keep silence (KS)? are also implemented.
GR module generates fixed greeting patterns by
using regular expression matching. In terms of
dialogue flow, these modules can be used at any
time.
2.2 User focus in information navigation
?Focus? in discourse is ?attentional state (that)
contains information about the objects, properties,
relations, and discourse intentions that are most
salient at any given point.? (Grosz and Sidner,
1986). The user has specific attention to an ob-
ject if the user utterance contains the focus. In
this work, we define the user focus as ?the main
piece of information of interest to the user.? It
makes a central component when making a reply
or selecting relevant topics at the current dialogue
state. For example, given ?Did Ichiro perform bril-
33
SYS, tp, ?Matsui, a part-time coach for the Giants, demonstrated 5 homeruns. The Giants president
told him "Come back as a player!?
USR, st, 0, ? Give me the detail??, ?Give:me/0 the:detail/0?
SYS, st, ?Matsui joined the extra batting training in the Giants' camp, and demonstrated his batting
to young players.?
USR, qa, 1, ?Will Matsui become the Giants' manager in the future??, ?Matsui/0 will:become/0
the Giants' manager/1 in the future/0?
SYS, qa, ?The director of Giants told him "Come back as a player!??
USR, tp, 0, ?Next news, please.?, ?Next:news/0 please/0?
SYS, tp, ? To be a ace pitcher, has Fujinami improved from the rookie year?
?
Figure 3: An example of annotation for collected dialogue. System utterances have a tuple of three
elements separated by a comma: speaker, called module and utterance. User utterances have a tuple of
four elements: speaker, the module the user request falls in, binary information of user focus, utterance
and user focus annotation on each phrase or P-A element. (This example is translated from Japanese)
liantly?,? user focus is ?Ichiro? because the sys-
tem reply should include information on Ichiro.
This information is annotated on content words or
named entities in a user utterance. In the POMDP,
decisions are made based on whether any user fo-
cus was detected in the user?s utterance.
3 Spoken Language Understanding
(SLU)
In this section, we present the spoken language un-
derstanding components of our system. It detects
the user?s focus and intention and provides these
to the dialogue manager. These spoken language
understanding modules are formulated with a sta-
tistical model to give likelihoods which are used
in POMDP.
3.1 Dialogue data
We collected 606 utterances (from 10 users) with a
rule-based dialogue system (Yoshino et al., 2011).
We annotated two kinds of tags: user intention (6
tags defined in Section 3.3), and focus information
defined in Section 2.2. An example of annotation
is shown in Figure 3. We highlighted annotation
points in the bold font.
To prepare the training data, each utterance was
labeled with one of the six modules, indicating the
best module to respond. In addition, each phrase
or P-A elements is labeled to indicated whether it
is the user?s focus or not. The user focus is deter-
mined by the attributes (=specifications of words
in the domain) and preference order of phrases to
identify the most appropriate information that the
user wants to know. For example, in the second
user utterance in Figure 3, the user?s focus is the
phrase ?the Giants? manager?. These tags are an-
notated by one person.
3.2 User focus detection based on CRF
To detect the user focus, we use a conditional
random field (CRF)
1
. The problem is defined as
a sequential labeling of the focus labels to a se-
quence of the phrases of the user utterance. Fea-
tures used are shown in the Table 1. ORDER fea-
tures are the order of the phrase in the sequence
and in the P-A structure. We incorporate these
features because the user focus often appears in
the first phrase of the user utterance. POS fea-
tures are part-of-speech (POS) tags and their pairs
in the phrase. P-A features are semantic role of the
P-A structure. We also incorporate the domain-
dependent predicate-argument (P-A) scores that
are defined with an unsupervised method (Yoshino
et al., 2011). The score is discretized to 0.01, 0.02,
0.05, 0.1, 0.2, 0.5.
Table 2 shows the accuracy of user focus de-
tection, which was conducted via five-fold cross-
validation. ?Phrase? is phrase-base accuracy and
?sentence? indicates whether the presence of any
user focus phrase was correctly detected (or not),
regardless of whether the correct phrase was iden-
tified. This table indicates that WORD features
are effective for detecting the user focus, but they
are not essential for in the sentence-level accuracy.
In this paper, we aim for portability across do-
mains; therefore the dialogue manager only uses
the sentence-level feature, so in our system we do
not user the WORD features.
3.3 User intention analysis based on LR
The module classifies the user intention from the
user utterance. We define six intentions as below.
? TP: request to the TP module.
1
CRFsuite (Okazaki, 2007).
34
Table 1: Features of user focus detection.
feature type feature
ORDER Rank in a sequence of phrases
Rank in a sequence of elements of P-A
POS POS tags in the phrase
POS tag sequence
POSORDER Pair of POS tag and its order in the
phrase
P-A Which semantic role the phrase has
Which semantic roles exist on the
utterance
P-AORDER Pair of semantic role and its order in
the utterance
P-A score P-A templates score
WORD Words in the phrase
Pair of words in the phrase
Pair of word and its order in the phrase
Table 2: Accuracy of user focus detection.
Accuracy
phrase 86.7%
phrase + (WORD) 90.3%
sentence (focus exist or not) 99.8%
sentence (focus exist or not) + (WORD) 99.8%
? ST: request to the ST module.
? QA: request to the QA module.
? GR: greeting to the GR module.
? NR: silence longer than a threshold.
? II: irrelevant input due to ASR errors or noise.
We adopt logistic regression (LR)-based dia-
logue act tagging approach (Tur et al., 2006). The
probability of user intention o given an ASR result
of the user utterance h is defined as,
P (o|h) =
exp(? ? ?(h, o))
?
o
exp(? ? ?(h, o))
. (1)
Here, ? is a vector of feature weights and ?(h, o)
is a feature vector. We use POS, P-A and P-A tem-
plates score as a feature set. In addition, we add a
typical expression feature (TYPICAL) to classify
TP, ST or GR tags. For example, typical expres-
sions in conversation are ?Hello? or ?Go on,? and
those in information navigation are ?News of the
day? or ?Tell me in detail.? Features for the clas-
sifier are shown in the Table 3.
The accuracy of the classification in five-fold
cross-validation is shown in Table 4. The TYP-
Table 3: Features of user intention analysis.
feature type feature
POS Bag of POS tags
Bag of POS bi-gram
P-A Bag of semantic role labels
Bag of semantic role labels bi-gram
Pair of semantic role label and its rank
P-A score P-A templates score
TYPICAL Occurrence of typical expressions
Table 4: Accuracy of user intention analysis.
All features without TYPICAL
TP 100% 100%
ST 75.3% 64.2%
QA 94.1% 93.5%
GR 100% 100%
II 16.7% 16.7%
All 92.1% 90.2%
ICAL feature improves the classification accuracy
while keeping the domain portability.
3.4 SLU for ASR output
ASR and intention analysis involves errors. Here,
s is a true user intention and o is an observed in-
tention. The observation model P (o|s) is given
by the likelihood of ASR result P (h|u) (Komatani
and Kawahara, 2000) and the likelihood of the in-
tention analysis P (o|h),
P (o|s) =
?
h
P (o, h|s) (2)
?
?
h
P (o|h)P (h|u). (3)
Here, u is an utterance of the user. We combine
the N-best (N = 5) hypotheses of the ASR result
h.
4 Dialogue Management for Information
Navigation
The conventional dialogue management for task-
oriented dialogue systems is designed to reach a
task goal as soon as possible (Williams and Young,
2007). In contrast, information navigation does
not always have a clear goal, and the aim of infor-
mation navigation is to provide as much relevant
information as the user is interested in. Therefore,
our dialogue manager refers user involvement or
engagement (=level of interest) and the user focus
35
(=object of interest). This section describes the
general dialogue management based on POMDP,
and then gives an explanation of the proposed dia-
logue management using the user focus.
4.1 Dialogue management based on POMDP
The POMDP-based statistical dialogue manage-
ment is formulated as below. The random vari-
ables involved at a dialogue turn t are as follows:
? s ? I
s
: user state
User intention.
? a ? K: system action
Module that the system selects.
? o ? I
s
: observation
Observed user state, including ASR and in-
tention analysis errors.
? b
s
i
= P (s
i
|o
1:t
): belief
Stochastic variable of the user state.
? pi: policy function
This function determines a system action a
given a belief of user b. pi
?
is the optimal pol-
icy function that is acquired by the training.
? r: reward function
This function gives a reward to a pair of the
user state s and the system action a.
The aim of the statistical dialogue management is
to output an optimal system action a?
t
given a se-
quence of observation o
1:t
from 1 to t time-steps.
Next, we give the belief update that includes the
observation and state transition function. The be-
lief update of user state s
i
in time-step t is defined
as,
b
t+1
s
?
j
? P (o
t+1
|s
?
j
)
? ?? ?
Obs.
?
s
i
P (s
?
j
|s
i
, a?
k
)
? ?? ?
Trans.
b
t
s
i
. (4)
Obs. is an observation function which is defined
in Equation (3) and Trans. is a state transition
probability of the user state. Once the system es-
timates the belief b
t
s
i
, the policy function outputs
the optimal action a? as follows:
a? = pi
?
(b
t
). (5)
4.2 Training of POMDP
We applied Q-learning (Monahan, 1982; Watkins
and Dayan, 1992) to acquire the optimal policy
pi
?
. Q-learning relies on the estimation of a Q-
function, which maximizes the discounted sum of
future rewards of the system action a
t
at a dialogue
turn t given the current belief b
t
. Q-learning is
performed by iterative updates on the training dia-
logue data:
Q(b
t
, a
t
) ? (1? ?)Q(b
t
, a
t
)
+ ?[R(s
t
, a
t
) + ? max
a
t+1
Q(b
t+1
, a
t+1
)], (6)
where ? is a learning rate, ? is a discount factor of
a future reward. We experimentally decided ? =
0.01 and ? = 0.9. The optimal policy given by the
Q-function is determined as,
pi
?
(b
t
) = argmax
a
t
Q(b
t
, a
t
). (7)
However, it is impossible to calculate the Q-
function for all possible real values of belief b.
Thus, we train a limited Q-function given by a
Grid-based Value Iteration (Bonet, 2002). The be-
lief is given by a function,
b
s
i
=
{
? if s = i
1??
|I
s
|
if s ?= i
. (8)
Here, ? is a likelihood of s = i that is output
of the intention analyzer, and we selected 11 dis-
crete points from 0.0 to 1.0 by 0.1. We also added
the case of uniform distribution. The observation
function of the belief update is also given in a sim-
ilar manner.
4.3 Dialogue management using user focus
Our POMDP-based dialogue management
chooses actions based on its belief in: the user
intention s and the user focus f (0 or 1 ? J
f
).
The observation o is controlled by hidden states
f and s that are decided by the state transition
probabilities,
P (f
t+1
|f
t
, s
t
, a
t
), (9)
P (s
t+1
|f
t+1
, f
t
, s
t
, a
t
). (10)
We constructed a user simulator by using the an-
notated data described in Section 3.1.
Equation (10) is also used for the state transition
probability of the belief update. The equation of
the belief update (4) is extended by introducing the
previous user focus f
l
and current user focus f
?
m
information,
b
t+1
s
?
j
= P (o
t+1
|s
?
j
)
? ?? ?
Obs.
?
?
i
P (s
?
j
|f
?
m
, f
l
, s
i
, a?
k
)
? ?? ?
Trans.
b
t
s
i
,f
l
. (11)
36
Table 5: Rewards in each turn.
state focus action a
s f TP ST QA PP GR KS
TP
0
+10 -10 -10 -10 -10 -10
1
ST
0
-10 +10 -10 0 -10 -10
1
QA
0
-10
+10 +10 -10
-10 -10
1 -10 +30 +10
GR
0
-10 -10 -10 -10 +10 -10
1
NR
0 +10
-10 -10
-10
-10 0
1 -10 +10
II
0
-10 -10 -10 -10 -10 +10
1
The resultant optimal policy is,
a? = pi
?
(b
t
, f
l
). (12)
4.4 Definition of rewards
Table 5 defines a reward list at the end of a each
turn. The reward of +10 is given to appropriate
actions, 0 to acceptable actions, and -10 to inap-
propriate actions.
In Table 5, pairs of a state and its apparently
corresponding action, TP and TP, ST and ST, QA
and QA, GR and GR, and II and KS, have posi-
tive rewards. Rewards in bold fonts (+10) are de-
fined for the following reasons. If the user asks a
question (QA) without a focus (e.g. ?What hap-
pened on the game??), the system can continue by
story telling (ST). But when the question has a fo-
cus, the system should answer the question (QA),
which is highly rewarded (+30). If the system can-
not find an answer, it can present relevant informa-
tion (PP). When the user says nothing (NR), the
system action should be decided by considering
the user focus; present a new topic if the user is
not interested in the current topic (f=0) or present
an article related to the dialogue history (f=1).
Reward of +200 is given if 20 turns are passed,
to reward a long continued dialogue. The user sim-
ulator terminates the dialogue if the system selects
an inappropriate action (action of r = ?10) five
times, and a large penalty -200 is given to the sys-
tem.
5 Evaluations of Dialogue
We evaluated the proposed system with two exper-
iments; dialogue state tracking with real users and
average reward with a user simulator. For the eval-
uation, we collected an additional 312 utterances
Average of rewards
Noise
-250
-200
-150
-100
-50
0
50
100
150
200
250
300
350
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
w. focus w.o. focus
Figure 4: Effect of introduction of the user focus
in simulation.
(8 users, 24 dialogues) with the proposed dialogue
system.
5.1 Evaluation of dialogue manager with
user simulator
First, we evaluated the dialogue manager with
user simulation that is constructed from the train-
ing corpus (Section 3.1). In this evaluation, the
system calculated average reward of 100,000 di-
alogue sessions between the system and the user
simulator given a fixed noise rate. Figure 4 shows
the effect of the user focus. By introducing the
user focus, the system receives higher rewards
than the model without the user focus. Especially,
the proposed model is more robust with a noise
level of 10?30% that spoken dialogue systems of-
ten encounter (Yoshino et al., 2013a).
5.2 Evaluation of dialogue state tracking
with real users
Dialogue state tracking (DST) is a task of track-
ing the correct user state with a noisy input (e.g.
ASR and NLU errors) (Williams et al., 2013).
It tries to maximize the probability of the belief
of the correct states, but we evaluated the accu-
racy of the 1-best result of the belief update. We
also evaluated the average reward of each dia-
logue. The baseline system is a rule-based dia-
logue manager which we previously implemented.
The baseline system is operated by a score of
the question-answering module using predicate-
argument structures (Yoshino et al., 2011) and reg-
ular expressions for TP and GR modules.
The DST accuracy and average reward are
shown in Table 6. This result shows that the pro-
posed method tracks the dialogue state of the user
with a high accuracy. Our statistical framework
37
Sys: Hello.
Usr: Hello. Tell me today's news.
Sys: Tanaka showed a crazy split. His colleague 
Sabathia was stunned.
Usr: What is the crazy split?
Sys: His split comes just like a four-seam, but it 
drops quickly in front of the home plate.
Usr: Where was he training?
Sys: Tanaka practiced at the bullpen in the New 
York Yankees' camp on Feb. 20th.
Usr: (pause)
Sys: By the way, he also practiced a drop 
two-seam on Feb. 18.
Usr: Does his two-seam drop?
Sys: The catcher said "Tanaka's two-seam not
only slides, but also drops."
Usr: OK, any other news?
Sys: Yankees' Ichiro is training strenuously!
Usr: Tell me more.
Sys: Ichiro came to the field an hour earlier than 
usual and trained strenuously.
?
=TP,    =0.8, (=TP)=0.800, =0
=TP, (=TP =0)=0.8
=QA,    =0.4, (=QA)=0.934, =1
=QA, (=QA, =1)=0.9
=QA,    =0.8, (=QA)=0.973, =1
=QA, (=QA, =1)=1.0
=NR,    =1.0, (=NR)=1.00, =1
=PP, (=NR, =1)=1.0
=QA,    =0.6, (=QA)=0.867, =1
=QA, (=QA, =1)=0.9
=TP,    =0.8, (=TP)=0.984, =0
=TP, (=TP, =0)=1.0
=ST,    =0.9, (=ST)=0.871, =0
=ST, (=ST, =0)=0.9
Dialogue 1
Sys: Hello.
Usr: Hello. Tell me today's news?
Sys: Matsui, a part-time coach for the Giants, 
demonstrated 5 homeruns. The Giants
president told him "Come back as a player!"
Usr: What happened?
Sys: The godzilla cannon came back to the Giants?
spring camp in Miyazaki.
Usr: What is the Godzilla cannon?
Sys: The Godzilla coach demonstrated his 
home-run batting to young players.
Usr: Next please.
Sys: To be a ace pitcher, has Fujinami improved 
from the rookie year?
?
=TP,    =0.8, (=TP)=0.800, =0
=TP, (=TP =0)=0.8
=QA,    =0.8, (=QA)=0.532, =0
=ST, (=QA, =0)=0.5
=QA,    =0.8, (=QA)=0.806, =1
=QA, (=QA, =1)=0.8
=TP,    =0.8, (=TP)=0.986, =0
=TP, (=TP, =0)=1.0
Dialogue 2
Figure 5: A dialogue example. (This example is translated from Japanese)
Table 6: Accuracy of dialogue state tracking.
rule focus POMDP
Accuracy of tracking 0.561 0.869
(1-best) (=175/312) (=271/312)
Average reward -22.9 188.6
improved SLU accuracy and robustness against
ASR errors, especially reducing confusions be-
tween question answering (QA) and topic presen-
tation (TP). Moreover, belief update can detect the
TP state even if the SLU incorrectly predicts QA
or ST.
5.3 Discussion of trained policy
An example dialogue is shown in Figure 5. In
the example, the system selects appropriate ac-
tions even if the observation likelihood is low. At
the 4th turn of Dialogue 1 in this example, the sys-
tem with the user focus responds with an action of
proactive presentation a=PP, but the system with-
out the user focus responds with an action of topic
presentation a=TP. At the 2nd turn of Dialogue 2,
the user asks a question without a focus. The con-
fidence of s=QA is lowered by the belief update,
and the system selects the story telling module
a=ST. These examples show that the training re-
sult (=learned policy) reflects our design described
in Section 4.4: It is better to make a proactive pre-
sentation when the user is interested in the topic.
6 Conclusions
We constructed a spoken dialogue system for in-
formation navigation ofWeb news articles updated
day-by-day. The system presents relevant infor-
38
mation according to the user?s interest, by track-
ing the user focus. We introduce the user focus
detection model, and developed a POMDP frame-
work which tracks user focus to select the appro-
priate action class (module) of the dialogue sys-
tem. In experimental evaluations, the proposed di-
alogue management approach determines the state
of the user more accurately than the existing sys-
tem based on rules. An evaluation with a user sim-
ulator shows that including user focus in the dia-
logue manager?s belief state improves robustness
to ASR/SLU errors.
In future work, we plan to evaluate the system
with a large number of real users on a variety of
domains, and optimize the reward function for the
information navigation task.
Acknowledgments
We thank Dr. Jason Williams for his valuable and
detailed advice to improve this paper on SIGDIAL
mentoring program. This work was supported by
Grant-in-Aid for JSPS Fellows 25-4537.
References
Dan Bohus and Alexander I. Rudnicky. 2003. Raven-
claw: Dialog management using hierarchical task
decomposition and an expectation agenda. In Pro-
ceedings of the 8th European Conference on Speech
Communication and Technology, pages 597?600.
Blai Bonet. 2002. An e-optimal grid-based algorithm
for partially observable Markov decision processes.
In Proceedings of International Conference on Ma-
chine Learning, pages 51?58.
Deborah A. Dahl, Madeleine Bates, Michael Brown,
William Fisher, Kate Hunicke-Smith, David Pallett,
Christine Pao, Alexander Rudnicky, and Elizabeth
Shriberg. 1994. Expanding the scope of the ATIS
task: the ATIS-3 corpus. In Proceedings of the
workshop on Human Language Technology, pages
43?48.
Barbara J. Grosz and Candace L. Sidner. 1986. Atten-
tion, intentions, and the structure of discourse. Com-
putational Linguistics, 12(3):175?204.
Ryuichiro Higashinaka, Katsuhito Sudoh, and Mikio
Nakano. 2006. Incorporating discourse features
into confidence scoring of intention recognition re-
sults in spoken dialogue systems. Speech Communi-
cation, 48(3):417?436.
Tatsuya Kawahara. 2009. New perspectives on spoken
language understanding: Does machine need to fully
understand speech? In Proceedings of IEEE work-
shop on Automatic Speech Recognition and Under-
standing, pages 46?50.
Kazunori Komatani and Tatsuya Kawahara. 2000.
Flexible mixed-initiative dialogue management us-
ing concept-level confidence measures of speech
recognizer output. In Proceedings of the 18th con-
ference on Computational linguistics, pages 467?
473.
Esther Levin, Roberto Pieraccini, and Wieland Eckert.
2000. A stochastic model of human-machine inter-
action for learning dialog strategies. IEEE Transac-
tions on Speech and Audio Processing, 8(1):11?23.
Toyomi Meguro, Ryuichiro Higashinaka, Yasuhiro Mi-
nami, and Kohji Dohsaka. 2010. Controlling
listening-oriented dialogue using partially observ-
able markov decision processes. In Proceedings
of the 23rd International Conference on Computa-
tional Linguistics, pages 761?769.
Teruhisa Misu and Tatsuya Kawahara. 2010. Bayes
risk-based dialogue management for document re-
trieval system with speech interface. Speech Com-
munication, 52(1):61?71.
George E. Monahan. 1982. State of the art? a survey
of partially observable Markov decision processes:
Theory, models, and algorithms. Management Sci-
ence, 28(1):1?16.
Naoaki Okazaki. 2007. CRFsuite: a fast implementa-
tion of Conditional Random Fields (CRFs).
Yi-Cheng Pan, Hung yi Lee, and Lin shan Lee. 2012.
Interactive spoken document retrieval with sug-
gested key terms ranked by a markov decision pro-
cess. IEEE Transactions on Audio, Speech, and
Language Processing, 20(2):632?645.
Emanuel A. Schegloff and Harvey Sacks. 1973. Open-
ing up closings. Semiotica, 8(4):289?327.
Tomohide Shibata, Yusuke Egashira, and Sadao Kuro-
hashi. 2014. Chat-like conversational system based
on selection of reply generating module with rein-
forcement learning. In Proceedings of the 5th In-
ternational Workshop Series on Spoken Dialog Sys-
tems.
Gokhan Tur, Umit Guz, and Dilek Hakkani-Tur. 2006.
Model adaptation for dialog act tagging. In Pro-
ceedings of IEEE workshop on Spoken Language
Technology, pages 94?97. IEEE.
Christopher JCH Watkins and Peter Dayan. 1992. Q-
learning. Machine learning, 8(3):279?292.
Jason D. Williams and Steve Young. 2007. Par-
tially observable Markov decision processes for spo-
ken dialog systems. Computer Speech & Language,
21(2):393?422.
Jason D. Williams, Antoine Raux, Deepak Ramachan-
dran, and Alan Black. 2013. The dialog state track-
ing challenge. In Proceedings of the 14th Annual
Meeting of the Special Interest Group on Discourse
and Dialogue, pages 404?413.
39
Koichiro Yoshino, Shinsuke Mori, and Tatsuya Kawa-
hara. 2011. Spoken dialogue system based on infor-
mation extraction using similarity of predicate argu-
ment structures. In Proceedings of the 12th Annual
Meeting of the Special Interest Group on Discourse
and Dialogue, pages 59?66.
Koichiro Yoshino, Shinsuke Mori, and Tatsuya Kawa-
hara. 2013a. Incorporating semantic information to
selection of web texts for language model of spoken
dialogue system. In Proceedings of IEEE Interna-
tional Conference on Acoustic, Speech and Signal
Processing, pages 8252?8256.
Koichiro Yoshino, Shinji Watanabe, Jonathan Le Roux,
and John R. Hershey. 2013b. Statistical dialogue
management using intention dependency graph. In
Proceedings of the 6th International Joint Confer-
ence on Natural Language Processing, pages 962?
966.
Steve Young, Milica Ga?si?c, Simon Keizer, Franc?ois
Mairesse, Jost Schatzmann, Blaise Thomson, and
Kai Yu. 2010. The hidden information state model:
A practical framework for POMDP-based spoken
dialogue management. Computer Speech & Lan-
guage, 24(2):150?174.
40
