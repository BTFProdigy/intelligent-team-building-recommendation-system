Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 201?204,
New York, June 2006. c?2006 Association for Computational Linguistics
Bridging the Inflection Morphology Gap for Arabic Statistical Machine
Translation
Andreas Zollmann and Ashish Venugopal and Stephan Vogel
School of Computer Science
Carnegie Mellon University
{zollmann,ashishv,stephan.vogel}@cs.cmu.edu
Abstract
Statistical machine translation (SMT) is
based on the ability to effectively learn
word and phrase relationships from par-
allel corpora, a process which is consid-
erably more difficult when the extent of
morphological expression differs signifi-
cantly across the source and target lan-
guages. We present techniques that se-
lect appropriate word segmentations in
the morphologically rich source language
based on contextual relationships in the
target language. Our results take ad-
vantage of existing word level morpho-
logical analysis components to improve
translation quality above state-of-the-art
on a limited-data Arabic to English speech
translation task.
1 Introduction
The problem of translating from a language ex-
hibiting rich inflectional morphology to a language
exhibiting relatively poor inflectional morphology
presents several challenges to the existing compo-
nents of the statistical machine translation (SMT)
process. This inflection gap causes an abundance of
surface word forms 1 in the source language com-
pared with relatively few forms in the target lan-
guage. This mismatch aggravates several issues
1We use the term surface form to refer to a series of charac-
ters separated by whitespace
found in natural language processing: more un-
known words forms in unseen data, more words oc-
curring only once, more distinct words and lower
token-to-type ratios (mean number of occurrences
over all distinct words) in the source language than
in the target language.
Lexical relationships under the standard IBM
models (Brown et al, 1993) do not account for
many-to-many mappings, and phrase extraction re-
lies heavily on the accuracy of the IBM word-to-
word alignment. In this work, we propose an ap-
proach to bridge the inflectional gap that addresses
the issues described above through a series of pre-
processing steps based on the Buckwalter Arabic
Morphological Analyzer (BAMA) tool (Buckwalter,
2004). While (Lee et al, 2003) develop accurate
segmentation models of Arabic surface word forms
using manually segmented data, we rely instead on
the translated context in the target language, lever-
aging the manually constructed lexical gloss from
BAMA to select the appropriate segmented sense for
each Arabic source word.
Our technique, applied as preprocessing to the
source corpus, splits and normalizes surface words
based on the target sentence context. In contrast
to (Popovic and Ney, 2004) and (Nie?en and Ney,
2004), we do not modify the IBM models, and we
leave reordering effects to the decoder. Statistically
significant improvements (Zhang and Vogel, 2004)
in BLEU and NIST translation score over a lightly
stemmed baseline are reported on the available and
well known BTEC IWSLT?05 Arabic-English cor-
pus (Eck and Hori, 2005).
201
2 Arabic Morphology in Recent Work
Arabic-to-English machine translation exemplifies
some of the issues caused by the inflection gap. Re-
fer to (Buckwalter, 2005) and (Larkey et al, 2002)
for examples that highlight morphological inflection
for a simple Modern Standard Arabic (MSA) word
and basic stemming operations that we use as our
baseline system.
(Nie?en and Ney, 2000) tackle the inflection gap
for German-to-English word alignment by perform-
ing a series of morphological operations on the Ger-
man text. They fragment words based on a full
morphological analysis of the sentence, but need to
use domain specific and hand written rules to deal
with ambiguous fragmentation. (Nie?en and Ney,
2004) also extend the corpus by annotating each
source word with morphological information and
building a hierarchical lexicon. The experimental
results show dramatic improvements from sentence-
level restructuring (question inversion, separated
verb prefixes and merging phrases), but limited im-
provement from the hierarchical lexicon, especially
as the size of the training data increases.
We conduct our morphological analysis at the
word level, using Buckwalter Arabic Morphological
Analyzer (BAMA) version 2.0 (Buckwalter, 2004).
BAMA analyzes a given surface word, returning a
set of potential segmentations (order of a dozen) for
the source word into prefixes, stems, and suffixes.
Our techniques select the appropriate splitting from
that set by taking into account the target sides (full
sentences) of that word?s occurrences in the training
corpus. We now describe each splitting technique
that we apply.
2.1 BAMA: Simple fragment splitting
We begin by simply replacing each Arabic word
with the fragments representing the first of the pos-
sible splittings returned by the BAMA tool. BAMA
uses simple word-based heuristics to rank the split-
ting alternatives.
2.2 CONTEXT: Single Sense selection
In the step CONTEXT, we take advantage of the
gloss information provided in BAMA?s lexicon.
Each potential splitting corresponds to a particular
choice of prefix, stem and suffix, all of which exist
in the manually constructed lexicon, along with a set
of possible translations (glosses) for each fragment.
We select a fragmentation (choice of splitting for the
source word) whose corresponding glosses have the
most target side matches in the parallel translation
(of the full sentence). The choice of fragmentation
is saved and used for all occurrences of the surface
form word in training and testing, introducing con-
text sensitivity without parsing solutions. In case of
unseen words during testing, we segment it simply
using the first alternative from the BAMA tool. This
allows us to still translate an unseen test word cor-
rectly even if the surface form was never seen during
training.
2.3 CORRMATCH: Correspondence matching
The Arabic language often encodes linguistic in-
formation within the surface word form that is not
present in English. Word fragments that represent
this missing information are misleading in the trans-
lation process unless explicitly aligned to the NULL
word on the target side. In this step we explicitly
remove fragments that correspond to lexical infor-
mation that is not represented in English. While
(Lee, 2004) builds part of speech models to recog-
nize such elements, we use the fact that their corre-
sponding English translations in the BAMA lexicon
are empty. Examples of such fragments are case and
gender markers. As an example of CORRMATCH
removal, we present the Arabic sentence ? h?*A lA
ya zAl u gayor naZiyf ? (after BAMA only) which
becomes ?h?*A lA ya zAl gayor naZiyf? after the
CORRMATCH stage. The ?u? has been removed.
3 Experimental Framework
We evaluate the impact of inflectional splitting on
the BTEC (Takezawa et al, 2002) IWSLT05 Ara-
bic language data track. The ?Supplied? data track
includes a 20K Arabic/English sentence pair train-
ing set, as well as a development (?DevSet?) and
test (?Test05?) set of 500 Arabic sentences each and
16 reference translations per Arabic sentence. De-
tails regarding the IWSLT evaluation criteria and
data topic and collection methods are available in
(Eck and Hori, 2005). We also evaluate on test and
development data randomly sampled from the com-
plete supplied dev and test data, due to considera-
202
tions noted by (Josep M.Crego, 2005) regarding the
similarity of the development and test data sets.
3.1 System description
Translation experiments were conducted using the
(Vogel et al, 2003) system with reordering and fu-
ture cost estimation. We trained translation parame-
ters for 10 scores (language model, word and phrase
count, and 6 translation model scores from (Vogel,
2005) ) with Minimum Error Rate training on the
development set. We optimized separately for both
the NIST (Doddington, 2002) and the BLEU metrics
(Papineni et al, 2002).
4 Translation Results
Table 1 and 2 shows the results of each stage
of inflectional splitting on the BLEU and NIST
metrics. Basic orthographic normalization serves
as a baseline (merging all Alif, tar marbuta, ee
forms to the base form). The test set NIST scores
show steady improvements of up to 5 percent rel-
ative, as more sophisticated splitting techniques
are used, ie BAMA+CONTEXT+CORRMATCH.
These improvements are statistically significant over
the baseline in both metrics as measured by the tech-
niques in (Zhang and Vogel, 2004).
Our NIST results for all the final stages of inflec-
tional splitting would place us above the top NIST
scores from the ISWLT evaluation on the supplied
test set.2 On both DevSet/Test05 and the randomly
split data, we see more dramatic improvements in
the NIST scores than in BLEU. This might be due to
the NIST metric?s sensitivity to correctly translating
certain high gain words in the test corpus. Inflec-
tional splitting techniques that cause previously un-
known surface form words to be translated correctly
after splitting can significantly impact the overall
score.
5 Conclusion and Future Work
This work shows the potential for significant im-
provements in machine translation quality by di-
rectly bridging the inflectional gap across language
pairs. Our method takes advantage of source and
2The IWSLT evaluation did not allow systems to train sep-
arately for evaluation on BLEU or NIST, but results from the
proceedings indicate that top performers in each metric opti-
mized towards the respective metric.
target language context when conducting morpho-
logical analysis of each surface word form, while
avoiding complex parsing engines or refinements to
the alignment training process. Our results are pre-
sented on moderately sized corpora rather than the
scarce resource domain that has been traditionally
employed to highlight the impact of detailed mor-
phological analysis.
By showing the impact of simple processing steps
we encourage the creation of simple word and gloss
level analysis tools for new languages and show
that small investments in this direction (compared
to high octane context sensitive parsing tools) can
yield dramatic improvements, especially when rapid
development of machine translation tools becomes
increasingly relevant to the research community.
While our work focused on processing the morpho-
logically rich language and then translating ?down?
into the morphologically poor language, we plan to
use the analysis tools developed here to model the
reverse translation process as well, the harder task
of translating ?up? into a highly inflected space.
References
Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della
Pietra, and Robert L. Mercer. 1993. The mathemat-
ics of statistical machine translation: parameter esti-
mation. Comput. Linguist., 19(2):263?311.
Tim Buckwalter. 2004. Buckwalter Arabic Mor-
phological Analyzer Version 2.0. LDC Cata-
log No. LDC2004L02, Linguistic Data Consortium,
www.ldc.upenn.edu/Catalog.
Tim Buckwalter. 2005.
www.qamus.org/morphology.htm.
George Doddington. 2002. Automatic evaluation of ma-
chine translation quality using n-gram co-occurrence
statistics. In In Proc. ARPA Workshop on Human Lan-
guage Technology.
Matthias Eck and Chiori Hori. 2005. Overview of the
IWSLT 2005 evaluation campaign. In Proceedings of
International Workshop on Spoken Language Transla-
tion, pages 11?17.
Jose B.Marino Josep M.Crego, Adria de Gispert. 2005.
The talp ngram-based smt system for iwslt?05. In Pro-
ceedings of International Workshop on Spoken Lan-
guage Translation, pages 191?198.
203
Inflection system NIST ? Dev. NIST ? Test BLEU ? Dev. BLEU ? Test
No preprocessing 9.33 9.44 51.1 49.7
Orthographic normalization (baseline) 9.41 9.51 51.0 49.7
BAMA 9.90 9.76 (+2.5%) 52.0 50.2 (+1%)
BAMA+CONTEXT+CORRMATCH 9.91 10.02 (+5.3%) 52.8 52.0 (+4.7%)
Table 1: Translation results for each stage of inflectional splitting for the merged, sampled dev. and test
data, highest scores in bold, relative improvements in brackets
Inflection system NIST ? Dev. NIST ? Test BLEU ? Dev. BLEU ? Test
No preprocessing 9.46 9.38 51.1 49.6
Orthographic normalization (baseline) 9.58 9.35 52.1 49.8
BAMA 10.10 9.60 (+2.7%) 53.8 48.8 (-2%)
BAMA+CONTEXT+CORRMATCH 10.08 9.79 (+4.7%) 53.7 50.6 (+1.6%)
Table 2: Translation results for each stage of inflectional splitting for the BTEC Supplied DevSet/Test05
data, highest scores in bold, relative improvements in brackets
Leah Larkey, Lisa Ballesteros, and Margaret Connell.
2002. Improving stemming for arabic information re-
trieval: Light stemming and co-occurrence analysis.
In Proc. of the 25th annual international ACM SIGIR
conference on Research and development information
retrieval.
Young-Suk Lee, Kishore Papineni, Salim Roukos, Os-
sama Emam, and Hany Hassan. 2003. Language
model based arabic word segmentation. In ACL, Sap-
poro, Japan, July 6-7.
Young-Suk Lee. 2004. Morphological analysis for sta-
tistical machine translation. In Proceedings of the Hu-
man Language Technology and North American As-
sociation for Computational Linguistics Conference
(HLT/NAACL), Boston,MA, May 27-June 1.
Sonja Nie?en and Hermann Ney. 2000. Improving SMT
quality with morpho-syntactic analysis. In The 18th
International Conference on Computational Linguis-
tics.
Sonja Nie?en and Herman Ney. 2004. Statistical ma-
chine translation with scarce resources using morpho-
syntactic information. Comput. Linguist., 30(2):181?
204.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
Association of Computational Linguistics, pages 311?
318.
H. Popovic and Hermann Ney. 2004. Improving word
alignment quality using morpho-syntactic informa-
tion. In 20th International Conference on Computa-
tional Linguistics (CoLing), Geneva, Switzerland.
Toshiyuki Takezawa, Eiichiro Sumita, Fumiaki Sugaya,
Hirofumi Yamamoto, and Seiichi Yamamoto. 2002.
Toward a broad-coverage bilingual corpus for speech
translation of travel conversations in the real world. In
Proc. of LREC 2002, pages 147?152, Las Palmas, Ca-
nary Islands, Spain, May.
Stephan Vogel, Ying Zhang, Fei Huang, Alicia Trib-
ble, Ashish Venogupal, Bing Zhao, and Alex Waibel.
2003. The CMU statistical translation system. In Pro-
ceedings of MT Summit IX, New Orleans, LA, Septem-
ber.
Stephan Vogel. 2005. PESA: Phrase pair extraction as
sentence splitting. In Proceedings of MT Summit X,
Phuket,Thailand, September.
Ying Zhang and Stephan Vogel. 2004. Measuring confi-
dence intervals for the machine translation evaluation
metrics. In Proceedings of the 10th International Con-
ference on Theoretical and Methodological Issues in
Machine Translation (TMII), Baltimore, MD, October.
204
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 236?244,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Preference Grammars: Softening Syntactic Constraints
to Improve Statistical Machine Translation
Ashish Venugopal Andreas Zollmann Noah A. Smith Stephan Vogel
Language Technologies Institute
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{ashishv,zollmann,nasmith,vogel}@cs.cmu.edu
Abstract
We propose a novel probabilistic syn-
choronous context-free grammar formalism
for statistical machine translation, in which
syntactic nonterminal labels are represented
as ?soft? preferences rather than as ?hard?
matching constraints. This formalism allows
us to efficiently score unlabeled synchronous
derivations without forgoing traditional
syntactic constraints. Using this score as a
feature in a log-linear model, we are able
to approximate the selection of the most
likely unlabeled derivation. This helps
reduce fragmentation of probability across
differently labeled derivations of the same
translation. It also allows the importance of
syntactic preferences to be learned alongside
other features (e.g., the language model)
and for particular labeling procedures. We
show improvements in translation quality on
small and medium sized Chinese-to-English
translation tasks.
1 Introduction
Probabilistic synchronous context-free grammars
(PSCFGs) define weighted production rules that are
automatically learned from parallel training data. As
in classical CFGs, these rules make use of nontermi-
nal symbols to generalize beyond lexical modeling
of sentences. In MT, this permits translation and re-
ordering to be conditioned on more abstract notions
of context. For example,
VP? ne VB1 pas # do not VB1
represents the discontiguous translation of the
French words ?ne? and ?pas? to ?do not?, in the con-
text of the labeled nonterminal symbol ?VB? (rep-
resenting syntactic category ?verb?). Translation
with PSCFGs is typically expressed as the problem
of finding the maximum-weighted derivation consis-
tent with the source sentence, where the scores are
defined (at least in part) by R-valued weights asso-
ciated with the rules. A PSCFG derivation is a syn-
chronous parse tree.
Defining the translation function as finding the
best derivation has the unfortunate side effect of
forcing differently-derived versions of the same tar-
get sentence to compete with each other. In other
words, the true score of each translation is ?frag-
mented? across many derivations, so that each trans-
lation?s most probable derivation is the only one that
matters. The more Bayesian approach of finding
the most probable translation (integrating out the
derivations) instantiates an NP-hard inference prob-
lem even for simple word-based models (Knight,
1999); for grammar-based translation it is known
as the consensus problem (Casacuberta and de la
Higuera, 2000; Sima?an, 2002).
With weights interpreted as probabilities, the
maximum-weighted derivation is the maximum a
posteriori (MAP) derivation:
e? ? argmax
e
max
d
p(e, d | f)
where f is the source sentence, e ranges over tar-
get sentences, and d ranges over PSCFG deriva-
tions (synchronous trees). This is often described
as an approximation to the most probable transla-
tion, argmaxe
?
d p(e, d | f). In this paper, we
will describe a technique that aims to find the most
probable equivalence class of unlabeled derivations,
rather than a single labeled derivation, reducing the
fragmentation problem. Solving this problem ex-
actly is still an NP-hard consensus problem, but we
provide approximations that build on well-known
PSCFG decoding methods. Our model falls some-
where between PSCFGs that extract nonterminal
symbols from parse trees and treat them as part of
236
the derivation (Zollmann and Venugopal, 2006) and
unlabeled hierarchical structures (Chiang, 2005); we
treat nonterminal labels as random variables chosen
at each node, with each (unlabeled) rule express-
ing ?preferences? for particular nonterminal labels,
learned from data.
The paper is organized as follows. In Section 2,
we summarize the use of PSCFG grammars for
translation. We describe our model (Section 3).
Section 4 explains the preference-related calcula-
tions, and Section 5 addresses decoding. Experi-
mental results using preference grammars in a log-
linear translation model are presented for two stan-
dard Chinese-to-English tasks in Section 6. We re-
view related work (Section 7) and conclude.
2 PSCFGs for Machine Translation
Probabilistic synchronous context-free grammars
(PSCFGs) are defined by a source terminal set
(source vocabulary) TS , a target terminal set (target
vocabulary) TT , a shared nonterminal set N and a
setR of rules of the form: X ? ??, ?,w? where
? X ? N is a labeled nonterminal referred to as the
left-hand-side of the rule.
? ? ? (N ? TS)? is the source side of the rule.
? ? ? (N ? TT )? is the target side of the rule.
? w ? [0,?) is a nonnegative real-valued weight
assigned to the rule.
For visual clarity, we will use the # character to sep-
arate the source side of the rule ? from the target
side ?. PSCFG rules also have an implied one-to-
one mapping between nonterminal symbols in ? and
nonterminals symbols in ?. Chiang (2005), Zoll-
mann and Venugopal (2006) and Galley et al (2006)
all use parameterizations of this PSCFG formalism1.
Given a source sentence f and a PSCFG G, the
translation task can be expressed similarly to mono-
lingual parsing with a PCFG. We aim to find the
most likely derivation d of the input source sentence
and read off the English translation, identified by
composing ? from each rule used in the derivation.
This search for the most likely translation under the
1Galley et al (2006) rules are formally defined as tree trans-
ducers but have equivalent PSCFG forms.
MAP approximation can be defined as:
e? = tgt
(
argmax
d?D(G):src(d)=f
p(d)
)
(1)
where tgt(d) is the target-side yield of a derivation
d, and D(G) is the set of G?s derivations. Using an
n-gram language model to score derivations and rule
labels to constraint the rules that form derivations,
we define p(d) as log-linear model in terms of the
rules r ? R used in d as:
p(d) = pLM(tgt(d))?0 ?
( m?
i=1
pi(d)?i
)
?psyn(d)?m+1/Z(~?)
pi(d) =
?
r?R
hi(r)freq(r;d) (2)
psyn(d) =
{ 1 if d respects label constraints
0 otherwise (3)
where ~? = ?0 ? ? ??m+1 are weights that reflect the
relative importance of features in the model. The
features include the n-gram language model (LM)
score of the target yield sequence, a collection of m
rule feature functions hi : R ? R?0, and a ?syn-
tax? feature that (redundantly) requires every non-
terminal token to be expanded by a rule with that
nonterminal on its left-hand side. freq(r; d) denotes
the frequency of the rule r in the derivation d. Note
that ?m+1 can be effectively ignored when psyn is
defined as in Equation 3. Z(~?) is a normalization
constant that does not need to be computed during
search under the argmax search criterion in Equa-
tion 1. Feature weights ~? are trained discrimina-
tively in concert with the language model weight
to maximize the BLEU (Papineni et al, 2002) au-
tomatic evaluation metric via Minimum Error Rate
Training (MERT) (Och, 2003).
We use the open-source PSCFG rule extraction
framework and decoder from Zollmann et al (2008)
as the framework for our experiments. The asymp-
totic runtime of this decoder is:
O
(
|f |3
[
|N ||TT |2(n?1)
]K) (4)
where K is the maximum number of nonterminal
symbols per rule, |f | the source sentence length, and
237
n is the order of the n-gram LM that is used to com-
pute pLM. This constant factor in Equation 4 arises
from the dynamic programming item structure used
to perform search under this model. Using notation
from Chiang (2007), the corresponding item struc-
ture is:
[X, i, j, q(?)] : w (5)
whereX is the nonterminal label of a derivation, i, j
define a span in the source sentence, and q(?) main-
tains state required to compute pLM(?). Under the
MAP criterion we can discard derivations of lower
weight that share this item structure, but in practice
we often require additional lossy pruning to limit the
number of items produced. The Syntax-Augmented
MT model of Zollmann and Venugopal (2006), for
instance, produces a very large nonterminal set us-
ing ?slash? (NP/NN? the great) and ?plus? labels
(NP+VB ? she went) to assign syntactically mo-
tivated labels for rules whose target words do not
correspond to constituents in phrase structure parse
trees. These labels lead to fragmentation of prob-
ability across many derivations for the same target
sentence, worsening the impact of the MAP approx-
imation. In this work we address the increased frag-
mentation resulting from rules with labeled nonter-
minals compared to unlabeled rules (Chiang, 2005).
3 Preference Grammars
We extend the PSCFG formalism to include soft ?la-
bel preferences? for unlabeled rules that correspond
to alternative labelings that have been encountered
in training data for the unlabeled rule form. These
preferences, estimated via relative frequency counts
from rule occurrence data, are used to estimate the
feature psyn(d), the probability that an unlabeled
derivation can be generated under traditional syn-
tactic constraints. In classic PSCFG, psyn(d) en-
forces a hard syntactic constraint (Equation 3). In
our approach, label preferences influence the value
of psyn(d).
3.1 Motivating example
Consider the following labeled Chinese-to-English
PSCFG rules:
(4) S ? (?? VB1 #
a place where I can VB1
(3) S ? (?? VP1 #
a place where I can VP1
(2) SBAR ? (?? VP1 #
a place where I can VP1
(1) FRAG ? (?? AUX1 #
a place where I can AUX1
(8) VB ? m # eat
(1) VP ? m # eat
(1) NP ? m # eat
(10) NN ? m # dish
where the numbers are frequencies of the rule from
the training corpus. In classical PSCFG we can think
of the nonterminals mentioned in the rules as hard
constraints on which rules can be used to expand a
particular node; e.g., a VP can only be expanded by
a VP rule. In Equation 2, psyn(d) explicitly enforces
this hard constraint. Instead, we propose softening
these constraints. In the rules below, labels are rep-
resented as soft preferences.
(10) X ? (?? X1 #
a place where I can X1?
?
?
p(H0 = S, H1 = VB | r) = 0.4
p(H0 = S, H1 = VP | r) = 0.3
p(H0 = SBAR, H1 = VP | r) = 0.2
p(H0 = FRAG, H1 = AUX | r) = 0.1
?
?
?
(10) X ? m # eat{ p(H0 = VB | r) = 0.8
p(H0 = VP | r) = 0.1
p(H0 = NP | r) = 0.1
}
(10) X ? m # dish
{ p(H0 = NN | r) = 1.0 }
Each unlabeled form of the rule has an associated
distribution over labels for the nonterminals refer-
enced in the rule; the labels are random variables
Hi, with H0 the left-hand-side label. These un-
labeled rule forms are simply packed representa-
tions of the original labeled PSCFG rules. In ad-
dition to the usual features hi(r) for each rule, esti-
mated based on unlabeled rule frequencies, we now
238
have label preference distributions. These are esti-
mated as relative frequencies from the labelings of
the base, unlabeled rule. Our primary contribution
is how we compute psyn(d)?the probability that
an unlabeled derivation adheres to traditional syn-
tactic constraints?for derivations built from prefer-
ence grammar rules. By using psyn(d) as a feature
in the log-linear model, we allow the MERT frame-
work to evaluate the importance of syntactic struc-
ture relative to other features.
The example rules above highlight the potential
for psyn(d) to affect the choice of translation. The
translation of the Chinese word sequence  ( ?
? m can be performed by expanding the non-
terminal in the rule ?a place where I can X1? with
either ?eat? or ?dish.? A hierarchical system (Chi-
ang, 2005) would allow either expansion, relying on
features like pLM to select the best translation since
both expansions occurred the same number of times
in the data.
A richly-labeled PSCFG as in Zollmann and
Venugopal (2006) would immediately reject the rule
generating ?dish? due to hard label matching con-
straints, but would produce three identical, compet-
ing derivations. Two of these derivations would pro-
duce S as a root symbol, while one derivation would
produce SBAR. The two S-labeled derivations com-
pete, rather than reinforce the choice of the word
?eat,? which they both make. They will also com-
pete for consideration by any decoder that prunes
derivations to keep runtime down.
The rule preferences indicate that VB and VP are
both valid labels for the rule translating to ?eat?, and
both of these labels are compatible with the argu-
ments expected by ?a place where I can X1?. Al-
ternatively, ?dish? produces a NN label which is
not compatible with the arguments of this higher-
up rule. We design psyn(d) to reflect compatibility
between two rules (one expanding a right-hand side
nonterminal in the other), based on label preference
distributions.
3.2 Formal definition
Probabilistic synchronous context-free preference
grammars are defined as PSCFGs with the follow-
ing additional elements:
? H: a set of implicit labels, not to be confused
with the explicit label set N .
? pi: H ? N , a function that associates each im-
plicit label with a single explicit label. We can
therefore think ofH symbols as refinements of
the nonterminals inN (Matsusaki et al, 2005).
? For each rule r, we define a probability distri-
bution over vectors ~h of implicit label bindings
for its nonterminals, denoted ppref(~h | r). ~h
includes bindings for the left-hand side nonter-
minal (h0) as well as each right-hand side non-
terminal (h1, ..., h|~h|). Each hi ? H.
When N ,H are defined to include just a single
generic symbol as in (Chiang, 2005), we produce the
unlabeled grammar discussed above. In this work,
we define
? N = {S,X}
? H = {NP,DT,NN ? ? ? } = NSAMT
where N corresponds to the generic labels of Chi-
ang (2005) and H corresponds to the syntactically
motivated SAMT labels from (Zollmann and Venu-
gopal, 2006), and pi maps all elements of H to
X . We will use hargs(r) to denote the set of all
~h = ?h0, h1, ..., hk? ? Hk+1 that are valid bindings
for the rule with nonzero preference probability.
The preference distributions ppref from each rule
used in d are used to compute psyn(d) as described
next.
4 Computing feature psyn(d)
Let us view a derivation d as a collection of nonter-
minal tokens nj , j ? {1, ..., |d|}. Each nj takes an
explicit label in N . The score psyn(d) is a product,
with one factor per nj in the derivation d:
psyn(d) =
|d|?
j=1
?j (6)
Each ?j factor considers the two rules that nj partic-
ipates in. We will refer to the rule above nonterminal
token nj as rj (the nonterminal is a child in this rule)
and the rule that expands nonterminal token j as rj .
The intuition is that derivations in which these
two rules agree (at each j) about the implicit label
239
for nj , in H are preferable to derivations in which
they do not. Rather than making a decision about
the implicit label, we want to reward psyn when rj
and rj are consistent. Our way of measuring this
consistency is an inner product of preference distri-
butions:
?j ?
?
h?H
ppref(h | rj)ppref(h | rj) (7)
This is not quite the whole story, because ppref(? | r)
is defined as a joint distribution of all the implicit
labels within a rule; the implicit labels are not in-
dependent of each other. Indeed, we want the im-
plicit labels within each rule to be mutually consis-
tent, i.e., to correspond to one of the rule?s preferred
labelings, for both hargs(r) and hargs(r).
Our approach to calculating psyn within the dy-
namic programming algorithm is to recursively cal-
culate preferences for each chart item based on (a)
the smaller items used to construct the item and
(b) the rule that permits combination of the smaller
items into the larger one. We describe how the pref-
erences for chart items are calculated. Let a chart
item be denoted [X, i, j, u, ...] where X ? N and i
and j are positions in the source sentence, and
u : {h ? H | pi(h) = X} ? [0, 1]
(where ?h u(h) = 1) denotes a distribution over
possible X-refinement labels. We will refer to it
below as the left-hand-side preference distribution.
Additional information (such as language model
state) may also be included; it is not relevant here.
The simplest case is for a nonterminal token nj
that has no nonterminal children. Here the left-hand-
side preference distribution is simply given by
u(h) = ppref(h | rj) .
and we define the psyn factor to be ?j = 1.
Now consider the dynamic programming step
of combining an already-built item [X, i, j, u, ...]
rooted by explicit nonterminal X , spanning source
sentence positions i to j, with left-hand-side prefer-
ence distribution u, to build a larger item rooted by
Y through a rule r = Y ? ??X1??, ?X1??, w?with
preferences ppref(? | r).2 The new item will have
2We assume for the discussion that ?, ?? ? T ?S and ?, ?? ?
signature [Y, i ? |?|, j + |??|, v, ...]. The left-hand-
side preferences v for the new item are calculated as
follows:
v(h) = v?(h)?
h? v?(h?)
where (8)
v?(h) = ?
h??H:?h,h???hargs(r)
ppref(?h, h?? | r)? u(h?)
Renormalizing keeps the preference vectors on the
same scale as those in the rules. The psyn factor ?,
which is factored into the value of the new item, is
calculated as:
? = ?
h??H:?h,h???hargs(r)
u(h?) (9)
so that the value considered for the new item is w ?
? ? ..., where factors relating to pLM, for example,
may also be included. Coming back to our example,
if we let r be the leaf rule producing ?eat? at shared
nonterminal n1, we generate an item with:
u = ?u(VB) = 0.8, u(VP) = 0.1, u(NP) = 0.1?
?1 = 1
Combining this item with X ? ?  ( ? ? X1
# a place where I can X1 ? as r2 at nonterminal n2
generates a new target item with translation ?a place
where I can eat?, ?2 = 0.9 and v as calculated in
Fig. 1. In contrast, ?2 = 0 for the derivation where
r is the leaf rule that produces ?dish?.
This calculation can be seen as a kind of single-
pass, bottom-up message passing inference method
embedded within the usual dynamic programming
search.
5 Decoding Approximations
As defined above, accurately computing psyn(d) re-
quires extending the chart item structure with u. For
models that use the n-gram LM feature, the item
structure would be:
[X, i, j, q(?), u] : w (10)
Since u effectively summarizes the choice of rules
in a derivation, this extension would partition the
T ?T . If there are multiple nonterminals on the right-hand side
of the rule, we sum over the longer sequences in hargs(r) and
include appropriate values from the additional ?child? items?
preference vectors in the product.
240
v?(S) = ppref (?h = S, h? = VB? | r)u(VB) + ppref (?h = S, h? = VP? | r)u(VP) = (0.4? 0.8) + (0.3? 0.1) = 0.35
v?(SBAR) = p(?h = SBAR, h? = VP? | r)u(VP) = (0.2? 0.1) = 0.02
v = ?v(S) = 0.35/(v?(S) + v?(SBAR)), v(SBAR) = 0.02/v?(S) + v?(SBAR)? = ?v(S) = 0.35/0.37, v(SBAR) = 0.02/0.37?
?2 = u(VB) + u(VP) = 0.8 + 0.1 = 0.9
Figure 1: Calculating v and ?2 for the running example.
search space further. To prevent this partitioning, we
follow the approach of Venugopal et al (2007). We
keep track of u for the best performing derivation
from the set of derivations that share [X, i, j, q(?)]
in a first-pass decoding. In a second top-down pass
similar to Huang and Chiang (2007), we can recal-
culate psyn(d) for alternative derivations in the hy-
pergraph; potentially correcting search errors made
in the first pass.
We face another significant practical challenge
during decoding. In real data conditions, the size
of the preference vector for a single rule can be very
high, especially for rules that include multiple non-
terminal symbols that are located on the left and
right boundaries of ?. For example, the Chinese-
to-English rule X ? ? X1 ? X2 # X1 ?s X2 ? has
over 24K elements in hargs(r) when learned for the
medium-sized NIST task used below. In order to
limit the explosive growth of nonterminals during
decoding for both memory and runtime reasons, we
define the following label pruning parameters:
? ?R: This parameter limits the size of hargs(r) to
the ?R top-scoring preferences, defaulting other
values to zero.
? ?L: This parameter is the same as ?R but applied
only to rules with no nonterminals. The stricter of
?L and ?R is applied if both thresholds apply.
? ?P : This parameter limits the number labels in
item preference vectors (Equation 8) to the ?P
most likely labels during decoding, defaulting
other preferences to zero.
6 Empirical Results
We evaluate our preference grammar model on
small (IWSLT) and medium (NIST) data Chinese-
to-English translation tasks (described in Table 1).
IWSLT is a limited domain, limited resource task
(Paul, 2006), while NIST is a broadcast news task
with wide genre and domain coverage. We use a
subset of the full training data (67M words of En-
glish text) from the annual NIST MT Evaluation.
Development corpora are used to train model pa-
rameters via MERT. We use a variant of MERT that
prefers sparse solutions where ?i = 0 for as many
features as possible. At each MERT iteration, a sub-
set of features ? are assigned 0 weight and optimiza-
tion is repeated. If the resulting BLEU score is not
lower, these features are left at zero.
All systems are built on the SAMT framework
described in Zollmann et al (2008), using a tri-
gram LM during search and the full-order LM dur-
ing a second hypergraph rescoring pass. Reorder-
ing limits are set to 10 words for all systems. Prun-
ing parameters during decoding limit the number of
derivations at each source span to 300.
The system ?Hier.? uses a grammar with a single
nonterminal label as in Chiang (2005). The system
?Syntax? applies the grammar from Zollmann and
Venugopal (2006) that generates a large number of
syntactically motivated nonterminal labels. For the
NIST task, rare rules are discarded based on their
frequency in the training data. Purely lexical rules
(that include no terminal symbols) that occur less
than 2 times, or non-lexical rules that occur less than
4 times are discarded.
IWSLT task: We evaluate the preference gram-
mar system ?Pref.? with parameters ?R = 100,
?L = 5, ?P = 2. Results comparing systems Pref.
to Hier. and Syntax are shown in Table 2. Auto-
matic evaluation results using the preference gram-
mar translation model are positive. The preference
grammar system shows improvements over both the
Hier. and Syntax based systems on both unseen eval-
uation sets IWSLT 2007 and 2008. The improve-
ments are clearest on the BLEU metric (matching
the MERT training criteria). On 2007 test data,
Pref. shows a 1.2-point improvement over Hier.,
while on the 2008 data, there is a 0.6-point improve-
ment. For the IWSLT task, we report additional au-
241
System Name Words in Target Text LM singleton 1-n-grams (n) Dev. Test
IWSLT 632K 431K (5) IWSLT06 IWSLT07,08
NIST 67M 102M (4) MT05 MT06
Table 1: Training data configurations used to evaluate preference grammars. The number of words in the target text
and the number of singleton 1-n-grams represented in the complete model are the defining statistics that characterize
the scale of each task. For each LM we also indicate the order of the n-gram model.
System Dev
BLEU
(lpen) ?
2007
BLEU
(lpen) ?
2008
BLEU
(lpen) ?
2008
WER ?
2008 PER
?
2008
MET. ?
2008
GTM ?
Hier. 28.0
(0.89)
37.0
(0.89)
45.9
(0.91)
44.5 39.9 61.8 70.7
Syntax 30.9
(0.96)
35.5
(0.94)
45.3
(0.95)
45.7 40.4 62.1 71.5
Pref. 28.3
(0.88)
38.2
(0.90)
46.3
(0.91)
43.8 40.0 61.7 71.2
Table 2: Translation quality metrics on the IWSLT translation task, with IWSLT 2006 as the development corpora, and
IWSLT 2007 and 2008 as test corpora. Each metric is annotated with an ? if increases in the metric value correspond
to increase in translation quality and a ? if the opposite is true. We also list length penalties for the BLEU metric to
show that improvements are not due to length optimizations alone.
tomatic evaluation metrics that generally rank the
Pref. system higher than Hier. and Syntax. As a fur-
ther confirmation, our feature selection based MERT
chooses to retain ?m+1 in the model. While the
IWSLT results are promising, we perform a more
complete evaluation on the NIST translation task.
NIST task: This task generates much larger rule
preference vectors than the IWSLT task simply due
to the size of the training corpora. We build sys-
tems with both ?R = 100, 10 varying ?P . Vary-
ing ?P isolates the relative impact of propagating
alternative nonterminal labels within the preference
grammar model. ?L = 5 for all NIST systems. Pa-
rameters ? are trained via MERT on the ?R = 100,
?L = 5, ?P = 2 system. BLEU scores for each
preference grammar and baseline system are shown
in Table 3, along with translation times on the test
corpus. We also report length penalties to show that
improvements are not simply due to better tuning of
output length.
The preference grammar systems outperform the
Hier. baseline by 0.5 points on development data,
and upto 0.8 points on unseen test data. While sys-
tems with ?R = 100 take significantly longer to
translate the test data than Hier., setting ?R = 10
takes approximately as long as the Syntax based sys-
tem but produces better slightly better results (0.3
points).
The improvements in translation quality with the
preference grammar are encouraging, but how much
of this improvement can simply be attributed to
MERT finding a better local optimum for parame-
ters ?? To answer this question, we use parameters
?? optimized by MERT for the preference grammar
system to run a purely hierarchical system, denoted
Hier.(??), which ignores the value of ?m+1 during
decoding. While almost half of the improvement
comes from better parameters learned via MERT for
the preference grammar systems, 0.5 points can be
still be attributed purely to the feature psyn. In addi-
tion, MERT does not set parameter ?m+1 to 0, cor-
roborating the value of the psyn feature again. Note
that Hier.(??) achieves better scores than the Hier.
system which was trained via MERT without psyn.
This highlights the local nature of MERT parameter
search, but also points to the possibility that train-
ing with the feature psyn produced a more diverse
derivation space, resulting in better parameters ?.
We see a very small improvement (0.1 point) by al-
lowing the runtime propagation of more than 1 non-
terminal label in the left-hand side posterior distribu-
tion, but the improvement doesn?t extend to ?P = 5.
Improved integration of the feature psyn(d) into de-
coding might help to widen this gap.
242
Test
Dev. Test time
System BLEU (lpen) BLEU (lpen) (h:mm)
Baseline Systems
Hier. 34.1 (0.99) 31.8 (0.95) 0:12
Syntax 34.7 (0.99) 32.3 (0.95) 0:45
Hier.(??) - 32.1 (0.95) 0:12
Preference Grammar: ?R = 100
?P = 1 - 32.5 (0.96) 3:00
?P = 2 34.6 (0.99) 32.6 (0.95) 3:00
?P = 5 - 32.5 (0.95) 3:20
Preference Grammar: ?R = 10
?P = 1 - 32.5 (0.95) 1:03
?P = 2 - 32.6 (0.95) 1:10
?P = 5 - 32.5 (0.95) 1:10
Table 3: Translation quality and test set translation time
(using 50 machines with 2 tasks per machine) measured
by the BLEU metric for the NIST task. NIST 2006 is
used as the development (Dev.) corpus and NIST 2007 is
used as the unseen evaluation corpus (Test). Dev. scores
are reported for systems that have been separately MERT
trained, Pref. systems share parameters from a single
MERT training. Systems are described in the text.
7 Related Work
There have been significant efforts in the both the
monolingual parsing and machine translation liter-
ature to address the impact of the MAP approxi-
mation and the choice of labels in their respective
models; we survey the work most closely related to
our approach. May and Knight (2006) extract n-
best lists containing unique translations rather than
unique derivations, while Kumar and Byrne (2004)
use the Minimum Bayes Risk decision rule to se-
lect the lowest risk (highest BLEU score) translation
rather than derivation from an n-best list. Tromble
et al (2008) extend this work to lattice structures.
All of these approaches only marginalize over alter-
native candidate derivations generated by a MAP-
driven decoding process. More recently, work by
Blunsom et al (2007) propose a purely discrimina-
tive model whose decoding step approximates the
selection of the most likely translation via beam
search. Matsusaki et al (2005) and Petrov et al
(2006) propose automatically learning annotations
that add information to categories to improve mono-
lingual parsing quality. Since the parsing task re-
quires selecting the most non-annotated tree, the an-
notations add an additional level of structure that
must be marginalized during search. They demon-
strate improvements in parse quality only when a
variational approximation is used to select the most
likely unannotated tree rather than simply stripping
annotations from the MAP annotated tree. In our
work, we focused on approximating the selection of
the most likely unlabeled derivation during search,
rather than as a post-processing operation; the meth-
ods described above might improve this approxima-
tion, at some computational expense.
8 Conclusions and Future Work
We have proposed a novel grammar formalism that
replaces hard syntactic constraints with ?soft? pref-
erences. These preferences are used to compute a
machine translation feature (psyn(d)) that scores un-
labeled derivations, taking into account traditional
syntactic constraints. Representing syntactic con-
straints as a feature allows MERT to train the cor-
responding weight for this feature relative to others
in the model, allowing systems to learn the relative
importance of labels for particular resource and lan-
guage scenarios as well as for alternative approaches
to labeling PSCFG rules.
This approach takes a step toward addressing
the fragmentation problems of decoding based on
maximum-weighted derivations, by summing the
contributions of compatible label configurations
rather than forcing them to compete. We have sug-
gested an efficient technique to approximate psyn(d)
that takes advantage of a natural factoring of deriva-
tion scores. Our approach results in improvements
in translation quality on small and medium resource
translation tasks. In future work we plan to focus on
methods to improve on the integration of the psyn(d)
feature during decoding and techniques that allow us
consider more of the search space through less prun-
ing.
Acknowledgements
We appreciate helpful comments from three anony-
mous reviewers. Venugopal and Zollmann were sup-
ported by a Google Research Award. Smith was sup-
ported by NSF grant IIS-0836431.
243
References
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2007.
A discriminative latent variable model for statistical
machine translation. In Proceedings of the Annual
Meeting of the Association for Computational Linguis-
tics (ACL).
Francisco Casacuberta and Colin de la Higuera. 2000.
Computational complexity of problems on probabilis-
tic grammars and transducers. In Proc. of the 5th Inter-
national Colloquium on Grammatical Inference: Al-
gorithms and Applications.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
the Annual Meeting of the Association for Compua-
tional Linguistics (ACL).
David Chiang. 2007. Hierarchical phrase based transla-
tion. Computational Linguistics.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inferences and training of
context-rich syntax translation models. In Proceed-
ings of the Annual Meeting of the Association for Com-
puational Linguistics (ACL), Sydney, Australia.
Liang Huang and David Chiang. 2007. Forest rescoring:
Faster decoding with integrated language models. In
Proceedings of the Annual Meeting of the Association
for Compuational Linguistics (ACL).
Kevin Knight. 1999. Decoding complexity in word-
replacement translation models. Computational Lin-
guistics, Squibs and Discussion.
Shankar Kumar and William Byrne. 2004. Min-
imum Bayes-risk decoding for statistical machine
translation. In Proceedings of the Human Lan-
guage Technology and North American Association for
Computational Linguistics Conference (HLT/NAACL),
Boston,MA, May 27-June 1.
Takuya Matsusaki, Yusuke Miyao, and Junichi Tsujii.
2005. Probabilistic CFG with latent annotations. In
Proceedings of the Annual Meeting of the Association
for Computational Linguistics (ACL).
Jonathan May and Kevin Knight. 2006. A better N-best
list: Practical determinization of weighted finite tree
automata. In Proceedings of the Human Language
Technology Conference of the North American Chap-
ter of the Association for Computational Linguistics
Conference (HLT/NAACL).
Franz J. Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
Annual Meeting of the Association for Compuational
Linguistics (ACL).
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proceedings of the
Annual Meeting of the Association for Compuational
Linguistics (ACL).
Michael Paul. 2006. Overview of the IWSLT 2006 eval-
uation campaign. In Proceedings of the International
Workshop on Spoken Language Translation (IWSLT).
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings of the Annual
Meeting of the Association for Compuational Linguis-
tics (ACL).
Khalil Sima?an. 2002. Computational complexity of
probabilistic disambiguation. Grammars, 5(2):125?
151.
Roy Tromble, Shankar Kumar, Franz Och, and Wolfgang
Macherey. 2008. Lattice minimum Bayes-risk decod-
ing for statistical machine translation. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing (EMNLP).
Ashish Venugopal, Andreas Zollmann, and Stephan Vo-
gel. 2007. An efficient two-pass approach to
Synchronous-CFG driven statistical MT. In Proceed-
ings of the Human Language Technology Conference
of the North American Chapter of the Association for
Computational Linguistics Conference (HLT/NAACL).
Andreas Zollmann and Ashish Venugopal. 2006. Syntax
augmented machine translation via chart parsing. In
Proceedings of the Workshop on Statistical Machine
Translation, HLT/NAACL, New York, June.
Andreas Zollmann, Ashish Venugopal, Franz J. Och, and
Jay Ponte. 2008. A systematic comparison of phrase-
based, hierarchical and syntax-augmented statistical
MT. In Proceedings of the Conference on Computa-
tional Linguistics (COLING).
244
Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 208?215,
Ann Arbor, June 2005. c?Association for Computational Linguistics, 2005
Training and Evaluating Error Minimization Rules for Statistical Machine
Translation
Ashish Venugopal
School of Computer Science
Carnegie Mellon University
arv@andrew.cmu.edu
Andreas Zollmann
School of Computer Science
Carnegie Mellon University
zollmann@cs.cmu.edu
Alex Waibel
School of Computer Science
Carnegie Mellon University
waibel@cs.cmu.edu
Abstract
Decision rules that explicitly account for
non-probabilistic evaluation metrics in
machine translation typically require spe-
cial training, often to estimate parame-
ters in exponential models that govern the
search space and the selection of candi-
date translations. While the traditional
Maximum A Posteriori (MAP) decision
rule can be optimized as a piecewise lin-
ear function in a greedy search of the pa-
rameter space, the Minimum Bayes Risk
(MBR) decision rule is not well suited to
this technique, a condition that makes past
results difficult to compare. We present a
novel training approach for non-tractable
decision rules, allowing us to compare and
evaluate these and other decision rules on
a large scale translation task, taking ad-
vantage of the high dimensional parame-
ter space available to the phrase based
Pharaoh decoder. This comparison is
timely, and important, as decoders evolve
to represent more complex search space
decisions and are evaluated against in-
novative evaluation metrics of translation
quality.
1 Introduction
State of the art statistical machine translation takes
advantage of exponential models to incorporate a
large set of potentially overlapping features to se-
lect translations from a set of potential candidates.
As discussed in (Och, 2003), the direct translation
model represents the probability of target sentence
?English? e = e1 . . . eI being the translation for a
source sentence ?French? f = f1 . . . fJ through an
exponential, or log-linear model
p?(e|f) =
exp(
?m
k=1 ?k ? hk(e, f))?
e??E exp(
?m
k=1 ?k ? hk(e
?, f))
(1)
where e is a single candidate translation for f
from the set of all English translations E, ? is the
parameter vector for the model, and each hk is a
feature function of e and f . In practice, we restrict
E to the set Gen(f) which is a set of highly likely
translations discovered by a decoder (Vogel et al,
2003). Selecting a translation from this model under
the Maximum A Posteriori (MAP) criteria yields
transl?(f) = argmax
e
p?(e|f) . (2)
This decision rule is optimal under the zero-
one loss function, minimizing the Sentence Error
Rate (Mangu et al, 2000). Using the log-linear
form to model p?(e|f) gives us the flexibility to
introduce overlapping features that can represent
global context while decoding (searching the space
of candidate translations) and rescoring (ranking a
set of candidate translations before performing the
argmax operation), albeit at the cost of the tradi-
tional source-channel generative model of transla-
tion proposed in (Brown et al, 1993).
A significant impact of this paradigm shift, how-
ever, has been the movement to leverage the flex-
ibility of the exponential model to maximize per-
formance with respect to automatic evaluation met-
208
rics. Each evaluation metric considers different as-
pects of translation quality, both at the sentence and
corpus level, often achieving high correlation to hu-
man evaluation (Doddington, 2002). It is clear that
the decision rule stated in (1) does not reflect the
choice of evaluation metric, and substantial work
has been done to correct this mismatch in crite-
ria. Approaches include integrating the metric into
the decision rule, and learning ? to optimize the
performance of the decision rule. In this paper
we will compare and evaluate several aspects of
these techniques, focusing on Minimum Error Rate
(MER) training (Och, 2003) and Minimum Bayes
Risk (MBR) decision rules, within a novel training
environment that isolates the impact of each compo-
nent of these methods.
2 Addressing Evaluation Metrics
We now describe competing strategies to address the
problem of modeling the evaluation metric within
the decoding and rescoring process, and introduce
our contribution towards training non-tractable error
surfaces. The methods discussed below make use
of Gen(f), the approximation to the complete can-
didate translation space E, referred to as an n-best
list. Details regarding n-best list generation from
decoder output can be found in (Ueffing et al, 2002).
2.1 Minimum Error Rate Training
The predominant approach to reconciling the mis-
match between the MAP decision rule and the eval-
uation metric has been to train the parameters ? of
the exponential model to correlate the MAP choice
with the maximum score as indicated by the evalu-
ation metric on a development set with known ref-
erences (Och, 2003). We differentiate between the
decision rule
transl?(f) = argmax
e?Gen(f)
p?(e|f) (3a)
and the training criterion
?? = argmin
?
Loss(transl?(~f),~r) (3b)
where the Loss function returns an evaluation re-
sult quantifying the difference between the English
candidate translation transl?(f) and its correspond-
ing reference r for a source sentence f . We indicate
that this loss function is operating on a sequence of
sentences with the vector notation. To avoid overfit-
ting, and since MT researchers are generally blessed
with an abundance of data, these sentences are from
a separate development set.
The optimization problem (3b) is hard since the
argmax of (3a) causes the error surface to change
in steps in Rm, precluding the use of gradient based
optimization methods. Smoothed error counts can
be used to approximate the argmax operator, but the
resulting function still contains local minima. Grid-
based line search approaches like Powell?s algorithm
could be applied but we can expect difficultly when
choosing the appropriate grid size and starting pa-
rameters. In the following, we summarize the opti-
mization algorithm for the unsmoothed error counts
presented in (Och, 2003) and the implementation de-
tailed in (Venugopal and Vogel, 2005).
? Regard Loss(transl?(~f),~r) as defined in (3b)
as a function of the parameter vector ? to
optimize and take the argmax to compute
transl?(~f) over the translations Gen(f) accord-
ing to the n-best list generated with an initial
estimate ?0.
? The error surface defined by Loss (as a func-
tion of ?) is piecewise linear with respect to a
single model parameter ?k, hence we can deter-
mine exactly where it would be useful (values
that change the result of the argmax) to evalu-
ate ?k for a given sentence using a simple line
intersection method.
? Merge the list of useful evaluation points
for ?k and evaluate the corpus level
Loss(transl?(~f),~r) at each one.
? Select the model parameter that represents the
lowest Loss as k varies, set ?k and consider the
parameter ?j for another dimension j.
This training algorithm, referred to as minimum er-
ror rate (MER) training, is a greedy search in each
dimension of ?, made efficient by realizing that
within each dimension, we can compute the points
at which changes in ? actually have an impact on
Loss. The appropriate considerations for termina-
tion and initial starting points relevant to any greedy
search procedure must be accounted for. From the
209
nature of the training procedure and the MAP de-
cision rule, we can expect that the parameters se-
lected by MER training will strongly favor a few
translations in the n-best list, namely for each source
sentence the one resulting in the best score, moving
most of the probability mass towards the translation
that it believes should be selected. This is due to the
decision rule, rather than the training procedure, as
we will see when we consider alternative decision
rules.
2.2 The Minimum Bayes Risk Decision Rule
The Minimum Bayes Risk Decision Rule as pro-
posed by (Mangu et al, 2000) for the Word Error
Rate Metric in speech recognition, and (Kumar and
Byrne, 2004) when applied to translation, changes
the decision rule in (2) to select the translation that
has the lowest expected loss E[Loss(e, r)], which
can be estimated by considering a weighted Loss
between e and the elements of the n-best list, the
approximation to E, as described in (Mangu et al,
2000). The resulting decision rule is:
transl?(f) = argmin
e?Gen(f)
?
e??Gen(f)
Loss(e, e?)p?(e
?|f) .
(4)
(Kumar and Byrne, 2004) explicitly consider select-
ing both e and a, an alignment between the Eng-
lish and French sentences. Under a phrase based
translation model (Koehn et al, 2003; Marcu and
Wong, 2002), this distinction is important and will
be discussed in more detail. The representation of
the evaluation metric or the Loss function is in the
decision rule, rather than in the training criterion for
the exponential model. This criterion is hard to op-
timize for the same reason as the criterion in (3b):
the objective function is not continuous in ?. To
make things worse, it is more expensive to evalu-
ate the function at a given ?, since the decision rule
involves a sum over all translations.
2.3 MBR and the Exponential Model
Previous work has reported the success of the MBR
decision rule with fixed parameters relating indepen-
dent underlying models, typically including only the
language model and the translation model as fea-
tures in the exponential model.
We extend the MBR approach by developing a
training method to optimize the parameters ? in the
exponential model as an explicit form for the condi-
tional distribution in equation (1). The training task
under the MBR criterion is
?? = argmin
?
Loss(transl?(~f),~r) (5a)
where
transl?(f) = argmin
e?Gen(f)
?
e??Gen(f)
Loss(e, e?)p?(e
?|f) .
(5b)
We begin with several observations about this opti-
mization criterion.
? The MAP optimal ?? are not the optimal para-
meters for this training criterion.
? We can expect the error surface of the MBR
training criterion to contain larger sections of
similar altitude, since the decision rule empha-
sizes consensus.
? The piecewise linearity observation made in
(Papineni et al, 2002) is no longer applicable
since we cannot move the log operation into the
expected value.
3 Score Sampling
Motivated by the challenges that the MBR training
criterion presents, we present a training method that
is based on the assumption that the error surface is
locally non-smooth but consists of local regions of
similar Loss values. We would like to focus the
search within regions of the parameter space that re-
sult in low Loss values, simulating the effect that
the MER training process achieves when it deter-
mines the merged error boundaries across a set of
sentences.
Let Score(?) be some function of
Loss(transl?(~f),~r) that is greater or equal
zero, decreases monotonically with Loss, and for
which
?
(Score(?) ? min?? Score(??))d? is finite;
e.g., 1 ? Loss(transl?(~f),~r) for the word-error
rate (WER) loss and a bounded parameter space.
While sampling parameter vectors ? and estimating
Loss in these points, we will constantly refine
our estimate of the error surface and thereby of
the Score function. The main idea in our score
210
sampling algorithm is to make use of this Score
estimate by constructing a probability distribution
over the parameter space that depends on the Score
estimate in the current iteration step i and sample
the parameter vector ?i+1 for the next iteration from
that distribution. More precisely, let S?c
(i)
be the
estimate of Score in iteration i (we will explain how
to obtain this estimate below). Then the probability
distribution from which we sample the parameter
vector to test in the next iteration is given by:
p(?) =
S?c
(i)
(?) ? min?? S?c
(i)
(??)
?
(S?c
(i)
(?) ? min?? S?c
(i)
(??)) d?
. (6)
This distribution produces a sequence ?1, . . . , ?n of
parameter vectors that are more concentrated in ar-
eas that result in a high Score. We can select the
value from this sequence that generates the highest
Score, just as in the MER training process.
The exact method of obtaining the Score estimate
S?c is crucial: If we are not careful enough and guess
too low values of S?c(?) for parameter regions that
are still unknown to us, the resulting sampling dis-
tribution p might be zero in those regions and thus
potentially optimal parameters might never be sam-
pled. Rather than aiming for a consistent estimator
of Score (i.e., an estimator that converges to Score
when the sample size goes to infinity), we design S?c
with regard to yielding a suitable sampling distribu-
tion p.
Assume that the parameter space is bounded such
that mink ? ?k ? maxk for each dimension k,
We then define a set of pivots P , forming a grid of
points in Rm that are evenly spaced between mink
and maxk for each dimension k. Each pivot repre-
sents a region of the parameter space where we ex-
pect generally consistent values of Score. We do not
restrict the values of ?m to be at these pivot points
as a grid search would do, rather we treat the pivots
as landmarks within the search space.
We approximate the distribution p(?) with the
discrete distribution p(? ? P), leaving the problem
of estimating |P| parameters. Initially, we set p to
be uniform, i.e., p(0)(?) = 1/|P|. For subsequent
iterations, we now need an estimate of Score(?) for
each pivot ? ? P in the discrete version of equation
(6) to obtain the new sampling distribution p. Each
iteration i proceeds as follows.
? Sample ??i from the discrete distribution
p(i?1)(? ? P) obtained by the previous it-
eration.
? Sample the new parameter vector ?i by choos-
ing for each k ? {1, . . . ,m}, ?ik := ??ik + ?k,
where ?k is sampled uniformly from the inter-
val (?dk/2, dk/2) and dk is the distance be-
tween neighboring pivot points along dimen-
sion k. Thus, ?i is sampled from a region
around the sampled pivot.
? Evaluate Score(?i) and distribute this score to
obtain new estimates S?c
(i)
(?) for all pivots ? ?
P as described below.
? Use the updated estimates S?c
(i)
to generate the
sampling distribution p(i) for the next iteration
according to
p(i)(?) =
S?c
(i)
(?) ? min?? S?c
(i)
(??)
?
??P(S?c
(i)
(?) ? min?? S?c
(i)
(??))
.
The score Score(?i) of the currently evaluated pa-
rameter vector does not only influence the score es-
timate at the pivot point of the respective region, but
the estimates at all pivot points. The closest piv-
ots are influenced most strongly. More precisely, for
each pivot ? ? P , S?c
(i)
(?) is a weighted average
of Score(?1), . . . , Score(?i), where the weights
w(i)(?) are chosen according to
w(i)(?) = infl(i)(?) ? corr(i)(?) with
infl(i)(?) = mvnpdf(?, ?i,?) and
corr(i)(?) = 1/p(i?1)(?) .
Here, mvnpdf(x, ?,?) denotes the m-dimensional
multivariate-normal probability density function
with mean ? and covariance matrix ?, evaluated
at point x. We chose the covariance matrix ? =
diag(d21, . . . , d
2
m), where again dk is the distance be-
tween neighboring grid points along dimension k.
The term infl(i)(?) quantifies the influence of the
evaluated point ?i on the pivot ?, while corr(i)(?)
is a correction term for the bias introduced by hav-
ing sampled ?i from p(i?1).
211
Smoothing uncertain regions In the beginning of
the optimization process, there will be pivot regions
that have not yet been sampled from and for which
not even close-by regions have been sampled yet.
This will be reflected in the low sum of influence
terms infl(1)(?) + ? ? ? + infl(i)(?) of the respective
pivot points ?. It is therefore advisable to discount
some probability mass from p(i) and distribute it
over pivots with low influence sums (reflecting low
confidence in the respective score estimates) accord-
ing to some smoothing procedure.
4 N-Best lists in Phrase Based Decoding
The methods described above make extensive use of
n-best lists to approximate the search space of can-
didate translations. In phrase based decoding we of-
ten interpret the MAP decision rule to select the top
scoring path in the translation lattice. Selecting a
particular path means in fact selecting the pair ?e, s?,
where s is a segmentation of the the source sentence
f into phrases and alignments onto their translations
in e. Kumar and Byrne (2004) represent this deci-
sion explicitly, since the Loss metrics considered in
their work evaluate alignment information as well as
lexical (word) level output. When considering lexi-
cal scores as we do here, the decision rule minimiz-
ing 0/1 loss actually needs to take the sum over all
potential segmentations that can generate the same
word sequence. In practice, we only consider the
high probability segmentation decisions, namely the
ones that were found in the n-best list. This gives
the 0/1 loss criterion shown below.
transl?(f) = argmax
e
?
s
p?(e, s|f) (7)
The 0/1 loss criterion favors translations that are
supported by several segmentation decisions. In the
context of phrase-based translations, this is a useful
criterion, since a given lexical target word sequence
can be correctly segmented in several different ways,
all of which would be scored equally by an evalua-
tion metric that only considers the word sequence.
5 Experimental Framework
Our goal is to evaluate the impact of the three de-
cision rules discussed above on a large scale trans-
lation task that takes advantage of multidimensional
features in the exponential model. In this section
we describe the experimental framework used in this
evaluation.
5.1 Data Sets and Resources
We perform our analysis on the data provided by the
2005 ACL Workshop in Exploiting Parallel Texts for
Statistical Machine Translation, working with the
French-English Europarl corpus. This corpus con-
sists of 688031 sentence pairs, with approximately
156 million words on the French side, and 138 mil-
lion words on the English side. We use the data as
provided by the workshop and run lower casing as
our only preprocessing step. We use the 15.5 mil-
lion entry phrase translation table as provided for the
shared workshop task for the French-English data
set. Each translation pair has a set of 5 associated
phrase translation scores that represent the maxi-
mum likelihood estimate of the phrase as well as in-
ternal alignment probabilities. We also use the Eng-
lish language model as provided for the shared task.
Since each of these decision rules has its respective
training process, we split the workshop test set of
2000 sentences into a development and test set using
random splitting. We tried two decoders for trans-
lating these sets. The first system is the Pharaoh de-
coder provided by (Koehn et al, 2003) for the shared
data task. The Pharaoh decoder has support for mul-
tiple translation and language model scores as well
as simple phrase distortion and word length models.
The pruning and distortion limit parameters remain
the same as in the provided initialization scripts,
i.e., DistortionLimit = 4, BeamThreshold =
0.1, Stack = 100. For further information on
these parameter settings, confer (Koehn et al, 2003).
Pharaoh is interesting for our optimization task be-
cause its eight different models lead to a search
space with seven free parameters. Here, a princi-
pled optimization procedure is crucial. The second
decoder we tried is the CMU Statistical Translation
System (Vogel et al, 2003) augmented with the four
translation models provided by the Pharaoh system,
in the following called CMU-Pharaoh. This system
also leads to a search space with seven free parame-
ters.
212
5.2 N-Best lists
As mentioned earlier, the model parameters ? play
a large role in the search space explored by a prun-
ing beam search decoder. These parameters affect
the histogram and beam pruning as well as the fu-
ture cost estimation used in the Pharaoh and CMU
decoders. The initial parameter file for Pharaoh pro-
vided by the workshop provided a very poor esti-
mate of ?, resulting in an n-best list of limited po-
tential. To account for this condition, we ran Min-
imum Error Rate training on the development data
to determine scaling factors that can generate a n-
best list with high quality translations. We realize
that this step biases the n-best list towards the MAP
criteria, since its parameters will likely cause more
aggressive pruning. However, since we have cho-
sen a large N=1000, and retrain the MBR, MAP, and
0/1 loss parameters separately, we do not feel that
the bias has a strong impact on the evaluation.
5.3 Evaluation Metric
This paper focuses on the BLEU metric as presented
in (Papineni et al, 2002). The BLEU metric is de-
fined on a corpus level as follows.
Score(~e,~r) = BP (~e,~r) ? exp(
1
N
N?
1
(log pn))
where pn represent the precision of n-grams sug-
gested in ~e and BP is a brevity penalty measur-
ing the relative shortness of ~e over the whole cor-
pus. To use the BLEU metric in the candidate pair-
wise loss calculation in (4), we need to make a de-
cision regarding cases where higher order n-grams
matches are not found between two candidates. Ku-
mar and Byrne (2004) suggest that if any n-grams
are not matched then the pairwise BLEU score is set
to zero. As an alternative we first estimate corpus-
wide n-gram counts on the development set. When
the pairwise counts are collected between sentences
pairs, they are added onto the baseline corpus counts
to and scored by BLEU. This scoring simulates the
process of scoring additional sentences after seeing
a whole corpus.
5.4 Training Environment
It is important to separate the impact of the decision
rule from the success of the training procedure. To
appropriately compare the MAP, 0/1 loss and MBR
decisions rules, they must all be trained with the
same training method, here we use the Score Sam-
pling training method described above. We also re-
port MAP scores using the MER training described
above to determine the impact of the training algo-
rithm for MAP. Note that the MER training approach
cannot be performed on the MBR decision rule, as
explained in Section 2.3. MER training is initialized
at random values of ? and run (successive greedy
search over the parameters) until there is no change
in the error for three complete cycles through the pa-
rameter set. This process is repeated with new start-
ing parameters as well as permutations of the para-
meter search order to ensure that there is no bias in
the search towards a particular parameter. To im-
prove efficiency, pairwise scores are cached across
requests for the score at different values of ?, and
for MBR only the E[Loss(e, r)] for the top twenty
hypotheses as ranked by the model are computed.
6 Results
The results in Table 1 compare the BLEU score
achieved by each training method on the develop-
ment and test data for both Pharaoh and CMU-
Pharaoh. Score-sampling training was run for 150
iterations to find ? for each decision rule. The MAP-
MER training was performed to evaluate the effect
of the greedy search method on the generalization
of the development set results. Each row represents
an alternative training method described in this pa-
per, while the test set columns indicate the criteria
used to select the final translation output ~e. The
bold face scores are the scores for matching train-
ing and testing methods. The underlined score is
the highest test set score, achieved by MBR decod-
ing using the CMU-Pharaoh system trained for the
MBR decision rule with the score-sampling algo-
rithm. When comparing MER training for MAP-
decoding with score-sampling training for MAP-
decoding, score-sampling surprisingly outperforms
MER training for both Pharaoh and CMU-Pharaoh,
although MER training is specifically tailored to
the MAP metric. Note, however, that our score-
sampling algorithm has a considerably longer run-
ning time (several hours) than the MER algorithm
(several minutes). Interestingly, within MER train-
213
training method Dev. set sc. test set sc. MAP test set sc. 0/1 loss test set sc. MBR
MAP MER (Pharaoh) 29.08 29.30 29.42 29.36
MAP score-sampl. (Pharaoh) 29.08 29.41 29.24 29.30
0/1 loss sc.-s. (Pharaoh) 29.08 29.16 29.28 29.30
MBR sc.-s. (Pharaoh) 29.00 29.11 29.08 29.17
MAP MER (CMU-Pharaoh) 28.80 29.02 29.41 29.60
MAP sc.-s. (CMU-Ph.) 29.10 29.85 29.75 29.55
0/1 loss sc.-s. (CMU-Ph.) 28.36 29.97 29.91 29.72
MBR sc.-s. (CMU-Ph.) 28.36 30.18 30.16 30.28
Table 1. Comparing BLEU scores generated by alternative training methods and decision rules
ing for Pharaoh, the 0/1 loss metric is the top per-
former; we believe the reason for this disparity be-
tween training and test methods is the impact of
phrasal consistency as a valuable measure within the
n-best list.
The relative performance of MBR score-sampling
w.r.t. MAP and 0/1-loss score sampling is quite dif-
ferent between Pharaoh and CMU-Pharaoh: While
MBR score-sampling performs worse than MAP
and 0/1-loss score sampling for Pharaoh, it yields the
best test scores across the board for CMU-Pharaoh.
A possible reason is that the n-best lists generated by
Pharaoh have a large percentage of lexically iden-
tical translations, differing only in their segmenta-
tions. As a result, the 1000-best lists generated by
Pharaoh contain only a small percentage of unique
translations, a condition that reduces the potential
of the Minimum Bayes Risk methods. The CMU
decoder, contrariwise, prunes away alternatives be-
low a certain score-threshold during decoding and
does not recover them when generating the n-best
list. The n-best lists of this system are therefore typi-
cally more diverse and in particular contain far more
unique translations.
7 Conclusions and Further Work
This work describes a general algorithm for the ef-
ficient optimization of error counts for an arbitrary
Loss function, allowing us to compare and evalu-
ate the impact of alternative decision rules for sta-
tistical machine translation. Our results suggest
the value and sensitivity of the translation process
to the Loss function at the decoding and reorder-
ing stages of the process. As phrase-based trans-
lation and reordering models begin to dominate
the state of the art in machine translation, it will
become increasingly important to understand the
nature and consistency of n-best list training ap-
proaches. Our results are reported on a complete
package of translation tools and resources, allow-
ing the reader to easily recreate and build upon our
framework. Further research might lie in finding
efficient representations of Bayes Risk loss func-
tions within the decoding process (rather than just
using MBR to rescore n-best lists), as well as
analyses on different language pairs from the avail-
able Europarl data. We have shown score sam-
pling to be an effective training method to con-
duct these experiments and we hope to establish its
use in the changing landscape of automatic trans-
lation evaluation. The source code is available at:
www.cs.cmu.edu/?zollmann/scoresampling/
8 Acknowledgments
We thank Stephan Vogel, Ying Zhang, and the
anonymous reviewers for their valuable comments
and suggestions.
References
Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della
Pietra, and Robert L. Mercer. 1993. The mathemat-
ics of statistical machine translation: parameter esti-
mation. Computational Linguistics, 19(2):263?311.
George Doddington. 2002. Automatic evaluation of ma-
chine translation quality using n-gram co-occurrence
statistics. In In Proc. ARPA Workshop on Human Lan-
guage Technology.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of the Human Language Technology and North
214
American Association for Computational Linguistics
Conference (HLT/NAACL), Edomonton, Canada, May
27-June 1.
Shankar Kumar and William Byrne. 2004. Minimum
bayes-risk decoding for statistical machine translation.
In Proceedings of the Human Language Technology
and North American Association for Computational
Linguistics Conference (HLT/NAACL), Boston,MA,
May 27-June 1.
Lidia Mangu, Eric Brill, and Andreas Stolcke. 2000.
Finding consensus in speech recognition: word error
minimization and other applications of confusion net-
works. CoRR, cs.CL/0010012.
Daniel Marcu and William Wong. 2002. A phrase-based,
joint probability model for statistical machine transla-
tion. In Proc. of the Conference on Empirical Meth-
ods in Natural Language Processing, Philadephia, PA,
July 6-7.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proc. of the Associ-
ation for Computational Linguistics, Sapporo, Japan,
July 6-7.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
Association of Computational Linguistics, pages 311?
318.
Nicola Ueffing, Franz Josef Och, and Hermann Ney.
2002. Generation of word graphs in statistical ma-
chine translation. In Proc. of the Conference on
Empirical Methods in Natural Language Processing,
Philadephia, PA, July 6-7.
Ashish Venugopal and Stephan Vogel. 2005. Consider-
ations in mce and mmi training for statistical machine
translation. In Proceedings of the Tenth Conference
of the European Association for Machine Translation
(EAMT-05), Budapest, Hungary, May. The European
Association for Machine Translation.
Stephan Vogel, Ying Zhang, Fei Huang, Alicia Trib-
ble, Ashish Venogupal, Bing Zhao, and Alex Waibel.
2003. The CMU statistical translation system. In Pro-
ceedings of MT Summit IX, New Orleans, LA, Septem-
ber.
215
Proceedings of the Workshop on Statistical Machine Translation, pages 138?141,
New York City, June 2006. c?2006 Association for Computational Linguistics
Syntax Augmented Machine Translation via Chart Parsing
Andreas Zollmann and Ashish Venugopal
School of Computer Science
Carnegie Mellon University
{zollmann,ashishv}@cs.cmu.edu
Abstract
We present translation results on the
shared task ?Exploiting Parallel Texts for
Statistical Machine Translation? gener-
ated by a chart parsing decoder operating
on phrase tables augmented and general-
ized with target language syntactic cate-
gories. We use a target language parser
to generate parse trees for each sentence
on the target side of the bilingual train-
ing corpus, matching them with phrase
table lattices built for the corresponding
source sentence. Considering phrases that
correspond to syntactic categories in the
parse trees we develop techniques to aug-
ment (declare a syntactically motivated
category for a phrase pair) and general-
ize (form mixed terminal and nonterminal
phrases) the phrase table into a synchro-
nous bilingual grammar. We present re-
sults on the French-to-English task for this
workshop, representing significant im-
provements over the workshop?s baseline
system. Our translation system is avail-
able open-source under the GNU General
Public License.
1 Introduction
Recent work in machine translation has evolved
from the traditional word (Brown et al, 1993) and
phrase based (Koehn et al, 2003a) models to in-
clude hierarchical phrase models (Chiang, 2005) and
bilingual synchronous grammars (Melamed, 2004).
These advances are motivated by the desire to in-
tegrate richer knowledge sources within the transla-
tion process with the explicit goal of producing more
fluent translations in the target language. The hi-
erarchical translation operations introduced in these
methods call for extensions to the traditional beam
decoder (Koehn et al, 2003a). In this work we
introduce techniques to generate syntactically mo-
tivated generalized phrases and discuss issues in
chart parser based decoding in the statistical ma-
chine translation environment.
(Chiang, 2005) generates synchronous context-
free grammar (SynCFG) rules from an existing
phrase translation table. These rules can be viewed
as phrase pairs with mixed lexical and non-terminal
entries, where non-terminal entries (occurring as
pairs in the source and target side) represent place-
holders for inserting additional phrases pairs (which
again may contain nonterminals) at decoding time.
While (Chiang, 2005) uses only two nonterminal
symbols in his grammar, we introduce multiple syn-
tactic categories, taking advantage of a target lan-
guage parser for this information. While (Yamada
and Knight, 2002) represent syntactical information
in the decoding process through a series of transfor-
mation operations, we operate directly at the phrase
level. In addition to the benefits that come from
a more structured hierarchical rule set, we believe
that these restrictions serve as a syntax driven lan-
guage model that can guide the decoding process,
as n-gram context based language models do in tra-
ditional decoding. In the following sections, we
describe our phrase annotation and generalization
process followed by the design and pruning deci-
sions in our chart parser. We give results on the
French-English Europarl data and conclude with
prospects for future work.
138
2 Rule Generation
We start with phrase translations on the parallel
training data using the techniques and implementa-
tion described in (Koehn et al, 2003a). This phrase
table provides the purely lexical entries in the final
hierarchical rule set that will be used in decoding.
We then use Charniak?s parser (Charniak, 2000) to
generate the most likely parse tree for each Eng-
lish target sentence in the training corpus. Next,
we determine all phrase pairs in the phrase table
whose source and target side occur in each respec-
tive source and target sentence pair defining the
scope of the initial rules in our SynCFG.
Annotation If the target side of any of these ini-
tial rules correspond to a syntactic category C of the
target side parse tree, we label the phrase pair with
that syntactic category. This label corresponds to the
left-hand side of our synchronous grammar. Phrase
pairs that do not correspond to a span in the parse
tree are given a default category ?X?, and can still
play a role in the decoding process. In work done af-
ter submission to the 2006 data track, we assign such
phrases an extended category of the form C1 + C2,
C1/C2, or C2\C1, indicating that the phrase pair?s
target side spans two adjacent syntactic categories
(e.g., she went: NP+V), a partial syntactic cate-
gory C1 missing a C2 to the right (e.g., the great:
NP/NN), or a partial C1 missing a C2 to the left (e.g.,
great wall: DT\NP), respectively.
Generalization In order to mitigate the effects
of sparse data when working with phrase and n-
gram models we would like to generate generalized
phrases, which include non-terminal symbols that
can be filled with other phrases. Therefore, after
annotating the initial rules from the current train-
ing sentence pair, we adhere to (Chiang, 2005) to
recursively generalize each existing rule; however,
we abstract on a per-sentence basis. The grammar
extracted from this evaluation?s training data con-
tains 75 nonterminals in our standard system, and
4000 nonterminals in the extended-category system.
Figure 1 illustrates the annotation and generalization
process.
NP->@
DT se
ssion/
DT se
ssion
S -> re
prise d
e @N
P/resu
mption
 of @N
P
NP->la
 sessi
on/the
 sessi
on
X -> re
prise d
e/resu
mption
 of
N->se
ssion/
sessio
n
DT->la
/the
IN->de
/of
N->rep
rise/re
sumpt
ion
reprise
de
la
sessio
n
S -> [N
P (N r
esump
tion) ] 
 [PP (I
N of)] 
[NP [ (
DT the
) (N se
ssion)
 ]
Figure 1: Selected annotated and generalized (dotted arc)
rules for the first sentence of Europarl.
3 Scoring
We employ a log-linear model to assign costs to the
SynCFG. Given a source sentence f , the preferred
translation output is determined by computing the
lowest-cost derivation (combination of hierarchical
and glue rules) yielding f as its source side, where
the cost of a derivation R1 ? ? ? ? ?Rn with respective
feature vectors v1, . . . , vn ? Rm is given by
m?
i=1
?i
n?
j=1
(vj)i .
Here, ?1, . . . , ?m are the parameters of the log-
linear model, which we optimize on a held-out por-
tion of the training set (2005 development data) us-
ing minimum-error-rate training (Och, 2003). We
use the following features for our rules:
? source- and target-conditioned neg-log lexical
weights as described in (Koehn et al, 2003b)
? neg-log relative frequencies: left-hand-
side-conditioned, target-phrase-conditioned,
source-phrase-conditioned
? Counters: n.o. rule applications, n.o. target
words
? Flags: IsPurelyLexical (i.e., contains only ter-
minals), IsPurelyAbstract (i.e., contains only
nonterminals), IsXRule (i.e., non-syntactical
span), IsGlueRule
139
? Penalties: rareness penalty exp(1 ?
RuleFrequency); unbalancedness penalty
|MeanTargetSourceRatio ? ?n.o. source words??
?n.o. target words?|
4 Parsing
Our SynCFG rules are equivalent to a probabilistic
context-free grammar and decoding is therefore an
application of chart parsing. Instead of the common
method of converting the CFG grammar into Chom-
sky Normal Form and applying a CKY algorithm
to produce the most likely parse for a given source
sentence, we avoided the explosion of the rule set
caused by the introduction of new non-terminals in
the conversion process and implemented a variant
of the CKY+ algorithm as described in (J.Earley,
1970).
Each cell of the parsing process in (J.Earley,
1970) contains a set of hypergraph nodes (Huang
and Chiang, 2005). A hypergraph node is an equiv-
alence class of complete hypotheses (derivations)
with identical production results (left-hand sides of
the corresponding applied rules). Complete hy-
potheses point directly to nodes in their backwards
star, and the cost of the complete hypothesis is cal-
culated with respect to each back pointer node?s best
cost.
This structure affords efficient parsing with mini-
mal pruning (we use a single parameter to restrict the
number of hierarchical rules applied), but sacrifices
effective management of unique language model
states contributing to significant search errors dur-
ing parsing. At initial submission time we simply
re-scored a K-Best list extracted after first best pars-
ing using the lazy retrieval process in (Huang and
Chiang, 2005).
Post-submission After our workshop submission,
we modified the K-Best list extraction process to in-
tegrate an n-gram language model during K-Best ex-
traction. Instead of expanding each derivation (com-
plete hypothesis) in a breadth-first fashion, we ex-
pand only a single back pointer, and score this new
derivation with its translation model scores and a
language model cost estimate, consisting of an ac-
curate component, based on the words translated so
far, and an estimate based on each remaining (not
expanded) back pointer?s top scoring hypothesis.
To improve the diversity of the final K-Best list,
we keep track of partially expanded hypotheses that
have generated identical target words and refer to the
same hypergraph nodes. Any arising twin hypothe-
sis is immediately removed from the K-Best extrac-
tion beam during the expansion process.
5 Results
We present results that compare our system against
the baseline Pharaoh implementation (Koehn et al,
2003a) and MER training scripts provided for this
workshop. Our results represent work done before
the submission due date as well as after with the fol-
lowing generalized phrase systems.
? Baseline - Pharaoh with phrases extracted from
IBM Model 4 training with maximum phrase
length 7 and extraction method ?diag-growth-
final? (Koehn et al, 2003a)
? Lex - Phrase-decoder simulation: using only
the initial lexical rules from the phrase table,
all with LHS X , the Glue rule, and a binary
reordering rule with its own reordering-feature
? XCat - All nonterminals merged into a single
X nonterminal: simulation of the system Hiero
(Chiang, 2005).
? Syn - Syntactic extraction using the Penn Tree-
bank parse categories as nonterminals; rules
containing up to 4 nonterminal abstraction
sites.
? SynExt - Syntactic extraction using the
extended-category scheme, but with rules only
containing up to 2 nonterminal abstraction
sites.
We also explored the impact of longer initial
phrases by training another phrase table with phrases
up to length 12. Our results are presented in Ta-
ble 1. While our submission time system (Syn using
LM for rescoring only) shows no improvement over
the baseline, we clearly see the impact of integrating
the language model into the K-Best list extraction
process. Our final system shows at statistically sig-
nificant improvement over the baseline (0.78 BLEU
points is the 95 confidence level). We also see a
trend towards improving translation quality as we
140
System Dev: w/o LM Dev: LM-rescoring Test: LM-r. Dev: integrated LM Test: int. LM
Baseline - max. phr. length 7 ? ? ? 31.11 30.61
Lex - max. phrase length 7 27.94 29.39 29.95 28.96 29.12
XCat - max. phrase length 7 27.56 30.27 29.81 30.89 31.01
Syn - max. phrase length 7 29.20 30.95 30.58 31.52 31.31
SynExt - max. phrase length 7 ? ? ? 31.73 31.41
Baseline - max. phr. length 12 ? ? ? 31.16 30.90
Lex - max. phr. length 12 ? ? ? 29.30 29.51
XCat - max. phr. length 12 ? ? ? 30.79 30.59
SynExt - max. phr. length 12 ? ? ? 31.07 31.76
Table 1: Translation results (IBM BLEU) for each system on the Fr-En ?06 Shared Task ?Development Set? (used for MER
parameter tuning) and ?06 ?Development Test Set? (identical to last year?s Shared Task?s test set). The system submitted for
evaluation is highlighted in bold.
employ richer extraction techniques. The relatively
poor performance of Lex with LM in K-Best com-
pared to the baseline shows that we are still making
search errors during parsing despite tighter integra-
tion of the language model.
We also ran an experiment with CMU?s phrase-
based decoder (Vogel et al, 2003) using the length-
7 phrase table. While its development-set score was
only 31.01, the decoder achieved 31.42 on the test
set, placing it at the same level as our extended-
category system for that phrase table.
6 Conclusions
In this work we applied syntax based resources
(the target language parser) to annotate and gener-
alize phrase translation tables extracted via exist-
ing phrase extraction techniques. Our work reaf-
firms the feasibility of parsing approaches to ma-
chine translation in a large data setting, and il-
lustrates the impact of adding syntactic categories
to drive and constrain the structured search space.
While no improvements were available at submis-
sion time, our subsequent performance highlights
the importance of tight integration of n-gram lan-
guage modeling within the syntax driven parsing en-
vironment. Our translation system is available open-
source under the GNU General Public License at:
www.cs.cmu.edu/?zollmann/samt
References
Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della
Pietra, and Robert L. Mercer. 1993. The mathemat-
ics of statistical machine translation: parameter esti-
mation. Comput. Linguist., 19(2):263?311.
Eugene Charniak. 2000. A maximum entropy-inspired
parser. In Proceedings of the North American Associ-
ation for Computational Linguistics (HLT/NAACL).
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proc. of the As-
sociation for Computational Linguistics.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proceedings of the 9th International Work-
shop on Parsing Technologies.
J.Earley. 1970. An efficient context-free parsing algo-
rithm. Communications of the Assocation for Com-
puting Machinery, 13(2):94?102.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003a. Pharaoh: A beam search decoder for phrase-
base statistical machine translation models. In Pro-
ceedings of the Sixth Confernence of the Association
for Machine Translation in the Americas, Edomonton,
Canada, May 27-June 1.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003b. Statistical phrase-based translation. In
Proceedings of the Human Language Technology
and North American Association for Computational
Linguistics Conference (HLT/NAACL), Edomonton,
Canada, May 27-June 1.
I. Dan Melamed. 2004. Statistical machine translation
by parsing. In ACL, pages 653?660.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proc. of the Associ-
ation for Computational Linguistics, Sapporo, Japan,
July 6-7.
Stephan Vogel, Ying Zhang, Fei Huang, Alicia Trib-
ble, Ashish Venogupal, Bing Zhao, and Alex Waibel.
2003. The CMU statistical translation system. In Pro-
ceedings of MT Summit IX, New Orleans, LA, Septem-
ber.
Kenji Yamada and Kevin Knight. 2002. A decoder for
syntax-based statistical mt. In Proc. of the Association
for Computational Linguistics.
141
Proceedings of NAACL HLT 2007, pages 500?507,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
An Efficient Two-Pass Approach to Synchronous-CFG Driven Statistical MT
Ashish Venugopal and Andreas Zollmann and Stephan Vogel
School of Computer Science, Carnegie Mellon University, Pittsburgh
interACT Lab, University of Karlsruhe
{ashishv,zollmann,vogel+}@cs.cmu.edu
Abstract
We present an efficient, novel two-pass
approach to mitigate the computational
impact resulting from online intersection
of an n-gram language model (LM) and
a probabilistic synchronous context-free
grammar (PSCFG) for statistical machine
translation. In first pass CYK-style decod-
ing, we consider first-best chart item ap-
proximations, generating a hypergraph of
sentence spanning target language deriva-
tions. In the second stage, we instantiate
specific alternative derivations from this
hypergraph, using the LM to drive this
search process, recovering from search er-
rors made in the first pass. Model search
errors in our approach are comparable to
those made by the state-of-the-art ?Cube
Pruning? approach in (Chiang, 2007) un-
der comparable pruning conditions evalu-
ated on both hierarchical and syntax-based
grammars.
1 Introduction
Syntax-driven (Galley et al, 2006) and hierarchi-
cal translation models (Chiang, 2005) take advan-
tage of probabilistic synchronous context free gram-
mars (PSCFGs) to represent structured, lexical re-
ordering constraints during the decoding process.
These models extend the domain of locality (over
phrase-based models) during decoding, represent-
ing a significantly larger search space of possible
translation derivations. While PSCFG models are
often induced with the goal of producing grammati-
cally correct target translations as an implicit syntax-
structured language model, we acknowledge the
value of n-gram language models (LM) in phrase-
based approaches.
Integrating n-gram LMs into PSCFGs based de-
coding can be viewed as online intersection of the
PSCFG grammar with the finite state machine rep-
resented by the n-gram LM, dramatically increasing
the effective number of nonterminals in the decoding
grammar, rendering the decoding process essentially
infeasible without severe, beam-based lossy prun-
ing. The alternative, simply decoding without the
n-gram LM and rescoring N-best alternative transla-
tions, results in substantially more search errors, as
shown in (Zollmann and Venugopal, 2006).
Our two-pass approach involves fast, approximate
synchronous parsing in a first stage, followed by a
second, detailed exploration through the resulting
hypergraph of sentence spanning derivations, using
the n-gram LM to drive that search. This achieves
search errors comparable to a strong ?Cube Pruning?
(Chiang, 2007), single-pass baseline. The first pass
corresponds to a severe parameterization of Cube
Pruning considering only the first-best (LM inte-
grated) chart item in each cell while maintaining un-
explored alternatives for second-pass consideration.
Our second stage allows the integration of long dis-
tance and flexible history n-gram LMs to drive the
search process, rather than simply using such mod-
els for hypothesis rescoring.
We begin by discussing the PSCFG model for
statistical machine translation, motivating the need
500
for effective n-gram LM integration during decod-
ing. We then present our two-pass approach and
discuss Cube Pruning as a state-of-the-art baseline.
We present results in the form of search error analy-
sis and translation quality as measured by the BLEU
score (Papineni et al, 2002) on the IWSLT 06 text
translation task (Eck and Hori, 2005)1, comparing
Cube Pruning with our two-pass approach.
2 Synchronous Parsing for SMT
Probabilistic Synchronous Context Free Grammar
(PSCFG) approaches to statistical machine transla-
tion use a source terminal set (source vocabulary)
TS , a target terminal set (target vocabulary) TT and
a shared nonterminal set N and induce rules of the
form
X ? ??, ?,?, w?
where (i) X ? N is a nonterminal, (ii) ? ? (N ?
TS)? is a sequence of nonterminals and source ter-
minals, (iii) ? ? (N ?TT )? is a sequence of nonter-
minals and target terminals, (iv) the number cnt(?)
of nonterminal occurrences in ? is equal to the num-
ber cnt(?) of nonterminal occurrences in ?, (v)
?: {1, . . . , cnt(?)} ? {1, . . . , cnt(?)} is a one-to-
one mapping from nonterminal occurrences in ? to
nonterminal occurrences in ?, and (vi) w ? [0,?)
is a non-negative real-valued weight assigned to the
rule. We will assume ? to be implicitly defined by
indexing the NT occurrences in ? from left to right
starting with 1, and by indexing the NT occurrences
in ? by the indices of their corresponding counter-
parts in ?. Syntax-oriented PSCFG approaches typ-
ically ignore source structure, instead focussing on
generating syntactically well formed target deriva-
tions. (Galley et al, 2006) use syntactic constituents
for the PSCFG nonterminal set and (Zollmann and
Venugopal, 2006) take advantage of CCG (Steed-
man, 1999) categories, while (Chiang, 2005) uses
a single generic nonterminal. PSCFG derivations
function analogously to CFG derivations. Given
a source sentence f , the translation task under a
PSCFG grammar can be expressed as
1While IWSLT represents a limited resource translation task
(120K sentences of training data for Chinese-English), the prob-
lem of efficient n-gram LM integration is still critically impor-
tant to efficient decoding, and our contributions can be expected
to have an even more significant impact when decoding with
grammars induced from larger corpora.
e? = argmax
{e | ?D. src(D)=f,tgt(D)=e}
P (D)
where tgt(D) refers to the target terminal symbols
generated by the derivation D and src(D) refers to
the source terminal symbols spanned by D. The
score (also laxly called probability, since we never
need to compute the partition function) of a deriva-
tion D under a log-linear model, referring to the
rules r used in D, is:
P (D) =
1
Z
PLM (tgt(D))
?LM ?
?
i
?
r?D
?i(r)
?i
where ?i refers to features defined on each rule,
and PLM is a g-gram LM probability applied to the
target terminal symbols generated by the derivation
D. Introducing the LM feature defines dependen-
cies across adjacent rules used in each derivation,
and requires modifications to the decoding strategy.
Viewing the LM as a finite-state machine, the de-
coding process involves performing an intersection
between the PSCFG grammar and the g-gram LM
(Bar-Hillel et al, 1964). We present our work under
the construction in (Wu, 1996), following notation
from (Chiang, 2007), extending the formal descrip-
tion to reflect grammars with an arbitrary number of
nonterminals in each rule.
2.1 Decoding Strategies
In Figure 1, we reproduce the decoding algorithm
from (Chiang, 2007) that applies a PSCFG to
translate a source sentence in the same notation (as
a deductive proof system (Shieber et al, 1995)),
generalized to handle more than two non-terminal
pairs. Chart items [X, i, j, e] : w span j ? i words
in the source sentence f1 ? ? ? fn, starting at position
i + 1, and have weight w (equivalent to P (D)), and
e ? (TT ? {?})? is a sequence of target terminals,
with possible elided parts, marked by ?. Functions
p, q whose domain is TT ? {?} are defined in
(Chiang, 2007) and are repeated here for clarity.
p(a1 ? ? ? am) =
Y
g?i?m,?/?ai?g+1???ai?1
PLM (ai|ai?g+1 ? ? ? ai?1)
q(a1 ? ? ? am) =
(
a1 ? ? ? ag?1 ? am?g+2 ? ? ? am if m ? g
a1 ? ? ? am else
The function q elides elements from a target lan-
guage terminal sequence, leaving the leftmost and
rightmost g ? 1 words, replacing the elided words
501
X ? ??, ?? : w
(X ? ??, ?, w?) ? G
X ? ?f ji+1, ?? : w
[X, i, j; q(?)] : wp(?)
Z ? ?f i1i+1(X
1)1f
i2
j1+1
? ? ? (Xm?1)m?1f
im
jm?1+1
(Xm)mf
j
jm+1
, ?? : w
?
X1, i1, j1; e1
?
: w1 ? ? ? [Xm, im, jm; em] : wm
[Z, i, j, q(??)] : ww1 ? ? ?wmp(??) (where ?? = ? [e1/(X1)1, . . . , em/(Xm)m])
Goal item:
?
S, 0, n; ?s?g?1 ? ?\s?g?1
?
Figure 1. CYK parsing with integrated g-gram LM. The inference rules are explored in ascending order of j ? i. Here
? [e/Yi] is the string ? where the NT occurrence Yi is replaced by e. Functions q and p are explained in the text.
with a single ? symbol. The function p returns g-
gram LM probabilities for target terminal sequences,
where the ? delineates context boundaries, prevent-
ing the calculation from spanning this boundary. We
add a distinguished start nonterminal S to gener-
ate sentences spanning target translations beginning
with g ? 1 ?s? symbols and ending with g ? 1 ?\s?
symbols. This can e.g. be achieved by adding for
each nonterminal X a PSCFG rule
S ? ?X, ?s?g?1X?\s?g?1, 1?
We are only searching for the derivation of highest
probability, so we can discard identical chart items
that have lower weight. Since chart items are de-
fined by their left-hand side nonterminal production,
span, and the LM contexts e, we can safely discard
these identical items since q has retained all context
that could possibly impact the LM calculation. This
process is commonly referred to as item recombina-
tion. Backpointers to antecedent cells are typically
retained to allow N -Best extraction using an algo-
rithm such as (Huang and Chiang, 2005).
The impact of g-gram LM intersection during de-
coding is apparent in the final deduction step. Gen-
erating the set of consequent Z chart items involves
combining m previously produced chart cells. Since
each of these chart cells with given source span [i, j]
is identified by nonterminal symbol X and LM con-
text e, we have at worst |N | ? |TT |2(g?1) such chart
cells in a span. The runtime of this algorithm is thus
O
(
n3
[
|N ||TT |
2(g?1)
]K
)
where K is the maximum number of NT pairs per
rule and n the source sentence length. Without se-
vere pruning, this runtime is prohibitive for even the
smallest induced grammars. Traditional pruning ap-
proaches that limit the number of consequents after
they are produced are not effective since they first re-
quire that the cost of each consequent be computed
(which requires calls to the g-gram LM).
Restrictions to the grammar afford alternative de-
coding strategies to reduce the runtime cost of syn-
chronous parsing. (Zhang et al, 2006) ?binarize?
grammars into CNF normal form, while (Watan-
abe et al, 2006) allow only Griebach-Normal form
grammars. (Wellington et al, 2006) argue that these
restrictions reduce our ability to model translation
equivalence effectively. We take an agnostic view
on the issue; directly addressing the question of effi-
cient LM intersection rather than grammar construc-
tion.
3 Two-pass LM Intersection
We propose a two-pass solution to the problem of
online g-gram LM intersection. A naive two-pass
approach would simply ignore the LM interactions
during parsing, extract a set of N derivations from
the sentence spanning hypergraph and rescore these
derivations with the g-gram LM. In practice, this ap-
proach performs poorly (Chiang, 2007; Zollmann
and Venugopal, 2006). While parsing time is dra-
matically reduced (and N -best extraction time is
negligible), N is typically significantly less than the
complete number of possible derivations and sub-
stantial search errors remain. We propose an ap-
proach that builds upon the concept of a second pass
but uses the g-gram LM to search for alternative,
better translations.
502
3.1 First pass: parsing
We begin by relaxing the criterion that determines
when two chart items are equivalent during parsing.
We consider two chart items to be equivalent (and
therefore candidates for recombination) if they have
matching left-hand side nonterminals, and span. We
no longer require them to have the same LM con-
text e. We do however propagate the e, w for the
chart item with highest score, causing the algorithm
to still compute LM probabilities during parsing. As
a point of notation, we refer to such a chart item by
annotating its e, w as e1, w1, and we refer to them
as approximate items (since they have made a first-
best approximation for the purposes of LM calcula-
tion). These approximate items labeled with e1, w1
are used in consequent parse calculations.
The parsing algorithm under this approximation
stays unchanged, but parsing time is dramatically re-
duced. The runtime complexity of this algorithm is
now O
(
n3|N |K
)
at the cost of significant search
errors (since we ignored most LM contexts that we
encountered).
This relaxation is different from approaches that
do not use the LM during parsing. The sentence
spanning item does have LM probabilities associ-
ated with it (but potentially valuable chart items
were not considered during parsing). Like in tra-
ditional parsing, we retain the recombined items in
the cell to allow us to explore new derivations in a
second stage.
3.2 Second pass: hypergraph search
The goal item of the parsing step represents a sen-
tence spanning hypergraph of alternative deriva-
tions. Exploring alternatives from this hyper-
graph and updating LM probabilities can now reveal
derivations with higher scores that were not consid-
ered in the first pass. Exploring the whole space of
alternative derivations in this hypergraph is clearly
infeasible. We propose a g-gram LM driven heuris-
tic search ?H.Search? of this space that allows the g-
gram LM to decide which section of the hypergraph
to explore. By construction, traversing a particular
derivation item from the parse chart in target-side
left-to-right, depth-first order yields the correctly or-
dered sequence of target terminals that is the transla-
tion represented by this item. Now consider a partial
traversal of the item in that order, where we gener-
ate only the first M target terminals, leaving the rest
of the item in its original backpointer form. We in-
formally define our second pass algorithm based on
these partial derivation items.
Consider a simple example, where we have parsed
a source sentence, and arrived at a sentence spanning
item obtained from a rule with the following target
side:
NP2 VP3 PP1
and that the item?s best-score estimate is w. A par-
tial traversal of this item would replace NP2 with
one of the translations available in the chart cell un-
derlying NP2 (called ?unwinding?), and recalculate
the weights associated with this item, taking into
account the alternative target terminals. Assuming
?the nice man? was the target side of the best scoring
item in NP2, the respective traversal would main-
tain the same weight. An alternative item at NP2
might yield ?a nice man?. This partial traversal rep-
resents a possible item that we did not consider dur-
ing parsing, and recalculating LM probabilities for
this new item (based on approximate items VP3 and
PP1) yields weight w2:
the nice man VP3 PP1 : w1 = w
a nice man VP3 PP1 : w2
Alternative derivation items that obtain a higher
score than the best-score estimates represent recov-
ery from search errors. Our algorithm is based on
the premise that these items should be traversed fur-
ther, with the LM continuing to score newly gener-
ated target words. These partially traversed items
are placed on an agenda (sorted by score). At each
step of the second pass search, we select those items
from the agenda that are within a search beam of Z
from the best item, and perform the unwind opera-
tion on each of these items. Since we unwind partial
items from left-to-right the g-gram LM is able to in-
fluence the search through the space of alternative
derivations.
Applying the g-gram LM on partial items with
leading only-terminal symbols allows the integra-
tion of high- / flexible-order LMs during this sec-
ond stage process, and has the advantage of explor-
ing only those alternatives that participate in sen-
tence spanning, high scoring (considering both LM
and translation model scores) derivations. While
503
we do not evaluate such models here, we note that
H.Search was developed specifically for the integra-
tion of such models during search.
We further note that partial items that have gen-
erated translations that differ only in the word po-
sitions up to g ? 1 words before the first nonter-
minal site can be recombined (for the same rea-
sons as during LM intersected parsing). For exam-
ple, when considering a 3-gram LM, the two par-
tial items above can be recombined into one equiv-
alence class, since partial item LM costs resulting
from these items would only depend on ?nice man?,
but not on ?a? vs. ?the?. Even if two partial items
are candidates for recombination due to their termi-
nal words, they must also have identical backpoint-
ers (representing a set of approximate parse deci-
sions for the rest of the sentence, in our example
VP3PP1 ). Items that are filed into existing equiv-
alence classes with a lower score are not put onto
the agenda, while those that are better, or have cre-
ated new equivalence classes are scheduled onto the
agenda. For each newly created partial derivation,
we also add a backpointer to the ?parent? partial
derivation that was unwound to create it.
This equivalence classing operation transforms
the original left-hand side NT based hypergraph into
an (ordinary) graph of partial items. Each equiva-
lence class is a node in this new graph, and recom-
bined items are the edges. Thus, N -best extraction
can now be performed on this graph. We use the
extraction method from (Huang and Chiang, 2005).
The expensive portion of our algorithm lies in the
unwinding step, in which we generate a new par-
tial item for each alternative at the non-terminal site
that we are ?unwinding?. For each new partial item,
we factor out LM estimates and rule weights that
were used to score the parent item, and factor in
the LM probabilities and rule weights of the alter-
native choice that we are considering. In addition,
we must also update the new item?s LM estimates
for the remaining non-terminal and terminal sym-
bols that depend on this new left context of termi-
nals. Fortunately, the number of LM calculations
per new item is constant, i.e., does not dependent on
the length of the partial derivation, or how unwound
it is. Only (g ? 1) ? 2 LM probabilities have to be
re-evaluated per partial item. We now define this
?unwind-recombine? algorithm formally.
3.2.1 The unwind-recombine algorithm
Going back to the first-pass parsing algorithm
(Figure 1), remember that each application of a
grammar rule containing nonterminals corresponds
to an application of the third inference rule of the
algorithm. We can assign chart items C created by
the third inference rule a back-pointer (BP) target
side as follows: When applying the third inference
rule, each nonterminal occurrence (Xk)k in the cor-
responding Z ? ??, ?? grammar rule corresponds
to a chart cell [Xk, ik, jk] used as an antecedent for
the inference rule. We assign a BP target side for C
by replacing NT occurrences in ? (from the rule that
created C) with backpointers to their corresponding
antecedent chart cells. Further we define the distin-
guished backpointer PS as the pointer to the goal
cell [S, 0, n] : w?.
The deductive program for our second-pass al-
gorithm is presented in Figure 2. It makes use of
two kind of items. The first, {P ? ?; e1} : w,
links a backpointer P to a BP target side, storing
current-item vs. best-item correction terms in form
of an LM context e1 and a relative score w. The
second item form [[e;?]] in this algorithm corre-
sponds to partial left-to-right traversal states as de-
scribed above, where e is the LM context of the tra-
versed and unwound translation part, and ? the part
that is yet to be traversed and whose backpointers
are still to be unwound. The first deduction rule
presents the logical axioms, creating BP items for
each backpointer used in a NT inference rule appli-
cation during the first-pass parsing step. The sec-
ond deduction rule represents the unwinding step
as discussed in the example above. These deduc-
tions govern a search for derivations through the hy-
pergraph that is driven by updates of rule weights
and LM probabilities when unwinding non-first-best
hypotheses. The functions p and q are as defined
in Section 2, except that the domain of q is ex-
tended to BP target sides by first replacing each
back-pointer with its corresponding chart cell?s LM
context and then applying the original q on the re-
sulting sequence of target-terminals and ? symbols.2
Note that w?, which was computed by the first de-
duction rule, adjusts the current hypothesis? weight
2Note also that p(?s?g?1 ??\s?g?1) = 1 as the product over
the empty set is one.
504
{P ? ?; e1} : w?/w
(P back-points to 1st-pass cell [X, i, j; e1] : w; ? and w? are BP-target-side and weight of one of that cell?s items)
[[e;P?end]] : w
?
P ? ?lex?mid; e1
?
: w?
[[q(e?lex);?mid?end]] : ww?p[eq(?lex?mid)]p[q(e?lex?mid)q(?end)]/p(ee1)/p[q(ee1)q(?end)]
?
?lex contains no BPs and
?mid = P ??? or ?mid = ?
?
Figure 2. Left-to-right LM driven hypergraph search of the sentence spanning hypergraph; ? denotes the empty word.
Non-logical (Start) axiom: [[?;PS ]] : w?; Goal item: [[?s?g?1 ? ?\s?g?1; ?]] : w
that is based on the first-best instance of P to the
actually chosen instance?s weight. Further, the ra-
tio p(eq(?lex?mid))/p(ee1) adjusts the LM prob-
abilities of P ?s instantiation given its left context,
and p[q(e?lex?mid)q(?end)]/p[q(ee1)q(?end)] ad-
justs the LM probabilities of the g ? 1 words right
of P .
4 Alternative Approaches
We evaluate our two pass hypergraph search
?H.Search? against the strong single pass Cube
Pruning (CP) baseline as mentioned in (Chiang,
2005) and detailed in (Chiang, 2007). In the latter
work, the author shows that CP clearly outperforms
both the naive single pass solution of severe prun-
ing as well as the naive two-pass rescoring approach.
Thus, we focus on comparing our approach to CP.
CP is an optimization to the intersected LM pars-
ing algorithm presented in Figure 1. It addresses
the creation of the
?K
k=1 | [Xk, ik, jk, ?] | chart items
when generating consequent items. CP amounts to
an early termination condition when generating the
set of possible consequents. Instead of generating
all consequents, and then pruning away the poor per-
formers, CP uses the K-Best extraction approach of
(Huang and Chiang, 2005) to select the best K con-
sequents only, at the cost of potential search errors.
CP?s termination condition can be defined in terms
of an absolute number of consequents to generate, or
by terminating the generation process when a newly
generated item is worse (by ?) than the current best
item for the same left-hand side and span. To sim-
ulate comparable pruning criteria we parameterize
each method with soft-threshold based criteria only
(? for CP and Z for H.Search) since counter based
limits like K have different effects in CP (selecting
e labeled items) vs H.Search (selecting rules since
items are not labeled with e).
5 Experimental Framework
We present results on the IWSLT 2006 Chinese to
English translation task, based on the Full BTEC
corpus of travel expressions with 120K parallel sen-
tences (906K source words and 1.2m target words).
The evaluation test set contains 500 sentences with
an average length of 10.3 source words.
Grammar rules were induced with the syntax-
based SMT system ?SAMT? described in (Zoll-
mann and Venugopal, 2006), which requires ini-
tial phrase alignments that we generated with
?GIZA++? (Koehn et al, 2003), and syntactic parse
trees of the target training sentences, generated by
the Stanford Parser (D. Klein, 2003) pre-trained on
the Penn Treebank. All these systems are freely
available on the web.
We experiment with 2 grammars, one syntax-
based (3688 nonterminals, 0.3m rules), and one
purely hierarchical (1 generic nonterminal, 0.05m
rules) as in (Chiang, 2005). The large number of
nonterminals in the syntax based systems is due to
the CCG extension over the original 75 Penn Tree-
bank nonterminals. Parameters ? used to calculate
P (D) are trained using MER training (Och, 2003)
on development data.
6 Comparison of Approaches
We evaluate each approach by considering both
search errors made on the development data for a
fixed set of model parameters, and the BLEU metric
to judge translation quality.
6.1 Search Error Analysis
While it is common to evaluate MT quality using the
BLEU score, we would like to evaluate search errors
made as a function of ?effort? made by each algo-
rithm to produce a first-best translation. We con-
sider two metrics of effort made by each algorithm.
505
We first evaluate search errors as a function of novel
queries made to the g-gram LM (since LM calls tend
to be the dominant component of runtime in large
MT systems). We consider novel queries as those
that have not already been queried for a particular
sentence, since the repeated calls are typically effi-
ciently cached in memory and do not affect runtime
significantly. Our goal is to develop techniques that
can achieve low search error with the fewest novel
queries to the g-gram LM.
To appreciate the practical impact of each algo-
rithm, we also measure search errors as a function of
the number of seconds required to translate a fixed
unseen test set. This second metric is more sensitive
to implementation and, as it turned out, even com-
piler memory management decisions.
We define search errors based on the weight of
the best sentence spanning item. Treating weights as
negative log probabilities (costs), we accumulate the
value of the lowest cost derivation for each sentence
in the testing data as we vary pruning settings ap-
propriate to each method. Search errors are reduced
when we are able to lower this accumulated model
cost. We prefer approaches that yield lowmodel cost
with the least number of LM calls or number of sec-
onds spent decoding.
It is important to note that model cost
(? log(P (D))) is a function of the parameters
? which have been trained using MER training. The
parameters used for these experiments were trained
with the CP approach; in practice we find that either
approach is effective for MER training.
6.2 Results
Figure 3 and Figure 4 plot model cost as a function
of LM cache misses for the IWSLT Hierarchical and
Syntax based systems, while Figure 5 plots decod-
ing time. The plots are based on accumulated model
cost, decoding time and LM cache misses over the
IWSLT Test 06 set. For H.Search, we vary the beam
parameter Z for a fixed value of ? = 5 during pars-
ing while for CP, we vary ?. We also limit the total
number of items on the agenda at any time to 1000
for H.Search as a memory consideration. We plot
each method until we see no change in BLEU score
for that method. BLEU scores for each parameter
setting are also noted on the plots.
For both the hierarchical and syntax based gram-
mars we see that the H.Search method achieves a
given model cost ?earlier? in terms of novel LM
calls for most of the plotted region, but ultimately
fails to achieve the same lowest model cost as the
CP method.3 While the search beam of Z mit-
igates the impact of the estimated scores during
H.Search?s second pass, the score is still not an ad-
missible heuristic for error-free search. We suspect
that simple methods to ?underestimate? the score of
a partial derivation?s remaining nonterminals could
bridge this gap in search error. BLEU scores in
the regions of lowest model cost tend to be reason-
ably stable and reflect comparable translation per-
formance for both methods.
Under both H.Search and CP, the hierarchical
grammar ultimately achieves a BLEU score of
19.1%, while the syntactic grammar?s score is ap-
proximately 1.5 points higher at 20.7%. The hierar-
chical grammar demonstrates a greater variance of
BLEU score for both CP and H.Search compared
to the syntax-based grammar. The use of syntac-
tic structure serves as an additional model of target
language fluency, and can explain the fact that syn-
tax based translation quality is more robust to differ-
ences in the number of g-gram LM options explored.
Decoding time plots shows a similar result,
but with diminished relative improvement for the
H.Search method. Profiling analysis of the H.Search
method shows that significant time is spent simply
on allocating and deallocating memory for partial
derivations on top of the scoring times for these
items. We expect to be able to reduce this overhead
significantly in future work.
7 Conclusion
We presented an novel two-pass decoding approach
for PSCFG-based machine translation that achieves
search errors comparable to the state of the art Cube
Pruning method. By maintaining comparable, sen-
tence spanning derivations we allow easy integration
of high or flexible order LMs as well as sentence
level syntactic features during the search process.
We plan to evaluate the impact of these more power-
ful models in future work. We also hope to address
the question of how much search error is tolerable to
3Analysis of total LM calls made by each method (not pre-
sented here) shows the H.Search makes significantly fewer (1/2)
total LM calls than CP to achieve each model cost.
506
IWSLT - LM Cache Misses Hierarchical
-3200-3100
-3000-2900
-2800-2700
-2600-2500
0.0E+00 2.5E+06 5.0E+06 7.5E+06Number of LM Misses
Model Cost
CPH.Search
0.175
0.178
0.181
0.191
0.188
0.177
0.180
0.182
0.186
0.191
0.191
0.174
Figure 3. LM caches misses for
IWSLT hierarchical grammar and
BLEU scores for varied pruning pa-
rameters
IWSLT - LM Cache Misses Syntax
3740037425
3745037475
37500
2.0E+05 7.0E+05 1.2E+06Number of LM Misses
Model Cost
CPH.Search0.205
0.206
0.206
0.2
0.2
0.206
0.207
0.207
0.207
Figure 4. LM caches misses
for IWSLT syntactic grammar and
BLEU scores for varied pruning pa-
rameters
IWSLT - Syntax Decoding Time
3740037420
3744037460
3748037500
9.0E+02 9.8E+02 1.1E+03 1.1E+03 1.2E+03Decoding Time (s)
Model Cost
CPH.Search0.205
0.206
0.207
0.205
0.206
0.206
0.206
0.207 0.207
Figure 5. Decoding time (s)
for IWSLT syntactic grammar and
BLEU scores for varied pruning pa-
rameters
run MER training and still generate parameters that
generalize well to test data. This point is particularly
relevant to evaluate the use of search error analysis.
References
Bar-Hillel, M.Perles, and E.Shamir. 1964. An efficient
context-free parsing algorithm. Communications of
the Assocation for Computing Machinery.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proc. of ACL.
David Chiang. 2007. Hierarchical phrase based transla-
tion. To Appear in the Journal of Computational Lin-
guistics.
C. Manning D. Klein. 2003. Accurate unlexicalized
parsing. In Proc. of ACL.
Matthias Eck and Chiori Hori. 2005. Overview of the
IWSLT 2005 evaluation campaign. In Proc. of Inter-
national Workshop on Spoken Language Translation,
pages 11?17.
Michael Galley, M. Hopkins, Kevin Knight, and Daniel
Marcu. 2006. Scalable inferences and training of
context-rich syntax translation models. In Proc. of
NAACL-HLT.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proc. of the 9th International Workshop on
Parsing Technologies.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
of HLT/NAACL.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proc. of ACL, Sap-
poro, Japan, July 6-7.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalua-
tion of machine translation. In Proc. of ACL.
Stuart Shieber, Yves Schabes, and Fernando Pereira.
1995. Principles and implementation of deductive
parsing. Journal of Logic Programming, 24:3?15.
Mark Steedman. 1999. Alternative quantifier scope in
CCG. In Proc. of ACL.
Taro Watanabe, Hajime Tsukada, and Hideki Isozaki.
2006. Left-to-right target generation for hierarchical
phrase based translation. In ACL.
Benjamin Wellington, Sonjia Waxmonsky, and I. Dan
Melamed. 2006. Empirical lower bounds on the com-
plexity of translational equivalence. In ACL.
Dekai Wu. 1996. A polynomial time algorithm for statis-
tical machine translation. In Proc. of the Association
for Computational Linguistics.
Hao Zhang, Liang Huang, Daniel Gildea, and Kevin
Knight. 2006. Synchronous binarization for machine
translation. In Proc. of HLT/NAACL.
Andreas Zollmann and Ashish Venugopal. 2006. Syntax
augmented machine translation via chart parsing. In
Proc. of the Workshop on Statistical Machine Transla-
tion, HLT/NAACL, New York, June.
507
Proceedings of the Second Workshop on Statistical Machine Translation, pages 216?219,
Prague, June 2007. c?2007 Association for Computational Linguistics
The Syntax Augmented MT (SAMT) System for the Shared Task in the 2007
ACL Workshop on Statistical Machine Translation
Andreas Zollmann and Ashish Venugopal and Matthias Paulik and Stephan Vogel
School of Computer Science, Carnegie Mellon University, Pittsburgh
interACT Lab, University of Karlsruhe
{ashishv,zollmann,paulik,vogel+}@cs.cmu.edu
Abstract
We describe the CMU-UKA Syntax Augmented
Machine Translation system ?SAMT? used for the
shared task ?Machine Translation for European Lan-
guages? at the ACL 2007 Workshop on Statistical
Machine Translation. Following an overview of syn-
tax augmented machine translation, we describe pa-
rameters for components in our open-source SAMT
toolkit that were used to generate translation results
for the Spanish to English in-domain track of the
shared task and discuss relative performance against
our phrase-based submission.
1 Introduction
As Chiang (2005) and Koehn et al (2003) note,
purely lexical ?phrase-based? translation models
suffer from sparse data effects when translating con-
ceptual elements that span or skip across several
source language words. Phrase-based models also
rely on distance and lexical distortion models to rep-
resent the reordering effects across language pairs.
However, such models are typically applied over
limited source sentence ranges to prevent errors in-
troduced by these models and to maintain efficient
decoding (Och and Ney, 2004).
To address these concerns, hierarchically struc-
tured models as in Chiang (2005) define weighted
transduction rules, interpretable as components of
a probabilistic synchronous grammar (Aho and Ull-
man, 1969) that represent translation and reordering
operations. In this work, we describe results from
the open-source Syntax Augmented Machine Trans-
lation (SAMT) toolkit (Zollmann and Venugopal,
2006) applied to the Spanish-to-English in-domain
translation task of the ACL?07 workshop on statisti-
cal machine translation.
We begin by describing the probabilistic model of
translation applied by the SAMT toolkit. We then
present settings for the pipeline of SAMT tools that
we used in our shared task submission. Finally, we
compare our translation results to the CMU-UKA
phrase-based SMT system and discuss relative per-
formance.
2 Synchronous Grammars for SMT
Probabilistic synchronous context-free grammars
(PSCFGs) are defined by a source terminal set
(source vocabulary) TS , a target terminal set (target
vocabulary) TT , a shared nonterminal setN and pro-
duction rules of the form
X ? ??, ?,?, w?
where following (Chiang, 2005)
? X ? N is a nonterminal
? ? ? (N ?TS)? : sequence of source nonterminals
and terminals
? ? ? (N ? TT )? : sequence of target nonterminals
and terminals
? the count #NT(?) of nonterminal tokens in ? is
equal to the count #NT(?) of nonterminal tokens
in ?,
? ?: {1, . . . ,#NT(?)} ? {1, . . . ,#NT(?)} one-
to-one mapping from nonterminal tokens in ? to
nonterminal tokens in ?
? w ? [0,?) : nonnegative real-valued weight
Chiang (2005) uses a single nonterminal cate-
gory, Galley et al (2004) use syntactic constituents
for the PSCFG nonterminal set, and Zollmann and
Venugopal (2006) take advantage of CCG (Combi-
natorial Categorical Grammar) (Steedman, 1999) in-
spired ?slash? and ?plus? categories, focusing on tar-
get (rather than source side) categories to generate
well formed translations.
We now describe the identification and estima-
tion of PSCFG rules from parallel sentence aligned
corpora under the framework proposed by Zollmann
and Venugopal (2006).
216
2.1 Grammar Induction
Zollmann and Venugopal (2006) describe a process
to generate a PSCFG given parallel sentence pairs
?f, e?, a parse tree pi for each e, the maximum a
posteriori word alignment a over ?f, e?, and phrase
pairs Phrases(a) identified by any alignment-driven
phrase induction technique such as e.g. (Och and
Ney, 2004).
Each phrase in Phrases(a) (phrases identifiable
from a) is first annotated with a syntactic category
to produce initial rules. If the target span of the
phrase does not match a constituent in pi, heuristics
are used to assign categories that correspond to par-
tial rewriting of the tree. These heuristics first con-
sider concatenation operations, forming categories
like ?NP+VP?, and then resort to CCG style ?slash?
categories like ?NP/NN? giving preference to cate-
gories found closer to the leaves of the tree.
To illustrate this process, consider the following
French-English sentence pair and selected phrase
pairs obtained by phrase induction on an automat-
ically produced alignment a, and matching target
spans with pi.
f = il ne va pas
e = he does not go
PRP ? il, he
VB ? va, go
RB+VB ? ne va pas, not go
S ? il ne va pas, he does not go
The alignment a with the associated target side
parse tree is shown in Fig. 1 in the alignment visual-
ization style defined by Galley et al (2004).
Following the Data-Oriented Parsing inspired
rule generalization technique proposed by Chiang
(2005), one can now generalize each identified
rule (initial or already partially generalized) N ?
f1 . . . fm/e1 . . . en for which there is an initial rule
M ? fi . . . fu/ej . . . ev where 1 ? i < u ? m and
1 ? j < v ? n, to obtain a new rule
N ? f1 . . . fi?1Mkfu+1 . . . fm/e1 . . . ej?1Mkev+1 . . . en
where k is an index for the nonterminal M that in-
dicates the one-to-one correspondence between the
new M tokens on the two sides (it is not in the space
of word indices like i, j, u, v,m, n). The initial rules
listed above can be generalized to additionally ex-
tract the following rules from f, e.
S ? PRP1 ne va pas , PRP1 does not go
S ? il ne VB1 pas , he does not VB1
S ? il RB+VB1, he does RB+VB1
S ? PRP1 RB+VB2, PRP1 does RB+VB2
RB+VB ? ne VB1 pas , not VB1
Fig. 2 uses regions to identify the labeled, source
and target side span for all initial rules extracted on
our example sentence pair and parse. Under this rep-
resentation, generalization can be viewed as a pro-
cess that selects a region, and proceeds to subtract
out any sub-region to form a generalized rule.
S
q
q
q
q
q
q
q
M
M
M
M
M
M
M
NP VP
q
q
q
q
q
q
q
M
M
M
M
M
M
M
PRN AUX RB VB
he does not
q
q
q
q
q
q
q
M
M
M
M
M
M
M
go
q
q
q
q
q
q
q
il ne va pas
Figure 1: Alignment graph (word alignment and target parse
tree) for a French-English sentence pair.
il 1 ne 2 va 3 pas 4
he 1
does 2
not 3
go 4

ffi
ff
ff
9
S
RB+VB
VB
VP
NP+AUX
NP
Figure 2: Spans of initial lexical phrases w.r.t. f, e. Each phrase
is labeled with a category derived from the tree in Fig. 1.
2.2 Decoding
Given a source sentence f , the translation task under
a PSCFG grammar can be expressed analogously to
monolingual parsing with a CFG. We find the most
likely derivation D with source-side f and read off
the English translation from this derivation:
e? = tgt
(
argmax
D:src(D)=f
p(D)
)
(1)
where tgt(D) refers to the target terminals and
src(D) to the source terminals generated by deriva-
tion D.
Our distribution p over derivations is defined by a
log-linear model. The probability of a derivation D
217
is defined in terms of the rules r that are used in D:
p(D) =
pLM (tgt(D))?LM
?
r?D
?
i ?i(r)
?i
Z(?)
(2)
where ?i refers to features defined on each rule,
pLM is a language model (LM) probability applied to
the target terminal symbols generated by the deriva-
tion D, and Z(?) is a normalization constant cho-
sen such that the probabilities sum up to one. The
computational challenges of this search task (com-
pounded by the integration of the LM) are addressed
in (Chiang, 2007; Venugopal et al, 2007). The
feature weights ?i are trained in concert with the
LM weight via minimum error rate (MER) training
(Och, 2003).
We now describe the parameters for the SAMT
implementation of the model described above.
3 SAMT Components
SAMT provides tools to perform grammar induc-
tion ( ?extractrules?, ?filterrules?), from bilingual
phrase pairs and target language parse trees, as well
as translation (?FastTranslateChart?) of source sen-
tences given an induced grammar.
3.1 extractrules
extractrules is the first step of the grammar induc-
tion pipeline, where rules are identified based on the
process described in section 2.1. This tool works on
a per sentence basis, considering phrases extracted
for the training sentence pair ?si, ti? and the corre-
sponding target parse tree pii. extractrules outputs
identified rules for each input sentence pair, along
with associated statistics that play a role in the esti-
mation of the rule features ?. These statistics take
the form of real-valued feature vectors for each rule
as well as summary information collected over the
corpus, such as the frequency of each nonterminal
symbol, or unique rule source sides encountered.
For the shared task evaluation, we ran extrac-
trules with the following extraction parameter
settings to limit the scope and number of rules
extracted. These settings produce the same initial
phrase table as the CMU-UKA phrase based sys-
tem. We limit the source-side length of the phrase
pairs considered as initial rules to 8 (parameter
MaxSourceLength). Further we set the max-
imum number of source and target terminals per
rule (MaxSource/MaxTargetWordCount)
to 5 and 8 respectively with 2 of nonter-
minal pairs (i.e., substitution sites) per rule
(MaxSubstititionCount). We limit the
total number of symbols in each rule to 8
(MaxSource/TargetSymbolCount) and
require all rules to contain at least one source-side
terminal symbol (noAllowAbstractRules,
noAllowRulesWithOnlyTargetTerminals)
since this reduces decoding time considerably. Ad-
ditionally, we discard all rules that contain source
word sequences that do not exist in the development
and test sets provided for the shared task (parameter
-r).
3.2 filterrules
This tool takes as input the rules identified by ex-
tractrules, and associates each rule with a feature
vector ?, representing multiple criteria by which the
decoding process can judge the quality of each rule
and, by extension, each derivation. filterrules is also
in charge of pruning the resulting PSCFG to ensure
tractable decoding.
? contains both real and Boolean valued features
for each rule. The following probabilistic features
are generated by filterrules:
? p?(r| lhs(X)) : Probability of a rule given its left-
hand-side (?result?) nonterminal
? p?(r| src(r)) : Prob. of a rule given its source side
? p?(ul(src(r)),ul(tgt(r))|ul(src(r)) : Probability
of the unlabeled source and target side of the rule
given its unlabeled source side.
Here, the function ul removes all syntactic la-
bels from its arguments, but retains ordering nota-
tion, producing relative frequencies similar to those
used in purely hierarchical systems. As in phrase-
based translation model estimation, ? also contains
two lexical weights (Koehn et al, 2003), counters
for number of target terminals generated. ? also
boolean features that describe rule types (i.e. purely
terminal vs purely nonterminal).
For the shared task submission, we pruned away
rules that share the same source side based on
p?(r| src(r)) (the source conditioned relative fre-
quency). We prune away a rule if this value is
less that 0.5 times the one of the best performing
rule (parameters BeamFactorLexicalRules,
BeamFactorNonlexicalRules).
3.3 FastTranslateChart
The FastTranslateChart decoder is a chart parser
based on the CYK+(Chappelier and Rajman, 1998)
algorithm. Translation experiments in this paper
are performed with a 4-gram SRI language model
trained on the target side of the corpus. Fast-
TranslateChart implements both methods of han-
dling the LM intersection described in (Venugopal
et al, 2007). For this submission, we use the Cube-
Pruning (Chiang, 2007) approach (the default set-
ting). LM and rule feature parameters ? are trained
with the included MER training tool. Our prun-
ing settings allow up to 200 chart items per cell
218
with left-hand side nonterminal ? S? (the reserved
sentence spanning nonterminal), and 100 items per
cell for each other nonterminal. Beam pruning
based on an (LM-scaled) additive beam of neg-
lob probability 5 is used to prune the search fur-
ther. These pruning settings correspond to setting
?PruningMap=0-100-5-@_S-200-5?.
4 Empirical Results
We trained our system on the Spanish-English in-
domain training data provided for the workshop. Ini-
tial data processing and normalizing is described
in the workshop paper for the CMU-UKA ISL
phrase-based system. NIST-BLEU scores are re-
ported on the 2K sentence development ?dev06? and
test ?test06? corpora as per the workshop guide-
lines (case sensitive, de-tokenized). We compare
our scores against the CMU-UKA ISL phrase-based
submission, a state-of-the art phrase-based SMT
system with part-of-speech (POS) based word re-
ordering (Paulik et al, 2007).
4.1 Translation Results
The SAMT system achieves a BLEU score of
32.48% on the ?dev06? development corpus and
32.15% on the unseen ?test06? corpus. This is
slightly better than the score of the CMU-UKA
phrase-based system, which achieves 32.20% and
31.85% when trained and tuned under the same in-
domain conditions. 1
To understand why the syntax augmented ap-
proach has limited additional impact on the Spanish-
to-English task, we consider the impact of reorder-
ing within our phrase-based system. Table 1 shows
the impact of increasing reordering window length
(Koehn et al, 2003) on translation quality for the
?dev06? data.2 Increasing the reordering window
past 2 has minimal impact on translation quality,
implying that most of the reordering effects across
Spanish and English are well modeled at the local or
phrase level. The benefit of syntax-based systems to
capture long-distance reordering phenomena based
on syntactic structure seems to be of limited value
for the Spanish to English translation task.
5 Conclusions
In this work, we briefly summarized the Syntax-
augmented MT model, described how we trained
and ran our implementation of that model on
1The CMU-UKA phrase-based workshop submission was
tuned on out-of-domain data as well.
2Variant of the CMU-UKA ISL phrase-based system with-
out POS based reordering. With POS-based reordering turned
on, additional window-based reordering even for window length
1 had no improvement in NIST-BLEU.
ReOrder 1 2 3 4 POS SAMT
BLEU 31.98 32.24 32.30 32.26 32.20 32.48
Table 1: Impact of phrase based reordering model settings com-
pared to SAMT on the ?dev06? corpus measured by NIST-
BLEU
the MT?07 Spanish-to-English translation task.
We compared SAMT translation results to
a strong phrase-based system trained under
the same conditions. Our system is available
open-source under the GNU General Pub-
lic License (GPL) and can be downloaded at
www.cs.cmu.edu/?zollmann/samt
References
Alfred Aho and Jeffrey Ullman. 1969. Syntax directed
translations and the pushdown assembler. Journal of
Computer and System Sciences.
Jean-Cedric. Chappelier and Martin Rajman. 1998.
A generalized CYK algorithm for parsing stochastic
CFG. In Proc. of Tabulation in Parsing and Deduction
(TAPD?98), Paris, France.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proc. of ACL.
David Chiang. 2007. Hierarchical phrase based transla-
tion. Computational Linguistics. To appear.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In Proc.
of HLT/NAACL, Boston, Massachusetts.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
of HLT/NAACL, Edmonton,Canada.
Franz Och and Hermann Ney. 2004. The alignment tem-
plate approach to statistical machine translation. Com-
put. Linguistics.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proc. of ACL, Sap-
poro, Japan, July 6-7.
Matthias Paulik, Kay Rottmann, Jan Niehues, Silja
Hildebrand, and Stephan Vogel. 2007. The ISL
phrase-based MT system for the 2007 ACL work-
shop on statistical MT. In Proc. of the Association
of Computational Linguistics Workshop on Statistical
Machine Translation.
Mark Steedman. 1999. Alternative quantifier scope in
CCG. In Proc. of ACL, College Park, Maryland.
Ashish Venugopal, Andreas Zollmann, and Stephan Vo-
gel. 2007. An efficient two-pass approach to syn-
chronous CFG driven MT. In Proc. of HLT/NAACL,
Rochester, NY.
Andreas Zollmann and Ashish Venugopal. 2006. Syntax
augmented machine translation via chart parsing. In
Proc. of the Workshop on Statistical Machine Transla-
tion, HLT/NAACL, New York, June.
219
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 1145?1152
Manchester, August 2008
A Systematic Comparison of Phrase-Based, Hierarchical and
Syntax-Augmented Statistical MT
Andreas Zollmann
?
and Ashish Venugopal
?
and Franz Och and Jay Ponte
Google Inc.
1600 Amphitheatre Parkway
Mountain View, CA 94303, USA
{zollmann,ashishv}@cs.cmu.edu {och,ponte}@google.com
Abstract
Probabilistic synchronous context-free
grammar (PSCFG) translation models
define weighted transduction rules that
represent translation and reordering oper-
ations via nonterminal symbols. In this
work, we investigate the source of the im-
provements in translation quality reported
when using two PSCFG translation mod-
els (hierarchical and syntax-augmented),
when extending a state-of-the-art phrase-
based baseline that serves as the lexical
support for both PSCFG models. We
isolate the impact on translation quality
for several important design decisions in
each model. We perform this comparison
on three NIST language translation tasks;
Chinese-to-English, Arabic-to-English
and Urdu-to-English, each representing
unique challenges.
1 Introduction
Probabilistic synchronous context-free grammar
(PSCFG) models define weighted transduction
rules that are automatically learned from parallel
training data. As in monolingual parsing, such
rules make use of nonterminal categories to gener-
alize beyond the lexical level. In the example be-
low, the French (source language) words ?ne? and
?pas? are translated into the English (target lan-
guage) word ?not?, performing reordering in the
context of a nonterminal of type ?VB? (verb).
VP ? ne VB pas, do not VB : w
1
?
Work done during internships at Google Inc.
?
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
VB ? veux,want : w
2
.
As with probabilistic context-free grammars, each
rule has a left-hand-side nonterminal (VP and VB
in the two rules above), which constrains the rule?s
usage in further composition, and is assigned a
weight w, estimating the quality of the rule based
on some underlying statistical model. Transla-
tion with a PSCFG is thus a process of compos-
ing such rules to parse the source language while
synchronously generating target language output.
PSCFG approaches such as Chiang (2005) and
Zollmann and Venugopal (2006) typically begin
with a phrase-based model as the foundation for
the PSCFG rules described above. Starting with
bilingual phrase pairs extracted from automatically
aligned parallel text (Och and Ney, 2004; Koehn et
al., 2003), these PSCFG approaches augment each
contiguous (in source and target words) phrase
pair with a left-hand-side symbol (like the VP in
the example above), and perform a generalization
procedure to form rules that include nonterminal
symbols. We can thus view PSCFG methods as
an attempt to generalize beyond the purely lexi-
cal knowledge represented in phrase based mod-
els, allowing reordering decisions to be explicitly
encoded in each rule. It is important to note that
while phrase-based models cannot explicitly repre-
sent context sensitive reordering effects like those
in the example above, in practice, phrase based
models often have the potential to generate the
same target translation output by translating source
phrases out of order, and allowing empty trans-
lations for some source words. Apart from one
or more language models scoring these reorder-
ing alternatives, state-of-the-art phrase-based sys-
tems are also equipped with a lexicalized distortion
model accounting for reordering behavior more di-
rectly. While previous work demonstrates impres-
1145
sive improvements of PSCFG over phrase-based
approaches for large Chinese-to-English data sce-
narios (Chiang, 2005; Chiang, 2007; Marcu et al,
2006; DeNeefe et al, 2007), these phrase-based
baseline systems were constrained to distortion
limits of four (Chiang, 2005) and seven (Chiang,
2007; Marcu et al, 2006; DeNeefe et al, 2007),
respectively, while the PSCFG systems were able
to operate within an implicit reordering window of
10 and higher.
In this work, we evaluate the impact of the ex-
tensions suggested by the PSCFG methods above,
looking to answer the following questions. Do the
relative improvements of PSCFG methods persist
when the phrase- based approach is allowed com-
parable long-distance reordering, and when the n-
gram language model is strong enough to effec-
tively select from these reordered alternatives? Do
these improvements persist across language pairs
that exhibit significantly different reodering effects
and how does resource availablility effect relative
performance? In order to answer these questions
we extend our PSCFG decoder to efficiently han-
dle the high order LMs typically applied in state-
of-the-art phrase based translation systems. We
evaluate the phrase-based system for a range of re-
ordering limits, up to those matching the PSCFG
approaches, isolating the impact of the nontermi-
nal based approach to reordering. Results are pre-
sented in multiple language pairs and data size
scenarios, highlighting the limited impact of the
PSCFG model in certain conditions.
2 Summary of approaches
Given a source language sentence f , statistical ma-
chine translation defines the translation task as se-
lecting the most likely target translation e under a
model P (e|f), i.e.:
?
e(f) = argmax
e
P (e|f) = argmax
e
m
?
i=1
h
i
(e, f)?
i
where the argmax operation denotes a search
through a structured space of translation ouputs
in the target language, h
i
(e, f) are bilingual fea-
tures of e and f and monolingual features of e,
and weights ?
i
are trained discriminitively to max-
imize translation quality (based on automatic met-
rics) on held out data (Och, 2003).
Both phrase-based and PSCFG approaches
make independence assumptions to structure this
search space and thus most features h
i
(e, f) are
designed to be local to each phrase pair or rule.
A notable exception is the n-gram language model
(LM), which evaluates the likelihood of the se-
quential target words output. Phrase-based sys-
tems also typically allow source segments to be
translated out of order, and include distortion mod-
els to evaluate such operations. These features
suggest the efficient dynamic programming al-
gorithms for phrase-based systems described in
Koehn et al (2004).
We now discuss the translation models com-
pared in this work.
2.1 Phrase Based MT
Phrase-based methods identify contiguous bilin-
gual phrase pairs based on automatically gener-
ated word alignments (Och et al, 1999). Phrase
pairs are extracted up to a fixed maximum length,
since very long phrases rarely have a tangible im-
pact during translation (Koehn et al, 2003). Dur-
ing decoding, extracted phrase pairs are reordered
to generate fluent target output. Reordered trans-
lation output is evaluated under a distortion model
and corroborated by one or more n-gram language
models. These models do not have an explicit rep-
resentation of how to reorder phrases. To avoid
search space explosion, most systems place a limit
on the distance that source segments can be moved
within the source sentence. This limit, along with
the phrase length limit (where local reorderings
are implicit in the phrase), determine the scope of
reordering represented in a phrase-based system.
All experiments in this work limit phrase pairs to
have source and target length of at most 12, and
either source length or target length of at most 6
(higher limits did not result in additional improve-
ments). In our experiments phrases are extracted
by the method described in Och and Ney (2004)
and reordering during decoding with the lexical-
ized distortion model from Zens and Ney (2006).
The reordering limit for the phrase based system
(for each language pair) is increased until no addi-
tional improvements result.
2.2 Hierarchical MT
Building upon the success of phrase-based meth-
ods, Chiang (2005) presents a PSCFG model of
translation that uses the bilingual phrase pairs of
phrase-based MT as starting point to learn hierar-
chical rules. For each training sentence pair?s set of
extracted phrase pairs, the set of induced PSCFG
rules can be generated as follows: First, each
1146
phrase pair is assigned a generic X-nonterminal as
left-hand-side, making it an initial rule. We can
now recursively generalize each already obtained
rule (initial or including nonterminals)
N ? f
1
. . . f
m
/e
1
. . . e
n
for which there is an initial rule
M ? f
i
. . . f
u
/e
j
. . . e
v
where 1 ? i < u ? m and 1 ? j < v ? n, to
obtain a new rule
N ? f
i?1
1
X
k
f
m
u+1
/e
j?1
1
X
k
e
n
v+1
where e.g. f
i?1
1
is short-hand for f
1
. . . f
i?1
, and
where k is an index for the nonterminal X that
indicates the one-to-one correspondence between
the new X tokens on the two sides (it is not in
the space of word indices like i, j, u, v,m, n). The
recursive form of this generalization operation al-
lows the generation of rules with multiple nonter-
minal symbols.
Performing translation with PSCFG grammars
amounts to straight-forward generalizations of
chart parsing algorithms for PCFG grammars.
Adaptations to the algorithms in the presence of n-
gram LMs are discussed in (Chiang, 2007; Venu-
gopal et al, 2007; Huang and Chiang, 2007).
Extracting hierarchical rules in this fashion can
generate a large number of rules and could in-
troduce significant challenges for search. Chiang
(2005) places restrictions on the extracted rules
which we adhere to as well. We disallow rules
with more than two nonterminal pairs, rules with
adjacent source-side nonterminals, and limit each
rule?s source side length (i.e., number of source
terminals and nonterminals) to 6. We extract rules
from initial phrases of maximal length 12 (exactly
matching the phrase based system).
1
Higher length
limits or allowing more than two nonterminals per
rule do not yield further improvements for systems
presented here.
During decoding, we allow application of all
rules of the grammar for chart items spanning up
to 15 source words (for sentences up to length 20),
or 12 source words (for longer sentences), respec-
tively. When that limit is reached, only a special
glue rule allowing monotonic concatenation of hy-
potheses is allowed. (The same holds for the Syn-
tax Augmented system.)
1
Chiang (2005) uses source length limit 5 and initial
phrase length limit 10.
2.3 Syntax Augmented MT
Syntax Augmented MT (SAMT) (Zollmann and
Venugopal, 2006) extends Chiang (2005) to in-
clude nonterminal symbols from target language
phrase structure parse trees. Each target sentence
in the training corpus is parsed with a stochas-
tic parser?we use Charniak (2000))?to produce
constituent labels for target spans. Phrases (ex-
tracted from a particular sentence pair) are as-
signed left-hand-side nonterminal symbols based
on the target side parse tree constituent spans.
Phrases whose target side corresponds to a con-
stituent span are assigned that constituent?s label as
their left-hand-side nonterminal. If the target span
of the phrase does not match a constituent in the
parse tree, heuristics are used to assign categories
that correspond to partial rewriting of the tree.
These heuristics first consider concatenation oper-
ations, forming categories such as ?NP+V?, and
then resort to CCG (Steedman, 1999) style ?slash?
categories such as ?NP/NN.? or ?DT\NP?. In the
spirit of isolating the additional benefit of syntactic
categories, the SAMT system used here also gen-
erates a purely hierarchical (single generic nonter-
minal symbol) variant for each syntax-augmented
rule. This allows the decoder to choose between
translation derivations that use syntactic labels and
those that do not. Additional features introduced
in SAMT rules are: a relative frequency estimated
probability of the rule given its left-hand-side non-
terminal, and a binary feature for the the purely
hierachial variants.
3 Large N-Gram LMs for PSCFG
decoding
Brants et al (2007) demonstrate the value of large
high-order LMs within a phrase-based system. Re-
cent results with PSCFG based methods have typ-
ically relied on significantly smaller LMs, as a
result of runtime complexity within the decoder.
In this work, we started with the publicly avail-
able PSCFG decoder described in Venugopal et al
(2007) and extended it to efficiently use distributed
higher-order LMs under the Cube-Pruning decod-
ing method from Chiang (2007). These extensions
allow us to verify that the benefits of PSCFG mod-
els persist in the presence of large, powerful n-
gram LMs.
3.1 Asynchronous N-Gram LMs
As described in Brants et al (2007), using large
distributed LMs requires the decoder to perform
1147
asynchronous LM requests. Scoring n-grams un-
der this distributed LM involves queuing a set
of n-gram probability requests, then distributing
these requests in batches to dedicated LM servers,
and waiting for the resulting probabilities, before
accessing them to score chart items. In order
to reduce the number of such roundtrip requests
in the chart parsing decoding algorithm used for
PSCFGs, we batch all n-gram requests for each
cell.
This single batched request per cell paradigm
requires some adaptation of the Cube-Pruning al-
gorithm. Cube-Pruning is an early pruning tech-
nique used to limit the generation of low quality
chart items during decoding. The algorithm calls
for the generation of N-Best chart items at each
cell (across all rules spanning that cell). The n-
gram LM is used to score each generated item,
driving the N-Best search algorithm of Huang and
Chiang (2005) toward items that score well from
a translation model and language model perspec-
tive. In order to accomodate batched asynchronous
LM requests, we queue n-gram requests for the top
N*K chart items without the n-gram LM where
K=100. We then generate the top N chart items
with the n-gram LM once these probabilties are
available. Chart items attempted to be generated
during Cube-Pruning that would require LM prob-
abilities of n-grams not in the queued set are dis-
carded. While discarding these items could lead
to search errors, in practice they tend to be poorly
performing items that do not affect final translation
quality.
3.2 PSCFG Minimal-State Recombination
To effectively compare PSCFG approaches to
state-of-the-art phrase-based systems, we must be
able to use high order n-gram LMs during PSCFG
decoding, but as shown in Chiang (2007), the
number of chart items generated during decoding
grows exponentially in the the order of the n-gram
LM. Maintaining full n?1 word left and right his-
tories for each chart item (required to correctly se-
lect the argmax derivation when considering a n-
gram LM features) is prohibitive for n > 3.
We note however, that the full n ? 1 left and
right word histories are unneccesary to safely com-
pare two competing chart items. Rather, given
the sparsity of high order n-gram LMs, we only
need to consider those histories that can actually
be found in the n-gram LM. This allows signifi-
cantly more chart items to be recombined during
decoding, without additional search error. The n-
gram LM implementation described in Brants et
al. (2007) indicates when a particular n-gram is
not found in the model, and returns a shortened
n-gram or (?state?) that represents this shortened
condition. We use this state to identify the left and
right chart item histories, thus reducing the number
of equivalence classes per cell.
Following Venugopal et al (2007), we also cal-
culate an estimate for the quality of each chart
item?s left state based on the words represented
within the state (since we cannot know the tar-
get words that might precede this item in the fi-
nal translation). This estimate is only used during
Cube-Pruning to limit the number of chart items
generated.
The extensions above allows us to experiment
with the same order of n-gram LMs used in state-
of-the-art phrase based systems. While experi-
ments in this work include up to 5-gram mod-
els, we have succesfully run these PSCFG systems
with higher order n-gram LM models as well.
4 Experiments
4.1 Chinese-English and Arabic-English
We report experiments on three data configura-
tions. The first configuration (Full) uses all the
data (both bilingual and monolingual) data avail-
able for the NIST 2008 large track translation
task. The parallel training data comprises of 9.1M
sentence pairs (223M Arabic words, 236M En-
glish words) for Arabic-English and 15.4M sen-
tence pairs (295M Chinese Words, 336M English
words) for Chinese-English. This configuration
(for both Chinese-English and Arabic-English) in-
cludes three 5-gram LMs trained on the target side
of the parallel data (549M tokens, 448M 1..5-
grams), the LDC Gigaword corpus (3.7B tokens,
2.9B 1..5-grams) and the Web 1T 5-Gram Cor-
pus (1T tokens, 3.8B 1..5-grams). The second
configuration (TargetLM) uses a single language
model trained only on the target side of the paral-
lel training text to compare approaches with a rela-
tively weaker n-gram LM. The third configuration
is a simulation of a low data scenario (10%TM),
where only 10% of the bilingual training data is
used, with the language model from the TargetLM
configuration. Translation quality is automatically
evaluated by the IBM-BLEU metric (Papineni et
al., 2002) (case-sensitive, using length of the clos-
est reference translation) on the following publicly
1148
Ch.-En. System \%BLEU Dev (MT04) MT02 MT03 MT05 MT06 MT08 TstAvg
FULL
Phraseb. reo=4 37.5 38.0 38.9 36.5 32.2 26.2 34.4
Phraseb. reo=7 40.2 40.3 41.1 38.5 34.6 27.7 36.5
Phraseb. reo=12 41.3* 41.0 41.8 39.4 35.2 27.9 37.0
Hier. 41.6* 40.9 42.5 40.3 36.5 28.7 37.8
SAMT 41.9* 41.0 43.0 40.6 36.5 29.2 38.1
TARGET-LM
Phraseb. reo=4 35.9* 36.0 36.0 33.5 30.2 24.6 32.1
Phraseb. reo=7 38.3* 38.3 38.6 35.8 31.8 25.8 34.1
Phraseb. reo=12 39.0* 38.7 38.9 36.4 33.1 25.9 34.6
Hier. 38.1* 37.8 38.3 36.0 33.5 26.5 34.4
SAMT 39.9* 39.8 40.1 36.6 34.0 26.9 35.5
TARGET-LM, 10%TM
Phraseb. reo=12 36.4* 35.8 35.3 33.5 29.9 22.9 31.5
Hier. 36.4* 36.5 36.3 33.8 31.5 23.9 32.4
SAMT 36.5* 36.1 35.8 33.7 31.2 23.8 32.1
Ar.-En. System \%BLEU Dev (MT04) MT02 MT03 MT05 MT06 MT08 TstAvg
FULL
Phraseb. reo=4 51.7 64.3 54.5 57.8 45.9 44.2 53.3
Phraseb. reo=7 51.7* 64.5 54.3 58.2 45.9 44.0 53.4
Phraseb. reo=9 51.7 64.3 54.4 58.3 45.9 44.0 53.4
Hier. 52.0* 64.4 53.5 57.5 45.5 44.1 53.0
SAMT 52.5* 63.9 54.2 57.5 45.5 44.9 53.2
TARGET-LM
Phraseb. reo=4 49.3 61.3 51.4 53.0 42.6 40.2 49.7
Phraseb. reo=7 49.6* 61.5 51.9 53.2 42.8 40.1 49.9
Phraseb. reo=9 49.6 61.5 52.0 53.4 42.8 40.1 50.0
Hier. 49.1* 60.5 51.0 53.5 42.0 40.0 49.4
SAMT 48.3* 59.5 50.0 51.9 41.0 39.1 48.3
TARGET-LM, 10%TM
Phraseb. reo=7 47.7* 59.4 50.1 51.5 40.5 37.6 47.8
Hier. 46.7* 58.2 48.8 50.6 39.5 37.4 46.9
SAMT 45.9* 57.6 48.7 50.7 40.0 37.3 46.9
Table 1: Results (% case-sensitive IBM-BLEU) for Ch-En and Ar-En NIST-large. Dev. scores with * indicate that the param-
eters of the decoder were MER-tuned for this configuration and also used in the corresponding non-marked configurations.
available NIST test corpora: MT02, MT03, MT05,
MT06, MT08. We used the NIST MT04 corpus
as development set to train the model parameters
?. All of the systems were evaluated based on the
argmax decision rule. For the purposes of stable
comparison across multiple test sets, we addition-
ally report a TstAvg score which is the average of
all test set scores.
2
Table 1 shows results comparing phrase-based,
hierarchical and SAMT systems on the Chinese-
English and Arabic-English large-track NIST 2008
tasks. Our primary goal in Table 1 is to evaluate
the relative impact of the PSCFG methods above
the phrase-based approach, and to verify that these
improvements persist with the use of of large n-
gram LMs. We also show the impact of larger
reordering capability under the phrase-based ap-
proach, providing a fair comparison to the PSCFG
approaches.
2
We prefer this over taking the average over the aggregate
test data to avoid artificially generous BLEU scores due to
length penalty effects resulting from e.g. being too brief in a
hard test set but compensating this by over-generating in an
easy test set.
Chinese-to-English configurations: We see
consistent improvements moving from phrase-
based models to PSCFG models. This trend
holds in both LM configurations (Full and Tar-
getLM) as well as the 10%TM case, with the ex-
ception of the hierarchical system for TargetLM,
which performs slightly worse than the maximum-
reordering phrase-based system.
We vary the reordering limit ?reo? for the
phrase-based Full and TargetLM configurations
and see that Chinese-to-English translation re-
quires significant reordering to generate fluent
translations, as shown by the TstAvg difference be-
tween phrase-based reordering limited to 4 words
(34.4) and 12 words (37.0). Increasing the reorder-
ing limit beyond 12 did not yield further improve-
ment. Relative improvements over the most capa-
ble phrase-based model demonstrate that PSCFG
models are able to model reordering effects more
effectively than our phrase-based approach, even
in the presence of strong n-gram LMs (to aid the
distortion models) and comparable reordering con-
straints.
1149
Our results with hierarchical rules are consis-
tent with those reported in Chiang (2007), where
the hierarchical system uses a reordering limit of
10 (implicit in the maximum length of the initial
phrase pairs used for the construction of the rules,
and the decoder?s maximum source span length,
above which only the glue rule is applied) and is
compared to a phrase-based system with a reorder-
ing limit of 7.
Arabic-to-English configurations: Neither the
hierarchical nor the SAMT system show consis-
tent improvements over the phrase-based baseline,
outperforming the baseline on some test sets, but
underperforming on others. We believe this is due
to the lack of sufficient reordering phenomena be-
tween the two languages, as evident by the mini-
mal TstAvg improvement the phrase-based system
can achieve when increasing the reordering limit
from 4 words (53.3) to 9 words (53.4).
N-Gram LMs: The impact of using addi-
tional language models in configuration Full in-
stead of only a target-side LM (configuration Tar-
getLM) is clear; the phrase-based system improves
the TstAvg score from 34.6 to 37.0 for Chinese-
English and from 50.0 to 53.4 for Arabic-English.
Interestingly, the hierarchical system and SAMT
benefit from the additional LMs to the same extent,
and retain their relative improvement compared to
the phrase-based system for Chinese-English.
Expressiveness: In order to evaluate how much
of the improvement is due to the relatively weaker
expressiveness of the phrase-based model, we tried
to regenerate translations produced by the hierar-
chical system with the phrase-based decoder by
limiting the phrases applied during decoding to
those matching the desired translation (?forced
translation?). By forcing the phrase-based system
to follow decoding hypotheses consistent with a
specific target output, we can determine whether
the phrase-based system could possibly generate
this output. We used the Chinese-to-English NIST
MT06 test (1664 sentences) set for this experi-
ment. Out of the hierarchical system?s translations,
1466 (88%) were generable by the phrase-based
system. The relevant part of a sentence for which
the hierarchical translation was not phrase-based
generable is shown in Figure 1. The reason for the
failure to generate the translation is rather unspec-
tacular: While the hierarchical system is able to
delete the Chinese word meaning ?already? using
the rule spanning [27-28], which it learned by gen-
eralizing a training phrase pair in which ?already?
was not explicitly represented in the target side, the
phrase-based system has to account for this Chi-
nese word either directly or in a phrase combining
the previous word (Chinese for ?epidemic?) or fol-
lowing word (Chinese for ?outbreak?).
Out of the generable forced translations, 1221
(83%) had a higher cost than the phrase-based sys-
tem?s preferred output; in other words, the fact
that the phrase-based system does not prefer these
forced translations is mainly inherent in the model
rather than due to search errors.
These results indicate that a phrase-based sys-
tem with sufficiently powerful reordering features
and LM might be able to narrow the gap to a hier-
archical system.
System \ %BLEU Dev MT08
Phr.b. reo=4 12.8 18.1
Phr.b. reo=7 14.2 19.9
Phr.b. reo=10 14.8* 20.2
Phr.b. reo=12 15.0 20.1
Hier. 16.0* 22.1
SAMT 16.1* 22.6
Table 2: Translation quality (% case-sensitive IBM-BLEU)
for Urdu-English NIST-large. We mark dev. scores with *
to indicate that the parameters of the corresponding decoder
were MER-tuned for this configuration.
4.2 Urdu-English
Table 2 shows results comparing phrase-based,
hierarchical and SAMT system on the Urdu-
English large-track NIST 2008 task. Systems were
trained on the bilingual data provided by the NIST
competition (207K sentence pairs; 2.2M Urdu
words / 2.1M English words) and used a n-gram
LM estimated from the English side of the parallel
data (4M 1..5-grams). We see clear improvements
moving from phrase-based to hierarchy, and addi-
tional improvements from hierarchy to syntax. As
with Chinese-to-English, longer-distance reorder-
ing plays an important role when translating from
Urdu to English (the phrase-based system is able
to improve the test score from 18.1 to 20.2), and
PSCFGs seem to be able to take this reordering
better into account than the phrasal distance-based
and lexical-reordering models.
4.3 Are all rules important?
One might assume that only a few hierarchical
rules, expressing reordering phenomena based on
common words such as prepositions, are sufficient
to obtain the desired gain in translation quality
1150
Figure 1: Example from NIST MT06 for which the hierarchical system?s first best hypothesis was not generable by the phrase-
based system. The hierarchical system?s decoding parse tree contains the translation in its leaves in infix order (shaded). Each
non-leaf node denotes an applied PSCFG rule of the form: [Spanned-source-positions:Left-hand-side->source/target]
Ch.-En. System \%BLEU Dev (MT04) MT02 MT03 MT05 MT06 MT08 TstAvg
Phraseb. 41.3* 41.0 41.8 39.4 35.2 27.9 37.0
Hier. default (mincount=3) 41.6* 40.9 42.5 40.3 36.5 28.7 37.8
Hier. mincount=4 41.4 41.0 42.5 40.4 36.1 28.4 37.7
Hier. mincount=8 41.0 41.0 42.0 40.5 35.7 27.8 37.4
Hier. mincount=16 40.7 40.3 41.5 40.0 35.2 27.8 37.0
Hier. mincount=32 40.4 40.0 41.5 39.5 34.8 27.5 36.6
Hier. mincount=64 39.8 40.0 40.9 39.1 34.6 27.3 36.4
Hier. mincount=128 39.4 39.8 40.3 38.7 34.0 26.6 35.9
Hier. 1NT 40.1* 39.8 41.1 39.1 35.1 28.1 36.6
Urdu-En. System \%BLEU Dev MT08
Phraseb. 15.0* 20.1
Hier. default (mincount=2) 16.0* 22.1
Hier. mincount=4 15.7 22.0
Hier. mincount=8 15.4 21.5
Hier. mincount=16 15.1 21.3
Hier. mincount=32 14.9 20.7
Hier. mincount=64 14.6 20.1
Hier. mincount=128 14.4 19.6
Hier. 1NT 15.3* 20.8
Table 3: Translation quality (% case-sensitive IBM-BLEU) for Chinese-English and Urdu-English NIST-large when restricting
the hierarchical rules. We mark dev. scores with * to indicate that the parameters of the corresponding decoder were MER-tuned
for this configuration.
over a phrase-based system. Limiting the number
of rules used could reduce search errors caused by
spurious ambiguity during decoding. Potentially,
hierarchical rules based on rare phrases may not
be needed, as these phrase pairs can be substituted
into the nonterminal spots of more general and
more frequently encountered hierarchical rules.
As Table 3 shows, this is not the case. In
these experiments for Hier., we retained all non-
hierarchical rules (i.e., phrase pairs) but removed
hierarchical rules below a threshold ?mincount?.
Increasing mincount to 16 (Chinese-English) or 64
(Urdu-English), respectively, already deteriorates
performance to the level of the phrase-based sys-
tem, demonstrating that the highly parameterized
reordering model implicit in using more rules does
result in benefits. This immediate reduction in
translation quality when removing rare rules can
be explained by the following effect. Unlike in
a phrase-based system, where any phrase can po-
tentially be reordered, rules in the PSCFG must
compose to generate sub-translations that can be
reordered. Removing rare rules, even those that
are highly lexicalized and do not perform any re-
ordering (but still include nonterminal symbols),
increases the likelihood that the glue rule is applied
simply concatenating span translations without re-
ordering.
Removing hierarchical rules occurring at most
twice (Chinese-English) or once (Urdu-English),
respectively, did not impact performance, and led
to a significant decrease in rule table size and de-
coding speed.
We also investigate the relative impact of the
1151
rules with two nonterminals, over using rules with
a single nonterminal. Using two nonterminals al-
lows more lexically specific reordering patterns at
the cost of decoding runtime. Configuration ?Hier.
1NT? represents a hierarchical system in which
only rules with at most one nonterminal pair are
extracted instead of two as in Configuration ?Hier.
default?. The resulting test set score drop is more
than one BLEU point for both Chinese-to-English
and Urdu-to-English.
5 Conclusion
In this work we investigated the value of PSCFG
approaches built upon state-of-the-art phrase-
based systems. Our experiments show that PSCFG
approaches can yield substantial benefits for lan-
guage pairs that are sufficiently non-monotonic.
Suprisingly, the gap (or non-gap) between phrase-
based and PSCFG performance for a given lan-
guage pair seems to be consistent across small and
large data scenarios, and for weak and strong lan-
guage models alike. In sufficiently non-monotonic
languages, the relative improvements of phrase-
based systems persist when compared against a
state-of-the art phrase-based system that is capable
of equally long reordering operations modeled by
a lexicalized distortion model and a strong n-gram
language model. We hope that this work addresses
several of the important questions that the research
community has regarding the impact and value of
these PSCFG approaches.
Acknowledgments
We thank Richard Zens and the anonymous re-
viewers for their useful comments and sugges-
tions.
References
Brants, Thorsten, Ashok C. Popat, Peng Xu, Franz J.
Och, and Jeffrey Dean. 2007. Large language mod-
els in machine translation. In Proc. of EMNLP-
CoNLL.
Charniak, Eugene. 2000. A maximum entropy-
inspired parser. In Proc. of HLT/NAACL.
Chiang, David. 2005. A hierarchical phrase-based
model for statistical machine translation. In Proc.
of ACL.
Chiang, David. 2007. Hierarchical phrase based trans-
lation. Computational Linguistics, 33(2):201?228.
DeNeefe, Steve, Kevin Knight, Wei Wang, and Daniel
Marcu. 2007. What can syntax-based MT learn
from phrase-based MT? In Proc. of EMNLP-
CoNLL.
Huang, Liang and David Chiang. 2005. Better k-best
parsing. In Proc. of IWPT.
Huang, Liang and David Chiang. 2007. Forest rescor-
ing: Faster decoding with integrated language mod-
els. In Proc. of ACL.
Koehn, Philipp, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
of HLT/NAACL.
Koehn, Philipp, Franz Josef Och, and Daniel Marcu.
2004. Pharaoh: A beam search decoder for phrase-
base statistical machine translation models. In Proc.
of AMTA.
Marcu, Daniel, Wei Wang, Abdessamad Echihabi,
and Kevin Knight. 2006. SPMT: Statistical Ma-
chine Translation with Syntactified Target Language
Phrases. In Proc. of EMNLP.
Och, Franz and Hermann Ney. 2004. The alignment
template approach to statistical machine translation.
Computational Linguistics, 30(4):417?449.
Och, Franz Josef, Christoph Tillmann, and Hermann
Ney. 1999. Improved alignment models for statis-
tical machine translation. In Proc. of EMNLP.
Och, Franz Josef. 2003. Minimum error rate training
in statistical machine translation. In Proc. of ACL.
Papineni, Kishore, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proc. of ACL.
Steedman, Mark. 1999. Alternative quantifier scope in
CCG. In Proc. of ACL.
Venugopal, Ashish, Andreas Zollmann, and Vogel
Stephan. 2007. An efficient two-pass approach to
synchronous-CFG driven statistical MT. In Proc. of
HLT/NAACL.
Zens, Richard and Hermann Ney. 2006. Discrimina-
tive reordering models for statistical machine trans-
lation. In Proc. of the Workshop on Statistical Ma-
chine Translation, HLT/NAACL.
Zollmann, Andreas and Ashish Venugopal. 2006. Syn-
tax augmented machine translation via chart pars-
ing. In Proc. of the Workshop on Statistical Machine
Translation, HLT/NAACL.
1152
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1?11,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
A Word-Class Approach to Labeling PSCFG Rules for Machine Translation
Andreas Zollmann and Stephan Vogel
Language Technologies Institute
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{zollmann,vogel+}@cs.cmu.edu
Abstract
In this work we propose methods to label
probabilistic synchronous context-free gram-
mar (PSCFG) rules using only word tags,
generated by either part-of-speech analysis
or unsupervised word class induction. The
proposals range from simple tag-combination
schemes to a phrase clustering model that can
incorporate an arbitrary number of features.
Our models improve translation quality over
the single generic label approach of Chiang
(2005) and perform on par with the syntacti-
cally motivated approach from Zollmann and
Venugopal (2006) on the NIST large Chinese-
to-English translation task. These results per-
sist when using automatically learned word
tags, suggesting broad applicability of our
technique across diverse language pairs for
which syntactic resources are not available.
1 Introduction
The Probabilistic Synchronous Context Free Gram-
mar (PSCFG) formalism suggests an intuitive ap-
proach to model the long-distance and lexically sen-
sitive reordering phenomena that often occur across
language pairs considered for statistical machine
translation. As in monolingual parsing, nonterminal
symbols in translation rules are used to generalize
beyond purely lexical operations. Labels on these
nonterminal symbols are often used to enforce syn-
tactic constraints in the generation of bilingual sen-
tences and imply conditional independence assump-
tions in the translation model. Several techniques
have been recently proposed to automatically iden-
tify and estimate parameters for PSCFGs (or related
synchronous grammars) from parallel corpora (Gal-
ley et al, 2004; Chiang, 2005; Zollmann and Venu-
gopal, 2006; Liu et al, 2006; Marcu et al, 2006).
While all of these techniques rely on word-
alignments to suggest lexical relationships, they dif-
fer in the way in which they assign labels to non-
terminal symbols of PSCFG rules. Chiang (2005)
describes a procedure to extract PSCFG rules from
word-aligned (Brown et al, 1993) corpora, where
all nonterminals share the same generic label X . In
Galley et al (2004) and Marcu et al (2006), tar-
get language parse trees are used to identify rules
and label their nonterminal symbols, while Liu et al
(2006) use source language parse trees instead. Zoll-
mann and Venugopal (2006) directly extend the rule
extraction procedure from Chiang (2005) to heuristi-
cally label any phrase pair based on target language
parse trees. Label-based approaches have resulted
in improvements in translation quality over the sin-
gleX label approach (Zollmann et al, 2008; Mi and
Huang, 2008); however, all the works cited here rely
on stochastic parsers that have been trained on man-
ually created syntactic treebanks. These treebanks
are difficult and expensive to produce and exist for a
limited set of languages only.
In this work, we propose a labeling approach that
is based merely on part-of-speech analysis of the
source or target language (or even both). To-
wards the ultimate goal of building end-to-end ma-
chine translation systems without any human anno-
tations, we also experiment with automatically in-
ferred word classes using distributional clustering
(Kneser and Ney, 1993). Since the number of classes
is a parameter of the clustering method and the re-
sulting nonterminal size of our grammar is a func-
tion of the number of word classes, the PSCFG
grammar complexity can be adjusted to the specific
translation task at hand.
Finally, we introduce a more flexible labeling ap-
proach based on K-means clustering, which allows
1
the incorporation of an arbitrary number of word-
class based features, including phrasal contexts, can
make use of multiple tagging schemes, and also al-
lows non-class features such as phrase sizes.
2 PSCFG-based translation
In this work we experiment with PSCFGs that have
been automatically learned from word-aligned par-
allel corpora. PSCFGs are defined by a source ter-
minal set (source vocabulary) TS , a target terminal
set (target vocabulary) TT , a shared nonterminal set
N and rules of the form: A? ??, ?,w? where
? A ? N is a labeled nonterminal referred to as the
left-hand-side of the rule,
? ? ? (N ? TS)? is the source side of the rule,
? ? ? (N ? TT )? is the target side of the rule,
? w ? [0,?) is a non-negative real-valued weight
assigned to the rule; in our model,w is the product
of features ?i raised to the power of weight ?i.
Chiang (2005) learns a single-nonterminal PSCFG
from a bilingual corpus by first identifying initial
phrase pairs using the technique from Koehn et al
(2003), and then performing a generalization opera-
tion to generate phrase pairs with gaps, which can be
viewed as PSCFG rules with generic ?X? nontermi-
nal left-hand-sides and substitution sites. Bilingual
features ?i that judge the quality of each rule are es-
timated based on rule extraction frequency counts.
3 Hard rule labeling from word classes
We now describe a simple method of inducing a
multi-nonterminal PSCFG from a parallel corpus
with word-tagged target side sentences. The same
procedure can straightforwardly be applied to a cor-
pus with tagged source side sentences. We use the
simple term ?tag? to stand for any kind of word-level
analysis?a syntactic, statistical, or other means of
grouping word types or tokens into classes, possibly
based on their position and context in the sentence,
POS tagging being the most obvious example.
As in Chiang?s hierarchical system, we rely on
an external phrase-extraction procedure such as the
one of Koehn et al (2003) to provide us with a set
of phrase pairs for each sentence pair in the train-
ing corpus, annotated with their respective start and
end positions in the source and target sentences.
Let f = f1 ? ? ? fm be the current source sentence,
e = e1 ? ? ? en the current target sentence, and t =
t1 ? ? ? tn its corresponding target tag sequence. We
convert each extracted phrase pair, represented by
its source span ?i, j? and target span ?k, `?, into an
initial rule
tk-t` ? fi ? ? ? fj | ek ? ? ? e`
by assigning it a nonterminal ?tk-t`? constructed by
combining the tag of the target phrase?s left-most
word with the tag of its right-most word.
The creation of complex rules based on all initial
rules obtained from the current sentence now pro-
ceeds just as in Chiang?s model.
Consider the target-tagged example sentence pair:
Ich habe ihn gesehen | I/PRP saw/VBD him/PRP
Then (depending on the extracted phrase pairs), the
resulting initial rules could be:
1: PRP-PRP? Ich | I
2: PRP-PRP? ihn | him
3: VBD-VBD? gesehen | saw
4: VBD-PRP? habe ihn gesehen | saw him
5: PRP-PRP? Ich habe ihn gesehen | I saw him
Now, by abstracting-out initial rule 2 from initial
rule 4, we obtain the complex rule:
VBD-PRP? habe PRP-PRP1 gesehen | saw PRP-PRP1
Intuitively, the labeling of initial rules with tags
marking the boundary of their target sides results in
complex rules whose nonterminal occurrences im-
pose weak syntactic constraints on the rules eligi-
ble for substitution in a PSCFG derivation: The left
and right boundary word tags of the inserted rule?s
target side have to match the respective boundary
word tags of the phrase pair that was replaced by
a nonterminal when the complex rule was created
from a training sentence pair. Since consecutive
words within a rule stem from consecutive words in
the training corpus and thus are already consistent,
the boundary word tags are more informative than
tags of words between the boundaries for the task
of combining different rules in a derivation, and are
therefore a more appropriate choice for the creation
of grammar labels than tags of inside words.
Accounting for phrase size A drawback of the
current approach is that a single-word rule such as
PRP-PRP? Ich | I
2
can have the same left-hand-side nonterminal as a
long rule with identical left and right boundary tags,
such as (when using target-side tags):
PRP-PRP? Ich habe ihn gesehen | I saw him
We therefore introduce a means of distinguishing
between one-word, two-word, and multiple-word
phrases as follows: Each one-word phrase with tag
T simply receives the label T , instead of T -T . Two-
word phrases with tag sequence T1T2 are labeled
T1-T2 as before. Phrases of length greater two with
tag sequence T1 ? ? ?Tn are labeled T1..Tn to denote
that tags were omitted from the phrase?s tag se-
quence. The resulting number of grammar nonter-
minals based on a tag vocabulary of size t is thus
given by 2t2 + t.
An alternative way of accounting for phrase size
is presented by Chiang et al (2008), who intro-
duce structural distortion features into a hierarchi-
cal phrase-based model, aimed at modeling nonter-
minal reordering given source span length. Our
approach instead uses distinct grammar rules and
labels to discriminate phrase size, with the advan-
tage of enabling all translation models to estimate
distinct weights for distinct size classes and avoid-
ing the need of additional models in the log-linear
framework; however, the increase in the number of
labels and thus grammar rules decreases the relia-
bility of estimated models for rare events due to in-
creased data sparseness.
Extension to a bilingually tagged corpus While
the availability of syntactic annotations for both
source and target language is unlikely in most trans-
lation scenarios, some form of word tags, be it part-
of-speech tags or learned word clusters (cf. Sec-
tion 3) might be available on both sides. In this case,
our grammar extraction procedure can be easily ex-
tended to impose both source and target constraints
on the eligible substitutions simultaneously.
Let Nf be the nonterminal label that would be
assigned to a given initial rule when utilizing the
source-side tag sequence, and Ne the assigned la-
bel according to the target-side tag sequence. Then
our bilingual tag-based model assigns ?Nf + Ne?
to the initial rule. The extraction of complex rules
proceeds as before. The number of nonterminals
in this model, based on a source tag vocabulary of
size s and a target tag vocabulary of size t, is thus
given by s2t2 for the regular labeling method and
(2s2 + s)(2t2 + t) when accounting for phrase size.
Consider again our example sentence pair (now
also annotated with source-side part-of-speech tags):
Ich/PRP habe/AUX ihn/PRP gesehen/VBN
I/PRP saw/VBD him/PRP
Given the same phrase extraction method as before,
the resulting initial rules for our bilingual model,
when also accounting for phrase size, are as follows:
1: PRP+PRP? Ich | I
2: PRP+PRP? ihn | him
3: VBN+VBD? gesehen | saw
4: AUX..VBN+VBD-PRP ? habe ihn
gesehen | saw him
5: PRP..VBN+PRP..PRP ? Ich habe ihn
gesehen | I saw him
Abstracting-out rule 2 from rule 4, for instance,
leads to the complex rule:
AUX..VBN+VBD-PRP ? habe PRP+PRP1
gesehen | saw PRP+PRP1
Unsupervised word class assignment by cluster-
ing As an alternative to POS tags, we experiment
with unsupervised word clustering methods based
on the exchange algorithm (Kneser and Ney, 1993).
Its objective function is maximizing the likelihood
n?
i=1
P (wi|w1, . . . , wi?1)
of the training data w = w1, . . . , wn given a par-
tially class-based bigram model of the form
P (wi|w1, . . . , wi?1) ? p(c(wi)|wi?1) ?p(wi|c(wi))
where c : V ? {1, . . . , N} maps a word (type, not
token) w to its class c(w), V is the vocabulary, and
N the fixed number of classes, which has to be cho-
sen a priori. We use the publicly available imple-
mentation MKCLS (Och, 1999) to train this model.
As training data we use the respective side of the
parallel training data for the translation system.
We also experiment with the extension of this
model by Clark (2003), who incorporated morpho-
logical information by imposing a Bayesian prior
on the class mapping c, based on N individual dis-
tributions over strings, one for each word class.
Each such distribution is a character-based hidden
Markov model, thus encouraging the grouping of
morphologically similar words into the same class.
3
4 Clustering phrase pairs directly using
the K-means algorithm
Even though we have only made use of the first and
last words? classes in the labeling methods described
so far, the number of resulting grammar nontermi-
nals quickly explodes. Using a scheme based on
source and target phrases with accounting for phrase
size, with 36 word classes (the size of the Penn En-
glish POS tag set) for both languages, yields a gram-
mar with (36+2?362)2 = 6.9m nonterminal labels.
Quite plausibly, phrase labeling should be in-
formed by more than just the classes of the first and
last words of the phrase. Taking phrase context into
account, for example, can aid the learning of syn-
tactic properties: a phrase beginning with a deter-
miner and ending with a noun, with a verb as right
context, is more likely to be a noun phrase than the
same phrase with another noun as right context. In
the current scheme, there is no way of distinguish-
ing between these two cases. Similarly, it is con-
ceivable that using non-boundary words inside the
phrase might aid the labeling process.
When relying on unsupervised learning of the
word classes, we are forced to chose a fixed num-
ber of classes. A smaller number of word clusters
will result in smaller number of grammar nonter-
minals, and thus more reliable feature estimation,
while a larger number has the potential to discover
more subtle syntactic properties. Using multiple
word clusterings simultaneously, each based on a
different number of classes, could turn this global,
hard trade-off into a local, soft one, informed by the
number of phrase pair instances available for a given
granularity.
Lastly, our method of accounting for phrase size
is somewhat displeasing: While there is a hard par-
titioning of one-word and two-word phrases, no dis-
tinction is made between phrases of length greater
than two. Marking phrase sizes greater than two
explicitly by length, however, would create many
sparse, low-frequency rules, and one of the strengths
of PSCFG-based translation is the ability to sub-
stitute flexible-length spans into nonterminals of a
derivation. A partitioning where phrase size is in-
stead merely a feature informing the labeling pro-
cess seems more desirable.
We thus propose to represent each phrase pair in-
stance (including its bilingual one-word contexts) as
feature vectors, i.e., points of a vector space. We
then use these data points to partition the space into
clusters, and subsequently assign each phrase pair
instance the cluster of its corresponding feature vec-
tor as label.
The feature mapping Consider the phrase pair in-
stance
(f0)f1 ? ? ? fm(fm+1) | (e0)e1 ? ? ? en(en+1)
(where f0, fm+1, e0, en+1 are the left and right,
source and target side contexts, respectively). We
begin with the case of only a single, target-side
word class scheme (either a tagger or an unsuper-
vised word clustering/POS induction method). Let
C = {c1, . . . , cN} be its set of word classes. Fur-
ther, let c0 be a short-hand for the result of looking
up the class of a word that is out of bounds (e.g., the
left context of the first word of a sentence, or the sec-
ond word of a one-word phrase). We now map our
phrase pair instance to the real-valued vector (where
1[P ] is the indicator function defined as 1 if property
P is true, and 0 otherwise):
?
1[e1=c0], . . . ,1[e1=cN ],1[en=c0], . . . ,1[en=cN ],
?sec1[e2=c0], . . . , ?sec1[e2=cN ],
?sec1[en?1=c0], . . . , ?sec1[en?1=cN ],
?ins
?n
i=1 1[ei=c0]
n
, . . . ,
?ins
?n
i=1 1[ei=cN ]
n
,
?cntxt1[e0=c0], . . . , ?cntxt1[e0=cN ],
?cntxt1[en+1=c0], . . . , ?cntxt1[en+1=cN ],
?phrsize
?
N + 1 log10(n)
?
The ? parameters determine the influence of the dif-
ferent types of information. The elements in the first
line represent the phrase boundary word classes, the
next two lines the classes of the second and penul-
timate word, followed by a line representing the ac-
cumulated contents of the whole phrase, followed by
two lines pertaining to the context word classes. The
final element of the vector is proportional to the log-
arithm of the phrase length.1 We chose the logarithm
assuming that length deviation of syntactic phrasal
units is not constant, but proportional to the average
length. Thus, all other features being equal, the dis-
tance between a two-word and a four-word phrase is
1The
?
N + 1 factor serves to make the feature?s influence in-
dependent of the number of word classes by yielding the same
distance (under L2) as N + 1 identical copies of the feature.
4
the same as the distance between a four-word and an
eight-word phrase.
We will mainly use the Euclidean (L2) distance to
compare points for clustering purposes. Our feature
space is thus the Euclidean vector space R7N+8.
To additionally make use of source-side word
classes, we append elements analogous to the ones
above to the vector, all further multiplied by a pa-
rameter ?src that allows trading off the relevance
of source-side and target-side information. In the
same fashion, we can incorporate multiple tagging
schemes (e.g., word clusterings of different gran-
ularities) into the same feature vector. As finer-
grained schemes have more elements in the fea-
ture vector than coarser-grained ones, and thus ex-
ert more influence, we set the ? parameter for each
scheme to 1/N (where N is the number of word
classes of the scheme).
The K-means algorithm To create the clusters,
we chose the K-means algorithm (Steinhaus, 1956;
MacQueen, 1967) for both its computational effi-
ciency and ease of implementation and paralleliza-
tion. Given an initial mapping from the data points
to K clusters, the procedure alternates between (i)
computing the centroid of each cluster and (ii) re-
allocating each data point to the closest cluster cen-
troid, until convergence.
We implemented two commonly used initializa-
tion methods: Forgy and Random Partition. The
Forgy method randomly chooses K observations
from the data set and uses these as the initial means.
The Random Partition method first randomly as-
signs a cluster to each observation and then proceeds
straight to step (ii). Forgy tends to spread the ini-
tial means out, while Random Partition places all
of them close to the center of the data set. As the
resulting clusters looked similar, and Random Parti-
tion sometimes led to a high rate of empty clusters,
we settled for Forgy.
5 Experiments
We evaluate our approach by comparing translation
quality, as evaluated by the IBM-BLEU (Papineni
et al, 2002) metric on the NIST Chinese-to-English
translation task using MT04 as development set to
train the model parameters ?, and MT05, MT06 and
MT08 as test sets. Even though a key advantage
of our method is its applicability to resource-poor
languages, we used a language pair for which lin-
guistic resources are available in order to determine
how close translation performance can get to a fully
syntax-based system. Accordingly, we use Chiang?s
hierarchical phrase based translation model (Chiang,
2007) as a base line, and the syntax-augmented MT
model (Zollmann and Venugopal, 2006) as a ?target
line?, a model that would not be applicable for lan-
guage pairs without linguistic resources.
We perform PSCFG rule extraction and decoding
using the open-source ?SAMT? system (Venugopal
and Zollmann, 2009), using the provided implemen-
tations for the hierarchical and syntax-augmented
grammars. Apart from the language model, the lex-
ical, phrasal, and (for the syntax grammar) label-
conditioned features, and the rule, target word,
and glue operation counters, Venugopal and Zoll-
mann (2009) also provide both the hierarchical and
syntax-augmented grammars with a rareness penalty
1/ cnt(r), where cnt(r) is the occurrence count of
rule r in the training corpus, allowing the system to
learn penalization of low-frequency rules, as well as
three indicator features firing if the rule has one, two
unswapped, and two swapped nonterminal pairs, re-
spectively.2 Further, to mitigate badly estimated
PSCFG derivations based on low-frequency rules of
the much sparser syntax model, the syntax grammar
also contains the hierarchical grammar as a back-
bone (cf. Zollmann and Vogel (2010) for details and
empirical analysis).
We implemented our rule labeling approach
within the SAMT rule extraction pipeline, resulting
in comparable features across all systems. For all
systems, we use the bottom-up chart parsing decoder
implemented in the SAMT toolkit with a reorder-
ing limit of 15 source words, and correspondingly
extract rules from initial phrase pairs of maximum
source length 15. All rules have at most two non-
terminal symbols, which must be non-consecutive
on the source side, and rules must contain at least
one source-side terminal symbol. The beam set-
tings for the hierarchical system are 600 items per
?X? (generic rule) cell, and 600 per ?S? (glue) cell.3
Due to memory limitations, the multi-nonterminal
grammars have to be pruned more harshly: We al-
2Penalization or reward of purely-lexical rules can be indirectly
learned by trading off these features with the rule counter fea-
ture.
3For comparison, Chiang (2007) uses 30 and 15, respectively,
and further prunes items that deviate too much in score from
the best item. He extracts initial phrases of maximum length
10.
5
low 100 ?S? items, and a total of 500 non-?S? items,
but maximally 40 items per nonterminal. For all sys-
tems, we further discard non-initial rules occurring
only once.4 For the multi-nonterminal systems, we
generally further discard all non-generic non-initial
rules occurring less than 6 times, but we additionally
give results for a ?slow? version of the Syntax target-
line system and our best word class based systems,
where only single-occurrences were removed.
For parameter tuning, we use the L0-regularized
minimum-error-rate training tool provided by the
SAMT toolkit. Each system is trained separately to
adapt the parameters to its specific properties (size
of nonterminal set, grammar complexity, features
sparseness, reliance on the language model, etc.).
The parallel training data comprises of 9.6M
sentence pairs (206M Chinese and 228M English
words). The source and target language parses for
the syntax-augmented grammar, as well as the POS
tags for our POS-based grammars were generated by
the Stanford parser (Klein and Manning, 2003).
The results are given in Table 1. Results for the
Syntax system are consistent with previous results
(Zollmann et al, 2008), indicating improvements
over the hierarchical system. Our approach, using
target POS tags (?POS-tgt (no phr. s.)?), outper-
forms the hierarchical system on all three tests sets,
and gains further improvements when accounting
for phrase size (?POS-tgt?). The latter approach is
roughly on par with the corresponding Syntax sys-
tem, slightly outperforming it on average, but not
consistently across all test sets. The same is true for
the ?slow? version (?POS-tgt-slow?).
The model based on bilingually tagged training
instances (?POS-src&tgt?) does not gain further im-
provements over the merely target-based one, but
actually performs worse. We assume this is due to
the huge number of nonterminals of ?POS-src&tgt?
((2 ? 332 + 33)(2 ? 362 + 36) = 5.8M in princi-
ple) compared to ?POS-tgt? (2 ? 362 + 36 = 2628),
increasing the sparseness of the grammar and thus
leading to less reliable statistical estimates.
We also experimented with a source-tag based
model (?POS-src?). In line with previous findings
for syntax-augmented grammars (Zollmann and Vo-
gel, 2010), the source-side-based grammar does not
reach the translation quality of its target-based coun-
terpart; however, the model still outperforms the hi-
4As shown in Zollmann et al (2008), the impact of these rules
on translation quality is negligible.
erarchical system on all test sets. Further, decod-
ing is much faster than for ?POS-ext-tgt? and even
slightly faster than ?Hierarchical?. This is due to
the fact that for the source-tag based approach, a
given chart cell in the CYK decoder, represented by
a start and end position in the source sentence, al-
most uniquely determines the nonterminal any hy-
pothesis in this cell can have: Disregarding part-
of-speech tag ambiguity and phrase size accounting,
that nonterminal will be the composition of the tags
of the start and end source words spanned by that
cell. At the same time, this demonstrates that there
is hence less of a role for the nonterminal labels to
resolve translational ambiguity in the source based
model than in the target based model.
Performance of the word-clustering based mod-
els To empirically validate the unsupervised clus-
tering approaches, we first need to decide how to de-
termine the number of word classes, N . A straight-
forward approach is to run experiments and report
test set results for many different N . While this
would allow us to reliably conclude the optimal
number N , a comparison of that best-performing
clustering method to the hierarchical, syntax, and
POS systems would be tainted by the fact that N
was effectively tuned on the test sets. We there-
fore chooseN merely based on development set per-
formance. Unfortunately, variance in development
set BLEU scores tends to be higher than test set
scores, despite of SAMT MERT?s inbuilt algorithms
to overcome local optima, such as random restarts
and zeroing-out. We have noticed that using an L0-
penalized BLEU score5 as MERT?s objective on the
merged n-best lists over all iterations is more stable
and will therefore use this score to determine N .
Figure 1 (left) shows the performance of the
distributional clustering model (?Clust?) and its
morphology-sensitive extension (?Clust-morph?) ac-
cording to this score for varying values of N =
1, . . . , 36 (the number Penn treebank POS tags, used
for the ?POS? models, is 36).6 For ?Clust?, we see a
comfortably wide plateau of nearly-identical scores
from N = 7, . . . , 15. Scores for ?Clust-morph? are
lower throughout, and peak at N = 7.
Looking back at Table 1, we now compare the
clustering models chosen by the procedure above?
5Given by: BLEU?? ? |{i ? {1, . . . ,K}|?i 6= 0}|, where
?1, . . . , ?K are the feature weights and the constant ? (which
we set to 0.00001) is the regularization penalty.
6All these models account for phrase size.
6
Dev (MT04) MT05 MT06 MT08 TestAvg Time
Hierarchical 38.63 36.51 33.26 25.77 31.85 14.3
Syntax 39.39 37.09 34.01 26.53 32.54 18.1
Syntax-slow 39.69 37.56 34.66 26.93 33.05 34.6
POS-tgt (no phr. s.) 39.31 37.29 33.79 26.13 32.40 27.7
POS-tgt 39.14 37.29 33.97 26.77 32.68 19.2
POS-src 38.74 36.75 33.85 26.76 32.45 12.2
POS-src&tgt 38.78 36.71 33.65 26.52 32.29 18.8
POS-tgt-slow 39.86 37.78 34.37 27.14 33.10 44.6
Clust-7-tgt 39.24 36.74 34.00 26.93 32.56 24.3
Clust-7-morph-tgt 39.08 36.57 33.81 26.40 32.26 23.6
Clust-7-src 38.68 36.17 33.23 26.55 31.98 11.1
Clust-7-src&tgt 38.71 36.49 33.65 26.33 32.16 15.8
Clust-7-tgt-slow 39.48 37.70 34.31 27.24 33.08 45.2
kmeans-POS-src&tgt 39.11 37.23 33.92 26.80 32.65 18.5
kmeans-POS-src&tgt-L1 39.33 36.92 33.81 26.59 32.44 17.6
kmeans-POS-src&tgt-cosine 39.15 37.07 33.98 26.68 32.58 17.7
kmeans-POS-src&tgt (?ins = .5) 39.07 36.88 33.71 26.26 32.28 16.5
kmeans-Clust-7-src&tgt 39.19 36.96 34.26 26.97 32.73 19.3
kmeans-Clust-7..36-src&tgt 39.09 36.93 34.24 26.92 32.70 17.3
kmeans-POS-src&tgt-slow 39.28 37.16 34.38 27.11 32.88 36.3
kmeans-Clust-7..36-s&t-slow 39.18 37.12 34.13 27.35 32.87 34.3
Table 1: Translation quality in % case-insensitive IBM-BLEU (i.e., brevity penalty based on closest reference length)
for Chinese-English NIST-large translation tasks, comparing baseline Hierarchical and Syntax systems with POS and
clustering based approaches proposed in this work. ?TestAvg? shows the average score over the three test sets. ?Time?
is the average decoding time per sentence in seconds on one CPU.
resulting in N = 7 for the morphology-unaware
model (?Clust-7-tgt?) as well as the morphology-
aware model (?Clust-7-morph-tgt?)?to the other
systems. ?Clust-7-tgt? improves over the hierarchi-
cal base line on all three test sets and is on par
with the corresponding Syntax and POS target lines.
The same holds for the ?Clust-7-tgt-slow? version.
We also experimented with a model variant based
on seven source and seven target language clusters
(?Clust-7-src&tgt?) and a source-only labeled model
(?Clust-7-src?)?both performing worse.
Surprisingly, the morphology-sensitive cluster-
ing model (?Clust-7-morph-tgt?), while still improv-
ing over the hierarchical system, performs worse
than the morphology-unaware model. An in-
spection of the trained word clusters showed that
the model, while far superior to the morphology-
unaware model in e.g. mapping all numbers to
the same class, is overzealous in discovering mor-
phological regularities (such as the ?-ed? suffix) to
partition functionally only slightly dissimilar words
(such present-tense and past-tense verbs) into dif-
ferent classes. While these subtle distinctions make
for good partitionings when the number of clusters
is large, they appear to lead to inferior results for
our task that relies on coarse-grained partitionings
of the vocabulary. Note that there are no ?src? or
?src&tgt? systems for ?Clust-morph?, as Chinese, be-
ing a monosyllabic writing system, does not lend it-
self to morphology-sensitive clustering.
K-means clustering based models To establish
suitable values for the ? parameters and investigate
the impact of the number of clusters, we looked at
the development performance over various param-
eter combinations for a K-means model based on
source and/or target part-of-speech tags.7 As can
be seen from Figure 1 (right), our method reaches
its peak performance at around 50 clusters and then
levels off slightly. Encouragingly, in contrast to
the hard labeling procedure, K-means actually im-
proves when adding source-side information. The
optimal ratio of weighting source and target classes
is 0.5:1, corresponding to ?src = .5. Incorporat-
ing context information also helps, and does best for
?cntxt = 0.25, i.e. when giving contexts 1/4 the in-
fluence of the phrase boundary words.
7We set ?sec = .25, ?ins = 0, and ?phrsize = .5 throughout.
7
Figure 1: Left: Performance of the distributional clustering model ?Clust? and its morphology-sensitive extension
?Clust-morph? according to L0-penalized development set BLEU score for varying numbers N of word classes. For
each data point N , its corresponding n.o. nonterminals of the induced grammar is stated in parentheses.
Right: Dev. set performance of K-means for various n.o. labels and values of ?src and ?cntxt.
Entry ?kmeans-POS-src&tgt? in Table 1 shows
the test set results for the development-set best K-
means configuration (i.e., ?src = .5, ?cntxt = 0.25,
and using 500 clusters). While beating the hier-
archical baseline, it is only minimally better than
the much simpler target-based hard labeling method
?POS-tgt?. We also tried K-means variants in which
the Euclidean distance metric is replaced by the
city block distance L1 and the cosine dissimilarity,
respectively, with slightly worse outcomes. Con-
figuration ?kmeans-POS-src&tgt (?ins = .5)? in-
vestigates the incorporation of non-boundary word
tags inside the phrase. Unfortunately, these features
appear to deteriorate performance, presumably be-
cause given a fixed number of clusters, accounting
for contents inside the phrase comes at the cost of
neglect of boundary words, which are more relevant
to producing correctly reordered translations.
The two completely unsupervised systems
?kmeans-Clust-7-src&tgt? (based on 7-class
MKCLS distributional word clustering) and
?kmeans-Clust-7..36-src&tgt? (using six different
word clustering models simultaneously: all the
MKCLS models from Figure 1 (left) except for the
two-, three- and five-class models) have the best
results, outperforming the other K-means models as
well as ?Syntax? and ?POS-tgt? on average, but not
on all test sets.
Lastly, we give results for ?slow? K-means config-
urations (?kmeans-POS-src&tgt-slow? and ?kmeans-
Clust-7..36-s&t-slow?). Unfortunately (or fortu-
nately, from a pragmatic viewpoint), the models are
outperformed by the much simpler ?POS-tgt-slow?
and ?Clust-7-tgt-slow? models.
6 Related work
Hassan et al (2007) improve the statistical phrase-
based MT model by injecting supertags, lexical in-
formation such as the POS tag of the word and its
subcategorization information, into the phrase table,
resulting in generalized phrases with placeholders in
them. The supertags are also injected into the lan-
guage model. Our approach also generates phrase
labels and placeholders based on word tags (albeit
in a different manner and without the use of subcat-
egorization information), but produces PSCFG rules
for use in a parsing-based decoding system.
Unsupervised synchronous grammar induction,
apart from the contribution of Chiang (2005) dis-
cussed earlier, has been proposed by Wu (1997) for
inversion transduction grammars, but as Chiang?s
model only uses a single generic nonterminal la-
bel. Blunsom et al (2009) present a nonparamet-
ric PSCFG translation model that directly induces
a grammar from parallel sentences without the use
of or constraints from a word-alignment model, and
8
Cohn and Blunsom (2009) achieve the same for
tree-to-string grammars, with encouraging results
on small data. Our more humble approach treats
the training sentences? word alignments and phrase
pairs, obtained from external modules, as ground
truth and employs a straight-forward generalization
of Chiang?s popular rule extraction approach to la-
beled phrase pairs, resulting in a PSCFG with mul-
tiple nonterminal labels.
Our phrase pair clustering approach is similar in
spirit to the work of Lin and Wu (2009), who use K-
means to cluster (monolingual) phrases and use the
resulting clusters as features in discriminative clas-
sifiers for a named-entity-recognition and a query
classification task. Phrases are represented in terms
of their contexts, which can be more than one word
long; words within the phrase are not considered.
Further, each context contributes one dimension per
vocabulary word (not per word class as in our ap-
proach) to the feature space, allowing for the dis-
covery of subtle semantic similarities in the phrases,
but at much greater computational expense. Another
distinction is that Lin and Wu (2009) work with
phrase types instead of phrase instances, obtaining
a phrase type?s contexts by averaging the contexts
of all its phrase instances.
Nagata et al (2006) present a reordering model
for machine translation, and make use of clustered
phrase pairs to cope with data sparseness in the
model. They achieve the clustering by reducing
phrases to their head words and then applying the
MKCLS tool to these pseudo-words.
Kuhn et al (2010) cluster the phrase pairs of
an SMT phrase table based on their co-occurrence
counts and edit distances in order to arrive at seman-
tically similar phrases for the purpose of phrase table
smoothing. The clustering proceeds in a bottom-up
fashion, gradually merging similar phrases while al-
ternating back and forth between the two languages.
7 Conclusion and discussion
In this work we proposed methods of labeling phrase
pairs to create automatically learned PSCFG rules
for machine translation. Crucially, our methods only
rely on ?shallow? lexical tags, either generated by
POS taggers or by automatic clustering of words into
classes. Evaluated on a Chinese-to-English transla-
tion task, our approach improves translation qual-
ity over a popular PSCFG baseline?the hierarchi-
cal model of Chiang (2005) ?and performs on par
with the model of Zollmann and Venugopal (2006),
using heuristically generated labels from parse trees.
Using automatically obtained word clusters instead
of POS tags yields essentially the same results, thus
making our methods applicable to all languages
pairs with parallel corpora, whether syntactic re-
sources are available for them or not.
We also propose a more flexible way of obtaining
the phrase labels from word classes using K-means
clustering. While currently the simple hard-labeling
methods perform just as well, we hope that the ease
of incorporating new features into the K-means la-
beling method will spur interesting future research.
When considering the constraints and indepen-
dence relationships implied by each labeling ap-
proach, we can distinguish between approaches that
label rules differently within the context of the sen-
tence that they were extracted from, and those that
do not. The Syntax system from Zollmann and
Venugopal (2006) is at one end of this extreme. A
given target span might be labeled differently de-
pending on the syntactic analysis of the sentence
that it is a part of. On the other extreme, the clus-
tering based approach labels phrases based on the
contained words alone.8 The POS grammar repre-
sents an intermediate point on this spectrum, since
POS tags can change based on surrounding words in
the sentence; and the position of the K-means model
depends on the influence of the phrase contexts on
the clustering process. Context insensitive labeling
has the advantage that there are less alternative left-
hand-side labels for initial rules, producing gram-
mars with less rules, whose weights can be more
accurately estimated. This could explain the strong
performance of the word-clustering based labeling
approach.
All source code underlying this work is available
under the GNU Lesser General Public License as
part of the Hadoop-based ?SAMT? system at:
www.cs.cmu.edu/?zollmann/samt
Acknowledgments
We thank Jakob Uszkoreit and Ashish Venugopal for
helpful comments and suggestions and Yahoo! for
the access to the M45 supercomputing cluster.
8Note, however, that the creation of clusters itself did take the
context of the clustered words into account.
9
References
Phil Blunsom, Trevor Cohn, Chris Dyer, and Miles Os-
borne. 2009. A Gibbs sampler for phrasal syn-
chronous grammar induction. In Proceedings of ACL,
Singapore, August.
Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della
Pietra, and Robert L. Mercer. 1993. The mathemat-
ics of statistical machine translation: parameter esti-
mation. Computational Linguistics, 19(2).
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In Proceedings of the 2008
Conference on Empirical Methods in Natural Lan-
guage Processing, Honolulu, Hawaii, October.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
the Annual Meeting of the Association for Computa-
tional Linguistics (ACL).
David Chiang. 2007. Hierarchical phrase based transla-
tion. Computational Linguistics, 33(2).
Alexander Clark. 2003. Combining distributional and
morphological information for part of speech induc-
tion. In Proceedings of the European chapter of the
Association for Computational Linguistics (EACL),
pages 59?66.
Trevor Cohn and Phil Blunsom. 2009. A Bayesian model
of syntax-directed tree to string grammar induction.
In Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing (EMNLP),
Singapore.
Michael Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a translation rule?
In Proceedings of the Human Language Technology
Conference of the North American Chapter of the As-
sociation for Computational Linguistics Conference
(HLT/NAACL).
Hany Hassan, Khalil Sima?an, and Andy Way. 2007. Su-
pertagged phrase-based statistical machine translation.
In Proceedings of the 45th Annual Meeting of the As-
sociation of Computational Linguistics, Prague, Czech
Republic, June.
Dan Klein and Christoper Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of the Annual
Meeting of the Association for Computational Linguis-
tics (ACL).
Reinhard Kneser and Hermann Ney. 1993. Improved
clustering techniques for class-based statistical lan-
guage modelling. In Proceedings of the 3rd European
Conference on Speech Communication and Technol-
ogy, pages 973?976, Berlin, Germany.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceedings of
the Human Language Technology Conference of the
North American Chapter of the Association for Com-
putational Linguistics Conference (HLT/NAACL).
Roland Kuhn, Boxing Chen, George Foster, and Evan
Stratford. 2010. Phrase clustering for smoothing
TM probabilities - or, how to extract paraphrases from
phrase tables. In Proceedings of the 23rd Interna-
tional Conference on Computational Linguistics (Col-
ing 2010), pages 608?616, Beijing, China, August.
Dekang Lin and Xiaoyun Wu. 2009. Phrase clustering
for discriminative learning. In Proceedings of the 47th
Annual Meeting of the Association for Computational
Linguistics (ACL).
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-
string alignment template for statistical machine trans-
lation. In Proceedings of the 21st International Con-
ference on Computational Linguistics and the 44th
annual meeting of the Association for Computational
Linguistics.
J. B. MacQueen. 1967. Some methods for classification
and analysis of multivariate observations. In L. M. Le
Cam and J. Neyman, editors, Proc. of the fifth Berkeley
Symposium on Mathematical Statistics and Probabil-
ity, volume 1, pages 281?297. University of California
Press.
Daniel Marcu, Wei Wang, Abdessamad Echihabi, and
Kevin Knight. 2006. SPMT: Statistical machine
translation with syntactified target language phrases.
In Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP), Syd-
ney, Australia.
Haitao Mi and Liang Huang. 2008. Forest-based transla-
tion rule extraction. In Proceedings of the Conference
on Empirical Methods in Natural Language Process-
ing (EMNLP).
Masaaki Nagata, Kuniko Saito, Kazuhide Yamamoto,
and Kazuteru Ohashi. 2006. A clustered global phrase
reordering model for statistical machine translation. In
Proceedings of the 21st International Conference on
Computational Linguistics and the 44th annual meet-
ing of the Association for Computational Linguistics,
ACL-44, pages 713?720.
Franz Josef Och. 1999. An efficient method for de-
termining bilingual word classes. In Proceedings of
the European chapter of the Association for Computa-
tional Linguistics (EACL), pages 71?76.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proceedings of the
Annual Meeting of the Association for Computational
Linguistics (ACL).
Hugo Steinhaus. 1956. Sur la division des corps
mate?riels en parties. Bull. Acad. Polon. Sci. Cl. III.
4, pages 801?804.
10
Ashish Venugopal and Andreas Zollmann. 2009. Gram-
mar based statistical MT on Hadoop: An end-to-end
toolkit for large scale PSCFG based MT. The Prague
Bulletin of Mathematical Linguistics, 91:67?78.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3).
Andreas Zollmann and Ashish Venugopal. 2006. Syntax
augmented machine translation via chart parsing. In
Proceedings of the Workshop on Statistical Machine
Translation, HLT/NAACL.
Andreas Zollmann and Stephan Vogel. 2010. New
parameterizations and features for PSCFG-based ma-
chine translation. In Proceedings of the 4th Work-
shop on Syntax and Structure in Statistical Translation
(SSST), Beijing, China.
Andreas Zollmann, Ashish Venugopal, Franz J. Och, and
Jay Ponte. 2008. A systematic comparison of phrase-
based, hierarchical and syntax-augmented statistical
MT. In Proceedings of the Conference on Computa-
tional Linguistics (COLING).
11
Proceedings of SSST-4, Fourth Workshop on Syntax and Structure in Statistical Translation, pages 110?117,
COLING 2010, Beijing, August 2010.
New Parameterizations and Features for PSCFG-Based Machine
Translation
Andreas Zollmann Stephan Vogel
Language Technologies Institute
School of Computer Science
Carnegie Mellon University
{zollmann,vogel}@cs.cmu.edu
Abstract
We propose several improvements to the
hierarchical phrase-based MT model of
Chiang (2005) and its syntax-based exten-
sion by Zollmann and Venugopal (2006).
We add a source-span variance model
that, for each rule utilized in a prob-
abilistic synchronous context-free gram-
mar (PSCFG) derivation, gives a confi-
dence estimate in the rule based on the
number of source words spanned by the
rule and its substituted child rules, with
the distributions of these source span sizes
estimated during training time.
We further propose different methods of
combining hierarchical and syntax-based
PSCFG models, by merging the grammars
as well as by interpolating the translation
models.
Finally, we compare syntax-augmented
MT, which extracts rules based on target-
side syntax, to a corresponding variant
based on source-side syntax, and experi-
ment with a model extension that jointly
takes source and target syntax into ac-
count.
1 Introduction
The Probabilistic Synchronous Context Free
Grammar (PSCFG) formalism suggests an intu-
itive approach to model the long-distance and lex-
ically sensitive reordering phenomena that often
occur across language pairs considered for statis-
tical machine translation. As in monolingual pars-
ing, nonterminal symbols in translation rules are
used to generalize beyond purely lexical opera-
tions. Labels on these nonterminal symbols are
often used to enforce syntactic constraints in the
generation of bilingual sentences and imply con-
ditional independence assumptions in the statis-
tical translation model. Several techniques have
been recently proposed to automatically iden-
tify and estimate parameters for PSCFGs (or re-
lated synchronous grammars) from parallel cor-
pora (Galley et al, 2004; Chiang, 2005; Zollmann
and Venugopal, 2006; Liu et al, 2006; Marcu et
al., 2006).
In this work, we propose several improvements
to the hierarchical phrase-based MT model of
Chiang (2005) and its syntax-based extension by
Zollmann and Venugopal (2006). We add a source
span variance model that, for each rule utilized
in a probabilistic synchronous context-free gram-
mar (PSCFG) derivation, gives a confidence es-
timate in the rule based on the number of source
words spanned by the rule and its substituted child
rules, with the distributions of these source span
sizes estimated during training (i.e., rule extrac-
tion) time.
We further propose different methods of com-
bining hierarchical and syntax-based PSCFG
models, by merging the grammars as well as by
interpolating the translation models.
Finally, we compare syntax-augmented MT,
which extracts rules based on target-side syntax,
to a corresponding variant based on source-side
syntax, and experiment with a model extension
based on source and target syntax.
We evaluate the different models on the
NIST large resource Chinese-to-English transla-
tion task.
110
2 Related work
Chiang et al (2008) introduce structural dis-
tortion features into a hierarchical phrase-based
model, aimed at modeling nonterminal reordering
given source span length, by estimating for each
possible source span length ` a Bernoulli distribu-
tion p(R|`) where R takes value one if reorder-
ing takes place and zero otherwise. Maximum-
likelihood estimation of the distribution amounts
to simply counting the relative frequency of non-
terminal reorderings over all extracted rule in-
stances that incurred a substitution of span length
`. In a more fine-grained approach they add a sep-
arate binary feature ?R, `? for each combination of
reordering truth value R and span length ` (where
all ` ? 10 are merged into a single value), and
then tune the feature weights discriminatively on a
development set. Our approach differs from Chi-
ang et al (2008) in that we estimate one source
span length distribution for each substitution site
of each grammar rule, resulting in unique distri-
butions for each rule, estimated from all instances
of the rule in the training data. This enables our
model to condition reordering range on the in-
dividual rules used in a derivation, and even al-
lows to distinguish between two rules r1 and r2
that both reorder arguments with identical mean
span lengths `, but where the span lengths encoun-
tered in extracted instances of r1 are all close to `,
whereas span length instances for r2 vary widely.
Chen and Eisele (2010) propose a hypbrid ap-
proach between hierarchical phrase based MT
and a rule based MT system, reporting improve-
ment over each individual model on an English-
to-German translation task. Essentially, the rule
based system is converted to a single-nonterminal
PSCFG, and hence can be combined with the
hierarchical model, another single-nonterminal
PSCFG, by taking the union of the rule sets
and augmenting the feature vectors, adding zero-
values for rules that only exist in one of the two
grammars. We face the challenge of combining
the single-nonterminal hierarchical grammar with
a multi-nonterminal syntax-augmented grammar.
Thus one hierarchical rule typically corresponds
to many syntax-augmented rules. The SAMT sys-
tem used by Zollmann et al (2008) adds hierar-
chical rules separately to the syntax-augmented
grammar, resulting in a backbone grammar of
well-estimated hierarchical rules supporting the
sparser syntactic rules. They allow the model
preference between hierarchical and syntax rules
to be learned from development data by adding
an indicator feature to all rules, which is one
for hierarchical rules and zero for syntax rules.
However, no empirical comparison is given be-
tween the purely syntax-augmented and the hy-
brid grammar. We aim to fill this gap by experi-
menting with both models, and further refine the
hybrid approach by adding interpolated probabil-
ity models to the syntax rules.
Chiang (2010) augments a hierarchical phrase-
based MT model with binary syntax features rep-
resenting the source and target syntactic con-
stituents of a given rule?s instantiations during
training, thus taking source and target syntax
into account while avoiding the data-sparseness
and decoding-complexity problems of multi-
nonterminal PSCFG models. In our approach, the
source- and target-side syntax directly determines
the grammar, resulting in a nonterminal set de-
rived from the labels underlying the source- and
target-language treebanks.
3 PSCFG-based translation
Given a source language sentence f , statistical
machine translation defines the translation task as
selecting the most likely target translation e under
a model P (e|f), i.e.:
e?(f) = argmax
e
P (e|f) = argmax
e
m?
i=1
hi(e, f)?i
where the argmax operation denotes a search
through a structured space of translation outputs
in the target language, hi(e, f) are bilingual fea-
tures of e and f and monolingual features of
e, and weights ?i are typically trained discrim-
inatively to maximize translation quality (based
on automatic metrics) on held out data, e.g., us-
ing minimum-error-rate training (MERT) (Och,
2003).
In PSCFG-based systems, the search space is
structured by automatically extracted rules that
model both translation and re-ordering operations.
111
Most large scale systems approximate the search
above by simply searching for the most likely
derivation of rules, rather than searching for the
most likely translated output. There are efficient
algorithms to perform this search (Kasami, 1965;
Chappelier and Rajman, 1998) that have been ex-
tended to efficiently integrate n-gram language
model features (Chiang, 2007; Venugopal et al,
2007; Huang and Chiang, 2007; Zollmann et al,
2008; Petrov et al, 2008).
In this work we experiment with PSCFGs
that have been automatically learned from word-
aligned parallel corpora. PSCFGs are defined by a
source terminal set (source vocabulary) TS , a tar-
get terminal set (target vocabulary) TT , a shared
nonterminal set N and rules of the form: X ?
??, ?,w? where
? X ? N is a labeled nonterminal referred to as
the left-hand-side of the rule.
? ? ? (N ? TS)? is the source side of the rule.
? ? ? (N ? TT )? is the target side of the rule.
? w ? [0,?) is a non-negative real-valued
weight assigned to the rule; in our model, w is
the exponential function of the inner product of
features h and weights ?.
3.1 Hierarchical phrase-based MT
Building upon the success of phrase-based meth-
ods, Chiang (2005) presents a PSCFG model of
translation that uses the bilingual phrase pairs
of phrase-based MT as starting point to learn
hierarchical rules. For each training sentence
pair?s set of extracted phrase pairs, the set of in-
duced PSCFG rules can be generated as follows:
First, each phrase pair is assigned a generic X-
nonterminal as left-hand-side, making it an initial
rule. We can now recursively generalize each al-
ready obtained rule (initial or including nontermi-
nals)
N ? f1 . . . fm/e1 . . . en
for which there is an initial rule
M ? fi . . . fu/ej . . . ev
where 1 ? i < u ? m and 1 ? j < v ? n, to
obtain a new rule
N ? f i?11 Xkfmu+1/ej?11 Xkenv+1
where e.g. f i?11 is short-hand for f1 . . . fi?1, and
where k is an index for the nonterminal X that
indicates the one-to-one correspondence between
the new X tokens on the two sides (it is not in
the space of word indices like i, j, u, v,m, n). The
recursive form of this generalization operation al-
lows the generation of rules with multiple nonter-
minal pairs.
Chiang (2005) uses features analogous to the
ones used in phrase-based translation: a lan-
guage model neg-log probability, a ?rule given
source-side? neg-log-probability, a ?rule given
target-side? neg-log-probability, source- and tar-
get conditioned ?lexical? neg-log-probabilities
based on word-to-word co-occurrences (Koehn et
al., 2003), as well as rule, target word, and glue
operation counters. We follow Venugopal and
Zollmann (2009) to further add a rareness penalty,
1/ count(r)
where count(r) is the occurrence count of rule
r in the training corpus, allowing the system to
learn penalization of low-frequency rules, as well
as three indicator features firing if the rule has
one, two unswapped, and two swapped nontermi-
nal pairs, respectively.1
3.2 Syntax Augmented MT
Syntax Augmented MT (SAMT) (Zollmann and
Venugopal, 2006) extends Chiang (2005) to in-
clude nonterminal symbols from target language
phrase structure parse trees. Each target sentence
in the training corpus is parsed with a stochas-
tic parser to produce constituent labels for target
spans. Phrase pairs (extracted from a particular
sentence pair) are assigned left-hand-side nonter-
minal symbols based on the target side parse tree
constituent spans.
Phrase pairs whose target side corresponds to
a constituent span are assigned that constituent?s
label as their left-hand-side nonterminal. If the
target side of the phrase pair is not spanned by
a single constituent in the corresponding parse
tree, we use the labels of subsuming, subsumed,
and neighboring parse tree constituents to assign
1Penalization or reward of purely-lexical rules can be in-
directly learned by trading off these features with the rule
counter feature.
112
an extended label of the form C1 + C2, C1/C2,
or C2\C1 (the latter two being motivated from
the operations in combinatory categorial gram-
mar (CCG) (Steedman, 2000)), indicating that the
phrase pair?s target side spans two adjacent syn-
tactic categories (e.g., she went: NP+VB), a par-
tial syntactic category C1 missing aC2 at the right
(e.g., the great: NP/NN), or a partial C1 missing
a C2 at the left (e.g., great wall: DT\NP), respec-
tively. The label assignment is attempted in the or-
der just described, i.e., assembling labels based on
?+? concatenation of two subsumed constituents is
preferred, as smaller constituents tend to be more
accurately labeled. If no label is assignable by ei-
ther of these three methods, a default label ?FAIL?
is assigned.
In addition to the features used in hierarchical
phrase-based MT, SAMT introduces a relative-
frequency estimated probability of the rule given
its left-hand-side nonterminal.
4 Modeling Source Span Length of
PSCFG Rule Substitution Sites
Extracting a rule with k right-hand-side nonter-
minal pairs, i.e., substitution sites, (from now on
called order-k rule) by the method described in
Section 3 involves k + 1 phrase pairs: one phrase
pair used as initial rule and k phrase pairs that are
sub phrase pairs of the first and replaced by non-
terminal pairs. Conversely, during translation, ap-
plying this rule amounts to combining k hypothe-
ses from k different chart cells, each represented
by a source span and a nonterminal, to form a new
hypothesis and file it into a chart cell. Intuitively,
we want the source span lengths of these k + 1
chart cells to be close to the source side lengths of
the k+1 phrase pairs from the training corpus that
were involved in extracting the rule. Of course,
each rule generally was extracted from multiple
training corpus locations, with different involved
phrase pairs of different lengths. We therefore
model k + 1 source span length distributions for
each order-k rule in the grammar.
Ignoring the discreteness of source span length
for the sake of easier estimation, we assume the
distribution to be log-normal. This is motivated
by the fact that source span length is positive and
that we expect its deviation between instances of
the same rule to be greater for long phrase pairs
than for short ones.
We can now add k? + 1 features to the transla-
tion framework, where k? is the maximum num-
ber of PSCFG rule nonterminal pairs, in our case
two. Each feature is computed during translation
time. Ideally, it should represent the probabil-
ity of the hypothesized rule given the respective
chart cell span length. However, as each com-
peting rule underlies a different distribution, this
would require a Bayesian setting, in which priors
over distributions are specified. In this prelimi-
nary work we take a simpler approach: Based on
the rule?s span distribution, we compute the prob-
ability that a span length no likelier than the one
encountered was generated from the distribution.
This probability thus yields a confidence estimate
for the rule. More formally, let ? be the mean and
? the standard deviation of the logarithm of the
span length random variableX concerned, and let
x be the span length encountered during decoding.
Then the computed confidence estimate is given
by
P (| ln(X)? ?| ? | ln(x)? ?|)
= 2 ? Z (?(| ln(x)? ?|)/?)
where Z is the cumulative density function of the
normal distribution with mean zero and variance
one.
The confidence estimate is one if the encoun-
tered span length is equal to the mean of the dis-
tribution, and decreases as the encountered span
length deviates further from the mean. The sever-
ity of that decline is determined by the distribution
variance: the higher the variance, the less a devia-
tion from the mean is penalized.
Mean and variance of log source span length are
sufficient statistics of the log-normal distribution.
As we extract rules in a distributed fashion, we
use a straightforward parallelization of the online
algorithm of Welford (1962) and its improvement
by West (1979) to compute the sample variance
over all instances of a rule.
113
5 Merging a Hierarchical and a
Syntax-Based Model
While syntax-based grammars allow for more re-
fined statistical models and guide the search by
constraining substitution possibilitites in a gram-
mar derivation, grammar sizes tend to be much
greater than for hierarchical grammars. Therefore
the average occurrence count of a syntax rule is
much lower than that of a hierarchical rule, and
thus estimated probabilitites are less reliable.
We propose to augment the syntax-based ?rule
given source side? and ?rule given target side? dis-
tributions by hierarchical counterparts obtained by
marginalizing over the left-hand-side and right-
hand-side rule nonterminals. For example, the
hierarchical equivalent of the ?rule given source
side? probability is obtained by summing occur-
rence counts over all rules that have the same
source and target terminals and substitution posi-
tions but possibly differ in the left- and/or right-
hand side nonterminal labels, divided by the sum
of occurrence counts of all rules that have the
same source side terminals and source side substi-
tution positions. Similarly, an alternative rareness
penalty based on the combined frequency of all
rules with the same terminals and substitution po-
sitions is obtained.
Using these syntax and hierarchical features
side by side amounts to interpolation of the re-
spective probability models in log-space, with
minimum-error-rate training (MERT) determining
the optimal interpolation coefficient. We also add
respective models interpolated with coefficient .5
in probability-space as additional features to the
system.
We further experiment with adding hierarchical
rules separately to the syntax-augmented gram-
mar, as proposed in Zollmann et al (2008), with
the respective syntax-specific features set to zero.
A ?hierarchical-indicator? feature is added to all
rules, which is one for hierarchical rules and zero
for syntax rules, allowing the joint model to trade
of hierarchical against syntactic rules. During
translation, the hierarchical and syntax worlds are
bridged by glue rules, which allow monotonic
concatenation of hierarchical and syntactic partial
sentence hypotheses. We separate the glue feature
used in hierarchical and syntax-augmented trans-
lation into a glue feature that only fires when a hi-
erarchical rule is glued, and a distinct glue feature
firing when gluing a syntax-augmented rule.
6 Extension of SAMT to a bilingually
parsed corpus
Syntax-based MT models have been proposed
both based on target-side syntactic annotations
(Galley et al, 2004; Zollmann and Venugopal,
2006) as well source-side annotations (Liu et al,
2006). Syntactic annotations for both source and
target language are available for popular language
pairs such as Chinese-English. In this case, our
grammar extraction procedure can be easily ex-
tended to impose both source and target con-
straints on the eligible substitutions simultane-
ously.
Let Nf be the nonterminal label that would be
assigned to a given initial rule when utilizing the
source-side parse tree, and Ne the assigned label
according to the target-side parse. Then our bilin-
gual model assigns ?Nf + Ne? to the initial rule.
The extraction of complex rules proceeds as be-
fore. The number of nonterminals in this model,
based on a source-model label set of size s and a
target label set of size t, is thus given by st.
7 Experiments
We evaluate our approaches by comparing trans-
lation quality according to the IBM-BLEU (Pap-
ineni et al, 2002) metric on the NIST Chinese-
to-English translation task using MT04 as devel-
opment set to train the model parameters ?, and
MT05, MT06 and MT08 as test sets.
We perform PSCFG rule extraction and de-
coding using the open-source ?SAMT? system
(Venugopal and Zollmann, 2009), using the pro-
vided implementations for the hierarchical and
syntax-augmented grammars. For all systems, we
use the bottom-up chart parsing decoder imple-
mented in the SAMT toolkit with a reordering
limit of 15 source words, and correspondingly ex-
tract rules from initial phrase pairs of maximum
source length 15. All rules have at most two non-
terminal symbols, which must be non-consecutive
on the source side, and rules must contain at least
114
one source-side terminal symbol.
For parameter tuning, we use the L0-
regularized minimum-error-rate training tool pro-
vided by the SAMT toolkit.
The parallel training data comprises of 9.6M
sentence pairs (206M Chinese Words, 228M En-
glish words). The source and target language
parses for the syntax-augmented grammar were
generated by the Stanford parser (Klein and Man-
ning, 2003).
The results are given in Table 1. The source
span models (indicated by +span) achieve small
test set improvements of 0.15 BLEU points on av-
erage for the hierarchical and 0.26 BLEU points
for the syntax-augmented system, but these are
not statistically significant.
Augmenting a syntax-augmented grammar
with hierarchical features (?Syntax+hiermodels?)
results in average test set improvements of 0.5
BLEU points. These improvements are not sta-
tistically significant either, but persist across all
three test sets. This demonstrates the benefit of
more reliable feature estimation. Further aug-
menting the hierarchical rules to the grammar
(?Syntax+hiermodels+hierrules?) does not yield
additional improvements.
The use of bilingual syntactic parses (?Syn-
tax/src&tgt?) turns out detrimental to translation
quality. We assume this is due to the huge number
of nonterminals in these grammars and the great
amount of badly-estimated low-occurrence-count
rules. Perhaps merging this grammar with a regu-
lar syntax-augmented grammar could yield better
results.
We also experimented with a source-parse
based model (?Syntax/src?). While not being able
to match translation quality of its target-based
counterpart, the model still outperforms the hier-
archical system on all test sets.
8 Conclusion
We proposed several improvements to the hierar-
chical phrase-based MT model of Chiang (2005)
and its syntax-based extension by Zollmann and
Venugopal (2006). We added a source span length
model that, for each rule utilized in a probabilis-
tic synchronous context-free grammar (PSCFG)
derivation, gives a confidence estimate in the rule
based on the number of source words spanned by
the rule and its substituted child rules, resulting in
small improvements for hierarchical phrase-based
as well as syntax-augmented MT.
We further demonstrated the utility of combin-
ing hierarchical and syntax-based PSCFG models
and grammars.
Finally, we compared syntax-augmented MT,
which extracts rules based on target-side syntax,
to a corresponding variant based on source-side
syntax, showing that target syntax is more ben-
efitial, and unsuccessfully experimented with a
model extension that jointly takes source and tar-
get syntax into account.
Hierarchical phrase-based MT suffers from
spurious ambiguity: A single translation for a
given source sentence can usually be accom-
plished by many different PSCFG derivations.
This problem is exacerbated by syntax-augmented
MT with its thousands of nonterminals, and made
even worse by its joint source-and-target exten-
sion. Future research should apply the work of
Blunsom et al (2008) and Blunsom and Osborne
(2008), who marginalize over derivations to find
the most probable translation rather than the most
probable derivation, to these multi-nonterminal
grammars.
All source code underlying this work is avail-
able under the GNU Lesser General Public Li-
cense as part of the ?SAMT? system at:
www.cs.cmu.edu/?zollmann/samt
Acknowledgements
This work is in part supported by NSF un-
der the Cluster Exploratory program (grant NSF
0844507), and in part by the US DARPA GALE
program. Any opinions, findings, and conclusions
or recommendations expressed in this material are
those of the authors and do not necessarily reflect
the views of NSF or DARPA.
References
Blunsom, Phil and Miles Osborne. 2008. Probabilistic
inference for machine translation. In EMNLP ?08:
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, pages 215?
223, Morristown, NJ, USA. Association for Com-
putational Linguistics.
115
Dev (MT04) MT05 MT06 MT08 TestAvg Time
Hierarchical 38.63 36.51 33.26 25.77 31.85 14.3
Hier+span 39.03 36.44 33.29 26.26 32.00 16.7
Syntax 39.17 37.17 33.87 26.81 32.62 59
Syntax+hiermodels 39.61 37.74 34.30 27.30 33.11 68.4
Syntax+hiermodels+hierrules 39.69 37.56 34.66 26.93 33.05 34.6
Syntax+span+hiermodels+hierrules 39.81 38.02 34.50 27.41 33.31 39.6
Syntax/src+span+hiermodels+hierrules 39.62 37.25 33.99 26.44 32.56 20.1
Syntax/src&tgt+span+hiermodels+hierrules 39.15 36.92 33.70 26.24 32.29 17.5
Table 1: Translation quality in % case-insensitive IBM-BLEU (i.e., brevity penalty based on closest reference length) for
different systems on Chinese-English NIST-large translation tasks. ?TestAvg? shows the average score over the three test sets.
?Time? is the average decoding time per sentence in seconds on one CPU.
Blunsom, Phil, Trevor Cohn, and Miles Osborne.
2008. A discriminative latent variable model for
statistical machine translation. In Proceedings of
the Annual Meeting of the Association for Compu-
tational Linguistics (ACL).
Brown, Peter F., Vincent J. Della Pietra, Stephen
A. Della Pietra, and Robert L. Mercer. 1993.
The mathematics of statistical machine translation:
parameter estimation. Computational Linguistics,
19(2).
Chappelier, J.C. and M. Rajman. 1998. A general-
ized CYK algorithm for parsing stochastic CFG. In
Proceedings of Tabulation in Parsing and Deduction
(TAPD), pages 133?137, Paris.
Chen, Yu and Andreas Eisele. 2010. Hierarchical hy-
brid translation between english and german. In
Hansen, Viggo and Francois Yvon, editors, Pro-
ceedings of the 14th Annual Conference of the Eu-
ropean Association for Machine Translation, pages
90?97. EAMT, EAMT, 5.
Chiang, David, Yuval Marton, and Philip Resnik.
2008. Online large-margin training of syntactic and
structural translation features. In Proceedings of
the 2008 Conference on Empirical Methods in Nat-
ural Language Processing, pages 224?233, Hon-
olulu, Hawaii, October. Association for Computa-
tional Linguistics.
Chiang, David. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of the Annual Meeting of the Association
for Computational Linguistics (ACL).
Chiang, David. 2007. Hierarchical phrase based trans-
lation. Computational Linguistics, 33(2).
Chiang, David. 2010. Learning to translate with
source and target syntax. In Proceedings of the
48th Annual Meeting of the Association for Com-
putational Linguistics, pages 1443?1452, Uppsala,
Sweden, July. Association for Computational Lin-
guistics.
Galley, Michael, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a translation rule?
In Proceedings of the Human Language Technology
Conference of the North American Chapter of the
Association for Computational Linguistics Confer-
ence (HLT/NAACL).
Huang, Liang and David Chiang. 2007. Forest rescor-
ing: Faster decoding with integrated language mod-
els. In Proceedings of the Annual Meeting of the
Association for Computational Linguistics (ACL).
Kasami, T. 1965. An efficient recognition
and syntax-analysis algorithm for context-free lan-
guages. Technical report, Air Force Cambridge Re-
search Lab.
Klein, Dan and Christoper Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of the Annual
Meeting of the Association for Computational Lin-
guistics (ACL).
Koehn, Philipp, Franz J. Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the Human Language Technology Con-
ference of the North American Chapter of the As-
sociation for Computational Linguistics Conference
(HLT/NAACL).
Liu, Yang, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment template for statistical machine
translation. In Proceedings of the 21st International
Conference on Computational Linguistics and the
44th annual meeting of the Association for Compu-
tational Linguistics.
Marcu, Daniel, Wei Wang, Abdessamad Echihabi,
and Kevin Knight. 2006. SPMT: Statistical ma-
chine translation with syntactified target language
phrases. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP), Sydney, Australia.
116
Och, Franz J. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of
the Annual Meeting of the Association for Compu-
tational Linguistics (ACL).
Papineni, Kishore, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2002. BLEU: a method for auto-
matic evaluation of machine translation. In Pro-
ceedings of the Annual Meeting of the Association
for Computational Linguistics (ACL).
Petrov, Slav, Aria Haghighi, and Dan Klein. 2008.
Coarse-to-fine syntactic machine translation using
language projections. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP).
Steedman, Mark. 2000. The Syntactic Process. MIT
Press.
Venugopal, Ashish and Andreas Zollmann. 2009.
Grammar based statistical MT on Hadoop: An end-
to-end toolkit for large scale PSCFG based MT.
The Prague Bulletin of Mathematical Linguistics,
91:67?78.
Venugopal, Ashish, Andreas Zollmann, and Stephan
Vogel. 2007. An efficient two-pass approach to
synchronous-CFG driven statistical MT. In Pro-
ceedings of the Human Language Technology Con-
ference of the North American Chapter of the As-
sociation for Computational Linguistics Conference
(HLT/NAACL).
Welford, B. P. 1962. Note on a method for calculating
corrected sums of squares and products. Techno-
metrics, 4(3):419?420.
West, D. H. D. 1979. Updating mean and variance
estimates: an improved method. Commun. ACM,
22(9):532?535.
Zollmann, Andreas and Ashish Venugopal. 2006.
Syntax augmented machine translation via chart
parsing. In Proceedings of the Workshop on Sta-
tistical Machine Translation, HLT/NAACL.
Zollmann, Andreas, Ashish Venugopal, Franz J. Och,
and Jay Ponte. 2008. A systematic comparison
of phrase-based, hierarchical and syntax-augmented
statistical MT. In Proceedings of the Conference on
Computational Linguistics (COLING).
117
